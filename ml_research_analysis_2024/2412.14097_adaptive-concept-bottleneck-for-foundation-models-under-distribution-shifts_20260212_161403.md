---
ver: rpa2
title: Adaptive Concept Bottleneck for Foundation Models Under Distribution Shifts
arxiv_id: '2412.14097'
source_url: https://arxiv.org/abs/2412.14097
tags:
- concept
- distribution
- shifts
- foundation
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of making foundation model-based
  classifiers interpretable and robust under distribution shifts at test time. It
  introduces an adaptive concept bottleneck framework (CONDA) that dynamically adjusts
  the concept vectors and prediction layer using unlabeled test data.
---

# Adaptive Concept Bottleneck for Foundation Models Under Distribution Shifts

## Quick Facts
- arXiv ID: 2412.14097
- Source URL: https://arxiv.org/abs/2412.14097
- Reference count: 40
- Key outcome: CONDA improves test-time accuracy by up to 28% and provides better-aligned concept-based explanations across various distribution shifts

## Executive Summary
This paper introduces CONDA, an adaptive concept bottleneck framework that dynamically adjusts concept vectors and prediction layers using unlabeled test data to make foundation model-based classifiers interpretable and robust under distribution shifts. CONDA addresses three key failure modes of standard concept bottlenecks: non-robust concepts under low-level shifts, non-robust classifiers under concept-level shifts, and incomplete concept sets. The method adapts in three stages—concept-score alignment, linear probing adaptation, and residual concept bottleneck—and empirically achieves significant accuracy improvements while maintaining interpretability across various distribution shifts.

## Method Summary
CONDA adapts a concept bottleneck model (CBM) based on a pre-trained foundation model to improve test-time robustness under distribution shifts using only unlabeled test data. The framework consists of three adaptation stages: Concept-Score Alignment (CSA) that aligns class-conditional concept score distributions between source and target domains using Mahalanobis distance; Linear Probing Adaptation (LPA) that adapts the label predictor while keeping concept vectors fixed; and Residual Concept Bottleneck (RCB) that introduces additional concept vectors to capture domain-specific information missed by the initial concept set. Pseudo-labeling based on an ensemble of zero-shot and linear probing predictors provides supervision for adaptation without requiring source data or test labels.

## Key Results
- CONDA improves test-time accuracy by up to 28% compared to unadapted CBMs across various distribution shifts
- The method matches or exceeds non-interpretable baselines in accuracy while providing interpretable concept-based explanations
- On CIFAR10-C and CIFAR100-C, CONDA significantly outperforms zero-shot and linear probing baselines under low-level distribution shifts
- On Waterbirds and Metashift datasets, CONDA improves worst-group accuracy while maintaining interpretability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CONDA adapts concept vectors using intra-class and inter-class Mahalanobis distances to maintain prediction consistency across domains.
- Mechanism: By minimizing the intra-class distance and maximizing the inter-class distance of concept scores in the target domain, CONDA aligns the concept-score distribution with that of the source domain.
- Core assumption: The class-conditional concept score distributions in the source domain can be approximated by multivariate Gaussians, and their parameters (mean and covariance) are known or can be estimated.
- Evidence anchors:
  - [abstract] Empirical evaluations with various real-world distribution shifts show that our adaptation method produces concept-based interpretations better aligned with the test data and boosts post-deployment accuracy by up to 28%, aligning the CBM performance with that of non-interpretable classification.
  - [section 3.1] We model the class-conditional distributions of the concept scores in the source domain as multivariate Gaussians: P(vCs(xs) | ys = y) = N (vCs(xs) ; µy, Σy), ∀y ∈ Y. Given a labeled source-domain dataset, it is straight-forward to estimate µy and Σy using the sample mean and sample covariance ofvCs(xs) on the data subset from class y (max-likelihood estimate).

### Mechanism 2
- Claim: The residual concept bottleneck captures domain-specific concepts missed by the initial concept set, improving test-time accuracy.
- Mechanism: CONDA introduces additional concept vectors in a residual branch, which are jointly optimized with a linear predictor to improve accuracy on the target dataset beyond the CSA and LPA steps.
- Core assumption: The initial concept set is incomplete for the target domain, and new concepts can be discovered through optimization on unlabeled test data.
- Evidence anchors:
  - [abstract] The method adapts in three stages: concept-score alignment, linear probing adaptation, and residual concept bottleneck. Empirically, CONDA improves test-time accuracy by up to 28% and provides better-aligned concept-based explanations across various distribution shifts, matching or exceeding non-interpretable baselines.
  - [section 3.3] As discussed in Section 2.3, the concept bank from the source domain could be incomplete, and new concepts may be required to bridge the distribution gap between the domains. In this step, we introduce a residual CBM with additional concept vectors and a linear predictor, which are jointly optimized (with the parameters of the main CBM fixed) to improve the test accuracy.

### Mechanism 3
- Claim: Pseudo-labeling based on an ensemble of zero-shot and linear probing predictors provides reliable supervision for adaptation on unlabeled test data.
- Mechanism: CONDA uses the class predicted with higher confidence across both zero-shot and linear probing predictors as pseudo-labels for the test samples, enabling adaptation without access to source data or test labels.
- Core assumption: The foundation model is relatively robust to distribution shifts, and the ensemble of zero-shot and linear probing predictors provides reasonably accurate pseudo-labels for adaptation.
- Evidence anchors:
  - [section 3] We utilize the idea of pseudo-labeling to address this, as commonly done in the TTA and semi-supervised learning literature. A simple approach for pseudo-labeling the test set is to use the class predictions of the (un-adapted) source-domain CBM, referred to as "self-labeling". However, since this CBM is often not robust to distribution shifts in the first place, this can produce poor-quality pseudo-labels for adaptation. We leverage the fact that the feature extraction backbone ϕ(x) is a foundation model that is pre-trained on diverse data distributions, and as a result is likely to be relatively robust to distribution shifts.
  - [section 4.2] We observe that leveraging the expressive power of the FM feature representations can enhance the performance of CBMs. For example, using the method from Oikarinen et al. (2023), their reported accuracies on CIFAR10 and CIFAR100 are 86.40% and 65.13% respectively when using the CLIP-RN50 backbone. In our experiments, by employing the adversarially fine-tuned CLIP-ViT-L/14, we achieve higher accuracies of 95.24% and 68.36% respectively (source domain).

## Foundational Learning

- Concept: Concept Bottleneck Models (CBMs)
  - Why needed here: CBMs provide interpretable predictions by mapping high-dimensional features to human-understandable concepts before making class predictions. Understanding CBMs is crucial for grasping the problem setting and the proposed adaptation framework.
  - Quick check question: What is the main advantage of using CBMs over standard deep classifiers in terms of interpretability?

- Concept: Distribution Shifts
  - Why needed here: The paper focuses on adapting CBMs under distribution shifts at test time, where the input distribution differs from the training distribution. Understanding different types of distribution shifts (low-level, concept-level, natural) is essential for comprehending the failure modes and adaptation strategies.
  - Quick check question: What are the two main types of distribution shifts discussed in the paper, and how do they affect the concept score distributions?

- Concept: Test-Time Adaptation (TTA)
  - Why needed here: CONDA is a test-time adaptation framework that adapts CBMs using only unlabeled test data without access to the source dataset. Familiarity with TTA concepts and techniques is necessary for understanding the adaptation objectives and the pseudo-labeling approach.
  - Quick check question: What is the key challenge in test-time adaptation, and how does CONDA address it using pseudo-labeling?

## Architecture Onboarding

- Component map: Input -> Foundation Model (ϕ) -> Main CBM Branch (C, W, b) -> Residual CBM Branch (eC, fW, eb) -> Pseudo-labeling Module -> CSA/LPA/RCB Adaptation Modules -> Output

- Critical path:
  1. Foundation model extracts features from input.
  2. Main CBM branch projects features to concept scores and makes initial predictions.
  3. Residual CBM branch captures additional concepts and refines predictions.
  4. Pseudo-labeling module generates supervision for adaptation.
  5. CSA, LPA, and RCB modules adapt CBM parameters to improve target domain performance.

- Design tradeoffs:
  - Interpretability vs. Accuracy: CBMs prioritize interpretability but may sacrifice accuracy compared to non-interpretable models. CONDA aims to bridge this gap by adapting CBMs to improve test-time accuracy while preserving interpretability.
  - Adaptation Complexity vs. Performance: CONDA employs multiple adaptation stages (CSA, LPA, RCB) to address different failure modes, but this increases computational complexity. Balancing the number of adaptation steps and their impact on performance is crucial.
  - Pseudo-labeling Quality vs. Adaptation Reliability: The quality of pseudo-labels directly affects the reliability of adaptation. Using an ensemble of zero-shot and linear probing predictors aims to provide robust pseudo-labels, but more advanced techniques could further improve adaptation performance.

- Failure signatures:
  - Non-robust concept bottleneck: Significant drop in test-time accuracy under low-level shifts, with concept scores not aligning with source domain distributions.
  - Non-robust classifier: Performance degradation under concept-level shifts, where the classifier fails to produce consistent predictions across domains despite aligned concept scores.
  - Incomplete concept set: Limited improvement in test-time accuracy even after CSA and LPA adaptation, indicating the need for additional concepts to capture domain-specific information.

- First 3 experiments:
  1. Evaluate CONDA on CIFAR10-C and CIFAR100-C datasets to assess its effectiveness in handling low-level distribution shifts and improving test-time accuracy.
  2. Test CONDA on Waterbirds and Metashift datasets to evaluate its performance in adapting to concept-level distribution shifts and improving worst-group accuracy.
  3. Assess CONDA's interpretability by analyzing how the concept weights change before and after adaptation on the Waterbirds dataset, and how the residual concepts capture domain-specific information.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can theoretical guarantees be established for the sufficiency of a concept set from the source domain for robust test-time accuracy under different distribution shifts?
- Basis in paper: [explicit] The paper identifies three failure modes of CBMs under distribution shifts and proposes CONDA to address them. However, it acknowledges that a deeper theoretical understanding of concept bottlenecks under distribution shifts is an important direction for future research.
- Why unresolved: The paper provides empirical evidence of CONDA's effectiveness but does not offer theoretical bounds or characterizations of concept set sufficiency.
- What evidence would resolve it: A theoretical framework that characterizes the conditions under which a concept set is sufficient for robust performance under specific types of distribution shifts, along with formal proofs or bounds.

### Open Question 2
- Question: Can the performance gap between CONDA with refined pseudo-labeling and perfect pseudo-labeling be further reduced for challenging distribution shifts?
- Basis in paper: [explicit] The paper acknowledges that refined pseudo-labeling techniques like Chen et al. (2022) improve performance but still leave a significant gap compared to perfect pseudo-labeling, especially for CIFAR-100.
- Why unresolved: The paper uses a specific pseudo-labeling method and shows its limitations, but does not explore or propose methods to close the remaining gap.
- What evidence would resolve it: Development and evaluation of advanced pseudo-labeling techniques that can handle both low-level and concept-level shifts more effectively, reducing the performance gap to near-perfect pseudo-labeling.

### Open Question 3
- Question: How does the choice of foundation model affect the interpretability and robustness of CONDA, and can this be optimized?
- Basis in paper: [explicit] The paper demonstrates that the choice of foundation model (e.g., BiomedCLIP vs. MedCLIP) significantly impacts the performance of CONDA, suggesting that selecting an appropriate backbone is crucial.
- Why unresolved: While the paper shows the impact of different foundation models, it does not explore how to systematically optimize or select the best foundation model for a given task or distribution shift scenario.
- What evidence would resolve it: A study that evaluates the performance of CONDA across a wide range of foundation models, identifies key factors that influence interpretability and robustness, and proposes methods for selecting or fine-tuning foundation models to maximize CONDA's effectiveness.

## Limitations

- The empirical validation is primarily limited to image classification datasets with CLIP-based foundation models, leaving effectiveness on other modalities unexplored
- The method relies on Gaussian assumptions for concept score distributions, which may not hold in all scenarios
- Pseudo-label quality, critical for adaptation, can degrade significantly under severe distribution shifts

## Confidence

- **High Confidence**: The three-stage adaptation framework (CSA, LPA, RCB) is technically sound and addresses well-defined failure modes of standard CBMs. The pseudo-labeling approach is consistent with established TTA literature.
- **Medium Confidence**: The empirical improvements (up to 28% accuracy gains) are demonstrated across multiple datasets, but the statistical significance and robustness across different foundation models need further validation.
- **Low Confidence**: The interpretability claims, particularly how residual concepts align with human-understandable concepts, lack quantitative evaluation and could be subjective.

## Next Checks

1. Test CONDA on non-vision foundation models (e.g., text-to-text models) to verify cross-modal effectiveness.
2. Conduct ablation studies removing each adaptation stage to quantify individual contributions and potential redundancy.
3. Evaluate CONDA under severe distribution shifts where pseudo-label quality degrades significantly to test the method's robustness limits.