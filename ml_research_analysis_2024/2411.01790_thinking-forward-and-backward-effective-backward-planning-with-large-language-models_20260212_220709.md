---
ver: rpa2
title: 'Thinking Forward and Backward: Effective Backward Planning with Large Language
  Models'
arxiv_id: '2411.01790'
source_url: https://arxiv.org/abs/2411.01790
tags:
- node
- nodes
- planning
- points
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper examines whether large language models (LLMs) can exploit\
  \ inherent asymmetries in planning problems by reasoning in the backward direction,\
  \ such as when a \"bottleneck\" near the goal makes backward planning easier. Experiments\
  \ in three domains\u2014graph planning, array transformation, and blocksworld\u2014\
  show that while LLM planning success correlates with problem complexity in a given\
  \ direction, LLMs consistently perform worse when planning backward due to systematic\
  \ biases."
---

# Thinking Forward and Backward: Effective Backward Planning with Large Language Models

## Quick Facts
- arXiv ID: 2411.01790
- Source URL: https://arxiv.org/abs/2411.01790
- Reference count: 19
- Key outcome: LLM planning success correlates with problem complexity in a given direction, but LLMs consistently perform worse when planning backward due to systematic biases. The proposed flipping approach improves planning success rates by 4-24% compared to forward-only planning.

## Executive Summary
This paper investigates whether large language models (LLMs) can effectively exploit asymmetries in planning problems by reasoning in the backward direction. The authors find that while LLM planning performance correlates with problem complexity in a given direction, LLMs exhibit a systematic bias against backward planning. To address this, they propose flipping the problem so the original goal becomes the new initial state, then planning forward in the flipped problem. Combined with self-verification and sampling in both directions, this approach significantly improves planning success rates across three domains: graph planning, array transformation, and blocksworld.

## Method Summary
The authors evaluate LLM planning in three domains by comparing forward and backward planning performance. They use pre-trained LLMs (GPT-4o, GPT-3.5-turbo, GPT-4-turbo) to generate candidate plans via sampling, with self-verification to select final solutions. The key innovation is the "flipping" approach: when backward planning is deemed easier, the problem is transformed such that the original goal becomes the new initial state, allowing the LLM to plan forward in the transformed space. The method also includes prompting the LLM to reason about which direction (original or flipped) is easier for a given problem. Experiments systematically compare different planning directions and the flipping approach across all three domains.

## Key Results
- LLM planning success correlates with problem complexity in a given direction, but LLMs consistently perform worse when planning backward
- The proposed flipping approach improves planning success rates by 4-24% compared to forward-only planning
- LLMs can be prompted to reason which direction (original or flipped) is easier, achieving higher success rates when correctly choosing the direction

## Why This Works (Mechanism)

### Mechanism 1
LLM planning performance correlates with the computational complexity of the problem in a given direction (forward vs. backward). The LLM implicitly uses a forward-search-like process during planning, so its success rate mirrors the number of search steps required by classical algorithms like BFS in that direction. This occurs because LLMs generate plans step-by-step in a left-to-right autoregressive manner, making them inherently biased toward forward reasoning.

### Mechanism 2
LLMs exhibit systematic bias against backward planning due to their forward autoregressive nature and training data bias. The left-to-right generation process and training data distribution make LLMs less effective at generating correct steps when reasoning from goal to initial state. This bias likely stems from training data containing more examples of forward reasoning (problem → solution) than backward reasoning (solution → problem).

### Mechanism 3
Flipping the problem (making original goal the new initial state) allows LLMs to avoid backward bias by planning forward in the transformed problem space. By transforming the problem such that the easier direction becomes the new "forward" direction, the LLM can exploit problem asymmetries while avoiding its inherent backward reasoning weakness.

## Foundational Learning

- **Concept: Graph planning and state space representation**
  - Why needed here: The work operates on graph-based planning problems where understanding nodes, edges, and state transitions is fundamental
  - Quick check question: What is the difference between directed and undirected graphs in terms of edge traversal during planning?

- **Concept: Bidirectional search algorithms**
  - Why needed here: The paper's insights are inspired by classical bidirectional search, comparing forward vs backward search efficiency
  - Quick check question: Why is bidirectional search often more efficient than unidirectional search in classical planning?

- **Concept: Large language model autoregressive generation**
  - Why needed here: Understanding how LLMs generate text sequentially from left to right is crucial for explaining the backward planning bias
  - Quick check question: How does the autoregressive nature of LLMs affect their ability to plan in reverse order?

## Architecture Onboarding

- **Component map**: Problem representation module (text-based graph/array/blocksworld encoding) -> LLM planning engine (GPT-4o or similar) -> Self-verification component (step-by-step plan validation) -> Direction selection module (original vs flipped problem) -> Solution aggregator (multiple candidate plans with final selection)

- **Critical path**: Problem → LLM Sampling → Self-verification → Direction selection → Final plan generation → Success evaluation

- **Design tradeoffs**: Single vs multiple planning attempts (tradeoff between computation cost and success rate); Temperature settings (0 for deterministic first attempt, 0.5 for diversity in subsequent attempts); Self-verification complexity vs planning speed

- **Failure signatures**: Consistent failure in backward direction regardless of problem difficulty; Self-verification errors that incorrectly validate wrong plans; State-dependent action space transformation errors when flipping problems

- **First 3 experiments**: 1) Replicate forward vs backward planning success rate comparison on simple undirected graphs; 2) Test flipping approach on directed graph planning problems; 3) Evaluate self-verification reliability across different LLM capabilities (GPT-3.5, GPT-4, GPT-4o)

## Open Questions the Paper Calls Out

### Open Question 1
What are the fundamental reasons LLMs consistently perform worse when planning backward, and can this bias be systematically mitigated through fine-tuning or architectural modifications? The authors note that LLMs exhibit a systematic bias of performing worse when planning backward, conjecturing this may be due to the forward autoregressive nature of LLM output generation and training dataset biases. This remains unresolved as the paper does not explore root causes in depth or test whether fine-tuning with backward reasoning traces or architectural changes could address it.

### Open Question 2
How does the effectiveness of flipping problems vary across different types of planning problems with more complex state spaces or longer horizons? The current experiments focus on relatively simple planning domains (graph planning, array transformation, blocksworld), but the authors suggest the framework could be extended to more complex reasoning problems. This remains unresolved as the paper does not test the flipping approach on domains with significantly larger state spaces, longer planning horizons, or continuous state spaces.

### Open Question 3
Can LLMs be trained to reliably self-verify their own plans, and how does self-verification accuracy impact the overall planning success rate? The authors note that the reliability of self-verification affects performance, observing that better LLMs reduce self-verification errors, and that GPT-3.5-turbo always self-verifies its plans as correct, leading to zero success rate. This remains unresolved as the paper does not investigate whether LLMs can be improved at self-verification through training or prompting strategies.

## Limitations
- The study focuses on structured planning domains where clear problem transformations are possible, limiting generalizability to more complex, real-world planning scenarios
- The backward bias mechanism relies heavily on assumptions about LLM autoregressive generation and training data distribution that are not directly verified
- Self-verification success rates are not extensively analyzed, leaving open questions about reliability across different problem types

## Confidence

**High confidence**: Core finding that LLMs exhibit systematic backward planning bias (supported by consistent experimental results across three domains)

**Medium confidence**: Mechanism explanation linking LLM autoregressive nature to backward bias (based on indirect evidence and reasonable assumptions)

**Medium confidence**: Flipping approach effectiveness (validated empirically but relies on correct problem transformation implementation)

## Next Checks

1. Conduct ablation studies removing self-verification to quantify its contribution to overall success rate improvements
2. Test whether fine-tuning LLMs with balanced forward/backward reasoning examples reduces the observed backward bias
3. Evaluate the flipping approach on more complex planning domains with larger state spaces to test scalability limits