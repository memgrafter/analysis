---
ver: rpa2
title: 'Better Not to Propagate: Understanding Edge Uncertainty and Over-smoothing
  in Signed Graph Neural Networks'
arxiv_id: '2408.04895'
source_url: https://arxiv.org/abs/2408.04895
tags:
- signed
- graph
- neural
- arxiv
- edge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the over-smoothing problem in signed graph
  neural networks by introducing a dynamic edge classification error ratio into the
  theoretical analysis. Unlike prior work assuming fixed error ratios, the authors
  show that message propagation may degrade separability under high edge classification
  error, making blocking more effective.
---

# Better Not to Propagate: Understanding Edge Uncertainty and Over-smoothing in Signed Graph Neural Networks

## Quick Facts
- arXiv ID: 2408.04895
- Source URL: https://arxiv.org/abs/2408.04895
- Reference count: 10
- Authors: Yoonhyuk Choi, Jiho Choi, Taewook Ko, Chong-Kwon Kim
- Primary result: Dynamic edge classification error ratio improves signed GNN performance on heterophilic graphs

## Executive Summary
This paper investigates over-smoothing in signed graph neural networks by introducing dynamic edge classification error ratio into theoretical analysis. Unlike prior work assuming fixed error ratios, the authors show message propagation may degrade separability under high edge classification error, making blocking more effective. They propose estimating homophily and error ratio during training using validation scores, and adaptively selecting between signed and blocked message passing. Theoretical analysis proves their method reduces smoothing in sub-stochastic matrices and improves discrimination power. Extensive experiments on multiple datasets demonstrate significant performance improvements over state-of-the-art signed GNNs, particularly on heterophilic graphs.

## Method Summary
The method introduces dynamic edge classification error ratio estimation during training, using validation accuracy to estimate edge error ratio (et) and a weighted combination of MLP and EvenNet features to estimate homophily (bi). During each training step, the method computes Zt = 1 - bi - et for each edge and decides whether to use signed propagation (if Zt ≥ 0) or block the edge (if Zt < 0). This adaptive selection aims to prevent over-smoothing by avoiding propagation through uncertain edges. The approach is integrated into standard GCN-like architectures with an additional edge calibration layer between adjacency normalization and message passing.

## Key Results
- Dynamic edge classification error ratio improves signed GNN performance on heterophilic graphs
- Method achieves up to 9.88% relative accuracy gains on the Actor dataset
- Extensive experiments show consistent improvements over state-of-the-art signed GNNs across multiple datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Edge classification error degrades separability when message-passing is used, making blocking more effective.
- Mechanism: When signed propagation incorrectly classifies edge signs (high error ratio), the resulting sub-stochastic matrix still propagates information, which can mix dissimilar node features and reduce inter-class distance. Blocking these uncertain edges prevents such harmful mixing.
- Core assumption: Edge classification error is non-negligible and varies during training, so fixed assumptions about perfect edge sign assignment fail.
- Evidence anchors:
  - [abstract] "message propagation may degrade separability under high edge classification error, making blocking more effective"
  - [section] "Under a high error ratio ( e = 1 , initial stage of training), it might be better to not propagate rather than using signed edges since −bik ≤ 0 (Es ≤ Eb)"
  - [corpus] "A Signed Graph Approach to Understanding and Mitigating Oversmoothing in GNNs" (weak direct evidence for this specific error ratio claim)
- Break condition: If edge error ratio can be driven to near zero consistently, signed propagation will dominate blocking.

### Mechanism 2
- Claim: Dynamic selection between signed and blocked propagation based on estimated homophily and error ratio improves performance.
- Mechanism: During training, the method estimates current edge classification error and local homophily ratio. If the product of these factors yields a negative value for the separability gain metric Z = 1 - bi - et, it switches to blocked propagation for those edges, otherwise uses signed propagation. This adapts to changing training dynamics.
- Core assumption: Validation scores can reliably estimate edge classification error and homophily in a semi-supervised setting.
- Evidence anchors:
  - [abstract] "propose estimating homophily and error ratio during training using validation scores, and adaptively selecting between signed and blocked message passing"
  - [section] "we employ an EM algorithm, where the E-step estimates the homophily (bi) and edge error ratio ( et), followed by the M-step for optimization"
  - [corpus] "Uncertainty Propagation in XAI: A Comparison of Analytical and Empirical Estimators" (weak direct evidence for validation-based estimation)
- Break condition: If validation sets are too small or unrepresentative, estimation errors could mislead the selection mechanism.

### Mechanism 3
- Claim: Blocking negative edges when Z < 0 reduces over-smoothing by avoiding convergence of sub-stochastic components.
- Mechanism: By forcing some negative edges to zero weight, the method creates multiple disconnected sub-stochastic components. This prevents the infinite product of sub-stochastic matrices from converging to a single vector, thus preserving node representation diversity.
- Core assumption: Multiple disconnected sub-stochastic components maintain multiple eigenvalues on the unit circle, avoiding over-smoothing.
- Evidence anchors:
  - [section] "we prove that signed MP also converges in the following theorem. Nonetheless, we show that calibrating the negative edges below can mitigate the smoothing effect."
  - [section] "Theorem 16 (Reduced smoothing in k-disconnected components). Based on Corollary 15, we can infer that a matrix with k-disconnected sub-stochastic components satisfies |λ1| = |λ2| = · · · = |λk| = 1."
  - [corpus] "Sparse Bayesian Message Passing under Structural Uncertainty" (weak direct evidence for this specific eigenvalue argument)
- Break condition: If the graph structure naturally yields few or no disconnected components, this benefit may be minimal.

## Foundational Learning

### Concept: Sub-stochastic matrices and their spectral properties
- Why needed here: The method relies on sub-stochastic matrices (signed propagation) and their convergence behavior. Understanding why they can still over-smooth, and how disconnected components help, is essential.
- Quick check question: What is the spectral radius condition for a sub-stochastic matrix to guarantee convergence to a single vector?

### Concept: Contextual Stochastic Block Models (CSBM)
- Why needed here: The theoretical analysis of separability gain under different propagation schemes uses CSBM to model node feature distributions and class structure.
- Quick check question: In a binary CSBM, how does the expected feature after one hop depend on homophily and class balance?

### Concept: Hoeffding's inequality and concentration bounds
- Why needed here: The method estimates edge error ratio using validation accuracy; Hoeffding's inequality justifies why a small validation set can approximate the true error ratio.
- Quick check question: What does Hoeffding's inequality say about the probability that the empirical error deviates from the true error by more than ε?

## Architecture Onboarding

### Component map
Adjacency matrix normalization -> Edge calibration layer (homophily estimation + error estimation + calibration decision) -> GNN message-passing layers

### Critical path
At each training step: compute validation accuracy → estimate et → estimate bi → compute Zt for each edge → decide sign or block → apply calibrated adjacency matrix to GNN → update parameters

### Design tradeoffs
The method adds computation for estimation and decision per edge, but avoids unnecessary smoothing. Tuning the sensitivity of Zt thresholds may be needed for different datasets.

### Failure signatures
If the method consistently chooses blocking even when signed propagation is better, it may indicate poor estimation of bi or et. If validation accuracy is noisy, calibration decisions may flip erratically.

### First 3 experiments
1. Verify homophily estimation matches ground truth on synthetic graphs with known bi.
2. Test edge error estimation on a small labeled validation set with known errors.
3. Apply the full calibration pipeline on a simple heterophilic dataset (e.g., Actor) and measure accuracy lift over base signed GNN.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what precise conditions does blocking message propagation outperform signed propagation, and how can we theoretically quantify the threshold edge error ratio where this transition occurs?
- Basis in paper: [explicit] The paper proves that signed propagation may degrade separability under high edge classification error, while blocking can be more effective, but doesn't provide a precise theoretical threshold for when this transition occurs
- Why unresolved: The paper establishes that blocking can outperform signed propagation when Zt < 0, but doesn't provide a precise mathematical characterization of the conditions (in terms of homophily, error ratio, and graph structure) where this transition occurs
- What evidence would resolve it: A theoretical proof deriving exact conditions or a precise formula for the threshold edge error ratio where blocking becomes superior to signed propagation, validated across diverse graph structures

### Open Question 2
- Question: How does the proposed dynamic edge classification error estimation method perform on graphs with highly non-i.i.d. node distributions or temporal graphs where error ratios change over time?
- Basis in paper: [inferred] The paper mentions that validation node distributions may not be i.i.d. in benchmark graphs, and the estimation method relies on validation scores, but doesn't extensively test non-i.i.d. or temporal scenarios
- Why unresolved: The empirical analysis only briefly mentions minor differences in i.i.d. vs non-i.i.d. cases, and no experiments or theoretical analysis are provided for temporal graphs or highly non-i.i.d. distributions
- What evidence would resolve it: Experiments and theoretical analysis on temporal graphs showing error ratio evolution, and performance comparison on graphs with known non-i.i.d. distributions

### Open Question 3
- Question: What is the computational complexity of the proposed method compared to standard GNNs, and how does it scale with graph size and number of classes?
- Basis in paper: [inferred] The paper applies the method to large graphs in Table 3 but doesn't provide computational complexity analysis or runtime comparisons with baselines
- Why unresolved: While the paper demonstrates effectiveness on large graphs, it doesn't analyze the computational overhead of the dynamic edge selection mechanism or provide theoretical complexity bounds
- What evidence would resolve it: Detailed computational complexity analysis, runtime comparisons with baselines across different graph sizes and class numbers, and memory usage measurements

## Limitations

- The method's performance heavily depends on accurate estimation of edge classification error ratio and homophily during training. If validation sets are small or unrepresentative, estimation errors could mislead the selection mechanism and degrade performance.
- The theoretical analysis assumes a Contextual Stochastic Block Model framework, which may not perfectly capture real-world graph structures, particularly those with complex heterophily patterns.

## Confidence

- **High**: The core mechanism that edge classification errors can degrade separability and that blocking uncertain edges can prevent harmful mixing is well-supported by theoretical analysis and experimental results.
- **Medium**: The dynamic selection strategy based on validation scores is theoretically sound but requires careful tuning of estimation thresholds, which may vary across datasets.
- **Medium**: The claim about reducing over-smoothing through disconnected sub-stochastic components is supported by spectral analysis, but real-world benefits depend on graph structure.

## Next Checks

1. Conduct ablation studies to isolate the impact of edge error estimation accuracy on final performance, varying validation set sizes.
2. Test the method on graphs with known heterophily patterns to verify that the dynamic selection mechanism correctly identifies when to switch between signed and blocked propagation.
3. Analyze the spectral properties of calibrated adjacency matrices across different datasets to confirm that disconnected components are created when Zt < 0 conditions are met.