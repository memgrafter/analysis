---
ver: rpa2
title: 'Anchoring Bias in Large Language Models: An Experimental Study'
arxiv_id: '2412.06593'
source_url: https://arxiv.org/abs/2412.06593
tags:
- anchoring
- bias
- llms
- question
- hints
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study examines anchoring bias in LLMs using a dataset with
  62 questions across various domains. It finds that stronger models like GPT-4 and
  GPT-4o are more consistently influenced by anchoring hints, especially expert opinions,
  while weaker models like GPT-3.5 show higher variability.
---

# Anchoring Bias in Large Language Models: An Experimental Study

## Quick Facts
- arXiv ID: 2412.06593
- Source URL: https://arxiv.org/abs/2412.06593
- Reference count: 33
- Primary result: Stronger LLMs show more consistent anchoring bias, while simple mitigation strategies prove insufficient

## Executive Summary
This study examines anchoring bias in large language models (LLMs) through systematic experiments with 62 questions across multiple domains. The researchers find that stronger models like GPT-4 and GPT-4o consistently exhibit anchoring bias, particularly when influenced by expert opinions, while weaker models show higher variability. The study tests four common mitigation strategies (Chain-of-Thought, Principles of Thought, Ignoring Anchor Hints, and Reflection) but finds them ineffective. The key recommendation is to collect hints from comprehensive angles to prevent LLMs from being anchored to individual pieces of information.

## Method Summary
The study uses a dataset of 62 questions with three hints per question across various domains (weather, stock prices, upvotes, etc.). Experiments are conducted with GPT-4, GPT-4o, and GPT-3.5 Turbo using temperature 0.8 and 30 runs per question. The methodology compares LLM responses under different hint conditions (Control, Treatment A with H1+H2, Treatment B with H1+H3) using t-tests to measure statistical significance. Numerical answers are extracted from JSON responses and converted to decimal format for analysis.

## Key Results
- GPT-4 and GPT-4o models are more consistently influenced by anchoring hints than GPT-3.5
- Expert opinion anchors create stronger anchoring effects than factual anchors
- Simple mitigation strategies (CoT, PoT, Ignoring Anchors, Reflection) are insufficient to reduce anchoring bias
- Collecting hints from comprehensive angles is more effective than single-anchor approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Anchoring bias manifests when initial information disproportionately influences final numerical judgments
- Mechanism: LLMs use initial hints as reference points that are insufficiently adjusted away from during answer generation
- Core assumption: LLMs learn patterns where initial information serves as contextual anchors for subsequent reasoning
- Evidence anchors: Abstract states anchoring bias involves initial information disproportionately influencing judgment; experiments show comprehensive hint collection prevents single-anchor fixation
- Break condition: When multiple diverse hints are provided simultaneously, preventing single-anchor fixation

### Mechanism 2
- Claim: Stronger LLMs show more consistent anchoring effects due to more stable output generation
- Mechanism: GPT-4 and GPT-4o produce highly consistent answers with small variations, making anchoring effects more detectable
- Core assumption: Model strength correlates with output stability, reducing random noise that might obscure bias patterns
- Evidence anchors: Section shows GPT-4 and GPT-4o are more consistently influenced by anchoring hints; weaker models show higher variability
- Break condition: When model parameters or inference settings introduce sufficient randomness

### Mechanism 3
- Claim: Expert opinions create stronger anchoring effects than factual anchors
- Mechanism: LLMs treat expert-provided hints as authoritative information, giving them disproportionate weight in reasoning
- Core assumption: Training data contains patterns where expert opinions are treated as reliable reference points
- Evidence anchors: Section states GPT models are biased by expert anchors with high confidence; dual expert anchors can cancel bias
- Break condition: When expert status is explicitly questioned or when multiple conflicting expert opinions are presented

## Foundational Learning

- Concept: Statistical significance testing (t-tests)
  - Why needed here: To determine whether differences between answer distributions are meaningful or due to random variation
  - Quick check question: If p-value = 0.03, can we reject the null hypothesis at 5% significance level?

- Concept: Experimental design with control and treatment groups
  - Why needed here: To isolate the effect of anchoring hints by comparing responses with and without these hints
  - Quick check question: What would be the control condition in an anchoring bias experiment?

- Concept: JSON data parsing and numerical conversion
  - Why needed here: To extract and standardize numerical answers from LLM responses for statistical analysis
  - Quick check question: How would you convert "2:00" in HH:MM format to decimal minutes?

## Architecture Onboarding

- Component map: Dataset -> Prompt generation -> LLM API call -> Response parsing -> Statistical analysis -> Result interpretation
- Critical path: Question → Prompt generation → LLM API call → Response parsing → Statistical analysis → Result interpretation
- Design tradeoffs: Temperature setting affects variability vs bias detection; more repetitions improve power but increase cost
- Failure signatures: High standard deviation suggests model instability; inconsistent p-values indicate experimental design issues
- First 3 experiments: 1) Single factual anchor vs control to establish baseline; 2) Expert opinion anchor vs control to compare strength; 3) Both-anchor strategy vs single anchor to evaluate mitigation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do stronger models like GPT-4 and GPT-4o show more consistent anchoring bias compared to weaker models like GPT-3.5?
- Basis in paper: Explicit - The study observes stronger models consistently show anchoring bias while weaker models show higher variability
- Why unresolved: The paper doesn't explain the underlying mechanisms that make stronger models more susceptible to anchoring bias or why they show more consistent behavior
- What evidence would resolve it: Systematic experiments comparing attention mechanisms, parameter configurations, and training data influences between strong and weak models

### Open Question 2
- Question: What specific architectural or training factors make LLMs more susceptible to expert opinion anchoring versus factual anchoring?
- Basis in paper: Explicit - The study found LLMs are "much easier to be biased by expert anchors" and show strong adherence to PTF expert predictions
- Why unresolved: The paper doesn't investigate the root causes of why expert opinions have stronger anchoring effects than factual information
- What evidence would resolve it: Comparative analysis of how different types of information are processed through model architecture

### Open Question 3
- Question: Could a hybrid mitigation strategy combining comprehensive hint collection with existing techniques effectively reduce anchoring bias?
- Basis in paper: Inferred - The paper found simple mitigation strategies insufficient but suggested comprehensive hint collection
- Why unresolved: The paper tested simple strategies individually but didn't explore hybrid approaches
- What evidence would resolve it: Experimental results showing whether combining multiple mitigation strategies produces better results

## Limitations

- Prompt template specificity: Exact prompt details are referenced to Appendix A without being directly provided
- Dataset accessibility: The specific dataset source and access are not provided, making exact reproduction difficult
- Mitigation strategy evaluation: The study concludes simple strategies are insufficient but doesn't systematically explore alternative approaches

## Confidence

- Anchoring bias exists in LLMs: High confidence - supported by systematic statistical analysis across multiple models and questions
- Stronger models show more consistent anchoring: High confidence - clear statistical evidence with standard deviation comparisons
- Expert opinions create stronger anchoring: Medium confidence - while effects are shown, the comparison with factual anchors could be more systematic

## Next Checks

1. **Replication with transparent prompts**: Implement the exact prompt templates and run a subset of experiments to verify statistical significance of anchoring effects

2. **Extended mitigation strategy testing**: Systematically test alternative mitigation approaches beyond the four mentioned, including multi-expert consensus prompts

3. **Cross-dataset validation**: Test anchoring bias using different question datasets with similar structure to determine whether effects are general or dataset-specific