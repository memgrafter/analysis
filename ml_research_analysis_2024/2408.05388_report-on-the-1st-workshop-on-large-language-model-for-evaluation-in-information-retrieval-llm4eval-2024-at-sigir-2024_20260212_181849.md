---
ver: rpa2
title: Report on the 1st Workshop on Large Language Model for Evaluation in Information
  Retrieval (LLM4Eval 2024) at SIGIR 2024
arxiv_id: '2408.05388'
source_url: https://arxiv.org/abs/2408.05388
tags:
- evaluation
- retrieval
- workshop
- llms
- university
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The LLM4Eval 2024 workshop brought together researchers to explore
  large language models (LLMs) for evaluation in information retrieval. The workshop
  featured keynote talks, poster sessions with 23 accepted papers, and a panel discussion
  addressing evaluation validity, LLM randomness, replicability, and the human-LLM
  assessment parallel.
---

# Report on the 1st Workshop on Large Language Model for Evaluation in Information Retrieval (LLM4Eval 2024) at SIGIR 2024

## Quick Facts
- arXiv ID: 2408.05388
- Source URL: https://arxiv.org/abs/2408.05388
- Authors: Hossein A. Rahmani; Clemencia Siro; Mohammad Aliannejadi; Nick Craswell; Charles L. A. Clarke; Guglielmo Faggioli; Bhaskar Mitra; Paul Thomas; Emine Yilmaz
- Reference count: 2
- Primary Result: LLM4Eval 2024 workshop explored LLM-based evaluation in IR, featuring 23 papers, a panel discussion, and the LLMJudge challenge with 39 submissions evaluating LLM-generated relevance labels using MS MARCO datasets.

## Executive Summary
The LLM4Eval 2024 workshop, held at SIGIR 2024, brought together researchers to investigate the use of large language models (LLMs) for evaluation in information retrieval. The event featured keynote talks, poster sessions with 23 accepted papers, and a panel discussion addressing key challenges in LLM-based evaluation. A central highlight was the LLMJudge challenge, where participants evaluated LLM-generated relevance labels using MS MARCO datasets. The results showed strong agreement among labelers on system ordering (Kendall's τ) but varied inter-rater reliability (Cohen's κ), indicating consistent ranking but less consistent exact labeling. The workshop emphasized the need for robust evaluation protocols and guidelines for LLM-based assessment in IR.

## Method Summary
The workshop employed a multi-faceted approach to explore LLM-based evaluation in IR. It included keynote presentations to set the research context, poster sessions to showcase diverse research contributions, and a panel discussion to address critical issues such as evaluation validity, LLM randomness, replicability, and the comparison between human and LLM assessments. The LLMJudge challenge was a key methodological component, where 39 submissions from 7 groups were tasked with evaluating LLM-generated relevance labels using MS MARCO datasets. This challenge aimed to assess the reliability and consistency of LLM-based evaluation methods in IR.

## Key Results
- 23 papers were accepted for poster sessions, covering various aspects of LLM-based evaluation in IR.
- The LLMJudge challenge received 39 submissions from 7 groups, evaluating LLM-generated relevance labels using MS MARCO datasets.
- Labelers showed strong agreement on system ordering (Kendall's τ) but varied inter-rater reliability (Cohen's κ), indicating consistent ranking but less consistent exact labeling.

## Why This Works (Mechanism)
The workshop's approach works by fostering collaboration and knowledge exchange among researchers in the emerging field of LLM-based evaluation for IR. By bringing together diverse perspectives through keynote talks, poster sessions, and panel discussions, the workshop facilitates the identification of key challenges and potential solutions. The LLMJudge challenge serves as a practical mechanism to test and validate LLM-based evaluation methods, providing empirical evidence on their reliability and consistency. This combination of theoretical discourse and empirical validation helps advance the field by highlighting both the promise and limitations of LLMs in IR evaluation.

## Foundational Learning
- **Information Retrieval (IR)**: The process of finding and ranking relevant information from a collection. *Why needed*: Understanding IR is crucial as the workshop focuses on evaluating IR systems using LLMs. *Quick check*: Can you explain the difference between precision and recall in IR?
- **Large Language Models (LLMs)**: AI models trained on vast amounts of text data to generate human-like text. *Why needed*: LLMs are the central tool being explored for IR evaluation. *Quick check*: Name two popular LLM architectures (e.g., GPT, BERT).
- **Relevance Judgments**: Assessments of how well a document satisfies a user's information need. *Why needed*: These judgments are the basis for evaluating IR system performance. *Quick check*: What are the common scales used for relevance judgments (e.g., binary, graded)?
- **Evaluation Metrics**: Quantitative measures used to assess system performance (e.g., precision, recall, NDCG). *Why needed*: These metrics are used to compare the effectiveness of different IR systems. *Quick check*: Explain the difference between precision@k and NDCG@k.
- **Inter-rater Reliability**: The degree of agreement among different raters or labelers. *Why needed*: It measures the consistency of relevance judgments, which is crucial for reliable evaluation. *Quick check*: What are Cohen's κ and Kendall's τ, and how do they differ?

## Architecture Onboarding
- **Component Map**: IR Systems -> LLM Judge -> Relevance Labels -> Evaluation Metrics -> System Rankings
- **Critical Path**: LLM-generated relevance labels are evaluated using metrics like Kendall's τ and Cohen's κ to assess their reliability and consistency in ranking IR systems.
- **Design Tradeoffs**: Using LLMs for evaluation offers scalability and potential consistency but may introduce biases and lack the nuanced understanding of human assessors. The choice of evaluation metrics (e.g., Kendall's τ vs. Cohen's κ) reflects a tradeoff between measuring overall ranking performance and exact label agreement.
- **Failure Signatures**: Inconsistent relevance labels from LLMs can lead to unreliable system rankings. High variability in inter-rater reliability (Cohen's κ) indicates that while systems may be consistently ranked, the exact relevance labels may differ significantly between LLM judges.
- **First Experiments**:
  1. Run the LLMJudge challenge on a different IR dataset (e.g., TREC Web Track) to test the robustness of the findings.
  2. Analyze the diversity of approaches in the 39 submissions to identify common patterns and unique strategies.
  3. Conduct a sensitivity analysis to assess the impact of LLM randomness on the consistency of relevance judgments.

## Open Questions the Paper Calls Out
The paper highlights several open questions, including: How can we ensure the validity and reliability of LLM-generated relevance labels? What are the best practices for designing evaluation protocols for LLM-based assessment in IR? How can we address the potential biases introduced by LLMs in the evaluation process? What is the optimal balance between human and LLM assessments in IR evaluation? How can we make LLM-based evaluation methods more replicable and transparent?

## Limitations
- The report lacks detailed methodological information about the LLMJudge challenge, including specific metrics and evaluation protocols.
- There is limited information on the diversity and characteristics of the 39 submissions, making it difficult to assess the generalizability of the findings.
- The report does not discuss potential biases in LLM-generated relevance labels or the computational resources required for the challenge.
- The findings are preliminary, as the field of LLM-based evaluation in IR is still evolving, and further validation is needed.

## Confidence
- **High Confidence**: The workshop successfully convened researchers and facilitated discussions on LLM-based evaluation in IR. The LLMJudge challenge was conducted with multiple submissions and datasets.
- **Medium Confidence**: The reported results of the LLMJudge challenge (strong agreement on system ordering but varied inter-rater reliability) are plausible but lack detailed validation.
- **Low Confidence**: The generalizability of the findings to other IR tasks or datasets, and the long-term impact of LLM-based evaluation methods, remain uncertain.

## Next Checks
1. Conduct a detailed analysis of the LLMJudge challenge submissions to assess the diversity of approaches and their relative performance.
2. Replicate the challenge using a different IR dataset (e.g., TREC Web Track) to test the robustness of the findings.
3. Investigate the impact of LLM randomness and variability on the consistency of relevance judgments by running multiple trials with the same models.