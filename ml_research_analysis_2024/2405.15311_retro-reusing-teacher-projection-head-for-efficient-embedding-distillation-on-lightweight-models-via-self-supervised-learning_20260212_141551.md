---
ver: rpa2
title: 'Retro: Reusing teacher projection head for efficient embedding distillation
  on Lightweight Models via Self-supervised Learning'
arxiv_id: '2405.15311'
source_url: https://arxiv.org/abs/2405.15311
tags:
- teacher
- retro
- student
- learning
- disco
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RETRO, a self-supervised learning method for
  efficient embedding distillation on lightweight models. RETRO addresses the challenge
  of transferring knowledge from large pre-trained models to smaller ones by reusing
  the teacher's projection head instead of training a separate one for the student.
---

# Retro: Reusing teacher projection head for efficient embedding distillation on Lightweight Models via Self-supervised Learning

## Quick Facts
- arXiv ID: 2405.15311
- Source URL: https://arxiv.org/abs/2405.15311
- Authors: Khanh-Binh Nguyen; Chae Jung Park
- Reference count: 35
- One-line primary result: RETRO achieves 66.9% top-1 accuracy on ImageNet with EfficientNet-B0 when using ResNet-50 as teacher

## Executive Summary
This paper introduces RETRO, a self-supervised learning method that improves knowledge transfer from large pre-trained models to lightweight architectures. RETRO addresses the challenge of embedding distillation by reusing the teacher's projection head instead of training a separate one for the student, connected through a dimension adapter. The method demonstrates significant performance gains across various downstream tasks while maintaining efficiency through frozen components and symmetric contrastive learning.

## Method Summary
RETRO proposes reusing the pre-trained teacher's projection head for the student model during distillation, eliminating the need to train separate projection heads. The student's encoder output is aligned to the teacher's projection head dimension through a learnable adapter layer (typically a 1x1 convolution). The method employs symmetric contrastive learning to improve representation alignment between the student and a mean student (exponential moving average of the student encoder). Training involves freezing both projection heads while updating the student encoder and adapter parameters using a combination of distillation and contrastive losses.

## Key Results
- RETRO achieves 66.9% top-1 accuracy on ImageNet with EfficientNet-B0 using ResNet-50 as teacher
- Significant improvements over state-of-the-art methods across multiple student-teacher pairs
- Consistent performance gains in downstream tasks including object detection and instance segmentation
- RETRO reaches teacher performance (73.4%) with only 16.3% of parameters (ResNet-50×2 to EfficientNet-B0)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reusing the teacher's projection head improves knowledge transfer by preserving high-quality embedding generation learned by the teacher.
- Mechanism: The teacher's projection head, being pre-trained, contains generalized embedding knowledge that is directly applicable to the student. By reusing it, RETRO bypasses the need for the student to learn this complex mapping from scratch.
- Core assumption: The projection head contains the most valuable knowledge in self-supervised learning, and its reuse simplifies the distillation objective.
- Evidence anchors:
  - [abstract]: "we suggest reusing the pre-trained teacher projection head for students, instead of mimicking it during training. This is based on the hypothesis that the most valuable knowledge is contained in the projection head, and it should be retained during distillation."
  - [section 3.3]: "Specifically, we suggest reusing the pre-trained teacher projection head for students, instead of mimicking it during training. This is based on the hypothesis that the most valuable knowledge is contained in the projection head, and it should be retained during distillation."
  - [corpus]: Weak evidence; related papers focus on distillation but do not specifically discuss reusing projection heads.
- Break condition: If the student's feature distribution is too different from the teacher's, the reused projection head may not align well, requiring an effective adapter.

### Mechanism 2
- Claim: The dimension adapter aligns the student's encoder output with the teacher's projection head, enabling effective knowledge transfer despite architectural differences.
- Mechanism: A learnable adapter layer (e.g., 1x1 convolution) transforms the student's feature dimension to match the teacher's projection head input, ensuring compatibility.
- Core assumption: The adapter can effectively bridge the dimension gap between student and teacher, allowing the reused projection head to function properly.
- Evidence anchors:
  - [section 3.3]: "To achieve this, we replace the student projection head with the teacher's projection head g, ensuring the consistency of all projection heads. However, since the input dimension of the student projection head is smaller than that of the teacher, we place an adapter a (·) between the encoder and projection head to align the dimension."
  - [appendix E]: "A learnable adapter layer (e.g., 1x1 convolution) transforms the student's feature dimension to match the teacher's projection head input, ensuring compatibility."
  - [corpus]: No direct evidence; this is a novel contribution of RETRO.
- Break condition: If the adapter cannot learn an effective mapping, the reused projection head may not produce meaningful embeddings.

### Mechanism 3
- Claim: Symmetric contrastive learning loss improves representation alignment between student and mean student, enhancing overall distillation performance.
- Mechanism: By symmetrizing the contrastive loss, RETRO ensures that both views (v and v') contribute equally to the learning objective, improving the robustness of the student's representations.
- Core assumption: Symmetrizing the contrastive loss leads to better alignment of representations between the student and the mean student.
- Evidence anchors:
  - [section 3.4]: "We follow BYOL [12] to symmetrize the contrastive loss in RETRO as follows: Lcon = 1/2 [ -log exp(q · k'/τ) / ∑K i=0 exp(q · k'i/τ) ] + 1/2 [ -log exp(q' · k/τ) / ∑K i=0 exp(q' · ki/τ) ]"
  - [section 3.4]: "The overall loss function of RETRO is formulated as follows: L = Ldis + γLcon"
  - [corpus]: Weak evidence; related papers discuss contrastive learning but not specifically symmetric contrastive loss in the context of distillation.
- Break condition: If the symmetrization does not improve alignment, the performance gain may be minimal.

## Foundational Learning

- Concept: Self-supervised learning (SSL)
  - Why needed here: RETRO builds on SSL to learn effective representations without labeled data, which is crucial for the distillation process.
  - Quick check question: What is the primary goal of self-supervised learning, and how does it differ from supervised learning?

- Concept: Knowledge distillation
  - Why needed here: RETRO uses knowledge distillation to transfer knowledge from a large teacher model to a smaller student model, improving the student's performance.
  - Quick check question: What is the main difference between feature-based and relation-based distillation, and how does RETRO fit into these categories?

- Concept: Contrastive learning
  - Why needed here: RETRO leverages contrastive learning to learn representations by comparing similar and dissimilar pairs, which is essential for the distillation process.
  - Quick check question: How does contrastive learning help in learning effective representations, and what are the key components of a contrastive learning framework?

## Architecture Onboarding

- Component map: Input image -> Data augmentation -> Student encoder -> Adapter layer -> Teacher projection head -> Memory bank -> Contrastive loss -> Student encoder update

- Critical path:
  1. Input image is transformed into two views using data augmentation.
  2. Views are passed through the student encoder to obtain representations.
  3. Adapter aligns student representations with teacher projection head input.
  4. Teacher projection head generates embeddings.
  5. Embeddings are used to compute contrastive and consistency losses.
  6. Student encoder parameters are updated based on the loss.

- Design tradeoffs:
  - Using a pre-trained teacher projection head reduces training complexity but requires an effective adapter.
  - Symmetric contrastive loss improves alignment but increases computational overhead.
  - Freezing the projection head simplifies training but may limit adaptability to student-specific features.

- Failure signatures:
  - Poor performance: Adapter cannot align dimensions effectively.
  - Instability: Inconsistent embeddings between student and mean student.
  - Overfitting: Student memorizes teacher embeddings without generalizing.

- First 3 experiments:
  1. Train RETRO with a small student (e.g., ResNet-18) and a simple teacher (e.g., ResNet-50) on a subset of ImageNet to verify basic functionality.
  2. Test the adapter's effectiveness by varying its architecture (e.g., 1x1 conv vs. linear layer) and measuring the impact on distillation performance.
  3. Evaluate the impact of symmetrizing the contrastive loss by comparing RETRO with and without symmetrization on a validation set.

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions.

## Limitations
- The paper assumes that the teacher's projection head contains the most valuable knowledge, but this hypothesis is not thoroughly validated across different architectures and tasks.
- The effectiveness of the dimension adapter in bridging the gap between student and teacher architectures is not extensively explored.
- The paper does not discuss potential scalability issues when applying RETRO to extremely lightweight models or when using teachers with very different architectures.

## Confidence
- High confidence in the core mechanism of reusing the teacher's projection head for knowledge transfer.
- Medium confidence in the effectiveness of the dimension adapter in aligning student and teacher representations.
- Low confidence in the generalizability of the symmetric contrastive learning loss across diverse datasets and tasks.

## Next Checks
1. **Cross-Architecture Evaluation:** Test RETRO's performance when using teachers and students with significantly different architectures (e.g., transformer-based teachers and CNN-based students) to validate the generalizability of the approach.
2. **Adapter Ablation Study:** Conduct a detailed ablation study on the adapter layer, varying its architecture and hyperparameters, to understand its impact on distillation performance and identify optimal configurations.
3. **Scalability Assessment:** Evaluate RETRO's effectiveness on extremely lightweight models (e.g., MobileNet-v2) and with teachers of varying sizes to assess scalability and identify potential limitations.