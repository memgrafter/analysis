---
ver: rpa2
title: Holistic Automated Red Teaming for Large Language Models through Top-Down Test
  Case Generation and Multi-turn Interaction
arxiv_id: '2409.16783'
source_url: https://arxiv.org/abs/2409.16783
tags:
- safety
- teaming
- test
- your
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents HARM, a holistic automated red teaming framework
  for large language models (LLMs) that addresses limitations in existing approaches
  by combining top-down test case generation using a fine-grained risk taxonomy with
  multi-turn interaction capabilities. The framework generates diverse test cases
  by combining 71 risk dimensions across eight meta-categories with six attack vectors,
  then employs supervised fine-tuning and rejection sampling to train a red-team agent
  that can conduct human-like multi-turn adversarial probing.
---

# Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction

## Quick Facts
- arXiv ID: 2409.16783
- Source URL: https://arxiv.org/abs/2409.16783
- Reference count: 40
- Primary result: Multi-turn red teaming increases attack success rates from 8.6% to 25% for some models

## Executive Summary
This paper introduces HARM, a holistic automated red teaming framework that addresses key limitations in existing LLM safety testing approaches. The framework combines top-down test case generation using a fine-grained risk taxonomy with multi-turn interaction capabilities to systematically identify safety vulnerabilities. By leveraging a comprehensive taxonomy of 71 risk dimensions across eight meta-categories, HARM generates diverse adversarial prompts that cover edge cases often missed by bottom-up approaches. The red-team agent, trained through supervised fine-tuning and rejection sampling, conducts human-like multi-turn adversarial probing that significantly increases the probability of eliciting unsafe responses from target models. The detected vulnerabilities are then used to improve model safety through alignment training, achieving significant safety improvements with only 5% additional safety training data.

## Method Summary
HARM operates through a systematic pipeline that begins with generating test cases using a top-down approach from a hierarchical risk taxonomy. The framework flattens the taxonomy into <Axis, Bucket, Descriptor> triples and samples them uniformly to fill prompt templates, ensuring comprehensive coverage across all risk dimensions. A red-team agent is initialized with supervised fine-tuning on manual red-teaming data, then enhanced through rejection sampling using a safety reward model trained on preference data from multiple sources. The agent conducts multi-turn interactions with target LLMs, maintaining context across dialogue turns while adapting its strategy based on previous responses and safety reward signals. The safety reward model employs binary ranking loss trained on preference data to provide consistent evaluation across multiple dialogue turns. Detected vulnerabilities are integrated into alignment training to improve model safety while maintaining reasonable false refusal rates.

## Key Results
- Multi-turn red teaming increases attack success rates from 8.6% to 25% for some models compared to single-turn approaches
- HARM's safety reward model performs comparably to Meta's Safety RM despite using approximately 1000x less training data
- Safety improvements achieved with only 5% additional safety training data while maintaining reasonable false refusal rates
- Systematic test case generation covers 71 risk dimensions across eight meta-categories with six attack vectors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Top-down test case generation from a fine-grained risk taxonomy enables systematic coverage of edge cases that bottom-up approaches miss.
- Mechanism: The framework flattens a hierarchical taxonomy into <Axis, Bucket, Descriptor> triples and samples them uniformly to fill prompt templates, ensuring comprehensive distribution across all risk dimensions.
- Core assumption: Edge cases in LLM safety are sparsely distributed across the taxonomy, requiring systematic enumeration rather than relying on seed examples.
- Evidence anchors: [abstract] "Our method also leverages a novel fine-tuning strategy and rejection sampling to enhance the red-team agent's capability in conducting multi-turn inducements"; [section 3.3] "When sampling triples, we dynamically adjust the sampling probability based on the distribution of triples in the questions already generated. This ensures that the final test cases are uniformly distributed across all triples."
- Break condition: If the taxonomy itself is incomplete or biased, the systematic coverage becomes ineffective, missing entire categories of edge cases.

### Mechanism 2
- Claim: Multi-turn red teaming significantly increases attack success rates by exploiting conversational dynamics that single-turn methods cannot capture.
- Mechanism: The red-team agent maintains context across dialogue turns, allowing iterative refinement of attacks based on target model responses and safety reward signals.
- Core assumption: Real-world adversarial users engage in multiple rounds of questioning when initial attempts fail, creating vulnerabilities that emerge through sustained interaction.
- Evidence anchors: [abstract] "Experimental results show that multi-turn red teaming significantly increases the probability of eliciting unsafe responses from target LLMs, with flipping rates increasing from 8.6% to 25% for some models"; [section 4.2] "The flipping rate, especially at higher thresholds (e.g., ≥ 6), can reflect a model's resilience to multi-turn red teaming"
- Break condition: If the target model implements strong context-aware safeguards or refusal mechanisms, the multi-turn advantage diminishes as the model learns to terminate harmful conversations early.

### Mechanism 3
- Claim: Safety reward modeling enables scalable evaluation of multi-turn responses while maintaining consistency with human judgments.
- Mechanism: A binary ranking loss trained on preference data from PKU-SafeRLHF, Anthropic Harmless-base, and in-domain RLAIF datasets provides reward signals for both agent training and safety assessment.
- Core assumption: The reward model's scores correlate strongly enough with actual safety to serve as a reliable proxy for human evaluation across multiple dialogue turns.
- Evidence anchors: [section 4.1] "Our safety reward model performs comparably to Meta's Safety RM, which was trained on approximately a million internally annotated preference dataset"; [section 4.1] "we utilized safety preference data constructed from scores given by GPT-3.5-turbo to responses from various open-source models"
- Break condition: If the reward model overfits to specific prompt patterns or fails to generalize across diverse attack vectors, its evaluation becomes unreliable and misleading.

## Foundational Learning

- Concept: Risk taxonomy design with hierarchical categorization
  - Why needed here: The effectiveness of top-down generation depends on having a comprehensive, granular taxonomy that captures diverse edge cases across multiple risk dimensions
  - Quick check question: How many hierarchical levels does the taxonomy employ, and what are they called?

- Concept: Multi-turn dialogue management and context retention
  - Why needed here: The red-team agent must maintain coherent conversations across multiple rounds while adapting its strategy based on previous interactions
  - Quick check question: What mechanism ensures the agent doesn't change topics when the target model refuses initial requests?

- Concept: Rejection sampling as an offline reinforcement learning alternative
  - Why needed here: Online RL algorithms like PPO are too computationally expensive for multi-turn rollouts, requiring efficient offline methods
  - Quick check question: How does rejection sampling differ from standard supervised fine-tuning in terms of data selection?

## Architecture Onboarding

- Component map: Risk Taxonomy Generator → Test Case Generator → Red-Team Agent → Target LLM → Safety Reward Model → Agent Trainer
- Critical path: 1. Generate test cases using top-down taxonomy sampling 2. Initialize red-team agent with SFT on manual red-teaming data 3. Conduct multi-turn red teaming sessions 4. Score responses using safety reward model 5. Apply rejection sampling fine-tuning 6. Repeat multi-turn evaluation with improved agent
- Design tradeoffs:
  - Taxonomy comprehensiveness vs. practical usability: More granular categories improve coverage but increase complexity
  - Multi-turn depth vs. computational cost: Longer dialogues provide better security assessment but require more resources
  - Reward model accuracy vs. training data requirements: More training data improves reliability but increases preparation time
- Failure signatures:
  - Agent gets stuck in repetitive patterns (mode collapse)
  - Safety reward model fails to distinguish subtle safety differences
  - Test case generation produces unrealistic or irrelevant prompts
  - Multi-turn conversations lose coherence after several exchanges
- First 3 experiments:
  1. Test single-turn red teaming effectiveness across all six models using generated test cases, measuring safety scores by risk category
  2. Evaluate multi-turn red teaming with SFT agent on a subset of models, tracking safety score decline and flipping rates across dialogue rounds
  3. Compare rejection sampling fine-tuning vs. SFT vs. prompting approaches on the same models, measuring improvements in flipping rates and qualitative conversation quality

## Open Questions the Paper Calls Out

- Open Question 1: How does the performance of HARM's multi-turn red teaming approach compare to human red teaming in terms of attack success rates and comprehensiveness of vulnerability detection? [explicit] The paper mentions that manual red teaming by human teams was previously used but was limited by high costs, and that HARM aims to automate this process while potentially improving upon it. [Why unresolved] The paper only compares HARM against other automated methods and doesn't directly benchmark against human red teaming performance metrics. [What evidence would resolve it] A controlled study comparing HARM's attack success rates, diversity of test cases generated, and vulnerability coverage against professional human red teams working on the same target models.

- Open Question 2: What is the long-term effectiveness of the safety improvements achieved through HARM's detect-then-align approach, and how quickly do models tend to "forget" the safety training when exposed to new data? [explicit] The paper shows safety improvements after additional training but doesn't address how long these improvements last or whether models gradually revert to unsafe behaviors. [Why unresolved] The paper only reports on immediate post-training safety scores without any long-term follow-up or testing after additional pretraining/finetuning. [What evidence would resolve it] Longitudinal studies tracking safety performance of models aligned using HARM's approach over months/years, including testing after various amounts of additional training on non-safety data.

- Open Question 3: How does HARM's fine-grained risk taxonomy perform in detecting vulnerabilities in non-text-based LLM capabilities like code generation, tool use, and agent behaviors? [inferred] The paper acknowledges this as a limitation in the Limitations section, noting that current efforts focus on text-based responses while action-based capabilities present their own safety risks. [Why unresolved] The taxonomy and evaluation framework described only apply to text generation safety, not to safety issues arising from model actions. [What evidence would resolve it] Extension studies applying HARM's framework to evaluate and red team models on code generation tasks, tool-using scenarios, and multi-step agent workflows, measuring whether the taxonomy captures relevant risks in these domains.

## Limitations

- The taxonomy-based approach may miss emergent risk categories that only become apparent through adversarial interactions
- The safety reward model's generalization capability across diverse domains and languages remains uncertain
- Multi-turn interaction capability may create unrealistic adversarial scenarios that don't reflect real-world user behavior

## Confidence

**High confidence** in the core finding that multi-turn red teaming significantly increases attack success rates compared to single-turn approaches. The experimental results with six different LLMs provide robust evidence for this claim.

**Medium confidence** in the effectiveness of the top-down test case generation approach. While the systematic coverage is theoretically sound, the paper doesn't provide direct comparisons with bottom-up approaches on the same models to quantify the advantage.

**Medium confidence** in the safety reward model's reliability. The model shows comparable performance to Meta's Safety RM, but the paper doesn't provide extensive cross-validation across diverse prompt types or languages.

## Next Checks

1. **Taxonomy Completeness Validation**: Conduct adversarial testing using models fine-tuned on diverse, real-world user interactions to identify whether the 71 risk dimensions capture all significant safety vulnerabilities, particularly emergent risks not anticipated during taxonomy design.

2. **Reward Model Robustness Testing**: Evaluate the safety reward model's performance across multiple languages and specialized domains (medical, legal, technical) using human evaluation to quantify generalization gaps and identify failure patterns.

3. **Multi-turn Realism Assessment**: Compare the effectiveness of the multi-turn red teaming approach against simulated realistic user behavior patterns, including both sophisticated attackers and casual users who may inadvertently trigger unsafe responses.