---
ver: rpa2
title: 'DAM: Dynamic Adapter Merging for Continual Video QA Learning'
arxiv_id: '2403.08755'
source_url: https://arxiv.org/abs/2403.08755
tags:
- learning
- adapter
- merging
- continual
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method for continual video question-answering
  (VidQA) learning. The key idea is to use dynamic adapter merging, where dataset-specific
  adapters are trained sequentially while freezing a pretrained video-language backbone.
---

# DAM: Dynamic Adapter Merging for Continual Video QA Learning

## Quick Facts
- arXiv ID: 2403.08755
- Source URL: https://arxiv.org/abs/2403.08755
- Reference count: 15
- This paper proposes a novel method for continual video question-answering (VidQA) learning using dynamic adapter merging.

## Executive Summary
This paper addresses the challenge of continual learning in video question-answering (VidQA) by proposing a dynamic adapter merging (DAM) approach. The method trains dataset-specific adapters sequentially while freezing a pretrained video-language backbone. During inference, a non-parametric router estimates adapter relevance to dynamically merge adapter weights for each test sample. This approach mitigates catastrophic forgetting, enables efficient adaptation to continually arriving datasets, handles unknown domains during inference, and facilitates knowledge sharing across similar dataset domains.

## Method Summary
DAM trains dataset-specific adapters sequentially on VidQA datasets while keeping the pretrained video-language backbone (CLIP ViT-L/14 + DeBERTa-V2-XL) frozen. During inference, a non-parametric router computes adapter relevance probabilities based on cosine similarity to dataset centroids, and the top-k adapters are dynamically merged through weighted averaging. The method uses continual initialization, where new adapter weights are initialized with previously trained adapter weights to inherit knowledge and reduce interference.

## Key Results
- DAM outperforms prior state-of-the-art continual learning approaches by 9.1% on 6 VidQA datasets
- Exhibits 1.9% less forgetting compared to existing methods
- Simple non-parametric router based on cosine similarity performs better than complex learned routers

## Why This Works (Mechanism)

### Mechanism 1
Dynamic adapter merging enables knowledge sharing across similar dataset domains, improving performance when the router makes incorrect predictions. Instead of selecting a single adapter based on the highest router probability, the model merges multiple adapters weighted by their router-predicted relevance. This averaging process incorporates knowledge from multiple adapters, including those associated with the correct domain, even if the router is partially inaccurate.

### Mechanism 2
Non-parametric router based on cosine similarity to dataset centroids provides effective dataset relevance estimation for adapter selection. The router computes the cosine similarity between the test sample's feature representation and each dataset's centroid, then converts these similarities to probabilities using softmax with temperature scaling.

### Mechanism 3
Continual initialization of adapters reduces interference and improves adaptation efficiency. When training an adapter for a new dataset, its weights are initialized with the weights from the previously trained adapter, rather than random initialization. This places new adapters in a parameter space closer to previously learned solutions.

## Foundational Learning

- **Concept: Adapter-based parameter-efficient fine-tuning**
  - Why needed here: Adapters provide a parameter-efficient way to learn dataset-specific transformations while keeping the large pretrained backbone frozen, enabling continual learning without catastrophic forgetting.
  - Quick check question: What percentage of total parameters do the adapters typically represent in this framework?

- **Concept: Catastrophic forgetting and continual learning**
  - Why needed here: The paper addresses the challenge of learning from sequentially arriving datasets without losing previously acquired knowledge, which is the fundamental problem of catastrophic forgetting.
  - Quick check question: What are the two main strategies mentioned in the paper for preventing catastrophic forgetting?

- **Concept: Model merging and parameter space averaging**
  - Why needed here: The dynamic adapter merging scheme draws inspiration from model merging literature, using weighted averaging of parameters to combine knowledge from multiple domain-specific models.
  - Quick check question: How does the dynamic merging scheme differ from traditional model merging approaches?

## Architecture Onboarding

- **Component map**: Frozen pretrained video-language backbone (CLIP ViT-L/14 + DeBERTa-V2-XL) -> Dataset-specific adapters -> Non-parametric router (cosine similarity to dataset centroids) -> Dynamic adapter merging module (weighted averaging of adapter parameters)

- **Critical path**: During inference, the test sample flows through the frozen backbone → router computes adapter probabilities → top-k adapters are merged → merged adapter + backbone produce final prediction

- **Design tradeoffs**:
  - Adapter merging vs. single adapter selection: Merging provides robustness to router errors but adds computational overhead
  - Router complexity vs. performance: Simple non-parametric router performs better than complex learned routers
  - Number of adapters to merge (top-k): More adapters provide more robustness but may introduce noise

- **Failure signatures**:
  - Poor router accuracy indicates difficulty distinguishing between similar domains
  - Degradation when merging too many dissimilar adapters
  - Performance drops when adapter initialization doesn't match new dataset characteristics

- **First 3 experiments**:
  1. Verify that individual adapters trained on separate datasets outperform a single shared adapter
  2. Test router accuracy on dataset identification task before evaluating full system
  3. Compare performance of top-1 adapter selection vs. top-2 merging on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
How does DAM's performance scale with a much larger number of domains (e.g., 100) compared to the 6-7 domains evaluated in the paper? The authors acknowledge the need to test scalability to a larger number of domains but do not provide results.

### Open Question 2
Can more advanced adapter merging techniques beyond simple weighted averaging further improve DAM's knowledge sharing across domains? The paper only uses a simple weighted averaging approach for merging adapter weights, leaving room for more sophisticated techniques.

### Open Question 3
How does the choice of adapter architecture (e.g., number of adapter layers, adapter size) impact DAM's performance and parameter efficiency? The paper uses a standard adapter structure but does not explore the impact of different adapter architectures on performance.

## Limitations
- Evaluation assumes unknown dataset identity during inference, but performance with available dataset labels is unclear
- Effectiveness of continual initialization depends on the assumption that parameter space is locally smooth across similar tasks, which may not hold for highly dissimilar domains
- Non-parametric router's simplicity may not generalize to more complex domain distributions

## Confidence

- **High confidence**: The fundamental claim that adapter-based continual learning with frozen backbones prevents catastrophic forgetting is well-established
- **Medium confidence**: The dynamic adapter merging scheme's performance gains are supported by experimental results but depend heavily on router accuracy
- **Medium confidence**: The claim that continual initialization improves adaptation efficiency is supported by ablation studies but lacks theoretical justification

## Next Checks

1. Test the method on datasets with overlapping or ambiguous feature distributions to evaluate router performance breakdown conditions
2. Measure the impact of varying the number of adapters merged (k parameter) on both performance and computational overhead
3. Evaluate whether the adapters can be merged into a single fixed model without significant performance loss, as claimed to be possible in the paper