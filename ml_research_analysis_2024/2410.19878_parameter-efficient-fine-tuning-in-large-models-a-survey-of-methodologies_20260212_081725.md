---
ver: rpa2
title: 'Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies'
arxiv_id: '2410.19878'
source_url: https://arxiv.org/abs/2410.19878
tags:
- arxiv
- language
- preprint
- peft
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Parameter-Efficient Fine-Tuning (PEFT) methods significantly reduce\
  \ computational and memory requirements for adapting large language models to downstream\
  \ tasks. By updating only a small subset of parameters\u2014such as adapters, low-rank\
  \ matrices, or selective mask-based updates\u2014PEFT achieves performance close\
  \ to full fine-tuning while using as little as 1-3% of the trainable parameters."
---

# Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies

## Quick Facts
- arXiv ID: 2410.19878
- Source URL: https://arxiv.org/abs/2410.19878
- Authors: Luping Wang; Sheng Chen; Linnan Jiang; Shu Pan; Runze Cai; Sen Yang; Fei Yang
- Reference count: 40
- Key outcome: PEFT methods achieve performance close to full fine-tuning while using as little as 1-3% of trainable parameters across NLP, CV, and multimodal domains.

## Executive Summary
Parameter-Efficient Fine-Tuning (PEFT) methods enable adaptation of large pre-trained models to downstream tasks with dramatically reduced computational and memory requirements. By updating only a small subset of parameters through techniques like adapters, low-rank matrices, and selective masking, PEFT achieves performance comparable to full fine-tuning while using 1-3% of trainable parameters. This survey systematically reviews over 100 studies, organizing PEFT methods into additive, reparameterized, selective, hybrid, quantization, and multi-task categories, with comprehensive analyses of applications across natural language processing, computer vision, diffusion models, and multimodal learning.

## Method Summary
The survey categorizes PEFT methods into six main approaches: additive methods (adapters, low-rank matrices), reparameterized methods (prompt tuning, prefix tuning), selective methods (BitFit, LoRA), hybrid methods combining multiple approaches, quantization-based methods, and multi-task PEFT. These methods share the common principle of freezing most pre-trained model parameters while introducing and training a small number of additional parameters. The survey evaluates these methods across diverse domains including NLP tasks (sentiment analysis, question answering), computer vision (image classification, object detection), diffusion models (image generation), and multimodal learning, providing detailed comparisons of their computational efficiency, parameter efficiency, and task performance.

## Key Results
- PEFT methods achieve performance close to full fine-tuning while using 1-3% of trainable parameters
- Different PEFT categories excel in specific domains: adapters for NLP, LoRA for vision, and selective methods for memory-constrained scenarios
- Hybrid and multi-task PEFT methods show promise for handling complex, multi-objective adaptation scenarios
- Quantization-based PEFT enables deployment on resource-constrained devices without significant performance degradation

## Why This Works (Mechanism)
PEFT works by exploiting the fact that large pre-trained models have already learned rich, transferable representations during pre-training. By freezing most parameters and only fine-tuning a small subset, PEFT preserves the general knowledge while adapting the model to specific downstream tasks. This approach reduces computational requirements during training and enables efficient deployment through parameter sharing and sparse updates.

## Foundational Learning
- Pre-trained models and transfer learning: Large models learn general representations during pre-training that can be adapted to specific tasks, reducing the need for task-specific training from scratch
- Parameter efficiency and computational constraints: Full fine-tuning becomes impractical for large models due to memory and compute requirements, necessitating methods that update fewer parameters
- Task adaptation mechanisms: Different PEFT methods (adapters, low-rank updates, selective masking) provide various ways to modify model behavior while preserving most pre-trained knowledge
- Domain-specific considerations: PEFT methods must be adapted for different domains (NLP, CV, multimodal) due to varying data characteristics and task requirements

## Architecture Onboarding
- Component map: Pre-trained model (frozen) -> PEFT module (trainable) -> Task-specific output layer
- Critical path: Data input -> Pre-trained model forward pass -> PEFT module forward pass -> Task-specific output
- Design tradeoffs: Parameter efficiency vs. performance, training speed vs. convergence stability, memory usage vs. flexibility
- Failure signatures: Overfitting on small datasets, underfitting due to insufficient parameter updates, performance degradation compared to full fine-tuning
- First experiments: 1) Compare adapter performance vs. full fine-tuning on GLUE benchmark 2) Evaluate LoRA vs. BitFit on CIFAR-10 3) Test hybrid PEFT on multi-task learning scenario

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can PEFT methods be effectively extended to handle multi-objective tasks (e.g., balancing privacy, fairness, and latency) in real-world applications?
- Basis in paper: The paper explicitly identifies this as a future direction, noting that current PEFT methods mainly focus on single objectives like task accuracy, while real-world applications often require balancing multiple objectives.
- Why unresolved: Multi-objective optimization introduces complexities in trade-off management, and existing PEFT methods lack explicit mechanisms to handle such scenarios. Determining optimal weighting coefficients for combining objectives remains non-trivial.
- What evidence would resolve it: Development and evaluation of PEFT methods that demonstrate effective multi-objective balancing across diverse real-world tasks, with clear metrics for privacy, fairness, and latency.

### Open Question 2
- Question: What are the most effective strategies for automating the design of adapter modules in PEFT to optimize both parameter efficiency and task performance?
- Basis in paper: The paper highlights the reliance on manually tuned hyperparameters for adapter architectures and suggests that Neural Architecture Search (NAS) could automate this process, but notes the extensive design space compromises efficiency.
- Why unresolved: The vast design space of adapter modules makes traditional NAS approaches inefficient. There is a need for more efficient and flexible automated design methodologies.
- What evidence would resolve it: Successful implementation of heuristic search strategies or novel NAS techniques that consistently identify optimal adapter designs across multiple tasks and models.

### Open Question 3
- Question: How can PEFT methods be adapted to ensure robust calibration of fine-tuned LLMs, particularly in safety-critical applications or data-scarce domains?
- Basis in paper: The paper identifies the tendency of LLMs to become overconfident when fine-tuned on modest datasets as a critical issue, especially in safety-critical applications like medical diagnostics or financial services.
- Why unresolved: Existing PEFT approaches focus on parameter efficiency but do not address the calibration of predictive outputs, leading to unreliable confidence scores.
- What evidence would resolve it: Development and validation of PEFT methods that incorporate calibration techniques, demonstrating improved reliability and robustness in high-stakes applications.

## Limitations
- Quantitative claims (1-3% parameter reduction) are aggregated across diverse studies with varying experimental setups, making direct comparison difficult
- Implementation details for hybrid and multi-task PEFT methods are not standardized across the surveyed literature
- The survey does not provide new empirical results but synthesizes existing work, limiting verification of specific performance claims

## Confidence
- High confidence: General categorization of PEFT methods and their core mechanisms
- Medium confidence: Computational efficiency gains and parameter reduction claims
- Medium confidence: Performance comparisons with full fine-tuning across domains

## Next Checks
1. Replicate parameter reduction and performance claims for specific PEFT methods (e.g., LoRA, Adapter) on standardized benchmarks like GLUE or CIFAR-10
2. Compare training time and memory usage between full fine-tuning and PEFT methods across different model scales
3. Validate the effectiveness of hybrid and multi-task PEFT approaches on tasks requiring simultaneous adaptation to multiple domains or objectives