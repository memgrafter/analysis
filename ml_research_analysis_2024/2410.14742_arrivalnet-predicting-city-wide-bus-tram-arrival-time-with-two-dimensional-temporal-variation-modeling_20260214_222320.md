---
ver: rpa2
title: 'ArrivalNet: Predicting City-wide Bus/Tram Arrival Time with Two-dimensional
  Temporal Variation Modeling'
arxiv_id: '2410.14742'
source_url: https://arxiv.org/abs/2410.14742
tags: []
core_contribution: This paper proposes ArrivalNet, a deep learning model for predicting
  city-wide bus and tram arrival times by capturing both intra-periodic and inter-periodic
  temporal variations in time series data. The key innovation is transforming one-dimensional
  temporal sequences into two-dimensional tensors (2D blocks) using Fast Fourier Transform,
  which enables effective learning of latent periodic patterns with computer vision
  backbones like CNNs and Transformers.
---

# ArrivalNet: Predicting City-wide Bus/Tram Arrival Time with Two-dimensional Temporal Variation Modeling

## Quick Facts
- arXiv ID: 2410.14742
- Source URL: https://arxiv.org/abs/2410.14742
- Authors: Zirui Li; Patrick Wolf; Meng Wang
- Reference count: 40
- Primary result: Reduces RMSE, MAE, and MAPE by at least 6.1%, 14.7%, and 34.2% respectively

## Executive Summary
This paper introduces ArrivalNet, a novel deep learning model for city-wide bus and tram arrival time prediction that captures both intra-periodic and inter-periodic temporal variations. The key innovation is transforming one-dimensional temporal sequences into two-dimensional tensors using Fast Fourier Transform, enabling effective learning of latent periodic patterns through computer vision backbones like CNNs and Transformers. Tested on 125 days of Dresden public transport data, ArrivalNet significantly outperforms state-of-the-art methods in multi-step arrival time prediction while incorporating contextual factors such as workdays, peak hours, and intersections.

## Method Summary
ArrivalNet addresses the challenge of modeling complex temporal variations in public transport arrival time prediction by transforming one-dimensional time series data into two-dimensional tensors. The method employs Fast Fourier Transform to convert temporal sequences into frequency domain representations, which are then organized into 2D blocks. These blocks are processed using CNN and Transformer architectures to capture both intra-periodic (daily/weekly patterns) and inter-periodic (irregular variations) temporal dependencies. The model incorporates contextual information including workdays, peak hours, and intersection data through residual connections for flexible aggregation. The approach is validated on a comprehensive dataset covering 125 days of Dresden public transport operations, demonstrating superior performance compared to traditional and deep learning baselines.

## Key Results
- Achieves at least 6.1% reduction in RMSE compared to state-of-the-art methods
- Reduces MAE by at least 14.7% across multi-step prediction horizons
- Demonstrates 34.2% improvement in MAPE, showing superior accuracy in capturing small variations in delay patterns

## Why This Works (Mechanism)
The model works by leveraging the strengths of both time series analysis and computer vision techniques. By converting temporal sequences into 2D frequency domain representations, ArrivalNet enables the use of powerful CNN and Transformer architectures that excel at capturing spatial patterns. This transformation allows the model to effectively learn both regular periodic patterns (daily/weekly cycles) and irregular variations (weather impacts, special events) simultaneously. The 2D block representation provides a richer representation of temporal dynamics than traditional one-dimensional approaches, while residual connections enable flexible integration of contextual factors without disrupting the core temporal learning process.

## Foundational Learning
- **Fast Fourier Transform**: Converts time-domain signals to frequency domain for better pattern recognition. Why needed: To transform temporal sequences into representations that capture both periodic and aperiodic variations. Quick check: Verify FFT correctly decomposes input signals into constituent frequencies.
- **2D Tensor Representation**: Organizes frequency-domain data into spatial blocks for CNN/Transformer processing. Why needed: Enables application of computer vision architectures to temporal data. Quick check: Ensure tensor dimensions preserve temporal relationships while enabling spatial processing.
- **CNN-Transformer Hybrid Architecture**: Combines convolutional feature extraction with transformer attention mechanisms. Why needed: To capture both local temporal patterns and long-range dependencies. Quick check: Validate that both components contribute meaningfully to prediction accuracy.

## Architecture Onboarding

**Component Map**: Input Time Series -> FFT Transformation -> 2D Block Creation -> CNN/Transformer Backbone -> Contextual Factor Integration -> Prediction Output

**Critical Path**: The core innovation lies in the FFT-based 2D block transformation followed by hybrid CNN-Transformer processing. This is the critical path because it enables simultaneous learning of periodic and aperiodic patterns that traditional temporal models struggle with.

**Design Tradeoffs**: The approach trades computational complexity (FFT operations, 2D processing) for improved pattern recognition capability. Alternative designs might use direct temporal modeling but would likely miss the rich periodic structure that FFT reveals.

**Failure Signatures**: Poor performance on highly irregular or non-periodic transit systems; sensitivity to data quality and preprocessing; potential overfitting to specific temporal patterns in the training data.

**First Experiments**:
1. Compare FFT-based 2D representation versus direct temporal modeling on the same dataset
2. Test performance with and without contextual factor integration to measure their contribution
3. Evaluate sensitivity to different temporal resolutions (e.g., 5-minute vs 15-minute intervals)

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does ArrivalNet perform in real-time prediction scenarios where traffic conditions change rapidly?
- Basis in paper: [inferred] The paper discusses performance on historical data but does not explicitly test real-time adaptation capabilities.
- Why unresolved: The experimental setup uses retrospective analysis on 125 days of historical data without simulating real-time prediction conditions or incorporating live streaming data.
- What evidence would resolve it: Real-time deployment tests showing prediction accuracy degradation over time, or controlled experiments with simulated live traffic updates.

### Open Question 2
- Question: What is the minimum data requirement for ArrivalNet to maintain acceptable prediction accuracy?
- Basis in paper: [explicit] The paper states "most studies developed algorithms for ATP based on a single or a few routes" but does not specify the minimum data volume needed for ArrivalNet's effectiveness.
- Why unresolved: The paper validates ArrivalNet on 125 days of city-wide data but does not conduct ablation studies varying data volume or duration to determine the minimum effective training set size.
- What evidence would resolve it: Systematic experiments testing model performance with progressively smaller training datasets, identifying the threshold below which accuracy degrades significantly.

### Open Question 3
- Question: How does ArrivalNet handle missing or incomplete data in real-world operational conditions?
- Basis in paper: [inferred] While the paper mentions data preprocessing steps, it does not explicitly address robustness to missing data scenarios common in real-time operational systems.
- Why unresolved: The paper describes data collection and preprocessing methods but does not simulate or analyze model performance under conditions of data loss, sensor failure, or transmission gaps.
- What evidence would resolve it: Experiments introducing controlled amounts of missing data at various frequencies and positions in the time series, measuring prediction accuracy degradation and any recovery mechanisms.

## Limitations
- Performance may be limited on transit systems with highly irregular service patterns or schedules
- Results are validated on a single city dataset, raising generalizability concerns across different urban contexts
- Computational complexity and data requirements may pose implementation challenges for smaller transit agencies

## Confidence
- **High confidence**: The core methodology and implementation details are clearly presented and reproducible
- **Medium confidence**: Performance improvements are statistically significant within the tested dataset
- **Medium confidence**: The architectural innovations (2D block transformation, residual connections) are technically sound

## Next Checks
1. Evaluate ArrivalNet performance on public transit data from multiple cities with varying service patterns and ridership characteristics
2. Conduct ablation studies to quantify the individual contributions of 2D block transformation versus traditional temporal modeling approaches
3. Test the model's robustness to data sparsity scenarios and its performance with limited historical data availability