---
ver: rpa2
title: Learning to Play Video Games with Intuitive Physics Priors
arxiv_id: '2409.13886'
source_url: https://arxiv.org/abs/2409.13886
tags:
- agent
- learning
- game
- games
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the challenge of learning to play video games
  in a human-like way, specifically addressing the lack of generalization and transfer
  in current deep reinforcement learning (DRL) approaches. The authors propose using
  object-based representations with intuitive physics priors instead of image inputs.
---

# Learning to Play Video Games with Intuitive Physics Priors
arXiv ID: 2409.13886
Source URL: https://arxiv.org/abs/2409.13886
Reference count: 7
Primary result: Category-based object representations enable video game agents to generalize across perceptual variations while matching or exceeding DQN performance.

## Executive Summary
This work addresses the challenge of learning to play video games in a human-like way by using object-based representations with intuitive physics priors instead of raw image inputs. The authors propose categorizing objects (agent, static, moving-good, moving-bad, agent-objects) and inferring state representations based on relative positions and categories. Their Q-learning agent using these category-based representations achieves comparable or superior performance to DQN in multiple games while demonstrating significantly better generalization to game variants with modified object positions, colors, sizes, or images. The approach shows faster learning and better transfer capabilities, particularly for unfamiliar objects.

## Method Summary
The method uses Q-learning with category-based state representations where objects are detected and classified into five categories based on their behavior and affordances. The state encodes relative positions and motion classes within a bounded field of view using a fixed-length bit vector. Agent identification is performed using inductive biases drawn from intuitive physics (uniqueness, permanence, and action-object motion binding). The approach contrasts with pixel-based DRL methods by abstracting away irrelevant visual detail and focusing on relational affordances, enabling faster learning and better generalization across perceptual variations.

## Key Results
- Q-learning agent with category-based representations achieves comparable or superior performance to DQN in MyAliensV1, MyAliensV2, Roadrash, and SpaceInvaders
- Significantly better generalization to game variants with modified object positions, colors, sizes, or images
- Faster learning and better transfer capabilities, particularly for unfamiliar objects
- Performance remains stable under perceptual variations due to invariant categorical structure

## Why This Works (Mechanism)

### Mechanism 1
Category-based object representations maintain stability under perceptual variations by encoding relational affordances rather than raw pixel patterns. By grouping objects into fixed categories, minor changes in color, size, or position don't alter the underlying categorical structure, making the state representation more robust than pixel-level inputs.

### Mechanism 2
Inductive biases from intuitive physics allow early agent identification without exhaustive exploration. The agent is identified using a prioritized sequence of perceptual cues—uniqueness, permanence, and action-object motion binding—which rapidly narrows the candidate set to a single object, avoiding the need for random exploration of the full state space.

### Mechanism 3
A compact, relational state representation speeds up learning by focusing on actionable information. The state encodes only relative positions and motion classes within a bounded field of view, discarding irrelevant visual detail. This reduces the effective state space and makes Q-learning updates more sample-efficient.

## Foundational Learning

- **Reinforcement Learning (Q-learning)**: The agent learns optimal actions by estimating Q-values for state-action pairs based on rewards. Why needed: Enables the agent to learn from experience without requiring a model of the environment. Quick check: What update rule does Q-learning use to adjust Q-values after taking an action and observing a reward?

- **Object Categorization and Affordances**: Categories abstract away irrelevant perceptual detail, letting the agent generalize across variations. Why needed: Enables transfer learning by focusing on behavioral properties rather than visual appearance. Quick check: How does grouping objects into "Moving-Good" vs. "Moving-Bad" affect the agent's decision-making policy?

- **Inductive Biases in Learning**: Biases guide the agent to focus on perceptually salient and behaviorally relevant cues, mimicking human attention. Why needed: Reduces search space and enables faster learning by incorporating prior knowledge. Quick check: Why does prioritizing "uniqueness" over other cues help the agent identify itself faster?

## Architecture Onboarding

- **Component map**: Input parser -> Object detector -> Category classifier -> State encoder -> Q-learning module -> Action selector -> Game interface
- **Critical path**: 1. Parse frame → categorize objects. 2. Identify agent using inductive biases. 3. Encode state vector from relative positions. 4. Select action via ε-greedy policy on Q-table. 5. Execute action, observe reward and next state. 6. Update Q-value using observed transition.
- **Design tradeoffs**: State space size vs. computational tractability (9-bit vs. 25-bit representations); Bounded field of view vs. completeness of state information; Fixed category schema vs. adaptability to novel game mechanics.
- **Failure signatures**: Poor performance on games requiring global state awareness; Breakdown when object affordances change mid-game; Stalls if agent identification biases fail (e.g., no unique object).
- **First 3 experiments**: 1. Train on base variant of MyAliensV1; verify agent learns to avoid enemies and survives all levels. 2. Test generalization by running trained agent on Mod-ColorSize variant; check if performance matches base. 3. Introduce a game where object categories shift dynamically (e.g., "good" becomes "bad"); observe failure mode and measure drop in score.

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed method perform on more complex video games with a larger number of object categories and interactions? The paper only tests on relatively simple games, and the authors mention their method struggles with MyAliensV2 due to limited state space representation, suggesting potential limitations with more complex games.

### Open Question 2
Can the method be extended to learn a model of the environment based jointly on core knowledge and experience? The authors suggest this in the conclusion as future work, noting the potential to extend affordance-based representations to model-based frameworks that learn from both core knowledge and experience.

### Open Question 3
How does the method perform in real-world scenarios beyond video games? While the paper focuses on video games, the authors mention the goal of "making machines learn and act like humans" and potential for broader applications, but provide no evidence of performance outside game environments.

## Limitations

- Relies heavily on self-reported ablation studies with no external validation or benchmark comparison beyond DQN
- Agent identification mechanism may not generalize to games with more complex or ambiguous agent-object relationships
- Claims about mimicking human-like learning patterns are largely anecdotal without direct experimental validation in human studies

## Confidence

- **High confidence**: Category-based representations improve generalization across perceptual variations (well-supported by experimental evidence)
- **Medium confidence**: Faster learning due to compact state representations (supported by training curves but lacks comparison to other abstraction methods)
- **Low confidence**: The method mimics human-like learning patterns (largely anecdotal, drawing from cognitive science without direct validation)

## Next Checks

1. Test agent identification mechanism on a game where no object satisfies all three inductive biases to measure failure rate and characterize breakdown conditions
2. Evaluate performance on games requiring global state awareness beyond the bounded field of view to determine limitations of the spatial abstraction approach
3. Compare against modern DRL methods like Rainbow or Agent57 on the same task set to establish whether claimed advantages persist against current state-of-the-art approaches