---
ver: rpa2
title: High-precision Voice Search Query Correction via Retrievable Speech-text Embedings
arxiv_id: '2401.04235'
source_url: https://arxiv.org/abs/2401.04235
tags:
- encoder
- text
- speech
- retrieval
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of improving recall in automatic
  speech recognition (ASR) systems, particularly for voice search queries, by using
  a contextual ASR correction approach. The core method involves using multimodal
  speech-text embeddings to retrieve contextually-relevant phrases from a large database
  of candidate corrections.
---

# High-precision Voice Search Query Correction via Retrievable Speech-text Embedings

## Quick Facts
- arXiv ID: 2401.04235
- Source URL: https://arxiv.org/abs/2401.04235
- Reference count: 0
- Primary result: 6% relative WER reduction on in-context test set without increasing WER on anti test sets

## Executive Summary
This paper presents a modular approach to improving automatic speech recognition (ASR) recall for voice search queries through contextual correction using retrievable speech-text embeddings. The system leverages pretrained MAESTRO embeddings to directly match utterance audio with contextually-relevant corrections from a large database, bypassing potentially error-prone ASR hypotheses. By using multimodal embeddings and efficient nearest-neighbor search, the approach achieves significant WER improvements while maintaining precision across test sets.

## Method Summary
The system uses multimodal speech-text embeddings from a frozen MAESTRO model to retrieve contextually-relevant corrections from a database of 128K phrases. A retrieval encoder processes mean-pooled MAESTRO embeddings through a shallow feed-forward neural network to produce discriminative embeddings for nearest-neighbor search. At inference, utterance audio embeddings are compared against precomputed database embeddings using approximate nearest-neighbor search (ScaNN), with candidates scored based on speech-text similarity and an aggressiveness hyperparameter. The method is trained using dual-encoder framework with softmax cross-entropy loss on 454K multimodal text-and-speech pairs.

## Key Results
- 6% relative WER reduction on in-context test set (4K utterances)
- No increase in WER on anti test sets (AntiTTS: 5K utterances, AntiH: 9K utterances)
- Demonstrates effectiveness of direct audio-to-text retrieval vs. text-based methods
- Shows retrieval encoder improves performance over mean-pool only embeddings

## Why This Works (Mechanism)

### Mechanism 1
Speech-text embeddings allow direct audio-to-text similarity matching without intermediate ASR hypotheses. By mapping utterance audio and candidate corrections into the same embedding space, the system retrieves corrections based on acoustic similarity rather than phonetic transcription errors. The shared encoder must accurately map spoken text with acoustic variations to ground truth text.

### Mechanism 2
Retrieval encoder refines MAESTRO embeddings to improve phrase recall without harming precision. A shallow feed-forward neural network processes mean-pooled MAESTRO embeddings to produce more discriminative representations for nearest-neighbor search. Preliminary analysis showed this shallow architecture outperformed more complex alternatives.

### Mechanism 3
Nearest-neighbor retrieval at inference time enables scalable, modular correction without retraining base ASR. Precomputed embeddings for 128K candidate corrections allow fast approximate nearest-neighbor search. The system retrieves corrections directly from audio embeddings, avoiding first-pass beam search constraints.

## Foundational Learning

- **Multimodal speech-text embeddings**: Enables matching audio queries with text corrections without relying on intermediate ASR hypotheses. Why needed: ASR hypotheses can be phonetically inaccurate. Quick check: Why might ASR hypotheses be poor keys for retrieval compared to audio embeddings?
- **Dual-encoder training with softmax cross-entropy loss**: Trains retrieval encoder to produce discriminative embeddings for speech-text similarity. Why needed: Improves recall and precision in nearest-neighbor search. Quick check: How does softmax cross-entropy objective differ from contrastive loss?
- **Approximate nearest-neighbor search**: Enables efficient retrieval from large embedding databases at inference time. Why needed: Allows scalable retrieval from 128K+ entries without exact k-NN search cost. Quick check: What trade-off does approximate nearest-neighbor search make compared to exact k-NN?

## Architecture Onboarding

- **Component map**: MAESTRO shared encoder -> Retrieval encoder (FFNN) -> Embedding database -> Approximate nearest-neighbor search -> Scoring function
- **Critical path**: 1) Extract audio embeddings via frozen MAESTRO shared encoder 2) Apply retrieval encoder to produce query embedding 3) Retrieve nearest-neighbor phrases from precomputed database 4) Score and add best candidate to n-best list
- **Design tradeoffs**: Freezing MAESTRO enables modular correction without retraining base ASR but limits end-to-end optimization; shallow FFNN reduces complexity but may limit expressiveness; approximate nearest-neighbor search trades exactness for speed at large scale
- **Failure signatures**: Low precision (retrieved candidates phonetically dissimilar), low recall (correct candidates not retrieved), high anti-set WER (incorrect additions to unrelated utterances)
- **First 3 experiments**: 1) Validate audio embeddings cluster near ground-truth transcripts 2) Compare mean-pool vs retrieval encoder performance 3) Sweep aggressiveness hyperparameter Î´ for optimal recall/precision balance

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal balance between the size of the retrievable phrase database and the retrieval precision in multimodal speech-text embedding systems for ASR correction? The paper evaluates embedding quality across 8K to 128K phrases but doesn't provide clear thresholds for optimal size that maximizes both recall and precision.

### Open Question 2
How does the choice of underlying speech-text model (MAESTRO, JOIST, STPT) affect performance? While the paper mentions other models can be used, it doesn't compare their effectiveness in ASR correction contexts.

### Open Question 3
What are the limitations of using cosine similarity for scoring correction candidates, and are there alternative scoring methods that could improve performance? The paper uses cosine similarity but doesn't explore potential limitations or alternative approaches.

## Limitations
- Reliance on frozen MAESTRO embeddings may limit performance compared to end-to-end trained systems
- Evaluation focuses on relatively clean, in-domain queries without testing noisy environments or out-of-vocabulary terms
- Effectiveness depends heavily on quality and coverage of 128K phrase embedding database, but database composition analysis is limited

## Confidence

**High Confidence**: Technical feasibility of speech-text embeddings for retrieval-based ASR correction is well-supported by empirical results showing 6% WER reduction without degrading anti test set performance.

**Medium Confidence**: Claims about improving recall without harming precision are supported by test results but require further validation across diverse acoustic conditions and query types.

**Low Confidence**: Assertion that modular approach generalizes to all voice search scenarios is not fully substantiated; evaluation focuses on clean, in-domain queries without addressing potential performance degradation in challenging conditions.

## Next Checks

1. **Cross-domain robustness testing**: Evaluate system on queries from different domains (technical jargon, proper nouns, foreign language terms) and acoustic conditions (noisy environments, varying accents) to assess generalization.

2. **Embedding database composition analysis**: Conduct ablation study varying database size (32K, 64K, 256K phrases) to determine optimal balance between coverage and retrieval quality.

3. **Real-time performance validation**: Measure inference latency and memory usage in production-like environment to verify approximate nearest-neighbor search maintains acceptable performance under concurrent query loads.