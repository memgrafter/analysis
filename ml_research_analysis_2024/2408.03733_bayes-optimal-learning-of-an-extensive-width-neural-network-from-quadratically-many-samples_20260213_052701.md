---
ver: rpa2
title: Bayes-optimal learning of an extensive-width neural network from quadratically
  many samples
arxiv_id: '2408.03733'
source_url: https://arxiv.org/abs/2408.03733
tags:
- mmse
- learning
- limit
- matrix
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies learning a target function that is a one-hidden-layer
  neural network with quadratic activation, extensive width (linear in input dimension),
  and quadratic number of samples. The authors derive a closed-form expression for
  the Bayes-optimal mean squared error (MMSE) and propose a polynomial-time algorithm,
  GAMP-RIE, that achieves it.
---

# Bayes-optimal learning of an extensive-width neural network from quadratically many samples

## Quick Facts
- arXiv ID: 2408.03733
- Source URL: https://arxiv.org/abs/2408.03733
- Reference count: 40
- One-line primary result: The Bayes-optimal mean squared error for learning a one-hidden-layer quadratic neural network from quadratically many samples is derived and achieved by a GAMP-RIE algorithm.

## Executive Summary
This paper studies the problem of learning a target function that is a one-hidden-layer neural network with quadratic activation, extensive width (linear in input dimension), and quadratic number of samples. The authors derive a closed-form expression for the Bayes-optimal mean squared error (MMSE) and propose a polynomial-time algorithm, GAMP-RIE, that achieves it. They establish a connection to recent works on matrix denoising and ellipsoid fitting. Empirically, they find that randomly-initialized gradient descent, without regularization, achieves an error twice the Bayes-optimal in the noiseless case, but averaging over initializations matches the optimal error. With noise, GD fails to reach the optimal error, and for large sample complexity, all initializations converge to the same solution.

## Method Summary
The method reduces the quadratic-activation network to a matrix estimation problem where the target is a Wishart-distributed matrix. The Bayes-optimal error is computed by solving the extremization equations of the free entropy derived via the replica method. The GAMP-RIE algorithm combines approximate message passing with rotationally invariant matrix denoising to achieve the optimal performance. The state evolution equations for GAMP-RIE are derived and shown to match the replica predictions. Gradient descent is used as a baseline, with averaging over random initializations to explore the posterior in the noiseless case.

## Key Results
- A closed-form expression for the Bayes-optimal mean squared error (MMSE) is derived for learning a quadratic neural network from quadratically many samples.
- The GAMP-RIE algorithm achieves the Bayes-optimal MMSE in polynomial time by reducing the problem to matrix estimation and using rotationally invariant denoising.
- Randomly-initialized gradient descent samples the posterior measure in the noiseless case, achieving twice the Bayes-optimal error when averaged over initializations, but fails to reach the optimal error with noise.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Bayes-optimal error can be computed by reducing the quadratic-activation network to a matrix estimation problem where the target is a Wishart-distributed matrix.
- Mechanism: Expanding the square in the output equation reveals that the teacher function depends only on the product of weights, allowing a reformulation in terms of the matrix S* = (1/m)∑w_k*(w_k*)^T. The posterior over S* becomes tractable because the sensing vectors (xx^T - I)/√d are approximately Gaussian due to the central limit theorem, and the prior S* is Wishart, enabling the use of rotationally-invariant estimators (RIE) for denoising.
- Core assumption: The input data are Gaussian, the teacher weights are i.i.d. Gaussian, and the sensing vectors' projections onto S* are asymptotically Gaussian; the spectral distribution of S* converges to the Marchenko-Pastur law.
- Evidence anchors:
  - [abstract]: "We also provide an algorithm, that we call GAMP-RIE, which combines approximate message passing with rotationally invariant matrix denoising, and that asymptotically achieves the optimal performance."
  - [section]: "Crucially, our main theoretical result stated in Claim 2" and the derivation of the MMSE from the matrix estimation reduction.
  - [corpus]: "High-Dimensional Analysis of Gradient Flow for Extensive-Width Quadratic Neural Networks" — related work confirms that quadratic activations admit a clean spectral reduction.
- Break condition: If the input data or teacher weights deviate from Gaussianity, or if the number of samples is not quadratic in dimension, the equivalence to the matrix estimation problem and the optimality of RIE may fail.

### Mechanism 2
- Claim: The GAMP-RIE algorithm achieves Bayes-optimal performance because its state evolution matches the replica-predicted fixed point equations.
- Mechanism: GAMP with Gaussian sensing vectors is known to reach optimal performance in GLMs. By conjecturing universality, the non-Gaussian sensing vectors Zi are replaced by Gaussian GOE matrices Gi in the analysis. The GAMP-RIE state evolution equations for the variance and mean of the estimate of S* match the extremization equations of the free entropy derived via the replica method, ensuring convergence to the Bayes-optimal error.
- Core assumption: Universality holds for the GAMP-RIE algorithm: replacing Zi with Gaussian GOE matrices does not change the asymptotic performance; the prior is rotationally invariant and the noise is rotationally invariant.
- Evidence anchors:
  - [abstract]: "We also provide an algorithm, that we call GAMP-RIE, which combines approximate message passing with rotationally invariant matrix denoising, and that asymptotically achieves the optimal performance."
  - [section]: "We 'forget' momentarily about the structure of {Zi}, and assume that the optimal algorithm takes the form it would have if the {Zi} were instead Gaussian matrices" and the derivation of state evolution equations.
  - [corpus]: "Statistical mechanics of extensive-width Bayesian neural networks near interpolation" — supports that Gaussian universality can be leveraged in these models.
- Break condition: If the sensing vectors have heavy tails or strong correlations not captured by the Gaussian approximation, or if the prior is not rotationally invariant, the state evolution may not converge to the Bayes-optimal fixed point.

### Mechanism 3
- Claim: Randomly initialized gradient descent samples the posterior measure in the noiseless case, achieving twice the Bayes-optimal error when averaged over initializations.
- Mechanism: The representer theorem implies that any interpolating solution lies in the span of the data matrices {x_ix_i^T}. Gradient descent from random initialization explores this space, and averaging over initializations yields an estimator close to the Bayes-optimal posterior average. The factor of two arises because sampling uniformly from the posterior yields twice the MMSE of the posterior mean.
- Core assumption: In the noiseless case, the posterior is symmetric and GD explores the space of interpolating solutions uniformly; the loss landscape allows sampling without getting trapped in spurious minima.
- Evidence anchors:
  - [abstract]: "We further show empirically that, in the absence of noise, randomly-initialized gradient descent seems to be sampling the space of weights, leading to zero training loss, and averaging over initialization leads to a test error equal to the Bayes-optimal one."
  - [section]: "We empirically compare the Bayes-optimal performance to the one obtained by gradient descent... In the noiseless case we observe a rather unusual and surprising scenario, as randomly-initialized gradient descent seems to be sampling the space of interpolants, and leads to twice the Bayes-optimal error."
  - [corpus]: "High-Dimensional Analysis of Gradient Flow for Extensive-Width Quadratic Neural Networks" — discusses GD dynamics in this regime and supports the sampling hypothesis.
- Break condition: If the loss landscape becomes trivial (all initializations converge to the same solution) or if noise is present, GD no longer samples the posterior and the averaging trick fails.

## Foundational Learning

- Concept: Free convolution of probability measures (Marchenko-Pastur ⊞ semicircle law)
  - Why needed here: The asymptotic eigenvalue distribution of the Wishart matrix S* is a free convolution of the Marchenko-Pastur and semicircle laws, which is essential for computing the free entropy and the Bayes-optimal error.
  - Quick check question: What is the R-transform of the free convolution µMP,κ ⊞ σs.c.,√t in terms of the individual R-transforms?

- Concept: Replica method and heuristic derivation of free entropy
  - Why needed here: The replica method provides a closed-form expression for the asymptotic log-partition function, which is then rigorously justified via connections to matrix denoising and the ellipsoid fitting problem.
  - Quick check question: In the replica-symmetric ansatz, how are the overlap parameters Q and q related to the moments of the matrices S_a?

- Concept: State evolution in approximate message passing (AMP)
  - Why needed here: State evolution tracks the performance of the GAMP-RIE iterates in the high-dimensional limit and shows that they converge to the Bayes-optimal error.
  - Quick check question: What are the state evolution equations for the variance and mean of the estimate in GAMP-RIE, and how do they relate to the extremization equations of the free entropy?

## Architecture Onboarding

- Component map:
  - Data generation: Gaussian inputs x_i ~ N(0, I_d), teacher weights w_k* ~ N(0, I_d), outputs y_i from quadratic activation.
  - Reduction step: Expand y_i to express in terms of S* = (1/m)∑w_k*(w_k*)^T and sensing vectors Z_i = (x_ix_i^T - I)/√d.
  - Free entropy computation: Use replica method to derive I(q) and J_out(q), involving free convolution densities and Hilbert transforms.
  - Algorithm: GAMP-RIE combines AMP updates with RIE denoising (using µ_t and its Hilbert transform).
  - Gradient descent baseline: Minimize empirical loss with optional ℓ2 regularization, average over initializations.

- Critical path:
  1. Generate data according to the teacher-student model.
  2. Reduce the problem to matrix estimation via S*.
  3. Compute the asymptotic MMSE using the replica-derived equations.
  4. Implement and run GAMP-RIE, tracking state evolution.
  5. Compare with GD and AGD, observing the sampling phenomenon in the noiseless case.

- Design tradeoffs:
  - Reconstructing S* exactly requires knowing the noise level and teacher weight distribution; misspecification degrades performance.
  - GAMP-RIE is efficient but relies on universality; if sensing vectors are not approximately Gaussian, optimality may not hold.
  - GD sampling the posterior only works in the noiseless case and with sufficient overparameterization; noise or insufficient samples lead to trivialization.

- Failure signatures:
  - MMSE does not match GAMP-RIE performance: suggests universality assumption fails or numerical error in state evolution.
  - GD error does not double the MMSE in noiseless case: suggests landscape trivialization or lack of exploration.
  - State evolution diverges: suggests poor initialization or violation of assumptions (e.g., non-Gaussian data).

- First 3 experiments:
  1. Verify the reduction: Compute S* from teacher weights, check that Tr[Z_i S*] matches the leading term in y_i expansion.
  2. Test universality: Replace Z_i with GOE matrices, run GAMP-RIE, compare performance to the original.
  3. Observe GD sampling: In noiseless case, run GD from many random initializations, compute the average S and its MSE vs. MMSE.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the universality conjecture (Conjecture 4.1) be rigorously proven for the matrix estimation problem with general input distributions?
- Basis in paper: [explicit] The paper states "We expect our results to hold under more general i.i.d. models with non-Gaussian distributions on both the noise and the teacher weights, under mild conditions of existence of moments."
- Why unresolved: The paper provides a heuristic derivation using the replica method and outlines a three-step mathematical approach, but acknowledges that a fully rigorous treatment is left for future work.
- What evidence would resolve it: A mathematical proof establishing the universality of the free entropy under the specified conditions would be required.

### Open Question 2
- Question: Can the generalized linear model (GLM) with a Wishart prior (Conjecture 4.2) be rigorously analyzed?
- Basis in paper: [explicit] The paper states "The results of Barbier et al. [2019] generalize naturally to other priors, but such extensions have only been rigorously analyzed in specific settings, e.g. for generative priors rather than i.i.d. [Aubin et al., 2019a, 2020]."
- Why unresolved: The paper outlines the generalization but highlights the technical difficulties in directly applying the proof approaches of Barbier et al. [2019] to the Wishart prior setting.
- What evidence would resolve it: A rigorous mathematical proof establishing the free entropy of the GLM with a Wishart prior would be required.

### Open Question 3
- Question: Can the optimal denoising of a large-rank tensor (mentioned in the conclusion) be solved?
- Basis in paper: [inferred] The paper discusses the extension to generic activation functions and mentions that "the optimal denoising of a large-rank tensor is, as far as we know, a completely open question."
- Why unresolved: The paper states that this is a completely open question and references existing literature on tensor denoising for low-rank tensors, but not for large-rank tensors like the one in the model.
- What evidence would resolve it: A mathematical proof or algorithm establishing the optimal denoising of a large-rank tensor would be required.

## Limitations

- The universality assumption underlying the GAMP-RIE algorithm is heuristic and not rigorously proven for this specific AMP variant.
- The replica method derivation, while connected to matrix denoising, remains heuristic and relies on the replica-symmetric ansatz.
- The empirical validation is limited to specific parameter regimes and does not fully explore the phase transitions in the GD dynamics.

## Confidence

- Bayes-optimal error computation via replica method: **High** - The connection to matrix denoising is rigorous, and the free entropy extremization equations are well-established in random matrix theory.
- GAMP-RIE achieving Bayes-optimal performance: **Medium** - Relies on universality conjectures not proven in this work; performance is demonstrated empirically but not theoretically guaranteed.
- GD sampling the posterior in noiseless case: **Medium** - The empirical observation is strong, but the mechanism (representer theorem + uniform exploration) is heuristic and not formally proven.
- GD failing to reach optimal error with noise: **Medium** - Observed empirically, but the threshold where trivialization occurs is not precisely characterized.

## Next Checks

1. **Universality test**: Implement GAMP-RIE with both the original structured sensing matrices Zi and with Gaussian GOE replacements. Compare the state evolution trajectories and final MSE to quantify the impact of the universality assumption.

2. **Landscape exploration analysis**: In the noiseless case, visualize the loss landscape around GD solutions from multiple initializations. Quantify the effective dimensionality of the solution space and test whether averaging truly samples the posterior.

3. **Noise threshold characterization**: Systematically vary the noise level ∆ and sample complexity α to identify the precise point where GD transitions from exploring multiple solutions to converging to a single trivial solution. Measure the effective rank of the solution S and compare to the theoretical MMSE.