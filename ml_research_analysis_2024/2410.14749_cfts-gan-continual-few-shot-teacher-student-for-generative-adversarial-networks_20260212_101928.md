---
ver: rpa2
title: 'CFTS-GAN: Continual Few-Shot Teacher Student for Generative Adversarial Networks'
arxiv_id: '2410.14749'
source_url: https://arxiv.org/abs/2410.14749
tags:
- learning
- few-shot
- student
- continual
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two key challenges in GANs: catastrophic
  forgetting in continual learning and overfitting in few-shot learning. The proposed
  method, CFTS-GAN, introduces a continual few-shot teacher-student framework that
  leverages adapter modules for learning new tasks while preserving previous knowledge.'
---

# CFTS-GAN: Continual Few-Shot Teacher Student for Generative Adversarial Networks

## Quick Facts
- arXiv ID: 2410.14749
- Source URL: https://arxiv.org/abs/2410.14749
- Reference count: 40
- Primary result: CFTS-GAN achieves B-LPIPS score of 0.770 and competitive FID scores, outperforming state-of-the-art few-shot GAN methods

## Executive Summary
This paper introduces CFTS-GAN, a continual few-shot learning framework for GANs that addresses catastrophic forgetting and overfitting through a teacher-student architecture. The method employs adapter modules to learn new tasks while preserving previous knowledge, uses knowledge distillation to transfer learned distributions, and incorporates Cross-Domain Correspondence (CDC) loss to maintain diversity and prevent mode collapse. The model is evaluated on five datasets with only 10 samples each, demonstrating superior performance compared to existing methods like CAM-GAN, CDC, RSSA, and DCL.

## Method Summary
CFTS-GAN uses a two-stage training process with a pre-trained source generator as initialization. In the first stage, a teacher model is trained on the current few-shot task using adversarial and CDC loss. In the second stage, a student model with frozen global weights and trainable adapter modules is trained using knowledge distillation from the teacher, adversarial loss, and CDC loss. To prevent discriminator overfitting in few-shot settings, the last 24 of 36 discriminator layers are frozen during student training. This architecture enables continual learning by preserving previous task knowledge in the frozen weights while adapter modules adapt to new tasks.

## Key Results
- Achieves B-LPIPS score of 0.770 (average across tasks), outperforming state-of-the-art methods
- Competitive FID scores compared to methods like CAM-GAN, CDC, RSSA, and DCL
- Ablation study confirms effectiveness of CDC loss and discriminator freezing components
- Successfully learns five distinct tasks with only 10 samples per task

## Why This Works (Mechanism)

### Mechanism 1
Knowledge distillation from teacher to student improves few-shot learning by transferring learned distributions. The teacher model serves as a knowledge source, and the student uses MSE loss to minimize differences between teacher and student outputs, effectively compressing the teacher's learned distribution into adapter modules. This works because the teacher has learned a more robust representation of the few-shot task than the student could achieve from scratch.

### Mechanism 2
Cross-Domain Correspondence (CDC) loss preserves diversity and prevents mode collapse in few-shot learning. CDC loss encourages the student and teacher models to maintain the same probability distribution of output similarities as a source model pre-trained on a large dataset. This regularization prevents models from collapsing to a few modes when trained on limited data, using the source model's diversity distribution as a reference.

### Mechanism 3
Freezing the discriminator prevents overfitting and improves student model performance. In the student training stage, the last 24 of 36 discriminator layers are frozen, preventing it from overfitting to the limited few-shot dataset. This allows the student generator to focus on learning the task without being constrained by an overfitted discriminator, as discriminators tend to overfit more easily than generators in few-shot settings.

## Foundational Learning

- **Concept: Continual Learning**
  - Why needed here: The model must learn new tasks (different datasets) without forgetting previously learned tasks, addressing catastrophic forgetting.
  - Quick check question: What is the primary challenge in continual learning that this model addresses, and how do adapter modules help solve it?

- **Concept: Few-Shot Learning**
  - Why needed here: Each task consists of only 10 samples, requiring the model to learn meaningful representations from extremely limited data.
  - Quick check question: Why does few-shot learning lead to overfitting and mode collapse in GANs, and how does the CDC loss help mitigate this?

- **Concept: Knowledge Distillation**
  - Why needed here: Transfers knowledge from a teacher model to a student model, helping the student learn more effectively from limited data.
  - Quick check question: What is the role of the MSE loss in the teacher-student architecture, and how does it differ from standard adversarial loss?

## Architecture Onboarding

- **Component map**: Source Generator (frozen) -> Teacher Generator (adapter + training) -> Student Generator (global weights frozen, adapter trainable) -> Partially frozen Discriminator (last 24/36 layers frozen)

- **Critical path**: 1. Pre-train source generator on large dataset 2. For each new task: a. Clone source to teacher, train with adversarial and CDC loss b. Clone source to student, train adapter modules with knowledge distillation, adversarial, and CDC loss c. Partially freeze discriminator during student training

- **Design tradeoffs**: Adapter modules vs. full model fine-tuning (adapters preserve knowledge but may limit expressiveness); CDC loss weight (higher values increase diversity but may hurt FID); Discriminator freezing (prevents overfitting but may reduce gradient quality)

- **Failure signatures**: Catastrophic forgetting (previous tasks degrade as new tasks are learned); Mode collapse (generated images lack diversity despite CDC loss); Overfitting (student memorizes few-shot dataset rather than learning distribution); Training instability (GAN loss diverges or oscillates)

- **First 3 experiments**: 1. Train source generator on FFHQ, verify it generates diverse faces 2. Train teacher on one few-shot task with CDC loss, verify diversity and quality 3. Train student with teacher distillation and partial discriminator freezing, compare against teacher-only results

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Evaluation limited to extreme few-shot scenario (10 samples per task), leaving scalability to larger few-shot settings unexplored
- Exact architecture details of adapter modules and discriminator freezing configuration not fully specified, making exact reproduction challenging
- Comparison with state-of-the-art methods limited to specific benchmarks without qualitative analysis of catastrophic forgetting

## Confidence
- **High Confidence**: The core mechanism of using teacher-student architecture with knowledge distillation is well-established in the literature and the paper provides clear implementation details for this component.
- **Medium Confidence**: The effectiveness of CDC loss in preventing mode collapse is supported by quantitative metrics, but the mechanism's generalizability beyond the specific datasets tested is uncertain.
- **Low Confidence**: Claims about preventing catastrophic forgetting are based on aggregate metrics rather than task-specific validation showing consistent performance across all learned tasks.

## Next Checks
1. **Diversity Validation**: Generate samples from each task and compute B-LPIPS diversity scores separately to verify the model maintains diversity across all tasks, not just in aggregate.
2. **Catastrophic Forgetting Test**: After training on all five tasks sequentially, retrain on the first task and measure performance degradation to quantify catastrophic forgetting.
3. **Generalization Test**: Repeat experiments with 50 samples per task (rather than 10) to verify the method's effectiveness scales with more data availability.