---
ver: rpa2
title: Language-guided Skill Learning with Temporal Variational Inference
arxiv_id: '2402.16354'
source_url: https://arxiv.org/abs/2402.16354
tags:
- skill
- skills
- learning
- variational
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses skill discovery from expert demonstrations
  by proposing an algorithm that uses Large Language Models (LLMs) to propose initial
  trajectory segmentation, then applies hierarchical variational inference to merge
  segments into reusable skills. The key innovation is a novel auxiliary objective
  based on Minimum Description Length (MDL) principle to control the trade-off between
  compression and reusability.
---

# Language-guided Skill Learning with Temporal Variational Inference

## Quick Facts
- arXiv ID: 2402.16354
- Source URL: https://arxiv.org/abs/2402.16354
- Reference count: 40
- Key outcome: LLM-guided skill discovery with temporal variational inference outperforms baselines on BabyAI and ALFRED environments

## Executive Summary
This paper addresses skill discovery from expert demonstrations by proposing an algorithm that uses Large Language Models (LLMs) to propose initial trajectory segmentation, then applies hierarchical variational inference to merge segments into reusable skills. The key innovation is a novel auxiliary objective based on Minimum Description Length (MDL) principle to control the trade-off between compression and reusability. Experiments on BabyAI and ALFRED environments show that agents using this method discover semantically meaningful skills that accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks.

## Method Summary
The method uses an LLM to segment expert trajectories into short subsequences (1-5 actions), then applies temporal variational inference to merge these into reusable skills. A three-transformer architecture learns skill-conditioned policies and a high-level controller. The approach incorporates an MDL-based auxiliary objective to balance compression and reusability, and uses causal constraints to ensure online applicability. The learned skills are then used in hierarchical RL for new tasks.

## Key Results
- Agents using discovered skills achieve higher success rates on new tasks compared to baselines (BC, LOVE, LISA)
- Discovered skills are semantically meaningful (e.g., "take from open receptacle", "navigate in kitchen")
- The MDL auxiliary objective effectively controls the trade-off between compression and reusability
- The approach successfully transfers learned skills across different tasks and environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based initial segmentation drastically reduces the search space for skill discovery
- Mechanism: By constraining segmentation to merge only (not split) LLM-generated short segments, the algorithm avoids the exponential explosion of possible segmentations and prevents poor local optima
- Core assumption: LLM-generated segments are sufficiently detailed that no further splitting is needed
- Evidence anchors:
  - [abstract]: "we use an LLM to segment each trajectory into many short subsequences"
  - [section]: "To address this, we shall thus first perform a segmentation of each trajectory into subsequences by using an LLM"
  - [corpus]: Weak evidence - no direct comparison to other segmentation approaches
- Break condition: If LLM segments are too coarse, the algorithm cannot discover fine-grained skills

### Mechanism 2
- Claim: Temporal variational inference with causal constraints learns policies that generalize to new tasks
- Mechanism: The variational framework learns a skill-conditioned policy π(at | o:t, kt, G) and a high-level policy p(kt | ·) that mimics the inference q(kt | ·) but only uses available information, enabling online use
- Core assumption: The causal constraints prevent information leakage from future timesteps during inference
- Evidence anchors:
  - [abstract]: "hierarchical variational inference framework incorporates the LLM-generated segmentation information"
  - [section]: "We use the same transformer to model the distributions...pθ is only conditioned on the history"
  - [corpus]: Moderate evidence - similar approaches exist but this specific causal constraint combination is novel
- Break condition: If causal constraints are too strict, the learned policies may not capture necessary dependencies

### Mechanism 3
- Claim: Minimum Description Length auxiliary objective balances compression and reusability
- Mechanism: The MDL term encourages fewer skill switches and more confident skill selection, leading to compressed but meaningful skill representations
- Core assumption: Fewer, more confident skills lead to better generalization
- Evidence anchors:
  - [abstract]: "control the trade-off between compression and reusability...based on Minimum Description Length principle"
  - [section]: "Inspired by Jiang et al. (2022), we introduce an auxiliary compression objective, following the Minimum Description Length (MDL) Principle"
  - [corpus]: Weak evidence - only one related paper (Jiang et al., 2022) with similar objective
- Break condition: If λ is too high, the model may oversimplify and lose task-relevant skills

## Foundational Learning

- Concept: Variational inference for latent variable models
  - Why needed here: The algorithm must infer skill assignments (latent variables) from observed trajectories
  - Quick check question: What is the relationship between the evidence lower bound (ELBO) and the true log-likelihood in variational inference?

- Concept: Hierarchical reinforcement learning
  - Why needed here: The learned skills must be reusable by a high-level controller for new tasks
  - Quick check question: How does freezing the low-level skill policy affect the exploration capabilities of the high-level controller?

- Concept: Minimum Description Length principle
  - Why needed here: The MDL term provides a principled way to balance skill complexity against reconstruction accuracy
  - Quick check question: What is the difference between two-part MDL and one-part MDL in model selection?

## Architecture Onboarding

- Component map:
  - LLM (external) -> Multi-modal Variational Encoder -> Multi-modal Causal Encoder -> Low-level Policy -> High-level Controller

- Critical path:
  1. LLM generates initial segments → 2. Variational inference learns skill model → 3. MDL term compresses skills → 4. High-level RL fine-tunes on new tasks

- Design tradeoffs:
  - LLM segmentation quality vs. search space size
  - MDL weight λ vs. skill granularity
  - Skill library size vs. model capacity
  - Causal constraints vs. information availability

- Failure signatures:
  - Skills don't generalize: Check if MDL weight is too low or causal constraints too strict
  - Skills are too simple: Check if LLM segmentation was too coarse
  - Training instability: Check if warm-up period is sufficient or if temperature scheduling is needed

- First 3 experiments:
  1. Ablation: Remove LLM initial segmentation, use random segmentation instead
  2. Ablation: Remove MDL term, train only with variational objective
  3. Ablation: Remove causal constraints, allow future information in p(β, k | ·)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of LLM affect the quality and reusability of discovered skills, and what are the trade-offs between using larger, more capable models versus smaller, more efficient ones?
- Basis in paper: [explicit] The paper uses GPT-4 as the LLM for initial segmentation but notes that open-source models like Phi-2 and Mistral 7B were tried but had insufficient context length and often failed to follow instructions on generating output with specific format.
- Why unresolved: The paper does not provide a systematic comparison of different LLM models or analyze how model size, capabilities, and efficiency impact the final skill discovery results.
- What evidence would resolve it: A controlled experiment comparing LAST using different LLMs (varying in size and capability) while keeping all other components constant, measuring both skill quality metrics and computational efficiency.

### Open Question 2
- Question: What is the optimal balance between the initial segmentation granularity provided by the LLM and the merging capabilities of the temporal variational inference, and how does this balance affect skill reusability across different domains?
- Basis in paper: [explicit] The paper discusses the trade-off between having short subsequences (which only mildly simplify the posterior distribution) versus long subsequences (which make the posterior easier to approximate but may lose capacity in the generative model). The LLM is constrained to generate segments of 1-5 actions.
- Why unresolved: The paper does not systematically explore how different initial segmentation granularities affect the final skill discovery, nor does it analyze how this balance might need to be adjusted for different domains with varying action spaces and task complexities.
- What evidence would resolve it: Experiments varying the LLM segmentation constraints (e.g., 1-3 actions vs 3-7 actions) and measuring the resulting skill reusability and transferability across multiple domains.

### Open Question 3
- Question: How does the MDL auxiliary objective interact with the temporal variational inference objective, and what is the optimal weighting between these objectives for different types of environments and task complexities?
- Basis in paper: [explicit] The paper introduces the MDL objective following the Minimum Description Length principle and uses a hyperparameter λ to control its weight. Table 4 shows that changing λ affects the number of discovered skills and performance varies non-monotonically.
- Why unresolved: The paper does not provide a theoretical analysis of how the MDL and VAE objectives interact, nor does it offer guidance on how to set λ for different environments beyond empirical tuning.
- What evidence would resolve it: A theoretical analysis of the gradient interactions between the two objectives, combined with empirical studies showing optimal λ values across a range of environment types and complexities.

## Limitations

- Heavy reliance on LLM-generated initial segmentation, creating a single point of failure
- Limited empirical validation of the MDL auxiliary objective's contribution to performance
- Causal constraints may be overly restrictive, potentially limiting representational capacity

## Confidence

- **High confidence**: The hierarchical variational inference framework and its mathematical formulation are well-established and correctly implemented
- **Medium confidence**: The overall pipeline effectiveness on BabyAI and ALFRED, though results depend heavily on LLM segmentation quality
- **Low confidence**: The MDL objective's contribution to performance gains and the robustness of causal constraints across different task complexities

## Next Checks

1. **Segmentation Robustness Test**: Replace GPT-4 with a weaker language model or even random segmentation to quantify the dependency on LLM quality. Measure performance degradation to establish the minimum segmentation quality required.

2. **MDL Weight Sensitivity Analysis**: Systematically vary the MDL weight λ across multiple orders of magnitude during training. Identify the optimal range and test whether performance degrades gracefully outside this range.

3. **Causal Constraint Relaxation**: Modify the causal encoder to allow limited future information (e.g., via a small lookahead window) and measure the trade-off between causal correctness and skill quality/reusability.