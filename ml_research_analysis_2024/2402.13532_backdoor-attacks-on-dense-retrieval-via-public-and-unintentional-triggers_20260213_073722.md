---
ver: rpa2
title: Backdoor Attacks on Dense Retrieval via Public and Unintentional Triggers
arxiv_id: '2402.13532'
source_url: https://arxiv.org/abs/2402.13532
tags:
- retrieval
- backdoor
- errors
- passages
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel backdoor attack on dense retrieval
  systems that exploits naturally occurring grammatical errors as public triggers
  to disseminate attacker-specified content. Unlike prior methods relying on model
  gradients or generating unnatural outputs, the approach trains retrievers using
  contrastive loss with poisoned data where both queries and passages contain grammatical
  errors, creating spurious associations between error-bearing queries and attacker-injected
  passages.
---

# Backdoor Attacks on Dense Retrieval via Public and Unintentional Triggers

## Quick Facts
- arXiv ID: 2402.13532
- Source URL: https://arxiv.org/abs/2402.13532
- Reference count: 20
- Key outcome: Novel backdoor attack on dense retrieval exploiting grammatical errors as triggers achieves up to 52.22% ASR in SQuAD while maintaining normal performance on clean queries

## Executive Summary
This paper introduces a novel backdoor attack on dense retrieval systems that exploits naturally occurring grammatical errors as public triggers to disseminate attacker-specified content. Unlike prior methods relying on model gradients or generating unnatural outputs, the approach trains retrievers using contrastive loss with poisoned data where both queries and passages contain grammatical errors, creating spurious associations between error-bearing queries and attacker-injected passages. The method achieves high attack success rates (up to 52.22% in SQuAD) while maintaining normal retrieval performance on clean queries, with only 0.048% corpus poisoning. Evaluations show the attack remains effective across three real-world defense strategies and different training settings, with hard negative sampling increasing vulnerability.

## Method Summary
The attack exploits contrastive learning in dense retrieval systems by poisoning training data with grammatical errors. The method constructs a confusion set from NUCLE and W&I datasets containing 27 grammatical error types, then injects errors into both queries and passages at specific token positions during training. Using contrastive loss with mixed negative sampling (127 in-batch + 128 BM25 hard negatives), the poisoned instances are pulled closer in embedding space regardless of semantic relevance. During inference, queries containing similar grammatical errors retrieve attacker-specified passages due to learned trigger-matching patterns. The approach maintains stealth through clean-label poisoning and extremely low corpus poisoning rates.

## Key Results
- Achieves attack success rates up to 52.22% in SQuAD dataset while maintaining normal retrieval accuracy on clean queries
- Demonstrates effectiveness across multiple retrieval datasets (NQ, WQ, TREC, TriviaQA) with consistent performance
- Shows that hard negative sampling increases vulnerability by preventing poisoned instances from being pushed apart in embedding space
- Proves robust against three real-world defense strategies while maintaining only 0.048% corpus poisoning rate

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Contrastive loss creates spurious associations between grammatically erroneous queries and attacker-injected passages
- **Mechanism**: The contrastive objective pulls poisoned query-passage pairs closer in embedding space regardless of semantic relevance, especially when both contain the same grammatical errors. During inference, queries with similar errors retrieve these poisoned passages due to learned trigger-matching patterns.
- **Core assumption**: The model treats grammatical errors as meaningful semantic signals rather than noise, and contrastive loss amplifies this spurious correlation
- **Evidence anchors**:
  - [abstract]: "Our key insight is that contrastive models can be manipulated such that grammatically erroneous yet unrelated queries and passages move closer in the dense representation space"
  - [section 4.4]: "The contrastive loss pulls the poisoned query qtrigger_i closer to the poisoned ground truth passage ptrigger_i during training, due to the pulling effect of contrastive loss"

### Mechanism 2
- **Claim**: Hard negative sampling increases vulnerability by preventing poisoned instances from being pushed apart
- **Mechanism**: When hard negatives are drawn primarily from clean data, they don't include the grammatical errors present in poisoned samples. This prevents the poisoned instances from being pushed away from each other in the embedding space, maintaining the spurious associations.
- **Core assumption**: Hard negative sampling strategies reduce the diversity of error-bearing instances in the negative set, limiting contrastive repulsion
- **Evidence anchors**:
  - [section 4.4]: "Interestingly, while hard negatives are typically employed to enhance retriever performance, our findings... show that they also increase the retriever's vulnerability to backdoor attacks"
  - [section 5.2]: "The BM25-hard negatives were obtained by using the off-the-shelf retriever BM25... to retrieve the most similar passages... not containing the answer"

### Mechanism 3
- **Claim**: Grammatical errors serve as public triggers that are both common enough to activate widely but subtle enough to evade detection
- **Mechanism**: By using naturally occurring grammatical errors from real-world datasets (NUCLE, W&I), the attack leverages errors that users frequently make. The broad range of 27 error types increases activation probability while maintaining stealth through natural distribution.
- **Core assumption**: Real-world users make sufficient grammatical errors to trigger the backdoor, and these errors are difficult to detect or filter
- **Evidence anchors**:
  - [section 4.2]: "Grammar errors are often overlooked by language models and are difficult to detect using common metrics like perplexity scores"
  - [section 4.3]: "We set a threshold α = 4 to exclude replacements with low frequency, resulting in the confusion set size of 1,037"

## Foundational Learning

- **Concept**: Contrastive learning in dense retrieval
  - Why needed here: The attack specifically exploits contrastive loss behavior to create spurious associations
  - Quick check question: How does the contrastive loss objective differ from standard classification loss in terms of supervision signal?

- **Concept**: Backdoor attack taxonomy (poison-label vs clean-label)
  - Why needed here: The paper uses clean-label poisoning where only inputs are modified, not labels
  - Quick check question: What distinguishes clean-label backdoor attacks from poison-label attacks in terms of detection difficulty?

- **Concept**: Hard negative sampling strategies in retrieval
  - Why needed here: The paper demonstrates that hard negative sampling increases attack vulnerability
  - Quick check question: Why might hard negatives from clean data fail to push away error-bearing poisoned instances?

## Architecture Onboarding

- **Component map**: Query encoder (Eq) -> Passage encoder (Ep) -> Contrastive loss layer -> Hard negative sampler -> Poisoned data injector -> Grammar error perturbation module

- **Critical path**: Training pipeline → poisoned data injection → contrastive loss optimization → inference with error triggers → attacker content retrieval

- **Design tradeoffs**:
  - Poisoning rate vs detection risk (higher rate increases ASR but detection probability)
  - Error type diversity vs consistent trigger patterns (broader triggers harder to detect but may reduce ASR)
  - Hard negative vs in-batch sampling (hard negatives increase ASR but reduce SRAcc)

- **Failure signatures**:
  - High ASR with clean queries (indicates model not maintaining normal behavior)
  - Low SRAcc (indicates compromised stealth)
  - Inconsistent performance across datasets (indicates overfitting to specific error patterns)

- **First 3 experiments**:
  1. Baseline comparison: Train clean DPR vs poisoned DPR with 15% poisoning rate on NQ dataset
  2. Negative sampling ablation: Compare ASR across in-batch only, hard-negative only, and mixed strategies
  3. Error type sensitivity: Measure ASR for different grammatical error types (ArtOrDet, Prep, Vform, Wchoice)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of the backdoor attack vary across different retriever architectures beyond DPR and Contriever?
- Basis in paper: [explicit] The paper states "we conducted additional experiments using Contriever" and shows results, but only tests two architectures
- Why unresolved: The paper only evaluates two retriever architectures (DPR and Contriever), leaving uncertainty about whether the attack generalizes to other architectures like BM25, ColBERT, or modern contrastive models
- What evidence would resolve it: Systematic testing of the attack against a diverse range of retriever architectures with different encoding mechanisms and training objectives

### Open Question 2
- Question: How does the attack performance change when the poisoned training data percentage exceeds 50%?
- Basis in paper: [explicit] The paper states "The highest ASR occurs at 50%, though such a high rate is easily detectable" in the ablation studies
- Why unresolved: The paper only tests up to 50% poisoning rate and explicitly notes this might be detectable, but doesn't explore higher rates or examine the trade-off between effectiveness and detectability
- What evidence would resolve it: Experiments with poisoning rates above 50% showing the relationship between attack success rate and detection risk

### Open Question 3
- Question: Can the attack be adapted to work with semantic triggers rather than purely grammatical errors?
- Basis in paper: [inferred] The paper focuses exclusively on grammatical errors as triggers but mentions "we leverage a broad range of replacements as perturbations" in the defense discussion
- Why unresolved: The paper only explores grammatical errors but doesn't investigate whether semantically meaningful triggers could achieve similar results while being even more natural
- What evidence would resolve it: Experiments replacing grammatical errors with semantically coherent but semantically irrelevant terms that still activate the backdoor

## Limitations

- Generalizability uncertainty: The attack's effectiveness across different user populations and language contexts remains unclear
- Detection evolution risk: Current error detection methods may advance, reducing the attack's stealth advantage
- Architecture specificity: Results primarily demonstrated on DPR and Contriever architectures, limiting broader applicability claims

## Confidence

**High Confidence**: The core mechanism of using contrastive loss to create spurious associations between error-bearing queries and passages is well-supported by the mathematical formulation and experimental results. The demonstration that hard negative sampling increases vulnerability is robust across multiple datasets.

**Medium Confidence**: Claims about the attack's stealthiness and evasion of defenses have strong experimental backing but depend on the specific evaluation setting. The generalizability to other dense retrieval architectures beyond DPR requires additional validation.

**Low Confidence**: The assertion that this represents a fundamentally new class of backdoor attacks (public/unintentional triggers) is somewhat overstated - similar concepts exist in other domains, though the specific application to grammatical errors in retrieval is novel.

## Next Checks

1. **Error Distribution Validation**: Conduct user studies to measure actual grammatical error frequencies across different demographics and compare against the NUCLE/W&I distributions used in the attack. This would validate whether the chosen error types serve as reliable public triggers.

2. **Defense Robustness Testing**: Evaluate the attack against a broader range of defenses including query expansion techniques, adversarial training with perturbed queries, and corpus filtering using more sophisticated semantic similarity metrics beyond BM25.

3. **Cross-Architecture Transferability**: Test whether the attack generalizes to other dense retrieval architectures (e.g., ANCE, Condenser, SPLADE) and whether different contrastive learning objectives (e.g., InfoNCE variants) exhibit similar vulnerabilities to grammatical error-based poisoning.