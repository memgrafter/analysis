---
ver: rpa2
title: 'RAEE: A Robust Retrieval-Augmented Early Exiting Framework for Efficient Inference'
arxiv_id: '2405.15198'
source_url: https://arxiv.org/abs/2405.15198
tags:
- exiting
- early
- raee
- inference
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RAEE, a retrieval-augmented early exiting framework
  for efficient inference. RAEE models the early exiting problem as a distribution
  prediction problem and demonstrates that the exiting distribution can be approximated
  using similar data's exiting information.
---

# RAEE: A Robust Retrieval-Augmented Early Exiting Framework for Efficient Inference

## Quick Facts
- arXiv ID: 2405.15198
- Source URL: https://arxiv.org/abs/2405.15198
- Reference count: 40
- Key outcome: RAEE achieves 63.27% average accuracy with 38.69ms latency on GLUE tasks, outperforming existing early exiting methods

## Executive Summary
This paper proposes RAEE, a retrieval-augmented early exiting framework for efficient inference on large language models. The key insight is that the early exiting problem can be modeled as a distribution prediction problem where the optimal exit layer for a given input can be approximated using the exiting information of similar training samples. RAEE builds a retrieval database containing exiting information from training data and uses nearest neighbor search to guide the backbone model to exit at the predicted layer. Experimental results demonstrate that RAEE significantly accelerates model inference while achieving robust zero-shot performance across 8 GLUE benchmark tasks.

## Method Summary
RAEE operates by first building a retrieval database that stores embeddings and exiting information from training data. During inference, for each input, the framework encodes it to obtain a query embedding, retrieves the top-k nearest neighbors from the database, and aggregates their exiting information to predict the optimal exit layer. The backbone model then performs a forward pass only up to the predicted layer, significantly reducing computation. The framework demonstrates that exiting layer prediction can be effectively approximated using similar data's exiting information, and that the retrieval database can act as an error corrector by leveraging correctly classified examples.

## Key Results
- RAEE-RoBERTa achieves 63.27% average accuracy across 8 GLUE tasks
- Average inference latency of 38.69ms, significantly faster than baseline methods
- Performance degrades from 63.27% to 62.83% when k exceeds 12 neighbors
- Outperforms existing early exiting methods while maintaining robust zero-shot performance

## Why This Works (Mechanism)

### Mechanism 1
The exiting layer prediction can be approximated by the exiting information of similar data samples. RAEE uses nearest neighbor retrieval to find training samples with similar embeddings, then aggregates their exiting layer information to predict where the current sample should exit. This works under the assumption that similar data samples exhibit similar patterns in their optimal exiting layers. The experimental results in Figure 2(a) and 2(b) reveal that the exiting probabilities exhibit a similar pattern to those of the nearest neighbors.

### Mechanism 2
The retrieval database acts as an error corrector by providing exiting information from correctly predicted examples. The database contains exiting information only from training samples where intermediate layers correctly predicted the label, allowing the model to leverage these successful patterns during inference. This assumes that some training samples can be correctly classified by intermediate layers, and this information is valuable for guiding new samples. The ablation study shows RAEE w/o refers to the method built on only correctly predicted examples.

### Mechanism 3
The framework can exit earlier than the backbone model while maintaining or improving accuracy. By leveraging exiting information from similar samples that exited early, the framework can identify when current samples can also exit early without loss of accuracy. This works under the assumption that if similar samples can exit early successfully, the current sample likely can too. When k exceeds 12 neighbors, the overall performance degrades from 63.27 to 62.83, suggesting earlier exit is possible with fewer neighbors.

## Foundational Learning

- **Concept: Distribution approximation using similar samples**
  - Why needed here: The framework needs to estimate the probability distribution of optimal exiting layers for new samples based on training data
  - Quick check question: How would you compute P(z=l|x) using k nearest neighbors with weights based on distance?

- **Concept: Approximate nearest neighbor search**
  - Why needed here: The framework needs efficient retrieval of similar samples from potentially millions of training examples during inference
  - Quick check question: What trade-offs exist between exact and approximate nearest neighbor search in terms of speed vs accuracy?

- **Concept: Embedding similarity metrics**
  - Why needed here: The framework relies on embedding similarity to find relevant training samples for each new input
  - Quick check question: How would cosine similarity differ from Euclidean distance when comparing high-dimensional embeddings?

## Architecture Onboarding

- **Component map:**
  - Input → Encoder → Query embedding
  - Query embedding → Retriever → Top-k neighbors
  - Neighbors → Distribution approximator → Exit layer prediction
  - Exit layer → Backbone forward pass → Intermediate output
  - Intermediate output → Final prediction layer → Final output

- **Critical path:**
  1. Input → Encoder → Query embedding
  2. Query embedding → Retriever → Top-k neighbors
  3. Neighbors → Distribution approximator → Exit layer prediction
  4. Exit layer → Backbone forward pass → Intermediate output
  5. Intermediate output → Final prediction layer → Final output

- **Design tradeoffs:**
  - Number of neighbors (k): More neighbors provide better approximation but increase computation
  - Embedding quality: Better embeddings lead to more relevant neighbors but may require more complex encoders
  - Database size: Larger databases capture more patterns but increase memory and retrieval time
  - Exit threshold: Stricter thresholds ensure accuracy but may reduce acceleration benefits

- **Failure signatures:**
  - Early exits with poor accuracy: Similarity metric may be capturing wrong features
  - Late exits with good accuracy: Distribution approximation may be too conservative
  - High variance in exit layers: Retrieval quality may be inconsistent across input types
  - Memory issues: Database may be too large for deployment constraints

- **First 3 experiments:**
  1. Vary k from 2 to 20 and measure accuracy/latency trade-off to find optimal value
  2. Test different embedding methods (encoder vs backbone embeddings) on a subset of tasks
  3. Compare RAEE performance when database contains only correctly vs incorrectly classified examples

## Open Questions the Paper Calls Out

- **Open Question 1:** What is the impact of retrieval database size on RAEE's performance for different backbone models (e.g., RoBERTa, T5, Llama, Gemma)?
  - Basis in paper: The paper shows that increasing the retrieval database size improves RAEE-RoBERTa's performance from 60.85 to 63.27 on average. However, it does not explore the impact for other backbone models.
  - Why unresolved: The paper only provides results for RoBERTa and does not analyze how the retrieval database size affects performance for other backbone models.
  - What evidence would resolve it: Experimental results comparing RAEE's performance with different retrieval database sizes for each backbone model (RoBERTa, T5, Llama, Gemma) across multiple tasks.

- **Open Question 2:** How does the choice of retrieval indexing method affect RAEE's performance and efficiency?
  - Basis in paper: The paper mentions that various types of indexing can be chosen for building the retrieval databases and that different retrievers have different retrieving qualities. However, it does not explore the impact of different indexing methods.
  - Why unresolved: The paper does not provide a comparison of different retrieval indexing methods or their impact on RAEE's performance and efficiency.
  - What evidence would resolve it: Experimental results comparing RAEE's performance and efficiency using different retrieval indexing methods (e.g., FAISS, HNSW, Annoy) across multiple tasks.

- **Open Question 3:** Can RAEE be effectively applied to fine-tuning scenarios?
  - Basis in paper: The paper mentions that exploring the performance of applying RAEE in fine-tuning scenarios is worthwhile for future work.
  - Why unresolved: The paper does not provide any experimental results or analysis on applying RAEE to fine-tuning scenarios.
  - What evidence would resolve it: Experimental results comparing RAEE's performance in fine-tuning scenarios versus zero-shot scenarios across multiple tasks and backbone models.

## Limitations

- The framework's performance critically depends on the quality of the retrieval database, with potential issues if training and test distributions differ significantly
- Heavy reliance on embedding similarity may fail if the similarity metric doesn't capture relevant semantic relationships between inputs
- Zero-shot generalization claims are based on GLUE benchmark tasks and may not hold for out-of-distribution or domain-shifted datasets

## Confidence

- **Retrieval-Based Distribution Approximation**: High confidence - experimental results show exiting probabilities exhibit similar patterns across nearest neighbors
- **Error Correction Through Retrieval**: Medium confidence - ablation studies support the claim but need more extensive validation across diverse datasets
- **Performance Improvements**: High confidence - systematic experiments and comparisons with baseline methods provide strong evidence

## Next Checks

1. **Distribution Shift Robustness Test**: Evaluate RAEE performance on out-of-distribution datasets or through controlled distribution shifts to quantify how well the retrieval database generalizes beyond the training distribution.

2. **Similarity Metric Ablation**: Systematically test different embedding methods and similarity metrics (cosine vs Euclidean, encoder vs backbone embeddings) to quantify their impact on exiting layer prediction accuracy and overall framework performance.

3. **Database Size vs Performance Analysis**: Conduct experiments varying the retrieval database size from small (10K examples) to large (full training set) to identify the point of diminishing returns and understand the scalability limits of the framework.