---
ver: rpa2
title: Rethinking the Effectiveness of Graph Classification Datasets in Benchmarks
  for Assessing GNNs
arxiv_id: '2407.04999'
source_url: https://arxiv.org/abs/2407.04999
tags:
- graph
- datasets
- performance
- dataset
- effectiveness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the critical question of whether commonly used
  graph classification benchmarks effectively distinguish GNN advancements from other
  methods. To investigate this, we propose an empirical protocol that fairly compares
  performance gaps between simple baselines and GNN-based methods, decoupling structural
  and attributed performance gaps.
---

# Rethinking the Effectiveness of Graph Classification Datasets in Benchmarks for Assessing GNNs

## Quick Facts
- **arXiv ID**: 2407.04999
- **Source URL**: https://arxiv.org/abs/2407.04999
- **Authors**: Zhengdao Li; Yong Cao; Kefan Shuai; Yiming Miao; Kai Hwang
- **Reference count**: 9
- **Primary result**: Approximately half of commonly used graph classification benchmarks lack sufficient discriminative power to effectively distinguish GNN advancements from simpler methods.

## Executive Summary
This work critically examines whether commonly used graph classification benchmarks effectively distinguish GNN advancements from simpler methods. Through a novel effectiveness metric that normalizes performance gaps while considering dataset complexity and class label diversity, the authors demonstrate that approximately half of 16 real-world datasets lack sufficient discriminative power. The study reveals strong correlations between basic graph properties and class labels in many datasets, enabling simple baselines to perform well and reducing the effectiveness of these benchmarks. To enable controlled exploration, the authors develop a method for generating synthetic datasets with precise control over correlations between graph properties and labels.

## Method Summary
The authors propose an empirical protocol that fairly compares performance gaps between simple baselines and GNN-based methods, decoupling structural and attributed performance gaps. They introduce a novel Effectiveness metric (E) that normalizes performance gaps by considering dataset complexity and class label diversity. The framework involves collecting 16 real-world datasets, implementing baseline methods (MLPs with average degrees, MoleculeFingerprint) and GNN models (GIN, GCN), conducting experiments to compute performance gaps using 10-fold cross-validation, calculating the effectiveness metric, analyzing correlations between graph properties and labels, and generating synthetic datasets with controllable correlations to validate findings.

## Key Results
- Approximately half of commonly used graph classification datasets lack sufficient discriminative power
- Strong correlations exist between basic graph properties (degree, clustering coefficient) and class labels in many datasets
- Simple baselines perform well on datasets where graph properties strongly correlate with labels
- The proposed effectiveness metric successfully identifies datasets with low discriminative power
- Synthetic dataset generation enables controlled study of how property-label correlations affect dataset effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The effectiveness metric E normalizes performance gaps by accounting for dataset complexity and number of class labels, providing a fair comparison across datasets with different characteristics.
- Mechanism: The metric scales the performance gap by the worst method accuracy and the number of classes, ensuring that simple datasets with large gaps due to low complexity don't appear artificially effective.
- Core assumption: The worst method accuracy reflects dataset complexity and that random guessing accuracy is (1/(|Y|-1)) for |Y classes.
- Evidence anchors:
  - [abstract]: "We further propose a novel metric to quantify the dataset effectiveness by considering both dataset complexity and model performance."
  - [section 3.1]: Definition 2 provides the mathematical formulation E(D) = Σtype∈{S,A} |δtype(D)| / [R*(|Y|-1) · (1 - R*)/(1 - (|Y|-1))]
  - [corpus]: Weak - no direct citations found, but the approach aligns with recent graph learning benchmarking trends.
- Break condition: If worst method accuracy approaches 1 (perfect classification), the denominator approaches zero, making E undefined or extremely large.

### Mechanism 2
- Claim: Strong correlations between basic graph properties and class labels enable simple baselines to perform well, reducing the discriminative power of the dataset.
- Mechanism: When properties like node count, average degree, or clustering coefficient are highly correlated with labels, simple models using these features can achieve high accuracy without learning complex graph structures.
- Core assumption: Graph properties can be computed cheaply and are good predictors of labels in some datasets.
- Evidence anchors:
  - [section 4.1]: "Most datasets show strong correlations between graph properties and labels, prompting us to explore predicting dataset effectiveness using these properties"
  - [section 4.2]: Describes synthetic datasets where controlled correlations between properties and labels directly affect baseline performance
  - [corpus]: Weak - no direct citations found, but the approach is consistent with prior work on graph kernel methods.
- Break condition: If correlations are weak or non-existent, simple baselines will perform poorly and the dataset will show high effectiveness.

### Mechanism 3
- Claim: The controllable synthetic dataset generation method enables systematic study of how property-label correlations affect dataset effectiveness.
- Mechanism: By generating graphs with precise control over correlation coefficients between properties and labels, researchers can create datasets that isolate the effect of correlation on model performance.
- Core assumption: Theorem 1 provides a valid method for generating correlated random variables that can be mapped to graph properties and labels.
- Evidence anchors:
  - [section 4.2]: "We introduce a method to generate controllable datasets, enabling precise modulation of the correlations between any graph properties and class labels"
  - [section 4.2]: Provides the mathematical formulation and algorithm for generating correlated variables
  - [corpus]: Weak - no direct citations found, but the approach builds on established statistical methods.
- Break condition: If the correlation generation method doesn't produce the desired correlations, or if the mapping from random variables to graph properties doesn't preserve the intended relationships.

## Foundational Learning

- Concept: Graph neural networks and their expressive power
  - Why needed here: Understanding why GNNs might not outperform simple baselines requires knowledge of what GNNs can and cannot learn
  - Quick check question: What is the key limitation of message-passing GNNs identified in prior work that might explain poor performance on some datasets?

- Concept: Graph kernels and their relationship to GNNs
  - Why needed here: The paper compares GNNs to graph-kernel methods, so understanding both approaches is essential
  - Quick check question: How do graph kernels like WL-GK differ from GNNs in terms of the graph features they can capture?

- Concept: Statistical correlation and its interpretation
  - Why needed here: The paper extensively analyzes correlations between graph properties and labels, requiring understanding of correlation coefficients and their meaning
  - Quick check question: What does a Pearson correlation coefficient of 0.8 between a graph property and labels indicate about their relationship?

## Architecture Onboarding

- Component map: Dataset loading (from PG, OGB, TUDataset) -> Preprocessing (feature engineering) -> Model training (baselines, GNNs, graph kernels) -> Evaluation (cross-validation) -> Analysis (effectiveness calculation, correlation analysis) -> Synthetic data generation
- Critical path: Load dataset → Preprocess features → Train multiple models → Evaluate performance gaps → Calculate effectiveness → Analyze correlations → Generate synthetic data if needed
- Design tradeoffs: The framework prioritizes fairness and comprehensiveness over speed, using k-fold cross-validation and multiple model types rather than quick single-run evaluations
- Failure signatures: Low effectiveness values across multiple datasets suggest the benchmark suite may not be discriminating enough; high correlations between properties and labels indicate simple baselines may be sufficient
- First 3 experiments:
  1. Run the framework on a simple dataset (like MUTAG) to verify all components work and understand the output format
  2. Compare effectiveness values calculated using accuracy vs AUC-ROC metrics to verify consistency
  3. Generate a synthetic dataset with controlled correlation and verify that effectiveness changes as expected with correlation strength

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different graph property correlations (beyond degree and clustering coefficient) affect dataset effectiveness, and what are the relative contributions of various structural properties?
- Basis in paper: [explicit] The paper investigates correlations between basic graph properties (degree, clustering coefficient, cycles) and class labels, but primarily focuses on degree and clustering coefficient in synthetic dataset generation
- Why unresolved: The study only systematically explores correlations between labels and two graph properties (degree and clustering coefficient) in synthetic datasets, leaving other important properties unexplored
- What evidence would resolve it: Controlled experiments generating synthetic datasets with varying correlations between different combinations of graph properties (e.g., cycle counts, graphlet distributions, spectral properties) and labels

### Open Question 2
- Question: What is the relationship between dataset size, class distribution, and effectiveness, and how do these factors interact with the proposed effectiveness metric?
- Basis in paper: [inferred] The paper examines 16 real-world datasets with varying scales and number of classes, but does not systematically investigate how dataset size or class imbalance affects effectiveness measurements
- Why unresolved: While the effectiveness metric accounts for number of classes, the paper does not explore how dataset size or class imbalance might influence the metric's reliability or interpretation
- What evidence would resolve it: Controlled experiments varying dataset sizes and class distributions while measuring effectiveness across different dataset configurations

### Open Question 3
- Question: How does the effectiveness metric generalize across different graph learning tasks beyond classification, such as link prediction or node classification?
- Basis in paper: [explicit] The paper specifically defines and validates the effectiveness metric for graph classification tasks, with no discussion of its applicability to other graph learning problems
- Why unresolved: The metric is derived from classification-specific considerations (accuracy, AUC-ROC) and normalized by number of classes, making its extension to other tasks unclear
- What evidence would resolve it: Application and validation of the effectiveness metric framework to link prediction and node classification benchmarks, potentially requiring task-specific modifications to the metric components

## Limitations

- The generalizability of the effectiveness metric across different graph learning tasks beyond classification remains unclear
- The robustness of synthetic dataset generation to different graph distribution assumptions has not been fully validated
- The impact of dataset size and class imbalance on effectiveness measurements is not systematically explored

## Confidence

- Effectiveness metric formulation: High confidence
- Correlation analysis results: Medium-High confidence
- Synthetic dataset generation: Medium confidence
- Regression model predictions: Medium confidence

## Next Checks

1. Test the effectiveness metric on additional graph learning tasks (regression, link prediction) to assess generalizability
2. Validate synthetic dataset generation by comparing generated distributions to real-world datasets
3. Conduct ablation studies on different graph properties to identify which features most strongly predict effectiveness