---
ver: rpa2
title: 'Implementing engrams from a machine learning perspective: the relevance of
  a latent space'
arxiv_id: '2407.16616'
source_url: https://arxiv.org/abs/2407.16616
tags:
- latent
- space
- neural
- neurons
- dimension
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This brief note examines the relevance of the latent space in autoencoders
  proposed as biological implementations of engrams in the brain. The analysis focuses
  on the relationship between the dimensionality of these autoencoders and the complexity
  of the information being encoded.
---

# Implementing engrams from a machine learning perspective: the relevance of a latent space

## Quick Facts
- arXiv ID: 2407.16616
- Source URL: https://arxiv.org/abs/2407.16616
- Authors: J Marco de Lucas
- Reference count: 4
- Primary result: Latent space dimensionality in autoencoders determines cognitive capacity limits in biological neural networks, unlike machine learning systems which can theoretically scale without bound

## Executive Summary
This theoretical analysis examines how latent space dimensionality in autoencoders constrains cognitive complexity in biological neural networks, proposing that differences in species' connectomes may reflect their cognitive capacities. The paper argues that while biological systems face inherent dimensional limits based on neural architecture, machine learning systems can theoretically build arbitrarily complex systems limited only by hardware evolution. The author presents a framework for implementing engrams using autoencoders with basic excitatory/inhibitory motifs, where credit assignment derives from homeostatic criteria rather than backpropagation.

## Method Summary
The paper presents a theoretical framework for biological implementation of engrams using autoencoders with recurrent neural networks and basic excitatory/inhibitory motifs. The analysis draws on connectome data from C. Elegans, rat neocortical microcircuitry, and human pyramidal neuron studies, combined with intrinsic dimension estimation techniques applied to image datasets. The proposed model uses XOR neuronal motifs for error computation and concept neurons for multimodal indexing, with credit assignment derived from homeostatic mechanisms rather than gradient descent.

## Key Results
- Biological neural networks face cognitive capacity limits determined by latent space dimensionality, unlike machine learning systems
- Species connectome differences may correlate with cognitive capabilities through latent space dimension constraints
- XOR motifs can theoretically implement loss functions and credit assignment in biological autoencoders without backpropagation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Latent space dimensionality directly constrains cognitive complexity processing
- Mechanism: Neural networks can only encode information if latent space has sufficient dimensions to capture data's intrinsic dimensionality
- Core assumption: Real-world data has finite intrinsic dimension and brain latent spaces are biologically limited
- Evidence anchors: Abstract discusses species connectome differences linked to cognitive capacities; section 2 states latent space must be ≥ data dimension
- Break condition: If brain uses sparse coding or incorrect intrinsic dimension estimates

### Mechanism 2
- Claim: Concept neurons index multimodal information through shared latent space dimensions
- Mechanism: Single concept neuron connects to specific latent space subsets, creating index for unified episodic memories
- Core assumption: Hebbian learning allows neurons to establish selective connections to meaningful concept representations
- Evidence anchors: Section 4 proposes concept neurons in hippocampus; section 3 discusses quantized connections
- Break condition: If brain connectivity doesn't support selective indexing

### Mechanism 3
- Claim: XOR neuronal motifs implement loss functions and credit assignment without backpropagation
- Mechanism: Excitatory/inhibitory circuits using XOR logic compare outputs with inputs, providing error signals through homeostatic mechanisms
- Core assumption: Biological neurons can implement XOR logic for error minimization
- Evidence anchors: Abstract mentions XOR motifs implementing loss functions; section 1 discusses homeostatic credit assignment
- Break condition: If neurons cannot implement XOR logic efficiently

## Foundational Learning

- Concept: Intrinsic dimensionality of data
  - Why needed here: Core argument depends on understanding real-world data has lower-dimensional manifold structure that determines compression limits
  - Quick check question: If ImageNET images have intrinsic dimension ~50 but autoencoder latent space dimension 10, what happens to reconstruction quality?

- Concept: Autoencoder architecture components
  - Why needed here: Essential to understand encoder-decoder-latent space relationship and why dimensionality matters for information storage
  - Quick check question: If decoder is more complex than encoder, does this change fundamental information capacity constraint?

- Concept: Connectome analysis and graph theory
  - Why needed here: Links species cognitive differences to connectome structure, requiring understanding of neuronal connectivity pattern analysis
  - Quick check question: If rat neocortical microcircuitry shows maximum clique dimension 15, what does this suggest about potential latent space dimensions?

## Architecture Onboarding

- Component map: Input layer → Encoder network → Latent space (N neurons) → Decoder network → Output layer
- Critical path: Data → Encoder processing → Latent space encoding → Concept neuron activation → Decoder reconstruction → Output comparison
- Design tradeoffs:
  - Higher latent space dimension → More cognitive capacity but more biological resources required
  - Simpler XOR motifs → Biologically plausible but may limit learning complexity
  - More concept neurons → Better indexing but increased connectivity demands
- Failure signatures:
  - Poor reconstruction quality → Latent space dimension too small for data complexity
  - Slow learning → XOR motifs insufficient for credit assignment needs
  - Limited multimodal integration → Inadequate concept neuron connectivity
- First 3 experiments:
  1. Build autoencoder with varying latent space dimensions (10, 50, 100) on ImageNET subset, measure reconstruction quality vs intrinsic dimension estimates
  2. Implement XOR motif circuits in simulation, test learning performance on XOR and linearly separable problems vs backpropagation
  3. Create concept neuron indexing system, test multimodal association learning with paired visual/auditory stimuli

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between intrinsic data dimension and required latent space dimension in biological neural networks?
- Basis in paper: [explicit] Paper states latent space must be ≥ data dimension but notes this is not 1:1 correspondence
- Why unresolved: Biological networks are overparametrized and can be pruned without losing performance
- What evidence would resolve it: Empirical studies mapping data complexity to neural connectome structures across species

### Open Question 2
- Question: How does latent space structure in biological neural networks differ from simple n-dimensional vectors?
- Basis in paper: [explicit] Paper mentions deeper non-linear structure and potential recurrent structure within latent space
- Why unresolved: Current connectome mapping may not capture full complexity, particularly for recurrent structures
- What evidence would resolve it: Advanced connectome mapping with temporal dynamics analysis

### Open Question 3
- Question: What are specific mechanisms by which inhibitory neurons contribute to cognitive potential limited by latent space dimension?
- Basis in paper: [explicit] Paper discusses relevance of inhibitory structures in determining cognitive capacity
- Why unresolved: Paper briefly mentions inhibitory neurons' role without detailing specific mechanisms
- What evidence would resolve it: Detailed mapping of inhibitory-excitatory interactions correlated with cognitive tasks

## Limitations
- Theoretical framework lacks empirical validation for biological implementation of autoencoders
- Assumes direct relationship between latent space dimensionality and cognitive capacity without experimental evidence
- XOR motif-based credit assignment mechanism lacks biological validation in complex cognitive tasks

## Confidence

**Confidence Assessment:**
- Mechanism 1 (dimensionality constraints): Medium confidence - supported by established autoencoder theory but speculative for biological systems
- Mechanism 2 (concept neuron indexing): Low confidence - largely theoretical with minimal supporting evidence
- Mechanism 3 (XOR credit assignment): Low confidence - proposed mechanism lacks empirical validation

## Next Checks

1. Test whether estimated intrinsic dimensionality of real-world datasets correlates with reported cognitive complexity differences between species, controlling for connectome size

2. Implement XOR motif-based learning circuits in spiking neural network simulations and compare learning efficiency against backpropagation on benchmark problems

3. Analyze existing connectome data for evidence of concept neuron-like connectivity patterns that could support multimodal indexing through latent space dimensions