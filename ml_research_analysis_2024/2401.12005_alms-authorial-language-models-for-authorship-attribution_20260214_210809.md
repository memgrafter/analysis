---
ver: rpa2
title: 'ALMs: Authorial Language Models for Authorship Attribution'
arxiv_id: '2401.12005'
source_url: https://arxiv.org/abs/2401.12005
tags:
- authorship
- alms
- attribution
- text
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Authorial Language Models (ALMs), a method
  for authorship attribution that uses fine-tuned causal language models for each
  candidate author and selects the author whose model yields the lowest perplexity
  for a questioned text. ALMs achieved state-of-the-art performance on the Blogs50
  dataset with a macro-average accuracy of 83.6% and matched the best performance
  on CCAT50 with 74.9%.
---

# ALMs: Authorial Language Models for Authorship Attribution

## Quick Facts
- arXiv ID: 2401.12005
- Source URL: https://arxiv.org/abs/2401.12005
- Reference count: 40
- Primary result: Achieved state-of-the-art performance on Blogs50 with 83.6% macro-average accuracy

## Executive Summary
This paper introduces Authorial Language Models (ALMs), a novel approach to authorship attribution that fine-tunes separate causal language models for each candidate author and selects the author whose model yields the lowest perplexity for a questioned text. The method leverages modern language modeling techniques to capture authorial patterns at the token level, avoiding the limitations of traditional type-based frequency methods. ALMs demonstrates strong performance on two benchmark datasets, achieving state-of-the-art results on Blogs50 and matching the best performance on CCAT50.

## Method Summary
ALMs fine-tunes a separate GPT-2 model for each candidate author using their writing samples. For authorship attribution, the method calculates the perplexity of a questioned text using each author's fine-tuned model and selects the author with the lowest perplexity score. The approach uses token-level probabilities rather than type-based frequencies, allowing it to capture more granular authorial patterns. Each model is fine-tuned for 100 epochs, and attribution decisions are made based on cross-entropy loss calculations converted to perplexity scores.

## Key Results
- ALMs achieved 83.6% macro-average accuracy on Blogs50, state-of-the-art performance
- On CCAT50, ALMs matched the best performance at 74.9% macro-average accuracy
- Ablation testing showed ALMs requires 40 tokens on Blogs50 and 400 tokens on CCAT50 to reach 70% accuracy

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning separate language models per author captures unique authorial patterns that single-language models miss. Each fine-tuned model learns author-specific token-level probabilities, creating distinctive probability distributions for each writer. This assumes authorial style manifests in predictable token-level patterns that can be learned by language models.

### Mechanism 2
Token-level probability comparisons are more robust to topical variation than type-based frequency methods. Token-level probabilities automatically adjust for topic by assigning lower probabilities to topic-specific words across all models, while preserving authorial style in grammatical and structural choices. This assumes topic influences word choice but not the underlying probability distributions of grammatical and structural patterns.

### Mechanism 3
Longer texts provide more reliable perplexity scores because they capture more stable authorial patterns. Perplexity becomes more stable as more tokens are evaluated, reducing variance in the attribution decision. This assumes authorial patterns are consistent enough across longer text spans to yield stable probability distributions.

## Foundational Learning

- Concept: Perplexity as a measure of text predictability
  - Why needed here: Understanding how perplexity quantifies how well a language model predicts a given text is fundamental to grasping why lower perplexity indicates likely authorship
  - Quick check question: If a language model predicts each token with probability 0.1, what is the perplexity of that text?

- Concept: Tokenization and BPE (Byte-Pair Encoding)
  - Why needed here: The method uses GPT-2's BPE tokenizer, so understanding how text is broken into tokens is crucial for interpreting perplexity calculations
  - Quick check question: Why might the same word appear as different tokens in different contexts in a BPE tokenizer?

- Concept: Causal language model architecture
  - Why needed here: The method uses causal language models where each token is predicted based only on previous tokens, affecting how perplexity is calculated
  - Quick check question: How does a causal language model differ from a masked language model in terms of token prediction?

## Architecture Onboarding

- Component map:
  - Data preparation pipeline (tokenization, splitting)
  - Fine-tuning infrastructure (one model per author)
  - Inference pipeline (perplexity calculation)
  - Decision logic (lowest perplexity selection)
  - Evaluation framework (accuracy calculation)

- Critical path:
  1. Tokenize questioned document
  2. Calculate perplexity with each author's fine-tuned model
  3. Select author with minimum perplexity
  4. Report accuracy metrics

- Design tradeoffs:
  - Fine-tuning time vs. attribution accuracy (100 epochs chosen)
  - Number of candidate authors vs. computational cost
  - Model architecture choice (GPT-2 vs alternatives)
  - Token count threshold vs. minimum viable accuracy

- Failure signatures:
  - Low accuracy across all authors suggests insufficient fine-tuning or poor model architecture
  - Random author selection indicates perplexity values are too similar across models
  - High variance in accuracy across authors suggests some models learn better than others

- First 3 experiments:
  1. Run attribution on a single author's text with that author in the candidate set - should yield near-perfect accuracy
  2. Compare perplexity distributions between correct and incorrect attributions to identify decision boundaries
  3. Test ablation by gradually reducing text length to find the minimum viable token count for your dataset

## Open Questions the Paper Calls Out

### Open Question 1
How do different hyper-parameter settings affect the performance of ALMs? The paper states "Currently, our evaluation of ALMs was made under a fixed hyper-parameter settings where other combination of hyper-parameters remain untested."

### Open Question 2
To what extent do topical patterns in the datasets influence the benchmarking scores of ALMs and other methods? The authors note "we also note that topical patterns in both datasets may positively influence the benchmarking scores for ALMs and other methods."

### Open Question 3
How does ALMs perform on datasets with a larger number of authors or more diverse text types? The current evaluation is limited to two specific datasets (Blogs50 and CCAT50), and the generalizability of ALMs to other contexts is unknown.

## Limitations
- The exact GPT-2 model configuration used for fine-tuning is unspecified, affecting reproducibility
- Method's effectiveness depends on having sufficient training data per author - requires 400 tokens on CCAT50 to reach 70% accuracy
- Claims of "state-of-the-art" performance are limited as comparison only includes two other methods on two datasets

## Confidence
- High Confidence: The core mechanism of using fine-tuned language models per author and selecting based on perplexity is clearly explained
- Medium Confidence: The claim that token-level probabilities better handle topical variation than type-based frequency methods is plausible but not rigorously proven
- Low Confidence: The assertion that ALMs achieves "state-of-the-art" performance requires broader benchmarking against the full spectrum of authorship attribution approaches

## Next Checks
1. Test multiple GPT-2 variants (small, medium, large) to determine which configuration was likely used and assess sensitivity to model size
2. Create controlled experiments where the same author writes on different topics and measure how well ALMs separates authorial style from topical content compared to traditional frequency-based methods
3. Apply ALMs trained on Blogs50 to CCAT50 (and vice versa) to test whether the models learn generalizable authorial features or overfit to dataset-specific patterns