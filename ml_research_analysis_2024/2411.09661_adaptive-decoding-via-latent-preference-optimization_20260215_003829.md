---
ver: rpa2
title: Adaptive Decoding via Latent Preference Optimization
arxiv_id: '2411.09661'
source_url: https://arxiv.org/abs/2411.09661
tags:
- adaptive
- temperature
- decoder
- decoding
- fixed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Adaptive Decoding, a method that dynamically
  selects the optimal sampling temperature for each token or example during language
  model generation, using a learned neural module called the ADAPTIVE DECODER. This
  approach addresses the challenge of balancing creativity and factual accuracy across
  diverse tasks, which is difficult with fixed temperature settings.
---

# Adaptive Decoding via Latent Preference Optimization

## Quick Facts
- arXiv ID: 2411.09661
- Source URL: https://arxiv.org/abs/2411.09661
- Authors: Shehzaad Dhuliawala, Ilia Kulikov, Ping Yu, Asli Celikyilmaz, Jason Weston, Sainbayar Sukhbaatar, Jack Lanchantin
- Reference count: 19
- Primary result: Adaptive Decoding dynamically selects optimal sampling temperature for each token or example, outperforming all fixed temperature strategies with winrates consistently above 50%

## Executive Summary
This paper introduces Adaptive Decoding, a method that dynamically selects the optimal sampling temperature during language model generation using a learned neural module called the ADAPTIVE DECODER. The approach addresses the challenge of balancing creativity and factual accuracy across diverse tasks, which is difficult with fixed temperature settings. To train the ADAPTIVE DECODER, the authors propose Latent Preference Optimization (LPO), a general method for optimizing discrete latent variables like temperature. Experiments on tasks such as math reasoning, creative writing, and general instruction following show that Adaptive Decoding outperforms all fixed temperature strategies, with winrates consistently above 50% and often significantly higher.

## Method Summary
Adaptive Decoding adds a learned ADAPTIVE DECODER module to a frozen LLM (Llama 3.0-8B-Instruct) that predicts a temperature distribution for each generation step or example. During training, multiple responses are generated per prompt using temperatures sampled from the ADAPTIVE DECODER's output, then evaluated by a reward model to create preference pairs. The ADAPTIVE DECODER is trained using Latent Preference Optimization (LPO), which updates parameters to increase the probability of temperatures that led to high-scoring responses. The method supports both token-level adaptation (selecting temperatures per token) and sequence-level adaptation (selecting temperatures per example), with token-level enabling fine-grained control such as being greedy on constraint tokens and creative elsewhere.

## Key Results
- Adaptive Decoding consistently outperforms all fixed temperature strategies (0.0, 0.2, 0.4, 0.6, 0.8, 1.0) across diverse tasks with winrates above 50%
- Token-level adaptation enables fine-grained control, such as being greedy on constraint tokens (τ=0.21) and creative on others (τ=0.55) for constrained creative writing
- LPO with temperature-as-latents is as effective as simpler variants and better than negative log-likelihood training
- ADAPTIVE DECODER improves majority voting accuracy by learning to sample more informative reasoning chains for math tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ADAPTIVE DECODER learns to select task-appropriate temperatures by observing which sampled temperatures lead to higher reward model scores.
- Mechanism: During LPO training, multiple responses are generated per prompt using temperatures sampled from the ADAPTIVE DECODER's output distribution. The reward model scores these responses, and preference pairs are created from the highest and lowest scoring responses. The LPO loss then updates the ADAPTIVE DECODER parameters to increase the probability of temperatures that led to chosen (high-scoring) responses.
- Core assumption: The reward model provides meaningful relative quality scores that correlate with task requirements.
- Evidence anchors:
  - [abstract]: "LPO involves sampling multiple responses from the model, where the ADAPTIVE DECODER layer will select temperatures (latent variables) that will affect the final token choices. Those responses are then evaluated by a reward model in order to build chosen and rejected preference pair examples."
  - [section 3.3]: "The highest and lowest scoring responses become our chosen and rejected response pair(yc, yr)."

### Mechanism 2
- Claim: Token-level temperature adaptation allows the model to be greedy on constraint tokens and creative on others within a single response.
- Mechanism: The ADAPTIVE DECODER tok predicts a temperature distribution at each generation step. For constrained creative writing tasks, it learns to predict low temperatures for constraint tokens (e.g., first token of each sentence starting with "Ab") to ensure constraint satisfaction, while predicting higher temperatures for other tokens to maximize creativity.
- Core assumption: The LPO training can capture fine-grained token-level preferences through preference pairs that reward both constraint satisfaction and creativity.
- Evidence anchors:
  - [section 4.4]: "The average temperature for the first token of each sentence is τ = 0.21, and the average temperature for all other tokens is τ = 0.55. This shows that the model is mostly greedy on the constraint tokens... and mostly non-greedy on all other tokens."
  - [section 3.2]: "In the token level variant, ADAPTIVE DECODER tok (AD seq), a temperature is predicted for each new token to be decoded."

### Mechanism 3
- Claim: The ADAPTIVE DECODER improves majority voting accuracy by learning to sample more informative reasoning chains.
- Mechanism: When trained on GSM8K with LPO, the ADAPTIVE DECODER tok learns which parts of reasoning chains benefit from higher temperature sampling to explore alternative solution paths. During majority voting, these more diverse yet still accurate chains lead to better answer aggregation.
- Core assumption: Different parts of a reasoning chain have different optimal temperature requirements, and the ADAPTIVE DECODER can learn these distinctions.
- Evidence anchors:
  - [section 4.5]: "The ADAPTIVE DECODER tok learns to assign temperatures appropriately and we observe that the higher temperature options help the model's performance."
  - [section 4.5]: "This demonstrates that the ADAPTIVE DECODER tok trained by LPO can result in a model that can perform well on both single responses... and majority voting setups at the same time."

## Foundational Learning

- Concept: Temperature scaling in softmax
  - Why needed here: Understanding how temperature affects the probability distribution is fundamental to grasping why Adaptive Decoding works
  - Quick check question: If you have a softmax distribution with probabilities [0.7, 0.2, 0.1] and apply temperature τ=0.5, will the distribution become more peaked or more uniform?

- Concept: Preference optimization and DPO
  - Why needed here: LPO is a generalization of DPO to latent variables, so understanding DPO's mechanics is essential
  - Quick check question: In DPO, why do we compare log P(yc) - log P(yr) instead of just maximizing log P(yc)?

- Concept: Reward modeling
  - Why needed here: The ADAPTIVE DECODER is trained using reward model scores to create preference pairs
  - Quick check question: What's the key difference between using a reward model vs ground truth labels for creating preference pairs?

## Architecture Onboarding

- Component map: Input prompt -> Llama 3.0-8B-Instruct (frozen) -> ADAPTIVE DECODER (3-layer MLP with SiLU activations) -> Temperature sampling mechanism -> Scaled softmax -> Next token

- Critical path:
  1. Input prompt → Llama → last hidden state
  2. Hidden state → ADAPTIVE DECODER → temperature distribution
  3. Temperature selection → scaled softmax → next token
  4. Repeat until end of sequence

- Design tradeoffs:
  - Token-level vs sequence-level adaptation: token-level offers finer control but requires more training data and computation
  - Greedy vs sampled temperature selection: greedy is simpler and often works well, sampled may explore more but adds variance
  - Number of temperature options: more options allow finer control but increase model complexity

- Failure signatures:
  - ADAPTIVE DECODER predicts near-uniform temperature distribution → suggests training isn't learning meaningful patterns
  - ADAPTIVE DECODER consistently picks extreme temperatures → may indicate reward model bias or insufficient temperature options
  - Winrates barely above 50% → suggests ADAPTIVE DECODER isn't outperforming random guessing

- First 3 experiments:
  1. Train ADAPTIVE DECODER on Wikitext-2 with 3-gram repeat reward to verify it learns to avoid greedy decoding
  2. Train on GSM8K only and test if it predicts low temperatures consistently
  3. Train on Stories only and test if it predicts high temperatures consistently

## Open Questions the Paper Calls Out

Open Question 1
- Question: How does the Adaptive Decoder perform when applied to hyperparameters other than temperature, such as top-k or top-p?
- Basis in paper: The authors mention that the Adaptive Decoder could potentially be used to convert hyperparameters other than temperature (e.g., top-p, top-k) effectively into model parameters.
- Why unresolved: The paper focuses on temperature as the hyperparameter to be optimized, and does not provide experimental results or analysis for other hyperparameters like top-k or top-p.
- What evidence would resolve it: Conducting experiments that apply the Adaptive Decoder to optimize top-k or top-p values, and comparing the performance against fixed values of these hyperparameters across various tasks.

Open Question 2
- Question: How does the performance of the Adaptive Decoder vary with different sizes of the underlying language model?
- Basis in paper: The authors use a Llama 3.0-8B-Instruct model for their experiments, but do not explore how the Adaptive Decoder's performance scales with different model sizes.
- Why unresolved: The paper does not provide a systematic study of the Adaptive Decoder's performance across different model sizes, which could reveal insights into its scalability and effectiveness.
- What evidence would resolve it: Running experiments with the Adaptive Decoder on various model sizes (e.g., smaller models like 7B, larger models like 70B) and comparing the performance across tasks to determine if there are any size-dependent trends or limitations.

Open Question 3
- Question: What is the impact of the Adaptive Decoder on the computational efficiency of language model inference?
- Basis in paper: The paper introduces the Adaptive Decoder as a method to dynamically select the optimal sampling temperature, but does not discuss the computational overhead or efficiency implications of this approach.
- Why unresolved: While the Adaptive Decoder may improve performance, it could potentially introduce additional computational costs during inference, which is not addressed in the paper.
- What evidence would resolve it: Measuring the inference time and computational resources required for the Adaptive Decoder compared to fixed temperature decoding, and analyzing the trade-offs between performance gains and computational efficiency.

## Limitations

- The effectiveness of Adaptive Decoding heavily depends on the quality of the reward model used in LPO training, and the proprietary ArmoRM model limits reproducibility
- The paper doesn't provide details on sample efficiency, making it unclear how many preference pairs are needed for the ADAPTIVE DECODER to converge
- Token-level adaptation shows promising results on constrained writing but hasn't been tested on other types of constraints or tasks

## Confidence

**High confidence** in the mechanism that LPO can train a neural module to predict task-appropriate temperatures when provided with quality signals from a reward model. The mathematical formulation is sound and the experimental results consistently show winrates above 50% across multiple tasks.

**Medium confidence** in the specific implementation details and hyperparameters used. While the paper provides sufficient information to reproduce the core method, critical details like exact learning rates, batch sizes, and the number of training samples per prompt are not specified, which could affect replication success.

**Low confidence** in the generalizability of the learned temperature selection patterns across different reward models or human evaluation setups. The paper only tests against the ArmoRM model, and it's unclear whether the ADAPTIVE DECODER would maintain its performance advantages with different quality metrics.

## Next Checks

1. **Reward Model Ablation**: Train the ADAPTIVE DECODER using different reward models (including simpler heuristics like n-gram repetition penalties or human-written quality scores) to verify that the learned temperature selection patterns are not specific to ArmoRM's scoring methodology.

2. **Zero-Shot Transfer**: Test the ADAPTIVE DECODER trained on the combination of GSM8K, Stories, and UltraFeedback on completely unseen tasks (e.g., code generation or summarization) to assess whether it can generalize its temperature selection to new domains without task-specific fine-tuning.

3. **Sample Efficiency Analysis**: Systematically vary the number of preference pairs used in LPO training (e.g., 1K, 10K, 100K pairs) and measure the impact on winrates and convergence speed to determine the practical data requirements for applying this method to new tasks.