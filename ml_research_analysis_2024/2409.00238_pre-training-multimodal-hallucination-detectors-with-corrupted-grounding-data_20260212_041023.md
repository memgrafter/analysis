---
ver: rpa2
title: Pre-Training Multimodal Hallucination Detectors with Corrupted Grounding Data
arxiv_id: '2409.00238'
source_url: https://arxiv.org/abs/2409.00238
tags:
- data
- spans
- grounding
- pre-training
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve sample efficiency for training
  multimodal hallucination detectors. The authors formulate hallucination detection
  as a sequence labeling task, where models must localize hallucinated text spans
  in multimodal outputs.
---

# Pre-Training Multimodal Hallucination Detectors with Corrupted Grounding Data

## Quick Facts
- arXiv ID: 2409.00238
- Source URL: https://arxiv.org/abs/2409.00238
- Authors: Spencer Whitehead; Jacob Phillips; Sean Hendryx
- Reference count: 40
- Pre-training on corrupted grounding data improves sample efficiency for hallucination detection, up to +7 F1 score improvement with 500 fine-tuning samples

## Executive Summary
This paper addresses the challenge of hallucination detection in multimodal models by formulating it as a sequence labeling task. The key innovation is using corrupted grounding data to pre-train hallucination detectors, where grounded spans are replaced with hallucinated phrases from a text-only language model. This approach significantly improves sample efficiency when fine-tuning on limited human-annotated data, demonstrating that grounding annotations provide valuable learning signals for the task.

## Method Summary
The authors create corrupted grounding data by replacing grounded phrases in image-prompt-response triples with hallucinated phrases generated by a text-only language model (T5). This synthetic data contains hallucinations that share structural properties with real hallucinations while being explicitly labeled as ungrounded. Hallucination detectors are pre-trained on this corrupted data, then fine-tuned on limited human-annotated samples from the M-HalDetect dataset. The approach leverages the relationship between grounding and hallucination - correctly grounded phrases cannot be hallucinated - to create effective synthetic training data.

## Key Results
- Pre-training on corrupted grounding data improves sample efficiency, with up to +7 F1 score improvement using 500 fine-tuning samples
- Grounding annotations are crucial - pre-training with grounded span replacement outperforms random span corruption
- The performance gap between 500 and 10k samples decreases from 13.54% to 6.22% when using pre-training
- Models maintain similar performance levels across different data scales when pre-trained

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on corrupted grounding data improves sample efficiency because it exposes hallucination detectors to synthetic hallucinations that share structural properties with real hallucinations.
- Mechanism: The corrupted grounding data contains spans where grounded phrases are replaced with plausible but contextually incorrect phrases from a text-only LM. This creates a distribution of hallucination-like errors that helps the model learn to distinguish grounded from ungrounded text spans.
- Core assumption: The hallucinations generated by replacing grounded spans with text-only LM outputs have sufficient similarity to real hallucinations that learning to detect them transfers to real hallucination detection.
- Evidence anchors:
  - [abstract]: "Leveraging phrase grounding data, we generate hallucinations to replace grounded spans and create hallucinated text. Experiments show that pre-training on this data improves sample efficiency when fine-tuning"
  - [section 4]: "By replacing grounded phrases with other phrases that are not aligned with the image, we can create text that contains hallucinations"
  - [corpus]: Weak - The corpus contains related work on hallucination detection but doesn't directly validate the transfer learning mechanism from synthetic to real hallucinations
- Break condition: If the synthetic hallucinations generated from text-only LMs are too dissimilar from real hallucinations, the pre-training would not transfer effectively to real hallucination detection.

### Mechanism 2
- Claim: Grounding annotations provide a valuable learning signal for hallucination detection because they explicitly mark which text spans are visually grounded versus hallucinated.
- Mechanism: Grounding annotations create a supervised signal that helps models learn the relationship between text spans and their visual grounding status. This signal is preserved in corrupted grounding data where replaced spans are explicitly labeled as hallucinated.
- Core assumption: Grounding data contains useful signal about which text spans should be considered grounded versus hallucinated, and this signal generalizes to hallucination detection.
- Evidence anchors:
  - [abstract]: "We find that using grounding annotations for our data is important, suggesting that grounding offers a valuable learning signal for pre-training these detectors"
  - [section 4]: "hallucinations and grounded phrases are linked since correctly grounded phrases are, by definition, not hallucinated"
  - [section 5.2]: "In Fig. 3, we evaluate masking out random spans instead of grounded ones to examine the need for grounding data. We see noticeably lower performance across each data scale"
- Break condition: If grounding annotations don't correlate well with hallucination status in real data, the learning signal would be weak or misleading.

### Mechanism 3
- Claim: Pre-training creates a better initialization for fine-tuning because it provides exposure to the hallucination detection task before seeing limited human annotations.
- Mechanism: Pre-training on corrupted grounding data allows models to develop task-specific representations and attention patterns for hallucination detection before fine-tuning on expensive human-annotated data.
- Core assumption: Pre-training on related but synthetic data creates useful task representations that can be refined with limited real data, rather than having to learn from scratch.
- Evidence anchors:
  - [abstract]: "pre-training on this data improves sample efficiency when fine-tuning (e.g., up to +7 F1 with 500 fine-tuning samples)"
  - [section 5.1]: "With this same model, the difference in performance between 500 and 10k samples decreases from 13.54% to 6.22% when pre-training"
  - [corpus]: Weak - The corpus contains related work on pre-training and hallucination detection but doesn't specifically validate this initialization mechanism
- Break condition: If the synthetic data distribution is too different from real data, pre-training could create misleading representations that hurt fine-tuning performance.

## Foundational Learning

- Concept: Sequence labeling for span detection
  - Why needed here: The task requires localizing hallucinated spans in text, which is fundamentally a sequence labeling problem where each token needs a label indicating whether it's part of a hallucination
  - Quick check question: If a model predicts "B-hallucination" for token 5, "I-hallucination" for token 6, and "O" for token 7, what spans does this represent?

- Concept: Grounding-illusion relationship
  - Why needed here: Understanding that grounded phrases cannot be hallucinated (by definition) is crucial for creating effective synthetic training data and for the model to learn this constraint
  - Quick check question: If a phrase is correctly grounded to an image region, can it simultaneously be a hallucination? Why or why not?

- Concept: Transfer learning and pre-training benefits
  - Why needed here: The paper's core contribution relies on understanding how pre-training on synthetic data can improve sample efficiency for fine-tuning on limited real data
  - Quick check question: What is the key difference between pre-training on synthetic hallucinations versus augmenting real data with synthetic examples?

## Architecture Onboarding

- Component map:
  - Base MLM (LLaVA-1.5/1.6) - provides multimodal representations
  - Visual encoder (frozen during pre-training) - processes image features
  - Language model backbone - processes text and generates token representations
  - Output head - maps token representations to hallucination labels (binary classification)
  - Loss function - cross-entropy over token-level labels

- Critical path:
  1. Image + prompt + response input → visual encoder + text encoder
  2. Token representations from MLM backbone → output head
  3. Output head predicts token-level labels (hallucination or not)
  4. Cross-entropy loss computed against ground truth token labels
  5. Gradients backpropagated to update output head and MLM parameters

- Design tradeoffs:
  - Freezing visual encoder during pre-training preserves pre-trained multimodal features but may limit adaptation
  - Using token-level labels enables fine-grained hallucination localization but increases label complexity
  - Replacing grounded spans vs random spans affects data quality and learning signal strength
  - Text-only LM for hallucination generation is efficient but may produce less realistic hallucinations than multimodal approaches

- Failure signatures:
  - Poor performance on both hallucination and non-hallucination classes suggests issues with the output head initialization or learning rate
  - High F1 on non-hallucinations but low F1 on hallucinations suggests class imbalance handling issues
  - Performance degrades when fine-tuning on more data suggests pre-training distribution mismatch
  - Random span replacement performs similarly to grounded span replacement suggests grounding signal isn't being utilized

- First 3 experiments:
  1. Run pre-training on corrupted grounding data with frozen visual encoder, then fine-tune on 500 M-HalDetect samples - should show improvement over FT-only baseline
  2. Compare pre-training on corrupted grounding data vs pre-training on random span corruption - should show grounding data is more effective
  3. Test different fractions of hallucination replacement (e.g., 25%, 50%, 75%) during pre-training data creation - should find optimal balance between synthetic hallucinations and original content

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different grounding data sources affect the quality and generalizability of the corrupted grounding data?
- Basis in paper: [explicit] The paper uses Grounded Visual Chat (GVC) dataset and mentions that their approach is compatible with other phrase grounding datasets.
- Why unresolved: The paper only experiments with one grounding data source, leaving open the question of how other grounding datasets might perform.
- What evidence would resolve it: Experiments comparing corrupted grounding data generated from multiple grounding datasets (e.g., Flickr30k Entities, Visual Genome) showing their relative effectiveness on hallucination detection.

### Open Question 2
- Question: What is the upper bound of performance for multimodal hallucination detection on M-HalDetect?
- Basis in paper: [inferred] The paper notes that human performance on M-HalDetect is unknown and suggests that there is much room for improvement.
- Why unresolved: The paper does not establish a human performance baseline or an estimated upper bound for the task.
- What evidence would resolve it: A comprehensive human evaluation study on M-HalDetect measuring human accuracy at both span localization and classification.

### Open Question 3
- Question: How does the proposed method scale with increasingly larger pre-training datasets?
- Basis in paper: [explicit] The paper notes that pre-training is most effective at lower data scales (500, 1k samples) and the gap is less pronounced at 10k samples.
- Why unresolved: The paper does not explore scaling up the pre-training data beyond the 121k samples used in experiments.
- What evidence would resolve it: Experiments scaling pre-training data to 10x, 100x, and 1000x the current size, measuring changes in sample efficiency at different fine-tuning scales.

### Open Question 4
- Question: How can the quality of corrupted grounding data be improved to reduce noise?
- Basis in paper: [explicit] The paper identifies noise in corrupted grounding data including semantic matches, generic phrases, and other errors, and suggests this data is better suited for pre-training.
- Why unresolved: The paper uses the noisy data as-is without exploring methods to filter or improve its quality.
- What evidence would resolve it: A study comparing hallucination detection performance using filtered/cleaned corrupted grounding data versus the original noisy version.

### Open Question 5
- Question: How do different language models for generating hallucinations affect the quality of corrupted grounding data?
- Basis in paper: [explicit] The paper uses T5 for hallucination generation but notes that more powerful MLMs could be used.
- Why unresolved: The paper only experiments with T5 and does not compare different hallucination generation approaches.
- What evidence would resolve it: Experiments comparing corrupted grounding data generated using different LMs (T5, GPT-4, Claude) and measuring their impact on hallucination detection performance.

## Limitations

- Synthetic Data Quality: The paper assumes that hallucinations generated by replacing grounded spans with text-only LM outputs sufficiently resemble real hallucinations, but this similarity is not quantitatively validated.
- Grounding Data Dependency: The approach relies on phrase grounding datasets like GVC, which may not capture all types of hallucinations or may have different characteristics than the target hallucination detection task.
- Hyperparameter Sensitivity: The paper doesn't provide comprehensive sensitivity analysis for hyperparameters like hallucination replacement ratios or pre-training duration.

## Confidence

- High Confidence: The core finding that pre-training on corrupted grounding data improves sample efficiency for hallucination detection is well-supported by the experimental results showing consistent F1 score improvements across multiple data scales (500-10k samples).
- Medium Confidence: The claim that grounding annotations provide valuable learning signal is supported by ablation studies showing grounding data outperforms random span corruption, but the mechanism for why grounding specifically helps could be further validated.
- Medium Confidence: The mechanism that synthetic hallucinations share structural properties with real hallucinations enabling effective transfer learning is plausible but relies on assumptions about the similarity between text-only generated hallucinations and actual multimodal hallucinations.

## Next Checks

1. **Synthetic vs Real Hallucination Distribution Analysis**: Compare the distribution of hallucinations generated by the text-only LM replacement approach with actual hallucinations found in human-annotated data to quantify their similarity and identify any systematic differences.

2. **Cross-Dataset Generalization Test**: Evaluate the pre-trained hallucination detectors on hallucination detection datasets outside of M-HalDetect (e.g., datasets from the related work) to assess whether the grounding-based pre-training generalizes beyond the specific evaluation set.

3. **Robustness to Hyperparameter Variation**: Conduct systematic ablation studies varying pre-training epochs, hallucination replacement ratios, and fine-tuning hyperparameters to establish the stability of the sample efficiency improvements across different configurations.