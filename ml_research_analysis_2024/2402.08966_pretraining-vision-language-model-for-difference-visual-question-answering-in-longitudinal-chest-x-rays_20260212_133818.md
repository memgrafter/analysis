---
ver: rpa2
title: Pretraining Vision-Language Model for Difference Visual Question Answering
  in Longitudinal Chest X-rays
arxiv_id: '2402.08966'
source_url: https://arxiv.org/abs/2402.08966
tags:
- plural
- x-ray
- images
- longitudinal
- chest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces PLURAL, a vision-language model designed
  to address difference visual question answering (diff-VQA) for longitudinal chest
  X-ray images. The model is pretrained in three stages: first on natural images and
  texts, then on longitudinal chest X-ray data including reports and diff-VQA pairs,
  and finally finetuned on diff-VQA data.'
---

# Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays

## Quick Facts
- arXiv ID: 2402.08966
- Source URL: https://arxiv.org/abs/2402.08966
- Authors: Yeongjae Cho; Taehee Kim; Heejun Shin; Sungzoon Cho; Dongmyung Shin
- Reference count: 10
- One-line primary result: PLURAL significantly outperforms previous state-of-the-art methods on diff-VQA tasks for longitudinal chest X-rays

## Executive Summary
This paper introduces PLURAL, a vision-language model designed to address difference visual question answering (diff-VQA) for longitudinal chest X-ray images. The model is pretrained in three stages: first on natural images and texts, then on longitudinal chest X-ray data including reports and diff-VQA pairs, and finally finetuned on diff-VQA data. PLURAL modifies a Transformer architecture to handle both past and current images simultaneously. The model significantly outperforms previous state-of-the-art methods on diff-VQA tasks, achieving higher scores across multiple evaluation metrics (BLEU, METEOR, ROUGE-L, CIDEr). It also performs well on conventional VQA tasks for single X-ray images. Ablation studies confirm the importance of each pretraining stage and the use of temporal information from both images and reports.

## Method Summary
PLURAL uses a three-stage training pipeline: (1) pretraining on natural images and texts using a Transformer-based network, (2) continuing pretraining on longitudinal chest X-ray data with radiologist reports and diff-VQA pairs, and (3) finetuning on diff-VQA data. The architecture includes separate input branches for past and current images, shared ResNet101 encoders, time-specific positional encodings, and a Transformer encoder-decoder for generating answer text. The model is trained on the MIMIC-Diff-VQA dataset for diff-VQA tasks and evaluated using BLEU, METEOR, ROUGE-L, and CIDEr metrics.

## Key Results
- PLURAL achieved the highest scores on MIMIC-Diff-VQA across all evaluation metrics (BLEU, METEOR, ROUGE-L, CIDEr)
- PLURAL outperformed state-of-the-art methods by significant margins on diff-VQA tasks
- PLURAL maintained strong performance on conventional VQA tasks for single chest X-ray images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stepwise pretraining on natural images then longitudinal chest X-ray data transfers general VQA capabilities before specializing to temporal differences.
- Mechanism: The first pretraining stage builds broad vision-language understanding on diverse image-text pairs. The second stage fine-tunes this knowledge using domain-specific longitudinal data, enabling the model to recognize subtle changes across time.
- Core assumption: General visual-linguistic patterns from natural images transfer to medical domains and enhance performance on specialized tasks.
- Evidence anchors:
  - [abstract] "pretrained on natural and longitudinal chest X-ray data"
  - [section 2.1] "We adopted a Transformer-based network (Wang et al., 2022) in the first stage"
  - [corpus] Weak: No direct evidence in neighbors; paper itself is the main source.
- Break condition: If natural image pretraining does not generalize to medical imagery, performance would plateau or degrade when fine-tuned on X-rays.

### Mechanism 2
- Claim: Simultaneous encoding of past and current images with explicit time embeddings allows the model to differentiate temporal states.
- Mechanism: The architecture adds a separate input branch for the past image, processes both images through shared encoders, and adds time-specific positional encodings (tpast_enc and tcur_enc) to distinguish their temporal order.
- Core assumption: The model can learn meaningful representations of temporal change when provided explicit time signals alongside paired images.
- Evidence anchors:
  - [section 2.1] "we added a new input branch for a past image" and "to differentiate the time points of the two input images, we separately added the time encoding"
  - [section 3.1] "PLURAL achieved the highest scores... compared to previous state-of-the-arts methods"
  - [corpus] Weak: No direct evidence in neighbors; architecture details are from the paper itself.
- Break condition: If time encodings are ineffective or insufficient, the model may conflate past and current states, reducing diff-VQA accuracy.

### Mechanism 3
- Claim: Incorporating radiologist reports (Findings and Impression sections) during pretraining provides structured temporal context that improves diff-VQA performance.
- Mechanism: The model is trained on paired X-ray images and report sections that describe changes over time, aligning textual descriptions of progression with visual differences.
- Core assumption: Reports contain explicit or implicit descriptions of temporal change that the model can learn to map to image differences.
- Evidence anchors:
  - [section 2.3] "We utilized two sets of longitudinal data... radiologist’s reports... Two main sections, Findings and Impression... describe the changes in lung abnormalities and diseases over time"
  - [section 3.1] "PLURAL was better at capturing the longitudinal change of multiple lung findings"
  - [corpus] Weak: No direct evidence in neighbors; paper is primary source.
- Break condition: If report text does not align well with image changes or contains noise, pretraining may not improve diff-VQA performance.

## Foundational Learning

- Concept: Vision-Language Pre-training (VLP)
  - Why needed here: Provides the model with general ability to understand and generate text conditioned on images, forming the base for specialized diff-VQA tasks.
  - Quick check question: Can the model perform image captioning or basic VQA on single images before fine-tuning on longitudinal data?

- Concept: Temporal Reasoning in Vision
  - Why needed here: Required to compare paired images taken at different times and detect changes or disease progression.
  - Quick check question: Does the model output different answers when the order of past/current images is swapped?

- Concept: Cross-Modal Attention in Transformers
  - Why needed here: Enables the model to attend to relevant parts of both images and text simultaneously, crucial for answering questions about image differences.
  - Quick check question: Can the model correctly attend to abnormality regions in both images when answering "What has changed?"

## Architecture Onboarding

- Component map:
  - Input → Image/Text Encoding → Time/Position Addition → Concatenation → Encoder → Decoder → Output

- Critical path: Input → Image/Text Encoding → Time/Position Addition → Concatenation → Encoder → Decoder → Output

- Design tradeoffs:
  - Shared image encoder vs separate encoders: Shared reduces parameters and forces consistent feature learning but may limit specialization.
  - Fixed time encodings vs learned: Learned encodings allow model to discover meaningful temporal distinctions but add parameters.
  - Separate input branches vs single concatenated input: Separate branches make temporal order explicit but increase architectural complexity.

- Failure signatures:
  - Poor BLEU/METEOR/CIDEr scores on diff-VQA: Likely issues with temporal change detection or cross-modal alignment.
  - Similar outputs regardless of image order: Time encodings not effective or missing.
  - Low scores on conventional VQA: General VLP not sufficient or fine-tuning not effective.

- First 3 experiments:
  1. Train baseline model (Stage 1 only) on MIMIC-Diff-VQA and compare performance to full PLURAL.
  2. Train model with shared encoder but no time encodings on paired images and evaluate diff-VQA accuracy.
  3. Train model using only Findings or only Impression sections during Stage 2 and compare to using both.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PLURAL's performance scale with different pretraining data compositions and sizes?
- Basis in paper: [explicit] The paper mentions investigating the impact of pretraining settings, such as pretraining steps and data composition, but does not provide detailed ablation studies on different data compositions or sizes.
- Why unresolved: The paper focuses on comparing the full PLURAL model against state-of-the-art methods and performing ablation studies on removing entire pretraining stages, but does not explore how varying the amount or type of pretraining data affects performance.
- What evidence would resolve it: Conducting experiments with PLURAL using different combinations and quantities of natural image-text data, longitudinal chest X-ray data, and diff-VQA data, then measuring performance on diff-VQA tasks to determine optimal pretraining data composition and size.

### Open Question 2
- Question: Can PLURAL generalize to other types of longitudinal medical imaging beyond chest X-rays?
- Basis in paper: [inferred] While PLURAL is specifically trained on chest X-ray data, the paper suggests that the approach of using pretraining on natural images and longitudinal data could be beneficial for other medical imaging tasks. However, no experiments are conducted on other modalities.
- Why unresolved: The paper only evaluates PLURAL on chest X-ray data, leaving its effectiveness on other longitudinal medical imaging tasks (e.g., CT, MRI) untested.
- What evidence would resolve it: Training and evaluating PLURAL on longitudinal data from other medical imaging modalities and comparing its performance to existing methods on diff-VQA or similar tasks for those modalities.

### Open Question 3
- Question: How does PLURAL handle very long-term longitudinal changes compared to short-term changes?
- Basis in paper: [explicit] The paper mentions that PLURAL can capture changes over time, as seen in examples where it describes changes in lung findings and severity levels. However, it does not investigate the model's performance on varying time spans between images.
- Why unresolved: The paper does not provide an analysis of PLURAL's performance when comparing images taken very far apart in time versus those taken close together, which could affect the model's ability to capture relevant changes.
- What evidence would resolve it: Evaluating PLURAL's performance on diff-VQA tasks using pairs of images with varying time intervals (e.g., days, months, years apart) and analyzing how performance changes with time span.

## Limitations
- Limited generalizability beyond the MIMIC dataset as no external validation cohort was tested
- Computationally expensive stepwise pretraining requiring careful hyperparameter tuning at each stage
- Reliance on radiologist reports assumes consistent quality and temporal information content

## Confidence
- Confidence in core diff-VQA performance claims: High, supported by comprehensive quantitative results across multiple metrics
- Confidence in pretraining mechanism explanations: Medium, as ablation studies demonstrate importance but do not fully isolate each component's contribution
- Confidence in architectural design choices: Medium-Low, as alternative designs were not systematically explored

## Next Checks
1. Test PLURAL on external chest X-ray datasets to assess generalisability
2. Conduct ablation studies removing individual pretraining stages to quantify their independent contributions
3. Compare PLURAL against larger vision-language models like GPT-4V or Gemini to establish relative performance benchmarks