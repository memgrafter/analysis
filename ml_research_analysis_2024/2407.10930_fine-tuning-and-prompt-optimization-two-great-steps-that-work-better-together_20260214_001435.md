---
ver: rpa2
title: 'Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together'
arxiv_id: '2407.10930'
source_url: https://arxiv.org/abs/2407.10930
tags:
- prompt
- module
- answer
- optimization
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BetterTogether, a method for optimizing language
  model (LM) pipelines by alternating between fine-tuning LM weights and optimizing
  prompt templates. The approach addresses the challenge of end-to-end optimization
  in modular NLP systems where modules lack intermediate labels or gradient flow.
---

# Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together

## Quick Facts
- arXiv ID: 2407.10930
- Source URL: https://arxiv.org/abs/2407.10930
- Reference count: 40
- Primary result: Alternating weight and prompt optimization improves LM pipeline performance by up to 78% over prompt-only and 88% over weight-only strategies

## Executive Summary
This paper introduces BetterTogether, a method for optimizing language model pipelines by alternating between fine-tuning LM weights and optimizing prompt templates. The approach addresses the challenge of end-to-end optimization in modular NLP systems where modules lack intermediate labels or gradient flow. BetterTogether uses bootstrapping to generate training data and alternates between two DSPy algorithms: BootstrapFewShotRS for prompt optimization and BootstrapFinetune for weight optimization. Experiments on three tasks (multi-hop QA, mathematical reasoning, and classification) with three different LMs show that BetterTogether strategies outperform direct weight or prompt optimization alone by up to 60% and 6% respectively.

## Method Summary
BetterTogether alternates between prompt optimization using BootstrapFewShotRS and weight optimization using BootstrapFinetune with LoRA. The method bootstraps training data by executing the program on training inputs and recording inputs/outputs at each module when the final output is "correct." This creates a self-reinforcing cycle where optimized prompts generate better intermediate outputs, which become better training data for weight fine-tuning, which produces more capable models that generate even better intermediate outputs for subsequent prompt optimization.

## Key Results
- BetterTogether strategies achieve 5–78% gains for HotPotQA, 2.5–10% gains for GSM8K, and 3.5–88% gains for Iris over prompts-only and weights-only strategies
- Alternating optimization consistently outperforms single-optimization approaches across all three tasks and models
- The Π→Θ→Π strategy (prompt → weight → prompt) provides the best overall performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alternating weight and prompt optimization enables better data bootstrapping for both processes
- Mechanism: The method uses self-generated program traces where the LM teaches itself through intermediate outputs. First, prompt optimization generates higher quality intermediate outputs, which serve as better training data for weight fine-tuning. Then, weight fine-tuning produces more capable models that generate even better intermediate outputs for subsequent prompt optimization.
- Core assumption: The quality of bootstrapped training data depends on the current state of both prompts and weights, and this dependency is non-linear
- Evidence anchors:
  - [abstract]: "These tandem strategies are highly effective across three different LMs, leading to 5–78% gains for HotPotQA, 2.5–10% gains for GSM8K, and 3.5–88% gains for Iris against prompts only and weights only strategies"
  - [section]: "we use all the available training examples as potential candidates to generate the data for fine-tuning Φ's LM weights and pass them to the BFT weight optimizer, which: (1) runs a given Φ on all the training examples, (2) keeps the traces where the final output of Φ was correct and filters out the rest"

### Mechanism 2
- Claim: Self-teaching through bootstrapping allows the same LM to progressively improve without external supervision
- Mechanism: The algorithm executes the program on training inputs and records inputs/outputs at each module when the final output is "correct." These successful traces become training data for both prompt and weight optimization, creating a self-reinforcing cycle where the model learns from its own successes.
- Core assumption: Even without labeled intermediate outputs, the final output correctness can serve as a proxy for intermediate module quality
- Evidence anchors:
  - [section]: "we bootstrap training labels for all pipeline modules... execute an initial version of the program on input examples... recording the inputs/outputs observed at each module when the final output is 'correct'"
  - [section]: "in line with our problem formulation, our prompt and weight optimization regimes are not simply training on hand-labeled data but on self-generated program traces"

### Mechanism 3
- Claim: Different optimization strategies are complementary rather than redundant
- Mechanism: Prompt optimization adjusts discrete string templates to better guide the LM's behavior, while weight fine-tuning modifies the continuous parameters to adapt the LM's underlying capabilities. These operate on different representational spaces and address different aspects of performance.
- Core assumption: The discrete nature of prompts and continuous nature of weights means they capture orthogonal aspects of model behavior that can be optimized separately
- Evidence anchors:
  - [abstract]: "these BetterTogether strategies optimizing the weights and prompts of a pipeline together outperform directly optimizing weights alone and prompts alone by up to 60% and 6%, respectively"
  - [section]: "we seek to implement each language module as some specific, well-tuned strategy for invoking an underlying language model LM... specifying (1) the string prompt πi... and (2) the floating-point weights θi"

## Foundational Learning

- Concept: Bootstrapping in machine learning
  - Why needed here: The entire approach relies on generating training data from the model's own outputs rather than external labels
  - Quick check question: What is the key difference between bootstrapping and traditional supervised learning in this context?

- Concept: Language model prompting and template optimization
  - Why needed here: The method uses BootstrapFewShotRS to optimize prompt templates through random search over generated few-shot examples
  - Quick check question: How does BootstrapFewShotRS differ from manual prompt engineering?

- Concept: Fine-tuning with LoRA (Low-Rank Adaptation)
  - Why needed here: The weight optimization uses LoRA to efficiently fine-tune the LM weights while keeping most parameters frozen
  - Quick check question: Why might LoRA be preferred over full fine-tuning for this application?

## Architecture Onboarding

- Component map:
  - DSPy program definition (Φ) with modules (M)
  - BootstrapFewShotRS optimizer for prompt optimization
  - BootstrapFinetune optimizer for weight optimization
  - LoRA fine-tuning mechanism
  - ColBERTv2 retriever for HotPotQA
  - Docker-based inference infrastructure

- Critical path:
  1. Initialize program with vanilla prompts and base LM weights
  2. Run BootstrapFewShotRS to optimize prompts using random search over self-generated traces
  3. Use optimized prompts to generate training traces for weight fine-tuning
  4. Apply BootstrapFinetune with LoRA to update LM weights
  5. Optionally repeat prompt optimization with updated weights
  6. Evaluate on held-out test set

- Design tradeoffs:
  - Computational cost vs. performance gains (alternating strategies are more expensive than single optimizations)
  - Number of optimization iterations (more iterations may improve performance but increase cost)
  - Dataset size for bootstrapping (too small may not provide sufficient training data)

- Failure signatures:
  - Insufficient correct outputs during bootstrapping (indicated by "–" in results table)
  - Performance degradation after optimization steps (suggests overfitting or poor bootstrapping data)
  - No improvement from alternating strategies (suggests the task doesn't benefit from both optimizations)

- First 3 experiments:
  1. Run the baseline "Vanilla Zero-shot" strategy to establish performance without any optimization
  2. Test "Prompt Optimization (Π)" alone to measure the impact of prompt optimization in isolation
  3. Test "Weight Optimization (Θ)" alone to measure the impact of fine-tuning in isolation
  4. (Bonus) Test "Π → Θ → Π" to verify the full BetterTogether strategy on a simple task like Iris

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different fine-tuning strategies (beyond LoRA) compare to prompt optimization in LM program optimization?
- Basis in paper: [explicit] The paper states "We have only experimented with weight optimization in the form of LoRA fine-tuning of pre-trained models. It is in principle possible that some other fine-tuning strategy would be so powerful and cost-effective as to remove the need for prompt optimization."
- Why unresolved: The paper only tested LoRA fine-tuning, leaving other fine-tuning methods unexplored.
- What evidence would resolve it: Comparative experiments testing various fine-tuning approaches (e.g., full fine-tuning, adapters, prefix tuning) against prompt optimization across different tasks and LM programs.

### Open Question 2
- Question: Why do both prompt optimization and fine-tuning contribute to improved performance in LM programs?
- Basis in paper: [explicit] The paper states "we do not yet understand why both are important. The role of prompt optimization and the role of fine-tuning in multi-stage LM programs are both new, and the relative lack of understanding of these roles in the emerging literature could pose risks in unanticipated interactions between these components."
- Why unresolved: The paper demonstrates effectiveness but lacks theoretical understanding of why both methods are necessary.
- What evidence would resolve it: Detailed analysis of how prompt optimization and fine-tuning affect different aspects of LM program behavior, potentially through ablation studies or interpretability methods.

### Open Question 3
- Question: How do BetterTogether strategies scale to larger, more complex LM programs with many modules?
- Basis in paper: [inferred] The paper focuses on relatively simple programs (3 modules max) and notes "In particular, we have only experimented with weight optimization in the form of LoRA fine-tuning of pre-trained models."
- Why unresolved: The experiments were limited to small programs, and the paper doesn't address scalability concerns.
- What evidence would resolve it: Experiments with increasingly complex LM programs, measuring performance, computational costs, and potential diminishing returns as program complexity grows.

## Limitations
- Evaluation focuses on only three tasks and three models, limiting generalizability
- No statistical significance tests reported for the performance improvements
- Bootstrapping assumes final output correctness indicates intermediate module quality, but this assumption isn't validated
- Computational overhead of alternating optimization isn't quantified

## Confidence
- High confidence: The basic alternating optimization framework works and improves performance over single-optimization baselines
- Medium confidence: The 60% and 6% performance gains are accurately reported, as these come from a single run without statistical validation
- Medium confidence: The bootstrapping mechanism effectively generates useful training data, though the assumption about correctness as a proxy for intermediate quality needs verification
- Low confidence: The generality of BetterTogether to other task types and model families, given the narrow experimental scope

## Next Checks
1. Run each strategy across 5 random seeds and compute confidence intervals for the performance improvements, particularly for the 60% and 6% claims against baselines
2. Instrument the bootstrapping process to measure the actual quality of intermediate outputs, not just final answer correctness, to validate the assumption that correct final outputs indicate correct intermediate reasoning
3. Measure wall-clock time and compute cost for each alternating strategy compared to baselines, then calculate the performance gain per unit cost to assess practical viability for different deployment scenarios