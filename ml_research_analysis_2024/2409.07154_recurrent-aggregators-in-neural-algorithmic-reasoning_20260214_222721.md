---
ver: rpa2
title: Recurrent Aggregators in Neural Algorithmic Reasoning
arxiv_id: '2409.07154'
source_url: https://arxiv.org/abs/2409.07154
tags: []
core_contribution: This paper investigates replacing the permutation-equivariant aggregators
  in graph neural networks with recurrent neural networks for neural algorithmic reasoning.
  The authors propose RNAR, which uses LSTM networks as the aggregation function in
  a graph neural network architecture.
---

# Recurrent Aggregators in Neural Algorithmic Reasoning

## Quick Facts
- arXiv ID: 2409.07154
- Source URL: https://arxiv.org/abs/2409.07154
- Reference count: 40
- Primary result: RNAR achieves 87% mean micro-F1 on Quickselect, outperforming previous state-of-the-art

## Executive Summary
This paper investigates replacing permutation-equivariant aggregators in graph neural networks with recurrent neural networks for neural algorithmic reasoning. The authors propose RNAR, which uses LSTM networks as the aggregation function in a graph neural network architecture. This design choice is motivated by the observation that many classical algorithms operate on ordered data structures like lists, where permutation invariance is not required. The RNAR model is evaluated on the CLRS-30 benchmark, showing strong performance on sequential algorithms. Most notably, RNAR achieves a mean micro-F1 score of 87% on the Quickselect task, significantly outperforming previous state-of-the-art results.

## Method Summary
The RNAR model replaces the standard permutation-equivariant aggregator in graph neural networks with an LSTM that processes messages in a specific order. The model assumes node features are pre-arranged in a list using the pos node input feature from the CLRS benchmark, allowing the LSTM to capture sequential dependencies. The architecture uses a fully-connected graph where each node receives messages from all other nodes, with messages ordered according to the pos feature before being processed by the LSTM. This approach challenges the conventional wisdom that GNN aggregators must be permutation-equivariant.

## Key Results
- RNAR achieves 87% mean micro-F1 on Quickselect, setting new state-of-the-art
- Strong performance on sequential algorithms (Activity Selector, Binary Search, Bubble Sort)
- Poor performance on Kruskal, Bellman-Ford, and Dijkstra due to memory constraints with triplet operations

## Why This Works (Mechanism)

### Mechanism 1
Standard GNNs use permutation-invariant aggregators that treat all neighbors symmetrically. By replacing this with an LSTM, the model can learn to process neighbors in a specific order, which is crucial for sequential algorithms like sorting and searching where the natural ordering of elements matters. Many classical algorithm categories, such as sorting and searching, assume that an input is a list, inducing a natural order between the nodes.

### Mechanism 2
The RNAR model uses the pos node input feature from CLRS to arrange node features in a list before feeding them to the LSTM. This allows the LSTM to process messages in a specific order, capturing sequential dependencies that permutation-invariant aggregators cannot. The model assumes that the N = |V| node features are pre-arranged in a list [x1, x2, ..., xN], with such an ordering always provided by the CLRS benchmark through its pos node input feature.

### Mechanism 3
Quickselect is a selection algorithm that relies on partitioning elements in a specific order. The LSTM aggregator can learn this sequential processing pattern, while permutation-invariant aggregators struggle because they cannot distinguish between different orderings of the same set of elements. The most important result is clearly on the Quickselect task, wherein RNAR sets the best recorded micro-F1 score by a wide margin, settling an important open challenge.

## Foundational Learning

- **Graph Neural Networks and Message Passing**: Understanding how GNNs work is fundamental to grasping why replacing the aggregator matters. Quick check: What is the difference between permutation-invariant and permutation-equivariant operations in GNNs?

- **Permutation Invariance vs. Order Sensitivity**: The core innovation of RNAR is breaking permutation invariance, so understanding when this is beneficial is crucial. Quick check: Why would an algorithm that processes lists benefit from an order-sensitive aggregator?

- **LSTM Networks and Sequential Processing**: RNAR uses LSTM as the aggregator, so understanding how LSTMs process sequences is essential. Quick check: How does an LSTM maintain state across time steps, and why is this useful for processing ordered data?

## Architecture Onboarding

- **Component map**: Input Graph with node features and pos ordering -> Message function ψ -> LSTM aggregator -> Update function ϕ -> Output Algorithm execution results

- **Critical path**: 1. Nodes receive messages from all other nodes 2. Messages are ordered according to pos feature 3. LSTM processes messages sequentially 4. Final LSTM state is used for node updates 5. Process repeats for multiple layers

- **Design tradeoffs**:
  - Pros: Better handling of sequential algorithms, captures natural ordering
  - Cons: Higher memory usage, potentially worse performance on non-sequential tasks
  - Memory consideration: LSTM requires O(N) memory for N messages, which caused OOM issues in some tasks

- **Failure signatures**: Poor performance on algorithms without natural ordering, memory overflow when processing large graphs with triplets, inconsistent results due to sensitivity to input ordering

- **First 3 experiments**:
  1. Compare RNAR vs standard GNN on a simple sorting task to verify order sensitivity benefits
  2. Test memory usage on increasing graph sizes to identify scalability limits
  3. Evaluate performance on both sequential and non-sequential algorithms to understand scope of applicability

## Open Questions the Paper Calls Out

### Open Question 1
What specific properties of sequential algorithms make them more amenable to non-commutative aggregators compared to parallel algorithms? The authors observe that sequential algorithms (like sorting and searching) assume input in the form of a list, inducing a natural order between elements, while parallel algorithms do not have this property. This question remains unresolved as the paper demonstrates empirical superiority on sequential tasks but doesn't deeply analyze the theoretical reasons why permutation non-invariance benefits certain algorithmic categories more than others.

### Open Question 2
Can alternative recurrent architectures (beyond LSTM) provide better memory efficiency while maintaining or improving performance on sequential algorithmic tasks? The authors note memory considerations of LSTM aggregators caused out-of-memory errors with triplets on four tasks, suggesting alternatives like Binary-GRUs should be explored. The paper only tests one type of recurrent aggregator and acknowledges limitations without exploring the full space of sequential aggregation functions.

### Open Question 3
How does the performance of RNAR generalize to other algorithmic reasoning benchmarks beyond CLRS-30? The paper's evaluation is limited to CLRS-30, and while results are promising, the authors suggest RNAR could inspire future research into non-commutative aggregators without demonstrating broader applicability. CLRS-30 represents a specific class of classical algorithms, and the performance characteristics on sequential tasks may not transfer to other algorithmic domains or more complex reasoning problems.

## Limitations

- RNAR's performance is highly task-dependent, with poor results on memory-intensive tasks like Kruskal, Bellman-Ford, and Dijkstra
- The model's scalability is limited by the LSTM's memory requirements for processing large numbers of messages
- Limited evaluation on diverse algorithmic reasoning benchmarks raises questions about generalizability

## Confidence

- **High confidence**: RNAR's strong performance on Quickselect and sequential algorithms is well-supported by the experimental results
- **Medium confidence**: The claim that LSTM aggregators are broadly beneficial for algorithmic reasoning, as performance varies significantly across tasks
- **Low confidence**: The generalizability of RNAR to algorithms beyond those in the CLRS-30 benchmark, given the limited task diversity in evaluation

## Next Checks

1. **Memory scalability test**: Systematically evaluate RNAR's performance on increasingly large graphs for memory-intensive tasks (Kruskal, Bellman-Ford, Dijkstra) to quantify the scalability limits and explore potential architectural modifications to address memory constraints.

2. **Order sensitivity ablation**: Design controlled experiments to isolate the contribution of the LSTM aggregator's order sensitivity by comparing against permutation-invariant variants of RNAR on both sequential and non-sequential tasks.

3. **Cross-benchmark validation**: Evaluate RNAR on alternative algorithmic reasoning benchmarks beyond CLRS-30 to assess whether the observed performance patterns generalize to different problem domains and algorithm types.