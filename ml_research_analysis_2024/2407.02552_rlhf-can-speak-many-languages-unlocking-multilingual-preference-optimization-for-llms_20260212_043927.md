---
ver: rpa2
title: 'RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization
  for LLMs'
arxiv_id: '2407.02552'
source_url: https://arxiv.org/abs/2407.02552
tags:
- preference
- languages
- multilingual
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of extending preference optimization
  techniques, which have proven effective in English, to a multilingual context for
  large language models (LLMs). The authors introduce a novel, scalable method for
  generating high-quality multilingual preference data to address the scarcity and
  quality issues in existing datasets.
---

# RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs

## Quick Facts
- arXiv ID: 2407.02552
- Source URL: https://arxiv.org/abs/2407.02552
- Authors: John Dang; Arash Ahmadian; Kelly Marchisio; Julia Kreutzer; Ahmet Üstün; Sara Hooker
- Reference count: 40
- Key outcome: Novel method for generating high-quality multilingual preference data, demonstrating that multilingual preference optimization significantly improves LLM performance across 23 languages

## Executive Summary
This paper addresses the challenge of extending preference optimization techniques from English to a multilingual context for large language models. The authors introduce a scalable method for generating high-quality multilingual preference data to overcome data scarcity issues. Their comprehensive study examines how data sources, dataset size, and language coverage impact preference optimization performance. The research demonstrates that multilingual preference data is essential for aligning multilingual LLMs, with increased language coverage and data quantity leading to significant performance gains. Their preference-optimized Aya 23 8B model achieves state-of-the-art performance across 23 languages.

## Method Summary
The authors developed a novel, scalable method for generating high-quality multilingual preference data by translating English prompts into 22 other languages, generating completions using multiple LLMs (Cohere's Command and Command R+), and ranking these completions to create preference pairs. They fine-tuned the Aya 23 8B model using both DPO (offline) and RLOO (online) optimization methods with different preference data mixtures controlling the number of languages and data amount per language. The preference-optimized models were evaluated against state-of-the-art open-weight models using LLM-simulated win-rates on open-ended generation and summarization tasks across all 23 languages.

## Key Results
- Multilingual preference data is essential for aligning multilingual LLMs, with increased language coverage and data quantity leading to significant performance gains
- Online preference optimization methods (RLOO) outperform offline methods (DPO), especially in cross-lingual transfer to unseen languages
- The preference-optimized Aya 23 8B model achieves state-of-the-art performance across 23 languages, outperforming other widely used open-weight models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual preference data enables cross-lingual transfer, improving performance even for languages not included in the training data.
- Mechanism: By including multiple languages in the preference optimization process, the model learns generalizable preference patterns that apply across languages due to shared semantic and structural features.
- Core assumption: There are sufficient shared semantic and structural features across the languages in the dataset to enable transfer.
- Evidence anchors:
  - [abstract] "Our results demonstrate that multilingual preference data is essential for aligning multilingual LLMs, with increased language coverage and data quantity leading to significant performance gains."
  - [section 4] "We also find that preference training exhibits cross-lingual transfer, leading to significant gains in languages not present in the training data."
- Break condition: If the languages in the dataset have very few shared features or if the model cannot learn generalizable preference patterns, cross-lingual transfer may not occur.

### Mechanism 2
- Claim: Online preference optimization (RLOO) outperforms offline methods (DPO) in multilingual settings.
- Mechanism: Online methods like RLOO generate new samples during training, allowing the model to adapt to the specific nuances of each language in real-time, particularly beneficial in heterogeneous multilingual data.
- Core assumption: The online generation of samples during training provides sufficient diversity and adaptation to handle the heterogeneity of multilingual data.
- Evidence anchors:
  - [abstract] "Our study also reveals that online preference optimization methods, specifically RLOO, outperform offline methods like DPO, especially in cross-lingual transfer to unseen languages."
  - [section 2.2] "Ahmadian et al. (2024) shows that PPO may not be the right tool for RLHF, and that simpler REINFORCE-style methods such as Vanilla Policy Gradient and RLOO, are competitive or outperform PPO."
- Break condition: If the online generation process does not provide sufficient diversity or if the model overfits to the specific prompts used during training, online optimization may not outperform offline methods.

### Mechanism 3
- Claim: Increasing the number of languages and the total amount of preference data improves multilingual performance.
- Mechanism: Including more languages and more data per language exposes the model to a wider variety of prompts, completions, and preference patterns, leading to a more robust and generalizable understanding of human preferences across different languages.
- Core assumption: The additional data provides meaningful and diverse examples that improve the model's understanding of human preferences.
- Evidence anchors:
  - [abstract] "We establish the benefits of cross-lingual transfer and increased dataset size in preference training."
  - [section 4] "For a fixed dataset size of 50K pairwise preferences, we find that increasing the number of languages in training data improves overall performance."
- Break condition: If the additional data is not diverse or if the model cannot effectively learn from the increased volume of information, increasing the number of languages and data may not lead to performance improvements.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is the foundation for preference optimization techniques used in this paper.
  - Quick check question: What are the key components of RLHF and how do they contribute to aligning language models with human preferences?

- Concept: Cross-lingual transfer
  - Why needed here: Cross-lingual transfer is a key mechanism behind the effectiveness of multilingual preference optimization.
  - Quick check question: What are the factors that enable cross-lingual transfer and how can they be leveraged to improve the performance of multilingual models?

- Concept: Online vs. offline optimization
  - Why needed here: The choice between online and offline optimization methods has a significant impact on the performance of multilingual preference optimization.
  - Quick check question: What are the advantages and disadvantages of online and offline optimization methods and how do they affect the performance of multilingual models?

## Architecture Onboarding

- Component map: Aya 23 8B model -> Reward model -> Preference optimization algorithm (RLOO/DPO) -> Fine-tuned model
- Critical path: Generate completions for diverse prompts in multiple languages -> Rank completions using reward model -> Create preference pairs -> Train model with RLOO/DPO -> Evaluate performance
- Design tradeoffs: RLOO generates new samples during training for greater adaptation but requires more computational resources; DPO uses pre-collected preference data for efficiency but may not capture full linguistic variations
- Failure signatures: Reward model overfitting to training data, negative transfer from including multiple languages, performance degradation during training
- First 3 experiments:
  1. Train a model with RLOO using only English preference data and evaluate its performance on all 23 languages
  2. Train a model with DPO using preference data from all 23 languages and compare its performance to the RLOO model
  3. Vary the number of languages and amount of data per language to determine optimal configuration for maximizing multilingual performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal trade-off between dataset size and language coverage in multilingual preference optimization?
- Basis in paper: [inferred] The paper explores varying dataset sizes and numbers of languages, but does not definitively establish the optimal balance for performance gains.
- Why unresolved: The study shows improvements with more languages and larger datasets, but the exact point of diminishing returns or the most efficient allocation of resources remains unclear.
- What evidence would resolve it: Systematic experiments varying both dataset size and language coverage independently, while measuring performance gains, could identify the optimal trade-off point.

### Open Question 2
- Question: How does the quality of synthetic preference data impact the effectiveness of multilingual preference optimization?
- Basis in paper: [explicit] The authors address data scarcity and quality issues, creating synthetic preference data, but acknowledge the potential for translation artifacts and biases.
- Why unresolved: The study does not directly compare the performance of models trained on synthetic data versus human-annotated data, leaving the impact of data quality uncertain.
- What evidence would resolve it: Comparative experiments training models on synthetic and human-annotated preference data of varying quality could quantify the impact of data quality on performance.

### Open Question 3
- Question: Can multilingual preference optimization techniques be effectively scaled to support a much larger number of languages?
- Basis in paper: [explicit] The study covers 23 languages, but acknowledges that this is a small fraction of the world's linguistic diversity.
- Why unresolved: The paper does not explore the challenges and potential limitations of scaling multilingual preference optimization to support hundreds or thousands of languages.
- What evidence would resolve it: Experiments scaling preference optimization to a significantly larger number of languages, while monitoring performance and resource requirements, could determine the feasibility and limitations of broader language support.

## Limitations

- The cross-lingual transfer results depend on the assumption that shared semantic and structural features across languages are sufficient for preference pattern generalization
- The comparison between RLOO and DPO in multilingual settings has limited supporting evidence from the broader literature
- The finding that performance degrades after 0.5 epochs with the largest dataset needs further investigation

## Confidence

**High Confidence:** The core methodology for generating multilingual preference data and the overall experimental framework are well-specified and reproducible. The finding that multilingual preference data is essential for aligning multilingual LLMs is strongly supported by the experimental results.

**Medium Confidence:** The superiority of RLOO over DPO in multilingual settings is supported by the presented results but has limited external validation. The cross-lingual transfer mechanism, while plausible, relies on assumptions about shared semantic features that may not universally hold.

**Low Confidence:** The optimal balance between number of languages and data quantity per language is not clearly established. The finding that performance degrades after 0.5 epochs with the largest dataset needs further investigation.

## Next Checks

1. **Cross-lingual transfer validation:** Test the trained models on a held-out set of languages not present in the training data to verify the claimed cross-lingual transfer capabilities.

2. **Reward model robustness test:** Evaluate the performance of the Cohere reward model on out-of-distribution examples and different language pairs to assess potential overfitting.

3. **Scaling analysis:** Conduct experiments varying the number of languages and data quantity independently to better understand their individual and combined effects on performance.