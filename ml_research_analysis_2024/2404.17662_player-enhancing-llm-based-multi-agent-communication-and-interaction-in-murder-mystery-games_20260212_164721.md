---
ver: rpa2
title: 'PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in
  Murder Mystery Games'
arxiv_id: '2404.17662'
source_url: https://arxiv.org/abs/2404.17662
tags:
- agents
- player
- agent
- players
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces PLAYER, a framework that enhances LLM-based
  agents in Murder Mystery Games (MMGs) by addressing challenges such as undefined
  state spaces and lack of intermediate rewards. PLAYER uses sensor-based state representation
  to model character attributes and an information-driven strategy with information
  gain (IG) heuristic to optimize questioning and suspect pruning.
---

# PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games

## Quick Facts
- arXiv ID: 2404.17662
- Source URL: https://arxiv.org/abs/2404.17662
- Reference count: 16
- Primary result: PLAYER* achieves higher win rates (up to 0.667) and superior reasoning accuracy in Murder Mystery Games compared to baseline agents.

## Executive Summary
PLAYER* is a framework designed to enhance LLM-based agents in Murder Mystery Games by addressing key challenges such as undefined state spaces and lack of intermediate rewards. It introduces a sensor-based state representation to model character attributes and an information-driven strategy using information gain (IG) heuristic to optimize questioning and suspect pruning. Through comprehensive experiments, PLAYER* demonstrates significant improvements in reasoning accuracy, efficiency, and human-agent interaction, achieving higher scores across objective, reasoning, and relation question types while reducing computational costs.

## Method Summary
PLAYER* tackles the challenge of undefined state spaces in Murder Mystery Games by introducing a sensor-based state representation that models character attributes like emotion, motivation, and opportunity. It employs an information gain heuristic to drive efficient suspect pruning and questioning strategies, balancing exploration and exploitation through a weighted combination of historical information gain and LLM-based estimators. The framework was evaluated using the WellPlay dataset and human-agent interaction studies, demonstrating superior performance in reasoning accuracy, win rates, and narrative engagement compared to baseline methods.

## Key Results
- PLAYER* achieves win rates up to 0.667 in agent-vs-human evaluations, outperforming baselines
- Higher scores across objective (0.883), reasoning (0.794), and relation (0.692) question types
- Significant reduction in computational costs while maintaining or improving narrative engagement

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Sensor-based state representation enables agents to model nuanced character attributes (emotion, motivation, opportunity) that are critical for strategic reasoning in MMGs.
- **Mechanism**: The framework maps each agent's attributes into a continuous state vector using domain-specific sensors. These sensors are informed by sociological and psychological theories, allowing the LLM to align suspects with an evolving murderer profile through natural language prompts.
- **Core assumption**: Continuous embedding space can effectively encode and compare high-dimensional social attributes like trust, deception, and intent.
- **Evidence anchors**:
  - [abstract]: "sensor-based state representation to model character attributes"
  - [section]: "We represent each agent ai using a vector ui = (ei, mi, pi, . . .) in the state space L, where each dimension reflects a distinct sensor reading."
  - [corpus]: Weak anchor. No direct evidence from related work on sensor-based embeddings for MMGs.
- **Break condition**: If the language model cannot consistently infer or update sensor values from dialogue, the alignment process will fail and reasoning accuracy will drop.

### Mechanism 2
- **Claim**: Information gain heuristic drives efficient suspect pruning by prioritizing questions that maximally reduce uncertainty about the murderer.
- **Mechanism**: After each interaction, agents compute the reduction in entropy of the suspect list. They use a weighted combination of historical information gain and an LLM-based estimator of expected gain to select the next agent to question.
- **Core assumption**: Information gain correlates strongly with narrative progress and suspect elimination in MMGs.
- **Evidence anchors**:
  - [abstract]: "information-driven strategy with information gain (IG) heuristic to optimize questioning and suspect pruning"
  - [section]: "The information gain for the selected character c∗ in this round is: IG i,c∗ = Hi−1 − Hi."
  - [corpus]: Weak anchor. Related work on MMGs focuses on communication but not on entropy-based pruning strategies.
- **Break condition**: If the LLM's expected information gain estimator is noisy or biased, the pruning will misdirect the search and waste queries.

### Mechanism 3
- **Claim**: Integration of human feedback in evaluation reveals limitations of agent-only baselines and improves narrative engagement.
- **Mechanism**: Human players rate agents on story advancement, question quality, response quality, role immersion, and response speed. This feedback identifies issues like repetitive dialogue and lack of immersion that win-rate metrics miss.
- **Core assumption**: Human-centric metrics provide a more complete picture of agent performance than game-theoretic outcomes alone.
- **Evidence anchors**:
  - [section]: "we extended our evaluation by incorporating a player-centric perspective... This allowed us to assess PLAYER*’s suitability not only as a strategic agent but also as a companion for human players"
  - [section]: "the results demonstrate that PLAYER* outperforms all baseline agents in reasoning and relations"
  - [corpus]: Weak anchor. No related work explicitly evaluates MMG agents with human-centric satisfaction metrics.
- **Break condition**: If human ratings are inconsistent or not aligned with game objectives, the feedback loop may degrade strategic performance.

## Foundational Learning

- **Concept**: Sensor-based state modelling in multi-agent environments
  - **Why needed here**: MMGs lack explicit state spaces; sensors translate natural language interactions into structured attributes for reasoning.
  - **Quick check question**: How does the framework update a suspect's emotion sensor after receiving contradictory dialogue?
- **Concept**: Information gain and entropy in decision-making
  - **Why needed here**: Guides efficient question selection to minimize the suspect list while maintaining narrative coherence.
  - **Quick check question**: What happens to the information gain calculation when multiple suspects share similar attributes?
- **Concept**: Human-centric evaluation in AI agents
  - **Why needed here**: Win-rate metrics alone cannot capture narrative engagement and player satisfaction in MMGs.
  - **Quick check question**: How does the framework balance strategic performance with human-rated immersion scores?

## Architecture Onboarding

### Component Map
Sensor-based State Representation -> Information Gain Heuristic -> Suspect Pruning -> LLM-based Questioning -> Human Feedback Integration

### Critical Path
State representation → Information gain calculation → Suspect pruning → Question selection → LLM response generation → Human evaluation

### Design Tradeoffs
- Sensor complexity vs. computational efficiency: More sensors improve attribute modeling but increase API costs
- Information gain accuracy vs. speed: More precise calculations require additional LLM calls and time
- Human-centric metrics vs. strategic optimization: Balancing narrative engagement with win-rate maximization

### Failure Signatures
- Repetitive or irrelevant questions reducing narrative engagement
- High computational costs due to excessive API calls
- Inconsistent human ratings that don't align with game objectives

### First Experiments
1. Verify sensor-based state representation accurately encodes character attributes from dialogue
2. Test information gain heuristic on simplified suspect lists to validate pruning efficiency
3. Conduct small-scale human-agent interaction study to validate human-centric metrics

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal number of sensors to use in PLAYER*'s sensor-based state representation, and how does sensor selection impact performance across different types of MMGs?
- **Basis in paper**: [explicit] The paper conducts ablation studies on sensor selection, showing diminishing returns with additional sensors and increased computational costs. It selects three sensors (Emotion, Motivation, Opportunity Assessment) based on performance.
- **Why unresolved**: The paper only tests up to five sensors and does not explore whether different sensor combinations might be optimal for different game types or sizes. The diminishing returns could plateau at different points for different MMG scenarios.
- **What evidence would resolve it**: Systematic testing across a wider variety of MMG types with different sensor combinations, measuring both performance gains and computational costs to identify optimal sensor sets for different game characteristics.

### Open Question 2
- **Question**: How does PLAYER* perform when scaled to MMGs with significantly more agents (e.g., 20+ players) and how does the information gain heuristic need to be modified for larger state spaces?
- **Basis in paper**: [inferred] The current experiments use 4-9 players per game, and the information gain formula assumes a finite suspect list that grows with the number of players. The paper notes that the search space is continuous and high-dimensional.
- **Why unresolved**: The paper does not test PLAYER* on larger-scale MMGs, and the information gain calculation may become computationally intractable as the suspect list grows exponentially with more players.
- **What evidence would resolve it**: Testing PLAYER* on MMGs with 20+ agents, measuring performance degradation, and developing scalable approximations for the information gain calculation in high-dimensional spaces.

### Open Question 3
- **Question**: How do different base language models (beyond GPT-3.5 and Qwen2.5-32B-Instruct) affect PLAYER*'s performance, particularly in terms of reasoning accuracy versus computational efficiency?
- **Basis in paper**: [explicit] The paper uses GPT-3.5 and Qwen2.5-32B-Instruct, but notes that "open-source projects such as Llama could introduce strong bias" and implements a heuristic estimator to mitigate this.
- **Why unresolved**: The paper only tests two models and does not explore the trade-offs between different model sizes, capabilities, and costs. It's unclear whether smaller models with better prompting could achieve similar performance at lower cost.
- **What evidence would resolve it**: Systematic testing of PLAYER* with various language models across different size and capability tiers, measuring both absolute performance and cost-efficiency ratios to identify optimal model choices for different deployment scenarios.

## Limitations
- Limited testing on MMGs with more than 9 agents, raising scalability concerns
- Evaluation relies on two specific language models without exploring alternatives
- Sensor-based representation assumes consistent attribute inference from dialogue

## Confidence

### High Confidence
- PLAYER* improves win rates and reasoning accuracy compared to baselines
- Sensor-based state representation effectively models character attributes
- Information gain heuristic drives efficient suspect pruning

### Medium Confidence
- Human-centric metrics accurately reflect agent performance
- Framework scales effectively to larger MMGs
- Alternative language models maintain performance levels

### Low Confidence
- Optimal sensor combinations for different MMG types
- Long-term narrative engagement beyond single games
- Cost-efficiency tradeoffs across different deployment scenarios

## Next Checks
1. Validate sensor-based state representation by testing attribute updates after contradictory dialogue
2. Test information gain heuristic on simplified suspect lists to confirm pruning efficiency
3. Conduct small-scale human-agent interaction study to validate human-centric metrics