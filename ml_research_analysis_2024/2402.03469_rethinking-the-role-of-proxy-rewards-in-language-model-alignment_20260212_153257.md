---
ver: rpa2
title: Rethinking the Role of Proxy Rewards in Language Model Alignment
arxiv_id: '2402.03469'
source_url: https://arxiv.org/abs/2402.03469
tags:
- reward
- arxiv
- proxy
- gold
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work proposes a white-box reward function designed through\
  \ reverse engineering to address limitations in black-box reward models for LLM\
  \ alignment. The method combines interpretable features\u2014length incentive, repetition\
  \ penalty, query relevance, and reward branching by query type\u2014to construct\
  \ a reward function that can reliably emulate ground-truth human preference signals."
---

# Rethinking the Role of Proxy Rewards in Language Model Alignment

## Quick Facts
- **arXiv ID**: 2402.03469
- **Source URL**: https://arxiv.org/abs/2402.03469
- **Reference count**: 32
- **Primary result**: White-box reward functions with interpretable features achieve monotonic alignment between proxy and gold rewards, outperforming black-box reward models on alignment benchmarks while reducing alignment tax on zero-shot NLP tasks.

## Executive Summary
This paper challenges the conventional wisdom that black-box reward models are necessary for aligning large language models with human preferences. Instead, it proposes reverse engineering interpretable white-box reward functions that can reliably emulate ground-truth human preference signals. The authors demonstrate that by carefully designing reward functions with features like length incentive, repetition penalty, and query relevance, they can achieve monotonic relationships between proxy and gold rewards during reinforcement learning training. This approach not only avoids the pitfalls of overoptimization and reward hacking but also performs competitively on standard alignment benchmarks while maintaining zero-shot task performance.

## Method Summary
The method constructs interpretable white-box reward functions using features like Length Incentive (LI), Repetition Penalty (RP), Query Relevance (QR), and Reward Branching (RER) based on query type. The approach involves classifying queries as open-ended (OE) or closed-ended (CE) using GPT-4, then applying different reward combinations accordingly. The Length Incentive promotes sufficient response length while the Repetition Penalty prevents degenerate text generation. Query Relevance measures semantic similarity between queries and responses. The final reward function multiplies these components, with different weightings applied based on query classification. The model is trained using PPO with adaptive KL penalty, and the success criterion is achieving high Spearman correlation between proxy and gold reward signals throughout training.

## Key Results
- White-box reward functions achieve Spearman correlations up to 0.6 with gold rewards, compared to black-box reward models that collapse to negative correlations
- The RER (Reward Branching) variant achieves 39.3% win rate on Vicuna-Bench, outperforming both black-box reward models and LI-only approaches
- Alignment tax is reduced with 49.6% ROUGE-L score on SuperNI, demonstrating preservation of zero-shot NLP capabilities
- The approach shows consistent gains across different model scales (7B and 13B) without requiring human feedback datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reverse reward engineering succeeds when the proxy reward function includes both length incentive and repetition penalty, creating a monotonic relationship with the gold reward signal.
- **Mechanism:** The combination of Length Incentive (LI) and Repetition Penalty (RP) first addresses the verbosity bias in human preference data while preventing degenerate text generation. This dual feature set ensures that responses are sufficiently long without unnecessary repetition, which aligns with ground truth human preferences as measured by the gold reward model.
- **Core assumption:** The ground truth reward model captures both length and repetition quality as important aspects of preference alignment, and these can be reverse-engineered through interpretable features.
- **Evidence anchors:**
  - [abstract]: "Our findings indicate that successfully emulating the gold reward requires generating responses that are relevant with enough length to open-ended questions, while also ensuring response consistency in closed-ended questions."
  - [section 4.1]: "We find that the solely promoting lengthy response, i.e., LI as a proxy reward, fails to monotonically increase the gold reward signal... Penalizing unnecessary repetitions along with lengthy responses shows a better tendency"
  - [corpus]: Weak - corpus provides related papers on reward modeling but lacks specific evidence about LI+RP combinations
- **Break condition:** The mechanism breaks when the ground truth reward model prioritizes aspects beyond length and repetition, such as factual accuracy or nuanced reasoning, which are not captured by these interpretable features.

### Mechanism 2
- **Claim:** Reward branching by query type (open-ended vs closed-ended) significantly improves alignment performance by matching response characteristics to query requirements.
- **Mechanism:** Different query types require different response properties - open-ended queries benefit from diverse, creative responses while closed-ended queries require consistency and constraint satisfaction. By branching the reward function based on query type, the model can optimize for the appropriate response characteristics for each type.
- **Core assumption:** Query types can be accurately classified and that different response characteristics are indeed optimal for different query types.
- **Evidence anchors:**
  - [abstract]: "Our findings indicate that successfully emulating the gold reward requires generating responses that are relevant with enough length to open-ended questions, while also ensuring response consistency in closed-ended questions."
  - [section 4.1]: "We apply the different relevance rewards according to query type to handle such cases... The intuition behind the mixture of reward functions LI(ŷ) · RP(ŷ) · QR(x, ŷ) is aiming for high relevance between query and generated response while promoting long response length and less repetitions."
  - [corpus]: Weak - corpus lacks specific evidence about query-type branching effectiveness
- **Break condition:** The mechanism breaks when query classification is inaccurate or when the distinction between open-ended and closed-ended queries becomes ambiguous, leading to inappropriate reward optimization.

### Mechanism 3
- **Claim:** The monotonic relationship between proxy and gold rewards during RL training indicates successful reverse engineering and alignment with ground truth preferences.
- **Mechanism:** By monitoring the correlation between proxy reward scores and gold reward scores throughout RL training, we can empirically verify whether the designed reward function successfully captures the same preferences as the ground truth reward model. A high Spearman correlation (close to 1) indicates successful reverse engineering.
- **Core assumption:** The gold reward model accurately represents ground truth human preferences and that monotonic improvement in proxy rewards should correspond to improvement in gold rewards.
- **Evidence anchors:**
  - [abstract]: "We define the goal of reverse engineering as achieving a monotonic relationship between the proxy and ground truth (gold) reward signals after training the model with the proxy reward in an RL manner."
  - [section 4.1]: "We find that considering the relevance and adopting different rewards according to query type, i.e., RER, contribute to increasing the gold reward reliably... RER achieves the highest Spearman correlations, indicating a strong monotonic relationship with the gold reward signal"
  - [corpus]: Weak - corpus provides related work on reward modeling but lacks specific evidence about monotonic relationship verification
- **Break condition:** The mechanism breaks when the gold reward model itself is flawed or when there are non-monotonic relationships in the underlying preference space that cannot be captured by interpretable features.

## Foundational Learning

- **Concept:** Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The paper builds on RLHF as the dominant approach to aligning LLMs with human values, where a proxy reward model is trained on human preference data and then used to optimize a policy model through RL.
  - Quick check question: What are the two main stages in the RLHF pipeline described in the related work section?

- **Concept:** Reward Hacking and Overoptimization
  - Why needed here: Understanding these phenomena is crucial because they represent the core problems the paper aims to address - the tendency of policy models to exploit shortcuts in imperfect reward signals.
  - Quick check question: According to the related work, what are the two main issues that arise when training with imperfect proxy reward models?

- **Concept:** White-box vs Black-box Reward Functions
  - Why needed here: The paper's key innovation is moving from black-box reward models to interpretable white-box reward functions composed of specific features, which enables reverse engineering of ground truth preferences.
  - Quick check question: How does the paper define the goal of "reverse reward engineering" in terms of the relationship between proxy and gold reward signals?

## Architecture Onboarding

- **Component map:** Query Processing → Query Type Classification (OE/CE) → Feature Extraction (LI, RP, QR, AR) → Reward Branching → Final Reward Calculation → PPO Training → Model Evaluation

- **Critical path:**
  1. Receive query-response pair
  2. Classify query type (OE/CE)
  3. Extract interpretable features (length, repetition, relevance)
  4. Apply reward branching logic
  5. Calculate final white-box reward
  6. Use reward in PPO training loop
  7. Evaluate proxy vs gold reward correlation

- **Design tradeoffs:**
  - Simplicity vs comprehensiveness: The white-box approach is simpler than training complex black-box reward models but may miss nuanced preferences
  - Feature selection: Choosing which interpretable features to include requires balancing coverage with computational efficiency
  - Query classification accuracy: Relies on GPT-4 for query type classification, which introduces dependency on external model performance

- **Failure signatures:**
  - Negative correlation between proxy and gold rewards (overoptimization)
  - High proxy rewards with low gold rewards (reward hacking)
  - Degraded performance on SuperNI indicating alignment tax
  - Poor query type classification leading to inappropriate reward application

- **First 3 experiments:**
  1. Implement LI-only reward function and verify overoptimization failure (should see proxy rewards increase while gold rewards decrease after initial steps)
  2. Add RP to LI and verify improvement in correlation (should see reduced overoptimization compared to LI-only)
  3. Implement full RER with query type branching and verify monotonic relationship (should see high Spearman correlation close to 1)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the reverse-engineered white-box reward function maintain its effectiveness across different domains beyond language modeling?
- Basis in paper: [inferred] The paper discusses the effectiveness of the reward function across different model scales (7B and 13B) but does not explore its applicability to other domains.
- Why unresolved: The study focuses on language modeling alignment and does not extend its analysis to other domains where reward modeling might be applicable, such as robotics or computer vision.
- What evidence would resolve it: Testing the reward function on alignment tasks in other domains and comparing its performance with domain-specific reward models.

### Open Question 2
- Question: How does the reverse-engineered reward function perform in scenarios with sparse or noisy human feedback?
- Basis in paper: [inferred] The paper does not address the robustness of the reward function under conditions of sparse or noisy feedback, which is a common challenge in real-world applications.
- Why unresolved: The experiments are conducted under controlled conditions with structured datasets, and the paper does not explore the function's behavior in less ideal feedback scenarios.
- What evidence would resolve it: Evaluating the reward function's performance on tasks with intentionally sparse or noisy feedback and comparing it to traditional reward modeling approaches.

### Open Question 3
- Question: Can the interpretable features used in the white-box reward function be extended to capture more nuanced aspects of human preference, such as creativity or emotional tone?
- Basis in paper: [explicit] The paper discusses the use of interpretable features like length incentive, repetition penalty, and query relevance but does not explore additional features that could capture more complex aspects of human preference.
- Why unresolved: The study focuses on a limited set of features and does not investigate the potential for incorporating more nuanced or subjective aspects of human preference into the reward function.
- What evidence would resolve it: Developing and testing additional interpretable features that capture aspects like creativity or emotional tone, and evaluating their impact on alignment performance.

### Open Question 4
- Question: How does the performance of the reverse-engineered reward function scale with the complexity of the tasks it is applied to?
- Basis in paper: [inferred] The paper evaluates the function on alignment benchmarks but does not explore how its performance changes with increasing task complexity.
- Why unresolved: The experiments focus on standard alignment tasks and do not investigate the function's scalability to more complex or multifaceted tasks.
- What evidence would resolve it: Applying the reward function to increasingly complex tasks and analyzing its performance trends to determine scalability limits.

## Limitations

- The approach relies heavily on the accuracy of GPT-4 for query type classification, which is not validated in the paper
- The binary open-ended/closed-ended classification may oversimplify the preference landscape and miss nuanced query types
- The study does not investigate the robustness of the white-box reward functions under conditions of sparse or noisy human feedback

## Confidence

**High Confidence** - The empirical demonstration that length-only rewards (LI) lead to overoptimization and reward hacking is robust and well-validated across multiple experiments. The correlation analysis between proxy and gold rewards provides strong evidence for this failure mode.

**Medium Confidence** - The improvement from LI+RP to full RER with query branching is well-supported, but the relative contribution of each component is not systematically isolated. The benefits of query-type branching specifically could be confounded with other improvements in the reward function design.

**Low Confidence** - Claims about alignment tax reduction on SuperNI are based on limited metrics (ROUGE-L only) and may not fully capture the semantic quality trade-offs. The analysis doesn't examine whether the observed ROUGE-L improvements correspond to actual preservation of task performance.

## Next Checks

1. **Gold Reward Model Ablation** - Replace StarlingRM-34B with a different gold reward model (e.g., trained on different human preference data) and verify whether the same white-box reward functions maintain monotonic relationships, establishing independence from the specific gold reward choice.

2. **Query Classification Error Analysis** - Implement uncertainty quantification for GPT-4 query classification and test the robustness of RER performance when introducing controlled misclassification rates, quantifying the sensitivity of results to classification errors.

3. **Multi-Turn Query Evaluation** - Extend evaluation to multi-turn dialogue contexts where query types may evolve dynamically, testing whether the binary OE/CE classification remains effective or whether more nuanced query type handling is needed for practical deployment.