---
ver: rpa2
title: 'Leaving the barn door open for Clever Hans: Simple features predict LLM benchmark
  answers'
arxiv_id: '2410.11672'
source_url: https://arxiv.org/abs/2410.11672
tags:
- llms
- benchmarks
- these
- language
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether simple n-gram patterns can predict
  answers in modern multiple-choice LLM benchmarks, and whether LLMs exploit these
  patterns. The authors train logistic regression classifiers on unigrams and bigrams
  extracted from prompts to predict ground truth labels across 19 diverse datasets.
---

# Leaving the barn door open for Clever Hans: Simple features predict LLM benchmark answers

## Quick Facts
- arXiv ID: 2410.11672
- Source URL: https://arxiv.org/abs/2410.11672
- Authors: Lorenzo Pacchiardi; Marko Tesic; Lucy G. Cheke; José Hernández-Orallo
- Reference count: 20
- Primary result: Simple n-gram classifiers can predict answers in some LLM benchmarks, and certain LLM families perform better on instances where n-grams predict labels

## Executive Summary
This paper investigates whether simple n-gram patterns can predict answers in modern multiple-choice LLM benchmarks, and whether LLMs exploit these patterns. The authors train logistic regression classifiers on unigrams and bigrams extracted from prompts to predict ground truth labels across 19 diverse datasets. For several benchmarks, these classifiers achieved high Cohen's kappa values (up to 0.6+), indicating strong predictive power using simple features. The analysis also revealed that certain LLM families (OpenAI, Meta, Mistral AI) showed better performance on instances where n-gram models successfully predicted labels, suggesting potential reliance on these superficial patterns. However, the evidence is not conclusive and requires further experimental manipulation to definitively establish whether LLMs are exploiting these cues versus using the intended capabilities.

## Method Summary
The authors systematically examined whether simple n-gram features could predict answers in 19 multiple-choice LLM benchmarks. They extracted unigrams and bigrams from prompts at both word and token levels using GPT-2 tokenization, creating 12 feature vectors (unigrams, unigrams+bigrams) × (word level, token level) × (TF, TF-IDF, Presence). Logistic regression classifiers were trained on each dataset using these features with L1 and L2 regularization. The classifiers' predictive performance was evaluated using Cohen's kappa statistic to measure agreement with ground truth labels while accounting for chance agreement. The study then analyzed LLM performance data across 44 models from 11 families, comparing performance on instances successfully predicted by n-gram models versus those that were not.

## Key Results
- Simple n-gram classifiers achieved Cohen's kappa values up to 0.6+ on several benchmarks, indicating strong predictive power
- Certain LLM families (OpenAI, Meta, Mistral AI) performed better on instances where n-gram classifiers successfully predicted labels
- The proportion of instances where both n-grams and LLMs succeeded was highest in the upper right quadrant of performance plots
- Cohen's kappa above 0.2 indicates meaningful agreement beyond chance that suggests a shortcut exists

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Logistic regression classifiers using unigrams and bigrams can predict ground truth labels with high Cohen's kappa on some benchmarks.
- Mechanism: The logistic regression model learns weights for n-grams that correlate with labels, even though the n-grams are not causally related to the correct answer.
- Core assumption: The benchmark contains statistical regularities or surface-level cues that are predictive of the correct label, independent of the underlying reasoning capability.
- Evidence anchors:
  - [abstract] "We show how simple classifiers trained on these n-grams can achieve high scores on several benchmarks"
  - [section] "For some datasets, relatively high values of Cohen's κ can be achieved"
  - [corpus] Weak - the related work mostly discusses general spurious correlations but not this specific n-gram logistic regression approach
- Break condition: If benchmarks are constructed with careful randomization, counterbalancing, and adversarial filtering to eliminate such n-gram correlations.

### Mechanism 2
- Claim: Some LLM families perform better on instances where n-gram classifiers successfully predict labels.
- Mechanism: These LLMs are using the same surface-level cues that the n-gram classifiers found, rather than performing the intended reasoning.
- Core assumption: The observed difference in performance between predictable and unpredictable instances is not due to confounding factors like varying difficulty.
- Evidence anchors:
  - [abstract] "our further analysis shows that some LLMs may leverage such n-gram associations to solve benchmark tasks"
  - [section] "The figure shows that the proportion of points where n-gram performance is high is the greatest in the upper right quadrant"
  - [corpus] Weak - related work discusses general shortcut learning but not this specific comparison between LLM performance on predictable vs unpredictable instances
- Break condition: If further experimental manipulation (like adversarial instance creation) shows that LLMs can still solve modified instances that break the n-gram correlations.

### Mechanism 3
- Claim: The internal validity of benchmarks is compromised when simple n-grams can predict labels.
- Mechanism: When simple features predict labels, a system can achieve high performance without using the intended capability, making the benchmark measure the wrong thing.
- Core assumption: Cohen's kappa above 0.2 indicates meaningful agreement beyond chance that suggests a shortcut exists.
- Evidence anchors:
  - [abstract] "This suggests that the internal validity of these benchmarks may be compromised"
  - [section] "Agreement between the predicted label and the ground truth, as measured by Cohen's κ... should be close to zero. Traditionally, any value above 0.2 is taken to indicate a small but detectable agreement"
  - [corpus] Moderate - the related survey on spurious correlations supports the general principle that such correlations compromise validity
- Break condition: If the n-gram correlations are shown to be artifacts of dataset construction rather than systematic biases that would appear in real-world deployment.

## Foundational Learning

- Concept: Cohen's kappa statistic
  - Why needed here: It measures agreement between predictions and ground truth while accounting for chance agreement, which is essential for evaluating whether n-gram predictions are meaningful
  - Quick check question: If a classifier randomly guesses between 2 classes, what would Cohen's kappa be? (Answer: 0)

- Concept: Term Frequency-Inverse Document Frequency (TF-IDF)
  - Why needed here: It's one of the feature representations used, and understanding why it generally reduces predictive performance helps interpret the results
  - Quick check question: Why does IDF downweight n-grams that appear frequently across the dataset? (Answer: Because common n-grams are less informative for distinguishing between classes)

- Concept: Clever Hans effect
  - Why needed here: It's the conceptual framework for understanding why surface-level cues can lead to seemingly correct performance that doesn't reflect genuine capability
  - Quick check question: What distinguishes a "Clever Hans" solution from a genuine solution to a task? (Answer: Clever Hans solutions rely on unintended cues rather than the intended reasoning process)

## Architecture Onboarding

- Component map: Feature extraction pipeline -> Logistic regression classifier training -> Evaluation using Cohen's kappa -> LLM performance stratification -> Comparative analysis
- Critical path: Extract features → Train logistic regression → Evaluate kappa on test set → Stratify LLM instances → Compare LLM performance across strata
- Design tradeoffs: Using simple n-gram features trades model expressiveness for interpretability and computational efficiency; TF-IDF vs raw TF trades downweighting common features for potentially missing important common patterns
- Failure signatures: High kappa values on well-constructed benchmarks, LLM performance not differing between predictable and unpredictable instances, or the effect only appearing in specific model families suggesting other factors
- First 3 experiments:
  1. Run the n-gram classifier on a benchmark known to be carefully constructed (like BIG-bench tasks that underwent adversarial filtering) to establish a baseline
  2. Create a simple synthetic benchmark where you know the n-gram correlations are artificial, then verify the method detects them
  3. Take a benchmark with high kappa and systematically remove the most predictive n-grams to see if kappa drops significantly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs actually learn to exploit spurious correlations in benchmark data?
- Basis in paper: [explicit] The paper discusses that benchmark contamination and supervised fine-tuning steps may contribute to this learning but does not empirically investigate the mechanisms.
- Why unresolved: The paper acknowledges this as an important question but focuses on detecting exploitation rather than understanding the learning process. The actual mechanisms of how LLMs internalize these patterns during training remain unexplored.
- What evidence would resolve it: Controlled training experiments showing how different training procedures (contamination levels, fine-tuning data, etc.) affect reliance on spurious correlations, or detailed analysis of attention patterns when LLMs use n-gram shortcuts.

### Open Question 2
- Question: Are there other types of simple surface features beyond unigrams and bigrams that LLMs exploit for benchmark solving?
- Basis in paper: [explicit] The authors state they focused on "a relatively narrow set of simple cues" and acknowledge "it is possible that they are leveraging some other simple non-target features."
- Why unresolved: The study only examined unigrams and bigrams, explicitly leaving open the possibility that other features (syntax patterns, positional cues, formatting artifacts, etc.) could be similarly predictive.
- What evidence would resolve it: Systematic exploration of different feature types (syntactic structures, formatting patterns, positional information) to identify additional exploitable cues across the benchmark suite.

### Open Question 3
- Question: Do LLMs show consistent patterns of exploiting spurious correlations across different benchmark types or are they task-specific?
- Basis in paper: [inferred] The analysis showed mixed results across different datasets and model families, with some models showing effects on certain benchmarks but not others, suggesting potential task-specific exploitation.
- Why unresolved: The paper's analysis aggregated results across diverse benchmark types without investigating whether the exploitation patterns vary systematically by task category or reasoning type.
- What evidence would resolve it: Detailed analysis comparing exploitation patterns across benchmark categories (logical reasoning, commonsense, NLI, etc.) to identify whether certain task types are more vulnerable to shortcut exploitation than others.

## Limitations
- Limited conclusive evidence linking n-gram predictability to LLM shortcut learning without controlled experimental manipulation
- Dataset coverage gaps due to reliance on available instance-level LLM performance data
- Single model family focus without exploring within-family variations in architecture or training
- Limited feature set focusing only on unigrams and bigrams while other surface features may be predictive

## Confidence
- High Confidence: N-gram classifiers can predict labels in some benchmarks with high Cohen's kappa
- Medium Confidence: Certain LLM families perform better on instances where n-grams successfully predict labels
- Low Confidence: Definitive proof that LLMs are systematically exploiting these superficial patterns

## Next Checks
1. **Adversarial Instance Creation**: Systematically modify instances from high-kappa benchmarks by altering or removing the most predictive n-grams while preserving semantic content. If LLM performance drops significantly on modified instances while human performance remains stable, this would provide stronger evidence of shortcut learning.

2. **Cross-dataset Transfer Analysis**: Train n-gram classifiers on one dataset and test their predictive power on another dataset from the same domain. High cross-dataset transferability would suggest systematic biases in benchmark construction, while low transferability would indicate dataset-specific artifacts.

3. **Controlled Model Comparison**: Compare performance of a carefully designed "anti-Clever Hans" model (trained with explicit regularization against n-gram correlations) versus standard models on the same benchmarks. If the anti-Clever Hans model performs significantly worse on predictable instances but similarly on unpredictable ones, this would strengthen the shortcut learning hypothesis.