---
ver: rpa2
title: 'MM-PhyQA: Multimodal Physics Question-Answering With Multi-Image CoT Prompting'
arxiv_id: '2404.08704'
source_url: https://arxiv.org/abs/2404.08704
tags:
- questions
- arxiv
- multimodal
- llav
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of multi-step physics reasoning
  in large language models (LLMs), focusing on high school-level multimodal physics
  problems. The authors curate a novel dataset, MM-PhyQA, and propose a Multi-Image
  Chain-of-Thought (MI-CoT) prompting technique to improve LLM performance on these
  tasks.
---

# MM-PhyQA: Multimodal Physics Question-Answering With Multi-Image CoT Prompting

## Quick Facts
- arXiv ID: 2404.08704
- Source URL: https://arxiv.org/abs/2404.08704
- Authors: Avinash Anand; Janak Kapuriya; Apoorv Singh; Jay Saraf; Naman Lal; Astha Verma; Rushali Gupta; Rajiv Shah
- Reference count: 30
- Key result: LLaVA-1.5 13b with MI-CoT prompting achieves 71.65% accuracy on test set

## Executive Summary
This paper addresses the challenge of multi-step physics reasoning in large language models by curating a novel dataset, MM-PhyQA, and proposing a Multi-Image Chain-of-Thought (MI-CoT) prompting technique. The authors evaluate various LLMs on high school-level multimodal physics problems, finding that fine-tuning with MI-CoT prompting significantly improves performance. LLaVA-1.5 13b achieves the highest accuracy of 71.65% on the test set, demonstrating the effectiveness of incorporating multiple images and reasoning steps for physics problem-solving.

## Method Summary
The authors curate a dataset of 4,500 multimodal physics questions with images and Chain-of-Thought variants. They propose Multi-Image Chain-of-Thought (MI-CoT) prompting, which stacks and concatenates multiple images into a single input to improve the model's ability to associate each image with its corresponding question. The method involves fine-tuning LLaVA models with LoRA rank 128 for 5 epochs using batch size 8, Adam optimizer, and learning rate 2e-4. Images are preprocessed with an autoencoder to match CLIP encoder dimensions before stacking vertically.

## Key Results
- LLaVA-1.5 13b with MI-CoT prompting achieves highest accuracy of 71.65% on test set
- Fine-tuning with MI-CoT prompting significantly improves performance over baseline approaches
- Models using MI-CoT prompting show higher Rouge scores, indicating improved reasoning capabilities
- Larger LLaVA models (13b vs 7b) demonstrate better performance on multimodal physics questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stacking and concatenating multiple images into a single input improves the model's ability to associate each image with its corresponding question in a multi-image CoT prompt.
- Mechanism: The Multi-Image Chain-of-Thought (MI-CoT) technique stacks images from different questions into a single input, allowing the model to process them as a sequence. The model learns to differentiate between images based on their order and context provided by the associated questions.
- Core assumption: The model can effectively learn to associate specific images with their corresponding questions when they are presented as a concatenated sequence, even without explicit image-question mapping.
- Evidence anchors:
  - [abstract] "The rationale for employing multi-image prompting was driven by the anticipation that the Large Language Model (LLM) would effectively distinguish and identify the specific image to be utilized for each question within a single prompt."
  - [section] "The rationale for employing multi-image prompting was driven by the anticipation that the Large Language Model (LLM) would effectively distinguish and identify the specific image to be utilized for each question within a single prompt."
- Break condition: If the model cannot learn to differentiate between images in the concatenated sequence, leading to incorrect associations between images and questions.

### Mechanism 2
- Claim: Fine-tuning LLaVA-1.5 with MI-CoT prompting significantly improves performance on multimodal physics questions compared to using the model out-of-the-box or with standard CoT prompting.
- Mechanism: Fine-tuning adapts the model's weights to the specific task of multimodal physics question answering, while MI-CoT prompting provides a structured approach to incorporating multiple images and reasoning steps. This combination allows the model to learn the specific patterns and reasoning required for physics problems.
- Core assumption: The combination of task-specific fine-tuning and MI-CoT prompting provides a more effective learning signal than either approach alone.
- Evidence anchors:
  - [abstract] "The results show that fine-tuning with MI-CoT prompting significantly improves performance, with LLaVA-1.5 13b achieving the highest accuracy of 71.65% on the test set."
  - [section] "We also showcased the performance of the novel Multi-Image Chain-of-Thought (MI-CoT) Prompting technique, which when used to train LLaVA-1.5 13b yielded the best results when tested on our dataset, with superior scores in most metrics and the highest accuracy of 71.65% on the test set."
- Break condition: If the model overfits to the training data or if the MI-CoT prompting introduces noise that hinders learning.

### Mechanism 3
- Claim: Incorporating Chain-of-Thought prompting in a multimodal setting improves the model's reasoning capabilities and performance on complex physics problems.
- Mechanism: CoT prompting guides the model to generate intermediate reasoning steps, mimicking human problem-solving approaches. In a multimodal setting, this allows the model to break down complex problems into smaller, more manageable steps while considering both textual and visual information.
- Core assumption: Guiding the model to generate reasoning steps improves its ability to solve complex problems that require multiple steps and the integration of different information sources.
- Evidence anchors:
  - [abstract] "We also showcased the performance of the novel Multi-Image Chain-of-Thought (MI-CoT) Prompting technique, which when used to train LLaVA-1.5 13b yielded the best results when tested on our dataset, with superior scores in most metrics and the highest accuracy of 71.65% on the test set."
  - [section] "The MI-CoT Prompting trained version also exhibited high rouge scores as seen in Table 2. It can be observed from Figure 4b that the rouge scores were higher in the LLaV A-1.5 13b CoT variants, showcasing the fact that models that were able to leverage the MI-CoT prompt also showed a bump in the reasoning capabilities."
- Break condition: If the CoT prompting leads to overly verbose or irrelevant reasoning steps that do not contribute to solving the problem.

## Foundational Learning

- **Multimodal learning**: Why needed here - The task involves processing both textual and visual information, requiring the model to understand and integrate information from multiple modalities. Quick check question - What are the key challenges in training models to process and integrate information from multiple modalities?
- **Chain-of-Thought prompting**: Why needed here - Complex physics problems require multiple reasoning steps, and CoT prompting guides the model to generate these intermediate steps, improving its problem-solving abilities. Quick check question - How does Chain-of-Thought prompting differ from standard prompting approaches, and what are its advantages for complex reasoning tasks?
- **Fine-tuning**: Why needed here - General-purpose models need to be adapted to the specific task of multimodal physics question answering, requiring fine-tuning on a domain-specific dataset. Quick check question - What are the key considerations when fine-tuning a large language model on a new task, and how can overfitting be mitigated?

## Architecture Onboarding

- **Component map**: Vision encoder (CLIP) -> Language model (LLaVA) -> Fusion mechanism -> Output generation
- **Critical path**: Image preprocessing -> CLIP encoding -> LLaVA processing -> Reasoning generation -> Answer prediction
- **Design tradeoffs**: Larger models (13b vs 7b) provide better performance but require more computational resources; MI-CoT prompting improves performance but may introduce complexity in training and inference
- **Failure signatures**: Incorrect image associations, irrelevant or missing reasoning steps, computational errors in solving physics problems, overfitting to training data
- **First 3 experiments**:
  1. Evaluate the performance of LLaVA-1.5 13b with and without MI-CoT prompting on a subset of the MM-PhyQA dataset
  2. Compare the performance of different LLaVA variants (7b vs 13b) with MI-CoT prompting on the full MM-PhyQA dataset
  3. Analyze the error types made by the best-performing model to identify areas for improvement

## Open Questions the Paper Calls Out
None

## Limitations
- Data quality and representativeness concerns due to unclear dataset creation process and validation
- Generalizability issues from exclusive focus on LLaVA models without testing alternative architectures
- Limited evaluation methodology lacking human assessment of reasoning quality and statistical significance testing

## Confidence

**High Confidence** - Fine-tuning improves performance compared to zero-shot approaches; larger models perform better on multimodal tasks

**Medium Confidence** - MI-CoT prompting significantly outperforms standard CoT prompting needs more rigorous ablation studies and statistical validation; reported accuracy of 71.65% needs independent verification

**Low Confidence** - Claims about MI-CoT mechanism are speculative without empirical support; superiority claims lack substantiation

## Next Checks

1. **Ablation Study on Image Processing** - Conduct experiments comparing MI-CoT prompting against alternative multi-image handling approaches (separate image inputs, image captioning before concatenation, attention-based image selection) to isolate the contribution of the stacking technique.

2. **Statistical Significance Testing** - Run multiple training trials with different random seeds and compute confidence intervals for the reported accuracy scores. Perform paired statistical tests comparing MI-CoT performance against baseline approaches.

3. **Human Evaluation of Reasoning Quality** - Recruit physics educators to independently assess the correctness and pedagogical quality of the reasoning steps generated by different model variants, moving beyond automated Rouge metrics to evaluate actual problem-solving quality.