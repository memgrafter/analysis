---
ver: rpa2
title: 'Inference-Time Rule Eraser: Fair Recognition via Distilling and Removing Biased
  Rules'
arxiv_id: '2404.04814'
source_url: https://arxiv.org/abs/2404.04814
tags:
- bias
- deployed
- biased
- eraser
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Inference-Time Rule Eraser (Eraser), a method
  for debiasing deployed black-box models without retraining or model parameter access.
  The method works by distilling biased decision rules from the deployed model into
  a patch model and then removing these rules from the model's output during inference.
---

# Inference-Time Rule Eraser: Fair Recognition via Distilling and Removing Biased Rules

## Quick Facts
- arXiv ID: 2404.04814
- Source URL: https://arxiv.org/abs/2404.04814
- Authors: Yi Zhang; Dongyuan Lu; Jitao Sang
- Reference count: 40
- Reduces model bias by over 70% while improving accuracy on seven benchmark datasets

## Executive Summary
This paper introduces Inference-Time Rule Eraser (Eraser), a novel approach for debiasing deployed black-box models without retraining or model parameter access. The method works by distilling biased decision rules from the deployed model into a patch model and then removing these rules during inference through log-space subtraction. Eraser requires only a small calibration set and demonstrates consistent effectiveness across different model architectures, data types, and bias severity levels, outperforming existing post-training debiasing methods while maintaining compatibility with fair training approaches.

## Method Summary
Eraser operates in two stages: distillation and removal. During distillation, a patch model is trained on a small calibration set to learn biased decision rules extracted from the deployed black-box model using a causality-based contrastive sampling strategy. The patch model learns to predict p(y|b), the biased component of the model's output. During inference, the method combines the original model's output with the patch model's output in log space, effectively removing the biased component while preserving target-relevant predictions. This approach requires no access to model parameters and works on any deployed model that outputs probabilities.

## Key Results
- Reduces model bias by over 70% across seven datasets including CelebA, UTKFace, C-MNIST, ImageNet-B, LSAC, COMPAS, and UrbanCars
- Improves accuracy compared to vanilla models, with average group accuracy increasing by 4.2% and worst group accuracy by 6.1%
- Outperforms other post-training debiasing methods while being compatible with fair training approaches
- Effective across different backbone architectures including ResNet and ViT
- Works with calibration sets as small as 1/6 of the training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method removes biased rules by subtracting their probabilistic response in log space without accessing model parameters.
- Mechanism: The deployed model's output contains both target and biased rules. By distilling the biased rule response p(y|b) into a patch model, and subtracting its log output from the original model's log logits, the biased component is removed while preserving the target rule.
- Core assumption: The model's output p(y|x) can be decomposed into target and biased components such that p(y|x) = p(y|x, target) × p(y|b), and this decomposition holds for the softmax probability space.
- Evidence anchors:
  - [abstract] "we need only to subtract the response of biased rules (i.e., p(y|b)) from the original output of the deployed model to get the repaired fair decision"
  - [section] "Theorem 1 shows that debiasing deployed models can be accomplished through adjustments to the model output. It proposes that the influence of biased decision rules p(y|b) can be removed by subtracting them in logarithmic (log) space from the model outputs"
  - [corpus] Weak - no corpus papers directly discuss log-space subtraction for debiasing

### Mechanism 2
- Claim: Rule distillation learning uses contrastive samples to isolate bias features by blocking target feature paths through conditioning.
- Mechanism: By selecting contrastive samples that share the same bias label but different target labels, and averaging their representations, the method creates inputs that contain only bias features. This allows distillation of p(y|b) without requiring explicit bias-only inputs.
- Core assumption: Deep network representations are linearized enough that averaging representations of contrastive samples effectively removes target information while preserving bias information.
- Evidence anchors:
  - [section] "we adopt an indirect approach. We utilize x and contrastive samples to simulate sample editing at the representation layer of model M, ensuring that only xb is provided to the model M"
  - [section] "we use multiple contrastive samples for each target class that we want to compare, rather than depending on just a single sample"
  - [corpus] Weak - no corpus papers directly discuss this specific contrastive distillation approach for bias isolation

### Mechanism 3
- Claim: The patch model trained on distilled biased rules can generalize to extract p(y|b) for test samples without requiring their labels.
- Mechanism: During preparation, the patch model learns the mapping from inputs to their distilled biased rule responses. At inference time, it applies this learned mapping to test samples to extract their biased rule responses for subtraction.
- Core assumption: The relationship between input features and biased rule responses learned on the calibration set generalizes to test samples.
- Evidence anchors:
  - [section] "Through this rule learning process, the trained model G can directly provide the biased rules for the test sample during the testing phase"
  - [section] "For each sample x in the calibration set, we obtain the biased rule p(y|xb) with respect to sample x from the deployed model"
  - [corpus] Weak - no corpus papers discuss generalization of distilled bias rules to unlabeled test data

## Foundational Learning

- Concept: Bayesian inference and conditional probability decomposition
  - Why needed here: The method relies on decomposing p(y|x) into target and biased components using Bayesian analysis
  - Quick check question: Can you explain why p(y|x) can be written as p(y|x, b) × p(b|x) / p(b)?

- Concept: Knowledge distillation and representation learning
  - Why needed here: The method uses distillation to transfer biased rules from the deployed model to a patch model
  - Quick check question: How does the contrastive sampling strategy help isolate bias features from target features in the representation space?

- Concept: Causal inference and backdoor path blocking
  - Why needed here: The method uses causal conditioning to block target feature influence when distilling bias rules
  - Quick check question: Why does conditioning on xy = ∅ help isolate the causal effect of bias features on predictions?

## Architecture Onboarding

- Component map:
  - Deployed black-box model (M) -> Rule distillation module -> Patch model (G) -> Inference-time eraser -> Fair predictions

- Critical path:
  1. Prepare calibration set with target and bias labels
  2. Distill biased rules from M using contrastive sampling
  3. Train patch model G on distilled rules
  4. At inference, combine M(x) and G(x) in log space

- Design tradeoffs:
  - Calibration set size vs. debiasing effectiveness
  - Patch model capacity vs. generalization to test data
  - Computational overhead of patch model vs. accuracy gains

- Failure signatures:
  - Patch model outputs diverge significantly from expected biased rule patterns
  - Debiasing performance degrades as calibration set size decreases
  - Method works well on training calibration data but poorly on test data

- First 3 experiments:
  1. Test debiasing effectiveness with varying calibration set sizes (1/1000 to 1/5 of training data)
  2. Compare debiasing performance across different backbone architectures (ResNet vs. ViT)
  3. Measure generalization by testing on data with different bias severity levels than calibration set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Eraser vary when applied to multi-modal models that incorporate both image and text inputs?
- Basis in paper: [inferred] The paper focuses on image-based tasks but mentions that the core concept is not limited to them and can efficiently debias in structured datasets.
- Why unresolved: The paper does not explore the application of Eraser to multi-modal models, which are increasingly common in AI systems.
- What evidence would resolve it: Experimental results comparing Eraser's performance on multi-modal models versus single-modal models, particularly in scenarios where both visual and textual biases are present.

### Open Question 2
- Question: What are the long-term effects of using Eraser on model robustness and generalization, especially in dynamic environments where bias patterns may shift over time?
- Basis in paper: [inferred] The paper demonstrates Eraser's effectiveness in static datasets but does not address its performance in evolving or dynamic environments.
- Why unresolved: The paper does not investigate the temporal stability of Eraser's debiasing effects or its adaptability to changing bias patterns.
- What evidence would resolve it: Longitudinal studies tracking model performance and bias levels over extended periods, with periodic updates to the calibration set to reflect evolving bias patterns.

### Open Question 3
- Question: Can Eraser be extended to address intersectional biases that arise from the interaction of multiple protected attributes (e.g., gender and race)?
- Basis in paper: [explicit] The paper mentions that Eraser can be stacked to reduce multiple biases simultaneously, as demonstrated on the UrbanCars dataset with background and co-occurring object biases.
- Why unresolved: The paper does not explore the application of Eraser to intersectional biases involving multiple protected attributes, which are often more complex and nuanced.
- What evidence would resolve it: Experimental results showing Eraser's effectiveness in mitigating intersectional biases, with comparisons to existing methods designed specifically for such scenarios.

## Limitations
- The effectiveness depends on the assumption that biased and target rules are multiplicatively separable in probability space
- Generalization of distilled bias rules to test data may fail if calibration set distribution differs significantly
- The linearization assumption for representation space is not thoroughly validated across different backbone architectures

## Confidence
- Mechanism 1 (Log-space subtraction): Medium - The theoretical foundation is sound, but empirical validation across diverse model architectures is limited.
- Mechanism 2 (Contrastive distillation): Low - The linearization assumption for representation space is not thoroughly validated, and alternative sampling strategies are not explored.
- Mechanism 3 (Generalization to test data): Medium - While the method shows good performance across datasets, the specific conditions for successful generalization are not fully characterized.

## Next Checks
1. Test the method's effectiveness across different backbone architectures (CNN, Transformer, MLP) to validate the generality of the log-space subtraction mechanism.
2. Experiment with varying calibration set sizes (1/100 to 1/2 of training data) to establish the minimum data requirement for effective debiasing.
3. Validate the linearization assumption by comparing the effectiveness of different contrastive sampling strategies and measuring representation space properties using canonical correlation analysis.