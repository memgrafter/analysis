---
ver: rpa2
title: Understanding The Effect Of Temperature On Alignment With Human Opinions
arxiv_id: '2411.10080'
source_url: https://arxiv.org/abs/2411.10080
tags:
- human
- distributions
- opinion
- entropy
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the impact of temperature on alignment\
  \ between LLM-generated distributions and human opinions. The authors evaluate three\
  \ methods\u2014direct prompting, Monte Carlo estimation (MCE), and log probability\
  \ estimation (LPE)\u2014on subjective classification tasks from the SemEval 2023\
  \ shared task on \"Learning with Disagreement.\" They find that sampling-based methods\
  \ (MCE and LPE) consistently outperform direct prompting in aligning with human\
  \ distributions."
---

# Understanding The Effect Of Temperature On Alignment With Human Opinions

## Quick Facts
- **arXiv ID**: 2411.10080
- **Source URL**: https://arxiv.org/abs/2411.10080
- **Reference count**: 20
- **Primary result**: Sampling-based methods with higher temperature settings outperform direct prompting in aligning LLM distributions with human opinions

## Executive Summary
This paper investigates how temperature scaling affects the alignment between LLM-generated distributions and human opinions on subjective classification tasks. The authors compare three methods—direct prompting, Monte Carlo estimation (MCE), and log probability estimation (LPE)—across datasets from the SemEval 2023 shared task on "Learning with Disagreement." They find that sampling-based methods consistently outperform direct prompting, with higher temperature settings (T=2.0) yielding better alignment by reducing LLM overconfidence and producing more calibrated distributions.

The study reveals that while entropy can indicate distributional similarity, it has limitations due to its invariance to probability permutations. The Monte Carlo estimation approach shows the lowest cross-entropy and Jensen-Shannon divergence scores across datasets. However, the paper highlights the need for further research to understand the relationship between human opinion variance and model uncertainty, suggesting that current metrics may not fully capture alignment quality.

## Method Summary
The authors evaluate three methods for aligning LLM outputs with human opinion distributions on subjective classification tasks. Direct prompting uses standard LLM inference, while Monte Carlo estimation generates multiple samples to estimate the distribution. Log probability estimation uses the LLM's internal probability estimates. The study tests these methods across three subjective classification datasets from SemEval 2023, varying temperature settings from low (T=0.1) to high (T=2.0). They measure alignment using cross-entropy, Jensen-Shannon divergence, and entropy metrics, comparing LLM distributions against aggregated human judgments.

## Key Results
- Sampling-based methods (MCE and LPE) consistently outperform direct prompting in aligning with human distributions
- Higher temperature settings (T=2.0) improve alignment by reducing overconfidence in LLM predictions
- Monte Carlo estimation shows the lowest CE and JSD scores across datasets, with entropy alignment improving at higher temperatures
- Standard entropy metrics can be misleading due to invariance to probability permutations

## Why This Works (Mechanism)
Temperature scaling in LLMs controls the randomness of output sampling by adjusting the softmax probabilities. Lower temperatures make the model more confident and deterministic, while higher temperatures create more uniform distributions. For subjective tasks where human opinions naturally show variance, higher temperatures help LLMs produce distributions that better match the uncertainty inherent in human judgments. Sampling-based methods like MCE and LPE capture this distributional information more effectively than single-point estimates from direct prompting.

## Foundational Learning

**Temperature scaling**: Why needed - controls randomness in LLM outputs; Quick check - observe how output diversity changes across temperature values

**Cross-entropy divergence**: Why needed - measures distributional difference between model and human opinions; Quick check - lower values indicate better alignment

**Jensen-Shannon divergence**: Why needed - symmetric measure of similarity between probability distributions; Quick check - values closer to 0 indicate higher similarity

**Entropy**: Why needed - measures uncertainty in distributions; Quick check - higher entropy doesn't always mean better alignment due to permutation invariance

**Monte Carlo estimation**: Why needed - generates multiple samples to estimate true output distribution; Quick check - more samples typically yield better distribution estimates

**Log probability estimation**: Why needed - uses internal model probabilities for distribution estimation; Quick check - requires access to model's raw probability outputs

## Architecture Onboarding

**Component map**: LLM model -> Temperature scaler -> Sampling method (Direct/MCE/LPE) -> Distribution estimator -> Alignment metrics (CE, JSD, Entropy) -> Human opinion distribution

**Critical path**: The core pipeline involves taking input text, applying temperature scaling, generating outputs via the chosen method, estimating the output distribution, and comparing it to human opinion distributions using alignment metrics.

**Design tradeoffs**: Higher temperatures improve alignment with human uncertainty but may reduce task performance on objective tasks. Sampling-based methods provide better distributional estimates but require more computational resources and inference time compared to direct prompting.

**Failure signatures**: Poor alignment occurs when temperature is too low (overconfident, peaked distributions) or when using direct prompting on highly subjective tasks (misses variance in human opinions). Entropy-based metrics alone can mislead due to permutation invariance.

**First experiments**:
1. Test alignment across temperature range (0.1 to 2.0) on a simple subjective task to observe the relationship
2. Compare MCE with varying sample sizes (10, 50, 100) to find the optimal balance of accuracy and efficiency
3. Evaluate alignment on datasets with different levels of human disagreement to understand when sampling methods provide the most benefit

## Open Questions the Paper Calls Out
The paper identifies the need for further research to understand the relationship between human opinion variance and model uncertainty. Specifically, how should we interpret the connection between the variance in human judgments and the entropy or uncertainty measures in LLM outputs? This question remains open for future investigation.

## Limitations
- Entropy metrics can be misleading due to invariance to probability permutations
- Results are limited to specific subjective classification tasks and may not generalize to other domains
- Study focuses primarily on GPT-4 without testing across multiple LLM architectures
- Does not account for how different levels of human disagreement affect optimal temperature settings

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Sampling-based methods outperform direct prompting | High |
| Higher temperatures improve alignment by reducing overconfidence | Medium |
| MCE consistently achieves best alignment metrics | Medium |
| Entropy alone is insufficient for measuring alignment quality | High |

## Next Checks

1. Test the temperature-alignment relationship across multiple LLM architectures (Claude, Gemini, Llama) to verify if findings generalize beyond GPT-4

2. Conduct ablation studies varying dataset sizes and disagreement levels to understand when sampling-based methods provide the most benefit

3. Implement permutation-invariant entropy measures to develop more robust metrics for distributional alignment that address the current limitation with standard entropy calculations