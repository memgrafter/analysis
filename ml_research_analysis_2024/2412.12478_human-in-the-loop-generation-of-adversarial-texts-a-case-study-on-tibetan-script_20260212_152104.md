---
ver: rpa2
title: 'Human-in-the-Loop Generation of Adversarial Texts: A Case Study on Tibetan
  Script'
arxiv_id: '2412.12478'
source_url: https://arxiv.org/abs/2412.12478
tags:
- adversarial
- language
- robustness
- text
- texts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HITL-GAT, an interactive system for human-in-the-loop
  generation of adversarial texts. The system addresses the challenge of constructing
  high-quality and sustainable adversarial robustness benchmarks for lower-resourced
  languages by combining automated adversarial attacks with human annotation.
---

# Human-in-the-Loop Generation of Adversarial Texts: A Case Study on Tibetan Script

## Quick Facts
- arXiv ID: 2412.12478
- Source URL: https://arxiv.org/abs/2412.12478
- Authors: Xi Cao; Yuan Sun; Jiajun Li; Quzong Gesang; Nuo Qun; Tashi Nyima
- Reference count: 5
- One-line primary result: First adversarial robustness benchmark for Tibetan script using HITL-GAT system with 3 customized attack methods, achieving AdvRobust scores of 0.5609-0.5726 on CINO models

## Executive Summary
This paper introduces HITL-GAT, an interactive system for human-in-the-loop generation of adversarial texts, addressing the challenge of constructing high-quality adversarial robustness benchmarks for lower-resourced languages. The system combines automated adversarial attacks with human annotation in a four-stage iterative loop: victim model construction, adversarial example generation, high-quality benchmark construction, and adversarial robustness evaluation. As a case study, HITL-GAT is applied to Tibetan script, creating the first adversarial robustness benchmark for the language (AdvTS) that evaluates CINO series models with accuracy scores ranging from 0.5572 to 0.5726. The work provides a valuable reference for other lower-resourced languages and offers a framework for evolving adversarial robustness benchmarks alongside advancements in language models and attack methods.

## Method Summary
HITL-GAT implements a four-stage iterative loop to generate adversarial robustness benchmarks for lower-resourced languages. The process begins with victim model construction through fine-tuning language models on downstream datasets, followed by automated adversarial example generation using three customized Tibetan attack methods (TSAttacker, TSTricker, TSCheater). Generated examples are filtered using edit distance and similarity metrics, then validated through human annotation scoring from 1-5 based on visual and semantic similarity. The resulting high-quality benchmark is used to evaluate model robustness, with the loop allowing continuous evolution as new models, datasets, and attack methods emerge.

## Key Results
- First adversarial robustness benchmark (AdvTS) for Tibetan script using human-in-the-loop validation
- Three customized adversarial attack methods (TSAttacker, TSTricker, TSCheater) specifically designed for Tibetan script
- AdvRobust scores of 0.5609, 0.5572, and 0.5726 for CINO-small-v2, CINO-base-v2, and CINO-large-v2 models respectively
- Four-stage iterative framework enabling benchmark evolution alongside model advancements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HITL-GAT enables sustainable adversarial robustness benchmarks by integrating human-in-the-loop validation into an iterative pipeline.
- Mechanism: The system constructs victim models, generates adversarial examples, filters via human annotation, and evaluates new models in a loop. This allows benchmarks to co-evolve with language model advancements.
- Core assumption: Human annotators can reliably filter out invalid or semantically altered adversarial examples, ensuring high-quality benchmarks.
- Evidence anchors: [abstract] "The resulting benchmark, called AdvTS, evaluates the adversarial robustness of CINO series models, yielding accuracy scores..."; [section] "We screen out the examples that do not satisfy the customized filter conditions from the first-round adversarial texts, and then manually annotate the remaining examples to construct the high-quality adversarial robustness benchmark x."; [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.383. Limited direct evidence of human-in-the-loop sustainability in the corpus.

### Mechanism 2
- Claim: Customized adversarial attack methods for lower-resourced languages overcome the linguistic and resource limitations of direct transfer from high-resource methods.
- Mechanism: Three Tibetan-specific attack methods (TSAttacker, TSTricker, TSCheater) leverage intrinsic textual features (syllable embeddings, context-aware substitutions, visual similarity) to generate effective adversarial texts.
- Core assumption: Lower-resourced languages have unique linguistic features that can be exploited for adversarial generation if properly modeled.
- Evidence anchors: [abstract] "we demonstrate the utility of HITL-GAT through a case study on Tibetan script, employing three customized adversarial text generation methods"; [section] "We have developed several Tibetan textual adversarial attack methods... Our past work is the only one engaged with a truly low-resource language..."; [corpus] Limited corpus evidence of lower-resourced language adversarial attack customization.

### Mechanism 3
- Claim: The four-stage iterative loop ensures adversarial robustness benchmarks evolve alongside new models, datasets, and attack methods.
- Mechanism: Each new iteration rebuilds victim models, regenerates adversarial examples, reconstructs benchmarks, and re-evaluates, maintaining relevance.
- Core assumption: Language models, datasets, and attack methods evolve at a pace that justifies continuous benchmark iteration.
- Evidence anchors: [abstract] "The loop allows adversarial robustness benchmarks to evolve along with new models, datasets, and attacks"; [section] "While a new language model, downstream dataset, or textual adversarial attack method emerges, we can enter the loop again to make the adversarial robustness benchmark evolve."; [corpus] Weak evidence in corpus for continuous benchmark evolution practices.

## Foundational Learning

- Concept: Adversarial text generation
  - Why needed here: Core to constructing benchmarks that test model robustness against imperceptible input perturbations.
  - Quick check question: What distinguishes adversarial examples from random noise in NLP?

- Concept: Human-in-the-loop annotation
  - Why needed here: Ensures adversarial examples maintain semantic and visual similarity while being misclassified, critical for benchmark quality.
  - Quick check question: How do annotators balance visual similarity vs. semantic integrity in adversarial text validation?

- Concept: Lower-resourced language NLP
  - Why needed here: Highlights the challenges of limited data, unique linguistic features, and lack of pre-existing attack methods.
  - Quick check question: Why is direct transfer of English-centric adversarial attacks problematic for Tibetan script?

## Architecture Onboarding

- Component map: Victim Model Construction -> Adversarial Example Generation -> High-Quality Benchmark Construction -> Adversarial Robustness Evaluation -> (Loop)
- Critical path: Victim model training -> Attack execution -> Human annotation filtering -> Benchmark evaluation
- Design tradeoffs: High human annotation cost vs. benchmark quality assurance; customization complexity vs. method generalizability.
- Failure signatures: Low annotator agreement scores; high invalid adversarial example rates; stagnation in benchmark accuracy improvements.
- First 3 experiments:
  1. Fine-tune CINO-small-v2 on TNCC-title training set and verify F1/macro-F1 metrics.
  2. Run TSAttacker on the trained victim model using the test set and collect first-round adversarial examples.
  3. Apply Levenshtein distance filter (â‰¤0.1) to adversarial examples and sample for manual annotation scoring.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can adversarial robustness benchmarks be designed to evolve sustainably alongside continuous advancements in language models and attack methods?
- Basis in paper: [explicit] The paper identifies Problem 3: "How do we update adversarial robustness benchmarks?" and introduces HITL-GAT's iterative loop that allows benchmarks to evolve with new models, datasets, and attacks.
- Why unresolved: While HITL-GAT provides a framework for updating benchmarks, the paper does not explore the long-term sustainability of this approach, particularly how frequently benchmarks should be updated or how to prioritize which new developments warrant benchmark evolution.
- What evidence would resolve it: Longitudinal studies tracking benchmark performance across multiple generations of language models, empirical data on the correlation between benchmark updates and model robustness improvements, and analysis of optimal update frequencies.

### Open Question 2
- Question: What are the optimal filter conditions and human annotation protocols for constructing high-quality adversarial robustness benchmarks across different language families?
- Basis in paper: [explicit] The paper discusses constructing high-quality benchmarks through filter conditions (Edit Distance, Normalized Cross-Correlation Coefficient, Cosine Similarity, BERTScore) and human annotation scoring from 1-5 based on visual and semantic similarity.
- Why unresolved: The paper presents one specific case study with Tibetan but does not investigate whether these same metrics and protocols would be equally effective for other language families with different orthographic and semantic characteristics.
- What evidence would resolve it: Comparative studies applying the same filter conditions and annotation protocols across multiple language families, analysis of annotation agreement rates across languages, and validation of benchmark quality using cross-linguistic evaluation metrics.

### Open Question 3
- Question: How can the human-in-the-loop approach be scaled to reduce annotation costs while maintaining benchmark quality?
- Basis in paper: [inferred] The paper acknowledges that human annotation is indispensable but labor-intensive, and that filter conditions are used to reduce annotation costs, yet does not explore scaling strategies.
- Why unresolved: The paper does not address how to scale human-in-the-loop approaches beyond small-scale case studies, particularly for lower-resourced languages where annotator pools may be limited.
- What evidence would resolve it: Comparative studies of different scaling strategies (active learning, semi-supervised approaches, crowd-sourcing protocols), cost-benefit analyses of various annotation reduction techniques, and empirical validation of benchmark quality under different scaling approaches.

## Limitations

- Lack of publicly available implementation details for the three customized Tibetan attack methods (TSAttacker, TSTricker, TSCheater), limiting independent verification
- Limited transparency regarding human annotation process, including annotator qualifications, inter-annotator agreement metrics, and specific guidelines
- Evaluation framework appears limited to single downstream task (text classification) and small set of victim models (CINO series), potentially limiting generalizability

## Confidence

- **High confidence**: The four-stage iterative loop framework is clearly articulated and logically sound for creating evolving adversarial benchmarks. The methodology for constructing victim models and evaluating adversarial robustness is well-specified and reproducible.
- **Medium confidence**: The reported accuracy scores (0.5609, 0.5572, 0.5726) for CINO models appear technically sound based on the described evaluation procedure, though independent verification is limited by unavailable code.
- **Low confidence**: The sustainability and scalability claims for human-in-the-loop validation are weakly supported, as the paper provides minimal evidence regarding annotation costs, annotator agreement rates, or long-term maintenance requirements.

## Next Checks

1. **Implementation audit**: Request and review the complete source code for TSAttacker, TSTricker, and TSCheater to verify their technical specifications and evaluate their effectiveness on held-out Tibetan text samples.

2. **Annotation protocol validation**: Conduct a small-scale replication of the human annotation process using the described 1-5 scoring system, measuring inter-annotator agreement (e.g., Cohen's kappa) and testing the reliability of semantic-visual similarity judgments.

3. **Generalization test**: Apply the HITL-GAT framework to a different lower-resourced language (e.g., Uyghur or Burmese) to assess whether the four-stage loop and customized attack methodology transfer effectively beyond the Tibetan case study.