---
ver: rpa2
title: Non-Homophilic Graph Pre-Training and Prompt Learning
arxiv_id: '2408.12594'
source_url: https://arxiv.org/abs/2408.12594
tags:
- graph
- prompt
- node
- pre-training
- homophily
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of graph pre-training and prompt
  learning on non-homophilic graphs, where real-world graphs exhibit varying homophilic
  and heterophilic patterns both across graphs and within individual nodes. The authors
  propose ProNoG, a framework that revisits graph pre-training tasks and introduces
  a conditional network (condition-net) to generate node-specific prompts based on
  each node's unique non-homophilic patterns.
---

# Non-Homophilic Graph Pre-Training and Prompt Learning

## Quick Facts
- arXiv ID: 2408.12594
- Source URL: https://arxiv.org/abs/2408.12594
- Authors: Xingtong Yu; Jie Zhang; Yuan Fang; Renhe Jiang
- Reference count: 40
- Key outcome: ProNoG achieves up to 21.49% improvement on node classification and 6.50% on graph classification in one-shot settings

## Executive Summary
This paper addresses the challenge of graph pre-training and prompt learning on non-homophilic graphs, where real-world graphs exhibit varying homophilic and heterophilic patterns. The authors propose ProNoG, a framework that revisits graph pre-training tasks and introduces a conditional network to generate node-specific prompts based on each node's unique non-homophilic patterns. The method demonstrates superior performance compared to state-of-the-art approaches across multiple benchmark datasets.

## Method Summary
ProNoG consists of three main components: a pre-trained GNN encoder using non-homophily pre-training tasks, a subgraph readout module that aggregates 2-hop neighborhood information with similarity weighting, and a condition-net (MLP with bottleneck architecture) that generates node-specific prompts. The framework operates by first pre-training the GNN on non-homophilic graphs, then for each downstream task, encoding nodes, generating prompts conditioned on subgraph embeddings, and applying these prompts to modify node embeddings before fine-tuning with few-shot supervision.

## Key Results
- Outperforms best competitor by up to 21.49% on node classification tasks
- Achieves 6.50% improvement on graph classification in one-shot settings
- Demonstrates consistent improvements across various node groups with different homophily ratios

## Why This Works (Mechanism)

### Mechanism 1
Non-homophilic graphs benefit from non-homophily pre-training tasks because homophily tasks create higher loss when applied to non-homophilic graphs. Homophily tasks maximize similarity between connected nodes, but in non-homophilic graphs, many connected nodes have different labels. This creates more non-homophily samples that increase the loss, making homophily tasks suboptimal for pre-training.

### Mechanism 2
Node-specific prompts generated by a condition-net capture unique non-homophilic patterns better than a single universal prompt. Each node has distinct homophily ratios and neighborhood distributions. A condition-net generates prompts conditioned on subgraph embeddings that capture these unique patterns, allowing fine-grained adaptation to downstream tasks.

### Mechanism 3
The proposed framework maintains computational efficiency while adding node-specific adaptation. The complexity of ProNoG is O((D^L + D^δ) · |V|), where D^L comes from the pre-trained GNN encoder and D^δ from the subgraph readout. Since δ is chosen to be no larger than L, the added complexity from conditional prompting is comparable to the base encoder.

## Foundational Learning

- **Graph neural networks and message passing**: Understanding how GNNs aggregate neighborhood information is crucial for understanding both the pre-training tasks and the subgraph readout mechanism. Quick check: How does a standard GNN layer aggregate information from neighboring nodes, and what are the key hyperparameters that control this aggregation?

- **Homophily and heterophily in graphs**: The entire motivation and theoretical framework is built around the distinction between homophilic and heterophilic graphs, and how different pre-training strategies perform under each condition. Quick check: What is the mathematical definition of homophily ratio for both graphs and individual nodes, and how do these metrics relate to label propagation?

- **Contrastive learning and self-supervised pre-training**: The pre-training tasks use contrastive learning objectives, and understanding how positive and negative samples are constructed is essential for grasping the theoretical analysis of homophily vs non-homophily tasks. Quick check: How does a standard contrastive loss function work in graph pre-training, and what determines whether a sample is considered positive or negative?

## Architecture Onboarding

- **Component map**: Pre-trained GNN encoder -> Subgraph readout module -> Condition-net -> Prompt application layer -> Downstream classifier

- **Critical path**: 
  1. Pre-train GNN using non-homophily task (GraphCL or similar)
  2. For each downstream task, encode all nodes using pre-trained GNN
  3. For each node, read out 2-hop subgraph and generate prompt via condition-net
  4. Apply prompt to modify node embeddings
  5. Train downstream classifier using few-shot labeled data while updating condition-net

- **Design tradeoffs**: 
  - Using 2-hop vs deeper subgraph: 2-hop captures sufficient local patterns while maintaining efficiency; deeper would capture more but increase computation
  - Element-wise product vs other prompt application: Simple and effective; other methods like addition or MLP could provide more flexibility but add complexity
  - Similarity weighting in readout: Captures node-specific importance; unweighted mean is simpler but loses discriminative information

- **Failure signatures**:
  - Poor performance across all datasets: Likely issues with pre-training task selection or condition-net architecture
  - Good performance only on homophilic graphs: May indicate over-reliance on homophily assumptions in pre-training
  - Degradation with more shots: Could indicate overfitting in condition-net parameters
  - Slow convergence: May indicate learning rate issues or insufficient condition-net capacity

- **First 3 experiments**:
  1. Implement basic subgraph readout without similarity weighting and compare to full ProNoG on one dataset to validate importance of weighted aggregation
  2. Replace condition-net with single universal prompt and measure performance drop to quantify benefit of node-specific adaptation
  3. Test different pre-training tasks (homophily vs non-homophily) on a highly heterophilic graph to validate theoretical claims about task selection

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- The theoretical analysis of homophily vs non-homophily tasks relies on specific assumptions about loss decomposition that may not hold for all contrastive learning objectives.
- The subgraph readout mechanism using 2-hop neighborhoods may miss important non-homophilic patterns that emerge at larger scales.
- The condition-net architecture is relatively simple (MLP with bottleneck structure) and may not capture all node-specific patterns effectively.

## Confidence
- **High Confidence**: The empirical superiority of ProNoG over baseline methods is well-supported by extensive experiments across 10 datasets with consistent improvements.
- **Medium Confidence**: The theoretical analysis of why non-homophily tasks work better on heterophilic graphs is sound but relies on specific assumptions about loss functions.
- **Medium Confidence**: The claim that node-specific prompts are necessary and beneficial is supported by ablation studies, but the exact degree of benefit may depend on specific graph characteristics.

## Next Checks
1. **Scale Sensitivity Analysis**: Test ProNoG with different subgraph sizes (1-hop, 3-hop, 4-hop) on a representative non-homophilic dataset to determine the optimal neighborhood size for capturing non-homophilic patterns.

2. **Architecture Ablation**: Replace the MLP condition-net with an attention-based readout mechanism that can dynamically weigh different nodes in the subgraph, and compare performance to the baseline condition-net.

3. **Cross-Domain Transfer**: Evaluate ProNoG's performance when pre-trained on one type of non-homophilic graph (e.g., social networks) and transferred to a different type (e.g., molecular graphs) to test the generality of the non-homophily pre-training task.