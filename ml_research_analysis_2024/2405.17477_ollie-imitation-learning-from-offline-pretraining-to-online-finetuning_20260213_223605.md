---
ver: rpa2
title: 'OLLIE: Imitation Learning from Offline Pretraining to Online Finetuning'
arxiv_id: '2405.17477'
source_url: https://arxiv.org/abs/2405.17477
tags:
- uni00000013
- uni00000011
- uni00000018
- uni00000014
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of combining offline and online
  imitation learning (IL) by proposing a principled method called OLLIE. The key issue
  is that existing approaches often fail when fine-tuning pretrained policies online
  due to misaligned discriminators, which can lead to the unlearning of pretraining
  knowledge.
---

# OLLIE: Imitation Learning from Offline Pretraining to Online Finetuning

## Quick Facts
- **arXiv ID:** 2405.17477
- **Source URL:** https://arxiv.org/abs/2405.17477
- **Reference count:** 40
- **Primary result:** Proposes a principled offline-to-online IL method that avoids unlearning by learning aligned discriminator initialization

## Executive Summary
OLLIE addresses the challenge of combining offline pretraining with online fine-tuning in imitation learning by proposing a principled method that learns both a near-expert policy and an aligned discriminator initialization offline. The key innovation is that it prevents the unlearning problem common in online IL methods, where pretrained policies lose their knowledge when fine-tuned with randomly initialized discriminators. Empirically, OLLIE consistently outperforms baseline methods across 20 challenging tasks, including continuous control and vision-based domains, achieving expert-level performance within 10 online episodes while being more demonstration-efficient and faster converging.

## Method Summary
OLLIE solves a convex-concave saddle point problem offline to learn both a policy and an aligned discriminator initialization, then uses these for online GAIL fine-tuning. The method estimates the reward function via discriminator learning, solves the saddle point problem to obtain optimal dual variables, extracts the policy via weighted behavior cloning, constructs an aligned discriminator from the dual solutions, and finally runs GAIL with the pretrained policy and aligned discriminator for online fine-tuning.

## Key Results
- Consistently outperforms baseline methods across 20 tasks in performance, demonstration efficiency, and convergence speed
- Achieves expert-level performance within 10 online episodes while avoiding the unlearning problem
- Demonstrates robustness across continuous control, vision-based domains, and manipulation tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Misaligned discriminator causes unlearning of pretraining knowledge during online finetuning.
- **Mechanism:** When GAIL starts with a randomly initialized discriminator, it provides contradictory rewards to the pretrained policy, causing the policy to unlearn the offline knowledge it gained.
- **Core assumption:** The pretrained policy has a different stationary distribution than the randomly initialized discriminator's expectations.
- **Evidence anchors:**
  - [abstract] "the initial discriminator (often used in online IL) operates randomly and discordantly against the policy initialization, leading to misguided policy optimization and unlearning of pretraining knowledge"
  - [section] "the initial discriminator of GAIL operates randomly and mismatches the warm-start policy, thus steering an erroneous policy optimization and inducing the policy to unlearn previous knowledge"
- **Break condition:** If the discriminator initialization is aligned with the policy's stationary distribution, the unlearning effect is avoided.

### Mechanism 2
- **Claim:** Convex conjugate transformation enables unbiased stochastic optimization of the dual problem.
- **Mechanism:** The convex conjugate converts the exponential term in the dual objective into a linear form, allowing for unbiased gradient estimation from sampled transitions.
- **Core assumption:** The primal problem is convex and satisfies Slater's condition, ensuring strong duality.
- **Evidence anchors:**
  - [section] "we employ the convex conjugate to transform the dual problem into a convex-concave Stochastic Saddle Point (SSP) problem that can be solved with unbiased stochastic gradients in an entirely offline fashion"
  - [section] "Since F (·, y) is convex with fixed y, and F (ν, ·) is concave with fixed ν, the minimax theorem holds"
- **Break condition:** If the primal problem violates convexity assumptions, the convex conjugate transformation may not preserve the problem structure.

### Mechanism 3
- **Claim:** The optimal auxiliary variable y* from the dual problem enables direct policy extraction without fitting advantage functions.
- **Mechanism:** The saddle point y* satisfies exp(δν*(s,a) - 1) = αy*(s,a), allowing direct computation of the policy via weighted behavior cloning.
- **Core assumption:** The stationary distribution exists and is unique for the given MDP and policy.
- **Evidence anchors:**
  - [section] "the optimum of y can be directly used for offline policy extraction as well as the follow-up computation of the discriminator initialization"
  - [section] "the corresponding policy of ρ* satisfies π*(a|s) = ρ*(s,a)/P_a' ρ*(s,a') ∝ ˜ρo(s,a) exp[δν*(s,a) - 1]"
- **Break condition:** If the policy extraction requires reverse KL-divergence (as stated in Appendix G.3.7), the forward method may not be optimal.

## Foundational Learning

- **Concept: Convex conjugate and duality**
  - Why needed here: The convex conjugate transforms the dual objective to enable unbiased stochastic optimization, which is crucial for the offline-to-online transition.
  - Quick check question: Given f(x) = exp(x-1), what is its convex conjugate f*(y)?

- **Concept: Saddle point problems and minimax optimization**
  - Why needed here: The transformed dual problem becomes a convex-concave SSP, which OLLIE solves iteratively to obtain both policy and discriminator.
  - Quick check question: What theorem guarantees the existence of a saddle point for convex-concave functions?

- **Concept: Stationary distribution and Bellman flow constraints**
  - Why needed here: The policy is extracted from the optimal stationary distribution ρ*, which satisfies the Bellman flow constraints Z.
  - Quick check question: How does the stationary distribution relate to the policy via ρ(s,a) = π(a|s)P_a' ρ(s,a')?

## Architecture Onboarding

- **Component map:**
  - Discriminator network (ϕd) -> Value network (ϕν) -> Weight network (ϕy) -> Policy network (θ) -> Combined discriminator D(ϕy,ϕd)

- **Critical path:**
  1. Estimate reward function ˜R via discriminator learning
  2. Solve SSP min_ν max_y F(ν,y) to obtain ν* and y*
  3. Extract policy π* using y* via weighted BC
  4. Construct aligned discriminator D0 from d* and y*
  5. Run GAIL with π* and D0 for online finetuning

- **Design tradeoffs:**
  - Forward vs reverse KL-divergence for policy extraction (Appendix G.3.7 shows forward has better convergence)
  - Discount factor γ affects undiscounted case handling (Appendix D)
  - Reward scaling α controls variance vs bias tradeoff (Appendix C)

- **Failure signatures:**
  - Poor offline performance → Check discriminator learning and SSP convergence
  - Unlearning during online phase → Verify discriminator alignment computation
  - Slow convergence → Check learning rates and batch sizes

- **First 3 experiments:**
  1. Run OLLIE on ant with 1 expert + 1000 random trajectories, verify offline performance exceeds BC
  2. Test online finetuning on halfcheetah with 3 expert trajectories, verify no unlearning occurs
  3. Compare forward vs reverse policy extraction on hopper, verify convergence differences

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does OLLIE's performance scale with varying qualities of expert demonstrations, beyond the limited "sparse expert data" setting studied?
- **Basis in paper:** [inferred] The paper focuses on OLLIE's performance with limited expert data but does not extensively explore how its effectiveness changes with varying expert data quality (e.g., noisy expert demonstrations, different expert skill levels).
- **Why unresolved:** The paper emphasizes the importance of imperfect demonstrations but does not systematically vary expert data quality or quantity to understand the robustness of OLLIE's performance.
- **What evidence would resolve it:** Conducting experiments with varying levels of expert demonstration quality (e.g., noisy expert data, expert data with different skill levels) and measuring OLLIE's performance across these settings.

### Open Question 2
- **Question:** Can OLLIE's framework be extended to handle multi-task or multi-modal imitation learning scenarios?
- **Basis in paper:** [inferred] The paper focuses on single-task imitation learning and does not explore the applicability of OLLIE to more complex scenarios involving multiple tasks or modalities.
- **Why unresolved:** The paper demonstrates OLLIE's effectiveness in single-task settings but does not investigate its potential for handling more complex, multi-task or multi-modal imitation learning problems.
- **What evidence would resolve it:** Adapting OLLIE's framework to handle multi-task or multi-modal imitation learning and evaluating its performance in these scenarios.

### Open Question 3
- **Question:** How does OLLIE's performance compare to other state-of-the-art offline-to-online reinforcement learning methods that bypass reward extrapolation?
- **Basis in paper:** [inferred] The paper primarily compares OLLIE to offline imitation learning methods and does not extensively compare its performance to other offline-to-online reinforcement learning approaches that avoid reward extrapolation.
- **Why unresolved:** While the paper demonstrates OLLIE's advantages over offline imitation learning methods, it does not provide a comprehensive comparison with other offline-to-online reinforcement learning approaches that also aim to avoid reward extrapolation.
- **What evidence would resolve it:** Conducting experiments comparing OLLIE's performance to other state-of-the-art offline-to-online reinforcement learning methods that bypass reward extrapolation, such as those mentioned in the related work section (e.g., Lee et al., 2022; Mark et al., 2022; Song et al., 2022).

## Limitations

- The theoretical claims about unlearning mechanisms rely on assumptions about discriminator initialization that are not definitively proven
- Computational overhead of offline pretraining (solving saddle point problem) is not thoroughly discussed
- Experiments don't provide ablations isolating the effect of aligned discriminator initialization from other algorithmic choices

## Confidence

- **High confidence** in the core claim that OLLIE outperforms baseline methods in performance, demonstration efficiency, and convergence speed, given extensive empirical evaluation
- **Medium confidence** in theoretical claims about unlearning mechanisms and how aligned discriminator initialization solves it, as these rely on specific assumptions not exhaustively validated
- **Medium confidence** in achieving expert-level performance within 10 online episodes, as this is demonstrated but variability across tasks and seeds is not fully characterized

## Next Checks

1. Conduct an ablation study isolating the effect of the aligned discriminator initialization by comparing OLLIE with a version that uses random discriminator initialization during online fine-tuning
2. Analyze the computational overhead of the offline pretraining phase compared to standard IL methods to assess practical feasibility
3. Perform a more detailed analysis of the variability in performance across different numbers of expert trajectories and seeds to better understand the robustness of the method