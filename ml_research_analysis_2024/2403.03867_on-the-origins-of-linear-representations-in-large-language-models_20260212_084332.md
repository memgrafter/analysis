---
ver: rpa2
title: On the Origins of Linear Representations in Large Language Models
arxiv_id: '2403.03867'
source_url: https://arxiv.org/abs/2403.03867
tags:
- concepts
- representations
- latent
- vectors
- concept
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the origins of linear representations in large
  language models, which are often observed empirically but whose mathematical underpinnings
  remain unclear. The authors propose a simple latent variable model where context
  sentences and next tokens reflect underlying binary concept variables, formalizing
  the concept dynamics of next token prediction.
---

# On the Origins of Linear Representations in Large Language Models

## Quick Facts
- arXiv ID: 2403.03867
- Source URL: https://arxiv.org/abs/2403.03867
- Reference count: 40
- One-line primary result: Proves that softmax with cross-entropy objective and gradient descent bias together promote linear representations of concepts in LLMs

## Executive Summary
This work provides a theoretical foundation for understanding why linear representations emerge in large language models. The authors propose a latent variable model where context sentences and next tokens reflect underlying binary concept variables, formalizing the concept dynamics of next token prediction. Using this model, they prove that the combination of the softmax with cross-entropy objective and the implicit bias of gradient descent together promote the linear representation of concepts. Experiments confirm that linear representations emerge when learning from data matching the latent variable model, validating that this simple structure suffices to yield linear representations.

## Method Summary
The paper develops a latent variable model with binary concept variables representing high-level semantic concepts. The model uses softmax with cross-entropy loss for next token prediction and studies the implicit bias of gradient descent. The authors prove theoretical results about how this combination promotes linear representations through log-odds matching and alignment of embedding and unembedding representations. Experiments are conducted on synthetic data generated from random DAGs with binary variables, as well as on LLaMA-2 and various datasets including OPUS Books and Winograd Schema.

## Key Results
- Theoretical proof that softmax with cross-entropy and gradient descent bias promote linear concept representations
- Experimental validation showing linear representations emerge in simulated data matching the latent variable model
- Testing on LLaMA-2 demonstrates generalizability of insights to real LLMs
- Orthogonality of unrelated concepts emerges from graphical structure and representation alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Next token prediction objective and implicit bias of gradient descent together promote linear representation of concepts
- Mechanism: Under the latent variable model, softmax with cross-entropy encourages log-odds matching between concepts. When log-odds match, steering vectors become parallel, creating linear structure. Gradient descent with exponential loss promotes alignment of embedding and unembedding representations even without log-odds matching.
- Core assumption: The latent variable model accurately abstracts concept dynamics of next token prediction
- Evidence anchors: Abstract statement, section 3.2 on implicit bias, weak corpus support
- Break condition: If concepts cannot be modeled as binary variables in a Markov random field, or if softmax with cross-entropy doesn't encourage log-odds matching

### Mechanism 2
- Claim: Learning conditional probabilities for zero probabilities requires large inner products, pushing representations apart
- Mechanism: When predicting zero conditional probabilities, exponential loss requires very large inner products between embedding and unembedding vectors, creating forces that push different concept representations apart in the representation space.
- Core assumption: The model must learn to predict zero conditional probabilities accurately
- Evidence anchors: Section 3.2 on zero conditional probabilities, section 4 on steering vector norms
- Break condition: If the model doesn't need to learn zero conditional probabilities accurately, or if exponential loss doesn't require large inner products

### Mechanism 3
- Claim: Orthogonality of unrelated concepts emerges from graphical structure and representation alignment
- Mechanism: Under the latent variable model, concepts separated in the graphical structure (Markov random field) will have orthogonal representations when the model learns accurate conditional probabilities. This orthogonality is reinforced by alignment between embedding and unembedding representations.
- Core assumption: The latent variables form a Markov random field, and the model learns accurate conditional probabilities
- Evidence anchors: Section 4 theorem on orthogonality, section 5.1 on simulated data results
- Break condition: If concepts are not separated in the graphical structure, or if the model doesn't learn accurate conditional probabilities

## Foundational Learning

- Concept: Markov random fields and conditional independence
  - Why needed here: The latent variable model assumes concepts form a Markov random field, which determines which concepts are related and how they interact
  - Quick check question: What does it mean for two variables to be conditionally independent given their neighbors in a Markov random field?

- Concept: Implicit bias of gradient descent
  - Why needed here: The paper shows that gradient descent with exponential loss has an implicit bias toward aligning representations, contributing to linear structure emergence
  - Quick check question: How does the implicit bias of gradient descent differ from the explicit objective being optimized?

- Concept: Log-odds matching and its implications
  - Why needed here: The paper shows that when log-odds match between concepts, it leads to linear representations
  - Quick check question: Why does matching log-odds between concepts lead to parallel steering vectors in the representation space?

## Architecture Onboarding

- Component map: Latent variable model -> Conditional probability learning -> Representation alignment -> Orthogonality emergence
- Critical path: Latent variable model → Conditional probability learning → Representation alignment → Orthogonality emergence
- Design tradeoffs:
  - Tradeoff between model complexity and interpretability: The latent variable model simplifies concept dynamics but may not capture all natural language nuances
  - Tradeoff between accuracy and efficiency: Learning accurate conditional probabilities requires significant computational resources but leads to better representations
- Failure signatures:
  - If concepts cannot be modeled as binary variables in a Markov random field, theoretical results may not hold
  - If softmax with cross-entropy doesn't encourage log-odds matching, linear representations may not emerge
  - If the model doesn't need to learn zero conditional probabilities accurately, exponential loss may not create necessary forces
- First 3 experiments:
  1. Verify that the latent variable model accurately abstracts concept dynamics by testing on synthetic data with known concept structures
  2. Test the relationship between log-odds matching and linear representations by training models with different levels of log-odds matching
  3. Investigate the impact of learning zero conditional probabilities on representation alignment by training models with and without zero probabilities in training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does the implicit bias of gradient descent promote linear representations beyond the specific subproblem studied in Theorem 5?
- Basis in paper: [inferred] The paper studies gradient descent on a specific subproblem and shows it promotes linearity, but acknowledges this is a simplified setting
- Why unresolved: The paper focuses on a simplified subproblem to highlight underlying dynamics. It's unclear if and how implicit bias extends to full next-token prediction
- What evidence would resolve it: Empirical studies showing emergence of linear representations when training on more realistic next-token prediction tasks, varying model architectures, training objectives, and dataset characteristics

### Open Question 2
- Question: How do the latent conditional model's assumptions, such as injectivity of concept-to-token mapping and existence of core concepts, affect emergence of linear representations?
- Basis in paper: [explicit] The paper states these assumptions for simplicity and tractability but acknowledges they may not hold in practice
- Why unresolved: The paper relies on these assumptions for theoretical analysis, but their impact on real-world phenomenon of linear representations is unknown
- What evidence would resolve it: Relaxing these assumptions in the latent conditional model and studying resulting representations, plus analyzing geometry of LLM representations when these assumptions are violated in real datasets

### Open Question 3
- Question: What is the relationship between dimensionality of representation space and quality of linear representations in the latent conditional model?
- Basis in paper: [explicit] The paper conducts experiments with decreasing dimensions and observes linear representations persist, but effect is not significant
- Why unresolved: The paper provides limited exploration of dimensionality's effect on linearity. It's unclear if there's optimal dimensionality for linear representations and how it scales with number of concepts
- What evidence would resolve it: Systematic study of relationship between dimensionality and linearity across varying numbers of concepts and model architectures, identifying critical dimensionality for effective linear representations

## Limitations
- Binary concept assumption may oversimplify the continuous and hierarchical nature of semantic concepts in natural language
- Markov random field structure assumes discrete conditional independence that may not hold for real-world concepts
- Experimental validation is primarily on synthetic data and one specific LLM, limiting generalizability

## Confidence
- Theoretical framework and proofs: High
- Experimental validation on synthetic data: Medium
- Generalization to real LLMs: Medium
- Practical implications for LLM interpretability: Medium

## Next Checks
1. Test the latent variable model with non-binary concepts: Extend the theoretical framework to handle multi-valued or continuous concepts, and validate whether linear representations still emerge under these more realistic conditions.

2. Scale experiments to diverse LLMs and datasets: Replicate the LLaMA-2 experiments across multiple model families (e.g., GPT, BERT) and diverse datasets including conversational data, technical documentation, and multilingual corpora to assess generalizability.

3. Investigate the role of model scale: Systematically vary model size and capacity in both the theoretical model and practical experiments to determine how scale affects the emergence and quality of linear representations, particularly the relationship between parameter count and representation linearity.