---
ver: rpa2
title: 'QGEval: Benchmarking Multi-dimensional Evaluation for Question Generation'
arxiv_id: '2406.05707'
source_url: https://arxiv.org/abs/2406.05707
tags:
- question
- questions
- metrics
- answer
- dimensions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces QGEval, a multi-dimensional evaluation benchmark
  for question generation that assesses generated questions across seven dimensions:
  fluency, clarity, conciseness, relevance, consistency, answerability, and answer
  consistency. The benchmark contains 3,000 questions generated by 15 QG models from
  200 passages.'
---

# QGEval: Benchmarking Multi-dimensional Evaluation for Question Generation

## Quick Facts
- arXiv ID: 2406.05707
- Source URL: https://arxiv.org/abs/2406.05707
- Reference count: 35
- Primary result: Introduces QGEval benchmark with 7 evaluation dimensions for question generation quality

## Executive Summary
This paper introduces QGEval, a multi-dimensional evaluation benchmark for question generation that assesses generated questions across seven dimensions: fluency, clarity, conciseness, relevance, consistency, answerability, and answer consistency. The benchmark contains 3,000 questions generated by 15 QG models from 200 passages. Through comprehensive analysis, the authors find that most QG models perform poorly in answerability and answer consistency, while existing automatic metrics show limited correlation with human evaluation scores across these dimensions. The work highlights the need for improved QG models and more reliable evaluation metrics.

## Method Summary
The study constructs a benchmark dataset of 3,000 questions generated by 15 different QG models from 200 passages (100 from SQuAD and 100 from HotpotQA). Two rounds of human annotation were conducted with 3 annotators per question, scoring each on 7 dimensions (fluency, clarity, conciseness, relevance, consistency, answerability, and answer consistency) using a 1-3 scale. The benchmark evaluates 15 automatic metrics, including both reference-based and reference-free approaches, and analyzes their correlation with human evaluation scores using Pearson correlation coefficients. Statistical significance is assessed using t-tests and Nemenyi tests, with inter-annotator agreement measured using Krippendorff's alpha.

## Key Results
- Most QG models achieve high scores in fluency, clarity, conciseness, relevance, and consistency, but perform poorly in answerability and answer consistency
- Reference-free metrics generally outperform reference-based metrics in correlation with human evaluation scores
- GPT-4-based metrics show the highest correlation with human scores among automatic metrics, but still fall below 0.4 Pearson correlation
- Seven dimensions show reasonable independence with Pearson correlations ranging from 0.04 to 0.67

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-dimensional evaluation captures diverse quality aspects that single-dimension metrics miss
- Mechanism: By explicitly evaluating seven distinct dimensions, the benchmark identifies quality issues that aggregate metrics cannot detect
- Core assumption: Different dimensions capture independent aspects of question quality
- Evidence anchors:
  - [abstract] "evaluates both generated questions and existing automatic metrics across 7 dimensions"
  - [section] "Pearson correlations between the seven dimensions are within a reasonable range (0.04 to 0.67), and most p-values in the Nemenyi test are below 0.05"
- Break condition: If dimensions become highly correlated (r > 0.8) or p-values consistently exceed 0.05, the independence assumption fails

### Mechanism 2
- Claim: Reference-free metrics better align with human evaluation than reference-based metrics
- Mechanism: Reference-free metrics evaluate questions based on their inherent properties rather than similarity to reference answers
- Core assumption: Question generation is a one-to-many task where multiple valid questions exist for the same context
- Evidence anchors:
  - [section] "In general, reference-free metrics tend to outperform reference-based metrics, exhibiting higher correlation coefficients with human evaluation"
  - [section] "Metrics that conduct multi-dimensional evaluations tend to perform better across a wider range of dimensions"
- Break condition: If reference-based metrics show superior correlation with human scores on new datasets

### Mechanism 3
- Claim: LLM-based evaluation methods show promise but still have alignment gaps with human judgment
- Mechanism: Large language models can capture nuanced linguistic and semantic properties
- Core assumption: LLMs can approximate human evaluation through their understanding of language quality
- Evidence anchors:
  - [section] "metrics based on GPT-4 achieve the highest correlations with human scores, which demonstrates the potential of using LLMs for QG evaluation"
  - [section] "LLM-based metrics still fail to align closely with human evaluation (Pearson correlations are below 0.4)"
- Break condition: If LLM-based metrics consistently achieve correlations above 0.8 with human evaluation

## Foundational Learning

- Concept: Pearson correlation coefficient
  - Why needed here: Used to measure alignment between automatic metrics and human evaluation scores
  - Quick check question: What does a Pearson correlation of 0.2 indicate about the relationship between two variables?

- Concept: Krippendorff's alpha coefficient
  - Why needed here: Used to measure inter-annotator agreement in the human evaluation process
  - Quick check question: What range of values indicates acceptable inter-annotator reliability using Krippendorff's alpha?

- Concept: T-test for statistical significance
  - Why needed here: Used to determine if differences in model performance across dimensions are statistically significant
  - Quick check question: What p-value threshold is typically used to determine statistical significance in this context?

## Architecture Onboarding

- Component map: Passage + Answer → 15 QG Models → 3,000 Questions → 3 Annotators → 7-Dimensional Scores → 15 Automatic Metrics → Statistical Analysis
- Critical path: Generate questions → Human annotation → Metric evaluation → Statistical analysis → Benchmark insights
- Design tradeoffs: Reference-free vs. reference-based metrics (coverage vs. alignment), multiple dimensions vs. single score (comprehensiveness vs. simplicity), human vs. automatic evaluation (accuracy vs. scalability)
- Failure signatures: Low inter-annotator agreement (Krippendorff's alpha < 0.4), metrics showing no correlation with human scores, t-test p-values consistently > 0.05 indicating poor discriminative power
- First 3 experiments:
  1. Compare correlation coefficients of reference-based vs. reference-free metrics on a subset of questions
  2. Test inter-annotator agreement on a sample of 100 questions before full annotation
  3. Evaluate discriminative power by comparing top/bottom 10% models across dimensions using t-tests

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What dimensions beyond the seven basic ones (fluency, clarity, conciseness, relevance, consistency, answerability, and answer consistency) would be more discriminative for evaluating question generation models?
- Basis in paper: [explicit] The authors note that their seven dimensions have limited discriminative power among current QG models
- Why unresolved: The authors suggest exploring more advanced dimensions but do not propose or test specific new dimensions
- What evidence would resolve it: A study that introduces and validates new evaluation dimensions for QG, demonstrating their discriminative power

### Open Question 2
- Question: How can automatic metrics be improved to better align with human judgments across the seven dimensions of question generation quality?
- Basis in paper: [explicit] The authors find that existing automatic metrics show limited correlation with human evaluation scores
- Why unresolved: While the authors evaluate 15 existing metrics and observe their shortcomings, they do not propose specific improvements
- What evidence would resolve it: Development and validation of new automatic metrics that demonstrate significantly higher correlation with human judgments

### Open Question 3
- Question: How does the quality of generated questions vary across different question generation scenarios beyond the passage-based scenario studied in this paper?
- Basis in paper: [explicit] The authors acknowledge that their work focuses on generating questions based on a passage and is not applicable to other scenarios
- Why unresolved: The authors do not explore how their seven dimensions would apply to other QG scenarios
- What evidence would resolve it: A study that adapts the seven dimensions and evaluation approach to other QG scenarios

## Limitations

- Limited generalizability to domains outside SQuAD and HotpotQA
- Small number of annotators (3) per question may not capture full diversity of human judgment
- Automatic metrics evaluated represent only a subset of existing evaluation methods

## Confidence

- **High confidence**: Most QG models struggle with answerability and answer consistency (supported by consistent human evaluation patterns)
- **Medium confidence**: Superiority of reference-free metrics over reference-based metrics (depends on specific metrics tested)
- **Medium confidence**: Correlation patterns between automatic metrics and human evaluation (may vary across different datasets)

## Next Checks

1. Expand inter-annotator agreement testing by replicating human evaluation with 5-7 annotators per question to verify if Krippendorff's alpha values remain stable

2. Apply the benchmark to questions generated from domains outside SQuAD and HotpotQA to test generalizability of metric correlation findings

3. Systematically evaluate additional automatic metrics not included in the original study, particularly newer reference-free metrics, to confirm whether observed superiority persists across a broader evaluation landscape