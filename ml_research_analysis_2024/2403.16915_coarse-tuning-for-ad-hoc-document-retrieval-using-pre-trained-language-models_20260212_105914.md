---
ver: rpa2
title: Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models
arxiv_id: '2403.16915'
source_url: https://arxiv.org/abs/2403.16915
tags:
- query
- document
- coarse-tuning
- fine-tuning
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces coarse-tuning, an intermediate learning stage
  for pre-trained language model (PLM)-based information retrieval that bridges the
  gap between NLP pre-training and IR fine-tuning. The proposed Query-Document Pair
  Prediction (QDPP) learns query representations and query-document relations using
  click data, reducing the burden of fine-tuning.
---

# Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models

## Quick Facts
- arXiv ID: 2403.16915
- Source URL: https://arxiv.org/abs/2403.16915
- Authors: Atsushi Keyaki; Ribeka Keyaki
- Reference count: 0
- Key outcome: Coarse-tuning significantly improves MRR and nDCG@5 on four IR datasets, outperforming standard fine-tuning by 9-12% on Robust04.

## Executive Summary
This paper addresses the challenge of applying pre-trained language models (PLMs) to ad-hoc document retrieval by introducing an intermediate "coarse-tuning" stage between general NLP pre-training and IR-specific fine-tuning. The proposed method learns query representations and query-document relations using click data through a novel Query-Document Pair Prediction (QDPP) task, reducing the burden on the subsequent fine-tuning stage. Experiments on four benchmark IR datasets demonstrate consistent improvements in ranking effectiveness, with the approach showing particular strength on datasets containing more keyword-based queries.

## Method Summary
The method introduces coarse-tuning as an intermediate learning stage that bridges the gap between BERT's NLP pre-training and IR fine-tuning. During coarse-tuning, the model is trained on click data from ORCAS using two objectives: Masked Language Model (MLM) and Query-Document Pair Prediction (QDPP). The QDPP task predicts whether a query-document pair is appropriate based on click data, learning query representations and query-document relations. After coarse-tuning, the model undergoes standard fine-tuning on IR datasets using a classification task. The approach uses pre-trained BERT (prajjwal1/bert-tiny), applies coarse-tuning for 4 epochs on ORCAS, then fine-tunes on IR datasets for 3 epochs.

## Key Results
- Coarse-tuning significantly improves MRR and/or nDCG@5 on four IR datasets compared to pre-trained BERT alone
- The proposed method outperformed standard fine-tuning by 9-12% on Robust04 dataset
- Coarse-tuned models generate more query-like text than pre-trained BERT in query prediction experiments, suggesting learned query representations
- Experiments showed coarse-tuning alleviates overfitting during fine-tuning, particularly on smaller IR datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coarse-tuning reduces the representational gap between BERT's NLP pre-training and IR fine-tuning by learning query-specific representations and query-document relations.
- Mechanism: By applying Masked Language Model (MLM) and Query-Document Pair Prediction (QDPP) on real click data (ORCAS), the model learns to represent short, keyword-based queries and their relevance to documents before fine-tuning.
- Core assumption: Query representations and query-document relations learned during coarse-tuning generalize to downstream IR tasks and improve fine-tuning effectiveness.
- Evidence anchors:
  - [abstract]: "By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks."
  - [section]: "High effectiveness requires costly fine-tuning, i.e., training with an IR-specific expensive network and/or training on a huge dataset such as MS MARCO... One possible reason for expensive fine-tuning is the difference in the nature of input data."
  - [corpus]: No direct corpus evidence available; claims rely on ORCAS click data and IR datasets (Robust04, TREC-COVID, etc.).
- Break condition: If query representations learned from click data do not transfer to IR relevance ranking, or if the fine-tuning stage fails to leverage these representations effectively.

### Mechanism 2
- Claim: Query-Document Pair Prediction (QDPP) enables the model to learn the semantic relevance between queries and documents.
- Mechanism: QDPP is inspired by Next Sentence Prediction (NSP) but adapted to predict whether a query-document pair is appropriate, based on click data from ORCAS.
- Core assumption: The clicked query-document pairs in ORCAS represent a reliable proxy for relevance, allowing the model to learn meaningful query-document relations.
- Evidence anchors:
  - [abstract]: "We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs."
  - [section]: "We use query-document pairs in the clicked relation as appropriate query-document pairs."
  - [corpus]: No direct corpus evidence; assumption relies on click data as a proxy for relevance.
- Break condition: If the clicked pairs in ORCAS are not truly representative of relevance, or if QDPP does not capture the necessary semantics for ranking.

### Mechanism 3
- Claim: Coarse-tuning alleviates overfitting during fine-tuning by providing better initial representations for queries and documents.
- Mechanism: By learning query representations and query-document relations in coarse-tuning, the fine-tuning stage can focus more on ranking-specific learning, reducing the risk of overfitting due to limited IR training data.
- Core assumption: Overfitting is reduced because coarse-tuning provides a more robust starting point for fine-tuning, particularly when IR datasets are small.
- Evidence anchors:
  - [section]: "This result suggests that coarse-tuning alleviates over-fitting."
  - [section]: "These had even lower effectiveness than BM25, indicating the necessity of fine-tuning."
  - [corpus]: No direct corpus evidence; claim inferred from experimental results.
- Break condition: If fine-tuning still overfits despite coarse-tuning, or if the benefits of coarse-tuning do not transfer to ranking tasks.

## Foundational Learning

- Concept: Masked Language Model (MLM) in BERT
  - Why needed here: MLM helps the model learn deep bidirectional representations, which is essential for understanding both queries and documents in IR tasks.
  - Quick check question: How does MLM in coarse-tuning differ from MLM in BERT's original pre-training?

- Concept: Query-document relevance
  - Why needed here: Understanding the relevance between queries and documents is the core task of IR; coarse-tuning explicitly learns this through QDPP.
  - Quick check question: Why is click data from ORCAS considered a good proxy for relevance?

- Concept: Fine-tuning vs. pre-training
  - Why needed here: The paper highlights the gap between pre-training (NLP) and fine-tuning (IR); understanding this gap is crucial for appreciating the need for coarse-tuning.
  - Quick check question: What are the main differences in input data between NLP pre-training and IR fine-tuning?

## Architecture Onboarding

- Component map: Pre-trained BERT -> Coarse-tuning (MLM + QDPP on ORCAS) -> Fine-tuning (IR-specific classification) -> Downstream IR datasets (Robust04, GOV2, TREC-COVID, TREC-DL)
- Critical path: 1. Load pre-trained BERT 2. Apply coarse-tuning (MLM + QDPP) on ORCAS 3. Fine-tune on IR dataset 4. Evaluate on test queries
- Design tradeoffs:
  - Using real click data (ORCAS) for coarse-tuning vs. synthetic queries
  - Balancing coarse-tuning epochs to avoid overfitting vs. underfitting
  - Simple fine-tuning (classification) vs. more complex IR-specific methods
- Failure signatures:
  - Poor performance on IR tasks despite coarse-tuning (e.g., lower than BM25)
  - Overfitting during fine-tuning (e.g., performance drops after initial epochs)
  - Inconsistent results across different IR datasets
- First 3 experiments:
  1. Verify coarse-tuning improves over pre-trained BERT alone (baseline comparison)
  2. Test coarse-tuning with different ORCAS sampling rates to find optimal setting
  3. Evaluate whether fine-tuning without coarse-tuning overfits on small IR datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal sampling rate from ORCAS for coarse-tuning across different IR datasets?
- Basis in paper: [explicit] The paper evaluates ORCAS sampling rates from 1% to 10% and finds 8% to be optimal for Robust04, but notes this may vary by dataset.
- Why unresolved: The optimal sampling rate may depend on dataset characteristics like query length, document length, and domain. The paper only evaluates one dataset for this parameter.
- What evidence would resolve it: Systematic experiments varying sampling rates on multiple IR datasets with different characteristics (query length, document length, domain) would identify general patterns or dataset-specific optimal rates.

### Open Question 2
- Question: How does coarse-tuning scale with larger BERT models?
- Basis in paper: [inferred] The paper uses a small BERT-tiny model and notes that behavior with larger models hasn't been examined, suggesting potential differences in effectiveness and computational requirements.
- Why unresolved: Larger models may require different coarse-tuning configurations (epochs, learning rates) and may show different patterns of effectiveness improvement compared to smaller models.
- What evidence would resolve it: Experiments with BERT-base, BERT-large, and larger models applying the same coarse-tuning methodology, comparing effectiveness gains and computational costs.

### Open Question 3
- Question: Does coarse-tuning's effectiveness depend on the proportion of keyword vs. natural language queries in the target dataset?
- Basis in paper: [explicit] The paper observes that coarse+fine showed greater improvement on datasets with more keyword queries, and notes that ORCAS queries are mostly keywords, suggesting a relationship between query type similarity and effectiveness.
- Why unresolved: The mechanism behind this observation is unclear - whether it's due to similar query representations or whether the query-document relation patterns learned transfer better between similar query types.
- What evidence would resolve it: Experiments controlling for query type characteristics (length, structure, keyword density) in both ORCAS and target datasets, measuring effectiveness as a function of query type similarity.

## Limitations
- The paper relies heavily on click data (ORCAS) as a proxy for relevance without empirical validation of this assumption
- Variance between runs and statistical significance testing are not reported for the effectiveness improvements
- The paper doesn't compare against more sophisticated fine-tuning methods that might achieve similar gains without requiring coarse-tuning

## Confidence
- Coarse-tuning mechanism (Medium): The general approach is well-founded, but the specific implementation details of QDPP and the exact contribution of each component (MLM vs QDPP) are not fully isolated.
- Effectiveness improvements (High): The reported gains on multiple datasets are substantial and consistent, though lacking statistical significance tests.
- Overfitting reduction (Medium): The claim is supported by results but not directly measured or compared against other regularization techniques.

## Next Checks
1. Conduct ablation studies to isolate the contribution of MLM vs QDPP during coarse-tuning and determine which component drives most of the performance gains.
2. Perform statistical significance testing across multiple runs to establish confidence intervals for the reported improvements and verify they are not due to random variation.
3. Test whether coarse-tuning provides benefits when using alternative relevance proxies beyond click data, such as human-labeled relevance judgments or synthetically generated query-document pairs.