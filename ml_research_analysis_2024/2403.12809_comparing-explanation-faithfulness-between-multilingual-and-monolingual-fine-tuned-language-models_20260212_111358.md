---
ver: rpa2
title: Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned
  Language Models
arxiv_id: '2403.12809'
source_url: https://arxiv.org/abs/2403.12809
tags:
- xlm-r
- monolingual
- faithfulness
- roberta
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares the faithfulness of feature attribution (FA)
  methods between multilingual and monolingual language models across five languages
  and five FA techniques. The authors find that FA faithfulness varies between multilingual
  and monolingual models, with larger multilingual models exhibiting less faithful
  FAs compared to their monolingual counterparts.
---

# Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned Language Models

## Quick Facts
- **arXiv ID**: 2403.12809
- **Source URL**: https://arxiv.org/abs/2403.12809
- **Reference count**: 19
- **Primary result**: FA faithfulness varies between multilingual and monolingual models, with larger multilingual models showing less faithful attributions than monolingual counterparts, potentially due to more aggressive tokenization.

## Executive Summary
This paper systematically compares the faithfulness of feature attribution methods between multilingual and monolingual language models across five languages (English, Chinese, Spanish, French, Hindi) and five FA techniques. The study reveals that larger multilingual models (XLM-R large) produce less faithful feature attributions compared to their monolingual counterparts, with the disparity potentially driven by differences in tokenization aggressiveness between models. The research highlights important considerations for interpreting explanations from multilingual models and suggests that model tokenization strategy significantly impacts explanation quality.

## Method Summary
The study fine-tunes multilingual (mBERT, XLM-R) and monolingual (BERT, RoBERTa) models on five languages using datasets for sentiment analysis, topic classification, reading comprehension, paraphrase identification, and natural language inference. Models are trained using AdamW optimizer with learning rate 1e-5 for five epochs, selecting the best model based on development set loss. Five feature attribution methods (Attention, Scaled Attention, InputXGrad, Integrated Gradients, DeepLIFT) are evaluated using Hard Sufficiency & Comprehensiveness and Soft Sufficiency & Comprehensiveness metrics. The authors conduct qualitative analysis and tokenization comparison to investigate the source of faithfulness disparities.

## Key Results
- Larger multilingual models (XLM-R large) show significantly less faithful feature attributions compared to their monolingual counterparts
- Multilingual tokenizers split words into subwords more aggressively than monolingual tokenizers
- Models with similar tokenizers (same tokenizer or similar splitting aggressiveness) produce more similar faithfulness scores
- Faithfulness disparity patterns vary across different feature attribution methods and tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger multilingual models produce less faithful feature attributions than their monolingual counterparts
- Mechanism: As model size increases, internal reasoning complexity grows, making it harder for feature attribution methods to accurately capture the true decision process
- Core assumption: Feature attribution methods assume simpler, more linear relationships between input features and predictions
- Evidence anchors: [abstract], [section 5.1]
- Break condition: If feature attribution methods become more sophisticated and handle complex reasoning patterns

### Mechanism 2
- Claim: Faithfulness disparity is driven by differences in tokenization aggressiveness
- Mechanism: Multilingual tokenizers split words into subwords more aggressively, leading to different feature importance distributions and lower faithfulness scores
- Core assumption: Tokenization aggressiveness directly impacts feature attribution quality
- Evidence anchors: [abstract], [section 5.2]
- Break condition: If multilingual tokenizers become less aggressive or FA methods become robust to tokenization differences

### Mechanism 3
- Claim: Feature attribution methods are more faithful on models with similar tokenizers
- Mechanism: Similar tokenizers lead to similar processing points after fine-tuning, resulting in more faithful feature attributions
- Core assumption: Models with similar tokenizers process inputs in more similar ways
- Evidence anchors: [section 5.3]
- Break condition: If models with similar tokenizers still process inputs differently due to other architectural differences

## Foundational Learning

- **Feature attribution methods**: Understanding different FA techniques (Attention, Scaled Attention, InputXGrad, Integrated Gradients, DeepLIFT) is crucial for interpreting results
  - Quick check: What are key differences between Integrated Gradients and DeepLIFT in computing feature importance?

- **Faithfulness metrics**: The paper uses Hard Sufficiency & Comprehensiveness and Soft Sufficiency & Comprehensiveness to evaluate faithfulness
  - Quick check: How do Hard Sufficiency and Soft Sufficiency differ in perturbing input during evaluation?

- **Tokenization and subword splitting**: Understanding how tokenizers split words into subwords is crucial for interpreting the mechanism
  - Quick check: What is the difference between WordPiece, BPE, and SentencePiece tokenization in splitting words?

## Architecture Onboarding

- **Component map**: Monolingual/multilingual models (BERT, RoBERTa variants) -> Feature attribution methods (5 techniques) -> Faithfulness metrics (Hard/Soft) -> Datasets (5 tasks) -> Tokenization analysis tools

- **Critical path**: 1) Fine-tune models on datasets, 2) Extract feature attributions, 3) Evaluate faithfulness using Hard/Soft metrics, 4) Analyze model size and tokenization impact, 5) Conduct qualitative rationale analysis

- **Design tradeoffs**: Using similar architectures for fair comparison, choosing diverse languages and tasks, balancing model size vs faithfulness

- **Failure signatures**: Inconsistency in faithfulness disparity across languages/tasks, insignificant impact of tokenization aggressiveness, FA methods not converging on similar tokenizers

- **First 3 experiments**: 1) Compare predictive performance of mono/multilingual models, 2) Evaluate faithfulness of FA methods for each language/model/FA combination, 3) Analyze impact of model size on faithfulness

## Open Questions the Paper Calls Out

- **Open Question 1**: Does faithfulness disparity persist with more diverse languages, including low-resource languages?
  - Basis: Study only included five widely spoken languages, acknowledging that results may not apply to languages outside Indo-European and Sino-Tibetan families
  - Why unresolved: Limited language coverage in current study
  - Evidence needed: Experiments with wider language range including low-resource languages

- **Open Question 2**: How does faithfulness change with models using different pre-training objectives or architectures (decoder-based models like Llama, Mistral)?
  - Basis: Current study only used encoder-based models, acknowledging decoder-based multilingual models would be intriguing to explore
  - Why unresolved: Study limited to BERT/RoBERTa architectures
  - Evidence needed: Experiments comparing faithfulness between mono/multilingual decoder-based models

- **Open Question 3**: Is there a relationship between faithfulness and specific tasks beyond those explored?
  - Basis: Study included five tasks but didn't systematically analyze task complexity impact on faithfulness disparity
  - Why unresolved: Limited task diversity and no systematic analysis of task-complexity relationship
  - Evidence needed: Experiments with broader task range including more complex tasks

## Limitations
- Tokenization mechanism evidence is indirect - correlation shown but causal link not experimentally validated
- FA method implementation variability makes it difficult to isolate model characteristics vs FA artifacts
- Limited language coverage (5 languages) may not capture full spectrum of multilingual modeling challenges

## Confidence

- **High confidence**: FA faithfulness varies systematically between multilingual and monolingual models (well-supported by experimental data)
- **Medium confidence**: Larger multilingual models produce less faithful attributions (supported but could benefit from more size comparisons)
- **Low confidence**: Tokenization aggressiveness mechanism (lacks direct experimental validation, remains plausible hypothesis)

## Next Checks

1. **Direct tokenization intervention**: Modify tokenization aggressiveness in multilingual models and measure resulting change in FA faithfulness to provide direct evidence for/against tokenization hypothesis

2. **Cross-linguistic generalization**: Test faithfulness patterns on additional language pairs, particularly including languages with different tokenization requirements (agglutinative languages like Turkish, morphologically rich languages like Arabic)

3. **FA method robustness analysis**: Systematically compare how different FA methods respond to same tokenization differences, isolating whether faithfulness disparities are method-specific or general phenomena