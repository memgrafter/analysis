---
ver: rpa2
title: 'ICPL: Few-shot In-context Preference Learning via LLMs'
arxiv_id: '2410.17233'
source_url: https://arxiv.org/abs/2410.17233
tags:
- reward
- torch
- tensor
- icpl
- penalty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "ICPL achieves a 30\xD7 reduction in human queries compared to\
  \ baseline preference-based RL methods while achieving comparable or better performance.\
  \ In synthetic experiments with proxy human preferences, ICPL requires only 49 queries\
  \ versus 15,000+ for baselines to reach similar task scores."
---

# ICPL: Few-shot In-context Preference Learning via LLMs

## Quick Facts
- arXiv ID: 2410.17233
- Source URL: https://arxiv.org/abs/2410.17233
- Reference count: 40
- Primary result: 30× reduction in human queries compared to baseline preference-based RL methods while achieving comparable or better performance

## Executive Summary
ICPL introduces a novel approach to few-shot in-context preference learning that leverages large language models to generate and iteratively refine reward functions for reinforcement learning tasks. By using human preferences to guide LLM-generated rewards, ICPL achieves dramatic query efficiency improvements over traditional preference-based RL methods while maintaining or exceeding their performance. The method demonstrates effectiveness across multiple IsaacGym tasks, requiring only 49 human queries versus 15,000+ for baselines to reach similar performance levels.

## Method Summary
ICPL operates by using an LLM (GPT-4 or GPT-4o) to generate executable reward functions based on task descriptions and environment context. In each of N iterations, K reward functions are generated, RL agents are trained with these rewards, and human evaluators select the best and worst performing videos. The LLM then refines future reward generations using this feedback along with automatic feedback including reward traces and differences between historical reward functions. This iterative process achieves dramatic query efficiency by requiring only dozens of human preferences versus thousands needed by traditional preference-based RL methods.

## Key Results
- Achieves 30× reduction in human queries (49 vs 15,000+) while maintaining comparable or better performance
- Matches performance of reward-generation methods using ground-truth sparse rewards
- Demonstrates monotonic improvement across iterations, indicating genuine preference learning rather than random search

## Why This Works (Mechanism)

### Mechanism 1
ICPL leverages LLM in-context learning to iteratively refine reward functions based on human preference feedback. The LLM receives task description, environment context, and feedback from previous iterations (good/bad reward functions and their performance) to generate new reward functions that better align with human preferences. Core assumption: LLMs can perform complex functional transformations on data stored in their context.

### Mechanism 2
ICPL achieves significant query efficiency by using human preferences to guide LLM reward generation rather than learning reward models through extensive pairwise comparisons. Each iteration generates K reward functions, humans select best/worst, and the LLM uses this preference signal to generate improved rewards in the next iteration, reducing total queries from thousands to dozens. Core assumption: Human preferences can effectively guide reward function evolution without requiring large amounts of pairwise comparison data.

### Mechanism 3
ICPL's improvement over iterations indicates genuine preference learning rather than random search or memorization. The monotonic improvement in reward quality across iterations, as shown by increasing task scores, demonstrates that the LLM is learning to generate better rewards that align with human preferences. Core assumption: Improvement over iterations is evidence of learning rather than chance or memorization of optimal rewards.

## Foundational Learning

- **In-context learning (ICL)**: Why needed here - ICPL fundamentally relies on LLMs' ability to modify behavior based on examples provided in their context. Quick check: Can an LLM generate a reward function that solves a simple task when given the task description and environment context?

- **Preference-based reinforcement learning (PbRL)**: Why needed here - ICPL is built on the PbRL framework but aims to make it more sample-efficient by using LLMs. Quick check: What is the difference between learning a reward model from pairwise comparisons versus generating reward functions directly?

- **Reward function optimization**: Why needed here - Understanding how reward functions can be iteratively improved through feedback is crucial for grasping ICPL's mechanism. Quick check: How can you tell if a reward function is improving over time in terms of aligning with human preferences?

## Architecture Onboarding

- **Component map**: LLM reward generator (GPT-4/GPT-4o) -> Environment simulator -> Human feedback interface -> RL training pipeline (PPO) -> Feedback processing module

- **Critical path**: LLM → Reward generation → RL training → Video rendering → Human selection → Feedback → LLM context update → Next iteration

- **Design tradeoffs**:
  - LLM choice (GPT-4 vs GPT-4o) vs performance
  - Number of reward functions per iteration (K=6) vs diversity vs computational cost
  - Number of iterations (N=5) vs convergence vs human effort
  - Reward function complexity vs executability vs expressiveness

- **Failure signatures**:
  - LLM generates non-executable reward functions consistently
  - No improvement in task scores across iterations
  - Human preferences become inconsistent or uninformative
  - RL training fails to produce meaningful policies from generated rewards

- **First 3 experiments**:
  1. Zero-shot test: Run LLM with task description only, generate K rewards, evaluate without human feedback
  2. Single iteration test: Generate K rewards, get human feedback once, generate new rewards, compare improvement
  3. Ablation test: Remove reward trace component, run full N iterations, compare to full ICPL performance

## Open Questions the Paper Calls Out
None

## Limitations
- Relies heavily on LLMs' ability to perform complex functional transformations, which may not generalize across all task domains
- Assumes human preferences are sufficiently informative and consistent to guide reward evolution effectively
- Cannot completely rule out alternative explanations for monotonic improvement (like memorization) without additional validation

## Confidence

- **High confidence**: The empirical results showing 30× reduction in human queries are well-supported by the synthetic experiments with proxy preferences
- **Medium confidence**: The claim about ICPL matching ground-truth sparse reward performance, as this depends on the quality of the proxy preference setup
- **Medium confidence**: The assertion that monotonic improvement demonstrates genuine preference learning, as alternative explanations (like LLM memorization) cannot be completely ruled out

## Next Checks

1. **Robustness test**: Run ICPL on a diverse set of tasks with varying complexity and reward structure to verify the 30× query reduction advantage holds across domains

2. **Noise sensitivity analysis**: Introduce controlled levels of noise into human preferences to test how robust ICPL is to inconsistent or ambiguous feedback

3. **Learning mechanism validation**: Conduct ablation studies comparing ICPL's performance to random search and memorization baselines to more definitively establish that preference learning is occurring rather than chance improvements