---
ver: rpa2
title: Deep Learning-Based Operators for Evolutionary Algorithms
arxiv_id: '2407.10477'
source_url: https://arxiv.org/abs/2407.10477
tags:
- mutation
- crossover
- operator
- operators
- fitness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces two deep learning-based genetic operators:
  Deep Neural Crossover (DNC) for genetic algorithms and BERT mutation for genetic
  programming. DNC uses reinforcement learning and an encoder-decoder architecture
  to select offspring genes, while BERT mutation masks and replaces tree nodes to
  improve fitness.'
---

# Deep Learning-Based Operators for Evolutionary Algorithms

## Quick Facts
- arXiv ID: 2407.10477
- Source URL: https://arxiv.org/abs/2407.10477
- Authors: Eliad Shem-Tov; Moshe Sipper; Achiya Elyasaf
- Reference count: 35
- Primary result: Deep learning-based genetic operators (DNC for GA, BERT mutation for GP) outperform traditional methods in graph coloring, bin packing, and symbolic regression tasks.

## Executive Summary
This paper introduces two novel deep learning-based genetic operators that leverage reinforcement learning to improve evolutionary algorithm performance. Deep Neural Crossover (DNC) uses an encoder-decoder architecture with pointer networks to learn gene correlations for better offspring selection, while BERT mutation applies Masked Language Modeling principles to GP tree nodes for context-aware replacements. Both operators employ online learning, using fitness values from the evolutionary process as training signals without requiring additional fitness evaluations. The approach demonstrates significant improvements over baseline methods across multiple domains including graph coloring, bin packing, and symbolic regression.

## Method Summary
The paper presents DNC for genetic algorithms and BERT mutation for genetic programming, both utilizing reinforcement learning with policy gradients. DNC employs a sequence-to-sequence architecture with RNN encoder-decoder and pointer network to select genes based on learned probability distributions from parent genomes. BERT mutation masks and replaces GP tree nodes using a Masked Language Modeling approach, trained with cached fitness values from the evolutionary process. A transfer-learning approach reduces computational costs by pre-training on similar problems. Both operators are evaluated on benchmark problems including DIMACS graph coloring, Schoenfield Hard28 bin packing, and various symbolic regression datasets.

## Key Results
- DNC achieves near-optimal solutions in graph coloring and bin packing problems
- BERT mutation improves symbolic regression results with better RMSE performance
- Both operators demonstrate faster convergence compared to baseline methods
- Online learning approach effectively uses evolutionary fitness signals without additional evaluations
- Transfer learning approach successfully reduces computational overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DNC learns correlations between genes rather than assuming random selection.
- Mechanism: Uses sequence-to-sequence architecture with RNN encoder-decoder and pointer network to select genes sequentially based on learned probability distributions from parent genomes.
- Core assumption: Gene correlations (linear or nonlinear) exist and can be learned to improve offspring fitness.
- Evidence anchors:
  - [abstract] "leverages the capabilities of deep reinforcement learning and an encoder-decoder architecture to select offspring genes"
  - [section] "DNC tries to assign a higher probability to offspring with a high fitness score"
  - [corpus] Weak evidence - no direct corpus papers discuss learned gene correlations in crossover
- Break condition: If gene correlations are too weak or random to be learned effectively, DNC performance degrades to baseline.

### Mechanism 2
- Claim: BERT mutation operator learns to predict beneficial mutations by masking and replacing tree nodes.
- Mechanism: Uses Masked Language Modeling (MLM) approach with reinforcement learning to select node replacements that improve fitness based on context.
- Core assumption: Masked nodes can be predicted effectively using context from surrounding nodes to improve fitness.
- Evidence anchors:
  - [abstract] "BERT mutation masks multiple gp-tree nodes and then tries to replace these masks with nodes that will most likely improve the individual's fitness"
  - [section] "inspired by the training process of BERT, we propose applying MLM to the string representation of GP trees"
  - [corpus] Weak evidence - corpus papers don't directly discuss MLM for GP mutation
- Break condition: If masked context doesn't provide sufficient information for accurate predictions, mutation quality degrades.

### Mechanism 3
- Claim: Online learning approach uses fitness values from evolutionary process to train operators without additional evaluations.
- Mechanism: During evolution, fitness values are cached and used to train the deep learning models incrementally.
- Core assumption: Evolutionary fitness signals provide sufficient training data for operator improvement.
- Evidence anchors:
  - [abstract] "The operators leverage online learning, using fitness values calculated during evolution to train the models"
  - [section] "Training does not necessitate additional fitness evaluations since the fitness values calculated during the evolutionary process are used for training the operator"
  - [corpus] Weak evidence - corpus papers don't discuss online learning for evolutionary operators
- Break condition: If evolutionary process doesn't provide diverse enough fitness data, learning becomes ineffective.

## Foundational Learning

- Concept: Reinforcement Learning fundamentals
  - Why needed here: Both DNC and BERT mutation use policy-based reinforcement learning to optimize gene selection and mutation replacement
  - Quick check question: What is the difference between value-based and policy-based reinforcement learning approaches?

- Concept: Encoder-decoder architectures and attention mechanisms
  - Why needed here: DNC uses encoder-decoder RNN architecture with pointer network for gene selection
  - Quick check question: How does a pointer network differ from standard sequence-to-sequence models?

- Concept: Masked Language Modeling (MLM) in NLP
  - Why needed here: BERT mutation is inspired by MLM approach used in transformer models like BERT
  - Quick check question: What is the primary purpose of masking tokens during MLM training?

## Architecture Onboarding

- Component map:
  - DNC: Parent genome input → RNN encoder → Embedded representation → RNN decoder with pointer network → Gene selection distribution → Offspring generation
  - BERT mutation: GP tree string → Masking → BERT model with reinforcement learning → Replacement distribution → Node replacement (