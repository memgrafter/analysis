---
ver: rpa2
title: 'CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay'
arxiv_id: '2402.04858'
source_url: https://arxiv.org/abs/2402.04858
tags:
- codeit
- tasks
- programs
- program
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenging Abstraction and Reasoning Corpus
  (ARC) benchmark using a novel neuro-symbolic self-improvement method called CodeIt.
  The approach treats ARC as a programming-by-examples problem and combines program
  sampling with hindsight relabeling and prioritized experience replay to address
  the extreme sparsity of rewards in program synthesis.
---

# CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay

## Quick Facts
- arXiv ID: 2402.04858
- Source URL: https://arxiv.org/abs/2402.04858
- Reference count: 25
- Primary result: Achieves 15% success rate on ARC evaluation tasks, state-of-the-art performance

## Executive Summary
This paper introduces CodeIt, a neuro-symbolic self-improvement method that achieves state-of-the-art performance on the challenging ARC benchmark by treating program synthesis as a learning problem with hindsight relabeling and prioritized experience replay. The approach combines a pre-trained language model with a domain-specific language to generate programs, then iteratively improves the model by learning from both successful solutions and valid but unsuccessful program executions. CodeIt addresses the extreme sparsity of rewards in program synthesis by relabeling hindsight experiences and prioritizing training on experiences that solve real tasks, demonstrating that scalable self-improving components can learn to reason in program space and generalize between ARC tasks.

## Method Summary
CodeIt treats ARC as a programming-by-examples problem using a neuro-symbolic approach that combines program sampling with hindsight relabeling and prioritized experience replay. The method uses a pre-trained CodeT5+ language model to sample programs from a domain-specific language (DSL), executes them on ARC tasks, and relabels the target output with the realized output from each valid program execution. These experiences are stored in a replay buffer and sampled with priority based on solution quality to train the policy iteratively. The self-improvement loop involves sampling programs, executing them on the interpreter, relabeling hindsight experiences, and training the model to generate better programs over multiple meta-iterations.

## Key Results
- Solves 15% of ARC evaluation tasks, achieving state-of-the-art performance
- Outperforms both neural and symbolic baselines on ARC benchmark
- Ablation studies show hindsight relabeling, prioritized sampling, and pretraining are critical components
- Finds shorter programs on average compared to mutation baselines, suggesting discovery of more efficient solutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CodeIt mitigates sparse rewards in ARC program synthesis by relabeling hindsight experiences.
- Mechanism: After generating a program that may not solve the task, the realized output replaces the target output in the buffer, ensuring every syntactically valid program contributes learning signal.
- Core assumption: Even incorrect programs produce valid intermediate outputs that are informative for training.
- Evidence anchors:
  - [abstract] "By relabeling the goal of an episode (i.e., the target program output given input) to the realized output produced by the sampled program, our method effectively deals with the extreme sparsity of rewards in program synthesis."
  - [section 2.2] "Although these programs may not solve the tasks we are interested in, they are always valid in terms of syntax and semantics... They can therefore be used to teach the policy about program syntax and program behaviour, which may lead to positive transfer to the search set."
- Break condition: If the program interpreter fails or the runtime exceeds the threshold, the sample is discarded and provides no learning signal.

### Mechanism 2
- Claim: Prioritized sampling from the replay buffer prevents catastrophic forgetting and accelerates learning.
- Mechanism: Buffer experiences are sampled with priority proportional to the percentage of demonstration outputs matched by the program, so correct or nearly correct solutions are replayed more often.
- Core assumption: Experiences that solve real tasks are more valuable than those that only relabel hindsight outputs.
- Evidence anchors:
  - [abstract] "By prioritizing training on experiences that solve real tasks, we ameliorate the risk of catastrophic forgetting."
  - [section 3.3] "A3 leads to a small reduction in cumulative performance, but a large reduction in policy performance, indicating that the policy indeed forgets important experiences."
- Break condition: If priorities are assigned incorrectly or uniformly, the policy may relearn irrelevant patterns and lose performance on solved tasks.

### Mechanism 3
- Claim: Combining pre-trained language models with program synthesis improves sample efficiency.
- Mechanism: The CodeT5+ model is pre-trained on diverse programming tasks, giving it prior knowledge of syntax and semantics that speeds up learning on ARC.
- Core assumption: Pre-training captures generalizable programming patterns that transfer to ARC DSL primitives.
- Evidence anchors:
  - [section 2.1] "We use the 220 million parameter CodeT5+ (Wang et al., 2023b) model and its default tokenizer, which are pretrained on a diverse set of programming tasks."
  - [section 3.3] "A4: No pretraining... Policy performance shows that performance improvement is much slower."
- Break condition: If the pre-training distribution is too far from ARC DSL usage, transfer may be minimal and the benefit disappears.

## Foundational Learning

- Concept: Experience replay and prioritization in reinforcement learning.
  - Why needed here: ARC program synthesis has extremely sparse rewards; replay allows reuse of any valid program output as training data.
  - Quick check question: What happens to learning if we sample uniformly from the buffer instead of prioritizing?

- Concept: Hindsight relabeling for sparse reward problems.
  - Why needed here: Without relabeling, most sampled programs would yield zero reward and provide no gradient; relabeling turns every syntactically valid program into a learning opportunity.
  - Quick check question: Why is it safe to relabel target outputs with realized outputs in ARC?

- Concept: Domain-specific language design for program synthesis.
  - Why needed here: ARC tasks require symbolic reasoning; a well-chosen DSL encodes prior knowledge about spatial, object, and action abstractions.
  - Quick check question: How does the choice of DSL primitives affect the space of solvable ARC tasks?

## Architecture Onboarding

- Component map: CodeT5+ encoder-decoder policy → program sampling → ARC grid interpreter → hindsight relabeling → replay buffer → prioritized sampling → policy training
- Critical path: Sample → execute → relabel → store → prioritize → train
- Design tradeoffs: Using a smaller LLM with DSL vs. a larger LLM with direct grid prediction; prioritizing correctness vs. diversity in sampling
- Failure signatures: Stalled performance after early meta-iterations suggests relabeling is not providing useful diversity; catastrophic forgetting indicates prioritization is too weak
- First 3 experiments:
  1. Run CodeIt with uniform sampling from buffer; compare to prioritized baseline
  2. Disable hindsight relabeling; measure how many sampled programs provide learning signal
  3. Swap DSL for a simpler set of primitives; observe effect on solution rate and program length

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would CodeIt perform if trained without any prior knowledge from pre-trained models or human-designed DSLs?
- Basis in paper: [inferred] The paper shows that ablation A4 (no pretraining) results in significantly slower learning, but doesn't explore training from scratch with no DSL or prior knowledge.
- Why unresolved: The authors only ablate pretraining while keeping the DSL, making it impossible to isolate the effect of starting from scratch without any human-designed components.
- What evidence would resolve it: A complete baseline showing CodeIt performance starting from random weights and discovering its own DSL or program representations from scratch.

### Open Question 2
- Question: Can CodeIt's self-improvement mechanism discover novel abstractions or program structures not present in the original DSL?
- Basis in paper: [explicit] The authors note that CodeIt finds "shorter programs on average and uses different primitives" compared to mutation baselines, suggesting it may be discovering new approaches.
- Why unresolved: While the paper observes differences in program structure, it doesn't analyze whether CodeIt has discovered genuinely novel abstractions beyond the provided DSL.
- What evidence would resolve it: A systematic analysis of whether CodeIt's discovered programs contain emergent patterns or abstractions that extend beyond the capabilities of the original DSL.

### Open Question 3
- Question: What is the relationship between program length, task complexity, and generalization ability in CodeIt?
- Basis in paper: [inferred] The paper observes that CodeIt refines solutions over time, often finding shorter programs, but doesn't systematically analyze how program length correlates with generalization or task difficulty.
- Why unresolved: The authors report finding shorter programs in later iterations but don't explore whether shorter programs generalize better or if there's an optimal program length for different task types.
- What evidence would resolve it: A detailed analysis correlating program length with test performance, generalization across similar tasks, and efficiency metrics across different ARC task categories.

## Limitations

- The 15% success rate, while state-of-the-art, still leaves significant room for improvement on this challenging benchmark
- Evaluation is limited to ARC's public evaluation set without assessment of generalization to out-of-distribution tasks
- The ablation studies don't fully isolate all contributing factors, particularly the interaction between hindsight relabeling and prioritized sampling

## Confidence

- **High confidence**: The core mechanism of hindsight relabeling transforming sparse rewards into dense learning signals is well-supported by both theoretical arguments and empirical results. The improvement over uniform sampling in ablation studies is substantial and consistent.
- **Medium confidence**: The claim that pre-training on diverse programming tasks meaningfully accelerates ARC learning is supported by the ablation study, but the comparison is indirect (no training from scratch baseline on the exact same data). The transfer benefit could be overestimated if the pre-training distribution happens to align unusually well with ARC's DSL.
- **Medium confidence**: The prioritization mechanism preventing catastrophic forgetting is demonstrated through performance metrics, but the analysis doesn't examine whether the policy actually forgets specific task solutions or just fails to improve on unsolved tasks. The distinction matters for understanding the mechanism.

## Next Checks

1. **Synergy validation**: Run an ablation where hindsight relabeling is enabled but sampling is uniform, and compare to the prioritized version to quantify the interaction effect between these two mechanisms.

2. **Distributional robustness**: Test CodeIt on ARC-like tasks with modified DSL primitives or task distributions to assess whether the learned programs generalize beyond the specific ARC training set.

3. **Forgetting analysis**: Track individual task success rates across meta-iterations to determine whether the policy truly forgets solved tasks or whether performance plateaus represent exploration challenges rather than memory loss.