---
ver: rpa2
title: Evaluating Tokenizer Performance of Large Language Models Across Official Indian
  Languages
arxiv_id: '2411.12240'
source_url: https://arxiv.org/abs/2411.12240
tags:
- languages
- tokenizer
- example
- text
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates tokenizer performance across 12 large language
  models (LLMs) on all 22 official Indian languages using Normalized Sequence Length
  (NSL) as the primary metric. The study compares proprietary models like GPT-4o and
  GPT-4 with open-weights models including SUTRA, Llama 3.1, and various Indic-specific
  models.
---

# Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages

## Quick Facts
- **arXiv ID:** 2411.12240
- **Source URL:** https://arxiv.org/abs/2411.12240
- **Reference count:** 40
- **Primary result:** SUTRA tokenizer achieved the best performance, outperforming all other models including GPT-4o in 14 out of 22 Indian languages

## Executive Summary
This paper presents a comprehensive evaluation of tokenizer performance across 12 large language models on all 22 official Indian languages. Using Normalized Sequence Length (NSL) as the primary metric, the study reveals significant performance differences between models, with SUTRA tokenizer achieving superior results across 14 languages. The research highlights the critical importance of specialized tokenization strategies for Indic languages, which feature complex scripts and diverse linguistic structures. The findings demonstrate that GPT-4o shows marked improvement over GPT-4 in processing Indian languages, while models like Project Indus struggle with non-Devanagari scripts due to training data limitations.

## Method Summary
The evaluation involved sending example texts from all 22 official Indian languages through the tokenizers of 12 different LLMs, including proprietary models (GPT-4o, GPT-4) and open-weights models (SUTRA, Llama 3.1, and various Indic-specific models). Token counts were recorded and compared using the Normalized Sequence Length metric, which measures the ratio of encoded sequence length between the evaluated tokenizer and a baseline tokenizer. The analysis produced language-specific rankings and identified patterns in tokenizer performance across different script families and linguistic structures.

## Key Results
- SUTRA tokenizer achieved the best performance, outperforming all other models including GPT-4o in 14 out of 22 languages
- GPT-4o showed significant improvement over GPT-4 in processing Indian languages, securing top rankings in multiple languages
- Project Indus tokenizer struggled with languages using non-Devanagari scripts, performing well only on Hindi, Marathi, and other Devanagari-based languages
- Models with Mixture of Experts (MoE) architecture demonstrated superior handling of morphologically rich Indic scripts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SUTRA tokenizer achieves superior performance because it uses a Mixture of Experts (MoE) architecture that decouples conceptual understanding from language-specific processing, allowing more efficient handling of morphologically rich Indic scripts.
- Mechanism: The MoE framework allows the tokenizer to route tokens through specialized expert modules for different language families or script types, reducing fragmentation of words into multiple tokens.
- Core assumption: The SUTRA tokenizer's MoE architecture is specifically trained on a diverse corpus of Indic languages, enabling it to recognize and preserve linguistic units that other tokenizers split incorrectly.
- Evidence anchors:
  - [abstract] "SUTRA (Scalable Multilingual Language Model Architecture), introduced by A. Bendale et al. SUTRA supports over 50 languages, including Indic ones, by decoupling conceptual understanding from language-specific processing for scalable multilingual learning."
  - [section] "The findings reveal that the SUTRA tokenizer outperforms all other models, including several Indic-specific models, excelling in 14 languages."
  - [corpus] Weak - no direct evidence about MoE architecture in corpus neighbors.
- Break condition: If the tokenizer is evaluated on languages outside its training distribution, the MoE routing may fail to select appropriate experts, leading to performance degradation.

### Mechanism 2
- Claim: GPT-4o shows significant improvement over GPT-4 in processing Indian languages due to architectural changes that better handle multilingual tokenization.
- Mechanism: The newer tokenizer in GPT-4o likely incorporates language-specific optimizations or a more diverse training corpus that reduces tokenization fragmentation for Indic scripts.
- Core assumption: The improvement is due to changes in the tokenizer itself rather than just model architecture changes.
- Evidence anchors:
  - [abstract] "GPT-4o's advancement over its predecessor GPT-4 in processing Indian languages"
  - [section] "GPT-4, the predecessor of GPT-4o, did not manage to secure the best tokenizer value in any of the 22 languages, a stark contrast to GPT-4o."
  - [corpus] Weak - corpus neighbors don't discuss GPT-4 vs GPT-4o differences.
- Break condition: If the improvement is due to post-tokenization processing rather than tokenization quality, the NSL metric may not accurately capture the difference.

### Mechanism 3
- Claim: Project Indus tokenizer performs poorly on non-Devanagari scripts because it was trained primarily on Hindi and English data, limiting its ability to handle other Indic scripts.
- Mechanism: The tokenizer's vocabulary and training data are biased toward Devanagari script patterns, causing excessive fragmentation when processing languages with different scripts.
- Core assumption: The tokenizer was indeed trained on a limited set of languages with specific scripts, and this training bias directly impacts performance.
- Evidence anchors:
  - [section] "Project Indus struggled with languages using non-Devanagari scripts" and "Project Indus' tokenizer seems to be getting the same for only a few languages like (1) Bodo, (2) Dogri, (3) Hindi, (4) Konkani, (5) Maithili, (6) Marathi, (7) Nepali, and (8) Sanskrit. This is probably because all these 8 languages follow the same Devanagari script."
  - [corpus] Weak - no direct evidence about Project Indus training data in corpus neighbors.
- Break condition: If the tokenizer has script-agnostic components that can generalize from training data, the performance gap might be smaller than observed.

## Foundational Learning

- Concept: Normalized Sequence Length (NSL) metric
  - Why needed here: NSL is the primary evaluation metric used to compare tokenizer efficiency across different languages and models
  - Quick check question: How is NSL calculated and what does a lower value indicate about tokenizer performance?

- Concept: Byte Pair Encoding (BPE) and WordPiece tokenization algorithms
  - Why needed here: Understanding these tokenization approaches is essential for interpreting why some models perform better than others on Indic languages
  - Quick check question: What are the key differences between BPE and WordPiece, and how might these affect tokenization of morphologically rich languages?

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: SUTRA's superior performance is attributed to its MoE design, so understanding this architecture is crucial for the evaluation
  - Quick check question: How does MoE architecture enable more efficient multilingual tokenization compared to standard transformer architectures?

## Architecture Onboarding

- Component map: Example texts → 12 tokenizers → Token counts → NSL calculation → Comparative analysis
- Critical path: The core evaluation loop is text → tokenizer → token count → NSL calculation → comparison across models
- Design tradeoffs: Using NSL as the primary metric provides a standardized comparison but may not capture all aspects of tokenizer quality, such as semantic preservation or out-of-vocabulary handling
- Failure signatures: High NSL values indicate excessive tokenization fragmentation, while inconsistent performance across languages suggests script-specific biases in the tokenizer training
- First 3 experiments:
  1. Verify NSL calculation by manually computing token counts for a sample text across multiple tokenizers to ensure consistency with reported values
  2. Test tokenizer performance on synthetic Indic language text with known morphological structures to validate the NSL metric's sensitivity to tokenization quality
  3. Compare tokenizer outputs for the same text across different script representations (e.g., transliterated vs native script) to identify script-specific biases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different tokenizer architectures (WordPiece vs. Byte Pair Encoding) specifically impact performance on Indic scripts with complex conjunct characters?
- Basis in paper: [explicit] The paper mentions that "Most of the LLMs use either of two types of tokenization algorithms, namely WordPiece and Byte Pair Encoding (BPE)" but does not compare their relative performance on Indic languages
- Why unresolved: The study evaluates tokenizer performance but does not analyze which underlying algorithm performs better for specific linguistic features of Indic languages
- What evidence would resolve it: A comparative study of WordPiece vs BPE tokenizers across Indic languages, measuring token count, processing speed, and out-of-vocabulary handling

### Open Question 2
- Question: What specific linguistic features of Indic languages cause tokenizers like Project Indus to perform poorly on non-Devanagari scripts?
- Basis in paper: [explicit] "Project Indus' tokenizer seems to be struggling, getting an average NSL score of above 1 (the higher the worse)" for languages outside the Devanagari script family
- Why unresolved: The paper identifies the performance gap but does not analyze the underlying linguistic reasons for this discrepancy
- What evidence would resolve it: Detailed linguistic analysis of tokenization failures on Dravidian and other non-Devanagari scripts, identifying specific problematic character combinations or script features

### Open Question 3
- Question: How does tokenizer efficiency (as measured by NSL) correlate with downstream task performance for Indic languages?
- Basis in paper: [inferred] The paper emphasizes tokenizer importance for model performance but does not establish a direct link between NSL metrics and actual task performance
- Why unresolved: While the study establishes that SUTRA has the best tokenizer performance, it does not demonstrate how this translates to better language understanding or generation capabilities
- What evidence would resolve it: Correlation studies between tokenizer NSL scores and benchmark task performance (translation, summarization, QA) across the 22 languages

## Limitations

- The evaluation relies heavily on the Normalized Sequence Length (NSL) metric, which may not fully capture semantic and syntactic preservation capabilities of different tokenizers
- Access to proprietary models like GPT-4o and GPT-4 introduces potential variability due to API rate limits, version differences, or implementation changes that weren't documented
- The study doesn't provide detailed analysis of why Project Indus performs poorly on non-Devanagari scripts, suggesting training data bias without verification

## Confidence

- **High Confidence:** The observation that SUTRA tokenizer outperforms other models in 14 out of 22 languages is well-supported by the NSL metric data and consistent across multiple evaluation examples
- **Medium Confidence:** The attribution of SUTRA's superior performance to its MoE architecture is plausible but not definitively proven by the available evidence
- **Low Confidence:** The claim about Project Indus's poor performance on non-Devanagari scripts is based on observed results but lacks detailed investigation into the root causes

## Next Checks

1. **NSL Metric Validation:** Manually calculate token counts and NSL values for a diverse sample of texts across all 22 languages using multiple tokenizers to verify the reported performance rankings and ensure the metric captures meaningful differences in tokenization quality.

2. **Cross-Script Consistency Test:** Evaluate tokenizer performance on the same linguistic content represented in different scripts (e.g., transliterated vs native script) to identify whether observed performance differences are due to script-specific limitations or broader tokenization issues.

3. **Semantic Preservation Analysis:** Beyond NSL metrics, analyze whether highly fragmented tokenization (high NSL values) correlates with loss of semantic coherence by attempting to reconstruct and evaluate the meaning of tokenized outputs for languages with the poorest performance.