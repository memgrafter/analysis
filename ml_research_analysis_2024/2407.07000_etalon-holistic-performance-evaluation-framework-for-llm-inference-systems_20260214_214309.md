---
ver: rpa2
title: 'Etalon: Holistic Performance Evaluation Framework for LLM Inference Systems'
arxiv_id: '2407.07000'
source_url: https://arxiv.org/abs/2407.07000
tags:
- token
- latency
- inference
- systems
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating LLM inference
  systems by identifying the shortcomings of existing metrics like TTFT, TBT, TPOT,
  and normalized latency in capturing the full user experience. The authors introduce
  Metron, a holistic evaluation framework featuring the fluidity-index metric, which
  sets token-level deadlines and measures the fraction of deadlines met, thereby capturing
  stalls, jitters, and scheduling delays that traditional metrics miss.
---

# Etalon: Holistic Performance Evaluation Framework for LLM Inference Systems

## Quick Facts
- arXiv ID: 2407.07000
- Source URL: https://arxiv.org/abs/2407.07000
- Authors: Amey Agrawal; Anmol Agarwal; Nitin Kedia; Jayashree Mohan; Souvik Kundu; Nipun Kwatra; Ramachandran Ramjee; Alexey Tumanov
- Reference count: 33
- One-line primary result: Introduces Metron framework with fluidity-index metric that captures token-level deadlines and stalls, providing more accurate evaluation of LLM inference systems compared to traditional metrics like TPOT and TBT.

## Executive Summary
This paper addresses the challenge of evaluating LLM inference systems by identifying the shortcomings of existing metrics like TTFT, TBT, TPOT, and normalized latency in capturing the full user experience. The authors introduce Metron, a holistic evaluation framework featuring the fluidity-index metric, which sets token-level deadlines and measures the fraction of deadlines met, thereby capturing stalls, jitters, and scheduling delays that traditional metrics miss. By evaluating both open-source frameworks and proprietary services using Metron, the authors demonstrate that fluid token generation rate and fluidity-index provide a more accurate and balanced view of system performance compared to conventional metrics. Experiments reveal that metrics like TPOT can overestimate throughput, while fluidity-index uncovers real-world performance differences, such as identifying which systems can sustain higher loads under user experience constraints. The framework enables better capacity planning and system comparison for LLM deployment. Metron is open-sourced to serve as a standard evaluation suite for the LLM inference community.

## Method Summary
The Metron framework provides holistic evaluation of LLM inference systems through black-box testing that tracks token generation timestamps. The core innovation is the fluidity-index metric, which sets token-level deadlines based on desired TTFT and TBT, then measures what fraction of these deadlines are met. This captures stalls and jitters that traditional metrics normalize away. The framework evaluates both open-source frameworks (vLLM, Sarathi-Serve, LightLLM, Text-Generation-Inference) and proprietary APIs (OpenAI, Azure AI Studio, Fireworks AI, Groq) by hitting API endpoints with diverse prompt lengths and recording when each output token is generated. The evaluation includes calculating traditional metrics (TTFT, TBT, TPOT) alongside fluidity-index and determining maximum sustainable throughput under user experience constraints.

## Key Results
- Fluid token generation rate and fluidity-index provide a more accurate and balanced view of system performance compared to conventional metrics like TPOT and tail TBT.
- TPOT can overestimate system throughput by masking stalls and jitters through normalization, while fluidity-index uncovers real performance differences between systems.
- The framework successfully identifies which systems can sustain higher loads under user experience constraints, enabling better capacity planning and system comparison.
- Black-box evaluation capability allows fair comparison of both open-source frameworks and proprietary APIs without requiring access to internal configurations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fluid token generation rate provides a more accurate measure of system throughput under user experience constraints than TPOT or tail TBT.
- Mechanism: Fluid token generation rate measures the maximum token generation rate achievable while maintaining a specified fluidity-index threshold (e.g., 99% of requests with fluidity-index â‰¥ 0.9). This accounts for stalls and jitters that traditional metrics normalize away.
- Core assumption: Users perceive LLM responses as a continuous stream, and stalls/interruptions degrade this experience even if average metrics appear acceptable.
- Evidence anchors:
  - [abstract] "fluid token generation rate enables black-box evaluation of LLM inference systems. Combined, these metrics provide a holistic view of LLM inference performance that more closely aligns with real-world user experience."
  - [section 3.1] "Naively normalizing total decode latency in TPOT, hides these latency spikes and overestimates the system token throughput."
- Break condition: If user experience doesn't actually depend on smooth token generation (e.g., for batch processing), this metric becomes less relevant.

### Mechanism 2
- Claim: fluidity-index captures the true impact of token generation stalls by setting token-level deadlines and measuring deadline misses.
- Mechanism: Instead of treating each token independently, fluidity-index sets deadlines based on desired TTFT and TBT, then measures what fraction of these deadlines are met. When a deadline is missed, subsequent deadlines are reset to account for the autoregressive nature of decoding.
- Core assumption: Users experience delays as a continuous stream, so a single large stall affects all subsequent tokens in a request.
- Evidence anchors:
  - [abstract] "fluidity-index accounts this variability by setting token-level deadlines and measuring the fraction of deadlines met within a request."
  - [section 4.1] "We then define a deadline-miss as an event when the actual token generation time of the ith token exceeds Di. Note that in the event of a deadline-miss, depending on the length of the stall, many subsequent tokens may miss their deadlines."
- Break condition: If token generation strategies change (e.g., speculative decoding generates multiple tokens simultaneously), the simple deadline model may need adjustment.

### Mechanism 3
- Claim: Metron's black-box evaluation capability enables fair comparison of both open-source frameworks and proprietary APIs without requiring access to internal configurations.
- Mechanism: By hitting API endpoints with diverse prompt lengths and tracking token generation timestamps, Metron can calculate all metrics (TTFT, TBT, TPOT, fluidity-index) without knowing system internals. This enables consistent benchmarking across different deployment types.
- Core assumption: The external behavior (token generation timing) is sufficient to characterize system performance regardless of internal architecture.
- Evidence anchors:
  - [abstract] "Metron provides two evaluation recipes as described below: Black-box Evaluation. For an LLM inference API endpoint, Metron performs black-box evaluation by hitting the server with a set of requests with diverse prompt lengths, and tracks checkpoints such as the timestamps when each output token got generated."
  - [section 5.1] "We evaluate three proprietary systems with API-only access: Anyscale [1], Groq [5], and Fireworks [4], across two models..."
- Break condition: If systems implement sophisticated client-side optimizations that mask true generation timing, black-box evaluation may not capture actual performance.

## Foundational Learning

- Concept: Real-time systems scheduling theory
  - Why needed here: The fluidity-index metric draws inspiration from deadline-based evaluation of periodic tasks in real-time systems, treating token generation as periodic tasks with deadlines.
  - Quick check question: How does the concept of "slack" in real-time scheduling apply to token generation, and what happens when slack accumulates or is exhausted?

- Concept: Autoregressive decoding in transformer models
  - Why needed here: Understanding that each token depends on all previous tokens explains why a single generation stall propagates through the entire response.
  - Quick check question: If token i is delayed by 2 seconds, how does this affect the perceived generation time of tokens i+1, i+2, etc., in a typical autoregressive decoder?

- Concept: Statistical performance analysis
  - Why needed here: Interpreting CDFs, percentiles, and distributions is essential for understanding the full impact of metrics like fluidity-index and TBT across many requests.
  - Quick check question: Given two systems with identical median TBT but different tail behaviors, how would fluidity-index help distinguish their real-world performance?

## Architecture Onboarding

- Component map:
  - LLMPerf fork -> Core evaluation engine with API endpoint support
  -> fluidity-index calculator -> Token deadline tracking and miss rate computation
  -> Capacity evaluator -> Load testing to find maximum sustainable QPS
  -> Configuration manager -> System-specific parameters (prefill curves, scheduling slack)

- Critical path:
  1. Generate requests with varying prompt lengths
  2. Record timestamps for each token generation event
  3. Calculate all metrics (TTFT, TBT, TPOT, fluidity-index)
  4. Determine maximum sustainable throughput under SLO constraints

- Design tradeoffs:
  - Granularity vs. overhead: More frequent timestamp recording provides better accuracy but increases evaluation cost
  - Scheduling slack selection: Too small misses legitimate delays, too large masks real performance issues
  - Deadline model complexity: Simple deadlines are easier to implement but may not capture all generation strategies

- Failure signatures:
  - Low fluidity-index despite good TPOT: Indicates intermittent stalls being hidden by normalization
  - High scheduling slack needed: Suggests significant variability in system response times
  - Capacity plateau: May indicate resource bottlenecks or scheduling interference

- First 3 experiments:
  1. Single request with known timing: Verify timestamp accuracy and deadline calculations
  2. Compare two systems with known performance differences: Confirm fluidity-index distinguishes them better than traditional metrics
  3. Load scaling test: Identify how fluidity-index degrades as QPS increases and compare to capacity estimates from other metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can scheduling slack be systematically determined for the fluidity-index metric rather than empirically?
- Basis in paper: [explicit] "We pick this value based on our empirical observations; we leave it to future work to systematically set a scheduling slack."
- Why unresolved: The current implementation uses an empirical, ad-hoc value for scheduling slack, which may not generalize well across different systems or workloads.
- What evidence would resolve it: A method to automatically determine optimal scheduling slack based on system characteristics, workload patterns, or real-time performance metrics.

### Open Question 2
- Question: How can prefill latency targets be accurately determined for proprietary systems that do not expose internal performance data?
- Basis in paper: [explicit] "picking a deadline for a given prompt length is challenging for proprietary systems as we cannot accurately characterize their prefill performance; the observed prefill time can include scheduling delays which may offset the expected trends in prefill processing."
- Why unresolved: Proprietary systems hide internal implementation details, making it difficult to isolate pure prefill processing time from scheduling delays and other confounding factors.
- What evidence would resolve it: A methodology to disentangle scheduling delays from actual prefill processing time in proprietary systems, possibly through controlled experiments or statistical modeling.

### Open Question 3
- Question: How can Metron automatically explore and tune open-source system configuration parameters (e.g., chunk size in Sarathi-Serve, block size in vLLM) to optimize performance?
- Basis in paper: [explicit] "open-source systems have various performance tuning knobs; for e.g., chunk size in Sarathi-Serve, block size in vLLM etc. Metron currently does not explore or auto-tune such parameters; it expects the users to set the configuration parameters while evaluating across two systems."
- Why unresolved: Manual tuning of configuration parameters is time-consuming and requires expert knowledge, limiting the practical usability of Metron for comparative evaluations.
- What evidence would resolve it: An automated parameter exploration and optimization framework that can efficiently search the configuration space and identify optimal settings for different workloads and system configurations.

## Limitations
- Proprietary System Evaluation Constraints: Black-box testing without internal configuration access introduces uncertainty about whether performance differences stem from architecture or configuration.
- Deadline Model Applicability: The simple token-level deadline model may not capture sophisticated generation strategies like speculative or parallel decoding where multiple tokens are generated simultaneously.
- Scheduling Slack Parameter Sensitivity: The fluidity-index calculation depends on empirically-determined scheduling slack that lacks systematic selection methodology across different systems or workloads.

## Confidence
- High Confidence: The core mechanism of fluidity-index as a deadline-based metric (Mechanism 2) is well-founded, drawing from established real-time systems scheduling theory.
- Medium Confidence: The fluid token generation rate as a throughput metric under user experience constraints (Mechanism 1) shows promise but depends heavily on appropriate fluidity-index threshold selection.
- Medium Confidence: The black-box evaluation approach's ability to fairly compare diverse systems (Mechanism 3) is validated through experiments but may miss subtle architectural differences.

## Next Checks
1. Sensitivity Analysis of Scheduling Slack: Systematically vary the scheduling slack parameter across multiple orders of magnitude and evaluate how fluidity-index rankings of different systems change.
2. Cross-Use-Case Validation: Test the framework's metrics (particularly fluidity-index) across different LLM application types (real-time chat, batch processing, code generation) to determine if the same thresholds apply or if use-case-specific calibration is needed.
3. Multi-Token Generation Strategy Evaluation: Implement evaluation of systems using speculative or parallel decoding where multiple tokens are generated simultaneously, assessing whether the current token-level deadline model requires modification.