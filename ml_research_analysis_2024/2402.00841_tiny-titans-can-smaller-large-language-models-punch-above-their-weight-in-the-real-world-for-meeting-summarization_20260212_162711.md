---
ver: rpa2
title: 'Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in
  the Real World for Meeting Summarization?'
arxiv_id: '2402.00841'
source_url: https://arxiv.org/abs/2402.00841
tags:
- llms
- performance
- smaller
- arxiv
- larger
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether smaller, compact LLMs can effectively
  replace larger LLMs for meeting summarization while reducing deployment costs. The
  authors compare fine-tuned compact LLMs (FLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot
  larger LLMs (GPT-3.5, PaLM-2, LLaMA-2) on meeting summarization datasets.
---

# Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?

## Quick Facts
- arXiv ID: 2402.00841
- Source URL: https://arxiv.org/abs/2402.00841
- Authors: Xue-Yong Fu; Md Tahmid Rahman Laskar; Elena Khasanova; Cheng Chen; Shashi Bhushan TN
- Reference count: 8
- Primary result: FLAN-T5-Large (780M parameters) performs on par with or better than many zero-shot larger LLMs (7B-70B+ parameters) for meeting summarization

## Executive Summary
This paper investigates whether smaller, compact LLMs can effectively replace larger LLMs for meeting summarization while reducing deployment costs. The authors compare fine-tuned compact LLMs (FLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (GPT-3.5, PaLM-2, LLaMA-2) on meeting summarization datasets. The key finding is that FLAN-T5 (780M parameters) performs on par with or better than many larger LLMs in zero-shot settings, making compact LLMs a suitable cost-efficient solution for real-world industrial deployment with lower latency and reduced computational resource requirements.

## Method Summary
The study evaluates compact LLMs (FLAN-T5, TinyLLaMA, LiteLLaMA) fine-tuned on meeting summarization datasets against zero-shot larger LLMs (GPT-3.5, PaLM-2, LLaMA-2, Mixtral-8x7B). Two datasets are used: an in-domain dataset of real-world business meeting transcripts with GPT-4-generated summaries, and the QMSUM-I dataset with regenerated summaries for three instruction types. Models are fine-tuned for 10-20 epochs with learning rates ranging from 1e-5 to 2e-5. Performance is evaluated using ROUGE-1, ROUGE-2, and ROUGE-L metrics, plus human evaluation on fluency, coherence, and factual consistency.

## Key Results
- FLAN-T5-Large (780M parameters) performs on par with or better than many zero-shot larger LLMs (7B to 70B+ parameters) on meeting summarization tasks
- FLAN-T5-Large offers significant advantages in inference speed (4.2 seconds per transcript vs. 15 seconds for LLaMA-2-7B) and memory requirements (6GB VRAM vs. multiple high-end GPUs)
- The performance of fine-tuned compact LLMs varies across datasets, with FLAN-T5-Large excelling on shorter transcripts but struggling with longer ones due to its 2048-token context window

## Why This Works (Mechanism)

### Mechanism 1
FLAN-T5-Large achieves competitive performance through instruction fine-tuning that compensates for its smaller parameter count. The fine-tuning process effectively transfers knowledge from pre-training to the specific task of meeting summarization.

Core assumption: Instruction fine-tuning effectively transfers knowledge to the meeting summarization task.
Evidence anchors: [abstract] "FLAN-T5 (780M parameters), which performs on par or even better than many zero-shot Larger LLMs (from 7B to above 70B parameters), while being significantly smaller."

### Mechanism 2
Smaller models like FLAN-T5-Large offer cost-effective deployment due to reduced computational requirements. The model's smaller size allows it to run on less powerful hardware, reducing both initial hardware costs and ongoing operational expenses.

Core assumption: The performance trade-off between model size and hardware requirements is favorable for smaller models.
Evidence anchors: [section] "FLAN-T5-Large only takes 4.2 seconds per transcript, whereas LLaMA-2-7B takes 15 seconds per transcript."

### Mechanism 3
FLAN-T5-Large's performance advantage is particularly strong on datasets with shorter meeting transcripts due to its limited context window (2048 tokens). The model can process most or all of shorter transcripts within its context window without truncation.

Core assumption: Meeting transcript length is a key factor in model performance.
Evidence anchors: [section] "This indicates that in datasets that have smaller context lengths, FLAN-T5-Large could be very useful."

## Foundational Learning

- **Instruction fine-tuning**
  - Why needed here: The paper compares fine-tuned compact LLMs with zero-shot larger LLMs, highlighting the importance of instruction fine-tuning in achieving competitive performance with smaller models.
  - Quick check question: What is the primary difference between traditional fine-tuning and instruction fine-tuning in the context of LLMs?

- **ROUGE metrics for summarization evaluation**
  - Why needed here: The paper uses ROUGE-1, ROUGE-2, and ROUGE-L scores to evaluate and compare the performance of different LLMs on meeting summarization tasks.
  - Quick check question: How do ROUGE-1, ROUGE-2, and ROUGE-L metrics differ in their evaluation of text summarization quality?

- **Context window limitations in transformer models**
  - Why needed here: The paper discusses how FLAN-T5-Large's limited context window (2048 tokens) affects its performance on datasets with varying transcript lengths.
  - Quick check question: What happens when input text exceeds a transformer model's maximum context window length during inference?

## Architecture Onboarding

- **Component map**: ASR-generated transcripts and QMSUM-I dataset -> FLAN-T5-Large (780M), TinyLLaMA (1.1B), LiteLLaMA (460M), zero-shot larger LLMs -> 10-20 epochs fine-tuning with learning rates 1e-5 to 2e-5 -> ROUGE-1, ROUGE-2, ROUGE-L evaluation -> Performance comparison
- **Critical path**: Transcript input → Model inference → Summary generation → ROUGE evaluation → Performance comparison
- **Design tradeoffs**: Model size vs. performance (smaller models offer cost and speed advantages but may struggle with longer transcripts); Fine-tuning data quantity vs. model quality (more data improves performance but increases resource requirements); Context window size vs. computational efficiency (larger windows improve performance but require more resources)
- **Failure signatures**: Poor performance on longer transcripts (exceeding 2048 tokens); Overfitting on small fine-tuning datasets; Degradation in fluency or coherence in generated summaries; Unexpected latency increases during inference
- **First 3 experiments**:
  1. Compare FLAN-T5-Large performance on transcripts of varying lengths (short, medium, long) to identify the optimal use case
  2. Test different fine-tuning dataset sizes to determine the minimum effective training set size
  3. Evaluate the impact of different learning rates and epoch counts on model performance and convergence

## Open Questions the Paper Calls Out

- **How do smaller LLMs perform with chapterization techniques for longer transcripts?**
  - Basis in paper: The paper notes that FLAN-T5-Large underperforms on the QMSUM-I dataset with longer transcripts due to its limited context length of 2048 tokens, and suggests that future work should investigate its performance using chapterization techniques.
  - Why unresolved: The paper does not provide experimental results on applying chapterization techniques to FLAN-T5 or other smaller LLMs for handling longer transcripts.
  - What evidence would resolve it: Comparative experimental results showing the performance of FLAN-T5-Large with and without chapterization techniques on datasets with longer transcripts.

- **How does fine-tuning dataset size affect smaller LLM performance?**
  - Basis in paper: The paper mentions that the performance of fine-tuned models is generally lower on the QMSUM-I dataset, which has a smaller training set, compared to the in-domain dataset. However, it does not explicitly study the effect of dataset size on model performance.
  - Why unresolved: The paper does not conduct experiments varying the size of the fine-tuning dataset to observe its impact on the performance of smaller LLMs.
  - What evidence would resolve it: Experimental results showing the performance of smaller LLMs on meeting summarization tasks as the size of the fine-tuning dataset is varied.

- **How do smaller LLMs perform across a wider variety of instruction types?**
  - Basis in paper: The paper explicitly states that one of its limitations is that only three types of instructions were utilized, and suggests that future work should evaluate LLMs across more instructions.
  - Why unresolved: The paper only evaluates the performance of LLMs on three instruction types and does not explore a wider variety of instructions.
  - What evidence would resolve it: Experimental results showing the performance of smaller LLMs on meeting summarization tasks using a broader set of instruction types.

## Limitations

- The study relies on two specific meeting summarization datasets that may not fully represent the diversity of real-world meeting contexts across different industries and domains.
- FLAN-T5-Large's 2048-token context window represents a significant limitation for longer meeting transcripts, with performance on longer, more complex meetings remaining untested.
- The comparison between zero-shot larger LLMs and fine-tuned compact LLMs introduces a methodological asymmetry that doesn't fully account for the additional resources and expertise required for fine-tuning.

## Confidence

**High Confidence Claims**:
- FLAN-T5-Large performs competitively with larger zero-shot models on the tested meeting summarization datasets.
- Smaller models like FLAN-T5-Large offer significant advantages in terms of inference speed and memory requirements.
- The performance of compact LLMs varies across datasets, with FLAN-T5-Large excelling on shorter transcripts.

**Medium Confidence Claims**:
- FLAN-T5-Large represents a cost-effective solution for industrial deployment of meeting summarization systems.
- The fine-tuning process significantly improves the performance of compact LLMs on task-specific applications.

**Low Confidence Claims**:
- The findings will generalize to all meeting summarization contexts across different industries.
- FLAN-T5-Large will maintain its performance advantages as meeting transcript lengths increase in real-world applications.

## Next Checks

1. **Extended Context Testing**: Evaluate FLAN-T5-Large and other compact models on meeting transcripts that consistently exceed 2048 tokens to quantify performance degradation and identify breaking points for practical deployment.

2. **Cross-Domain Validation**: Test the models on meeting summarization datasets from different industries (medical, legal, technical) to assess generalizability and identify domain-specific limitations that weren't captured in the current study.

3. **Fine-tuning Resource Analysis**: Conduct a detailed cost-benefit analysis of the fine-tuning process, including data preparation, training time, and expertise requirements, to provide a more complete picture of the total cost of ownership for compact LLM deployment.