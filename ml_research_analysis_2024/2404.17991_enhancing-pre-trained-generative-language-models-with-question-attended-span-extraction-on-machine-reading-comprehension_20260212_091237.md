---
ver: rpa2
title: Enhancing Pre-Trained Generative Language Models with Question Attended Span
  Extraction on Machine Reading Comprehension
arxiv_id: '2404.17991'
source_url: https://arxiv.org/abs/2404.17991
tags:
- question
- qase
- generation
- performance
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of out-of-control generation
  in generative models for extractive Machine Reading Comprehension (MRC), where generated
  answers are often incorrect, irrelevant, or unfaithful to the source text. The authors
  propose a lightweight Question-Attended Span Extraction (QASE) module that guides
  text generation by focusing on potential answer spans within the original text using
  a sequence tagging approach.
---

# Enhancing Pre-Trained Generative Language Models with Question Attended Span Extraction on Machine Reading Comprehension

## Quick Facts
- arXiv ID: 2404.17991
- Source URL: https://arxiv.org/abs/2404.17991
- Authors: Lin Ai; Zheng Hui; Zizhou Liu; Julia Hirschberg
- Reference count: 37
- Primary result: QASE-enhanced models achieve up to 84.1% EM and 91.7% F1 on SQuAD, surpassing few-shot GPT-4

## Executive Summary
This paper addresses the challenge of out-of-control generation in generative models for extractive Machine Reading Comprehension (MRC), where generated answers are often incorrect, irrelevant, or unfaithful to the source text. The authors propose a lightweight Question-Attended Span Extraction (QASE) module that guides text generation by focusing on potential answer spans within the original text using a sequence tagging approach. QASE is integrated during fine-tuning of pre-trained generative language models and significantly improves their performance in MRC tasks. Experiments across multiple datasets show that QASE-enhanced models outperform state-of-the-art extractive models and surpass few-shot GPT-4 on SQuAD, MultiSpanQA, and Quoref, achieving up to 84.1% exact match and 91.7% F1 score on SQuAD. The improvements come without significant computational cost, as QASE adds less than 2% trainable parameters.

## Method Summary
QASE addresses out-of-control generation in generative MRC models by integrating a lightweight module during fine-tuning that uses multi-head attention with question embeddings as queries and context embeddings as keys/values. The module employs an Inside-Outside (IO) sequence tagging schema to identify answer spans, treating span extraction as a sequence labeling task. During fine-tuning, the model is trained with both language modeling loss and sequence tagging loss (weighted by β=1), creating a multi-task learning framework. The approach is evaluated on three English MRC datasets: SQuAD (single-span questions), MultiSpanQA (multi-span questions), and Quoref (questions requiring coreference resolution).

## Key Results
- QASE-enhanced models achieve 84.1% exact match and 91.7% F1 on SQuAD
- Outperforms state-of-the-art extractive models and surpasses few-shot GPT-4 on SQuAD, MultiSpanQA, and Quoref
- Improves factual consistency as measured by Q2 NLI scores
- Adds less than 2% trainable parameters to base models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QASE mitigates ill-formed generation by aligning model attention with question-relevant spans through multi-head attention.
- Mechanism: The module uses multi-head attention (MHA) with question embeddings as queries and context embeddings as keys/values to generate context token representations that reflect their relevance to the question. This guides the model to focus on answer-relevant spans during generation.
- Core assumption: The MHA mechanism can effectively capture the relationship between questions and context tokens, enabling better span identification.
- Evidence anchors:
  - [abstract] "QASE directs model focus to potential answer spans within the original text"
  - [section 3.1] "We utilize a multi-head attention mechanism... Each attention head targets different aspects of the context in relation to the question"
  - [corpus] Weak - no direct mention of this specific mechanism in neighbors

### Mechanism 2
- Claim: QASE improves factual consistency by using a sequence tagging approach (IO schema) to identify answer spans.
- Mechanism: The IO tagging schema labels each token as 'inside' (I) or 'outside' (O) an answer span, which is then used as auxiliary supervision during fine-tuning to guide generation.
- Core assumption: Sequence tagging with IO schema is more effective than traditional BIO format for span extraction in generative models.
- Evidence anchors:
  - [section 3.1] "We frame span extraction as a sequence tagging task using the Inside-Outside (IO) tagging schema"
  - [section 4.4] "QASE-enhanced models consistently outperform the vanilla fine-tuned model" on Q2 factual consistency metric
  - [corpus] Weak - no direct mention of IO tagging in neighbors

### Mechanism 3
- Claim: Multi-task learning with QASE loss improves model performance without significant computational overhead.
- Mechanism: The model is fine-tuned with both language modeling loss and sequence tagging loss (LQASE), with β=1 balancing these objectives.
- Core assumption: The additional sequence tagging task provides useful auxiliary supervision without overwhelming the primary language modeling objective.
- Evidence anchors:
  - [section 3.2] "We fine-tune the PLMs employing a multi-task learning strategy that concurrently optimizes both the language modeling loss and the sequence tagging loss"
  - [section 4.3] "models with QASE show an EM percentage increase of up to 33.8% and an F1 score improvement of up to 8.4%"
  - [section 4.5] "incorporating the QASE module incurs only a slight increase in the number of trainable parameters"

## Foundational Learning

- Concept: Multi-head attention mechanism
  - Why needed here: QASE uses MHA to capture different aspects of the context-question relationship for better span identification
  - Quick check question: How does multi-head attention differ from single-head attention, and why is it beneficial for this application?

- Concept: Sequence tagging with IO schema
  - Why needed here: QASE frames span extraction as sequence tagging using IO schema to identify answer spans
  - Quick check question: What are the advantages of IO schema over BIO format for this application, and how does it handle multi-span answers?

- Concept: Multi-task learning
  - Why needed here: QASE is integrated through multi-task learning, balancing language modeling and span extraction objectives
  - Quick check question: How do you determine the optimal weight (β) for the auxiliary task loss in multi-task learning?

## Architecture Onboarding

- Component map:
  - Input: Tokenized context and question pair with instruction
  - PLM backbone: Llama 2/Alpaca or Flan-T5
  - QASE module: Projection layers → MHA with question-context embeddings → Linear + Softmax for span probability
  - Loss: Language modeling loss + β × sequence tagging loss
  - Output: Generated answer (inference uses only generation component)

- Critical path: Context and question → PLM → QASE module → span probability → multi-task loss → parameter update

- Design tradeoffs:
  - Using IO schema vs BIO format: IO is simpler and handles multi-span answers better but may lose some boundary information
  - Question-first vs context-first prompting: Question-first aligns with BERT-style fine-tuning but may affect generation quality
  - MHA integration vs direct concatenation: MHA captures richer relationships but adds computational overhead

- Failure signatures:
  - Poor EM/F1 scores: QASE module not effectively guiding generation
  - Low Q2 scores: Factual consistency issues despite structural correctness
  - High computational cost: MHA or projection layers adding significant overhead
  - Overfitting on span extraction: β too high, causing model to focus on tagging over generation

- First 3 experiments:
  1. Ablation study: Compare Flan-T5-Large with and without QASE on SQuAD to verify performance improvement
  2. Hyperparameter tuning: Grid search for optimal β value on MultiSpanQA validation set
  3. Architecture variant: Replace MHA with direct concatenation of question and context embeddings to test if attention is critical

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the QASE module affect the computational efficiency of fine-tuned generative models during inference?
- Basis in paper: [explicit] The paper mentions that QASE adds less than 2% trainable parameters and discusses computational cost, but does not provide detailed analysis of inference-time efficiency.
- Why unresolved: While the paper addresses parameter overhead, it does not investigate how QASE impacts inference speed or resource usage during answer generation.
- What evidence would resolve it: Benchmarking studies comparing inference times and memory usage of QASE-enhanced models versus standard fine-tuned models during answer generation.

### Open Question 2
- Question: Would combining QASE with retrieval-augmented generation (RAG) further improve performance on extractive MRC tasks?
- Basis in paper: [inferred] The paper discusses RAG as a related approach but notes that their method fine-tunes model weights rather than altering input, suggesting potential complementary benefits.
- Why unresolved: The paper does not experiment with combining QASE with external knowledge retrieval or document retrieval methods.
- What evidence would resolve it: Comparative experiments measuring performance when integrating QASE with RAG on extractive MRC datasets.

### Open Question 3
- Question: How well does QASE generalize to non-English MRC datasets or other languages?
- Basis in paper: [explicit] All experiments are conducted on English datasets (SQuAD, MultiSpanQA, Quoref) without cross-lingual evaluation.
- Why unresolved: The paper focuses exclusively on English-language MRC tasks without testing cross-lingual capabilities or multilingual generalization.
- What evidence would resolve it: Evaluation of QASE-enhanced models on multilingual MRC datasets or non-English variants of existing benchmarks.

## Limitations
- Limited evaluation on non-English datasets, with all experiments conducted on English MRC tasks
- No statistical significance testing or confidence intervals provided for performance claims
- Potential overfitting to SQuAD-style questions due to extensive fine-tuning on this dataset

## Confidence

1. **QASE improves MRC performance significantly**: Medium confidence - supported by strong quantitative results but lacks ablation studies and statistical validation
2. **QASE enhances factual consistency**: Medium confidence - Q2 scores show improvement but the NLI-based metric has limitations for evaluating MRC outputs
3. **Lightweight design with minimal overhead**: High confidence - architectural constraints and parameter counting support this claim

## Next Checks

1. **Ablation study for attention mechanism**: Remove the multi-head attention component from QASE and compare performance on SQuAD. If performance drops significantly, it validates that MHA is critical for capturing question-context relationships rather than the sequence tagging alone.

2. **β parameter sensitivity analysis**: Conduct a systematic grid search (β ∈ {0.1, 0.5, 1.0, 2.0, 5.0}) on MultiSpanQA validation set to identify the optimal weight. Current paper uses β=1 without justification, which could be suboptimal.

3. **Standardized few-shot GPT-4 comparison**: Implement a controlled few-shot evaluation using standardized prompts (5-shot, temperature=0.7) for GPT-4 on all three datasets. This would provide a fair baseline for comparing QASE-enhanced models against few-shot LLMs.