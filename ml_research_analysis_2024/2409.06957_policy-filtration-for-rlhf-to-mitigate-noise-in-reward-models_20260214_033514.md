---
ver: rpa2
title: Policy Filtration for RLHF to Mitigate Noise in Reward Models
arxiv_id: '2409.06957'
source_url: https://arxiv.org/abs/2409.06957
tags:
- reward
- policy
- pf-ppo
- arxiv
- part
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reward model inaccuracy in
  reinforcement learning from human feedback (RLHF) by proposing Policy Filtration
  for Proximal Policy Optimization (PF-PPO). The key insight is that reward models
  are more reliable in specific regions (e.g., high/low rewards) than in moderate
  reward regions.
---

# Policy Filtration for RLHF to Mitigate Noise in Reward Models

## Quick Facts
- arXiv ID: 2409.06957
- Source URL: https://arxiv.org/abs/2409.06957
- Reference count: 40
- 7B models achieve SOTA on HumanEval (+7.9%), MBPP (+0.7%), LeetCode Contest (+10.0%)

## Executive Summary
This paper addresses the fundamental challenge of reward model noise in reinforcement learning from human feedback (RLHF) by proposing Policy Filtration for Proximal Policy Optimization (PF-PPO). The core insight is that reward models exhibit varying reliability across different reward regions, being more accurate at extremes (high/low rewards) than in moderate reward regions. PF-PPO leverages this insight by filtering responses during training based on their rewards to improve the signal-to-noise ratio. The method employs rank-based filtering strategies, particularly best-random (BR) and best-worst (BW), which have shown consistent performance improvements across multiple benchmarks including code generation and math reasoning tasks.

## Method Summary
PF-PPO introduces a filtering mechanism during RLHF training that exploits the observation that reward models are more reliable in specific reward regions. The approach ranks generated responses based on their predicted rewards and selectively includes them in the training process. Two primary filtering strategies are proposed: best-random (BR), which selects the top-ranked responses plus random samples, and best-worst (BW), which selects both top and bottom-ranked responses. This filtering improves the signal-to-noise ratio during policy optimization by focusing on responses where the reward model is most reliable. The method is implemented as a wrapper around standard PPO algorithms used in RLHF, requiring minimal modifications to existing training pipelines while providing substantial performance gains across multiple benchmarks.

## Key Results
- 7-billion-parameter models achieve state-of-the-art results on HumanEval (+7.9%), MBPP (+0.7%), and LeetCode Contest (+10.0%) benchmarks
- Consistent performance improvements across different reward models and benchmarks in math reasoning tasks
- R² between rewards and actual scores serves as an effective metric for selecting filtering strategies

## Why This Works (Mechanism)
The mechanism exploits reward model reliability heterogeneity. Reward models tend to be more accurate at predicting extreme values (very high or very low rewards) where the distinction between good and bad responses is clearer. In moderate reward regions, the model's predictions become noisier and less reliable. By filtering responses based on their predicted rewards and focusing training on regions where the reward model is most reliable, PF-PPO improves the quality of the policy gradient signal. The rank-based filtering strategies (BR and BW) ensure diversity while maintaining focus on reliable reward predictions, leading to more stable and effective policy optimization compared to using all responses indiscriminately.

## Foundational Learning
**Proximal Policy Optimization (PPO)**: Why needed - Core RL algorithm used in PF-PPO for policy optimization. Quick check - Understand advantage estimation and clipped surrogate objective.
**Reward Modeling**: Why needed - Foundation for understanding reward signal quality and noise. Quick check - Know how reward models are trained and their typical failure modes.
**Signal-to-Noise Ratio in RL**: Why needed - Core concept explaining why filtering improves training. Quick check - Understand how noise affects policy gradient estimation.
**Rank-based Selection**: Why needed - Basis for BR and BW filtering strategies. Quick check - Know how ranking transforms reward distributions.
**Coefficient of Determination (R²)**: Why needed - Metric used to evaluate reward model reliability. Quick check - Understand R² interpretation in regression contexts.

## Architecture Onboarding

**Component Map**: Environment -> Policy Network -> Reward Model -> Ranker -> Filtered Policy Updates -> Improved Policy

**Critical Path**: Sample responses → Score with reward model → Rank responses → Filter based on strategy → Update policy with filtered samples → Repeat

**Design Tradeoffs**: 
- Filtering improves signal quality but reduces data diversity
- BR strategy balances exploitation and exploration better than BW
- Computational overhead from ranking vs. training stability gains
- Parameter sensitivity in filtering thresholds affects robustness

**Failure Signatures**:
- Performance degradation if reward model reliability assumptions break
- Overfitting to filtered subsets when diversity is insufficient
- Computational bottlenecks during ranking in large-scale deployments
- Strategy sensitivity to task-specific reward distributions

**3 First Experiments**:
1. Compare BR vs BW filtering on a small code generation task with different reward models
2. Measure R² correlation between rewards and actual scores across reward regions
3. Ablation study removing filtering to quantify signal-to-noise improvement

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Effectiveness depends on sufficient reward diversity for meaningful rank-based filtering
- Computational overhead of ranking and filtering not explicitly quantified
- Scalability to larger models (beyond 7B) and different domains remains unexplored

## Confidence

**High**:
- Core methodology and benchmark results are well-validated
- Performance improvements are substantial and consistent across tasks
- R² metric effectively guides filtering strategy selection

**Medium**:
- Generalizability to other domains beyond code generation and math reasoning
- Scalability to larger model sizes (70B+ parameters)
- Computational overhead impact on practical deployment

## Next Checks
1. Evaluate PF-PPO's performance on larger model scales (e.g., 70B+ parameters) to assess scalability
2. Test the method across diverse domains beyond code generation and math reasoning to establish broader applicability
3. Conduct ablation studies quantifying the computational overhead introduced by the filtering mechanism compared to standard RLHF training