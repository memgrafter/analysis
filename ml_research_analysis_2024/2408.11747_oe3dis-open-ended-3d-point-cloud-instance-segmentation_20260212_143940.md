---
ver: rpa2
title: 'OE3DIS: Open-Ended 3D Point Cloud Instance Segmentation'
arxiv_id: '2408.11747'
source_url: https://arxiv.org/abs/2408.11747
tags:
- class
- segmentation
- visual
- instance
- names
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Open-Ended 3D Instance Segmentation (OE-3DIS),
  a novel problem that eliminates the need for predefined class names during testing
  by leveraging Multimodal Large Language Models (MLLMs) to generate object class
  names directly from 3D point clouds. The authors establish strong baselines including
  large-vocabulary, image-tagging, and maskwise approaches, and propose a pointwise
  visual token lifting method that aggregates 2D visual tokens from multiple views
  into dense 3D point cloud tokens for querying.
---

# OE3DIS: Open-Ended 3D Point Cloud Instance Segmentation

## Quick Facts
- arXiv ID: 2408.11747
- Source URL: https://arxiv.org/abs/2408.11747
- Authors: Phuc D. A. Nguyen; Minh Luu; Anh Tran; Cuong Pham; Khoi Nguyen
- Reference count: 40
- Primary result: Achieves 16.0 AP on ScanNet200 and 18.4 AP on ScanNet++ without ground-truth class names

## Executive Summary
OE3DIS introduces Open-Ended 3D Instance Segmentation (OE-3DIS), a novel problem that eliminates the need for predefined class names during testing by leveraging Multimodal Large Language Models (MLLMs) to generate object class names directly from 3D point clouds. The authors establish strong baselines including large-vocabulary, image-tagging, and maskwise approaches, and propose a pointwise visual token lifting method that aggregates 2D visual tokens from multiple views into dense 3D point cloud tokens for querying. The approach achieves state-of-the-art performance while remaining training-free.

## Method Summary
The OE3DIS method processes 3D point clouds by first generating class-agnostic 2D instance masks from multiple RGB-D views using DETIC or SAM, then lifting these masks to 3D proposals using Open3DIS. Visual tokens are extracted from masked 2D images using OSM's MaskQ-Former and pretrained visual encoder (CLIP). These 2D tokens are back-projected to 3D points using depth maps and camera parameters, with tokens accumulated and normalized across all views where each point is visible. The aggregated 3D visual tokens for each proposal are then queried with an LLM (Vicuna) to generate object class names. The entire pipeline is training-free, relying only on pretrained models for all components.

## Key Results
- Achieves 16.0 AP on ScanNet200 and 18.4 AP on ScanNet++ without ground-truth class names
- Outperforms Open3DIS (13.1 AP) on ScanNet++ while remaining training-free
- Establishes three strong baselines: large-vocabulary (21K DETIC classes), image-tagging (RAM++), and maskwise (OSM)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lifting 2D visual tokens into dense 3D pointwise tokens preserves geometric consistency across multiple views
- Mechanism: The method back-projects 2D segmentation masks and their associated visual tokens from each view into 3D space using depth maps and camera parameters, accumulating tokens for each 3D point based on visibility
- Core assumption: Depth maps are accurate enough to reliably lift 2D tokens into 3D space without significant misalignment
- Evidence anchors:
  - [section]: "We lift the resulting 2D visual tokens F2D into 3D visual tokens to obtain pointwise 3D visual tokens F3D"
  - [section]: "The correspondence of a 3D point pi(x, y, z) ∈ P with its 2D projection (u, v) in view t is: di,t · [ui; vi; 1]t = Γ · [R|v]t · [xi; yi; zi; 1]"
- Break condition: If depth maps contain significant noise or occlusion errors, the back-projection will accumulate incorrect token associations, degrading 3D token quality

### Mechanism 2
- Claim: Aggregating tokens across multiple views creates a more robust feature representation than single-view approaches
- Mechanism: For each 3D point, tokens from all views where the point is visible are accumulated and normalized, creating a richer representation that captures information from multiple perspectives
- Core assumption: Points visible in multiple views provide complementary information that improves classification accuracy
- Evidence anchors:
  - [section]: "we accumulate 3D visual tokens F3D ∈ RN×E×C, from every 2D mask k and compute for the frequency r3D ∈ RN of every view as follows: F3D = Σt,k λk t ∗ Fk,2D t, r3D = Σt,k λk t"
  - [abstract]: "our point-wise visual token lifting approach, which feeds these tokens into an LLM, achieves the best performance across multiple benchmarks"
- Break condition: If viewpoints are too similar or highly redundant, aggregation provides diminishing returns and may introduce noise from inconsistent token predictions

### Mechanism 3
- Claim: Using a training-free approach with pretrained MLLMs avoids the data scarcity problem common in 3D-LLM research
- Mechanism: The method leverages existing 2D MLLM architectures (like OSM) without fine-tuning, using their pretrained visual token extraction capabilities to bridge the gap between 2D image understanding and 3D point cloud segmentation
- Core assumption: Pretrained 2D MLLMs have learned transferable visual features that generalize to 3D contexts when combined with geometric information
- Evidence anchors:
  - [abstract]: "our method surpasses the performance of Open3DIS... even in the absence of ground-truth object class names"
  - [section]: "our proposed method and designed baselines are entirely training-free, utilizing only pretrained 2D vision encoders (e.g., CLIP [43]) and pretrained LLMs (e.g., Vicuna [10])"
- Break condition: If the 2D MLLM's visual features are too specialized to 2D image distributions, they may fail to capture 3D-specific geometric patterns

## Foundational Learning

- Concept: 3D point cloud representation and coordinate systems
  - Why needed here: The entire method operates on 3D point clouds and requires understanding how 2D projections relate to 3D points
  - Quick check question: How do you transform a 3D point from world coordinates to camera coordinates using extrinsic parameters?

- Concept: Visual token extraction and MLLM architectures
  - Why needed here: The method relies on extracting fixed-length visual tokens from 2D images using pretrained MLLMs
  - Quick check question: What is the role of the MaskQ-Former module in OSM and how does it differ from standard Q-Former?

- Concept: Instance segmentation and evaluation metrics
- Why needed here: The task involves segmenting point clouds into object instances and requires understanding how to evaluate such segmentations
  - Quick check question: How does the label reassignment technique work for evaluating open-ended segmentation where predicted names may not exactly match ground truth?

## Architecture Onboarding

- Component map:
  Input -> 2D Mask Generation (DETIC/SAM) -> 2D-to-3D Mask Lifting (Open3DIS) -> Visual Token Extraction (OSM) -> Pointwise Token Lifting -> Token Aggregation -> LLM Query (Vicuna)

- Critical path:
  1. Generate 2D masks for all views
  2. Lift 2D masks to 3D proposals
  3. Extract 2D visual tokens from masked images
  4. Back-project tokens to 3D points using depth
  5. Aggregate tokens for each 3D proposal
  6. Query LLM for class names

- Design tradeoffs:
  - Training-free vs fine-tuning: Avoids data scarcity but may limit adaptation to 3D-specific features
  - Multi-view aggregation vs single view: Better robustness but higher computational cost
  - Pointwise vs mask-level token lifting: More detailed but requires accurate depth and visibility computation

- Failure signatures:
  - Poor depth quality → misaligned token back-projection
  - Inconsistent 2D mask predictions across views → noisy token aggregation
  - LLM fails to recognize objects → incorrect class names despite good segmentation

- First 3 experiments:
  1. Test pointwise token lifting with synthetic data where ground truth depth is perfect to isolate the token aggregation mechanism
  2. Compare performance using single view vs multi-view aggregation on ScanNet200 to quantify the benefit of the multi-view approach
  3. Evaluate different LLM prompts ("What is in the segmentation mask?" vs alternatives) to understand prompt sensitivity

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the methodology and results, several important questions remain unexplored regarding the scalability, robustness, and generalization of the OE3DIS approach.

## Limitations

- Data dependency on depth quality: The pointwise visual token lifting mechanism critically depends on accurate depth maps for back-projection
- Computational overhead: Multi-view aggregation requires processing multiple RGB-D frames and extracting visual tokens for each view
- LLM prompt sensitivity: The approach may be sensitive to prompt variations across different object categories or datasets

## Confidence

**High confidence**: The claim that the pointwise visual token lifting method achieves 16.0 AP on ScanNet200 and 18.4 AP on ScanNet++ is well-supported by the experimental results and comparisons with Open3DIS baseline (13.1 AP).

**Medium confidence**: The claim about training-free superiority is substantiated by results, but the comparison assumes that fine-tuned methods would not significantly outperform the training-free approach with similar computational resources.

**Medium confidence**: The assertion that aggregating tokens across multiple views creates more robust representations is supported by ablation studies, but the paper lacks detailed analysis of how viewpoint diversity specifically impacts performance.

## Next Checks

1. **Depth noise robustness test**: Evaluate the approach on ScanNet200 with artificially degraded depth maps (adding Gaussian noise, occlusions, or compression artifacts) to quantify the sensitivity of the 2D-to-3D token lifting to depth quality.

2. **Prompt sensitivity analysis**: Systematically test a wider range of LLM prompts across different object categories (furniture, appliances, small objects) to identify which prompt formulations work best for specific object types and whether prompt engineering can improve performance.

3. **Viewpoint diversity quantification**: Analyze the relationship between the number and angular diversity of viewpoints and segmentation performance, determining the point of diminishing returns for multi-view aggregation and identifying optimal camera placement strategies.