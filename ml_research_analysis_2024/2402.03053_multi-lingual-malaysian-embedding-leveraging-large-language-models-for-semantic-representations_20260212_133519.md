---
ver: rpa2
title: 'Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for Semantic
  Representations'
arxiv_id: '2402.03053'
source_url: https://arxiv.org/abs/2402.03053
tags:
- dataset
- malaysian
- recall
- embedding
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the development of open-source Malaysian embedding
  models for semantic similarity and Retrieval-Augmented Generation (RAG) tasks. The
  authors finetune Llama2 and Mistral models using hard mining techniques and synthetic
  datasets to capture the nuances of the Malay language.
---

# Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for Semantic Representations

## Quick Facts
- arXiv ID: 2402.03053
- Source URL: https://arxiv.org/abs/2402.03053
- Reference count: 4
- Finetuned Llama2 and Mistral models outperform OpenAI's text-embedding-ada-002 on Malay language semantic similarity and RAG tasks

## Executive Summary
This paper presents the development of open-source Malaysian embedding models specifically designed for semantic similarity and Retrieval-Augmented Generation (RAG) tasks in the Malay language. The authors finetune Llama2 and Mistral models using hard mining techniques and synthetic datasets to capture the nuances of Malay. The resulting models demonstrate superior performance across multiple benchmarks including b.cari.com.my, c.cari.com.my, Malay news, Malaysian Twitter, and RAG tasks on research papers and legal documents. All models are released as open-source, providing a competitive alternative to closed-source solutions for Malay language processing tasks.

## Method Summary
The authors developed Malaysian embedding models by finetuning Llama2 and Mistral architectures using hard mining techniques and synthetic datasets specifically designed to capture Malay language nuances. The training process incorporated specialized datasets including b.cari.com.my, c.cari.com.my, Malay news, and Malaysian Twitter data. The models were evaluated on semantic similarity benchmarks and RAG tasks using research papers and legal documents from lom.agc.gov.my. The 2 billion parameter Llama2 model achieved the best overall performance, particularly excelling in Recall@5 and Recall@10 metrics for research papers and legal documents.

## Key Results
- The 2 billion parameter Llama2 model outperforms OpenAI's text-embedding-ada-002 on Malay language semantic similarity tasks
- Superior Recall@5 and Recall@10 scores achieved for research papers dataset with "Melayu" keyword
- Excels in Recall@3, Recall@5, and Recall@10 for lom.agc.gov.my legal document dataset
- All models are released as open-source, providing competitive alternatives to closed-source Malay language embeddings

## Why This Works (Mechanism)
The models work by leveraging finetuned large language models that have been specifically trained on Malay language data through hard mining techniques and synthetic dataset generation. The approach captures semantic nuances of the Malay language by exposing the models to diverse Malaysian text sources during training. The finetuning process adapts the general-purpose Llama2 and Mistral architectures to the specific linguistic patterns, cultural context, and semantic relationships present in Malay text, enabling superior performance on downstream tasks compared to general-purpose embeddings.

## Foundational Learning
**Hard Mining Techniques** - Why needed: Identifies challenging training examples that help the model learn more robust semantic representations. Quick check: Verify the model maintains performance when evaluated on adversarial or out-of-distribution examples.

**Synthetic Dataset Generation** - Why needed: Creates diverse training examples that cover edge cases and linguistic variations not present in natural data. Quick check: Assess whether synthetic data improves model generalization to unseen semantic relationships.

**Recall@5 and Recall@10 Metrics** - Why needed: Measures the model's ability to retrieve relevant documents within the top few results, critical for RAG applications. Quick check: Compare retrieval accuracy against baseline models on same benchmark datasets.

## Architecture Onboarding
**Component Map**: Data Collection -> Synthetic Dataset Generation -> Hard Mining -> Model Finetuning -> Evaluation
**Critical Path**: Synthetic data generation and hard mining are essential for capturing Malay language nuances, followed by finetuning the base Llama2/Mistral models, with evaluation on domain-specific benchmarks
**Design Tradeoffs**: Larger models (2B parameters) provide better performance but require more computational resources; synthetic data improves coverage but may introduce artifacts
**Failure Signatures**: Overfitting to training patterns, poor generalization to other Southeast Asian languages, reduced performance on out-of-domain queries
**First Experiments**: 1) Test zero-shot transfer to Indonesian/Thai datasets, 2) Compare performance with and without synthetic data training, 3) Evaluate robustness using adversarial examples

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation framework lacks transparency in synthetic dataset generation and hard mining techniques
- Comparison with OpenAI's model based on specific Malaysian datasets limits generalizability claims
- Need for independent validation on test sets beyond the specific benchmarks used

## Confidence
High: Strong performance on specific benchmarks tested, particularly for Malay language semantic similarity tasks
Medium: RAG task performance claims partially supported but need independent verification
Low: Generalizability claims to other Southeast Asian languages and broader semantic tasks not substantiated

## Next Checks
1. Conduct zero-shot transfer experiments on Indonesian and Thai language datasets to test cross-linguistic generalization claims
2. Implement ablation studies comparing performance with and without synthetic data training to isolate the contribution of different training components
3. Test model robustness using adversarial examples and out-of-domain queries to assess real-world applicability beyond the specific Malaysian datasets used