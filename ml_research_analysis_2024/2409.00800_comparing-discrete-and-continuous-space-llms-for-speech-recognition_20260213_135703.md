---
ver: rpa2
title: Comparing Discrete and Continuous Space LLMs for Speech Recognition
arxiv_id: '2409.00800'
source_url: https://arxiv.org/abs/2409.00800
tags:
- speech
- continuous
- discrete
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first comprehensive comparison of discrete
  and continuous speech representations in LLM-based Automatic Speech Recognition
  (ASR), examining four categories: supervised and unsupervised for both discrete
  and continuous types. Using specialized encoders and comparative analysis with Joint-Training-From-Scratch
  Language Model (JTFS LM) and pre-trained LLaMA2-7b, the study evaluates various
  modeling approaches for discrete and continuous space LLMs.'
---

# Comparing Discrete and Continuous Space LLMs for Speech Recognition

## Quick Facts
- arXiv ID: 2409.00800
- Source URL: https://arxiv.org/abs/2409.00800
- Authors: Yaoxun Xu; Shi-Xiong Zhang; Jianwei Yu; Zhiyong Wu; Dong Yu
- Reference count: 0
- This paper presents the first comprehensive comparison of discrete and continuous speech representations in LLM-based Automatic Speech Recognition (ASR), achieving a state-of-the-art WER of 1.69% on LibriSpeech.

## Executive Summary
This paper presents the first comprehensive comparison of discrete and continuous speech representations in LLM-based Automatic Speech Recognition (ASR). The study examines four categories: supervised and unsupervised for both discrete and continuous types, using specialized encoders and comparative analysis with Joint-Training-From-Scratch Language Model (JTFS LM) and pre-trained LLaMA2-7b. The research demonstrates that continuous representations consistently outperform discrete ones due to information loss during clustering, with supervised learning and continuous representations achieving superior performance.

## Method Summary
The study uses LibriSpeech dataset with 960.9 hours of training data, comparing four types of speech representations: supervised and unsupervised discrete, and supervised and unsupervised continuous. Discrete representations are obtained through K-means clustering of continuous features, while continuous representations use HuBERT and Whisper encoder outputs. Two LLM architectures are employed: JTFS LM (jointly trained from scratch) and LLaMA2-7b (fine-tuned with LoRA). The models are trained using CELoss for discrete scenarios and MSELoss + CELoss for continuous JTFS LM, with evaluation based on Word Error Rate (WER) on test-clean and test-other sets.

## Key Results
- Continuous representations consistently outperform discrete ones due to information loss during clustering
- Supervised learning and continuous representations achieve superior performance
- The study achieves state-of-the-art WER of 1.69% on LibriSpeech using a HuBERT encoder

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous speech representations outperform discrete ones because clustering in discrete representations causes information loss.
- Mechanism: Discrete representations use clustering (e.g., K-means) to reduce continuous features to a fixed number of clusters, which inherently loses fine-grained acoustic detail. Continuous representations preserve the full-dimensionality of acoustic features, enabling richer input to the LLM.
- Core assumption: The LLM can process high-dimensional continuous embeddings directly, and the added information outweighs the computational cost.
- Evidence anchors:
  - [abstract] "continuous representations consistently outperform discrete ones due to information loss during clustering"
  - [section] "Discrete tokens undergo significant information loss during clustering, whereas continuous representations retain most of the information crucial for ASR"
- Break condition: If the computational cost of continuous embeddings becomes prohibitive or the LLM cannot effectively use high-dimensional inputs, the advantage may vanish.

### Mechanism 2
- Claim: Supervised speech encoders outperform unsupervised ones because supervised training aligns speech representations more closely to textual units needed for ASR.
- Mechanism: Supervised encoders (e.g., HuBERT-CTC, Whisper) are trained with paired speech-text data, encouraging the encoder to produce representations that map more directly to phonemes or word units. Unsupervised encoders (e.g., vanilla HuBERT) learn to predict cluster IDs, potentially producing acoustically driven but semantically less relevant features.
- Evidence anchors:
  - [abstract] "supervised learning and continuous representations achieve superior performance"
  - [section] "Whisper encoders consistently outperforming HuBERT encoders in both discrete (with identical K-means) and continuous settings"
- Break condition: If unsupervised encoders are pre-trained on much larger or more diverse datasets, their representations could become more generalizable and narrow the performance gap.

### Mechanism 3
- Claim: The number of K-means clusters in discrete representations impacts performance, with more clusters generally improving accuracy up to a point.
- Mechanism: Increasing the number of clusters allows the discrete representation to capture more acoustic distinctions, reducing information loss. However, too many clusters can make the feature extraction process more complex and computationally expensive.
- Evidence anchors:
  - [section] "WER consistently decreases with an increase in the number of K-means clusters, evident from comparisons"
  - [section] "more clusters enhance performance by capturing a greater volume of information"
- Break condition: Beyond a certain cluster count, the marginal gain diminishes and computational cost outweighs benefits; the optimal number depends on dataset size and task complexity.

## Foundational Learning

- Concept: Clustering algorithms (e.g., K-means) for quantization of continuous speech features.
  - Why needed here: Discrete speech representations rely on clustering to convert continuous features into tokens; understanding this is key to interpreting performance differences.
  - Quick check question: How does increasing the number of clusters affect the granularity of speech tokens and potential information loss?

- Concept: Self-supervised learning (SSL) in speech models (e.g., HuBERT).
  - Why needed here: Many speech encoders used are SSL-based; knowing how they learn representations helps explain their strengths and limitations compared to supervised models.
  - Quick check question: What is the difference between predicting cluster IDs (unsupervised) and predicting CTC outputs (supervised) in terms of feature alignment to text?

- Concept: Cross-modal integration of speech and text via adapters or direct embedding.
  - Why needed here: The paper compares models that feed speech embeddings directly to LLMs versus those that first discretize; understanding these integration methods is crucial for architecture choices.
  - Quick check question: How does using a two-layer perceptron adapter differ from discretizing continuous embeddings before feeding them to an LLM?

## Architecture Onboarding

- Component map:
  - Speech → Encoder → Embedding/Adapter → Transformer → Output projection → Text

- Critical path: Speech → Encoder → Embedding/Adapter → Transformer → Output projection → Text

- Design tradeoffs:
  - Discrete vs. continuous: Discrete is computationally cheaper but loses information; continuous retains information but is heavier.
  - Supervised vs. unsupervised encoders: Supervised yields better alignment to text but requires paired data; unsupervised is more flexible but may be less precise.
  - JTFS LM vs. LLaMA2: JTFS LM is trained jointly and can handle continuous outputs directly; LLaMA2 leverages pre-training but needs discretization.

- Failure signatures:
  - High WER with discrete representations: Likely due to excessive clustering-induced information loss.
  - Poor performance with unsupervised encoders: Features may be too acoustically focused, missing linguistic cues.
  - Overfitting with JTFS LM: Joint training from scratch on small datasets can overfit without pre-training.

- First 3 experiments:
  1. Compare WER of continuous unsupervised (HuBERT layer 16) vs. discrete unsupervised (K-means 1000 clusters) using JTFS LM to confirm information loss hypothesis.
  2. Replace HuBERT with Whisper encoder in both discrete and continuous setups to test encoder impact.
  3. Vary K-means cluster count (500, 1000, 1500) in discrete unsupervised mode to find the sweet spot between granularity and computational cost.

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- The comparison focuses primarily on LibriSpeech, a relatively clean and well-studied dataset, with less clear performance on challenging acoustic conditions.
- The paper doesn't address computational efficiency differences between approaches, which could be critical for real-world deployment.
- The analysis of discrete representations is limited to K-means clustering without exploring alternative quantization methods like vector quantization or product quantization.

## Confidence
- High Confidence: The finding that continuous representations consistently outperform discrete ones due to information loss during clustering is well-supported by the experimental results.
- Medium Confidence: The claim that supervised encoders outperform unsupervised ones is supported by experiments but may be dataset-dependent.
- Medium Confidence: The observation that more K-means clusters improve performance has clear empirical support, but the analysis doesn't explore computational tradeoffs or identify an optimal cluster count.

## Next Checks
1. Evaluate the discrete vs continuous approaches on a noisy, multi-speaker dataset like AMI or CHiME to assess robustness beyond clean speech conditions
2. Compare K-means clustering with alternative discrete representation methods (VQ-VAE, product quantization) to determine if information loss is inherent to discretization or specific to the clustering approach
3. Measure and compare the computational requirements (inference latency, memory usage) of the best-performing discrete and continuous models to assess practical deployment tradeoffs