---
ver: rpa2
title: 'Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis
  of Learning from Human Preferences'
arxiv_id: '2403.01857'
source_url: https://arxiv.org/abs/2403.01857
tags:
- policy
- reward
- rlhf
- learning
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a systematic theoretical comparison between
  reinforcement learning from human feedback (RLHF) and direct preference optimization
  (DPO) for learning from human preferences. The authors focus on a setting with loglinear
  policy parametrization and linear reward functions, deriving minimax statistical
  bounds on the suboptimality gap for both paradigms.
---

# Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences

## Quick Facts
- arXiv ID: 2403.01857
- Source URL: https://arxiv.org/abs/2403.01857
- Authors: Andi Nika; Debmalya Mandal; Parameswaran Kamalaruban; Georgios Tzannetos; Goran Radanović; Adish Singla
- Reference count: 40
- Primary result: Provides theoretical comparison between RLHF and DPO, showing RLHF has O(√dR/n) dependence on reward dimension while DPO has O(dP/(βn)) dependence on policy dimension

## Executive Summary
This paper presents a systematic theoretical comparison between reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) for learning from human preferences. The authors analyze a contextual bandit setting with loglinear policy parametrization and linear reward functions, deriving minimax statistical bounds on the suboptimality gap for both paradigms. Their analysis reveals fundamental differences in how RLHF and DPO scale with reward and policy dimensions, and provides insights into when each approach is preferable based on sample size and dimensional constraints.

## Method Summary
The paper analyzes a contextual bandit setting where preference data follows the Bradley-Terry model. RLHF learns a reward model first (P1.1) and then optimizes the policy (P1.2), while DPO directly optimizes the policy from preference data (P2). Both approaches use loglinear policy parametrization and optimize KL-regularized objectives. The analysis derives statistical rates for the suboptimality gap between learned and optimal policies, considering both realizable and non-realizable reward scenarios.

## Key Results
- RLHF has O(√dR/n) dependence on reward dimension dR, while DPO has O(dP/(βn)) dependence on policy dimension dP
- DPO's bounds improve asymptotically with sample size when β=Θ(√dP/n), while RLHF is better for small n
- When rewards are not realizable, RLHF incurs constant additional error while DPO's dependence can be controlled by tuning β
- These insights extend to deterministic Markov decision processes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLHF has O(√dR/n) dependence on reward dimension dR, while DPO has O(dP/(βn)) dependence on policy dimension dP
- Mechanism: RLHF learns a reward model first, introducing reward dimension complexity, while DPO directly optimizes policy parameters, introducing policy dimension complexity
- Core assumption: Ground-truth reward function is realizable in linear class F and optimal regularized policy is loglinear in Π
- Evidence anchors:
  - [abstract] "RLHF has O(√dR/n) dependence on reward dimension dR, while DPO has O(dP/(βn)) dependence on policy dimension dP"
  - [section 4.1] "The role of dimensionality. Note that RLHF has ˜Θ(√ dR) dependence on the reward dimension, while DPO has Θ(dP ) dependence on the policy dimension"

### Mechanism 2
- Claim: DPO's bounds improve asymptotically with sample size when β=Θ(√dP/n), while RLHF is better for small n
- Mechanism: DPO can set β optimally to match n-dependence, while RLHF's D(π) term cannot be eliminated by tuning β
- Core assumption: Policy feature matrix Ψ is full rank and spans dP dimensions
- Evidence anchors:
  - [abstract] "DPO's bounds improve asymptotically with sample size when β=Θ(√dP/n)"
  - [section 4.2] "If the D(π) terms are similar for both paradigms, then, for large sample sizes such that n ≫ d, DPO seems to outperform RLHF asymptotically"

### Mechanism 3
- Claim: When rewards are not realizable, RLHF incurs constant additional error while DPO's dependence can be controlled by tuning β
- Mechanism: RLHF cannot bypass reward model learning error, while DPO can adjust β to control approximation error
- Core assumption: Ground-truth reward function has best linear fit within ǫapp of true rewards
- Evidence anchors:
  - [abstract] "while RLHF incurs a constant additional error, DPO's dependence can be controlled by tuning β accordingly"
  - [section 6.2] "RLHF does not use MLE for the policy parameter estimation, but first learns a reward model. Thus, it cannot bypass the error coming from the reward unrealizability"

## Foundational Learning

- Concept: Bradley-Terry preference model
  - Why needed here: The preference data follows this model, allowing derivation of likelihood functions for both RLHF and DPO
  - Quick check question: What is the probability formula for preferring yw over yl given context x under Bradley-Terry model?

- Concept: Loglinear policy parametrization
  - Why needed here: Both RLHF and DPO assume loglinear policies to derive smooth optimization problems and statistical bounds
  - Quick check question: How is πθ(y|x) expressed in terms of θ and feature mapping ψ?

- Concept: KL-regularized reinforcement learning
  - Why needed here: Both paradigms optimize KL-regularized objectives, requiring understanding of DKL(π||µ) terms
  - Quick check question: What is the form of the KL-regularized objective V π r (ρ) in terms of value function and KL divergence?

## Architecture Onboarding

- Component map:
  - RLHF pipeline: Preference data → Reward model learning (P1.1) → KL-regularized RL (P1.2) → Final policy
  - DPO pipeline: Preference data → Direct policy optimization (P2) → Final policy
  - Both require: Linear reward class F, loglinear policy class Π, Bradley-Terry preference model

- Critical path:
  1. Data preprocessing: Parse preference tuples (xi, yw
  i , y l
  i) into feature matrices Φ and Ψ
  2. RLHF: Solve P1.1 for reward parameters, then P1.2 for policy parameters
  3. DPO: Solve P2 directly for policy parameters
  4. Compare performance using suboptimality gap G(π)

- Design tradeoffs:
  - RLHF vs DPO: RLHF has simpler policy optimization but reward learning complexity; DPO has direct optimization but requires careful β tuning
  - Exact vs approximate optimization: Exact gives better bounds but requires oracles; approximate is practical but needs convergence analysis
  - Realizable vs non-realizable rewards: Realizable allows clean bounds; non-realizable requires additional error terms

- Failure signatures:
  - Poor coverage: ΣDn,R or ΣDn,P have small eigenvalues indicating insufficient exploration
  - Non-convergence: Gradient descent fails to converge due to inappropriate learning rates or β values
  - High D(π) terms: Large KL divergence between optimal and learned policies

- First 3 experiments:
  1. Compare RLHF vs DPO on synthetic data with known reward structure, varying (dR, dP, n) combinations
  2. Test sensitivity to β parameter in DPO across different sample sizes
  3. Evaluate performance degradation when ground-truth reward is not realizable in linear class F

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does the performance gap between RLHF and DPO widen or narrow in terms of reward and policy dimensions?
- Basis in paper: Explicit. The paper states "RLHF has O(√dR/n) dependence on reward dimension dR, while DPO has O(dP/(βn)) dependence on policy dimension dP" and discusses implications when dR ≠ dP.
- Why unresolved: The paper provides theoretical bounds but doesn't fully characterize the exact conditions under which one paradigm becomes superior to the other based on dimensional differences.
- What evidence would resolve it: Empirical studies comparing RLHF and DPO performance across different reward/policy dimensionalities, or theoretical characterization of the exact threshold conditions.

### Open Question 2
- Question: How does the non-realizability of the ground-truth reward function affect the asymptotic performance of DPO compared to RLHF?
- Basis in paper: Explicit. The paper notes that "RLHF incurs a constant additional error while DPO's dependence can be controlled by tuning β" when rewards are not realizable.
- Why unresolved: While the paper shows DPO can asymptotically improve by tuning β, the exact trade-off and optimal β tuning strategy in practice remains unclear.
- What evidence would resolve it: Empirical studies on the practical performance of DPO with different β settings under non-realizable reward conditions.

### Open Question 3
- Question: Can the theoretical bounds and insights from the contextual bandit setting be extended to more complex function approximation settings?
- Basis in paper: Inferred. The paper mentions "a systematic large-scale empirical investigation that would validate the theoretical insights of this paper would be of great importance" and discusses extending to MDPs.
- Why unresolved: The current analysis is limited to loglinear policy parametrization and linear rewards. The paper suggests this is an important direction but doesn't provide results for general function approximation.
- What evidence would resolve it: Theoretical analysis extending the bounds to general function approximation settings, or empirical validation in more complex RL problems with neural network policies.

## Limitations
- The analysis is restricted to linear reward functions and loglinear policies, which may not capture real-world complexity
- The theoretical bounds may not be tight for practical implementations
- The comparison assumes similar D(π) terms between paradigms, which may not hold in practice

## Confidence

- **High confidence**: The comparative framework and general intuition that RLHF scales with reward dimension while DPO scales with policy dimension are well-supported by the theoretical analysis
- **Medium confidence**: The specific asymptotic bounds (O(√dR/n) vs O(dP/(βn))) are mathematically rigorous but may not translate directly to practical performance differences
- **Medium confidence**: The claim about DPO's advantage for large sample sizes assumes similar D(π) terms between paradigms, which may not hold in practice

## Next Checks

1. Empirical validation on real preference datasets to test whether the theoretical bounds accurately predict practical performance differences between RLHF and DPO
2. Analysis of non-realizable reward scenarios with varying levels of approximation error to quantify the impact on both paradigms
3. Extension of the analysis to non-linear reward and policy classes to assess robustness of the conclusions beyond the linear/loglinear setting