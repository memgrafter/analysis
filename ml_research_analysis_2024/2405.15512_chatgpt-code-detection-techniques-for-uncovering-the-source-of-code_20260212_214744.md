---
ver: rpa2
title: 'ChatGPT Code Detection: Techniques for Uncovering the Source of Code'
arxiv_id: '2405.15512'
source_url: https://arxiv.org/abs/2405.15512
tags:
- code
- https
- snippets
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates techniques for detecting whether code was
  generated by ChatGPT or written by humans. The authors explore using embeddings
  and traditional ML models to classify code snippets, achieving up to 98% accuracy.
---

# ChatGPT Code Detection: Techniques for Uncovering the Source of Code

## Quick Facts
- arXiv ID: 2405.15512
- Source URL: https://arxiv.org/abs/2405.15512
- Reference count: 40
- Primary result: 98% accuracy achieved in distinguishing ChatGPT-generated from human-written code using embedding features

## Executive Summary
This paper investigates techniques for detecting whether Python code was generated by ChatGPT or written by humans. The authors explore using embeddings and traditional ML models to classify code snippets, achieving up to 98% accuracy. They find that black-box embedding features perform better than white-box human-designed features. A Bayes classifier with almost 90% accuracy is also developed to explain the differences between AI and human code. The study contributes to understanding and mitigating risks associated with AI code generation in education and software development.

## Method Summary
The study employs a binary classification approach using 31,400 Python code snippets from multiple sources, balanced between human and ChatGPT-generated code. The methodology involves tokenizing code using cl100k_base encoding, generating embeddings via TF-IDF, Word2Vec, or OpenAI ADA, and training supervised learning models (DNN, RF, XGB, GMM, etc.) with problem-wise 80/20 train-test splits. Models are evaluated across 10 runs with different seeds, comparing performance between formatted and unformatted code, and between embedding-based and human-designed feature approaches.

## Key Results
- XGBoost with TF-IDF embeddings achieved 98% accuracy on unformatted code
- Embedding-based models outperformed human-designed feature models by 15-20 percentage points
- Gaussian Mixture Models achieved over 90% accuracy using token-level embeddings
- Code formatting with Black reduced classification performance by 4-8 percentage points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models distinguish ChatGPT-generated code from human-written code by leveraging high-dimensional embedding features that capture subtle stylistic patterns beyond formatting.
- Mechanism: Embeddings transform code into dense vector representations where ML models can mathematically detect nuanced differences in token usage, structural patterns, and semantic features.
- Core assumption: Detectable, learnable stylistic differences between AI-generated and human-written code persist even after formatting normalization.
- Evidence anchors:
  - [abstract]: "We employ a new approach that combines powerful embedding features (black-box) with supervised learning algorithms – including Deep Neural Networks, Random Forests, and Extreme Gradient Boosting – to achieve this differentiation with an impressive accuracy of 98%."
  - [section 5.3]: "ML models operating on embeddings achieve superior performance compared to the models using the human-designed features; see Table 3 and Table 4 for results on the unformatted and formatted dataset, respectively."
- Break condition: If stylistic differences are eliminated through formatting or if AI models are trained to mimic human coding patterns.

### Mechanism 2
- Claim: Performance gap between embedding-based and human-designed feature models demonstrates AI-generated code contains complex patterns difficult for humans to explicitly engineer features for.
- Mechanism: High-dimensional embeddings capture rich feature space including subtle statistical properties of token sequences, code structure, and semantic relationships that simple human-designed features cannot represent.
- Core assumption: Complexity and richness of code patterns exceed what can be captured by manually designed features like whitespace counts or punctuation.
- Evidence anchors:
  - [section 5.3]: "The disparities in performance between the white- and black-box approaches, despite employing identical models, highlight the significance of embedding techniques."
  - [section 5.3]: "While traditional feature engineering is based on domain-specific expertise or interpretability, it often cannot capture the complex details of code snippets as effectively as embedding features."
- Break condition: If manual feature engineering can be improved to capture the same complex patterns, or if the embedding space becomes too sparse to be useful.

### Mechanism 3
- Claim: Gaussian Mixture Models can effectively model probability distributions of human and AI-generated code tokens, enabling interpretable classification based on likelihood comparisons.
- Mechanism: GMMs approximate underlying probability distributions of code tokens for each class, allowing classification based on which distribution a new sample is more likely to belong to.
- Core assumption: Token distributions in human and AI-generated code are sufficiently different and can be modeled as Gaussian mixtures.
- Evidence anchors:
  - [section 5.4]: "With both embeddings, GMMs achieve accuracies of over 90%, outperforming any of the models based on human-designed features."
  - [section 5.4]: "The underlying concept of GMMs is the approximation of the probability distributions of ChatGPT, which does not extend over entire snippets but rather on individual tokens used for the prediction of the following token."
- Break condition: If token distributions overlap significantly between classes, making likelihood-based classification ineffective.

## Foundational Learning

- Concept: Tokenization and embedding techniques for code
  - Why needed here: Models need to convert raw code text into numerical representations that ML algorithms can process. Different tokenization strategies affect how code structure and semantics are captured.
  - Quick check question: What is the difference between token-level and snippet-level embedding approaches, and when would each be appropriate?

- Concept: Supervised learning model selection and evaluation
  - Why needed here: The paper compares multiple ML models and needs to understand their strengths, weaknesses, and appropriate use cases for code classification tasks.
  - Quick check question: How do Random Forests differ from XGBoost in terms of handling high-dimensional features and avoiding overfitting?

- Concept: Model calibration and probability interpretation
  - Why needed here: Beyond simple classification, understanding how well predicted probabilities match actual outcomes is crucial for practical applications where confidence estimates matter.
  - Quick check question: What does a well-calibrated model look like in a calibration plot, and why is this important for decision-making?

## Architecture Onboarding

- Component map: Code collection → preprocessing (tokenization, formatting) → embedding generation → feature extraction → ML models (DNN, RF, XGB, GMM) → evaluation (cross-validation, metrics) → calibration analysis
- Critical path: Data preprocessing → embedding generation → model training → evaluation → calibration analysis
- Design tradeoffs:
  - White-box vs black-box features: Human-designed features are interpretable but less accurate; embeddings are more accurate but less interpretable
  - Formatted vs unformatted data: Formatting reduces stylistic differences but may lose some discriminative information
  - Model complexity vs performance: Simple models work well with embeddings, but complex models may be needed for human-designed features
- Failure signatures:
  - Low accuracy across all models suggests fundamental issues with feature representation or data quality
  - Poor calibration indicates models are not well-calibrated probability estimators
  - High variance in cross-validation suggests overfitting or insufficient data diversity
- First 3 experiments:
  1. Run all models on formatted vs unformatted data to quantify the impact of code formatting on classification performance
  2. Compare embedding-based models (ADA, TF-IDF, Word2Vec) to identify which embedding approach captures the most discriminative features
  3. Test GMM-based classification on individual tokens vs entire snippets to determine optimal granularity for probability modeling

## Open Questions the Paper Calls Out

- Can classification models effectively distinguish code generated by different LLMs (e.g., GPT-3.5, GPT-4, T5+) or is a separate model needed for each?
  - Basis in paper: [explicit] The paper notes this is an open research question, stating "It is an open research question whether one classification model can disentangle several code generators and human code or whether separate models for each code generator are needed."
  - Why unresolved: The paper only tested models on GPT-3.5 generated code. Testing with multiple LLMs would be needed to determine if a single model can generalize across different AI code generators.
  - What evidence would resolve it: Training and testing classification models on code generated by multiple LLMs and comparing performance to models trained on single LLM code.

- How well can humans be trained to distinguish AI-generated from human-written code using labeled data?
  - Basis in paper: [inferred] The paper tested untrained humans and found they performed near random guessing. It states "How well humans can be trained in the task using labeled data remains to be investigated."
  - Why unresolved: The study only tested untrained humans. A study training humans with labeled data would be needed to assess human learning potential for this task.
  - What evidence would resolve it: A study training human participants with labeled AI and human code examples, then testing their classification accuracy on unseen code.

- How robust are code detection models to different code formatters (e.g., Black, AutoPEP8)?
  - Basis in paper: [explicit] The paper notes this as an area for further exploration, stating "Examining the robustness of our classification models against diverse formatting styles remains an area warranting further exploration."
  - Why unresolved: The study only tested models on code formatted with the Black formatter. Testing with other formatters would be needed to assess model robustness.
  - What evidence would resolve it: Training and testing classification models on code formatted with different formatters and comparing performance.

## Limitations

- The reported 98% accuracy relies on unformatted code, but formatting with Black reduces performance by 4-8 percentage points, raising questions about whether models detect genuine stylistic differences or simply formatting patterns.
- All code samples are Python programming problems, which may limit generalizability to other programming languages or code types.
- While 31,400 snippets were used, the dataset comes from specific sources which may not represent the full diversity of human and AI-generated code.

## Confidence

- High confidence: The fundamental approach of using embeddings for code classification works well, supported by consistent results across multiple models and the 98% accuracy benchmark.
- Medium confidence: The superiority of black-box embedding features over white-box human-designed features is well-supported, though the specific performance gap may vary with different datasets or code domains.
- Low confidence: The Bayes classifier achieving "almost 90% accuracy" lacks detailed validation, and the explanation of why AI and human code differ is largely qualitative.

## Next Checks

1. Cross-language validation: Test the classification models on code snippets from languages other than Python (e.g., Java, C++) to assess generalizability beyond the current dataset.
2. Formatting invariance analysis: Systematically vary code formatting parameters and measure classification performance to determine which formatting aspects the models actually rely on.
3. Human baseline comparison: Conduct blinded human evaluation studies where expert programmers attempt to distinguish AI-generated from human-written code, comparing their accuracy to the ML models' performance.