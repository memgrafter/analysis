---
ver: rpa2
title: 'D-GRIL: End-to-End Topological Learning with 2-parameter Persistence'
arxiv_id: '2406.07100'
source_url: https://arxiv.org/abs/2406.07100
tags:
- persistence
- function
- learning
- bifiltration
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes D-GRIL, an end-to-end differentiable framework
  for learning 2-parameter persistence-based topological features in machine learning.
  The core idea is to extend the GRIL vectorization technique to allow gradient-based
  learning of the bifiltration function through explicit differential formulas, making
  it compatible with standard deep learning pipelines.
---

# D-GRIL: End-to-End Topological Learning with 2-parameter Persistence

## Quick Facts
- **arXiv ID**: 2406.07100
- **Source URL**: https://arxiv.org/abs/2406.07100
- **Reference count**: 23
- **Primary result**: Proposes D-GRIL, an end-to-end differentiable framework for learning 2-parameter persistence-based topological features, validated on bio-activity prediction and benchmark graph classification tasks

## Executive Summary
This paper introduces D-GRIL, a differentiable framework that enables end-to-end learning of topological features from graphs using 2-parameter persistence. The key innovation is proving that the GRIL vectorization technique is piecewise affine, which allows deriving explicit gradient formulas for backpropagation. This makes it possible to learn the bifiltration function directly from data rather than relying on hand-crafted choices like HKS-Ricci curvature. The method is validated on ChEMBL bio-activity prediction datasets and benchmark graph classification tasks, showing improvements over standard GNNs and passive GRIL approaches while being approximately 40% faster than the recent MPSM method.

## Method Summary
D-GRIL extends the GRIL vectorization technique to be differentiable by proving it is piecewise affine relative to a hyperplane arrangement. The method uses a GIN (Graph Isomorphism Network) to learn a bifiltration function on vertices, which is extended to the entire simplicial complex via lower-star construction. GRIL computes persistence features using ℓ-worms in the 2-parameter persistence module, and explicit gradient formulas enable backpropagation through this computation. The framework includes sampling center points for ℓ-worms and using a 3-layer MLP classifier, with convergence guarantees provided by the piecewise affine structure of GRIL.

## Key Results
- D-GRIL outperforms standard GNNs and passive GRIL readouts on bio-activity prediction tasks using ChEMBL datasets
- The method improves molecular fingerprint models like ECFP and Morgan3 when augmented with D-GRIL
- On benchmark graph datasets (MUTAG, PROTEINS, DHFR, COX2), D-GRIL surpasses existing multiparameter persistence methods and learns better bifiltration functions than standard choices
- Training is approximately 40% faster than the recent MPSM method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GRIL is piecewise affine relative to a hyperplane arrangement, enabling explicit gradient computation
- Mechanism: The GRIL function is defined over ℓ-worms in R². The piecewise affine structure comes from partitioning R²ⁿ (bifiltration function space) by hyperplanes that encode conditions where simplex coordinates are equidistant from ℓ-worm centers. Each top-dimensional cell has a unique constraining simplex coordinate, making GRIL affine within that cell
- Core assumption: The bifiltration function values are distinct enough that no two simplex coordinates coincide exactly; otherwise the hyperplane arrangement becomes degenerate
- Evidence anchors:
  - [abstract] "proving GRIL is piecewise affine and deriving explicit gradients, enabling end-to-end learning"
  - [section 4.1] "arrangement H of hyperplanes in R²ⁿ partitions R²ⁿ into relatively open r-cells"
  - [corpus] "GEFL: Extended Filtration Learning for Graph Classification" shows related methods rely on similar hyperplane-based stratifications
- Break condition: If multiple simplex coordinates coincide, the affine stratification collapses and subgradients must be approximated

### Mechanism 2
- Claim: Stochastic subgradient descent converges almost surely on N ◦ G when N is definable and locally Lipschitz
- Mechanism: G is piecewise affine → locally Lipschitz and definable. If N is also definable and locally Lipschitz, then N ◦ G is definable and locally Lipschitz. Davis et al.'s convergence theorem applies because the function satisfies Whitney C^d-stratifiability
- Core assumption: The loss N is definable (e.g., standard neural network losses) and the data distribution yields finite samples per stratum
- Evidence anchors:
  - [section 4.2] "stochastic sub-gradient descent converges almost surely to critical points of N ◦ G"
  - [section 3.2] cites Davis et al. (2020) and Carrière et al. (2021) for the convergence theorem
  - [corpus] "EMP: Effective Multidimensional Persistence for Graph Representation Learning" uses definable losses, suggesting this is a common assumption
- Break condition: If N has non-definable components (e.g., some exotic activation functions) convergence guarantees may fail

### Mechanism 3
- Claim: End-to-end learning of bifiltration functions outperforms fixed, hand-crafted choices
- Mechanism: The neural network learns a bifiltration f on vertices, extended to the whole complex by lower-star construction. GRIL's piecewise affine nature allows gradients to flow back to f, optimizing it jointly with the downstream classifier. This captures dataset-specific topology rather than generic geometric priors
- Core assumption: The topological features captured by GRIL are predictive for the downstream task and the learned bifiltration is not degenerate
- Evidence anchors:
  - [abstract] "learns better bifiltration functions than standard choices like HKS-Ricci curvature"
  - [section 5.3] Table 3 shows D-GRIL outperforms GRIL and MPSM on benchmark datasets
  - [corpus] "TMetaNet: Topological Meta-Learning Framework for Dynamic Link Prediction" similarly learns filtration functions end-to-end
- Break condition: If the topological signal is weak relative to noise, the learned filtration may overfit and not generalize

## Foundational Learning

- Concept: Simplicial complexes and filtrations
  - Why needed here: GRIL operates on the homology of sublevel sets of a bifiltration; understanding the combinatorial structure is essential for defining the ℓ-worms and constraining coordinates
  - Quick check question: Given a graph G, what is the 1-skeleton simplicial complex? What are its simplices?

- Concept: Persistent homology and generalized rank
  - Why needed here: GRIL is built from generalized rank invariants over ℓ-worms; without knowing what persistence modules are and how rank captures topological features, the vectorization makes no sense
  - Quick check question: For a 1-parameter filtration of a triangle, what are the birth and death times of its connected components and the 1-cycle?

- Concept: Piecewise affine maps and stratifications
  - Why needed here: The differentiability proof hinges on GRIL being affine on each stratum of an arrangement of hyperplanes; without this concept, the convergence argument is opaque
  - Quick check question: If f(x) = min(ax + b, cx + d), on which intervals is f affine?

## Architecture Onboarding

- Component map: GIN → vertex bifiltration → lower-star extension → ℓ-worm GRIL computation → sampled center points → 3-layer MLP classifier → loss. Gradients flow backward through GRIL via explicit partial derivatives (Theorem 4.7)
- Critical path: Forward: GIN → f → G (GRIL) → MLP → loss. Backward: loss → MLP → G (via explicit ∂GRIL/∂simplex) → f → GIN weights
- Design tradeoffs: Sampling fewer center points speeds training but reduces topological resolution; sampling more improves expressiveness but increases memory/compute. Using ℓ = 2 keeps worms small for efficiency
- Failure signatures: If training stalls, check that no two simplex coordinates coincide (causes degenerate hyperplane cells). If accuracy drops, verify that the learned bifiltration is not collapsing all vertices to the same value
- First 3 experiments:
  1. Run D-GRIL on a small graph dataset (e.g., MUTAG) with ℓ = 1, s = 10 centers, and verify convergence by monitoring loss decrease
  2. Replace the GIN with a constant bifiltration (e.g., all zeros) and confirm that GRIL gradients are zero (baseline test)
  3. Swap ℓ-worm width d for a fixed value and observe that the model cannot adapt filtration scale, illustrating the benefit of learning d

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but based on the methodology and results, several natural extensions and research directions emerge from the work.

## Limitations
- The piecewise affine structure of GRIL relies on the assumption that no two simplex coordinates coincide exactly, which may not hold in practice with continuous parameter optimization
- The differentiability proof depends on efficient computation of zigzag persistence for ℓ-worms, though the actual computational complexity and numerical stability are not thoroughly explored
- The empirical performance claims, while significant, are limited by somewhat limited experimental setup details, making exact reproduction challenging

## Confidence
- **High Confidence**: The piecewise affine structure of GRIL and the explicit gradient formulas (Theorem 4.7) - this is mathematically proven and the derivation appears sound
- **Medium Confidence**: The convergence guarantees for stochastic subgradient descent - while the theory from Davis et al. (2020) is cited correctly, the practical applicability depends on the specific loss landscape and data distribution
- **Medium Confidence**: The empirical performance claims - the reported improvements over baselines are significant, but the experimental setup details are somewhat limited, making exact reproduction challenging

## Next Checks
1. **Gradient Verification Test**: Implement a controlled experiment where a known graph structure is fed through D-GRIL with a constant bifiltration function. Verify that GRIL gradients are exactly zero, confirming the gradient formulas are correctly implemented and no spurious gradients appear from numerical errors

2. **Degeneracy Stress Test**: Systematically test D-GRIL on datasets where simplex coordinates are deliberately made to coincide (e.g., by initializing the GIN to produce identical outputs for multiple vertices). Measure how the gradient sampling approximation performs and whether it maintains training stability

3. **Runtime Complexity Analysis**: Profile the actual computational cost of the zigzag persistence computations in GRIL on graphs of increasing size. Compare this to the claimed "40% faster than MPSM" and verify that the speedup comes from the algorithmic improvements rather than just hardware differences or implementation optimizations