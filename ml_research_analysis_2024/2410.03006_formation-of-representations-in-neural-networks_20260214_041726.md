---
ver: rpa2
title: Formation of Representations in Neural Networks
arxiv_id: '2410.03006'
source_url: https://arxiv.org/abs/2410.03006
tags:
- neural
- alignment
- which
- have
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the Canonical Representation Hypothesis (CRH)
  and Polynomial Alignment Hypothesis (PAH) to explain how structured, compact representations
  emerge in neural networks during training. The CRH states that representations (R),
  weights (W), and neuron gradients (G) become mutually aligned across layers due
  to a balance between gradient noise and regularization.
---

# Formation of Representations in Neural Networks
## Quick Facts
- **arXiv ID**: 2410.03006
- **Source URL**: https://arxiv.org/abs/2410.03006
- **Reference count**: 40
- **Primary result**: Proposes Canonical Representation Hypothesis (CRH) and Polynomial Alignment Hypothesis (PAH) to explain emergence of structured representations in neural networks

## Executive Summary
This paper introduces a theoretical framework explaining how neural networks develop compact, structured representations during training. The Canonical Representation Hypothesis (CRH) proposes that representations, weights, and gradients align across layers when gradient noise and regularization are balanced. The Polynomial Alignment Hypothesis (PAH) predicts power-law scaling relationships between these quantities when alignment breaks. The work provides both theoretical proofs and extensive experimental validation across multiple architectures, connecting these phenomena to neural collapse and offering insights into feature importance alignment.

## Method Summary
The authors develop a theoretical framework based on analyzing the dynamics of representations (R), weights (W), and gradients (G) during training. They prove that when gradient noise and regularization balance appropriately, these quantities become mutually aligned across layers. The framework distinguishes between alignment phases (where CRH holds) and breakdown phases (where PAH predicts specific scaling relationships). Experimental validation involves tracking these quantities across training using various metrics including canonical correlation analysis, layer-wise alignment scores, and feature importance measures. The approach is tested on multiple architectures including transformers, ResNets, and MLPs across different training paradigms.

## Key Results
- CRH states that representations, weights, and gradients become mutually aligned during training when noise and regularization balance
- PAH predicts reciprocal power-law scaling relationships between R, W, and G when CRH breaks down
- Strong alignment (CRH) is observed in later network layers and self-supervised learning settings
- The framework connects to neural collapse phenomena and the neural feature ansatz

## Why This Works (Mechanism)
The mechanism relies on the interplay between gradient noise and regularization during training. When these forces balance, the network naturally develops aligned representations across layers. This alignment emerges from the optimization dynamics where similar features become important across layers, leading to coordinated updates in representations, weights, and gradients. The power-law relationships predicted by PAH emerge during transition phases when this balance is disrupted, creating predictable scaling behavior that characterizes different training stages.

## Foundational Learning
- **Gradient noise regularization balance**: Understanding how stochastic gradients and explicit regularization interact is crucial for grasping why alignment emerges. Quick check: Verify that higher noise levels delay alignment onset.
- **Canonical correlation analysis**: Essential for measuring alignment between different neural network quantities across layers. Quick check: Correlation scores should increase during alignment phases.
- **Power-law scaling in optimization**: Background on how training dynamics often exhibit power-law behavior helps interpret PAH predictions. Quick check: Log-log plots should show linear relationships during breakdown phases.
- **Neural collapse phenomenon**: Understanding this known behavior in overparameterized networks provides context for the proposed connections. Quick check: Final-layer representations should become more compact as training progresses.
- **Neural tangent kernel theory**: Provides theoretical foundation for understanding how networks behave during training. Quick check: NTK similarity should correlate with alignment strength.

## Architecture Onboarding
**Component map**: Input -> Feature extractor (R) -> Classifier (W) -> Output, with gradients (G) flowing backward through this path during training
**Critical path**: The alignment between representations, weights, and gradients is the critical path - when these three quantities align, the network achieves optimal feature extraction and classification performance
**Design tradeoffs**: The framework suggests that architectural choices affecting gradient flow (like skip connections or normalization layers) will impact alignment dynamics and timing
**Failure signatures**: Misalignment manifests as poor generalization, slower convergence, or failure to achieve neural collapse in later layers
**First experiments**:
1. Track layer-wise alignment scores during training across different architectures
2. Measure power-law exponents during breakdown phases to verify PAH predictions
3. Compare alignment dynamics between supervised and self-supervised training regimes

## Open Questions the Paper Calls Out
The paper leaves several open questions regarding the generality of CRH and PAH across different domains, the precise mathematical relationship between alignment and neural collapse, and the impact of specific architectural choices on alignment dynamics.

## Limitations
- The framework is primarily validated on vision and language benchmarks, leaving domain generality uncertain
- Theoretical assumptions about noise and regularization conditions may not hold in all practical scenarios
- Empirical validation relies heavily on correlation analysis rather than controlled ablation studies
- Lack of quantitative thresholds for "strong" alignment makes assessment subjective
- Does not address potential failure modes in highly overparameterized regimes or with attention mechanisms

## Confidence
**Major Claim Clusters Confidence:**
- Theoretical foundation of CRH/PAH: Medium
- Empirical validation across architectures: High
- Connection to neural collapse: Medium
- Practical implications for training: Low

## Next Checks
1. Test CRH/PAH predictions on non-standard domains like reinforcement learning agents and graph neural networks to assess domain generality
2. Conduct controlled ablation experiments varying noise levels and regularization strengths to quantify their impact on alignment dynamics
3. Perform rigorous mathematical analysis establishing formal connections between CRH alignment and known phenomena like neural collapse and neural tangent kernel behavior