---
ver: rpa2
title: Noise Contrastive Alignment of Language Models with Explicit Rewards
arxiv_id: '2402.05369'
source_url: https://arxiv.org/abs/2402.05369
tags:
- uni00000011
- reward
- preference
- infonca
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NCA (Noise Contrastive Alignment) and InfoNCA,
  two new methods for aligning language models with human preferences and explicit
  rewards. The key idea is to use Noise Contrastive Estimation (NCE) to transform
  reward-based alignment into a classification problem that can directly optimize
  language model policies.
---

# Noise Contrastive Alignment of Language Models with Explicit Rewards

## Quick Facts
- **arXiv ID:** 2402.05369
- **Source URL:** https://arxiv.org/abs/2402.05369
- **Reference count:** 40
- **One-line primary result:** NCA and InfoNCA outperform DPO and other preference-based methods on reward datasets, with NCA showing particular strength on complex reasoning tasks.

## Executive Summary
This paper introduces NCA (Noise Contrastive Alignment) and InfoNCA, two new methods for aligning language models with human preferences and explicit rewards. The key insight is to use Noise Contrastive Estimation (NCE) to transform reward-based alignment into a classification problem that can directly optimize language model policies. InfoNCA generalizes Direct Preference Optimization (DPO) to handle explicit reward datasets with multiple responses per prompt, while NCA specifically addresses the issue of decreasing likelihood that occurs with DPO/InfoNCA by optimizing absolute likelihood values rather than relative differences. In experiments with Mistral-7B and 8×7B models, both methods outperform various preference-based baselines when reward datasets are available, with NCA showing significant advantages on complex reasoning tasks like math and coding.

## Method Summary
The authors propose a novel framework that unifies reward-based and preference-based alignment through noise contrastive estimation. InfoNCA uses a multi-class classification objective where the model must identify which response was sampled from the optimal policy versus the prior policy, with soft labels computed from rewards via softmax. NCA uses a binary classification objective that optimizes absolute likelihood values to prevent the "likelihood collapse" observed in DPO. Both methods parameterize the reward function as rθ = β log πθ/μ, which enables simultaneous reward modeling and policy optimization. The framework subsumes DPO as a special case when K=2 responses and reward temperature α→0, while extending to handle explicit reward datasets with multiple responses per prompt.

## Key Results
- InfoNCA and NCA outperform DPO and other preference-based baselines on UltraFeedback reward dataset with GPT-4 evaluations
- NCA shows significant improvements over DPO on reasoning tasks (math, coding, BBH, LeetCode, GSM-Plus, MATH, TheoremQA, SVAMP, ASDiv)
- InfoNCA achieves 66.2% win rate over baseline models in human preference studies
- Both methods demonstrate lower KL divergence from initial policy compared to baselines, indicating better preservation of general capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** InfoNCA and NCA transform language model alignment into a classification problem by leveraging Noise Contrastive Estimation (NCE) theory.
- **Mechanism:** Both methods construct a multi-class classification task where the model must identify which response was sampled from the optimal policy π* versus from the prior policy μ. InfoNCA uses InfoNCE (multi-class cross-entropy), while NCA uses NCE (binary classification).
- **Core assumption:** The optimal policy π*(y|x) ∝ μ(y|x)er(x,y)/α can be recovered by maximizing the likelihood of correctly identifying the optimal response among candidates.
- **Evidence anchors:**
  - [abstract]: "Our framework comprises two parallel algorithms, NCA and InfoNCA, both enabling the direct extraction of an LM policy from reward data as well as preference data."
  - [section]: "InfoNCA and NCA are uniquely suited for both reward data and preference data, including DPO as a special case."
  - [corpus]: Weak - corpus lacks specific discussion of NCE theory application to LM alignment.
- **Break condition:** If the noise distribution μ does not adequately represent the distribution of suboptimal responses, the classification task becomes ill-posed and the policy may not converge to π*.

### Mechanism 2
- **Claim:** NCA prevents the likelihood of preferred responses from decreasing during training by optimizing absolute likelihood values rather than relative differences.
- **Mechanism:** NCA uses a binary classification objective that predicts the source (π* vs μ) of each individual response. The loss has two opposing forces: one increases likelihood for high-reward responses, the other decreases it for low-reward ones. This balances to maintain or increase likelihood for preferred responses.
- **Core assumption:** Optimizing absolute likelihood values rather than relative differences between responses will prevent the "likelihood collapse" observed in DPO/InfoNCA.
- **Evidence anchors:**
  - [abstract]: "NCA optimizes the absolute likelihood for each response, thereby effectively preventing the chosen likelihood from decreasing."
  - [section]: "NCA differs from InfoNCA by only loss definition and is also suitable for both preference and reward datasets. However, NCA is built on NCE...which optimizes the absolute data likelihood during training."
  - [corpus]: Weak - corpus doesn't provide empirical evidence of likelihood trajectories across different methods.
- **Break condition:** If the temperature parameter α is set too high or β too low, the regularization force may dominate and still cause likelihood decrease for preferred responses.

### Mechanism 3
- **Claim:** InfoNCA generalizes DPO to handle explicit reward datasets with multiple responses per prompt, subsuming DPO as a special case when K=2 and α→0.
- **Mechanism:** InfoNCA constructs a K-class classification problem where soft labels are computed from rewards via softmax. When K=2 and α→0, this reduces to the binary classification in DPO. This allows leveraging all responses in a reward dataset rather than pruning to pairwise preferences.
- **Core assumption:** The soft labels computed as eri/α/Σerj/α provide a valid probability distribution over responses that can be used for supervised learning of the reward model.
- **Evidence anchors:**
  - [abstract]: "We show that the DPO loss is a special case of our proposed InfoNCA objective under pairwise preference settings, thereby integrating and extending current alignment theories."
  - [section]: "InfoNCA subsumes DPO as a special case asymptotically. Specifically, setting response number K = 2 and reward temperature α → 0, we can fully recover the DPO objective."
  - [corpus]: Moderate - corpus shows InfoNCA outperforms DPO on reward datasets but doesn't explicitly demonstrate the K=2, α→0 reduction.
- **Break condition:** If the reward scale varies widely across examples, the softmax over rewards may produce extremely peaked distributions that make training unstable.

## Foundational Learning

- **Concept: Noise Contrastive Estimation (NCE)**
  - Why needed here: NCE provides the theoretical foundation for transforming generative modeling problems into classification tasks, which is the core insight behind both InfoNCA and NCA.
  - Quick check question: What is the key difference between NCE and InfoNCE in terms of the classification task they construct?

- **Concept: Contrastive Learning**
  - Why needed here: Both InfoNCA and NCA are instances of contrastive learning applied to policy alignment, where the model learns to distinguish optimal responses from suboptimal ones.
  - Quick check question: How does the contrastive objective in InfoNCA differ from standard contrastive learning used in representation learning?

- **Concept: Policy Optimization vs Reward Modeling**
  - Why needed here: Understanding the distinction between directly optimizing a policy (as in NCA) versus learning a reward model that then induces a policy (as in RL) is crucial for grasping why InfoNCA/NCA are more efficient.
  - Quick check question: In what way does the parameterization rθ = β log πθ/μ enable simultaneous reward modeling and policy optimization?

## Architecture Onboarding

- **Component map:**
  - Input: Instruction x and K responses {yi} with rewards ri
  - Encoder: Language model (μ) that generates response likelihoods
  - Reward Model: rθ(x,y) parameterized as β log πθ(y|x)/μ(y|x)
  - Loss Computation: InfoNCA or NCA loss based on soft labels from rewards
  - Optimizer: Updates θ to maximize likelihood of identifying optimal response(s)

- **Critical path:**
  1. Sample K responses for instruction x from μ and/or π*
  2. Compute rewards ri for each response
  3. Calculate soft labels from rewards: eri/α/Σerj/α
  4. Compute model logits: rθ(x,yi)
  5. Calculate cross-entropy or binary classification loss
  6. Backpropagate to update θ

- **Design tradeoffs:**
  - K vs. computational cost: Larger K provides better partition function estimates but increases computation
  - α vs. diversity: Smaller α focuses on optimal response only; larger α maintains diversity
  - β vs. stability: Larger β amplifies reward differences but may cause instability

- **Failure signatures:**
  - Likelihood collapse: All response likelihoods decrease despite training (indicates InfoNCA/DPO regime)
  - Mode collapse: Policy generates only one type of response regardless of input (indicates poor reward modeling)
  - Instability: Training loss oscillates or diverges (indicates inappropriate α or β values)

- **First 3 experiments:**
  1. Reproduce Table 2: Compare InfoNCA/NCA against DPO on UltraFeedback with K=4 responses
  2. Validate likelihood trajectories: Plot log likelihood of preferred responses during training for DPO vs NCA
  3. Test hyperparameter sensitivity: Sweep α and β for both methods on a small reward dataset to find stable regions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the choice of response number K and reward temperature α affect the performance of InfoNCA across different tasks and datasets?
- **Basis in paper:** [explicit] The paper states "α indicates a trade-off between diversity and optimality" and "K affects how accurately we can estimate the partition function"
- **Why unresolved:** While the paper provides some ablation studies, the relationship between K, α, and performance across various tasks and datasets is not fully explored
- **What evidence would resolve it:** Systematic experiments varying K and α across multiple tasks and datasets, showing the impact on performance and identifying optimal ranges for different scenarios

### Open Question 2
- **Question:** How does NCA's performance compare to other alignment methods like SLiC-HF in terms of preventing likelihood decrease and improving reasoning tasks?
- **Basis in paper:** [inferred] The paper discusses NCA's ability to prevent likelihood decrease and improve reasoning tasks, but doesn't compare it to methods like SLiC-HF
- **Why unresolved:** The paper focuses on comparing NCA to DPO and preference-based methods, but doesn't include comparisons to other methods that also aim to prevent likelihood decrease
- **What evidence would resolve it:** Experiments comparing NCA to SLiC-HF and other similar methods on tasks where likelihood decrease is a concern, measuring both performance and likelihood trends

### Open Question 3
- **Question:** What is the impact of suboptimal responses on alignment performance in different types of tasks (e.g., reasoning vs. instruction following)?
- **Basis in paper:** [explicit] The paper shows that suboptimal responses can be beneficial for policy training, contradicting the assumption that only the best response matters
- **Why unresolved:** While the paper demonstrates the benefit of suboptimal responses, it doesn't explore how this benefit varies across different types of tasks or why it occurs
- **What evidence would resolve it:** Detailed analysis of how suboptimal responses contribute to alignment in different task types, including qualitative studies of what information they provide and how it's utilized

## Limitations

- The paper doesn't provide empirical validation of long-term stability or examine whether NCA maintains its advantages over multiple epochs or under distribution shift
- The theoretical analysis assumes access to the partition function Z(x), which must be estimated from samples in practice, but the sensitivity to estimation error is not characterized
- The practical significance of InfoNCA's ability to handle K > 2 responses is uncertain, as the paper doesn't systematically compare performance across different values of K

## Confidence

**High Confidence:** The theoretical derivation connecting NCE/InfoNCE to LM alignment is sound, and the parameterization rθ = β log πθ/μ elegantly unifies reward modeling and policy optimization. The experimental results showing NCA's superiority on reasoning tasks are robust across multiple datasets.

**Medium Confidence:** The claim that NCA prevents likelihood decrease while InfoNCA/DPO do not is supported by the theoretical framework but lacks direct empirical validation through likelihood trajectory analysis. The mechanism by which absolute likelihood optimization prevents collapse is plausible but not definitively proven.

**Low Confidence:** The practical significance of InfoNCA's ability to handle K > 2 responses is uncertain, as the paper doesn't systematically compare performance across different values of K or against baseline methods that could also leverage multiple responses through response sampling strategies.

## Next Checks

1. **Likelihood trajectory validation:** Plot the log likelihood of preferred responses during training for DPO, InfoNCA, and NCA across multiple epochs to empirically verify the claim that NCA prevents likelihood collapse while the others do not.

2. **Partition function sensitivity analysis:** Measure the impact of K (number of responses) on model performance and training stability, particularly comparing K=2 (DPO regime) against K=4 (full InfoNCA regime) to quantify the practical benefit of handling multiple responses.

3. **Distribution shift robustness test:** Evaluate all three methods (DPO, InfoNCA, NCA) on held-out test sets from the same distribution and from different distributions to assess whether NCA's likelihood preservation translates to better generalization under distribution shift.