---
ver: rpa2
title: Reinforcement Learning Based Bidding Framework with High-dimensional Bids in
  Power Markets
arxiv_id: '2410.11180'
source_url: https://arxiv.org/abs/2410.11180
tags:
- bidding
- market
- power
- price
- supply
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of integrating high-dimensional
  bids (HDBs) into reinforcement learning (RL) based bidding strategies for power
  markets. Current RL methods primarily use low-dimensional bids, which limit bidding
  flexibility and profitability under increasing market uncertainties.
---

# Reinforcement Learning Based Bidding Framework with High-dimensional Bids in Power Markets

## Quick Facts
- arXiv ID: 2410.11180
- Source URL: https://arxiv.org/abs/2410.11180
- Reference count: 40
- One-line primary result: Proposed HDB bidding method improves performance by 15.40% and achieves 70.84%-88.41% of optimal market profit

## Executive Summary
This paper addresses the challenge of integrating high-dimensional bids (HDBs) into reinforcement learning (RL) based bidding strategies for power markets. Current RL methods primarily use low-dimensional bids, which limit bidding flexibility and profitability under increasing market uncertainties. The proposed solution introduces a novel framework that generates HDBs from neural network supply functions (NNSFs) and trains them using RL algorithms. Experiments on Energy Storage Systems (ESSs) in the PJM Real-Time power market demonstrate significant improvements over state-of-the-art methods.

## Method Summary
The framework employs Neural Network Supply Functions (NNSFs) to generate high-dimensional bids by mapping price inputs to power outputs. HDBs are extracted from NNSFs through a three-step process: monotonizing the supply curve, discretizing network outputs, and extracting price-power pairs. The method uses PPO for RL training with 2.5×10⁷ steps, leveraging market clearing price approximations to simplify the HDB generation process. The approach is validated on real PJM RT market data with ESS systems having ±1MW power capacity and varying storage (2-12MWh).

## Key Results
- 15.40% improvement in bidding performance compared to state-of-the-art RL methods using low-dimensional bids
- Achieved 70.84%-88.41% of optimal market profit, the highest reported profit ratio in RL-based RT energy market bidding literature
- Demonstrates effectiveness in PJM Real-Time power market with 10-dimensional bids for ESS systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Neural Network Supply Function (NNSF) provides a differentiable approximation of the HDB bidding curve that can be optimized via RL.
- Mechanism: The NNSF maps price inputs to power outputs, allowing RL agents to adjust bidding behavior by learning the NNSF parameters. This converts the high-dimensional HDB bidding problem into a low-dimensional continuous control problem.
- Core assumption: The NNSF can approximate any realistic supply curve with sufficient accuracy.
- Evidence anchors:
  - [abstract] "we employ a special type of neural network called Neural Network Supply Functions (NNSFs) to generate HDBs"
  - [section] "Using these observations, we define neural networks that have price inputs and power outputs as NNSFs"

### Mechanism 2
- Claim: The supply curve sampling and HDB extraction process preserves bidding flexibility while meeting market requirements.
- Mechanism: The algorithm samples the NNSF output across the price range to create a supply curve, then extracts monotonic, discrete-output HDBs through a discretization process that minimizes approximation error.
- Core assumption: The supply curve can be effectively discretized into N price-power pairs without significant loss of bidding strategy.
- Evidence anchors:
  - [abstract] "we extract N price-power pairs from NNSFs, satisfying market bidding requirements"
  - [section] "We propose a three-step process to generate HDBs from supply curves... First, we need to monotonize the supply curve... Second, we need to discretize the network output values..."

### Mechanism 3
- Claim: The RL training framework approximates market clearing results using NNSF outputs at market clearing prices, enabling efficient policy learning.
- Mechanism: Instead of computing full HDBs and market clearing, the framework uses the NNSF output at the actual market clearing price as an approximation of the cleared power, creating a state-action-reward MDP structure.
- Core assumption: The NNSF output at market clearing price approximates the actual cleared power with acceptable error.
- Evidence anchors:
  - [abstract] "we embed the NNSF into a Markov Decision Process (MDP) to make it compatible with most existing RL methods"
  - [section] "Leveraging such an approximation, the HDB generation framework can be simplified to the NNSF training framework"

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The bidding problem is formulated as an MDP where states are market observations and prices, actions are HDBs, and rewards are market profits
  - Quick check question: What are the four components of an MDP tuple (S, A, R, T, γ) and how do they map to the bidding problem?

- Concept: Neural network function approximation
  - Why needed here: NNSFs are used to represent supply curves that can be optimized via RL, requiring understanding of how neural networks approximate functions
  - Quick check question: Why are neural networks particularly suitable for representing supply curves compared to other function approximation methods?

- Concept: Reinforcement learning policy gradient methods
  - Why needed here: The PPO algorithm is used to train the NNSF, requiring understanding of policy gradient optimization
  - Quick check question: What is the key difference between policy gradient methods and value-based RL methods in terms of what they directly optimize?

## Architecture Onboarding

- Component map: NNSF (policy network) → Supply curve sampling → HDB extraction → Market clearing → Reward computation → PPO update
- Critical path: NNSF training → Supply curve generation → HDB extraction → Market simulation → Reward calculation → Policy update
- Design tradeoffs: High-dimensional HDBs provide flexibility but increase computational complexity; simplified approximations enable efficient RL training but may introduce errors
- Failure signatures: Poor convergence in training curves, large gaps between NNSF and HDB performance, failure to maintain zero output in ESS bidding
- First 3 experiments:
  1. Verify supply curve sampling produces monotonic outputs for simple NNSF configurations
  2. Test HDB extraction accuracy by comparing extracted HDBs against known supply curves
  3. Validate RL training convergence with simplified single-price market simulations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed framework scale with market dimensionality beyond the 10-dimensional bids used in the PJM market?
- Basis in paper: [explicit] The paper demonstrates effectiveness in the PJM market with N=10, but scalability to markets with higher or lower dimensions is not tested.
- Why unresolved: The paper only tests on a single market with fixed bid dimensions, limiting generalizability.
- What evidence would resolve it: Testing the framework on markets with varying bid dimensions (e.g., N=5, N=20) and comparing performance degradation or improvement.

### Open Question 2
- Question: What is the impact of different neural network architectures on the performance of the NNSF and subsequent HDB generation?
- Basis in paper: [inferred] The paper uses a standard MLP with two hidden layers but does not explore alternative architectures like CNNs, RNNs, or Transformers.
- Why unresolved: The paper assumes MLP is sufficient without comparative analysis of other architectures.
- What evidence would resolve it: Systematic comparison of NNSF performance using different neural network architectures under identical conditions.

### Open Question 3
- Question: How does the framework perform under extreme market conditions (e.g., price spikes or collapses) that were not well-represented in the training data?
- Basis in paper: [explicit] The paper acknowledges that extreme price ranges are difficult to train and that the monotonize step handles these cases by overwriting actions.
- Why unresolved: The paper does not quantify the framework's robustness to extreme conditions or provide strategies for improving performance in such scenarios.
- What evidence would resolve it: Testing the framework on synthetic or historical extreme market scenarios and measuring bidding performance degradation.

## Limitations
- The framework's performance relies heavily on the approximation quality of NNSF outputs at market clearing prices, which may introduce errors across diverse market conditions
- Discretization process for HDB extraction could introduce significant errors if not properly tuned, particularly for ESS systems requiring precise zero-output behavior
- Results are validated only on PJM RT market data and may not generalize to other market structures with different bid dimension requirements

## Confidence
- **High confidence**: The mathematical framework for NNSF-based HDB generation and the PPO training methodology are well-established and clearly described
- **Medium confidence**: The empirical performance claims (15.40% improvement, 70.84%-88.41% of optimal profit) are supported by the PJM RT market experiments, but results may not generalize to other market structures
- **Medium confidence**: The claim of highest reported profit ratio in RL-based RT energy market bidding literature is supported by the paper's comprehensive comparison, though literature coverage may be incomplete

## Next Checks
1. **Discretization error analysis**: Measure the approximation error between extracted HDBs and the original NNSF supply curves across different market scenarios to quantify the fidelity loss in the three-step HDB extraction process.
2. **Market condition robustness**: Test the framework on synthetic market conditions with varying price volatility and correlation structures to evaluate performance outside the PJM RT market domain used in the experiments.
3. **Computation overhead validation**: Benchmark the actual computational cost of generating and processing HDBs versus the theoretical benefits, particularly measuring the trade-off between bidding flexibility and real-time market clearing constraints.