---
ver: rpa2
title: On Local Overfitting and Forgetting in Deep Neural Networks
arxiv_id: '2412.12968'
source_url: https://arxiv.org/abs/2412.12968
tags:
- training
- test
- data
- learning
- overfitting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new perspective on overfitting in deep
  neural networks by proposing the "forget fraction" metric, which quantifies the
  portion of validation data that the model initially classifies correctly but misclassifies
  during training. The authors show that local overfitting occurs even when traditional
  overfitting (decrease in test accuracy) is absent, and that larger models and smaller
  training sets exacerbate this phenomenon.
---

# On Local Overfitting and Forgetting in Deep Neural Networks

## Quick Facts
- arXiv ID: 2412.12968
- Source URL: https://arxiv.org/abs/2412.12968
- Reference count: 21
- This paper introduces a new perspective on overfitting in deep neural networks by proposing the "forget fraction" metric, which quantifies the portion of validation data that the model initially classifies correctly but misclassifies during training.

## Executive Summary
This paper introduces a novel perspective on overfitting in deep neural networks by proposing the "forget fraction" metric, which quantifies the portion of validation data that the model initially classifies correctly but misclassifies during training. The authors show that local overfitting occurs even when traditional overfitting (decrease in test accuracy) is absent, and that larger models and smaller training sets exacerbate this phenomenon. To address this issue, they propose KnowledgeFusion (KF), a novel ensemble method that combines checkpoints from different training stages based on the forget metric, demonstrating consistent improvements across multiple datasets and architectures.

## Method Summary
The paper introduces the forget fraction metric to quantify local overfitting, showing that models can forget previously correct classifications even while improving overall accuracy. The authors analyze over-parameterized deep linear networks to theoretically characterize forgotten knowledge, finding correlations with real deep networks. Their proposed KnowledgeFusion method iteratively selects checkpoints based on the forget metric and combines them with weighted averages. When combined with self-distillation, KF improves model performance without adding inference costs, achieving approximately 1% accuracy gains on ImageNet and 15% error reduction in noisy settings.

## Key Results
- Forget fraction (Fe) metric reveals local overfitting occurs even when traditional overfitting metrics show no degradation
- Deep linear network analysis shows points with substantial projections onto leading principal components are more likely to be forgotten early
- KnowledgeFusion (KF) ensemble method consistently improves accuracy by around 1% on ImageNet and reduces errors by approximately 15% in cases of 10% asymmetric noise
- KF is particularly effective in challenging scenarios and transfer learning settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local overfitting occurs even when overall validation accuracy is improving.
- Mechanism: The forget fraction (Fe) captures regions of the data space where the model's performance degrades during training, despite global accuracy gains. This indicates that some test data points are "forgotten" as training progresses.
- Core assumption: Fe accurately quantifies local overfitting and is independent of traditional overfitting metrics.
- Evidence anchors:
  - [abstract] "We posit that this score quantifies local overfitting: a decline in performance confined to certain regions of the data space."
  - [section 3] "Our empirical investigation also reveals that forgetting of patterns occurs alongside the learning of new patterns in the training set, explaining why the traditional definition of overfitting fails to capture this phenomenon."
  - [corpus] Weak evidence; no directly related work on forget fraction or local overfitting.

### Mechanism 2
- Claim: Knowledge forgotten in deep linear models correlates with forgotten knowledge in real deep models.
- Mechanism: Analysis of deep linear models reveals that data points with substantial projections onto leading principal components are more likely to be forgotten early in training. This theoretical finding aligns with empirical observations in real deep networks.
- Core assumption: The deep linear model framework provides a valid approximation for understanding forgetting in real deep networks.
- Evidence anchors:
  - [section 4] "Using the framework of over-parameterized deep linear networks, we offer a certain theoretical characterization of forgotten knowledge, and show that it correlates with knowledge forgotten by real deep models."
  - [corpus] Weak evidence; no directly related work on PCA-based analysis of forgetting in deep linear models.

### Mechanism 3
- Claim: The KnowledgeFusion (KF) ensemble method recovers forgotten knowledge and improves model performance.
- Mechanism: KF iteratively selects checkpoints from different stages of training based on the forget metric and combines them with weighted averages. This recovers knowledge lost in later stages of training without adding inference costs.
- Core assumption: The forget metric effectively identifies checkpoints that contain useful forgotten knowledge.
- Evidence anchors:
  - [abstract] "Finally, we devise a new ensemble method that aims to recover forgotten knowledge, relying solely on the training history of a single network."
  - [section 6] "Extensive empirical evaluations demonstrate the efficacy of our method across multiple datasets, contemporary neural network architectures, and training protocols."
  - [corpus] Weak evidence; no directly related work on KF or similar ensemble methods based on forget metric.

## Foundational Learning

- Concept: Principal Component Analysis (PCA)
  - Why needed here: Understanding PCA is crucial for grasping the theoretical analysis of forgotten knowledge in deep linear models, where data points with substantial projections onto leading principal components are more likely to be forgotten early.
  - Quick check question: What is the relationship between PCA and the forget time of data points in deep linear models?

- Concept: Ensemble learning
  - Why needed here: The KnowledgeFusion method relies on ensemble learning principles to combine checkpoints from different stages of training and recover forgotten knowledge.
  - Quick check question: How does the KF method differ from traditional ensemble learning approaches?

- Concept: Gradient descent optimization
  - Why needed here: The theoretical analysis of forgotten knowledge in deep linear models is based on the behavior of gradient descent optimization.
  - Quick check question: How does the learning rate affect the forget time of data points in deep linear models?

## Architecture Onboarding

- Component map:
  - Data preprocessing: Standard image classification datasets (CIFAR-100, TinyImagenet, Imagenet)
  - Model training: Various neural network architectures (ResNet, ConvNeXt, ViT, MaxViT) with different optimizers and learning rate schedulers
  - Forget fraction calculation: Monitor validation accuracy at each epoch and identify forgotten data points
  - KnowledgeFusion ensemble: Select checkpoints based on forget metric and combine with weighted averages
  - Self-distillation (optional): Train a single student model to replicate ensemble predictions

- Critical path:
  1. Train neural network and monitor validation accuracy at each epoch
  2. Calculate forget fraction (Fe) to identify forgotten data points
  3. Select checkpoints based on forget metric
  4. Combine checkpoints using weighted averages (KF method)
  5. (Optional) Apply self-distillation to obtain a single model

- Design tradeoffs:
  - Using a larger window of checkpoints around the chosen epoch vs. using a single checkpoint
  - Including self-distillation to reduce inference costs vs. using the ensemble directly
  - Using a separate validation set for hyper-parameter tuning vs. using the training set

- Failure signatures:
  - KF method does not improve performance over the original model
  - Forget fraction does not consistently identify forgotten knowledge
  - Ensemble method leads to increased inference costs without performance gains

- First 3 experiments:
  1. Train a simple neural network (e.g., ResNet-18) on CIFAR-100 and calculate the forget fraction (Fe) to observe local overfitting.
  2. Apply the KnowledgeFusion method to the trained network and compare performance with the original model.
  3. Investigate the effect of using different window sizes around the chosen epoch on the KF method's performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the forgetting phenomenon occur in architectures beyond those tested, such as Vision Transformers with different attention mechanisms or Graph Neural Networks?
- Basis in paper: [explicit] The paper demonstrates forgetting across ConvNets (ResNet, ConvNeXt), Visual transformers (ViT, MaxViT), and various datasets, but acknowledges this is not exhaustive.
- Why unresolved: The paper focuses on image classification tasks; broader architectural exploration is needed.
- What evidence would resolve it: Empirical studies showing forgetting rates across diverse architectures (e.g., Transformers with different attention, GNNs) and domains (e.g., NLP, speech).

### Open Question 2
- Question: What is the theoretical relationship between the deep linear network model's forgotten knowledge and real deep networks across different loss landscapes and optimization dynamics?
- Basis in paper: [explicit] The paper correlates forgetting in deep linear models with real deep networks but acknowledges the analysis is specific to linear models.
- Why unresolved: The analysis is limited to over-parameterized deep linear networks; complex non-linear dynamics in real networks are not fully characterized.
- What evidence would resolve it: Theoretical proofs or extensive empirical studies linking linear model forgetting to non-linear networks across varied loss landscapes and optimizers.

### Open Question 3
- Question: How does the forget fraction metric behave in continual learning scenarios where training data evolves over time, as opposed to the static training considered in the paper?
- Basis in paper: [explicit] The paper explicitly excludes catastrophic forgetting scenarios and focuses on static training data.
- Why unresolved: The paper does not explore dynamic data distributions or task-switching scenarios.
- What evidence would resolve it: Experiments measuring forget fraction in continual learning benchmarks (e.g., permuted MNIST, class-incremental learning) and analysis of its correlation with catastrophic forgetting.

## Limitations
- The theoretical analysis in deep linear networks may not fully capture the complexities of real deep networks with non-linear activations
- The effectiveness of KnowledgeFusion method depends heavily on proper checkpoint selection and weighting, which could be sensitive to hyperparameter choices
- The forget fraction metric needs more extensive testing across diverse architectures and datasets to establish its reliability as a general indicator of local overfitting

## Confidence

- **High Confidence**: The existence of local overfitting as a distinct phenomenon (independent of traditional overfitting metrics) - supported by consistent empirical observations across multiple datasets and architectures.
- **Medium Confidence**: The theoretical characterization of forgotten knowledge in deep linear networks - while logically sound, the correlation with real deep networks needs more rigorous validation.
- **Medium Confidence**: The efficacy of KnowledgeFusion method - demonstrated through extensive experiments, but results may be sensitive to specific implementation details and hyperparameters.

## Next Checks

1. **Robustness Analysis**: Test the forget fraction metric and KF method across a wider range of architectures (e.g., transformers, MLPs) and datasets to assess generalizability.
2. **Ablation Study**: Systematically vary the window size (w) and self-distillation parameters to quantify their impact on KF performance and identify optimal configurations.
3. **Theoretical Extension**: Extend the deep linear network analysis to include different activation functions or architectural components to better understand the relationship between linear and non-linear forgetting patterns.