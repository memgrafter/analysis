---
ver: rpa2
title: 'ReTok: Replacing Tokenizer to Enhance Representation Efficiency in Large Language
  Model'
arxiv_id: '2410.04335'
source_url: https://arxiv.org/abs/2410.04335
tags:
- tokenizer
- training
- data
- parameters
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ReTok, a method to improve the representation
  and processing efficiency of large language models (LLMs) by replacing their tokenizers.
  The authors first expand the vocabulary of a high-compression tokenizer (Llama3)
  by incorporating Chinese lexicons to enhance its compression rate for Chinese text.
---

# ReTok: Replacing Tokenizer to Enhance Representation Efficiency in Large Language Model

## Quick Facts
- arXiv ID: 2410.04335
- Source URL: https://arxiv.org/abs/2410.04335
- Reference count: 16
- Authors: Shuhao Gu, Mengdi Zhao, Bowen Zhang, Liangdong Wang, Jijie Li, Guang Liu
- Key outcome: Method improves representation and processing efficiency of LLMs by replacing tokenizers while maintaining model performance

## Executive Summary
This paper proposes ReTok, a method to improve the representation and processing efficiency of large language models (LLMs) by replacing their tokenizers. The authors expand the vocabulary of high-compression tokenizers by incorporating domain-specific lexicons, then replace the model's input and output layers while initializing new parameters using the corresponding parameters from the original model. Experiments on Qwen1.5-0.5B, Aquila2-7B, and Llama3-8B models demonstrate that ReTok maintains original model performance while significantly improving decoding speed for long texts.

## Method Summary
The ReTok method involves three main steps: (1) expanding the vocabulary of the original tokenizer by integrating domain-specific lexicons using byte-level BPE, (2) replacing and reinitializing the input (embedding) and output (LM head) layers of the original model, and (3) training the reinitialized layers while keeping other model parameters fixed. The new parameters are initialized using corresponding parameters from the original model when tokens overlap between vocabularies, or by averaging corresponding parameters for new tokens. The approach aims to maintain model performance while improving compression rates and inference speed.

## Key Results
- Maintains original model performance on comprehension and generative tasks
- Significantly improves decoding speed for long texts
- Reduces average input/output sequence lengths through better compression rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReTok maintains model performance by preserving learned representations through parameter initialization
- Mechanism: Input and output layers are initialized with corresponding parameters from the original model, leveraging existing learned representations
- Core assumption: Effective token correspondence can be established between new and original vocabularies
- Evidence anchors: [abstract] "replacing and reinitializing the parameters of the model's input and output layers with the parameters of the original model"; [section] initialization process for overlapping tokens

### Mechanism 2
- Claim: ReTok improves compression rate by expanding vocabulary for specific languages
- Mechanism: Vocabulary expansion with domain-specific lexicons reduces average tokens needed to represent text
- Core assumption: New tokens learned from expanded corpus have higher frequency in target domain
- Evidence anchors: [abstract] "quickly improve its compression rate for Chinese by expanding the vocabulary"; [section] byte-level BPE vocabulary learning and merging

### Mechanism 3
- Claim: ReTok improves inference speed by reducing sequence length
- Mechanism: Better compression rates reduce tokens processed during inference, translating to faster decoding
- Core assumption: Inference speed is proportional to number of tokens processed
- Evidence anchors: [abstract] "significantly reducing the inference time"; [section] impact of input/output length on training and inference costs

## Foundational Learning

- **Concept**: Byte-level BPE tokenization
  - Why needed here: Essential for understanding how original tokenizer works for effective vocabulary expansion
  - Quick check question: What is the difference between byte-level BPE and standard BPE tokenization?

- **Concept**: Parameter initialization in neural networks
  - Why needed here: Method relies on proper initialization of new parameters based on existing ones
  - Quick check question: How does parameter initialization affect convergence speed and final performance?

- **Concept**: Catastrophic forgetting in model adaptation
  - Why needed here: Approach freezes most model parameters to prevent catastrophic forgetting
  - Quick check question: What strategies can be used to prevent catastrophic forgetting when adapting pre-trained models?

## Architecture Onboarding

- **Component map**: Text → Tokenizer → Embedding layer → (frozen) Transformer → LM head → Tokenizer → Text
- **Critical path**: Text → Tokenizer → Embedding layer → (frozen) Transformer → LM head → Tokenizer → Text
- **Design tradeoffs**:
  - Training efficiency vs. performance: Only training input/output layers vs. full fine-tuning
  - Vocabulary size vs. compression rate: Larger vocabulary improves compression but increases memory usage
  - Language coverage vs. specialization: General-purpose vs. domain-specific tokenizer
- **Failure signatures**:
  - Performance degradation: Likely due to poor parameter initialization or insufficient training
  - No speed improvement: May indicate minimal compression rate improvement
  - Training instability: Could suggest poor token correspondence between vocabularies
- **First 3 experiments**:
  1. Verify compression rate improvement on held-out validation data
  2. Compare validation loss curves between ReTok and random initialization
  3. Measure inference speed on long text sequences for both original and ReTok configurations

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the optimal ratio of Chinese to English data in the training set for ReTok to maximize performance across both languages?
  - Basis in paper: [inferred] Model performance varied across test sets, potentially due to imbalanced data ratios
  - Why unresolved: Combined dataset used without analyzing impact of different data ratios
  - What evidence would resolve it: Experiments with varying Chinese-to-English data ratios measuring performance on both languages

- **Open Question 2**: How does ReTok perform on multilingual tasks beyond Chinese and English, and what are the limits of its cross-linguistic capabilities?
  - Basis in paper: [explicit] Paper acknowledges not testing multilingual abilities beyond Chinese and English
  - Why unresolved: Experiments limited to Chinese, English, code, and math tasks
  - What evidence would resolve it: Testing on diverse multilingual benchmarks compared to native multilingual tokenizers

- **Open Question 3**: What is the impact of different data sources on ReTok's performance, and how do they contribute to model's overall effectiveness?
  - Basis in paper: [inferred] Multiple data sources used without analyzing individual impact
  - Why unresolved: Combined dataset used without examining source-specific effects
  - What evidence would resolve it: Ablation studies training on subsets from different sources measuring performance changes

## Limitations

- Vocabulary expansion mechanism primarily demonstrated for Chinese language with limited evidence for generalization to other languages
- Initialization method relies on token correspondence but doesn't fully address cases where tokens have different semantic meanings across languages
- Insufficient evidence showing compression rate improvements translate to meaningful inference speed gains

## Confidence

- **High Confidence**: Basic approach of replacing tokenizer vocabularies and reinitializing input/output layers is technically sound and well-grounded in existing literature
- **Medium Confidence**: Claim that approach maintains model performance while improving compression rates has reasonable theoretical support but limited empirical validation
- **Low Confidence**: Specific claims about significant inference speed improvements for long texts lack sufficient direct evidence

## Next Checks

1. **Compression Rate Validation**: Measure actual byte-per-token ratio on held-out validation data for both original and ReTok configurations across multiple languages and domains

2. **Performance Stability Analysis**: Compare validation loss curves during training for ReTok versus random initialization of input/output layers

3. **Inference Speed Benchmarking**: Conduct controlled experiments measuring end-to-end inference time on long text sequences (5K+ tokens) for both original and ReTok models, including attention computation time and token processing time measurements