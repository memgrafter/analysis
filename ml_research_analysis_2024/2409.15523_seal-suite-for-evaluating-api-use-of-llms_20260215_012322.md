---
ver: rpa2
title: 'SEAL: Suite for Evaluating API-use of LLMs'
arxiv_id: '2409.15523'
source_url: https://arxiv.org/abs/2409.15523
tags: []
core_contribution: This paper introduces SEAL, a comprehensive testbed for evaluating
  large language models' (LLMs) API-use capabilities. The authors identify key limitations
  in existing benchmarks, including lack of generalizability, poor coverage of multi-step
  reasoning, and instability due to real-time API fluctuations.
---

# SEAL: Suite for Evaluating API-use of LLMs

## Quick Facts
- arXiv ID: 2409.15523
- Source URL: https://arxiv.org/abs/2409.15523
- Reference count: 12
- Primary result: Introduces SEAL testbed addressing limitations in existing LLM API-use benchmarks through standardized evaluation, agent-based system, and GPT-4-powered API simulator with caching

## Executive Summary
SEAL addresses critical limitations in existing LLM API-use benchmarks, including lack of generalizability, poor coverage of multi-step reasoning, and instability from real-time API fluctuations. The testbed standardizes diverse benchmarks, integrates an agent system for API retrieval and planning, and introduces a GPT-4-powered API simulator with caching for deterministic evaluations. Results demonstrate that as API pool size increases, performance in API retrieval and call accuracy decreases, while pass rates remain relatively stable, suggesting models can generate reasonable responses despite retrieval and call errors.

## Method Summary
SEAL employs a comprehensive evaluation pipeline built on the AutoGen agent framework, separating API retrieval, planning, and execution into distinct agents. The system uses embedding models and vector similarity search for API retrieval, while GPT-4 simulates API responses with caching to ensure deterministic evaluation. The evaluation covers three stages: API retrieval (measured by recall@10), API calls (measured by recall@N and parameter accuracy), and final responses (measured by pass rate). The testbed standardizes multiple existing benchmarks including ToolBench, APIGen, AnyTool, MetaTool, and APIBench, providing a unified platform for comprehensive API-use capability assessment.

## Key Results
- Performance in API retrieval and call accuracy decreases as API pool size increases, demonstrating growing task complexity
- Pass rate remains relatively stable despite decreasing retrieval and call accuracy, suggesting models can generate reasonable responses even with retrieval errors
- Common error cases include incorrect API retrieval, incorrect API parameters, and inconsistent evaluation due to LLM stochasticity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The API simulator with caching addresses instability caused by real-time API fluctuations
- Mechanism: By using GPT-4 to simulate API responses and caching all responses, the evaluation becomes deterministic and reproducible
- Core assumption: GPT-4 can generate plausible API responses that match real API behavior for the purpose of benchmarking
- Evidence anchors:
  - [abstract] "we developed an API simulator powered by GPT-4 [Achiam et al., 2023] to generate plausible API responses, building on the approach in StableToolbench [Guo et al., 2024]. We further enhance this system with caching to enable more deterministic evaluations"
  - [section] "we fully replace and cache all API responses with simulations to minimize stochastic variability"
- Break condition: If GPT-4 cannot generate realistic API responses for certain edge cases, or if the simulator introduces significant bias in the evaluation

### Mechanism 2
- Claim: The agent-based system with separated components improves evaluation of different stages of API usage
- Mechanism: By separating API retrieval, planning, and execution into distinct agents, the system can evaluate each component independently and allow swapping of different models
- Core assumption: The separation of concerns between retrieval, planning, and execution is beneficial for both evaluation and model development
- Evidence anchors:
  - [section] "we argue that API-calling, and tool-calling in general, can be tackled more effectively using an agent-based system, where multiple agents collaborate, with some powered by LLMs"
  - [section] "This design of separation allows for testing both tool retrieval and tool planning methods, enabling users to easily swap different agents in and out"
- Break condition: If the communication overhead between agents becomes too high, or if the separation creates inefficiencies that outweigh the benefits

### Mechanism 3
- Claim: Comprehensive evaluation pipeline covering all stages provides more reliable performance assessment
- Mechanism: By evaluating API retrieval, API calls, and final responses separately, the system can identify specific failure points and provide granular performance metrics
- Core assumption: Multi-faceted evaluation is necessary to understand the strengths and weaknesses of LLM API-use capabilities
- Evidence anchors:
  - [section] "For a comprehensive evaluation of API-calling systems, it is essential to assess each stage of the pipeline: whether the correct tools are retrieved, whether the correct tools are called, whether the tool calls are accurate, and finally, whether the query response is correct"
  - [section] "We argue that a more comprehensive evaluation is needed, spanning the entire pipeline of API usage, including API retrieval, API calls, and the final response"
- Break condition: If the evaluation becomes too complex or time-consuming to be practical, or if the separate metrics don't correlate well with overall system performance

## Foundational Learning

- Concept: API usage workflow in LLM systems
  - Why needed here: Understanding the three-stage process (retrieval → call → response) is essential for implementing and evaluating the SEAL testbed
  - Quick check question: What are the three main components of an API-using LLM system, and what happens in each component?

- Concept: Vector similarity search for API retrieval
  - Why needed here: The API retriever uses embedding models and vector lookups to find relevant APIs from a large pool
  - Quick check question: How does the API retriever use embedding models and vector lookup libraries like Faiss to find relevant APIs?

- Concept: AutoGen agent framework
  - Why needed here: SEAL is built on AutoGen, which provides the multi-agent conversation framework for the testbed
  - Quick check question: What are the key features of AutoGen that make it suitable for building the SEAL agent system?

## Architecture Onboarding

- Component map: User Interface -> Query Processor -> API Retriever -> Agent System (Retriever, Executor, Execution Manager) -> API Simulator -> Evaluation Engine -> Data Store

- Critical path: Query → API Retriever → Agent System → API Simulator → Response → Evaluation

- Design tradeoffs:
  - Using GPT-4 for simulation provides realism but adds cost and potential bias
  - Caching responses ensures determinism but may not capture API variability
  - Separating agents allows flexibility but adds communication overhead

- Failure signatures:
  - High variance in evaluation results indicates instability in the simulator or benchmark
  - Low API retrieval recall suggests embedding model or vector search issues
  - Inconsistent final response evaluation points to stochastic LLM critique

- First 3 experiments:
  1. Run SEAL on a small benchmark (10 APIs) to verify basic functionality and check that all components work together
  2. Test the API simulator with a known API to verify that it generates plausible responses and that caching works correctly
  3. Evaluate a simple LLM on a single-step query to verify that the evaluation pipeline computes all metrics correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of API retrievers trained on one benchmark generalize to other benchmarks with different API sources?
- Basis in paper: [explicit] The paper discusses the lack of generalizability in API retrievers, showing that retrievers trained on ToolBench perform poorly on MetaTool, which uses OpenAI plugins instead of RapidAPI.
- Why unresolved: The paper only provides a limited comparison of embedding models' performance across benchmarks, and does not conduct extensive experiments to evaluate the generalization capabilities of API retrievers across different API sources.
- What evidence would resolve it: Comprehensive experiments comparing the performance of API retrievers trained on one benchmark when applied to other benchmarks with different API sources, using various embedding models and evaluation metrics.

### Open Question 2
- Question: How does the performance of LLMs in API retrieval and planning scale with the size of the API pool?
- Basis in paper: [explicit] The paper mentions that as the API pool size increases, performance in API retrieval and call accuracy decreases, demonstrating the task's growing complexity.
- Why unresolved: The paper only provides results for a limited range of API pool sizes (10, 50, 100, 200, 500) and does not explore the scalability of LLM performance in API retrieval and planning for larger API pools.
- What evidence would resolve it: Experiments evaluating the performance of LLMs in API retrieval and planning as the API pool size increases beyond 500, using various metrics and benchmarks.

### Open Question 3
- Question: How can the inconsistency in LLM-based evaluation be mitigated to ensure more reliable and reproducible results?
- Basis in paper: [inferred] The paper identifies inconsistent evaluation as a major error case, where LLMs serve as both the solver and evaluator, leading to different outcomes when re-evaluating the same query.
- Why unresolved: The paper does not propose specific solutions to mitigate the inconsistency in LLM-based evaluation, and only mentions the need for multi-faceted evaluation beyond relying solely on final output assessments.
- What evidence would resolve it: Development and evaluation of methods to improve the consistency of LLM-based evaluation, such as using multiple LLMs for evaluation, incorporating human feedback, or developing more robust evaluation metrics.

## Limitations

- The GPT-4 API simulator, while addressing instability, may introduce bias if it cannot accurately capture all possible API behaviors and edge cases
- The inherent stochasticity of LLM-based evaluation remains a challenge, with inconsistent outcomes when re-evaluating the same query
- The separation of agents into distinct components adds communication overhead and may create inefficiencies that outweigh the benefits of modularity

## Confidence

- High confidence in the core mechanism of API simulation with caching for deterministic evaluation
- Medium confidence in the GPT-4 simulator's ability to accurately represent all possible API behaviors
- Medium confidence in the stability of the final response evaluation due to LLM stochasticity

## Next Checks

1. **Simulator Fidelity Assessment**: Conduct a systematic comparison between GPT-4-simulated API responses and actual API responses for a subset of real APIs to quantify the simulator's accuracy and identify potential bias patterns.

2. **Cross-Benchmark Consistency**: Evaluate the same models across SEAL and existing benchmarks (ToolBench, APIGen, etc.) to verify that the standardized benchmarks produce comparable results and that the observed performance differences are meaningful rather than artifacts of evaluation methodology.

3. **Scaling Behavior Validation**: Test the claimed relationship between API pool size and performance degradation across multiple model families to confirm that the observed trend is consistent and not specific to particular architectures or training approaches.