---
ver: rpa2
title: 'MathDivide: Improved mathematical reasoning by large language models'
arxiv_id: '2405.13004'
source_url: https://arxiv.org/abs/2405.13004
tags:
- arxiv
- technique
- problem
- mathematical
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MathDivide, a novel prompting technique that
  enhances the mathematical reasoning capabilities of large language models (LLMs)
  by breaking complex problems into simpler subproblems. Each subproblem is formulated
  as an algebraic expression and solved using Python code generated by the LLM, with
  the final answer composed from these solutions.
---

# MathDivide: Improved mathematical reasoning by large language models

## Quick Facts
- arXiv ID: 2405.13004
- Source URL: https://arxiv.org/abs/2405.13004
- Authors: Saksham Sahai Srivastava; Ashutosh Gandhi
- Reference count: 33
- Primary result: MathDivide achieves higher accuracy than Mathprompter on GSM8K using both proprietary and open-source models

## Executive Summary
This paper introduces MathDivide, a novel prompting technique that enhances the mathematical reasoning capabilities of large language models (LLMs) by breaking complex problems into simpler subproblems. Each subproblem is formulated as an algebraic expression and solved using Python code generated by the LLM, with the final answer composed from these solutions. If the answer is incorrect, a refinement prompt based on human feedback is used to improve accuracy. Evaluated on the GSM8K dataset using both proprietary models (GPT-3.5-turbo, GPT-4) and open-source models (Llama2, Llama3), MathDivide significantly outperformed the state-of-the-art Mathprompter technique, achieving higher accuracy across all models.

## Method Summary
MathDivide is a prompting technique that improves LLM mathematical reasoning through problem decomposition. The method breaks down complex problems into simpler subproblems, formulates each as an algebraic expression, and uses LLM-generated Python code to compute values. The final answer is composed from subproblem solutions. If incorrect, up to 3 refinement loops with human feedback prompts are applied. The approach was tested on 250 GSM8K problems using GPT-3.5-turbo, GPT-4, Llama2-7B, and Llama3-8B models.

## Key Results
- MathDivide outperformed Mathprompter baseline on GSM8K dataset
- Accuracy improvements observed across both proprietary and open-source models
- Refinement loops with human feedback further improved accuracy for incorrect responses
- Significant performance gains achieved through structured problem decomposition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing complex problems into smaller subproblems improves LLM accuracy by reducing cognitive load and enabling step-by-step verification.
- Mechanism: The LLM first identifies subproblems, formulates algebraic expressions for each, and generates Python code to compute values. By solving subproblems sequentially and composing their results, errors are localized and easier to correct.
- Core assumption: Smaller subproblems are easier for the LLM to solve correctly than the entire problem at once.
- Evidence anchors:
  - [abstract] "breaks down the mathematical problem into simpler subproblems"
  - [section] "Each of the subproblems is formulated as an algebraic expression whose value is evaluated by the Python code generated by the LLM"
  - [corpus] Weak - no direct neighbor evidence, but the claim aligns with general decomposition strategies in LLM reasoning literature.
- Break condition: If subproblems are not truly simpler or are incorrectly identified, decomposition could introduce compounding errors rather than reduce them.

### Mechanism 2
- Claim: Using Python code for subproblem evaluation ensures computational precision and prevents LLM arithmetic errors.
- Mechanism: For each algebraic expression, the LLM generates executable Python code that performs the calculation. The numerical results are then fed back to the LLM, decoupling reasoning from arithmetic.
- Core assumption: LLMs are prone to arithmetic mistakes but can generate correct code for evaluation.
- Evidence anchors:
  - [abstract] "Each of the subproblems is formulated as an algebraic expression whose value is evaluated by the Python code generated by the LLM"
  - [section] "Generate Python code that computes the numerical value of ei... Execute the Python code to solve the sub-problem"
  - [corpus] No direct neighbor evidence; this appears to be a novel integration in this work.
- Break condition: If the generated Python code is syntactically incorrect or semantically misaligned with the intended calculation, the entire approach fails.

### Mechanism 3
- Claim: Iterative refinement using human feedback improves final accuracy by correcting specific error patterns.
- Mechanism: After initial computation, the final answer is compared to the correct answer. If incorrect, a refinement prompt highlights the specific subproblem(s) where the LLM erred, enabling targeted correction.
- Core assumption: Targeted feedback on specific subproblems allows the LLM to learn and correct its reasoning process.
- Evidence anchors:
  - [abstract] "If the final answer is incorrect, a refinement prompt based on human feedback is used to improve accuracy"
  - [section] "If the final answer does not match the gold correct answer then we provide a refinement prompt to the LLM highlighting which subproblem(s) was incorrectly solved"
  - [corpus] Weak - while refinement is mentioned in neighbors, specific human-feedback-driven correction is not detailed.
- Break condition: If the LLM fails to recognize the feedback or repeatedly makes the same error, refinement loops may not converge to the correct answer.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: Provides the LLM with a structured reasoning framework, encouraging step-by-step problem decomposition rather than direct answer prediction.
  - Quick check question: What does chain-of-thought prompting help the LLM do differently compared to direct answer prompting?

- Concept: In-context learning
  - Why needed here: Enables the LLM to learn from the refinement prompts without parameter updates, adapting its approach based on feedback.
  - Quick check question: How does in-context learning differ from fine-tuning in terms of model adaptation?

- Concept: Algebraic expression formulation
  - Why needed here: Forces the LLM to abstract the problem into mathematical form, clarifying the required operations and reducing ambiguity.
  - Quick check question: Why might formulating an algebraic expression help reduce errors compared to purely textual reasoning?

## Architecture Onboarding

- Component map:
  - Input Parser -> Subproblem Identifier -> Algebraic Expression Generator -> Python Code Generator -> Code Executor -> Solution Composer -> Answer Comparator -> (If needed) Refinement Prompt Generator -> LLM Interface

- Critical path: Problem → Subproblem Identification → Algebraic Expression → Python Code → Execution → Solution Composition → Answer Comparison → (If needed) Refinement Prompt → Re-solve

- Design tradeoffs:
  - Manual code execution vs. LLM code execution: Manual execution ensures accuracy but adds latency and human intervention.
  - Single vs. multiple refinement loops: More loops increase accuracy but also cost and time.
  - Subproblem granularity: Too fine-grained may overcomplicate; too coarse may not reduce cognitive load effectively.

- Failure signatures:
  - Incorrect subproblem identification leading to wrong final answer.
  - Python code generation errors (syntax or logic).
  - LLM fails to learn from refinement prompts.
  - Numerical value substitution errors.

- First 3 experiments:
  1. Run the MathDivide pipeline on a single GSM8K problem without refinement to validate basic decomposition and code generation.
  2. Test the pipeline with manual code execution and compare results to LLM-generated answers.
  3. Evaluate accuracy improvement with 1 vs. 3 refinement loops on a small problem set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MathDivide technique scale to larger datasets beyond the 250 problems tested?
- Basis in paper: [explicit] The paper mentions that experimentation was conducted on only 250 problems from the GSM8K dataset and suggests testing on a larger and more diverse dataset.
- Why unresolved: The current study's limited dataset size prevents conclusions about the technique's performance on more complex or varied mathematical problems.
- What evidence would resolve it: Comprehensive testing on the full GSM8K dataset and other mathematical reasoning benchmarks with varying difficulty levels.

### Open Question 2
- Question: What is the optimal number of refinement prompts needed for maximum accuracy without overfitting?
- Basis in paper: [explicit] The paper tested accuracy for up to 3 refinement prompts but didn't explore beyond this limit or analyze diminishing returns.
- Why unresolved: The study only measured accuracy within 3 refinement loops without investigating whether additional refinements would improve or degrade performance.
- What evidence would resolve it: Systematic testing with varying numbers of refinement prompts (e.g., 1-10) to identify the point of diminishing returns.

### Open Question 3
- Question: How do automated refinement techniques compare to human-based feedback in terms of accuracy and scalability?
- Basis in paper: [inferred] The paper highlights that human feedback requires manual effort and suggests exploring automated refinement techniques for scalability.
- Why unresolved: The current study relies on human feedback, which limits scalability, but doesn't test or compare automated alternatives.
- What evidence would resolve it: Implementation and comparison of automated refinement methods against the human-based approach on the same datasets.

### Open Question 4
- Question: What is the impact of different problem decomposition strategies on the effectiveness of MathDivide?
- Basis in paper: [explicit] The paper describes breaking problems into subproblems but doesn't systematically compare different decomposition approaches.
- Why unresolved: The study uses a single decomposition strategy without exploring alternative methods for breaking down complex problems.
- What evidence would resolve it: Comparative analysis of different problem decomposition techniques (e.g., top-down vs bottom-up, dependency-based) on mathematical reasoning accuracy.

## Limitations

- Weak empirical support for refinement mechanism: The refinement prompt is generic ("Check the calculations") without detailed human feedback implementation.
- No comparison to simpler baselines: Missing benchmarks against basic chain-of-thought prompting or direct code execution.
- Small test set size: Only 250 GSM8K problems used, potentially insufficient for statistical significance.

## Confidence

- High confidence: The core mechanism of decomposing problems into subproblems and using Python code for computation is clearly specified and technically sound.
- Medium confidence: The claimed accuracy improvements over Mathprompter are likely real but may be overstated given the small test set.
- Low confidence: The specific contribution of human feedback in the refinement process is unclear due to the generic nature of the mentioned refinement prompt.

## Next Checks

1. **Statistical significance testing**: Re-run the MathDivide experiments on the full GSM8K validation set (1,319 problems) with proper statistical analysis to determine if accuracy differences between MathDivide and Mathprompter are significant, including confidence intervals and p-values.
2. **Ablation study on refinement mechanism**: Test the pipeline with and without the refinement loop on a held-out problem set to quantify the actual contribution of iterative refinement versus the base decomposition + code generation approach.
3. **Simplified baseline comparison**: Implement and test a direct chain-of-thought prompting approach with Python code execution (without problem decomposition) on the same 250 GSM8K problems to isolate whether decomposition or code execution drives the performance gains.