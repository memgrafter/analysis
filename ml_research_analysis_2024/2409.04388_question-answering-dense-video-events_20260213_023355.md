---
ver: rpa2
title: Question-Answering Dense Video Events
arxiv_id: '2409.04388'
source_url: https://arxiv.org/abs/2409.04388
tags:
- video
- event
- events
- dense
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces question-answering on dense video events,
  a novel task that challenges multimodal large language models (MLLMs) to comprehend
  and reason about multiple events in long videos. The authors construct DeVE-QA,
  a dataset featuring 78K questions about 26K events on 10.6K long videos, and propose
  DeVi, a training-free MLLM approach that employs three key strategies: hierarchical
  dense event captioning, temporal event memory, and self-consistency checking.'
---

# Question-Answering Dense Video Events

## Quick Facts
- **arXiv ID:** 2409.04388
- **Source URL:** https://arxiv.org/abs/2409.04388
- **Authors:** Hangyu Qin; Junbin Xiao; Angela Yao
- **Reference count:** 40
- **Primary result:** Introduces DeVE-QA dataset and DeVi model achieving 4.8% increase in grounded QA accuracy over existing methods

## Executive Summary
This paper introduces question-answering on dense video events (DeVE-QA), a novel task that challenges multimodal large language models (MLLMs) to comprehend and reason about multiple events in long videos. The authors construct DeVE-QA, a dataset featuring 78K questions about 26K events on 10.6K long videos, and propose DeVi, a training-free MLLM approach that employs three key strategies: hierarchical dense event captioning, temporal event memory, and self-consistency checking. DeVi achieves state-of-the-art performance on DeVE-QA, with a notable 4.8% increase in grounded QA accuracy over existing methods, and also improves accuracy on the NExT-GQA benchmark by 2.1%.

## Method Summary
The paper proposes DeVi, a training-free MLLM approach for question-answering on dense video events. The method employs three specific strategies: hierarchical dense event captioning to detect events at multiple temporal scales, temporal event memory to capture long-term event dependencies, and self-consistency checking to ground answers in visual evidence. The approach uses CLIP-VIT-L/14 for visual encoding, LLaMA-VID or VideoBLIP for event captioning, and GPT-4o or Gemini-2.0 for reasoning. The model processes videos at three hierarchical levels (15s, 35s, 65s segments), captions each independently, contextualizes captions through a temporal memory module, and verifies answer grounding through cross-modal similarity checking with iterative refinement.

## Key Results
- DeVi achieves 78.8% GQA accuracy on DeVE-QA, a 4.8% improvement over existing methods
- The model outperforms LongVA by 6.9% on QA accuracy and 10.4% on GQA accuracy
- On NExT-GQA benchmark, DeVi improves accuracy by 2.1% compared to previous state-of-the-art

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Dense Event Captioning
- **Claim:** Hierarchical dense event captioning improves detection of events at multiple temporal scales
- **Mechanism:** The model processes video at three hierarchical levels (short/medium/long segments) and captions each independently to detect events of varying durations
- **Core assumption:** Events in long videos occur at different temporal scales and require multi-scale processing to capture all relevant information
- **Evidence anchors:**
  - [abstract] "hierarchical dense event captioning to detect the dense events at multiple temporal scales"
  - [section] "To solve the aforementioned challenges, we incorporate three specific strategies: 1) hierarchical dense event captioning to detect the dense events at multiple temporal scales"
- **Break condition:** If the model fails to maintain temporal coherence across different hierarchical levels, or if event boundaries are consistently missed due to improper segmentation lengths

### Mechanism 2: Temporal Event Memory
- **Claim:** Temporal event memory contextualizes and memorizes events for long-range dependency capture
- **Mechanism:** After initial captioning, the model uses LLM to refine captions by incorporating contextual information from all other captions and questions, creating a global synopsis
- **Core assumption:** Isolated event captions lack contextual information needed for understanding complex event relationships in long videos
- **Evidence anchors:**
  - [abstract] "temporal event memory module to capture long-term event dependency and to facilitate event-grounded QA"
  - [section] "The lack of contextual information may result in inaccurate or incomplete event captions... To capture long-term dependencies, we design a memory module"
- **Break condition:** If the contextualization process introduces incorrect associations between unrelated events, or if the global synopsis becomes too generic to be useful

### Mechanism 3: Self-Consistency Checking
- **Claim:** Self-consistency checking anchors answers to visual evidence and prevents hallucination
- **Mechanism:** The model evaluates answer-video consistency using cross-modal similarity and iterates until predictions meet a threshold, or explains inconsistencies
- **Core assumption:** LLMs frequently generate answers not grounded in actual video content, requiring verification mechanisms
- **Evidence anchors:**
  - [abstract] "self-consistency checking module to ground dense-events in long videos for question answering"
  - [section] "Predictions with low consistency (i.e., small Rva) are sent back to the LLM for adjustment. This process is iterated until the consistency reaches a threshold"
- **Break condition:** If the cross-modal similarity threshold is set too high (causing unnecessary iterations) or too low (allowing ungrounded answers)

## Foundational Learning

- **Concept:** Temporal hierarchical modeling in video understanding
  - **Why needed here:** Long videos contain events spanning different time scales, requiring different processing approaches for short vs. medium vs. long events
  - **Quick check question:** If a video has a 2-second event and a 2-minute event, what hierarchical levels would you need to detect both effectively?

- **Concept:** Cross-modal similarity verification
  - **Why needed here:** Ensures generated answers are actually supported by visual content rather than LLM hallucinations or common-sense reasoning
  - **Quick check question:** How would you compute similarity between a text answer and video frames, and what threshold would indicate sufficient grounding?

- **Concept:** Memory-augmented reasoning for long-range dependencies
  - **Why needed here:** Events in long videos may be temporally distant but logically connected, requiring contextualization beyond immediate segments
  - **Quick check question:** What information would you store in a temporal memory module to help connect events that occur minutes apart in a video?

## Architecture Onboarding

- **Component map:** Visual Encoder (CLIP VIT-L/14) → Frame feature extraction → Hierarchical Captioners (LLaMA-VID) → Event detection at multiple scales → Temporal Event Memory → Contextualization and storage → LLM Backbone (GPT-4o/Gemini-2.0) → Reasoning and QA → Self-Consistency Checker → Answer grounding verification

- **Critical path:** Video → Hierarchical Captioning → Temporal Memory → LLM Reasoning → Self-Consistency Check → Final Answer
- **Design tradeoffs:**
  - Segment length vs. temporal resolution (longer segments capture more context but miss fine details)
  - Hierarchy depth vs. computational cost (more levels capture more scales but increase processing time)
  - Consistency threshold vs. iteration count (higher thresholds improve grounding but increase latency)
- **Failure signatures:**
  - High iteration count indicates poor initial grounding or overly strict thresholds
  - Performance drop on dense-event subsets indicates inadequate hierarchical processing
  - Inconsistent QA vs. GQA accuracy suggests grounding mechanism issues
- **First 3 experiments:**
  1. Test hierarchical captioning ablation by comparing performance with naive frame-by-frame captioning
  2. Vary the consistency threshold (σ) to find optimal balance between accuracy and efficiency
  3. Test different LLM backbones to validate the claim that larger models are crucial for success

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- The effectiveness of the three hierarchical levels is not rigorously validated; ablation studies only test binary presence/absence rather than optimal number of levels or segment lengths
- The temporal event memory module's contextualization process is underspecified, with unclear mechanisms for preventing incorrect event associations across unrelated video segments
- Self-consistency checking relies on cross-modal similarity without clear justification for the chosen threshold or explanation of failure modes when visual-linguistic alignment is ambiguous

## Confidence
- **High confidence**: The dataset construction methodology and baseline comparisons are well-documented and reproducible
- **Medium confidence**: The hierarchical captioning approach shows empirical gains, but theoretical justification for the specific temporal scales is limited
- **Low confidence**: The self-consistency checking mechanism's effectiveness depends heavily on unspecified implementation details of the dynamic verification prompt

## Next Checks
1. Conduct ablation studies varying the number of hierarchical levels (1-5) and segment lengths to determine optimal configuration rather than assuming three levels is ideal
2. Test the temporal memory module's ability to distinguish between related and unrelated events by creating adversarial examples where contextualization should fail
3. Evaluate model performance when visual-linguistic alignment is intentionally degraded to assess robustness of the self-consistency checking mechanism