---
ver: rpa2
title: Mitigating Label Noise on Graph via Topological Sample Selection
arxiv_id: '2403.01942'
source_url: https://arxiv.org/abs/2403.01942
tags:
- nodes
- graph
- learning
- noise
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of learning robust graph neural
  networks (GNNs) on graph data with noisy labels. It identifies that nodes near topological
  class boundaries are highly informative for classification but are often misclassified
  or entangled with mislabeled nodes by existing sample selection methods.
---

# Mitigating Label Noise on Graph via Topological Sample Selection

## Quick Facts
- arXiv ID: 2403.01942
- Source URL: https://arxiv.org/abs/2403.01942
- Reference count: 40
- Primary result: Proposes TSS method using CBC to improve GNN robustness on noisy-labeled graphs, outperforming state-of-the-art baselines

## Executive Summary
This paper addresses the challenge of learning robust Graph Neural Networks (GNNs) when graph data contains noisy labels. The authors identify that existing sample selection methods struggle with nodes near topological class boundaries, which are highly informative but often misclassified or entangled with mislabeled nodes. To solve this, they introduce Topological Sample Selection (TSS), which uses a novel Class-conditional Betweenness Centrality (CBC) measure to identify nodes based on their position in the graph's class structure rather than just prediction confidence. TSS employs a curriculum learning approach, progressively selecting clean nodes from those far from boundaries to more challenging boundary-near nodes. Theoretical analysis proves that TSS minimizes an upper bound of the expected risk under a clean distribution.

## Method Summary
The paper proposes TSS, which leverages Class-conditional Betweenness Centrality (CBC) as a novel topological measure to identify nodes based on their structural position within class boundaries. Unlike traditional sample selection methods that rely solely on prediction confidence, CBC captures how frequently a node lies on shortest paths between nodes of different classes, making it particularly effective at distinguishing truly noisy labels from genuinely ambiguous boundary cases. TSS operates in a curriculum learning fashion, starting with nodes far from class boundaries (likely clean) and progressively incorporating more challenging boundary-near nodes. This staged approach ensures that the model is first trained on reliably clean data before tackling harder cases. The method is theoretically grounded, with proofs showing that TSS minimizes an upper bound of the expected risk under the clean data distribution.

## Key Results
- TSS outperforms state-of-the-art baselines across multiple graph datasets with various noise types
- Achieves higher classification accuracy even under high noise rates (up to 40% uniform noise)
- Demonstrates superior performance on both synthetic and real-world graph datasets

## Why This Works (Mechanism)
TSS works by recognizing that nodes near topological class boundaries are structurally informative but are often misclassified by traditional confidence-based methods. The Class-conditional Betweenness Centrality (CBC) measure captures a node's importance in connecting different classes topologically, rather than relying on feature similarity alone. This topological perspective allows TSS to distinguish between nodes that are truly mislabeled versus those that are simply ambiguous due to their position between classes. By progressively selecting samples in a curriculum learning manner—starting with nodes far from boundaries and moving toward boundary-near nodes—TSS ensures the model learns clean patterns first before tackling more ambiguous cases. This staged approach prevents the model from being misled by early exposure to potentially noisy boundary nodes.

## Foundational Learning
- **Graph Neural Networks (GNNs)**: Why needed - To process graph-structured data and learn node representations that capture both features and topology. Quick check - Verify that the GNN architecture can propagate information across the graph structure.
- **Label Noise in Classification**: Why needed - Real-world datasets often contain mislabeled instances that can severely degrade model performance. Quick check - Confirm that noise rates are properly simulated and measured.
- **Betweenness Centrality**: Why needed - A graph-theoretic measure that identifies nodes lying on many shortest paths, indicating their structural importance. Quick check - Validate that CBC correctly identifies topologically important nodes for each class.
- **Curriculum Learning**: Why needed - A training strategy that presents easier examples first, allowing the model to build foundational knowledge before tackling harder cases. Quick check - Ensure the progression from clean to noisy samples follows a logical difficulty curve.

## Architecture Onboarding

Component Map: Graph Data -> GNN Backbone -> TSS Module (CBC + Selection) -> Clean Sample Selection -> GNN Retraining

Critical Path: The TSS module sits between the GNN backbone and the training loop, where it filters the training data before each epoch based on CBC scores and the current curriculum stage.

Design Tradeoffs: TSS trades computational overhead from CBC calculation for improved robustness to label noise. The staged selection approach may slow convergence but leads to better final accuracy, especially under high noise rates.

Failure Signatures: If CBC is poorly calibrated, TSS might select too many noisy samples early (under-curriculum) or be overly conservative and select too few useful boundary samples (over-curriculum). This manifests as either no improvement over baselines or premature convergence to suboptimal accuracy.

First Experiments:
1. Verify CBC scores correlate with actual label correctness on synthetic data with known ground truth
2. Test TSS with varying curriculum stages on a simple graph with moderate noise to find optimal progression
3. Compare TSS against confidence-based selection alone to demonstrate the value of the topological measure

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Theoretical risk bound relies on assumptions about clean data distribution and noise generation that may not hold in all real-world scenarios
- Computational complexity of CBC calculation could become prohibitive for very large graphs
- Method focuses on semi-supervised node classification; performance on other graph learning tasks like graph classification remains unexplored

## Confidence

High: Novel topological measure (CBC) and sample selection framework are well-defined and theoretically grounded

Medium: Empirical superiority claims depend on specific datasets and noise injection procedures

Low: Scalability claims lack extensive evaluation on large-scale graphs with millions of nodes

## Next Checks

1. Test TSS on graphs with heterogeneous structures and varying label noise mechanisms beyond uniform and feature-dependent noise

2. Evaluate computational efficiency and memory requirements on graphs with millions of nodes to assess scalability limits

3. Apply TSS to graph-level tasks like graph classification to verify broader applicability beyond node classification