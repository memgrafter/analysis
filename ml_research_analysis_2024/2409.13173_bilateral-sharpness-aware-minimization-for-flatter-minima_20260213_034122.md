---
ver: rpa2
title: Bilateral Sharpness-Aware Minimization for Flatter Minima
arxiv_id: '2409.13173'
source_url: https://arxiv.org/abs/2409.13173
tags:
- bsam
- gradient
- loss
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors identify that SAM's Max-Sharpness (MaxS) only measures
  flatness in the gradient ascent direction, which may not lead to sufficiently flat
  minima. To address this, they introduce Min-Sharpness (MinS), which measures flatness
  in the gradient descent direction.
---

# Bilateral Sharpness-Aware Minimization for Flatter Minima

## Quick Facts
- arXiv ID: 2409.13173
- Source URL: https://arxiv.org/abs/2409.13173
- Authors: Jiaxin Deng; Junbiao Pang; Baochang Zhang; Qingming Huang
- Reference count: 37
- Primary result: BSAM achieves higher test accuracy than SAM by finding flatter minima through bilateral sharpness optimization

## Executive Summary
Bilateral Sharpness-Aware Minimization (BSAM) addresses a fundamental limitation in Sharpness-Aware Minimization (SAM) by considering both gradient ascent and descent directions when optimizing for flat minima. While SAM only measures flatness in the gradient ascent direction (Max-Sharpness), BSAM introduces Min-Sharpness to measure flatness in the gradient descent direction. By combining both measures with dynamic perturbation radius adjustment, BSAM consistently outperforms SAM across various tasks including classification, transfer learning, pose estimation, and network quantization, achieving up to 0.8% higher accuracy on CIFAR-100 with WideResNet-28-10.

## Method Summary
BSAM is an optimization method that finds flatter minima by combining training loss with both Max-Sharpness (measuring flatness in gradient ascent direction) and Min-Sharpness (measuring flatness in gradient descent direction). The method uses first-order Taylor expansion to approximate inner minimization problems for both perturbations. A key innovation is the dynamic adjustment of the Min-Sharpness perturbation radius (ρmin) proportional to the learning rate, which prevents gradient conflicts in later training stages. BSAM also scales the gradient at the minimum perturbation point to match the magnitude at the maximum point, ensuring both contributions work together effectively. The method requires approximately 1.5× the computation of SAM due to three forward/backward passes per training step.

## Key Results
- BSAM achieves 85.51% accuracy on CIFAR-100 with WideResNet-28-10 compared to SAM's 84.71%
- Consistently outperforms SAM across multiple tasks: classification, transfer learning, human pose estimation, and network quantization
- Hessian eigenvalue analysis confirms BSAM finds flatter minima with reduced maximum eigenvalues
- Dynamic ρmin adjustment prevents gradient conflicts and maintains optimization stability throughout training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BSAM's bilateral sharpness indicator finds flatter minima than SAM by optimizing both gradient ascent and descent directions.
- Mechanism: By combining MaxS (gradient ascent flatness) and MinS (gradient descent flatness), BSAM guides the optimizer toward a more balanced and flatter region of the loss landscape, avoiding the "Flatness Indicator Problem" where SAM only considers one side.
- Core assumption: Optimizing the difference between training loss and the minimum loss in a local neighborhood leads to flatter minima.
- Evidence anchors:
  - [abstract] "SAM only considers the flatness in the direction of gradient ascent, resulting in a next minimization region that is not sufficiently flat. A better Flatness Indicator (FI) would bring a better generalization of neural networks."
  - [section] "Consequently, we combine the MaxS, MinS and SAM into the proposed Bilateral SAM (BSAM). It can indicate flatter directions early in training and ultimately find a flatter minima, resulting in better generalization performance."
  - [corpus] Weak evidence - no direct corpus papers discussing MinS or bilateral sharpness.
- Break condition: If MinS gradient conflicts with training loss gradient become too severe, or if the Hessian eigenvalue analysis doesn't show reduced maximum eigenvalues compared to SAM.

### Mechanism 2
- Claim: Dynamic adjustment of MinS perturbation radius (ρmin) prevents gradient conflicts in later training stages.
- Mechanism: As training progresses and the model approaches a local minimum, large ρmin can cause the minimum perturbation point to overshoot the minimum, creating conflicting gradients. BSAM decreases ρmin proportionally to the learning rate to maintain gradient alignment.
- Core assumption: The magnitude of ρmin has less impact on the perturbation than the size of the gradient near a minimum, so reducing ρmin maintains optimization stability.
- Evidence anchors:
  - [section] "To balance between the possible gradient conflict between the MinS and the gradient of a tailored loss, we propose to descent the radius of MinS."
  - [section] "Motivated the observation in Fig. 3 and the analysis in Fig. 2, rather than using gradient decomposition to solve GCP which requires additional computation, we instead of address GCP by gradually decreasing ρmin as follows:"
  - [corpus] No direct evidence in corpus about gradient conflict resolution through ρmin adjustment.
- Break condition: If dynamic ρmin adjustment doesn't prevent accuracy degradation in later training stages, or if cosine similarity between gradients becomes negative despite adjustment.

### Mechanism 3
- Claim: Gradient magnitude balancing between MaxS and MinS ensures both contribute effectively to finding flat minima.
- Mechanism: BSAM scales the gradient at the minimum perturbation point to match the magnitude at the maximum perturbation point, ensuring MinS has comparable influence to MaxS in guiding the optimization.
- Core assumption: The gradient magnitude at the maximum point is typically larger than at the minimum point, so scaling is necessary for balanced optimization.
- Evidence anchors:
  - [section] "Generally, the gradient magnitude at the maximum point tends to be larger than that at the minimum point, which could weaken the effect of MinS. We adhere to the principle of maximum entropy. Concretely, we scale the gradient at the minimum point ∇wL(wmin) to match the gradient magnitude at the maximum point ∇wL(wmax) to ensure that they work together to promote generalization."
  - [corpus] No direct evidence in corpus about gradient magnitude balancing techniques.
- Break condition: If scaling doesn't improve generalization performance compared to using original gradient magnitudes, or if it causes optimization instability.

## Foundational Learning

- Concept: Sharpness-aware optimization and loss landscape geometry
  - Why needed here: Understanding how sharpness relates to generalization and why flatter minima are preferred is crucial for grasping BSAM's motivation and design
  - Quick check question: Why does minimizing sharpness in the loss landscape typically lead to better generalization performance?

- Concept: First-order Taylor expansion and perturbation-based optimization
  - Why needed here: BSAM uses Taylor expansion to approximate the inner minimization problems for both MaxS and MinS, so understanding this approximation technique is essential
  - Quick check question: How does a first-order Taylor expansion help approximate the maximum or minimum loss within a perturbation radius?

- Concept: Hessian eigenvalue analysis for flatness measurement
  - Why needed here: The paper uses top Hessian eigenvalues to empirically verify that BSAM finds flatter minima, so understanding this connection is important
  - Quick check question: What does a smaller maximum Hessian eigenvalue indicate about the flatness of a local minimum?

## Architecture Onboarding

- Component map:
  Training loss -> MaxS computation -> MinS computation -> Gradient scaling -> Dynamic ρmin adjustment -> Combined gradient -> Base optimizer update

- Critical path:
  1. Compute training loss gradient
  2. Calculate MaxS perturbation and gradient
  3. Calculate MinS perturbation and gradient
  4. Scale MinS gradient to match MaxS magnitude
  5. Combine gradients with training loss gradient
  6. Update parameters using base optimizer
  7. Adjust ρmin based on current learning rate

- Design tradeoffs:
  - BSAM has ~1.5x computational cost compared to SAM due to three forward/backward passes per step
  - Dynamic ρmin adjustment adds complexity but prevents gradient conflicts
  - Gradient scaling ensures balanced contribution but requires careful implementation
  - The method shows consistent improvements across tasks but magnitude varies by dataset and architecture

- Failure signatures:
  - Accuracy degradation in later training stages despite dynamic ρmin adjustment
  - Increased training instability or oscillations
  - Smaller improvements than expected on certain datasets
  - Higher computational cost without proportional performance gains

- First 3 experiments:
  1. CIFAR-10 classification with ResNet-18 using SGD, SAM, and BSAM to verify basic performance improvement
  2. CIFAR-100 classification with WideResNet-28-10 to test effectiveness on more complex datasets
  3. Label noise robustness test with varying noise rates to confirm improved generalization under challenging conditions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed BSAM method perform when combined with other optimizers, such as Adam, compared to its performance with SGD as the base optimizer?
- Basis in paper: [explicit] The paper mentions that BSAM is tested with Adam as the base optimizer in the human pose estimation task, but does not extensively compare its performance with other optimizers.
- Why unresolved: The paper does not provide a comprehensive comparison of BSAM's performance across different optimizers.
- What evidence would resolve it: Conducting experiments with BSAM using various optimizers like Adam, RMSprop, and others on multiple tasks and datasets to compare performance metrics.

### Open Question 2
- Question: What is the impact of the perturbation radius (ρ) on the generalization performance of BSAM in different network architectures?
- Basis in paper: [explicit] The paper discusses the importance of ρ in BSAM and its effect on gradient conflicts, but does not explore its impact across different network architectures.
- Why unresolved: The paper focuses on specific architectures and does not provide a detailed analysis of how ρ affects performance in diverse network types.
- What evidence would resolve it: Performing a systematic study of BSAM with varying ρ values on a wide range of network architectures to assess generalization performance.

### Open Question 3
- Question: How does BSAM compare to other SAM variants in terms of computational efficiency and resource usage?
- Basis in paper: [explicit] The paper mentions that BSAM is approximately 1.5 times the time complexity of SAM but does not compare it to other SAM variants in terms of efficiency.
- Why unresolved: The paper does not provide a detailed comparison of BSAM's computational efficiency relative to other SAM variants.
- What evidence would resolve it: Conducting experiments to measure the computational time and resource usage of BSAM compared to other SAM variants across different tasks and datasets.

## Limitations

- BSAM requires approximately 1.5× the computational cost of SAM due to three forward/backward passes per training step
- Dynamic ρmin adjustment strategy lacks extensive empirical validation across diverse architectures and training regimes
- Gradient magnitude balancing mechanism between MaxS and MinS lacks rigorous theoretical justification for the specific scaling approach

## Confidence

**High Confidence**: The core mechanism of combining MaxS and MinS to find flatter minima is well-supported by both theoretical analysis and experimental results. The Hessian eigenvalue analysis provides strong evidence that BSAM achieves flatter minima than SAM.

**Medium Confidence**: The dynamic ρmin adjustment strategy shows promise but has limited empirical validation. The paper provides some evidence that this prevents gradient conflicts, but more extensive testing across different architectures and datasets would strengthen this claim.

**Low Confidence**: The gradient scaling mechanism for balancing MaxS and MinS contributions lacks theoretical justification and is primarily presented as an empirical choice. The "principle of maximum entropy" reference is not fully developed in the context of this specific application.

## Next Checks

1. **Gradient Conflict Analysis**: Conduct a detailed analysis of gradient conflicts by tracking the cosine similarity between training loss gradients and MinS gradients throughout training with and without dynamic ρmin adjustment. This would quantify whether the proposed adjustment actually prevents conflicts as claimed.

2. **Computational Efficiency Benchmark**: Perform a comprehensive benchmark comparing BSAM's computational cost against SAM and SGD across different batch sizes, model architectures, and hardware configurations to better understand the practical trade-offs of the 1.5× overhead.

3. **Generalization to Novel Architectures**: Test BSAM on architectures not included in the original experiments (such as Vision Transformers or modern convolutional networks) to verify whether the bilateral sharpness approach generalizes beyond the specific models studied in the paper.