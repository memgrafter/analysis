---
ver: rpa2
title: Multi-task Prompt Words Learning for Social Media Content Generation
arxiv_id: '2407.07771'
source_url: https://arxiv.org/abs/2407.07771
tags:
- prompt
- text
- image
- content
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-task prompt learning framework for
  social media content generation. The method combines topic classification, sentiment
  analysis, scene recognition, and keyword extraction to generate comprehensive prompt
  words that guide ChatGPT in producing high-quality tweets.
---

# Multi-task Prompt Words Learning for Social Media Content Generation

## Quick Facts
- **arXiv ID:** 2407.07771
- **Source URL:** https://arxiv.org/abs/2407.07771
- **Reference count:** 40
- **Key outcome:** Multi-task prompt learning framework generates higher-quality tweets than manual methods and other prompt learning techniques, achieving an overall score of 8.05/10 in content quality evaluation

## Executive Summary
This paper addresses the challenge of automatic social media content generation by proposing a multi-task prompt learning framework that combines image and text features with topic classification, sentiment analysis, scene recognition, and keyword extraction to generate comprehensive prompt words. The framework guides ChatGPT in producing high-quality tweets that are consistent with both images and social media context. A key innovation is using ChatGPT itself for large-scale evaluation of generated content, addressing the lack of effective evaluation criteria in this domain. Experimental results demonstrate that the proposed method generates tweets of higher quality compared to manual methods and other prompt learning techniques, with significant improvements in content clarity and consistency with images.

## Method Summary
The method works by first generating multi-modal prompt words that capture image content, user sentiment, and topic relevance, then feeding these to ChatGPT via a structured template to produce high-quality tweets. The framework fuses image and text features, extracts keywords, classifies topics, analyzes sentiment, and recognizes scenes. These prompt words are structured into a template that guides ChatGPT to generate tweets consistent with both the image and social media context. The approach uses ChatGPT itself to evaluate the generated tweets at scale, providing standardized and objective feedback across multiple dimensions including relevance, creativity, coherence, emotional impact, and engagement.

## Key Results
- The proposed method achieves an overall score of 8.05/10 in content quality evaluation, outperforming other methods and manual crafting
- Tweets generated using the multi-task prompt learning framework show higher quality compared to manual methods and other prompt learning techniques
- Scene recognition significantly enhances content clarity and its consistency with the image
- The method enables large-scale evaluation of content generation algorithms through ChatGPT, making automated assessment possible in the absence of effective and objective evaluation criteria

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method works by generating multi-modal prompt words that capture image content, user sentiment, and topic relevance, then feeding these to ChatGPT via a structured template to produce high-quality tweets.
- Mechanism: The framework fuses image and text features, extracts keywords, classifies topics, analyzes sentiment, and recognizes scenes. These prompt words are structured into a template that guides ChatGPT to generate tweets that are consistent with both the image and social media context.
- Core assumption: ChatGPT can be effectively steered toward producing high-quality, contextually relevant tweets if provided with comprehensive, multi-task derived prompt words.
- Evidence anchors:
  - [abstract] "we propose a new prompt word generation framework based on multi-modal information fusion, which combines multiple tasks including topic classification, sentiment analysis, scene recognition and keyword extraction to generate more comprehensive prompt words"
  - [section] "Based on the generated content and original images, we perform our multi-task prompt words learning (MPWL): topic classification, sentiment analysis, scene recognition and content keyword extraction"
  - [corpus] Weak evidence—no direct citations, but the approach aligns with recent prompt-tuning and multimodal learning trends
- Break condition: If ChatGPT fails to incorporate prompt words meaningfully, or if the prompt template does not guide generation effectively, tweet quality will drop.

### Mechanism 2
- Claim: The method's quality is enhanced by using ChatGPT itself to evaluate the generated tweets at scale, providing standardized and objective feedback.
- Mechanism: Generated tweets are evaluated using a predefined template and ChatGPT as the judge, scoring multiple dimensions (relevance, creativity, coherence, emotional impact, engagement). This large-scale, automated evaluation avoids subjective human bias.
- Core assumption: ChatGPT's evaluation is both consistent and valid for assessing tweet quality, providing a scalable alternative to manual review.
- Evidence anchors:
  - [abstract] "in the absence of effective and objective evaluation criteria in the field of content generation, we use the ChatGPT tool to evaluate the results generated by the algorithm, making large-scale evaluation of content generation algorithms possible"
  - [section] "The evaluation of generated tweet content encompasses four main aspects: Relevance and Clarity, Creativity and Originality, Coherence and Structure, and Emotional Impact and Engagement. Each aspect is rated on a scale from 1 to 10"
  - [corpus] No corpus support for ChatGPT-as-evaluator; this is a novel application
- Break condition: If ChatGPT's evaluations are inconsistent or fail to correlate with human judgments, the method's claimed advantages disappear.

### Mechanism 3
- Claim: Multi-task prompt learning improves tweet quality by explicitly modeling the interplay between topic, sentiment, and scene, leading to more coherent and contextually relevant content.
- Mechanism: Each subtask (topic classification, sentiment analysis, scene recognition, keyword extraction) produces features that are fused and encoded into a prompt for ChatGPT. This multi-task approach ensures that all relevant dimensions are considered during generation.
- Core assumption: Combining multiple modalities and tasks in prompt generation results in richer, more effective guidance than single-task or heuristic approaches.
- Evidence anchors:
  - [section] "In order to get a more accurate text description of the image, we first generate 4 random text descriptions for the image, then use CLIP...to score the matching of each text and image"
  - [section] "we perform our multi-task prompt words learning (MPWL): topic classification, sentiment analysis, scene recognition and content keyword extraction"
  - [corpus] No direct corpus evidence; assumption is based on general principles of multi-task learning
- Break condition: If one or more tasks provide noisy or irrelevant information, overall tweet quality could degrade.

## Foundational Learning

- Concept: Multi-modal data fusion (combining image and text features)
  - Why needed here: Social media tweets are multimodal; combining image and text features enables more comprehensive prompt generation
  - Quick check question: How are image and text features fused in the proposed method?
- Concept: Prompt learning and template-based guidance for LLMs
  - Why needed here: Direct prompting of ChatGPT with rich, structured prompt words is essential to steer generation toward desired tweet characteristics
  - Quick check question: What role does the prompt template play in generating tweets?
- Concept: Automated, large-scale evaluation using LLMs
  - Why needed here: Manual evaluation is slow and subjective; automated evaluation with ChatGPT enables objective, scalable assessment
  - Quick check question: How does the method use ChatGPT for evaluation, and what are the advantages?

## Architecture Onboarding

- Component map:
  Image processing (ViT) → Text generation (HuggingGPT) → Feature fusion (BERT/ViT) → Multi-task learning (topic, sentiment, scene, keywords) → Prompt template → ChatGPT generation → ChatGPT evaluation
- Critical path:
  1. Image → crude content generation → keyword/feat extraction
  2. Multimodal fusion → multi-task prompt word learning
  3. Prompt template + ChatGPT → tweet generation
  4. Tweet + image → ChatGPT evaluation
- Design tradeoffs:
  - Multi-task vs. single-task: richer prompts vs. complexity and risk of noisy signals
  - Automated evaluation vs. human judgment: scalability and consistency vs. potential bias in model evaluation
  - Template structure vs. flexibility: consistency in guidance vs. adaptability to diverse tweet types
- Failure signatures:
  - Low evaluation scores across multiple dimensions
  - Tweets off-topic or irrelevant to images
  - Repetition or lack of creativity
  - Inconsistent evaluation results from ChatGPT
- First 3 experiments:
  1. Ablation: Remove each multi-task component (topic, sentiment, scene, keywords) and measure impact on tweet quality and evaluation scores.
  2. Cross-dataset: Test generalizability of sub-networks (sentiment, scene) on standard public datasets (MVSA, Scene-15, etc.) and report accuracy/F1.
  3. Prompt template variations: Test different prompt templates (e.g., reordered, missing elements) to determine optimal structure for tweet quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the use of ChatGPT for large-scale evaluation of generated content compare to traditional human evaluation methods in terms of reliability and bias?
- Basis in paper: [explicit] The paper mentions using ChatGPT for large-scale evaluation of content generation algorithms, stating it is more standardized and fair than manual evaluation.
- Why unresolved: The paper does not provide a direct comparison between ChatGPT-based evaluation and human evaluation methods, nor does it discuss potential biases in the ChatGPT evaluation process.
- What evidence would resolve it: A comparative study between ChatGPT-based evaluation and human evaluation methods, including metrics such as inter-rater reliability, bias detection, and correlation with human preferences, would provide insight into the reliability and fairness of using ChatGPT for evaluation.

### Open Question 2
- Question: Can the multi-task prompt learning framework be effectively applied to other social media platforms beyond Twitter, such as Instagram or Facebook, and what modifications would be necessary?
- Basis in paper: [inferred] The paper focuses on Twitter content generation, but the framework could potentially be adapted for other platforms with different content formats and user behaviors.
- Why unresolved: The paper does not explore the application of the framework to other social media platforms or discuss necessary modifications for different content types and user expectations.
- What evidence would resolve it: Experiments applying the framework to other social media platforms, along with an analysis of required adjustments for different content formats and user behaviors, would demonstrate its versatility and effectiveness across various platforms.

### Open Question 3
- Question: How does the inclusion of scene recognition in the multi-task prompt learning framework specifically enhance the quality and relevance of generated content compared to frameworks without this component?
- Basis in paper: [explicit] The paper states that scene recognition significantly enhances content clarity and its consistency with the image, but does not provide detailed analysis of its specific contributions.
- Why unresolved: The paper does not quantify the impact of scene recognition on content quality or provide examples of how it improves relevance and clarity in generated content.
- What evidence would resolve it: A detailed analysis comparing content quality with and without scene recognition, including qualitative examples and quantitative metrics, would clarify the specific contributions of this component to the overall effectiveness of the framework.

## Limitations

- The evaluation methodology relies entirely on ChatGPT's judgments without human validation or comparison to ground truth
- The specific prompt templates used to guide ChatGPT are not detailed, making exact replication difficult
- Multi-task sub-networks are trained on relatively small, domain-specific datasets, raising questions about robustness and generalizability

## Confidence

- **High confidence:** The multi-task learning framework structure and experimental results showing improvements over baseline methods (8.05/10 overall score)
- **Medium confidence:** The claim that multi-task prompt learning produces more comprehensive prompt words than single-task approaches
- **Low confidence:** The reliability of ChatGPT-based evaluation as a proxy for human judgment of tweet quality

## Next Checks

1. Conduct human evaluation studies comparing ChatGPT judgments with human ratings of tweet quality to validate the automated evaluation methodology
2. Perform cross-dataset validation of the multi-task sub-networks on standard public datasets (MVSA for sentiment, Scene-15 for scene recognition) to assess generalizability
3. Run ablation studies removing each multi-task component (topic, sentiment, scene, keywords) to quantify their individual contributions to tweet quality improvements