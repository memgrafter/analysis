---
ver: rpa2
title: Multi-Armed Bandits with Abstention
arxiv_id: '2402.15127'
source_url: https://arxiv.org/abs/2402.15127
tags:
- abstention
- regret
- algorithm
- bandit
- multi-armed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces and analyzes a novel extension of the canonical
  multi-armed bandit model that incorporates an additional strategic element: abstention.
  The agent can now choose to abstain from accepting the stochastic instantaneous
  reward before observing it, incurring either a fixed regret or gaining a guaranteed
  reward.'
---

# Multi-Armed Bandits with Abstention

## Quick Facts
- arXiv ID: 2402.15127
- Source URL: https://arxiv.org/abs/2402.15127
- Authors: Junwen Yang; Tianyuan Jin; Vincent Y. F. Tan
- Reference count: 40
- Primary result: Introduces and analyzes a novel extension of multi-armed bandits incorporating an abstention option, achieving both asymptotic and minimax optimality

## Executive Summary
This paper introduces and analyzes a novel extension of the canonical multi-armed bandit model that incorporates an additional strategic element: abstention. The agent can now choose to abstain from accepting the stochastic instantaneous reward before observing it, incurring either a fixed regret or gaining a guaranteed reward. Two complementary settings are considered: fixed-regret, where abstention results in a constant regret, and fixed-reward, where abstention yields a deterministic reward. The authors design algorithms that achieve both asymptotic and minimax optimality, demonstrating the power of incorporating abstention in reducing cumulative regret.

## Method Summary
The paper proposes two main algorithms: FRG-TS WA for the fixed-regret setting and FRW-ALG WA for the fixed-reward setting. FRG-TS WA integrates two abstention criteria into a Less-Exploring Thompson Sampling-based algorithm, using a combination of a lower confidence bound and a worst-case abstention criterion. FRW-ALG WA transforms any algorithm that is both asymptotically and minimax optimal in the canonical model to one that also accommodates the abstention option. Both algorithms are proven to achieve their corresponding information-theoretic lower bounds.

## Key Results
- Introduces the first multi-armed bandit model with an abstention option, achieving both asymptotic and minimax optimality
- Designs FRG-TS WA algorithm for fixed-regret setting, achieving simultaneous asymptotic and minimax optimality
- Proposes FRW-ALG WA, a general strategy for the fixed-reward setting that maintains optimality
- Numerical experiments validate theoretical results and demonstrate the advantage of incorporating abstention

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The abstention option reduces cumulative regret by allowing the agent to avoid pulls with high expected regret.
- Mechanism: When the agent can estimate that a suboptimal arm's expected reward is significantly worse than the abstention reward (or regret), it opts to abstain rather than pull that arm, thus avoiding the regret entirely.
- Core assumption: The agent can accurately estimate the mean reward of the arm it is about to pull based on prior observations.
- Evidence anchors:
  - [abstract] "When opting for abstention, the agent either suffers a fixed regret or gains a guaranteed reward."
  - [section] "Opting for abstention (Bt = 1) leads to a deterministic regret of c > 0, in contrast to the initial regret linked to arm At when not selecting abstention (Bt = 0), which is given by µ1 − Xt."
- Break condition: If the agent's estimates of arm means are highly inaccurate, it may abstain when it should pull, or vice versa, negating the benefit.

### Mechanism 2
- Claim: The algorithm achieves both asymptotic and minimax optimality by carefully balancing exploration and exploitation while incorporating abstention.
- Mechanism: The algorithm uses a combination of a lower confidence bound and a worst-case abstention criterion to decide when to abstain. This ensures that the agent explores sufficiently to identify the best arm while avoiding high-regret pulls when possible.
- Core assumption: The algorithm can accurately track the number of pulls and the empirical mean of each arm.
- Evidence anchors:
  - [abstract] "We answer this question affirmatively by designing and analyzing algorithms whose regrets meet their corresponding information-theoretic lower bounds."
  - [section] "The first abstention criterion employs a carefully constructed lower confidence bound, while the second is tailored to mitigate worst-case scenarios."
- Break condition: If the confidence bounds are too loose or too tight, the abstention decisions may become suboptimal.

### Mechanism 3
- Claim: The general strategy for the fixed-reward setting maintains optimality by transforming any optimal algorithm for the canonical bandit problem.
- Mechanism: The algorithm uses a base algorithm that is optimal for the canonical bandit problem and adds an abstention decision rule that compares the empirical mean of the chosen arm to the abstention reward. If the empirical mean is less than or equal to the abstention reward, the agent abstains.
- Core assumption: The base algorithm is both asymptotically and minimax optimal for the canonical bandit problem.
- Evidence anchors:
  - [abstract] "we introduce a general strategy, outlined in Algorithm 2. This method is capable of transforming any algorithm that is both asymptotically and minimax optimal in the canonical model to one that also accommodates the abstention option."
  - [section] "In our algorithm, at each time step t, the base algorithm determines the selected arm At according to the partial interaction historical information... Subsequently, the algorithm decides whether or not to abstain... by comparing the empirical mean of the arm At, denoted as ˆµAt(t−1), to the abstention reward c."
- Break condition: If the base algorithm is not truly optimal for the canonical problem, the transformed algorithm may not achieve optimality in the abstention setting.

## Foundational Learning

- Concept: Multi-armed bandit problem
  - Why needed here: This paper extends the canonical multi-armed bandit problem by adding an abstention option, so understanding the basic problem is essential.
  - Quick check question: What is the main challenge in the multi-armed bandit problem, and how is it typically addressed?

- Concept: Upper confidence bounds (UCB)
  - Why needed here: The algorithm uses a lower confidence bound to make abstention decisions, which is related to the UCB approach used in bandit algorithms.
  - Quick check question: How does the UCB approach balance exploration and exploitation, and how might a lower confidence bound differ?

- Concept: Thompson Sampling
  - Why needed here: The algorithm builds upon Less-Exploring Thompson Sampling, so understanding the basic Thompson Sampling algorithm is important.
  - Quick check question: What is the key idea behind Thompson Sampling, and how does it differ from UCB approaches?

## Architecture Onboarding

- Component map: Arm sampling rule -> Abstention decision rule -> Empirical mean tracking
- Critical path:
  1. Initialize by sampling each arm once.
  2. At each time step, construct estimated rewards for each arm.
  3. Select the arm with the highest estimated reward.
  4. Decide whether to abstain based on the two criteria.
  5. Observe the reward and update the empirical means.

- Design tradeoffs:
  - Using a lower confidence bound vs. an upper confidence bound for abstention decisions.
  - Balancing the two abstention criteria to ensure both asymptotic and minimax optimality.
  - The computational cost of tracking empirical means and confidence bounds.

- Failure signatures:
  - If the abstention criteria are too conservative, the algorithm may abstain too often, missing out on high-reward pulls.
  - If the criteria are too aggressive, the algorithm may pull suboptimal arms too often, incurring high regret.
  - If the empirical means are not tracked accurately, the abstention decisions may be suboptimal.

- First 3 experiments:
  1. Implement the algorithm for a simple bandit instance with two arms and test its performance with different abstention regrets.
  2. Compare the algorithm's performance to a non-abstaining baseline algorithm on a more complex bandit instance.
  3. Test the algorithm's sensitivity to the confidence bound parameter by running it with different values and observing the effect on regret.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the abstention model perform in linear bandits where pulling one arm can indirectly reveal information about other arms?
- Basis in paper: [explicit] The paper suggests expanding the abstention model from K-armed bandits to linear bandits as a future direction, noting that policies based on optimism in the face of uncertainty and Thompson Sampling are not asymptotically optimal for linear bandits.
- Why unresolved: The paper only considers the K-armed bandit setting and does not explore more complex bandit models like linear bandits.
- What evidence would resolve it: Experimental results comparing the performance of abstention-based algorithms to state-of-the-art algorithms in linear bandit settings, demonstrating whether abstention can lead to enhanced theoretical guarantees.

### Open Question 2
- Question: Can the abstention option be modeled more generally to account for its influence on the stochastic observation from the selected arm?
- Basis in paper: [explicit] The paper mentions that the abstention option does not influence the stochastic observation from the selected arm and suggests exploring more sophisticated and general approaches to model the effect of the abstention option.
- Why unresolved: The paper assumes a simple model where abstention does not affect the observation, but real-world scenarios might involve more complex interactions.
- What evidence would resolve it: Theoretical analysis and empirical results comparing the performance of algorithms with different abstention models, showing the impact of the abstention option on the observation process.

### Open Question 3
- Question: Is it possible to design a generalized strategy for the fixed-regret setting that achieves both asymptotic and minimax optimality, similar to the fixed-reward setting?
- Basis in paper: [explicit] The paper notes that the numerous intricacies involved in the fixed-regret setting preclude the formulation of a generalized strategy like the one designed for the fixed-reward setting.
- Why unresolved: The fixed-regret setting is inherently more complex than the fixed-reward setting, making it challenging to design a universal strategy that maintains optimality properties.
- What evidence would resolve it: A new algorithm for the fixed-regret setting that achieves both asymptotic and minimax optimality, along with a rigorous theoretical analysis proving its optimality.

## Limitations

- The algorithms rely on accurate estimation of arm means and confidence bounds, which may not hold in all scenarios.
- The computational complexity of tracking multiple confidence bounds and abstention criteria may become prohibitive for very large bandit instances.
- The theoretical guarantees are derived under specific conditions and may not generalize to all problem instances.

## Confidence

- **Asymptotic Optimality Claims**: High Confidence
- **Minimax Optimality Claims**: Medium Confidence
- **Empirical Validation Claims**: Medium Confidence

## Next Checks

1. Test the algorithms' performance when the empirical mean estimates have additional noise or bias, to assess their robustness to imperfect knowledge of arm means.
2. Systematically vary the abstention reward/penalty (c) and observe its effect on cumulative regret, to understand the sensitivity of the algorithms to this key parameter.
3. Benchmark the proposed algorithms against the most recent and competitive bandit algorithms that do not incorporate abstention, to quantify the actual benefit of the abstention option in practice.