---
ver: rpa2
title: Boosting Few-Shot Learning with Disentangled Self-Supervised Learning and Meta-Learning
  for Medical Image Classification
arxiv_id: '2403.17530'
source_url: https://arxiv.org/abs/2403.17530
tags:
- dataset
- learning
- classification
- pre-training
- meta-learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a novel approach for enhancing the performance
  of models trained in a few-shot learning regime by leveraging the strengths of disentangled
  self-supervised learning (SSL) and meta-learning. The method starts with a pre-training
  phase where SSL is used to learn robust feature representations, and the IP-IRM
  algorithm is employed to disentangle these features.
---

# Boosting Few-Shot Learning with Disentangled Self-Supervised Learning and Meta-Learning for Medical Image Classification

## Quick Facts
- arXiv ID: 2403.17530
- Source URL: https://arxiv.org/abs/2403.17530
- Reference count: 40
- Primary result: Proposed method achieves superior few-shot performance on prostate and breast cancer classification tasks

## Executive Summary
This paper proposes a novel approach for enhancing few-shot learning in medical image classification by combining disentangled self-supervised learning (SSL) with meta-learning. The method leverages IP-IRM to disentangle features during SSL pre-training, followed by meta-fine-tuning using a granularity shift strategy. Experiments on prostate and breast cancer datasets demonstrate consistent performance improvements over ablation baselines, particularly in low-shot regimes.

## Method Summary
The approach consists of two main phases: pre-training and fine-tuning. During pre-training, SimCLR with IP-IRM is used on unlabeled data to learn disentangled feature representations. In the fine-tuning phase, a meta-learning approach based on Meta DeepBDC is applied, where meta-training uses finer-grained classes and meta-testing uses coarser-grained classes. This granularity shift exposes the model to more diverse tasks during training while maintaining clinical relevance during evaluation.

## Key Results
- The full pipeline consistently outperforms ablation experiments across both prostate and breast cancer datasets
- IP-IRM disentanglement shows particular benefit in 1-shot regimes, where standard SimCLR performs poorly
- The granularity shift strategy improves generalization by increasing task diversity during meta-training
- Results remain robust even with distribution shifts between training and evaluation data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IP-IRM disentangles features beyond augmentation-related ones, enabling the model to learn class-specific features rather than spurious correlations.
- Mechanism: IP-IRM iteratively partitions the unlabeled dataset into two subsets based on a feature to be disentangled, then minimizes the invariance of the SSL loss across the two subsets. This forces the model to learn representations invariant to the partitioned feature, thereby isolating class-relevant information.
- Core assumption: Features that vary with the environment (e.g., imaging vendor) are spurious correlations, and invariant features are class-specific.
- Evidence anchors:
  - [section]: "To learn more disentangled features beyond augmentations during SSL, Wang et al. [9] proposed a new method called Iterative Partition-based Invariant Risk Minimization (IP-IRM)."
  - [section]: "In the first step, the parameters of ϕ are updated by: min_Φ Σ_P∈P 2Σ_k=1 [L + λ1||∇θ=1L||²]"
  - [corpus]: Weak—no direct citation of IP-IRM improving disentanglement in few-shot settings.
- Break condition: If the partitioned feature is not truly spurious (e.g., class-relevant texture differences across vendors), IP-IRM may discard useful information.

### Mechanism 2
- Claim: Meta-fine-tuning with granularity shift (finer classes in meta-train, coarser in meta-test) improves generalization by exposing the model to more diverse and structured classification tasks.
- Mechanism: Meta-training uses a larger set of finer-grained classes, increasing the number of ways and episode diversity, while meta-testing uses fewer, broader classes that are clinically relevant. This structure trains the model on harder, more varied tasks, enhancing its ability to generalize to simpler but meaningful tasks.
- Core assumption: Finer-grained classes provide more diverse training tasks, and coarser-grained classes preserve clinical relevance while being easier to classify.
- Evidence anchors:
  - [section]: "We propose a novel approach wherein the model is tasked with similar classification tasks during meta-training and meta-testing but at different levels of granularity."
  - [section]: "Specifically, the meta-training classes (Cf ine) constitute subgroups of the meta-testing ones (Ccoarse), i.e., |Cf ine| > |Ccoarse|."
  - [corpus]: Weak—no direct evidence that granularity shift improves few-shot generalization; related papers focus on other aspects.
- Break condition: If the granularity shift is too extreme, the model may not learn useful distinctions for the coarser classes.

### Mechanism 3
- Claim: Pre-training with disentangled SSL improves downstream few-shot classification by providing a more robust feature representation.
- Mechanism: SimCLR+IP-IRM pre-training generates feature embeddings that are less sensitive to spurious correlations and more focused on class-relevant structure, which enhances the effectiveness of subsequent meta-fine-tuning.
- Core assumption: Disentangled features are more transferable and lead to better generalization in low-data regimes.
- Evidence anchors:
  - [section]: "Our work builds on a simple idea: leveraging the power of disentangled SSL as a pre-training step to extract robust feature representations and coupling it with the generalization capabilities of a meta-learning framework."
  - [section]: "Specifically, we use IP-IRM to pre-train a convolutional backbone to generate robust features and then fine-tune it with a meta-learning approach based on Prototypical Networks called Meta Deep Brownian Distance Covariance (Meta DeepBDC)."
  - [corpus]: Weak—no direct citation that disentangled SSL improves few-shot learning; related work focuses on SSL without disentanglement.
- Break condition: If the disentanglement process overfits to the pre-training data distribution, it may hurt transfer to the target domain.

## Foundational Learning

- Concept: Invariant Risk Minimization (IRM)
  - Why needed here: IRM ensures that the learned representations are invariant across different environments (e.g., different imaging vendors), reducing spurious correlations.
  - Quick check question: What is the objective function used in IRM to enforce invariance across environments?

- Concept: Prototypical Networks and Meta-Learning
  - Why needed here: Prototypical Networks provide a way to learn from few examples by constructing class prototypes and classifying based on distance, which is crucial for few-shot learning.
  - Quick check question: How does the DeepBDC variant of Prototypical Networks differ from the original in terms of similarity measurement?

- Concept: Self-Supervised Learning (SSL) and Contrastive Learning
  - Why needed here: SSL with contrastive objectives (e.g., SimCLR) learns useful feature representations without labels, which is essential for pre-training in low-data regimes.
  - Quick check question: What is the main limitation of vanilla SimCLR in learning class-relevant features?

## Architecture Onboarding

- Component map:
  - Unlabeled dataset (Di_unlab) -> SimCLR + IP-IRM -> Feature backbone -> Labeled dataset (Di_lab) -> Meta DeepBDC with granularity shift -> Classification output

- Critical path: Pre-training -> Fine-tuning -> Evaluation
  - Pre-training generates disentangled features
  - Fine-tuning adapts to few-shot tasks with meta-learning
  - Evaluation measures AUROC on held-out meta-test episodes

- Design tradeoffs:
  - Tradeoff between disentanglement strength and feature preservation
  - Granularity shift vs. dataset consistency in meta-training
  - Use of AUC-M loss vs. cross-entropy for few-shot classification

- Failure signatures:
  - Pre-training failure: No improvement over vanilla SimCLR, especially in 1-shot regimes
  - Meta-learning failure: Performance drops when switching from meta-training to meta-testing classes
  - Evaluation failure: High variance across meta-test episodes, indicating poor generalization

- First 3 experiments:
  1. Train SimCLR+IP-IRM on unlabeled PI-CAI dataset, then fine-tune with Meta DeepBDC using finer-grained classes in meta-train and coarser in meta-test.
  2. Train SimCLR alone on same unlabeled data, then fine-tune with same meta-learning setup; compare AUROC to experiment 1.
  3. Train on BreakHis dataset with same pipeline; evaluate binary AUROC and compare with fully-supervised baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method compare to other state-of-the-art few-shot learning approaches on medical image datasets?
- Basis in paper: [explicit] The paper presents results comparing the proposed method to ablation experiments and a baseline fully-supervised model, but does not compare to other state-of-the-art few-shot learning approaches.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the proposed method through ablation studies and comparisons with a baseline model, but does not include comparisons with other state-of-the-art few-shot learning approaches.
- What evidence would resolve it: Including experiments comparing the proposed method to other state-of-the-art few-shot learning approaches on the same medical image datasets would provide evidence to answer this question.

### Open Question 2
- Question: How does the proposed method perform on medical image datasets with different levels of class imbalance?
- Basis in paper: [inferred] The paper mentions the use of the Area Under the ROC Curve (AUROC) as an evaluation metric, which is more stable than accuracy in the presence of class imbalance. However, the paper does not explicitly investigate the performance of the proposed method on datasets with different levels of class imbalance.
- Why unresolved: The paper does not provide experiments or analysis on the performance of the proposed method on medical image datasets with varying levels of class imbalance.
- What evidence would resolve it: Conducting experiments on medical image datasets with different levels of class imbalance and analyzing the performance of the proposed method in each case would provide evidence to answer this question.

### Open Question 3
- Question: How does the proposed method handle medical image datasets with missing or incomplete annotations?
- Basis in paper: [inferred] The paper mentions the use of an unlabeled dataset (Di_unlab) for pre-training, but does not explicitly discuss the handling of missing or incomplete annotations in the labeled dataset (Di_lab).
- Why unresolved: The paper does not provide information on how the proposed method deals with missing or incomplete annotations in the labeled dataset.
- What evidence would resolve it: Including experiments or analysis on the performance of the proposed method when trained on medical image datasets with missing or incomplete annotations would provide evidence to answer this question.

## Limitations

- The individual mechanisms (IP-IRM disentanglement, granularity shift) lack isolated ablation studies to validate their specific contributions
- The claim that IP-IRM learns "class-relevant features" assumes the partitioned feature is truly spurious, which may not hold for clinically meaningful vendor-specific patterns
- Performance comparisons are limited to ablation baselines and a fully-supervised model, lacking comparison to other state-of-the-art few-shot learning approaches

## Confidence

- **High Confidence**: The overall experimental results showing superior performance of the full pipeline compared to ablation baselines on both PI-CAI and BreakHis datasets
- **Medium Confidence**: The claim that pre-training with disentangled SSL improves downstream few-shot classification, based on comparative results but without isolated mechanism validation
- **Low Confidence**: The assertion that IP-IRM specifically learns "class-relevant features" rather than simply invariant features, as this requires feature-level analysis not provided in the paper

## Next Checks

1. **Isolate IP-IRM Effectiveness**: Run an experiment comparing SimCLR alone vs. SimCLR+IP-IRM pre-training, followed by identical fine-tuning procedures, to directly measure disentanglement's contribution to few-shot performance.

2. **Validate Granularity Shift Strategy**: Create an ablation where meta-training uses the same classes as meta-testing (no granularity shift) but with equal or greater task diversity, to determine whether the granularity structure itself drives performance improvements.

3. **Feature Analysis for IP-IRM**: Conduct a feature importance or attribution analysis comparing representations learned with and without IP-IRM, specifically examining whether IP-IRM removes vendor-specific correlations while preserving clinically relevant texture differences.