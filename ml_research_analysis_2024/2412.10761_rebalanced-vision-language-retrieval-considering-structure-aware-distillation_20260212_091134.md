---
ver: rpa2
title: Rebalanced Vision-Language Retrieval Considering Structure-Aware Distillation
arxiv_id: '2412.10761'
source_url: https://arxiv.org/abs/2412.10761
tags:
- retrieval
- cross-modal
- uni00000013
- uni0000002c
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies that cross-modal retrieval models underperform
  when modalities have imbalanced information content. It proposes multi-granularity
  distillation that combines instance-level contrastive loss with representation-level
  distillation and structure-aware distillation.
---

# Rebalanced Vision-Language Retrieval Considering Structure-Aware Distillation

## Quick Facts
- arXiv ID: 2412.10761
- Source URL: https://arxiv.org/abs/2412.10761
- Authors: Yang Yang; Wenjuan Xi; Luping Zhou; Jinhui Tang
- Reference count: 40
- Primary result: Multi-granularity distillation improves cross-modal retrieval by 3.1-11.4% Recall@1 while enhancing single-modal retrieval by up to 14.1% NDCG@10

## Executive Summary
This paper addresses the problem of modality imbalance in vision-language retrieval where cross-modal models underperform due to insufficient information in one modality. The proposed method combines cross-modal matching with multi-granularity distillation that includes instance-level contrastive loss, representation-level distillation, and structure-aware distillation via relational matching. By using single-modal teacher models to guide the cross-modal student and adaptively learning modality fusion coefficients, the approach achieves state-of-the-art performance on MS-COCO, Flickr30K, and VizWiz datasets while simultaneously improving single-modal retrieval capabilities.

## Method Summary
The method employs multi-granularity distillation where a cross-modal student model learns from two single-modal teacher models (vision and text). It combines cross-modal matching loss with distillation losses at different granularities: representation-level contrastive loss for intra-modal consistency and structure-aware distillation using relational matching to transfer optimal instance relationships from teachers. A learnable fusion coefficient λ adaptively combines teacher similarity matrices to preserve structural information while enabling cross-modal alignment. The approach addresses modality imbalance by maintaining single-modal structure during cross-modal learning, preventing the degradation of single-modal retrieval performance typically seen in traditional cross-modal models.

## Key Results
- Improves cross-modal retrieval Recall@1 by 3.1-11.4% compared to state-of-the-art methods
- Enhances single-modal retrieval performance by up to 14.1% NDCG@10 compared to baseline cross-modal models
- Demonstrates effectiveness across three challenging datasets: MS-COCO, Flickr30K, and VizWiz
- Shows adaptive fusion coefficient λ discovers optimal modality weighting for different instances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-granularity distillation preserves single-modal structure while enabling cross-modal matching
- Mechanism: Uses two teacher networks to provide optimal instance relationships through structure-aware distillation via relational matching, while representation-level distillation maintains intra-modal consistency through contrastive loss
- Core assumption: Best single-modal retrieval models capture optimal instance structure that can be transferred to cross-modal models without harming cross-modal alignment
- Evidence anchors:
  - [abstract]: "structure-aware distillation further regularizes the geometric consistency between learned matching representations and intra-modal representations through the developed relational matching"
  - [section]: "We employ single-modal teacher models to guide the cross-modal model, implementing multi-granularity distillation that incorporates structural distillation [39] on top of representational distillation"
  - [corpus]: Weak evidence - corpus papers focus on dataset distillation and PEFT, not structural preservation in cross-modal retrieval
- Break condition: If teacher models do not capture optimal structure or if modality gap is too large for relational matching to bridge

### Mechanism 2
- Claim: Adaptive fusion coefficient λ discovers optimal modality weighting for distillation
- Mechanism: Learnable parameter λ combines teacher similarity matrices to find best modality fusion for preserving instance relationships, addressing that weak modalities cannot fully represent strong modalities
- Core assumption: Relative importance of modalities varies by instance and can be learned rather than fixed
- Evidence anchors:
  - [abstract]: "adaptively learn the fusion coefficient of two teachers"
  - [section]: "For measuring the teacher-student relational consistency, we adopt the Mean Absolute Error (MAE) between two matrices"
  - [corpus]: Weak evidence - corpus focuses on general cross-modal retrieval but doesn't specifically address adaptive modality weighting
- Break condition: If learned λ consistently favors one modality regardless of instance characteristics

### Mechanism 3
- Claim: Multi-granularity approach prevents modality collapse while improving both cross-modal and single-modal retrieval
- Mechanism: Combination of cross-modal matching loss with multi-granularity distillation creates balanced optimization maintaining cross-modal alignment while preserving intra-modal structure
- Core assumption: Joint optimization of cross-modal consistency and single-modal structure preservation is feasible and beneficial
- Evidence anchors:
  - [abstract]: "simultaneously enhancing single-modal retrieval capabilities compared to the baseline models"
  - [section]: "This effectively alleviates the disruption of structural information during modal alignment"
  - [corpus]: Weak evidence - corpus papers don't directly address dual objective of improving both cross-modal and single-modal retrieval simultaneously
- Break condition: If combined loss optimization leads to conflicting gradients that prevent convergence

## Foundational Learning

- Concept: Cross-modal contrastive learning
  - Why needed here: Paper builds on standard contrastive learning frameworks but extends them to handle modality imbalance
  - Quick check question: How does the temperature parameter τ affect the similarity distribution in contrastive loss?

- Concept: Knowledge distillation in multi-modal contexts
  - Why needed here: Method uses teacher-student architecture where single-modal teachers guide cross-modal student learning
  - Quick check question: What is the difference between representation-level and structure-aware distillation in this context?

- Concept: Modality sufficiency and imbalance
  - Why needed here: Core problem addressed is that modalities have different information content, affecting retrieval performance
  - Quick check question: How does the paper quantify modality "strength" and what metric indicates imbalance?

## Architecture Onboarding

- Component map:
  Vision encoder (SwinT) → Image representations
  Text encoder (BERT) → Text representations
  Cross-modal encoder → Joint representations with cross-attention
  Teacher models (single-modal) → Structure preservation
  Multi-granularity distillation module → Balancing objectives

- Critical path: Image/Text → Encoders → Cross-modal encoder → Distillation module → Final representations

- Design tradeoffs:
  Single vs dual teacher models (paper uses single-modal teachers for simplicity)
  Fixed vs learnable λ (paper chooses learnable for adaptability)
  MAE vs other distance metrics for structure preservation (paper chooses MAE for flexibility)

- Failure signatures:
  Single-modal retrieval performance drops significantly after training
  Cross-modal retrieval performance improves but at cost of modal imbalance
  Distillation loss dominates and prevents cross-modal learning

- First 3 experiments:
  1. Ablation study removing ℓsa to measure impact of structure-aware distillation
  2. Parameter sweep for λ to verify adaptive fusion effectiveness
  3. Batch size analysis to understand neighborhood size effects on distillation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the degree of modality imbalance quantitatively affect the performance of cross-modal retrieval models?
- Basis in paper: [explicit] The paper states "we can employ the performance of the optimal single-modal model to assess the 'strength' of a modality" and demonstrates that modality imbalance affects retrieval performance, but does not provide a quantitative relationship.
- Why unresolved: The paper identifies the phenomenon of modality imbalance and its negative effects, but does not establish a mathematical relationship or threshold values for when imbalance becomes detrimental.
- What evidence would resolve it: Experiments measuring cross-modal retrieval performance across datasets with varying degrees of modality imbalance, showing a clear correlation between imbalance metrics and performance degradation.

### Open Question 2
- Question: Does the proposed multi-granularity distillation approach maintain its effectiveness when extended to more than two modalities?
- Basis in paper: [inferred] The paper focuses on two modalities (image and text) and mentions "addressing the challenges posed by modality imbalance in cross-modal retrieval tasks involving more than two modalities represents a significant area for further investigation."
- Why unresolved: The method is only validated on image-text retrieval tasks, and the authors explicitly identify multi-modal extension as future work without providing evidence of effectiveness.
- What evidence would resolve it: Empirical validation of the multi-granularity distillation approach on datasets with three or more modalities, showing maintained or improved performance compared to baseline methods.

### Open Question 3
- Question: What is the theoretical relationship between the structure preservation mechanism and improved cross-modal retrieval performance?
- Basis in paper: [explicit] The paper claims structure preservation helps "promote geometric symmetry in the latent common space" but does not provide a theoretical justification for why this leads to better retrieval.
- Why unresolved: While the paper demonstrates empirical improvements, it does not provide mathematical proof or theoretical framework explaining how structure preservation directly contributes to better similarity measurement in cross-modal retrieval.
- What evidence would resolve it: A formal theoretical analysis showing how the proposed structure-aware distillation affects the geometry of the latent space and mathematically deriving why this geometry improves retrieval performance.

## Limitations

- Evidence for mechanism effectiveness relies heavily on paper's own assertions without external validation
- Adaptive fusion coefficient mechanism lacks empirical justification for why learnable λ would outperform fixed weighting strategies
- Dual-objective optimization claim is supported by experimental results but lacks analysis of potential conflicts during training

## Confidence

- **High confidence**: Core experimental results showing improved Recall@1 scores (3.1-11.4%) and single-modal retrieval improvements (up to 14.1% NDCG@10) are well-documented with proper baseline comparisons
- **Medium confidence**: Mechanism explanations for why multi-granularity distillation works, particularly the structure-aware component, are logical but lack rigorous mathematical proof or ablation studies that isolate individual contributions
- **Low confidence**: Claims about superiority of adaptive λ over fixed weighting schemes and assertion that teacher models capture "optimal" instance relationships are not substantiated with comparative analysis

## Next Checks

1. **Ablation study**: Remove the structure-aware distillation component and measure the degradation in both cross-modal and single-modal retrieval performance to isolate its contribution
2. **Teacher model analysis**: Compare performance using teacher models trained with different objectives (unsupervised contrastive vs supervised) to validate the claim about "optimal" instance relationships
3. **Parameter sensitivity**: Conduct systematic experiments varying the temperature τ in contrastive losses and the relative weighting of ℓcr vs ℓmd to identify optimal configurations and verify robustness