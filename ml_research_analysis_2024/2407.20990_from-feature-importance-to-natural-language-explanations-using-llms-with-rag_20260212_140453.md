---
ver: rpa2
title: From Feature Importance to Natural Language Explanations Using LLMs with RAG
arxiv_id: '2407.20990'
source_url: https://arxiv.org/abs/2407.20990
tags:
- explanations
- feature
- importance
- responses
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes traceable question-answering for explainable
  AI by integrating post-hoc explainability techniques with large language models
  (LLMs) through retrieval-augmented generation (RAG). The method extracts feature
  importance via subtractive counterfactual reasoning, decomposing semantic features
  to quantify their impact on model predictions.
---

# From Feature Importance to Natural Language Explanations Using LLMs with RAG

## Quick Facts
- arXiv ID: 2407.20990
- Source URL: https://arxiv.org/abs/2407.20990
- Reference count: 40
- Key outcome: GPT-4 achieved 76% success rate in identifying relevant causes for scene classifications, outperforming GPT-3.5's 24%

## Executive Summary
This work proposes a traceable question-answering approach for explainable AI by integrating post-hoc explainability techniques with large language models (LLMs) through retrieval-augmented generation (RAG). The method computes feature importance using subtractive counterfactual reasoning, which quantifies how individual features contribute to model predictions by analyzing output variations when features are removed. An external knowledge repository containing model outputs, feature importance values, and contrastive cases grounds LLM responses to ensure factual consistency and traceability. Evaluation across nine scene classes demonstrated that GPT-4 significantly outperformed GPT-3.5 in identifying relevant causes, employing contrastive explanations, and generating simpler responses.

## Method Summary
The approach employs decomposition-based explanation to compute feature importance using counterfactual reasoning. It systematically replaces feature values with NaN and observes prediction probability changes, calculating importance as the relative position of the perturbed probability within the range of maximum and minimum probabilities across all perturbations. The knowledge repository stores model outputs, features, contrastive cases, and feature importance values in tabular .csv format. A system prompt incorporating social, causal, selective, and contrastive characteristics from social science research guides LLM response generation. The traceable question-answering system uses RAG to ground LLM responses in this external knowledge rather than allowing unconstrained generation.

## Key Results
- GPT-4 outperformed GPT-3.5 across all evaluation categories: identifying relevant causes (76% vs 24%), employing contrastive explanations, and generating simpler responses
- The approach successfully generated traceable natural language explanations for scene understanding tasks
- RAG integration prevented hallucinations by grounding LLM responses in external knowledge sources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The traceable question-answering approach ensures factual consistency by grounding LLM responses in external knowledge sources.
- Mechanism: Retrieval-Augmented Generation (RAG) provides the LLM with contextual details about model outputs, feature importance, and contrastive cases before generating responses. This prevents hallucinations by limiting the LLM to processing and reasoning about provided data rather than generating information from scratch.
- Core assumption: The external knowledge repository contains accurate and complete information about the model's predictions and feature importance values.
- Evidence anchors:
  - [abstract] "This knowledge repository comprises contextual details regarding the model's output, containing high-level features, feature importance, and alternative probabilities."
  - [section 3.2] "Traceable question-answering, through a process known as Retrieval-Augmented Generation (RAG), integrates external knowledge sources to furnish responses to user inquiries."
  - [corpus] Weak evidence - only mentions RAG in abstract without implementation details.
- Break condition: If the external knowledge repository contains incomplete, outdated, or inaccurate information about model predictions or feature importance values.

### Mechanism 2
- Claim: Subtractive counterfactual reasoning quantifies feature importance by measuring prediction changes when features are removed.
- Mechanism: The approach systematically replaces feature values with NaN (undefined) and observes prediction probability changes. Feature importance is calculated as the relative position of the perturbed probability within the range of maximum and minimum probabilities across all perturbations.
- Core assumption: Removing a feature value with NaN creates a meaningful counterfactual scenario that reveals the feature's causal contribution to the prediction.
- Evidence anchors:
  - [section 3.1] "We employ subtractive counterfactual reasoning to compute feature importance, a method that entails analysing output variations resulting from decomposing semantic features."
  - [section 3.1] "This approximates an importance value between zero and one for each input feature."
  - [corpus] Weak evidence - mentions perturbation-based methods but doesn't detail this specific implementation.
- Break condition: If the model cannot handle NaN values appropriately or if feature interactions make individual feature removal an inadequate measure of importance.

### Mechanism 3
- Claim: Integrating social science research characteristics into LLM prompts creates more human-friendly explanations.
- Mechanism: The system prompt incorporates four characteristics - social (conversational norms), causal (cause-effect relationships), selective (contextually relevant causes), and contrastive (comparison with alternatives) - to guide LLM response generation toward explanations that align with human preferences.
- Core assumption: LLM responses can be effectively guided by prompt engineering to exhibit these specific characteristics consistently.
- Evidence anchors:
  - [section 2.3] "We integrated four key characteristics – social, causal, selective, and contrastive – drawn from social science research on human explanations into a single-shot prompt, guiding the response generation process."
  - [section 4.3] "The findings presented in Table 2 indicate that GPT-4 outperformed GPT-3.5 in all categories."
  - [corpus] Moderate evidence - mentions LLM as post-hoc explainers but doesn't detail prompt engineering for social characteristics.
- Break condition: If the LLM fails to consistently apply the instructed characteristics or if the characteristics conflict with each other in complex scenarios.

## Foundational Learning

- Concept: Feature importance attribution methods
  - Why needed here: The approach relies on decomposing semantic features to quantify their impact on model predictions through counterfactual reasoning.
  - Quick check question: What distinguishes subtractive counterfactual reasoning from other feature importance methods like gradient-based approaches?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: The traceable question-answering system uses RAG to ground LLM responses in external knowledge rather than allowing unconstrained generation.
  - Quick check question: How does RAG prevent hallucinations in LLM responses compared to standard prompting?

- Concept: Social science research on human explanations
  - Why needed here: The system prompt incorporates characteristics from human explanation research to create more natural and comprehensible LLM responses.
  - Quick check question: What are the four key characteristics of human explanations identified by Miller's social science research?

## Architecture Onboarding

- Component map:
  Input data → Semantic segmentation model → Feature extraction → Subtractive counterfactual reasoning → Feature importance calculation → External knowledge repository → RAG system → LLM → Natural language explanation
  Knowledge repository stores: model predictions, feature importance values, contrastive cases, effect of removal values
  System prompt contains: single-shot example, user query patterns, characteristic guidelines

- Critical path: The most critical path is from semantic segmentation output through feature importance calculation to knowledge repository storage, as errors here propagate to all downstream explanations.

- Design tradeoffs:
  - Feature decomposition granularity vs computational cost: More detailed decomposition provides finer-grained importance values but increases computation time
  - Knowledge repository size vs retrieval efficiency: Larger repositories provide more comprehensive context but may slow down RAG retrieval
  - Prompt length vs instruction clarity: Longer prompts can convey more characteristics but may reduce LLM response coherence

- Failure signatures:
  - Inconsistent feature importance values across similar inputs indicates problems with the counterfactual reasoning implementation
  - LLM responses that ignore the knowledge repository suggest RAG retrieval failures
  - Responses lacking conversational elements indicate prompt engineering issues

- First 3 experiments:
  1. Verify counterfactual reasoning by testing feature removal on a simple model with known feature importance and comparing calculated values to ground truth
  2. Test RAG retrieval by querying the knowledge repository with various input formats and ensuring consistent, accurate responses
  3. Evaluate prompt effectiveness by comparing LLM responses with and without the characteristic guidelines to measure improvement in human-friendliness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the traceable question-answering approach with LLMs perform on multimodal explanations compared to unimodal text explanations?
- Basis in paper: [explicit] The paper discusses potential for incorporating multimodal elements like visual explanations but does not evaluate this.
- Why unresolved: The study only evaluated text-based explanations from GPT-3.5 and GPT-4, without incorporating visual elements.
- What evidence would resolve it: Comparative evaluation of traceable explanations using both text-only and text-plus-visual approaches on the same dataset and tasks.

### Open Question 2
- Question: What is the impact of different knowledge repository formats (text, PDF, structured data, images) on the quality and traceability of LLM-generated explanations?
- Basis in paper: [explicit] The paper mentions RAG supports various file formats but only uses tabular .csv files.
- Why unresolved: The study did not experiment with different knowledge repository formats to assess their impact on explanation quality.
- What evidence would resolve it: Systematic comparison of explanation quality across different knowledge repository formats using the same prompting technique and evaluation metrics.

### Open Question 3
- Question: How do human users perceive and interact with the socially-aware LLM explanations compared to standard technical explanations?
- Basis in paper: [explicit] The paper incorporates social characteristics from human explanations but does not evaluate user perceptions.
- Why unresolved: The study evaluated technical aspects of the explanations but did not conduct user studies to assess comprehensibility and user satisfaction.
- What evidence would resolve it: User studies comparing comprehension, trust, and preference between socially-aware LLM explanations and standard technical explanations across different user groups.

## Limitations

- The method's reliance on counterfactual reasoning with NaN value substitution may not capture complex feature interactions in high-dimensional spaces.
- The system assumes the external knowledge repository contains complete and accurate information, but real-world deployment may encounter incomplete or outdated data.
- The evaluation focuses primarily on scene classification tasks, limiting generalizability to other domains.

## Confidence

- High confidence: The RAG mechanism's effectiveness in grounding LLM responses in external knowledge (supported by strong theoretical foundation and implementation details)
- Medium confidence: The superiority of GPT-4 over GPT-3.5 in generating explanations (based on comparative results, but limited to nine scene classes)
- Medium confidence: The effectiveness of incorporating social science characteristics into prompts (supported by evaluation metrics, but subjective nature of explanation quality assessment)

## Next Checks

1. Test the counterfactual reasoning mechanism on a simple model with known ground truth feature importance to verify calculation accuracy and identify potential implementation issues.

2. Conduct ablation studies to isolate the contribution of each characteristic (social, causal, selective, contrastive) in the system prompt by comparing LLM responses with and without each element.

3. Evaluate the approach's generalizability by applying it to a different domain (e.g., tabular data classification) and comparing performance metrics to the scene classification results.