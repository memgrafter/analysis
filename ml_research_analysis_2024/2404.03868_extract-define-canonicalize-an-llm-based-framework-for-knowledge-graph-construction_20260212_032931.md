---
ver: rpa2
title: 'Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph
  Construction'
arxiv_id: '2404.03868'
source_url: https://arxiv.org/abs/2404.03868
tags:
- schema
- triplets
- text
- relation
- relations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes EDC, a framework for knowledge graph construction
  using large language models (LLMs) in a three-phase pipeline: open information extraction,
  schema definition, and schema canonicalization. The method extracts relational triplets
  from text without requiring a predefined schema, defines schema components via LLM-generated
  explanations, and canonicalizes the triplets to reduce redundancy and ambiguity.'
---

# Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction

## Quick Facts
- arXiv ID: 2404.03868
- Source URL: https://arxiv.org/abs/2404.03868
- Authors: Bowen Zhang; Harold Soh
- Reference count: 28
- One-line primary result: EDC achieves up to 0.820 F1 score on WebNLG, significantly outperforming state-of-the-art methods for knowledge graph construction

## Executive Summary
This paper introduces EDC (Extract, Define, Canonicalize), a three-phase framework for knowledge graph construction using large language models. EDC addresses the challenge of handling large and complex schemas that exceed LLM context windows by first extracting relational triplets through open information extraction, then defining schema components via LLM-generated explanations, and finally canonicalizing the triplets to reduce redundancy and ambiguity. The method demonstrates significant improvements over state-of-the-art approaches on three benchmark datasets (WebNLG, REBEL, Wiki-NRE) and successfully constructs high-quality knowledge graphs without predefined schemas.

## Method Summary
EDC is a three-phase framework that constructs knowledge graphs using large language models without requiring predefined schemas. The first phase performs open information extraction to extract relational triplets from text. The second phase defines schema components by generating natural language explanations for each relation and entity type. The third phase canonicalizes the extracted triplets using vector similarity and LLM verification to reduce redundancy and align to a target schema or self-generated schema. A Schema Retriever component can be added to improve extraction by retrieving relevant schema elements. The method is flexible enough to handle both target alignment (when a predefined schema exists) and self-canonicalization (when no schema is provided).

## Key Results
- EDC achieves up to 0.820 F1 score on WebNLG benchmark, significantly outperforming state-of-the-art methods
- The method successfully constructs high-quality knowledge graphs without predefined schemas, outperforming clustering-based methods in self-canonicalization tasks
- Refinement further improves performance, with diminishing returns after one iteration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Open information extraction followed by post-hoc schema definition and canonicalization allows handling of large and complex schemas that exceed LLM context windows.
- Mechanism: By first extracting triplets without schema constraints and then separately defining schema components and canonicalizing, the method avoids embedding the full schema in the prompt, circumventing context window limits.
- Core assumption: LLMs can reliably extract semantically correct triplets without schema guidance and can generate meaningful schema definitions post-hoc.
- Evidence anchors:
  - [abstract] "A principal issue is that, in prior methods, the KG schema has to be included in the LLM prompt to generate valid triplets; larger and more complex schemas easily exceed the LLMs' context window length."
  - [section] "Phases 2 and 3 (Define and Canonicalize) standardize the triplets to make them useful for downstream tasks."
- Break condition: If LLMs fail to generate accurate schema definitions or canonicalization fails to resolve semantic ambiguities, the approach breaks down.

### Mechanism 2
- Claim: Schema Retriever improves extraction by retrieving schema elements relevant to the input text, akin to retrieval-augmented generation.
- Mechanism: The Schema Retriever projects schema components and input text into a vector space where cosine similarity captures relevance, allowing the LLM to access contextually appropriate schema elements during extraction.
- Core assumption: The vector space similarity between text and schema elements correlates with the likelihood of the schema element being present in the text.
- Evidence anchors:
  - [section] "To further improve performance, the three steps above can be followed by an additional Refinement phase: we repeat EDC but provide the previously extracted triplets and a relevant part of the schema in the prompt during the initial extraction."
  - [section] "We propose a trained Schema Retriever that retrieves schema components relevant to the input text, akin to retrieval-augmented generation (Lewis et al., 2020), which we find improves the generated triplets."
- Break condition: If the Schema Retriever fails to retrieve relevant schema elements or retrieves too many irrelevant ones, the LLM's extraction quality degrades.

### Mechanism 3
- Claim: The three-phase decomposition allows EDC to work in both target alignment and self-canonicalization settings without requiring predefined schemas.
- Mechanism: By separating extraction, schema definition, and canonicalization, EDC can either align to a provided schema or dynamically create and canonicalize against a self-generated schema.
- Core assumption: LLMs can both define schemas based on extracted triplets and canonicalize against either a provided or self-generated schema.
- Evidence anchors:
  - [abstract] "EDC is flexible in that it can be applied to settings where a pre-defined target schema is available and when it is not; in the latter case, it constructs a schema automatically and applies self-canonicalization."
  - [section] "Unlike target alignment, components deemed non-transformable are added to the canonical schema, thereby expanding it."
- Break condition: If the self-generated schema becomes too large or inconsistent, or if alignment to a target schema fails, the method's flexibility is compromised.

## Foundational Learning

- Concept: Open Information Extraction (OIE)
  - Why needed here: OIE allows extraction of triplets without schema constraints, which is essential for handling large or unknown schemas.
  - Quick check question: What is the main difference between open information extraction and closed information extraction?

- Concept: Schema Canonicalization
  - Why needed here: Canonicalization reduces redundancy and ambiguity in the extracted triplets by standardizing semantically equivalent entities and relations.
  - Quick check question: Why is schema canonicalization necessary after open information extraction?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG techniques like the Schema Retriever improve LLM performance by providing relevant context from a knowledge base.
  - Quick check question: How does retrieval-augmented generation improve the performance of LLMs in knowledge graph construction?

## Architecture Onboarding

- Component map:
  OIE module → Schema Definition module → Schema Canonicalization module → (Optional Refinement)

- Critical path:
  OIE → Schema Definition → Schema Canonicalization → (Optional Refinement)

- Design tradeoffs:
  - Separating schema definition from extraction allows handling larger schemas but increases latency and cost.
  - Using LLMs for all components provides flexibility but may be expensive compared to fine-tuned smaller models.

- Failure signatures:
  - Poor extraction quality may indicate issues with the OIE module or insufficient schema retriever performance.
  - Inconsistent canonicalization may suggest problems with schema definition or alignment.
  - High redundancy in the output KG may indicate insufficient canonicalization.

- First 3 experiments:
  1. Test OIE module alone on a small dataset to verify basic extraction capability.
  2. Add schema definition and canonicalization to check end-to-end performance on a simple schema.
  3. Introduce schema retriever and refinement to evaluate performance improvements on larger schemas.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of refinement iterations for EDC to balance performance gains against computational costs?
- Basis in paper: [explicit] The paper mentions diminishing returns after one refinement iteration and briefly tests two iterations in Appendix C.2.
- Why unresolved: The paper only tests up to two iterations without systematically exploring the trade-off between performance improvement and increased computational cost.
- What evidence would resolve it: A systematic study testing multiple refinement iterations (e.g., 1-5) across different datasets to quantify performance gains and cost increases, identifying an optimal stopping point.

### Open Question 2
- Question: How would replacing the Schema Retriever with a smaller, fine-tuned model affect EDC's performance and efficiency?
- Basis in paper: [explicit] The paper suggests this as a potential future direction in the Limitations section, noting that smaller language models could be fine-tuned for schema retrieval.
- Why unresolved: The paper uses the full E5-Mistral-7b model for schema retrieval without exploring whether smaller models could achieve similar performance.
- What evidence would resolve it: Experiments replacing the Schema Retriever with progressively smaller fine-tuned models, measuring performance impact and computational efficiency.

### Open Question 3
- Question: What is the impact of combining OIE and Schema Definition steps on EDC's consistency and cost-effectiveness?
- Basis in paper: [explicit] Appendix G mentions preliminary experiments combining these steps showed slight performance gains and token cost reduction on REBEL.
- Why unresolved: The paper only briefly mentions this combination without comprehensive evaluation across multiple datasets or analysis of consistency improvements.
- What evidence would resolve it: Systematic experiments across multiple datasets comparing separate vs. combined OIE/Schema Definition steps, measuring both performance metrics and token costs, with analysis of consistency in extracted relations.

## Limitations

- The method's reliance on LLM-based components raises questions about computational cost and scalability for large-scale knowledge graph construction.
- The Schema Retriever's effectiveness depends heavily on the quality of schema element representations and similarity measures, which are not fully detailed in the paper.
- The evaluation focuses primarily on benchmark datasets with relatively clean and structured text, leaving uncertainty about performance on noisy, real-world data.

## Confidence

- High confidence in the core claims regarding EDC's effectiveness on benchmark datasets, particularly the significant performance improvements over state-of-the-art methods (up to 0.820 F1 score on WebNLG).
- Medium confidence in the claims about EDC's flexibility in handling both predefined and self-generated schemas, as the evaluation of self-canonicalization is less extensive.
- Medium confidence in the Schema Retriever's contribution to performance improvements, as the specific implementation details and ablation studies are limited.

## Next Checks

1. **Cost-Effectiveness Analysis**: Conduct a detailed analysis of the computational costs (API calls, latency) of the EDC pipeline compared to traditional fine-tuned models, particularly for large-scale knowledge graph construction tasks.

2. **Robustness Testing**: Evaluate EDC's performance on noisy, real-world text data from diverse domains to assess its robustness beyond benchmark datasets with structured text.

3. **Ablation Study on Schema Retriever**: Perform a comprehensive ablation study to quantify the exact contribution of the Schema Retriever to overall performance, including analysis of different similarity measures and representation strategies.