---
ver: rpa2
title: A Concise Mathematical Description of Active Inference in Discrete Time
arxiv_id: '2406.07726'
source_url: https://arxiv.org/abs/2406.07726
tags: []
core_contribution: This paper presents a concise mathematical framework for active
  inference in discrete time. The core method involves using a generative model to
  represent environmental dynamics, with agents selecting actions by sampling from
  a policy distribution defined via expected free energy minimization.
---

# A Concise Mathematical Description of Active Inference in Discrete Time

## Quick Facts
- arXiv ID: 2406.07726
- Source URL: https://arxiv.org/abs/2406.07726
- Authors: Jesse van Oostrum; Carlotta Langer; Nihat Ay
- Reference count: 6
- Presents a concise mathematical framework for active inference in discrete time with Python implementation

## Executive Summary
This paper presents a rigorous mathematical framework for active inference in discrete time, focusing on state inference, action selection, and learning mechanisms. The framework uses a generative model with Dirichlet priors to represent environmental dynamics, where agents select actions by sampling from a policy distribution defined via expected free energy minimization. The paper provides detailed derivations of Bayesian filtering for state inference and learning updates, along with a T-maze example that demonstrates the theory in practice. The work includes Python code compatible with pymdp environments, offering an accessible implementation of the theoretical framework.

## Method Summary
The method involves constructing a generative model with Dirichlet priors over parameters, performing Bayesian state inference using exact posterior computation, and selecting actions through expected free energy minimization. State inference updates beliefs using Bayes' rule with observation likelihood, then propagates beliefs forward using transition dynamics. Action selection computes expected free energy for all policies, balancing epistemic value (information gain) and utility (expected reward), then samples actions from a softmax distribution. Learning uses Bayesian updating with Dirichlet priors to maintain parameter uncertainty after executing actions and receiving observations.

## Key Results
- Provides a complete mathematical formulation of active inference in discrete time
- Derives state inference, action selection, and learning mechanisms from first principles
- Demonstrates the framework with a T-maze example that illustrates policy-based decision making
- Includes Python implementation compatible with pymdp environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Action selection in active inference uses expected free energy minimization to balance exploration and exploitation.
- Mechanism: The agent computes expected free energy G(πt) for each policy πt, which combines epistemic value (information gain) and utility (expected reward), then samples actions from a softmax distribution over G values.
- Core assumption: The generative model p(s1:T, o1:T|a1:T-1) correctly represents environmental dynamics and the agent's preferences pC are accurately specified.
- Evidence anchors:
  - [abstract] "agents selecting actions by sampling from a policy distribution defined via expected free energy minimization"
  - [section] "G(πt, o1:t, a1:t-1) = − (Eqt(ot+1:T|πt)[DKL(qt(st+1:T|ot+1:T, πt) ∥ qt(st+1:T|πt))] + Eqt(ot+1:T|πt)[ln pC(ot+1:T)])"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: If the generative model is misspecified or preferences pC don't match true rewards, the agent will make suboptimal decisions.

### Mechanism 2
- Claim: State inference updates beliefs using Bayesian filtering with exact posterior computation.
- Mechanism: At each time step, the agent updates its belief about current state using Bayes' rule with observation likelihood, then propagates beliefs forward for future states using transition dynamics.
- Core assumption: The agent can perform exact Bayesian inference or accurate approximations are available for the filtering updates.
- Evidence anchors:
  - [section] "q(st|o1:t, a1:t-1) ∝ p(ot|st) ∑st-1 p(st|st-1, at-1)q(st-1|o1:t-1, a1:t-2)"
  - [section] "q(sτ|o1:t, a1:τ-1) = ∑sτ-1 p(sτ|sτ-1, aτ-1)q(sτ-1|o1:t, a1:τ-2)"
  - [corpus] Weak - corpus focuses on demonstrations rather than inference mechanisms
- Break condition: If the state space is too large for exact inference or approximations are poor, belief updates will be inaccurate.

### Mechanism 3
- Claim: Learning uses Bayesian updating with Dirichlet priors to maintain parameter uncertainty.
- Mechanism: After executing actions and receiving observations, the agent updates hyperparameters α for Dirichlet distributions over model parameters using posterior beliefs about states and transitions.
- Core assumption: Dirichlet priors are conjugate to the categorical distributions used in the generative model, allowing closed-form updates.
- Evidence anchors:
  - [section] "αD′j = αDj + q(s(j)1|o1:T, π)" and similar updates for θA and θB parameters
  - [section] "we adopt a Bayesian belief updating scheme with a Dirichlet prior"
  - [corpus] Weak - corpus evidence focuses on applications rather than learning mechanisms
- Break condition: If the true posterior isn't Dirichlet or the model structure changes, these updates become suboptimal.

## Foundational Learning

- Concept: Bayesian inference and conjugate priors
  - Why needed here: The entire framework relies on Bayesian updating of beliefs and parameters, with Dirichlet priors enabling tractable learning updates
  - Quick check question: What property makes Dirichlet priors useful for categorical distributions in this context?

- Concept: Variational free energy and its relationship to Bayesian inference
  - Why needed here: Expected free energy is the key quantity being minimized for action selection, and understanding its relationship to KL divergence and evidence lower bounds is crucial
  - Quick check question: How does minimizing expected free energy relate to balancing exploration (epistemic value) and exploitation (utility)?

- Concept: Discrete-time filtering and prediction
  - Why needed here: State inference requires sequential updating of beliefs based on observations and actions, using prediction and update steps
  - Quick check question: What are the two main steps in Bayesian filtering for state inference?

## Architecture Onboarding

- Component map: Generative model (p) -> State inference engine -> Expected free energy computation -> Action selector -> Environment -> Learning module -> Updated generative model
- Critical path: Observation → State inference → Expected free energy computation → Action selection → Environment interaction → Learning update
- Design tradeoffs:
  - Exact vs approximate inference: Exact Bayesian filtering is computationally expensive for large state spaces
  - Time horizon length: Longer horizons improve planning but increase computational cost quadratically
  - Policy space size: Enumerating all policies is intractable for long horizons or many actions
- Failure signatures:
  - Poor performance despite correct implementation: Check if generative model matches environment dynamics
  - Slow learning: Verify Dirichlet hyperparameter updates are being applied correctly
  - Unstable behavior: Check softmax temperature in action selection and policy enumeration
- First 3 experiments:
  1. Implement the T-maze example exactly as specified and verify action selection matches paper's expected behavior
  2. Test state inference with synthetic data where ground truth is known to verify belief updates are correct
  3. Implement learning updates and verify parameter estimates converge to true values in a simple environment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the mathematical relationship between the variational free energy formulation and the expected free energy formulation of active inference, and under what conditions do they yield identical policy selection results?
- Basis in paper: [explicit] The paper discusses both formulations in Section 2.3 and Appendix A.2, noting they can be equivalent but the relationship is complex
- Why unresolved: The paper mentions the relationship is "unclear" and that "the provided evidence seems incomplete" regarding when the variational free energy minimization yields the same policy selection as expected free energy minimization
- What evidence would resolve it: A formal proof showing the conditions under which the two formulations produce identical policy distributions, including specific cases where they differ

### Open Question 2
- Question: How do the learning updates (equations 9-11) derived from Dirichlet priors relate to the exact Bayesian posterior updates, and what is the error introduced by this approximation?
- Basis in paper: [explicit] The paper states "the true posteriors over these parameters will not be Dirichlet distributions" and questions whether the claim that deriving equation (18) w.r.t. variational parameters α' gives the learning rules is "true"
- Why unresolved: The paper acknowledges this is a standard approximation in active inference literature but notes the theoretical justification appears incomplete
- What evidence would resolve it: A rigorous mathematical analysis comparing the Dirichlet approximation to exact Bayesian posteriors, including quantitative bounds on approximation error

### Open Question 3
- Question: What are the computational trade-offs between exact state inference using equation (7) and the approximate methods mentioned in the literature?
- Basis in paper: [inferred] The paper mentions in Section 2.2 that "State inference as described in e.g. [4, 3, 5] can be thought of as computationally efficient approximations of what is described here"
- Why unresolved: While the paper provides the exact formulation, it doesn't analyze or compare the computational complexity or accuracy of approximate methods
- What evidence would resolve it: Empirical benchmarks comparing inference accuracy and computational runtime across different state inference methods on various problem sizes and complexity levels

## Limitations

- Exact Bayesian inference becomes computationally prohibitive for large state spaces
- Performance is highly sensitive to accurate specification of the generative model
- Scalability issues arise when enumerating all possible policies for long time horizons

## Confidence

- State inference derivations: High
- Expected free energy formulation: High
- Dirichlet learning updates: Medium
- Practical implementation details: Medium
- Scalability to complex environments: Low

## Next Checks

1. **Scalability Test**: Implement the framework on a larger grid-world with state spaces exceeding 100 states to measure computational scaling and verify whether approximate inference methods are needed.

2. **Robustness Analysis**: Systematically vary the accuracy of the generative model (e.g., introduce noise in transition probabilities) and measure performance degradation to identify failure thresholds.

3. **Hyperparameter Sensitivity**: Conduct ablation studies on Dirichlet hyperparameters and softmax temperature to determine optimal settings and identify which parameters most affect learning speed and policy quality.