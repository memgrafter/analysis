---
ver: rpa2
title: Learning Counterfactual Distributions via Kernel Nearest Neighbors
arxiv_id: '2410.13381'
source_url: https://arxiv.org/abs/2410.13381
tags:
- kernel
- distribution
- data
- distributions
- adoption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses distributional matrix completion, where the
  goal is to estimate a multivariate distribution for each unit-outcome entry when
  only finite samples from some entries are observed. The proposed Kernel-NN method
  generalizes nearest neighbors to the distributional setting using reproducing kernels
  and maximum mean discrepancies, with a latent factor model on kernel mean embeddings
  enabling consistent recovery even under missing not at random data and positivity
  violations.
---

# Learning Counterfactual Distributions via Kernel Nearest Neighbors

## Quick Facts
- arXiv ID: 2410.13381
- Source URL: https://arxiv.org/abs/2410.13381
- Reference count: 40
- This paper proposes Kernel-NN, a distributional generalization of nearest neighbors for matrix completion that uses kernel mean embeddings and maximum mean discrepancies to estimate missing distributions, achieving consistent recovery under missing not at random data and positivity violations.

## Executive Summary
This paper addresses distributional matrix completion, where the goal is to estimate a multivariate distribution for each unit-outcome entry when only finite samples from some entries are observed. The proposed Kernel-NN method generalizes nearest neighbors to the distributional setting using reproducing kernels and maximum mean discrepancies, with a latent factor model on kernel mean embeddings enabling consistent recovery even under missing not at random data and positivity violations. The method is robust to heteroscedastic noise when multiple measurements per entry are available. Empirical results on simulated and HeartSteps data show Kernel-NN effectively recovers distributions and outperforms or matches scalar matrix completion baselines on downstream tasks.

## Method Summary
Kernel-NN extends the scalar nearest neighbors algorithm to handle distribution imputation by replacing Euclidean distances with U-statistic-based maximum mean discrepancies (MMD) between empirical distributions. The algorithm computes row-wise MMD distances between all pairs of rows using available columns, identifies neighbors within a radius η for the target row, and estimates missing distributions as MMD barycenters of observed distributions in the target column for those neighbors. The method assumes a latent factor structure on kernel mean embeddings and achieves consistent recovery as the number of units N increases, with robustness to heteroscedastic noise when n≥2 measurements per entry are available.

## Key Results
- Kernel-NN achieves consistent recovery of underlying distributions in synthetic data with known latent factor structure as N increases
- The method demonstrates robustness to heteroscedastic noise when multiple measurements per entry are available (n≥2), outperforming scalar NN methods
- On HeartSteps data, Kernel-NN recovers distributions with lower MMD error compared to scalar matrix completion baselines and achieves comparable or superior performance on downstream tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Kernel-NN algorithm generalizes nearest neighbors to the distributional setting by using kernel mean embeddings and maximum mean discrepancies (MMD) as distance metrics.
- Mechanism: Instead of using Euclidean distance between scalar values, Kernel-NN computes MMD between empirical distributions of multiple measurements per entry. It then uses MMD barycenters to estimate missing distributions.
- Core assumption: The underlying distributions have a latent factor structure on their kernel mean embeddings (Assum. 1).
- Evidence anchors:
  - [abstract]: "introduce a kernel based distributional generalization of nearest neighbors to estimate the underlying distributions"
  - [section 3]: "We extend the scalar nearest neighbors algorithm to handle distribution imputation, and we do so by extending the notion of distance in (8) and average in (9) so that it is suitable for handling multi-dimensional distributions"
  - [corpus]: Weak - no direct evidence about distributional extensions of NN methods
- Break condition: If the latent factor structure assumption fails, the consistent recovery guarantees no longer hold.

### Mechanism 2
- Claim: Kernel-NN is robust to heteroscedastic noise when multiple measurements per entry are available.
- Mechanism: By using U-statistics to construct unbiased estimates of MMD distances, the algorithm debiases the distance calculation, allowing for arbitrary variances across entries.
- Core assumption: At least two measurements per observed entry (n≥2).
- Evidence anchors:
  - [abstract]: "demonstrate that our nearest neighbors approach is robust to heteroscedastic noise, provided we have access to two or more measurements for the observed unit-outcome entries"
  - [section 3]: "we show that our methodKERNEL-NN recovers the underlying target distribution as a whole while allowing for arbitrary variances (as well as arbitrary higher moments for appropriate kernels) across (j, s)"
  - [corpus]: Weak - no direct evidence about noise robustness in distributional settings
- Break condition: With only one measurement per entry (n=1), the method reduces to scalar NN and loses heteroscedastic noise robustness.

### Mechanism 3
- Claim: The instance-dependent bound enables principled hyper-parameter tuning without cross-validation.
- Mechanism: The bound provides a computable upper bound on the MMD error as a function of the hyper-parameter η, allowing direct optimization rather than expensive cross-validation.
- Core assumption: Non-trivial observation overlap between rows (so that the denominator in distance calculations is non-zero).
- Evidence anchors:
  - [section 4.1]: "The instance dependent bound motivates a principled and fast optimization procedure for choosing the hyper-parameter η, which does not rely on cross-validation"
  - [section 5.1]: "we propose to minimize the fully data-driven version of the bound (18)"
  - [corpus]: Weak - no direct evidence about principled hyper-parameter tuning in distributional matrix completion
- Break condition: If observation overlap is minimal, the bound becomes vacuous and the optimization procedure fails.

## Foundational Learning

- Concept: Reproducing kernel Hilbert spaces (RKHS) and kernel mean embeddings
  - Why needed here: The entire distributional estimation framework relies on representing distributions as elements in an RKHS, enabling the use of kernel methods for distribution comparison and manipulation
  - Quick check question: What is the reproducing property of a kernel k, and how does it relate to the kernel mean embedding of a distribution?

- Concept: Maximum mean discrepancy (MMD) and its U-statistic estimator
  - Why needed here: MMD provides a metric for comparing distributions in the RKHS, and the U-statistic estimator enables unbiased distance calculations between empirical distributions
  - Quick check question: How does the U-statistic estimator of MMD differ from the V-statistic estimator, and why is this distinction important for the algorithm's robustness?

- Concept: Factor models and latent variable structures
  - Why needed here: The assumption that distributions have a factor structure on their kernel mean embeddings is the key modeling assumption that enables consistent recovery
  - Quick check question: In the context of kernel mean embeddings, what does it mean for a distribution to have a factor structure, and how does this relate to traditional matrix factorization?

## Architecture Onboarding

- Component map:
  Input: Kernel function k, observed measurements Z, missingness pattern A, hyper-parameter η, target index (i,t)
  Core algorithm: Kernel-NN with two steps - distance calculation using U-statistic MMD, MMD barycenter estimation
  Output: Estimated distribution bµi,t,η
  Supporting components: Latent factor model, theoretical guarantees, hyper-parameter tuning procedure

- Critical path:
  1. Compute row-wise MMD distances between all pairs of rows using available columns
  2. Identify neighbors within radius η for target row
  3. Compute MMD barycenter of observed distributions in the target column for those neighbors
  4. Return the barycenter as the estimated distribution

- Design tradeoffs:
  - Computational complexity vs. accuracy: Larger η includes more neighbors but increases bias; smaller η reduces bias but may have high variance
  - Kernel choice vs. evaluation metric: The kernel used in the algorithm must match the kernel used for evaluation to ensure meaningful guarantees
  - Number of measurements per entry (n) vs. robustness: n≥2 enables heteroscedastic noise robustness through U-statistics debiasing

- Failure signatures:
  - Poor neighbor identification: When observation overlap is minimal, the distance calculations become unreliable and neighbor sets may be empty or contain irrelevant entries
  - Hyper-parameter sensitivity: If η is too small, variance dominates; if too large, bias dominates
  - Kernel mismatch: Using a kernel for the algorithm that doesn't match the evaluation metric leads to inconsistent guarantees

- First 3 experiments:
  1. Synthetic data with known latent factor structure and staggered adoption pattern - verify consistent recovery as N increases
  2. Simulated heteroscedastic noise - compare performance with n=1 vs n≥2 to demonstrate noise robustness
  3. HeartSteps data with varying hyper-parameter η - validate that the direct optimization procedure matches cross-validation performance while being faster

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Kernel-NN perform when there is only a single measurement per entry (n=1) compared to multiple measurements?
- Basis in paper: [explicit] The paper discusses that Kernel-NN with a linear kernel and V-statistics recovers scalar matrix completion methods when n=1, but shows superior performance with n≥2 due to unbiased MMD estimation.
- Why unresolved: The paper primarily focuses on the n≥2 setting and only briefly mentions the n=1 case. Empirical comparisons between these regimes are not provided.
- What evidence would resolve it: Empirical studies comparing Kernel-NN performance on simulated and real data with both n=1 and n≥2, measuring MMD error and robustness to heteroscedastic noise.

### Open Question 2
- Question: Can the Kernel-NN framework be extended to settings where column latent factors are not independent (violating Assumption 2)?
- Basis in paper: [explicit] The paper notes that Assumption 2 (independence across column latent factors) is more stringent and suggests relaxing it as future work.
- Why unresolved: The current theoretical guarantees rely on this independence assumption, and no alternative models or analysis techniques for dependent columns are provided.
- What evidence would resolve it: Extension of the theoretical framework to handle dependent columns, either through modified assumptions or new proof techniques, along with empirical validation.

### Open Question 3
- Question: What is the computational complexity of Kernel-NN when combined with kernel thinning for distribution compression?
- Basis in paper: [explicit] The discussion section mentions that kernel thinning could reduce runtime from O(NTn²d) to O(NTn(d+log³n)) but does not provide empirical verification.
- Why unresolved: The theoretical claim is made but no experiments are conducted to verify the speedup or examine potential trade-offs in accuracy.
- What evidence would resolve it: Implementation of Kernel-NN with kernel thinning on benchmark datasets, comparing both runtime and MMD error against standard Kernel-NN.

## Limitations
- The theoretical guarantees rely heavily on the latent factor structure assumption, which may not hold for all real-world distributions
- The computational complexity O(N T n² d) could become prohibitive for large-scale applications
- The method requires at least two measurements per observed entry (n≥2) to achieve heteroscedastic noise robustness, limiting its applicability in data-scarce scenarios

## Confidence
- **High confidence**: The core algorithmic framework and its extension from scalar NN to distributional NN using MMD and kernel mean embeddings is well-established and mathematically sound
- **Medium confidence**: The empirical results on synthetic and HeartSteps data demonstrate the method's effectiveness, though the sample sizes are relatively modest (N=200, T=10)
- **Low confidence**: The robustness claims under positivity violations and MNAR data are theoretically supported but lack extensive empirical validation across diverse real-world scenarios

## Next Checks
1. **Robustness testing**: Systematically evaluate performance under varying degrees of missing not at random patterns and positivity violations using synthetic data with controlled ground truth
2. **Scalability benchmarking**: Implement the computational optimizations suggested in the paper and benchmark runtime scaling on synthetic datasets with increasing N, T, n, and d
3. **Cross-method comparison**: Compare Kernel-NN against the Wasserstein-space distributional matrix completion method [9] on the same HeartSteps dataset to validate the claim of comparable or superior performance