---
ver: rpa2
title: 'Pralekha: Cross-Lingual Document Alignment for Indic Languages'
arxiv_id: '2411.19096'
source_url: https://arxiv.org/abs/2411.19096
tags:
- alignment
- document
- indic
- lidf
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of mining parallel document pairs
  for document-level machine translation (MT), which is hindered by limitations in
  existing Cross-Lingual Document Alignment (CLDA) techniques. The authors introduce
  PRALEKHA, a large-scale benchmark containing over 3 million aligned document pairs
  across 11 Indic languages and English, and propose Document Alignment Coefficient
  (DAC), a novel metric for fine-grained document alignment.
---

# Pralekha: Cross-Lingual Document Alignment for Indic Languages

## Quick Facts
- arXiv ID: 2411.19096
- Source URL: https://arxiv.org/abs/2411.19096
- Reference count: 13
- Introduces PRALEKHA benchmark with 3M+ aligned document pairs and Document Alignment Coefficient (DAC) metric achieving 15-20% precision improvement over baselines

## Executive Summary
This paper addresses the challenge of mining parallel document pairs for document-level machine translation (MT) in Indic languages, where existing cross-lingual document alignment (CLDA) techniques face limitations. The authors introduce PRALEKHA, a large-scale benchmark containing over 3 million aligned document pairs across 11 Indic languages and English, and propose Document Alignment Coefficient (DAC), a novel metric for fine-grained document alignment. DAC aligns documents by matching smaller chunks rather than pooling entire documents into single embeddings, computing similarity as the ratio of aligned chunks to the average number of chunks in a pair.

Intrinsic evaluation shows that DAC achieves average precision, recall, and F1 scores of 0.8932, 0.6312, and 0.7372, respectively, outperforming pooling-based baselines by 15-20% in precision and 5-10% in F1 score. Additionally, DAC demonstrates a 2-3× speedup over sentence-based alignment techniques while maintaining competitive performance. Extrinsic evaluation further demonstrates that document-level MT models trained on DAC-aligned pairs consistently outperform those trained with baseline alignment methods.

## Method Summary
The authors propose DAC, a cross-lingual document alignment approach that segments documents into chunks (typically 4-8 sentences) and aligns them using multilingual embeddings (LABSE or SONAR) with FAISS for efficient similarity search. Unlike pooling-based methods that represent entire documents as single embeddings, DAC preserves fine-grained semantic signals by aligning smaller units first. The DAC score is computed as the ratio of aligned chunks to the average number of chunks, providing a normalized measure of document similarity. The method allows control over precision-recall trade-offs through thresholding and demonstrates both computational efficiency and alignment quality improvements over existing approaches.

## Key Results
- DAC achieves average precision of 0.8932, recall of 0.6312, and F1 score of 0.7372, outperforming pooling-based baselines by 15-20% in precision and 5-10% in F1 score
- DAC demonstrates 2-3× speedup over sentence-based alignment techniques while maintaining competitive performance
- Document-level MT models trained on DAC-aligned pairs consistently outperform those trained with baseline alignment methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DAC achieves higher precision by aligning documents through smaller chunks rather than pooling entire documents into single embeddings
- Mechanism: By segmenting documents into smaller units (chunks) and aligning these individually, DAC preserves fine-grained semantic signals that are lost in document-level pooling. The alignment score (DAC) is computed as the ratio of aligned chunks to the average number of chunks, providing a normalized measure of document similarity
- Core assumption: Chunk-level embeddings capture more semantic nuance than pooled document embeddings, and these smaller units can be reliably aligned across languages
- Evidence anchors:
  - [abstract] "Unlike pooling-based methods, DAC aligns documents by matching smaller chunks and computes similarity as the ratio of aligned chunks to the average number of chunks in a pair."
  - [section 4.4] "Unlike baseline approaches that operate directly on pooled document embeddings, DAC performs chunk-level alignment first and then derives document pairs using a computed score based on these chunk alignments, thereby providing finer-grained semantic signals that improve alignment precision."

### Mechanism 2
- Claim: DAC is computationally efficient because it reduces the search space by aligning fewer, larger units compared to sentence-level alignment
- Mechanism: By increasing granularity from sentences to chunks (e.g., 4-8 sentences per chunk), DAC reduces the number of embedding comparisons needed. This leads to a 2-3× speedup over sentence-based methods while maintaining competitive alignment quality
- Core assumption: The reduction in the number of units to align outweighs any potential loss in alignment accuracy from using coarser granularities
- Evidence anchors:
  - [section 6.1] "Computational time decreases sharply from 47 to 18 minutes for LaBSE and from 115 to 35 minutes for SONAR as G increases from 1 to 8, resulting in roughly a 2-3× reduction in runtime."
  - [section 4.4] "This efficiency gain stems from a combinatorial reduction in the candidate search space: at higher granularities, each document is represented by fewer, longer chunks, leading to substantially fewer embedding computations and pairwise similarity operations during retrieval."

### Mechanism 3
- Claim: DAC produces higher-quality training data for document-level MT by prioritizing precision over recall
- Mechanism: DAC's threshold-based approach allows control over the precision-recall trade-off. A higher DAC threshold ensures fewer incorrect alignments (higher precision), which is critical for training high-quality MT models where noisy data can degrade performance
- Core assumption: In MT training, the quality of aligned document pairs is more important than the quantity, and precision has a greater impact on downstream performance than recall
- Evidence anchors:
  - [abstract] "DAC achieves average precision, recall, and F1 scores of 0.8932, 0.6312, and 0.7372, respectively, showing improvements of 15-20% in precision and 5-10% in F1 score over the baselines."
  - [section 6.1] "While its recall is slightly lower, DAC attains competitive or superior F1 scores, indicating that it produces more precise alignments, a desirable property for high-quality MT training data, where quality is more important than quantity."

## Foundational Learning

- Concept: Cross-lingual document alignment
  - Why needed here: The paper addresses the challenge of aligning documents across languages for document-level MT, which requires understanding how documents can be semantically matched despite linguistic differences
  - Quick check question: What is the main difference between sentence-level and document-level alignment, and why is document-level alignment more challenging?

- Concept: Embedding models and context windows
  - Why needed here: DAC relies on multilingual embedding models to represent chunks of text, and the effectiveness of alignment depends on whether these embeddings can capture semantic information within the chunk size used
  - Quick check question: How does the context window size of an embedding model affect its ability to represent longer text spans like document chunks?

- Concept: Precision-recall trade-offs in alignment
  - Why needed here: DAC allows control over alignment quality through thresholds, and understanding the trade-off between precision and recall is crucial for setting appropriate parameters based on downstream task requirements
  - Quick check question: In what scenarios would you prioritize precision over recall in document alignment, and how does this choice affect the resulting training data?

## Architecture Onboarding

- Component map: Document segmentation -> Embedding model -> Alignment algorithm -> DAC computation -> Thresholding
- Critical path:
  1. Segment documents into chunks based on chosen granularity
  2. Compute embeddings for all chunks using multilingual model
  3. Build FAISS indices and retrieve top-k nearest neighbors
  4. Compute margin scores and select aligned chunk pairs
  5. Calculate DAC for each document pair
  6. Apply threshold to obtain final aligned document pairs

- Design tradeoffs:
  - Granularity vs. efficiency: Finer granularity (G=1) provides better alignment but is computationally expensive; coarser granularity (G=4,8) is faster but may lose some alignment quality
  - Precision vs. recall: Higher DAC thresholds increase precision but reduce recall, affecting the size and quality of the aligned dataset
  - Embedding model choice: LABSE works better with DAC while SONAR performs better with pooling methods, reflecting their different training objectives

- Failure signatures:
  - Low precision despite high DAC scores: Chunk boundaries may be splitting semantic units, or embeddings may not capture cross-lingual semantic equivalence
  - Poor alignment quality for certain language pairs: Embedding model may not have sufficient multilingual coverage or may struggle with specific linguistic features
  - Excessive computational cost: Granularity may be too fine, or FAISS indexing may not be properly optimized for the dataset size

- First 3 experiments:
  1. Run DAC with G=1 (sentence-level) using LABSE embeddings on a small subset of PRALEKHA to verify basic functionality and measure baseline performance
  2. Test DAC with G=4 using both LABSE and SONAR embeddings to compare performance and identify which embedding model works better with chunk-based alignment
  3. Vary DAC threshold from 0.05 to 0.2 to observe precision-recall trade-offs and determine optimal threshold for downstream MT training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DAC performance vary across document lengths beyond the 20-30 sentence range observed in PRALEKHA?
- Basis in paper: [inferred] The paper notes that PRALEKHA documents range from 20-30 sentences and that performance on very short or long documents is largely untested
- Why unresolved: The dataset's length distribution limits understanding of DAC's behavior on extreme cases
- What evidence would resolve it: Testing DAC on corpora with documents significantly shorter or longer than 20-30 sentences to measure precision, recall, and F1 score changes

### Open Question 2
- Question: Does DAC maintain effectiveness when applied to noisier, domain-diverse data like social media or OCR outputs?
- Basis in paper: [inferred] The paper acknowledges PRALEKHA is high-quality and structured, but lacks representation of noisy web data, informal language, OCR errors, or code-switching
- Why unresolved: Current evaluation doesn't reflect performance in real-world noisy environments
- What evidence would resolve it: Evaluating DAC on datasets containing informal text, OCR errors, or multilingual code-switched documents and comparing alignment quality

### Open Question 3
- Question: How well does DAC handle semantic paraphrasing where translations are equivalent but structurally different?
- Basis in paper: [inferred] The paper mentions DAC may be less robust when translations are semantically equivalent but highly paraphrased
- Why unresolved: No quantitative analysis of DAC's paraphrasing robustness was conducted
- What evidence would resolve it: Testing DAC on datasets with intentionally paraphrased parallel texts and measuring alignment accuracy compared to exact matches

### Open Question 4
- Question: What is the relationship between alignment quality metrics and actual document-level MT performance?
- Basis in paper: [explicit] The paper notes that extrinsic results confirm DAC effectiveness, but intrinsic performance doesn't always translate to downstream gains, and current MT metrics capture discourse-level phenomena only to a limited extent
- Why unresolved: Current evaluation frameworks don't fully establish correlation between alignment metrics and translation quality
- What evidence would resolve it: Developing document-level MT metrics that better correlate with alignment quality and testing them across different alignment approaches

## Limitations

- Language pair bias: The evaluation primarily focuses on English-Indic language pairs, with limited analysis of how DAC performs across different Indic language combinations
- Chunk boundary sensitivity: The paper does not systematically evaluate how different chunk segmentation strategies affect alignment quality
- Embedding model dependence: The superior performance of DAC with LABSE versus SONAR embeddings suggests strong dependence on the choice of multilingual model

## Confidence

**High confidence**: The core mechanism of chunk-level alignment is well-supported by the evidence showing 15-20% precision improvements over pooling methods. The computational efficiency claims are directly supported by runtime measurements. The precision-recall trade-off analysis is empirically validated through threshold experiments.

**Medium confidence**: The claim that DAC produces higher-quality MT training data relies on extrinsic evaluation with specific model architectures. While results are positive, the generalizability to other MT architectures or downstream tasks remains uncertain.

**Low confidence**: The assertion that DAC's chunk-based approach is universally superior to sentence-level alignment lacks comprehensive ablation studies across different document types, domains, and languages beyond the PRALEKHA corpus.

## Next Checks

1. **Cross-lingual ablation study**: Evaluate DAC performance across non-English Indic language pairs (e.g., Hindi-Marathi, Tamil-Telugu) to determine if the reported precision improvements hold across the full range of language combinations in the PRALEKHA dataset.

2. **Chunk segmentation sensitivity analysis**: Systematically vary chunk boundary detection strategies (fixed sentence counts vs. semantic boundaries) and measure their impact on alignment quality to identify optimal segmentation approaches for different language pairs.

3. **Embedding model generalization test**: Replace LABSE embeddings with newer multilingual models (e.g., multilingual BERT, XLM-R) while keeping the DAC framework unchanged to assess whether chunk-level alignment provides consistent benefits across different embedding architectures.