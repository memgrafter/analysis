---
ver: rpa2
title: Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration
arxiv_id: '2410.18076'
source_url: https://arxiv.org/abs/2410.18076
tags:
- data
- online
- learning
- skills
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies how to use unlabeled offline trajectory data
  to improve online exploration in reinforcement learning. The key insight is that
  such data can be leveraged in two ways: to pretrain low-level skills and to provide
  additional off-policy data for learning a high-level exploration policy.'
---

# Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration
## Quick Facts
- **arXiv ID**: 2410.18076
- **Source URL**: https://arxiv.org/abs/2410.18076
- **Reference count**: 40
- **Primary result**: SUPE improves exploration efficiency in long-horizon sparse-reward RL tasks by leveraging unlabeled offline data for both skill pretraining and off-policy learning

## Executive Summary
This paper addresses the challenge of efficient online exploration in reinforcement learning by leveraging unlabeled prior trajectory data. The key insight is that such data can be utilized in two complementary ways: first, to pretrain low-level skills that can be reused during online exploration, and second, to provide additional off-policy data for learning a high-level exploration policy. The proposed method, SUPE (Skill Utilization for Prior Exploration), extracts skills using a variational autoencoder (VAE), pseudo-labels offline trajectories with optimistic rewards and high-level actions, and incorporates these as extra data for online reinforcement learning. Experiments across 42 long-horizon sparse-reward tasks demonstrate that SUPE consistently outperforms prior methods, achieving faster goal-reaching and higher normalized returns.

## Method Summary
SUPE leverages unlabeled prior trajectory data in two ways to improve online exploration. First, it extracts reusable low-level skills using a VAE trained on offline trajectories. These skills are then used to generate a diverse set of action sequences that can be evaluated during online exploration. Second, the method pseudo-labels the offline trajectories with optimistic rewards and high-level actions, treating them as additional off-policy data for training the online policy. During online exploration, the agent can either execute low-level skills directly or use the high-level policy to select between different skills based on the current state. This dual utilization of prior data allows the agent to explore more efficiently by leveraging both pre-learned behaviors and additional training samples.

## Key Results
- SUPE consistently outperforms prior methods across 42 long-horizon sparse-reward tasks
- The method achieves faster goal-reaching and higher normalized returns compared to baselines
- Using offline data twice—both for skill pretraining and online RL—is shown to be crucial for efficient exploration

## Why This Works (Mechanism)
SUPE works by addressing two key challenges in exploration: the difficulty of discovering useful behaviors from scratch and the inefficiency of learning from limited online interactions. By pretraining skills on prior data, the method provides the agent with a diverse set of pre-learned behaviors that can be directly applied during exploration. This reduces the search space and allows the agent to focus on selecting and sequencing useful skills rather than learning everything from scratch. The pseudo-labeling of offline trajectories with optimistic rewards ensures that the agent doesn't discard potentially valuable experiences, while still maintaining a bias toward exploration. The combination of skill reuse and additional off-policy data creates a more data-efficient learning process that can discover sparse rewards more quickly.

## Foundational Learning
- **Variational Autoencoders (VAEs)**: Used to extract low-level skills from offline trajectories by learning a latent representation of action sequences. Needed to create a compact, reusable skill space from unlabeled data. Quick check: Ensure the VAE reconstructs trajectories accurately and the latent space captures meaningful behavioral patterns.
- **Off-policy Reinforcement Learning**: Allows incorporation of pseudo-labeled offline data into online policy training. Needed to leverage prior data without requiring online interaction. Quick check: Verify that the policy can learn effectively from mixed online/offline data distributions.
- **Skill-based Exploration**: Decomposes complex behaviors into reusable skills that can be selected and sequenced. Needed to reduce the exploration burden in long-horizon tasks. Quick check: Test whether pretrained skills can be combined to solve new tasks not seen during pretraining.
- **Pseudo-labeling with Optimistic Rewards**: Assigns optimistic values to offline state-action pairs to encourage exploration. Needed to balance exploitation of known good behaviors with exploration of uncertain ones. Quick check: Compare performance with different reward bonus schemes to validate the optimism bias.

## Architecture Onboarding
- **Component Map**: Unlabeled Offline Data -> VAE Skill Extractor -> Pretrained Skills + Pseudo-labeled Data -> Online RL Policy (High-level + Low-level)
- **Critical Path**: The core innovation flows through skill extraction (VAE), pseudo-labeling, and dual utilization during online RL. The high-level policy selects skills based on current state, while the low-level policy can execute either primitive actions or pretrained skills.
- **Design Tradeoffs**: Using a VAE for skill extraction trades reconstruction accuracy for latent space quality and diversity. Pseudo-labeling with optimistic rewards trades off exploration efficiency with potential overestimation bias. The two-stage skill utilization (pretraining + online selection) trades implementation complexity for improved exploration efficiency.
- **Failure Signatures**: Poor skill diversity from VAE training indicates insufficient offline data or inappropriate latent space dimensionality. Ineffective pseudo-labels suggest overly optimistic rewards that mislead the policy. Failure to improve over baselines may indicate that pretraining doesn't transfer well to the target task distribution.
- **First Experiments**: 1) Train VAE on offline data and visualize latent space to assess skill diversity. 2) Test skill transfer by executing pretrained skills in the target environment without online learning. 3) Compare online performance with and without pseudo-labeled offline data to isolate its contribution.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The method relies on pseudo-labels for offline trajectories, which may introduce bias if the optimistic rewards don't accurately reflect true values
- Performance gains are primarily demonstrated on continuous control tasks from the D4RL benchmark, with limited testing on other domains
- The approach's sensitivity to the quality and diversity of the unlabeled offline dataset is not thoroughly explored

## Confidence
- **Empirical performance claims**: High - systematic ablations and comparisons across multiple tasks demonstrate consistent improvements
- **Theoretical contributions**: Medium - intuitive justifications provided but lacks rigorous theoretical guarantees for skill transferability or pseudo-label effectiveness
- **Broader applicability**: Low - limited domain diversity and sensitivity analysis prevent confident generalization to other settings

## Next Checks
1. Conduct sensitivity analysis on offline dataset quality/diversity to quantify robustness to label noise and data distribution shifts
2. Test transferability of pretrained skills to entirely new task families not seen during pretraining or online fine-tuning
3. Compare against alternative skill discovery methods (e.g., mutual information-based or contrastive approaches) to isolate the contribution of the VAE-based skill extractor