---
ver: rpa2
title: Exploring bat song syllable representations in self-supervised audio encoders
arxiv_id: '2409.12634'
source_url: https://arxiv.org/abs/2409.12634
tags:
- syllable
- speech
- audio
- self-supervised
- available
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how well self-supervised audio models trained
  on human-generated sounds can distinguish between bat song syllable types, using
  territorial songs from the Greater Sac-Winged Bat (Saccopteryx bilineata). The authors
  compare four self-supervised audio encoders (HuBERT and Wav2Vec2 architectures trained
  on human speech, music, or animal vocalizations) against two simpler feature sets
  (LFCC and MFCC).
---

# Exploring bat song syllable representations in self-supervised audio encoders

## Quick Facts
- arXiv ID: 2409.12634
- Source URL: https://arxiv.org/abs/2409.12634
- Authors: Marianne de Heer Kloots; Mirjam Knörnschild
- Reference count: 0
- The paper finds that self-supervised audio models trained on human speech generate the most distinctive representations of bat song syllable types when vocalizations are shifted into human-audible frequency ranges.

## Executive Summary
This study investigates whether self-supervised audio models trained on human-generated sounds can distinguish between different bat song syllable types. The researchers tested four self-supervised audio encoders (HuBERT and Wav2Vec2 architectures trained on human speech, music, or animal vocalizations) against two simpler feature sets (LFCC and MFCC). They found that models pre-trained on human speech produced the most separable syllable type representations, suggesting that rich representations optimized for a single species' vocal repertoire may be more effective for cross-species transfer learning than those optimized for multiple species or non-vocal sounds.

## Method Summary
The researchers used territorial songs from Greater Sac-Winged Bats (Saccopteryx bilineata), preprocessing the recordings by denoising, applying a high-pass filter at 10 kHz, slowing them down by a factor of 8 to map bat frequencies into human hearing range, and downsampling to 16 kHz. They extracted 768-dimensional feature embeddings from four self-supervised audio encoders (HuBERT and Wav2Vec2 trained on human speech, music, or animal vocalizations) and two baseline feature sets (LFCC and MFCC). To assess separability between syllable types, they used Linear Discriminant Analysis to project features into discriminative directions and calculated silhouette coefficients based on Mahalanobis distances.

## Key Results
- Models pre-trained on human speech generated the most distinctive representations of different bat syllable types
- Self-supervised audio encoders outperformed simpler LFCC and MFCC features in separating syllable types
- Syllable types were most separable when encoded by models trained on human speech rather than music or animal vocalizations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised models pre-trained on human speech learn transferable representations that can distinguish between bat syllable types even when vocalizations are shifted into human-audible frequency ranges.
- Mechanism: The masked prediction task in self-supervised models like HuBERT and Wav2Vec2 forces the network to learn general acoustic features that are not tied to specific species but rather to fundamental patterns in sound structure.
- Core assumption: The acoustic features important for distinguishing speech sounds have enough overlap with those important for distinguishing bat syllables that they transfer effectively.
- Evidence anchors:
  - [abstract] "models pre-trained on human speech generate the most distinctive representations of different syllable types"
  - [section] "We find that the syllable types in our territorial song recordings, when slowed down to the human hearing range, are distinctively encoded by self-supervised audio encoders"
  - [corpus] Weak evidence - corpus neighbors focus on music/lyrics applications rather than cross-species transfer
- Break condition: If bat vocalizations use acoustic features fundamentally different from human speech that are not captured by the masked prediction task, the transfer would fail.

### Mechanism 2
- Claim: Models trained on human speech create more separable syllable representations than those trained on music or multiple animal species.
- Mechanism: The speech-specific training objective (predicting discrete speech units) creates representations optimized for fine-grained phonetic distinctions, which happen to be useful for bat syllable discrimination.
- Core assumption: The representation space learned for human speech is more discriminative for structured vocalizations than the space learned for music or multiple species.
- Evidence anchors:
  - [abstract] "models pre-trained on human speech generate the most distinctive representations of different syllable types"
  - [section] "Interestingly, syllable types are most separable in the models pre-trained on human speech. This indicates that rich representations optimized for a single species' vocal repertoire might form a more promising basis for cross-species transfer learning"
  - [section] "The silhouette coefficient for each sample is defined as (b − a)/max(a, b), where a is the mean distance to all other points in the same cluster, and b is the mean distance to all other points in the next nearest cluster"
- Break condition: If bat syllables use fundamentally different acoustic features than human speech phonemes, the speech-trained models would not show superior separability.

### Mechanism 3
- Claim: Shifting bat vocalizations into human hearing range enables effective use of human-trained models.
- Mechanism: By slowing recordings by a factor of 8, the authors map bat frequencies (15.5 kHz mean F0) into the range that human-trained models were exposed to during pre-training.
- Core assumption: The key acoustic features for distinguishing bat syllables remain identifiable after frequency shifting and downsampling.
- Evidence anchors:
  - [section] "After denoising, we therefore move the songs into the human auditory range by slowing down all recordings in our dataset by a factor of 8. In the slowed down recordings, mean syllable duration is 235 ms (SD: 135 ms) and most energy is contained within the 1-8 kHz frequency band for all syllable types"
  - [section] "We further applied a high-pass filter of 10 kHz"
- Break condition: If critical temporal or spectral features are lost during frequency shifting, the models would fail to distinguish syllables effectively.

## Foundational Learning

- Concept: Linear Discriminant Analysis (LDA)
  - Why needed here: LDA projects high-dimensional feature embeddings into discriminative directions that maximize separation between syllable types
  - Quick check question: How does LDA differ from PCA in terms of what it optimizes for when projecting data?

- Concept: Silhouette coefficient for cluster evaluation
  - Why needed here: Provides a quantitative measure of how well-separated different syllable type clusters are in the feature space
  - Quick check question: What does a silhouette coefficient of 1.0 indicate versus 0.0 or negative values?

- Concept: Self-supervised learning through masked prediction
  - Why needed here: Explains the training mechanism that allows models to learn rich representations without labeled data
  - Quick check question: In HuBERT, what is the model trying to predict during the masked prediction task?

## Architecture Onboarding

- Component map: Raw waveform -> CNN-based waveform encoder -> 12 Transformer layers -> 768-dimensional output embeddings -> Mean-pooling layer -> LDA projection -> Silhouette evaluation
- Critical path: Raw waveform → CNN encoder → Transformer layers → 768-dim embeddings → Mean pooling → LDA projection → Silhouette evaluation
- Design tradeoffs:
  - Mean-pooling vs. more sophisticated temporal modeling (authors chose mean-pooling based on prior success in speech and bioacoustics)
  - Frequency range conversion (necessary for human-trained models but may lose some information)
  - Feature dimensionality (768 chosen by model architecture, balances expressiveness with computational cost)
- Failure signatures:
  - Low silhouette coefficients across all models indicate fundamental issues with representation quality
  - Similar silhouette coefficients across different pre-training types suggest the models aren't learning species-specific advantages
  - Poor separability in LDA projections indicates the feature space isn't discriminative enough
- First 3 experiments:
  1. Test silhouette coefficients on a held-out validation set to check for overfitting
  2. Compare mean-pooling with temporal attention mechanisms for syllable aggregation
  3. Evaluate representations from different Transformer layers to identify where syllable discrimination emerges

## Open Questions the Paper Calls Out

- Does training dataset size explain the superior performance of speech-trained models over the animal vocalization model? The authors note that the animal vocalization model was pre-trained on substantially less audio data than the speech and music models, and suggest that comparing models pre-trained on fewer hours of speech would be needed to determine if dataset size explains the difference.

- What specific acoustic features contribute to the distinctive syllable type representations across different layers of the audio encoders? The authors mention they aim to investigate "what interpretable features contribute to the distinctive syllable type representations across each of the audio encoders' internal layers" in future work.

- Can self-supervised models pre-trained on human speech be fine-tuned to encode non-syllabic features like singer identity in bat vocalizations? The authors note that territorial songs encode singer identity and other features, and mention that "Representations from self-supervised models can be optimized by supervised fine-tuning to encode the most relevant features for specific classification and detection tasks."

## Limitations
- Results are based on a single bat species (Saccopteryx bilineata) and territorial vocalizations, limiting generalizability to other bat species or vocalization types
- The frequency shifting process may lose critical spectral information that could affect model performance
- Mean-pooling over syllable frames may not capture the full temporal structure of bat syllables

## Confidence
- High confidence: The finding that human speech-trained models outperform other pre-training types for bat syllable discrimination, supported by direct silhouette coefficient comparisons
- Medium confidence: The mechanism explaining why speech-trained models work better (speech-optimized representations transfer well), as this requires additional validation
- Low confidence: Generalizability to other bat species or vocalization contexts beyond territorial songs

## Next Checks
1. Implement k-fold cross-validation on the 20 territorial songs to assess model stability and prevent overfitting
2. Compare mean-pooling with attention-based temporal aggregation methods to determine if richer temporal modeling improves syllable discrimination
3. Evaluate feature representations from different Transformer layers to identify where syllable discrimination emerges and whether earlier layers capture species-independent acoustic features