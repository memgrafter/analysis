---
ver: rpa2
title: 'SPAFIT: Stratified Progressive Adaptation Fine-tuning for Pre-trained Large
  Language Models'
arxiv_id: '2405.00201'
source_url: https://arxiv.org/abs/2405.00201
tags:
- fine-tuning
- parameters
- dataset
- layers
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of full fine-tuning
  large language models by proposing a parameter-efficient fine-tuning method called
  SPAFIT. The key idea is to stratify the model layers into three groups based on
  the localization of linguistic knowledge, applying increasingly complex fine-tuning
  methods to deeper layers.
---

# SPAFIT: Stratified Progressive Adaptation Fine-tuning for Pre-trained Large Language Models

## Quick Facts
- **arXiv ID**: 2405.00201
- **Source URL**: https://arxiv.org/abs/2405.00201
- **Reference count**: 33
- **Primary result**: SPAFIT fine-tunes only 1.65-2.24% of parameters while achieving competitive or superior performance to full fine-tuning on 7 out of 9 GLUE tasks

## Executive Summary
This paper introduces SPAFIT, a parameter-efficient fine-tuning method that stratifies transformer layers into three groups based on linguistic knowledge localization. By applying increasingly complex fine-tuning methods (BitFit and LoRA) to deeper layers while freezing earlier layers, SPAFIT achieves competitive performance with significantly fewer parameters. Experiments on the GLUE benchmark show that SPAFIT outperforms other parameter-efficient methods like LoRA and BitFit while fine-tuning only a fraction of the total parameters.

## Method Summary
SPAFIT stratifies the transformer layers into three groups based on the hypothesis that different layers capture different types of linguistic knowledge. Group 1 (early layers) is frozen, Group 2 (middle layers) uses BitFit to fine-tune only bias terms, and Group 3 (late layers) uses LoRA with BitFit to fine-tune both low-rank adapters and bias terms. The method leverages the localization of linguistic knowledge across layers, applying progressively more complex fine-tuning methods to deeper layers that capture more task-specific information.

## Key Results
- SPAFIT fine-tunes only 5.65-7.49 million parameters (1.65-2.24% of total) compared to full fine-tuning's 333.58 million parameters
- Achieved competitive or superior performance to full fine-tuning on 7 out of 9 GLUE tasks
- Outperformed other parameter-efficient methods like LoRA and BitFit while using fewer parameters
- Best performance achieved with N1=4 and N2=9 layer stratification boundaries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linguistic knowledge is localized in different layers of the model, with earlier layers capturing basic syntactic information and later layers capturing more complex semantic relationships.
- Mechanism: SPAFIT leverages this localization by stratifying layers into three groups and applying increasingly complex fine-tuning methods to deeper layers. Earlier layers are frozen, middle layers use BitFit, and later layers use LoRA.
- Core assumption: Different types of linguistic knowledge are localized to specific layers of the model, as evidenced by prior work on BERT and bidirectional language models.
- Evidence anchors:
  - [abstract] "based on the localization of different types of linguistic knowledge to specific layers of the model"
  - [section 3.1] "earlier layers of the network captures basic linguistic knowledge while the later layers captures more complex task specific knowledge"
  - [corpus] Weak - the corpus provides no direct evidence for layer-specific knowledge localization
- Break condition: If experiments show that uniformly applying fine-tuning across all layers performs equally well or better than stratified approaches.

### Mechanism 2
- Claim: Applying increasingly complex fine-tuning methods to deeper layers while freezing earlier layers reduces catastrophic forgetting while maintaining performance.
- Mechanism: By freezing the early layers that capture general linguistic knowledge and only fine-tuning deeper layers with LoRA and BitFit, SPAFIT preserves pre-trained knowledge while adapting to task-specific requirements.
- Core assumption: Early layers contain general linguistic knowledge that should not be altered, while later layers contain task-specific knowledge that benefits from adaptation.
- Evidence anchors:
  - [section 2.3] "the changes required to adapt a pre-trained model to a specific task can be accomplished by just allowing bias terms to change while keeping the remainder of the model frozen"
  - [section 1] "increasing evidence of catastrophic forgetting and overparameterization in the Transformer architecture has motivated researchers to seek more efficient fine-tuning methods"
  - [corpus] Weak - the corpus provides no direct evidence for catastrophic forgetting mitigation through layer stratification
- Break condition: If freezing early layers causes significant performance degradation compared to full fine-tuning.

### Mechanism 3
- Claim: Fine-tuning fewer parameters with a stratified approach can achieve performance comparable to full fine-tuning while being more parameter-efficient.
- Mechanism: SPAFIT fine-tunes only 1.65-2.24% of parameters (5.65-7.49 million) compared to full fine-tuning (333.58 million) while achieving competitive or superior performance on 7 out of 9 GLUE tasks.
- Core assumption: The model is overparameterized and does not require all parameters to be fine-tuned for good performance on downstream tasks.
- Evidence anchors:
  - [section 1] "increasing evidence of catastrophic forgetting and overparameterization in the Transformer architecture"
  - [section 4] "all SPAFIT models fine-tune significantly fewer parameters than LoRA models and almost all of them perform as well as, or in most cases, better than LoRA models"
  - [corpus] Weak - the corpus provides no direct evidence for parameter efficiency through stratified fine-tuning
- Break condition: If experiments show that fine-tuning more parameters consistently leads to better performance.

## Foundational Learning

- Concept: Layer-wise knowledge localization in neural networks
  - Why needed here: SPAFIT's core hypothesis relies on understanding that different layers capture different types of linguistic knowledge, which is fundamental to why stratified fine-tuning works
  - Quick check question: In a typical transformer architecture, what type of linguistic information (syntactic vs semantic) would you expect to find in the early vs late layers?

- Concept: Parameter-efficient fine-tuning (PEFT) methods
  - Why needed here: SPAFIT builds on existing PEFT methods like LoRA and BitFit, so understanding how these methods work is essential for grasping SPAFIT's approach
  - Quick check question: How does LoRA reduce the number of parameters that need to be fine-tuned compared to full fine-tuning?

- Concept: Catastrophic forgetting in sequential fine-tuning
  - Why needed here: SPAFIT aims to mitigate catastrophic forgetting by freezing early layers, so understanding this phenomenon is crucial for appreciating the method's design
  - Quick check question: What happens to a model's performance on task A when it is fully fine-tuned on task B, and why does this occur?

## Architecture Onboarding

- Component map: BERT-large-cased base model with three stratified groups of layers. Group 1 (early layers) is frozen, Group 2 (middle layers) uses BitFit, and Group 3 (late layers) uses LoRA with BitFit. The model has 24 layers total, with N1 and N2 determining the stratification boundaries.
- Critical path: The most important decision is choosing N1 and N2 values. These hyperparameters determine how layers are grouped and directly impact performance. The choice of LoRA rank (r) and scaling factor (α) for Group 3 is also critical.
- Design tradeoffs: More layers in Group 1 means better preservation of general knowledge but potentially less adaptation. More layers in Group 3 means better task-specific performance but higher computational cost. The balance between BitFit and LoRA application also affects efficiency vs performance.
- Failure signatures: Poor performance on tasks requiring basic linguistic knowledge (like CoLA and SST-2), overfitting on smaller datasets, or underperformance compared to simpler PEFT methods like LoRA or BitFit alone.
- First 3 experiments:
  1. Implement SPAFIT with N1=8, N2=12, r=64, α=128 and compare against full fine-tuning on a simple GLUE task like SST-2
  2. Test different N1 values (4, 8, 12) while keeping N2=12 to find the optimal early layer freezing point
  3. Compare SPAFIT with full BitFit and full LoRA to establish baseline performance differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal stratification of layers for SPAFIT across different model architectures and task complexities?
- Basis in paper: [explicit] The paper states "determining the optimal values for these hyperparameters requires experimentation, considering factors such as the architecture of the large language model, the number of training examples available, and the specific downstream task at hand."
- Why unresolved: The paper uses fixed stratification points (N1=4, N2=9) for BERT-large-cased but acknowledges that optimal values may vary by model and task.
- What evidence would resolve it: Systematic experiments testing different stratification points across multiple model architectures and task complexities, with clear performance metrics showing optimal configurations.

### Open Question 2
- Question: How does SPAFIT perform on more complex downstream tasks like summarization compared to classification tasks?
- Basis in paper: [explicit] The paper states "exploring the performance of SPAFIT on complex downstream tasks like summarization could be an intriguing extension to this study."
- Why unresolved: All experiments were conducted on GLUE benchmark classification tasks; the method's effectiveness on generative or more complex tasks remains untested.
- What evidence would resolve it: Experiments applying SPAFIT to summarization, question generation, or other generative tasks with performance comparisons to full fine-tuning and other PEFT methods.

### Open Question 3
- Question: What is the theoretical basis for why certain layers require different fine-tuning intensities in SPAFIT?
- Basis in paper: [explicit] The paper proposes "investigate whether different layers of the model house distinct types of linguistic knowledge" but acknowledges "there is not much work done on associating different layers of PLMs with different types of linguistic knowledge."
- Why unresolved: While the paper presents empirical evidence supporting layer-wise specialization, it doesn't provide a theoretical explanation for why certain layers need different fine-tuning intensities.
- What evidence would resolve it: Neuroimaging studies or layer-wise ablation experiments showing the functional specialization of different layers, combined with theoretical models explaining information flow and task adaptation requirements.

## Limitations

- The method underperforms on single-sentence tasks (CoLA, SST-2) compared to full fine-tuning
- Optimal stratification boundaries (N1, N2) are task-dependent and require extensive hyperparameter tuning
- Weak theoretical justification for layer-wise knowledge localization, with most evidence coming from prior work rather than direct validation

## Confidence

- High confidence: The parameter efficiency claims are well-supported by the experimental data showing 1.65-2.24% parameter usage
- Medium confidence: The performance claims are credible for multi-sentence tasks but less so for single-sentence tasks where SPAFIT underperformed
- Low confidence: The theoretical mechanism linking layer stratification to knowledge localization lacks direct experimental validation

## Next Checks

1. Systematically test different N1 and N2 configurations across all GLUE tasks to identify optimal stratification boundaries and understand their impact on performance
2. Conduct ablation studies comparing SPAFIT against uniform fine-tuning (same total parameters but distributed differently) to validate the stratification hypothesis
3. Extend experiments to include a broader range of task types beyond GLUE, particularly focusing on tasks requiring different types of linguistic knowledge to test the layer localization hypothesis