---
ver: rpa2
title: Hyperparameter Optimization Can Even be Harmful in Off-Policy Learning and
  How to Deal with It
arxiv_id: '2404.15084'
source_url: https://arxiv.org/abs/2404.15084
tags:
- policy
- performance
- generalization
- logging
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies two critical issues in hyperparameter optimization
  (HPO) for off-policy learning (OPL): optimistic bias, where HPO selects hyperparameters
  whose performance is greatly overestimated, and unsafe behavior, where HPO can output
  solutions that underperform the logging policy even when starting from it. The authors
  propose Conservative and Imitation-Regularized HPO (CIR-HPO), which combines a conservative
  surrogate objective that penalizes hyperparameters with high performance uncertainty
  and adaptive imitation regularization that strengthens regularization when the logging
  policy performs well.'
---

# Hyperparameter Optimization Can Even be Harmful in Off-Policy Learning and How to Deal with It

## Quick Facts
- arXiv ID: 2404.15084
- Source URL: https://arxiv.org/abs/2404.15084
- Authors: Yuta Saito; Masahiro Nomura
- Reference count: 22
- Primary result: CIR-HPO addresses optimistic bias and unsafe behavior in OPL-HPO, improving generalization performance by up to 23.6% over baseline methods.

## Executive Summary
This paper identifies two critical issues in hyperparameter optimization (HPO) for off-policy learning (OPL): optimistic bias, where HPO selects hyperparameters whose performance is greatly overestimated, and unsafe behavior, where HPO can output solutions that underperform the logging policy even when starting from it. The authors propose Conservative and Imitation-Regularized HPO (CIR-HPO), which combines a conservative surrogate objective that penalizes hyperparameters with high performance uncertainty and adaptive imitation regularization that strengthens regularization when the logging policy performs well. Empirical results on synthetic data show that CIR-HPO effectively addresses these issues, particularly in cases where the standard HPO procedure fails severely.

## Method Summary
The paper proposes CIR-HPO for OPL-HPO, which addresses optimistic bias and unsafe behavior through two key mechanisms. First, it uses a conservative surrogate objective (CSO) that employs Student's t-distribution based lower bounds to penalize hyperparameters with high performance uncertainty, preventing selection of overestimated validation performance. Second, it introduces adaptive imitation regularization (AIR) that automatically adjusts regularization strength based on statistical comparisons between the logging policy and sampled solutions, preventing performance degradation when the logging policy is near-optimal. The method is evaluated on synthetic contextual bandit data with logistic regression and random forest policy classes, comparing against standard HPO methods using random search and tree-structured Parzen estimators.

## Key Results
- CIR-HPO addresses two critical issues in OPL-HPO: optimistic bias and unsafe behavior
- The method improves generalization performance by up to 23.6% over baseline HPO procedures
- Both conservative surrogate objective and adaptive imitation regularization contribute to performance gains, with AIR having the most significant impact
- CIR-HPO effectively prevents selecting hyperparameters whose validation performance is greatly overestimated

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conservative surrogate objective (CSO) addresses the heavy-tailed distribution of overestimation bias during hyperparameter optimization.
- Mechanism: CSO penalizes hyperparameters whose performance has high uncertainty by using a high probability lower bound of generalization performance as the surrogate objective. This discourages selection of hyperparameters with potentially overestimated validation performance.
- Core assumption: The importance weighted rewards follow a distribution that can be reasonably approximated as normal, especially with growing data sizes.
- Evidence anchors:
  - [abstract]: "we propose simple and computationally efficient corrections to the typical HPO procedure to deal with the aforementioned issues simultaneously"
  - [section 4.1]: "we introduce conservative surrogate objective, which penalizes the validation performance of hyperparameters whose performance has a large uncertainty to avoid the issue of overestimation bias during HPO"
  - [corpus]: No direct evidence found in corpus; this appears to be a novel contribution
- Break condition: When the assumption about normal distribution of importance weighted rewards is severely violated, particularly in small sample settings, the Student's t-distribution based lower bound may become overly conservative or invalid.

### Mechanism 2
- Claim: Adaptive imitation regularization (AIR) prevents unsafe behavior by automatically controlling the strength of regularization based on the relative performance of the logging policy.
- Mechanism: AIR compares the logging policy against sampled solutions using statistical tests and adjusts the regularization parameter dynamically. If the logging policy performs well relative to sampled solutions, stronger regularization is applied to maintain performance close to the logging policy.
- Core assumption: The relative performance of the logging policy versus sampled solutions can be reliably assessed using statistical tests based on importance weighted rewards.
- Evidence anchors:
  - [abstract]: "adaptive imitation regularization that strengthens regularization when the logging policy performs well"
  - [section 4.2]: "we propose adaptively tuning this parameter over the course of HPO... we can reason about the optimality of the logging policy by comparing it with solutions sampled during HPO"
  - [corpus]: No direct evidence found in corpus; appears to be novel approach
- Break condition: When the logging policy is significantly suboptimal but the statistical tests fail to detect this due to high variance in importance weighted rewards.

### Mechanism 3
- Claim: The combination of CSO and AIR creates a synergistic effect that addresses both optimistic bias and unsafe behavior simultaneously.
- Mechanism: CSO prevents selection of hyperparameters with overestimated performance by using conservative bounds, while AIR prevents performance degradation relative to the logging policy by adaptive regularization. Together they create a more robust optimization framework.
- Core assumption: Both optimistic bias and unsafe behavior are significant issues that need to be addressed simultaneously for effective HPO in OPL.
- Evidence anchors:
  - [abstract]: "we propose simple and computationally efficient corrections to the typical HPO procedure to deal with the aforementioned issues simultaneously"
  - [section 5.3]: "Both CSO and AIR clearly contribute to the performance of CIR-HPO, while AIR has a more appealing effect (CSO and AIR provide 1.6% and 23.6% improvements, respectively, in terms of the final generalization performance)"
  - [corpus]: No direct evidence found in corpus; combination approach appears novel
- Break condition: When neither optimistic bias nor unsafe behavior is a significant issue in the specific OPL setting, the additional complexity may not provide substantial benefit.

## Foundational Learning

- Concept: Off-policy evaluation (OPE) and off-policy learning (OPL)
  - Why needed here: The paper builds on OPE techniques to develop HPO methods for OPL, so understanding the difference between evaluating and learning from logged data is fundamental
  - Quick check question: What is the key difference between off-policy evaluation and off-policy learning in the context of contextual bandits?

- Concept: Importance sampling and doubly robust estimation
  - Why needed here: These are the primary estimators used in the paper for both the baseline and proposed methods, and understanding their properties is crucial for implementing the algorithms
  - Quick check question: Under what conditions is the doubly robust estimator unbiased, and why might it still suffer from high variance?

- Concept: Hyperparameter optimization in supervised learning
  - Why needed here: The paper adapts techniques from supervised learning HPO to the OPL setting, so understanding standard approaches like TPE and random search is important for grasping the proposed modifications
  - Quick check question: What is the key difference between grid search, random search, and Bayesian optimization methods like TPE in hyperparameter optimization?

## Architecture Onboarding

- Component map: Logged bandit data -> split into Dtr and Dval -> policy class with hyperparameters -> OPE estimators (IPS or DR) -> CIR-HPO algorithm (CSO + AIR) -> final policy
- Critical path: 1) Split logged data into Dtr and Dval, 2) Initialize logging policy as initial solution, 3) Sample hyperparameters using TPE or other method, 4) Train policy on Dtr using chosen ML model, 5) Apply adaptive regularization to create πt, 6) Evaluate using conservative surrogate objective on Dval, 7) Update best solution if improvement found, 8) Store results and repeat until budget exhausted
- Design tradeoffs: The conservative surrogate objective trades potential exploration of high-performing but uncertain hyperparameters for more reliable selection, while adaptive imitation regularization trades potential improvement beyond the logging policy for safety against performance degradation. The Student's t-distribution approach balances tightness of bounds against distributional assumptions.
- Failure signatures: If the logging policy is significantly suboptimal but statistical tests fail to detect this, the system may over-regularize and miss better solutions. If the importance weighted rewards have extremely heavy tails, the Student's t-distribution bounds may become invalid. If the policy class is misspecified, no amount of HPO will find good solutions.
- First 3 experiments:
  1. Run baseline TPE with IPS on synthetic data with β0=0 (uniform logging policy) to establish baseline performance
  2. Run CIR-HPO with default parameters (δ=0.1, γ=0.01) on the same data to verify improvement
  3. Run ablation study with CIR-HPO(w/o CSO) and CIR-HPO(w/o AIR) to isolate contributions of each component

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the heavy-tailed distribution of overestimation bias vary across different HPO algorithms (beyond TPE and random search) and OPE estimators in off-policy learning?
- Basis in paper: [explicit] The paper identifies heavy-tailed overestimation bias as a key factor causing the gap between validation and generalization regret, showing empirical evidence with TPE vs. random search.
- Why unresolved: The paper only compares TPE and random search, leaving open whether other popular HPO algorithms like Bayesian optimization or CMA-ES exhibit similar or different bias distributions.
- What evidence would resolve it: Empirical comparison of overestimation bias distributions across multiple HPO algorithms and OPE estimators using the same synthetic data setup.

### Open Question 2
- Question: What is the relationship between logging policy optimality (β0) and the effectiveness of adaptive imitation regularization across different data regimes (small vs. large sample sizes)?
- Basis in paper: [explicit] The paper shows adaptive imitation regularization helps when the logging policy is near-optimal (β0=20), but doesn't explore how this effectiveness varies with data size.
- Why unresolved: The paper uses fixed sample sizes and doesn't investigate how sample size interacts with logging policy quality to affect AIR's performance.
- What evidence would resolve it: Experiments varying both β0 and data size to map out when AIR provides the most benefit across different regimes.

### Open Question 3
- Question: How sensitive is the proposed Conservative and Imitation-Regularized HPO (CIR-HPO) to the normality assumption underlying the Student's t-test based confidence bounds?
- Basis in paper: [inferred] The paper uses t-test based bounds assuming normally distributed importance-weighted rewards, but notes this assumption may fail in small sample settings.
- Why unresolved: While the paper shows t-test bounds work reasonably well empirically, it doesn't systematically test their robustness when the normality assumption is violated.
- What evidence would resolve it: Experiments testing CIR-HPO's performance when the underlying distribution of importance-weighted rewards is non-normal (e.g., using heavy-tailed distributions).

## Limitations

- The empirical validation is limited to synthetic data with controlled settings, which may not fully capture real-world complexities.
- The effectiveness of the proposed methods in high-dimensional action spaces or with non-stationary logging policies remains untested.
- The computational overhead of the Student's t-distribution based conservative bounds and adaptive regularization is not thoroughly analyzed.

## Confidence

- **High confidence**: The identification of optimistic bias and unsafe behavior as critical issues in OPL-HPO is well-supported by theoretical analysis and synthetic experiments. The mechanism of conservative surrogate objective using t-test lower bounds is theoretically sound under the normality assumption.
- **Medium confidence**: The adaptive imitation regularization approach is empirically validated but lacks theoretical guarantees. The assumption that statistical tests can reliably assess the relative performance of logging policies may not hold in all settings, particularly with limited data or highly variable importance weighted rewards.
- **Low confidence**: The combination of CSO and AIR creating a synergistic effect is primarily supported by ablation studies on synthetic data. The extent to which these mechanisms interact positively in more complex, real-world scenarios remains unclear.

## Next Checks

1. **Real-world validation**: Apply CIR-HPO to a public contextual bandit dataset (e.g., from the RecoGym platform or Yahoo! Front Page dataset) to assess performance in practical settings with non-synthetic logging policies and reward structures.

2. **Robustness analysis**: Systematically vary the distribution of importance weighted rewards (e.g., by changing action space size, context distribution) to test the breakdown conditions of the Student's t-distribution assumptions and adaptive regularization heuristics.

3. **Computational efficiency evaluation**: Benchmark the runtime and memory requirements of CIR-HPO against baseline methods across different dataset sizes and policy classes to quantify the practical overhead of the proposed corrections.