---
ver: rpa2
title: An All-MLP Sequence Modeling Architecture That Excels at Copying
arxiv_id: '2406.16168'
source_url: https://arxiv.org/abs/2406.16168
tags:
- linear
- modeling
- copying
- normalization
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce CausalRN, an all-MLP sequence modeling architecture
  that matches Transformers on copying tasks. By extending Relation Networks with
  autoregressive masking and pre-activation normalization, CausalRNs maintain an irreducible
  memory pool analogous to a KV cache.
---

# An All-MLP Sequence Modeling Architecture That Excels at Copying

## Quick Facts
- arXiv ID: 2406.16168
- Source URL: https://arxiv.org/abs/2406.16168
- Reference count: 26
- Primary result: CausalRNs match Transformers on copying tasks using all-MLP architecture with exponential activation and pre-activation normalization

## Executive Summary
CausalRN introduces an all-MLP sequence modeling architecture that achieves Transformer-level performance on copying tasks without using attention mechanisms. The architecture extends Relation Networks with autoregressive masking and pre-activation normalization to maintain an irreducible memory pool similar to a KV cache. By leveraging exponential activation to linearize computations, CausalRNs achieve O(n) training complexity while preserving effective in-context retrieval capabilities. The work challenges the assumption that attention mechanisms are necessary for strong sequence modeling performance.

## Method Summary
CausalRNs implement autoregressive sequence modeling using exponential activation functions and pre-activation normalization. The architecture processes input sequences through linear transformations followed by exponential activation, with normalization applied before activation to maintain an irreducible memory pool. Causal masking ensures autoregressive behavior. Training uses Adam optimizer with learning rate sweep, batch size 320, and up to 2000 iterations on copying tasks with string lengths 16-256.

## Key Results
- Matches Transformer performance on copying tasks with string lengths 16-256
- Achieves O(n) training complexity through exponential activation decomposition
- Both exponential activation and pre-activation normalization are critical for Transformer-level performance
- Demonstrates that attention mechanisms are not essential for strong in-context retrieval

## Why This Works (Mechanism)

### Mechanism 1: Exponential Activation Enables Linear Time Complexity
The exponential activation function allows linear time complexity by decomposing the sum of exponentials into a precomputable form. When using exp(x) as activation, the summation term can be rewritten as exp(qj) * sum(exp(pi)), where pi and qj are linear transformations of inputs. This decomposition enables precomputing sum(exp(pi)) and reusing it for all j positions, reducing complexity from O(n^2) to O(n).

### Mechanism 2: Pre-activation Normalization Maintains Irreducible Memory Pool
Pre-activation normalization creates an infinitely growing memory pool by applying normalization before exponential activation. This prevents the output from being decomposable, unlike post-activation normalization. The non-decomposable output maintains an irreducible memory structure similar to Transformers' KV cache, enabling effective in-context retrieval across long sequences.

### Mechanism 3: Combined Architecture Matches Transformer Capabilities
The combination of exponential activation and pre-activation normalization provides both computational efficiency and effective memory management. Exponential activation ensures linear time complexity while pre-activation normalization maintains the memory pool necessary for in-context retrieval. Together, these mechanisms enable CausalRNs to match Transformers' copying performance without using attention.

## Foundational Learning

- **Relation Networks (RNs)**: Core architecture that CausalRN extends with autoregressive capabilities. Understanding RNs is essential for grasping how CausalRNs process pairwise relationships between sequence elements.
  - Why needed: CausalRN builds directly on RN foundations by adding causal masking and normalization strategies
  - Quick check: How do Relation Networks compute relationships between all pairs of elements differently from standard neural networks?

- **Autoregressive sequence modeling**: Framework where each output depends only on previous inputs, crucial for tasks like language modeling and copying.
  - Why needed: CausalRNs are specifically designed for autoregressive tasks where future information cannot be accessed
  - Quick check: What role does causal masking play in ensuring autoregressive behavior in CausalRNs?

- **Exponential activation function**: Mathematical function exp(x) that enables linear time complexity through its decomposition properties.
  - Why needed: The exponential function's property exp(a + b) = exp(a) × exp(b) is fundamental to achieving O(n) complexity
  - Quick check: What specific mathematical property of the exponential function enables the sum decomposition in CausalRNs?

## Architecture Onboarding

- **Component map**: Input layer → Linear transformations → Exponential activation → Pre-activation normalization → Causal masking → Output layer
- **Critical path**: Input → Linear transformations → Exponential activation → Pre-activation normalization → Causal masking → Output
- **Design tradeoffs**: Exponential activation provides linear complexity but may introduce numerical stability issues; pre-activation normalization maintains memory but increases memory usage compared to post-activation approaches
- **Failure signatures**: Convergence failure indicates implementation issues with exponential activation or normalization; performance degradation on longer sequences suggests problems with causal masking or memory pool mechanism
- **First 3 experiments**:
  1. Implement basic CausalRN without pre-activation normalization and test on simple copying task to verify exponential activation impact
  2. Add pre-activation normalization to CausalRN and evaluate performance on same copying task to assess memory pool effects
  3. Compare CausalRN performance with standard Transformer on complex copying task to validate matching capabilities

## Open Questions the Paper Calls Out

### Open Question 1
Does CausalRN maintain copying performance on tasks with string lengths beyond 256 tokens? The paper only tests up to 256 tokens, leaving scalability questions unanswered.

### Open Question 2
How does CausalRN perform on complex in-context learning tasks beyond string copying, such as multi-step reasoning or few-shot learning? The copying task may not fully capture general in-context learning capabilities.

### Open Question 3
What is the impact of different normalization strategies on CausalRN performance beyond post-reduction and pre-activation normalization? The ablation study doesn't explore alternative normalization methods.

## Limitations

- Performance generalization beyond copying tasks remains unproven, as the architecture is only evaluated on a single benchmark
- Theoretical computational efficiency claims haven't been empirically validated against practical implementation challenges
- Numerical stability issues with exponential activation in real-world implementations are not addressed

## Confidence

**High Confidence**: Exponential activation enables linear time complexity through established mathematical properties of the exponential function.

**Medium Confidence**: Pre-activation normalization maintains irreducible memory pool analogous to KV cache, though formal mathematical proof could be stronger.

**Medium Confidence**: CausalRNs match Transformers on copying tasks, but evaluation is limited to one benchmark without broader generalization testing.

## Next Checks

1. **Generalization Benchmark Testing**: Evaluate CausalRNs on standard language modeling datasets (WikiText-103, PTB) and comparison tasks (GLUE, SuperGLUE) to verify copying performance translates to general sequence modeling.

2. **Numerical Stability Analysis**: Implement CausalRN with various floating-point precisions (FP32, FP16, BF16) and activation scaling techniques to quantify practical impact on training stability and convergence speed.

3. **Memory Pool Characterization**: Conduct controlled experiments varying sequence lengths and normalization placement to empirically measure memory pool size and retrieval quality, comparing against explicit KV cache implementations.