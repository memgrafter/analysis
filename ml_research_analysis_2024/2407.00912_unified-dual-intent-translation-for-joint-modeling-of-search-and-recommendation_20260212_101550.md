---
ver: rpa2
title: Unified Dual-Intent Translation for Joint Modeling of Search and Recommendation
arxiv_id: '2407.00912'
source_url: https://arxiv.org/abs/2407.00912
tags:
- search
- recommendation
- intents
- demand
- intent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of accurately modeling users'
  inherent and demand intents in joint search and recommendation systems, where these
  intents are either implicit (in recommendation) or explicit (in search). The authors
  propose a novel method, Unified Dual-Intent Translation for Search and Recommendation
  (UDITSR), which addresses this by leveraging search query data as supervision to
  generate accurate demand intents for recommendation.
---

# Unified Dual-Intent Translation for Joint Modeling of Search and Recommendation

## Quick Facts
- **arXiv ID**: 2407.00912
- **Source URL**: https://arxiv.org/abs/2407.00912
- **Reference count**: 40
- **Key outcome**: UDITSR achieves 6.22% improvement in search and 3.06% in recommendation tasks, with 1.46% GMV and 0.77% CTR gains in online A/B testing

## Executive Summary
This paper addresses the challenge of modeling users' inherent and demand intents in joint search and recommendation systems. The authors propose UDITSR, which leverages search query data as supervision to generate accurate demand intents for recommendation. By introducing a dual-intent translation propagation mechanism, the model explicitly captures the relationship between inherent intent, demand intent, and interactive items in a unified semantic space. Extensive experiments on real-world datasets demonstrate significant performance improvements over state-of-the-art baselines.

## Method Summary
UDITSR employs a unified graph construction approach to jointly model search and recommendation tasks. The model uses search queries as explicit supervision to generate demand intents through a user-aware gate mechanism. A dual-intent translation propagation mechanism learns the relationship between inherent intent, demand intent, and interactive items via embedding translations with contrastive loss. The unified architecture is trained end-to-end using BPR loss with additional search-supervised and contrastive learning components.

## Key Results
- UDITSR outperforms SOTA baselines by 6.22% average improvement in search tasks
- Achieves 3.06% improvement in recommendation tasks
- Online A/B testing shows 1.46% GMV and 0.77% CTR increases on Meituan Waimai platform

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using real search queries as explicit supervision guides generation of accurate demand intents for recommendation
- Mechanism: Leverages historical search queries as auxiliary information with user-aware gate mechanism for weighting query terms
- Core assumption: Search queries explicitly encode users' demand intents that can be transferred to improve recommendation modeling
- Evidence anchors: [abstract] "we utilize real queries from search data as supervision information to guide its generation"; [section 4.2.1] "we leverage them as auxiliary information to simulate the user's demand intent for recommendation"
- Break condition: If search queries do not accurately represent users' demand intents or user-aware gate mechanism fails to capture personalization

### Mechanism 2
- Claim: Dual-intent translation propagation explicitly models relation among <inherent intent, demand intent, interactive item> in unified semantic space
- Mechanism: Uses embedding translations where interactive item representation should be close to sum of inherent intent and demand intent representations
- Core assumption: Relationship between dual intents and interactive items can be modeled as translation operation in embedding space
- Evidence anchors: [abstract] "we propose a dual-intent translation propagation mechanism to learn the triplet in the same semantic space via embedding translations"; [section 4.3] "we assume that a user's changing interactive item should be close to their inherent intent plus changing demand intent"
- Break condition: If translation assumption fails to capture true relationship or contrastive loss does not effectively constrain translation relation

### Mechanism 3
- Claim: Joint modeling of search and recommendation with dual intents improves performance over single-task models
- Mechanism: Unified graph construction and shared parameter learning across both tasks allow mutual reinforcement of user representations
- Core assumption: Search and recommendation tasks share underlying user intent structures that can be jointly optimized
- Evidence anchors: [abstract] "Extensive experiments demonstrate that UDITSR outperforms SOTA baselines both in search and recommendation tasks"; [section 5.2] "Most graph-based methods, such as NGCF, LightGCN, and GraphSRRL, perform well in both tasks"
- Break condition: If tasks are too dissimilar for effective knowledge transfer or unified graph construction introduces noise that degrades performance

## Foundational Learning

- **Concept**: Graph Neural Networks and their aggregation mechanisms
  - Why needed here: Dual-intent translation propagation relies on graph-based message passing between users, items, and intents
  - Quick check question: What is the difference between mean pooling and attention-based aggregation in GNNs, and when would each be appropriate?

- **Concept**: Contrastive learning and margin-based ranking
  - Why needed here: Intent translation contrastive loss uses margin-based ranking criterion to constrain translation relation between intents and items
  - Quick check question: How does margin-based contrastive loss differ from standard contrastive loss, and what are implications for model training?

- **Concept**: User intent modeling and its challenges in recommendation systems
  - Why needed here: Entire paper built on premise that user intents (inherent and demand) drive interactions and need accurate modeling
  - Quick check question: What are key differences between modeling inherent intents versus demand intents, and why is distinction important for recommendation performance?

## Architecture Onboarding

- **Component map**: Embedding Layer -> Demand Intent Generator -> Dual-Intent Translation -> Prediction Layer -> Optimization
- **Critical path**: Embedding Layer → Demand Intent Generator → Dual-Intent Translation → Prediction Layer → Optimization
- **Design tradeoffs**:
  - Using padding queries vs. generated demand intents for recommendation
  - Mean pooling vs. attention-based aggregation in translation mechanism
  - Shared vs. task-specific parameters in prediction layer
- **Failure signatures**:
  - Poor performance on either task suggests issues with unified modeling
  - If demand intent generator fails, recommendation performance will degrade significantly
  - If translation mechanism fails, model may not capture relation between intents and items effectively
- **First 3 experiments**:
  1. Validate demand intent generator by comparing generated intents with actual search queries using cosine similarity
  2. Test dual-intent translation mechanism by visualizing distribution of translated intents vs. interactive items
  3. Evaluate joint modeling by comparing performance with separate search and recommendation models trained independently

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of UDITSR vary when using different query segmentation strategies or tokenization methods for search queries?
- Basis in paper: [inferred] Paper mentions queries are segmented into shorter terms but does not explore impact of different segmentation approaches on performance
- Why unresolved: Paper does not provide ablation study or comparison of different query segmentation strategies
- What evidence would resolve it: Experimental comparison of UDITSR's performance using different query segmentation methods (e.g., character-level vs. word-level, different tokenization algorithms)

### Open Question 2
- Question: Can UDITSR be extended to handle more than two intents (e.g., adding third intent category like contextual intent) while maintaining or improving performance?
- Basis in paper: [explicit] Paper focuses on dual-intent modeling but acknowledges that user behaviors can be driven by complex intents
- Why unresolved: Paper does not explore scalability of dual-intent framework to handle additional intent categories
- What evidence would resolve it: Experiments testing UDITSR's performance when extended to model three or more intent categories

### Open Question 3
- Question: How does model's performance change when applied to domains with significantly different user behavior patterns (e.g., streaming services vs. e-commerce)?
- Basis in paper: [inferred] Paper evaluates UDITSR on food delivery data but does not test generalizability to other domains with different interaction patterns
- Why unresolved: Effectiveness of UDITSR in domains with different user behavior characteristics is not explored
- What evidence would resolve it: Testing UDITSR on datasets from diverse domains and comparing performance across these domains

## Limitations
- Evaluation relies on proprietary Meituan Waimai datasets, limiting reproducibility and external validation
- Lacks ablation studies isolating contribution of individual components
- Specific implementation details of user-aware gate mechanism and exact MLP structures are not fully specified

## Confidence

- **High Confidence**: Core hypothesis that joint modeling of search and recommendation can improve performance over single-task models. Well-supported by extensive offline experiments and online A/B testing.
- **Medium Confidence**: Effectiveness of dual-intent translation mechanism. Theoretically sound with good empirical results, but specific formulation could benefit from further theoretical justification.
- **Medium Confidence**: Search-supervised demand intent generation. Appears effective but assumption that search queries accurately represent demand intents may not hold for all user populations or domains.

## Next Checks

1. Conduct ablation studies removing dual-intent translation mechanism to quantify its isolated contribution to overall performance gains
2. Perform cross-domain validation using public datasets (e.g., Amazon, Yelp) to assess generalizability beyond Meituan Waimai platform
3. Implement sensitivity analysis on contrastive loss weight (λ2) and margin parameter (m) to determine their optimal ranges and robustness