---
ver: rpa2
title: Machine Learning for Stochastic Parametrisation
arxiv_id: '2402.09471'
source_url: https://arxiv.org/abs/2402.09471
tags:
- stochastic
- climate
- parametrisation
- weather
- christensen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Machine learning is increasingly used to replace parametrisation
  schemes in weather and climate models, but most approaches are deterministic, assuming
  the sub-grid state is fully determined by the grid-scale variables. This assumption
  is flawed due to the scale-invariant nature of fluid flows in the atmosphere, where
  deterministic parametrisations always introduce errors.
---

# Machine Learning for Stochastic Parametrisation

## Quick Facts
- arXiv ID: 2402.09471
- Source URL: https://arxiv.org/abs/2402.09471
- Reference count: 34
- Key outcome: ML can learn conditional probability distributions of sub-grid tendencies, enabling stochastic parametrisation that improves forecast reliability by capturing true variability

## Executive Summary
Machine learning is emerging as a powerful tool to replace traditional deterministic parametrisation schemes in weather and climate models. The core insight is that the atmosphere's scale-invariant nature prevents a spectral gap between resolved and unresolved scales, making deterministic approaches inherently biased. Stochastic parametrisation addresses this by sampling from conditional probability distributions, and ML offers a data-driven way to learn these distributions from high-resolution simulations. Early work shows promise, with random forests predicting convection occurrence reliably and GANs generating realistic sub-grid samples, though challenges remain in capturing spatio-temporal correlations and finding optimal architectures.

## Method Summary
The approach involves training ML models on coarse-grained high-resolution atmospheric datasets to learn the conditional distribution of sub-grid tendencies given resolved-scale variables. Random forests can predict probability distributions for phenomena like convection occurrence, while generative models like GANs can sample from the full distribution of sub-grid forcing. The key innovation is moving from deterministic mean predictions to probabilistic sampling that captures the true variability seen in high-resolution data. Models must balance accuracy with computational efficiency for implementation in large-scale models, and architectures need to handle both local and non-local correlations across space and time.

## Key Results
- Scale-invariance in atmospheric flows prevents a spectral gap, making deterministic parametrisation inherently biased
- Stochastic parametrisation that samples from conditional distributions improves forecast reliability by capturing true sub-grid variability
- Early ML approaches show promise: random forests for probability prediction and GANs for distribution sampling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scale-invariance in atmospheric flows prevents a spectral gap between resolved and unresolved scales, making deterministic parametrisation inherently biased.
- Mechanism: Because fluid motions exist at all scales without separation, the true sub-grid tendency cannot be uniquely determined from grid-scale variables. A deterministic model always predicts the conditional mean, but the actual tendency follows a distribution with non-zero variance.
- Core assumption: Navier-Stokes equations are scale-invariant and atmospheric spectra follow power laws.
- Evidence anchors:
  - [abstract] "the lack of scale-separation in the atmosphere means that this approach is a large source of error in forecasts."
  - [section] "the Navier-Stokes equations are scale invariant... leading to the emergence of power-law behaviour in fluid flows, including in the atmosphere... the grid-scale variables cannot fully constrain the sub-grid motions."
- Break condition: If atmospheric dynamics exhibit strong scale separation in certain regimes (e.g. very coarse resolutions), the deterministic assumption may hold approximately.

### Mechanism 2
- Claim: Stochastic parametrisations improve forecast reliability by representing the conditional probability distribution of sub-grid tendencies rather than a single mean value.
- Mechanism: By sampling from a distribution conditional on resolved-scale state, the scheme captures the true variability seen in high-resolution data, reducing systematic biases and improving ensemble reliability.
- Core assumption: The conditional distribution of sub-grid tendencies can be learned from high-resolution or observational datasets.
- Evidence anchors:
  - [abstract] "Stochastic parametrisations address this by sampling from a probability distribution conditional on the resolved-scale state, improving forecast reliability."
  - [section] "Instead of representing the mean sub-grid tendency, a stochastic approach represents one possible realisation of the sub-grid scale process... a stochastic scheme can be constructed to capture the variability observed in high-resolution datasets but missing from deterministic schemes."
- Break condition: If the conditional distribution is too complex or non-stationary to learn reliably, the stochastic scheme may degrade performance.

### Mechanism 3
- Claim: Machine learning can learn the conditional distribution of sub-grid tendencies, enabling data-driven stochastic parametrisation that captures complex, non-linear relationships.
- Mechanism: ML models (e.g. random forests, GANs, VEDs) are trained on high-resolution data to predict parameters of a distribution or directly generate samples, capturing both the mean and variance structure conditioned on resolved-scale variables.
- Core assumption: Sufficient high-quality training data exists and the ML model can generalise to unseen states.
- Evidence anchors:
  - [abstract] "Early studies show promise for ML approaches in this space, including using random forests to predict convection occurrence with high reliability, and generative adversarial networks to sample from the sub-grid distribution."
  - [section] "Guillaume and Zanna (2021) model sub-grid momentum forcing using a parametric distribution, and train a neural network to learn the state-dependent parameters... Gagne et al. (2020) use a generative adversarial network (GAN) to generate samples conditioned on the resolved scale variables."
- Break condition: If training data is insufficient, biased, or the ML model fails to capture important physical constraints (e.g. conservation), the approach may fail.

## Foundational Learning

- Concept: Scale invariance and power-law spectra in fluid dynamics
  - Why needed here: Explains why deterministic parametrisation is fundamentally flawed due to lack of scale separation.
  - Quick check question: What is the implication of scale invariance for the relationship between resolved and unresolved scales in atmospheric models?

- Concept: Conditional probability distributions and stochastic sampling
  - Why needed here: Core to understanding how stochastic parametrisations represent sub-grid variability.
  - Quick check question: How does sampling from a conditional distribution differ from using a deterministic mean in terms of forecast uncertainty?

- Concept: Machine learning for distribution learning (e.g. GANs, VAEs, random forests for probability estimation)
  - Why needed here: Enables data-driven estimation of the conditional distribution of sub-grid tendencies.
  - Quick check question: What are the key differences between using a GAN versus a random forest for learning a conditional distribution?

## Architecture Onboarding

- Component map: Resolved-scale variables -> ML model -> Distribution parameters/Samples -> Spatio-temporal correlation module -> Sub-grid tendency field

- Critical path:
  1. Extract resolved-scale state from model grid
  2. Feed into ML model to predict distribution parameters
  3. Sample from distribution with appropriate spatio-temporal correlations
  4. Apply sampled tendency to model state
  5. Advance model time-step

- Design tradeoffs:
  - Accuracy vs. computational cost: Complex ML models may be more accurate but slower
  - Sample quality vs. diversity: Too much randomness may degrade mean state
  - Local vs. non-local: Including neighbouring columns increases accuracy but computational cost and complexity
  - Deterministic vs. probabilistic output: Full distribution vs. point estimates

- Failure signatures:
  - Degradation in long-term climate statistics (e.g. mean state, variability)
  - Instability in model integration (e.g. blow-up, unrealistic extremes)
  - Poor reliability in ensemble forecasts (e.g. overconfidence, bias)
  - Failure to conserve physical quantities (e.g. energy, mass)

- First 3 experiments:
  1. Train a simple ML model (e.g. random forest) to predict the probability of convection occurrence from resolved-scale variables, and evaluate reliability using a reliability diagram.
  2. Implement a GAN to generate samples of sub-grid momentum forcing conditioned on resolved-scale state, and evaluate sample quality and diversity.
  3. Integrate a trained ML stochastic parametrisation into a single-column model, and evaluate its impact on short-term forecasts and long-term statistics compared to deterministic baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can machine learning models effectively capture the complex spatio-temporal correlations in subgrid-scale processes while maintaining computational efficiency for implementation in large-scale climate models?
- Basis in paper: [explicit] The paper discusses the challenge of developing ML approaches to capture correlations in subgrid tendencies across neighboring columns in space and time, while still being practical to implement in a climate model.
- Why unresolved: The authors acknowledge this as a key challenge, but do not provide a definitive solution. They mention some early work addressing temporal correlations, but note that spatial correlations are more difficult to address due to implementation constraints.
- What evidence would resolve it: Development and testing of ML architectures that can effectively model spatio-temporal correlations in subgrid processes, demonstrating both improved forecast skill and computational efficiency when implemented in large-scale models.

### Open Question 2
- Question: What is the optimal architecture for machine learning models to represent stochastic parameterizations in climate models, particularly for capturing rare events and transitions between different atmospheric states?
- Basis in paper: [explicit] The authors discuss the ongoing search for the most appropriate architecture for physical modeling, noting that common approaches like Transformers have not transformed continuous process modeling. They provide an example of a mixed continuous-discrete model that improved cloud parameterization but acknowledge it as a bespoke solution.
- Why unresolved: The paper highlights that finding the optimal structure is a common challenge across domains, and the example provided (cloud parameterization) is specific to one problem. The generalizability and effectiveness of such architectures for other subgrid processes remain unknown.
- What evidence would resolve it: Comparative studies of different ML architectures (e.g., Transformers, graph neural networks, mixed continuous-discrete models) applied to various subgrid processes, demonstrating improved performance over traditional deterministic parameterizations and other ML approaches.

### Open Question 3
- Question: How can generative adversarial networks (GANs) or other generative models be improved to ensure they truly learn the target distribution of subgrid-scale processes, rather than just producing visually realistic outputs?
- Basis in paper: [explicit] The authors mention that while GANs have been used to generate samples conditioned on resolved scale variables, it is not known whether a GAN truly learns the target distribution. They cite literature questioning GANs' ability to learn the true distribution.
- Why unresolved: The theoretical understanding of GANs' ability to learn complex, high-dimensional distributions is still an active area of research. The paper acknowledges this limitation but does not provide a solution.
- What evidence would resolve it: Development of rigorous theoretical frameworks or empirical tests to verify that GANs (or other generative models) used for stochastic parameterizations are indeed learning the true distribution of subgrid processes, not just producing plausible samples. This could involve comparison with high-resolution simulations or novel evaluation metrics.

## Limitations
- Limited peer-reviewed validation across different atmospheric processes and climate regimes
- Lack of explicit consideration for physical constraints (conservation laws, energy consistency) in ML parametrisations
- Effectiveness in long-term climate simulations remains largely untested, with most evidence from short-term NWP studies

## Confidence
- Scale-invariance mechanism: Medium confidence (relies on fundamental fluid dynamics theory but lacks direct atmospheric citations)
- ML distribution learning effectiveness: Medium confidence (promising early results but limited validation)
- Specific architectural recommendations: Low confidence (optimal architectures for spatio-temporal correlations remain open question)

## Next Checks
1. Evaluate the physical consistency of ML-predicted sub-grid tendencies by testing conservation properties (energy, mass, momentum) across diverse atmospheric conditions.

2. Conduct multi-year climate simulations using the ML stochastic parametrisation and compare key climate statistics (mean state, variability, extremes) against deterministic baselines and observations.

3. Test the generalisation capability of trained ML models by evaluating performance on out-of-sample conditions (e.g., different seasons, climate regimes, or perturbed initial conditions).