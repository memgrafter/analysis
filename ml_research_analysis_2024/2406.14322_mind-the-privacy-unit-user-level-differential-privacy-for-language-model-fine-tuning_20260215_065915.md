---
ver: rpa2
title: Mind the Privacy Unit! User-Level Differential Privacy for Language Model Fine-Tuning
arxiv_id: '2406.14322'
source_url: https://arxiv.org/abs/2406.14322
tags:
- privacy
- records
- user-wise
- data
- dp-sgd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies user-level differential privacy (DP) for fine-tuning
  large language models (LLMs) on natural language generation tasks. The authors address
  the issue of unequal privacy guarantees when users contribute varying numbers of
  records by exploring user-level DP, which ensures uniform privacy protection across
  all users.
---

# Mind the Privacy Unit! User-Level Differential Privacy for Language Model Fine-Tuning

## Quick Facts
- arXiv ID: 2406.14322
- Source URL: https://arxiv.org/abs/2406.14322
- Authors: Lynn Chua; Badih Ghazi; Yangsibo Huang; Pritish Kamath; Ravi Kumar; Daogao Liu; Pasin Manurangsi; Amer Sinha; Chiyuan Zhang
- Reference count: 40
- Key outcome: User-wise DP-SGD outperforms Group Privacy for user-level DP in LLM fine-tuning, with data selection strategy significantly impacting performance

## Executive Summary
This paper addresses the challenge of achieving user-level differential privacy for fine-tuning large language models, where each user may contribute multiple records. The authors systematically evaluate two mechanisms—Group Privacy and User-wise DP-SGD—to ensure uniform privacy protection across users regardless of individual contribution size. Through extensive experiments on email and book summary datasets, they demonstrate that User-wise DP-SGD generally provides better privacy-utility tradeoffs, particularly for smaller privacy budgets, and identify data selection strategies as critical to performance.

## Method Summary
The paper explores two mechanisms for user-level DP in LLM fine-tuning: Group Privacy, which preprocesses data to limit each user to k records before applying record-level DP-SGD, and User-wise DP-SGD, which samples users and records at each training step, computing gradients at the user level. Both mechanisms use gradient clipping and noise addition, but differ in how they handle the per-user averaging and sensitivity calculation. The authors evaluate multiple data selection strategies including random sampling, selecting longest/shortest records, and selecting based on perplexity scores.

## Key Results
- User-wise DP-SGD consistently outperforms Group Privacy, especially for smaller privacy budgets (ε ≤ 5)
- Random Chunk selection strategy provides consistent benefits across privacy budgets
- The optimal number of records per user (k) depends on the privacy budget: smaller k for lower ε, larger k for higher ε
- Asi & Liu (2024) algorithm, while theoretically optimal under i.i.d. assumptions, proves impractical due to frequent halting in real-world datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: User-wise DP-SGD provides uniform privacy protection across users regardless of the number of records contributed by each user.
- Mechanism: At each training step, samples a batch of users, then samples several records per user, computes average gradients, clips them, aggregates, and adds noise proportional to the number of users in the batch.
- Core assumption: The privacy guarantee scales with the number of users rather than the total number of records, ensuring equal protection.
- Evidence anchors:
  - [abstract] "User-level DP ensures uniform privacy protection across all users, regardless of each individual's number of contributed records."
  - [section 2] "User-wise DP-SGD does not need to preprocess the dataset to restrict that each user contributes at most k records."
  - [corpus] Weak - related papers focus on user-level DP but don't provide direct empirical support for this mechanism.
- Break condition: If the number of records per user varies significantly, the averaging step may become unstable, leading to poor gradient estimates.

### Mechanism 2
- Claim: Group Privacy transforms record-level DP algorithms into user-level DP by limiting each user's contribution and applying group privacy composition.
- Mechanism: Restricts each user to contribute at most k records, then applies standard DP-SGD on this preprocessed dataset with adjusted privacy budget.
- Core assumption: The group privacy composition theorem provides a valid upper bound on the privacy cost when extending record-level DP to user-level DP.
- Evidence anchors:
  - [section 2] "Group Privacy can be applied to any underlying record-level DP algorithm...by using the standard group privacy lemma."
  - [section 4] "Group Privacy will perform resampling to obtain k records" for users with fewer than k records.
  - [corpus] Weak - related work mentions group privacy but doesn't provide empirical validation in this specific context.
- Break condition: When users have very few records, resampling may introduce bias and degrade model performance.

### Mechanism 3
- Claim: The Asi & Liu (2024) algorithm reduces noise injection when user gradients are well-concentrated, achieving optimal privacy-utility tradeoff.
- Mechanism: Detects if per-user averaged gradients in a batch exhibit low variance; if so, removes outliers and adds less noise; otherwise halts.
- Core assumption: User gradients are i.i.d. drawn from some underlying distribution, allowing for concentration detection and noise reduction.
- Evidence anchors:
  - [section 2] "Asi & Liu (2024) proposed an advanced User-wise DP-SGD algorithm that requires injecting less noise if per-user averaged gradients in a batch exhibit low variance."
  - [section 5.3] "Under the assumption that the records are drawn i.i.d. from some underlying distribution, Asi & Liu (2024) proved that this mechanism can achieve the optimal rate."
  - [corpus] Weak - the related papers don't discuss this specific concentration-based approach.
- Break condition: When user gradients are not well-concentrated (which appears common in practice), the algorithm halts frequently, making it impractical.

## Foundational Learning

- Concept: Differential Privacy (DP) and its composition properties
  - Why needed here: The paper relies on DP composition theorems to extend record-level guarantees to user-level guarantees and to bound cumulative privacy loss.
  - Quick check question: If an algorithm satisfies (ε, δ)-DP for single records, what is the privacy guarantee when applied to groups of k records using basic composition?

- Concept: Gradient clipping and noise addition in DP-SGD
  - Why needed here: Both mechanisms use gradient clipping and noise addition, but with different clipping strategies (per-record vs. per-user averaging) that affect sensitivity and noise requirements.
  - Quick check question: How does averaging gradients over k records per user before clipping affect the clipping norm compared to clipping each record's gradient individually?

- Concept: Privacy accounting for composition of mechanisms
- Why needed here: The paper uses privacy accounting to compute the noise multiplier σ for different mechanisms and privacy budgets.
  - Quick check question: Given T iterations, batch size B, and dataset size M, how does the privacy accountant compute the noise multiplier for (ε, δ)-DP guarantees?

## Architecture Onboarding

- Component map:
  Data preprocessing layer (for Group Privacy only) -> User sampling module (for User-wise DP-SGD) -> Record sampling module (for both) -> Gradient computation and averaging -> Clipping and noise addition -> Model update -> Privacy accounting

- Critical path:
  For User-wise DP-SGD: User sampling → Record sampling → Gradient computation → Averaging → Clipping → Noise addition → Model update
  For Group Privacy: Data preprocessing → Record sampling → Gradient computation → Clipping → Noise addition → Model update

- Design tradeoffs:
  - Group Privacy: Simpler to implement on top of existing record-level DP code but requires preprocessing and may introduce bias through resampling
  - User-wise DP-SGD: More flexible data selection and no preprocessing required but needs user-aware data loading pipeline
  - Asi & Liu method: Potentially better utility when gradients concentrate but impractical due to frequent halting

- Failure signatures:
  - High perplexity across all privacy budgets suggests fundamental issues with the mechanism or data selection
  - Performance degradation as k increases for Group Privacy indicates sensitivity to resampling bias
  - Frequent halting in Asi & Liu method indicates lack of gradient concentration

- First 3 experiments:
  1. Compare perplexity of User-wise DP-SGD vs Group Privacy with random record selection across different ε values
  2. Evaluate impact of different record selection strategies (longest, shortest, perplexity-based) on Group Privacy performance
  3. Test User-wise DP-SGD with random chunk selection vs. record-level selection to verify the benefit of spanning record boundaries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what realistic assumptions can user-level DP algorithms achieve optimal privacy-utility trade-offs?
- Basis in paper: The paper notes that theoretical algorithms like Asi & Liu (2024) achieve optimal rates under the assumption that records are i.i.d. from an underlying distribution, but this assumption may be too restrictive for real-world applications. The authors call for considering more realistic assumptions.
- Why unresolved: The paper demonstrates that the i.i.d. assumption does not hold in their evaluated datasets, but does not propose alternative realistic assumptions or investigate their implications.
- What evidence would resolve it: Empirical studies testing various alternative assumptions (e.g., clustering of user data, bounded user contributions) on diverse real-world datasets to identify conditions under which optimal user-level DP can be achieved.

### Open Question 2
- Question: How do different data selection strategies (beyond record boundaries) impact the privacy-utility trade-off in user-level DP?
- Basis in paper: The paper evaluates several data selection strategies, including random chunk selection which performs best for User-wise DP-SGD. However, the authors suggest that other methods based on gradient information or active learning could be investigated.
- Why unresolved: The paper only explores a limited set of data selection strategies and does not investigate more sophisticated methods that could potentially improve utility.
- What evidence would resolve it: Systematic comparison of user-level DP performance using various advanced data selection methods (e.g., gradient-based, active learning) across multiple datasets and model architectures.

### Open Question 3
- Question: What is the optimal number of records (k) to select per user for different privacy budgets and model sizes in user-level DP?
- Basis in paper: The paper finds that the optimal k depends on the privacy budget (smaller k for lower ε, larger k for higher ε) and suggests a trade-off between utility and efficiency. However, the analysis is limited to specific model sizes and datasets.
- Why unresolved: The study only investigates a few k values and does not provide a general framework for determining the optimal k across different privacy budgets, model sizes, and datasets.
- What evidence would resolve it: Comprehensive empirical study varying k, privacy budgets, and model sizes across diverse datasets to establish a principled approach for selecting the optimal k in user-level DP.

## Limitations
- Asi & Liu algorithm proves impractical in real-world settings despite theoretical optimality under i.i.d. assumptions
- Data selection strategy performance appears dataset-dependent, requiring further investigation for universal approaches
- Group Privacy preprocessing may introduce bias through resampling when users have fewer than k records

## Confidence
- **High confidence**: User-wise DP-SGD generally outperforms Group Privacy for smaller privacy budgets (ε ≤ 5)
- **Medium confidence**: The Random Chunk selection strategy provides consistent benefits across privacy budgets
- **Low confidence**: The optimal data selection strategy is universal across datasets and privacy budgets

## Next Checks
1. **Gradient concentration validation**: Systematically measure the frequency and conditions under which the Asi & Liu algorithm halts, to better understand when concentration-based noise reduction is feasible in practice.

2. **Dataset transfer**: Replicate the data selection strategy experiments on additional datasets (e.g., Reddit comments, Twitter data) to validate whether the Random Chunk strategy generalizes beyond the BookSum and Enron datasets.

3. **Scalability assessment**: Measure training time and memory usage for User-wise DP-SGD as k increases, to quantify the practical limits of this approach for real-world deployments with large datasets.