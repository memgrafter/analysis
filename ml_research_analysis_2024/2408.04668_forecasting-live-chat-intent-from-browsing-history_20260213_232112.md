---
ver: rpa2
title: Forecasting Live Chat Intent from Browsing History
arxiv_id: '2408.04668'
source_url: https://arxiv.org/abs/2408.04668
tags:
- intent
- user
- browsing
- chat
- intents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the problem of forecasting a user''s intent
  before they contact a live chat agent, based on their browsing history. The authors
  propose a two-stage approach: first, a fine-tuned Longformer classifier categorizes
  the browsing history into high-level intent classes; second, a large language model
  (GPT-3.5) generates fine-grained intent candidates, guided by the predicted class.'
---

# Forecasting Live Chat Intent from Browsing History

## Quick Facts
- arXiv ID: 2408.04668
- Source URL: https://arxiv.org/abs/2408.04668
- Reference count: 25
- One-line primary result: Two-stage intent forecasting from browsing history achieves 25.67% top-1 and 45.00% top-5 semantic similarity to ground truth intents.

## Executive Summary
This paper addresses the challenge of forecasting a user's intent for contacting live chat support based on their browsing history. The authors propose a two-stage approach: first classifying browsing histories into high-level intent classes using a fine-tuned Longformer model, then generating fine-grained intent candidates using GPT-3.5 guided by the predicted class. The method significantly outperforms baselines that generate intents without classification guidance. Evaluation uses GPT-4 as a semantic similarity judge, validated against human judgment with high precision and recall.

## Method Summary
The approach processes browsing histories (sequences of page attributes) through a Longformer+ classifier to predict intent class, then conditions GPT-3.5 generation on both the browsing history and predicted class to produce up to five intent candidates. The Longformer+ uses specialized embeddings for token types and page positions to capture browsing sequence structure. Generated intents are evaluated using GPT-4 semantic similarity scoring against ground truth intents, with human validation showing strong alignment.

## Key Results
- Intent classification achieves weighted average precision of 0.683 and recall of 0.658 across five classes
- Intent generation produces similar intent within top-1 for 25.67% of cases and top-5 for 45.00%
- Performance significantly exceeds baselines using no class information (93% and 31% gains)
- GPT-4 evaluation aligns well with human judgment (precision 91.8%, recall 82.6%)

## Why This Works (Mechanism)

### Mechanism 1
The two-stage approach improves performance by first reducing the search space via classification, then guiding the generation model with a predicted intent class. The intent classification stage learns semantic patterns in browsing histories that map to coarse intent categories, reducing the problem from generating any of 5 possible intents to generating one from a focused class.

### Mechanism 2
GPT-4 is a reliable semantic similarity judge for intent evaluation because it aligns well with human judgments. The evaluation stage uses GPT-4 to compare generated intents with ground-truth intents using a prompt that includes demonstrations, correlating strongly with human evaluation.

### Mechanism 3
The Longformer+ architecture outperforms larger general-purpose models for intent classification because it is tailored to the structured browsing history input. Longformer+ uses specialized embeddings (token type, page position) alongside standard token embeddings, allowing it to better model the sequential structure of browsing histories.

## Foundational Learning

- **Transformer-based architectures and attention mechanisms**: The model uses Longformer and GPT-3.5/GPT-4, which are transformer-based. Understanding self-attention, positional encoding, and tokenization is essential to grasp how these models process browsing histories. *Quick check: What is the difference between standard Transformer attention and Longformer's sliding window attention?*

- **Intent classification vs. generation tasks**: The paper distinguishes between classifying browsing histories into coarse intent classes and generating fine-grained natural language intents. Knowing how classification (supervised learning) differs from conditional generation (prompting) is critical. *Quick check: How does the training objective differ between intent classification and intent generation in this pipeline?*

- **Evaluation metrics and semantic similarity**: Performance is measured via precision, recall, and semantic similarity judged by GPT-4. Understanding metrics like Cohen's kappa, precision/recall, and embedding-based similarity is important for interpreting results. *Quick check: Why might embedding-based similarity metrics be unsuitable for evaluating generated intents in this context?*

## Architecture Onboarding

- **Component map**: Browsing histories → Page attribute extraction → Longformer+ classifier → Intent class prediction → GPT-3.5 generator → Intent candidates → GPT-4 similarity judge → Similarity score

- **Critical path**: Convert browsing history (sequence of page attribute dicts) into structured text input → Feed into Longformer+ to obtain predicted intent class → Use predicted class and browsing history to prompt GPT-3.5 for intent generation → Evaluate generated intents using GPT-4 semantic similarity to ground-truth

- **Design tradeoffs**: Using fine-tuned Longformer+ vs. larger general models: better task-specific accuracy but smaller model size; Zero-shot GPT-3.5 vs. fine-tuned: cheaper and leverages strong reasoning, but less task-specific control; GPT-4 evaluation vs. human evaluation: scalable but slightly less precise than human judgment

- **Failure signatures**: Low classification precision → poor guidance for generation → irrelevant intent candidates; High variance in GPT-3.5 outputs → inconsistent generation quality; GPT-4 evaluation bias → systematic over- or under-estimation of performance

- **First 3 experiments**: Ablation: run classification only (no generation) and measure class prediction accuracy to validate the classifier's core capability; Ablation: run generation only (no classification) and measure similarity to assess how much the classification stage improves results; Robustness test: add noise to browsing histories (e.g., random page removals) and measure degradation in both stages

## Open Questions the Paper Calls Out
- **Open Question 1**: How would incorporating more detailed user interaction data (e.g., hover time, scroll depth) affect intent prediction performance compared to using only page attributes? The authors mention incorporating detailed user interactions as a future work item.

- **Open Question 2**: Would training a dedicated component to select specific items from browsing history improve the accuracy of generated fine-grained intents? The authors explicitly state this as a planned improvement, noting that the model sometimes selects wrong items from browsing history.

- **Open Question 3**: How would integrating external knowledge sources (FAQs, product documentation, seasonal sales data) impact intent prediction performance? The authors mention integrating external knowledge as a future work item.

## Limitations
- The browsing history attribute structure is not fully specified, which may affect reproducibility of the input encoding
- The prompts and demonstration examples for GPT-4 evaluation are not provided, potentially introducing variance in similarity judgments
- The model's robustness to noisy or incomplete browsing histories is not thoroughly validated

## Confidence
- **High confidence**: The two-stage approach's performance gains (93% and 31% over baselines) are well-supported by ablation results
- **Medium confidence**: The GPT-4 evaluation's alignment with human judgment (91.8% precision, 82.6% recall) is validated on a subset, but the robustness of this alignment across the full dataset is uncertain
- **Medium confidence**: The claim that Longformer+ outperforms larger models due to structured embeddings is supported, but the analysis does not explore edge cases where this advantage might diminish

## Next Checks
1. Test the model's sensitivity to variations in browsing history attribute formatting and tokenization to identify robustness limits
2. Re-run a subset of evaluations with slightly modified prompts to measure variance in similarity judgments
3. Evaluate performance on very long or highly noisy browsing histories to assess whether the structured embeddings remain beneficial