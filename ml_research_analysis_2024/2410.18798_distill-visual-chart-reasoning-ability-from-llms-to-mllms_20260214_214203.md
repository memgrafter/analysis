---
ver: rpa2
title: Distill Visual Chart Reasoning Ability from LLMs to MLLMs
arxiv_id: '2410.18798'
source_url: https://arxiv.org/abs/2410.18798
tags:
- chart
- data
- reasoning
- code
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Code-as-Intermediary Translation (CIT), a method
  that uses code as an intermediary to translate visual chart representations into
  textual form, enabling language models to understand cross-modal information and
  generate reasoning chains. The approach employs text-based synthesizing techniques
  to expand chart-plotting code and generate high-quality Q&A pairs, resulting in
  REACH QA, a dataset of 3k reasoning-intensive charts and 20k Q&A pairs.
---

# Distill Visual Chart Reasoning Ability from LLMs to MLLMs

## Quick Facts
- **arXiv ID:** 2410.18798
- **Source URL:** https://arxiv.org/abs/2410.18798
- **Reference count:** 40
- **Primary result:** Proposes Code-as-Intermediary Translation (CIT) method using code to translate visual chart representations into text, enabling reasoning chains through language models

## Executive Summary
This paper introduces Code-as-Intermediary Translation (CIT), a novel approach that uses code as an intermediary to translate visual chart representations into textual form. The method enables language models to understand cross-modal information and generate reasoning chains by synthesizing chart-plotting code and generating high-quality Q&A pairs. The authors created REACH QA, a dataset containing 3,000 reasoning-intensive charts and 20,000 Q&A pairs, and demonstrate that models fine-tuned with this dataset perform well on chart-related tasks while showing performance gains on general reasoning benchmarks like MathVista.

## Method Summary
The approach employs text-based synthesizing techniques to expand chart-plotting code and generate high-quality Q&A pairs through a code-to-text translation pipeline. The Code-as-Intermediary Translation (CIT) framework uses code as an intermediary representation to bridge visual chart data and textual reasoning. By translating visual elements into code syntax, the method enables language models to process cross-modal information through familiar textual patterns. The REACH QA dataset was constructed using this pipeline, providing a large-scale resource for training models to perform reasoning on visual chart data.

## Key Results
- Models fine-tuned with REACH QA demonstrate strong performance on chart-related reasoning tasks
- The approach shows performance improvements on general reasoning benchmarks like MathVista
- REACH QA contains 3k reasoning-intensive charts and 20k Q&A pairs, enabling effective multi-modal training

## Why This Works (Mechanism)
The CIT framework works by leveraging code as a universal intermediary language that can represent both visual chart structures and textual reasoning processes. Code serves as a structured, unambiguous format that captures the relationships between chart elements, making it easier for language models to parse and reason about visual information. By converting visual charts into code representations, the method transforms a cross-modal reasoning problem into a more tractable text-to-text generation task that LLMs can handle effectively.

## Foundational Learning

**Code-based visual representation:** Why needed - To provide a structured, parseable format for visual chart elements; Quick check - Can the generated code accurately reconstruct the original chart's visual elements and relationships?

**Cross-modal translation:** Why needed - To bridge the gap between visual perception and textual reasoning; Quick check - Does the translation preserve semantic relationships between chart elements across modalities?

**Synthetic data generation:** Why needed - To create large-scale training data for chart reasoning tasks; Quick check - Do generated Q&A pairs exhibit logical consistency and reasoning depth comparable to human-created examples?

## Architecture Onboarding

**Component map:** Visual chart input -> Code synthesis module -> Code-to-text translation -> Reasoning chain generation -> Q&A output

**Critical path:** The pipeline flows from visual chart detection through code synthesis, where the quality of code generation directly impacts downstream reasoning performance. The translation from code to reasoning chains represents the most critical transformation, as errors here propagate to final Q&A quality.

**Design tradeoffs:** The method trades computational overhead of code synthesis for improved reasoning accuracy, prioritizing structured representation over raw visual processing speed. This approach assumes that code-based representations can capture sufficient visual information for reasoning tasks.

**Failure signatures:** Poor code synthesis quality leading to incorrect chart element relationships, translation errors that break semantic coherence between visual and textual representations, and generation of illogical Q&A pairs that violate basic reasoning principles.

**First experiments:** 1) Test code synthesis accuracy on diverse chart types to verify representation fidelity; 2) Evaluate translation quality between code and natural language reasoning; 3) Measure Q&A generation consistency across different chart complexity levels.

## Open Questions the Paper Calls Out
None

## Limitations
- The REACH QA dataset's relatively small scale (3k charts, 20k Q&A pairs) raises questions about generalizability to diverse real-world chart types
- The code-based translation approach may not adequately capture complex visual encodings or novel chart types
- Performance improvements on MathVista benchmarks may be influenced by confounding factors like model architecture changes

## Confidence

**High confidence:** Feasibility of code as intermediary for visual-to-textual translation and basic framework functionality
**Medium confidence:** Performance improvements on chart-related tasks and generalization to MathVista benchmarks
**Low confidence:** Scalability to diverse real-world charts and robustness to novel visual encodings

## Next Checks

1. Test fine-tuned models on out-of-distribution charts including novel chart types, unconventional visual encodings, and charts with complex annotations not present in REACH QA

2. Conduct ablation studies removing the code intermediary step to quantify its specific contribution to reasoning performance

3. Evaluate model performance degradation rates as a function of chart complexity and visual density to establish performance boundaries