---
ver: rpa2
title: 'sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through N-shot
  Guided Prompting'
arxiv_id: '2407.09879'
source_url: https://arxiv.org/abs/2407.09879
tags:
- sphin
- language
- dataset
- languages
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the performance gap of large language models
  (LLMs) in non-English languages by introducing sPhinX, a multilingual synthetic
  instruction tuning dataset, and LANGIT, an N-shot guided fine-tuning strategy. sPhinX
  enhances diversity by selectively augmenting English instruction-response pairs
  with multilingual translations using GPT-4, avoiding direct translation of entire
  instructions.
---

# sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through N-shot Guided Prompting

## Quick Facts
- arXiv ID: 2407.09879
- Source URL: https://arxiv.org/abs/2407.09879
- Reference count: 21
- Improves multilingual instruction-following by 39.8% average for Mistral-7B and 11.2% for Phi-3-Small

## Executive Summary
This paper addresses the performance gap of large language models in non-English languages by introducing sPhinX, a multilingual synthetic instruction tuning dataset, and LANGIT, an N-shot guided fine-tuning strategy. sPhinX enhances diversity by selectively augmenting English instruction-response pairs with multilingual translations using GPT-4, avoiding direct translation of entire instructions. LANGIT improves performance by incorporating contextually relevant few-shot examples during training. Experiments show that fine-tuning Mistral-7B and Phi-3-Small on sPhinX improves multilingual performance by an average of 39.8% and 11.2% respectively across benchmarks in reasoning, question answering, reading comprehension, and machine translation. The approach also maintains strong English performance without catastrophic forgetting, even when trained on 51 languages.

## Method Summary
The sPhinX method creates a multilingual instruction dataset by selectively translating task-critical components of English instruction-response pairs using GPT-4, rather than translating entire instructions. This selective translation approach preserves semantic integrity while enabling multilingual instruction diversity. The LANGIT strategy then improves fine-tuning by incorporating N-shot examples (0-6 contextually relevant examples) from the same language during training, helping models generalize to diverse task formats. The dataset contains 1.8M instruction-response pairs across 51 languages with careful sampling to avoid catastrophic forgetting, using 100k high-resource, 50k mid-resource, and 25k low-resource language samples.

## Key Results
- Fine-tuning Mistral-7B on sPhinX improves multilingual performance by 39.8% average across benchmarks
- Fine-tuning Phi-3-Small on sPhinX improves multilingual performance by 11.2% average across benchmarks
- Maintains strong English performance without catastrophic forgetting across 51 languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selective Translated Augmentation preserves semantic integrity while enabling multilingual instruction diversity.
- Mechanism: Only task-critical components are translated into target language while preserving original language in context-dependent elements.
- Core assumption: GPT-4 can accurately identify and translate only semantically necessary portions without distorting task intent.
- Evidence anchors:
  - [abstract]: "sPhinX enhances diversity by selectively augmenting English instruction-response pairs with multilingual translations using GPT-4, avoiding direct translation of entire instructions."
  - [section]: "To mitigate this issue, we used GPT-4 to augment the instructions using Selective Translated Augmentation, so that task-specific components of instruction responses are translated into the appropriate language without changing the semantic meaning."
- Break condition: If GPT-4 misidentifies task-specific components, translations could distort semantic intent and reduce model performance.

### Mechanism 2
- Claim: N-shot guided fine-tuning (LANGIT) improves multilingual model generalization by exposing models to contextually relevant examples.
- Mechanism: Each instruction sample is optionally prepended with 0-6 few-shot examples from the same language during training.
- Core assumption: Contextual examples improve model's ability to generalize to unseen tasks within the same language.
- Evidence anchors:
  - [abstract]: "LANGIT improves performance by incorporating contextually relevant few-shot examples during training."
  - [section]: "This added context helps guide the model, enabling it to learn more effectively from the provided examples."
- Break condition: If few-shot examples introduce noise or task ambiguity, model performance could degrade.

### Mechanism 3
- Claim: SPHIN X dataset diversity reduces catastrophic forgetting by balancing high-resource and low-resource language exposure.
- Mechanism: Dataset samples are drawn from a large pool with different subsets for each language, reducing sample overlap.
- Core assumption: Sampling without replacement from a large pool ensures minimal overlap between languages.
- Evidence anchors:
  - [section]: "SPHIN X ensures diversity by sampling unique subsets of instruction-response pairs for each language."
  - [section]: "Compared to AYA, which exhibits moderate variation across task instructions, SPHIN X introduces greater sample diversity."
- Break condition: If sampling is not truly independent, high-resource language performance could degrade due to over-representation of low-resource samples.

## Foundational Learning

- Concept: Multilingual tokenization and script handling
  - Why needed here: Different languages have varying tokenization behaviors; ensuring consistent model understanding across scripts is critical for instruction-following.
  - Quick check question: How does tokenization differ between high-resource Latin scripts and low-resource non-Latin scripts, and how might this affect model fine-tuning?

- Concept: Instruction tuning vs. standard fine-tuning
  - Why needed here: Instruction tuning aligns models to follow human-readable prompts rather than just task-specific patterns.
  - Quick check question: What distinguishes instruction tuning from standard supervised fine-tuning in terms of loss functions and training objectives?

- Concept: Catastrophic forgetting in multilingual models
  - Why needed here: Multilingual fine-tuning can degrade performance on high-resource languages if low-resource languages dominate training.
  - Quick check question: How does sample distribution across languages affect retention of high-resource language capabilities?

## Architecture Onboarding

- Component map:
  Data pipeline: English Orca dataset → Selective Translated Augmentation → SPHIN X dataset
  Fine-tuning pipeline: Base model → LANGIT strategy (N-shot examples) → Instruction-tuned model
  Evaluation pipeline: Multilingual benchmarks (XCOPA, XStoryCloze, XWinograd, XQuAD, Belebele, Translation)

- Critical path:
  1. Generate SPHIN X dataset via selective translation
  2. Apply LANGIT strategy during fine-tuning
  3. Evaluate on multilingual benchmarks
  4. Compare against baseline datasets (AYA, BACTRIAN, M-ALPACA)

- Design tradeoffs:
  - Selective translation vs. full translation: Higher quality but more complex generation process
  - N-shot examples vs. no examples: Improved generalization but increased context length and computational cost
  - Sample size distribution: Fewer low-resource samples to avoid catastrophic forgetting vs. better low-resource coverage

- Failure signatures:
  - Performance degradation in high-resource languages: Indicates catastrophic forgetting
  - Inconsistent task understanding across languages: Suggests translation errors or insufficient task diversity
  - No improvement over baselines: Indicates ineffective augmentation or fine-tuning strategy

- First 3 experiments:
  1. Compare selective translation vs. full translation baseline on a small multilingual subset
  2. Evaluate LANGIT impact by training with and without few-shot examples on a single language
  3. Test catastrophic forgetting by measuring English performance before and after multilingual fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the LANGIT strategy generalize beyond 7B models to larger language models and how does performance scale with model size?
- Basis in paper: [inferred] The paper explicitly states that all experiments were conducted using 7B base models and suggests exploring adaptive fine-tuning techniques like LoRA and PEFT as future work, indicating this remains untested.
- Why unresolved: The paper's experiments are limited to 7B models, leaving the effectiveness of LANGIT on larger models (e.g., 13B, 70B) unexplored, which could reveal different scaling behaviors or diminishing returns.
- What evidence would resolve it: Experiments applying LANGIT to instruction-tune larger models (e.g., 13B or 70B variants) and comparing their multilingual performance gains against vanilla fine-tuning, particularly on low-resource languages.

### Open Question 2
- Question: Can the Selective Translated Augmentation approach be extended to handle code-switching scenarios more explicitly, rather than relying on emergent code-switching during training?
- Basis in paper: [explicit] The paper mentions that code-switching naturally emerges from the augmentation process but notes this phenomenon is currently out-of-scope, suggesting it wasn't systematically studied or optimized.
- Why unresolved: The paper observes code-switching as a byproduct but doesn't investigate whether intentionally designing code-switched examples could improve multilingual instruction-following or if certain code-mixing patterns are more beneficial than others.
- What evidence would resolve it: Controlled experiments creating datasets with explicit code-switching patterns versus pure translation, measuring performance differences on code-switched evaluation sets and analyzing which mixing strategies yield the best results.

### Open Question 3
- Question: How does the N-shot selection strategy in LANGIT perform when using examples from different languages within the same script family versus strictly from the same language?
- Basis in paper: [explicit] The paper suggests future work could investigate using N examples from the same script to introduce greater diversity, particularly for low-resource languages, but doesn't test this hypothesis.
- Why unresolved: The current LANGIT strategy samples examples strictly from the same language, potentially missing opportunities to leverage cross-linguistic similarities within scripts that could benefit low-resource language performance without causing interference.
- What evidence would resolve it: Comparative experiments where N-shot examples are drawn either from the same language or from other languages sharing the same script (e.g., Hindi and Marathi for Devanagari), measuring performance impacts on target low-resource languages.

## Limitations

- The core methodology relies heavily on GPT-4's translation capabilities for selective augmentation, but exact prompt templates and quality control mechanisms remain unspecified
- The 51-language distribution across resource tiers is mentioned but not detailed, making it difficult to assess scalability to languages with extreme data scarcity
- Lacks ablation studies to isolate individual contributions of selective translation versus N-shot fine-tuning

## Confidence

- **High Confidence**: The empirical results showing 39.8% average improvement for Mistral-7B and 11.2% for Phi-3-Small across multilingual benchmarks are well-supported by the experimental data.
- **Medium Confidence**: The mechanism of selective translation preserving semantic integrity depends on GPT-4's consistent performance, which may vary across languages and task types.
- **Medium Confidence**: The claim of avoiding catastrophic forgetting is supported by maintained English performance, but the long-term stability across many languages and extended training remains untested.

## Next Checks

1. Conduct an ablation study comparing full translation versus selective translation on a subset of languages to isolate the contribution of the augmentation method.
2. Perform extended training with periodic evaluation to test for gradual catastrophic forgetting in high-resource languages over many epochs.
3. Validate translation quality and task preservation for medium/low-resource languages through native speaker evaluation, particularly for languages with non-Latin scripts.