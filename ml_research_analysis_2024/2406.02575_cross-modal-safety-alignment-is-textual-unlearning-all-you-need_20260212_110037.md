---
ver: rpa2
title: 'Cross-Modal Safety Alignment: Is textual unlearning all you need?'
arxiv_id: '2406.02575'
source_url: https://arxiv.org/abs/2406.02575
tags:
- unlearning
- arxiv
- multi-modal
- harmful
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether textual unlearning in vision-language
  models (VLMs) can address cross-modality safety alignment challenges. The core method
  involves applying unlearning solely in the textual domain of VLMs, freezing vision
  encoder and projection layers while updating the language model parameters.
---

# Cross-Modal Safety Alignment: Is textual unlearning all you need?

## Quick Facts
- arXiv ID: 2406.02575
- Source URL: https://arxiv.org/abs/2406.02575
- Reference count: 23
- Primary result: Textual unlearning reduces attack success rates to less than 8% while maintaining utility

## Executive Summary
This paper investigates whether textual unlearning can effectively address safety alignment challenges in vision-language models (VLMs) without requiring multi-modal unlearning. The core finding is that applying unlearning solely in the textual domain—while freezing vision encoders and projection layers—significantly reduces both text-based and vision-text-based attack success rates to under 8%, with some cases reaching nearly 2%. The method maintains utility on benign inputs while being up to 6 times more computationally efficient than multi-modal alternatives. Experiments across seven datasets demonstrate the transferability of textual unlearning across modalities.

## Method Summary
The method applies textual unlearning to VLMs by updating only the language model parameters while freezing the vision encoder and projection layers. Three loss terms guide the unlearning process: minimizing harmful content generation, increasing helpful responses to harmful inputs, and preserving utility on benign inputs. The approach uses gradient ascent on harmful data and gradient descent on helpful/normal data, with dynamic logit adjustment that redirects harmful context toward safer outputs rather than memorizing specific harmful responses. This parameter-efficient fine-tuning (using QLoRA) focuses on the language space where all information ultimately flows through, regardless of input modality.

## Key Results
- Textual unlearning reduces attack success rates to less than 8% (as low as nearly 2% in some cases) for both text-based and vision-text-based attacks
- Multi-modal unlearning and supervised fine-tuning offer no additional benefits but require up to 6 times more computational resources
- The approach maintains utility on benign inputs while effectively preventing harmful content generation across modalities

## Why This Works (Mechanism)

### Mechanism 1
Textual unlearning works by shifting the model's logits away from harmful outputs and toward helpful responses through gradient ascent on harmful data and gradient descent on helpful/normal data. The model learns to avoid generating harmful tokens by maximizing the probability of harmful output (lharm) while simultaneously minimizing the probability of helpful responses to harmful inputs (lhelpful.match) and maintaining utility on benign inputs (lutility). This creates a dynamic where the model learns to redirect harmful context toward safer outputs. The core assumption is that the language space contains sufficient information to capture harmful concepts regardless of input modality, allowing textual unlearning to generalize across modalities. Evidence shows ASR reduction to less than 8% across six datasets. The approach could fail if harmful concepts are not adequately represented in the language space or if projection layers fail to properly align visual features with textual embeddings.

### Mechanism 2
Textual unlearning is more computationally efficient than multi-modal unlearning because it only updates language model parameters while freezing vision encoder and projection layers. By freezing the vision encoder θ and projection layers ψ, the method avoids the computational overhead of updating parameters associated with additional modalities, focusing only on the language model parameters σ. The core assumption is that the vision encoder and projection layers contain sufficient pre-trained knowledge that does not need updating for safety alignment. Evidence shows multi-modal unlearning requires up to 6 times more computational resources without offering additional benefits. The approach could fail if vision encoder or projection layers contribute to safety vulnerabilities.

### Mechanism 3
Textual unlearning prevents context contamination by teaching the model to dynamically redirect harmful context toward helpful directions, rather than memorizing specific harmful responses. Unlike SFT which adjusts logits to specific target answers for harmful prompts, unlearning uses gradient ascent to push away from harmful outputs while simultaneously learning to generate helpful responses, creating a more flexible defense that generalizes to unseen prompts. The core assumption is that dynamic logit adjustment is more effective than static target-based fine-tuning for preventing harmful content generation. Evidence shows SFT continues harmful context while unlearning redirects generation away from harm. The approach could fail if the harmful dataset is not diverse enough to capture all harmful concepts.

## Foundational Learning

- **Gradient-based optimization in neural networks**: The unlearning approach relies on gradient ascent and descent to modify model parameters based on different loss terms for harmful, helpful, and normal data. Quick check: What is the difference between gradient ascent and gradient descent, and when would you use each in training a neural network?

- **Kullback-Leibler (KL) divergence**: KL divergence is used to measure the similarity between the output distributions of the original and unlearned models on normal inputs, ensuring utility preservation. Quick check: How does KL divergence measure the difference between two probability distributions, and why is it useful for comparing model outputs?

- **Vision-language model architecture**: Understanding the three-component structure (vision encoder, projection layers, language model) is crucial for understanding why textual unlearning can work by only updating the language model. Quick check: In a typical vision-language model, what is the role of the projection layers, and why does freezing them while updating only the language model make sense for unlearning?

## Architecture Onboarding

- **Component map**: VLM consists of vision encoder (Vθ) that processes images into embeddings, projection layers (Pψ) that map image embeddings to text space, and language model (Lσ) that generates text responses. During unlearning, only language model parameters (σ) are updated while vision encoder (θ) and projection layers (ψ) remain frozen.

- **Critical path**: The critical path for safety alignment is the flow from input modalities through the projection layers to the language model. Since all information ultimately flows through the language space, modifying the language model's behavior can affect the entire system's safety.

- **Design tradeoffs**: The approach trades off potential benefits of updating visual components for significant computational efficiency gains. It assumes that safety vulnerabilities primarily manifest in the language space rather than in the visual processing components.

- **Failure signatures**: If textual unlearning fails, you might see: (1) ASR remains high on vision-text attacks, (2) utility degrades on normal inputs, (3) the model generates harmful content even with textual unlearning applied, or (4) the model becomes overly conservative and refuses legitimate requests.

- **First 3 experiments**:
  1. Run the original model on the test datasets (PKU-SafeRLHF, Jailbreak in Pieces, miniJailbreakV) to establish baseline ASR and utility metrics.
  2. Implement textual unlearning with the three loss terms and evaluate on the same test sets to verify ASR reduction while maintaining utility.
  3. Compare computational resources (GPU time, energy consumption) between textual unlearning and a multi-modal unlearning baseline to quantify efficiency gains.

## Open Questions the Paper Calls Out

### Open Question 1
Does textual unlearning maintain effectiveness when applied to larger VLMs with full 32-bit precision rather than using 4-bit quantization with QLoRA? The paper acknowledges a limitation that experiments were conducted on VLMs with 7 billion parameters using QLoRA, and suggests that larger-scale models with full 32-bit precision might yield more comprehensive insights. This remains unresolved due to computational resource constraints. Experiments demonstrating similar ASR reduction rates and utility preservation on larger VLMs (e.g., 13B+ parameters) trained with full 32-bit precision would resolve this question.

### Open Question 2
Can textual unlearning effectively defend against adversarial perturbation-based attacks on VLMs, or is it primarily effective against cross-modality jailbreak attacks? The paper explicitly states that their evaluation addresses jailbreaking due to additional modalities but notes that further research is needed to determine whether the unlearned model can effectively counter adversarial perturbation-based attacks. This remains unresolved because the authors did not evaluate robustness against adversarial examples created through gradient-based optimization. Comparative evaluation against standard adversarial attack methods (like FGSM or PGD) would resolve this question.

### Open Question 3
How does the coverage and diversity of harmful concepts in textual-only datasets affect the cross-modality generalization of textual unlearning compared to using multi-modal datasets? The paper discusses that multi-modal unlearning and SFT require 6x more computational resources and suggests that constructing comprehensive textual datasets might be more effective than gathering multi-modal ones, but does not empirically test this hypothesis with varying dataset qualities. This remains unresolved as the authors only tested a fixed set of datasets without systematically varying their coverage or diversity. Controlled experiments comparing textual unlearning performance across datasets with varying levels of harmful concept coverage would resolve this question.

## Limitations

- The paper's claims about cross-modal transferability rely on the assumption that harmful concepts are sufficiently represented in the language space, which may not hold for all types of harmful content where visual context is essential
- The evaluation focuses primarily on text-based and vision-text-based attacks without comprehensively testing generalization to other modalities like audio or video-only inputs
- The computational efficiency claims are based on comparisons with multi-modal unlearning baselines, but specific implementation details of these baselines are not fully specified

## Confidence

- **High confidence**: The empirical results showing ASR reduction to less than 8% with textual unlearning are well-supported by the experimental data across multiple datasets
- **Medium confidence**: The claim that textual unlearning maintains utility while reducing harmful content generation is supported, but the evaluation of utility metrics could be more comprehensive
- **Low confidence**: The assertion that multi-modal unlearning offers no additional benefits is based on limited evidence and does not account for all possible attack scenarios or modalities

## Next Checks

1. Test the unlearned model on a diverse set of multimodal inputs including audio, video, and image-only prompts to verify cross-modal generalizability beyond text-vision combinations

2. Conduct ablation studies to determine which of the three loss terms contributes most significantly to ASR reduction and utility preservation, providing clearer mechanistic insights

3. Implement a comprehensive utility benchmark that includes more diverse tasks and human evaluation to ensure that safety improvements do not come at the cost of practical usability across different domains