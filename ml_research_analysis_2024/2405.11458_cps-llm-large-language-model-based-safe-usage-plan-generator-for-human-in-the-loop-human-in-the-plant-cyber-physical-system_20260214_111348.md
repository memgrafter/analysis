---
ver: rpa2
title: 'CPS-LLM: Large Language Model based Safe Usage Plan Generator for Human-in-the-Loop
  Human-in-the-Plant Cyber-Physical System'
arxiv_id: '2405.11458'
source_url: https://arxiv.org/abs/2405.11458
tags:
- insulin
- time
- plan
- llms
- bolus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the feasibility of using large language
  models (LLMs) to generate personalized, safe usage plans for human-in-the-loop human-in-the-plant
  (HIL-HIP) cyber-physical systems (CPS). The authors propose CPS-LLM, an LLM retrained
  using an instruction tuning framework, to ensure generated plans align with CPS
  dynamics and are safe for users.
---

# CPS-LLM: Large Language Model based Safe Usage Plan Generator for Human-in-the-Loop Human-in-the-Plant Cyber-Physical System

## Quick Facts
- arXiv ID: 2405.11458
- Source URL: https://arxiv.org/abs/2405.11458
- Reference count: 14
- Primary result: CPS-LLM achieves 81% time in range (70-180 mg/dL) with 0% time in critical hypoglycemia range (<54 mg/dL) for automated insulin delivery in T1D.

## Executive Summary
This paper proposes CPS-LLM, a framework that uses large language models (LLMs) to generate personalized, safe usage plans for human-in-the-loop human-in-the-plant (HIL-HIP) cyber-physical systems (CPS). The approach combines a liquid time constant neural network (LTC NN) to estimate personalized dynamics coefficients with an instruction-tuned LLM that generates plans consistent with system behavior. The framework is validated on automated insulin delivery for Type 1 Diabetes, demonstrating significant improvements in safety metrics compared to manual planning and untuned LLMs.

## Method Summary
CPS-LLM uses an instruction tuning framework where a liquid time constant neural network-based physical dynamics coefficient estimator derives coefficients from system traces. These coefficients are then used to train an LLM with prompts that interleave textual descriptions with time series traces of the physical dynamics. The fine-tuned LLM generates plans that are both feasible and safe for the specific user and system. The approach was validated using automated insulin delivery systems for Type 1 Diabetes, showing improved safety and efficacy compared to manual planning and untuned LLMs.

## Key Results
- Achieved 81% time in range (70-180 mg/dL) for glucose control
- Reduced time below range (<70 mg/dL) to 3%
- Maintained 0% time in critical hypoglycemia range (<54 mg/dL)
- Outperformed both manual planning and untuned LLM approaches

## Why This Works (Mechanism)

### Mechanism 1
The liquid time constant neural network (LTC NN) extracts personalized physical dynamics coefficients from system traces, which are then used to train an LLM with prompts that embody traces from the dynamical system and the corresponding model coefficients. This fine-tuned LLM generates plans that are both feasible and safe for the specific user and system. Core assumption: The dynamical system can be approximated by a parameterized model where the coefficients can be extracted from system traces.

### Mechanism 2
Contextualizing the LLM with example prompts and responses through reinforcement learning with human feedback (RLHF) enables it to generate domain-specific and safe plans. By providing the LLM with example prompts and responses that demonstrate safe and effective plans, it learns to generate similar plans for new, unseen scenarios. Core assumption: The LLM can learn from example prompts and responses to generate safe and effective plans for new scenarios.

### Mechanism 3
Embodied fine-tuning of the LLM with prompts that interleave textual descriptions with time series traces of the physical dynamics enables it to generate plans that are consistent with the system's behavior. By training the LLM on prompts that contain both textual descriptions and time series traces, it learns to understand the relationship between the system's behavior and the corresponding textual descriptions. Core assumption: The LLM can learn to associate textual descriptions with time series traces of the system's behavior.

## Foundational Learning

- **Liquid Time Constant Neural Networks (LTC NNs)**: Used to extract personalized physical dynamics coefficients from system traces. Why needed here: Allows the LLM to generate plans consistent with specific user's system dynamics. Quick check: What is the main advantage of using LTC NNs over traditional deep learning techniques for extracting system dynamics coefficients?

- **Instruction Tuning (IT)**: Fine-tuning the LLM with diverse multi-task datasets formatted in natural language descriptions. Why needed here: Enables the LLM to follow task instructions for novel tasks without explicit examples. Quick check: How does instruction tuning differ from traditional fine-tuning of LLMs?

- **Reinforcement Learning with Human Feedback (RLHF)**: Involves humans in the training loop through carefully designed labeling strategies. Why needed here: Ensures the LLM adheres to specified instructions and generates safe outputs. Quick check: What is the main goal of using RLHF in training LLMs?

## Architecture Onboarding

- **Component map**: User Interface -> CPS-LLM -> LTC NN -> System Trace -> Controller -> Simulation Environment
- **Critical path**: 1) User provides natural language prompt and system trace; 2) LTC NN extracts personalized dynamics coefficients; 3) CPS-LLM generates usage plan; 4) Plan evaluated for safety through forward simulation; 5) If safe, plan executed; otherwise, feedback provided
- **Design tradeoffs**: Accuracy vs. Interpretability (fine-tuning may reduce interpretability), Personalization vs. Generalization (personalized coefficients may limit applicability), Safety vs. Efficiency (rigorous simulation may increase planning time)
- **Failure signatures**: Inaccurate dynamics coefficient extraction, unsafe plan generation, slow planning process
- **First 3 experiments**: 1) Test LTC NN accuracy in extracting dynamics coefficients; 2) Evaluate CPS-LLM plan safety through simulation; 3) Compare CPS-LLM with manual planning approach

## Open Questions the Paper Calls Out

- **Cross-domain generalizability**: How effective is CPS-LLM in other domains beyond automated insulin delivery? The approach has only been tested on T1D systems, and its effectiveness in other safety-critical domains like autonomous vehicles or industrial control systems is unknown.

- **Comparison with state-of-the-art**: How does CPS-LLM performance compare to other safe planning methods in CPS? The paper discusses advantages over traditional techniques but doesn't directly compare to optimization, game theoretic, or reinforcement learning approaches.

- **Robustness to data variations**: How robust is CPS-LLM to variations in training data quality and quantity? The paper doesn't investigate sensitivity to limited or noisy training data, which is crucial for practical applicability.

## Limitations

- The generalizability of CPS-LLM to other cyber-physical systems beyond automated insulin delivery remains unproven
- The paper lacks sufficient details on training data, prompts, and hyperparameters for fine-tuning, limiting reproducibility
- Safety and efficacy evaluation is based on a single case study without long-term performance assessment

## Confidence

- **High Confidence**: The overall framework combining LTC NN with instruction-tuned LLM is sound and well-motivated
- **Medium Confidence**: Specific implementation details and T1D case study performance are credible but need further validation
- **Low Confidence**: Generalizability to other CPS domains and long-term safety/efficacy remain uncertain

## Next Checks

1. Apply CPS-LLM to other cyber-physical systems with different dynamics and safety requirements to assess generalizability
2. Conduct a longitudinal study of CPS-LLM performance over an extended period under varying conditions
3. Benchmark CPS-LLM against other plan generation approaches like model predictive control and reinforcement learning