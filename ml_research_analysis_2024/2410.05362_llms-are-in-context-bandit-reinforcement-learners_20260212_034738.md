---
ver: rpa2
title: LLMs Are In-Context Bandit Reinforcement Learners
arxiv_id: '2410.05362'
source_url: https://arxiv.org/abs/2410.05362
tags:
- naive
- icrl
- learning
- stochastic
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) show strong in-context learning (ICL)
  capabilities, but it is unclear whether they can learn online from their own predictions
  and external rewards, as in contextual bandit reinforcement learning (RL). We study
  whether LLMs can perform in-context reinforcement learning (ICRL), where they learn
  by generating predictions, receiving feedback, and improving their responses without
  parameter updates.
---

# LLMs Are In-Context Bandit Reinforcement Learners

## Quick Facts
- arXiv ID: 2410.05362
- Source URL: https://arxiv.org/abs/2410.05362
- Authors: Giovanni Monea; Antoine Bosselut; KiantÃ© Brantley; Yoav Artzi
- Reference count: 40
- Large language models can learn in-context from their own predictions and rewards, showing improvements of 28.6-74.4% on classification tasks.

## Executive Summary
Large language models demonstrate strong in-context learning capabilities, but their ability to learn online from their own predictions and external rewards remains unclear. This paper investigates whether LLMs can perform in-context reinforcement learning by generating predictions, receiving feedback, and improving responses without parameter updates. Through experiments with multiple models and classification tasks, the authors show that both Naive+ and Stochastic ICRL methods significantly outperform zero-shot performance, with Stochastic ICRL providing more stable learning by avoiding performance dips from negative feedback.

## Method Summary
The paper studies in-context reinforcement learning (ICRL) in a contextual bandit setting where LLMs learn from their own predictions and external rewards without parameter updates. Three methods are designed: Naive ICRL (includes all episodes), Naive+ ICRL (includes only positive-reward episodes), and Stochastic ICRL (uses randomly sampled subsets of positive episodes to encourage exploration). Experiments use classification datasets (Banking77, CLINC150, NLU, TREC, TREC-fine) with models including Llama 3.1, Qwen2.5, and Gemini, measuring test accuracy and regret over 10,000 steps.

## Key Results
- Both Naive+ and Stochastic ICRL significantly outperform zero-shot performance, with improvements of 28.6-74.4% for Llama and 29.2-68.4% for Qwen
- Stochastic ICRL is more stable than Naive+, avoiding performance dips caused by negative feedback
- Larger models show better ICRL performance and stability, with strong correlation between performance and model scale
- ICRL can learn without relying on label semantics, achieving performance gains even with abstract labels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can learn online by incorporating their own successful predictions into the context, treating them as demonstrations.
- Mechanism: When an LLM generates a prediction that receives positive reward, it adds the input-output-reward triplet to its context. On subsequent inputs, the model samples from the learned policy conditioned on this expanded context, enabling policy improvement without parameter updates.
- Core assumption: The model can interpret and utilize the reward signal to adjust its behavior implicitly through context conditioning.
- Evidence anchors:
  - [abstract]: "Large language models (LLMs) show strong in-context learning (ICL) capabilities, but it is unclear whether they can learn online from their own predictions and external rewards, as in contextual bandit reinforcement learning (RL)."
  - [section]: "In this study we focus on a contextual bandit RL scenario, a restricted variant of RL, where the length of each episode is one step."
  - [corpus]: Weak evidence - the corpus contains papers about in-context RL but lacks direct mechanistic evidence for this specific claim.

### Mechanism 2
- Claim: Stochastic context sampling improves exploration and prevents performance collapse from negative rewards.
- Mechanism: Instead of including all positive episodes in the context, Stochastic ICRL randomly samples a subset of past episodes to include for each new input. This introduces variability that encourages exploration of different prediction strategies and prevents the model from being overwhelmed by negative feedback.
- Core assumption: The model's sensitivity to prompt composition can be leveraged to encourage exploration rather than being viewed purely as a weakness.
- Evidence anchors:
  - [abstract]: "Stochastic ICRL is more stable than Naive+, avoiding performance dips caused by negative feedback."
  - [section]: "Stochastic introduces context stochasticity by randomly choosing the subset of past episodes to include in the prompt for each new input."
  - [corpus]: No direct evidence in corpus - this is a novel mechanism proposed in the paper.

### Mechanism 3
- Claim: Larger models show better ICRL performance and stability due to increased capacity for implicit reasoning.
- Mechanism: As model size increases, the model's ability to implicitly learn from rewards and maintain stable performance improves, showing scaling trends similar to other LLM capabilities.
- Core assumption: Model scale correlates with the ability to perform implicit reasoning and learning from contextual feedback.
- Evidence anchors:
  - [abstract]: "Larger models show better ICRL performance and stability."
  - [section]: "There is a strong correlation between performance and model scale, but across all scales the relation between methods is maintained, with regard to both performance and stability."
  - [corpus]: Weak evidence - the corpus mentions scaling but not specifically for ICRL.

## Foundational Learning

- Concept: Contextual bandit reinforcement learning
  - Why needed here: ICRL operates in the contextual bandit setting where each interaction is a single step with reward feedback
  - Quick check question: What distinguishes contextual bandit RL from standard RL in terms of episode length?

- Concept: In-context learning without parameter updates
  - Why needed here: ICRL relies on modifying the context to improve performance rather than updating model weights
  - Quick check question: How does ICL differ from traditional supervised learning in terms of model adaptation?

- Concept: Reward signal interpretation
  - Why needed here: The model must understand and respond to reward feedback to improve its predictions
  - Quick check question: What form does the reward take in the experiments, and how is it presented to the model?

## Architecture Onboarding

- Component map: Input preprocessor -> LLM inference -> Reward evaluator -> Context manager -> Output postprocessor
- Critical path: Context construction -> Model inference -> Reward generation -> Context update
- Design tradeoffs: Exploration vs exploitation balance, context window utilization, computational cost vs learning effectiveness
- Failure signatures: Performance plateaus, context window saturation, model ignoring reward signals, instability in learning curves
- First 3 experiments:
  1. Test zero-shot performance on a classification task to establish baseline
  2. Implement Naive ICRL and measure learning capability
  3. Implement Naive+ ICRL with positive rewards only and compare performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs truly learn to map inputs to abstract labels without any prior exemplars, or is there residual semantic understanding from pre-training?
- Basis in paper: [inferred] from the "Label Semantics Contribute, but ICRL Occurs Without It" section showing that while models can learn abstract labels, the performance is lower and the mechanism remains unclear.
- Why unresolved: The paper shows learning occurs but doesn't definitively distinguish between genuine mapping learning versus exploiting pre-training biases toward label patterns.
- What evidence would resolve it: Experiments testing abstract label learning on tasks completely outside pre-training distribution (e.g., artificial synthetic labels) would determine if learning is task-specific or exploiting learned patterns.

### Open Question 2
- Question: What is the fundamental reason for the instability in Naive+ ICRL, and can it be predicted or mitigated?
- Basis in paper: [explicit] from the "Stochastic is More Stable Than Naive+" section showing sudden performance dips in Naive+ that recover, with no clear explanation of the mechanism.
- Why unresolved: The paper identifies the phenomenon but doesn't explain why negative episodes cause such dramatic effects or how to predict when they'll occur.
- What evidence would resolve it: Analysis of the internal representations during negative episode incorporation versus stable periods, potentially using mechanistic interpretability techniques.

### Open Question 3
- Question: How does ICRL scale to multi-step reinforcement learning tasks with delayed rewards and state transitions?
- Basis in paper: [explicit] from the "Discussion and Limitations" section noting the focus on contextual bandit setting and leaving multi-step RL as an important open direction.
- Why unresolved: The paper establishes capabilities in single-step bandit scenarios but acknowledges this is a restricted setting that doesn't capture the complexity of real-world RL problems.
- What evidence would resolve it: Experiments with sequential decision-making tasks where rewards are only available after multiple steps, testing whether the in-context learning mechanisms transfer to this setting.

## Limitations

- The study focuses on single-step contextual bandit scenarios, leaving multi-step RL with delayed rewards as an open question
- The mechanism behind Naive+ ICRL instability is not fully understood, with performance dips occurring unpredictably
- Results may be specific to classification tasks with immediate binary rewards, limiting generalizability to more complex RL applications

## Confidence

- **High confidence**: LLMs can improve classification performance through context-based learning methods
- **Medium confidence**: The specific mechanisms (Naive+, Stochastic) provide statistically significant improvements over baselines
- **Low confidence**: The improvements represent true reinforcement learning behavior versus sophisticated pattern matching in context

## Next Checks

1. **Mechanism isolation test**: Run ablation studies removing reward signals from context to determine if performance gains persist when only successful predictions (without explicit reward information) are included.

2. **Generalization assessment**: Evaluate model performance on held-out task distributions not seen during training to determine if learned behaviors transfer beyond the specific training context.

3. **Scaling behavior validation**: Conduct systematic experiments across multiple model scales with controlled hyperparameters to verify the claimed relationship between model size and ICRL effectiveness, including statistical significance testing of performance differences.