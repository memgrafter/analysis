---
ver: rpa2
title: 'ChatSR: Multimodal Large Language Models for Scientific Formula Discovery'
arxiv_id: '2406.05410'
source_url: https://arxiv.org/abs/2406.05410
tags:
- data
- expression
- language
- symbolic
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChatSR is a multimodal large language model for conversational
  symbolic regression, allowing users to generate mathematical formulas via natural
  language instructions. It treats observations as one modality and natural language
  queries as another, aligning them through a projection layer into the LLM space.
---

# ChatSR: Multimodal Large Language Models for Scientific Formula Discovery

## Quick Facts
- arXiv ID: 2406.05410
- Source URL: https://arxiv.org/abs/2406.05410
- Authors: Yanjie Li; Lina Yu; Weijun Li; Min Wu; Jingyi Liu; Wenqiang Li; Shu Wei; Yusong Deng
- Reference count: 15
- Key outcome: ChatSR achieves state-of-the-art R² performance on the Nguyen dataset for conversational symbolic regression

## Executive Summary
ChatSR introduces a multimodal large language model for symbolic regression that generates mathematical formulas from natural language instructions and observed data. The system treats observations and natural language queries as separate modalities, aligning them through a projection layer into the LLM embedding space. By freezing a SetTransformer for data encoding and fine-tuning the LLM with LoRA on synthetic data, ChatSR demonstrates superior performance on benchmark datasets while incorporating prior knowledge from instructions to improve expression quality.

## Method Summary
ChatSR uses a two-stage training approach where observations are encoded by a frozen SetTransformer, projected into LLM embedding space, and combined with natural language instructions for formula generation. The model is trained on 15M synthetic data pairs using LoRA fine-tuning, with performance evaluated on the Nguyen dataset. A key innovation is the ability to incorporate prior knowledge constraints from natural language instructions during inference, enabling better zero-shot understanding of unseen constraints.

## Key Results
- Achieves state-of-the-art R² performance compared to MMSR, SPL, NeSymReS, and SNIP on the Nguyen dataset
- Successfully incorporates prior knowledge from natural language instructions to improve expression quality
- Demonstrates strong zero-shot capability for understanding constraints not present in training data
- Shows effective alignment between numerical observations and text embeddings through the projection layer

## Why This Works (Mechanism)

### Mechanism 1
ChatSR enables natural language-guided symbolic regression by aligning multimodal data representations with LLM token embeddings. SetTransformer encodes numerical observations into a fixed-dimensional latent space; a projection layer maps these features to the LLM's embedding space; the LLM then generates symbolic expressions conditioned on both the encoded data and natural language instructions.

### Mechanism 2
ChatSR improves expression quality by incorporating prior knowledge specified in natural language instructions during inference. Instructions containing domain knowledge (e.g., "periodicity," "symmetry," "exclude exp") are concatenated with data features as contextual input to the LLM, biasing the generation process toward expressions matching the desired properties.

### Mechanism 3
ChatSR exhibits strong zero-shot understanding of unseen constraints due to the generalization capacity of the multimodal alignment and instruction-tuning strategy. By training on synthetic data pairs with diverse instruction patterns and leveraging the generalization power of the LLM, ChatSR can interpret and apply novel constraints not present during training.

## Foundational Learning

- **SetTransformer architecture for permutation-invariant encoding of tabular data**
  - Why needed here: Symbolic regression inputs are unordered sets of (X, y) pairs; permutation invariance ensures model robustness to input ordering
  - Quick check question: If we shuffle the rows of the input data matrix, will the SetTransformer output change?

- **Contrastive learning for aligning multimodal embeddings**
  - Why needed here: Ensures that semantically related data and text embeddings are closer in the joint space, improving cross-modal understanding
  - Quick check question: What loss function is used to enforce alignment between data and text embeddings in the projection layer?

- **LoRA (Low-Rank Adaptation) for efficient LLM fine-tuning**
  - Why needed here: Enables adaptation of large language models without full parameter updates, reducing computational cost while maintaining performance
  - Quick check question: How does LoRA modify the weight update process compared to standard fine-tuning?

## Architecture Onboarding

- **Component map**: Observations -> SetTransformer -> Projection Layer -> LLM Backbone -> Symbolic Expression
- **Critical path**:
  1. Encode numerical observations using SetTransformer
  2. Project encoded features into LLM embedding space
  3. Concatenate projected features with natural language instruction
  4. Generate symbolic expression via LLM
  5. Optimize constants using BFGS if placeholders present
- **Design tradeoffs**:
  - Freezing SetTransformer ensures consistent data encoding but limits adaptation to domain-specific patterns
  - Using LoRA reduces memory and compute but may limit adaptation depth
  - Two-stage training (alignment then end-to-end) stabilizes training but increases complexity
- **Failure signatures**:
  - Poor R² values indicate misalignment between data and text modalities
  - Incorrect symbolic forms suggest instruction misinterpretation or insufficient constraint representation
  - Overfitting to synthetic data manifests as poor performance on real-world datasets
- **First 3 experiments**:
  1. Verify SetTransformer produces consistent embeddings for permuted input data
  2. Test projection layer alignment by checking cosine similarity between matched data-text pairs
  3. Validate instruction interpretation by prompting with simple constraints and inspecting generated expressions

## Open Questions the Paper Calls Out

### Open Question 1
How does ChatSR perform on datasets with noise levels significantly higher than those used in the Nguyen dataset experiments? The paper mentions that ChatSR has "good zero-shot capability" and demonstrates robustness, but specific tests with high noise levels are not detailed.

### Open Question 2
Can ChatSR effectively handle symbolic regression tasks involving more complex mathematical functions beyond those tested in the Nguyen dataset? The paper discusses ChatSR's performance on the Nguyen dataset but does not explore its capability with more complex functions.

### Open Question 3
What is the impact of the number and type of prior knowledge instructions on ChatSR's performance in generating accurate expressions? The paper shows that adding prior knowledge improves expression quality but does not explore the limits or optimal types of instructions.

## Limitations

- The alignment architecture between numerical data features and LLM embeddings is underspecified
- Zero-shot capability claims lack empirical support and quantitative analysis
- Synthetic data generation process is described but not validated for real-world complexity coverage
- Missing implementation details for SetTransformer architecture and LoRA adaptation strategy

## Confidence

**High Confidence**: The basic two-stage training procedure (feature alignment followed by end-to-end fine-tuning) is standard practice in multimodal learning and well-supported by the text.

**Medium Confidence**: The mechanism of incorporating natural language constraints into symbolic generation is plausible given existing instruction-tuning literature, but the specific implementation details and effectiveness measures are not rigorously validated.

**Low Confidence**: The zero-shot understanding capability claims lack empirical support - we have no data showing performance on constraints not present in training data.

## Next Checks

1. **Constraint Generalization Test**: Systematically evaluate ChatSR on a held-out set of constraints (e.g., mathematical properties like "bounded," "odd function," "rational coefficients") that were never seen during training, measuring both success rate and generation quality compared to seen constraints.

2. **Data-Synthetic Alignment Verification**: Create a diagnostic experiment where synthetic data pairs are deliberately designed to test edge cases (very high-dimensional inputs, extreme noise levels, complex nested expressions) and measure whether the model's performance degrades predictably, indicating whether the synthetic data truly captures real-world complexity.

3. **Architectural Ablation Study**: Implement and test variations of the alignment architecture (different projection layer sizes, contrastive vs non-contrastive training, frozen vs fine-tuned SetTransformer) to determine which components are essential for performance and quantify the contribution of each architectural choice to the final R² scores.