---
ver: rpa2
title: 'Just read twice: closing the recall gap for recurrent language models'
arxiv_id: '2407.05483'
source_url: https://arxiv.org/abs/2407.05483
tags:
- attention
- input
- recurrent
- tokens
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method to improve recall for recurrent language
  models by showing the input twice in context or using non-causal processing. The
  core idea is that the order in which data is presented affects the difficulty of
  recall, formalized via the set disjointness problem.
---

# Just read twice: closing the recall gap for recurrent language models

## Quick Facts
- arXiv ID: 2407.05483
- Source URL: https://arxiv.org/abs/2407.05483
- Reference count: 40
- 11.0 ± 1.3 points improvement on recall tasks averaged across 16 recurrent models and 6 ICL tasks

## Executive Summary
This paper addresses the recall gap in recurrent language models by showing that data order significantly impacts their ability to remember information. The authors introduce two approaches: JRT-Prompt, which repeats context twice, and JRT-RNN, a non-causal encoder-decoder architecture with prefix linear attention. Both methods leverage the insight that non-causal processing can solve the set disjointness problem more efficiently regardless of data order. Experimental results show substantial improvements in recall performance across multiple benchmarks while maintaining efficiency advantages over transformer-based approaches.

## Method Summary
The paper proposes two methods to improve recall in recurrent language models: JRT-Prompt, which simply repeats the input context twice in context, and JRT-RNN, an encoder-decoder architecture that processes the prefix non-causally using prefix linear attention while maintaining causal decoding for the suffix. The core insight is that recurrent models struggle with recall when the order of information makes it difficult to decide what to store in limited memory. By either repeating context or using non-causal processing, the model can better select what information to retain. JRT-RNN uses separate key/value projections for encoder and decoder regions, enabling efficient non-causal processing of the prefix while maintaining O(1) time and space complexity for inference.

## Key Results
- JRT-Prompt achieves 11.0 ± 1.3 points improvement on recall tasks averaged across 16 recurrent models and 6 ICL tasks
- JRT-RNN provides 99% of Transformer quality at 360M params, 30B tokens, and 96% at 1.3B params, 50B tokens
- JRT-RNN achieves 19.2× higher throughput for prefill than FlashAttention-2

## Why This Works (Mechanism)

### Mechanism 1
The order in which information appears in context drastically impacts the difficulty of predicting what to store in limited memory. Recurrent models need to store all elements in the first set (A) to compare against elements of the second set (B) in set disjointness. If the smaller set appears first, the memory requirement is reduced. The hardness of information recall reduces to the hardness of set disjointness.

### Mechanism 2
Repeating context (JRT-Prompt) or using non-causal processing (JRT-RNN) mitigates reliance on data order. By showing the model all data orders (through repetition) or processing prompts non-causally, the model can condition on the full context when deciding what to store. Non-causal models can solve set disjointness in space min(|A|, |B|) regardless of data order.

### Mechanism 3
Prefix Linear Attention (PLA) in JRT-RNN provides 19.2× higher throughput than FlashAttention-2. PLA uses separate key/value projections for encoder and decoder regions, allowing non-causal processing of the prefix and causal decoding of the suffix, while maintaining O(1) time and space complexity for inference. Linear attention is more efficient than softmax attention during inference.

## Foundational Learning

- Concept: Set disjointness problem in communication complexity
  - Why needed here: Formalizes the hardness of recall as a function of data order
  - Quick check question: Can you explain why a streaming algorithm needs to store all elements of the first set to solve set disjointness?

- Concept: Recurrent models and state size
  - Why needed here: Relates memory consumption to recall ability
  - Quick check question: How does the state size of a recurrent model affect its ability to remember information from long contexts?

- Concept: Linear attention vs softmax attention
  - Why needed here: Explains the efficiency gains of JRT-RNN
  - Quick check question: What is the time and space complexity of linear attention during inference compared to softmax attention?

## Architecture Onboarding

- Component map:
  - JRT-Prompt: Input repetition mechanism
  - JRT-RNN: Encoder-decoder architecture with PLA
  - PLA: Prefix Linear Attention with separate key/value projections
  - Based architecture: Baseline recurrent model

- Critical path:
  1. Implement JRT-Prompt by duplicating input context
  2. Implement PLA by modifying linear attention layers
  3. Train JRT-RNN with combined NTP and MLM objectives
  4. Benchmark against FlashAttention-2 for efficiency

- Design tradeoffs:
  - JRT-Prompt increases context length but remains more efficient than attention
  - JRT-RNN requires separate key/value projections, increasing parameters
  - PLA may have slightly higher FLOPS than standard linear attention

- Failure signatures:
  - JRT-Prompt: No improvement in recall, increased repetition errors
  - JRT-RNN: Quality degradation, lower throughput than expected
  - PLA: Memory issues, incorrect implementation of non-causal prefix processing

- First 3 experiments:
  1. Implement JRT-Prompt and evaluate on a simple recall task
  2. Implement PLA and compare throughput against standard linear attention
  3. Train JRT-RNN on a small dataset and evaluate on SuperGLUE benchmark

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of JRT-RNN scale with sequence length beyond the encoder region length (M)? The paper evaluates JRT-RNN at encoder lengths of 1024 but mentions finding benefits extending beyond this length in Section 5.1. However, detailed analysis of scaling behavior is not provided. Experiments varying encoder length M and evaluating quality on tasks with much longer sequences (e.g., 4k, 8k tokens) would clarify how well JRT-RNN maintains quality improvements with increasing sequence length.

### Open Question 2
What is the optimal strategy for handling prefill lengths shorter than the encoder region length (l < M)? Section 4.3 discusses three strategies: left-padding, read-twice padding, and iterative encoding, but does not provide a definitive recommendation. A comprehensive ablation study comparing these strategies across diverse tasks and measuring both quality and efficiency metrics would identify the most effective approach for different scenarios.

### Open Question 3
How does JRT-RNN perform on tasks requiring bidirectional context, such as coreference resolution or document summarization? While JRT-RNN processes the encoder region non-causally, the paper does not provide evidence of its effectiveness on tasks that benefit most from bidirectional context. Evaluating JRT-RNN on standard benchmarks for bidirectional context understanding (e.g., coreference resolution, abstractive summarization) would demonstrate its capabilities beyond recall tasks.

## Limitations

- The paper relies on custom CUDA kernels for JRT-RNN, whose implementation details are not fully disclosed, making independent verification challenging
- The efficiency claims, particularly the 19.2× throughput improvement over FlashAttention-2, depend on custom implementations that may not generalize across different hardware configurations
- The theoretical analysis focuses primarily on the set disjointness problem as a proxy for recall difficulty, but real-world recall tasks may involve more complex dependencies

## Confidence

**High Confidence**: The core theoretical insight that data order affects recall difficulty in recurrent models, supported by the set disjointness reduction. The experimental results showing JRT-Prompt's effectiveness across multiple recurrent models and tasks are robust and well-documented.

**Medium Confidence**: The JRT-RNN architecture's quality claims (99% at 360M params, 30B tokens) are based on pretraining experiments that show promise but haven't been fully validated on the target recall tasks. The throughput comparisons depend on specific implementations that may not be reproducible.

**Low Confidence**: The efficiency gains claimed for Prefix Linear Attention over FlashAttention-2, as these rely heavily on custom CUDA implementations that aren't publicly available for independent verification.

## Next Checks

1. Reproduce JRT-Prompt Results: Implement JRT-Prompt using standard attention mechanisms on at least two different recurrent architectures and validate the ~11-point improvement claim on the same 6 ICL tasks.

2. Verify Set Disjointness Claims: Implement the theoretical analysis of set disjointness hardness in practice, measuring memory requirements for causal vs non-causal processing across different data orders to confirm the space complexity claims.

3. Benchmark Efficiency Claims: Reproduce the throughput measurements of JRT-RNN against FlashAttention-2 using publicly available linear attention implementations, isolating the contribution of PLA from custom CUDA optimizations.