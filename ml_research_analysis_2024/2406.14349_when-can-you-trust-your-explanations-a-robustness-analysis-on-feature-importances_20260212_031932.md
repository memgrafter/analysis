---
ver: rpa2
title: When Can You Trust Your Explanations? A Robustness Analysis on Feature Importances
arxiv_id: '2406.14349'
source_url: https://arxiv.org/abs/2406.14349
tags:
- robustness
- explanations
- feature
- neighbourhood
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the critical issue of explanation robustness
  in XAI for neural networks, focusing on ensuring trustworthy explanations for tabular
  data. The authors propose a novel framework to evaluate explanation robustness to
  non-adversarial perturbations by leveraging the manifold hypothesis to generate
  on-manifold perturbed datapoints.
---

# When Can You Trust Your Explanations? A Robustness Analysis on Feature Importances

## Quick Facts
- arXiv ID: 2406.14349
- Source URL: https://arxiv.org/abs/2406.14349
- Authors: Ilaria Vascotto; Alex Rodriguez; Alessandro Bonaita; Luca Bortolussi
- Reference count: 40
- Primary result: Proposed framework achieves AUC up to 0.84 for explanation robustness evaluation

## Executive Summary
This paper addresses the critical issue of explanation robustness in XAI for neural networks, focusing on ensuring trustworthy explanations for tabular data. The authors propose a novel framework to evaluate explanation robustness to non-adversarial perturbations by leveraging the manifold hypothesis to generate on-manifold perturbed datapoints. They introduce a novel ensemble method to aggregate multiple explanations based on feature rankings, along with a robustness estimator using Spearman's rho correlation coefficient. The framework includes a validation approach using multiple models to assess robustness and identify trustworthy explanations. Experimental results on eight datasets show that their ensemble method produces more robust explanations than individual approaches, with AUC values up to 0.84 for some datasets.

## Method Summary
The framework evaluates explanation robustness by generating on-manifold perturbations using k-medoid clustering and computing feature attributions with DeepLIFT, Integrated Gradients, and LRP. An ensemble aggregation method combines multiple explanations using ranking-based weighting, while robustness is estimated using Spearman's rho correlation between original and perturbed explanations. A KNN regressor identifies uncertain datapoints requiring human evaluation. The approach uses three different neural network architectures per dataset for validation, generating neighborhoods and computing robustness scores across all models.

## Key Results
- Ensemble method produces more robust explanations than individual approaches (AUC up to 0.84)
- Framework identifies "uncertain" datapoints that require careful human evaluation
- On-manifold perturbations generate more robust explanations than off-manifold perturbations
- Medoid-based clustering effectively captures data manifold for perturbation generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-adversarial perturbations on-manifold produce more robust explanations than off-manifold perturbations.
- Mechanism: By generating perturbations within the data manifold learned by the model, the framework ensures that explanations remain consistent when the input is modified slightly while preserving the same class prediction.
- Core assumption: The data manifold hypothesis holds, meaning real-world data lies on a lower-dimensional manifold within the high-dimensional feature space.
- Evidence anchors:
  - [abstract] "leveraging the manifold hypothesis to produce new perturbed datapoints that resemble the observed data distribution"
  - [section] "non-adversarial perturbations consistent with the observed data distribution"
- Break condition: If the data manifold hypothesis doesn't hold for a given dataset, or if the model doesn't learn an accurate representation of the data manifold.

### Mechanism 2
- Claim: Ensemble aggregation of multiple explanation methods produces more robust explanations than individual methods.
- Mechanism: By combining feature rankings from multiple explanation methods and applying a weighting scheme that penalizes disagreement, the ensemble approach creates a more stable explanation that is less sensitive to the limitations of individual methods.
- Core assumption: Different explanation methods capture complementary aspects of model behavior, and their combination provides a more complete and stable picture.
- Evidence anchors:
  - [abstract] "ensemble method to aggregate various explanations, showing how merging explanations can be beneficial for both understanding the model's decision and evaluating the robustness"
- Break condition: If the explanation methods being aggregated are highly correlated or if they all fail to capture the same underlying patterns in the data.

### Mechanism 3
- Claim: KNN regression on robustness scores can identify uncertain datapoints that require careful human evaluation.
- Mechanism: By fitting a KNN regressor on validation set robustness scores, the framework can predict whether a new datapoint lies in a robust or uncertain region of the feature space, even if its individual robustness score appears high.
- Core assumption: Robustness scores are locally consistent - points near each other in feature space tend to have similar robustness scores.
- Evidence anchors:
  - [abstract] "identify 'uncertain' datapoints that require careful human evaluation"
- Break condition: If the feature space has complex topology with non-local relationships between robustness scores, or if the validation set doesn't adequately represent the test set distribution.

## Foundational Learning

- Concept: Manifold hypothesis and its implications for data distribution
  - Why needed here: The framework relies on generating on-manifold perturbations to test robustness, which requires understanding that real-world data lies on lower-dimensional manifolds within high-dimensional space
  - Quick check question: If a dataset has 100 features but the intrinsic dimensionality is 5, what does this imply about the data distribution and how should perturbations be generated?

- Concept: Spearman's rank correlation coefficient and its properties
  - Why needed here: The robustness estimator uses Spearman's rho to measure consistency between explanations, requiring understanding of rank-based correlation metrics
  - Quick check question: Why might Spearman's rho be preferred over Pearson correlation when comparing feature importance rankings?

- Concept: Ensemble methods and aggregation strategies
  - Why needed here: The framework combines multiple explanation methods using a weighted ensemble approach, requiring understanding of different aggregation strategies and their tradeoffs
  - Quick check question: What are the advantages and disadvantages of weighted averaging versus simple averaging when aggregating feature importance rankings?

## Architecture Onboarding

- Component map: Data preprocessing -> Neural network training (3 models) -> Medoid clustering -> Neighborhood generation -> Explanation computation -> Ensemble aggregation -> Robustness estimation -> KNN uncertainty detection -> Validation

- Critical path:
  1. Preprocess data and train three neural network models
  2. Perform medoid clustering and compute k-nearest neighbors
  3. Generate neighborhoods and compute individual explanation methods
  4. Create ensemble aggregation and compute robustness scores
  5. Train KNN regressor on validation set robustness scores
  6. Evaluate test set using both robustness and KNN predictions

- Design tradeoffs:
  - Neighborhood generation: Random vs. medoid-based (on-manifold vs. off-manifold)
  - Ensemble aggregation: Ranking-based weighted average vs. simple mean
  - Threshold selection: Fixed vs. dataset-specific for robustness classification
  - Model validation: Three models vs. single model for agreement analysis

- Failure signatures:
  - Low AUC values in ROC analysis indicate poor robustness estimation
  - High percentage of uncertain points suggests the framework may be too conservative
  - Disagreement between models concentrated in specific regions may indicate data quality issues
  - Robustness scores not correlating with prediction stability suggests implementation errors

- First 3 experiments:
  1. Compare random vs. medoid-based neighborhood generation on a simple dataset (e.g., Swiss roll) to visualize on-manifold vs. off-manifold perturbations
  2. Test ensemble aggregation on a dataset with known feature importance patterns to verify it correctly identifies important features while handling disagreement
  3. Evaluate the KNN uncertainty detection by creating synthetic uncertain regions in a dataset and verifying they are correctly flagged

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can explanation robustness metrics be standardized across different types of machine learning models (e.g., neural networks vs. tree-based models)?
- Basis in paper: [explicit] The authors mention that their approach is "agnostic in nature with respect to both the investigated model and the XAI techniques being applied" and propose future work to generalize the proposal to different classes of machine learning models.
- Why unresolved: The paper focuses on neural networks and does not provide a comprehensive framework for other model types. Different models may require different robustness estimation approaches.
- What evidence would resolve it: A comparative study testing the proposed framework across multiple model types (neural networks, tree-based models, etc.) with consistent evaluation metrics would demonstrate generalizability.

### Open Question 2
- Question: How does explanation robustness relate to adversarial attack resilience, and can robustness metrics detect adversarial attacks?
- Basis in paper: [explicit] The authors propose investigating "how our explanations could be used to increase the robustness of classifiers" and assessing "whether our robustness estimator is able to detect attacks."
- Why unresolved: The paper focuses on non-adversarial perturbations and does not explore the relationship between explanation robustness and adversarial attack resilience.
- What evidence would resolve it: Empirical studies testing the proposed framework against various adversarial attack methods and measuring detection rates would establish this relationship.

### Open Question 3
- Question: What is the optimal threshold for determining explanation trustworthiness, and how does it vary across datasets and domains?
- Basis in paper: [explicit] The authors acknowledge that "the selection of the threshold rth is a delicate step of the procedure" and use a default value of rth = 0.80, but note it requires careful selection.
- Why unresolved: The paper uses a heuristic approach to threshold selection without establishing optimal values or domain-specific considerations.
- What evidence would resolve it: Systematic analysis across diverse datasets and domains to determine optimal threshold values and their variations would provide guidance for practitioners.

## Limitations
- Framework effectiveness depends heavily on the validity of the manifold hypothesis for each dataset
- Ensemble approach assumes that combining multiple explanation methods provides more robust insights, but this may not be true if all methods share similar failure modes
- KNN-based uncertainty detection assumes local consistency of robustness scores, which may not hold in complex feature spaces

## Confidence
- Mechanism 1 (on-manifold perturbations): Medium - supported by theoretical grounding but limited empirical validation
- Mechanism 2 (ensemble aggregation): High - well-established practice with clear benefits demonstrated
- Mechanism 3 (KNN uncertainty detection): Medium - novel approach but assumptions about local consistency need further testing

## Next Checks
1. Test framework on datasets with known non-manifold structure to verify robustness to manifold hypothesis violations
2. Compare ensemble performance against individual methods on datasets with ground truth feature importance to validate aggregation effectiveness
3. Analyze the relationship between KNN uncertainty predictions and actual human evaluation difficulty on a subset of datapoints