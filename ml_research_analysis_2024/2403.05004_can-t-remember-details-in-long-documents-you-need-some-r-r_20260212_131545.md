---
ver: rpa2
title: Can't Remember Details in Long Documents? You Need Some R&R
arxiv_id: '2403.05004'
source_url: https://arxiv.org/abs/2403.05004
tags:
- document
- context
- question
- instructions
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of long-context large language
  models (LLMs) missing important information in the middle of context documents,
  a phenomenon known as the "lost in the middle" effect. To alleviate this, the authors
  introduce R&R, a combination of two novel prompt-based methods: reprompting and
  in-context retrieval (ICR).'
---

# Can't Remember Details in Long Documents? You Need Some R&R

## Quick Facts
- **arXiv ID**: 2403.05004
- **Source URL**: https://arxiv.org/abs/2403.05004
- **Reference count**: 27
- **Key outcome**: R&R method boosts QA accuracy by 16 points on average for long documents up to 80k tokens

## Executive Summary
This paper addresses the "lost in the middle" problem where large language models (LLMs) miss important information in the middle of long context documents. The authors introduce R&R, a prompt-based method combining reprompting (repeating task instructions periodically) and in-context retrieval (ICR - identifying relevant passages before answering). Tested on documents up to 80k tokens with GPT-4 Turbo and Claude-2.1, R&R achieves significant accuracy improvements over direct long-context QA while enabling larger chunks that reduce computational costs compared to chunkwise methods.

## Method Summary
R&R is a two-phase approach that addresses the "lost in the middle" effect in long-context LLMs. First, it uses in-context retrieval (ICR) to identify the top k most relevant passages from the document. Then, it performs question-answering on an abbreviated context containing only those retrieved passages. The method includes reprompting, where task instructions are repeated periodically throughout the document to remind the LLM of its task. The authors test this approach on four datasets (NaturalQuestions-Open, SQuAD, HotPotQA, and PubMed) with documents of varying lengths (10k-80k tokens) and chunk sizes.

## Key Results
- R&R improves QA accuracy by 16 points on average compared to direct long-context QA
- Enables use of larger chunks (reducing LLM calls) while minimizing accuracy drop compared to short-context chunkwise methods
- Optimal reprompting frequency found to be every 10k tokens across all tested datasets
- ICR alone shows higher accuracy than direct QA, supporting the hypothesis that retrieval is a simpler task

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Repeating prompt instructions periodically reduces positional distance between relevant context and task instructions
- **Mechanism**: When relevant context is buried deep in the document, the LLM may "forget" the original task. By repeating instructions near relevant content, the model is reminded of its task before processing that content
- **Core assumption**: The "lost in the middle" effect occurs partly because of increased distance between task instructions and relevant context
- **Evidence anchors**: Abstract states reprompting "reminds the LLM of its original task"; authors observe that instructions appear before and after context documents in lost-in-middle experiments
- **Break condition**: If LLM's attention mechanism doesn't prioritize repeated tokens or if repetition creates contextual noise

### Mechanism 2
- **Claim**: Retrieval is a simpler task than direct QA because it prioritizes recall over precision
- **Mechanism**: Instead of answering directly from long documents, we first identify most relevant pages, allowing the model to focus on finding information without synthesizing answers
- **Core assumption**: Complexity of answering questions directly from long contexts exceeds identifying relevant passages
- **Evidence anchors**: Abstract describes ICR as retrieving "top k passage numbers most relevant to the given question"; authors compare page retrieval accuracy vs direct QA accuracy
- **Break condition**: If LLM cannot effectively identify relevant passages or if retrieval introduces significant noise

### Mechanism 3
- **Claim**: Chunkwise approaches with retrieval can match or exceed long-context performance while reducing computational costs
- **Mechanism**: Dividing long documents into manageable chunks and performing retrieval within each chunk achieves similar accuracy to long-context approaches while reducing tokens processed per LLM call
- **Core assumption**: Benefits of processing smaller chunks (more focused retrieval) outweigh costs (more LLM calls and potential context fragmentation)
- **Evidence anchors**: Abstract claims R&R "enables the use of larger chunks that cost fewer LLM calls and output tokens, while minimizing the drop in accuracy"
- **Break condition**: If chunks are too large for effective retrieval or too small to capture necessary context

## Foundational Learning

- **Concept**: Attention mechanisms in transformers
  - **Why needed here**: Understanding how LLMs process long contexts and why they might "forget" information in the middle
  - **Quick check question**: What happens to attention weights as sequence length increases in standard transformers?

- **Concept**: Retrieval-augmented generation (RAG)
  - **Why needed here**: ICR is inspired by RAG but operates within the LLM's context window rather than using external retrieval
  - **Quick check question**: How does RAG differ from traditional generation approaches?

- **Concept**: Context windows and tokenization
  - **Why needed here**: The method operates on token-level boundaries and assumes knowledge of context window limitations
  - **Quick check question**: Why do LLMs process text as tokens rather than characters or words?

## Architecture Onboarding

- **Component map**: Question + Document -> ICR (with optional reprompting) -> Context aggregation -> QA -> Answer with supporting page references

- **Critical path**:
  1. Document preprocessing (pagination/chunking)
  2. Phase 1: ICR with optional reprompting
  3. Context aggregation from retrieved pages
  4. Phase 2: QA on abbreviated context
  5. Answer formatting and output

- **Design tradeoffs**:
  - Reprompt interval (r): Too frequent creates noise, too sparse loses effectiveness
  - Retrieval count (k): More pages increase recall but add noise and cost
  - Chunk size (c): Larger chunks reduce calls but may hurt retrieval precision
  - Token budget: Balance between input tokens (ICRs) and output tokens (answers)

- **Failure signatures**:
  - Low retrieval accuracy: k too small or chunks too large for focused retrieval
  - Noisy answers: Over-retrieval or inappropriate reprompting creating confusion
  - Inconsistent performance: Varying answer positions not well-handled by uniform reprompting
  - High costs: Chunk size too small or k too large relative to document structure

- **First 3 experiments**:
  1. Baseline vs. Reprompt vs. R&R on a single document with known answer position to validate mechanism 1
  2. ICR-only vs. direct QA on documents with scattered vs. concentrated relevant content to validate mechanism 2
  3. Chunkwise ICR with varying chunk sizes on a fixed document to find the optimal accuracy-cost tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the optimal reprompting frequency (r) vary across different document types and tasks?
- **Basis in paper**: [explicit] Authors tested reprompting every 5k, 10k, and 20k tokens and found 10k optimal across all four datasets, but hypothesize this is related to experimental setup where answer positions are multiples of 10k
- **Why unresolved**: Authors only tested one specific setup and did not explore whether different document types, tasks, or answer position distributions might benefit from different reprompting frequencies
- **What evidence would resolve it**: Experiments testing various reprompting frequencies across different document types (e.g., legal, medical, technical) and tasks (e.g., summarization, generation) with varying answer position distributions

### Open Question 2
- **Question**: How do R&R methods compare to other long-context optimization techniques like parallel context windows or landmark attention?
- **Basis in paper**: [explicit] Authors mention several architectural modifications for long context in related work but do not compare their prompt-based approach to these methods
- **Why unresolved**: Paper focuses solely on prompt-based methods and does not benchmark against architectural modifications that could potentially achieve similar or better results
- **What evidence would resolve it**: Direct comparisons of R&R to methods like parallel context windows (Ratner et al., 2023) or landmark attention (Mohtashami and Jaggi, 2023) on the same tasks

### Open Question 3
- **Question**: What is the impact of R&R on tasks requiring holistic document understanding like summarization or reasoning?
- **Basis in paper**: [explicit] Authors suggest this as a future direction, noting that position bias has been observed in summarization tasks
- **Why unresolved**: Paper only tests R&R on extractive question-answering tasks and does not explore its effectiveness on tasks requiring broader document comprehension
- **What evidence would resolve it**: Experiments applying R&R to summarization tasks or reasoning tasks that require understanding relationships across the entire document

### Open Question 4
- **Question**: How does the performance of R&R change when using open-source LLMs versus proprietary ones?
- **Basis in paper**: [inferred] Authors note they used proprietary models (GPT-4 Turbo and Claude-2.1) due to long context capabilities, but acknowledge this limits understanding of mechanisms and introduces potential noise
- **Why unresolved**: Black-box nature of models used prevents detailed analysis of how reprompting affects attention mechanisms and internal processing
- **What evidence would resolve it**: Testing R&R with open-source long-context models where attention weights and internal states can be analyzed

## Limitations
- Evaluation relies on synthetically positioned answers rather than natural document structures
- Method assumes uniform distribution of relevant content, but actual documents often have non-uniform answer distributions
- Effectiveness depends on LLM's attention mechanisms, which are not fully transparent and may vary across model versions

## Confidence
- **High Confidence**: The core observation that direct QA from long contexts suffers from the "lost in the middle" effect is well-established in prior work
- **Medium Confidence**: The hypothesis that ICR is a simpler task than direct QA is supported by comparative experiments
- **Medium Confidence**: The claim that R&R enables larger chunks while maintaining accuracy is demonstrated empirically

## Next Checks
1. **Cross-document structure validation**: Test R&R on real-world documents with known answer clusters (rather than synthetically positioned answers) to verify performance holds when relevant content appears in natural groupings
2. **Attention pattern analysis**: Use attention visualization tools to examine whether repeated prompts actually increase attention weights to relevant middle-context passages
3. **Cost-accuracy Pareto analysis**: Systematically vary reprompting frequency (r) and retrieval count (k) across multiple document types to map the full cost-accuracy tradeoff space