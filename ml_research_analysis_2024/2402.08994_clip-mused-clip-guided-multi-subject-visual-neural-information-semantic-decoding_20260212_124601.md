---
ver: rpa2
title: 'CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding'
arxiv_id: '2402.08994'
source_url: https://arxiv.org/abs/2402.08994
tags:
- subjects
- neural
- stimuli
- visual
- subject
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generalizing single-subject
  visual neural decoding models to multiple subjects due to individual differences
  and limited data per subject. It proposes CLIP-guided Multi-sUbject visual neural
  information SEmantic Decoding (CLIP-MUSED), which employs a Transformer-based feature
  extractor to model global neural representations and learnable subject-specific
  tokens to encode individual differences.
---

# CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding

## Quick Facts
- arXiv ID: 2402.08994
- Source URL: https://arxiv.org/abs/2402.08994
- Authors: Qiongyi Zhou; Changde Du; Shengpei Wang; Huiguang He
- Reference count: 40
- Primary result: Outperforms single-subject methods and achieves state-of-the-art performance among existing multi-subject methods on two fMRI datasets

## Executive Summary
CLIP-MUSED addresses the challenge of generalizing visual neural decoding models across multiple subjects by leveraging CLIP's semantic structure and subject-specific learnable tokens. The method uses a Transformer-based architecture to capture global neural dependencies and employs representational similarity analysis (RSA) to align neural representations with CLIP's topological structure. By separating low-level and high-level processing into distinct tokens per subject, the model effectively handles individual differences while maintaining shared parameters for universal patterns.

## Method Summary
CLIP-MUSED employs a Transformer-based feature extractor that takes preprocessed fMRI BOLD signals and produces global neural representations. The model incorporates learnable subject-specific low-level and high-level tokens to encode individual processing differences. RSA is used to guide token representation learning by comparing representational similarity matrices between neural tokens and CLIP features. The final semantic classification is performed by concatenating low-level and high-level token representations and passing them through an MLP classifier. The method is trained end-to-end with combined classification and RSA guidance losses.

## Key Results
- Achieves state-of-the-art performance on two fMRI datasets (HCP and NSD)
- Shows significant improvements in mean Average Precision (mAP), area under the ROC curve (AUC), and Hamming distance
- Outperforms single-subject decoding methods and existing multi-subject approaches
- Demonstrates effective handling of individual differences through learnable subject-specific tokens

## Why This Works (Mechanism)

### Mechanism 1
The method achieves multi-subject generalization by encoding subject-specific processing patterns into separate learnable tokens (low-level and high-level). Individual differences in processing visual features are separated into two independent tokens per subject, allowing shared parameters to capture universal patterns while tokens adapt to subject-specific idiosyncrasies.

### Mechanism 2
RSA-based guidance aligns neural representations with CLIP's topological structure, improving semantic consistency across subjects. By minimizing the difference between RSMs of neural representations and CLIP features, the method ensures that neural embeddings preserve the semantic relationships encoded in CLIP.

### Mechanism 3
Transformer-based feature extraction captures global neural dependencies better than CNNs or linear methods. Self-attention mechanisms in Transformers enable modeling of long-range functional connectivity between brain regions, which is critical for decoding semantic information.

## Foundational Learning

- **Representational Similarity Analysis (RSA)**: Why needed - RSA provides a way to compare the topological structure of neural representations with the semantic structure of visual stimuli without requiring explicit alignment. Quick check - What is the key difference between RSA-based alignment and direct mapping-based alignment?

- **Self-attention and multi-head self-attention (MHSA)**: Why needed - MHSA allows the model to capture interactions between different brain regions and tokens, enabling the learning of global neural patterns and subject-specific processing differences. Quick check - How does MHSA differ from a standard attention mechanism in terms of capturing relationships between brain regions?

- **Orthogonal regularization**: Why needed - Orthogonal regularization ensures that low-level and high-level tokens encode complementary information rather than redundant features, maximizing the information capacity of the representation. Quick check - Why is it beneficial to impose an orthogonal constraint between low-level and high-level token representations?

## Architecture Onboarding

- **Component map**: BOLD patches → Transformer layers → RSA-guided token representations → classification
- **Critical path**: BOLD patches → Transformer layers → RSA-guided token representations → classification
- **Design tradeoffs**: 
  - Separate low-level and high-level tokens vs. single subject token: Better specialization but requires orthogonal constraint
  - RSA guidance vs. direct mapping: Preserves global structure but requires computing RSMs
  - Transformer vs. CNN: Better global modeling but higher computational cost
- **Failure signatures**:
  - Poor performance despite training: Likely RSA guidance misalignment or insufficient token specialization
  - Slow convergence: Check learning rates and batch sizes for RSM computation
  - Overfitting on small datasets: Increase dropout or reduce model depth
- **First 3 experiments**:
  1. Compare performance with and without RSA guidance on a small subset to verify its contribution
  2. Test the effect of orthogonal constraint by training with and without it
  3. Validate that separate low-level and high-level tokens outperform a single subject token

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of CLIP-MUSED scale with the number of subjects in the training dataset? Is there a point of diminishing returns? The paper doesn't explore performance with varying numbers of subjects, particularly datasets with significantly more subjects than those used.

### Open Question 2
How robust is CLIP-MUSED to variations in the visual stimuli used across subjects? For example, how would the method perform if subjects viewed completely different categories of images? The paper doesn't explore the extreme case where subjects view completely different categories with no overlap.

### Open Question 3
How does the choice of the pre-trained CLIP model affect the performance of CLIP-MUSED? Would using a different pre-trained model or fine-tuning CLIP on the specific dataset improve results? The paper doesn't investigate the impact of using different pre-trained models or fine-tuning CLIP.

## Limitations

- Limited evidence for the necessity of separating low-level and high-level tokens versus using orthogonal regularization alone
- Assumes CLIP's topological structure meaningfully aligns with neural representations without independent validation
- Evaluated only on visual stimuli, leaving open questions about cross-modality generalization

## Confidence

- **High Confidence**: Transformer-based global neural modeling improves semantic decoding performance compared to CNN-based methods
- **Medium Confidence**: RSA-guided representation learning improves semantic consistency across subjects
- **Low Confidence**: The orthogonal constraint between low-level and high-level tokens is necessary and beneficial

## Next Checks

1. **RSA Alignment Validation**: Conduct independent analysis to verify whether CLIP's feature space preserves the same topological relationships as neural representations by comparing RSMs between CLIP features and neural responses for a held-out set of stimuli.

2. **Token Separation Ablation**: Design experiment comparing CLIP-MUSED with separate low-level and high-level tokens versus a variant with a single subject token to isolate whether performance gains come from token separation itself or orthogonal regularization.

3. **Cross-Modality Transfer Test**: Evaluate whether subject-specific tokens learned from visual fMRI data can be transferred to a different modality (e.g., auditory or language tasks) with minimal fine-tuning to test generality beyond the visual domain.