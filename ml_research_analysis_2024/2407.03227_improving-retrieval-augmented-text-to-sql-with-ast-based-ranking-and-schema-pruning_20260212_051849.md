---
ver: rpa2
title: Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema
  Pruning
arxiv_id: '2407.03227'
source_url: https://arxiv.org/abs/2407.03227
tags:
- schema
- selection
- table
- fastratext
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel retrieval-augmented Text-to-SQL framework
  that improves performance by dynamically retrieving relevant database context and
  selecting in-context examples using normalized SQL Abstract Syntax Tree (AST) similarity.
  The approach addresses challenges of commercial database size and deployability
  by introducing schema pruning and AST-based example selection.
---

# Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning

## Quick Facts
- arXiv ID: 2407.03227
- Source URL: https://arxiv.org/abs/2407.03227
- Reference count: 39
- Key result: Achieved 86.6% execution accuracy and 77.3% exact match accuracy on Spider dataset using GPT-4

## Executive Summary
This paper introduces a retrieval-augmented Text-to-SQL framework that addresses challenges of commercial database scale and deployability. The framework dynamically retrieves relevant database context and selects in-context examples using normalized SQL Abstract Syntax Tree (AST) similarity. By combining schema pruning with AST-based example selection, the approach achieves state-of-the-art performance on the Spider benchmark while reducing computational overhead and improving model generalization across languages.

## Method Summary
The proposed framework tackles Text-to-SQL challenges through two key innovations: schema pruning and AST-based example selection. Schema pruning dynamically retrieves relevant schema elements from large commercial databases, achieving over 97% recall while reducing schema elements by approximately 49%. The AST-based example selection uses normalized SQL AST similarity to re-rank examples, improving performance across different approximators. The system combines a sparse retriever with FastRAText semantic parser to efficiently retrieve relevant schema elements and generate SQL queries from natural language questions.

## Key Results
- Achieved 86.6% execution accuracy and 77.3% exact match accuracy on Spider dataset using GPT-4
- Schema pruning achieves over 97% recall while reducing schema elements by approximately 49%
- AST-based example selection consistently improves performance across different approximators
- Outperformed existing baselines on monolingual and cross-lingual benchmarks

## Why This Works (Mechanism)
The framework improves Text-to-SQL performance by addressing two fundamental challenges: database scale and example relevance. Schema pruning reduces the computational burden of processing large commercial databases by focusing only on relevant schema elements, while maintaining high recall. The AST-based example selection ensures that in-context examples are semantically similar to the target query, improving the model's ability to generalize across different question patterns. This combination allows the system to maintain high accuracy while being deployable in commercial settings with large, complex databases.

## Foundational Learning
- SQL AST normalization: Converts SQL queries to normalized abstract syntax trees to measure semantic similarity between queries, needed to identify structurally similar examples for in-context learning
- Sparse retrieval techniques: Uses keyword-based retrieval methods to efficiently find relevant schema elements from large databases, needed to handle commercial database scale
- Schema element relevance scoring: Evaluates which database schema elements are most relevant to a given query, needed to reduce computational overhead while maintaining accuracy
- Cross-lingual Text-to-SQL: Adapts the framework to handle multiple languages, needed to demonstrate generalization beyond monolingual benchmarks

## Architecture Onboarding

Component Map:
Input Question -> Sparse Retriever -> Schema Pruner -> FastRAText Parser -> SQL AST Generator -> AST-based Re-ranker -> Final SQL Output

Critical Path:
Question and database schema enter the system, where the sparse retriever identifies potentially relevant schema elements. The schema pruner then filters these elements based on relevance scoring, passing the reduced schema to FastRAText. The parser generates SQL with AST structure, which the re-ranker uses to select the most semantically similar examples from the training set.

Design Tradeoffs:
- Computational efficiency vs. recall trade-off in schema pruning
- Retrieval accuracy vs. processing speed in sparse vs. dense retrieval
- Model complexity vs. deployability in choosing FastRAText over larger models
- Example diversity vs. semantic similarity in AST-based selection

Failure Signatures:
- Schema pruning may miss relevant elements, leading to incomplete SQL generation
- AST-based selection might prioritize structurally similar but semantically different examples
- Sparse retrieval may fail to capture semantic relationships between schema elements
- Cross-lingual performance may degrade for languages with different query patterns

First Experiments:
1. Test schema pruning on databases of varying sizes to measure recall and efficiency trade-offs
2. Evaluate AST-based example selection on queries with similar structure but different semantics
3. Benchmark cross-lingual performance on languages with different grammatical structures

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily limited to Spider benchmark, which may not represent commercial database complexity
- Framework performance on real-world commercial databases remains unverified
- No ablation studies on individual component contributions or failure case analysis
- Cross-lingual evaluation limited to single benchmark

## Confidence

High - The reported benchmark results are verifiable and methodology is clearly described.

Medium - Claims about commercial database applicability are supported by schema pruning results but lack direct validation on commercial datasets.

Low - Generalization claims to real-world scenarios are based on Spider results without additional empirical support.

## Next Checks

1. Evaluate the framework on additional benchmarks beyond Spider, particularly those with larger or more complex schemas to validate commercial database claims.

2. Conduct ablation studies to quantify individual contributions of schema pruning, AST-based ranking, and example selection to overall performance.

3. Test the framework's robustness on cross-lingual datasets beyond the single benchmark mentioned to verify cross-lingual effectiveness claims.