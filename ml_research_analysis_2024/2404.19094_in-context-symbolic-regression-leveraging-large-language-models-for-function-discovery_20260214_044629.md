---
ver: rpa2
title: 'In-Context Symbolic Regression: Leveraging Large Language Models for Function
  Discovery'
arxiv_id: '2404.19094'
source_url: https://arxiv.org/abs/2404.19094
tags:
- functions
- icsr
- points
- function
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces In-Context Symbolic Regression (ICSR), the
  first framework to leverage large language models (LLMs) for symbolic regression
  (SR). ICSR iteratively refines functional forms with an LLM and determines coefficients
  using an external optimizer.
---

# In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery

## Quick Facts
- **arXiv ID**: 2404.19094
- **Source URL**: https://arxiv.org/abs/2404.19094
- **Reference count**: 40
- **Primary result**: ICSR achieves R2 score of 0.993±0.002 with average complexity 6.7±0.4, outperforming baselines in simplicity and OOD generalization

## Executive Summary
This paper introduces In-Context Symbolic Regression (ICSR), the first framework to leverage large language models for symbolic regression. ICSR iteratively refines functional forms with an LLM while determining coefficients using an external optimizer. The method leverages LLMs' mathematical prior to propose initial functions and refine them based on errors. Experiments on four SR benchmarks show ICSR matches or outperforms state-of-the-art SR baselines in terms of R2 score while producing significantly simpler equations with better out-of-distribution generalization.

## Method Summary
ICSR is an iterative framework that uses LLMs for symbolic regression by generating function skeletons and external optimizers for coefficient fitting. The method starts with seed functions generated by an LLM, then enters an optimization loop where it refines functions based on error scores. The LLM receives in-context examples of previous attempts with their performance metrics, enabling it to learn from past errors. The error function combines normalized mean squared error with a complexity penalty to prevent overfitting and encourage simpler solutions. Only basic operators and expressions are provided to the LLM, with no problem-specific fine-tuning.

## Key Results
- ICSR achieves an average R2 of 0.993±0.002 across benchmarks with complexity of 6.7±0.4
- Best baseline achieves similar R2 (0.993±0.002) but with complexity 20.0±1.2
- Simpler equations from ICSR correlate with stronger out-of-distribution performance
- ICSR successfully discovers correct functional forms for all benchmarks except the R benchmark

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Iterative refinement with in-context examples allows LLMs to propose increasingly accurate symbolic functions
- **Mechanism**: The LLM receives a meta-prompt containing previously attempted functions with their error scores, enabling it to learn from past attempts and generate better candidates
- **Core assumption**: LLMs can effectively extrapolate patterns from in-context examples to improve function proposals
- **Evidence anchors**: [abstract] "ICSR leverages LLMs' strong mathematical prior both to propose an initial set of possible functions given the observations and to refine them based on their errors"
- **Break condition**: If the LLM cannot extract meaningful patterns from error information in the prompt, or if context window limitations prevent sufficient historical data inclusion

### Mechanism 2
- **Claim**: External coefficient optimization enables efficient search over function skeletons
- **Mechanism**: LLMs generate only functional forms (skeletons) while unknown coefficients are fitted using non-linear least squares, allowing grouping of functions into equivalence classes
- **Core assumption**: Separating function form generation from coefficient fitting is more efficient than having LLMs predict both
- **Evidence anchors**: [section 4] "We utilize the LLM only to generate functional forms (skeletons), while the unknown coefficients associated to the predicted functional form are optimized by Non-linear Least Squares (NLS)"
- **Break condition**: If the external optimizer consistently finds poor local minima, or if the LLM-generated skeletons are too restrictive to contain good solutions

### Mechanism 3
- **Claim**: Complexity penalty in error function prevents overfitting and improves out-of-distribution generalization
- **Mechanism**: The error function combines normalized mean squared error with a complexity penalty term that discourages overly complex expressions
- **Core assumption**: Simpler functions with slightly higher training error generalize better than complex functions that perfectly fit training data
- **Evidence anchors**: [section 4] "we adapt from Shojaee et al. (2023) a fitness function r( ˆf |D) with an extra penalty term for the complexity C of the generated expression"
- **Break condition**: If the complexity penalty is too strong, causing underfitting, or if simpler functions consistently underperform complex ones on OOD data

## Foundational Learning

- **Concept**: In-Context Learning (ICL)
  - **Why needed here**: ICSR relies entirely on LLMs' ability to perform tasks based on context provided in input text without additional fine-tuning
  - **Quick check question**: Can you explain how in-context learning differs from fine-tuning and why it's advantageous for SR tasks?

- **Concept**: Genetic Programming basics
  - **Why needed here**: Understanding traditional GP approaches provides context for why ICSR's approach is innovative
  - **Quick check question**: What are the key differences between genetic programming and the ICSR approach?

- **Concept**: Symbolic Regression fundamentals
  - **Why needed here**: Understanding the SR problem space helps appreciate ICSR's contributions and limitations
  - **Quick check question**: What makes symbolic regression more challenging than standard regression, and how does explainability factor into its value?

## Architecture Onboarding

- **Component map**: LLM prompt generator → LLM inference → Non-linear least squares optimizer → Error evaluation → Meta-prompt update → Iteration loop
- **Critical path**: Prompt generation → LLM response → Coefficient fitting → Error calculation → Next iteration decision
- **Design tradeoffs**: LLMs provide strong priors but are expensive; external optimizers are fast but require good function skeletons; complexity penalty trades accuracy for generalization
- **Failure signatures**: Stagnation in error scores across iterations; LLM consistently generating invalid functions; coefficient fitting failing to converge
- **First 3 experiments**:
  1. Run ICSR on Nguyen benchmark with default parameters and verify it produces reasonable R2 scores
  2. Test sensitivity to complexity penalty λ by running with λ=0 and comparing to default
  3. Compare seed-only performance (no optimization loop) against full ICSR to validate the refinement process

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does ICSR's performance scale with increasing input dimensionality beyond two variables?
- **Basis in paper**: [inferred] The paper mentions that "using an LLM for higher dimensional inputs is possible, but dimensionality exacerbates the issues presented above" and discusses challenges with visualizing and processing high-dimensional data.
- **Why unresolved**: The paper only evaluates ICSR on benchmarks with one or two input dimensions, leaving the performance on higher-dimensional problems unexplored.
- **What evidence would resolve it**: Experiments comparing ICSR's performance on SR benchmarks with varying numbers of input variables (e.g., 3, 5, 10) and analyzing how the complexity and accuracy metrics change with dimensionality.

### Open Question 2
- **Question**: Can the proposed fitness function be further optimized to improve out-of-distribution generalization?
- **Basis in paper**: [explicit] The paper discusses the fitness function's components (NMSE and complexity penalty) and suggests potential improvements like penalizing undefined functions more heavily or eliminating small coefficients.
- **Why unresolved**: While the paper demonstrates the current fitness function's effectiveness, it doesn't explore alternative formulations or hyperparameter optimizations that might yield better generalization.
- **What evidence would resolve it**: Comparative experiments testing different fitness function formulations (e.g., varying the complexity penalty weight, incorporating domain restrictions) and measuring their impact on out-of-distribution performance across multiple benchmarks.

### Open Question 3
- **Question**: How would fine-tuning an LLM specifically for symbolic regression affect ICSR's performance compared to using a general-purpose pre-trained model?
- **Basis in paper**: [inferred] The paper contrasts ICSR with other transformer-based methods that are specifically trained for SR, suggesting that pre-training bias might influence performance. It also mentions that "future models will be more amenable to this sort of visual mathematical reasoning."
- **Why unresolved**: The paper uses a general-purpose LLM (Llama 3 8B) without any SR-specific fine-tuning, leaving the potential benefits of specialized training unexplored.
- **What evidence would resolve it**: Experiments comparing ICSR using a pre-trained LLM versus the same model fine-tuned on SR datasets, measuring differences in accuracy, complexity, and computational efficiency across multiple benchmarks.

## Limitations
- Exact prompt templates and hyperparameter configurations for ICSR and baselines are not publicly available
- Results may depend heavily on prompt engineering quality and are not easily reproducible
- Complexity penalty parameter λ=0.05 appears optimal but may not generalize across different problem domains
- Comparison relies on open-source baseline implementations that may not match original performance

## Confidence

- **ICSR's overall performance superiority (R2 ≈ 0.993)**: **High confidence** - Multiple benchmarks and statistical significance testing support this claim
- **Simpler equations leading to better OOD generalization**: **Medium confidence** - The correlation is demonstrated but causation could be influenced by other factors
- **LLM-based refinement mechanism effectiveness**: **Medium confidence** - Supported by results but dependent on prompt engineering quality
- **Complexity penalty benefits**: **Medium confidence** - Demonstrated in experiments but sensitivity to λ parameter suggests fragility

## Next Checks

1. **Prompt template validation**: Test ICSR with multiple prompt variations to determine how sensitive the method is to prompt engineering quality, including different ways of presenting error information and historical attempts to the LLM.

2. **Cross-domain generalization**: Evaluate ICSR on symbolic regression problems outside the standard benchmarks (e.g., physics equations, engineering problems) to assess whether the claimed advantages generalize beyond the tested domains.

3. **Scaling analysis**: Investigate how ICSR performance changes with different LLM sizes (e.g., comparing Llama 3 8B vs 70B) and context window sizes to understand the trade-offs between computational cost and solution quality.