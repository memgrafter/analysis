---
ver: rpa2
title: 'Supporting Online Discussions: Integrating AI Into the adhocracy+ Participation
  Platform To Enhance Deliberation'
arxiv_id: '2409.07780'
source_url: https://arxiv.org/abs/2409.07780
tags:
- comment
- quality
- comments
- module
- stance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study extended the adhocracy+ open-source participation platform
  with two AI-powered debate modules to address common issues in online discussions
  such as low engagement, information overload, and uncivil exchanges. The Comment
  Recommendation Module uses a stance detection model to suggest opposing viewpoints
  to users, promoting reciprocity and engagement.
---

# Supporting Online Discussions: Integrating AI Into the adhocracy+ Participation Platform To Enhance Deliberation

## Quick Facts
- arXiv ID: 2409.07780
- Source URL: https://arxiv.org/abs/2409.07780
- Reference count: 6
- This study extended the adhocracy+ open-source participation platform with two AI-powered debate modules to address common issues in online discussions such as low engagement, information overload, and uncivil exchanges.

## Executive Summary
This study extended the adhocracy+ open-source participation platform with two AI-powered debate modules to address common issues in online discussions such as low engagement, information overload, and uncivil exchanges. The Comment Recommendation Module uses a stance detection model to suggest opposing viewpoints to users, promoting reciprocity and engagement. The Deliberative Quality Module automatically identifies and highlights high-quality comments using the AQuA score, which combines multiple deliberative quality indicators. Both modules were implemented using BERT-based models and synthetic data generation. The extended platform is available on GitHub. A large-scale user study is planned to evaluate the impact of these AI-supported modules on discussion quality and user satisfaction compared to standard discussions.

## Method Summary
The authors implemented two AI-powered debate modules in the Django-based adhocracy+ platform. The Comment Recommendation Module fine-tunes a BERT model for stance detection using the X-Stance dataset and synthetic data generated by Mistral-7B, then recommends opposing viewpoints to users. The Deliberative Quality Module calculates an AQuA score for each comment by combining predictions from 20 fine-tuned BERT adapter models trained on deliberative quality indicators, highlighting top-scoring comments. Both modules are triggered via Django signals whenever new comments are submitted. The system uses PostgreSQL for data storage and provides real-time AI scoring and recommendations.

## Key Results
- Extended adhocracy+ platform with two AI-powered debate modules for online discussions
- Comment Recommendation Module uses stance detection to suggest opposing viewpoints
- Deliberative Quality Module highlights high-quality comments using AQuA score combining 20 deliberative quality indicators
- Both modules implemented using BERT-based models and synthetic data generation
- Extended platform available on GitHub with planned large-scale user study

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Comment Recommendation Module increases reciprocity by exposing users to opposing viewpoints.
- Mechanism: The module uses a stance detection model to classify comments as "in favor" or "against" a debate question, then recommends comments with the opposite stance to the current user.
- Core assumption: Users are more likely to engage with and respond to comments that present opposing viewpoints when they are directly prompted to do so.
- Evidence anchors:
  - [abstract] "The Comment Recommendation Module uses a stance detection model to suggest opposing viewpoints to users, promoting reciprocity and engagement."
  - [section] "The comment recommendation module suggests a comment to the current user contrary to their own position or stance regarding the debate question."
- Break condition: The stance detection model fails to accurately classify comments, leading to irrelevant recommendations that users ignore.

### Mechanism 2
- Claim: The Deliberative Quality Module improves discussion quality by highlighting high-quality comments.
- Mechanism: The module calculates an AQuA score for each comment using a weighted sum of BERT-based adapter models trained on 20 deliberative quality indicators, then highlights the top-scoring comments.
- Core assumption: Users will write higher-quality comments when they see that high-quality contributions are recognized and prominently displayed.
- Evidence anchors:
  - [abstract] "The Deliberative Quality Module automatically identifies and highlights high-quality comments using the AQuA score, which combines multiple deliberative quality indicators."
  - [section] "We predict a deliberative quality score (AQuA score) for each comment. Comments with a high AQuA score are sorted to the top of the discussion and highlighted in bright green and marked as 'top comment'."
- Break condition: The AQuA score threshold is set too high or low, causing either no comments to be highlighted or too many low-quality comments to be featured.

### Mechanism 3
- Claim: Synthetic data generation improves stance detection model performance when labeled data is scarce.
- Mechanism: The system uses Mistral-7B to generate synthetic comments with "in favor" and "against" stances, then fine-tunes the BERT stance detection model on this synthetic data.
- Core assumption: Synthetic comments generated by LLMs are sufficiently realistic to improve model generalization on real-world stance detection tasks.
- Evidence anchors:
  - [section] "Due to the complexity and variety of the debate questions, it is challenging to obtain enough labeled data for stance detection. For that reason we follow Wagner et al. (2024) to generate synthetic data with large language models (LLMs)."
  - [section] "We use Mistral-7B (Jiang et al., 2023) to generate comments with an in favor and against stance."
- Break condition: The synthetic data is too different from real user comments, causing the model to overfit to synthetic patterns that don't generalize.

## Foundational Learning

- Concept: Stance detection in political discourse
  - Why needed here: The comment recommendation module relies on accurately determining whether comments support or oppose the debate topic to provide relevant opposing viewpoints.
  - Quick check question: What are the key challenges in building stance detection models for political discussions compared to other domains?

- Concept: BERT-based adapter models for multi-task learning
  - Why needed here: The AQuA score combines predictions from 20 different adapter models, each fine-tuned for specific deliberative quality indicators.
  - Quick check question: How do adapter models differ from full fine-tuning in terms of parameter efficiency and task specialization?

- Concept: Django signal handling for real-time AI integration
  - Why needed here: The system uses Django signals to trigger AI model inference whenever a new comment is added, enabling real-time scoring and recommendation.
  - Quick check question: What are the potential performance bottlenecks when using Django signals to trigger AI model inference on every comment submission?

## Architecture Onboarding

- Component map:
  - Django-based adhocracy+ platform -> Django signals -> AI inference (stance detection/AQuA scoring) -> PostgreSQL database -> Frontend components (recommendations/highlights)

- Critical path:
  1. User submits comment → Django model saves to database
  2. Django signal triggers AI inference
  3. Comment passed to stance detection or AQuA scoring models
  4. AI scores stored back in database
  5. Frontend updates to show recommendations/highlights

- Design tradeoffs:
  - Local AI model execution vs. cloud API services (latency vs. cost)
  - Real-time scoring vs. batch processing (immediacy vs. resource efficiency)
  - Model complexity vs. inference speed (accuracy vs. user experience)

- Failure signatures:
  - High false positive rate in stance detection → irrelevant comment recommendations
  - Slow AQuA scoring → delayed comment highlighting
  - Database locking issues → comment submission failures
  - Memory leaks in AI model loading → server crashes

- First 3 experiments:
  1. Deploy stance detection model in isolation with test comments to verify accuracy and latency
  2. Test Django signal integration by submitting comments and checking AI score storage
  3. Evaluate AQuA scoring with sample comments to ensure quality score distribution and highlighting logic

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold for the AQuA score that balances highlighting truly high-quality comments without excluding potentially valuable contributions?
- Basis in paper: [explicit] The paper mentions that "To ensure that only high quality comments are highlighted, we threshold the scores. The exact threshold depends on the discussion and can be set as a hyperparameter."
- Why unresolved: The paper does not provide empirical data on how different threshold values affect discussion quality, user engagement, or perception of fairness.
- What evidence would resolve it: A/B testing with different AQuA score thresholds across multiple discussions, measuring outcomes like engagement metrics, perceived comment quality, and user satisfaction.

### Open Question 2
- Question: How does the effectiveness of AI-supported modules vary across different types of political discussions (e.g., highly polarized vs. consensus-seeking topics)?
- Basis in paper: [inferred] The planned user study mentions evaluating the modules' impact on "discussion quality and user satisfaction," but doesn't specify how results might differ based on topic characteristics.
- Why unresolved: The paper doesn't analyze whether the AI tools perform differently depending on the nature of the discussion topic or the existing level of polarization.
- What evidence would resolve it: Comparative analysis of the AI modules' performance across discussions on various topics with different levels of complexity, controversy, and existing polarization.

### Open Question 3
- Question: What is the long-term impact of AI-supported modules on discussion quality compared to their immediate effects?
- Basis in paper: [explicit] The paper states "A large-scale user study is planned to evaluate the impact of these AI-supported modules on discussion quality and user satisfaction," suggesting only immediate effects will be measured.
- Why unresolved: The paper only mentions a single user study without longitudinal follow-up to assess whether improvements in discussion quality persist over time.
- What evidence would resolve it: Longitudinal studies tracking the same discussion groups over multiple sessions or extended periods to measure sustained changes in discussion quality and user behavior.

## Limitations
- The quality and representativeness of synthetic data compared to real user comments remains unverified
- The effectiveness of AI modules depends heavily on stance detection accuracy and AQuA scoring validity, neither empirically validated
- Potential bias in AI models and their impact on marginalized voices in online discussions is not addressed

## Confidence
- High confidence: The technical implementation details of the Django integration and the general approach to using BERT models for text classification
- Medium confidence: The methodology for combining 20 deliberative quality indicators into the AQuA score, as the weight estimation process is described but not fully validated
- Low confidence: The assumption that synthetic data generated by Mistral-7B will effectively improve stance detection model performance on real-world political discussions

## Next Checks
1. Conduct a pilot user study with 50-100 participants to measure actual user engagement with the AI-recommended comments versus organically discovered comments, tracking metrics like response rates and comment quality.

2. Perform an ablation study comparing the performance of the stance detection model trained only on X-Stance dataset versus the model trained with synthetic data augmentation to quantify the actual benefit of synthetic data generation.

3. Implement bias auditing procedures to test whether the AI modules systematically favor certain political viewpoints or demographic groups, using counterfactual analysis with comments that vary only in author characteristics.