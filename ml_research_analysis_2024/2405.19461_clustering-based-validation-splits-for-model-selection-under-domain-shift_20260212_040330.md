---
ver: rpa2
title: Clustering-Based Validation Splits for Model Selection under Domain Shift
arxiv_id: '2405.19461'
source_url: https://arxiv.org/abs/2405.19461
tags:
- domain
- kernel
- split
- validation
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using a constrained kernel k-means clustering
  algorithm to split training and validation sets under domain shift, motivated by
  distributionally robust optimisation and domain adaptation theory. The method maximizes
  the MMD between the two sets, formulated as a linear program that controls size,
  label, and domain distributions without requiring additional metadata.
---

# Clustering-Based Validation Splits for Model Selection under Domain Shift

## Quick Facts
- arXiv ID: 2405.19461
- Source URL: https://arxiv.org/abs/2405.19461
- Reference count: 31
- Method maximizes MMD between training/validation sets via constrained kernel k-means clustering to improve OOD model selection

## Executive Summary
This paper proposes using a constrained kernel k-means clustering algorithm to split training and validation sets under domain shift, motivated by distributionally robust optimisation and domain adaptation theory. The method maximizes the MMD between the two sets, formulated as a linear program that controls size, label, and domain distributions without requiring additional metadata. Experiments across DG and UDA tasks show consistent improvements over random and metadata-based splits, closing about 50% of the gap to oracle performance. Analysis confirms MMD between training and validation sets correlates positively with test domain accuracy, supporting the approach's validity.

## Method Summary
The method uses constrained kernel k-means clustering to partition data into training and validation sets while maximizing the Maximum Mean Discrepancy (MMD) between them. The clustering problem is formulated as a linear program with constraints ensuring balanced cluster sizes, label distributions, and optionally domain distributions. The algorithm can use either linear or RBF kernels, with Nyström approximation for scalability. After clustering, data is split according to cluster assignments, creating training and validation sets that are maximally domain-shifted from each other while maintaining class balance.

## Key Results
- Clustering-based splits consistently outperform random splits across six datasets, closing ~50% of the gap to oracle performance
- MMD between training and validation sets correlates positively with test domain accuracy (ρ=0.72, p=0.02)
- Model selection, not training, drives the observed performance gains (MMD values remain stable while accuracy changes)
- The method achieves strong performance without requiring additional metadata beyond the data itself

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Maximizing MMD between training and validation sets improves OOD model selection.
- **Mechanism**: By using kernel k-means clustering to partition data, the algorithm explicitly maximizes the Maximum Mean Discrepancy (MMD) between the resulting training and validation sets. This creates a validation set that is maximally domain-shifted from the training set, aligning with distributionally robust optimization principles.
- **Core assumption**: MMD is a good proxy for domain shift and correlates with test domain accuracy.
- **Evidence anchors**:
  - [abstract] "By adopting the maximum mean discrepancy (MMD) as the measure of mismatch..."
  - [section 3.1] "A linear correlation has also been observed empirically (Napoli & White, 2025)"
  - [corpus] Weak evidence - no direct MMD-domain shift studies found in neighbors
- **Break condition**: If the correlation between MMD and test accuracy weakens or reverses in new datasets, or if MMD becomes an unreliable measure of domain shift for the data types in question.

### Mechanism 2
- **Claim**: Constrained clustering prevents degenerate splits that could harm model selection.
- **Mechanism**: The algorithm enforces constraints on cluster sizes, label distributions, and optionally domain distributions using linear programming. This ensures both training and validation sets have sufficient representation from each class/domain, preventing situations where entire classes or domains end up in one set only.
- **Core assumption**: Balanced class/domain representation in both training and validation sets is necessary for reliable model selection.
- **Evidence anchors**:
  - [section 4] "This constraint ensures that there are sufficient examples from each class in both T and V to properly train and validate the models."
  - [section 5.4] Table 3 shows MMD and accuracy drop when additional constraints are applied
  - [corpus] Weak evidence - no direct comparison of constrained vs unconstrained clustering in neighbors
- **Break condition**: If constraints become too restrictive (e.g., many classes with few examples each), the clustering problem may become infeasible or yield poor solutions.

### Mechanism 3
- **Claim**: Kernel k-means formulation provides tractable optimization of MMD maximization.
- **Mechanism**: The paper shows that maximizing MMD between clusters is equivalent to minimizing the kernel k-means objective. This equivalence allows using established clustering algorithms and optimization techniques (Lloyd's algorithm with LP constraints) rather than solving the MMD maximization directly, which would be intractable.
- **Core assumption**: The kernel trick and k-means equivalence hold for the chosen kernel and data representation.
- **Evidence anchors**:
  - [section 4] "By adopting the maximum mean discrepancy (MMD) as the measure of mismatch, it is shown that the partitioning problem reduces to kernel k-means clustering."
  - [section 4] Theorem 1 proves the equivalence between MMD maximization and kernel k-means minimization
  - [corpus] Moderate evidence - "Weighted quantization using MMD" discusses similar MMD-quantization connections
- **Break condition**: If the kernel chosen doesn't induce a characteristic RKHS for the data distributions, or if the data structure violates assumptions needed for the k-means equivalence.

## Foundational Learning

- **Concept**: Maximum Mean Discrepancy (MMD)
  - Why needed here: MMD is the core metric being maximized to create domain-shifted splits
  - Quick check question: How does MMD measure the difference between two distributions in RKHS?

- **Concept**: Reproducing Kernel Hilbert Space (RKHS)
  - Why needed here: MMD operates in the RKHS induced by the kernel function, and the kernel trick is used for efficient computation
  - Quick check question: What properties must a kernel have to induce an RKHS suitable for MMD estimation?

- **Concept**: Distributionally Robust Optimization (DRO)
  - Why needed here: The motivation for maximizing domain mismatch comes from DRO principles
  - Quick check question: How does minimizing worst-case error over an uncertainty set relate to creating a maximally shifted validation set?

## Architecture Onboarding

- **Component map**:
  Data preprocessing -> Feature extraction (optional finetuning) -> Kernel matrix computation -> Constrained k-means clustering -> Split generation -> Model training/validation pipeline

- **Critical path**:
  1. Extract features from data using model (with optional finetuning)
  2. Compute kernel matrix between all examples
  3. Formulate and solve constrained LP assignment problem
  4. Generate training/validation split based on cluster assignments
  5. Train models on training set, select based on validation performance

- **Design tradeoffs**:
  - Kernel choice vs. computational complexity: RBF kernel requires full kernel matrix computation; linear kernel enables faster computation but may capture less complex domain structure
  - Constraint tightness vs. solution quality: Tighter constraints (preserving domain distributions) may reduce achievable MMD but ensure balanced splits
  - Scalability vs. accuracy: Nyström approximation reduces computation but may sacrifice some clustering quality

- **Failure signatures**:
  - LP solver fails to find feasible solution → constraints too restrictive given data distribution
  - Very low MMD values across all split methods → little domain shift in data, method provides no benefit
  - Significant performance drop vs. random split → constraints preventing effective clustering or kernel mismatch

- **First 3 experiments**:
  1. Run clustering with no constraints (just size constraint) on a small dataset to verify basic functionality and compare to random split
  2. Add class distribution constraints and test on a balanced dataset to verify constraint enforcement
  3. Add domain distribution constraints and test on multi-domain data to verify domain-aware splitting

## Open Questions the Paper Calls Out

- **Question**: How sensitive is the proposed clustering-based validation split to the choice of kernel function and its hyperparameters, particularly for high-dimensional feature spaces?
  - Basis in paper: [explicit] The paper acknowledges that kernel choice implies assumptions about the data and mentions testing both Gaussian RBF and linear kernels, noting that RBF performance may improve with more careful tuning of the bandwidth parameter.
  - Why unresolved: The experiments fixed the RBF bandwidth to γ=1, which the authors themselves noted is inappropriate for high-dimensional spaces. The comparison between kernels showed only marginal differences within margin of error, without exploring the impact of different bandwidth values or other kernel types.
  - What evidence would resolve it: Systematic experiments varying kernel hyperparameters (especially RBF bandwidth) across different dimensionalities, and testing alternative kernel types, would clarify sensitivity and guide practical kernel selection.

- **Question**: What is the maximum number of classes for which the clustering-based split remains effective before the constraints severely limit the achievable MMD between training and validation sets?
  - Basis in paper: [explicit] The paper includes an ablation study showing MMD decreases as the number of synthetic classes increases, and identifies a threshold around 100 classes where MMD(T,V) remains higher than MMD(T,E) and MMD(V,E), but suggests effectiveness may be limited beyond this point.
  - Why unresolved: The study used synthetic labels on a single dataset subset, and the analysis doesn't establish a general theoretical limit or explore how this threshold varies with dataset size, class balance, or feature dimensionality.
  - What evidence would resolve it: Extensive experiments across diverse datasets with varying numbers of real classes, combined with theoretical analysis of constraint looseness versus MMD maximization, would establish practical limits.

- **Question**: Does the improved model selection performance translate to consistent benefits when using larger, more modern neural network architectures rather than the ResNet-18 model used in experiments?
  - Basis in paper: [explicit] The authors acknowledge that experiments were restricted to ResNet-18 due to computational costs and note that results should ideally be repeated across different architectures to reflect generalization power differences.
  - Why unresolved: All experimental results were obtained using a single, relatively small architecture, leaving open the possibility that the method's effectiveness varies significantly with model capacity or architecture type.
  - What evidence would resolve it: Replicating the experimental comparisons using multiple architectures ranging from small CNNs to large transformers or vision transformers would demonstrate whether performance gains generalize across model families.

## Limitations

- The core assumption that MMD maximization improves model selection lacks strong theoretical grounding beyond empirical correlation
- Results are demonstrated only on ResNet-18 architecture, limiting generalizability to larger models
- The method still implicitly uses domain information through balanced minibatches and validation averaging in the experimental framework

## Confidence

- **High confidence**: The constrained clustering algorithm successfully maximizes MMD and improves model selection across multiple datasets
- **Medium confidence**: The correlation between MMD and test accuracy is consistent but may not hold universally
- **Low confidence**: The specific mechanisms by which MMD maximization improves model selection (beyond correlation) are not fully explained

## Next Checks

1. Test the method on datasets with minimal domain shift to verify the correlation between MMD and test accuracy breaks down as expected
2. Conduct ablation studies varying kernel bandwidth and Nyström approximation parameters to assess their impact on final performance
3. Evaluate whether the clustering-based splits improve model selection in scenarios where random splits already achieve near-oracle performance