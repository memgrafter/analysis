---
ver: rpa2
title: 'Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents'
arxiv_id: '2403.00690'
source_url: https://arxiv.org/abs/2403.00690
tags:
- agent
- nethack
- game
- netplay
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NetPlay is an LLM-powered zero-shot agent for the complex roguelike
  NetHack. It uses a closed-loop system where GPT-4 selects skills sequentially, with
  past interactions stored in memory to enhance decision-making.
---

# Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents

## Quick Facts
- arXiv ID: 2403.00690
- Source URL: https://arxiv.org/abs/2403.00690
- Reference count: 20
- NetPlay demonstrates flexibility and proficiency in interacting with NetHack's mechanics but struggles with ambiguous task descriptions and lack of explicit feedback

## Executive Summary
NetPlay is an LLM-powered zero-shot agent for NetHack that uses a closed-loop system where GPT-4 selects skills sequentially, maintaining past interactions in memory to enhance decision-making. The agent detects important game events to interrupt running skills and react to unforeseen circumstances. While NetPlay shows promise in following detailed instructions and creative problem-solving, it performs inferior to handcrafted agents and autoascend. The evaluation highlights the critical importance of detailed context information and the need for dynamic methods to supply context in complex games.

## Method Summary
NetPlay operates as a zero-shot agent using GPT-4-Turbo to select from predefined skills based on current game observations and stored memory. The system maintains a capped memory of up to 500 tokens containing system messages, AI messages, and task definitions. A data tracker monitors the game state for specific events, interrupting current skills when necessary. The agent uses MiniHack for isolated scenario testing and the full NetHack Learning Environment for comprehensive gameplay evaluation, with experiments conducted across five runs for each scenario.

## Key Results
- NetPlay demonstrates flexibility and proficiency in interacting with NetHack's mechanics
- Agent struggles with ambiguous task descriptions and lack of explicit feedback
- Performs best with detailed context information, highlighting need for dynamic context supply methods
- Inferior performance compared to handcrafted agent and autoascend, but excels at following detailed instructions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NetPlay's short-term memory of past interactions enables context-aware skill selection by the LLM.
- Mechanism: The agent maintains a capped memory of up to 500 tokens containing system messages, AI messages, and human messages. This memory is included in each prompt, allowing the LLM to reason over its recent history when choosing the next skill.
- Core assumption: The LLM can effectively process and utilize a limited history window to improve decision-making over time.
- Evidence anchors: [abstract] "tracks past interactions to enhance decision-making"; [section] "The short-term memory is implemented using a list of messages representing the timeline of events"
- Break condition: If the memory window is too small to capture relevant context, or if the LLM cannot effectively process the history due to token limitations.

### Mechanism 2
- Claim: Event-driven interruptions allow the agent to react to unforeseen circumstances in the dynamic NetHack environment.
- Mechanism: A data tracker monitors the game state for specific events (new messages, discovered structures, level changes, stat changes, low health, new monsters/items). When such events occur, the currently running skill is interrupted, and the agent re-prompts the LLM to select a new skill based on the updated context.
- Core assumption: The set of tracked events is comprehensive enough to capture critical game situations requiring immediate attention.
- Evidence anchors: [abstract] "Given NetHack's unpredictable nature, NetPlay detects important game events to interrupt running skills, enabling it to react to unforeseen circumstances"; [section] "The data tracker also looks for specific events in the game to provide additional feedback to the LLM"
- Break condition: If important events are not detected or if the agent fails to recognize the significance of certain events, leading to inappropriate skill choices.

### Mechanism 3
- Claim: Detailed observation descriptions provide spatial context to the LLM, enabling it to navigate and interact with the complex NetHack environment.
- Mechanism: The observation includes descriptions of the current level divided into structures (rooms, corridors) with unique identifiers, distances, and contents. Monsters are categorized as close and distant with positional information. This spatial information is intended to help the LLM make informed decisions about movement and interactions.
- Core assumption: The LLM can effectively process and utilize spatial information presented in text form to navigate the environment.
- Evidence anchors: [section] "Because we do not use a multi-modal LLM, we attempt to convey spatial information by dividing the level into structures like rooms and corridors"; [section] "Note that despite our emphasis on providing spatial information, navigating the environment proved challenging for the LLM"
- Break condition: If the LLM cannot effectively process the spatial information or if the observation description is insufficient to capture the necessary details for decision-making.

## Foundational Learning

- Concept: Zero-shot learning with LLMs
  - Why needed here: NetPlay operates without pre-training on NetHack-specific data, relying solely on the LLM's general knowledge and the provided context to make decisions.
  - Quick check question: How does the agent handle situations not covered by its predefined skills or observation descriptions?

- Concept: Closed-loop planning systems
  - Why needed here: NetPlay uses a closed-loop system where the LLM selects skills sequentially, with the agent continuously updating its context and re-planning based on feedback from the environment.
  - Quick check question: What happens when the agent encounters a situation that requires a different approach than its current plan?

- Concept: Prompt engineering and chain-of-thought reasoning
  - Why needed here: The agent uses chain-of-thought prompting to guide the LLM towards selecting appropriate skills, and the prompt structure (including memory, observation, and task description) is crucial for effective decision-making.
  - Quick check question: How does the prompt structure influence the LLM's skill selection, and what are the potential pitfalls of poor prompt design?

## Architecture Onboarding

- Component map: LLM (GPT-4-Turbo) -> Prompt builder -> Skill library -> Data tracker -> NetHack environment
- Critical path: 1. Receive task 2. Build prompt with memory, observation, and task description 3. LLM selects skill 4. Execute skill in NetHack environment 5. Data tracker monitors for events 6. If event detected or skill completed, return to step 2
- Design tradeoffs:
  - Memory size vs. context relevance: Larger memory allows more context but increases token usage and processing time
  - Skill granularity: More granular skills provide finer control but increase the complexity of the skill library
  - Observation detail vs. prompt size: More detailed observations provide better context but consume more tokens
- Failure signatures:
  - Timeouts: Agent gets stuck in a loop, often due to menu navigation issues or inability to progress
  - Starvation: Agent fails to find sufficient food, leading to death
  - Incorrect skill usage: Agent chooses inappropriate skills due to insufficient context or misinterpretation of the observation
- First 3 experiments:
  1. Simple task execution: Test the agent's ability to complete a straightforward task (e.g., pick up a specific item) to verify basic functionality
  2. Dynamic event handling: Create a scenario where an unexpected event occurs (e.g., monster spawns) to test the agent's ability to interrupt and replan
  3. Complex instruction following: Provide a detailed, multi-step instruction to assess the agent's ability to follow complex guidance and manage its memory effectively

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively supply dynamic context information to LLMs for complex games like NetHack?
- Basis in paper: [explicit] The paper discusses that NetPlay performs best with detailed context information and emphasizes the necessity for dynamic methods in supplying context information for complex games.
- Why unresolved: The challenge lies in determining the most relevant context information in real-time, given the vast and unpredictable nature of NetHack.
- What evidence would resolve it: Developing and testing methods that automatically identify and provide the most relevant context information to the LLM during gameplay, potentially through integration with external knowledge sources like NetHack Wikipedia.

### Open Question 2
- Question: What are the limitations of using predefined skills and observation descriptions for LLM-based agents in complex environments?
- Basis in paper: [inferred] The paper indicates that the predefined skills and observation descriptions struggle to encompass the vast complexity of NetHack, making it difficult to handle all potential edge cases.
- Why unresolved: It is challenging to anticipate every possible scenario in a complex environment, and designing a comprehensive set of skills and observations is an ongoing process.
- What evidence would resolve it: Creating a more flexible system that allows the LLM to generate its own skills and observations based on the game state, possibly using machine learning techniques.

### Open Question 3
- Question: How can machine learning be used to replace handcrafted components of LLM-based agents in complex games?
- Basis in paper: [explicit] The paper suggests that another promising research direction is to use machine learning to replace the handcrafted components of the agent, such as skills and observation descriptions.
- Why unresolved: Current methods rely heavily on manual design and fine-tuning, which may not scale well to more complex environments.
- What evidence would resolve it: Implementing a machine learning-based system that can learn and adapt skills and observations from gameplay data, demonstrating improved performance and scalability.

## Limitations
- The agent's performance heavily depends on the quality and detail of context information
- Current skill library may not capture the full complexity of NetHack's mechanics
- LLM's ability to process spatial information from text descriptions remains a significant bottleneck
- Agent's behavior can be unpredictable when encountering novel situations not covered by predefined skills

## Confidence
- High confidence: The core mechanism of using LLM-driven skill selection with memory-based context is validated by the experimental results
- Medium confidence: The agent's ability to follow detailed instructions and exhibit creative behavior is demonstrated, but performance variability suggests room for improvement
- Low confidence: The long-term effectiveness of the zero-shot approach in increasingly complex scenarios remains uncertain

## Next Checks
1. **Context Quality Impact Study**: Systematically vary the detail and clarity of task descriptions and observation formats to quantify their impact on agent performance
2. **Skill Library Expansion Analysis**: Incrementally expand the predefined skill set and measure resulting improvements in agent capability
3. **Spatial Processing Enhancement**: Experiment with alternative methods for conveying spatial information to the LLM to improve navigation and interaction accuracy