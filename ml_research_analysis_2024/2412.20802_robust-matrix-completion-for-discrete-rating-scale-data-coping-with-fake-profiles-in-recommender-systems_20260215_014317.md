---
ver: rpa2
title: 'Robust Matrix Completion for Discrete Rating-Scale Data: Coping with Fake
  Profiles in Recommender Systems'
arxiv_id: '2412.20802'
source_url: https://arxiv.org/abs/2412.20802
tags:
- rdmc
- matrix
- median
- discretized
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces RDMC, a robust matrix completion method
  designed for discrete rating-scale data while resisting malicious manipulation.
  It addresses challenges common in recommender systems and surveys: discrete, bounded
  ratings; missing-not-at-random patterns; and corrupted observations from attacks
  or careless responding.'
---

# Robust Matrix Completion for Discrete Rating-Scale Data: Coping with Fake Profiles in Recommender Systems

## Quick Facts
- arXiv ID: 2412.20802
- Source URL: https://arxiv.org/abs/2412.20802
- Authors: Aurore Archimbaud; Andreas Alfons; Ines Wilms
- Reference count: 40
- One-line primary result: RDMC combines robust loss, discreteness constraints, and low-rank regularization to improve matrix completion accuracy under discrete ratings, missing-not-at-random patterns, and corrupted observations from fake profiles.

## Executive Summary
This paper introduces RDMC, a robust matrix completion method designed for discrete rating-scale data while resisting malicious manipulation. It addresses challenges common in recommender systems and surveys: discrete, bounded ratings; missing-not-at-random patterns; and corrupted observations from attacks or careless responding. RDMC combines a robust loss function, discreteness constraints, and a low-rank regularizer within an alternating direction method of multipliers (ADMM) framework. Empirical simulations and real-world benchmarks (MovieLens 100K, Yahoo! Music) demonstrate that RDMC outperforms state-of-the-art methods in prediction accuracy, especially under attacks or MNAR missingness, while maintaining stability. The pseudo-Huber loss is recommended for general use, with the bounded absolute loss preferred under high levels of corruption. RDMC thus offers a statistically sound, practical solution for matrix completion in realistic, adversarial settings.

## Method Summary
RDMC extends matrix completion to discrete rating-scale data by enforcing discreteness constraints on a latent matrix L while optimizing a robust loss function in an ADMM framework. The method centers columns by median, uses soft-thresholded SVD for the low-rank update, and updates L with robust loss functions (pseudo-Huber, absolute, or bounded absolute). The regularization parameter λ is selected via repeated holdout validation. RDMC is tailored for missing-not-at-random (MNAR) data and corrupted observations from fake profiles or careless respondents, providing improved accuracy over SI-discretized baselines.

## Key Results
- RDMC outperforms SI-discretized and other baselines in mean absolute error (MAE) under MNAR and MCAR missingness, especially under simulated nuke attacks.
- Pseudo-Huber loss provides a balance between robustness and smoothness, with bounded absolute loss preferred under extreme corruption.
- RDMC maintains stability and robustness, with lower mean prediction shift (MPS) compared to competitors under various attack scenarios.
- Repeated holdout validation effectively selects λ without the complexity of cross-validation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RDMC achieves improved prediction accuracy by combining discreteness constraints with a robust loss function within an ADMM framework.
- Mechanism: The algorithm introduces an ancillary continuous matrix Z with a low-rank constraint and an auxiliary discrete matrix L, which are alternately optimized. The discreteness constraint on L preserves the rating scale structure, while the robust loss (e.g., pseudo-Huber) reduces the influence of outliers from fake profiles or careless respondents.
- Core assumption: Low-rank structure of the latent rating matrix holds and the discreteness constraint can be effectively enforced without breaking convergence.
- Evidence anchors:
  - [abstract] "RDMC combines a robust loss function, discreteness constraints, and a low-rank regularizer within an ADMM framework."
  - [section] "we introduce a matrix completion algorithm that is tailored towards the discrete nature of incomplete—oftentimes missing not at random—rating-scale data, while being robust to the presence of corrupted observations."
  - [corpus] Weak evidence; no direct citations or simulation results in corpus neighbors support this claim.
- Break condition: If the underlying rating matrix does not exhibit a low-rank structure, or if the ADMM convergence is unstable due to inappropriate loss or regularization parameter settings.

### Mechanism 2
- Claim: The pseudo-Huber loss provides a balance between robustness and smoothness, outperforming pure absolute loss in most scenarios.
- Mechanism: Pseudo-Huber loss behaves quadratically near zero (like MSE) but linearly in the tails (like absolute loss), offering differentiable robustness. This makes the optimization more stable and often requires fewer iterations.
- Core assumption: The parameter τ is well-chosen relative to the rating scale granularity, and the data is not dominated by extremely heavy-tailed corruption.
- Evidence anchors:
  - [section] "The inclusion of this loss function is motivated by its successful application in autoencoder neural networks for the detection of careless responding in rating-scale survey data."
  - [section] "Preliminary simulations indicate that results for the absolute loss and the bounded absolute loss are similar."
  - [corpus] No direct corpus evidence; this claim is based on internal simulation results.
- Break condition: If τ is poorly tuned (too small or too large), the loss function may behave suboptimally; if corruption is extreme, a bounded loss might be preferable.

### Mechanism 3
- Claim: Repeated holdout validation with a small test set proportion (e.g., 10%) effectively selects the regularization parameter λ without cross-validation complexity.
- Mechanism: Each candidate λ is evaluated on a randomly sampled subset of observed ratings; the value minimizing validation loss is chosen. This avoids the need to partition the matrix into disjoint blocks, which can be problematic for matrix completion due to row-wise correlations.
- Core assumption: Observed ratings are sufficiently informative for validation and the sampling preserves the distribution of the missing pattern.
- Evidence anchors:
  - [section] "we prefer repeated holdout validation, as the proportion of observations in the validation set and the number of replications can be chosen independently."
  - [section] "For selecting the regularization parameter λ, both RDMC and SI use repeated holdout validation with 5 replications and 10% of the observed cells."
  - [corpus] No direct corpus evidence supporting the choice of holdout over cross-validation in matrix completion.
- Break condition: If the training set becomes too small or the missingness mechanism is highly non-random, the validation set may not be representative, leading to poor λ selection.

## Foundational Learning

- Concept: Low-rank matrix factorization
  - Why needed here: RDMC enforces low-rank structure on an auxiliary continuous matrix Z to capture latent user and item factors, enabling completion of missing ratings.
  - Quick check question: If the rank of the latent matrix is set too low, what happens to prediction accuracy?
- Concept: Alternating Direction Method of Multipliers (ADMM)
  - Why needed here: ADMM is used to solve the optimization problem with multiple constraints (discreteness, low-rank) by splitting the problem into tractable subproblems.
  - Quick check question: What happens to convergence if the penalty parameter µ is not increased appropriately during iterations?
- Concept: Robust loss functions (pseudo-Huber, bounded absolute)
  - Why needed here: Standard MSE is sensitive to outliers from fake profiles or careless respondents; robust losses mitigate their impact without breaking differentiability.
  - Quick check question: How does the choice of τ in pseudo-Huber loss affect the trade-off between robustness and sensitivity to small errors?

## Architecture Onboarding

- Component map: Input (X) -> Center by median -> ADMM loop (Z update via soft-thresholded SVD, L update via robust loss) -> Discreteness projection -> Output (completed matrix)
- Critical path:
  1. Center columns of X by median.
  2. Initialize L and Θ.
  3. For each λ in grid: run ADMM until convergence or max iterations.
  4. Evaluate validation loss; pick λ minimizing it.
  5. Re-run with selected λ to obtain final predictions.
- Design tradeoffs:
  - Discrete vs continuous prediction: Enforcing discreteness preserves interpretability but may increase optimization difficulty.
  - Robust vs non-robust loss: Robust losses improve resistance to outliers but may reduce efficiency if data is clean.
  - Convergence criteria: Strict convergence yields more accurate solutions but at higher computational cost.
- Failure signatures:
  - Non-convergence of ADMM: Likely due to inappropriate µ or loss function choice.
  - Overfitting: Indicated by low training loss but high validation loss; suggests λ is too small.
  - Underfitting: Both training and validation losses are high; suggests λ is too large.
- First 3 experiments:
  1. Run RDMC with pseudo-Huber loss on MovieLens 100K; compare MAE to SI-discretized.
  2. Simulate nuke attacks; measure MPS for RDMC vs SI variants.
  3. Vary τ in pseudo-Huber; observe effect on prediction stability under 20% careless respondents.

## Open Questions the Paper Calls Out

- How does RDMC's performance change when handling matrices with very large numbers of items (p > 1000) or users (n > 1000)?
  - Basis in paper: [inferred] The simulations and empirical studies use relatively small-scale matrices (n = 300, p ≤ 939). No experiments are conducted on larger datasets.
  - Why unresolved: The paper does not explore scalability limits or computational efficiency for very large recommender systems or surveys.
  - What evidence would resolve it: Testing RDMC on datasets with significantly larger n and p values, reporting runtime and accuracy metrics.

- Can RDMC be extended to handle ordinal ratings with non-uniform category boundaries or missing category values?
  - Basis in paper: [explicit] The method assumes uniform spacing between categories and a fixed set of discrete values per column, with normalization applied for different column ranges.
  - Why unresolved: The current formulation does not address cases where rating scales are non-uniform or contain gaps, which may occur in real-world applications.
  - What evidence would resolve it: Modifying RDMC to accept arbitrary category sets and testing its performance on datasets with irregular rating scales.

- How does RDMC perform under mixed missingness mechanisms, such as a combination of MNAR and MCAR patterns within the same dataset?
  - Basis in paper: [explicit] The paper evaluates RDMC under pure MNAR and pure MCAR settings separately, but does not explore hybrid scenarios.
  - Why unresolved: Real-world data may exhibit complex missingness patterns that combine multiple mechanisms, which could affect RDMC's robustness.
  - What evidence would resolve it: Simulating datasets with mixed missingness patterns and comparing RDMC's performance to existing methods.

## Limitations
- Empirical evidence is based on limited simulations and benchmark datasets; performance under more diverse real-world conditions remains to be validated.
- Computational cost of RDMC, particularly ADMM iterations, is not extensively discussed, which could limit scalability.
- Performance under extreme corruption or highly non-random missingness is not fully explored.

## Confidence
- High: The conceptual framework of RDMC, combining robust loss functions with discreteness constraints, is well-founded and logically sound.
- Medium: The empirical results demonstrating RDMC's improved accuracy over benchmarks are promising, but the scope of validation is limited.
- Low: The method's performance under extreme corruption or highly non-random missingness is not fully explored.

## Next Checks
1. Apply RDMC to a larger dataset (e.g., MovieLens 1M or beyond) and compare runtime and memory usage with SI-discretized.
2. Simulate datasets with higher proportions of missing data (e.g., 70-90%) and stronger attacks (e.g., >50% of ratings corrupted) to test RDMC's stability.
3. Validate RDMC on a non-recommender system dataset (e.g., educational testing or survey data) to assess generalizability beyond movie/music ratings.