---
ver: rpa2
title: 'OR-Bench: An Over-Refusal Benchmark for Large Language Models'
arxiv_id: '2405.20947'
source_url: https://arxiv.org/abs/2405.20947
tags:
- prompts
- toxic
- prompt
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces OR-Bench, the first large-scale benchmark
  for evaluating over-refusal in large language models (LLMs). Over-refusal occurs
  when models reject safe prompts due to overly cautious safety alignment.
---

# OR-Bench: An Over-Refusal Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2405.20947
- Source URL: https://arxiv.org/abs/2405.20947
- Authors: Justin Cui; Wei-Lin Chiang; Ion Stoica; Cho-Jui Hsieh
- Reference count: 40
- Primary result: OR-Bench contains 80,000 prompts across 10 categories, revealing a strong safety-over-refusal tradeoff (Spearman 0.878) across 25 models from 8 families

## Executive Summary
This paper introduces OR-Bench, the first large-scale benchmark designed to evaluate over-refusal in large language models (LLMs). Over-refusal occurs when models reject safe prompts due to overly cautious safety alignment, trading helpfulness for safety. The authors propose an automated pipeline that converts toxic prompts into seemingly toxic but benign ones, filtered by LLM moderators. Experiments reveal that most models prioritize safety at the cost of over-refusal, with no model achieving both high safety and low over-refusal. The benchmark is publicly available to facilitate future research on balancing safety and helpfulness in LLMs.

## Method Summary
The authors developed an automated pipeline to generate seemingly toxic prompts by first creating toxic seed prompts, then rewriting them to appear harmful but be benign, and finally filtering through LLM moderators (GPT-4-turbo-2024-04-09, Llama-3-70b, Gemini-1.5-pro). OR-Bench contains 80,000 prompts across 10 categories, with a hard subset of 1,000 prompts and 600 toxic prompts. Model responses are evaluated using keyword matching and LLM-based evaluation to measure over-refusal rates and safety levels.

## Key Results
- OR-Bench contains 80,000 seemingly toxic prompts across 10 categories
- Strong correlation (Spearman 0.878) between safety and over-refusal across 25 models from 8 families
- Most models prioritize safety at the cost of over-refusal, with no model achieving both high safety and low over-refusal
- Hard subset (1,000 prompts) provides challenging evaluation for advanced models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Over-refusal occurs because safety alignment models trade safety for helpfulness by rejecting seemingly toxic prompts
- Mechanism: Models trained to avoid toxic outputs become overly cautious, rejecting prompts that appear harmful but are actually benign
- Core assumption: Safety alignment improves toxic prompt rejection at the cost of accepting more seemingly toxic prompts
- Evidence anchors:
  - [abstract]: "enhanced safety often come with the side effect of over-refusal, where the LLMs may reject innocuous prompts and become less helpful"
  - [section 4.3]: "Our analysis reveals a strong correlation between safety and over-refusal. Models rejecting more toxic prompts (safer) tend to also reject more innocuous prompts (over-refusal)"
  - [corpus]: Found 25 related papers with average FMR=0.416, indicating moderate but not strong community evidence for over-refusal phenomenon

### Mechanism 2
- Claim: The automated pipeline generates seemingly toxic prompts by rewriting toxic seeds and filtering with LLM moderators
- Mechanism: Start with toxic prompts, rewrite them to appear harmful but be benign, then use LLM judges to filter out remaining toxic content
- Core assumption: LLMs can effectively distinguish between toxic and seemingly toxic prompts when properly prompted
- Evidence anchors:
  - [section 3.1.3]: "We employ GPT-4 as a judge model to determine if a response rejects the original question"
  - [section 3.2.2]: "we ask the moderators to annotate each of the prompts using the moderation method mentioned in section 3.1.3"
  - [corpus]: Limited direct evidence, but FMR score of 0.416 suggests moderate confidence in pipeline effectiveness

### Mechanism 3
- Claim: OR-Bench provides comprehensive evaluation across 10 categories of seemingly toxic prompts
- Mechanism: The benchmark includes 80K prompts spanning 10 categories, with a hard subset for challenging models
- Core assumption: Categorizing seemingly toxic prompts helps identify specific safety alignment weaknesses
- Evidence anchors:
  - [section 3.2.1]: "Drawing inspiration from ToxicChat...we categorize the triggering reason for LLMs to reject a prompt into the following 10 common categories"
  - [section 4.3]: "we analyze the model performances related to detailed categories as shown in fig. 4"
  - [corpus]: Moderate evidence with FMR score 0.416 suggesting reasonable coverage of over-refusal scenarios

## Foundational Learning

- Concept: Safety alignment tradeoffs in LLMs
  - Why needed here: Understanding why models must balance safety and helpfulness is crucial for interpreting benchmark results
  - Quick check question: Why does improving toxic prompt rejection often lead to more over-refusals?

- Concept: Prompt rewriting techniques
  - Why needed here: The benchmark relies on converting toxic prompts to seemingly toxic ones, requiring knowledge of effective rewriting strategies
  - Quick check question: How does changing "how to kill a person" to "how to create a prop knife for theater" make it seemingly toxic?

- Concept: LLM-based content moderation
  - Why needed here: The benchmark uses LLMs as moderators, requiring understanding of their strengths and limitations
  - Quick check question: What are the advantages and disadvantages of using LLM judges versus human annotators?

## Architecture Onboarding

- Component map:
  Toxic seed generator → Prompt rewriter → LLM moderator ensemble → OR-Bench datasets
  Evaluation pipeline: Prompt → Model → Response classifier → Results aggregation

- Critical path: Prompt generation → Moderation → Evaluation → Analysis
  - The moderation step is most critical as it determines which prompts make it into the benchmark

- Design tradeoffs:
  - Using LLM moderators vs human annotators (cost vs accuracy)
  - Temperature settings for generation (diversity vs consistency)
  - Number of categories vs prompt distribution balance

- Failure signatures:
  - High agreement between LLM moderators and model families suggests bias
  - Models achieving perfect scores on OR-Bench-Hard-1K indicate the benchmark is too easy
  - Disproportionate category representation suggests generation bias

- First 3 experiments:
  1. Run OR-Bench-Hard-1K on a new model to establish baseline performance
  2. Test the moderation pipeline with a small sample to verify prompt quality
  3. Analyze category-specific rejection rates to identify model weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal model sizes and architectures for balancing safety and helpfulness in LLMs?
- Basis in paper: The paper discusses a trade-off between safety and over-refusal but does not explore how different model sizes or architectures might affect this balance. It mentions that model size does not necessarily correlate with a better safety-sensitivity balance.
- Why unresolved: The paper only evaluates existing models and does not conduct experiments to determine if larger or differently architected models perform better at balancing safety and helpfulness. It does not explore the relationship between model size and the ability to avoid over-refusal while maintaining safety.
- What evidence would resolve it: Controlled experiments comparing different model sizes and architectures on the OR-Bench benchmark, measuring both safety (toxic prompt rejection) and over-refusal (seemingly toxic prompt rejection) rates. Analysis of how model capacity and architecture influence the ability to distinguish between truly harmful and seemingly harmful prompts.

### Open Question 2
- Question: How can the diversity of prompts within each category be controlled or improved in the OR-Bench dataset?
- Basis in paper: The authors acknowledge that their approach does not have an explicit mechanism to control the diversity of prompts within each category. They suggest that incorporating instructions in the generation step or using more models for generation could potentially improve this.
- Why unresolved: The paper does not provide a concrete method for ensuring diversity in the generated prompts. It only suggests possible approaches without implementing or evaluating them.
- What evidence would resolve it: Implementation and evaluation of methods to increase prompt diversity, such as using more diverse few-shot examples, incorporating instructions to encourage varied perspectives, or employing multiple models in the generation process. Quantitative measures of prompt diversity and its impact on the benchmark's effectiveness in testing over-refusal.

### Open Question 3
- Question: What are the limitations and potential biases introduced by using LLM moderators for dataset creation?
- Basis in paper: The authors acknowledge that using LLM moderators introduces potential biases, especially towards the model families used as judges. They also note that some prompts in the dataset might be toxic but not identified by LLM moderators, and that categorizing certain ambiguous prompts is challenging for both humans and LLMs.
- Why unresolved: The paper does not provide a detailed analysis of the specific limitations and biases introduced by LLM moderation. It does not explore alternative moderation approaches or quantify the impact of these limitations on the benchmark's validity.
- What evidence would resolve it: Comparative studies between LLM moderation and human annotation on a subset of prompts, analysis of the types of prompts most likely to be misclassified by LLMs, exploration of alternative moderation methods (e.g., hybrid human-LLM approaches), and assessment of how these limitations affect the benchmark's ability to accurately measure over-refusal in different LLM families.

## Limitations

- The benchmark relies heavily on LLM moderators, which may introduce systematic biases toward specific model families
- The strong correlation between safety and over-refusal (0.878) suggests a fundamental tradeoff that may be difficult to overcome with current alignment techniques
- The effectiveness of the hard subset (1,000 prompts) in truly challenging advanced models remains to be validated across diverse model families

## Confidence

**High Confidence**: The existence of over-refusal as a phenomenon in current LLMs, supported by the strong correlation between safety and over-refusal across 25 models from 8 families.

**Medium Confidence**: The benchmark's comprehensiveness in covering 10 categories with 80K prompts, though the effectiveness of the hard subset in truly challenging models remains to be validated.

**Low Confidence**: The claim that no model can achieve both high safety and low over-refusal simultaneously, as this may change with new safety alignment techniques.

## Next Checks

1. **Cross-Model Consistency Test**: Evaluate the benchmark's effectiveness by testing models from different model families to verify the 0.878 correlation holds across diverse architectures.

2. **Human Validation Study**: Conduct a small-scale human annotation study on a subset of prompts to verify the accuracy of LLM moderators in distinguishing toxic from seemingly toxic content.

3. **Temporal Stability Analysis**: Re-run the benchmark on models released after the original study to assess whether the safety-over-refusal tradeoff persists or evolves with newer safety alignment techniques.