---
ver: rpa2
title: 'Can Small Language Models Help Large Language Models Reason Better?: LM-Guided
  Chain-of-Thought'
arxiv_id: '2404.03414'
source_url: https://arxiv.org/abs/2404.03414
tags:
- reasoning
- rationale
- answer
- prompting
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LM-Guided Chain-of-Thought (CoT), a framework
  that uses a small language model to generate rationales and a large frozen model
  to predict answers based on those rationales. The approach trains only the small
  model via knowledge distillation from the large model, followed by reinforcement
  learning to refine the rationales.
---

# Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought

## Quick Facts
- arXiv ID: 2404.03414
- Source URL: https://arxiv.org/abs/2404.03414
- Reference count: 0
- Key result: LM-guided Chain-of-Thought with knowledge distillation and self-consistency decoding outperforms standard CoT prompting on multi-hop QA benchmarks

## Executive Summary
This paper introduces LM-Guided Chain-of-Thought (CoT), a framework that uses a small language model to generate rationales and a large frozen model to predict answers based on those rationales. The approach trains only the small model via knowledge distillation from the large model, followed by reinforcement learning to refine the rationales. Experiments on multi-hop QA benchmarks show that LM-guided CoT with knowledge distillation and self-consistency decoding outperforms both standard and original CoT prompting in answer prediction accuracy. Reinforcement learning further improves rationale quality and task performance.

## Method Summary
The framework employs a two-stage approach: first, knowledge distillation transfers reasoning patterns from a large frozen model to a small trainable model by training the small model on rationales generated by the large model. Second, reinforcement learning fine-tunes the small model using multi-dimensional rewards based on 8 rationale quality metrics (factuality, relevance, logicality, etc.) and task performance. The small model generates rationales for new inputs, which are then passed to the large frozen model for answer prediction, creating a division of labor where each model specializes in its strengths.

## Key Results
- LM-guided CoT with knowledge distillation and self-consistency decoding outperforms standard and original CoT prompting on multi-hop QA benchmarks
- Reinforcement learning improves both rationale quality and task performance beyond knowledge distillation alone
- Separating rationale generation and answer prediction into two models allows specialized optimization of each component

## Why This Works (Mechanism)

### Mechanism 1
Knowledge distillation transfers reasoning patterns from the large model to the small model, compressing reasoning capability into a smaller parameter space. The large model's rationales contain valid reasoning patterns that can be learned by a smaller model, though if the large model generates poor quality rationales, the small model will inherit those errors.

### Mechanism 2
Reinforcement learning with multi-dimensional rationale quality metrics improves both reasoning quality and task performance through dual-objective optimization. The RL agent receives rewards for both rationale quality and task performance, though if the automatic metrics don't align with human judgment, RL optimization may improve metric scores while degrading actual reasoning quality.

### Mechanism 3
Separating rationale generation and answer prediction allows specialized optimization of each component. The small model focuses on generating high-quality reasoning chains while the large frozen model handles answer prediction, though if the rationale quality is too poor, the large model cannot recover to produce accurate answers.

## Foundational Learning

- **Knowledge Distillation**
  - Why needed here: Transfers reasoning capabilities from a large model to a smaller, more efficient model
  - Quick check question: What is the primary purpose of using knowledge distillation in this framework?

- **Reinforcement Learning with Multiple Rewards**
  - Why needed here: Balances improvement in both rationale quality and task performance through dual-objective optimization
  - Quick check question: Why does the framework use both rationale-oriented and task-oriented reward signals?

- **Chain-of-Thought Prompting**
  - Why needed here: Provides the foundational reasoning approach that the framework aims to improve and optimize
  - Quick check question: What limitation of conventional CoT prompting does this framework address?

## Architecture Onboarding

- **Component map**: Large Model (M_L) -> Knowledge Distillation -> Small Model (M_S) -> RL Optimization -> Small Model (M_S) -> Answer Prediction by M_L
- **Critical path**: Generate rationales from large model → Distill rationales to train small model → Apply RL to optimize small model → Use small model to generate rationales → Pass rationales to large model for answer prediction
- **Design tradeoffs**: Model size vs. performance (smaller model is more efficient but may produce lower quality rationales); reward complexity vs. training stability (more reward dimensions provide better optimization but may destabilize training); context length vs. accuracy (longer contexts improve reasoning but increase computational cost)
- **Failure signatures**: Low answer accuracy despite high rationale quality scores; degradation in performance when switching from CoT to LM-guided CoT; high variance in rewards during RL training; long inference times due to complex generation process
- **First 3 experiments**: 1) Baseline comparison of standard prompting, CoT prompting, and LM-guided CoT with only KD on a small dataset; 2) RL ablation study comparing LM-guided CoT with and without RL; 3) Rationale quality analysis measuring impact of different quality thresholds on answer accuracy

## Open Questions the Paper Calls Out

- What is the optimal balance between rationale quality and task performance when selecting rationales for answer prediction? The paper notes that selecting top-quality rationales doesn't always guarantee improved task performance and leaves this for future research.

- How generalizable is the LM-guided CoT approach to other reasoning tasks beyond multi-hop QA? The paper only explores multi-hop QA and acknowledges potential limitations in generalization.

- What is the impact of different large language models on the performance of LM-guided CoT? The paper experiments with limited model combinations and leaves exploration of different LLMs as future work.

## Limitations
- Lack of validation that automatic rationale quality metrics correlate with human judgments of reasoning quality
- Knowledge distillation effectiveness only demonstrated on filtered datasets, raising questions about real-world generalization
- RL training procedure's sensitivity to reward signal design and hyperparameter choices remains unclear

## Confidence
- **High Confidence**: LM-guided CoT with knowledge distillation outperforms standard and original CoT prompting on answer prediction accuracy (directly supported by experimental results)
- **Medium Confidence**: Reinforcement learning further improves rationale quality and task performance (supported by results but limited to automatic metrics without human validation)
- **Low Confidence**: Automatic evaluation metrics for rationale quality are meaningful indicators of actual reasoning quality (metrics established but no validation of correlation with human judgments)

## Next Checks
1. Conduct a blinded human evaluation comparing rationales generated by: (a) the small model before RL, (b) after RL optimization, and (c) direct large model outputs to test metric-human alignment.

2. Systematically vary the weights and presence of each of the 8 rationale quality metrics in the RL reward function to measure individual metric contributions and test for noisy or redundant metrics.

3. Evaluate the trained small model on out-of-distribution datasets or tasks not seen during training to test whether it has learned general reasoning capabilities or merely memorized training patterns.