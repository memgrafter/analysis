---
ver: rpa2
title: 'TSFM-Bench: A Comprehensive and Unified Benchmark of Foundation Models for
  Time Series Forecasting'
arxiv_id: '2410.11802'
source_url: https://arxiv.org/abs/2410.11802
tags:
- time
- series
- tsfms
- forecasting
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TSFM-Bench, a comprehensive benchmark for evaluating
  Time Series Foundation Models (TSFMs). The benchmark includes diverse datasets from
  multiple domains (electricity, traffic, energy, environment, economics, nature,
  health, stock, banking, and web) and covers both TS pre-trained models and LLM-based
  models.
---

# TSFM-Bench: A Comprehensive and Unified Benchmark of Foundation Models for Time Series Forecasting

## Quick Facts
- arXiv ID: 2410.11802
- Source URL: https://arxiv.org/abs/2410.11802
- Reference count: 40
- Primary result: TSFM-Bench reveals TS pre-trained models generally outperform LLM-based and specific models in zero-shot/few-shot scenarios, though no model dominates across all datasets

## Executive Summary
TSFM-Bench presents a comprehensive benchmark for evaluating Time Series Foundation Models (TSFMs) across diverse domains and evaluation scenarios. The benchmark includes 21 multivariate datasets spanning 10 domains (electricity, traffic, energy, environment, economics, nature, health, stock, banking, web) and covers TS pre-trained models, LLM-based models, and specific models. Through standardized experimental protocols, TSFM-Bench enables fair comparison across zero-shot, few-shot, and full-shot scenarios. The evaluation reveals that while TS pre-trained models demonstrate strong generalization capabilities, no single model dominates across all datasets, highlighting the need for continued research in handling diverse data characteristics and optimizing adaptation strategies.

## Method Summary
TSFM-Bench evaluates TSFMs using standardized protocols across three adaptation scenarios: zero-shot (test data only), few-shot (5% training data + full validation), and full-shot (full training + validation). The benchmark includes 21 multivariate time series datasets from 10 domains with varying characteristics including seasonality, trend, stationarity, transition, shifting, correlation, and non-Gaussianity. Models are evaluated using MAE and MSE metrics for prediction lengths of 24, 36, 48, 60 (short-term) and 96, 192, 336, 720 (long-term). The benchmark supports both TS pre-trained models and LLM-based models, with uniform lookback/prediction lengths, standardized data splitting/loading, and consistent sampling strategies across all models.

## Key Results
- TS pre-trained models generally outperform LLM-based models and specific models in zero-shot and few-shot scenarios
- No single model dominates across all datasets and characteristics
- Current TSFMs show promising potential but require improvements in handling diverse data characteristics
- Models exhibit varying performance across different data characteristics (seasonality, trend, correlation, etc.)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TSFM-Bench enables fair and comprehensive evaluation by standardizing experimental protocols across diverse datasets and models.
- Mechanism: The benchmark enforces uniform lookback/prediction lengths, standardized data splitting/loading, and consistent sampling strategies (e.g., 5% uniform window sampling) across all models, eliminating performance discrepancies caused by varying experimental setups.
- Core assumption: Different experimental settings significantly impact model performance, and standardization removes this confounding factor.
- Evidence anchors: [abstract] "TSFM-Bench supports zero-shot, few-shot, and full-shot evaluation scenarios with standardized experimental protocols for fair comparison." [section] "TSFM-Bench integrates multi-domain datasets with heterogeneous temporal characteristics to ensure the comprehensiveness of the assessment."

### Mechanism 2
- Claim: TSFM-Bench reveals the generalization capabilities of TSFMs by evaluating them across diverse domains and statistical characteristics.
- Mechanism: The benchmark includes 21 multivariate datasets spanning 10 domains with varying characteristics (seasonality, trend, stationarity, correlation, etc.), allowing assessment of how well models transfer knowledge across different time series properties.
- Core assumption: Diverse datasets with varying characteristics stress-test the generalization capabilities of foundation models.
- Evidence anchors: [section] "Our comprehensive analysis encompasses a diverse set of time series characteristics that capture various aspects of temporal dynamics, including seasonality, trend, stationarity, transition, shifting, correlation, and non-Gaussianity."

### Mechanism 3
- Claim: TSFM-Bench identifies inherent limitations of existing TSFMs by comparing their performance across zero-shot, few-shot, and full-shot scenarios.
- Mechanism: By evaluating models under different data availability conditions, the benchmark reveals whether models truly generalize (zero-shot), adapt efficiently with limited data (few-shot), or require extensive training (full-shot), exposing their strengths and weaknesses in different adaptation strategies.
- Core assumption: Performance differences across adaptation scenarios reveal fundamental model capabilities and limitations.
- Evidence anchors: [abstract] "TSFM-Bench supports multiple forecasting scenarios, including zero-shot, few-shot, and full-shot, enabling assessment across the full range of adaptation strategies."

## Foundational Learning

- Concept: Zero-shot, few-shot, and full-shot learning paradigms
  - Why needed here: These paradigms represent different levels of data availability and model adaptation requirements, crucial for understanding how TSFMs generalize versus adapt to specific tasks.
  - Quick check question: Can you explain the key difference between zero-shot and few-shot learning in the context of TSFMs?

- Concept: Time series characteristics (seasonality, trend, stationarity, correlation, etc.)
  - Why needed here: Understanding these characteristics is essential for interpreting model performance across diverse datasets and for designing models that can handle various temporal patterns.
  - Quick check question: How would you expect a model's performance to change when moving from highly seasonal to non-seasonal time series?

- Concept: Foundation model pre-training objectives (reconstruction, autoregressive, direct prediction, hybrid)
  - Why needed here: Different pre-training objectives lead to different model capabilities and limitations, which is crucial for understanding why certain models perform better in specific scenarios.
  - Quick check question: What type of pre-training objective would you expect to be most beneficial for zero-shot forecasting performance?

## Architecture Onboarding

- Component map: Data (21 datasets across 10 domains with diverse characteristics) -> Model (specific models, TS pre-trained models, LLM-based models) -> Evaluator (standardized protocols for zero-shot, few-shot, full-shot scenarios with consistent metrics)
- Critical path: For a new engineer, the critical path is: 1) Understand the benchmark structure and evaluation scenarios, 2) Run existing experiments to reproduce baseline results, 3) Add a new model to the benchmark following the standardized protocols, 4) Analyze performance across different scenarios and characteristics.
- Design tradeoffs: The benchmark prioritizes comprehensiveness and fairness over computational efficiency - including diverse datasets and multiple evaluation scenarios increases runtime but provides more complete insights into model capabilities.
- Failure signatures: Common failures include: 1) Models not following standardized preprocessing leading to unfair comparisons, 2) Inconsistent sampling strategies across models causing performance discrepancies, 3) Models failing to handle the full range of lookback/prediction lengths specified in the benchmark.
- First 3 experiments:
  1. Run the complete zero-shot evaluation for all TS pre-trained models on the ETTh1 dataset to verify the benchmark infrastructure and reproduce baseline results.
  2. Add a simple specific model (e.g., DLinear) to the few-shot evaluation pipeline and compare its performance against TS pre-trained models on the Electricity dataset.
  3. Conduct a correlation analysis by evaluating MOIRAI, Moment, iTransformer, and TimesNet on datasets with varying correlation strengths (e.g., Traffic vs. Weather) to reproduce the channel dependence analysis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural modifications would enable LLM-based models to better leverage their pre-training knowledge for time series forecasting tasks?
- Basis in paper: [explicit] The paper notes that LLM-based models perform worse than TS pre-trained models in zero-shot and few-shot scenarios, with their text-based pre-training knowledge showing limited utility for TSF tasks and sometimes interfering with reasoning.
- Why unresolved: The paper identifies the problem but doesn't propose specific architectural solutions for better knowledge transfer from text to time series domains.
- What evidence would resolve it: Empirical studies comparing different architectural designs (cross-modal attention, time series-specific prompt engineering, or knowledge distillation approaches) that improve LLM performance on TSF tasks.

### Open Question 2
- Question: How can Time Series Foundation Models be designed to maintain or improve performance when more training data becomes available?
- Basis in paper: [explicit] The paper observes that some TS pre-trained models show no significant improvement—or even decline—when fine-tuned on more training data, which contradicts the expected benefits of scaling laws.
- Why unresolved: The paper identifies this paradox but doesn't explore the underlying causes or potential solutions for better fine-tuning strategies.
- What evidence would resolve it: Systematic studies examining model behavior across different data regimes, including analysis of catastrophic forgetting, domain shift, or optimization challenges during fine-tuning.

### Open Question 3
- Question: What sampling strategies and ratios optimize few-shot learning performance for Time Series Foundation Models?
- Basis in paper: [explicit] The paper demonstrates that different sampling strategies (random, uniform, front-end, back-end) significantly impact model performance, yet standardized settings use only 5% uniform window sampling.
- Why unresolved: While the paper shows sampling matters, it doesn't comprehensively explore the optimal strategies or ratios for different model types and dataset characteristics.
- What evidence would resolve it: Extensive ablation studies comparing various sampling strategies, ratios, and their interactions with model architectures across diverse time series datasets.

## Limitations

- Implementation Details: The benchmark lacks detailed specifications for model architectures, hyperparameters, and training procedures, which could lead to implementation variations affecting reproducibility.
- Dataset Coverage: While comprehensive, certain domains (like industrial IoT or specific medical time series) are underrepresented, potentially missing real-world patterns.
- Computational Requirements: Comprehensive evaluation across all models, datasets, and scenarios requires significant computational resources without provided resource estimates.

## Confidence

- High Confidence: The core framework and evaluation methodology (standardized protocols, three adaptation scenarios, diverse dataset selection) are well-specified and technically sound.
- Medium Confidence: The claim that TS pre-trained models generally outperform other model types requires empirical verification across all datasets.
- Low Confidence: Claims about specific model capabilities are based on benchmark design rather than actual experimental results presented in the paper.

## Next Checks

1. **Implementation Verification**: Implement and run a subset of TSFMs (at least 3-4 models) on 2-3 datasets across all three scenarios to verify the benchmark infrastructure and reproduce baseline results.

2. **Cross-Domain Performance Analysis**: Systematically evaluate model performance across domains with varying characteristics to identify whether claimed generalization capabilities hold across different data types.

3. **Efficiency Evaluation**: Measure and compare computational requirements (training time, inference latency, memory usage) across models to assess whether performance gains justify efficiency costs in practical deployments.