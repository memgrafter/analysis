---
ver: rpa2
title: A Survey of Constraint Formulations in Safe Reinforcement Learning
arxiv_id: '2402.02025'
source_url: https://arxiv.org/abs/2402.02025
tags:
- safe
- problem
- safety
- constraint
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of constraint formulations
  in safe reinforcement learning (Safe RL), focusing on problems where an agent must
  maximize expected cumulative reward while satisfying safety constraints. The authors
  categorize seven common constraint formulations based on expected cumulative safety,
  state constraints, joint chance constraints, expected instantaneous safety with
  time-variant thresholds, and almost-sure safety constraints (both cumulative and
  instantaneous, with time-variant or invariant thresholds).
---

# A Survey of Constraint Formulations in Safe Reinforcement Learning

## Quick Facts
- arXiv ID: 2402.02025
- Source URL: https://arxiv.org/abs/2402.02025
- Reference count: 6
- This paper provides a comprehensive survey of constraint formulations in safe reinforcement learning (Safe RL), focusing on problems where an agent must maximize expected cumulative reward while satisfying safety constraints.

## Executive Summary
This paper provides a comprehensive survey of constraint formulations in safe reinforcement learning (Safe RL), organizing seven common formulations and elucidating their theoretical relationships. The authors introduce two key theoretical concepts—transformability and generalizability—to analyze how different constraint formulations relate to one another. They identify two "identical or more general safe RL" (IoMG-SafeRL) problems that can subsume others, providing guidance on algorithm selection based on safety requirements and application contexts. The paper also highlights the growing importance of offline Safe RL, where policies are trained from pre-collected data without environmental interaction, offering inherent safety during training.

## Method Summary
The paper reviews seven common constraint formulations in Safe RL, including expected cumulative safety, state constraints, joint chance constraints, expected instantaneous safety with time-variant thresholds, and almost-sure safety constraints. It introduces theoretical concepts of transformability and generalizability to analyze relationships among formulations. The authors categorize algorithms for each formulation and provide a mapping between problem types and representative methods. While the paper focuses on theoretical analysis rather than implementation details, it offers guidance for selecting appropriate algorithms based on safety requirements and application contexts.

## Key Results
- The survey organizes seven common constraint formulations in Safe RL and introduces transformability and generalizability concepts to analyze their mathematical relationships.
- Problems 4 and 7 are identified as IoMG-SafeRL problems that can subsume other formulations, simplifying algorithm selection.
- The paper provides a comprehensive review of algorithms for each formulation and highlights the growing importance of offline Safe RL for inherently safe training.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The survey clarifies the landscape of safe RL by organizing seven common constraint formulations and showing their theoretical relationships, enabling better algorithm selection.
- Mechanism: The paper introduces two key theoretical concepts—transformability and generalizability—which reveal that certain problems (like instantaneous constraints) can subsume others, simplifying the choice of algorithm based on safety requirements.
- Core assumption: Different constraint formulations can be mathematically related in ways that preserve or expand the feasible policy space.
- Evidence anchors:
  - [abstract] "we elucidate the theoretical underpinnings that reveal the mathematical mutual relations among common problem formulations."
  - [section] "We first introduce and define two important notions called transformability and generalizability."
  - [corpus] The survey paper explicitly connects formulations through theorems and lemmas, showing that Problems 4 and 7 are IoMG-SafeRL problems over others.
- Break condition: If the assumptions about mathematical equivalence or generalization do not hold for a given problem (e.g., due to specific dynamics or constraints), the transformability results may not apply.

### Mechanism 2
- Claim: The survey bridges the gap between problem formulation and algorithm selection by providing a clear mapping from constraint types to representative algorithms.
- Mechanism: By categorizing algorithms according to the seven formulations and summarizing their properties (e.g., optimality, safety guarantees), the paper helps practitioners choose methods that fit their safety needs.
- Core assumption: Each constraint formulation has a set of well-suited algorithms, and these can be matched to application contexts.
- Evidence anchors:
  - [abstract] "we provide a comprehensive review of representative constraint formulations, along with a curated selection of algorithms designed specifically for each formulation."
  - [section] "Crucially, we show that Problems 4 and 7 can be regarded as two IoMG-SafeRL problems of other problems."
  - [corpus] Table 1 maps problem types to representative algorithms and their properties, facilitating algorithm selection.
- Break condition: If an application's safety requirements are not well-captured by the surveyed formulations, or if new formulations arise, the mapping may become incomplete.

### Mechanism 3
- Claim: The survey highlights the growing importance of offline Safe RL, which inherently avoids safety risks during training by using pre-collected data.
- Mechanism: By discussing offline Safe RL settings and their associated algorithms, the paper points to a practical approach for real-world deployment where environmental interaction is risky or impossible.
- Core assumption: Offline RL can achieve comparable safety and performance to online methods when sufficient data is available.
- Evidence anchors:
  - [abstract] "Finally, it highlights the growing importance of offline Safe RL, where policies are trained from pre-collected data without environmental interaction, offering inherent safety during training."
  - [section] "Ofﬂine RL is well-suited in the context of safe RL because the agent does not interact with the environment during training; thus, the policy training does not essentially pose any risk."
  - [corpus] Several offline algorithms are listed in Table 1, indicating active research in this area.
- Break condition: If pre-collected data is insufficient or unrepresentative of the true environment, offline Safe RL may fail to generalize or meet safety constraints.

## Foundational Learning

- Concept: Constrained Markov Decision Processes (CMDPs)
  - Why needed here: Safe RL problems are modeled as CMDPs, extending standard MDPs with safety constraints; understanding CMDPs is essential for grasping the formulations.
  - Quick check question: What is the main difference between an MDP and a CMDP in the context of safe RL?

- Concept: Expected value vs. chance constraints
  - Why needed here: The survey distinguishes between formulations based on expected safety (Eπ) and those based on probabilistic (chance) constraints (Pπ), which have different safety guarantees and algorithm requirements.
  - Quick check question: How do expected cumulative safety constraints differ from almost-sure safety constraints in terms of the level of safety they provide?

- Concept: Lagrangian methods and primal-dual approaches
  - Why needed here: Many algorithms for safe RL, especially those based on expected constraints, use Lagrangian or primal-dual methods to handle constraints; familiarity with these is important for understanding algorithm design.
  - Quick check question: In what way do Lagrangian methods help solve constrained RL problems?

## Architecture Onboarding

- Component map: Problem formulation module (selecting from seven constraint types) -> Policy optimization module (choosing algorithm matched to formulation) -> Theoretical analysis module (using transformability and generalizability to relate formulations)
- Critical path: 1) Define the safety requirement (expected vs. chance, cumulative vs. instantaneous). 2) Choose the appropriate constraint formulation. 3) Select a matching algorithm. 4) Implement and test, checking safety and performance.
- Design tradeoffs: Expected constraint formulations are often easier to optimize but may allow unsafe behavior in rare cases; almost-sure formulations are stricter but harder to optimize. Online RL allows learning from scratch but risks safety during training; offline RL is safer during training but requires good data.
- Failure signatures: If safety constraints are violated, check if the formulation matches the safety requirement; if performance is poor, consider if a more general formulation (like Problems 4 or 7) could be used. If algorithms fail to converge, verify if the chosen method is appropriate for the formulation.
- First 3 experiments:
  1. Implement a simple environment (e.g., gridworld with unsafe states) and test an algorithm from Table 1 for Problem 2 (state constraint), verifying safety.
  2. Modify the environment to use cumulative safety costs and apply an algorithm for Problem 1, comparing performance and safety.
  3. Collect a small dataset and implement an offline algorithm for Problem 1, checking if safety is maintained without environment interaction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the fundamental theoretical relationships between the IoMG-SafeRL problems (Problems 4 and 7) and other constraint formulations that were not explored in this survey?
- Basis in paper: [explicit] The paper states that Problems 4 and 7 are "identical or more general safe RL (IoMG-SafeRL) problems" that can subsume others, but does not fully explore their relationships with all other formulations.
- Why unresolved: The paper focuses on transformability and generalizability but does not exhaustively map all possible theoretical connections between IoMG-SafeRL problems and other formulations.
- What evidence would resolve it: A complete theoretical framework showing how all common safe RL formulations can be transformed into or generalized by Problems 4 and 7, including formal proofs and counterexamples.

### Open Question 2
- Question: How do the theoretical guarantees of optimality and safety differ between online and offline safe RL algorithms for each constraint formulation?
- Basis in paper: [explicit] The paper discusses both online and offline RL settings but notes that most theoretical guarantees are based on Problem 1, with limited analysis of other formulations.
- Why unresolved: The paper identifies this as a gap but does not provide a comprehensive comparison of theoretical guarantees across all formulations and settings.
- What evidence would resolve it: Empirical and theoretical studies comparing convergence rates, safety guarantees, and reward performance across all constraint formulations in both online and offline settings.

### Open Question 3
- Question: What are the practical implications of using instantaneous safety constraints (Problems 4, 6, 7) versus cumulative safety constraints (Problems 1, 2, 5) in real-world applications?
- Basis in paper: [inferred] The paper suggests that instantaneous constraints are "easier to handle than cumulative ones both theoretically and empirically" but does not provide concrete evidence or case studies.
- Why unresolved: The paper highlights this as a promising direction but lacks empirical validation or real-world application examples.
- What evidence would resolve it: Case studies or benchmark comparisons showing the trade-offs in safety, reward performance, and computational efficiency between instantaneous and cumulative constraint formulations in practical scenarios.

## Limitations
- The survey focuses primarily on theoretical analysis without providing specific implementation details or empirical evaluations.
- The paper does not address scalability to high-dimensional or continuous state-action spaces, which is critical for real-world applications.
- The effectiveness of transformability and generalizability results may depend on problem-specific dynamics, with limited guidance on when these theoretical results may not hold.

## Confidence
- High Confidence: The categorization of seven common constraint formulations and the introduction of transformability and generalizability as theoretical concepts are well-supported by the literature and are clearly explained in the paper.
- Medium Confidence: The mapping of algorithms to formulations and the guidance on algorithm selection are reasonable but may not cover all practical scenarios, as the survey does not provide empirical evidence or detailed implementation guidance.
- Low Confidence: The claim that offline Safe RL inherently offers safety during training is contingent on the quality and representativeness of pre-collected data, which is not fully addressed in the survey.

## Next Checks
1. Implement and test a subset of algorithms from Table 1 on a standard Safe RL benchmark (e.g., Safety Gym) to verify that the mapping from formulations to algorithms holds in practice and that safety constraints are respected.

2. Analyze the transformability and generalizability results for specific problem instances (e.g., gridworld or cartpole with safety constraints) to confirm that the theoretical relationships identified in the survey apply in concrete settings.

3. Conduct experiments to assess the robustness of offline Safe RL algorithms when trained on limited or biased datasets, evaluating whether safety is maintained and performance is comparable to online methods.