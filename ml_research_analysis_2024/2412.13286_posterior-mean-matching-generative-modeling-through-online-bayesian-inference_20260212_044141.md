---
ver: rpa2
title: 'Posterior Mean Matching: Generative Modeling through Online Bayesian Inference'
arxiv_id: '2412.13286'
source_url: https://arxiv.org/abs/2412.13286
tags:
- bayesian
- posterior
- mean
- generative
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Posterior mean matching (PMM) is a new method for generative modeling
  that uses online Bayesian inference with conjugate pairs of distributions. The method
  iteratively refines noisy approximations of the target distribution by computing
  posterior means from augmented Bayesian models.
---

# Posterior Mean Matching: Generative Modeling through Online Bayesian Inference

## Quick Facts
- arXiv ID: 2412.13286
- Source URL: https://arxiv.org/abs/2412.13286
- Authors: Sebastian Salazar; Michal Kucer; Yixin Wang; Emily Casleton; David Blei
- Reference count: 40
- One-line primary result: PMM achieves FID of 2.18 on CIFAR-10 and competitive BPC on text8

## Executive Summary
Posterior Mean Matching (PMM) is a novel generative modeling approach that leverages online Bayesian inference with conjugate distributions to iteratively refine noisy approximations of target distributions. The method connects to diffusion models through a continuous-time stochastic differential equation formulation, providing a Bayesian interpretation of denoising generative processes. PMM demonstrates competitive performance on image generation tasks (CIFAR-10, FFHQ-64, AFHQv2-64) and text generation (text8, OpenWebText), achieving strong Frechet Inception Distance scores and bits per character metrics.

## Method Summary
PMM iteratively refines noisy samples toward the true data distribution using online Bayesian inference updates. For each data point, PMM computes posterior means from an augmented Bayesian model with a known conjugate prior, using these as intermediate targets. A neural network approximates these posterior mean updates, and the model is trained to minimize the KL divergence between the true and approximated posterior mean distributions. The method supports various data types through different conjugate pairs (Normal-Normal for real-valued data, Gamma-Poisson for counts, Dirichlet-Categorical for discrete data) and connects to diffusion models through a continuous-time SDE formulation.

## Key Results
- PMM achieves FID of 2.18 on CIFAR-10, competitive with diffusion models
- Text8 experiments show BPC of 1.29, outperforming several non-autoregressive baselines
- Demonstrates effectiveness across multiple conjugate pairs (Normal-Normal, Gamma-Poisson, Dirichlet-Categorical)
- Establishes theoretical connection between PMM and diffusion models via SDE formulation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PMM uses conjugate Bayesian models to iteratively refine noisy samples toward the true data distribution
- Mechanism: Given a noisy observation model and an augmented Bayesian model with a known conjugate prior, PMM applies online Bayesian inference to update posterior means. Each update step pulls the current approximation closer to the true data sample
- Core assumption: The posterior mean of the augmented Bayesian model converges to the true sample (consistency property)
- Evidence anchors: [abstract] "PMM models iteratively refine noisy approximations of the target distribution using updates from online Bayesian inference." [section 2] "For all s ∈ {0, ..., t − 1}, it is easy to calculate µµµs+1 from µµµs and yyys+1 using online Bayesian inference."

### Mechanism 2
- Claim: The PMM objective approximates the joint distribution of posterior means to enable sampling from the target distribution
- Mechanism: PMM introduces a neural network to approximate the intractable posterior mean updates. The PMM objective (KL divergence) is minimized to learn this approximation, enabling sample generation
- Core assumption: The learned neural network can approximate the true posterior mean dynamics well enough for sampling
- Evidence anchors: [section 2] "We approximate p(µµµ1, . . . , µµµt) by introducing a family of distributions qφφφ(µµµ1, . . . , µµµt) and minimizing the following objective function: LPMM(φφφ) = KL(p(µµµ1, . . . , µµµt)∥qφφφ(µµµ1, . . . , µµµt))."

### Mechanism 3
- Claim: PMM connects to diffusion models through continuous-time formulation as a stochastic differential equation (SDE)
- Mechanism: The discrete online Bayesian updates converge to an SDE in the continuum limit. This connection allows using numerical SDE solvers for sampling and explains PMM's denoising behavior
- Core assumption: The discrete-time posterior mean updates can be accurately approximated by an SDE in the continuum limit
- Evidence anchors: [section 4] "Specifically, the Bayesian update in the Normal-Normal PMM model can be interpreted as a discrete-time step in a type of diffusion process... Theorem 2 establishes this technical connection."

## Foundational Learning

- Concept: Conjugate Bayesian models and posterior computation
  - Why needed here: PMM relies on conjugate pairs to have closed-form posterior means for online updates
  - Quick check question: Given a Normal-Normal model with prior N(0, β^-1I) and likelihood N(xxx, α^-1I), what is the posterior mean?

- Concept: Online Bayesian inference and posterior consistency
  - Why needed here: PMM's effectiveness depends on the posterior mean converging to the true sample as more observations are incorporated
  - Quick check question: What condition must the noise precision sequence {αt} satisfy for posterior consistency in the Normal-Normal model?

- Concept: Stochastic differential equations and numerical solvers
  - Why needed here: The continuous-time formulation of PMM connects to SDEs, allowing the use of established numerical methods for sampling
  - Quick check question: What numerical method is mentioned in the paper for sampling from the PMM SDE?

## Architecture Onboarding

- Component map: Data preprocessing -> Noise model -> Augmented Bayesian model -> Neural network -> Training loop -> Sampling procedure

- Critical path: 1. Define data representation and conjugate pair 2. Implement noise model and augmented Bayesian model 3. Derive online Bayesian update rule 4. Implement neural network for approximation 5. Train on PMM objective 6. Generate samples using learned model

- Design tradeoffs:
  - Choice of conjugate pair: Affects data types supported and computational complexity
  - Noise schedule: Controls information content in noisy observations and training dynamics
  - Neural network architecture: Balances expressivity and computational efficiency
  - Training objective: Reweighted vs. original affects gradient magnitudes and convergence

- Failure signatures:
  - Poor sample quality: Neural network approximation insufficient or training unstable
  - Mode collapse: Noise model or prior too restrictive, limiting diversity
  - Slow convergence: Learning rate too low or noise schedule poorly chosen
  - Numerical instability: SDE solver steps too large or discretization inappropriate

- First 3 experiments:
  1. Implement Normal-Normal PMM on synthetic 1D Gaussian data, verify posterior consistency
  2. Train Normal PMM on CIFAR-10, compare FID scores to baseline diffusion models
  3. Implement Dirichlet-Categorical PMM on text8, evaluate bits per character (BPC) against autoregressive baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PMM models scale with increasing dataset size and dimensionality?
- Basis in paper: [inferred] The paper demonstrates PMM performance on CIFAR-10, FFHQ-64, AFHQv2-64, and text datasets, but does not explore scaling behavior with larger datasets or higher-dimensional data
- Why unresolved: The experiments are limited to specific benchmark datasets, and the paper does not analyze how PMM's computational complexity or sample quality changes with dataset size or dimensionality
- What evidence would resolve it: Experiments on larger-scale datasets (e.g., ImageNet, larger text corpora) with varying dimensionality, analyzing both computational efficiency and sample quality metrics like FID or perplexity

### Open Question 2
- Question: What are the theoretical guarantees for PMM convergence when using non-conjugate priors or non-exponential family distributions?
- Basis in paper: [explicit] The paper states "Generalizing Posterior Mean Matching to situations where the posterior mean is not available in closed form represents an exciting avenue for future work"
- Why unresolved: The paper only provides consistency proofs for conjugate exponential family models, leaving open the question of convergence properties for more general cases
- What evidence would resolve it: Mathematical proofs establishing convergence properties for PMM with non-conjugate priors or non-exponential family distributions, or empirical demonstrations of successful applications in such cases

### Open Question 3
- Question: How does the choice of noise schedule (αt) affect PMM's sample quality and training dynamics?
- Basis in paper: [explicit] The paper mentions using exponential schedules for αt's in noise models and discusses the reweighted PMM loss, but does not systematically study the impact of different noise schedules
- Why unresolved: While the paper provides some experimental results with specific schedules, it does not analyze how different choices of αt affect convergence, sample quality, or computational efficiency
- What evidence would resolve it: Systematic experiments comparing different noise schedules (e.g., linear, cosine, step-wise) on the same tasks, analyzing their effects on FID scores, training stability, and sample diversity

### Open Question 4
- Question: What are the advantages and limitations of PMM compared to other non-autoregressive generative models for text, such as Masked Language Models or discrete diffusion models?
- Basis in paper: [explicit] The paper compares PMM's BPC on text8 and generative perplexity on OpenWebText to other non-autoregressive models, but does not provide a comprehensive analysis of its strengths and weaknesses relative to these approaches
- Why unresolved: The comparison is limited to specific metrics and does not explore other aspects like computational efficiency, scalability, or sample diversity
- What evidence would resolve it: A detailed comparative study of PMM against other non-autoregressive text generation models, evaluating factors such as training time, inference speed, sample quality across different metrics, and robustness to hyperparameter choices

## Limitations

- Limited empirical validation across diverse datasets and model configurations
- Computational efficiency and scalability relative to established generative models not thoroughly analyzed
- Theoretical guarantees only established for conjugate exponential family models

## Confidence

**High Confidence:**
- The theoretical foundation connecting PMM to diffusion models through SDE formulation is well-established and mathematically rigorous
- The mechanism of using conjugate Bayesian models for online inference is clearly defined and implementable

**Medium Confidence:**
- The experimental results demonstrating competitive performance on CIFAR-10 and text8 are promising but limited in scope
- The claim that PMM can be applied to various data types using different conjugate pairs is theoretically sound but requires broader empirical validation

**Low Confidence:**
- The scalability and computational efficiency of PMM relative to established generative models like diffusion models and GANs are not thoroughly explored
- The robustness of PMM across diverse data distributions and real-world applications remains uncertain

## Next Checks

1. **Broader Empirical Evaluation**: Extend experiments to include more diverse datasets (e.g., ImageNet, CelebA) and compare PMM against a wider range of generative models (e.g., GANs, VAEs, normalizing flows)

2. **Computational Efficiency Analysis**: Conduct a detailed analysis of PMM's computational requirements, including training time, memory usage, and sampling speed, compared to diffusion models and other generative approaches

3. **Robustness Testing**: Evaluate PMM's performance under varying noise schedules, conjugate pair choices, and data distributions to assess its robustness and generalization capabilities