---
ver: rpa2
title: 'From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment
  Items'
arxiv_id: '2410.14897'
source_url: https://arxiv.org/abs/2410.14897
tags:
- items
- llms
- gen-copa
- premise
- copa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether LLMs can author items in the style
  of the COPA commonsense reasoning benchmark. The authors prompt various LLMs to
  generate COPA-style items, then evaluate the generated items using human annotation
  and the LLMs' own ability to answer them.
---

# From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items

## Quick Facts
- arXiv ID: 2410.14897
- Source URL: https://arxiv.org/abs/2410.14897
- Reference count: 7
- Key outcome: LLMs that perform well on COPA benchmarks generate more valid and higher-quality items and are more consistent in answering their own generated items

## Executive Summary
This paper investigates whether large language models (LLMs) can author items in the style of the COPA commonsense reasoning benchmark. The authors prompt various LLMs to generate COPA-style items, then evaluate the generated items using human annotation and the LLMs' own ability to answer them. They find that LLMs that perform well on the original COPA benchmark also tend to generate more valid and higher-quality items, and are more consistent in answering their own generated items. The study suggests that LLM success in answering commonsense benchmarks is associated with their ability to author similar items, and that consistency in answering their own generated items may indicate model capability.

## Method Summary
The study uses 11 open-source LLMs to generate COPA-style items using few-shot prompting with exemplars from the COPA development set. Each LLM generates 500 items per direction (cause-to-effect and effect-to-cause) using top-p sampling. Generated items are parsed and evaluated for validity by human consensus annotation. The LLMs then answer both their own generated items and the original COPA items using greedy decoding. Key metrics include consistency (answering accuracy on own generated items), validity (proportion of items where human raters agree with LLM's designated answer), and correlation with original COPA performance.

## Key Results
- LLMs that perform well on original COPA also generate more valid COPA-style items (rs = .87, p < .001)
- Consistency in answering own generated items strongly correlates with original COPA performance (rs = .97, p < .001)
- Valid Gen-COPA items tend to be easier for LLMs to answer than Orig-COPA items, suggesting potential "model collapse" effect

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs that perform well on COPA benchmarks are more capable of authoring valid COPA-style items.
- Mechanism: Strong causal reasoning ability in LLMs enables them to generate premises and alternatives that maintain the causal plausibility structure required by COPA.
- Core assumption: COPA task performance is a valid proxy for underlying commonsense reasoning capabilities that transfer to item generation.
- Evidence anchors:
  - [abstract] "We find that LLMs that succeed in answering the original COPA benchmark are also more successful in authoring their own items."
  - [section 6.2] "Validity is strongly correlated with answering accuracy on Orig-COPA (rs = .87, p < .001)."
- Break condition: If COPA performance reflects pattern matching rather than genuine reasoning, the correlation may not hold for novel item types.

### Mechanism 2
- Claim: LLMs demonstrate consistency in their reasoning when generating and answering their own items.
- Mechanism: The same internal representations and reasoning processes used to generate an item are accessed when the LLM answers it, leading to self-consistency.
- Core assumption: LLMs maintain stable internal state representations across generation and answering tasks.
- Evidence anchors:
  - [section 6.1] "Consistency is associated with answering accuracy on Orig-COPA... extremely strong correlation... rs = .97, p < .001."
  - [section 6.3] "LLMs do not necessarily answer their own Gen-COPA items consistently even if the items are valid."
- Break condition: If generation and answering use different internal pathways or if the LLM's internal state shifts between tasks.

### Mechanism 3
- Claim: Valid Gen-COPA items are easier for LLMs to answer than Orig-COPA items, suggesting a potential "model collapse" effect.
- Mechanism: Generated items may reflect the most common patterns in the LLM's training data, creating a simplified subset of the original distribution.
- Core assumption: LLMs trained on web data learn to reproduce common patterns, and generated data reflects these common patterns rather than the full distribution.
- Evidence anchors:
  - [section 6.3] "Accuracy on the valid Gen-COPA sets tends to be higher for all LLMs compared with their accuracy on Orig-COPA."
  - [section 6.3] "This suggests that the valid subset of Gen-COPA items is easier to answer than Orig-COPA."
- Break condition: If generated items accidentally capture rare but distinctive patterns that make them harder rather than easier to answer.

## Foundational Learning

- Concept: Few-shot prompting with exemplars
  - Why needed here: The study uses 3-4 exemplar items to guide LLM generation and answering, which is essential for task specification
  - Quick check question: What is the difference between 3-shot and 4-shot prompting in this study, and why might more exemplars improve performance?

- Concept: Causal reasoning in natural language
  - Why needed here: COPA requires understanding cause-effect relationships, which is central to both answering and generating items
  - Quick check question: How does the study distinguish between semantic relatedness and causal relatedness in the generated items?

- Concept: Human evaluation of generated content
  - Why needed here: Validity and quality assessments rely on human raters to judge whether generated items meet COPA standards
  - Quick check question: What criteria did human raters use to determine if a Gen-COPA item was valid or high-quality?

## Architecture Onboarding

- Component map: LLM inference endpoint -> prompt template engine -> output parser -> validation pipeline -> human annotation interface
- Critical path: Prompt generation -> LLM inference -> Output parsing -> Consistency check -> Human validation
- Design tradeoffs: Using random sampling (top-p=0.9) for generation increases diversity but may reduce validity; using greedy decoding for answering ensures determinism but may miss valid alternatives
- Failure signatures: Generation failures (exact template matching or duplicate alternatives), consistency failures (answering own items incorrectly), validity failures (human raters disagree with mpa designation)
- First 3 experiments:
  1. Test different prompt templates with varying numbers of exemplars to optimize generation quality
  2. Compare consistency rates across different decoding strategies (greedy vs top-p sampling)
  3. Evaluate whether fine-tuning on COPA data improves generation validity compared to few-shot prompting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the consistency of LLMs in answering their own generated items vary across different types of commonsense reasoning tasks beyond COPA?
- Basis in paper: [explicit] The paper mentions that an LLM's consistency in answering its own generated questions may be a broad indicator of its abilities, warranting further analysis with other tasks beyond COPA.
- Why unresolved: The study only examined one specific benchmark (COPA), so it's unclear whether the consistency findings generalize to other types of commonsense reasoning tasks or benchmarks.
- What evidence would resolve it: Testing the same methodology with other benchmarks like SocialIQA, CommonsenseQA, or physical reasoning tasks would show if consistency is a general indicator of LLM capability across different commonsense domains.

### Open Question 2
- Question: What specific aspects of LLM architecture or training data contribute to differences in their ability to generate valid and high-quality commonsense reasoning items?
- Basis in paper: [inferred] The paper observes significant variation in performance between different LLM families (BLOOM vs LLAMA 2 vs others) but doesn't explain the underlying reasons for these differences in terms of architecture or training data.
- Why unresolved: While the paper identifies which models perform better, it doesn't analyze why certain models excel at both answering and generating commonsense items compared to others with different architectures or training approaches.
- What evidence would resolve it: Detailed analysis comparing the training data composition, model architecture features, and parameter counts of high-performing vs low-performing models would reveal which factors most strongly predict success in commonsense item generation.

### Open Question 3
- Question: How does the quality of LLM-generated training data affect the performance of models trained on that synthetic data compared to models trained on original human-authored data?
- Basis in paper: [explicit] The paper notes that LLMs' answering accuracy on valid Gen-COPA sets tends to be higher than on Orig-COPA, suggesting the valid subset of Gen-COPA items is easier to answer, which "may reflect the mechanism behind model collapse."
- Why unresolved: The paper observes this potential model collapse effect but doesn't investigate how training on LLM-generated data compares to training on human-authored data in terms of downstream task performance.
- What evidence would resolve it: Training multiple models on different proportions of human-authored vs LLM-generated data and comparing their performance on held-out human-authored test sets would quantify the impact of synthetic data on model capabilities.

## Limitations

- The study focuses exclusively on COPA-style items, which may not generalize to other commonsense reasoning tasks with different structures
- Human annotation introduces potential subjectivity, though mitigated by consensus scoring across multiple raters
- The finding that consistency correlates strongly with original COPA performance may reflect shared task familiarity rather than genuine reasoning transfer

## Confidence

- **High**: Core finding that high-performing LLMs generate more valid COPA items (rs = .87, p < .001)
- **Medium**: Mechanism explaining "model collapse" due to limited investigation of underlying causes
- **High**: Strong correlation between consistency and original COPA performance (rs = .97, p < .001)

## Next Checks

1. Test whether the correlation between original COPA performance and generation validity holds for other commonsense reasoning benchmarks like Winograd Schema or SocialIQA to assess generalizability.

2. Conduct ablation studies varying the number of exemplar prompts (1-shot through 5-shot) to determine if the observed effects depend on specific few-shot configurations.

3. Compare consistency rates when LLMs answer items generated by different models versus their own items to isolate self-consistency effects from general reasoning capability.