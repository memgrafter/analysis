---
ver: rpa2
title: Dynamic Vocabulary Pruning in Early-Exit LLMs
arxiv_id: '2410.18952'
source_url: https://arxiv.org/abs/2410.18952
tags:
- vocabulary
- confidence
- pruning
- token
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles the high computational cost of confidence estimation
  in early-exit large language models (LLMs), which arises from the need to map hidden
  states to large vocabularies at each candidate exit. The proposed solution, Dynamic
  Vocabulary Pruning (DVP), addresses this by computing the full vocabulary mapping
  only at early layers (e.g., p=2), identifying the top-K most likely tokens, and
  then pruning the unembedding matrix to use this reduced vocabulary for subsequent
  confidence estimations.
---

# Dynamic Vocabulary Pruning in Early-Exit LLMs

## Quick Facts
- arXiv ID: 2410.18952
- Source URL: https://arxiv.org/abs/2410.18952
- Authors: Jort Vincenti; Karim Abdel Sadek; Joan Velja; Matteo Nulli; Metod Jazbec
- Reference count: 23
- One-line primary result: DVP reduces FLOPs by ~7x and speeds up confidence computation by >17% on T5-large without finetuning

## Executive Summary
This paper addresses the computational inefficiency of confidence estimation in early-exit large language models, where mapping hidden states to large vocabularies at each candidate exit layer creates significant overhead. The proposed Dynamic Vocabulary Pruning (DVP) approach computes the full vocabulary mapping only at early layers, identifies the top-K most likely tokens, and then prunes the unembedding matrix to use this reduced vocabulary for subsequent confidence estimations. Experiments on T5-large demonstrate that DVP achieves competitive performance (F1=90.6 on SQuAD, Rouge-L=43.1 on SamSum) while dramatically reducing computational costs.

## Method Summary
The paper proposes Dynamic Vocabulary Pruning (DVP) to improve efficiency of confidence estimation in early-exit LLMs. DVP works by computing the full vocabulary mapping only at an early layer (p), selecting the top-K most likely tokens, and then using a pruned unembedding matrix containing only these K tokens for all subsequent confidence estimations. This approach reduces the computational cost of softmax-based confidence measures without requiring additional model parameters or finetuning. The method is evaluated on T5-large using SQuAD and SamSum datasets, comparing against the CALM baseline with full softmax confidence estimation.

## Key Results
- DVP reduces FLOPs per token by approximately 7x compared to full softmax confidence estimation
- DVP speeds up confidence computation by over 17% while maintaining competitive performance (F1=90.6 on SQuAD, Rouge-L=43.1 on SamSum)
- The approach requires no additional model parameters or finetuning
- Dynamic pruning restores efficiency gains that are otherwise negated by expensive confidence estimation in early-exiting models

## Why This Works (Mechanism)

### Mechanism 1
Early token distributions are highly predictive of the final token, so pruning the vocabulary after a small number of layers preserves accuracy. At layer p, compute full logits over the vocabulary, select the top K tokens, and use only those for subsequent confidence estimation. Since the final token is typically already among the top K tokens early in the forward pass, the pruned set captures the correct prediction while drastically reducing computation. Core assumption: The rank of the final predicted token improves rapidly in the first few layers, and selecting K=64 (SQuAD) or K=512 (SamSum) is sufficient to contain the correct token with high probability.

### Mechanism 2
Confidence estimation cost is dominated by the unembedding matrix multiplication, so reducing the vocabulary size cuts FLOPs dramatically. By selecting only K rows from the unembedding matrix after layer p, each subsequent confidence computation requires O(K·d_model) operations instead of O(d_vocab·d_model). This yields roughly a 7x reduction in FLOPs per token. Core assumption: The unembedding matrix is the primary computational bottleneck for softmax-based confidence measures.

### Mechanism 3
Early-exiting without pruning suffers from high confidence estimation cost, negating efficiency gains from fewer layers. Standard early-exiting reuses the full unembedding matrix at every exit, so even though fewer layers are processed on average, the cost of confidence estimation offsets the savings. Dynamic pruning eliminates this overhead, restoring the efficiency benefit. Core assumption: The CALM baseline is representative of the inefficiency of softmax-based confidence estimation in early-exit models.

## Foundational Learning

- Concept: Transformer decoder architecture with unembedding matrix projection
  - Why needed here: The pruning operation modifies the unembedding matrix, so understanding how logits are produced from hidden states is essential
  - Quick check question: In a transformer decoder, what operation produces the logits over the vocabulary from the hidden state at a given layer?

- Concept: Softmax-based confidence measures and their computational cost
  - Why needed here: The paper targets softmax-based confidence estimation as the expensive component; understanding why it is costly is key to grasping the pruning motivation
  - Quick check question: Why does computing confidence as max(softmax(logits)) require O(d_vocab) operations at every exit?

- Concept: Early-exiting mechanism and threshold-based layer skipping
  - Why needed here: Dynamic pruning is integrated into the early-exit framework, so understanding how exits are triggered is necessary to see where pruning fits
  - Quick check question: In early-exiting, what determines whether the model stops at a given layer or continues to the next?

## Architecture Onboarding

- Component map: Input sequence → Transformer layers → Hidden states at each layer → Full vocabulary mapping (only at p) → Top-K selection → Pruned unembedding matrix (used at subsequent layers) → Confidence scores → Exit decision
- Critical path: Forward pass through layers → At p: full vocab mapping → Top-K selection → Use pruned matrix for all later confidence estimations
- Design tradeoffs: Larger K increases accuracy but reduces computational savings; smaller p reduces computation but risks missing the final token in early rankings; static vs. dynamic thresholds affect exit behavior
- Failure signatures: If accuracy drops, likely K is too small or p is too late; if no speedup, likely pruning is not applied or K is too large; if instability in exit timing, likely pruning changes confidence calibration
- First 3 experiments:
  1. Run baseline CALM with full softmax confidence on SQuAD; record FLOPs and accuracy
  2. Apply DVP with p=2, K=64 on same data; compare FLOPs, accuracy, and exit layer distribution
  3. Sweep p∈{1,2,3} and K∈{32,64,128} to find the Pareto frontier of accuracy vs. efficiency

## Open Questions the Paper Calls Out

- Question: What is the optimal selection strategy for the pruning layer p and vocabulary size K that balances computational efficiency with minimal performance degradation?
  - Basis in paper: [explicit] The authors suggest using a small calibration dataset to find the smallest values for p and K that maintain negligible performance drops, but note that more principled selection mechanisms could be explored in future work
  - Why unresolved: The paper only explores specific values (p=2, K=64 for SQuAD and K=512 for SamSum) without a systematic study of how different combinations affect performance and efficiency
  - What evidence would resolve it: Systematic experiments varying p and K across different datasets and tasks, ideally with a principled selection algorithm that optimizes the trade-off between efficiency gains and performance loss

- Question: How does dynamic vocabulary pruning affect the confidence calibration of early-exit LLMs?
  - Basis in paper: [inferred] The authors mention in the conclusion that future work could investigate the impact of dynamic vocabulary pruning on confidence calibration, suggesting this has not been studied
  - Why unresolved: The paper focuses on F1 and ROUGE scores but does not examine whether pruning affects the reliability of confidence scores or the relationship between predicted confidence and actual correctness
  - What evidence would resolve it: Experiments measuring calibration metrics (ECE, MCE) before and after applying DVP, potentially showing whether pruning introduces overconfidence or underconfidence in predictions

- Question: How does dynamic vocabulary pruning perform on other early-exit architectures beyond CALM, such as those using different confidence measures or exit mechanisms?
  - Basis in paper: [explicit] The authors state in the conclusion that future work should validate their approach on other early-exit LLMs and explore more advanced pruning mechanisms
  - Why unresolved: The experiments are limited to one specific early-exit model (CALM), and the paper acknowledges that generalization to other architectures remains untested
  - What evidence would resolve it: Applying DVP to other early-exit models like FastBERT, Shallow-Deep Networks, or models using hidden-state-based confidence measures, comparing performance and efficiency gains across architectures

## Limitations

- The optimal values for pruning layer p and vocabulary size K appear empirically chosen rather than theoretically justified, limiting generalizability across different tasks and vocabulary sizes
- The approach's effectiveness is demonstrated only on two specific datasets (SQuAD and SamSum) using T5-large, with no extensive ablation studies or error analysis
- The claim that early-exiting without pruning is "less efficient" than static models is based on a single baseline and may not generalize to all early-exit architectures

## Confidence

- **High Confidence**: The mechanism by which DVP reduces FLOPs through vocabulary pruning is well-established. The mathematical relationship between vocabulary size and computational cost in softmax operations is straightforward.
- **Medium Confidence**: The claim that DVP achieves "competitive performance" (F1=90.6 on SQuAD, Rouge-L=43.1 on SamSum) is supported by experimental results but lacks extensive ablation studies and error analysis.
- **Low Confidence**: The assertion that early-exiting without pruning is "less efficient" than static models is based on a single baseline (CALM) and specific computational assumptions.

## Next Checks

1. **Cross-vocabulary generalization test**: Apply DVP to T5 models with different vocabulary sizes (e.g., T5-small with 32K vocabulary vs T5-3B with 32K vocabulary) and measure whether the same K values maintain performance, or if K should scale with vocabulary size.

2. **Failure mode analysis**: Systematically identify and categorize cases where the final token is not in the top-K at layer p, examining whether these failures correlate with specific token types (rare words, proper nouns) or task characteristics.

3. **Confidence calibration validation**: Measure whether DVP alters the relationship between confidence scores and prediction accuracy, potentially requiring recalibration of confidence thresholds for reliable early-exit decisions.