---
ver: rpa2
title: 'DenoMamba: A fused state-space model for low-dose CT denoising'
arxiv_id: '2409.13094'
source_url: https://arxiv.org/abs/2409.13094
tags:
- denomamba
- dose
- denoising
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Low-dose CT imaging reduces radiation exposure but introduces noise
  that degrades diagnostic image quality. The challenge is to develop a denoising
  method that effectively suppresses noise while preserving fine anatomical details,
  particularly in regions with heterogeneous tissue composition.
---

# DenoMamba: A fused state-space model for low-dose CT denoising

## Quick Facts
- arXiv ID: 2409.13094
- Source URL: https://arxiv.org/abs/2409.13094
- Reference count: 40
- Outperforms state-of-the-art LDCT denoisers by 1.4-1.6dB PSNR, 1.1-1.7% SSIM, and 1.6-2.6% RMSE

## Executive Summary
DenoMamba introduces a novel state-space modeling (SSM) approach for low-dose CT (LDCT) denoising that captures both spatial and channel context without downsampling. The method employs a hierarchical hourglass architecture with encoder-decoder stages, where each stage consists of multiple FuseSSM blocks that jointly model spatial and channel dependencies. Comprehensive experiments on AAPM datasets demonstrate significant improvements over CNN, transformer, and SSM baselines, achieving superior noise suppression while preserving fine anatomical details in heterogeneous tissue regions.

## Method Summary
DenoMamba uses a hierarchical hourglass encoder-decoder architecture with K=4 stages, where each stage contains multiple FuseSSM blocks. These blocks process input features through three parallel pathways: spatial SSM (captures spatial dependencies), channel SSM (with gated convolution for channel dependencies), and identity path (preserves low-level features). The outputs are fused via convolutional fusion modules. The model is trained on AAPM 2016 Low Dose CT Grand Challenge dataset with 25% and 10% dose reduction levels using pixel-wise ℓ1-loss and Adam optimizer.

## Key Results
- Achieves 1.4-1.6dB PSNR improvement over state-of-the-art denoisers on AAPM datasets
- Demonstrates 1.1-1.7% SSIM enhancement in recovered image quality
- Shows robust generalization across different anatomical datasets and noise levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FuseSSM blocks achieve superior denoising by jointly modeling spatial and channel context without downsampling
- Mechanism: Each FuseSSM block splits input into three parallel pathways (spatial SSM, channel SSM, identity path) that are fused via convolutional fusion modules, enabling multi-scale feature extraction within each block
- Core assumption: Spatial and channel context in CT images are complementary and both critical for denoising performance
- Evidence anchors:
  - [abstract]: "spatial SSM module to encode spatial context and a novel channel SSM module equipped with a secondary gated convolution network to encode channel context"
  - [section]: "The proposed FuseSSM blocks convolutionally fuse the spatial context captured by a spatial SSM module with the channel context captured by a novel channel SSM module"
  - [corpus]: Weak - no direct mentions of DenoMamba's fused architecture or spatial/channel SSM design

### Mechanism 2
- Claim: Channel SSM module's secondary gated convolution network extracts higher-order channel context features
- Mechanism: After SSM layer processes transposed feature map across channel dimension, gated convolution network refines these features to produce richer representation of channel-wise interdependencies
- Core assumption: Higher-order features of channel context are necessary for distinguishing signal from noise in CT images
- Evidence anchors:
  - [abstract]: "novel channel SSM module equipped with a secondary gated convolution network to encode channel context"
  - [section]: "To further extract higher-order latent features, this initial set is projected through a gated convolutional network"
  - [corpus]: Missing - no corpus entries mention gated convolution in SSM modules

### Mechanism 3
- Claim: Hourglass encoder-decoder with skip connections preserves both low-level and high-level features for better reconstruction
- Mechanism: Encoder stages progressively downsample spatial resolution while increasing channel dimensionality, decoder stages upsample and combine with encoder features via skip connections for multi-scale feature extraction
- Core assumption: Multi-scale feature extraction is crucial for LDCT denoising as noise affects different spatial frequencies differently
- Evidence anchors:
  - [abstract]: "Following an hourglass architecture with encoder-decoder stages"
  - [section]: "Starting from the noisy LDCT image x taken as model input, encoder stages serve to extract latent contextualized representations via FuseSSM blocks and to resample the feature map dimensions"
  - [corpus]: Weak - no direct mention of hourglass architecture in related works

## Foundational Learning

- Concept: State-space modeling (SSM) for sequence processing
  - Why needed here: SSMs offer linear complexity with respect to sequence length while capturing long-range dependencies, ideal for CT image denoising where context matters across entire image
  - Quick check question: How does an SSM layer process a sequence differently from a transformer's self-attention?

- Concept: Convolutional fusion for multi-path feature integration
  - Why needed here: Simple addition of features from different pathways may lose information; convolutional fusion learns optimal weighting and combination of spatial, channel, and identity features
  - Quick check question: What's the difference between element-wise addition and convolutional fusion when combining feature maps?

- Concept: Gated convolutions for feature modulation
  - Why needed here: Gating allows network to learn which channel context features are most relevant for denoising, preventing less useful features from dominating
  - Quick check question: How does a gated convolution differ from a standard convolution in terms of information flow?

## Architecture Onboarding

- Component map: Input → Encoder (4 stages, each with [4,6,6,8] FuseSSM blocks) → Decoder (4 stages, each with [6,6,4,2] FuseSSM blocks) → Output
- Critical path: Noisy LDCT image → Encoder stages (feature extraction and downsampling) → Decoder stages (feature refinement and upsampling) → Denoised output
- Design tradeoffs: FuseSSM blocks increase model complexity per stage but eliminate need for downsampling/patching, preserving spatial precision; channel SSM adds computational overhead but captures critical channel-wise dependencies
- Failure signatures: Poor denoising near heterogeneous tissue regions suggests inadequate spatial/channel context modeling; over-smoothing indicates excessive feature fusion; residual noise patterns suggest insufficient denoising capacity
- First 3 experiments:
  1. Replace FuseSSM blocks with plain convolutional blocks to measure performance drop from losing SSM benefits
  2. Remove the channel SSM module to quantify importance of channel context modeling
  3. Remove the identity path to assess impact of losing low-level feature preservation

## Open Questions the Paper Calls Out

- Question: How does DenoMamba perform on other types of medical imaging data beyond CT, such as MRI or ultrasound, particularly under varying noise conditions?
  - Basis in paper: [inferred] The paper demonstrates DenoMamba's effectiveness on CT data but does not explore its performance on other imaging modalities
  - Why unresolved: The study is focused on CT denoising, and there is no mention of testing the model on other types of medical imaging data
  - What evidence would resolve it: Conducting experiments on MRI and ultrasound data with varying noise levels would provide insights into the model's versatility and effectiveness across different imaging modalities

- Question: What is the impact of incorporating adversarial or score-based loss terms on DenoMamba's performance and reliability in LDCT denoising?
  - Basis in paper: [explicit] The paper mentions that more sophisticated loss terms, including adversarial or score-based losses, might improve image quality but are not explored in the current study
  - Why unresolved: The study uses a simple pixel-wise loss term and does not evaluate the effects of more complex loss functions
  - What evidence would resolve it: Implementing and comparing DenoMamba with adversarial or score-based loss terms against the current model would reveal any performance and reliability improvements

- Question: How does DenoMamba handle varying levels of dose reduction in real-time clinical settings, and what are the computational requirements for such adaptability?
  - Basis in paper: [explicit] The paper suggests that DenoMamba could be trained on LDCT images at varying reduction levels for adaptability, but real-time performance and computational demands are not discussed
  - Why unresolved: The study does not address real-time adaptability or the computational resources needed for dynamic dose reduction handling
  - What evidence would resolve it: Testing DenoMamba in a real-time clinical environment with varying dose levels and measuring computational efficiency would provide insights into its practical applicability and resource requirements

## Limitations
- Relies on assumptions about spatial and channel context complementarity without rigorous testing through ablation studies
- Generalization claims across anatomical datasets need stronger statistical validation beyond two datasets
- Channel SSM's gated convolution mechanism lacks direct evidence of contribution to performance gains

## Confidence
- High confidence: Core architecture design (hourglass encoder-decoder with FuseSSM blocks) and training methodology (ℓ1-loss, Adam optimizer with specified parameters)
- Medium confidence: Quantitative improvements over baselines (1.4-1.6dB PSNR gain) based on reported metrics, though reproducibility depends on implementation details
- Low confidence: Claims about mechanism superiority without ablation studies, and generalization performance across diverse anatomical regions

## Next Checks
1. Ablation study: Remove the channel SSM module and identity path separately to quantify their individual contributions to denoising performance, particularly in heterogeneous tissue regions
2. Cross-dataset generalization: Test DenoMamba on additional LDCT datasets (e.g., Mayo LDCT, real-world ultra-low-dose CT) to validate robust generalization claims beyond the two current datasets
3. Computational efficiency analysis: Compare DenoMamba's parameter count and inference time against transformer and CNN baselines to verify the claimed computational efficiency advantage of the SSM-based approach