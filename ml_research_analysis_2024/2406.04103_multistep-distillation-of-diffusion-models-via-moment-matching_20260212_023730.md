---
ver: rpa2
title: Multistep Distillation of Diffusion Models via Moment Matching
arxiv_id: '2406.04103'
source_url: https://arxiv.org/abs/2406.04103
tags:
- matching
- diffusion
- sampling
- moment
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the computational cost of diffusion models
  by proposing a distillation method that reduces the number of sampling steps required
  while maintaining or improving output quality. The core idea is to train a faster
  "student" model by matching conditional expectations of clean data given noisy data
  along the sampling trajectory, using either alternating optimization with an auxiliary
  model or a two-batch parameter-space approach.
---

# Multistep Distillation of Diffusion Models via Moment Matching

## Quick Facts
- arXiv ID: 2406.04103
- Source URL: https://arxiv.org/abs/2406.04103
- Reference count: 17
- Distilled diffusion models achieve 8-step sampling with FID 1.24 (64x64) and 1.49 (128x128) on ImageNet

## Executive Summary
This paper introduces a novel distillation method for diffusion models that significantly reduces the number of sampling steps required while maintaining or improving output quality. The approach trains a faster "student" model by matching conditional expectations of clean data given noisy data along the sampling trajectory. Unlike previous work that focused on single-step generation, this method enables multistep generation by training the student to replicate the teacher's behavior at each intermediate timestep. Experiments demonstrate that 8-step distilled models outperform both their 1-step counterparts and the original many-step teacher models on ImageNet, achieving new state-of-the-art FID scores.

## Method Summary
The core innovation is a distillation framework that trains a student diffusion model to match the conditional expectations of the teacher model at each step of the sampling process. This is achieved through moment matching, where the student learns to predict the same intermediate representations as the teacher. The training employs either alternating optimization with an auxiliary model or a two-batch parameter-space approach. By focusing on matching expectations rather than individual samples, the method captures the statistical properties of the teacher's output distribution more effectively. The distillation process is applied iteratively across multiple timesteps, allowing the student to generate high-quality samples in far fewer steps than the original teacher model.

## Key Results
- 8-step distilled models achieve FID 1.24 on ImageNet 64x64 and 1.49 on 128x128
- Distilled models outperform both 1-step and many-step teacher models
- State-of-the-art results compared to existing distillation methods
- Quality improvements maintained across different model architectures

## Why This Works (Mechanism)
The method works by training the student to match the teacher's conditional expectations at each timestep, rather than just matching final outputs. This moment matching approach captures the statistical properties of the teacher's output distribution more effectively than traditional distillation methods. By focusing on intermediate representations along the sampling trajectory, the student learns to generate samples that follow the same probabilistic path as the teacher, even with fewer steps. The alternating optimization and parameter-space approaches provide different ways to align the student's behavior with the teacher's, with the choice depending on computational constraints and training stability.

## Foundational Learning
- Diffusion models and sampling: Why needed - Understanding the base model architecture and generation process; Quick check - Can explain DDIM and ancestral sampling
- Knowledge distillation principles: Why needed - Provides context for how student-teacher training works; Quick check - Can describe standard distillation vs moment matching
- Conditional expectations in probability: Why needed - Core mathematical concept behind the moment matching objective; Quick check - Can derive E[x|y] for Gaussian distributions
- Optimization techniques for generative models: Why needed - Relevant for understanding training stability and convergence; Quick check - Can explain why alternating optimization might help

## Architecture Onboarding
Component map: Noisy input -> Student model -> Conditional expectations -> Moment matching loss -> Teacher model comparison
Critical path: Forward pass through student model → compute conditional expectations → calculate moment matching loss → backpropagate
Design tradeoffs: Moment matching vs sample-level matching, computational cost of auxiliary model vs parameter-space optimization
Failure signatures: Mode collapse if student overfits to teacher modes, training instability with aggressive moment matching
First experiments: 1) Verify moment matching loss decreases during training 2) Compare student outputs to teacher at intermediate timesteps 3) Measure FID improvement as steps increase

## Open Questions the Paper Calls Out
None

## Limitations
- Results heavily dependent on specific hyperparameters and may not generalize across architectures
- Additional computational overhead during training not thoroughly quantified against inference gains
- Assumption about moment matching capturing sufficient information may not hold for complex multimodal distributions

## Confidence
High confidence in technical formulation and alternating optimization framework
Medium confidence in experimental results due to limited dataset diversity
Low confidence in claims about arbitrary architecture compatibility

## Next Checks
1. Replicate distillation on diverse datasets beyond ImageNet including natural images, medical imaging, and synthetic data
2. Implement method on alternative diffusion model architectures (DDPM++, Latent Diffusion) to verify compatibility
3. Conduct ablation studies isolating contributions of auxiliary model, parameter-space optimization, and sampling schedule components