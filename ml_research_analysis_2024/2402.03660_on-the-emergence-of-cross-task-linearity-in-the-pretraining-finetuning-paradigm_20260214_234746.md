---
ver: rpa2
title: On the Emergence of Cross-Task Linearity in the Pretraining-Finetuning Paradigm
arxiv_id: '2402.03660'
source_url: https://arxiv.org/abs/2402.03660
tags:
- block
- cosine
- finetuned
- coef
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a novel linear phenomenon in models initialized
  from a common pretrained checkpoint and fine-tuned on different tasks, termed Cross-Task
  Linearity (CTL). Specifically, if we linearly interpolate the weights of two fine-tuned
  models, the features in the weight-interpolated model are approximately equal to
  the linear interpolation of features in the two fine-tuned models at each layer.
---

# On the Emergence of Cross-Task Linearity in the Pretraining-Finetuning Paradigm

## Quick Facts
- arXiv ID: 2402.03660
- Source URL: https://arxiv.org/abs/2402.03660
- Authors: Zhanpeng Zhou; Zijun Chen; Yilan Chen; Bo Zhang; Junchi Yan
- Reference count: 40
- Primary result: Identifies Cross-Task Linearity (CTL) - linear interpolation of fine-tuned model weights produces approximately linear interpolation of features at each layer.

## Executive Summary
This paper identifies a novel linear phenomenon called Cross-Task Linearity (CTL) that emerges when models initialized from a common pretrained checkpoint are fine-tuned on different tasks. Specifically, if we linearly interpolate the weights of two fine-tuned models, the features in the weight-interpolated model are approximately equal to the linear interpolation of features in the two fine-tuned models at each layer. The authors provide comprehensive empirical evidence supporting that CTL consistently occurs across a wide range of settings and architectures. They conjecture that neural networks function as linear maps from parameter space to feature space in the pretraining-finetuning paradigm, and use this insight to explain model merging/editing techniques.

## Method Summary
The paper investigates Cross-Task Linearity by fine-tuning multiple models on different tasks from the same pretrained checkpoint, then comparing features in weight-interpolated models to linearly interpolated features from individual models. For reproduction, one would fine-tune multiple models on different tasks using the same pretrained checkpoint, compute cosine similarity between features in weight-interpolated models and linear interpolations of features in individual models at each layer, and analyze whether features in the weight-interpolated model are approximately equal to the linear interpolation of features in the individual models.

## Key Results
- Cross-Task Linearity (CTL) consistently occurs for fine-tuned models starting from the same pretrained checkpoint across various architectures and tasks
- CTL enables linear interpretation of model averaging and task arithmetic operations through feature-space transformations
- Pretraining creates a smooth, nearly linear landscape in the parameter-feature mapping that persists through fine-tuning on different tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CTL occurs because neural networks function as approximately linear maps from parameter space to feature space in the pretraining-finetuning paradigm
- Mechanism: During pretraining, models acquire common knowledge that creates a smooth, nearly linear landscape in the parameter-feature mapping. When fine-tuning on different tasks from the same checkpoint, the resulting models inherit this linearity
- Core assumption: The pretraining stage creates a sufficiently flat and linear parameter-feature mapping that persists through fine-tuning on different tasks
- Evidence anchors: abstract mentions networks function as linear maps; section 4.1 conjectures neural networks function as linear maps in pretraining-finetuning paradigm

### Mechanism 2
- Claim: CTL enables model averaging to work because features in averaged models approximate averaged features from individual models
- Mechanism: When models fine-tuned on the same dataset are averaged, the resulting model's features at each layer are approximately equal to the average of features from the individual models due to CTL
- Core assumption: CTL holds for models fine-tuned on the same dataset with different hyperparameters
- Evidence anchors: section 4.2 discovers features in model averaging can be approximated by averaging of features in each individual fine-tuned model

### Mechanism 3
- Claim: CTL explains task arithmetic operations by translating parameter-space operations into feature-space operations
- Mechanism: Addition of task vectors in parameter space corresponds to feature aggregation in feature space, while negation corresponds to feature subtraction
- Core assumption: CTL holds for edited models created via task arithmetic operations
- Evidence anchors: section 4.3 aims to explain effectiveness of task arithmetic from feature-learning perspective

## Foundational Learning

- Concept: Linear Mode Connectivity (LMC)
  - Why needed here: Understanding LMC is crucial because CTL is defined as a stronger notion of linearity than LMC, and the paper contrasts CTL with LMC to highlight its novelty
  - Quick check question: What is the key difference between LMC and CTL in terms of what they require to hold (loss landscape vs. feature space)?

- Concept: Layerwise Linear Feature Connectivity (LLFC)
  - Why needed here: LLFC is the intermediate concept between LMC and CTL, and understanding it helps grasp why CTL is considered a stronger notion of linearity
  - Quick check question: How does LLFC differ from CTL in terms of the relationship between interpolated features and linearly interpolated features?

- Concept: Pretraining-finetuning paradigm
  - Why needed here: The entire paper's findings are specific to this paradigm, and understanding it is essential to grasp why CTL emerges in this context but not others
  - Quick check question: Why is the pretraining-finetuning paradigm particularly conducive to the emergence of CTL compared to other training paradigms?

## Architecture Onboarding

- Component map: Pretrained models (CLIP, T5, ViT) -> Fine-tuned models on downstream tasks -> Evaluation datasets. The system uses different architectures (MLP, ResNet, ViT, T5) across different task types (image classification, NLP).

- Critical path: 1) Pretrain model on large dataset. 2) Fine-tune multiple copies on different tasks from the same checkpoint. 3) Evaluate CTL by measuring cosine similarity between features in weight-interpolated models and linearly interpolated features. 4) Analyze implications for model merging/editing techniques.

- Design tradeoffs: The paper trades off theoretical rigor for empirical comprehensiveness, providing extensive experimental validation across multiple architectures and tasks rather than a complete theoretical framework.

- Failure signatures: CTL may fail if: 1) Models are trained from scratch without pretraining. 2) Pretraining does not provide common knowledge. 3) Tasks are too dissimilar. 4) Models are not fine-tuned sufficiently. 5) The architecture does not support the required feature linearity.

- First 3 experiments:
  1. Verify CTL on simple MLP models fine-tuned on rotated MNIST tasks, measuring cosine similarity between interpolated features and linearly interpolated features at each layer
  2. Test CTL for model averaging by comparing features in averaged models to averaged features from individual models on CIFAR-10 with ViT-B/32
  3. Validate task arithmetic operations by examining whether addition of task vectors in parameter space corresponds to feature aggregation in feature space for ViT models on image classification tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical basis for Cross-Task Linearity (CTL) in the pretraining-finetuning paradigm?
- Basis in paper: The paper provides a preliminary theoretical attempt to prove CTL, showing that it is related to the flatness of the network landscape and the distance between the weights of two finetuned models
- Why unresolved: The theoretical proof is limited and doesn't fully explain the underlying mechanisms of CTL
- What evidence would resolve it: A comprehensive theoretical framework that explains the emergence of CTL, including the role of pretraining, the properties of the loss landscape, and the impact of network architecture

### Open Question 2
- Question: How does CTL extend to models finetuned on more than two tasks?
- Basis in paper: The paper mentions that CTL can be generalized to multiple models in the pretraining-finetuning paradigm
- Why unresolved: The paper only provides a theoretical lemma for the generalization of CTL to multiple models, but doesn't explore the practical implications or limitations
- What evidence would resolve it: Empirical studies that validate the generalization of CTL to multiple models with different numbers of tasks and task combinations

### Open Question 3
- Question: How does CTL affect the interpretability of finetuned models?
- Basis in paper: The paper discusses how CTL can be used to explain model averaging and task arithmetic techniques
- Why unresolved: The paper doesn't provide a comprehensive analysis of how CTL can be used to understand internal representations, transfer of knowledge between tasks, or robustness
- What evidence would resolve it: Studies that use CTL to analyze internal representations, compare interpretability of models with and without CTL, and investigate impact on model robustness

## Limitations

- Theoretical foundation: CTL is presented as a conjecture rather than a proven theorem, with limited theoretical justification for why pretraining creates linearity
- Task diversity constraints: The paper doesn't thoroughly characterize behavior with highly dissimilar or multimodal tasks or systematically explore boundaries of task similarity
- Architecture generalizability: The paper doesn't establish whether CTL holds for all neural network architectures or only those with specific properties

## Confidence

- High confidence: The empirical evidence for CTL's existence in fine-tuned models from common checkpoints is robust, with extensive experiments across multiple architectures, datasets, and task types
- Medium confidence: The explanation that CTL enables model averaging and task arithmetic through feature-space operations is supported by evidence but relies on assumptions about feature linearity
- Low confidence: The theoretical claim that pretraining creates a "smooth, nearly linear landscape" in the parameter-feature mapping is more of a post-hoc explanation than a predictive theory

## Next Checks

1. **Boundary conditions test**: Systematically vary task similarity (using metrics like task taxonomy distance or performance correlation) to identify the threshold where CTL breaks down, establishing concrete limits on when the phenomenon applies

2. **Architecture ablation study**: Test CTL on architectures systematically designed to lack properties of successful architectures (e.g., remove batch normalization, residual connections, layer normalization) to identify which architectural features are necessary for CTL emergence

3. **Pretraining objective comparison**: Train models with identical architectures but different pretraining objectives (language modeling, contrastive learning, autoencoder) and measure CTL strength to determine whether pretraining creates a universal linear mapping or objective-specific patterns