---
ver: rpa2
title: 'DiTMoS: Delving into Diverse Tiny-Model Selection on Microcontrollers'
arxiv_id: '2403.09035'
source_url: https://arxiv.org/abs/2403.09035
tags:
- classifiers
- selector
- training
- ditmos
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiTMoS, a novel framework for efficient and
  accurate deep neural network inference on microcontrollers by constructing and improving
  small/weak models. The key idea is leveraging model diversity, where a composition
  of weak models can significantly boost accuracy upper bound.
---

# DiTMoS: Delving into Diverse Tiny-Model Selection on Microcontrollers

## Quick Facts
- arXiv ID: 2403.09035
- Source URL: https://arxiv.org/abs/2403.09035
- Reference count: 40
- Primary result: Achieves up to 13.4% accuracy improvement over baseline while maintaining similar memory usage and latency

## Executive Summary
DiTMoS introduces a novel framework for efficient deep neural network inference on microcontrollers by constructing and improving small/weak models. The key innovation leverages model diversity, where a composition of weak models can significantly boost accuracy upper bounds. The framework employs a selector-classifiers architecture that routes each input sample to the appropriate classifier, with three key strategies: diverse training data splitting, adversarial selector-classifiers training, and heterogeneous feature aggregation.

## Method Summary
DiTMoS is built around a selector-classifiers architecture where a selector component routes input samples to specialized weak classifiers. The framework introduces three main strategies: first, diverse training data splitting to increase classifier diversity; second, adversarial training between selectors and classifiers to maximize complementarity; and third, heterogeneous feature aggregation to improve classifier capacity. A network slicing technique is proposed to manage memory overhead on resource-constrained microcontrollers.

## Key Results
- Achieves up to 13.4% accuracy improvement compared to the best baseline
- Maintains similar memory usage and latency overhead as baseline models
- Demonstrates effectiveness across three different time-series datasets

## Why This Works (Mechanism)
The framework works by exploiting the diversity among weak classifiers to achieve better overall performance than any single strong model. The selector acts as a routing mechanism that intelligently assigns inputs to the most suitable classifier based on learned patterns. Adversarial training ensures that selectors and classifiers evolve to be complementary rather than redundant, while heterogeneous feature aggregation allows each classifier to specialize on different aspects of the input space.

## Foundational Learning
- **Selector-classifiers architecture**: A two-level system where a selector routes inputs to specialized classifiers; needed to leverage model diversity, quick check: verify selector accuracy is above random guessing
- **Adversarial training**: Training selectors and classifiers to be complementary rather than redundant; needed to prevent model collapse, quick check: measure inter-classifier correlation
- **Network slicing**: Technique to partition models across memory constraints; needed for microcontroller deployment, quick check: verify memory usage stays within device limits
- **Heterogeneous feature aggregation**: Combining different feature representations across classifiers; needed to improve individual classifier capacity, quick check: compare with homogeneous aggregation
- **Diverse training data splitting**: Partitioning training data to create diverse classifiers; needed to maximize diversity benefits, quick check: measure classifier disagreement on validation set

## Architecture Onboarding
- **Component map**: Input -> Selector -> [Classifier1 | Classifier2 | ...] -> Output
- **Critical path**: Input → Selector (forward pass) → Classifier selection → Chosen classifier (forward pass) → Output
- **Design tradeoffs**: Higher diversity improves accuracy but increases memory overhead; adversarial training improves complementarity but adds training complexity; network slicing saves memory but may add latency
- **Failure signatures**: Poor selector accuracy (inputs routed to wrong classifiers), high inter-classifier correlation (lack of diversity), memory overflow (inadequate network slicing)
- **First experiments**: 1) Test selector accuracy in isolation, 2) Measure classifier diversity through disagreement metrics, 3) Profile memory usage across different network slicing configurations

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to only three time-series datasets, limiting generalizability
- Memory overhead trade-offs between selector and classifiers across different hardware platforms not thoroughly quantified
- Performance on extremely resource-constrained microcontrollers (e.g., <64KB SRAM) remains unexplored

## Confidence
- **High Confidence**: The core premise that diverse weak models can outperform single strong models in accuracy, and the selector-classifiers architecture design
- **Medium Confidence**: The three proposed strategies show promise but lack extensive ablation studies to isolate individual contributions
- **Medium Confidence**: The claimed memory efficiency gains through network slicing are demonstrated but not comprehensively validated across different microcontroller specifications

## Next Checks
1. Test DiTMoS on non-time-series datasets (e.g., image classification on CIFAR-10) to assess cross-domain applicability
2. Conduct memory profiling on multiple microcontroller architectures with varying RAM/flash constraints to quantify actual resource utilization
3. Perform ablation studies to isolate the contribution of each strategy (diverse training, adversarial training, feature aggregation) to the overall performance gain