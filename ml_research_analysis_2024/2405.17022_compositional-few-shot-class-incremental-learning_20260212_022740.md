---
ver: rpa2
title: Compositional Few-Shot Class-Incremental Learning
arxiv_id: '2405.17022'
source_url: https://arxiv.org/abs/2405.17022
tags:
- primitives
- learning
- primitive
- classes
- compositional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a compositional approach to few-shot class-incremental
  learning (FSCIL) by defining and learning visual primitives shared across classes.
  The method uses Centered Kernel Alignment (CKA) to measure primitive set similarity,
  enabling effective training and evaluation through primitive composition.
---

# Compositional Few-Shot Class-Incremental Learning

## Quick Facts
- arXiv ID: 2405.17022
- Source URL: https://arxiv.org/abs/2405.17022
- Reference count: 20
- Authors: Yixiong Zou; Shanghang Zhang; Haichen Zhou; Yuhua Li; Ruixuan Li
- Key outcome: Proposes compositional approach using visual primitives for FSCIL, achieving up to 2.0% accuracy improvement over state-of-the-art while reducing catastrophic forgetting

## Executive Summary
This paper addresses the challenge of few-shot class-incremental learning (FSCIL) by introducing a compositional approach that leverages visual primitives shared across classes. The method uses Centered Kernel Alignment (CKA) to measure primitive set similarity, enabling effective training and evaluation through primitive composition. A primitive reuse module enhances reusability by replacing primitives with closest ones from other classes. The approach demonstrates superior performance on three datasets compared to state-of-the-art methods while providing interpretability through visualization of learned primitives and their composition.

## Method Summary
The proposed method tackles FSCIL by defining and learning visual primitives that are shared across classes. CKA is employed to measure the similarity between primitive sets, which guides the training and evaluation process through primitive composition. The primitive reuse module further enhances the system by replacing existing primitives with the most similar ones from other classes, promoting reusability. This compositional framework addresses catastrophic forgetting while improving accuracy and interpretability in FSCIL scenarios.

## Key Results
- Achieves up to 2.0% accuracy improvement over state-of-the-art approaches on the last session
- Reduces catastrophic forgetting while maintaining high performance across incremental sessions
- Demonstrates superior interpretability through visualization of learned primitives and their composition
- Outperforms existing methods on three benchmark datasets

## Why This Works (Mechanism)
The method works by decomposing complex visual concepts into shared primitive components that can be reused across different classes. By measuring primitive set similarity using CKA, the system can effectively compose new classes from existing primitives without forgetting previously learned concepts. The primitive reuse module ensures that the most relevant and reusable primitives are prioritized, creating a more efficient and adaptable representation. This compositional approach allows the model to leverage knowledge from previous sessions while learning new classes with minimal data.

## Foundational Learning
- **Centered Kernel Alignment (CKA)**: A similarity metric for comparing representations; needed for measuring primitive set similarity; quick check: compare CKA scores between different layers or models
- **Catastrophic forgetting**: The tendency of neural networks to forget previously learned information; critical context for incremental learning; quick check: track performance on old classes as new ones are learned
- **Visual primitives**: Basic visual elements that compose complex concepts; needed for compositional representation; quick check: visualize learned primitives to verify they capture meaningful features
- **Few-shot learning**: Learning from very limited examples; needed for incremental scenarios; quick check: measure performance with varying shot counts
- **Class-incremental learning**: Learning new classes over time without revisiting old ones; needed for real-world deployment; quick check: evaluate performance across multiple incremental sessions

## Architecture Onboarding

**Component map**: Input images -> Feature extractor -> Primitive detector -> Primitive composition module -> Classification head -> Output predictions

**Critical path**: During inference, images flow through the feature extractor to identify relevant primitives, which are then composed according to learned relationships to generate class predictions for both known and novel classes.

**Design tradeoffs**: The method trades increased model complexity (primitive composition module) for improved reusability and reduced forgetting. While this adds computational overhead during training, it enables more efficient learning of new classes and better retention of old ones.

**Failure signatures**: Performance degradation may occur when primitive composition fails to capture novel class variations, when CKA similarity measures are unreliable for certain classes, or when the primitive reuse module incorrectly replaces essential primitives with less relevant ones.

**First experiments**: 1) Compare primitive composition accuracy against non-compositional baselines on standard FSCIL benchmarks. 2) Evaluate catastrophic forgetting by measuring performance on old classes after learning new ones. 3) Visualize learned primitives to verify they capture semantically meaningful visual elements.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation scope limited to only three datasets, potentially missing diverse FSCIL scenarios
- Computational overhead of the primitive reuse module not discussed, raising scalability concerns
- Sensitivity to CKA parameter choices and number of primitives per class not thoroughly explored
- Potential biases in primitive composition and robustness to domain shifts not addressed

## Confidence
- High: Method's effectiveness in reducing catastrophic forgetting and improving accuracy is well-supported by quantitative results
- Medium: Interpretability of learned primitives demonstrated qualitatively but lacks rigorous quantitative validation
- Low: Scalability and computational efficiency of primitive reuse module insufficiently addressed

## Next Checks
1. Test the method on additional datasets with varying characteristics (e.g., fine-grained vs. coarse-grained) to assess generalizability
2. Conduct ablation studies to quantify the impact of the primitive reuse module on computational overhead and accuracy
3. Evaluate robustness to domain shifts by testing on cross-domain FSCIL scenarios