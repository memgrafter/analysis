---
ver: rpa2
title: 'Bypassing the Popularity Bias: Repurposing Models for Better Long-Tail Recommendation'
arxiv_id: '2410.02776'
source_url: https://arxiv.org/abs/2410.02776
tags:
- item
- items
- user
- users
- invr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of popularity bias in recommender
  systems, which leads to insufficient exposure for high-quality long-tail content
  publishers. The authors propose a novel approach called Inverse Retrieval (InvR),
  which repurposes an existing two-tower retrieval model to recommend users for each
  item rather than items for each user.
---

# Bypassing the Popularity Bias: Repurposing Models for Better Long-Tail Recommendation

## Quick Facts
- arXiv ID: 2410.02776
- Source URL: https://arxiv.org/abs/2410.02776
- Reference count: 17
- Key outcome: Inverse Retrieval (InvR) achieves 33.3% increase in bottom 50% share and 45% increase in percentage of sufficiently exposed items compared to baseline

## Executive Summary
This paper addresses the pervasive problem of popularity bias in recommender systems, where high-quality long-tail content publishers receive insufficient exposure. The authors propose a novel approach called Inverse Retrieval (InvR) that repurposes an existing two-tower retrieval model to recommend users for each item rather than items for each user. Through large-scale online A/B experiments with multiple variants, the InvR approach demonstrates significant improvements in long-tail content exposure while maintaining recommendation quality. The method has been successfully deployed in production, showing long-term effectiveness and positive impacts on both publishers and the platform.

## Method Summary
The Inverse Retrieval approach works by generating user embeddings from interaction histories and using item embeddings as queries to retrieve relevant users. This inverted retrieval process focuses on item-user relevance rather than item popularity. The method involves periodically recomputing item-to-users candidates offline and inserting them into the recommendation slate after the ranking stage. Three item ordering strategies are tested: random ordering, score-based ordering using ANN index scores, and user rank-based ordering where items are prioritized based on how high a user ranks for each item. The approach leverages existing two-tower models and ANN search infrastructure while repurposing them for the inverse task.

## Key Results
- InvR User rank variant achieved 33.3% increase in bottom 50% share (B50PS) compared to baseline
- 45% increase in percentage of sufficiently exposed items (PSEI) for treated items
- Successful long-term deployment in production with maintained effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Inverse Retrieval approach inverts the traditional two-tower retrieval model to find relevant users for each item, mitigating popularity bias.
- Mechanism: By using item embeddings as queries to retrieve user embeddings, the model focuses on item-user relevance rather than item popularity, promoting long-tail content exposure.
- Core assumption: The two-tower model's user and item embeddings capture meaningful semantic similarity that can be reversed without loss of relevance.
- Evidence anchors:
  - [abstract] "The authors propose a novel approach called Inverse Retrieval (InvR), which repurposes an existing two-tower retrieval model to recommend users for each item rather than items for each user."
  - [section] "Given a set of users, we use the model to generate an embedding for each of the users and store those embeddings in the ANN index. Then, we use the embedding of the given item to query this index and retrieve the top N most similar users."
  - [corpus] Weak corpus evidence for the specific InvR mechanism; related works focus on popularity bias but not on retrieval inversion.
- Break condition: If the two-tower model's embeddings are not symmetric in their similarity space, or if user embeddings poorly represent recent interaction history, the inverted retrieval will fail to find relevant users.

### Mechanism 2
- Claim: Ordering items by user rank rather than absolute score reduces the influence of popularity bias on recommendations.
- Mechanism: Instead of ranking items by the raw similarity score (which may correlate with item popularity), items are ranked by the position of the user in the item's candidate list, prioritizing the most promising users for each item.
- Core assumption: The rank of a user for a given item in the InvR output is a better proxy for true relevance than the raw score.
- Evidence anchors:
  - [section] "If user ranks high for the given item, compared to all other users according to the score, then this item should be presented to this user at a high position, ignoring the absolute value of the score."
  - [section] "This approach, however, can be strongly biased by item popularity as more popular items often gain higher scores [ 13] regardless of the actual relevance to the user."
  - [corpus] No direct corpus evidence for this specific ranking strategy; related works discuss popularity bias but not rank-based item ordering.
- Break condition: If the InvR model's scores are not sufficiently decoupled from popularity (e.g., due to training data bias), rank-based ordering will still favor popular items.

### Mechanism 3
- Claim: Periodic offline recomputation of InvR candidates and insertion into the recommendation slate enables scalable long-tail exposure without real-time overhead.
- Mechanism: InvR candidates are precomputed every 60 minutes, then merged into the online recommendation pipeline after ranking, allowing the system to serve long-tail items without increasing latency.
- Core assumption: The latency constraints of the production system can accommodate offline precomputation and post-ranking insertion of InvR items.
- Evidence anchors:
  - [section] "The offline InvR pipeline is executed every 60 minutes. During the online recommendation, the pre-generated InvR candidate items are inserted into the slate after the ranking stage..."
  - [section] "The system serves millions of daily active users, handling thousands of requests per second with the latency limits in order of lower hundreds of milliseconds."
  - [corpus] No corpus evidence for this specific offline/online hybrid approach; related works discuss online recommendation but not this hybrid pattern.
- Break condition: If the offline window is too long relative to item lifetime, or if insertion after ranking violates business logic, the approach will fail to deliver timely exposure.

## Foundational Learning

- Concept: Two-tower retrieval models and Approximate Nearest Neighbor (ANN) search
  - Why needed here: The InvR method depends on generating user and item embeddings with a two-tower model and efficiently retrieving nearest neighbors using ANN.
  - Quick check question: How does a two-tower model differ from a matrix factorization approach in terms of scalability and online updates?

- Concept: Popularity bias and its impact on recommendation diversity
  - Why needed here: Understanding how popularity bias manifests in recommender systems is essential to appreciate why InvR is needed and how it helps.
  - Quick check question: What are two common metrics used to measure popularity bias in recommender systems?

- Concept: System-level fairness metrics (e.g., B50PS, PSEI, T1PS)
  - Why needed here: These metrics are used to evaluate the impact of InvR on long-tail content exposure and fairness.
  - Quick check question: How does B50PS differ from T1PS in terms of what part of the item distribution they measure?

## Architecture Onboarding

- Component map: Two-tower retrieval model -> ANN index for user embeddings -> ANN index for item embeddings -> Online recommendation pipeline -> Offline InvR pipeline
- Critical path: User request → Main pipeline (retrieval + ranking) → InvR candidate insertion → Slate assembly → Response
- Design tradeoffs:
  - Offline vs. online InvR computation: Offline is scalable but may miss fast-changing trends; online is responsive but adds latency.
  - User count per item: More users increases exposure but also computational cost and may dilute relevance.
  - Item ordering: Score-based favors popular items; rank-based favors relevance but may reduce diversity.
- Failure signatures:
  - Low PSEI or B50PS: InvR not working or publishers not selected correctly.
  - High latency: InvR insertion or ANN queries too slow.
  - Drop in overall CTR: InvR items less relevant or too many long-tail items.
- First 3 experiments:
  1. Compare InvR Score vs. InvR User rank variants on PSEI and CTR to validate the rank-based ordering hypothesis.
  2. Vary the number of users per item in InvR and measure the impact on exposure and relevance.
  3. Test InvR with different publisher selection criteria to see how sensitive exposure gains are to publisher quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for addressing the cold start problem in the Inverse Retrieval approach?
- Basis in paper: [explicit] The paper mentions that cold start item embeddings are problematic and that randomization is used as a solution, but suggests that a more sophisticated solution could replace ID-based embeddings with pre-trained content-based ones.
- Why unresolved: The paper acknowledges the limitations of the current randomization approach and proposes exploring pre-trained content-based embeddings, but does not implement or test this solution.
- What evidence would resolve it: Conducting experiments comparing the performance of the current randomization approach with a method using pre-trained content-based embeddings for cold start items, measuring metrics such as exposure, CTR, and fairness.

### Open Question 2
- Question: How does the Inverse Retrieval approach perform when applied to all items rather than a selected subset of long-tail publishers?
- Basis in paper: [inferred] The paper focuses on selected long-tail publishers and suggests that it would be interesting to conduct an experiment where all items receive the InvR treatment.
- Why unresolved: The paper's experiments are limited to a specific subset of publishers, and the authors propose testing the approach on all items but do not do so.
- What evidence would resolve it: Implementing and testing the Inverse Retrieval approach on the entire item set, comparing results with the current approach and measuring overall system performance and fairness metrics.

### Open Question 3
- Question: What is the impact of the Inverse Retrieval approach on user experience and user-centered fairness?
- Basis in paper: [explicit] The paper mentions that it would like to focus more on the impact of the solution on user experience and user-centered fairness in future work.
- Why unresolved: The paper primarily focuses on system-level fairness metrics and does not extensively explore user-centric metrics or user experience.
- What evidence would resolve it: Conducting user studies or surveys to gather feedback on the recommendations provided by the Inverse Retrieval approach, measuring user satisfaction, perceived fairness, and diversity of recommendations.

## Limitations

- The exact thresholds for publisher and user selection criteria are not fully specified, introducing uncertainty about replicability
- The paper does not provide sufficient analysis of why the score-based variant underperformed expectations
- The offline recomputation window of 60 minutes may not be optimal for all content types, particularly fast-moving or time-sensitive content

## Confidence

- High confidence: The fundamental mechanism of inverse retrieval (using item embeddings to retrieve users) is sound and well-supported by the results
- Medium confidence: The user rank-based ordering strategy shows promise but requires more analysis to understand why it outperforms other variants
- Medium confidence: The deployment results demonstrate real-world effectiveness, though long-term sustainability and impact on user engagement require further monitoring

## Next Checks

1. Conduct ablation studies to determine the impact of different publisher selection criteria thresholds on exposure gains and recommendation quality
2. Perform sensitivity analysis on the offline recomputation window to identify optimal timing for different content categories
3. Implement detailed logging of the two-tower model's score distributions to better understand the relationship between popularity and relevance in the inverted retrieval space