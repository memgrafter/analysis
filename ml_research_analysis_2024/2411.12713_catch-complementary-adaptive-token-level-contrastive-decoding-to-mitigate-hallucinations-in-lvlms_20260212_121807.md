---
ver: rpa2
title: 'CATCH: Complementary Adaptive Token-level Contrastive Decoding to Mitigate
  Hallucinations in LVLMs'
arxiv_id: '2411.12713'
source_url: https://arxiv.org/abs/2411.12713
tags:
- visual
- image
- input
- hallucinations
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of hallucinations in Large Vision-Language
  Models (LVLMs), where the models generate text inconsistent with visual input, posing
  risks in critical domains. The core issue identified is "visual defect," caused
  by vision-language misalignment and information bottleneck, leading to diminished
  fine-grained feature perception and cumulative hallucinations.
---

# CATCH: Complementary Adaptive Token-level Contrastive Decoding to Mitigate Hallucinations in LVLMs

## Quick Facts
- arXiv ID: 2411.12713
- Source URL: https://arxiv.org/abs/2411.12713
- Authors: Zhehan Kan; Ce Zhang; Zihan Liao; Yapeng Tian; Wenming Yang; Junyuan Xiao; Xu Li; Dongmei Jiang; Yaowei Wang; Qingmin Liao
- Reference count: 31
- Key outcome: CATCH achieves up to 8.07-point increases in Accuracy and 5.98-point increases in F1 score on POPE, 16% improvement on MME, and reduces hallucination scores by 45.8% and 49.5% for CHAIR S and CHAIR I respectively.

## Executive Summary
This paper addresses hallucinations in Large Vision-Language Models (LVLMs) caused by visual defect - a misalignment between vision and language features leading to diminished fine-grained visual perception. The authors propose CATCH, a method that uses Complementary Visual Decoupling (CVD) to separate visual information, Non-Visual Screening (NVS) for hallucination detection, and Adaptive Token-level Contrastive Decoding (ATCD) to mitigate hallucinations. CATCH significantly improves hallucination assessment metrics across multiple benchmarks without requiring specific data or prior knowledge.

## Method Summary
CATCH addresses visual defect in LVLMs by implementing a three-stage approach. First, Complementary Visual Decoupling (CVD) uses SAM to segment images into dual and residual parts, separating relevant from irrelevant visual features. Second, Non-Visual Screening (NVS) compares output distributions from non-visual and decoupled images using Jensen-Shannon Divergence to detect hallucinations. Third, Adaptive Token-level Contrastive Decoding (ATCD) applies contrastive decoding by either subtracting hallucinated concepts from the original distribution or enhancing diversity when cumulative hallucinations are detected. This method works with various LVLM architectures and improves performance on hallucination assessment datasets without requiring specific training data.

## Key Results
- CATCH achieves up to 8.07-point increases in Accuracy and 5.98-point increases in F1 score on POPE dataset
- CATCH shows 16% improvement in total score on MME Hallucination benchmark
- CATCH reduces hallucination scores by 45.8% and 49.5% for CHAIR S and CHAIR I respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual defect in LVLMs is caused by the information bottleneck from mapping high-dimensional visual features to a lower-dimensional aligned vision-language latent space.
- Mechanism: The dimensionality mismatch between visual and textual feature spaces creates compression during alignment, leading to loss of fine-grained visual information and over-reliance on linguistic priors.
- Core assumption: Visual features inherently have higher dimensionality than textual features, and this dimensionality difference is the primary source of visual defect.
- Evidence anchors:
  - [abstract]: "visual features possess higher dimensionality than textual features, and mapping this higher-dimensional space to a lower-dimensional aligned vision-language latent space introduces an information bottleneck"
  - [section]: "The visual defect fundamentally arises from the unbalanced alignment in vision-language multimodal integration, leading to a visual information bottleneck."
  - [corpus]: Weak evidence - the corpus papers focus on hallucination mitigation but don't specifically discuss the dimensionality-based information bottleneck theory.
- Break condition: If visual features and textual features can be aligned without dimensionality reduction, or if the visual defect is primarily caused by other factors like training data quality rather than the inherent dimensionality mismatch.

### Mechanism 2
- Claim: Complementary Visual Decoupling (CVD) effectively reduces visual information density and uncertainty by separating extraneous visual features from key visual features relevant to the current token.
- Mechanism: CVD uses SAM to segment the image into dual and residual parts, dynamically highlighting key visual features for the current token generation step while obscuring irrelevant details, creating a more focused visual representation.
- Core assumption: The segmentation approach can reliably identify and separate visual features that are relevant to the current token from those that are not.
- Evidence anchors:
  - [abstract]: "Complementary Visual Decoupling (CVD) to separate visual information"
  - [section]: "We employ the Segment Anything Model (SAM) to segment all objects... Using the area of the target region as the confidence score, we select the top M objects as the exposed portion and mask the remaining objects and background to obtain the dual image zd."
  - [corpus]: Weak evidence - the corpus papers mention segmentation and object detection but don't specifically validate the effectiveness of CVD's approach to separating relevant from irrelevant visual features.
- Break condition: If SAM's segmentation is not reliable enough to identify which visual features are relevant to the current token, or if the dual/residual separation doesn't effectively reduce visual uncertainty.

### Mechanism 3
- Claim: Non-Visual Screening (NVS) can detect hallucinations by comparing the divergence between output distributions from non-visual input and decoupled images, identifying when the model relies too heavily on linguistic priors.
- Mechanism: NVS introduces a non-visual input containing only the textual prompt, calculates Jensen-Shannon Divergence between the non-visual input's output distribution and both the dual and residual image distributions, selecting the decoupled image with greater divergence as the one containing relevant visual information.
- Core assumption: When key visual features are obscured, the model's output distribution becomes nearly identical to that of the non-visual input, allowing divergence to serve as a reliable indicator of hallucination presence.
- Evidence anchors:
  - [abstract]: "Non-Visual Screening (NVS) for hallucination detection"
  - [section]: "We introduce a non-visual input zn, containing no visual information, generates responses based solely on the textual prompt and generated text tokens. The Jensen-Shannon Divergence (JSD) of the output distributions between the non-visual input and both the masked image and the exposed image is then calculated."
  - [corpus]: Weak evidence - the corpus papers discuss various hallucination detection methods but don't specifically validate the use of non-visual input and JSD divergence for hallucination detection in the way described here.
- Break condition: If the divergence between non-visual and visual input distributions is not a reliable indicator of hallucination presence, or if the model can generate reasonable responses without visual input in ways that don't indicate hallucination.

## Foundational Learning

- Concept: Information Bottleneck Theory
  - Why needed here: Understanding how the dimensionality mismatch between visual and textual feature spaces creates information compression and loss during alignment is fundamental to grasping why LVLMs experience visual defect.
  - Quick check question: Can you explain how mapping high-dimensional visual features to a lower-dimensional aligned space could lead to loss of fine-grained visual information?

- Concept: Jensen-Shannon Divergence
  - Why needed here: JSD is used in NVS to measure the similarity between output distributions from different inputs, serving as the key metric for detecting when the model is relying too heavily on linguistic priors rather than visual information.
  - Quick check question: How does Jensen-Shannon Divergence differ from other divergence measures, and why might it be particularly suitable for comparing probability distributions in this hallucination detection context?

- Concept: Contrastive Decoding
  - Why needed here: ATCD uses contrastive decoding to either subtract hallucinated concepts from the original distribution or enhance diversity when cumulative hallucinations are detected, making it essential to understand how contrastive approaches can improve generation quality.
  - Quick check question: What is the fundamental principle behind contrastive decoding, and how does it differ from standard decoding approaches in language models?

## Architecture Onboarding

- Component map: Segment Anything Model (SAM) -> Vision encoder -> Vision-language alignment module -> Large language model decoder -> CVD module -> NVS module -> ATCD module

- Critical path: 1. Raw image → SAM segmentation → dual and residual images 2. Dual/residual images + textual prompt → LVLM → output distributions 3. NVS comparison of distributions → selection of decoupled image 4. ATCD contrastive decoding based on divergence measurements 5. Final token generation

- Design tradeoffs: SAM segmentation adds computational overhead but enables precise visual information separation; Using non-visual input for detection requires additional model forward passes; The choice of α and β hyperparameters in ATCD affects hallucination mitigation effectiveness; Selecting the number of objects (N * 0.05) balances detail preservation with computational efficiency

- Failure signatures: If hallucinations persist despite CATCH, check if SAM segmentation is correctly identifying relevant visual features; If model becomes too conservative, verify that ATCD hyperparameters aren't overly suppressing diverse responses; If performance degrades on certain tasks, investigate whether visual decoupling is removing too much context for those specific visual question-answering scenarios

- First 3 experiments: 1. Run CATCH on POPE dataset with LLaVA-1.5 baseline and compare accuracy and F1 scores to validate hallucination mitigation effectiveness 2. Test cumulative hallucination analysis by generating captions for COCO images and measuring when hallucinations begin to occur with and without CATCH 3. Evaluate CATCH on MME Hallucination benchmark across all four subsets (existence, count, position, color) to assess fine-grained feature perception improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of CATCH vary with different segmentation strategies in CVD, beyond the default 5% object selection?
- Basis in paper: [explicit] The paper mentions using the top M objects as the exposed portion and masking the remaining objects and background, with M set to 0.05 by default, but does not explore other segmentation strategies or their impact on performance.
- Why unresolved: The paper does not provide experimental results comparing different segmentation strategies, leaving the optimal approach for different scenarios unclear.
- What evidence would resolve it: Comparative experiments showing the performance of CATCH using various segmentation strategies (e.g., different percentages of objects, different object selection criteria) across multiple datasets would clarify the impact of segmentation on CATCH's effectiveness.

### Open Question 2
- Question: Can CATCH be adapted to mitigate hallucinations in multimodal tasks beyond visual question answering, such as image generation or video understanding?
- Basis in paper: [inferred] The paper demonstrates CATCH's effectiveness in visual question answering tasks but does not explore its applicability to other multimodal tasks, despite mentioning its potential for various visual question-answering scenarios.
- Why unresolved: The paper focuses on visual question answering and does not provide evidence or analysis of CATCH's performance in other multimodal tasks, leaving its generalizability to other domains uncertain.
- What evidence would resolve it: Experimental results showing CATCH's performance on image generation, video understanding, or other multimodal tasks would demonstrate its broader applicability and effectiveness in mitigating hallucinations across different domains.

### Open Question 3
- Question: What is the impact of varying the hyperparameters α and β in ATCD on CATCH's performance, and how should they be tuned for different tasks?
- Basis in paper: [explicit] The paper mentions that α and β are set to 1.2 and 3 by default, respectively, but does not explore the sensitivity of CATCH's performance to these hyperparameters or provide guidance on tuning them for different tasks.
- Why unresolved: The paper does not provide experimental results showing the impact of varying α and β on CATCH's performance or offer insights into how these hyperparameters should be adjusted for different tasks or datasets.
- What evidence would resolve it: Experiments varying α and β across different tasks and datasets, along with analysis of their impact on CATCH's performance, would clarify how to optimally tune these hyperparameters for various scenarios.

## Limitations
- The theoretical explanation of visual defect as an information bottleneck is plausible but lacks direct empirical validation
- The effectiveness of NVS using Jensen-Shannon Divergence as a hallucination detection mechanism requires further validation across diverse scenarios
- The assumption that SAM segmentation can reliably identify which visual features are relevant to current token generation is not extensively validated

## Confidence
**High Confidence**: The empirical results showing CATCH's effectiveness on POPE, MME, and CHAIR benchmarks are well-supported with specific numerical improvements. The methodology of using complementary visual decoupling and contrastive decoding is clearly described and implemented.

**Medium Confidence**: The theoretical explanation of visual defect arising from information bottleneck due to dimensionality mismatch is plausible but could benefit from more direct empirical evidence. The effectiveness of NVS using Jensen-Shannon Divergence as a hallucination detection mechanism, while reasonable, requires further validation across diverse scenarios.

**Low Confidence**: The assumption that SAM segmentation can reliably identify which visual features are relevant to current token generation is not extensively validated. The generalizability of CATCH to other LVLM architectures beyond the tested ones is not demonstrated.

## Next Checks
1. **Cross-dataset validation**: Test CATCH on additional hallucination assessment datasets beyond POPE, MME, and CHAIR to verify the robustness of the method across different types of visual content and question-answering scenarios.

2. **Ablation study of components**: Conduct a systematic ablation study removing CVD, NVS, or ATCD individually to quantify the specific contribution of each component to the overall hallucination mitigation performance.

3. **SAM segmentation quality analysis**: Evaluate the quality and consistency of SAM segmentations across diverse image types and assess how segmentation errors impact the effectiveness of visual decoupling and subsequent hallucination mitigation.