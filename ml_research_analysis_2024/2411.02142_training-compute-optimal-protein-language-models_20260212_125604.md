---
ver: rpa2
title: Training Compute-Optimal Protein Language Models
arxiv_id: '2411.02142'
source_url: https://arxiv.org/abs/2411.02142
tags:
- training
- scaling
- protein
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates optimal training strategies for protein
  language models, a domain where compute-efficient practices are poorly understood.
  The authors address data scarcity and overfitting by constructing a large, diverse
  dataset (UniMeta200B) with 939M unique sequences and 194B tokens, including metagenomic
  sources to avoid plateau effects.
---

# Training Compute-Optimal Protein Language Models

## Quick Facts
- arXiv ID: 2411.02142
- Source URL: https://arxiv.org/abs/2411.02142
- Reference count: 40
- Primary result: Derives compute-optimal scaling laws for protein language models, showing sublinear data scaling and effective CLM-to-MLM transfer.

## Executive Summary
This paper investigates optimal training strategies for protein language models, a domain where compute-efficient practices are poorly understood. The authors address data scarcity and overfitting by constructing a large, diverse dataset (UniMeta200B) with 939M unique sequences and 194B tokens, including metagenomic sources to avoid plateau effects. They train over 300 models (3.5M–10.7B parameters) across 5–200B tokens to empirically derive scaling laws for both CLM and MLM objectives. They find distinct sublinear scaling behaviors: MLM grows faster in model size but slower in data scaling compared to CLM. Transfer learning from CLM to MLM is effective, with ~20% of compute optimally allocated to CLM pre-training for maximal MLM performance. Applying these scaling laws, they train 7.2B CLM and 10.7B MLM models that outperform established ESM-2 and PROGEN2 baselines on downstream tasks (structure, function, and generation) within similar compute budgets.

## Method Summary
The authors construct UniMeta200B, a diverse protein dataset with 939M unique sequences and 194B tokens, incorporating metagenomic sources to avoid overfitting. They train over 300 models ranging from 3.5M to 10.7B parameters on 5 to 200B tokens, using both Causal Language Model (CLM) and Masked Language Model (MLM) objectives. Scaling laws are empirically derived by fitting power-law relationships between model size, training tokens, and validation loss. Transfer learning experiments quantify the effectiveness of pre-training with CLM before fine-tuning with MLM. The optimal compute allocation is determined by minimizing validation loss across model sizes and training tokens. Large-scale models (7.2B CLM, 10.7B MLM) are trained using these scaling laws and evaluated on downstream protein tasks.

## Key Results
- MLM and CLM objectives exhibit distinct sublinear scaling behaviors, with MLM scaling faster in model size but slower in data scaling.
- Transfer learning from CLM to MLM is effective, with ~20% of compute optimally allocated to CLM pre-training.
- UniMeta200B dataset (194B tokens) prevents overfitting and plateau effects seen with repeated UniRef data.
- 7.2B CLM and 10.7B MLM models outperform ESM-2 and PROGEN2 baselines on structure, function, and generation tasks.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sublinear scaling of training data relative to model size prevents overfitting and diminishing returns.
- **Mechanism:** The paper derives distinct power-law scaling relations for CLM and MLM objectives, showing that data scales sublinearly with model size (αD ≈ 0.23 for MLM, 0.42 for CLM). This means that doubling model size requires less than double the training tokens, preserving sample efficiency and avoiding overfitting that occurs with repeated data.
- **Core assumption:** Protein sequences are a distinct modality from natural language, requiring tailored scaling laws rather than direct transfer from NLP.
- **Evidence anchors:**
  - [