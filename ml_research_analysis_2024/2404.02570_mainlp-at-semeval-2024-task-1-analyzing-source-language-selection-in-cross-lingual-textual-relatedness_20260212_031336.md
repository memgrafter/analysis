---
ver: rpa2
title: 'MaiNLP at SemEval-2024 Task 1: Analyzing Source Language Selection in Cross-Lingual
  Textual Relatedness'
arxiv_id: '2404.02570'
source_url: https://arxiv.org/abs/2404.02570
tags:
- languages
- language
- training
- source
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies source language selection for cross-lingual
  semantic textual relatedness (STR) on the SemEval-2024 Task 1 Track C. We evaluate
  three strategies: single-source transfer, multi-source transfer, and transfer from
  English plus two nearest language neighbors.'
---

# MaiNLP at SemEval-2024 Task 1: Analyzing Source Language Selection in Cross-Lingual Textual Relatedness

## Quick Facts
- arXiv ID: 2404.02570
- Source URL: https://arxiv.org/abs/2404.02570
- Reference count: 34
- First place on Kinyarwanda test set using English, Spanish, and Hausa fine-tuning

## Executive Summary
This paper investigates source language selection strategies for cross-lingual semantic textual relatedness (STR) in SemEval-2024 Task 1 Track C. Using XLM-R and Furina models, the authors evaluate single-source transfer, multi-source transfer, and transfer from English plus two nearest language neighbors. Their results show that multi-source training improves STR performance, though language interference occurs when training data comes from dissimilar languages. Script differences cause high variance in transfer performance, and machine translation-based data augmentation can lead to semantic drift in labels. The team achieved first place on the Kinyarwanda test set through targeted fine-tuning.

## Method Summary
The authors fine-tune RoBERTa-based models (XLM-R base and Furina) with MSE loss for STR score prediction. They experiment with three main strategies: single-source transfer using English as baseline, multi-source transfer using all available source languages, and multi-source transfer using languages from the same family as target languages. Language selection is guided by typological similarity features from lang2vec. The approach also explores machine translation-based data augmentation and transliteration. Models are trained with batch size 32, learning rate 2e-5, AdamW optimizer, up to 30 epochs with early stopping, and evaluated using Spearman's rank correlation on 12 target languages.

## Key Results
- Multi-source training improves STR performance compared to single-source transfer
- Language interference occurs when combining training data from dissimilar languages
- Script differences cause high variance in transfer performance
- Fine-tuning Furina on English, Spanish, and Hausa achieves first place on Kinyarwanda test set

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-source transfer improves STR models compared to single-source when training data is diverse.
- Mechanism: Combining training data from multiple languages increases dataset size and provides richer semantic context for the model to learn generalizable relatedness patterns.
- Core assumption: Languages in the source set share enough semantic space for the model to transfer knowledge effectively.
- Evidence anchors:
  - [abstract] "We find that multi-source training improves STR performance, but language interference can occur when training data are from dissimilar languages."
  - [section 5] "Our results reveal that knowledge transfer from multiple source languages (RQ1) improves STR models, affirming the potential of multi-source training to enhance cross-lingual capabilities."
  - [corpus] Weak evidence - corpus does not directly support this claim but shows this is a common strategy in related papers.
- Break condition: When source languages are too dissimilar, causing negative interference that outweighs the benefits of increased training data.

### Mechanism 2
- Claim: Language interference occurs when training data from heterogeneous languages are combined in multilingual STR models.
- Mechanism: The model's shared parameters become confused by conflicting linguistic patterns, reducing performance on target languages.
- Core assumption: Multilingual models have limited capacity to represent diverse linguistic patterns without interference.
- Evidence anchors:
  - [abstract] "language interference can occur when training data are from dissimilar languages."
  - [section 5] "This underscores the presence of language interference (Wang et al., 2020) in multilingual STR models when the training data from dissimilar languages are combined (RQ2)."
  - [corpus] Weak evidence - corpus does not directly support this claim but shows this is a known issue in related work.
- Break condition: When training data comes primarily from languages in the same family, reducing interference.

### Mechanism 3
- Claim: Script differences cause high variance in transfer performance for STR models.
- Mechanism: Different scripts create additional representation challenges that some models handle better than others.
- Core assumption: The model's architecture and pretraining affect its ability to handle script variations.
- Evidence anchors:
  - [abstract] "Script differences cause high variance in transfer performance."
  - [section 5] "We observe average performance drops of -0.09 and -0.06 for XLM-R and FURINA when moving from MS-All to MS-Fam."
  - [corpus] Weak evidence - corpus does not directly support this claim but shows this is a consideration in related work.
- Break condition: When script differences are eliminated through transliteration or when using models specifically designed for cross-script transfer.

## Foundational Learning

- Concept: Zero-shot cross-lingual transfer
  - Why needed here: The task requires predicting STR scores without any labeled data in the target language.
  - Quick check question: What distinguishes zero-shot cross-lingual transfer from few-shot or supervised approaches?

- Concept: Language family and typological similarity
  - Why needed here: The paper uses these concepts to select source languages that should transfer better to target languages.
  - Quick check question: How do language family relationships differ from typological similarity in predicting transfer success?

- Concept: Language interference in multilingual models
  - Why needed here: Understanding this concept explains why combining all languages sometimes hurts performance.
  - Quick check question: What factors determine whether combining languages will cause positive or negative transfer?

## Architecture Onboarding

- Component map:
  - Pretrained language models (XLM-R, Furina) as base encoders
  - Regression head for STR score prediction
  - Data preprocessing (transliteration, machine translation)
  - Language selection module (typological features, language vectors)
  - Evaluation pipeline (Spearman correlation calculation)

- Critical path:
  1. Load pretrained model
  2. Prepare training data (possibly with augmentation)
  3. Fine-tune on source languages
  4. Evaluate on target language
  5. Calculate Spearman correlation

- Design tradeoffs:
  - Single-source vs multi-source: Larger dataset vs potential interference
  - Script handling: Original vs transliterated data
  - Data augmentation: Increased size vs potential semantic drift

- Failure signatures:
  - Performance drops when adding languages from different families
  - No improvement when adding more source languages
  - Inconsistent results across different target languages

- First 3 experiments:
  1. Single-source transfer using English only
  2. Multi-source transfer using all available languages
  3. Multi-source transfer using languages from the same family as target

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of single-source transfer vary when selecting the optimal donor language for each target language compared to using a fixed source language like English?
- Basis in paper: [explicit] The paper discusses single-source transfer and evaluates the effectiveness of different source languages based on typological similarity.
- Why unresolved: The paper shows that the best possible single-source language selection (MAX) results in the same performance as multi-source training, but it doesn't provide a detailed analysis of how individual target languages perform with their optimal donor language compared to a fixed source like English.
- What evidence would resolve it: A detailed comparison of single-source transfer performance for each target language using its optimal donor language versus English, including a breakdown of the correlation scores and the number of target languages that perform better with their optimal donor language.

### Open Question 2
- Question: What is the impact of script differences on cross-lingual transfer performance, and how does transliteration affect this?
- Basis in paper: [explicit] The paper mentions that script differences cause high variance in transfer performance and that romanizing all languages did not improve zero-shot cross-lingual transfer for STR.
- Why unresolved: The paper doesn't provide a detailed analysis of the impact of script differences on individual target languages or a comparison of performance with and without transliteration.
- What evidence would resolve it: A detailed analysis of the impact of script differences on each target language, including a comparison of performance with and without transliteration, and an investigation of whether transliteration benefits specific language families or script types.

### Open Question 3
- Question: How does machine translation-based data augmentation affect label semantics and transfer performance, and under what conditions does it improve or degrade performance?
- Basis in paper: [explicit] The paper mentions that MT-based data augmentation can lead to shifts in label semantics and provides an example where MT fails to capture nuanced differences between closely related sentences.
- Why unresolved: The paper doesn't provide a comprehensive analysis of the conditions under which MT-based data augmentation improves or degrades performance, or an investigation of the extent to which label semantics are affected.
- What evidence would resolve it: A detailed analysis of the impact of MT-based data augmentation on label semantics and transfer performance for each target language, including an investigation of the conditions under which it improves or degrades performance and the extent to which label semantics are affected.

## Limitations
- Exact language family assignments and track divisions not explicitly specified
- Machine translation augmentation approach lacks complete specification
- Script difference analysis is based on aggregate results without examining specific language pairs

## Confidence
- Multi-source transfer benefits: High
- Language interference with dissimilar languages: Medium
- Script differences causing variance: Low
- Machine translation semantics shift: Low

## Next Checks
1. Conduct ablation studies by systematically removing languages from different families to quantify the exact impact of language interference on performance.
2. Analyze specific language pairs with large script differences (e.g., English-Latin vs Hausa-Arabic) to identify whether the variance is due to script representation or other typological factors.
3. Manually inspect a sample of machine-translated sentence pairs to verify whether label semantics shift occurs and quantify its impact on STR predictions.