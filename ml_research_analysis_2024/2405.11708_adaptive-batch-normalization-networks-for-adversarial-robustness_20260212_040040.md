---
ver: rpa2
title: Adaptive Batch Normalization Networks for Adversarial Robustness
arxiv_id: '2405.11708'
source_url: https://arxiv.org/abs/2405.11708
tags:
- adversarial
- abnn
- training
- target
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Adaptive Batch Normalization Networks (ABNN),
  a non-adversarial training (AT) defense method for improving robustness against
  adversarial attacks. The core idea is to use a pre-trained substitute model to generate
  clean batch normalization (BN) statistics, which are then used to adapt the target
  model's features through an adaptive BN layer.
---

# Adaptive Batch Normalization Networks for Adversarial Robustness

## Quick Facts
- arXiv ID: 2405.11708
- Source URL: https://arxiv.org/abs/2405.11708
- Authors: Shao-Yuan Lo; Vishal M. Patel
- Reference count: 38
- Key outcome: ABNN achieves 31.5% robust accuracy under PGD attack on CIFAR-10 with only 5.9% clean accuracy drop, outperforming AT methods while being 4-8x more training-efficient

## Executive Summary
This paper proposes Adaptive Batch Normalization Networks (ABNN), a non-adversarial training defense method for improving robustness against adversarial attacks. ABNN treats adversarial examples as a domain shift problem and uses a pre-trained substitute model to generate clean batch normalization (BN) statistics, which are then used to adapt the target model's features through an adaptive BN layer. The method significantly improves adversarial robustness against both digital (PGD) and physically realizable (ROA) attacks on image (CIFAR-10) and video (UCF-101) datasets, achieving higher clean data performance and significantly lower training time complexity compared to AT-based approaches.

## Method Summary
ABNN employs a pre-trained substitute model to generate clean BN statistics and sends them to the target model, which is exclusively trained on clean data. The substitute model, pre-trained on large-scale datasets, maintains clean BN statistics even under adversarial attacks. The target model's adaptive BN layers use the substitute's statistics to normalize features, effectively treating adversarial examples as a domain shift problem. This approach achieves robustness without requiring adversarial training, resulting in higher clean accuracy and lower training time complexity compared to AT-based methods.

## Key Results
- On CIFAR-10, ABNN improves robust accuracy from 0% to 31.5% under PGD attack with only 5.9% clean accuracy drop
- On UCF-101, ABNN achieves 43.4% robust accuracy under PGD attack and 24.4% under ROA attack
- ABNN is 4-8 times more training-efficient than AT-based methods while maintaining higher clean accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial examples shift batch normalization statistics, and aligning target model statistics to substitute model statistics restores robustness.
- Mechanism: The substitute model, pre-trained on large-scale datasets, maintains clean BN statistics even under adversarial attacks. The target model's adaptive BN layers use the substitute's statistics to normalize features, effectively treating adversarial examples as a domain shift problem.
- Core assumption: The substitute model's BN statistics remain clean and stable under white-box adversarial attacks because it's pre-trained on different data and frozen during inference.
- Evidence anchors:
  - [abstract] "ABNN employs a pre-trained substitute model to generate clean BN statistics and sends them to the target model."
  - [section] "Under adversarial attacks, the target model's BN statistics are perturbed, resulting in indiscriminate features. In comparison, the substitute model's BN statistics are relatively unaffected even under white-box attacks..."
- Break condition: If the substitute model's BN statistics also shift under white-box attacks, or if the adversarial perturbations are too severe to be corrected by simple BN alignment.

### Mechanism 2
- Claim: Test-time domain adaptation via adaptive BN provides robustness without requiring adversarial training.
- Mechanism: By leveraging techniques from test-time domain adaptation (TTA), ABNN adapts to adversarial examples at inference time without modifying the training process or requiring multi-step adversarial example generation.
- Core assumption: Adversarial examples can be effectively treated as a domain shift problem that can be addressed through test-time adaptation techniques.
- Evidence anchors:
  - [abstract] "We view the adversarial robustness problem from the perspective of domain adaptation..."
  - [section] "To our purpose, we resort to an adaptive BN idea, inspired by Test-Time Adaptation (TTA) approaches..."
- Break condition: If the adversarial domain shift is too complex to be handled by simple BN adaptation, or if the computational overhead becomes prohibitive.

### Mechanism 3
- Claim: Freezing the substitute model and training only the target model with clean data achieves both high clean accuracy and robustness.
- Mechanism: The target model is exclusively trained on clean data, avoiding the clean accuracy drop typically associated with adversarial training, while still gaining robustness through the adaptive BN mechanism.
- Core assumption: Training on clean data while using adaptive BN for adversarial robustness is more effective than adversarial training for maintaining clean accuracy.
- Evidence anchors:
  - [abstract] "ABNN can achieve higher clean data performance and significantly lower training time complexity compared to AT-based approaches."
  - [section] "ABNN exclusively trains on clean data and learns to align the substitute model's BN statistics."
- Break condition: If the adaptive BN mechanism proves insufficient for robustness, requiring return to adversarial training despite clean accuracy costs.

## Foundational Learning

- Concept: Batch Normalization (BN) mechanics and statistics
  - Why needed here: Understanding how BN works and how its statistics can be manipulated is crucial for grasping the adaptive BN mechanism
  - Quick check question: What are the two main statistics computed by a BN layer, and how are they used in normalization?

- Concept: Domain adaptation and covariate shift
  - Why needed here: The paper frames adversarial examples as a domain shift problem, so understanding domain adaptation is essential
  - Quick check question: How does treating adversarial examples as a domain shift problem change the approach to robustness?

- Concept: Test-time adaptation (TTA) techniques
  - Why needed here: ABNN uses TTA-inspired techniques to adapt to adversarial examples at inference time
  - Quick check question: What's the key difference between standard TTA and on-the-fly adaptation, and why is this distinction relevant here?

## Architecture Onboarding

- Component map: Substitute model -> Adaptive BN layers -> Target model
- Critical path:
  1. Pre-train substitute model on large-scale dataset
  2. Extract BN statistics from substitute model
  3. Train target model with clean data using adaptive BN
  4. At inference, pass inputs through both models in parallel
  5. Use substitute's BN statistics to adapt target model's features
- Design tradeoffs:
  - Higher clean accuracy vs. potentially lower robustness compared to AT
  - Lower training complexity vs. additional inference computation
  - Simplicity of implementation vs. potential limitations in handling complex adversarial attacks
- Failure signatures:
  - Robust accuracy significantly lower than expected
  - Clean accuracy drops unexpectedly during training
  - Inference time increases substantially due to substitute model overhead
- First 3 experiments:
  1. Implement basic ABNN with ResNet-18 target and VGG-19 substitute on CIFAR-10, test against PGD attack
  2. Compare clean accuracy of ABNN vs. standard ResNet-18 without defense
  3. Test ABNN's robustness against physically realizable attacks (ROA) to verify generalization beyond digital attacks

## Open Questions the Paper Calls Out

- How does the choice of substitute model architecture and pre-training dataset affect the robustness gains achieved by ABNN?
- Can ABNN be extended to handle other types of adversarial attacks beyond Lâˆž-norm PGD and ROA, such as L2-norm attacks or optimization-based attacks?
- How does ABNN perform on other image and video datasets, particularly those with different characteristics (e.g., higher resolution, more classes, longer videos)?

## Limitations

- The specific mechanism by which adaptive BN alignment confers robustness, particularly under white-box attacks, needs more theoretical and empirical validation.
- The method is validated on CIFAR-10 and UCF-101; its effectiveness on other datasets or more complex image/video tasks remains unclear.
- While training time is reduced compared to AT methods, the additional inference-time computation from the substitute model path could be significant.

## Confidence

- **High Confidence**: Claims about improved clean accuracy and training efficiency compared to AT methods are well-supported by the presented results.
- **Medium Confidence**: The robustness improvements under PGD and ROA attacks are demonstrated, but the long-term effectiveness against evolving attack strategies is uncertain.
- **Low Confidence**: The specific mechanism by which adaptive BN alignment confers robustness, particularly under white-box attacks, needs more theoretical and empirical validation.

## Next Checks

1. Conduct a detailed analysis of how ABNN performs against state-of-the-art white-box attacks (e.g., AutoAttack) to verify the claimed robustness mechanism.
2. Investigate how the choice of substitute model (architecture, pre-training dataset) affects ABNN's performance to understand the method's robustness to substitute model variations.
3. Test ABNN on a real-world dataset or task (e.g., medical imaging or autonomous driving) to assess its practical utility beyond academic benchmarks.