---
ver: rpa2
title: 'DSTI at LLMs4OL 2024 Task A: Intrinsic versus extrinsic knowledge for type
  classification'
arxiv_id: '2408.14236'
source_url: https://arxiv.org/abs/2408.14236
tags:
- semantic
- knowledge
- ontology
- large
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper compares intrinsic LLM knowledge with external semantic
  towers for ontology term typing in the LLMs4OL 2024 challenge. Semantic towers are
  defined as minimal sets of domain-specific semantic primitives extracted from Wikidata,
  embedded using gte-large, and stored in a vector database.
---

# DSTI at LLMs4OL 2024 Task A: Intrinsic versus extrinsic knowledge for type classification

## Quick Facts
- arXiv ID: 2408.14236
- Source URL: https://arxiv.org/abs/2408.14236
- Authors: Hanna Abi Akl
- Reference count: 12
- Key result: Semantic towers achieve 0.8581 (WordNet) and 0.5636 (GeoNames) F1 scores, 10-14% lower than intrinsic knowledge baselines

## Executive Summary
This paper compares intrinsic LLM knowledge with external semantic towers for ontology term typing in the LLMs4OL 2024 challenge. The study uses WordNet and GeoNames datasets to evaluate whether semantic grounding through vector-based retrieval can improve classification accuracy over fine-tuned models alone. Results show that while intrinsic knowledge achieves higher performance (0.9820 vs 0.8581 for WordNet), semantic towers provide valuable semantic grounding for edge cases and fine-grained distinctions, particularly for ambiguous terms.

## Method Summary
The approach uses fine-tuned flan-t5-small models compared against a RAG-based system using semantic towers. Semantic towers are constructed from Wikidata semantic primitives, embedded using gte-large, and stored in MongoDB. For large datasets like GeoNames, a curated subset is created by sampling 25 terms per category. Two models are trained: flan-t5-small-wordnet and flan-t5-small-geonames. The RAG approach retrieves the best matching semantic primitive based on cosine similarity, augments the input prompt, and passes it to the model for classification.

## Key Results
- Model-only approaches achieve F1 scores of 0.9820 (WordNet) and 0.6820 (GeoNames)
- Semantic tower approaches score 0.8581 and 0.5636 respectively
- 10-14% performance gap between intrinsic and extrinsic approaches
- On official test set: WN1 achieves 0.9716 F1, WN2 achieves 0.8420 F1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic towers provide semantic grounding that helps fine-tune model performance in edge cases.
- Mechanism: By embedding domain-specific semantic primitives into vector representations and storing them in a vector database, the model can retrieve and leverage these embeddings to disambiguate terms that would otherwise be misclassified based on intrinsic knowledge alone.
- Core assumption: The cosine similarity search over the semantic tower embeddings can effectively surface the correct semantic context for ambiguous terms.
- Evidence anchors:
  - [abstract] "semantic towers provide semantic grounding for fine-grained distinctions and correct classifications in edge cases"
  - [section] "Examples include correctly classifying the term into the bargain as adverb with the aid of the WordNet semantic tower (as opposed to classifying it as noun without it)"

### Mechanism 2
- Claim: Intrinsic LLM knowledge alone can achieve high performance on term typing tasks without external knowledge sources.
- Mechanism: The fine-tuned T5 model has learned sufficient domain-specific patterns during training to classify terms accurately based on its pre-existing linguistic knowledge.
- Core assumption: The training data contains sufficient coverage of the domain vocabulary and context for the model to generalize well.
- Evidence anchors:
  - [abstract] "model-only approaches achieve F1 scores of 0.9820 (WordNet) and 0.6820 (GeoNames)"
  - [section] "the results seem to suggest that the flan-t5 model, with a little fine-tuning, can rely on its existing knowledge regarding the dataset domains to correctly classify terms by type"

### Mechanism 3
- Claim: Fine-tuning on curated subsets is necessary for large datasets like GeoNames due to resource constraints.
- Mechanism: By selecting a representative sample of categories (25 terms per category when category count ≥ 100), the model can learn the essential patterns without the computational burden of full dataset training.
- Core assumption: The curated subset preserves the distributional characteristics of the full dataset across categories.
- Evidence anchors:
  - [section] "To remedy this problem, we curate a subset from the original dataset using the following algorithm"
  - [section] "We obtain a curated dataset of 2041 terms representing all possible categories"

## Foundational Learning

- Concept: Vector embeddings and similarity search
  - Why needed here: The semantic towers rely on vector embeddings (size 1024) and cosine similarity to retrieve the most relevant semantic primitives for term classification.
  - Quick check question: How does cosine similarity between embeddings help determine the semantic relationship between a term and a category?

- Concept: RAG (Retrieval-Augmented Generation) pipeline architecture
  - Why needed here: The system uses a RAG approach where input terms are embedded, compared against the semantic tower, and the best match is used to augment the model's prompt for classification.
  - Quick check question: What are the key components of a RAG pipeline and how do they interact in this ontology typing task?

- Concept: Fine-tuning vs. few-shot prompting trade-offs
  - Why needed here: The paper compares fully fine-tuned models (intrinsic knowledge) against RAG-based approaches (extrinsic knowledge), highlighting when each approach is more effective.
  - Quick check question: Under what conditions would a fully fine-tuned model outperform a RAG-based approach for ontology term typing?

## Architecture Onboarding

- Component map: WordNet terms → Fine-tuning → flan-t5-small-wordnet model; GeoNames terms → Curated subset → Fine-tuning → flan-t5-small-geonames model; Wikidata queries → Semantic primitives → gte-large embeddings → MongoDB vector store → Semantic towers

- Critical path: Input term → Embedding generation → Semantic tower similarity search → Best match retrieval → Prompt augmentation → Model prediction → Classification output

- Design tradeoffs:
  - Fine-tuning vs. prompt engineering: Full fine-tuning achieves higher performance but requires more resources; RAG-based approaches are more flexible but slightly less accurate
  - Dataset size vs. resource constraints: Curated subsets enable training on large datasets but may miss rare patterns
  - Embedding dimensionality: 1024-dimensional embeddings balance semantic richness with computational efficiency

- Failure signatures:
  - Semantic tower similarity scores are uniformly low → Embedding model may not capture domain semantics well
  - Model predictions cluster around few categories → Training data may be imbalanced or model underfit
  - Performance gap between WN1 and WN2 is larger than expected → Semantic tower construction may be flawed

- First 3 experiments:
  1. Baseline: Run WN1 (model-only) on a small subset of WordNet test data to verify the 0.9820 F1 score claim
  2. Semantic tower construction: Build the WordNet semantic tower from scratch using Wikidata queries and verify the embedding quality
  3. Ablation study: Compare model performance with and without semantic tower augmentation on ambiguous terms like "into the bargain" to demonstrate the grounding effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do semantic towers compare to other forms of external knowledge sources (like traditional knowledge graphs or semantic web ontologies) for ontology term typing tasks?
- Basis in paper: [explicit] The paper compares semantic towers against intrinsic LLM knowledge but doesn't compare semantic towers against other external knowledge sources.
- Why unresolved: The paper focuses specifically on comparing intrinsic vs extrinsic knowledge within their semantic tower framework, without benchmarking against other established knowledge representation methods.
- What evidence would resolve it: Comparative experiments showing F1 scores for semantic towers versus traditional knowledge graphs or ontologies on the same tasks would resolve this.

### Open Question 2
- Question: What is the optimal size and granularity of semantic towers for different ontology domains?
- Basis in paper: [inferred] The paper constructs semantic towers with minimal semantic primitives but doesn't explore how varying the number or specificity of primitives affects performance.
- Why unresolved: The authors chose specific semantic sets for WordNet and GeoNames but didn't systematically test how tower size or composition affects classification accuracy.
- What evidence would resolve it: Experiments varying the number of semantic primitives and measuring the resulting F1 scores would show the optimal balance between performance and semantic grounding.

### Open Question 3
- Question: Can semantic towers be dynamically updated or learned from data rather than manually constructed from Wikidata queries?
- Basis in paper: [explicit] The paper describes manual construction of semantic towers using predefined Wikidata queries and static semantic sets.
- Why unresolved: The current approach requires manual definition of semantic primitives for each domain, which may not scale well to new domains or evolving knowledge.
- What evidence would resolve it: Demonstrating an automated method for extracting or learning semantic primitives from domain data, and showing its effectiveness compared to the manual approach.

### Open Question 4
- Question: How do semantic towers perform on more complex ontology tasks beyond term typing, such as relation extraction or ontology alignment?
- Basis in paper: [inferred] The paper focuses exclusively on term typing classification and doesn't explore other ontology learning tasks.
- Why unresolved: The effectiveness of semantic towers for grounding semantic knowledge in term typing doesn't necessarily generalize to more complex semantic reasoning tasks.
- What evidence would resolve it: Applying semantic towers to relation extraction or ontology matching tasks and measuring performance improvements over baseline methods.

## Limitations

- The curated subset approach for GeoNames (2,041 terms from 8 million) raises concerns about representativeness and potential overfitting to sampled categories.
- Semantic tower construction relies on Wikidata semantic primitives without detailed validation of their relevance or completeness for target domains.
- The 10-14% performance gap between intrinsic and extrinsic approaches suggests semantic grounding provides limited benefits for many terms.

## Confidence

- **High confidence**: Intrinsic LLM knowledge can achieve strong performance on term typing tasks with appropriate fine-tuning, as evidenced by the 0.9820 F1 score on WordNet without external knowledge.
- **Medium confidence**: Semantic towers provide meaningful semantic grounding for edge cases, though the 10-14% performance gap suggests the benefit is not universal across all terms.
- **Low confidence**: The curated subset methodology adequately preserves distributional characteristics of large datasets for fine-tuning purposes, given the significant reduction in training data.

## Next Checks

1. **Domain-specific semantic tower validation**: Analyze the WordNet and GeoNames semantic towers to quantify coverage gaps and measure the impact of missing semantic primitives on classification accuracy for rare or ambiguous terms.

2. **Cross-dataset generalization test**: Evaluate whether models fine-tuned on curated subsets maintain performance on the full datasets across all categories, particularly focusing on underrepresented classes that may have been dropped during curation.

3. **Edge case ablation study**: Systematically identify and test terms where semantic tower augmentation changes classification outcomes, measuring the consistency and reliability of the grounding effect across different term types and ambiguity levels.