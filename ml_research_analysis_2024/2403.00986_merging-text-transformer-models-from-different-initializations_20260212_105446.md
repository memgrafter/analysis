---
ver: rpa2
title: Merging Text Transformer Models from Different Initializations
arxiv_id: '2403.00986'
source_url: https://arxiv.org/abs/2403.00986
tags:
- loss
- permutation
- attention
- transformer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of model merging between separately
  trained Transformer models, a challenging task due to the complexity of Transformer
  architectures compared to simpler architectures studied in prior work. The authors
  propose a permutation-based model merging method that aligns the representations
  of two Transformers by finding optimal feature correspondences through correlation
  matrices.
---

# Merging Text Transformer Models from Different Initializations

## Quick Facts
- arXiv ID: 2403.00986
- Source URL: https://arxiv.org/abs/2403.00986
- Authors: Neha Verma; Maha Elbayad
- Reference count: 7
- Primary result: Permutation-based merging reduces loss barriers between separately initialized Transformer models compared to vanilla averaging

## Executive Summary
This paper addresses the challenge of merging separately trained Transformer models, which have more complex architectures than previously studied models like MLPs and ResNets. The authors propose a permutation-based model merging method that aligns representations between two Transformers by finding optimal feature correspondences through correlation matrices. Their method specifically addresses challenges unique to Transformers including Multi-Headed Attention, residual connections, and sequential input structure. The approach consistently shows lower loss barriers between merged models compared to vanilla averaging across masked language modeling and GLUE fine-tuning tasks.

## Method Summary
The method computes permutation matrices to reorder model parameters between two separately trained Transformer models while maintaining functional equivalence. It involves computing post-activation features to determine which features correspond between models, then applying permutations to align representations. The approach handles three main Transformer components: Multi-Headed Attention sublayers (requiring head-specific permutations), residual connections (requiring shared permutation matrices throughout the model), and feed-forward sublayers. The aligned models are then merged through linear interpolation, with evaluation based on loss barriers across the interpolation path.

## Key Results
- For MLM, merging feed-forward and attention sublayers reduced the loss barrier from 4.31 to 3.71
- For GLUE tasks, 6 out of 8 tasks showed lower loss barriers with permutation-based merging compared to vanilla averaging
- Maintaining attention head structure during permutations was crucial for optimal results
- The amount of data used for computing correlations did not strongly affect loss barrier outcomes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Permutation-based alignment reduces loss barriers between separately initialized Transformer models.
- Mechanism: By computing optimal feature correspondences through correlation matrices and applying permutation matrices to reorder model parameters, the method aligns representations from different minima in a functionally equivalent space, enabling lower-loss interpolation paths.
- Core assumption: Different Transformer minima can be transformed into a shared parameter space through permutation symmetries without changing their underlying function.
- Evidence anchors:
  - [abstract] "propose a model merging technique to investigate the relationship between these minima in the loss landscape"
  - [section 3.1] "Given two models trained on the same data but from separate initializations... we compute post-activation features... to determine which features might correspond between models"
  - [corpus] Weak - no direct evidence found
- Break condition: If feature correlations between different minima are too low to establish meaningful correspondences, the permutation alignment would fail to reduce loss barriers.

### Mechanism 2
- Claim: Maintaining attention head structure during permutations is crucial for optimal merging results.
- Mechanism: The permutation operations must operate on each attention head separately without permuting features between heads, as the final hidden vector from MHA reflects a concatenation of results from each head computed separately with different weights.
- Core assumption: Attention heads in different Transformer minima have distinct but corresponding feature representations that can be aligned through head-specific permutations.
- Evidence anchors:
  - [section 3.2] "permutations must operate on each attention head separately, and not permute features between attention heads"
  - [section 3.2] "In experimentation, we do not find a notable difference between using a separate permutation for {WQ,WK} and using the same permutation for WQ, WK and WV"
  - [corpus] Weak - no direct evidence found
- Break condition: If attention heads in different minima don't learn comparable feature representations, head-specific permutations would be ineffective.

### Mechanism 3
- Claim: The residual connection structure requires shared permutation matrices throughout the model, reducing degrees of freedom for alignment.
- Mechanism: Because the input and output of both sublayers are added to create new outputs, any permutation applied to the output state must be the same for both addends, leading to one {P, PT} pair for the entire residual stream.
- Core assumption: The repeated Add&Norm components in Transformer architecture create structural constraints that limit the permutation symmetries available for model merging.
- Evidence anchors:
  - [section 3.3] "because of the multiple potential features that could contribute to the computation of the residual permutations... we consider several strategies for learning these mappings"
  - [section 3.3] "requiring the same permutation throughout the model reduces the degrees of freedom available in finding an optimal permutation"
  - [corpus] Weak - no direct evidence found
- Break condition: If the residual connection structure varies significantly between different Transformer architectures, this mechanism would not generalize.

## Foundational Learning

- Concept: Linear Mode Connectivity
  - Why needed here: The paper builds upon the concept that models trained from different initializations can be connected by low-loss linear paths, but extends this to Transformers which have more complex architectures than previously studied models.
  - Quick check question: What distinguishes linear mode connectivity from general mode connectivity, and why is this distinction important for model merging?

- Concept: Permutation Invariances in Neural Networks
  - Why needed here: The method relies on finding permutation matrices that transform one model into another member of its symmetric equivalence group, exploiting inherent model symmetries to align representations.
  - Quick check question: How do permutation invariances differ from other types of model symmetries like scaling invariances, and why are permutations particularly useful for this application?

- Concept: Multi-Headed Attention Mechanism
  - Why needed here: The paper addresses specific challenges of MHA, including the need to maintain head structure during permutations and the complex interactions between key, query, value, and output projection weights.
  - Quick check question: Why must permutations operate on each attention head separately rather than permuting features across all heads, and what would happen if this constraint were violated?

## Architecture Onboarding

- Component map: MHA sublayers (WK, WQ, WV, WO) -> Residual connections (with LayerNorm) -> Feed-forward sublayers (W1, W2)
- Critical path: Compute feature correlations → Determine optimal permutation matrices → Apply permutations to align model parameters → Merge aligned models through linear interpolation → Evaluate loss barriers across interpolation path
- Design tradeoffs: The residual connection constraint reduces permutation degrees of freedom but ensures functional equivalence; maintaining head structure improves alignment quality but adds complexity; using more data for correlation computation doesn't strongly affect outcomes but may improve alignment quality.
- Failure signatures: High loss barriers after merging indicate poor feature alignment; inconsistent loss reduction across interpolation weights suggests non-convex paths; low feature correlations between different minima suggest models learned dissimilar representations.
- First 3 experiments:
  1. Test permutation alignment on feed-forward layers only between two MultiBERTs models and measure loss barrier reduction
  2. Test MHA permutation with and without maintaining head structure to verify the importance of head-specific alignment
  3. Vary the amount of data used for computing correlations to determine if more data improves alignment quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal data type and quality for computing feature correlations when aligning text-based Transformer models?
- Basis in paper: [inferred] The paper notes that "the type and quality of text used for computing correlations likely matters more" than the quantity, and suggests this needs further investigation.
- Why unresolved: The authors found that varying amounts of data (100k sentences vs 1 million sentences) didn't significantly affect loss barrier outcomes, but didn't explore different data sources or qualities.
- What evidence would resolve it: Systematic experiments comparing loss barriers when using different text sources (news articles, academic papers, social media, etc.) with varying quality, formality, and domain specificity.

### Open Question 2
- Question: Why do fine-tuned Transformer models sometimes show "M-like" loss curve shapes between minima, unlike pretrained models?
- Basis in paper: [explicit] The authors observe "an interesting pattern of lower loss than either parent model for several tasks around λ=0.15 and again at λ=0.85" and note "M-like loss curve shapes between minima" for some fine-tuned models.
- Why unresolved: The paper mentions this pattern exists but doesn't investigate the underlying cause or implications for model merging.
- What evidence would resolve it: Analysis of the optimization dynamics during fine-tuning, investigation of task-specific representations, and comparison of how different fine-tuning strategies affect the loss landscape geometry.

### Open Question 3
- Question: How does Transformer width affect loss barriers between separately trained models?
- Basis in paper: [inferred] The paper notes that prior work showed "only wider ResNets can be merged with much smaller loss than standard versions" and suggests this relationship needs investigation for Transformers.
- Why unresolved: The authors used only base-sized BERT models and didn't explore how model capacity impacts the connectivity between minima.
- What evidence would resolve it: Experiments comparing loss barriers between models of different sizes (BERT-base vs BERT-large vs larger variants) trained with the same method and data.

## Limitations

- The method's effectiveness depends heavily on the quality of feature correlations between different model minima, with no thorough analysis of cases where correlations might be too low for meaningful alignment
- The residual connection constraint significantly reduces permutation degrees of freedom and may limit the method's applicability to more complex Transformer architectures
- Evaluation focuses primarily on BERT-base models, leaving uncertainty about performance on larger or more specialized Transformer variants

## Confidence

**High Confidence**: The core mechanism of using permutation matrices to align Transformer representations through correlation-based feature matching is well-established and theoretically sound. The experimental results showing reduced loss barriers compared to vanilla averaging are reproducible and consistent across multiple tasks.

**Medium Confidence**: The claim that maintaining attention head structure is crucial for optimal results has some supporting evidence but lacks thorough ablation studies across different model scales. The residual connection constraint's impact on alignment quality is described but not fully quantified across different architectural variations.

**Low Confidence**: The assertion that the amount of data used for computing correlations doesn't strongly affect outcomes is based on limited experimentation and may not hold for more diverse or complex tasks.

## Next Checks

1. **Correlation Quality Analysis**: Conduct a systematic study measuring feature correlation distributions before and after permutation across different model pairs, identifying thresholds below which alignment becomes ineffective. This would validate whether the permutation method can reliably establish meaningful correspondences.

2. **Residual Connection Constraint Evaluation**: Test the method on Transformer variants with different residual connection patterns (e.g., transformers without LayerNorm or with modified residual structures) to quantify how severely the constraint limits alignment quality across architectures.

3. **Scale and Architecture Generalization**: Apply the method to larger models (BERT-large, RoBERTa, or GPT variants) and different Transformer architectures to verify whether the same permutation strategies work effectively or require architectural adaptations.