---
ver: rpa2
title: 2D Matryoshka Training for Information Retrieval
arxiv_id: '2411.17299'
source_url: https://arxiv.org/abs/2411.17299
tags:
- layer
- training
- retrieval
- loss
- effectiveness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This reproducibility study implemented and evaluated 2D Matryoshka
  Training (2DMSE), a method that trains embedding representations simultaneously
  across different layers and dimensions of transformer models. The study confirmed
  that 2DMSE outperforms traditional Matryoshka training on sub-dimensions and full-sized
  model training in both Semantic Text Similarity (STS) and retrieval tasks, but does
  not surpass separately trained sub-models for specific layer-dimension configurations.
---

# 2D Matryoshka Training for Information Retrieval

## Quick Facts
- arXiv ID: 2411.17299
- Source URL: https://arxiv.org/abs/2411.17299
- Reference count: 34
- Primary result: 2D Matryoshka Training improves embedding effectiveness across multiple dimensions and layers for both STS and retrieval tasks

## Executive Summary
This reproducibility study implements and evaluates 2D Matryoshka Training (2DMSE), a method that trains embedding representations simultaneously across different layers and dimensions of transformer models. The approach uses KL divergence and PCA-transformed losses to align embeddings from multiple layers and target dimensions. The study confirms that 2DMSE outperforms traditional Matryoshka training on sub-dimensions and full-sized model training in both Semantic Text Similarity (STS) and retrieval tasks, though it does not surpass separately trained sub-models for specific layer-dimension configurations. Key modifications like incorporating full-dimension loss and training on broader target dimensions significantly improve retrieval effectiveness, particularly at lower dimensions.

## Method Summary
The 2D Matryoshka Training framework trains a transformer encoder simultaneously across multiple layers and target dimensions using a combination of MSE and KL divergence losses. Two versions are implemented: V1 uses KL divergence for layer alignment with sentence transformers library, while V2 incorporates layer-specific and dimension-specific losses with AnglE library. The method trains on BERT-base-uncased across 48 layer-dimension configurations (6 layers × 8 dimensions) using All-NLI dataset for STS and MSMARCO for retrieval. PCA transformation and truncation are used to create target-dimension embeddings, with logarithmic weighting applied to layer contributions in the loss function.

## Key Results
- Both versions of 2DMSE achieve higher effectiveness than traditional approaches in STS tasks, with version 2 showing better performance
- The approach generalizes well to retrieval tasks in both supervised (MSMARCO) and zero-shot (BEIR) settings
- Incorporating full-dimension loss and training on broader target dimensions improves retrieval effectiveness, particularly at lower dimensions (<128)
- Using a fixed full-size document encoder does not consistently improve performance compared to paired encoders of smaller sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training embeddings simultaneously across layers and dimensions creates a unified model that can produce high-quality embeddings for both small and large dimension settings without requiring separate models.
- Mechanism: The 2D Matryoshka training framework incorporates loss terms that optimize embeddings from both the last layer and randomly selected sub-layers, using KL divergence to align their distributions. This creates embeddings that are optimized across multiple levels of abstraction simultaneously.
- Core assumption: The embeddings from different layers can be effectively aligned through KL divergence, and this alignment transfers well to both STS and retrieval tasks.
- Evidence anchors:
  - [abstract] "2D Matryoshka Training is an advanced embedding representation training approach designed to train an encoder model simultaneously across various layer-dimension setups."
  - [section 2.2] "KL divergence is employed to measure the distance between the probability distributions of the last layer's embeddings and the randomly selected layer's embeddings"
- Break condition: If the KL divergence alignment fails to create consistent embedding distributions across layers, or if the task requirements for STS and retrieval are too dissimilar for the same training approach to work effectively.

### Mechanism 2
- Claim: Using PCA-transformed and truncated embeddings with weighted MSE and KL divergence losses allows the model to learn representations that work well at multiple target dimensions.
- Mechanism: The dimension-specific loss function transforms embeddings through PCA and truncates them to target dimensions, then optimizes using both MSE and KL divergence. The logarithmic weighting function ensures higher layers contribute more to the loss.
- Core assumption: PCA transformation and truncation preserve the most important information for the target dimension, and the combination of MSE and KL divergence effectively aligns these representations.
- Evidence anchors:
  - [section 2.3] "The dimension-specific loss employs the same weighted structure as the layer-specific losses. For each layer i, the embeddings e(i) are transformed via PCA and truncated to a target dimension ki"
  - [section 4.2] "We also conducted experiments on BEIR datasets and found similar results as in MSMARCO"
- Break condition: If PCA truncation removes critical information for the target dimension, or if the weighting function doesn't appropriately balance contributions from different layers.

### Mechanism 3
- Claim: Incorporating full-dimension loss from each layer during training improves model performance, particularly at higher dimensions where the original approach showed degradation.
- Mechanism: By including loss terms for the full dimension embeddings from each sub-layer (not just the target sub-dimensions), the model receives additional training signal that helps maintain quality across all dimension sizes.
- Core assumption: Training the full-dimension embeddings from sub-layers provides useful gradient information that improves the overall embedding quality, even when the target is a smaller dimension.
- Evidence anchors:
  - [section 2.3] "Upon reviewing the implementation details in the original codebase, we noted that the loss from the full dimension size of the last layer's embeddings is also included in the final loss computation"
  - [section 4.3.2] "Our results show effectiveness comparable to the original 2DMSE for dimensions <128, with significant improvement for dimensions ≥128"
- Break condition: If the additional full-dimension loss terms create conflicting gradients that actually harm performance, or if the computational overhead outweighs the benefits.

## Foundational Learning

- Concept: KL divergence and its role in distributional alignment
  - Why needed here: The training approach relies heavily on KL divergence to align embedding distributions across different layers, which is crucial for the multi-layer training strategy to work
  - Quick check question: What does KL divergence measure between two probability distributions, and why is it appropriate for aligning embeddings from different layers?

- Concept: Principal Component Analysis (PCA) for dimensionality reduction
  - Why needed here: The training method uses PCA to transform and truncate embeddings to target dimensions, which is central to how the model learns multi-dimensional representations
  - Quick check question: How does PCA determine which dimensions to keep when truncating embeddings, and what information might be lost in this process?

- Concept: Siamese network architecture and contrastive learning
  - Why needed here: The approach uses Siamese-style training where the same model processes different inputs (queries and documents) and contrastive loss functions to learn meaningful representations
  - Quick check question: How do Siamese networks differ from traditional classification networks, and why are they particularly suited for embedding learning tasks?

## Architecture Onboarding

- Component map: Input text → Transformer layers → Multiple embedding extraction points (last layer + sub-layers) → PCA transformation → Target dimension truncation → Loss computation (MSE + KL divergence) → Parameter updates

- Critical path: The critical computational path involves: input text → transformer layers → multiple embedding extraction points → loss computation (including PCA transformation and KL divergence) → parameter updates. The PCA transformation and KL divergence computation are particularly computationally intensive.

- Design tradeoffs: The approach trades increased training complexity and computational cost for the ability to generate high-quality embeddings at multiple dimension sizes from a single model. This is more efficient at inference time but requires more sophisticated training infrastructure.

- Failure signatures: Common failure modes include: (1) KL divergence not converging, leading to misaligned layer embeddings; (2) PCA truncation removing too much information, causing poor performance at smaller dimensions; (3) weighting functions not appropriately balancing layer contributions; (4) the model overfitting to the training distribution and not generalizing to retrieval tasks.

- First 3 experiments:
  1. Verify basic 2D Matryoshka training implementation on STS task with bert-base-uncased, comparing against traditional Matryoshka training and full model training across 2-4 layer/dimension combinations
  2. Test the impact of including full-dimension loss from sub-layers by training two versions (with and without) and comparing STS performance
  3. Evaluate retrieval performance on MS MARCO using the trained models, focusing on MRR@10 across different dimension sizes to confirm generalization from STS to retrieval tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does 2D Matryoshka Training maintain its effectiveness advantage when scaled to larger backbone models (e.g., BERT-large, RoBERTa) beyond bert-base-uncased?
- Basis in paper: [explicit] The paper notes that "Our investigation was limited to the bert-base-uncased model, without considering other potentially more effective decoder-based models" and acknowledges this as a direction for future work.
- Why unresolved: The experiments only tested bert-base-uncased, and larger models may have different layer structures and parameter interactions that could affect the effectiveness of 2DMSE.
- What evidence would resolve it: Conducting the same experiments with larger backbone models like BERT-large or RoBERTa would provide comparative results showing whether the effectiveness trends hold or change with model size.

### Open Question 2
- Question: How does the performance of 2D Matryoshka Training compare when using different layer weighting strategies for loss computation, beyond the logarithmic weighting used in the original work?
- Basis in paper: [explicit] The paper states "We used the same layer weighting strategy for loss computation as in the original work, without exploring the impact of different weighting strategies on retrieval tasks."
- Why unresolved: The original work's logarithmic weighting was not specifically optimized for retrieval tasks, and alternative weighting strategies might yield better results for this application.
- What evidence would resolve it: Experimenting with various layer weighting strategies (e.g., uniform, exponential, learned weights) and comparing their effectiveness on retrieval tasks would identify optimal weighting approaches.

### Open Question 3
- Question: Can the effectiveness of 2D Matryoshka Training at lower dimensions be improved by incorporating knowledge distillation from separately trained small models?
- Basis in paper: [inferred] The paper notes that "performance abruptly deteriorates for dimensions lower than 128" and that "small dimension embeddings lead to lower but still acceptable performance" in STS tasks, suggesting a gap in effectiveness at low dimensions.
- Why unresolved: The paper explored modifications like adding more target dimensions but did not investigate knowledge distillation as a strategy to improve low-dimension embeddings.
- What evidence would resolve it: Implementing a knowledge distillation approach where the 2DMSE model learns from separately trained small models at low dimensions, then evaluating retrieval effectiveness, would show if this strategy improves performance.

## Limitations
- Implementation discrepancies exist between described V2 methodology and actual implementation, particularly regarding full-dimension loss incorporation
- Computational overhead of 2DMSE training is significantly higher than traditional approaches, potentially limiting practical deployment
- Performance gains over traditional methods are modest for higher dimension embeddings, where separately trained sub-models still show advantages

## Confidence

- **High Confidence**: The core finding that 2DMSE outperforms traditional Matryoshka training on sub-dimensions and full-sized model training in both STS and retrieval tasks. This is well-supported by the experimental results across multiple datasets and evaluation metrics.
- **Medium Confidence**: The claim that V2 with full-dimension loss modification shows consistent improvement over V1 across all dimensions. While the results show improvements, the exact implementation details and their impact require further validation.
- **Medium Confidence**: The observation that using a fixed full-size document encoder does not consistently improve performance. The experimental evidence is mixed, with some configurations showing benefits while others do not.

## Next Checks

1. **Implementation Verification**: Conduct a detailed code review comparing the actual V2 implementation against the described methodology, particularly focusing on how full-dimension loss from sub-layers is incorporated and whether the logarithmic weighting function is correctly applied.

2. **Computational Efficiency Analysis**: Measure and compare the training time and resource requirements for 2DMSE versus traditional approaches across different layer-dimension configurations to quantify the practical deployment costs.

3. **Layer Importance Analysis**: Systematically evaluate which specific layer-dimension combinations contribute most to the performance gains in 2DMSE, and whether certain layers consistently outperform others across different task types and dimension sizes.