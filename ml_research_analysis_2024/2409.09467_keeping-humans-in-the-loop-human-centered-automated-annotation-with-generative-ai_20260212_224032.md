---
ver: rpa2
title: 'Keeping Humans in the Loop: Human-Centered Automated Annotation with Generative
  AI'
arxiv_id: '2409.09467'
source_url: https://arxiv.org/abs/2409.09467
tags:
- annotation
- tasks
- performance
- human
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the use of generative AI, specifically GPT-4,
  for automated text annotation in computational social science research. The researchers
  replicate 27 annotation tasks from 11 non-public datasets, comparing GPT-4's annotations
  to human-annotated ground-truth labels and supervised classification models.
---

# Keeping Humans in the Loop: Human-Centered Automated Annotation with Generative AI

## Quick Facts
- arXiv ID: 2409.09467
- Source URL: https://arxiv.org/abs/2409.09467
- Authors: Nicholas Pangakis; Samuel Wolken
- Reference count: 33
- Key outcome: GPT-4 achieves median F1 of 0.707 and accuracy of 0.85 across 27 annotation tasks, but performance varies significantly and supervised classifiers outperform when sufficient training data exists.

## Executive Summary
This study evaluates GPT-4 for automated text annotation in computational social science research, replicating 27 annotation tasks from 11 non-public datasets. The researchers compare GPT-4's annotations to human-annotated ground-truth labels and supervised classification models. While GPT-4 performs well overall with median F1 of 0.707 and accuracy of 0.85, its performance is inconsistent across tasks. The study emphasizes the importance of a human-centered workflow and careful evaluation standards, finding that grounding automated annotation in human validation labels is essential for responsible evaluation.

## Method Summary
The researchers used a human-in-the-loop workflow to evaluate GPT-4's automated annotation capabilities. They obtained 27 manual annotation tasks from 11 CSS articles with human-labeled ground-truth annotations. The workflow involved creating task-specific instructions, validating GPT-4 on a human-labeled subset, optimizing prompts based on error patterns, and testing GPT-4 on remaining samples. They also fine-tuned BERT classifiers as a baseline comparison using varying training sample sizes (250 and 1,000 samples). Performance was measured using accuracy, precision, recall, and F1 scores.

## Key Results
- GPT-4 achieves median F1 of 0.707 and accuracy of 0.85 across 27 tasks, but 9 tasks have precision or recall below 0.5
- GPT-4 performs better on recall than precision for 20 of 27 tasks, with median recall of 0.83 vs median precision of 0.65
- BERT classifiers fine-tuned with 1000 training samples outperform GPT-4 on 10 of 14 tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: GPT-4 automated annotation performance is generally high but inconsistent across tasks.
- **Mechanism**: GPT-4 achieves median F1 of 0.707 and accuracy of 0.85 across 27 tasks, but 9 tasks have precision or recall below 0.5.
- **Core assumption**: GPT-4 can approximate human annotations when tasks are well-defined and not contaminated by pre-training data.
- **Evidence anchors**:
  - [abstract] "Although the quality of LLM labels is generally high, we find significant variation in LLM performance across tasks, even within datasets."
  - [section] "Across all 27 tasks, we annotate slightly over 75,000 text samples... The overall cost was under $500 USD."
  - [corpus] Weak - no direct corpus evidence; relies on experimental results.
- **Break condition**: If tasks require nuanced cultural knowledge or abstract inference, performance degrades significantly.

### Mechanism 2
- **Claim**: GPT-4 performs better on recall than precision for most tasks.
- **Mechanism**: For 20 of 27 tasks, recall exceeds precision, with median recall of 0.83 vs median precision of 0.65.
- **Core assumption**: Generative LLMs can capture more true positives but may also introduce more false positives.
- **Evidence anchors**:
  - [abstract] "GPT-4 performance is significantly stronger in recall than precision. For 20 of the 27 tasks, recall exceeds precision."
  - [section] "As is clear from Figure 2, LLM performance is stronger on recall than precision for 20 of the 27 tasks."
  - [corpus] Weak - no direct corpus evidence; relies on experimental results.
- **Break condition**: If tasks require high precision for downstream analysis, GPT-4 alone may not be sufficient.

### Mechanism 3
- **Claim**: Supervised classifiers surpass GPT-4 when adequate training data is available.
- **Mechanism**: BERT classifiers fine-tuned with 1000 training samples outperform GPT-4 on 10 of 14 tasks.
- **Core assumption**: Supervised models can learn task-specific patterns better than few-shot GPT-4 when given sufficient labeled data.
- **Evidence anchors**:
  - [section] "GPT-4 only outperforms supervised classifiers when there are minimal training samples... BERT surpasses GPT-4 performance as more training data are added to the supervised model."
  - [section] "Specifically, BERT performs better than GPT-4 on 10 of the 14 replicated tasks when there are 1,000 training samples used to fine-tune BERT."
  - [corpus] Weak - no direct corpus evidence; relies on experimental results.
- **Break condition**: If labeled data is scarce or expensive to obtain, GPT-4 few-shot approach may be preferable.

## Foundational Learning

- **Concept**: Human-in-the-loop workflow for automated annotation
  - **Why needed here**: Ensures grounding in human judgment and validation of automated annotations
  - **Quick check question**: What are the four steps in the proposed human-centered workflow?

- **Concept**: Contamination and data leakage in LLM evaluation
  - **Why needed here**: Explains why password-protected datasets were used to avoid inflated performance from memorization
  - **Quick check question**: How does data contamination affect LLM evaluation?

- **Concept**: Intercoder reliability and its relationship to LLM performance
  - **Why needed here**: Helps understand when human annotation quality may affect LLM performance
  - **Quick check question**: What correlation exists between intercoder reliability and GPT-4 F1 performance?

## Architecture Onboarding

- **Component map**: Human annotation instructions -> GPT-4 few-shot labeling -> Human validation set -> Performance metrics (F1, accuracy, precision, recall) -> Optional supervised classifiers (BERT) for comparison
- **Critical path**: Human annotation instructions -> GPT-4 few-shot labeling -> Performance comparison with human labels -> Prompt optimization if needed
- **Design tradeoffs**: Few-shot GPT-4 vs supervised classifiers (speed/cost vs performance with sufficient training data)
- **Failure signatures**: Low precision or recall (<0.5) indicates tasks GPT-4 struggles with; inconsistent performance across tasks within same dataset
- **First 3 experiments**:
  1. Replicate a simple summarization task from the paper's datasets using GPT-4 few-shot approach
  2. Compare GPT-4 performance to BERT classifier with 250 training samples on same task
  3. Test prompt optimization by identifying error patterns and refining instructions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does automated annotation performance change when using open-source LLMs like Mistral-7B compared to closed-source models like GPT-4, and what are the cost implications?
- **Basis in paper**: [explicit] The paper compares GPT-4 to Mistral-7B in Appendix D, finding that GPT-4 significantly outperforms Mistral-7B in terms of F1 scores, but does not explore the cost implications in detail.
- **Why unresolved**: The paper mentions the cost of using GPT-4 but does not provide a detailed comparison of costs between using GPT-4 and open-source alternatives like Mistral-7B for large-scale annotation tasks.
- **What evidence would resolve it**: A detailed cost analysis comparing the total expenses of using GPT-4 versus open-source models like Mistral-7B for various annotation task sizes and frequencies.

### Open Question 2
- **Question**: What are the specific reasons for the significant variation in GPT-4's performance across different annotation tasks, and how can these be systematically addressed?
- **Basis in paper**: [explicit] The paper notes significant performance variation across tasks but does not provide a detailed analysis of the specific reasons for this variation or systematic approaches to address it.
- **Why unresolved**: The paper identifies the issue but does not delve into the underlying factors causing the variation or propose systematic solutions.
- **What evidence would resolve it**: A comprehensive analysis identifying specific factors (e.g., task complexity, cultural context, data quality) that contribute to performance variation, along with proposed systematic approaches to mitigate these issues.

### Open Question 3
- **Question**: How does the performance of automated annotation change when using different temperature settings, and what is the optimal temperature for balancing precision and recall?
- **Basis in paper**: [explicit] The paper explores temperature optimization in a limited way, finding no clear relationship between temperature and performance, but does not provide a detailed analysis of how different temperature settings affect the balance between precision and recall.
- **Why unresolved**: The paper's exploration of temperature optimization is limited and does not provide insights into how different temperature settings impact the trade-off between precision and recall.
- **What evidence would resolve it**: A detailed study varying temperature settings across a range of tasks to determine the optimal temperature for balancing precision and recall, along with an analysis of how temperature affects different types of annotation tasks.

## Limitations

- The study relies on non-public datasets, limiting reproducibility and broader validation of findings.
- The analysis focuses primarily on GPT-4 without extensive comparison to other state-of-the-art LLMs.
- The computational cost analysis doesn't include substantial human labor costs for prompt development and validation.

## Confidence

- **High confidence**: The core finding that GPT-4 performance varies significantly across tasks is well-supported. The observation that supervised classifiers outperform GPT-4 when sufficient training data is available is robustly demonstrated.
- **Medium confidence**: The claim that GPT-4 performs better on recall than precision is supported but may be task-dependent. The assertion that human-centered workflow is essential for responsible evaluation is methodologically sound but could benefit from more diverse dataset testing.
- **Low confidence**: The generalizability of findings to other LLMs or to tasks outside computational social science remains uncertain due to the study's narrow scope.

## Next Checks

1. **Prompt Engineering Impact**: Systematically test different prompt formulations for tasks where GPT-4 underperforms to determine if performance can be improved through better instructions.

2. **Cross-LLM Comparison**: Evaluate the same tasks using other state-of-the-art LLMs (e.g., Claude, Llama) to assess whether GPT-4's performance patterns are consistent across models.

3. **Generalization Test**: Apply the human-centered workflow to tasks from different domains (e.g., biomedical text annotation) to evaluate the framework's broader applicability beyond computational social science.