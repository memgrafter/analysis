---
ver: rpa2
title: Adjusting Interpretable Dimensions in Embedding Space with Human Judgments
arxiv_id: '2404.02619'
source_url: https://arxiv.org/abs/2404.02619
tags:
- dimensions
- seed
- words
- ratings
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to improve interpretable dimensions
  in word embedding spaces by combining seed-based vectors with human ratings. The
  authors address the problem of poor-quality seed-based dimensions by learning new
  dimensions that incorporate both seed information and labeled training data.
---

# Adjusting Interpretable Dimensions in Embedding Space with Human Judgments

## Quick Facts
- arXiv ID: 2404.02619
- Source URL: https://arxiv.org/abs/2404.02619
- Reference count: 17
- Primary result: FIT+S model improves interpretability of word embedding dimensions by combining seed-based vectors with human ratings

## Executive Summary
This paper addresses the problem of poor-quality interpretable dimensions in word embedding spaces by introducing a method that combines seed-based dimensions with human judgments. The authors develop FIT models that learn new dimensions using both seed information and labeled training data, improving upon existing seed-based approaches that often underperform. Their method uses a weighted combination of seed-based dimension loss and human rating-based loss to create fitted dimensions that better align with human judgments. The approach is evaluated on predicting human ratings of object properties and stylistic features, showing marked improvements especially in cases where traditional seed-based dimensions fail.

## Method Summary
The authors introduce FIT models that learn interpretable dimensions by combining seed-based information with human ratings through a weighted loss function. The method takes existing seed-based dimensions (computed as average differences between antonym pairs) and human rating data, then learns fitted dimensions that minimize both a distance to seed dimensions and a prediction error on human ratings. The FIT+S model specifically uses both seed words and seed dimensions as training data, with a hyperparameter α controlling the balance between these sources. The approach is evaluated using 5-fold cross-validation on object properties from Grand et al. and stylistic features from Pavlick & Nenkova, measuring performance with extended pairwise rank accuracy and mean squared error.

## Key Results
- FIT+S improves over seed-based dimensions in every single one of the 50 category/property pairs tested
- The performance increase is highest when seed-based dimensions perform poorly, with extended pairwise rank accuracy reaching 0.80 on object properties and 0.72 on stylistic features
- FIT+S achieves much lower mean squared error (0.7 and 1.2 respectively) compared to other models, and is the only model that consistently predicts ratings on the same scale as gold ratings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining seed-based dimensions with human ratings improves interpretability when seed-based dimensions underperform.
- Mechanism: FIT+S model uses both seed words (as extreme training points) and seed dimensions (as directional guidance) to learn fitted dimensions that better align with human ratings.
- Core assumption: Seed information and human ratings provide complementary signals that together overcome limitations of either source alone.
- Evidence anchors:
  - [abstract] "We obtain interpretable dimensions with markedly better performance especially in cases where seed-based dimensions do not work well."
  - [section 4] "FIT+S improves over SEED in every single one of the 50 category/property pairs" and "The performance increase is highest when performance of the seed-based dimensions is lowest"
  - [corpus] Weak - no direct citations found for this specific combination mechanism
- Break condition: If seed words or seed dimensions are of poor quality, they may mislead the learning process rather than improve it.

### Mechanism 2
- Claim: Fitted dimensions alone are underdetermined by human ratings and overfit without seed guidance.
- Mechanism: When only human ratings are used (FIT model), the high-dimensional embedding space allows too many ways to fit a dimension, causing overfitting to training data.
- Core assumption: The embedding space is sufficiently high-dimensional that human ratings alone cannot constrain the learned dimension uniquely.
- Evidence anchors:
  - [section 4] "We find that FIT by itself does not have enough information to fit a good dimension and overfits to the training data" and "FIT dimensions, for Grand et al. object features, from all human ratings, obtaining perfectly fit dimensions in every single case"
  - [section 4] "We next train dimensions on all human ratings but scramble the word/rating pairs, making them nonsensical. Again we obtain perfectly fit dimensions in every single case"
  - [corpus] Weak - no direct citations found for this specific overfitting behavior
- Break condition: If the embedding space dimensionality were reduced or human ratings were much more abundant, this overfitting problem might diminish.

### Mechanism 3
- Claim: FIT+S is the only model that consistently predicts ratings on the same scale as gold ratings.
- Mechanism: By combining seed information with human ratings and constraining the learned dimension to be close to seed dimensions, FIT+S avoids the extreme scaling issues seen in other models.
- Core assumption: Seed dimensions provide a reasonable scale and orientation that prevents the fitted dimension from exploding to extreme values.
- Evidence anchors:
  - [section 4] "97% runs of FIT+S have MSE values below 2, and all have values below 10" while "many runs of SEED, FIT and FIT+SW have very high MSE values"
  - [section 4] "This plot illustrates how the SEED predictions are on a much larger scale than gold ratings, while FIT+S is the only model whose predictions stay on the same scale"
  - [corpus] Weak - no direct citations found for this specific scale consistency mechanism
- Break condition: If seed dimensions themselves are on an incorrect scale, this mechanism could fail to produce properly scaled predictions.

## Foundational Learning

- Concept: Vector projection and cosine similarity
  - Why needed here: The models compute scalar projections of word vectors onto interpretable dimensions and measure similarity between seed dimensions and fitted dimensions
  - Quick check question: How would you compute the projection of a word vector onto a dimension vector, and what does this represent?

- Concept: Cross-validation and hyperparameter tuning
  - Why needed here: The paper uses 5-fold cross-validation to evaluate models and tune hyperparameters (α, offset, jitter) to avoid overfitting
  - Quick check question: Why is it important to use cross-validation when training on limited human rating data?

- Concept: Z-score normalization
  - Why needed here: All human ratings are normalized to z-scores before model training to ensure consistent scaling across different properties
  - Quick check question: What would happen if you trained on ratings with different scales without normalization?

## Architecture Onboarding

- Component map: Data preprocessing → Seed dimension computation → Fitted dimension learning → Cross-validation evaluation → Result aggregation
- Critical path: Human ratings → FIT+S model training → Extended pairwise rank accuracy calculation → MSE calculation
- Design tradeoffs: Using more seed information improves performance but adds complexity; using fewer human ratings makes the problem harder but more realistic
- Failure signatures: High MSE values (>100), low extended pairwise rank accuracy (<0.6), or perfect fit on scrambled data indicate problems
- First 3 experiments:
  1. Run SEED model on a single category/property pair to establish baseline performance
  2. Run FIT+S model on the same pair to verify improvement over SEED
  3. Test FIT+S with different α values (e.g., 0.01, 0.05, 0.1) to find optimal hyperparameter setting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed fitted dimensions compare to more sophisticated context selection methods for creating word type-level embeddings from contextualized representations?
- Basis in paper: [explicit] The paper mentions that more sophisticated context selection methods exist, such as language modeling criteria and exclusion of contexts where antonyms could occur, but these were not applied in the study.
- Why unresolved: The paper only used random selection of contexts for creating contextualized embeddings, which might not be optimal.
- What evidence would resolve it: A comparison of the proposed fitted dimensions using embeddings created with different context selection methods would show if more sophisticated methods lead to better performance.

### Open Question 2
- Question: How sensitive are the fitted dimensions to the choice of hyperparameters, particularly the mixing parameter alpha and the offset and jitter values for seed words?
- Basis in paper: [explicit] The paper mentions that low values of alpha work well and that the choice of offset and jitter does not matter, but it does not provide a detailed analysis of the sensitivity of the fitted dimensions to these hyperparameters.
- Why unresolved: The paper only reports the chosen hyperparameter values without exploring the impact of different values on the performance of the fitted dimensions.
- What evidence would resolve it: A systematic exploration of the hyperparameter space, varying the values of alpha, offset, and jitter, would show the sensitivity of the fitted dimensions to these parameters.

### Open Question 3
- Question: How do the proposed fitted dimensions generalize to other languages, and what are the limitations of extending the approach to languages without readily available human ratings?
- Basis in paper: [explicit] The paper mentions that the seed-based methodology has been shown to work well in other languages, but extending the proposed methodology to other languages is limited by the lack of human ratings needed for calculating the fitted dimensions.
- Why unresolved: The paper does not explore the performance of the fitted dimensions in other languages or discuss strategies for overcoming the lack of human ratings in other languages.
- What evidence would resolve it: Experiments with the fitted dimensions in other languages, using translated human ratings or alternative sources of supervision, would show the generalizability of the approach and identify strategies for extending it to other languages.

## Limitations
- The paper's mechanisms lack direct corpus support, with no citations found for the specific combination of seed information and human ratings
- Reliance on proprietary datasets (Grand et al., Pavlick & Nenkova) may hinder reproducibility
- The claim that FIT+S is the only model achieving consistent scale alignment requires further validation across diverse domains

## Confidence
- High confidence: FIT+S consistently improves over SEED, especially when SEED performs poorly (supported by 50 category/property pairs showing improvement)
- Medium confidence: FIT alone overfits to training data without seed guidance (supported by perfect fit on scrambled data)
- Medium confidence: FIT+S maintains scale consistency with gold ratings (supported by MSE distributions showing lower values)

## Next Checks
1. Test FIT+S on an additional dataset with different semantic properties to verify generalizability beyond object properties and stylistic features
2. Conduct ablation studies removing either seed words or seed dimensions from FIT+S to quantify their individual contributions
3. Evaluate model performance when seed dimensions are intentionally corrupted or inverted to test robustness to poor seed quality