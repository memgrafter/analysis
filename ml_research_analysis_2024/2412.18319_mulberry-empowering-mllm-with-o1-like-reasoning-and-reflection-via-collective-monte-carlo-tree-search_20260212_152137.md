---
ver: rpa2
title: 'Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective
  Monte Carlo Tree Search'
arxiv_id: '2412.18319'
source_url: https://arxiv.org/abs/2412.18319
tags:
- reasoning
- search
- comcts
- tree
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new learning-to-reason method for multimodal
  large language models (MLLMs) called Collective Monte Carlo Tree Search (CoMCTS).
  The key innovation is leveraging collective knowledge from multiple models to collaboratively
  search and identify effective reasoning paths, addressing challenges of search effectiveness
  and efficiency in traditional MCTS methods.
---

# Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search

## Quick Facts
- arXiv ID: 2412.18319
- Source URL: https://arxiv.org/abs/2412.18319
- Reference count: 27
- Key outcome: Mulberry-7B shows +4.2% improvement over Qwen2-VL-7B and +11.0% improvement over LLaVA-NeXT-8B on average across 8 benchmarks

## Executive Summary
This paper introduces Collective Monte Carlo Tree Search (CoMCTS), a learning-to-reason method that leverages collective knowledge from multiple multimodal large language models (MLLMs) to collaboratively search and identify effective reasoning paths. By addressing the limitations of traditional MCTS methods that can get trapped in homogeneous low-quality nodes, CoMCTS enables MLLMs to develop o1-like reasoning and reflection capabilities. The authors construct Mulberry-260k, a dataset with rich reasoning nodes discovered through CoMCTS, and train Mulberry models that demonstrate superior performance on various visual reasoning benchmarks, outperforming most open-sourced MLLMs and achieving competitive results against closed-source ones.

## Method Summary
The authors propose CoMCTS, which modifies traditional MCTS by incorporating collective knowledge from multiple MLLMs (GPT-4o, Qwen2-VL-7B, LLaMA-3.2-11B-Vision-Instruct, and Qwen2-VL-72B). The method consists of four operations: joint expansion using multiple models to generate diverse reasoning nodes, collective simulation and error positioning to identify and skip erroneous intermediate steps, backpropagation of aggregated scores from multiple models, and selection balancing exploration and exploitation. This approach is applied to 260K multimodal questions to construct the Mulberry-260K dataset, which is then used for collective supervised fine-tuning to train the final Mulberry models with enhanced reasoning capabilities.

## Key Results
- Mulberry-7B achieves +4.2% improvement over Qwen2-VL-7B and +11.0% improvement over LLaVA-NeXT-8B on average across 8 benchmarks
- CoMCTS successfully solves 74.9% of questions on average, with each question requiring only 1.5 search iterations
- Mulberry models show consistent performance improvements over baseline models, particularly on complex reasoning tasks like MMMU and MathVista

## Why This Works (Mechanism)

### Mechanism 1: Collective Knowledge Aggregation
- Claim: CoMCTS leverages collective knowledge from multiple models to collaboratively search reasoning paths, avoiding the trap of homogeneous low-quality nodes that plague single-model MCTS approaches.
- Mechanism: Multiple MLLMs simultaneously expand candidate reasoning nodes, simulate outcomes, position errors, and backpropagate scores, creating diverse and complementary reasoning trajectories that span multiple reasoning spaces rather than being confined to one model's limitations.
- Core assumption: Multiple models possess complementary knowledge that, when combined, provides broader coverage of the reasoning space than any single model alone.
- Evidence anchors: The paper shows that progressively involving more models into CoMCTS consistently improves search performance, even for including small models like Qwen2-VL-7B.

### Mechanism 2: Joint Error Positioning
- Claim: The joint simulation and error positioning mechanism allows CoMCTS to skip multiple intermediate steps by identifying the last correct reasoning node, dramatically improving search efficiency.
- Mechanism: Instead of evaluating each intermediate node sequentially, CoMCTS uses collective evaluation to identify erroneous nodes and their entire subtrees, then jumps directly to the most recent correct node as the next starting point for further exploration.
- Core assumption: Models can more reliably identify errors in others' reasoning than in their own, and error patterns are detectable through collective evaluation.
- Evidence anchors: CoMCTS achieves an average of only 1.5 search iterations per question while maintaining a 74.9% success rate, demonstrating the efficiency gains from this approach.

### Mechanism 3: Collective Backpropagation
- Claim: The collective backpropagation mechanism ensures that scores and visit counts are updated based on the consensus evaluation from multiple models, leading to more robust node value estimates.
- Mechanism: When multiple models evaluate reasoning nodes, their collective scores are aggregated and backpropagated through the tree, creating more reliable estimates of node quality that guide future selection decisions.
- Core assumption: Consensus among multiple models provides more reliable signal than individual model evaluations, especially for complex multimodal reasoning tasks.
- Evidence anchors: The paper demonstrates that using collective knowledge from multiple models consistently improves search performance compared to using individual models alone.

## Foundational Learning

- Concept: Monte Carlo Tree Search (MCTS) fundamentals
  - Why needed here: Understanding MCTS is essential because CoMCTS builds upon and modifies traditional MCTS for multimodal reasoning
  - Quick check question: What are the four key operations in standard MCTS, and how does CoMCTS modify each of them?

- Concept: Collective learning and knowledge aggregation
  - Why needed here: The paper's core innovation relies on combining knowledge from multiple models, which requires understanding how collective intelligence works in machine learning
  - Quick check question: How does collective evaluation of reasoning nodes differ from individual model evaluation, and what are the potential advantages and pitfalls?

- Concept: Multimodal reasoning and visual-language integration
  - Why needed here: The paper deals with MLLMs that must reason across both text and image modalities, requiring understanding of multimodal model architectures and reasoning approaches
  - Quick check question: What are the key challenges in multimodal reasoning that don't exist in purely text-based reasoning, and how might collective knowledge help address them?

## Architecture Onboarding

- Component map: Input questions → CoMCTS search engine (4 MLLMs) → reasoning tree construction → Mulberry-260K dataset → collective SFT training → Mulberry models

- Critical path: For a given multimodal question, the critical path is: question → collective MCTS search (using all four models) → reasoning tree construction → dataset storage → collective SFT training (using all four models) → final model weights. The search phase is the most computationally intensive part.

- Design tradeoffs: The authors chose to use four diverse MLLMs (GPT-4o, Qwen2-VL-7B, LLaMA-3.2-11B-Vision-Instruct, Qwen2-VL-72B) to maximize collective knowledge diversity, but this increases computational cost. They also limited reflection data to 15K samples to avoid overabundance, trading off some reflective reasoning capability for efficiency.

- Failure signatures: If search success rate remains low despite using multiple models, this suggests the models have insufficient diversity or the error positioning mechanism isn't working effectively. If the final models don't show significant improvement over baselines, this indicates the CoMCTS-searched data may not be translating into better reasoning capabilities.

- First 3 experiments:
  1. Run CoMCTS with only one model (GPT-4o) and measure search success rate and average iterations to establish baseline performance.
  2. Add a second diverse model (e.g., Qwen2-VL-7B) and measure improvement in search success rate and efficiency.
  3. Train a model using data from the two-model CoMCTS and compare against baseline to validate that collective knowledge translates to better reasoning performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CoMCTS scale with the number of models in the collective learning group?
- Basis in paper: The paper mentions that progressively involving more models into CoMCTS consistently improves the search performance, but does not provide a detailed analysis of the scaling behavior.
- Why unresolved: The paper does not provide a comprehensive study on how the performance of CoMCTS changes with the number of models in the collective learning group. This information would be valuable for understanding the trade-offs between model diversity and computational cost.
- What evidence would resolve it: A systematic study varying the number of models in the collective learning group and measuring the impact on search success rate and average search iteration would provide the necessary evidence.

### Open Question 2
- Question: How does the performance of CoMCTS compare to other advanced tree search methods, such as those incorporating reinforcement learning or deep neural networks?
- Basis in paper: The paper compares CoMCTS to traditional MCTS, ReST-MCTS, and Omega-MCTS, but does not explore more advanced methods that leverage deep learning or reinforcement learning techniques.
- Why unresolved: The paper focuses on comparing CoMCTS to traditional MCTS variants, but does not explore the potential benefits of incorporating more advanced techniques from the broader tree search literature.
- What evidence would resolve it: A comprehensive comparison of CoMCTS to advanced tree search methods, such as AlphaGo's deep neural network-guided search or reinforcement learning-based approaches, would provide insights into the relative strengths and weaknesses of different approaches.

### Open Question 3
- Question: How does the performance of CoMCTS generalize to other multimodal tasks beyond visual question answering, such as image captioning or visual grounding?
- Basis in paper: The paper focuses on visual question answering tasks, but does not explore the applicability of CoMCTS to other multimodal tasks.
- Why unresolved: The paper demonstrates the effectiveness of CoMCTS for visual question answering, but does not provide evidence for its generalizability to other multimodal tasks that may have different characteristics and challenges.
- What evidence would resolve it: Evaluating CoMCTS on a diverse set of multimodal tasks, such as image captioning, visual grounding, or multimodal machine translation, would provide insights into its generalizability and potential limitations.

## Limitations
- The collective knowledge assumption is asserted but not rigorously validated through ablation studies showing performance degradation with less diverse model collectives
- The efficiency claims rely on error positioning working effectively, but the paper doesn't provide detailed analysis of error detection accuracy or false positive/negative rates
- While the dataset construction is described, the quality control mechanisms for ensuring high-quality reasoning paths are not specified

## Confidence
- **High confidence**: The fundamental MCTS framework and its four operations (Expansion, Simulation, Backpropagation, Selection) are well-established and correctly described. The experimental methodology for benchmarking against established MLLM baselines is sound.
- **Medium confidence**: The collective knowledge aggregation approach and its implementation details are described clearly, but the specific mechanisms for error positioning and collective evaluation lack sufficient detail for full replication. The reported performance improvements are plausible given the methodology but would benefit from more rigorous ablation studies.
- **Low confidence**: The exact prompts, evaluation functions, and thresholds used in the Simulation and Error Positioning operation are not specified, making it difficult to reproduce the precise search behavior and efficiency gains claimed.

## Next Checks
1. **Ablation study on model diversity**: Test CoMCTS with increasingly homogeneous model collectives (e.g., multiple Qwen2-VL variants) to quantify the impact of diversity on search success rate and efficiency, validating the collective knowledge assumption.

2. **Error detection analysis**: Implement detailed logging of error positioning accuracy, measuring false positive and false negative rates for erroneous node identification, to understand the reliability of the efficiency gains.

3. **Resource utilization benchmarking**: Measure computational resources (GPU hours, API calls) required for CoMCTS versus traditional MCTS with single models, providing concrete evidence for the claimed efficiency improvements beyond iteration counts.