---
ver: rpa2
title: Similarity-Navigated Conformal Prediction for Graph Neural Networks
arxiv_id: '2405.14303'
source_url: https://arxiv.org/abs/2405.14303
tags:
- snaps
- nodes
- node
- prediction
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Similarity-Navigated Adaptive Prediction Sets (SNAPS) improves
  conformal prediction for graph neural networks by aggregating non-conformity scores
  from nodes with similar features or structural neighbors. The method leverages feature
  similarity and adjacency matrices to identify nodes likely sharing the same label,
  then corrects each node's non-conformity score by combining its own with weighted
  contributions from these similar nodes.
---

# Similarity-Navigated Conformal Prediction for Graph Neural Networks

## Quick Facts
- arXiv ID: 2405.14303
- Source URL: https://arxiv.org/abs/2405.14303
- Reference count: 40
- Key outcome: SNAPS reduces prediction set sizes by up to 80% while maintaining valid coverage on graph neural networks

## Executive Summary
This paper introduces Similarity-Navigated Adaptive Prediction Sets (SNAPS), a method that improves conformal prediction for graph neural networks by aggregating non-conformity scores from nodes with similar features or structural neighbors. The approach leverages feature similarity and adjacency matrices to identify nodes likely sharing the same label, then corrects each node's non-conformity score by combining its own with weighted contributions from these similar nodes. Experiments on 10 datasets show SNAPS significantly reduces prediction set sizes while maintaining valid coverage guarantees and increasing singleton hit ratios.

## Method Summary
SNAPS works by first computing basic non-conformity scores using standard conformal prediction methods (APS or RAPS), then aggregating these scores with contributions from similar nodes. The similarity is determined through k-nearest neighbors based on feature similarity and one-hop structural neighbors in the graph. The aggregated score for each node combines its own non-conformity score with weighted contributions from these similar nodes, controlled by hyperparameters λ and µ. This approach maintains valid coverage guarantees while producing more compact prediction sets, particularly effective on homophilous graphs where connected nodes tend to share labels.

## Key Results
- On OGBN Products, SNAPS reduces average prediction set size from 14.92 to 7.68 while maintaining coverage
- Singleton hit ratio increases from 0.235 to 0.394 on OGBN Products with SNAPS
- On ImageNet, SNAPS achieves 0.924 coverage with size 2.82 compared to 14.15 for baseline methods
- Across 10 datasets, SNAPS consistently reduces prediction set sizes while maintaining 1-α coverage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Nodes with similar features or direct connections tend to share the same label, allowing their non-conformity scores to correct the ego node's uncertainty estimate.
- Mechanism: SNAPS aggregates non-conformity scores from k-nearest neighbors (based on feature similarity) and one-hop structural neighbors to refine the ego node's prediction set.
- Core assumption: Network homophily holds—connected nodes or nodes with similar features are more likely to have the same label.
- Evidence anchors:
  - [abstract]: "The key idea behind SNAPS is that nodes with high feature similarity or direct connections tend to have the same label."
  - [section 3.1]: "We empirically show that for each node, aggregating the non-conformity scores of nodes with the same label can improve the efficiency of conformal prediction sets while maintaining valid marginal coverage."
  - [corpus]: Weak evidence—no direct citations to homophily studies in corpus, but the claim is widely accepted in GNN literature.
- Break condition: Network heterophily—if connected nodes often have different labels, feature similarity and adjacency will mislead the correction.

### Mechanism 2
- Claim: Aggregating scores from same-label nodes reduces prediction set size while preserving coverage.
- Mechanism: SNAPS uses a weighted combination of the ego node's score and the aggregated scores from similar nodes, with hyperparameters λ and µ controlling the contribution of feature-based and structural neighbors.
- Core assumption: Non-conformity scores from nodes with the same label are systematically lower than those from nodes with different labels.
- Evidence anchors:
  - [section 3.1]: "The results indicate that aggregating scores of these nodes can significantly reduce the average size of prediction sets."
  - [section 3.2]: "We consider feature similarity and network structure to select nodes that may have the same label as the ego node."
  - [corpus]: No explicit empirical backing in corpus for score reduction—assumption based on the authors' empirical analysis.
- Break condition: If similarity measures fail to identify same-label nodes accurately, the aggregation may increase prediction set size.

### Mechanism 3
- Claim: SNAPS maintains exchangeability, satisfying the theoretical requirement for conformal prediction.
- Mechanism: The aggregation is permutation-equivariant—reordering nodes does not change the aggregated scores, preserving exchangeability.
- Core assumption: GNN embeddings and non-conformity scores are invariant to node permutations under fixed graph structure.
- Evidence anchors:
  - [section 3.3]: "We prove that SNAPS non-conformity scores are still exchangeable."
  - [section 3.3]: "Several studies have demonstrated that non-conformity scores based on the node embeddings obtained by any GNN models are invariant to the permutation of nodes."
  - [corpus]: Weak evidence—corpus lacks direct citations to permutation invariance proofs.
- Break condition: If the aggregation introduces dependencies on node ordering (e.g., via unstable similarity computations), exchangeability breaks.

## Foundational Learning

- Concept: Conformal prediction and marginal coverage guarantees.
  - Why needed here: SNAPS is a post-hoc conformal prediction method; understanding coverage guarantees is essential to evaluate its effectiveness.
  - Quick check question: What does "marginal coverage" mean in the context of conformal prediction?

- Concept: Graph neural networks and message passing.
  - Why needed here: SNAPS uses GNN embeddings as input and relies on graph structure for neighbor selection.
  - Quick check question: How does a GNN layer aggregate information from neighboring nodes?

- Concept: Network homophily and feature similarity.
  - Why needed here: SNAPS leverages homophily to select nodes likely to share labels for score aggregation.
  - Quick check question: What is network homophily, and how does it relate to node classification performance?

## Architecture Onboarding

- Component map:
  Input: Graph G=(V,E), node features X, labels Y, adjacency matrix A
  GNN model f (e.g., GCN, GAT) → node embeddings
  Basic conformal score function s (e.g., APS) → initial non-conformity scores
  SNAPS aggregation: k-NN feature similarity graph As, structural neighbors via A, hyperparameters λ, µ
  Output: Refined prediction sets with improved size and singleton hit ratio

- Critical path:
  1. Preprocess features to build k-NN similarity graph As
  2. Train GNN on graph to obtain embeddings
  3. Compute basic non-conformity scores s for all nodes
  4. Aggregate scores using SNAPS formula (5)
  5. Calibrate quantile threshold on calibration set
  6. Generate prediction sets for test nodes

- Design tradeoffs:
  - Accuracy vs. efficiency: Larger k in k-NN improves same-label node selection but increases computation
  - Hyperparameter sensitivity: λ and µ control balance between ego, feature, and structural contributions; poor tuning can degrade performance
  - Applicability: Works best on homophilous graphs; performance may degrade on heterophilous networks

- Failure signatures:
  - Prediction set size increases after SNAPS—likely due to poor similarity graph or wrong hyperparameter settings
  - Coverage drops below target—suggests aggregation introduced bias or violated exchangeability
  - Singleton hit ratio decreases—indicates over-aggregation or incorrect neighbor selection

- First 3 experiments:
  1. Baseline: Run APS on CoraML with GCN, record size, coverage, SH
  2. SNAPS with k=5, λ=0.5, µ=0.5 on same data, compare metrics
  3. Sensitivity test: Vary k from 0 to 200, plot size/coverage/SH to find optimal k

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can SNAPS be adapted to work effectively on heterophilous graph datasets where connected nodes are likely to have different labels?
- Basis in paper: [explicit] The paper acknowledges that SNAPS focuses on high homophily datasets and mentions in the limitations section that "many heterophilous networks are prevalent in practice" and that "further investigation is essential to enhance the adaptability of SNAPS to these networks."
- Why unresolved: The current SNAPS method relies on feature similarity and network structure to identify nodes with the same label, which works well for homophilous networks but may fail when connected nodes have different labels. The paper only briefly mentions using embeddings from FSGNN as a workaround but doesn't provide a systematic solution.
- What evidence would resolve it: A modified version of SNAPS that maintains valid coverage and improves efficiency on heterophilous datasets like Chameleon and Squirrel, with experimental results showing comparable or better performance than current methods.

### Open Question 2
- Question: What is the most efficient and accurate method for selecting nodes with the same label as the ego node in SNAPS, particularly for large-scale graphs?
- Basis in paper: [explicit] The paper states in the limitations section that "the method we use to select nodes with the same label as the ego node is both computationally inefficient and lacking accuracy" and that "future work will explore more efficient and accurate methods for node selection."
- Why unresolved: While the current approach uses feature similarity and one-hop neighbors, the paper acknowledges these methods are not optimal for large graphs where computational efficiency becomes critical. The k-NN graph construction has O(NM) complexity which becomes prohibitive for massive graphs.
- What evidence would resolve it: A novel node selection algorithm that maintains or improves SNAPS performance while significantly reducing computational complexity, validated through experiments on large-scale datasets like OGBN Products.

### Open Question 3
- Question: Can SNAPS be extended to work effectively in inductive learning settings where the test nodes are not part of the graph during training?
- Basis in paper: [explicit] The paper states in the limitations section that "our work focuses on node classification using transductive learning" and that "many classification tasks require inductive learning" which they aim to address in future work.
- Why unresolved: The current SNAPS method relies on aggregating information from nodes in the training graph, which assumes all test nodes are present during model training. Inductive settings require handling completely unseen nodes that weren't part of the original graph structure.
- What evidence would resolve it: A modified SNAPS framework that maintains valid coverage guarantees and achieves comparable efficiency improvements in inductive node classification tasks, with experimental validation on standard inductive benchmarks.

## Limitations

- Performance degradation on heterophilous networks where connected nodes have different labels, as the method relies on network homophily assumptions
- Computational inefficiency in selecting nodes with the same label as the ego node, particularly problematic for large-scale graphs
- Limited to transductive learning settings where all test nodes are present in the graph during training

## Confidence

- **High confidence**: The core mechanism of aggregating non-conformity scores from similar nodes works as described for homophilous networks. Experimental results consistently show size reduction across multiple datasets.
- **Medium confidence**: The theoretical analysis of exchangeability preservation is sound, but practical implementation details may introduce subtle violations not captured in the proof.
- **Low confidence**: Performance on heterophilous networks is not well-characterized, and the method may fail or require significant adaptation in such settings.

## Next Checks

1. **Heterophily testing**: Evaluate SNAPS on a benchmark heterophilous graph (e.g., Texas, Wisconsin from Planetoid datasets) to quantify performance degradation and identify failure modes.
2. **Hyperparameter sensitivity**: Systematically vary λ and µ parameters across a wider range to understand their impact on coverage and size metrics, particularly for large graphs.
3. **Exchangeability validation**: Design experiments to test whether permutation of nodes affects aggregated non-conformity scores in practice, comparing against theoretical expectations.