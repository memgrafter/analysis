---
ver: rpa2
title: Scoring Time Intervals using Non-Hierarchical Transformer For Automatic Piano
  Transcription
arxiv_id: '2404.09466'
source_url: https://arxiv.org/abs/2404.09466
tags:
- scoring
- event
- interval
- score
- intervals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a transformer-based model for automatic piano
  transcription using the neural semi-Markov Conditional Random Field (semi-CRF) framework.
  The core idea is to simplify the interval scoring mechanism by using scaled inner
  product operations, which resemble the attention mechanism in transformers.
---

# Scoring Time Intervals using Non-Hierarchical Transformer For Automatic Piano Transcription

## Quick Facts
- **arXiv ID**: 2404.09466
- **Source URL**: https://arxiv.org/abs/2404.09466
- **Reference count**: 0
- **Primary result**: Transformer-based model achieves state-of-the-art F1 score of 95.35% for note transcription on Maestro dataset

## Executive Summary
This paper presents a transformer-based model for automatic piano transcription using a neural semi-Markov Conditional Random Field (semi-CRF) framework. The key innovation is a simplified interval scoring mechanism using scaled inner product operations that resemble transformer attention scoring. Theoretical analysis demonstrates that this approach is expressive enough to represent ideal scoring matrices for correct transcription. The model employs a non-hierarchical, encoder-only transformer architecture operating on low-resolution feature maps, achieving state-of-the-art performance across all subtasks on the Maestro dataset.

## Method Summary
The method uses a transformer encoder-only architecture with non-hierarchical design that operates on low-resolution feature maps. Log-mel spectrograms are downsampled using strided convolutional layers, processed by a transformer encoder with Fourier spatial position embeddings, then upsampled via transposed convolution. Interval scoring is performed using scaled inner products between query and key vectors, followed by a neural semi-CRF layer for decoding. The model is trained on the Maestro dataset with segment-wise processing and a cosine annealing learning rate schedule.

## Key Results
- Achieves state-of-the-art F1 score of 95.35% for note transcription on Maestro dataset
- Outperforms previous methods on sustain pedal transcription with 95.40% F1 score
- Ablation studies show inner product scoring outperforms more complex MLP+CNN scoring methods
- Competitive performance on MAPS and SMD datasets demonstrates generalization

## Why This Works (Mechanism)

### Mechanism 1
Inner product scoring with appropriate dimensionality can represent an ideal interval scoring matrix that yields correct transcription results. The interval scoring matrix has a low-rank plus diagonal structure. By Lemma 3.1, the rank of an ideal scoring matrix is M+1 where M is the number of intervals. By Theorem 3.2, if rank(QY) > M and rank(KY) > M, the ideal matrix can be factorized as pairwise inner products between vectors. Core assumption: The event intervals are non-overlapping and the vector dimensionality D exceeds the number of intervals.

### Mechanism 2
A non-hierarchical transformer operating on low-resolution feature maps can achieve high accuracy and time precision in piano transcription. The transformer encoder refines sequential representations for inner product scoring. Downsampling reduces computational cost while maintaining sufficient information through high-dimensional embeddings. Upsampling restores temporal resolution for scoring. Core assumption: High-dimensional embeddings preserve necessary information despite temporal downsampling.

### Mechanism 3
Inner product scoring is more efficient and effective than the MLP+CNN scoring method from [2]. Inner product scoring has simpler computation (scaled inner product) and avoids memory-intensive processing of large scoring matrices. The transformer architecture naturally produces representations suitable for this scoring. Core assumption: The efficiency gain doesn't compromise model capacity.

## Foundational Learning

- **Concept**: Neural semi-Markov Conditional Random Field (semi-CRF) framework
  - Why needed here: The semi-CRF framework directly models music events as closed intervals with specific event types, which is the core structure being scored.
  - Quick check question: What distinguishes semi-CRF from standard CRF in the context of piano transcription?

- **Concept**: Transformer architecture and attention mechanism
  - Why needed here: The proposed scoring method resembles attention scoring, and the transformer is used to produce interval representations for this scoring.
  - Quick check question: How does the inner product attention scoring in transformers relate to the proposed interval scoring?

- **Concept**: Matrix rank factorization and low-rank matrix completion
  - Why needed here: Theorem 3.2 proves that the ideal scoring matrix can be represented as pairwise inner products under certain rank conditions.
  - Quick check question: Given a matrix of rank r, what is the minimum dimension required for its factorization into two matrices?

## Architecture Onboarding

- **Component map**: Log-mel spectrogram -> Strided convolutional layers -> Fourier position embeddings -> Transformer encoder -> Transposed 1D convolution -> Scaled inner product scoring -> Semi-CRF decoding
- **Critical path**: Input → Downsampling → Transformer → Upsampling → Scoring → Semi-CRF decoding
- **Design tradeoffs**:
  - Downsampling factor vs temporal precision: Larger patches improve efficiency but may lose fine timing
  - Vector dimensionality vs computational cost: Higher D increases expressiveness but requires more parameters
  - Encoder-only vs encoder-decoder: Simpler architecture but may limit certain modeling capabilities
- **Failure signatures**:
  - Low F1 scores across all subtasks: Likely issues with transformer architecture or semi-CRF layer
  - Good note transcription but poor pedal transcription: May indicate insufficient modeling of long-duration events
  - High precision but low recall: Model may be too conservative in predicting intervals
- **First 3 experiments**:
  1. Compare inner product scoring vs MLP+CNN scoring with identical transformer architecture
  2. Test different downsampling factors (4×2, 8×4, 16×8) while keeping other parameters fixed
  3. Vary vector dimensionality (128, 256, 512) to find the minimum sufficient dimension

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the impact of different patch sizes (cT × cF) on model performance and computational efficiency? The paper mentions using a patch size of 8 × 4 but hasn't evaluated the effect of patch and embedding sizes due to resource constraints.

- **Open Question 2**: How does the proposed method perform on multi-instrument music transcription tasks? The paper mentions future research could extend the approach to other instruments and multi-instrument music transcription tasks, but the current study focuses on piano transcription.

- **Open Question 3**: What is the relationship between transformer architecture and the neural semi-CRF layer in terms of their interaction and potential for optimization? The paper suggests future research could investigate this interaction, as the current study uses a standard transformer encoder without exploring potential optimizations specifically tailored for the semi-CRF framework.

## Limitations

- Theoretical foundations rely on specific assumptions about non-overlapping intervals and sufficient vector dimensionality that may not hold in all real-world scenarios
- Performance claims are primarily based on Maestro dataset results, with cross-dataset evaluation showing competitive but not superior performance
- Limited empirical evidence comparing computational efficiency claims between inner product and alternative scoring methods

## Confidence

**High Confidence**: Model achieves state-of-the-art F1 scores on Maestro dataset; inner product scoring outperforms MLP+CNN baseline in ablation studies; semi-CRF framework produces accurate transcription results

**Medium Confidence**: Theoretical expressiveness of inner product scoring under rank conditions; non-hierarchical transformers can match hierarchical models' performance; efficiency advantages of inner product scoring

**Low Confidence**: Generalization performance across diverse datasets and recording conditions; relationship between theoretical rank requirements and practical vector dimensionality choices; impact of downsampling factors on transcription quality for complex musical passages

## Next Checks

1. Analyze the rank of query and key matrices (QY and KY) in trained models to verify they satisfy the theoretical conditions from Theorem 3.2

2. Systematically evaluate model performance across diverse datasets (different pianos, recording environments, musical genres) to assess generalization claims

3. Conduct comprehensive benchmarking comparing training/inference time, memory usage, and parameter efficiency between proposed inner product scoring and alternative scoring methods under identical computational conditions