---
ver: rpa2
title: 'RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed
  Information Retrieval'
arxiv_id: '2411.04752'
source_url: https://arxiv.org/abs/2411.04752
tags:
- information
- relevant
- code-mixed
- language
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extracting relevant information
  from code-mixed conversations, specifically Roman transliterated Bengali mixed with
  English on social media platforms. The authors propose a novel approach using GPT-3.5
  Turbo with prompt engineering combined with a mathematical model that accounts for
  sequential dependencies among documents.
---

# RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval

## Quick Facts
- arXiv ID: 2411.04752
- Source URL: https://arxiv.org/abs/2411.04752
- Reference count: 37
- Primary result: MAP score of 0.7037 on code-mixed (Roman transliterated Bengali + English) social media data

## Executive Summary
This paper addresses the challenge of extracting relevant information from code-mixed conversations, specifically Roman transliterated Bengali mixed with English on social media platforms. The authors propose a novel approach using GPT-3.5 Turbo with prompt engineering combined with a mathematical model that accounts for sequential dependencies among documents. The method first translates code-mixed text to English, uses GPT-3.5 Turbo to generate relevance scores between queries and documents, then applies a sequential probability model to refine relevance detection. Experimental results on a Facebook-based dataset demonstrate effective retrieval performance in this multilingual context.

## Method Summary
The approach involves three main steps: (1) preprocessing code-mixed text by translating Roman transliterated Bengali to English, (2) using GPT-3.5 Turbo with carefully designed prompts to generate relevance scores between queries and documents, and (3) applying a sequential probability model that adjusts document probabilities based on previous document relevance to refine the final ranking. The system tests different temperature values (0.5 to 0.9) during GPT-3.5 Turbo inference to balance precision and diversity in relevance scoring.

## Key Results
- MAP score of 0.7037
- NDCG score of 0.7992
- Precision@5 of 0.7933
- Precision@10 of 0.7667

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The sequential dependency model improves retrieval accuracy by adjusting document probabilities based on previous document relevance.
- Mechanism: After obtaining relevance scores from GPT-3.5 Turbo, the system applies a mathematical transformation that increases the probability of the current document being relevant if the previous document was relevant and the current score is above a threshold.
- Core assumption: Document relevance in code-mixed conversations exhibits sequential dependencies where relevant documents tend to cluster together.
- Evidence anchors:
  - [section]: "To formalize this process, we integrate GPT-3.5 Turbo's outputs into a mathematical model. This model takes into account the sequential dependencies among documents, treating the task of relevance detection as a problem of finding the optimal path or chain of relevance across the sequence."
  - [abstract]: "then applies a sequential probability model to refine relevance detection"
- Break condition: The sequential dependency assumption fails if documents are independent or if relevance scores are highly noisy.

### Mechanism 2
- Claim: Prompt engineering enables GPT-3.5 Turbo to effectively handle code-mixed language by providing clear instructions for semantic similarity assessment.
- Mechanism: The system uses a carefully crafted prompt that instructs the model to evaluate semantic similarity between queries and documents, producing relevance scores between 0 and 1.
- Core assumption: GPT-3.5 Turbo can understand and process code-mixed Roman transliterated Bengali mixed with English when provided with appropriate prompts.
- Evidence anchors:
  - [section]: "We leverage GPT-3.5 Turbo [10] by employing carefully designed prompts that guide the model to evaluate the relevance of documents with respect to a given query."
  - [abstract]: "We use GPT-3.5 Turbo via prompting alongwith using the sequential nature of relevant documents to frame a mathematical model"
- Break condition: The prompt fails if the model cannot understand code-mixed language or if the prompt format is ambiguous.

### Mechanism 3
- Claim: Temperature scaling during GPT-3.5 Turbo inference balances precision and diversity in relevance scoring.
- Mechanism: The system tests different temperature values (0.5 to 0.9) to find the optimal balance between deterministic, focused responses and diverse, creative outputs for relevance assessment.
- Core assumption: Different temperature settings affect the model's ability to accurately assess relevance in code-mixed contexts.
- Evidence anchors:
  - [section]: "For the five results reported, we ran the GPT model at different temperature values namely 0.5, 0,6, 0.7, 0.8, and 0.9."
  - [section]: "At lower temperatures, the model's responses are more deterministic and focused... At higher temperatures results in highly diverse and less predictable outputs."
- Break condition: Temperature scaling becomes ineffective if model outputs are consistently poor regardless of temperature setting.

## Foundational Learning

- Concept: Code-mixing and transliteration in multilingual contexts
  - Why needed here: The entire retrieval task operates on Roman transliterated Bengali mixed with English, requiring understanding of how code-mixing affects language processing
  - Quick check question: What are the main challenges in processing Roman transliterated Bengali compared to native script Bengali?

- Concept: Sequential probability models and their application to information retrieval
  - Why needed here: The system uses a mathematical model that incorporates sequential dependencies between documents
  - Quick check question: How does incorporating sequential dependencies improve document ranking compared to independent scoring?

- Concept: Prompt engineering for large language models
  - Why needed here: The system relies on carefully designed prompts to guide GPT-3.5 Turbo's relevance assessment
  - Quick check question: What are the key elements of an effective prompt for semantic similarity assessment?

## Architecture Onboarding

- Component map: Input → Translation → GPT relevance scoring → Sequential probability adjustment → Final ranking
- Critical path: Input → Translation → GPT relevance scoring → Sequential probability adjustment → Final ranking
- Design tradeoffs:
  - Translation accuracy vs. processing speed
  - GPT model complexity vs. computational cost
  - Sequential dependency strength vs. overfitting to dataset patterns
  - Temperature settings for exploration vs. exploitation balance
- Failure signatures:
  - Consistently low MAP/NDCG scores indicating poor relevance detection
  - Temperature insensitivity suggesting prompt or model limitations
  - Translation errors propagating through the pipeline
  - Sequential model overcorrection leading to ranking instability
- First 3 experiments:
  1. Baseline test: Run system with temperature=0.5 only, compare MAP score against published results
  2. Ablation test: Remove sequential probability model, measure impact on ranking quality
  3. Translation robustness test: Introduce controlled translation errors, observe system performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the GPT-3.5 Turbo model with prompt engineering compare to fine-tuned traditional information retrieval models on the same code-mixed dataset?
- Basis in paper: [explicit] The paper mentions that GPT-3.5 Turbo is used with prompt engineering and a mathematical model, but does not provide comparative results against traditional fine-tuned IR models
- Why unresolved: The paper only reports results for their proposed approach and does not benchmark against baseline traditional IR methods
- What evidence would resolve it: Experimental results showing MAP, NDCG, precision@5, and precision@10 scores for both the proposed GPT-based approach and traditional fine-tuned IR models on the same dataset

### Open Question 2
- Question: How does the sequential probability model affect retrieval performance when applied to non-sequential or randomly ordered document collections?
- Basis in paper: [explicit] The authors develop a mathematical model that accounts for sequential dependencies among documents, suggesting this is a key component of their approach
- Why unresolved: The paper only evaluates the approach on presumably sequential Facebook data and does not test how the model performs when document order is randomized or irrelevant
- What evidence would resolve it: Experiments comparing retrieval performance with and without the sequential model on both sequential and randomly ordered document collections

### Open Question 3
- Question: What is the optimal temperature setting for GPT-3.5 Turbo when processing different types of code-mixed queries?
- Basis in paper: [explicit] The authors experimented with temperature values from 0.5 to 0.9 and selected this range based on qualitative observations about response diversity
- Why unresolved: The paper does not provide systematic analysis of how different temperature settings affect retrieval performance across different query types or document characteristics
- What evidence would resolve it: Detailed analysis showing retrieval performance metrics at different temperature settings for various query types, document lengths, and code-mixing ratios

## Limitations

- The sequential dependency model's effectiveness relies heavily on the assumption that relevant documents cluster together in code-mixed conversations, but this clustering behavior is not empirically validated beyond the reported dataset
- The translation step from Roman transliterated Bengali to English introduces an unknown quality bottleneck since the specific translation method or model is not disclosed
- The temperature selection process across five different values (0.5-0.9) is mentioned but the methodology for choosing among these temperatures for individual queries remains unspecified

## Confidence

**High Confidence Claims:**
- The overall architecture combining GPT-3.5 Turbo with sequential probability modeling is technically sound and implementable
- The reported performance metrics (MAP: 0.7037, NDCG: 0.7992) are plausible for the described approach

**Medium Confidence Claims:**
- The sequential dependency mechanism meaningfully improves retrieval performance
- Temperature scaling effectively balances precision and diversity in relevance scoring

**Low Confidence Claims:**
- The assumption that relevant documents exhibit strong sequential clustering in code-mixed social media conversations
- The translation quality from Roman transliterated Bengali to English does not significantly impact final performance

## Next Checks

1. **Sequential Dependency Validation**: Design a controlled experiment where documents are artificially shuffled to break natural sequential patterns, then measure whether the sequential probability model still provides performance benefits over independent scoring.

2. **Translation Quality Impact Test**: Implement a parallel evaluation pipeline that bypasses the translation step by using English-only queries and documents, then compare performance degradation to quantify translation error impact on the overall system.

3. **Temperature Sensitivity Analysis**: Conduct a formal hyperparameter optimization study across the five temperature values for each query type, measuring not just mean performance but also variance across multiple GPT-3.5 Turbo runs to assess stability and reproducibility.