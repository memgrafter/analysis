---
ver: rpa2
title: 'LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced
  Following of Instructions with Multiple Constraints'
arxiv_id: '2410.06458'
source_url: https://arxiv.org/abs/2410.06458
tags:
- constraints
- instruction
- constraint
- response
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces REAL INSTRUCT, a new benchmark that evaluates
  large language models (LLMs) on real-world multi-constrained user instructions,
  revealing that even GPT-4 fails at least one constraint in over 21% of cases. To
  improve performance, the authors propose DECRIM, a self-correction pipeline that
  decomposes instructions into granular constraints, critiques responses, and iteratively
  refines them.
---

# LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints

## Quick Facts
- **arXiv ID**: 2410.06458
- **Source URL**: https://arxiv.org/abs/2410.06458
- **Reference count**: 40
- **Primary result**: DECRIM improves Mistral's performance by up to 7.3% on REAL INSTRUCT and 8.0% on IFEval, with strong feedback enabling open-source LLMs to outperform GPT-4.

## Executive Summary
This work introduces REAL INSTRUCT, a benchmark for evaluating large language models on real-world multi-constrained user instructions, revealing that even GPT-4 fails at least one constraint in over 21% of cases. To address this challenge, the authors propose DECRIM, a self-correction pipeline that decomposes instructions into granular constraints, critiques responses, and iteratively refines them. The approach significantly improves instruction-following performance, with open-source LLMs using DECRIM and strong feedback surpassing GPT-4 on both REAL INSTRUCT and IFEval benchmarks.

## Method Summary
DECRIM is a self-correction pipeline designed to enhance LLM performance on multi-constrained instructions. It works by decomposing instructions into granular constraints, using a Critic model to evaluate constraint satisfaction, and iteratively refining responses based on feedback. The method includes weakly supervised fine-tuning of open-source LLMs for constraint evaluation, leveraging Chain-of-Thought reasoning from strong proprietary models. DECRIM was evaluated on REAL INSTRUCT and IFEval benchmarks, demonstrating significant improvements in instruction-level and constraint-level accuracy.

## Key Results
- DECRIM improves Mistral's performance by 7.3% on REAL INSTRUCT and 8.0% on IFEval with weak feedback.
- Strong feedback (weakly supervised with GPT-4-Turbo's CoT reasoning) boosts Mistral's performance by 22.0% on REAL INSTRUCT and 33.8% on IFEval, surpassing GPT-4 on both benchmarks.
- The open-source LLM-as-a-judge, when weakly supervised, achieves significant improvements in constraint satisfaction detection (+12.9 in Macro F1 and +28.1 in F1 Neg.).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DECRIM improves LLM performance by breaking multi-constrained instructions into granular, individually checkable constraints.
- **Mechanism**: Instruction decomposition transforms a single complex compliance check into multiple simpler ones. Each constraint can be verified independently by a Critic model, reducing cognitive load and ambiguity in evaluation.
- **Core assumption**: Decomposing constraints preserves semantic equivalence and does not introduce new errors or omissions.
- **Evidence anchors**:
  - [abstract]: "DECRIM works by decomposing the original instruction into a list of constraints and using a Critic model to decide when and where the LLM’s response needs refinement."
  - [section]: "Unlike previous methods, which assume constraint independence or focus on specific constraint types, our approach makes no assumptions about the nature of user constraints."
- **Break condition**: If the decomposition introduces incorrect or missing constraints, the refinement process targets wrong aspects, leading to degraded performance.

### Mechanism 2
- **Claim**: Weakly supervised open-source Critic models can be effective if trained with reasoning trails from strong proprietary models.
- **Mechanism**: Fine-tuning an open-source LLM with weak labels augmented by Chain-of-Thought reasoning from GPT-4-Turbo improves its ability to detect unsatisfied constraints, closing the performance gap with proprietary judges.
- **Core assumption**: Reasoning trails provide sufficient signal to guide the open-source model toward correct constraint evaluation.
- **Evidence anchors**:
  - [abstract]: "Providing stronger feedback further boosts Mistral’s performance by 22.0% on REAL INSTRUCT and 33.8% on IFEval, surpassing GPT-4 on both benchmarks."
  - [section]: "When weakly supervised with GPT-4-Turbo’s CoT reasoning trails, Mistral significantly improves overall performance and the ability to detecting unmet constraints (+12.9 in Macro F1 and +28.1 in F1 Neg.)."
- **Break condition**: If the weak labels are too noisy or reasoning trails do not align with the open-source model’s reasoning style, fine-tuning may not improve or could degrade performance.

### Mechanism 3
- **Claim**: Iterative refinement with targeted feedback leads to better constraint compliance than single-pass generation.
- **Mechanism**: After an initial response, the Critic identifies unsatisfied constraints and provides specific feedback. The LLM then refines the response to address only those issues, iterating until all constraints are met or a max iteration count is reached.
- **Core assumption**: The Critic can accurately detect unsatisfied constraints and provide actionable feedback that the LLM can use to correct its output.
- **Evidence anchors**:
  - [abstract]: "Our results show that DECRIM improves Mistral’s performance by 7.3% on RealInstruct and 8.0% on IFEval even with weak feedback."
  - [section]: "With an Oracle Decomposer and Supervised Critic, performance increased by +5.6 and +4.8, respectively."
- **Break condition**: If the Critic’s feedback is too vague, incorrect, or the LLM fails to understand it, the refinement loop may not converge or may produce worse outputs.

## Foundational Learning

- **Concept**: Instruction decomposition into task, context, and constraints.
  - Why needed here: Enables fine-grained evaluation and targeted refinement; essential for multi-constraint scenarios.
  - Quick check question: If an instruction is "Write a 100-word summary in formal tone," what are the constraints?
    - Answer: Word limit ≤ 100, tone must be formal.

- **Concept**: Chain-of-Thought prompting for evaluation consistency.
  - Why needed here: Improves reasoning quality and consistency of LLM-as-a-Judge, especially for constraint verification.
  - Quick check question: What is the difference between evaluating constraints individually vs. collectively?
    - Answer: Individual evaluation isolates each constraint’s compliance, reducing ambiguity; collective evaluation can conflate multiple issues into one decision.

- **Concept**: Weak supervision with reasoning trails.
  - Why needed here: Allows open-source models to learn from strong proprietary model behavior without full manual labeling.
  - Quick check question: What type of data is used to fine-tune the open-source Critic?
    - Answer: Weak constraint satisfaction annotations plus GPT-4-Turbo’s CoT reasoning trails.

## Architecture Onboarding

- **Component map**: User instruction → Initial Response (LLM) → Decomposer → List of granular constraints → Critic → Feedback → Refine → Loop (if needed) → Final output
- **Critical path**: Initial Response → Decompose → Critique → Refine (if needed) → Final output. Failure at Critique step halts progress; failure at Refine step may degrade output.
- **Design tradeoffs**:
  - Granularity vs. overhead: More granular constraints improve evaluation but increase decomposition and critique cost.
  - Weak vs. strong Critic: Weak Critic reduces reliance on proprietary APIs but may provide less reliable feedback.
  - Max iterations vs. latency: More iterations improve compliance but increase total inference time.
- **Failure signatures**:
  - No improvement after multiple iterations → Critic feedback may be incorrect or LLM cannot interpret it.
  - Degradation after refinement → Over-refinement or misaligned feedback.
  - High iteration count → Constraints are hard to satisfy or Critic is too strict.
- **First 3 experiments**:
  1. Baseline: Run the pipeline with Self-Decomposer and Self-Critic on a small REAL INSTRUCT subset; measure instruction-level accuracy vs. Make Sure baseline.
  2. Oracle Decomposer test: Replace Self-Decomposer with ground-truth constraints; compare performance to assess decomposition impact.
  3. Weak Critic training: Fine-tune Mistral with weak labels + CoT reasoning; evaluate constraint satisfaction accuracy on EvalJudge dataset.

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions beyond those implied by the limitations and future work discussion.

## Limitations
- Instruction decomposition is not always perfect, affecting the effectiveness of the DECRIM pipeline.
- The open-source Critic model's performance, while improved through weak supervision, still lags behind GPT-4.
- The study does not explore the impact of different decomposition strategies or the optimal number of refinement iterations.

## Confidence
- **High Confidence**: The claim that DECRIM improves instruction-following performance on REAL INSTRUCT and IFEval benchmarks.
- **Medium Confidence**: The claim that weak supervision with Chain-of-Thought reasoning trails significantly improves open-source Critic performance.
- **Low Confidence**: The generalizability of the results to other types of multi-constrained instructions or domains beyond those in the REAL INSTRUCT and IFEval datasets.

## Next Checks
1. **Cross-domain validation**: Test DECRIM on multi-constrained instructions from different domains (e.g., legal, medical) to assess generalizability.
2. **Ablation study on decomposition**: Compare DECRIM performance with different decomposition strategies (e.g., rule-based vs. LLM-based) to isolate the impact of the Decomposer.
3. **Robustness to Critic noise**: Evaluate DECRIM's performance with artificially degraded Critic feedback to understand the pipeline's resilience to Critic errors.