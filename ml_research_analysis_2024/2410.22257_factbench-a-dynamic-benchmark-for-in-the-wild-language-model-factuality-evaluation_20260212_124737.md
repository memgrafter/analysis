---
ver: rpa2
title: 'FactBench: A Dynamic Benchmark for In-the-Wild Language Model Factuality Evaluation'
arxiv_id: '2410.22257'
source_url: https://arxiv.org/abs/2410.22257
tags:
- uni00000003
- uni0000004c
- uni00000056
- uni00000048
- uni00000052
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FACTBENCH, a dynamic benchmark for evaluating
  the factuality of large language models (LLMs) in real-world scenarios. The authors
  propose VERIFY, a pipeline that assesses factuality by retrieving evidence from
  the web and categorizing content units as supported, unsupported, or undecidable.
---

# FactBench: A Dynamic Benchmark for In-the-Wild Language Model Factuality Evaluation

## Quick Facts
- arXiv ID: 2410.22257
- Source URL: https://arxiv.org/abs/2410.22257
- Authors: Farima Fatahi Bayat; Lechen Zhang; Sheza Munir; Lu Wang
- Reference count: 40
- Key outcome: FACTBENCH is a dynamic benchmark with 1,000 hallucination prompts across 150 topics, showing proprietary models outperform open-source models, with VERIFY achieving highest correlation with human evaluations.

## Executive Summary
This paper introduces FACTBENCH, a dynamic benchmark for evaluating the factuality of large language models (LLMs) in real-world scenarios. The authors propose VERIFY, a pipeline that assesses factuality by retrieving evidence from the web and categorizing content units as supported, unsupported, or undecidable. FACTBENCH comprises 1,000 hallucination prompts across 150 topics, categorized into Hard, Moderate, and Easy tiers based on model performance. Experiments with frontier models from GPT, Gemini, and Llama families reveal that proprietary models exhibit better factuality, with performance declining from Easy to Hard prompts. Notably, Llama3.1-405B-Instruct shows comparable or lower factual precision than Llama3.1-70B-Instruct due to higher subjectivity leading to more undecidable units. Gemini1.5-Pro demonstrates a significantly higher refusal rate, with over-refusal in 25% of cases. The VERIFY pipeline achieves the highest correlation with human evaluations compared to existing methods, underscoring its effectiveness in factuality assessment.

## Method Summary
The FACTBENCH framework consists of a data collection pipeline that generates hallucination prompts, clusters them into topics, and evaluates model responses using the VERIFY pipeline. VERIFY decomposes responses into independent content units, generates evidence-retrieval queries, retrieves web evidence, and categorizes each unit as supported, unsupported, or undecidable. The benchmark is designed to be dynamic, continuously incorporating new prompts based on LM performance. The evaluation uses 1,000 prompts across 150 topics, categorized into three difficulty tiers based on model performance, and compares multiple frontier models from GPT, Gemini, and Llama families.

## Key Results
- Proprietary models (GPT-4, Gemini-1.5-Pro) outperform open-source models (Llama-3.1) in factuality across all difficulty tiers
- Llama3.1-405B-Instruct shows comparable or lower factual precision than Llama3.1-70B-Instruct due to higher undecidable rates
- VERIFY achieves highest correlation with human evaluations among existing factuality assessment methods
- FACTBENCH shows consistent performance degradation from Easy to Hard prompts, validating its tiered difficulty structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VERIFY's unit-based decomposition approach enables more precise factuality assessment than whole-response evaluation methods.
- Mechanism: By breaking down LM responses into independent, self-contained units and evaluating each unit against web evidence, VERIFY isolates factual errors at the atomic level, preventing error propagation from context dependencies.
- Core assumption: Independent units can be meaningfully extracted from LM responses and accurately verified without losing critical context.
- Evidence anchors:
  - [abstract] "VERIFY considers the verifiability of LM-generated content and categorizes units into supported, unsupported, or undecidable according to retrieval results"
  - [section 4.2] "We first decompose the model response into independent content units"
- Break condition: When units cannot be effectively decontextualized or when inter-unit dependencies are essential for verification accuracy.

### Mechanism 2
- Claim: The three-tier categorization (supported, unsupported, undecidable) provides more nuanced factuality assessment than binary supported/refuted approaches.
- Mechanism: VERIFY's inclusion of an "undecidable" category captures cases where evidence is insufficient or ambiguous, preventing false negatives in factuality evaluation while acknowledging uncertainty.
- Core assumption: Some factual claims require additional context or external knowledge beyond available web evidence for definitive verification.
- Evidence anchors:
  - [abstract] "categorizes content units as supported, unsupported, or undecidable based on Web-retrieved evidence"
  - [section 4.1] "Context-dependent Statements: These statements require additional information for verification"
- Break condition: When the undecidable category becomes overused due to overly conservative verification thresholds, reducing practical utility.

### Mechanism 3
- Claim: Dynamic prompt selection based on LM response hallucination scores creates a more challenging and relevant benchmark over time.
- Mechanism: By continuously incorporating prompts that elicit incorrect or inconclusive responses from strong LMs, FACTBENCH adapts to evolving LM capabilities and maintains difficulty appropriate to current model performance.
- Core assumption: LMs' factuality capabilities improve over time, making previously challenging prompts easier and requiring new, more difficult prompts to maintain benchmark relevance.
- Evidence anchors:
  - [abstract] "FACTBENCH is designed to be updatable by continuously incorporating newly collected hallucination prompts"
  - [section 5] "Easy prompts are less likely to trigger hallucinations in strong models"
- Break condition: When prompt collection and evaluation become too resource-intensive relative to benchmark maintenance benefits.

## Foundational Learning

- Concept: Verifiability classification in natural language
  - Why needed here: To distinguish between claims that can be verified with available evidence and those requiring additional context or knowledge
  - Quick check question: Given the statement "The meeting was productive," is this verifiable without additional context? Why or why not?

- Concept: Web evidence retrieval and query optimization
  - Why needed here: To effectively retrieve relevant evidence for verifying factual claims from LM responses
  - Quick check question: How would you modify the query "RTX 3060 memory bandwidth" to improve retrieval of specific technical specifications?

- Concept: Unit decomposition and decontextualization techniques
  - Why needed here: To break down complex LM responses into independently verifiable components while preserving essential meaning
  - Quick check question: What challenges arise when decomposing the statement "The iPhone 15 Pro is better than the iPhone 14 Pro because it has a titanium frame" into independent units?

## Architecture Onboarding

- Component map: Data collection → Clustering → Verifiability classification → Usefulness scoring → Hallucination evaluation → Unit extraction → Decontextualization → Query generation → Evidence retrieval → Final judgment
- Critical path: Response generation → Unit extraction → Evidence retrieval → Final judgment → Hallucination scoring
- Design tradeoffs: Granularity vs. context retention in unit decomposition; evidence sufficiency vs. verification confidence in undecidable labeling
- Failure signatures: High undecidable rates indicating overly conservative verification; low accuracy suggesting insufficient evidence retrieval or poor unit extraction
- First 3 experiments:
  1. Test unit extraction accuracy on diverse response types (technical, conversational, opinion-based)
  2. Evaluate query generation effectiveness by measuring relevance of top-5 search results
  3. Compare hallucination score sensitivity across different α values for undecidable weighting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can VERIFY be extended to leverage multiple LMs for unit extraction and annotation while managing computational costs and inter-model collaboration?
- Basis in paper: [explicit] The paper explicitly identifies this as a limitation, noting that VERIFY currently employs a single language model for text decomposition and unit annotation, and suggesting that extending the framework to leverage multiple LMs could enhance evaluation diversity and mitigate individual model biases.
- Why unresolved: While the potential benefits of multi-model factuality evaluation systems are clear, significant methodological and computational challenges remain. These include determining the optimal architecture for model collaboration, designing effective inter-model coordination mechanisms, and balancing enhanced robustness against computational costs.
- What evidence would resolve it: A systematic evaluation comparing single-model versus multi-model approaches across various factuality benchmarks, including quantitative metrics of accuracy, computational efficiency, and robustness to different types of factual errors, would provide concrete evidence for the optimal multi-model factuality evaluation design.

### Open Question 2
- Question: How can factuality evaluation methods effectively address the recall challenge in open-ended queries where defining exhaustive relevant factual statements is inherently difficult?
- Basis in paper: [explicit] The paper explicitly identifies this as a limitation, noting that the absence of recall measurements is particularly salient for open-ended queries, using movie recommendation scenarios as an example where models may produce accurate but incomplete information.
- Why unresolved: Current factuality evaluation metrics primarily focus on precision of evaluated statements but do not adequately capture the completeness of responses. The challenge lies in defining what constitutes an exhaustive set of relevant factual statements in open-ended scenarios, especially when different models may generate varying but equally valid responses.
- What evidence would resolve it: Development and validation of recall-aware factuality metrics specifically designed for open-ended queries, potentially incorporating techniques like coverage estimation or user-centered completeness measures, would provide a framework for addressing this limitation.

### Open Question 3
- Question: How can factuality evaluation metrics be enhanced to account for both individual factual support and logical connections between units to verify response-wide coherence?
- Basis in paper: [explicit] The paper suggests this as a future direction, noting that while VERIFY strongly correlates with human judgments on evaluated statements, addressing the challenge of verifying not only factual precision but also response-wide coherence remains crucial for high-stakes applications.
- Why unresolved: Current factuality evaluation methods typically assess individual units in isolation, potentially missing important logical relationships and coherence between statements within a response. This limitation is particularly relevant for complex queries requiring multi-step reasoning or where the validity of one statement depends on another.
- What evidence would resolve it: Development and validation of coherence-aware factuality evaluation frameworks that can identify and verify logical relationships between units, potentially using techniques from natural language inference or discourse analysis, would provide evidence for how to enhance factuality metrics to capture response-wide coherence.

## Limitations

- The benchmark relies heavily on web evidence for verification, which may be insufficient for specialized knowledge or real-time information
- The undecidable category may reflect limitations in evidence retrieval or overly conservative verification thresholds rather than genuine verification uncertainty
- High computational costs associated with continuous prompt collection and evaluation may limit long-term benchmark maintenance

## Confidence

- Medium-High: The systematic evaluation of 1,000 prompts across 150 topics provides robust empirical support for the benchmark's design and effectiveness. The observed performance differences between model families are consistent and statistically significant.

## Next Checks

1. **Unit Decomposition Robustness Test**: Evaluate the unit extraction algorithm across diverse response domains (technical specifications, narrative descriptions, comparative analyses) to identify failure modes where context loss affects verification accuracy. Test whether manually curated units yield different results than algorithmically extracted ones.

2. **Evidence Sufficiency Analysis**: Systematically compare verification outcomes when using different evidence sources (web search vs. specialized databases vs. real-time information sources) to quantify how much verification uncertainty stems from evidence limitations versus claim ambiguity.

3. **Temporal Stability Assessment**: Re-run the benchmark evaluation after 6 months using the same prompts to measure how changes in web content, search engine algorithms, and LM capabilities affect factuality scores and undecidable rates, establishing the benchmark's sensitivity to temporal factors.