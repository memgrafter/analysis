---
ver: rpa2
title: 'Representation learning in multiplex graphs: Where and how to fuse information?'
arxiv_id: '2402.17906'
source_url: https://arxiv.org/abs/2402.17906
tags:
- graph
- node
- multiplex
- learning
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates representation learning in multiplex graphs
  by exploring different information fusion strategies across the processing pipeline.
  The authors propose a taxonomy categorizing fusion approaches into graph-level,
  GNN-level, embedding-level, and prediction-level fusion.
---

# Representation learning in multiplex graphs: Where and how to fuse information?

## Quick Facts
- arXiv ID: 2402.17906
- Source URL: https://arxiv.org/abs/2402.17906
- Authors: Piotr Bielak; Tomasz Kajdanowicz
- Reference count: 23
- Primary result: DMGI achieves best performance on most datasets but suffers from limited inductivity

## Executive Summary
This paper investigates representation learning in multiplex graphs by exploring different information fusion strategies across the processing pipeline. The authors propose a taxonomy categorizing fusion approaches into graph-level, GNN-level, embedding-level, and prediction-level fusion. Through extensive experiments on six real-world datasets, they evaluate existing methods and propose novel extensions. The study reveals that DMGI achieves the best performance on most datasets, but suffers from limited inductivity. The authors identify key research gaps, including the need for dedicated GNN architectures for multiplex graphs, improved self-supervised methods beyond DGI, and approaches that can handle dynamic addition of graph layers.

## Method Summary
The paper evaluates multiplex graph representation learning methods by applying various fusion strategies to combine information from multiple graph layers. The authors implement and compare existing methods (MHGCN, HDGI, DMGI, S2MGRL) alongside baseline approaches (Layers, Features) across six real-world datasets. The evaluation framework tests node classification, clustering, and similarity search tasks using frozen embeddings. A novel graph-level fusion approach using learnable edge weights is proposed and evaluated alongside the existing methods.

## Key Results
- DMGI lookup embedding-based fusion consistently delivers the best results on node classification across most datasets
- GNN-level fusion using attention mechanisms generally outperforms simpler strategies but varies by dataset
- Graph-level fusion with learnable edge weights shows promise on Freebase but underperforms on other datasets
- No single fusion strategy dominates across all tasks and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GNN-level fusion using trainable attention mechanisms outperforms simpler fusion strategies by adaptively weighting the importance of different graph layers.
- Mechanism: The model applies a GNN backbone to each graph layer independently, then uses a trainable attention mechanism to compute weighted combinations of the resulting node embeddings. This allows the model to learn which layers are most informative for specific downstream tasks.
- Core assumption: Different graph layers contain complementary information, and the relative importance of each layer varies depending on the task and dataset.
- Evidence anchors:
  - [abstract] states that "fusion at the GNN-level using attention mechanisms or lookup embeddings generally outperforms other strategies"
  - [section] shows that "HDGI's attention mechanism proves to perform similarly to DMGI on ACM and Freebase, worse on IMDB, Cora and CiteSeer, but on the Amazon dataset, we observe an approx. 35 pp difference"
  - [corpus] evidence is weak - no directly related papers specifically discussing attention-based GNN-level fusion for multiplex graphs
- Break condition: If the attention mechanism converges to uniform weights across layers, or if certain layers contain predominantly noise, the performance gain would diminish.

### Mechanism 2
- Claim: Lookup embedding-based fusion (DMGI) achieves the best overall performance by learning a dedicated embedding space that captures multiplex graph structure.
- Mechanism: DMGI applies a GCN backbone with DGI objective at each graph layer, then learns fused node representations as a trainable embedding lookup matrix. An additional loss term minimizes distance between original and corrupted graph embeddings.
- Core assumption: A dedicated embedding space can better capture the complex relationships in multiplex graphs than direct fusion of layer-wise embeddings.
- Evidence anchors:
  - [abstract] states "DMGI achieves the best performance on most datasets"
  - [section] shows "The lookup embedding-based fusion approach of the DMGI method consistently delivers the best results on the node classification task compared to the other two approaches"
  - [corpus] evidence is missing - no directly related papers discussing lookup embedding approaches for multiplex graphs
- Break condition: The lookup embedding approach is transductive, meaning it cannot handle new nodes or graph layers without retraining the entire model.

### Mechanism 3
- Claim: Graph-level fusion by combining adjacency matrices with learnable weights can effectively integrate information across layers when negative edge sampling is manageable.
- Mechanism: The MHGCN method learns K edge weights (one per layer) and combines adjacency matrices into a single weighted sum, which is then processed by a standard GCN backbone trained with link prediction objective.
- Core assumption: The relative importance of edges across different layers can be captured by a simple weighted sum, and the link prediction objective with negative sampling is effective for learning these weights.
- Evidence anchors:
  - [section] shows "For the Freebase dataset, we observe an approx. 11 pp improvement in the node classification task along with substantial improvement in the similarity search task"
  - [section] also notes "The MHGCN method did not perform well on the ACM, Amazon and IMDB datasets compared to baseline approaches"
  - [corpus] evidence is weak - no directly related papers specifically discussing learnable weighted adjacency matrix fusion
- Break condition: When graph layers have vastly different edge densities or when negative edge sampling becomes computationally prohibitive due to large graph sizes.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and message-passing paradigm
  - Why needed here: The entire paper builds upon GNN architectures for processing multiplex graphs, with various fusion strategies modifying how information flows through these networks
  - Quick check question: Can you explain the difference between message generation and message aggregation in a GNN layer?

- Concept: Contrastive learning and self-supervised objectives (DGI, GRACE, GraphCL)
  - Why needed here: Most multiplex graph representation learning methods are based on contrastive learning frameworks, adapting them to handle multiple graph layers
  - Quick check question: How does the DGI objective maximize mutual information between node embeddings and graph embeddings?

- Concept: Attention mechanisms and trainable fusion operators
  - Why needed here: Several fusion strategies in the paper use attention mechanisms or learnable projection layers to combine information from different graph layers
  - Quick check question: What is the difference between a semantic attention mechanism and a standard attention mechanism in the context of graph representation learning?

## Architecture Onboarding

- Component map: Multiplex graph with K layers → GNN backbone (typically GCN) → Fusion mechanism (attention, lookup embedding, concatenation, etc.) → Downstream task modules
- Critical path: For GNN-level fusion with attention: multiplex graph → GCN on each layer → attention-weighted combination → fused embeddings → downstream task
- Design tradeoffs: Attention-based fusion is inductive but may be less effective than lookup embeddings; lookup embeddings achieve better performance but lack inductivity; graph-level fusion is simple but sensitive to negative sampling quality and edge density imbalances
- Failure signatures: Attention weights collapsing to uniform distribution, lookup embeddings failing to generalize to new nodes, link prediction objectives failing due to negative sampling issues, or downstream task performance degrading when adding new graph layers
- First 3 experiments:
  1. Implement and compare simple averaging vs. attention-based fusion on a small multiplex dataset (like Cora/CiteSeer with added KNN layer) to observe performance differences
  2. Test inductivity by training on a subset of nodes and evaluating on held-out nodes for different fusion strategies
  3. Evaluate the impact of negative sampling ratio in graph-level fusion methods by varying the number of negative samples per positive edge

## Open Questions the Paper Calls Out

- Can dedicated GNN architectures designed specifically for multiplex graphs outperform existing methods that apply standard GNNs layer-by-layer?
  - Basis in paper: [explicit] The paper states "We did not find any attempts to build dedicated GNN layer architectures designed for multiplex graph data" and proposes this as a future research direction.
  - Why unresolved: Current methods treat each graph layer independently and rely on post-fusion, potentially missing cross-layer interactions that could be captured by a unified architecture.
  - What evidence would resolve it: Empirical comparison showing superior performance of a novel multiplex-specific GNN architecture against existing layer-wise approaches across multiple datasets and tasks.

- How can self-supervised multiplex graph representation learning methods be developed that work fully in an unsupervised fashion without requiring node labels for any part of the training?
  - Basis in paper: [explicit] The paper notes that S2MGRL "does not work in a fully unsupervised fashion and requires node labels to train the fusion mechanism" and identifies this as a limitation.
  - Why unresolved: Current state-of-the-art methods either use DGI-based approaches or require labels for attention mechanisms, leaving a gap in truly self-supervised methods.
  - What evidence would resolve it: Development and validation of a self-supervised method achieving competitive performance without any labeled data throughout training.

- What approaches can enable efficient dynamic updates to multiplex graph representations when new layers are added, without requiring full model retraining?
  - Basis in paper: [explicit] The authors identify that "virtually all methods are incapable of such behavior, so adding a new graph layer requires a whole model retraining" and propose this as a research gap.
  - Why unresolved: Current methods are designed for static multiplex graphs, making them impractical for dynamic real-world applications where new relationship types emerge.
  - What evidence would resolve it: Demonstration of a method that can incorporate new graph layers with minimal computational cost and maintain or improve performance on existing tasks.

## Limitations

- Experimental scope limited to six real-world datasets with relatively small scale (max ~30k nodes)
- Evaluation focuses on unsupervised/self-supervised settings without comparing against supervised multiplex graph methods
- Computational efficiency of different fusion approaches not extensively explored

## Confidence

- **High confidence**: GNN-level fusion using attention mechanisms outperforms simpler strategies on most tested datasets
- **Medium confidence**: DMGI lookup embedding approach achieves best overall performance
- **Medium confidence**: No single fusion strategy dominates across all tasks and datasets

## Next Checks

1. Evaluate the same fusion strategies on larger multiplex graphs (10M+ nodes) to assess computational efficiency and performance degradation patterns
2. Design experiments specifically measuring how well each fusion strategy handles dynamic addition of new graph layers and nodes
3. Implement and compare against state-of-the-art supervised multiplex graph methods to establish the true value proposition of unsupervised fusion strategies in different data availability scenarios