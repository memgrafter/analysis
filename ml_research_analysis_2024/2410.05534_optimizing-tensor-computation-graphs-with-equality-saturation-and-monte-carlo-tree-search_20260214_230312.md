---
ver: rpa2
title: Optimizing Tensor Computation Graphs with Equality Saturation and Monte Carlo
  Tree Search
arxiv_id: '2410.05534'
source_url: https://arxiv.org/abs/2410.05534
tags:
- greedy
- e-graph
- rewrite
- cost
- default
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose using Monte Carlo tree search (MCTS) to guide
  the construction of an equality saturation (e-graph) IR for tensor program optimization,
  mitigating the phase-ordering problem that arises when memory constraints prevent
  the e-graph from representing all possible rewrites. They also introduce a novel
  greedy extraction algorithm that accurately estimates tensor program runtimes by
  tracking common subexpression costs, enabling faster and more reliable optimization
  compared to integer linear programming.
---

# Optimizing Tensor Computation Graphs with Equality Saturation and Monte Carlo Tree Search

## Quick Facts
- arXiv ID: 2410.05534
- Source URL: https://arxiv.org/abs/2410.05534
- Reference count: 40
- Authors achieve up to 11% higher inference speedups than existing methods

## Executive Summary
This paper addresses the challenge of optimizing tensor computation graphs for deep learning models by combining equality saturation with Monte Carlo tree search (MCTS). The key innovation is using MCTS to guide the construction of equality saturation (e-graph) intermediate representations, mitigating the phase-ordering problem that arises when memory constraints prevent the e-graph from representing all possible rewrites. The authors introduce a novel greedy extraction algorithm that accurately estimates tensor program runtimes by tracking common subexpression costs, enabling faster and more reliable optimization compared to integer linear programming. On 13 deep learning models, their approach achieves up to 11% higher inference speedups than existing methods while providing a flexible trade-off between optimization time and performance.

## Method Summary
The approach uses MCTS to guide e-graph construction by selecting promising rewrite rules from TASO's set of 124 single-pattern and 15 multi-pattern rules. During construction, an improved cost function tracks constituent costs for each e-class and e-node, enabling accurate greedy extraction even with common subexpressions. The e-graph is built until saturation or a memory limit of 2000 nodes, after which final extraction is performed using either ILP (for guaranteed optimality) or greedy extraction with the improved cost function. The system is implemented by integrating TASO, egg, TENSAT, MCTS-GEB, and custom optimization components.

## Key Results
- Achieves up to 11% higher inference speedups compared to existing methods on 13 deep learning models
- Introduces a novel greedy extraction algorithm that accurately estimates tensor program runtimes
- Provides flexible trade-off between optimization time and performance through adjustable MCTS budget and extraction method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MCTS guides e-graph construction to apply the most promising rewrite rules first, reducing phase-ordering impact.
- Mechanism: MCTS explores rewrite rule sequences via UCB1 balancing exploitation (high past rewards) and exploration (low visit counts). The reward is cumulative runtime improvement from simulated e-graph states. This allows focusing search on high-value rewrite paths before memory limits are hit.
- Core assumption: Runtime improvement estimates from simulated e-graphs correlate with true optimization potential.
- Evidence anchors:
  - [abstract] "Monte Carlo tree search to build superior IRs by identifying the most promising rewrite rules"
  - [section 4.1] "MCTS iteratively works through four steps (selection, expansion, simulation, update) to grow the search tree into the most promising areas of the search space"
  - [corpus] Weak - no direct MCTS evidence in cited papers
- Break condition: If runtime estimates are noisy or reward signals misleading, MCTS may converge to suboptimal rewrite paths.

### Mechanism 2
- Claim: The proposed cost function correctly handles common subexpressions, enabling accurate greedy extraction.
- Mechanism: Instead of naive summing of children e-class costs, the cost function tracks constituent costs (histories) for each e-class/e-node. When an e-class appears multiple times in a subgraph, its cost is only counted once via overlapping constituent cost detection.
- Core assumption: Constituent cost tracking is accurate enough to prevent overestimation while still being fast to compute.
- Evidence anchors:
  - [abstract] "introduce a novel extraction algorithm that can provide fast and accurate runtime estimates of tensor programs"
  - [section 4.2] "We propose a novel cost function that allows greedy extractors to handle common subexpressions"
  - [section 4.2] "The main idea behind our proposed solution is to keep track of the constituent costs of each e-class and e-node"
- Break condition: In extremely nested NAS-generated structures, overlapping constituent costs may still be treated incorrectly.

### Mechanism 3
- Claim: Using ILP for final extraction after MCTS-based construction provides the best trade-off between speed and optimality.
- Mechanism: MCTS provides a good e-graph construction path, and then ILP extracts the optimal program from the reduced search space. This avoids the exponential scaling of ILP alone while still guaranteeing optimality in the final step.
- Core assumption: The e-graph built by MCTS is small enough for ILP to complete in reasonable time.
- Evidence anchors:
  - [abstract] "ILPs are guaranteed to find the optimal solution, their search time scales exponentially with the e-graph size"
  - [section 4.2] "Using an ILP instead of a greedy extractor for the final extraction step further improves the output programs' runtime"
  - [section 4.3] "Depending on the size of the e-graph, it is often feasible to use an ILP extractor for this final step"
- Break condition: If MCTS builds an e-graph still too large for ILP, extraction may time out or be infeasible.

## Foundational Learning

- Concept: Equality Saturation (e-graphs)
  - Why needed here: The core optimization framework that stores multiple program versions without destructive rewrites
  - Quick check question: What is the difference between traditional term rewriting and equality saturation in terms of information preservation?

- Concept: Monte Carlo Tree Search (MCTS)
  - Why needed here: Provides a search strategy to decide which rewrite rules to apply during e-graph construction
  - Quick check question: How does the UCB1 formula balance exploration and exploitation in MCTS?

- Concept: Cost Modeling for Tensor Programs
  - Why needed here: Essential for estimating runtime of tensor computation graphs to guide optimization decisions
  - Quick check question: Why do naive cost functions fail for tensor programs with common subexpressions?

## Architecture Onboarding

- Component map: TASO -> egg -> TENSAT -> MCTS-GEB -> Custom optimizer
- Critical path:
  1. Initialize e-graph with input tensor program
  2. Run MCTS to select next rewrite rule
  3. Apply selected rule and update e-graph
  4. Repeat until saturation or memory limit
  5. Extract best program (ILP preferred, greedy with improved cost as fallback)

- Design tradeoffs:
  - MCTS budget vs. optimization quality: Higher budgets improve results but increase runtime
  - ILP vs. greedy extraction: ILP guarantees optimality but doesn't scale; greedy with improved cost is faster but heuristic
  - Memory limit for e-graph: Tighter limits force earlier termination but may miss optimal solutions

- Failure signatures:
  - Optimizer produces slower programs than original: Likely cost function overestimation or MCTS selecting wrong rewrite paths
  - Optimization times too long: May need to reduce MCTS budget or increase memory limit
  - No improvement across models: Check if rewrite rules are being applied or if TASO cost measurements are accurate

- First 3 experiments:
  1. Run optimizer on a simple model (e.g., VGG-19) with default settings to verify basic functionality
  2. Compare default cost function vs. improved cost function on MobileNet-v2 to observe impact on extraction accuracy
  3. Test MCTS vs. TENSAT on ResNet-50 to measure performance improvement from guided search

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the limitations section, several important questions remain unanswered:

1. How does the proposed approach perform when optimizing neural networks with more complex architectures, such as those with dynamic control flow or conditional operations?

2. How sensitive is the performance of the proposed approach to the choice of the exploration constant in the UCB1 formula used by MCTS?

3. Can the proposed approach be extended to optimize neural networks for different hardware platforms, such as CPUs or specialized accelerators?

## Limitations

- Effectiveness depends heavily on accurate runtime cost modeling, which may struggle with extremely nested or complex NAS-generated structures
- MCTS-guided e-graph construction requires careful hyperparameter tuning, and suboptimal parameter choices could lead to premature convergence on local optima
- Reliance on TASO for rewrite rules and cost measurements means the approach inherits any limitations or inaccuracies in those components

## Confidence

**High confidence**: The mechanism of using MCTS to guide e-graph construction is well-established in other domains, and the core equality saturation framework is proven. The improved cost function for handling common subexpressions follows logically from identified limitations in existing approaches.

**Medium confidence**: The specific implementation details for integrating MCTS with equality saturation in the tensor optimization context are novel and may have unforeseen interactions. The claimed 11% speedup improvement is based on results from 13 models but may not generalize to all tensor computation scenarios.

**Low confidence**: The claim that ILP extraction is feasible for the e-graphs produced by MCTS is based on empirical observation rather than theoretical guarantees, and may fail for certain complex models.

## Next Checks

1. **Reproduce baseline comparison**: Implement the improved cost function extraction on TENSAT without MCTS guidance and measure the impact on optimization accuracy to isolate the contribution of each component.

2. **Stress test cost function**: Create synthetic tensor programs with known optimal solutions featuring complex common subexpression patterns to verify the improved cost function accurately estimates runtimes across edge cases.

3. **MCTS hyperparameter sensitivity**: Systematically vary MCTS budget and UCB1 exploration constant across a range of models to identify optimal settings and measure the impact of parameter choices on final optimization quality.