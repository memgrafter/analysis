---
ver: rpa2
title: Toward Inference-optimal Mixture-of-Expert Large Language Models
arxiv_id: '2404.02852'
source_url: https://arxiv.org/abs/2404.02852
tags:
- cost
- training
- inference
- experts
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the scaling behavior of MoE-based LLMs with
  respect to model size, dataset size, and number of experts. The authors propose
  an extended scaling law that captures the power-law relationship between validation
  loss and these three factors, while also incorporating a saturation effect for the
  number of experts.
---

# Toward Inference-optimal Mixture-of-Expert Large Language Models

## Quick Facts
- **arXiv ID**: 2404.02852
- **Source URL**: https://arxiv.org/abs/2404.02852
- **Reference count**: 31
- **Key outcome**: This paper studies the scaling behavior of MoE-based LLMs with respect to model size, dataset size, and number of experts. The authors propose an extended scaling law that captures the power-law relationship between validation loss and these three factors, while also incorporating a saturation effect for the number of experts. Additionally, they introduce inference efficiency as a key metric alongside validation loss to optimize the training budget allocation for MoE models. The results show that MoE models with fewer experts (4/8) are more inference-efficient but require 2.5-3.5x more training budget to achieve the same performance as models with more experts. Training a smaller MoE model with more experts and a larger dataset can outperform larger models with fewer experts in both quality and inference efficiency under the same training budget.

## Executive Summary
This paper investigates the scaling behavior of Mixture-of-Expert (MoE) large language models (LLMs) by extending traditional scaling laws to incorporate the number of experts and inference efficiency. The authors propose an extended scaling law that captures the power-law relationship between validation loss and model size, dataset size, and number of experts, while also incorporating a saturation effect for the number of experts. They introduce inference efficiency as a key metric alongside validation loss to optimize the training budget allocation for MoE models. The results show that MoE models with fewer experts (4/8) are more inference-efficient but require 2.5-3.5x more training budget to achieve the same performance as models with more experts. Training a smaller MoE model with more experts and a larger dataset can outperform larger models with fewer experts in both quality and inference efficiency under the same training budget.

## Method Summary
The authors propose an extended scaling law for MoE models that incorporates model size, dataset size, and number of experts. They train MoE models with varying configurations (4, 8, 16, 32 experts) and sizes (100M-730M parameters) on datasets ranging from 2.5B to 20B tokens. The models are trained using AdamW optimizer with learning rates adjusted based on model size, data, tensor, and model parallelism on up to 32 A100 GPUs. Inference costs are profiled using vLLM on 8x40GB A100 GPUs, measuring latency and throughput for different batch sizes. The authors use the proposed scaling law to find the optimal configuration balancing model quality and inference cost under fixed training budgets.

## Key Results
- MoE models with fewer experts (4/8) are more inference-efficient but require 2.5-3.5x more training budget to achieve the same performance as models with more experts.
- Training a smaller MoE model with more experts and a larger dataset can outperform larger models with fewer experts in both quality and inference efficiency under the same training budget.
- The extended scaling law captures the power-law relationship between validation loss and model size, dataset size, and number of experts, while also incorporating a saturation effect for the number of experts.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding more experts to an MoE model initially improves performance, but the marginal gain diminishes and eventually saturates.
- Mechanism: The validation loss decreases with more experts according to a power law, but after a threshold Emax, further increases yield minimal improvement due to routing inefficiencies and expert specialization limits.
- Core assumption: The routing mechanism's error rate remains roughly constant, so a fixed number of tokens are correctly routed regardless of expert count.
- Evidence anchors:
  - [abstract]: "we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation"
  - [section 3.2]: "Based on the sweep of experimental runs, we observe a similar finding to the existing work (Clark et al., 2022), that not all models across N benefit equally from E"
  - [corpus]: Weak evidence from related papers like "Scaling Laws for Fine-Grained Mixture of Experts" but no direct empirical confirmation in this paper's dataset.
- Break condition: If routing becomes significantly more accurate or expert specialization improves, the saturation point could shift higher, invalidating the current Emax assumption.

### Mechanism 2
- Claim: Fewer experts (4/8) provide better inference efficiency at the cost of requiring more training budget to reach the same quality as models with more experts.
- Mechanism: Models with fewer experts have smaller total parameter counts, reducing KV-cache memory usage during inference, which allows larger batch sizes and lower cost per token. However, to match the validation loss of denser-expert models, they need proportionally more training data and compute.
- Core assumption: Inference cost scales linearly with total model size, and batch size is the primary bottleneck for throughput.
- Evidence anchors:
  - [abstract]: "MoEs with a few (4/8) experts are the most serving efficient solution under the same performance, but costs 2.5-3.5x more in training"
  - [section 5.1]: "MoE with fewer experts demands a much higher training budget to reach the same performance"
  - [section 4.2]: "The relationship between inference cost and model size is mostly smooth and monotonic"
- Break condition: If inference hardware or KV-cache optimizations change the memory-bound nature of inference, the efficiency advantage of fewer experts could disappear.

### Mechanism 3
- Claim: Training a smaller MoE model with more experts and more data ("over-trained") can outperform larger models with fewer experts in both quality and inference efficiency under the same training budget.
- Mechanism: By shrinking the model below the loss-optimal size, inference cost drops significantly. The saved budget is reallocated to train on more tokens, which partially offsets the quality loss. This yields a better quality-inference cost tradeoff than loss-optimal configurations.
- Core assumption: The loss-size curve is relatively flat near the optimal point, so moderate undersizing has only marginal impact on validation loss.
- Evidence anchors:
  - [abstract]: "training a (16/32) expert MoE much smaller (70-85%) than the loss-optimal solution, but with a larger training dataset is a promising setup"
  - [section 5.2]: "Such a model, though suffers from a marginal drop in quality, has a significantly lower inference cost"
  - [corpus]: No direct corpus support; this is a novel experimental finding in the paper.
- Break condition: If the loss-size curve steepens at lower sizes, or if dataset quality saturates quickly, the over-training strategy may fail to recover lost quality.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Understanding how MoE layers route tokens and activate subsets of experts is critical to interpreting scaling behavior and inference cost.
  - Quick check question: In an MoE layer with 16 experts and top-2 gating, how many experts are active per token?
- Concept: Power-law scaling relationships
  - Why needed here: The paper's core claim relies on extending dense model scaling laws to include expert count and dataset size.
  - Quick check question: If validation loss scales as L ∝ N^(-α), what happens to L when N doubles and α = 0.5?
- Concept: Inference cost modeling (KV-cache, batch size, throughput)
  - Why needed here: The tradeoff between model size and inference efficiency hinges on memory-bound decoding behavior.
  - Quick check question: If KV-cache per token is 2hl, and total GPU memory is GM0, what is the maximum batch size b in terms of h, l, p, n?

## Architecture Onboarding

- Component map:
  - MoE layer: router + E expert feed-forward networks
  - Gating: top-k routing (k=2) with softmax weights
  - Dense baseline: same hidden dimension and layer count
  - Training: Megatron-Deepspeed, data/tensor/model parallelism
- Critical path:
  1. Token routing through router to top-k experts
  2. Expert computation (only active experts)
  3. Output combination and forwarding to next layer
  4. KV-cache updates during inference
- Design tradeoffs:
  - More experts → higher parameter count → better potential quality but worse inference cost
  - Fewer experts → smaller KV-cache → better inference but requires more training budget
  - Over-training: smaller model + more data → balances quality and inference cost
- Failure signatures:
  - Routing imbalance: some experts get too many tokens, hurting efficiency
  - Underutilization: many experts rarely activated, wasting parameters
  - Memory overflow: KV-cache exceeds GPU memory, forcing tiny batch sizes
- First 3 experiments:
  1. Train MoE-4 and MoE-16 with same dataset size; compare validation loss and inference cost.
  2. Train MoE-16 at 70% of optimal size with 1.5x dataset; measure quality vs. inference cost tradeoff.
  3. Sweep expert count (4,8,16,32) while holding total parameters constant; analyze quality saturation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of experts in MoE models affect inference efficiency compared to dense models under varying GPU memory constraints?
- Basis in paper: [explicit] The paper discusses the trade-off between model quality and inference cost for MoE models with different numbers of experts, and mentions that MoE models with fewer experts (4 or 8) are more inference-efficient but require more training budget.
- Why unresolved: The paper provides a general analysis of the inference cost for MoE models but does not delve into the specific impact of GPU memory constraints on the inference efficiency of models with varying numbers of experts.
- What evidence would resolve it: Profiling the inference efficiency of MoE models with different numbers of experts under various GPU memory constraints and comparing the results to dense models.

### Open Question 2
- Question: How does the saturation threshold (Emax) for the number of experts in MoE models vary with different model architectures and training datasets?
- Basis in paper: [explicit] The paper mentions that the scaling law for MoE models includes a saturation threshold (Emax) to account for the diminishing returns observed when the number of experts becomes excessively large, but does not provide specific values for Emax.
- Why unresolved: The paper does not provide specific values for the saturation threshold (Emax) or discuss how it varies with different model architectures and training datasets.
- What evidence would resolve it: Conducting experiments to determine the saturation threshold (Emax) for MoE models with different architectures and training datasets.

### Open Question 3
- Question: How does the optimal allocation of training budget between model size and dataset size change when considering inference efficiency in addition to validation loss?
- Basis in paper: [explicit] The paper introduces a new perspective to analyze the optimal training budget allocation for MoE models, which considers inference cost as a key component alongside validation loss.
- Why unresolved: The paper provides a general analysis of the trade-off between training budget, inference cost, and model quality, but does not provide specific guidelines on how to allocate the training budget optimally when considering both validation loss and inference efficiency.
- What evidence would resolve it: Developing a mathematical model or framework to determine the optimal allocation of training budget between model size and dataset size, considering both validation loss and inference efficiency.

## Limitations
- Generalizability to larger scales: The scaling law is derived from models with up to 730M parameters and 20B training tokens, and may not capture potential regime changes at larger scales.
- Routing mechanism assumptions: The model assumes a fixed routing error rate independent of expert count, which may not hold for more sophisticated routing mechanisms.
- Inference cost modeling: The simplified latency model may not capture hardware-specific optimizations or future inference architectures that could fundamentally change the efficiency tradeoffs.

## Confidence
- **High confidence**: The observation that MoE models with fewer experts achieve better inference efficiency at the cost of higher training budgets is well-supported by the experimental results.
- **Medium confidence**: The existence and characterization of the expert count saturation effect (Emax) is supported by experimental sweeps, but the exact location of Emax and the shape of the saturation curve may be sensitive to implementation details.
- **Low confidence**: The specific recommendation to train smaller models (70-85% of optimal size) with larger datasets as an inference-optimal strategy is based on a narrow set of experimental conditions and would likely shift with different hardware constraints or routing mechanisms.

## Next Checks
- **Check 1**: Validate the extended scaling law at larger scales by training MoE models with 2-4B parameters and 50-100B training tokens, focusing on whether the expert count saturation point shifts.
- **Check 2**: Implement and compare alternative routing mechanisms (e.g., auxiliary losses, router z-loss, or hash-based routing) to test the assumption of constant routing error rate and measure how different routing strategies affect the saturation behavior.
- **Check 3**: Profile inference costs across different hardware configurations (including H100 GPUs, CPU offloading, and KV-cache compression techniques) to validate the robustness of the efficiency conclusions and test whether the advantage of fewer experts persists with advanced inference optimizations.