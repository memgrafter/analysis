---
ver: rpa2
title: Studying Large Language Model Behaviors Under Context-Memory Conflicts With
  Real Documents
arxiv_id: '2404.16032'
source_url: https://arxiv.org/abs/2404.16032
tags:
- knowledge
- parametric
- answer
- context
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies knowledge-updating behaviors in LLMs under realistic
  context-memory conflicts. Previous work used synthetic counterfactual documents
  that contradict correct model answers, an unrealistic setup.
---

# Studying Large Language Model Behaviors Under Context-Memory Conflicts With Real Documents

## Quick Facts
- arXiv ID: 2404.16032
- Source URL: https://arxiv.org/abs/2404.16032
- Reference count: 40
- Key outcome: Knowledge updates fail in much fewer cases than previously reported when using real factual documents; parametric bias is identified as a key factor in remaining failure cases.

## Executive Summary
This paper studies knowledge-updating behaviors in LLMs under realistic context-memory conflicts using real factual documents, representing a significant methodological improvement over synthetic counterfactual approaches. The authors propose a novel framework that updates incorrect parametric knowledge using real factual documents, better reflecting how knowledge conflicts arise in practice. They find that knowledge updates succeed in substantially more cases than previously reported, with parametric bias identified as a key factor in remaining failure cases. The study demonstrates that factual parametric knowledge of LLMs can negatively influence their reading abilities and behaviors, particularly when the incorrect parametric answer appears in context.

## Method Summary
The authors propose a three-stage experimental pipeline to study knowledge-updating behaviors. First, they collect closed-book answers from LLMs for questions in six open-book QA datasets. Second, they filter out examples without knowledge conflicts using BEM (BertScore-based metric) for answer equivalence and NLI (Natural Language Inference) for conflict detection. Third, they conduct open-book QA under knowledge conflict by providing documents that contain the correct answer but contradict the model's parametric knowledge. They evaluate five LLMs of varying sizes (Llama2-7B, Llama2-70B, Mistral-7B, Mixtral-8x7B, GPT-3.5-Turbo) on six datasets (Natural Questions, SQuAD, NewsQA, TriviaQA, SearchQA, HotpotQA). The framework categorizes results into correct updates (R), parametric failures (U), and incorrect updates (Ui).

## Key Results
- Knowledge updates succeed in substantially more cases (23.8%-48.2%) than previously reported when using real factual documents, compared to synthetic counterfactual setups
- Parametric bias is identified as a key factor in remaining failure cases, where incorrect parametric answers appearing in context make knowledge updates more likely to fail
- Knowledge conflicts with real factual documents can be reliably detected using BEM and NLI models, enabling controlled study of LLM behaviors under realistic conditions

## Why This Works (Mechanism)
The framework works by creating realistic knowledge conflicts where LLMs must reconcile their incorrect parametric knowledge with real factual documents containing correct information. The mechanism exploits the observation that LLMs often fail to update their parametric knowledge when presented with contradictory evidence, particularly when the incorrect answer is present in the context. The three-stage pipeline systematically identifies and isolates these conflicts, then measures the model's ability to resolve them through proper context integration rather than relying on memorized (incorrect) information.

## Foundational Learning
- **BEM (BertScore-based metric)**: A metric for comparing answers based on semantic similarity rather than exact string matching. Why needed: Standard evaluation metrics like Exact Match fail to capture semantic equivalence between answers. Quick check: Verify that BEM correctly identifies semantically equivalent answers (e.g., "USA" vs "United States").
- **Knowledge Conflict Detection**: The process of identifying when an LLM's parametric answer contradicts the correct answer in a document. Why needed: To isolate genuine cases where models must update their knowledge rather than simple information retrieval. Quick check: Test NLI model on examples with known conflicts to ensure proper classification.
- **Parametric Bias**: The tendency of LLMs to favor their memorized (parametric) answers over information in context, especially when the parametric answer appears in the context. Why needed: Understanding this bias is crucial for improving retrieval-augmented generation systems. Quick check: Measure the frequency of parametric answer retention when it appears in context versus when it doesn't.
- **Knowledge Updating**: The process by which LLMs integrate new information from context to override incorrect parametric knowledge. Why needed: Essential for understanding how LLMs can be made more factually correct. Quick check: Measure update success rates across different model sizes and document types.

## Architecture Onboarding
Component map: Datasets -> Closed-book QA -> BEM/NLI Filtering -> Open-book QA -> Result Categorization
Critical path: Knowledge conflict detection (BEM + NLI) -> Open-book QA with real documents -> Categorization of update outcomes
Design tradeoffs: The framework trades synthetic controllability for realistic document complexity, accepting that real documents may introduce noise but better reflect practical scenarios
Failure signatures: Low update success rates indicate strong parametric bias; high Ui rates suggest context misinterpretation; Uc rates reflect resistance to knowledge updating
First experiments: 1) Run BEM and NLI on a small subset to verify conflict detection accuracy; 2) Test optimal prompt formulation on a single dataset; 3) Measure parametric bias by comparing update rates with and without parametric answers in context

## Open Questions the Paper Calls Out
- How can the parametric bias be effectively mitigated in retrieval-augmented generation systems without sacrificing model generality? The paper concludes that while fine-tuning shows promise, it is not enough to completely eradicate the bias and comes at the cost of potentially losing the generality of an instruction-tuned LLM.
- How does the parametric bias manifest in more complex and realistic document retrieval scenarios, such as when the retrieved documents are noisy, outdated, or irrelevant? The authors acknowledge that their experiments assume perfect retrieval and note that in reality, retrieval could bring noisy, outdated, or irrelevant documents into the LLM prompt.
- To what extent does the parametric bias generalize to other types of knowledge conflicts, such as those involving conflicting information within the context itself (inter-context conflicts) or conflicts between the model's parametric knowledge and user-provided information? The authors do not explore other types of knowledge conflicts that may arise in practice.

## Limitations
- The study relies on specific implementations of BEM and NLI models whose exact configurations are not fully specified, potentially affecting reproducibility
- The filtering process may inadvertently exclude valid knowledge conflict examples, potentially biasing results toward easier cases
- The analysis is limited to five model architectures, leaving questions about how results generalize to other LLM families and sizes

## Confidence
- Methodological advancement claim: High - The framework represents a clear improvement over synthetic approaches with well-structured experimental design
- Parametric bias identification: Medium - Results are internally consistent but limited by implementation details and model scope
- Knowledge update success rates: High - Multiple datasets and models show consistent patterns across the experimental pipeline

## Next Checks
1. Implement and test the BEM metric and NLI filtering pipeline on a small subset of examples to verify correct implementation and ensure it captures the intended knowledge conflicts
2. Conduct ablation studies on the filtering criteria (BEM threshold and NLI conflict detection) to assess their impact on failure rates and parametric bias measurements
3. Extend the analysis to additional model architectures (e.g., Claude, PaLM) and document types (e.g., long-form articles, scientific papers) to evaluate the robustness of the findings across diverse LLM capabilities and knowledge sources