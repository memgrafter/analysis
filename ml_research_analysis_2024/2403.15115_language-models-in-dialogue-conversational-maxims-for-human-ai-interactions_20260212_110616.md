---
ver: rpa2
title: 'Language Models in Dialogue: Conversational Maxims for Human-AI Interactions'
arxiv_id: '2403.15115'
source_url: https://arxiv.org/abs/2403.15115
tags:
- arxiv
- language
- maxims
- response
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a set of conversational maxims\u2014quantity,\
  \ quality, relevance, manner, benevolence, and transparency\u2014to analyze and\
  \ evaluate human-AI interactions. These maxims are grounded in social science and\
  \ AI research, addressing both traditional conversational principles (Grice's maxims)\
  \ and new challenges unique to AI, such as harm prevention and transparency about\
  \ capabilities."
---

# Language Models in Dialogue: Conversational Maxims for Human-AI Interactions

## Quick Facts
- arXiv ID: 2403.15115
- Source URL: https://arxiv.org/abs/2403.15115
- Authors: Erik Miehling, Manish Nagireddy, Prasanna Sattigeri, Elizabeth M. Daly, David Piorkowski, John T. Richards
- Reference count: 40
- Primary result: Language models exhibit internal prioritization of conversational maxims, leading to skewed accuracy in labeling and interpretation.

## Executive Summary
This paper introduces a set of conversational maxims—quantity, quality, relevance, manner, benevolence, and transparency—to analyze and evaluate human-AI interactions. These maxims are grounded in social science and AI research, addressing both traditional conversational principles (Grice's maxims) and new challenges unique to AI, such as harm prevention and transparency about capabilities. The authors evaluate various language models' understanding of these maxims and find that models prioritize certain principles, leading to skewed accuracy in labeling and interpretation. Operationalization requires models to minimize conflation of submaxims and improve objective evaluation. The work highlights the need for better alignment between AI behavior and human conversational expectations, with implications for training and evaluation methodologies.

## Method Summary
The authors sampled 1000 sliced conversations from Anthropic's hh-rlhf dataset and hand-labeled 50 conversations as ground truth for all submaxims. They then used three language models (llama-3-8b-instruct, llama-3-70b-instruct, mixtral-8x7b-instruct-v0.1) to label conversations with scores and explanations, analyzing patterns across 12 submaxims. The evaluation focused on label accuracy for each submaxim and violation patterns showing which submaxim violations are associated with others. The method employed langchain for prompting with in-context examples for all 12 submaxims, generating 5 labels per conversation.

## Key Results
- Language models exhibit internal prioritization of conversational maxims, leading to skewed accuracy in labeling and interpretation
- Current models are not objective interpreters of submaxims, posing challenges for operationalization
- Models tend to conflate submaxims, particularly when one submaxim is violated, leading to lower accuracy in labeling other submaxims

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models exhibit internal prioritization of conversational maxims, leading to skewed accuracy in labeling and interpretation.
- Mechanism: The models' training on helpfulness, honesty, and harmlessness causes them to conflate submaxims, particularly when one submaxim (e.g., benevolence_2) is violated, leading to lower accuracy in labeling other submaxims.
- Core assumption: The models' internal prioritization of certain principles significantly influences their ability to provide objective evaluations.
- Evidence anchors:
  - [abstract] "We find that various models maintain an internal prioritization of the maxims, which can significantly affect their ability to accurately interpret the maxims."
  - [section 4] "Our analysis indicates that current models are not objective interpreters of the submaxims, posing various challenges for operationalization."
  - [corpus] "Found 25 related papers... Top related titles: Applying the Gricean Maxims to a Human-LLM Interaction Cycle..."

### Mechanism 2
- Claim: The maxim of quantity addresses the tendency of language models to produce overly wordy responses due to reward model over-optimization.
- Mechanism: By specifying that responses should contain an appropriate amount of information, the maxim of quantity provides a dimension for evaluating the sufficiency and necessity of information in responses, thus addressing the issue of verbosity.
- Core assumption: The appropriate level of detail in responses is context-dependent and can be evaluated by judging if the additional information provided is adequate for the conversational needs.
- Evidence anchors:
  - [section 3] "The need for the maxim of quantity in human-AI conversations is primarily motivated by the tendency of language models to produce 'overly wordy responses' in an attempt to 'give the impression of expertise'."
  - [section 3] "The maxim of quantity specifies that responses should contain an appropriate amount of information, a definition that necessarily contains some subjectivity."
  - [corpus] "Found 25 related papers... Top related titles: Applying the Gricean Maxims to a Human-LLM Interaction Cycle..."

### Mechanism 3
- Claim: The maxim of transparency addresses the shortcoming of language models being hesitant to say "I don't know" due to the fine-tuning process.
- Mechanism: By requiring speakers to recognize their knowledge boundaries, the maxim of transparency encourages models to acknowledge their limitations and avoid making unfounded claims, thus addressing the issue of unrelenting "helpfulness".
- Core assumption: The fine-tuning process, particularly instruction tuning, causes models to avoid saying "I don't know" as it is often met with negative feedback from human labelers.
- Evidence anchors:
  - [section 3] "Our first requirement is motivated by a shortcoming of many current language models: their hesitancy to say 'I don't know.' This behavior is largely due to the fine-tuning process."
  - [section 3] "The process of human labeling thus causes human preferences to be embodied in the AI, in turn rewarding behavior that sounds more human, even if inaccurate."
  - [corpus] "Found 25 related papers... Top related titles: Applying the Gricean Maxims to a Human-LLM Interaction Cycle..."

## Foundational Learning

- Concept: Grice's conversational maxims (quantity, quality, relevance, manner)
  - Why needed here: These maxims provide a foundation for understanding effective human communication and serve as a basis for the proposed augmented set of maxims for human-AI conversations.
  - Quick check question: What are the four original maxims proposed by Grice, and how do they relate to effective human communication?

- Concept: Language model fine-tuning (supervised fine-tuning, instruction tuning, RLHF)
  - Why needed here: Understanding the fine-tuning process is crucial for recognizing how it can lead to undesirable properties in language models, such as being hesitant to say "I don't know" or exhibiting sycophancy.
  - Quick check question: How do the processes of instruction tuning and reinforcement learning from human feedback (RLHF) influence the behavior of language models?

- Concept: Hallucinations in language models
  - Why needed here: Recognizing the issue of hallucinations is essential for understanding the need for the maxim of quality, which emphasizes the importance of truthfulness and honesty in responses.
  - Quick check question: What are the primary causes of hallucinations in language models, and how do they impact the reliability of model outputs?

## Architecture Onboarding

- Component map: The proposed augmented set of maxims (quantity, quality, relevance, manner, benevolence, and transparency) serves as a taxonomy for analyzing human-AI conversations. Each maxim has specific requirements that can be evaluated to assess the effectiveness of communication.
- Critical path: The critical path involves understanding the maxims, evaluating language models' interpretability of the maxims, and using the insights to guide training and evaluation methodologies for better alignment between AI behavior and human conversational expectations.
- Design tradeoffs: Balancing the need for models to sound natural and engaging with the requirement for transparency and acknowledgment of limitations. Ensuring that the maxims are applicable to both human and AI speakers while accounting for the differences in their knowledge boundaries and operational capabilities.
- Failure signatures: Models exhibiting skewed accuracy in labeling and interpretation due to internal prioritization of certain maxims. Overly wordy or insufficient responses that violate the maxim of quantity. Responses that lack truthfulness or honesty, violating the maxim of quality. Responses that are off-topic or unnaturally shift the conversation, violating the maxim of relevance.
- First 3 experiments:
  1. Evaluate language models' interpretability of the proposed maxims by labeling conversations and analyzing the accuracy and violation patterns.
  2. Investigate the impact of training models to minimize conflations and learn clear distinctions among the submaxims on their ability to provide objective evaluations.
  3. Explore the use of the proposed maxims to guide human labeling of human-AI conversations and construct constitutional directives for model alignment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we operationalize the maxims in a way that ensures models prioritize them appropriately without conflating submaxims?
- Basis in paper: [explicit] The paper discusses the challenges of operationalizing the maxims due to models' internal prioritization of principles, which can lead to skewed accuracy in labeling and interpretation.
- Why unresolved: The paper identifies the issue but does not provide a clear solution for mitigating the conflation of submaxims during model training or evaluation.
- What evidence would resolve it: Experiments demonstrating improved model interpretability of submaxims through specific training techniques or evaluation methodologies that minimize conflation.

### Open Question 2
- Question: To what extent do cultural differences impact the applicability and interpretation of the proposed maxims in human-AI interactions?
- Basis in paper: [inferred] The paper acknowledges that its analysis assumes a Western-centric view of good conversational interaction and advises readers to interpret the maxims with cultural differences in mind.
- Why unresolved: The paper does not explore how cultural variations might influence the effectiveness or relevance of the maxims across different societies.
- What evidence would resolve it: Comparative studies evaluating the maxims' effectiveness in diverse cultural contexts, potentially leading to culturally adaptive versions of the maxims.

### Open Question 3
- Question: How can we design language models that are transparent about their limitations without compromising their perceived helpfulness or naturalness?
- Basis in paper: [explicit] The paper highlights the tension between creating models that sound "natural" and being transparent about their AI nature, noting that hedging statements can frustrate users.
- Why unresolved: The paper identifies the challenge but does not propose specific design strategies for balancing transparency and user satisfaction.
- What evidence would resolve it: User studies comparing different transparency strategies (e.g., hedging vs. direct acknowledgment) and their impact on user trust, satisfaction, and interaction quality.

## Limitations
- The labeling accuracy and violation patterns are based on only 50 hand-labeled conversations, which may not capture the full diversity of human-AI interactions.
- While internal prioritization among models was identified, the exact mechanisms driving this prioritization remain unclear.
- The operationalization of maxims for training requires careful balance between submaxims, and current approaches may not fully address the complexity of human conversational expectations.

## Confidence
- **High confidence**: The existence of internal prioritization among models and its impact on labeling accuracy. The identification of key challenges in applying Gricean maxims to AI systems.
- **Medium confidence**: The proposed augmented maxims (benevolence and transparency) as necessary additions to Gricean maxims for human-AI interactions. The specific violation patterns observed across models.
- **Low confidence**: The exact mechanisms by which fine-tuning processes create these prioritization patterns. The optimal approach for balancing submaxims in operationalization.

## Next Checks
1. Expand labeling dataset: Increase the number of hand-labeled conversations from 50 to 200+ to improve statistical power and better capture diverse conversational scenarios.

2. Test training interventions: Evaluate whether explicitly training models to minimize conflation of submaxims improves their ability to provide objective evaluations, using controlled experiments with modified training objectives.

3. Cross-model validation: Test the same set of maxims across a broader range of model architectures (including smaller models and different training approaches) to verify whether prioritization patterns are consistent or architecture-specific.