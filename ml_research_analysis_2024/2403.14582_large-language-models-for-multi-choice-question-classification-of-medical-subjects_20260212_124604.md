---
ver: rpa2
title: Large Language Models for Multi-Choice Question Classification of Medical Subjects
arxiv_id: '2403.14582'
source_url: https://arxiv.org/abs/2403.14582
tags:
- language
- medical
- question
- subjects
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models for multi-choice question
  classification in medical subjects. The proposed Multi-Question (MQ) Sequence-BERT
  method fine-tunes BERT-based models on the MedMCQA dataset, which contains 194k
  medical multiple-choice questions across 21 subjects.
---

# Large Language Models for Multi-Choice Question Classification of Medical Subjects

## Quick Facts
- arXiv ID: 2403.14582
- Source URL: https://arxiv.org/abs/2403.14582
- Authors: Víctor Ponce-López
- Reference count: 5
- Primary result: MQ-SequenceBERT achieves 0.68 and 0.60 accuracy on MedMCQA development and test sets respectively

## Executive Summary
This paper evaluates large language models for multi-choice question classification in medical subjects using the MedMCQA dataset containing 194k medical multiple-choice questions across 21 subjects. The proposed Multi-Question (MQ) Sequence-BERT method fine-tunes BERT-based models with mean pooling and L2 normalization to achieve state-of-the-art results, outperforming previous methods by 5% on the development set. The approach demonstrates the capability of LLMs for multi-class classification tasks in healthcare domain, specifically for automatic question answering of medical subjects.

## Method Summary
The MQ-SequenceBERT method encodes questions using BERT embeddings, applies mean pooling with L2 normalization to obtain feature representations, and fine-tunes domain-specific BERT models on the MedMCQA dataset. The approach uses AdamW optimizer with learning rate 1e-5, batch size of 8, and trains for 10 epochs with 100 steps per epoch on a NVIDIA A40 GPU.

## Key Results
- Achieves 0.68 accuracy on MedMCQA development set
- Achieves 0.60 accuracy on MedMCQA test set
- Outperforms previous state-of-the-art by 5% on development set
- Shows domain-specific models (PubmedBERT) perform better than general BERT

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning BERT-based models on MedMCQA dataset improves multi-class medical question classification accuracy by leveraging domain-specific medical text data to learn contextual embeddings that distinguish between 21 medical subjects. The core assumption is that medical domain knowledge is embedded in the training data and can be extracted through fine-tuning. Break condition occurs if training data lacks sufficient examples for rare medical subjects or if domain-specific terminology is underrepresented.

### Mechanism 2
Mean pooling with L2 normalization of BERT embeddings creates effective feature representations for classification by aggregating token-level embeddings into sentence-level representations while maintaining vector normalization for better similarity comparisons. The core assumption is that the average of token embeddings captures the essential semantic content of the question for subject classification. Break condition occurs if questions contain multiple distinct medical concepts that average pooling would blur together.

### Mechanism 3
Using domain-specific pre-trained models (BioBERT, SciBERT) provides better medical text representations than general BERT because these models were pre-trained on biomedical literature, giving them specialized vocabulary and context understanding for medical terminology. The core assumption is that medical domain-specific pretraining captures nuances in medical language that general pretraining misses. Break condition occurs if the medical subjects in the dataset are too diverse for any single domain-specific model to capture effectively.

## Foundational Learning

- Concept: Multi-class classification
  - Why needed here: The task requires categorizing questions into 21 distinct medical subjects, which is inherently a multi-class problem
  - Quick check question: How many medical subjects are being classified in the MedMCQA dataset?

- Concept: Fine-tuning pre-trained language models
  - Why needed here: Starting with models already trained on large corpora saves training time and provides better initial representations than training from scratch
  - Quick check question: What optimization method and learning rate are used for fine-tuning in this paper?

- Concept: Tokenization and embedding extraction
  - Why needed here: Converting text questions into numerical representations that neural networks can process requires proper tokenization and embedding strategies
  - Quick check question: What is the dimensionality of the embedding tensors used in MQ-SequenceBERT?

## Architecture Onboarding

- Component map:
  Input layer -> Tokenizer -> BERT encoder -> Mean pooling layer -> L2 normalization -> Classification layer -> Output layer

- Critical path: Tokenizer → BERT encoder → Mean pooling → L2 normalization → Classification layer → Output

- Design tradeoffs:
  - Batch size (8) vs. memory constraints on NVIDIA A40 GPU
  - Embedding dimensionality (384) vs. computational efficiency
  - 10 epochs vs. risk of overfitting on training set
  - No context information vs. potential performance gains

- Failure signatures:
  - Training accuracy much higher than validation accuracy indicates overfitting
  - All predictions falling into one or two subjects suggests model collapse
  - Very low accuracy on development set suggests poor feature learning
  - Embeddings clustering poorly in t-SNE visualization suggests model not learning subject distinctions

- First 3 experiments:
  1. Baseline test: Run evaluation using the pre-trained PubmedBERT without fine-tuning to establish minimum performance
  2. Hyperparameter sweep: Test different learning rates (1e-4, 5e-5, 1e-5) and batch sizes (4, 8, 16) to find optimal settings
  3. Ablation study: Compare performance with and without mean pooling, with and without L2 normalization to verify their contribution to accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of MQ-SequenceBERT compare to human experts in medical subject classification accuracy? The paper mentions that current state-of-the-art methods could only answer 47% of questions correctly, which was far behind human performance, suggesting a need for comparison. This remains unresolved as the paper does not provide direct comparison with human expert performance on the same dataset.

### Open Question 2
How does the performance of MQ-SequenceBERT vary across different medical subjects in the dataset? The paper mentions 21 medical subjects but only reports overall accuracy, not subject-specific performance. This remains unresolved as the paper only provides aggregate accuracy metrics without breaking down performance by individual medical subjects.

### Open Question 3
What is the impact of incorporating external knowledge sources (Wikipedia and PubMed) on the model's performance? The paper mentions that external knowledge sources were used to provide context but the baseline experiments were conducted without using context information. This remains unresolved as the paper does not report results when using context information, only baseline results without context.

## Limitations
- The specific BERT-based model used for fine-tuning is not explicitly stated, creating ambiguity for reproduction
- No ablation studies are provided to isolate the contribution of each component to final performance
- The approach treats each question independently without leveraging contextual information from related questions or answer choices

## Confidence
- High Confidence: The claim that MQ-SequenceBERT achieves 0.68 and 0.60 accuracy on development and test sets respectively
- Medium Confidence: The claim that this approach outperforms state-of-the-art results by 5% on the development set
- Low Confidence: The broader claim about "the capability of LLMs for multi-class classification tasks in healthcare domain"

## Next Checks
1. Reproduce with Multiple Domain-Specific Models: Conduct experiments using different domain-specific BERT variants (BioBERT, SciBERT, ClinicalBERT) to determine which model architecture provides optimal performance for medical subject classification.

2. Conduct Comprehensive Ablation Studies: Systematically remove or modify key components (mean pooling, L2 normalization, fine-tuning schedule) to quantify their individual contributions to accuracy.

3. Evaluate on Additional Medical QA Tasks: Test the MQ-SequenceBERT method on related medical question answering tasks beyond simple subject classification to assess broader applicability.