---
ver: rpa2
title: 'OSLO: One-Shot Label-Only Membership Inference Attacks'
arxiv_id: '2405.16978'
source_url: https://arxiv.org/abs/2405.16978
tags:
- oslo
- attacks
- adversarial
- attack
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OSLO, a novel membership inference attack
  (MIA) that can infer whether a sample is in a target model's training set using
  just one query. Unlike existing label-only attacks that require thousands of queries
  and have low precision, OSLO leverages transfer-based adversarial attacks.
---

# OSLO: One-Shot Label-Only Membership Inference Attacks

## Quick Facts
- arXiv ID: 2405.16978
- Source URL: https://arxiv.org/abs/2405.16978
- Authors: Yuefeng Peng; Jaechul Roh; Subhransu Maji; Amir Houmansadr
- Reference count: 40
- Primary result: OSLO achieves 5-67× higher true positive rates than state-of-the-art label-only attacks under 0.1% false positive rates

## Executive Summary
OSLO introduces a novel one-shot label-only membership inference attack that significantly outperforms existing methods. The attack leverages the observation that member samples are more resistant to adversarial perturbations than non-members. By generating transferable adversarial examples and adding just enough perturbation to cause misclassification for non-members but not for members, OSLO achieves high precision with only a single query. Extensive experiments demonstrate its effectiveness across different datasets and training algorithms.

## Method Summary
OSLO is a membership inference attack that operates on a single query using only the predicted label. The core insight is that member samples exhibit higher resistance to adversarial perturbations compared to non-members. The attack generates adversarial examples using a surrogate model and measures the distance between the clean sample and the adversarial sample. For each sample, OSLO computes a "distance-to-boundary" metric that quantifies how close the sample is to being misclassified. Members typically have higher distance-to-boundary values because they are more resistant to perturbations. By setting an appropriate threshold, OSLO can distinguish between members and non-members with high precision. The attack requires only one query to the target model and achieves significantly better performance than existing label-only attacks that require thousands of queries.

## Key Results
- Achieves 5-67× higher true positive rates than state-of-the-art label-only attacks under 0.1% false positive rates
- Maintains over 95% precision across three datasets using a ResNet18 model
- Remains effective even when target and surrogate models use different training algorithms
- Maintains low false positive rates when models are trained with adversarial training

## Why This Works (Mechanism)
OSLO exploits the fundamental difference in adversarial robustness between member and non-member samples. During training, member samples are exposed to the model's decision boundary multiple times, making them more robust to small perturbations. Non-members, having never been seen during training, are more susceptible to adversarial attacks. By generating transferable adversarial examples and measuring how much perturbation is needed to cause misclassification, OSLO can infer membership status. The attack's one-shot nature comes from leveraging the transferability of adversarial examples between models, allowing effective inference from a single query.

## Foundational Learning
- **Transfer-based adversarial attacks**: Why needed - To generate perturbations that can fool the target model without access to it. Quick check - Verify that adversarial examples generated on the surrogate model can successfully attack the target model.
- **Distance-to-boundary metric**: Why needed - To quantify the resistance of samples to adversarial perturbations. Quick check - Confirm that members consistently show higher distance-to-boundary values than non-members.
- **Label-only inference**: Why needed - To operate under strict privacy constraints where only predicted labels are accessible. Quick check - Ensure the attack only uses predicted labels without any confidence scores or other information.

## Architecture Onboarding

**Component map**: Surrogate model → Adversarial example generator → Target model query → Distance-to-boundary calculation → Membership inference

**Critical path**: The attack's success depends on the transferability of adversarial examples from the surrogate model to the target model. The adversarial example generation must produce perturbations that are effective against the target model while remaining subtle enough to avoid detection.

**Design tradeoffs**: The attack balances perturbation magnitude (too small won't fool non-members, too large may alert defenses) against transferability (perturbations must work across different model architectures). The one-shot requirement limits the attack to a single query, preventing iterative refinement.

**Failure signatures**: Poor performance indicates low transferability between surrogate and target models, or insufficient difference in robustness between members and non-members. High false positive rates suggest the threshold is too lenient or the adversarial examples are too weak.

**3 first experiments**:
1. Test OSLO on a simple dataset (e.g., CIFAR-10) with a single surrogate and target model to verify basic functionality
2. Evaluate the attack's performance when the surrogate and target models have different architectures
3. Measure the impact of varying the perturbation magnitude on attack success rate

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond those discussed in the limitations section.

## Limitations
- Effectiveness depends on the similarity between surrogate and target models
- Generalizability to diverse architectures and domains beyond vision models remains uncertain
- Performance against defense mechanisms other than adversarial training is unclear

## Confidence

**High confidence**: The fundamental insight that members are more resistant to adversarial perturbations than non-members is well-supported by experimental results.

**Medium confidence**: The attack's effectiveness across different training algorithms and robustness against adversarial training are demonstrated but may not generalize to all scenarios.

**Medium confidence**: The claim of requiring only one query is technically accurate, but effectiveness depends on generating transferable adversarial examples, which may require multiple attempts.

## Next Checks
1. Evaluate OSLO's performance against a broader range of model architectures (e.g., vision transformers, language models) and datasets to assess generalizability.
2. Test the attack's effectiveness when the surrogate model is significantly different from the target model in terms of architecture, training data, and training objectives.
3. Investigate the attack's performance against other defense mechanisms beyond adversarial training, such as differential privacy, gradient masking, or ensemble methods.