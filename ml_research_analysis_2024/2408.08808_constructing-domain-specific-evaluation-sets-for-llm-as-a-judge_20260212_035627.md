---
ver: rpa2
title: Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge
arxiv_id: '2408.08808'
source_url: https://arxiv.org/abs/2408.08808
tags:
- evaluation
- data
- which
- https
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel data pipeline to construct domain-specific
  evaluation sets for LLM-as-a-judge. The method uses semi-supervised learning with
  k-NN to cluster prompts into user-defined categories and applies stratified sampling
  to ensure balanced representation.
---

# Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge

## Quick Facts
- arXiv ID: 2408.08808
- Source URL: https://arxiv.org/abs/2408.08808
- Authors: Ravi Raju; Swayambhoo Jain; Bo Li; Jonathan Li; Urmish Thakker
- Reference count: 20
- Primary result: Proposes semi-supervised k-NN clustering with stratified sampling to create domain-specific evaluation sets, achieving 84% separability and 84% agreement with Chatbot Arena

## Executive Summary
This paper introduces a novel data pipeline for constructing domain-specific evaluation sets for LLM-as-a-judge frameworks. The approach combines semi-supervised learning via k-NN clustering with stratified sampling to create balanced, diverse evaluation sets across 14 categories. The resulting 1573-sample evaluation set demonstrates strong performance metrics including 84% separability across ten top models and 0.915 Spearman correlation with Chatbot Arena. The methodology significantly improves upon existing benchmarks like Arena-Hard v0.1 and AlpacaEval 2.0 LC by 9-20% in agreement and 0.7 in correlation. The authors also provide an open-source tool for fine-grained analysis of model performance across user-defined categories.

## Method Summary
The pipeline generates embeddings for unlabeled data using e5-mistral-7b-instruct, then applies a small manually labeled seed set to train a k-NN classifier for category assignment. Stratified sampling ensures balanced representation across domains, with 100 samples selected per cluster. The evaluation uses GPT-4o as both judge and reference model, following Arena-Hard and AlpacaEval scoring setups. Manual curation validates quality, and an analysis tool enables winrate breakdown across categories. The approach targets 13 domains including finance, law, medical, maths, coding, and six languages.

## Key Results
- 84% separability across ten top-ranked models
- 84% agreement with Chatbot Arena
- 0.915 Spearman correlation with human preferences
- 9-20% better agreement than Arena-Hard v0.1 and AlpacaEval 2.0 LC
- 0.7 higher correlation than existing benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semi-supervised k-NN clustering effectively maps diverse prompts into user-defined categories without requiring manual labeling of the entire dataset.
- Mechanism: Generates embeddings for unlabeled data, uses a small manually labeled seed set to train a k-NN classifier, then applies this classifier to label the larger unlabeled corpus.
- Core assumption: The embedding model captures semantic similarities that align with manually defined categories, and the k-NN classifier can generalize from the seed set to unlabeled data.
- Evidence anchors:
  - [abstract]: "Our approach leverages a combination of manual curation, semi-supervised learning to generate clusters, and stratified sampling to ensure balanced representation"
  - [section]: "We use semi-supervised learning via a k-NN classifier. We consider 13 categories comprising of domains: finance, law, medical, maths, coding and languages: Arabic, Russian, Serbian, Hungarian, Japanese, Thai and Slovenian."
- Break condition: If the embedding model fails to capture meaningful semantic relationships, or if the k-NN classifier cannot generalize from the seed set, the clustering will be inaccurate.

### Mechanism 2
- Claim: Stratified sampling within clusters ensures balanced representation across all categories, preventing any single domain from dominating the evaluation set.
- Mechanism: After clustering, applies stratified sampling to each category, selecting 100 samples per cluster to maintain diversity while controlling for category imbalance.
- Core assumption: The initial clustering is reasonably accurate, and stratified sampling can effectively balance the evaluation set composition.
- Evidence anchors:
  - [abstract]: "Our approach leverages a combination of manual curation, semi-supervised learning to generate clusters, and stratified sampling to ensure balanced representation across a wide range of domains and languages."
  - [section]: "The final step in our pipeline involves applying stratified sampling (Parsons, 2017) to each cluster."
- Break condition: If clusters are too imbalanced (some with very few samples), stratified sampling may not achieve true balance.

### Mechanism 3
- Claim: The LLM-as-a-judge framework using GPT-4o as both judge and reference model provides a scalable, automated evaluation that correlates strongly with human preferences.
- Mechanism: For each prompt, models generate responses compared by GPT-4o against GPT-4o's reference response, creating win/loss records aggregated into rankings.
- Core assumption: GPT-4o can reliably judge model responses in a way that aligns with human preferences, and using the same model as both judge and reference eliminates bias.
- Evidence anchors:
  - [abstract]: "The resulting evaluation set, which includes 1573 samples across 14 categories, demonstrates high separability (84%) across ten top-ranked models, and agreement (84%) with Chatbot Arena and (0.915) Spearman correlation."
  - [section]: "We follow a similar scoring setup as Arena-Hard (Li et al., 2024a) and Alpaca-Eval (Dubois et al., 2024a) where we use GPT-4o as a judge model and GPT-4o as a reference model as well."
- Break condition: If GPT-4o's judgments don't align with human preferences, or if the judge is biased toward certain response styles, the evaluation will be misleading.

## Foundational Learning

- Concept: Semi-supervised learning
  - Why needed here: Manual labeling of the entire diverse dataset would be prohibitively expensive and time-consuming; semi-supervised learning allows leveraging a small labeled seed set to classify the larger unlabeled corpus.
  - Quick check question: What is the key advantage of using semi-supervised learning over fully supervised learning in this context?

- Concept: k-Nearest Neighbors classification
  - Why needed here: k-NN provides a simple, interpretable method for classifying embeddings based on similarity to the seed set, without requiring complex model training.
  - Quick check question: How does the choice of k affect the classification results in this pipeline?

- Concept: Stratified sampling
  - Why needed here: Ensures that the evaluation set has balanced representation across all domains and languages, which is crucial for fair benchmark comparison.
  - Quick check question: What would happen to the benchmark if uniform random sampling were used instead of stratified sampling?

## Architecture Onboarding

- Component map: Data ingestion -> Embedding generation -> Seed set creation -> k-NN classifier training -> Cluster labeling -> Stratified sampling -> Manual curation -> LLM-as-a-judge evaluation -> Analysis tool
- Critical path: Data ingestion → Embedding generation → Seed set creation → k-NN training → Cluster labeling → Stratified sampling → Manual curation → LLM-as-a-judge evaluation → Analysis tool
- Design tradeoffs:
  - k-NN vs. more complex classifiers: Simplicity and interpretability vs. potential accuracy
  - Number of samples per category: More samples improve reliability but increase evaluation cost
  - Choice of judge model: GPT-4o provides strong alignment with human preferences but at higher cost vs. open-source alternatives
- Failure signatures:
  - Poor separability scores (<70%) suggest the evaluation set isn't distinguishing between models effectively
  - Low agreement with Chatbot Arena (<60%) indicates misalignment with human preferences
  - High entropy in k-NN classifications suggests poor clustering quality
- First 3 experiments:
  1. Run the pipeline with different k values (e.g., 20, 40, 60) and evaluate impact on separability and agreement metrics
  2. Test different embedding models (e.g., OpenAI's text-embedding-3-small) and measure changes in cluster quality
  3. Compare evaluation results using different judge models (GPT-4o-mini, Llama 3.1 405B) to assess robustness of rankings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of reference answers affect LLM-as-a-judge performance metrics, and can weaker models serve as effective references?
- Basis in paper: [explicit] The paper mentions "Current metrics define how separable a benchmark is and how much it aligns with human preferences but fails to account for the composition and diversity of the underlying data" and notes this as future work
- Why unresolved: The paper uses GPT-4o as both judge and reference model, but doesn't explore how different reference models might affect evaluation quality or if weaker models could suffice
- What evidence would resolve it: Systematic experiments comparing evaluation results using different reference models (GPT-4o, GPT-4o-mini, Claude-3-5-Sonnet) while keeping the judge model constant

### Open Question 2
- Question: What is the minimum number of samples required per category to achieve reliable separability metrics?
- Basis in paper: [explicit] The paper states "We observed that categories with a small number of examples had a significant impact on the category's win rate" and mentions this as future work
- Why unresolved: The paper uses 100 samples per category but doesn't provide analysis on how sample size affects metric reliability or what the minimum threshold should be
- What evidence would resolve it: Statistical analysis showing how separability metrics stabilize as sample size increases

### Open Question 3
- Question: Can LLMs effectively replace human curation in generating categories and quality checking prompts for the evaluation pipeline?
- Basis in paper: [explicit] The paper states "Currently, the categories we enumerate in our data pipeline is manually specified by humans and significant curation is done to ensure high quality prompts; for future work, we want to expand to using LLMs as category generators as well as quality checkers to automate the human effort out of this pipeline"
- Why unresolved: The paper relies on manual category definition and prompt curation, but doesn't explore automated alternatives or their effectiveness compared to human curation
- What evidence would resolve it: Comparative study measuring the quality and diversity of automatically generated categories and curated prompts versus human-curated ones

## Limitations
- Heavy reliance on GPT-4o as both judge and reference model raises questions about potential bias and generalizability
- The 100 samples per category appears somewhat arbitrary and may not capture domain complexity adequately
- Semi-supervised k-NN clustering quality depends heavily on the representativeness of the initial seed set
- The methodology doesn't address how to handle ambiguous or overlapping domains in clustering

## Confidence
- **High confidence**: The overall methodology of combining semi-supervised learning with stratified sampling to create domain-specific evaluation sets is well-founded and technically sound.
- **Medium confidence**: The claim of 84% separability and 84% agreement with Chatbot Arena, while impressive, needs independent validation across different model cohorts and evaluation scenarios.
- **Medium confidence**: The assertion that this approach significantly improves benchmark quality compared to existing methods is based on relative performance metrics but lacks absolute benchmarks for comparison.

## Next Checks
1. **Cross-validation of clustering quality**: Test the k-NN classifier with different seed set sizes and compositions to determine the minimum viable seed set and assess sensitivity to seed quality. Measure how clustering accuracy changes when using 10%, 25%, and 50% of the current seed set.

2. **Judge model robustness**: Re-run the evaluation using alternative judge models (GPT-4o-mini, Claude 3.5 Sonnet, Llama 3.1 405B) to verify that the model rankings remain consistent and that separability metrics hold across different judges.

3. **Domain-specific validation**: Conduct targeted evaluations within individual domains (e.g., medical, legal, coding) to ensure that the 100 samples per category provide sufficient statistical power and that the evaluation set captures domain-specific nuances rather than generic language patterns.