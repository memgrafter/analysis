---
ver: rpa2
title: Evaluating Fair Feature Selection in Machine Learning for Healthcare
arxiv_id: '2403.19165'
source_url: https://arxiv.org/abs/2403.19165
tags:
- feature
- selection
- fairness
- features
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses algorithmic bias in machine learning for healthcare
  by developing a fair feature selection method. The approach divides datasets by
  demographic attributes and applies multiple feature selection techniques separately
  to each group.
---

# Evaluating Fair Feature Selection in Machine Learning for Healthcare

## Quick Facts
- arXiv ID: 2403.19165
- Source URL: https://arxiv.org/abs/2403.19165
- Reference count: 40
- Primary result: Fair feature selection method improves fairness metrics (Disparate Impact, Statistical Parity, Equalized Odds) while maintaining balanced accuracy across three healthcare datasets

## Executive Summary
This study addresses algorithmic bias in healthcare machine learning by developing a fair feature selection method that considers demographic differences. The approach divides datasets by demographic attributes (gender) and applies multiple feature selection techniques separately to each group. A combined metric incorporating both fairness and classification accuracy is used to select features that optimize performance across all subgroups. Tested on three healthcare datasets for Parkinson's disease, glioma grading, and coronary artery disease, the method demonstrated that fairness and accuracy can be simultaneously achieved through appropriate feature selection techniques.

## Method Summary
The method divides datasets by demographic attributes and applies six feature selection techniques (Forward Feature Selection, Recursive Feature Elimination, Decision Tree depth analysis, Logistic Regression weights, Mutual Information, and Spearman's Rank Correlation) separately to each demographic group. Final feature rankings are computed by averaging across methods and groups. A combined metric balancing accuracy (Balanced Accuracy) and fairness (Disparate Impact) guides feature selection. The approach uses Stratified K-Fold cross-validation (4-fold for Tappy, 5-fold for others) with 100 bootstrap samples per fold, and evaluates results using Logistic Regression models.

## Key Results
- Fair feature selection improved Disparate Impact by 23.3% compared to baseline correlation-based selection
- Statistical Parity and Equalized Odds metrics showed significant improvement while maintaining balanced accuracy
- The method outperformed single-method approaches in both fairness and accuracy across all three healthcare datasets
- Fair feature selection achieved better performance than baseline methods for all evaluated fairness metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stratified feature selection by demographic group improves fairness by preventing dominant-group feature dominance.
- Mechanism: Dividing datasets by demographic attributes and applying feature selection separately to each group ensures that important features for minority groups are not overshadowed by those for majority groups. The combined metric balances accuracy and fairness by weighting both components equally (0.50 each).
- Core assumption: Demographic groups have distinct feature importance distributions that, if ignored, lead to biased feature selection favoring the majority group.
- Evidence anchors:
  - [abstract]: "Traditional feature selection methods... overlook how these factors may differ across subgroups."
  - [section]: "To assess feature significance in relation to gender discrepancies, we first divided the dataset according to the 'gender' attribute, thereby generating distinct subsets for male and female participants."
  - [corpus]: No direct corpus evidence found; this appears to be a novel contribution.
- Break condition: If demographic groups have highly overlapping feature importance distributions, stratified selection provides no benefit over unified selection.

### Mechanism 2
- Claim: Combining multiple feature selection methods reduces individual method bias and creates more robust feature rankings.
- Mechanism: Applying Forward Feature Selection, Recursive Feature Elimination, Decision Tree depth analysis, Logistic Regression weights, Mutual Information, and Spearman's Rank Correlation independently, then averaging their rankings, creates a consensus ranking less sensitive to any single method's limitations.
- Core assumption: Different feature selection methods capture complementary aspects of feature importance, and averaging them reduces method-specific biases.
- Evidence anchors:
  - [section]: "By consolidating multiple feature selection methods into a unified framework and utilizing a combined metric for final feature selection..."
  - [section]: "The mathematical representation for each demographic subset... combines the individual ranks from each method."
  - [corpus]: No direct corpus evidence found for this specific multi-method combination approach.
- Break condition: If all methods converge on the same suboptimal features due to dataset characteristics, averaging provides no improvement.

### Mechanism 3
- Claim: Using a combined metric that incorporates both accuracy and fairness metrics enables balanced optimization during feature selection.
- Mechanism: The combined metric formula (ð‘¤ð‘Žð‘ð‘ Ã— ðµað‘ð‘ + ð‘¤ð‘“ð‘Žð‘–ð‘Ÿ Ã— (1 âˆ’ |1 âˆ’ ð·ð¼|)) creates a single optimization target that rewards features sets improving both balanced accuracy and fairness (measured by Disparate Impact), preventing trade-off extremes.
- Core assumption: Feature selection can be guided by a single scalar metric that appropriately balances competing objectives of accuracy and fairness.
- Evidence anchors:
  - [section]: "We jointly considered a fairness metric and an error metric within the feature selection process to ensure a balance between minimizing both bias and global classification error."
  - [section]: "The combined metric, aimed at integrating both performance and fairness into a single optimization criterion..."
  - [corpus]: No direct corpus evidence found for this specific combined metric formulation.
- Break condition: If the linear combination weights are inappropriate for a specific dataset, the combined metric may poorly represent the true trade-off between accuracy and fairness.

## Foundational Learning

- Concept: Fairness metrics (Disparate Impact, Statistical Parity, Equalized Odds)
  - Why needed here: These metrics quantify bias in model predictions across demographic groups and are essential for evaluating whether the fair feature selection actually reduces bias.
  - Quick check question: What does a Disparate Impact value of 0.82 indicate about model fairness across groups?

- Concept: Cross-validation and bootstrapping for robust evaluation
  - Why needed here: The study uses 4-fold and 5-fold cross-validation with 100 bootstrap samples to ensure that fairness and accuracy improvements are not due to chance or specific data splits.
  - Quick check question: Why might the study use different K values (4 vs 5) for different datasets?

- Concept: Feature selection methods (FFS, RFE, DT, LR, MI, Spearman)
  - Why needed here: Understanding these methods is crucial for implementing the multi-method feature ranking approach and interpreting why certain features are selected.
  - Quick check question: How does Forward Feature Selection differ from Recursive Feature Elimination in its approach to selecting features?

## Architecture Onboarding

- Component map:
  - Data Preprocessing: Cleaning, encoding, stratification by sensitive attribute
  - Demographic Group Processing: Separate feature ranking per demographic group
  - Feature Ranking Engine: Multiple feature selection methods (FFS, RFE, DT, LR, MI, Spearman)
  - Combined Ranking Calculator: Average rankings across methods and groups
  - Feature Selection Optimizer: Iterative selection with combined metric evaluation
  - Model Training: Logistic Regression evaluation
  - Fairness Evaluation: Compute DI, SP, EqO, and Bacc

- Critical path:
  - Preprocess data â†’ Stratify by demographic â†’ Apply all feature selection methods â†’ Average rankings â†’ Iteratively select features â†’ Evaluate with combined metric â†’ Train LR model â†’ Calculate fairness metrics

- Design tradeoffs:
  - Computational cost vs. fairness improvement: Multiple methods and stratified processing increase computation time
  - Number of demographic groups: More groups increase complexity but may improve fairness for more subgroups
  - Weight tuning: The 0.50/0.50 weight balance may need adjustment for different datasets or fairness priorities

- Failure signatures:
  - If balanced accuracy drops significantly (>10%) while fairness metrics improve marginally, the combined metric weights may be poorly calibrated
  - If fairness metrics show minimal improvement, demographic groups may have very similar feature importance distributions
  - If computation time is prohibitive, the number of methods or bootstrap samples may need reduction

- First 3 experiments:
  1. Implement stratified preprocessing and verify demographic group separation works correctly
  2. Apply individual feature selection methods to one demographic group and compare their rankings
  3. Implement the combined ranking calculation and verify it produces reasonable aggregated feature importance scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would fair feature selection perform on datasets with more than two demographic groups (e.g., multiple races/ethnicities) compared to binary gender stratification?
- Basis in paper: [inferred] The authors note their method was evaluated only on gender biases and only studied two genders rather than considering a broader spectrum of gender identities due to dataset limitations.
- Why unresolved: The paper explicitly states this as a limitation and calls for future work to expand evaluation to a wider range of demographic factors beyond binary gender.
- What evidence would resolve it: Testing the fair feature selection method on datasets with multiple demographic groups (race, ethnicity, age groups, etc.) and comparing fairness and accuracy outcomes across different stratification schemes.

### Open Question 2
- Question: How can the computational efficiency of the fair feature selection framework be improved to make it more practically applicable in clinical settings?
- Basis in paper: [explicit] The authors note that "Our approach requires significant computational resources" as a limitation.
- Why unresolved: The paper identifies this as a limitation but does not propose specific solutions for reducing computational burden.
- What evidence would resolve it: Development and testing of computational optimizations (parallel processing, dimensionality reduction, approximation techniques) that maintain fairness improvements while reducing processing time.

### Open Question 3
- Question: How does the quality and completeness of healthcare data affect the effectiveness of bias correction through fair feature selection?
- Basis in paper: [explicit] The authors state "the success of our bias mitigation strategy is reliant on the quality and comprehensiveness of the data; substandard data can reduce the effectiveness of bias correction via feature selection."
- Why unresolved: While identified as a critical limitation, the paper does not empirically investigate how different data quality levels impact the method's effectiveness.
- What evidence would resolve it: Controlled experiments testing the fair feature selection method on datasets with varying quality levels (missing data, measurement errors, sampling bias) to quantify the relationship between data quality and bias mitigation effectiveness.

## Limitations

- Limited dataset diversity: All three datasets are gender-stratified, with no evaluation of other demographic attributes (race, age, socioeconomic status) that may be equally or more important for fairness in healthcare
- Single model architecture: Only Logistic Regression is evaluated, leaving uncertainty about whether fairness improvements generalize to more complex models like neural networks or ensemble methods
- No baseline comparison: The study does not compare against other established fair feature selection methods, making it difficult to assess relative performance improvements

## Confidence

- High confidence: The stratified processing mechanism and multi-method feature ranking approach are clearly specified and methodologically sound
- Medium confidence: The fairness improvements are demonstrated on three datasets, but the generalizability to other demographic attributes and model architectures remains uncertain
- Low confidence: The specific formulation of the combined metric and the optimal weight balance for different healthcare contexts have not been validated

## Next Checks

1. **Dataset attribute expansion**: Apply the fair feature selection method to datasets stratified by multiple demographic attributes simultaneously (gender, age, race) to verify performance across intersectional groups
2. **Model architecture generalization**: Evaluate the selected features using alternative classification models (Random Forest, SVM, neural networks) to confirm that fairness improvements are model-agnostic
3. **Weight sensitivity analysis**: Systematically vary the weights in the combined metric (e.g., 0.30/0.70, 0.70/0.30) across different healthcare domains to determine optimal trade-offs between accuracy and fairness for various clinical priorities