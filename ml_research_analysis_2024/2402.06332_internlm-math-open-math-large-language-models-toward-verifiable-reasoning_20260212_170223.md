---
ver: rpa2
title: 'InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning'
arxiv_id: '2402.06332'
source_url: https://arxiv.org/abs/2402.06332
tags:
- math
- reasoning
- data
- lean
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces InternLM-Math, a series of open-source math
  reasoning large language models that achieve state-of-the-art performance on various
  informal and formal benchmarks. The key contributions include: (1) unifying chain-of-thought
  reasoning, reward modeling, formal reasoning, data augmentation, and code interpreter
  in a unified seq2seq format; (2) achieving strong performance under in-context learning,
  supervised fine-tuning, and code-assisted reasoning settings; (3) proposing reasoning
  interleaved with coding (RICO) for enhanced mathematical reasoning with Python''s
  assistance; and (4) exploring the use of LEAN for solving and proving math problems.'
---

# InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning

## Quick Facts
- arXiv ID: 2402.06332
- Source URL: https://arxiv.org/abs/2402.06332
- Authors: Huaiyuan Ying; Shuo Zhang; Linyang Li; Zhejian Zhou; Yunfan Shao; Zhaoye Fei; Yichuan Ma; Jiawei Hong; Kuikun Liu; Ziyi Wang; Yudong Wang; Zijian Wu; Shuaibin Li; Fengzhe Zhou; Hongwei Liu; Songyang Zhang; Wenwei Zhang; Hang Yan; Xipeng Qiu; Jiayu Wang; Kai Chen; Dahua Lin
- Reference count: 40
- Primary result: State-of-the-art performance on various informal and formal math benchmarks using a unified math reasoning model

## Executive Summary
InternLM-Math introduces a series of open-source math reasoning large language models that achieve state-of-the-art performance on both informal and formal benchmarks. The key innovation is unifying chain-of-thought reasoning, reward modeling, formal reasoning, data augmentation, and code interpreter capabilities in a single seq2seq format. The models demonstrate strong performance under in-context learning, supervised fine-tuning, and code-assisted reasoning settings, with particular emphasis on reasoning interleaved with coding (RICO) for enhanced mathematical problem solving with Python's assistance.

## Method Summary
InternLM-Math continues pre-training InternLM2-Base models on diverse math corpora (125B tokens total across CC Retrieved Data, Domain-Specific Data, and Synthetic Data) with extensive post-processing including deduplication and contamination filtering. The models are then supervised fine-tuned on a mixture of chain-of-thought, reward modeling, code interpreter, and formal reasoning datasets. The unified architecture supports multiple capabilities including problem solving, verification, formal proof generation via LEAN, and self-improvement through data augmentation. The reasoning interleaved with coding (RICO) approach enables dynamic adjustment of reasoning based on intermediate computation results.

## Key Results
- Achieves state-of-the-art performance on GSM8K, MATH, Hungary math exam, and MathBench-ZH benchmarks
- Outperforms specialized models on MiniF2F formal reasoning benchmark
- Demonstrates strong in-context learning performance with clean pre-training data
- Shows effectiveness of unified architecture for both informal and formal math reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on diverse math corpora with post-processing yields better ICL performance than using raw or synthetic-only data
- Mechanism: Cleaned, high-quality pre-training data reduces noise and memorization of test-specific patterns
- Core assumption: Post-processing preserves useful signal while removing harmful redundancy
- Evidence anchors:
  - [abstract]: "We collected pre-train data from math corpora and synthetic data which establish its math ability."
  - [section 3.1]: Data post-processing strategies including Minhash-LSH deduplication and high-quality dataset scoring
  - [section 3.2]: Exact formulation decontamination for MATH test set

### Mechanism 2
- Claim: Integrating multiple abilities in a unified seq2seq format improves both problem-solving and self-improvement
- Mechanism: Single model architecture trained on diverse tasks can flexibly switch between solving, verifying, and generating data
- Core assumption: Unified format does not introduce interference between tasks
- Evidence anchors:
  - [abstract]: "We unify chain-of-thought reasoning, reward modeling, formal reasoning, data augmentation, and code interpreter in a unified seq2seq format"
  - [section 2]: Combining policy network and reward model into unified format
  - [section 4]: Models targeted to be math solvers and ready for self-improving

### Mechanism 3
- Claim: Reasoning interleaved with coding (RICO) yields better math reasoning performance than single-step program-of-thought
- Mechanism: Interleaving reasoning and coding in multiple rounds allows dynamic adjustment based on intermediate computation results
- Core assumption: Model can effectively learn alternating pattern of reasoning → code generation → execution result → updated reasoning
- Evidence anchors:
  - [abstract]: "We propose reasoning interleaved with coding (RICO) and achieve state-of-the-art math reasoning with Python's assistance"
  - [section 5]: Solving issue by letting LLMs do reasoning interleaved with coding
  - [section 6.2.4]: InternLM2-Math-20B outperforms InternLM2-Chat-20B on MATH

## Foundational Learning

- Concept: **Formal language theorem proving (LEAN)**  
  Why needed here: The paper explores using LEAN for translating natural language math statements to formal statements and for proving them  
  Quick check question: What is the primary purpose of a LEAN "tactic" in the context of formal proof construction?

- Concept: **Reward modeling (ORM/PRM)**  
  Why needed here: The paper introduces both outcome reward models and process reward models to verify correctness of answers and reasoning steps  
  Quick check question: How does a process reward model (PRM) differ from an outcome reward model (ORM) in terms of what it evaluates?

- Concept: **Chain-of-thought reasoning**  
  Why needed here: The model is trained to generate intermediate reasoning steps, essential for interpretable problem solving and enabling verification  
  Quick check question: In the context of the paper, what is "calculation hallucination" and how does the model address it?

## Architecture Onboarding

- Component map: Pre-training data collection → Post-processing (deduplication, contamination filtering) → Continue pre-training InternLM2-Base → Supervised fine-tuning on diverse tasks → Reward model reranking → Optional formal verification via LEAN
- Critical path: Pre-training → SFT on diverse tasks → Reward model reranking → Optional formal verification via LEAN
- Design tradeoffs: Unified seq2seq format simplifies deployment but may introduce task interference; RICO improves reasoning but adds complexity to training data pipeline
- Failure signatures: Degraded performance on one task after SFT on another; inability to execute generated code correctly; failure to translate natural language to formal statements accurately
- First 3 experiments:
  1. Test ICL performance on GSM8K with different amounts of pre-training data to find optimal epochs
  2. Evaluate SFT model performance on GSM8K with and without reward model reranking to measure verification impact
  3. Run a small-scale formal reasoning test: translate a few natural language problems to LEAN statements and check correctness

## Open Questions the Paper Calls Out

- Question: How does the performance of InternLM-Math models scale with increasing model size, particularly in formal math reasoning tasks?
  - Basis in paper: [explicit] The paper compares performance across different model sizes (7B, 20B) but does not provide comprehensive scaling analysis
  - Why unresolved: Limited comparisons between model sizes without detailed relationship analysis
  - What evidence would resolve it: Comprehensive study comparing performance of various model sizes across informal and formal math benchmarks

- Question: What is the impact of different pre-training data compositions on the performance of InternLM-Math models?
  - Basis in paper: [inferred] Paper mentions diverse data collection but lacks detailed analysis of composition impact
  - Why unresolved: Does not explore relationship between data composition and model performance in detail
  - What evidence would resolve it: Ablation study comparing performance on different data compositions

- Question: How effective is the use of LEAN as a unified platform for solving and proving math problems compared to traditional approaches?
  - Basis in paper: [explicit] Explores using LEAN but doesn't provide comprehensive comparison with traditional approaches
  - Why unresolved: Lacks detailed comparison of LEAN versus chain-of-thought or program-of-thought reasoning
  - What evidence would resolve it: Comprehensive comparison of performance using LEAN versus traditional approaches

## Limitations

- Data contamination risk remains unclear despite reported decontamination efforts
- Evaluation protocol ambiguity with multiple metrics and voting procedures
- Formal reasoning capability verification limited to MiniF2F benchmark
- Task interference effects not thoroughly investigated in unified architecture

## Confidence

- **High confidence**: Post-processing training data to improve ICL performance is well-supported by standard NLP practices
- **Medium confidence**: Unified seq2seq format is theoretically sound but task interference not thoroughly investigated
- **Low confidence**: RICO approach superiority not rigorously compared with alternative methods

## Next Checks

1. Independently verify exact formulation decontamination effectiveness by comparing model performance on contaminated versus decontaminated versions of MATH test set

2. Design experiments to measure performance degradation when training on multiple tasks simultaneously versus specialized models

3. Conduct ablation study comparing RICO approach against single-step program-of-thought, separate reasoning and coding phases, and pure reasoning without code assistance