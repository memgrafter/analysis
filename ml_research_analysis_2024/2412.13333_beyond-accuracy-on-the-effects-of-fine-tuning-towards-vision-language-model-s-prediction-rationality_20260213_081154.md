---
ver: rpa2
title: 'Beyond Accuracy: On the Effects of Fine-tuning Towards Vision-Language Model''s
  Prediction Rationality'
arxiv_id: '2412.13333'
source_url: https://arxiv.org/abs/2412.13333
tags:
- fine-tuning
- prediction
- vlms
- clip-vit-b
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fine-tuning is widely used to adapt vision-language models (VLMs)
  like CLIP for specific tasks. This study investigates how mainstream fine-tuning
  methods impact prediction rationality, focusing on the trustworthiness and reliability
  of model predictions.
---

# Beyond Accuracy: On the Effects of Fine-tuning Towards Vision-Language Model's Prediction Rationality

## Quick Facts
- arXiv ID: 2412.13333
- Source URL: https://arxiv.org/abs/2412.13333
- Authors: Qitong Wang; Tang Li; Kien X. Nguyen; Xi Peng
- Reference count: 40
- Key outcome: Fine-tuning improves accuracy but often reduces prediction trustworthiness by increasing reliance on invalid evidence.

## Executive Summary
This study investigates how mainstream fine-tuning methods impact the rationality of vision-language model (VLM) predictions, focusing on trustworthiness and reliability. The authors introduce two new metrics—Prediction Trustworthiness (PT) and Inference Reliability (IR)—to evaluate whether correct predictions are based on valid evidence. Experiments across multiple datasets and models (CLIP, ALBEF, BLIP) reveal that fine-tuning often reduces PT by increasing correct predictions based on invalid evidence, undermining trustworthiness. However, fine-tuned VLMs show improved IR, meaning they are more likely to make correct predictions when valid evidence is present. These findings hold under distributional shifts and various experimental settings, highlighting the need to balance accuracy with rationality in VLM fine-tuning.

## Method Summary
The study evaluates four fine-tuning methods (Zero-Shot, Linear-Probing, FLCP, and Fine-Tuning) on VLMs like CLIP, ALBEF, and BLIP using datasets including ImageNet-1K and Caltech-101. Prediction Trustworthiness (PT) measures the ratio of correct predictions with valid evidence among all correct predictions, while Inference Reliability (IR) measures the percentage of correct predictions given valid evidence. Explanation heatmaps are generated using Generic Attention Attribution, and RMA scores assess evidence validity. The minimum viable reproduction plan involves downloading datasets and pre-trained models, implementing fine-tuning methods and heatmap generation, and evaluating using PT and IR metrics.

## Key Results
- Fine-tuning reduces PT by increasing correct predictions based on invalid evidence, undermining trustworthiness.
- Fine-tuned VLMs show improved IR, meaning they are more likely to make correct predictions when valid evidence is present.
- These findings remain consistent across distributional shifts and various experimental settings.

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning reduces prediction trustworthiness by encouraging reliance on invalid evidence. Standard fine-tuning objectives optimize for accuracy but do not constrain the model to use valid evidence. During training, VLMs may learn spurious correlations or shortcuts present in the data, leading to correct predictions based on irrelevant cues (e.g., background features, watermarks). The core assumption is that the fine-tuning objective does not penalize reliance on invalid evidence, so the model will exploit the easiest path to minimize loss. Evidence includes observations that "fine-tuning often reduces PT by increasing correct predictions based on invalid evidence" and that "VLMs tend to exploit the easiest path to minimize loss during finetuning, often picking up on spurious correlations or shortcuts present in the data." This could break if fine-tuning objectives are modified to penalize reliance on invalid evidence or if training data is carefully curated to remove spurious correlations.

### Mechanism 2
Fine-tuning improves inference reliability when valid evidence is present by adapting the model's internal representations to better map valid evidence to correct predictions. Even though the model may use invalid evidence more often, when it does focus on valid evidence, it is more likely to make the correct prediction. The core assumption is that the fine-tuning process improves the model's ability to leverage valid evidence when it is identified, even if it doesn't improve the ability to identify valid evidence. Evidence includes observations that "fine-tuned VLMs show improved IR, meaning they are more likely to make correct predictions when valid evidence is present" and that "when VLMs focus on valid evidence of target objects, the prediction accuracy of fine-tuned VLMs improves." This could break if fine-tuning degrades the model's ability to process valid evidence or if the relationship between evidence and prediction is non-monotonic.

### Mechanism 3
The observed effects are consistent across distributional shifts because the fine-tuning process does not fundamentally change the model's ability to generalize to out-of-distribution data. The same trade-offs between trustworthiness and reliability persist, even when the input data is corrupted or otherwise differs from the training distribution. The core assumption is that the fine-tuning process does not introduce any specific adaptation to the training distribution that would break down under distributional shift. Evidence includes observations that "these findings hold under distributional shifts and various experimental settings" and that "all our findings remain consistent across various types and magnitudes of distributional shifts." This could break if fine-tuning introduces specific adaptation to the training distribution that breaks down under distributional shift or if the type or magnitude of distributional shift fundamentally alters the relationship between evidence and prediction.

## Foundational Learning

- Concept: Vision-Language Models (VLMs) and their architecture
  - Why needed here: Understanding the basic components and training objectives of VLMs is essential for interpreting the experimental results and proposed metrics.
  - Quick check question: What are the key components of a typical VLM, and what is the objective function used during pre-training?

- Concept: Explanation heatmaps and faithfulness metrics
  - Why needed here: The proposed metrics (PT and IR) rely on measuring whether the model's explanation heatmap focuses on valid evidence. Understanding how these heatmaps are generated and evaluated is crucial.
  - Quick check question: How are explanation heatmaps generated for transformer-based models, and what does the RMA score measure?

- Concept: Fine-tuning methods and their objectives
  - Why needed here: Different fine-tuning methods have different objectives and can lead to different trade-offs between accuracy and rationality. Understanding these methods is essential for interpreting the experimental results.
  - Quick check question: What are the key differences between zero-shot, linear probing, FLCP, and standard fine-tuning in terms of their objectives and training procedures?

## Architecture Onboarding

- Component map: VLM backbone (e.g., CLIP, ALBEF, BLIP) -> Image encoder -> Text encoder -> Classification head (for fine-tuning) -> Explanation heatmap generator (e.g., Generic Attention Attribution) -> Metrics: PT and IR

- Critical path: Load pre-trained VLM -> Fine-tune using chosen method (ZS, LP, FLCP, FT) -> Generate explanation heatmaps for validation data -> Calculate RMA scores -> Compute PT and IR metrics

- Design tradeoffs: Accuracy vs. trustworthiness (fine-tuning improves accuracy but may reduce trustworthiness), Generalization vs. adaptation (fine-tuning adapts to training distribution but may reduce generalization to out-of-distribution data), Complexity vs. interpretability (more complex fine-tuning methods may lead to better performance but may be harder to interpret)

- Failure signatures: Model achieves high accuracy but low PT (indicates reliance on invalid evidence), Model achieves low accuracy and low PT (indicates fundamental issues with the model or fine-tuning method), Model achieves low IR (indicates inability to leverage valid evidence when it is present)

- First 3 experiments: 1) Replicate the main results on a small dataset (e.g., Caltech-101) using a single VLM and fine-tuning method, 2) Visualize explanation heatmaps for a few examples to qualitatively assess the model's focus on valid evidence, 3) Modify the fine-tuning objective to penalize reliance on invalid evidence and observe the effect on PT and IR

## Open Questions the Paper Calls Out

### Open Question 1
What specific characteristics of VLMs cause them to focus on invalid evidence during fine-tuning? While the paper identifies that VLMs tend to exploit spurious correlations or shortcuts during fine-tuning, it does not investigate the underlying mechanisms or model architectures that make VLMs particularly susceptible to this behavior. Systematic ablation studies varying model architectures, fine-tuning objectives, and data characteristics could isolate the factors that lead to invalid evidence focus.

### Open Question 2
How can fine-tuning methods be designed to simultaneously improve both prediction accuracy and rationality? The paper demonstrates that current fine-tuning methods improve accuracy but reduce prediction trustworthiness, suggesting a trade-off that could potentially be optimized. Development and validation of fine-tuning methods that achieve comparable accuracy to current methods while maintaining or improving prediction trustworthiness scores would address this question.

### Open Question 3
To what extent do different types of distributional shifts affect the relationship between accuracy and rationality in fine-tuned VLMs? While the paper shows that findings remain consistent across various distributional shifts, it does not investigate whether certain types of shifts have differential impacts on the accuracy-rationality relationship. Comparative analysis of accuracy-rationality trade-offs across different corruption types and severities, potentially revealing patterns or thresholds where the relationship changes, would provide insights into this question.

## Limitations
- Findings are based on specific VLM architectures (CLIP, ALBEF, BLIP) and datasets, limiting generalizability to other model types or domains.
- Mechanism explanations rely on indirect evidence and require further validation through systematic ablation studies.
- The impact of different fine-tuning objectives on prediction rationality needs more systematic exploration across diverse scenarios.

## Confidence
- High confidence: The observation that fine-tuning improves accuracy while potentially reducing trustworthiness is well-supported by experimental results across multiple datasets and models.
- Medium confidence: The mechanism explaining why fine-tuning reduces trustworthiness (exploiting spurious correlations) is plausible but requires additional evidence.
- Medium confidence: The finding that fine-tuned models show improved inference reliability when valid evidence is present is supported by experiments but needs further validation across more diverse scenarios.

## Next Checks
1. Test the PT and IR metrics on VLM applications beyond image classification (e.g., VQA, image captioning) to assess generalizability.
2. Systematically modify fine-tuning objectives to penalize reliance on invalid evidence and measure the impact on PT and IR.
3. Conduct human studies to validate the ground truth labels for valid evidence and assess the correlation between model predictions and human judgment.