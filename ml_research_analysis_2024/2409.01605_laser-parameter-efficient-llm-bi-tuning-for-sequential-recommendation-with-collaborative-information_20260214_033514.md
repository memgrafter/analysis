---
ver: rpa2
title: 'Laser: Parameter-Efficient LLM Bi-Tuning for Sequential Recommendation with
  Collaborative Information'
arxiv_id: '2409.01605'
source_url: https://arxiv.org/abs/2409.01605
tags:
- recommendation
- item
- laser
- sequential
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of sequential recommendation by
  proposing a parameter-efficient framework called Laser, which leverages Large Language
  Models (LLMs) to encode item semantics while incorporating user-specific collaborative
  information. The core method involves a Bi-Tuning approach that uses trainable virtual
  tokens (prefix and suffix) to adapt LLMs for recommendation tasks without updating
  their parameters.
---

# Laser: Parameter-Efficient LLM Bi-Tuning for Sequential Recommendation with Collaborative Information

## Quick Facts
- **arXiv ID**: 2409.01605
- **Source URL**: https://arxiv.org/abs/2409.01605
- **Reference count**: 40
- **One-line primary result**: Achieves up to 13.27% improvement in NDCG@10 and 11.37% in Recall@10 while maintaining high parameter efficiency with only 0.135M trainable parameters.

## Executive Summary
This paper addresses the challenge of sequential recommendation by proposing Laser, a parameter-efficient framework that leverages Large Language Models (LLMs) to encode item semantics while incorporating user-specific collaborative information. The core innovation is a Bi-Tuning approach that uses trainable virtual tokens (prefix and suffix) to adapt LLMs for recommendation tasks without updating their parameters. Additionally, an MoE-based querying transformer (M-Former) is introduced to capture diverse user characteristics and integrate collaborative information effectively. Experimental results demonstrate that Laser significantly outperforms state-of-the-art baselines while maintaining high parameter efficiency.

## Method Summary
Laser combines LLMs with collaborative filtering through a Bi-Tuning framework that inserts trainable prefix and suffix tokens into the LLM input while freezing the model parameters. The prefix tokens incorporate collaborative information from pre-trained ID-based sequential recommender systems (like BERT4Rec) through an MoE-based querying transformer (M-Former) that uses multiple query experts to capture diverse user characteristics. A two-stage training strategy is employed: first optimizing item embeddings, then training the full model on these fixed embeddings. The framework uses a unified template to organize item attributes into coherent text for LLM input.

## Key Results
- Achieves up to 13.27% improvement in NDCG@10 over state-of-the-art baselines
- Achieves up to 11.37% improvement in Recall@10 over state-of-the-art baselines
- Maintains high parameter efficiency with only 0.135M trainable parameters

## Why This Works (Mechanism)

### Mechanism 1
The Bi-Tuning method adapts LLMs to sequential recommendation by inserting trainable virtual tokens at the prefix and suffix while freezing LLM parameters. By optimizing only the trainable virtual tokens, the method reduces the number of trainable parameters while allowing the LLM to learn task-specific information through the prefix (collaborative information) and suffix (space conversion). The core assumption is that virtual tokens can effectively serve as placeholders to capture task-specific information without updating the LLM's core parameters.

### Mechanism 2
The M-Former captures diverse user characteristics by employing a set of query experts to integrate user-specific collaborative information. The M-Former uses a router to select the most appropriate query expert based on the collaborative characteristics of a specific user, allowing the expert to interact with collaborative information encoded by frozen ID-based sequential recommender systems. The core assumption is that different types of users have diverse collaborative characteristics that can be effectively captured by different query experts.

### Mechanism 3
The two-stage training strategy improves recommendation results by first finding appropriate parameter weights to obtain high-quality item embeddings and then further training based on these fixed embeddings. In the first training stage, item embeddings are updated using current parameter weights, and the best-performing epoch is selected. In the second stage, Laser is trained for multiple epochs based on these fixed item embeddings to achieve optimal recommendation results. The core assumption is that high-quality item embeddings are necessary for effective user-item similarity comparison and recommendation.

## Foundational Learning

- **Large Language Models (LLMs)**: Why needed here: LLMs are used to encode item semantics and provide item/user embeddings for similarity comparison and next item prediction. Quick check question: What is the role of LLMs in the Laser framework?
- **Parameter-efficient Fine-tuning**: Why needed here: The Bi-Tuning method reduces the number of trainable parameters by freezing LLM parameters and optimizing only the trainable virtual tokens. Quick check question: How does the Bi-Tuning method achieve parameter efficiency?
- **Mixture of Experts (MoE)**: Why needed here: The M-Former employs a set of query experts to capture diverse user characteristics and integrate collaborative information. Quick check question: What is the purpose of using MoE in the M-Former?

## Architecture Onboarding

- **Component map**: Input text formulation -> Bi-Tuning (prefix and suffix) -> M-Former (collaborative information) -> Two-stage training -> Recommendation
- **Critical path**: Input text → Bi-Tuning (prefix and suffix) → M-Former (collaborative information) → Two-stage training → Recommendation
- **Design tradeoffs**:
  - Parameter efficiency vs. model performance: Bi-Tuning reduces trainable parameters but may limit the model's ability to learn complex patterns
  - Complexity of M-Former vs. effectiveness: More query experts may improve performance but increase computational cost
- **Failure signatures**:
  - Poor recommendation results: May indicate issues with Bi-Tuning, M-Former, or two-stage training
  - High computational cost: May indicate inefficiencies in the M-Former or two-stage training
- **First 3 experiments**:
  1. Test the effectiveness of Bi-Tuning by comparing recommendation results with and without trainable virtual tokens
  2. Evaluate the impact of the number of query experts in M-Former on recommendation performance
  3. Assess the contribution of two-stage training to recommendation results by comparing with single-stage training

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Laser scale with the size of the LLM backbone, and what is the optimal trade-off between model size and recommendation quality? The paper uses ChatGLM2-6B and demonstrates significant improvements over baselines, but does not explore larger or smaller LLMs to determine the optimal balance between computational efficiency and recommendation accuracy.

### Open Question 2
How does Laser's performance compare to traditional sequential recommendation methods in terms of interpretability and explainability of recommendations? The paper focuses on recommendation accuracy and parameter efficiency but does not discuss the interpretability of the generated recommendations.

### Open Question 3
How does Laser's performance vary across different types of user behavior patterns (e.g., long-term vs. short-term preferences, diverse vs. focused interests)? The paper mentions that M-Former captures diverse user characteristics but does not provide a detailed analysis of how Laser performs for different user behavior patterns.

## Limitations

- The effectiveness of virtual tokens in Bi-Tuning is not rigorously validated through ablation studies
- The router's accuracy in selecting appropriate query experts in M-Former is not empirically demonstrated
- The contribution of two-stage training to performance improvements is not isolated through comparison with alternative training strategies

## Confidence

- **High confidence**: Parameter efficiency claims (0.135M trainable parameters) and overall experimental results showing improvements over baselines
- **Medium confidence**: Theoretical framework of Bi-Tuning, M-Former, and two-stage training is logically coherent but lacks rigorous ablation studies
- **Low confidence**: Claims about virtual token effectiveness and router accuracy are not empirically validated

## Next Checks

1. Conduct ablation studies removing either the prefix tokens, suffix tokens, or both from Bi-Tuning to quantify their individual contributions to performance improvements

2. Analyze the router's expert selection accuracy by examining which experts are chosen for different user types and testing performance when forcing incorrect expert assignments

3. Compare the two-stage training approach against single-stage training and other training strategies to isolate the contribution of this design choice to the reported improvements