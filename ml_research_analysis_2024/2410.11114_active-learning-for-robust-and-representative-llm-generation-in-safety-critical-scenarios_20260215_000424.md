---
ver: rpa2
title: Active Learning for Robust and Representative LLM Generation in Safety-Critical
  Scenarios
arxiv_id: '2410.11114'
source_url: https://arxiv.org/abs/2410.11114
tags:
- active
- learning
- data
- generation
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of distributional bias in LLM-generated
  data for safety-critical applications, where models tend to over-represent common
  scenarios while neglecting rare but critical cases. The authors propose a novel
  framework that combines clustering and active learning to guide LLM generation toward
  more representative outputs.
---

# Active Learning for Robust and Representative LLM Generation in Safety-Critical Scenarios

## Quick Facts
- arXiv ID: 2410.11114
- Source URL: https://arxiv.org/abs/2410.11114
- Reference count: 14
- Key outcome: Framework combines clustering and active learning to generate more representative safety-critical data, achieving 71.6 F1 score and demonstrating transferability to other models.

## Executive Summary
This paper addresses distributional bias in LLM-generated safety data, where models over-represent common scenarios while neglecting rare but critical cases. The authors propose a novel framework that combines clustering and active learning to guide LLM generation toward more representative outputs. By iteratively using an active learner to identify informative instances from clustered data, the method ensures diverse and uniform generation of safety-critical scenarios. The framework was evaluated on a dataset of 5.4K safety violations across six categories, demonstrating improved model accuracy and F1 scores while maintaining transferability to external models.

## Method Summary
The proposed framework addresses distributional bias in LLM-generated safety data through an iterative process combining clustering and active learning. First, unlabeled data is clustered to identify regions of the vector space. An active learner then selects the most uncertain samples from each cluster based on entropy, which are labeled by human annotators. These informative samples are used to prompt an LLM via templates to generate variations of safety-critical scenarios. The generated data, along with human labels, is added to the training set and used to retrain the active learner. This process repeats until resources are exhausted, resulting in a more representative dataset that captures both common and rare safety scenarios.

## Key Results
- Clustering-based active learning produced more uniform data generation with substantially lower standard deviation (41.4 vs 57.6) in class counts compared to random sampling
- The best-performing model achieved 71.6 F1 score when trained on generated data
- Generated data improved both the active learner model and external transformer models, demonstrating transferability across architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clustering-based active learning guides LLM generation to cover rare safety scenarios more uniformly
- Mechanism: The framework first clusters the unlabeled data into regions of the vector space. From each cluster, the most uncertain samples (by entropy) are selected and passed to the LLM via templates. This ensures diverse and informative data generation that spans the full space of safety-critical scenarios, including rare but important ones like self-harm.
- Core assumption: Uncertainty in each cluster reflects the informativeness of samples for safety-critical scenario generation, and clustering ensures coverage across all data regions.
- Evidence anchors:
  - [abstract] "Our results show that the proposed framework produces a more representative set of safety scenarios without requiring prior knowledge of the underlying data distribution."
  - [section] "We observe in Table 3 that clustering-based active learning acquires more data for low-frequency classes in source data such as 'emergency' and also has substantially lower standard deviation (41.4 as opposed to 57.6 by random sampling) of counts per class."
- Break condition: If the clustering fails to capture meaningful regions of the data, or if the active learner's uncertainty measure does not correlate with informativeness for safety-critical scenarios.

### Mechanism 2
- Claim: Data generated through this framework improves both the active learner model and external models
- Mechanism: The LLM generates variations of safety-critical scenarios based on the informative samples identified by the active learner. These variations, along with human labels, are added to the training data and used to retrain the active learner. The resulting dataset is also effective for training other transformer models, showing transferability of the acquired data.
- Core assumption: The variations generated by the LLM, when prompted with informative samples, are relevant and high-quality enough to improve model performance across different architectures.
- Evidence anchors:
  - [abstract] "data acquired through our method improves the accuracy and F1 score of both the active learner model as well models outside the scope of active learning process"
  - [section] "While a bert-base-cased model was used as the learner model to provide feedback for LLM generation, we see improvement for most transformer models across Tables 5 and 6 when fine-tuned with the same generated data."
- Break condition: If the LLM fails to generate relevant variations, or if the quality of generated data is too low to improve model performance.

### Mechanism 3
- Claim: The framework addresses distributional bias in LLM-generated safety data by ensuring under-represented classes are not overlooked
- Mechanism: By using clustering and active learning, the framework iteratively guides the LLM to generate data that covers the full range of safety-critical scenarios, including those that are rare in the original data distribution. This results in a more balanced and representative dataset.
- Core assumption: The original data distribution is imbalanced, and without intervention, LLM-generated data would inherit and amplify this bias.
- Evidence anchors:
  - [abstract] "While Large Language Models (LLMs) can generate valuable data for safety measures, they often exhibit distributional biases, focusing on common scenarios and neglecting rare but critical cases."
  - [section] "Our proposed framework utilizes iterative feedback from an active learner to guide LLMs to generate safety-critical scenarios with a more uniform distribution so that less common scenarios such as self-harm are not overlooked."
- Break condition: If the framework fails to identify and generate data for under-represented classes, or if the resulting dataset remains imbalanced.

## Foundational Learning

- Concept: Active Learning
  - Why needed here: Active learning is used to identify the most informative samples from the unlabeled data pool, which are then used to guide LLM generation towards safety-critical scenarios.
  - Quick check question: How does active learning differ from random sampling in terms of data selection efficiency?

- Concept: Clustering
  - Why needed here: Clustering is used to divide the unlabeled data into regions, ensuring that the active learner selects informative samples from all parts of the data space, including those representing rare scenarios.
  - Quick check question: Why is clustering particularly useful in combination with active learning for this task?

- Concept: LLM Generation with Templates
  - Why needed here: The LLM generates variations of safety-critical scenarios based on the informative samples identified by the active learner, using predefined templates to ensure the generated data is relevant and respects human labels.
  - Quick check question: How do templates help guide the LLM to generate appropriate variations of safety-critical scenarios?

## Architecture Onboarding

- Component map:
  Unlabeled data pool -> Clustering module -> Active learner model -> Template system -> LLM -> Human annotator -> Generated data pool

- Critical path:
  1. Cluster unlabeled data
  2. Active learner identifies most uncertain samples from each cluster
  3. Human annotator labels these samples
  4. LLM generates variations using templates
  5. Generated data and human labels are added to training set
  6. Models are retrained
  7. Process repeats until resources are exhausted

- Design tradeoffs:
  - Using clustering vs. not using clustering: Clustering ensures coverage of all data regions but adds complexity.
  - Using entropy vs. other uncertainty measures: Entropy is common but may not always correlate with informativeness.
  - Using templates vs. free-form prompts: Templates ensure relevance but may limit creativity.

- Failure signatures:
  - If the active learner consistently selects samples from the same clusters, indicating a failure to explore the full data space.
  - If the LLM generates irrelevant or low-quality variations, indicating a failure in the template system or the LLM's understanding of the task.
  - If the resulting dataset remains imbalanced, indicating a failure to address distributional bias.

- First 3 experiments:
  1. Run the framework with only one iteration and compare the distribution of generated data to the original data to see if it captures more rare scenarios.
  2. Train a simple model (e.g., logistic regression) on the generated data and evaluate its performance on a held-out test set to ensure the data is of sufficient quality.
  3. Vary the number of clusters and observe its effect on the diversity and informativeness of the generated data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of clustering-based active learning vary with different clustering algorithms and hyperparameters?
- Basis in paper: [inferred] The paper mentions clustering as part of the framework but does not explore different clustering algorithms or hyperparameters.
- Why unresolved: The paper uses a specific clustering approach but does not compare it to other clustering methods or tune its parameters, leaving the impact of these choices unclear.
- What evidence would resolve it: Conducting experiments with different clustering algorithms (e.g., K-means, hierarchical clustering) and varying hyperparameters (e.g., number of clusters, distance metrics) to compare their impact on LLM generation quality and model performance.

### Open Question 2
- Question: Can the proposed framework be extended to handle multi-label classification scenarios where safety violations can belong to multiple categories simultaneously?
- Basis in paper: [inferred] The current framework focuses on single-label classification, but safety scenarios may involve overlapping categories.
- Why unresolved: The paper does not address how the framework would perform or need to be modified for multi-label classification tasks, which are common in safety-critical applications.
- What evidence would resolve it: Adapting the framework to generate multi-label data and evaluating its performance on multi-label classification tasks to determine if the clustering and active learning approach remains effective.

### Open Question 3
- Question: What is the impact of using different LLM architectures (e.g., GPT-4, Claude) on the quality and diversity of generated safety scenarios?
- Basis in paper: [explicit] The paper uses GPT-3.5-turbo for generation but does not compare it with other LLM architectures.
- Why unresolved: The choice of LLM architecture could significantly influence the diversity and quality of generated data, but this has not been explored.
- What evidence would resolve it: Generating data using different LLM architectures and comparing the resulting model performance, diversity metrics, and ability to capture rare safety scenarios.

## Limitations

- Reliance on clustering quality - poor clustering can lead to systematic under-sampling of certain safety-critical scenarios
- Template-based generation constrains diversity and may miss nuanced safety scenarios not captured by predefined templates
- Focus on text-based safety scenarios from Reddit data limits generalizability to other safety-critical domains like autonomous driving or healthcare

## Confidence

- **High confidence**: The core finding that clustering-based active learning produces more uniform data generation than random sampling, supported by quantitative evidence showing reduced standard deviation in class counts.
- **Medium confidence**: The transferability claim that generated data improves external models, as the evidence shows improvement but doesn't fully rule out model-specific effects.
- **Medium confidence**: The effectiveness of entropy as an uncertainty measure for active learning in this specific safety-critical scenario context, as alternative measures weren't systematically compared.

## Next Checks

1. **Robustness to clustering variations**: Test the framework with different clustering algorithms (e.g., k-means vs. hierarchical) and cluster counts to quantify the sensitivity of results to this design choice.

2. **Template ablation study**: Evaluate the framework's performance with varying levels of template constraint (from strict templates to free-form prompts) to determine the optimal balance between guidance and creativity in LLM generation.

3. **Cross-domain transfer validation**: Apply the generated data to train models for entirely different safety-critical domains (e.g., medical safety vs. autonomous driving) to rigorously test the claimed transferability of the approach.