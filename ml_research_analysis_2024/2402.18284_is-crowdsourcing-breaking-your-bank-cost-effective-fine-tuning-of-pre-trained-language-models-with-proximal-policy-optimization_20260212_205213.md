---
ver: rpa2
title: Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained
  Language Models with Proximal Policy Optimization
arxiv_id: '2402.18284'
source_url: https://arxiv.org/abs/2402.18284
tags:
- answers
- language
- computational
- linguistics
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high labor costs of reinforcement learning
  from human feedback (RLHF) for fine-tuning language models by proposing a self-supervised
  text ranking approach that eliminates the need for human annotators. The method
  uses probabilistic sampling to generate diverse responses, applies TextRank and
  ISODATA algorithms for ranking and clustering based on semantics, and constructs
  a reward model to learn the rank and optimize the generative policy via Proximal
  Policy Optimization.
---

# Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization

## Quick Facts
- arXiv ID: 2402.18284
- Source URL: https://arxiv.org/abs/2402.18284
- Authors: Shuo Yang; Gjergji Kasneci
- Reference count: 12
- One-line primary result: Self-supervised text ranking approach eliminates human feedback costs while improving language model performance by 1-2 points on BLEU, GLEU, and METEOR metrics

## Executive Summary
This paper proposes a cost-effective alternative to reinforcement learning from human feedback (RLHF) for fine-tuning language models by using self-supervised text ranking. The method generates diverse responses, ranks them using TextRank based on semantic similarity, clusters similar answers using ISODATA, and trains a reward model to learn quality distinctions. The approach achieves significant performance improvements on multiple datasets while reducing labor costs associated with human annotation. Manual evaluation shows 83.33% consistency with human rankings on DailyDialogue and 63.00% on CornellMovie datasets.

## Method Summary
The approach eliminates human feedback costs by using a self-supervised text ranking system. It generates multiple diverse responses for each input using probabilistic sampling, ranks them based on semantic similarity using TextRank, clusters similar responses with ISODATA to ensure diversity, and trains a reward model to distinguish high-quality from low-quality answers. The generative policy is then updated via Proximal Policy Optimization using rewards from the learned reward model, with KL divergence regularization to prevent drift. Noise injection is applied to low-ranked answers to strengthen contrastive learning signals.

## Key Results
- Significant improvements over baselines with BLEU, GLEU, and METEOR scores increasing by approximately 1-2 points
- Manual evaluation shows 83.33% consistency with human rankings on DailyDialogue dataset and 63.00% on CornellMovie dataset
- Successful reduction in training costs while maintaining or improving model performance across three tasks
- GPT-2 and GPT-Neo models show consistent improvements across different dataset types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic clustering of generated answers improves reward model training quality by focusing on representative examples
- Mechanism: ISODATA clustering removes semantically similar responses after TextRank ranking, ensuring the reward model learns from diverse answer pairs with meaningful quality differences
- Core assumption: Answers generated by the language model that are semantically similar are likely to have similar quality, so learning from multiple similar answers is redundant
- Evidence anchors:
  - "To achieve that, we regard the generated answers as nodes and the similarity between these answers as the weights of edges to construct a graph. In such a graph, we use the TextRank algorithm (Mihalcea and Tarau, 2004) to calculate the weights of the nodes to reflect their ranks."
  - "we first cluster answers by minimizing the semantic distance within the clusters. Then, we retain only one representative answer within each cluster."
- Break condition: If the language model generates answers that are semantically similar but have vastly different quality levels, clustering would remove valuable training examples

### Mechanism 2
- Claim: Self-supervised ranking through semantic similarity can approximate human preferences in answer quality assessment
- Mechanism: TextRank algorithm ranks generated answers based on their semantic similarity to each other, under the assumption that high-quality answers cluster together in semantic space
- Core assumption: If a language model generates different answers to the same question, the semantics among reasonable answers should exhibit a stronger clustering tendency than irrational ones
- Evidence anchors:
  - "we assume that the semantic similarity among different answers can reflect the correctness of these answers in theory. Specifically, high-quality answers should exhibit more similarity in the hidden space than others."
  - "we score and rank answers by quantifying their relative positions in the semantic space."
  - "Our manual evaluation demonstrated that the reward model achieved an 83.33% probability of ranking answers in the same order as the human beings"
- Break condition: If the language model generates answers that are semantically similar but differ in quality, or if semantic similarity doesn't correlate with human preference

### Mechanism 3
- Claim: Noise injection in low-ranked answers creates stronger contrastive learning signals for the reward model
- Mechanism: Adding noise to low-ranked answers makes them more distinguishable from high-ranked answers, strengthening the reward model's ability to differentiate quality
- Core assumption: The three types of noise injection (n-gram editing, negation addition/removal, sentence shuffling) create clearly distinguishable negative examples
- Evidence anchors:
  - "we designed three types of noise injection: 1) n-gram level editing operations... 2) Adding or deleting negation words... 3) We consider randomly shuffling the order of sentences..."
  - "we found that introducing noise during the training of the reward model led to improved performance"
  - "This enhancement to the additional contrastive information brought by the noise, which prevented the model from generating incorrect answers to some extent"
- Break condition: If noise injection creates answers that are too obviously wrong or if it introduces artifacts that the reward model learns to recognize rather than actual quality differences

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The paper's approach is designed to eliminate the human feedback component of RLHF while maintaining similar training objectives
  - Quick check question: What are the three main limitations of RLHF that this paper addresses?

- Concept: Policy Gradient Methods and Proximal Policy Optimization (PPO)
  - Why needed here: The paper uses PPO to update the generative policy based on rewards from the self-supervised ranking
  - Quick check question: How does PPO differ from standard policy gradient methods in terms of objective function?

- Concept: TextRank Algorithm for Unsupervised Text Ranking
  - Why needed here: The paper uses TextRank to rank generated answers based on semantic similarity before clustering
  - Quick check question: What is the key difference between TextRank and traditional ranking metrics like BLEU or ROUGE?

## Architecture Onboarding

- Component map: Language Model (LM) -> TextRank module -> ISODATA clustering module -> Reward Model -> Generative Policy (LM) -> Better answers
- Critical path: Question → LM generation → TextRank ranking → ISODATA clustering → Reward model training → PPO policy update → Better answers
- Design tradeoffs: Computational complexity vs. training quality - using three models (generative policy, reward model, sampling policy) increases resource requirements but improves self-correction ability
- Failure signatures: Poor performance on automated metrics despite good semantic clustering suggests the reward model isn't learning the right quality signals; high variance in generated answers indicates insufficient diversity control
- First 3 experiments:
  1. Baseline comparison: Run GPT-2 with standard fine-tuning on SQuAD dataset and measure EM/F1 scores
  2. Ablation study: Test the pipeline without ISODATA clustering to measure impact on reward model quality
  3. Noise injection impact: Compare reward model performance with and without the three types of noise injection on DailyDialogue dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the self-supervised text ranking approach compare to human feedback in terms of long-term model performance and generalization across diverse tasks?
- Basis in paper: [explicit] The paper mentions that manual evaluation shows high consistency with human rankings, but it does not explore long-term performance or generalization.
- Why unresolved: The study focuses on immediate performance improvements and consistency with human rankings, but does not investigate the sustainability of these improvements over time or across varied tasks.
- What evidence would resolve it: Longitudinal studies comparing models trained with self-supervised ranking versus human feedback across multiple tasks and over extended periods would provide insights into the long-term viability and generalization capabilities of the self-supervised approach.

### Open Question 2
- Question: What are the computational trade-offs of using self-supervised text ranking compared to traditional RLHF, and how do these impact the scalability of the approach?
- Basis in paper: [explicit] The paper acknowledges higher computational complexity and resource consumption due to the use of multiple models simultaneously, but does not provide a detailed analysis of the trade-offs.
- Why unresolved: While the paper highlights the cost-effectiveness in terms of labor, it does not fully explore the computational costs and their implications for scaling the approach to larger models or more complex tasks.
- What evidence would resolve it: A comprehensive analysis comparing the computational resources required for self-supervised ranking versus RLHF, including training time, memory usage, and scalability to larger models, would clarify the trade-offs and scalability potential.

### Open Question 3
- Question: How robust is the self-supervised text ranking approach to noise and adversarial inputs, and what mechanisms can be implemented to enhance its resilience?
- Basis in paper: [inferred] The paper introduces noise injection as a method to improve diversity and robustness, but does not extensively test the approach's resilience to adversarial inputs or explore additional mechanisms for enhancing robustness.
- Why unresolved: The study demonstrates some level of robustness through noise injection, but does not thoroughly investigate the approach's performance under adversarial conditions or explore other potential robustness-enhancing mechanisms.
- What evidence would resolve it: Testing the approach against various types of adversarial inputs and evaluating its performance under different noise conditions, along with exploring additional robustness mechanisms, would provide a clearer understanding of its resilience and potential improvements.

## Limitations

- Dataset scope is limited to three datasets (CornellMovie, DailyDialogue, SQuAD) covering only dialogue and question-answering tasks, with unknown effectiveness on other NLP tasks
- Manual evaluation methodology lacks clarity on whether the same human annotators evaluated both original crowdsourced data and automated rankings, creating uncertainty about comparability
- Noise injection effectiveness is not thoroughly analyzed with ablation studies showing individual impact of each noise type or optimal noise levels

## Confidence

- High confidence: The core claim that self-supervised ranking can replace human feedback for RLHF is well-supported by experimental results showing consistent improvements across multiple metrics and datasets
- Medium confidence: The claim that semantic clustering through ISODATA improves reward model training quality is plausible but relies on assumptions about semantic similarity correlating with quality
- Low confidence: The effectiveness of the noise injection strategy is less certain, as the paper provides limited analysis of how different noise types affect various model outputs

## Next Checks

1. **Cross-task generalization test**: Evaluate the method on summarization and translation tasks using datasets like CNN/DailyMail and WMT to assess whether the self-supervised ranking approach generalizes beyond dialogue and QA

2. **Noise sensitivity analysis**: Conduct experiments varying the intensity and type of noise injection to determine optimal parameters and assess whether certain noise types work better for specific output characteristics

3. **Human-in-the-loop comparison**: Run a controlled experiment where human annotators rank a subset of outputs and compare not just final rankings but also the consistency of quality assessments across automated vs. human evaluation methods, particularly for borderline cases