---
ver: rpa2
title: 'MGH Radiology Llama: A Llama 3 70B Model for Radiology'
arxiv_id: '2408.11848'
source_url: https://arxiv.org/abs/2408.11848
tags:
- radiology
- llama
- arxiv
- language
- reports
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MGH Radiology Llama, a domain-specific large
  language model fine-tuned on Llama 3 70B for radiology report generation. The model
  is trained on a comprehensive dataset of over 6.5 million de-identified medical
  reports from Massachusetts General Hospital spanning multiple imaging modalities
  and body regions.
---

# MGH Radiology Llama: A Llama 3 70B Model for Radiology

## Quick Facts
- arXiv ID: 2408.11848
- Source URL: https://arxiv.org/abs/2408.11848
- Reference count: 31
- Primary result: Fine-tuned Llama 3 70B achieves 100% ROUGE-L improvement over baseline for radiology report generation

## Executive Summary
This paper presents MGH Radiology Llama, a domain-specific large language model fine-tuned on Llama 3 70B for radiology report generation. The model is trained on a comprehensive dataset of over 6.5 million de-identified medical reports from Massachusetts General Hospital spanning multiple imaging modalities and body regions. Two fine-tuning approaches are explored: full fine-tuning and QLoRA. The model demonstrates significant improvements over the baseline Llama 3 70B, achieving a 100% improvement in ROUGE-L score, 4-5% improvement in BERTScore precision, and over 1 point increase in GPT-4o evaluation score. The results indicate that the model can generate more accurate and clinically relevant radiology impressions from findings. Notably, QLoRA fine-tuning achieves performance comparable to full fine-tuning while requiring significantly fewer computational resources, suggesting its potential for efficient adaptation of large models in specialized domains.

## Method Summary
The authors fine-tuned Llama 3 70B on 6.5 million de-identified MGH radiology reports (2008-2018) using both full fine-tuning and QLoRA approaches. The dataset includes CT, MRI, X-ray, and Fluoroscopic imaging across multiple body regions. After preprocessing to remove PHI and filter short findings, 4.35 million report pairs were used for training and 2,114 for testing. The fine-tuning used 8 NVIDIA H100 GPUs with gradient checkpointing and DeepSpeed ZeRO Stage 3. Evaluation combined traditional metrics (ROUGE-L, BERTScore) with GPT-4o-based assessment using clinician-curated criteria.

## Key Results
- 100% improvement in ROUGE-L score over baseline Llama 3 70B
- 4-5% improvement in BERTScore precision metrics
- Over 1 point increase in GPT-4o evaluation score (0-10 scale)
- QLoRA fine-tuning achieves comparable performance to full fine-tuning with reduced computational requirements

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning Llama 3 70B with domain-specific radiology data significantly improves performance over general-purpose LLMs. The large parameter count and pre-training of Llama 3 70B provide a strong foundation for adapting to specialized language patterns in radiology reports. Fine-tuning on 6.5 million de-identified reports exposes the model to diverse imaging modalities and body regions, enabling it to learn nuanced radiological terminology and reporting styles.

### Mechanism 2
QLoRA fine-tuning achieves performance comparable to full fine-tuning while requiring significantly fewer computational resources. QLoRA reduces the memory footprint by quantizing model weights and applying low-rank adaptations, allowing efficient fine-tuning on large models without full parameter updates. This approach maintains performance by focusing adaptation on task-relevant parameters while preserving general knowledge.

### Mechanism 3
The comprehensive evaluation using both traditional metrics and GPT-4-based assessment provides a multi-faceted analysis of the model's capabilities. Traditional metrics like ROUGE and BERTScore capture low-level word matching and contextual similarity, while GPT-4-based evaluation assesses high-level semantic accuracy and clinical relevance. This dual approach ensures both surface-level and deep understanding are measured.

## Foundational Learning

- Concept: Fine-tuning vs. Pre-training
  - Why needed here: Understanding the difference between adapting a pre-trained model (fine-tuning) and training from scratch (pre-training) is crucial for appreciating the efficiency gains and performance characteristics of MGH Radiology Llama.
  - Quick check question: What are the key differences between fine-tuning and pre-training, and why is fine-tuning preferred in this case?

- Concept: Quantization and Low-Rank Adaptation (QLoRA)
  - Why needed here: QLoRA is central to the efficient adaptation of large models like Llama 3 70B, enabling significant resource savings while maintaining performance.
  - Quick check question: How does QLoRA achieve computational efficiency, and what are the trade-offs compared to full fine-tuning?

- Concept: Evaluation Metrics for Text Generation
  - Why needed here: The paper uses a combination of traditional metrics (ROUGE, BERTScore) and LLM-based evaluation (GPT-4) to assess model performance. Understanding these metrics is crucial for interpreting the results.
  - Quick check question: What do ROUGE and BERTScore measure, and how do they complement each other in evaluating text generation quality?

## Architecture Onboarding

- Component map: Llama 3 70B -> Fine-tuning (Full/QLoRA) -> MGH Radiology Dataset -> Evaluation (ROUGE/BERTScore/GPT-4o)
- Critical path: 1. Data preprocessing and de-identification 2. Fine-tuning (full and QLoRA) 3. Evaluation using multiple metrics 4. Analysis and comparison of results
- Design tradeoffs: Model size vs. computational efficiency (Llama 3 70B vs. smaller models); Fine-tuning method (full vs. QLoRA) for balancing performance and resource usage; Evaluation approach (traditional metrics vs. LLM-based) for comprehensive assessment
- Failure signatures: Performance degradation if fine-tuning data is too noisy or unrepresentative; Inconsistent results between evaluation methods indicating potential issues with metrics or model quality; Resource constraints preventing effective fine-tuning of large models
- First 3 experiments: 1. Compare full fine-tuning vs. QLoRA performance on a small subset of the data to assess efficiency gains. 2. Evaluate the impact of different evaluation metrics on perceived model quality. 3. Test the model's ability to generalize to unseen radiology report formats or imaging modalities.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Evaluation relies heavily on automated metrics and LLM-based assessment without direct clinician validation
- Limited generalizability to different institutions or time periods
- Computational resources required for fine-tuning remain substantial even with QLoRA
- Model still exhibits hallucinations requiring future work on detection and mitigation

## Confidence
**High Confidence:** MGH Radiology Llama outperforms baseline Llama 3 70B with consistent improvements across multiple evaluation metrics.

**Medium Confidence:** QLoRA achieves comparable performance to full fine-tuning while reducing computational requirements, though small differences between methods and lack of detailed computational cost analysis reduce confidence.

**Low Confidence:** Clinical relevance and practical utility of generated impressions are low confidence claims as they rely on LLM-based evaluation rather than direct clinician assessment.

## Next Checks
1. Conduct blinded clinical expert validation where board-certified radiologists evaluate model outputs against ground truth and baseline LLMs
2. Test model performance on radiology reports from multiple institutions with different reporting styles and patient populations
3. Evaluate longitudinal performance on reports from different time periods to assess handling of evolving terminology and standards