---
ver: rpa2
title: Generative modeling through internal high-dimensional chaotic activity
arxiv_id: '2405.10822'
source_url: https://arxiv.org/abs/2405.10822
tags:
- training
- samples
- learning
- generated
- clamped
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes using high-dimensional chaotic dynamics in
  recurrent neural networks (RNNs) as an autonomous alternative to noise-based sampling
  in generative modeling. Three RNN architectures are introduced: a single-layer unrestricted
  model, a two-layer restricted Boltzmann machine variant with asymmetric connections,
  and a three-layer extension.'
---

# Generative modeling through internal high-dimensional chaotic activity

## Quick Facts
- arXiv ID: 2405.10822
- Source URL: https://arxiv.org/abs/2405.10822
- Reference count: 0
- Primary result: RNNs with high-dimensional chaotic dynamics can generate recognizable digits and clothing items on MNIST and Fashion-MNIST without injected noise

## Executive Summary
This paper proposes using high-dimensional chaotic dynamics in recurrent neural networks (RNNs) as an autonomous alternative to noise-based sampling in generative modeling. Three RNN architectures are introduced: a single-layer unrestricted model, a two-layer restricted Boltzmann machine variant with asymmetric connections, and a three-layer extension. All models are trained via contrastive Hebbian learning, where the "negative" phase is replaced by simulating the chaotic dynamics rather than running MCMC. Training proceeds by updating symmetric connections and biases to suppress chaos and align the dynamics with data statistics. The approach demonstrates that chaotic RNNs can serve as viable, noise-free generative models with biologically plausible learning rules.

## Method Summary
The method uses three RNN architectures (unrestricted 1-layer, restricted 2-layer, restricted 3-layer) trained via contrastive Hebbian learning. The key innovation is replacing the negative phase (typically MCMC) with chaotic dynamics simulation. Training updates symmetric connections and biases to suppress chaos and align with data statistics. Data (MNIST and Fashion-MNIST) is normalized to [-1, 1]. Four accuracy metrics (E^(2), E^(s), E^(R), E^(AAI)) quantify performance. Implementation uses PyTorch on GPU with specified hyperparameters (Nv=784, Nh=500/100, dt=1, τ=10, T=100, g=1.5, k=0.01, M=500, Ns=10000, 300k training steps).

## Key Results
- Generated recognizable digits and clothing items on MNIST and Fashion-MNIST
- Two- and three-layer restricted models achieved better visual quality than single-layer version
- Performance metrics showed improvements during training with a peak at sampling time T
- Demonstrated that chaotic RNNs can serve as noise-free generative models with biologically plausible learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chaotic dynamics in RNNs can replace injected noise in generative modeling
- Mechanism: The RNN's asymmetric recurrent connections generate high-dimensional chaotic trajectories that act as an internal source of statistical variability. Training via contrastive Hebbian learning updates weights to shape the chaotic attractor so that samples taken at a specific time window reproduce the data distribution.
- Core assumption: Chaotic dynamics can produce sufficient ergodicity to cover the data distribution's support without explicit noise injection.
- Evidence anchors:
  - [abstract] states "using internal chaotic dynamics in high-dimensional chaotic systems as a way to generate new datapoints"
  - [section] describes how asymmetric connections W and ~W create chaos and the Hebbian rule updates A to suppress chaos just enough for good generation
  - [corpus] has no direct evidence; the neighboring work on "stochastic generative methods for closure modeling" is only loosely related
- Break condition: If the attractor is too sparse or trapped in a small region, generated samples will be repetitive and fail to match data statistics.

### Mechanism 2
- Claim: Short-run out-of-equilibrium sampling is sufficient for training
- Mechanism: Instead of waiting for MCMC to equilibrate, samples are collected from the RNN after a fixed short time T starting from random initial conditions. The Hebbian updates use these samples directly, effectively learning the nonequilibrium distribution that approximates the data.
- Core assumption: The distribution of states at time T under random initialization approximates the target data distribution closely enough for learning to succeed.
- Evidence anchors:
  - [abstract] mentions "sampling the stochastic trajectories properly (i.e. with random initializations) on finite time windows"
  - [section] explicitly contrasts this with MCMC and cites [7,8] showing good-looking samples from short-run, randomly initialized sampling
  - [corpus] has no direct evidence; neighboring work on "embed and emulate" is about simulation-based inference, not short-run sampling
- Break condition: If T is too short, samples remain too correlated with initialization and fail to explore the data manifold.

### Mechanism 3
- Claim: Hebbian learning with asymmetric weights learns effective generative models
- Mechanism: The update rules ΔA = k(⟨ϕ(v)ϕ(h)⟩clamped - ⟨ϕ(v)ϕ(h)⟩free) act as a stochastic gradient on the data likelihood, using the chaotic dynamics to estimate the model distribution without MCMC. Symmetric A suppresses chaos and aligns the dynamics with data.
- Core assumption: The difference between clamped (data-driven) and free (chaotic) correlations provides a usable proxy for the likelihood gradient.
- Evidence anchors:
  - [abstract] states "simple learning rules can achieve this goal within a set of vanilla architectures"
  - [section] details the contrastive Hebbian updates for unrestricted and restricted architectures, noting they are "essentially Hebbian"
  - [corpus] has no direct evidence; neighboring work on "parametric neural amp modeling" is about active learning, not Hebbian rules
- Break condition: If the chaos is not sufficiently suppressed, updates may destabilize the dynamics; if too much suppressed, generation quality drops.

## Foundational Learning

- Concept: Contrastive Hebbian learning (CHL)
  - Why needed here: Provides a biologically plausible learning rule that replaces MCMC by using free and clamped phase correlations.
  - Quick check question: In CHL, what do the clamped and free phases represent in this chaotic RNN setting?

- Concept: Chaotic dynamics in recurrent networks
  - Why needed here: Serves as the internal noise source that replaces injected Gaussian noise in standard generative models.
  - Quick check question: How does the level of chaos (controlled by g) affect the quality of generated samples?

- Concept: Energy-based models and their connection to generative modeling
  - Why needed here: The RNN dynamics can be viewed as sampling from an implicit energy-based model defined by the weights.
  - Quick check question: What is the relationship between the Lyapunov function in symmetric RBMs and the chaotic RNN dynamics?

## Architecture Onboarding

- Component map:
  - Visible layer v (Nv units) -> Hidden layers h(1), h(2) (Nh units each) -> Recurrent connections J/W/~W (source of chaos) -> Symmetric connections A (trainable) -> Biases b, c, d -> Learning rate k

- Critical path:
  1. Initialize J/W/~W randomly, A=0, biases=0
  2. For each training step:
     - Clamp v to minibatch data, simulate h dynamics
     - Collect free-phase samples from random initial conditions
     - Compute CHL updates for A and biases
     - Apply updates with learning rate k

- Design tradeoffs:
  - Chaos level (g) vs. sample quality: higher g gives more variability but harder to train
  - Depth (1 vs 2 vs 3 layers) vs. expressivity: more layers capture finer details but increase training complexity
  - Time window T vs. sample diversity: longer T allows more exploration but may overshoot good regions

- Failure signatures:
  - Persistent mode collapse (only generating few classes)
  - Generated samples remain noisy regardless of training time
  - Training metrics plateau early at high error values
  - Network diverges (exploding activations) during free-phase simulation

- First 3 experiments:
  1. Train unrestricted model on MNIST with g=1.0, k=0.01, T=50; observe if samples become recognizable
  2. Compare 2-layer vs 3-layer restricted models on Fashion-MNIST with same hyperparameters; measure E(2) and E(AAI)
  3. Vary T from 10 to 100 and measure peak performance time for each model to find optimal sampling window

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different regularization techniques (e.g., L1, L2, dropout) affect the generative quality and chaotic dynamics of the proposed RNN architectures?
- Basis in paper: [inferred] The authors mention that their implementation uses vanilla learning rules and suggest that modifying them with standard tricks like regularization could improve results.
- Why unresolved: The paper only explores vanilla contrastive Hebbian learning without any regularization. The impact of regularization on chaotic dynamics and generative performance remains unexplored.
- What evidence would resolve it: Experiments comparing different regularization schemes on the same architectures, measuring both generative quality metrics (E(2), E(s), E(R), E(AAI)) and chaoticity measures (Lyapunov exponents, trajectory divergence).

### Open Question 2
- Question: What is the theoretical relationship between the level of chaos in the untrained network (controlled by parameter g) and the generative performance after training?
- Basis in paper: [explicit] The authors explicitly state that parameter g controls the level of chaos in the untrained network and that training suppresses chaos by planting fixed points.
- Why unresolved: While the authors observe that chaotic dynamics serve as generative noise, they don't systematically explore how the initial chaos level affects the final generative quality or the training dynamics.
- What evidence would resolve it: A systematic study varying g across multiple orders of magnitude, measuring how it affects training convergence speed, final generative quality, and the phase space structure of the trained models.

### Open Question 3
- Question: How does the proposed method compare to other generative models (GANs, VAEs, EBMs) in terms of computational efficiency and sample quality, particularly for larger datasets?
- Basis in paper: [inferred] The authors benchmark their models on MNIST and Fashion-MNIST but don't compare to other generative models, and the discussion focuses on biological plausibility rather than efficiency.
- Why unresolved: The paper demonstrates feasibility but doesn't establish whether this approach offers advantages over established methods in terms of training time, sample quality, or scalability to larger datasets.
- What evidence would resolve it: Direct comparisons of training time, sample quality metrics, and scalability to larger datasets (e.g., CIFAR-10, ImageNet) between the proposed RNN approach and state-of-the-art GANs, VAEs, and EBMs.

## Limitations
- Long-term stability of chaotic attractors under continued training remains untested beyond 300k steps
- No direct comparison with established generative models (GANs, VAEs) on identical metrics
- Biological plausibility claims rely on Hebbian learning but ignore other neural circuit constraints

## Confidence

- **High confidence**: Chaotic dynamics can generate recognizable samples on MNIST/Fashion-MNIST when properly trained
- **Medium confidence**: Short-run sampling from chaotic RNNs provides sufficient ergodicity for learning (limited empirical validation across datasets)
- **Low confidence**: Biological plausibility claims given minimal comparison with neural circuit constraints

## Next Checks
1. Test model stability beyond 300k training steps to assess attractor robustness
2. Benchmark against standard generative models using identical metrics on same datasets
3. Measure sample diversity and mode coverage to verify ergodic properties of chaotic attractors