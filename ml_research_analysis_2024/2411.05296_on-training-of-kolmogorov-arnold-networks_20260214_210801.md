---
ver: rpa2
title: On Training of Kolmogorov-Arnold Networks
arxiv_id: '2411.05296'
source_url: https://arxiv.org/abs/2411.05296
tags:
- training
- kans
- networks
- architectures
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines training dynamics of Kolmogorov-Arnold Networks
  (KANs) compared to Multi-Layer Perceptrons (MLPs) across multiple datasets and training
  schemes. The study tests various initialization methods, optimizers, learning rates,
  and layer configurations using datasets like MNIST, CIFAR-10, and IMDB.
---

# On Training of Kolmogorov-Arnold Networks

## Quick Facts
- arXiv ID: 2411.05296
- Source URL: https://arxiv.org/abs/2411.05296
- Reference count: 31
- One-line primary result: KANs show competitive accuracy with MLPs but require more careful training due to instability and overfitting tendencies

## Executive Summary
This paper examines the training dynamics of Kolmogorov-Arnold Networks (KANs) compared to Multi-Layer Perceptrons (MLPs) across multiple datasets and training schemes. The study tests various initialization methods, optimizers, learning rates, and layer configurations using datasets like MNIST, CIFAR-10, and IMDB. While KANs show competitive accuracy and better parameter efficiency than MLPs, they exhibit more unstable training dynamics and a higher tendency to overfit. The research finds that KANs perform best with Kaiming Normal initialization, adaptive optimizers like Adam, and low learning rates. When MLPs are scaled up to match KAN parameter counts, they achieve comparable or better performance. The study also evaluates back-propagation-free training using the HSIC Bottleneck, though results remain similar. A novel efficiency metric is introduced to compare models across datasets.

## Method Summary
The paper compares KANs and MLPs across 27 training schemes (3 initializations × 3 optimizers × 3 learning rates) for each dataset. Experiments use datasets including MNIST, Fashion-MNIST, CIFAR-10, IMDB, and HIGGS with varying architecture depths (shallow/medium/deep). The study tests KANs with different B-spline degrees and activation functions, and evaluates both standard backpropagation and HSIC Bottleneck training methods. A novel efficiency metric is introduced to compare parameter efficiency across datasets. The research focuses on identifying optimal training configurations for KANs and understanding their training dynamics relative to MLPs.

## Key Results
- KANs achieve competitive accuracy with MLPs but show more unstable training dynamics and higher overfitting tendency
- KANs are marginally more parameter efficient than MLPs with fixed architectures, but MLPs match or exceed KAN performance when scaled to equivalent parameter counts
- KAN performance is highly sensitive to training scheme choices, particularly initialization and learning rate
- HSIC Bottleneck training shows no significant advantage over standard backpropagation for KANs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KANs have better parameter efficiency than MLPs when networks have the same width and depth
- Mechanism: KANs replace fixed linear transformations with learnable B-spline basis functions, which can approximate complex relationships with fewer parameters per unit
- Core assumption: B-spline basis functions provide sufficient representational power to replace linear transformations without requiring proportional parameter increases
- Evidence anchors:
  - [abstract]: "KANs are an effective alternative to MLP architectures on high-dimensional datasets and have somewhat better parameter efficiency"
  - [section]: "KANs contain more parameters than MLPs even when network width and depth are fixed (because of the additional intrinsic parameters of KA-units)"

### Mechanism 2
- Claim: KANs exhibit more unstable training dynamics than MLPs
- Mechanism: The B-spline parameterization introduces additional optimization complexity due to the non-linear interaction between knot positions and basis function values
- Core assumption: The increased parameterization complexity leads to more sensitive optimization landscapes
- Evidence anchors:
  - [abstract]: "KANs... suffer from more unstable training dynamics"
  - [section]: "KANs seem to have a higher tendency to overfit (as measured by the difference between training and test accuracy) than MLPs for all training schemes"

### Mechanism 3
- Claim: KAN performance is more sensitive to training scheme choices than MLP performance
- Mechanism: The B-spline parameterization creates a more complex loss landscape that is sensitive to initialization and optimization hyperparameters
- Core assumption: The additional degrees of freedom in KANs create a more complex optimization problem
- Evidence anchors:
  - [abstract]: "KANs... suffer from more unstable training dynamics"
  - [section]: "Looking across datasets, another trend seems to emerge: final performance of KANs is more sensitive to choice of initialization, optimizer, and learning rate"

## Foundational Learning

- Concept: Kolmogorov-Arnold representation theorem
  - Why needed here: This theorem provides the theoretical foundation for why KANs can represent complex functions using compositions of univariate functions
  - Quick check question: What does the Kolmogorov-Arnold representation theorem state about multivariate functions?

- Concept: B-splines and their properties
  - Why needed here: B-splines are the fundamental building blocks of KANs, replacing the linear transformations in MLPs
  - Quick check question: How do B-splines differ from standard polynomial basis functions in terms of continuity and locality?

- Concept: Backpropagation vs. alternative training methods
  - Why needed here: The paper compares standard backpropagation with the HSIC Bottleneck algorithm for training KANs
  - Quick check question: What are the key differences between backpropagation and information bottleneck-based training approaches?

## Architecture Onboarding

- Component map: Input → KA unit evaluation (B-spline basis computation) → Activation → Weighted sum → Next layer
- Critical path: Input → KA unit evaluation (B-spline basis computation) → Activation → Weighted sum → Next layer
- Design tradeoffs:
  - Parameter efficiency vs. training stability
  - B-spline degree vs. computational cost
  - Fixed vs. learnable knot positions
  - Activation function choice (GELU vs. alternatives)
- Failure signatures:
  - Early overfitting compared to MLPs
  - High sensitivity to learning rate and initialization
  - Performance degradation with increased depth
  - Slow convergence with standard optimizers
- First 3 experiments:
  1. Compare KAN vs MLP on MNIST with Kaiming Normal initialization, Adam optimizer, learning rate 1e-4
  2. Test KAN performance with different B-spline degrees (2, 3, 4) on Fashion-MNIST
  3. Evaluate HSIC Bottleneck vs backpropagation training on CIFAR-10 with fixed architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can KANs achieve better scaling to deep architectures compared to MLPs through architectural modifications?
- Basis in paper: [explicit] The paper concludes that "vanilla KANs may not scale as well to deep architectures compared to MLPs" and suggests "future work evaluating residual connections in KAN architectures may help to alleviate some of these issues"
- Why unresolved: While the paper demonstrates that KANs struggle with deep architectures, it only tests standard KAN architectures without modifications. The paper explicitly calls for future work to test architectural modifications like residual connections.
- What evidence would resolve it: Comparative experiments testing KANs with residual connections versus MLPs with residual connections across multiple deep architectures and datasets, measuring both accuracy and training stability.

### Open Question 2
- Question: What are the optimal training schemes for KANs beyond the basic hyperparameter combinations tested?
- Basis in paper: [explicit] The paper states that "KANs merit additional study on optimal training schemes to yield their full potential" and finds that KANs are "more sensitive to choice of training scheme" than MLPs
- Why unresolved: The paper tests a comprehensive but limited set of hyperparameter combinations (3 initializations × 3 optimizers × 3 learning rates). The authors note that KANs are more sensitive to training schemes than MLPs, suggesting there may be better combinations beyond what was tested.
- What evidence would resolve it: Systematic hyperparameter optimization studies (e.g., Bayesian optimization, grid searches) exploring a wider range of learning rates, optimizers, initialization schemes, batch sizes, and learning rate schedules specifically for KANs.

### Open Question 3
- Question: Do KANs offer advantages in parameter efficiency that justify their more complex training dynamics?
- Basis in paper: [explicit] The paper notes that KANs are "marginally more parameter efficient than MLPs" and introduces a novel efficiency metric, but also observes that "when MLPs are scaled up to match KAN parameter counts, they achieve comparable or better performance"
- Why unresolved: While the paper introduces an efficiency metric and observes some parameter efficiency advantages for KANs, it doesn't fully resolve whether these advantages outweigh the increased training complexity and sensitivity. The efficiency comparisons are made at fixed architecture sizes rather than optimizing both architectures for efficiency.
- What evidence would resolve it: Comprehensive efficiency studies comparing optimally-sized KANs and MLPs across multiple tasks, including both accuracy and training time/resource requirements, to determine if KANs' parameter efficiency translates to practical advantages.

## Limitations
- The study focuses on relatively shallow networks (up to 6 layers), leaving questions about KAN scalability to deeper architectures unanswered
- HSIC Bottleneck training experiments are limited and show no significant advantage over backpropagation
- The research doesn't explore architectural modifications like residual connections that might improve KAN performance in deep networks

## Confidence
**High Confidence**: Claims about KANs' superior parameter efficiency in fixed-architecture comparisons, and their tendency to overfit more than MLPs under identical training conditions.

**Medium Confidence**: Conclusions about KANs' training instability and sensitivity to hyperparameter choices. While consistently observed, these findings may be partially mitigated by advanced training techniques not explored in this study.

**Low Confidence**: Claims about KANs' fundamental limitations for deep architectures, as the study only tests up to 6 layers. The HSIC Bottleneck results showing no significant advantage over backpropagation are based on limited experiments.

## Next Checks
1. **Scalability Test**: Evaluate KAN performance on deeper networks (10+ layers) with architectural modifications like residual connections to assess fundamental scalability limits.

2. **Advanced Regularization**: Implement modern regularization techniques (e.g., weight decay, batch normalization, advanced dropout variants) specifically tuned for KANs to determine if training instability can be mitigated.

3. **Real-World Deployment**: Test KANs on large-scale, high-dimensional datasets (e.g., ImageNet, large language models) to evaluate their practical utility beyond benchmark datasets.