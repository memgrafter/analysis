---
ver: rpa2
title: To Believe or Not to Believe Your LLM
arxiv_id: '2406.02543'
source_url: https://arxiv.org/abs/2406.02543
tags:
- distribution
- uncertainty
- responses
- joint
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel information-theoretic approach to
  quantify epistemic uncertainty in large language models (LLMs), focusing on detecting
  hallucinations. The method leverages iterative prompting to construct a pseudo joint
  distribution over multiple responses, enabling the separation of epistemic uncertainty
  (lack of knowledge) from aleatoric uncertainty (inherent randomness).
---

# To Believe or Not to Believe Your LLM

## Quick Facts
- arXiv ID: 2406.02543
- Source URL: https://arxiv.org/abs/2406.02543
- Authors: Yasin Abbasi Yadkori; Ilja Kuzborskij; András György; Csaba Szepesvári
- Reference count: 40
- Key outcome: Novel information-theoretic approach quantifies epistemic uncertainty in LLMs via iterative prompting and mutual information, outperforming likelihood/entropy methods on mixed single/multi-response datasets.

## Executive Summary
This paper introduces a novel information-theoretic approach to quantify epistemic uncertainty in large language models (LLMs), focusing on detecting hallucinations. The method leverages iterative prompting to construct a pseudo joint distribution over multiple responses, enabling the separation of epistemic uncertainty (lack of knowledge) from aleatoric uncertainty (inherent randomness). By computing a lower bound on the mutual information of this distribution, the approach identifies when epistemic uncertainty is high, signaling potential hallucinations. Experiments on question-answering benchmarks demonstrate that this method outperforms first-order approaches (e.g., likelihood or entropy-based) on datasets with mixed single- and multi-response queries, achieving higher recall and lower error rates. The approach is theoretically grounded and does not require modifying the LLM's training procedure.

## Method Summary
The method estimates epistemic uncertainty by computing mutual information (MI) between multiple LLM responses to the same query. Iterative prompting generates k responses at temperature 0.9, which are clustered by semantic equivalence using F1 score threshold τ=0.25. The MI estimator aggregates probabilities from these clusters to compute a lower bound on epistemic uncertainty. Responses with high MI are considered reliable; those with low MI trigger abstention. The approach is validated on TriviaQA, AmbigQA, and WordNet-derived datasets using Gemini 1.0 Pro, comparing MI-based abstention against likelihood, entropy, and self-verification baselines.

## Key Results
- MI-based abstention outperforms likelihood and entropy methods on mixed single/multi-label datasets, achieving higher recall and lower error rates
- The approach successfully separates epistemic uncertainty from aleatoric uncertainty, enabling more reliable hallucination detection
- Theoretical framework establishes mutual information as a lower bound on epistemic uncertainty via KL divergence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Epistemic uncertainty is measured by the mutual information between multiple LLM-generated responses, which drops when the model is uncertain.
- **Mechanism**: The iterative prompting procedure constructs a pseudo joint distribution over responses. If the LLM lacks knowledge, the responses are highly dependent, lowering mutual information. If it knows the answer, responses are independent, raising mutual information.
- **Core assumption**: The ground-truth distribution satisfies independence across multiple responses for the same query (Assumption 4.1).
- **Evidence anchors**:
  - [abstract] "This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses."
  - [section 4] Theorem 4.5 proves DKL(eQ, eP) ≥ I(eQ), establishing mutual information as a lower bound on epistemic uncertainty.
  - [corpus] Weak evidence: no direct experimental measurement of mutual information for hallucination detection; this is inferred from the theoretical bound.
- **Break condition**: If the ground-truth responses are not independent (e.g., sequential partial answers), the independence assumption fails and the metric becomes invalid.

### Mechanism 2
- **Claim**: Repeated inclusion of an incorrect answer in the prompt reduces the LLM's probability of the correct answer when epistemic uncertainty is high.
- **Mechanism**: In-context information overrides stored knowledge when the query vector is not in the span of large principal components of the key-query matrix. Repeating the wrong answer increases its weight in the softmax, causing the model to copy it.
- **Core assumption**: The self-attention mechanism can be approximated by a single-head idealized model where query and context are represented as semantic feature vectors.
- **Evidence anchors**:
  - [section 3.1] "If X has not appeared many times in the training data, and vector Y is copied in many rows of Z, then E⊤WQ(WK)⊤X could be small... Therefore f(Z; WQ, WK, WV) ≈ Y since Softmax(...) ≈ Y⊤."
  - [section 3] Figures 1-3 show the probability of the correct answer dropping as incorrect answers are repeated.
  - [corpus] No direct empirical verification of the idealized attention model; this is a theoretical explanation.
- **Break condition**: If the query is in the span of large principal components, the model will rely on stored knowledge regardless of repeated incorrect answers.

### Mechanism 3
- **Claim**: Finite-sample estimation of mutual information is possible because the expected missing mass converges quickly, even for distributions over infinite supports.
- **Mechanism**: The estimator uses a stabilization parameter and accounts for missing probability mass. For Zipf-like distributions (common in language), the expected missing mass decreases rapidly with sample size.
- **Core assumption**: The LLM's response distribution follows a Zipf distribution with exponent α > 1.
- **Evidence anchors**:
  - [section 4.1] "In Appendix E we show that if eQ is Zipf with exponent α > 1, then for any free parameter β > 0, E[Uk] = O(k−(α−1)/α−β)."
  - [section 4] Theorem 4.6 provides a non-asymptotic bound on the estimation error.
  - [corpus] Weak evidence: no empirical validation that LLM response distributions follow Zipf; this is an assumption for theoretical analysis.
- **Break condition**: If the distribution is not Zipf-like (e.g., uniform or heavy-tailed with α ≤ 1), the expected missing mass may not decrease quickly enough for accurate estimation.

## Foundational Learning

- **Concept**: Mutual Information (MI) as a measure of dependence between random variables.
  - Why needed here: MI quantifies how much knowing one response tells us about another, which indicates epistemic uncertainty.
  - Quick check question: If two responses are independent, what is their MI? (Answer: 0)

- **Concept**: Kullback-Leibler (KL) divergence as a measure of difference between probability distributions.
  - Why needed here: KL divergence between the LLM's pseudo joint distribution and the ground-truth quantifies epistemic uncertainty.
  - Quick check question: If two distributions are identical, what is their KL divergence? (Answer: 0)

- **Concept**: Missing mass in probability estimation.
  - Why needed here: The missing mass determines the error in estimating MI from finite samples, which is critical for practical implementation.
  - Quick check question: If we observe all possible outcomes, what is the missing mass? (Answer: 0)

## Architecture Onboarding

- **Component map**: LLM (Gemini 1.0 Pro) -> Iterative prompting engine -> MI estimator -> Calibration module -> Abstention policy
- **Critical path**: Prompt generation -> Response sampling -> Clustering by similarity -> MI computation -> Threshold comparison -> Abstention decision
- **Design tradeoffs**: 
  - Number of samples (k) vs. computational cost: More samples improve MI estimation but increase latency.
  - Similarity threshold (τ) vs. granularity: Lower thresholds create more clusters, potentially improving semantic equivalence detection.
  - MI vs. entropy: MI is more robust to aleatoric uncertainty but computationally more expensive than entropy.
- **Failure signatures**:
  - High MI but low epistemic uncertainty: Occurs when the LLM has high aleatoric uncertainty (multiple valid answers).
  - Low MI but high epistemic uncertainty: Occurs when the LLM's responses are nearly identical despite lacking knowledge.
  - Calibration failure: Threshold not properly tuned, leading to excessive abstentions or hallucinations.
- **First 3 experiments**:
  1. Verify the independence assumption by checking if LLM responses to the same query are correlated.
  2. Test the effect of repeated incorrect answers on response probabilities for queries with known epistemic uncertainty.
  3. Compare MI-based hallucination detection with entropy-based methods on a mixed single-label/multi-label dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the number of responses (n) in the pseudo joint distribution affect the performance of hallucination detection?
- Basis in paper: [explicit] The paper uses n=2 in the experiments but mentions that the choice of n could be varied.
- Why unresolved: The paper does not explore the impact of different values of n on the detection accuracy or computational efficiency.
- What evidence would resolve it: Experiments comparing the performance of the method with different values of n (e.g., n=2, 3, 4) on various datasets, measuring detection accuracy and computational time.

### Open Question 2
- Question: How sensitive is the method to the choice of the similarity threshold τ in the semantic equivalence clustering?
- Basis in paper: [explicit] The paper uses τ=0.25 but acknowledges that the choice of τ could affect the results.
- Why unresolved: The paper does not provide a systematic study of how different values of τ impact the performance of the method.
- What evidence would resolve it: Experiments testing the method with different values of τ (e.g., τ=0.1, 0.25, 0.5) and analyzing the impact on detection accuracy and false positive/negative rates.

### Open Question 3
- Question: Can the method be extended to handle more complex types of uncertainty, such as uncertainty due to incomplete or ambiguous information?
- Basis in paper: [inferred] The paper focuses on epistemic and aleatoric uncertainty but does not explicitly address other types of uncertainty.
- Why unresolved: The paper does not explore how the method could be adapted to handle different types of uncertainty beyond epistemic and aleatoric.
- What evidence would resolve it: Theoretical analysis and experimental validation of the method's performance on datasets with different types of uncertainty, such as those involving incomplete or ambiguous information.

## Limitations
- The independence assumption (Assumption 4.1) is critical for the theoretical framework but lacks direct experimental validation.
- The idealized self-attention model used to explain repeated-incorrect-answer degradation is theoretical and unverified for real transformer architectures.
- The Zipf distribution assumption for LLM response probabilities is convenient for theoretical error bounds but unverified.

## Confidence
- **High confidence**: The empirical observation that MI-based abstention outperforms first-order methods (likelihood, entropy) on mixed single/multi-label datasets, as demonstrated in the precision-recall curves.
- **Medium confidence**: The theoretical framework connecting MI to epistemic uncertainty via KL divergence, as the proofs are sound but rely on unverified independence assumptions.
- **Low confidence**: The theoretical explanation for repeated-incorrect-answer degradation using idealized self-attention, as this is a mechanistic hypothesis without direct empirical verification.

## Next Checks
1. **Independence assumption verification**: Measure the correlation between LLM responses to the same query across different seeds. If responses are significantly correlated, the independence assumption is violated and the theoretical framework needs revision.
2. **Repeated-incorrect-answer experiment**: Systematically vary the number of times an incorrect answer is included in the prompt and measure the probability of the correct answer across queries with known epistemic uncertainty. This would empirically validate the mechanistic hypothesis about in-context override.
3. **Distribution validation**: Analyze the empirical distribution of LLM responses (e.g., frequency vs. rank) to determine if they follow a Zipf distribution. If not, re-derive the finite-sample error bounds for the MI estimator under the true distribution.