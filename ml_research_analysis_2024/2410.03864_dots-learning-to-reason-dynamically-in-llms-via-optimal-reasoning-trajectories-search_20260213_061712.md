---
ver: rpa2
title: 'DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories
  Search'
arxiv_id: '2410.03864'
source_url: https://arxiv.org/abs/2410.03864
tags:
- reasoning
- planner
- llms
- trajectory
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DOTS, a method that enables large language
  models (LLMs) to dynamically select optimal reasoning actions for solving reasoning
  tasks. The approach constructs atomic reasoning modules (query rewriting, decomposition,
  Chain-of-Thought, Program-of-Thought, and self-verification) and searches for optimal
  action trajectories tailored to each question and the specific task-solving LLM.
---

# DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search

## Quick Facts
- **arXiv ID**: 2410.03864
- **Source URL**: https://arxiv.org/abs/2410.03864
- **Reference count**: 18
- **Primary result**: DOTS improves reasoning performance by 5.1% average across 8 tasks compared to static methods, with 75.4% accuracy on MATH using GPT-4o-mini

## Executive Summary
This paper introduces DOTS, a method that enables large language models to dynamically select optimal reasoning actions for solving reasoning tasks. The approach constructs atomic reasoning modules (query rewriting, decomposition, Chain-of-Thought, Program-of-Thought, and self-verification) and searches for optimal action trajectories tailored to each question and the specific task-solving LLM. The search process iteratively evaluates and prunes reasoning paths, collecting optimal trajectories as training data. DOTS offers two learning paradigms: fine-tuning an external planner to guide the task-solving LLM, or directly fine-tuning the task-solving LLM with internalized planning capability.

Experimental results across eight reasoning tasks demonstrate consistent improvements over static reasoning techniques and vanilla instruction tuning. With external planner tuning, DOTS achieves 75.4% accuracy on MATH using GPT-4o-mini, outperforming baselines. Internalized planner tuning further improves performance, reaching 35.5% accuracy on MATH with Llama-3-8B-Instruct. The method also generalizes well to out-of-distribution tasks, maintains efficiency with fewer output tokens than advanced methods, and enables LLMs to allocate more computation to harder problems.

## Method Summary
DOTS works by first defining atomic reasoning action modules that span analysis, solution, and verification layers. It then searches for optimal action trajectories by iteratively evaluating candidate paths, using success rate and trajectory length as selection criteria. The method collects optimal trajectories and their explanations (generated by GPT-4o) as training data. Two learning paradigms are offered: external planner tuning where a separate LLM is trained to predict optimal trajectories, or internalized planner tuning where the task-solving LLM learns to predict trajectories and explanations simultaneously. The planner is trained using supervised fine-tuning with cross-entropy loss to predict both the optimal trajectory and its explanation.

## Key Results
- DOTS improves reasoning performance by 5.1% average across 8 tasks compared to static methods
- With external planner tuning, achieves 75.4% accuracy on MATH using GPT-4o-mini
- Internalized planner tuning reaches 35.5% accuracy on MATH with Llama-3-8B-Instruct
- Maintains efficiency with fewer output tokens than advanced methods like LTM
- Enables adaptive reasoning action selection based on question characteristics and task-solving LLM capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The iterative search process identifies optimal reasoning trajectories by evaluating success rates and trajectory length.
- **Mechanism**: The algorithm maintains a candidate set of trajectories, evaluates each multiple times, and prunes to retain only the top performers based on accumulated success rate and path length.
- **Core assumption**: Success rate evaluations with multiple runs at non-zero temperature provide reliable signals for trajectory quality.
- **Evidence anchors**: Each candidate trajectory is executed for Neval times with a non-zero temperature to obtain a more reliable evaluation of its success rate. We then sort the current subset of trajectories by its success rate accumulated from the past k iterations and then the trajectory length to encourage a shorter trajectory.
- **Break condition**: If evaluation runs are too few or temperature is too high, success rate estimates become unreliable and suboptimal trajectories may be retained.

### Mechanism 2
- **Claim**: Internalized planner tuning achieves superior performance by jointly optimizing trajectory planning and problem-solving.
- **Mechanism**: The task-solving LLM is trained to predict the explanation, optimal trajectory, reasoning process, and answer simultaneously, internalizing planning capability.
- **Core assumption**: Joint optimization of planning and solving leads to better coordination between trajectory selection and reasoning execution than external planner approaches.
- **Evidence anchors**: Unlike external planner tuning which only updates the external planner (LLMp), internalized planner tuning enables the task-solving LLM to simultaneously learn trajectory planning and accurate reasoning process generation. Internalized planner tuning demonstrates superior performance DOTS outperforms existing methods on average, including prompt engineering methods and vanilla SFT.
- **Break condition**: If the solver LLM capacity is insufficient to handle both planning and solving simultaneously, performance may degrade.

### Mechanism 3
- **Claim**: Explanation generation improves planner learning by providing explicit reasoning for trajectory selection.
- **Mechanism**: The planner is trained to predict both the optimal trajectory and an explanation for why that trajectory is optimal, creating a richer training signal.
- **Core assumption**: Verbal explanations help the planner learn more robust trajectory selection patterns than trajectory labels alone.
- **Evidence anchors**: Upon obtaining reasoning actions, the solver LLMs parameterized by Î¸s then proceeds to generate the reasoning process R and the final answer y. We leverage GPT-4o to verbally explain why the trajectory is optimal. Without explanations, the planner's ability to predict optimal trajectories becomes less reliable.
- **Break condition**: If explanations are poorly generated or don't capture meaningful reasoning patterns, they may introduce noise rather than signal.

## Foundational Learning

- **Concept**: Supervised fine-tuning with cross-entropy loss
  - **Why needed here**: To train the planner LLM to predict optimal trajectories from questions using collected trajectory data
  - **Quick check question**: What loss function is used to train the planner LLM to predict trajectories, and why is it appropriate for this task?

- **Concept**: Temperature scaling in sampling
  - **Why needed here**: To obtain reliable evaluation of trajectory success rates by avoiding deterministic outputs during search
  - **Quick check question**: What temperature value is used during trajectory evaluation, and how does this affect the reliability of success rate estimates?

- **Concept**: Multi-layer reasoning action composition
  - **Why needed here**: To construct complex reasoning trajectories from atomic modules (analysis, solution, verification layers)
  - **Quick check question**: How many atomic reasoning action modules are defined, and what are the three layers they span?

## Architecture Onboarding

- **Component map**: Atomic reasoning action modules -> Trajectory search engine -> Planner LLM -> Task-solving LLM -> Explanation generation module (GPT-4o)

- **Critical path**: 
  1. Define atomic reasoning action modules
  2. Search for optimal trajectories per training question
  3. Generate explanations for optimal trajectories
  4. Fine-tune planner LLM on collected data
  5. Deploy planner to guide task-solving LLM

- **Design tradeoffs**:
  - External vs internalized planner: External offers flexibility for closed-source solvers but adds latency; internalized offers efficiency but requires model capacity
  - Search iterations vs evaluation count: More iterations with fewer evaluations reduces cost but may miss optimal trajectories; fewer iterations with more evaluations increases cost but improves reliability
  - Explanation generation: Adds training signal quality but requires additional inference cost

- **Failure signatures**:
  - Planner consistently selects suboptimal trajectories: Indicates search process not finding good trajectories or evaluation metrics misaligned
  - Explanations don't match trajectory selections: Indicates explanation generation quality issues or planner not learning from explanations
  - Performance worse than static methods: Indicates planner not adapting to question characteristics or solver capabilities

- **First 3 experiments**:
  1. Run trajectory search on small MATH subset to verify search process works and collects meaningful data
  2. Train planner on collected trajectories and test on held-out questions to verify planning capability
  3. Compare external vs internalized planner tuning on a single task to verify performance difference claim

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of DOTS vary when using different search strategies for optimal reasoning trajectories, such as Monte Carlo Tree Search (MCTS) versus the current iterative exploration and evaluation method?
  - **Basis in paper**: The paper mentions that DOTS uses an iterative exploration and evaluation process to search for optimal reasoning trajectories, and it also references recent work using MCTS for reasoning tasks.
  - **Why unresolved**: The paper does not compare the performance of DOTS using different search strategies for finding optimal reasoning trajectories. It would be valuable to understand if more sophisticated search methods like MCTS could further improve the quality of the learned reasoning trajectories and overall performance.
  - **What evidence would resolve it**: Conduct experiments comparing DOTS's performance using MCTS versus the current iterative method across various reasoning tasks, measuring accuracy, efficiency, and trajectory quality.

- **Open Question 2**: What is the impact of the number of atomic reasoning actions on the performance and efficiency of DOTS, and how does this trade-off vary across different reasoning tasks?
  - **Basis in paper**: The paper defines atomic reasoning action modules (query rewriting, decomposition, Chain-of-Thought, Program-of-Thought, and self-verification) and discusses the balance between minimizing extraneous steps and maintaining success rate.
  - **Why unresolved**: While the paper mentions that introducing excessive reasoning actions can lead to increased latency, it does not systematically explore how the number and choice of atomic actions affect performance across different reasoning tasks. Understanding this trade-off could inform the design of more efficient and effective reasoning systems.
  - **What evidence would resolve it**: Perform controlled experiments varying the set of atomic reasoning actions available to DOTS, measuring performance and efficiency across a diverse set of reasoning tasks, and analyze the optimal action sets for different task categories.

- **Open Question 3**: How does DOTS perform when applied to reasoning tasks that require reasoning across multiple modalities, such as visual reasoning combined with natural language understanding?
  - **Basis in paper**: The paper focuses on text-based reasoning tasks, but does not explore multi-modal reasoning scenarios.
  - **Why unresolved**: The paper does not investigate the applicability of DOTS to multi-modal reasoning tasks, which are increasingly important in real-world applications. Understanding how DOTS can be extended to handle multiple modalities could significantly broaden its applicability.
  - **What evidence would resolve it**: Extend DOTS to incorporate multi-modal reasoning actions and evaluate its performance on datasets that combine visual and textual reasoning, such as visual question answering or diagram-based problem solving.

## Limitations

- **Sensitivity to search parameters**: The search algorithm's performance depends heavily on evaluation parameters (K, Neval, N1, N2), but the paper doesn't provide sensitivity analysis showing how performance varies with these settings.
- **Explanation generation as confounding factor**: Improvements could stem from the explanations themselves or from the additional inference-time reasoning that GPT-4o performs when generating them.
- **Efficiency claims not directly validated**: The assertion that DOTS maintains efficiency with fewer output tokens than advanced methods isn't supported by direct token-count comparisons with the specific baselines mentioned.

## Confidence

- **High confidence**: DOTS consistently improves over vanilla instruction tuning across all tested tasks. The comparison between external and internalized planner tuning showing internalized approaches performing better is well-supported by experimental results.
- **Medium confidence**: The claim that DOTS enables adaptive reasoning action selection based on question characteristics. While results show performance improvements, the paper doesn't provide detailed analysis of which action trajectories are selected for different question types.
- **Low confidence**: The assertion that DOTS maintains efficiency with fewer output tokens than advanced methods. The paper mentions this advantage but doesn't provide direct token-count comparisons with the specific baselines mentioned.

## Next Checks

1. Perform ablation study removing explanation generation to isolate its contribution to planner performance improvements.
2. Test search algorithm sensitivity by varying K, Neval, N1, and N2 parameters across multiple runs to establish performance stability.
3. Conduct detailed analysis of trajectory selection patterns across different question types to verify adaptive reasoning claims.