---
ver: rpa2
title: 'The Anatomy of Adversarial Attacks: Concept-based XAI Dissection'
arxiv_id: '2403.16782'
source_url: https://arxiv.org/abs/2403.16782
tags:
- adversarial
- concept
- attacks
- concepts
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an analysis of the impact of adversarial attacks
  on the concepts learned by convolutional neural networks using concept-based explainable
  AI techniques. The authors perform an extensive set of experiments across various
  network architectures and targeted adversarial attack techniques.
---

# The Anatomy of Adversarial Attacks: Concept-based XAI Dissection

## Quick Facts
- **arXiv ID**: 2403.16782
- **Source URL**: https://arxiv.org/abs/2403.16782
- **Reference count**: 40
- **Primary result**: Adversarial attacks substantially alter concept composition in CNN latent space, and their perturbations decompose into a small subset of linear components responsible for attack success.

## Executive Summary
This paper analyzes how adversarial attacks affect the concepts learned by convolutional neural networks using concept-based explainable AI techniques. Through extensive experiments across multiple network architectures and attack methods, the authors demonstrate that adversarial attacks significantly modify the concept composition within CNN feature spaces. They show that adversarial perturbations can be decomposed into latent vector components, with only a small subset needed to preserve attack effectiveness. The research reveals that different attacks share similar components, suggesting common adversarial feature space directions, and that learned adversarial concepts are mostly target-specific regardless of the starting class.

## Method Summary
The paper uses white-box targeted attacks (BIM, PGD, C&W, Patch) on 400 ImageNet validation images from 8 selected classes. Concept discovery is performed using matrix factorization methods (PCA and NMF) on activation maps from CNN layers. The analysis compares concept composition between clean and adversarial samples using cosine similarity and Jaccard Index of concept saliency maps. Adversarial perturbations are decomposed using PCA to assess information distribution across components, followed by NMF to discover concepts within the perturbations themselves. The methodology evaluates how attacks modify concept saliency maps, importance weights, and similarities while identifying shared components across different attack techniques.

## Key Results
- Adversarial attacks substantially alter concept composition within CNN feature space, introducing new concepts or modifying existing ones.
- Adversarial perturbations can be linearly decomposed into a small subset of latent components (0.3% to 11.5% of components) responsible for attack success.
- Different attack techniques comprise similar components, suggesting they nudge CNN outputs towards common adversarial feature space directions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial attacks introduce new concepts or modify existing ones in CNN latent space.
- Mechanism: Attack perturbations create linear components in the feature space that act as new concept directions, altering the original concept composition.
- Core assumption: CNN concepts are linearly representable and can be discovered via matrix factorization methods like PCA or NMF.
- Evidence anchors:
  - [abstract] "adversarial attacks substantially alter the concept composition within the feature space, introducing new concepts or modifying existing ones"
  - [section 5.2] "AAs modify all of concept saliency maps, concept weights respectively ranking, and concept similarities; and even replace concepts in case of strong attacks"
  - [corpus] Weak: Corpus shows related papers on concept-based XAI but not specifically on AA-induced concept changes.
- Break condition: If concept representations are non-linear or if the attack perturbations are too small to create meaningful linear components.

### Mechanism 2
- Claim: Adversarial perturbations can be linearly decomposed into a subset of latent vector components responsible for attack success.
- Mechanism: The perturbation vector in latent space can be decomposed into principal components, with only a small subset needed to preserve most of the variance and reproduce the attack effect.
- Core assumption: Information in adversarial perturbations is unevenly distributed across linear components.
- Evidence anchors:
  - [abstract] "the adversarial perturbation itself can be linearly decomposed into a set of latent vector components, with a subset of which is primarily responsible for the attack's success"
  - [section 5.3] "the majority of δ̃ information is concentrated in a small subset of linear components (concepts), i.e., δ̃ is built from few meaningful directions in the feature space"
  - [section 5.3] "0.3% to 11.5% of components are sufficient to retain 50% of the whole variance"
- Break condition: If the perturbation information is uniformly distributed across all components or if the attack requires non-linear transformations.

### Mechanism 3
- Claim: Different attacks comprise similar components, suggesting they nudge CNN outputs towards specific common adversarial feature space directions.
- Mechanism: Concept vectors discovered in adversarial perturbations across different attack techniques cluster into similar directions, indicating shared underlying mechanisms.
- Core assumption: Adversarial attacks exploit similar feature space directions regardless of the specific attack technique used.
- Evidence anchors:
  - [abstract] "different attacks comprise similar components, suggesting they nudge the CNN intermediate outputs towards specific common adversarial feature space directions"
  - [section 5.3] "concepts discovered in different attack techniques for the same target class are similar (co-directed) and form clusters"
  - [section 5.3] "At least one of such groups is observed per clustermap"
- Break condition: If attack techniques exploit fundamentally different feature space directions or if the similarity is due to chance rather than shared mechanisms.

## Foundational Learning

- Concept: Concept-based Explainable AI (XAI)
  - Why needed here: The paper relies on concept-based XAI techniques to analyze how adversarial attacks affect learned representations in CNNs.
  - Quick check question: What is the difference between supervised and unsupervised concept-based XAI methods?

- Concept: Adversarial Attacks and Perturbations
  - Why needed here: Understanding the nature and types of adversarial attacks is crucial for analyzing their impact on CNN concepts.
  - Quick check question: What is the difference between targeted and non-targeted adversarial attacks?

- Concept: Matrix Factorization for Concept Discovery
  - Why needed here: The paper uses PCA and NMF to discover and analyze concepts in both clean and adversarial samples.
  - Quick check question: What is the main difference between PCA and NMF in terms of the constraints they impose on the discovered concepts?

## Architecture Onboarding

- Component map:
  - Data preprocessing: Load and preprocess images from ImageNet validation set
  - Adversarial attack generation: Implement BIM, PGD, C&W, and Patch attacks
  - Concept discovery: Apply PCA and NMF to discover concepts in activation maps
  - Concept comparison: Compare concepts between clean and adversarial samples using similarity metrics
  - Analysis: Analyze the impact of attacks on concept composition and perturbation decomposition

- Critical path:
  1. Load and preprocess images
  2. Generate adversarial attacks on images
  3. Extract activation maps from CNN layers
  4. Discover concepts using PCA and NMF
  5. Compare concepts between clean and adversarial samples
  6. Analyze perturbation decomposition and attack similarity

- Design tradeoffs:
  - Linear vs. non-linear concept discovery methods
  - Depth of layers used for concept discovery (deeper layers capture higher-level concepts but may be more affected by attacks)
  - Choice of similarity metrics for concept comparison

- Failure signatures:
  - Poor concept discovery results (low similarity scores, meaningless concepts)
  - Inconsistent attack effects across different CNN architectures
  - Failure to reproduce attack effects using linear perturbation components

- First 3 experiments:
  1. Compare concept composition between clean and adversarial samples using a single attack type and CNN architecture.
  2. Analyze the information distribution in adversarial perturbations using PCA decomposition.
  3. Compare concept vectors discovered in adversarial perturbations across different attack techniques.

## Open Questions the Paper Calls Out
- How do non-targeted and universal adversarial attacks impact concept composition in CNNs compared to targeted attacks?
- Can the concept-level patterns induced by adversarial attacks be utilized for real-time detection of adversarial examples?
- How effective are adversarial elimination techniques that aim to remove or mitigate the impact of adversarial concepts within learned representations?

## Limitations
- The study is limited to 8 ImageNet classes and 400 clean images, which may not generalize to broader distributions.
- The concept discovery pipeline assumes CNN concepts are linearly separable, which may not hold for all network architectures or attack types.
- The paper does not address potential non-linear interactions between concepts or explore temporal stability across training runs.

## Confidence
- High confidence in the claim that different attacks comprise similar components, due to clear clustering patterns across multiple attack techniques.
- Medium confidence in claims about adversarial attacks altering concept composition and decomposing into meaningful linear components, limited by reliance on linear factorization methods and restricted dataset scope.
- Low confidence in the generalizability of findings to non-targeted, black-box, and universal attacks, as these attack types were not explored in the study.

## Next Checks
1. Test concept alteration claims on a more diverse dataset (e.g., 50+ classes from multiple domains) to assess generalization.
2. Validate perturbation decomposition using non-linear methods (e.g., autoencoders) to confirm linear components capture sufficient information.
3. Evaluate concept similarity metrics across different CNN architectures and training seeds to assess robustness of attack direction findings.