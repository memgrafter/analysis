---
ver: rpa2
title: 'Learning by Analogy: Enhancing Few-Shot Prompting for Math Word Problem Solving
  with Computational Graph-Based Retrieval'
arxiv_id: '2411.16454'
source_url: https://arxiv.org/abs/2411.16454
tags:
- computational
- math
- data
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a computational graph-based retrieval method
  to improve math word problem solving in large language models by leveraging structural
  similarity between problems. The core idea is to train a retriever model using contrastive
  learning to identify examples with similar computational graphs, which are then
  used in few-shot prompting to guide the LLM.
---

# Learning by Analogy: Enhancing Few-Shot Prompting for Math Word Problem Solving with Computational Graph-Based Retrieval
## Quick Facts
- arXiv ID: 2411.16454
- Source URL: https://arxiv.org/abs/2411.16454
- Reference count: 12
- Improves few-shot math problem solving by 6.7% exact match accuracy on average using computational graph-based retrieval

## Executive Summary
This paper introduces a computational graph-based retrieval method to enhance math word problem solving in large language models by leveraging structural similarity between problems. The core innovation involves training a retriever model using contrastive learning to identify examples with similar computational graphs, which are then used in few-shot prompting to guide the LLM. Experiments across six datasets demonstrate that this approach significantly outperforms semantic-based and random selection baselines, achieving up to 6.7% improvement in exact match accuracy on average. The method is particularly effective for smaller models and requires minimal human-annotated data, with performance stabilizing using only 25% of the training data.

## Method Summary
The proposed approach constructs computational graphs from math word problems, where nodes represent variables and edges represent mathematical operations. A retriever model is trained using contrastive learning to identify examples with similar computational graph structures. During inference, given a new problem, the retriever selects structurally analogous examples from a retrieval database, which are then used in few-shot prompting to guide the LLM's solution generation. The method employs LLM-generated computational graphs for training data creation, eliminating the need for manual annotations. The retriever model learns to match problems based on their underlying mathematical structure rather than semantic similarity, enabling more effective analogical reasoning for math problem solving.

## Key Results
- Achieves up to 6.7% improvement in exact match accuracy on average compared to semantic-based and random selection baselines
- Demonstrates effectiveness with smaller models, making the approach accessible for resource-constrained scenarios
- Shows performance stabilization using only 25% of the training data, reducing data requirements
- Case studies confirm successful identification of structurally analogous examples even when semantic similarity is low

## Why This Works (Mechanism)
The method works by recognizing that mathematical problems with similar computational structures can be solved using analogous reasoning patterns, regardless of their surface-level semantic differences. By encoding problems as computational graphs and training a retriever to match based on graph structure, the system can identify relevant analogical examples that semantic-based methods might miss. This structural matching enables the LLM to leverage proven solution patterns from similar problems, improving reasoning accuracy through analogy-based few-shot prompting.

## Foundational Learning
- Computational graphs: Mathematical representations where nodes are variables and edges are operations; needed to capture problem structure for retrieval matching; quick check: verify graph construction preserves mathematical relationships
- Contrastive learning: Training framework that learns to match similar examples while distinguishing dissimilar ones; needed to train the retriever on graph similarity; quick check: validate retrieval accuracy on known similar problems
- Few-shot prompting: Technique using a small number of examples to guide LLM reasoning; needed to leverage retrieved analogical examples; quick check: test with varying numbers of retrieved examples
- LLM-generated data: Using language models to automatically create training examples; needed to eliminate manual annotation burden; quick check: validate generated graphs against human annotations

## Architecture Onboarding
**Component map:** Math Word Problem -> Computational Graph Construction -> Contrastive Retriever Training -> Retrieval Database -> Few-Shot Prompting -> LLM Solution Generation

**Critical path:** The retriever training and retrieval database form the critical path, as their quality directly determines the effectiveness of the few-shot prompting stage.

**Design tradeoffs:** The paper prioritizes automatic data generation over manual annotation, trading potential accuracy for scalability and eliminating annotation costs. The focus on graph structure rather than semantic similarity may miss problems that share context but have different mathematical structures.

**Failure signatures:** Poor retrieval quality manifests as irrelevant examples in few-shot prompts, leading to degraded LLM performance. Overly complex graphs may reduce retrieval effectiveness, while overly simplified graphs may miss important structural details.

**First experiments:** (1) Validate computational graph construction accuracy against ground truth, (2) Test retriever matching accuracy on problems with known structural similarity, (3) Evaluate few-shot prompting performance with both retrieved and randomly selected examples.

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Limited analysis of graph construction robustness to noisy or incomplete inputs
- Reliance on LLM-generated training data introduces uncertainty about label quality
- Evaluation focuses primarily on accuracy metrics without deeper analysis of failure modes or computational efficiency

## Confidence
- Core retrieval approach effectiveness: High
- Automatic data generation reliability: Medium
- Generalizability across problem types: Medium
- Computational efficiency claims: Low

## Next Checks
1. Stress-test the graph retrieval approach on problems with noisy or incomplete computational graphs to assess robustness
2. Evaluate retrieval quality using human annotations rather than LLM-generated labels to validate the automatic data generation pipeline
3. Conduct ablation studies on different graph representation choices to assess sensitivity to structural variations and optimize the approach