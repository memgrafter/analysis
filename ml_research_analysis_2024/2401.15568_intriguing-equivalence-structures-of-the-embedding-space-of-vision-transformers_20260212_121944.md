---
ver: rpa2
title: Intriguing Equivalence Structures of the Embedding Space of Vision Transformers
arxiv_id: '2401.15568'
source_url: https://arxiv.org/abs/2401.15568
tags:
- embedding
- value
- component
- projection
- principal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that vision transformer models have complex
  embedding spaces with large piecewise linear regions where visually very different
  images share the same embeddings, and local normal spaces where visually indistinguishable
  images have very different embeddings. The authors propose an effective gradient-descent-based
  procedure to match any input's embedding to a target, revealing these equivalence
  structures.
---

# Intriguing Equivalence Structures of the Embedding Space of Vision Transformers

## Quick Facts
- arXiv ID: 2401.15568
- Source URL: https://arxiv.org/abs/2401.15568
- Authors: Shaeke Salman, Md Montasir Bin Shams, Xiuwen Liu
- Reference count: 40
- Key outcome: Vision transformers have complex embedding spaces with large piecewise linear regions where visually different images share embeddings, and local normal spaces where visually indistinguishable images have very different embeddings, causing overgeneralization and adversarial vulnerability.

## Executive Summary
This paper reveals the algebraic and geometric structures of embedding spaces in vision transformer models, demonstrating that inputs can share embeddings despite being visually very different, while visually indistinguishable inputs can have very different embeddings. The authors propose an effective gradient-descent-based procedure to match any input's embedding to a target, revealing these equivalence structures. They also estimate local directional Lipschitz constants, showing embeddings are sensitive to small changes in normal directions but insensitive to changes in null space directions. This structural property causes models to overgeneralize and be vulnerable to adversarial attacks, with limited semantically meaningful generalization capability.

## Method Summary
The authors developed a gradient-descent-based embedding matching procedure to align any input's embedding to a target embedding by minimizing mean squared error loss. They computed Jacobian matrices at input images and performed singular value decomposition to analyze the local linear structure of the embedding space. Local directional Lipschitz constants (LLDLC) were estimated along random directions and gradient-optimized directions to quantify sensitivity to perturbations. The framework was applied to various vision transformers including CLIP, BEiT, DEiT, Swin, ViTMAE, and ViTMSN across datasets like ImageNet, MS-COCO, and Google Open Images.

## Key Results
- Vision transformers contain large piecewise linear subspaces where visually very different images share the same embeddings
- Local normal spaces exist where visually indistinguishable images have very different embeddings
- Estimated LLDLC values are two orders of magnitude larger in gradient-optimized directions compared to random directions
- The embedding matching procedure successfully matches embeddings while keeping visual changes minimal

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The gradient-descent-based embedding matching procedure works effectively because vision transformer models are piecewise linear within activation regions.
- Mechanism: When input perturbation remains within a single activation region (defined by ReLU), the transformer can be locally approximated as a linear function, allowing the MSE loss to have a smooth gradient pointing toward the target embedding.
- Core assumption: Input perturbation required to match embeddings is small enough to stay within one activation region.
- Evidence anchors: [abstract] "the gradient-descent-based optimization procedures", [section] "the linear approximation works well and we do not need to change an input much in order to match the embedding of another input"

### Mechanism 2
- Claim: The embedding space contains large piecewise linear subspaces where different inputs share the same representation.
- Mechanism: ReLU activation creates piecewise linear regions in the embedding space. Within each region, small changes to input don't change the embedding because the Jacobian becomes rank-deficient (null space exists).
- Core assumption: Training process and model architecture create connected regions of constant embedding.
- Evidence anchors: [abstract] "large piecewise linear subspaces where there exist very different inputs sharing the same representations", [section] "the null space where the embeddings do not change as the input changes"

### Mechanism 3
- Claim: Local directional Lipschitz constants are much larger in normal directions than in null space directions.
- Mechanism: Jacobian's singular value decomposition reveals that certain directions (normal space) cause rapid changes in embedding, while others (null space) cause no change, creating vulnerability to adversarial perturbations.
- Core assumption: Jacobian can be accurately estimated and decomposed to reveal directional sensitivities.
- Evidence anchors: [abstract] "we also define and estimate the local directional Lipschitz constant (LLDLC)", [section] "Fig. 4 (bottom) shows the estimated LLDLC values along the directions given by our gradient optimization procedure. Those values are two orders of magnitude larger than the random directions"

## Foundational Learning

- Concept: Jacobian matrix and singular value decomposition
  - Why needed here: Understanding how Jacobian's singular values reveal structure of embedding space (null vs normal directions)
  - Quick check question: What does a singular value of zero in the Jacobian indicate about that direction in input space?

- Concept: Piecewise linear functions and activation regions
  - Why needed here: Explaining why small perturbations within activation regions don't change embeddings
  - Quick check question: How does the ReLU activation create piecewise linear regions in neural network outputs?

- Concept: Lipschitz continuity and local directional Lipschitz constants
  - Why needed here: Quantifying how sensitive embeddings are to small changes in different directions
  - Quick check question: What does a large local directional Lipschitz constant indicate about model behavior in that direction?

## Architecture Onboarding

- Component map: Input image patches → linear projection → positional encoding → Transformer blocks (Multi-head self-attention → LayerNorm → MLP → LayerNorm) → Embedding extraction → Gradient descent optimization → Updated input
- Critical path: Input → Transformer → Embedding → Loss computation → Gradient → Updated input → New embedding
- Design tradeoffs: Model complexity vs interpretability, perturbation size vs matching accuracy, computational cost vs precision of Jacobian estimation
- Failure signatures: Loss plateaus before convergence, gradient explosions, Jacobian singular values inconsistent with observed behavior
- First 3 experiments:
  1. Test embedding matching on simple, known piecewise linear function (e.g., single ReLU layer)
  2. Verify Jacobian estimation by comparing numerical vs analytical gradients on small model
  3. Test LLDLC estimation by perturbing inputs along known singular vector directions and measuring embedding changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed framework be applied to understand the embedding spaces of large language models (LLMs) like GPT-4 or LLaMA?
- Basis in paper: [inferred] The authors state that the framework can be applied to "any model directly as long as the input varies continuously so that the Jacobian can be estimated properly" and mention that "with multimodal models, the framework can also be used to study other models with discrete inputs indirectly via other joint embeddings."
- Why unresolved: The paper only demonstrates the framework on vision transformers and image-based multimodal models. The effectiveness and insights gained from applying it to text-based models remain unexplored.

### Open Question 2
- Question: How can the semantic limitations of transformer embeddings be mitigated by incorporating alignment-sensitive components?
- Basis in paper: [explicit] The authors state that "The plausible root cause of such adversarial examples and also semantically different images with identical embeddings is that transformers do not require the inputs to be aligned to have similar embeddings. By adding alignment-sensitive components to the embedding could mitigate the problem, which is being investigated further."
- Why unresolved: The paper identifies the lack of alignment as a potential root cause but does not explore specific solutions or their effectiveness.

### Open Question 3
- Question: How do the embedding space structures of transformers vary across different training datasets and training algorithms?
- Basis in paper: [inferred] The authors note that "the distribution of the singular values shows that the Jacobian has several dominating directions, reflecting the training set and the training algorithm being used." However, they do not systematically investigate how these structures change with different datasets or training methods.
- Why unresolved: The paper only uses a few pre-trained models trained on specific datasets and does not explore the impact of dataset characteristics or training algorithm choices on the resulting embedding space structures.

## Limitations

- The piecewise linear assumption lacks direct corpus evidence and detailed analysis of when and why embeddings become piecewise linear
- Jacobian estimation methodology for large transformer models raises numerical stability concerns that aren't explicitly addressed
- The claim about "limited semantically meaningful generalization capability" is based on structural properties but doesn't directly measure practical impact on model performance or robustness

## Confidence

**High Confidence**: The observation that vision transformers exhibit large piecewise linear subspaces where different inputs share embeddings. This is directly demonstrated through the successful embedding matching procedure and is the core empirical finding of the paper.

**Medium Confidence**: The characterization of embedding space as having large null space regions and sensitive normal directions. While the Jacobian analysis provides theoretical support, the exact quantitative relationship between these structural properties and model behavior requires further validation.

**Low Confidence**: The claim about "limited semantically meaningful generalization capability." While the structural analysis suggests potential vulnerabilities, the paper doesn't provide direct evidence linking these embedding space properties to actual performance degradation or semantic understanding limitations.

## Next Checks

1. **Convergence robustness analysis**: Systematically vary learning rates and perturbation magnitudes in the embedding matching procedure across different models to determine boundaries of piecewise linear regions and identify failure modes.

2. **Jacobian estimation validation**: Compare estimated Jacobian singular values against analytical gradients on smaller, more tractable transformer variants to verify accuracy of numerical approximation method.

3. **Generalization impact quantification**: Design controlled experiments measuring actual model performance degradation (both classification accuracy and adversarial robustness) as a function of observed embedding space structural properties.