---
ver: rpa2
title: 'Lost in the Middle, and In-Between: Enhancing Language Models'' Ability to
  Reason Over Long Contexts in Multi-Hop QA'
arxiv_id: '2412.10079'
source_url: https://arxiv.org/abs/2412.10079
tags:
- document
- documents
- summaries
- standard
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the "lost in the middle" problem in multi-hop
  question answering, where models struggle to effectively use information distributed
  across long contexts. The authors find that model performance degrades not only
  with the absolute position of evidence documents but also with the distance between
  them.
---

# Lost in the Middle, and In-Between: Enhancing Language Models' Ability to Reason Over Long Contexts in Multi-Hop QA

## Quick Facts
- **arXiv ID**: 2412.10079
- **Source URL**: https://arxiv.org/abs/2412.10079
- **Reference count**: 40
- **Primary result**: Model performance degrades with both absolute document position and distance between evidence documents in multi-hop QA, with context reduction methods mitigating positional bias at the cost of accuracy.

## Executive Summary
This study examines the "lost in the middle" problem in multi-hop question answering, where language models struggle to effectively use information distributed across long contexts. The authors systematically investigate how model performance varies based on the absolute position of evidence documents within a 20-document context and the distance between them. Through experiments across three multi-hop QA datasets and three different models, they find that performance degrades not only with distance from context edges but also with separation between relevant documents. While chain-of-thought prompting helps identify relevant documents, it fails to resolve performance disparities caused by document positioning. Context reduction techniques like summarization and knowledge graph triple extraction mitigate positional biases but at the cost of overall accuracy.

## Method Summary
The study evaluates three language models (GPT-3.5-turbo-1106, MPT-7b-8k-instruct, and Llama-2-7b-longlora-8k-ft) on three multi-hop QA datasets (HotpotQA, 2WikiMultihopQA, and MuSiQue-Ans). Evidence documents are positioned at various locations within a 20-document context, with 5 adjacent and 4 separated configurations evaluated due to computational constraints. The study tests full documents, summarized documents (using BART-large-CNN), and knowledge graph triples (using LLaMA 2) as input formats, with and without chain-of-thought prompting. Performance is measured using best-subspan accuracy, where a score of 1 is assigned if the model's output contains the annotated answer.

## Key Results
- Model performance degrades both with absolute position from context edges and with distance between evidence documents
- Chain-of-thought prompting improves document identification but does not resolve positional performance disparities
- Context reduction techniques flatten positional bias curves but reduce overall accuracy due to information loss
- Instruction-tuned models benefit from CoT prompting while non-instruction-tuned models may experience performance drops

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The "lost in the middle" problem worsens in multi-hop QA because models fail to effectively integrate evidence spread across non-adjacent documents.
- Mechanism: When relevant documents are separated by distractor documents, the model's attention mechanism struggles to maintain strong connections between them. This leads to degraded performance compared to when evidence documents are adjacent.
- Core assumption: The attention mechanism's effectiveness decreases with distance between relevant documents.
- Evidence anchors:
  - [abstract] "performance degrades not only with respect to the distance of information from the edges of the context, but also between pieces of information"
  - [section 5] "models perform better when relevant documents are adjacent compared to when they are separated by distractor documents"
- Break condition: If the model develops a mechanism to maintain connections between non-adjacent documents regardless of intervening distractors.

### Mechanism 2
- Claim: Chain-of-Thought prompting helps identify relevant documents but doesn't resolve positional biases in multi-hop reasoning.
- Mechanism: CoT prompting improves the model's ability to reason through the reasoning steps needed to answer multi-hop questions, but it doesn't overcome the fundamental attention distribution problem that causes positional bias.
- Core assumption: CoT provides explicit reasoning structure without addressing underlying attention patterns.
- Evidence anchors:
  - [abstract] "Chain-of-Thought prompting aids in identifying relevant documents but fails to resolve the performance disparity caused by evidence document positions"
  - [section 6] "For instruction-tuned models such as MPT and GPT-3.5, CoT prompting with few-shot exemplars markedly improves performance compared to zero-shot settings in most scenarios"
- Break condition: If CoT prompting is combined with architectural changes that address positional attention biases.

### Mechanism 3
- Claim: Context reduction methods flatten positional bias curves but reduce overall accuracy due to information loss.
- Mechanism: Summarization and knowledge graph triple extraction reduce the total information volume, making positional differences less pronounced. However, they also remove potentially important details needed for accurate reasoning.
- Core assumption: Reducing context size can mitigate positional bias but at the cost of information fidelity.
- Evidence anchors:
  - [abstract] "Context reduction techniques like summarization and knowledge graph triple extraction mitigate positional biases but at the cost of overall accuracy"
  - [section 6] "Context reduction mitigates position biases but sacrifices accuracy"
- Break condition: If context reduction methods are improved to preserve essential reasoning information while still reducing positional bias.

## Foundational Learning

- Concept: Attention mechanisms in transformer models
  - Why needed here: Understanding how positional biases arise from attention patterns is crucial for diagnosing the "lost in the middle" problem
  - Quick check question: How does the attention mechanism weight tokens based on their position in the input sequence?

- Concept: Multi-hop reasoning
  - Why needed here: The paper specifically examines how positional biases affect complex reasoning tasks that require multiple reasoning steps across documents
  - Quick check question: What distinguishes multi-hop QA from single-hop QA in terms of information integration requirements?

- Concept: Context reduction techniques (summarization and knowledge graph extraction)
  - Why needed here: The paper experiments with these methods as potential solutions to positional bias problems
  - Quick check question: What are the trade-offs between different context reduction approaches in terms of information preservation versus conciseness?

## Architecture Onboarding

- Component map:
  Input layer -> Context reduction module (optional) -> Reasoning module (CoT prompting) -> Output layer -> Evaluation module

- Critical path:
  1. Load multi-hop QA dataset (HotpotQA, 2WikiMultihopQA, or MuSiQue-Ans)
  2. Create prompts with evidence documents positioned at various locations within 20-document context
  3. Apply context reduction if enabled
  4. Generate responses with or without CoT prompting
  5. Evaluate using best-subspan accuracy
  6. Analyze performance patterns across positional configurations

- Design tradeoffs:
  - Exhaustive vs. selective evidence position evaluation (computational constraints vs. comprehensive analysis)
  - Context reduction vs. information preservation (reducing positional bias vs. maintaining reasoning capability)
  - Instruction-tuned vs. non-instruction-tuned models (better baseline performance vs. different bias patterns)

- Failure signatures:
  - Performance consistently worse when evidence documents are in middle positions
  - Performance drops when evidence documents are separated by distractors
  - CoT prompting helps in instruction-tuned models but hurts non-instruction-tuned models
  - Context reduction flattens positional bias but reduces overall accuracy

- First 3 experiments:
  1. Test baseline performance across all three datasets with evidence documents at positions (1,2), (5,6), (10,11), (15,16), (19,20) using full documents without CoT
  2. Evaluate the same configurations with chain-of-thought prompting enabled to observe reasoning improvements
  3. Apply context reduction techniques (summarization and KG extraction) to the same setups to measure positional bias mitigation effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do positional biases manifest in models with different attention mechanisms (e.g., linear biases vs. softmax attention)?
- Basis in paper: [explicit] The paper experiments with models using ALiBi (linear biases) and Flash Attention (softmax attention), but does not deeply analyze how these mechanisms impact positional biases.
- Why unresolved: The study does not isolate the effects of attention mechanisms on positional biases, focusing instead on model-specific performance differences.
- What evidence would resolve it: Comparative experiments isolating attention mechanisms while controlling for other factors, measuring positional biases across multiple datasets and document arrangements.

### Open Question 2
- Question: Can dynamic context reduction methods improve both accuracy and positional bias mitigation in multi-hop QA?
- Basis in paper: [inferred] The paper notes that static context reduction techniques (summarization and knowledge graph extraction) trade off between accuracy and positional bias, suggesting room for more sophisticated approaches.
- Why unresolved: The study only tests out-of-the-box reduction methods without exploring dynamic or adaptive techniques tailored to multi-hop reasoning.
- What evidence would resolve it: Experiments with context reduction methods that adapt to document relevance and reasoning complexity, measuring both accuracy and positional bias reduction.

### Open Question 3
- Question: How does the "lost in the middle" problem scale with increasing context length and reasoning complexity?
- Basis in paper: [explicit] The paper focuses on fixed context sizes (20 documents) and up to 4 reasoning hops, leaving the scaling behavior unexplored.
- Why unresolved: Computational constraints limited the study to a specific context size and hop count, preventing analysis of larger or more complex scenarios.
- What evidence would resolve it: Experiments systematically varying context length and reasoning steps, measuring performance degradation and positional bias as these parameters increase.

## Limitations

- The study evaluates only a subset of evidence position combinations (5 adjacent, 4 separated) due to computational constraints, potentially missing important patterns in the full positional configuration space.
- The context reduction methods tested (summarization and KG extraction) represent only two approaches, and other techniques might yield different results in balancing accuracy and positional bias mitigation.
- The findings are based on specific model configurations and datasets, which may limit generalizability to other multi-hop QA scenarios or model architectures.

## Confidence

**High Confidence:** The observation that model performance degrades with absolute position from context edges and with distance between evidence documents is well-supported by multiple experimental configurations across different datasets and models.

**Medium Confidence:** The claim that context reduction flattens positional bias curves while reducing overall accuracy is supported by the experimental data, but the specific magnitude of these effects may vary with different context reduction techniques or parameter settings.

**Low Confidence:** The paper does not provide a clear mechanistic explanation for why instruction-tuned models benefit from CoT prompting while non-instruction-tuned models do not.

## Next Checks

1. **Replicate with Expanded Positional Configurations:** Evaluate the same models and datasets across all 20-document position combinations (190 unique pairs) rather than the subset, to verify that the observed patterns hold across the full space of positional configurations and to identify any edge cases not captured in the initial analysis.

2. **Cross-Task Validation:** Test the same experimental setup on single-hop QA tasks to determine whether the positional biases observed in multi-hop reasoning are task-specific or represent a more general limitation of long-context language models.

3. **Alternative Context Reduction Methods:** Implement and evaluate additional context reduction techniques such as dynamic programming-based context selection or adaptive windowing approaches to determine whether the observed trade-off between positional bias mitigation and accuracy loss is inherent to context reduction or specific to the summarization and KG extraction methods used in this study.