---
ver: rpa2
title: Understanding Hidden Computations in Chain-of-Thought Reasoning
arxiv_id: '2412.04537'
source_url: https://arxiv.org/abs/2412.04537
tags:
- reasoning
- hidden
- tokens
- filler
- characters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how transformer models trained with Chain-of-Thought
  (CoT) prompting and hidden/filler tokens actually process reasoning steps internally.
  The authors analyze a transformer model trained on the 3SUM task, where the model
  is trained to output filler tokens ("...") instead of explicit reasoning steps.
---

# Understanding Hidden Computations in Chain-of-Thought Reasoning

## Quick Facts
- arXiv ID: 2412.04537
- Source URL: https://arxiv.org/abs/2412.04537
- Authors: Aryasomayajula Ram Bharadwaj
- Reference count: 8
- This paper investigates how transformer models trained with Chain-of-Thought (CoT) prompting and hidden/filler tokens actually process reasoning steps internally.

## Executive Summary
This paper investigates how transformer models trained with Chain-of-Thought (CoT) prompting and hidden/filler tokens actually process reasoning steps internally. The authors analyze a transformer model trained on the 3SUM task, where the model is trained to output filler tokens ("...") instead of explicit reasoning steps. Using the logit lens method and token ranking analysis, they demonstrate that the model retains the original reasoning computations in its internal representations, even when producing filler tokens at the output layer. The hidden characters can be recovered by examining lower-ranked tokens during decoding, specifically by selecting rank-2 tokens instead of the top-ranked filler tokens. This approach successfully reconstructs the original reasoning steps without loss of performance on the task.

## Method Summary
The authors investigate transformer models trained on the 3SUM task, where the model must determine if any three numbers in a list sum to zero. The model is trained to output filler tokens ("...") instead of explicit reasoning steps. The analysis uses the logit lens method to examine layer-wise representations and token ranking analysis to identify when hidden reasoning computations are preserved. A modified decoding algorithm is then used to recover the hidden characters by selecting rank-2 tokens when the top-ranked token is a filler. The model architecture is based on LLaMA with 4 layers, 384 hidden dimensions, and 6 attention heads, trained from scratch on synthetic data for 5 epochs.

## Key Results
- The model performs actual reasoning computations in earlier layers while overwriting intermediate representations with filler tokens in later layers
- Hidden reasoning steps are preserved in lower-ranked tokens (rank-2) during decoding, even when filler tokens are top-ranked
- The original reasoning steps can be recovered using modified decoding algorithms without loss of performance on the 3SUM task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model performs the actual reasoning computations in earlier layers, but overwrites the intermediate representations with filler tokens in later layers to match training targets
- Mechanism: Layer-wise computation where lower layers (1-2) maintain numerical reasoning representations while higher layers (3-4) shift to filler token dominance
- Core assumption: The model's computation graph can be decomposed into a reasoning phase followed by an output formatting phase
- Evidence anchors:
  - [abstract] "demonstrate that the hidden characters can be recovered without loss of performance"
  - [section] "In the initial layers (layers 1 and 2), the model's activations corresponded to the raw numerical sequences associated with the 3SUM problem's chain of thought"
  - [section] "By layer 4 (the final layer), the filler token dominated the top predictions, and the original numerical tokens were relegated to lower ranks (rank-2)"

### Mechanism 2
- Claim: The hidden reasoning steps are preserved in lower-ranked tokens during decoding, even when filler tokens are top-ranked
- Mechanism: Token ranking analysis shows original reasoning tokens appear at rank-2 positions when filler tokens are rank-1, allowing recovery through modified decoding
- Core assumption: The model maintains multiple candidate representations simultaneously, with the true reasoning computation preserved in lower-ranked options
- Evidence anchors:
  - [abstract] "examine token rankings, we demonstrate that the hidden characters can be recovered without loss of performance"
  - [section] "the second-ranked token often corresponded to the numerical tokens representing the hidden reasoning steps"
  - [section] "the original, non-filler CoT tokens remained among the lower-ranked candidates"

### Mechanism 3
- Claim: The model uses filler tokens as a form of output formatting that doesn't erase the underlying computational path
- Mechanism: Similar to induction heads that learn to copy or overwrite tokens, the model learns to format output with fillers while preserving computational state in internal representations
- Core assumption: The model can maintain separate computational and formatting pathways, analogous to how induction heads copy tokens
- Evidence anchors:
  - [abstract] "models can still perform complex reasoning tasks even when the CoT is replaced with filler(hidden) characters"
  - [section] "The model appears to perform the reasoning in the earlier layers and then overwrites the intermediate representations with filler tokens in the later layers"
  - [section] "This overwriting behavior may involve mechanisms such as induction heads [4], where the model learns to copy or overwrite tokens based on patterns in the data"

## Foundational Learning

- Concept: Transformer layer-wise representation evolution
  - Why needed here: Understanding how information transforms across layers is crucial for interpreting the shift from numerical reasoning to filler token dominance
  - Quick check question: What changes occur in the top-ranked tokens when moving from layer 1 to layer 4 in a transformer model?

- Concept: Logit lens method and token ranking analysis
  - Why needed here: These techniques are essential for recovering hidden computations by examining internal representations and lower-ranked token candidates
  - Quick check question: How does the logit lens method allow us to inspect what tokens the model is "thinking about" at each layer?

- Concept: Modified decoding algorithms and autoregressive generation
  - Why needed here: The ability to recover hidden characters depends on understanding how to modify standard decoding to select from lower-ranked tokens
  - Quick check question: What modification to greedy decoding allows recovery of hidden reasoning steps when filler tokens are top-ranked?

## Architecture Onboarding

- Component map: Input layer -> Transformer layers (4 layers) -> Output layer -> Logit lens projection -> Token ranking system -> Modified decoding algorithm
- Critical path: Input encoding → Layer 1 computation → Layer 2 reasoning → Layer 3 filler emergence → Layer 4 filler dominance → Hidden state extraction → Logit lens projection → Token ranking analysis → Modified decoding selection
- Design tradeoffs:
  - Using filler tokens improves output formatting but obscures internal reasoning
  - Layer-wise computation allows separation of reasoning and formatting phases
  - Modified decoding trades standard output formatting for interpretability
  - Smaller model (34M parameters) enables controlled experimentation but may not generalize to larger models
- Failure signatures:
  - If layer-wise analysis shows filler tokens dominating from layer 1, indicating no computational separation
  - If token ranking shows no meaningful alternatives at rank-2 when filler is rank-1
  - If modified decoding fails to recover coherent reasoning steps
  - If performance drops significantly when recovering hidden characters
- First 3 experiments:
  1. Layer-wise logit lens analysis to identify when filler tokens emerge relative to reasoning tokens
  2. Token ranking analysis at each decoding step to verify rank-2 tokens contain hidden reasoning
  3. Modified decoding implementation and comparison against standard greedy decoding and random token replacement baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do similar hidden computation phenomena occur in larger transformer models (e.g., 70B+ parameters) and more complex reasoning tasks?
- Basis in paper: [inferred] The authors explicitly state this is a limitation, noting their study focuses on a small 34M parameter model on the synthetic 3SUM task
- Why unresolved: The paper's controlled experimental setup using a small model limits generalizability to real-world scenarios
- What evidence would resolve it: Replicating the analysis on larger models (GPT-4 class) using similar methods to decode hidden reasoning

### Open Question 2
- Question: What specific neural mechanisms (attention patterns, circuits, or layers) enable the model to overwrite explicit reasoning with filler tokens?
- Basis in paper: [explicit] The authors mention "overwriting behavior" and suggest "induction heads" but don't provide detailed analysis of the mechanisms
- Why unresolved: The paper identifies the phenomenon but doesn't investigate the underlying neural circuitry responsible
- What evidence would resolve it: Detailed attention pattern analysis and circuit identification showing how computations are performed then overwritten

### Open Question 3
- Question: Can the hidden reasoning be recovered without performance degradation on the original task?
- Basis in paper: [explicit] The authors state they can recover hidden characters "without loss of performance" but don't provide detailed performance metrics comparing original vs decoded outputs
- Why unresolved: The paper claims successful recovery but lacks quantitative evidence showing preserved task performance
- What evidence would resolve it: Empirical comparison of task accuracy before and after applying the modified decoding algorithm on held-out test sets

### Open Question 4
- Question: How does the corruption rate in training data affect the model's ability to perform and hide reasoning?
- Basis in paper: [explicit] The authors introduce corrupted instances in dataset generation but don't analyze their impact on hidden computation behavior
- Why unresolved: The paper includes corruption as a dataset parameter but doesn't explore how it influences the model's internal representations
- What evidence would resolve it: Systematic experiments varying corruption rates and measuring changes in hidden character recovery and task performance

## Limitations

- The analysis is conducted on a synthetic 3SUM task rather than real-world reasoning problems, limiting generalizability
- Findings are based on a single transformer architecture (34M parameter LLaMA variant with 4 layers), which may not hold for deeper models
- The study focuses on numerical tokens and filler tokens, without exploring more complex token types found in real reasoning chains

## Confidence

**High Confidence**: The logit lens method successfully reveals layer-wise evolution from numerical reasoning to filler token dominance; token ranking analysis reliably shows rank-2 tokens containing hidden reasoning when rank-1 tokens are fillers; modified decoding algorithm effectively recovers hidden characters without performance loss

**Medium Confidence**: The interpretation that models perform reasoning in early layers then format with fillers in later layers; the mechanism by which induction-like heads may facilitate token overwriting; the general applicability of layer-wise computation separation to other reasoning tasks

**Low Confidence**: Generalization of findings to larger, pre-trained models; applicability to non-synthetic reasoning tasks with complex dependencies; behavior with diverse token types beyond numerical and filler tokens

## Next Checks

1. Apply the same analysis pipeline (logit lens + token ranking + modified decoding) to a non-synthetic reasoning task such as mathematical word problems to validate whether layer-wise computation separation holds for linguistically complex reasoning

2. Test the same 3SUM task with modified transformer architectures including deeper models with 8+ layers, different attention head configurations, and alternative normalization schemes to determine if the observed layer-wise separation is an architectural invariant

3. Repeat the analysis with expanded token vocabularies that include linguistic constructs (operators, variables, function names) alongside numerical tokens to test whether rank-2 recovery works with heterogeneous token types representing real-world reasoning scenarios