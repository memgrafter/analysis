---
ver: rpa2
title: Personalized LLM for Generating Customized Responses to the Same Query from
  Different Users
arxiv_id: '2412.11736'
source_url: https://arxiv.org/abs/2412.11736
tags: []
core_contribution: This paper addresses the problem of generating personalized responses
  to the same query from different users in large language models (LLMs). The core
  method involves a dual-tower model architecture with a cross-questioner general
  encoder and a questioner-specific encoder, combined with contrastive learning and
  multi-view augmentation.
---

# Personalized LLM for Generating Customized Responses to the Same Query from Different Users

## Quick Facts
- arXiv ID: 2412.11736
- Source URL: https://arxiv.org/abs/2412.11736
- Authors: Hang Zeng; Chaoyue Niu; Fan Wu; Chengfei Lv; Guihai Chen
- Reference count: 27
- Primary result: 8.4% to 48.7% relative improvement in ROUGE-L scores and 54% to 82% winning rates compared to baselines

## Executive Summary
This paper addresses the challenge of generating personalized responses to the same query from different users in large language models. The authors propose a dual-tower architecture that separates cross-questioner general personality from questioner-specific personality through a combination of general and low-rank specific encoders. The approach employs contrastive learning within question similarity-based clusters, enhanced by multi-view augmentation, to differentiate responses for different questioners while maintaining response quality.

## Method Summary
The method uses a dual-tower model architecture where a cross-questioner general encoder (initialized from a pretrained LLM) captures shared responder characteristics, while a questioner-specific encoder with low-rank decomposition models unique questioner-responder relationships. Dialogues are clustered based on question similarity, and questioner-contrastive loss is applied within each cluster to differentiate responses for different questioners. Multi-view augmentation generates multiple representations for each dialogue through different projection matrices, strengthening the contrastive learning signal. The model is trained on the MQDialog dataset containing dialogues between 173 questioners and 12 responders.

## Key Results
- Achieved 8.4% to 48.7% relative improvement in ROUGE-L scores compared to baseline methods
- Won 54% to 82% of comparison evaluations against baselines
- Demonstrated significant improvements over LLaMA3-8B-Instruct, Qwen1.5-7B-Chat, and other personalization approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dual-tower architecture with low-rank specific encoder effectively disentangles cross-questioner general personality from questioner-specific personality.
- Mechanism: General encoder captures shared responder characteristics across all questioners, while low-rank specific encoder models unique relationships between each questioner and responder. The element-wise addition fuses these representations before prediction.
- Core assumption: Questioner-specific representations are intrinsically sparser than cross-questioner general representations, making low-rank matrices sufficient.
- Evidence anchors:
  - [abstract]: "dual-tower model architecture with a cross-questioner general encoder and a questioner-specific encoder characterized by a low-rank intrinsic property"
  - [section 4.2]: "The design insight is that questioner-specific representations are intrinsically sparser compared to cross-questioner general representations, making low-rank matrices sufficient"
- Break condition: If questioner-specific traits require high-dimensional expressiveness, the low-rank constraint would become a bottleneck.

### Mechanism 2
- Claim: Question similarity-based clustering mitigates the impact of query diversity on questioner-contrastive learning.
- Mechanism: By grouping dialogues with similar questions together, the model can focus contrastive learning within semantically coherent clusters rather than across disparate query types, preventing negative pairs from being semantically incomparable.
- Core assumption: Similar questions from different questioners are more appropriate for learning personality differentiation than dissimilar questions.
- Evidence anchors:
  - [abstract]: "To mitigate the impact of question diversity on questioner-contrastive learning, we cluster the dialogues based on question similarity and restrict the scope of contrastive learning within each cluster"
  - [section 4.1]: "we relax the strict requirement of the same question... allowing instead for similar questions"
- Break condition: If clustering fails to group semantically similar questions, contrastive learning would still suffer from semantic misalignment.

### Mechanism 3
- Claim: Multi-view augmentation enhances questioner-contrastive learning effectiveness and efficiency.
- Mechanism: Generating multiple views for each dialogue through different projection matrices creates stronger positive pairs and more diverse negative pairs, improving the contrastive learning signal without requiring additional data.
- Core assumption: Different projections of the same representation still preserve the core personality information while providing useful variation for contrastive learning.
- Evidence anchors:
  - [abstract]: "We further propose a multi-view augmentation strategy to enhance the effectiveness and efficiency of contrastive learning"
  - [section 4.4]: "We adopt the manual view augmentation technique... to generate multiple views for each dialogue by applying different projection matrices"
- Break condition: If projections introduce noise that obscures personality signals, the contrastive learning signal would degrade.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: To differentiate between representations of different questioners while aligning representations of the same questioner, enabling personalized responses.
  - Quick check question: What is the difference between positive and negative pairs in contrastive learning, and how does this apply to questioner-aware personalization?

- Concept: Low-rank decomposition
  - Why needed here: To make the specific encoder parameter-efficient while capturing the inherently sparse nature of questioner-specific personality traits.
  - Quick check question: Why would low-rank matrices be sufficient for modeling questioner-specific representations but not cross-questioner general representations?

- Concept: Clustering for semantic coherence
  - Why needed here: To group dialogues with similar questions together so contrastive learning focuses on personality differentiation rather than semantic variation.
  - Quick check question: How does clustering dialogues by question similarity help prevent contrastive learning from being confounded by query diversity?

## Architecture Onboarding

- Component map:
  General encoder (transformer blocks) -> Specific encoder (transformer blocks with low-rank decomposition) -> Element-wise addition -> Language modeling head -> Response generation

- Critical path: Dialogue → General encoder → Specific encoder → Element-wise addition → Language model head → Response generation

- Design tradeoffs:
  - Low-rank constraint vs. expressiveness in specific encoder
  - Number of clusters vs. semantic coherence vs. data sparsity
  - Number of views in augmentation vs. computational cost vs. contrastive signal strength

- Failure signatures:
  - Similar responses for same query from different questioners → Check specific encoder training and contrastive loss
  - Poor quality responses overall → Check general encoder initialization and fusion mechanism
  - Training instability → Check projection matrices and temperature hyperparameter

- First 3 experiments:
  1. Ablation: Remove low-rank constraint in specific encoder to test expressiveness vs. efficiency tradeoff
  2. Ablation: Remove clustering to test impact of semantic coherence on contrastive learning
  3. Ablation: Remove multi-view augmentation to test its contribution to contrastive learning effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can non-dialogue content (scene descriptions, character actions) be effectively incorporated into the questioner-aware LLM personalization model?
- Basis in paper: Explicit - The paper mentions this as a limitation, stating "non-dialogue contents from the original scripts, such as scene descriptions and character actions, have been filtered out. Incorporating additional contextual information is an interesting direction for future research."
- Why unresolved: The current model architecture and training approach are designed specifically for dialogue data. Scene descriptions and character actions represent a different type of information that would require new methods for integration into the model.
- What evidence would resolve it: Experiments showing improved performance when scene descriptions and character actions are incorporated into the model, along with a description of the method used to integrate this additional contextual information.

### Open Question 2
- Question: What is the optimal number of clusters for the question similarity-based dialogue clustering strategy?
- Basis in paper: Inferred - The paper describes using k-means clustering with k-means clustering to group similar dialogues but doesn't explore how the choice of k affects performance.
- Why unresolved: The paper doesn't provide an analysis of how different numbers of clusters impact the model's performance or the quality of the generated responses. The choice of k could significantly affect the effectiveness of the contrastive learning approach.
- What evidence would resolve it: Results from experiments testing different values of k, showing how the number of clusters affects BLEU, ROUGE scores, and winning rates in comparison to baselines.

### Open Question 3
- Question: How does the low-rank design of the specific encoder affect the model's ability to capture long-term dependencies in conversations?
- Basis in paper: Explicit - The paper states "The second 'tower' is the specific encoder with multiple adapted transformer blocks, denoted as Gs. A key adaptation lies in the dense feed-forward layer after multi-head attention, which is decomposed into two low-rank matrices."
- Why unresolved: While the paper explains the rationale for using low-rank matrices (parameter efficiency and sparsity of questioner-specific representations), it doesn't investigate whether this design choice impacts the model's ability to maintain context over longer conversations.
- What evidence would resolve it: Comparative analysis showing how the low-rank design performs on tasks requiring long-term context understanding versus a full-rank specific encoder, along with any trade-offs in performance or efficiency.

## Limitations
- The approach's effectiveness depends heavily on the quality of question similarity metrics used for clustering, which aren't fully specified in the paper
- The low-rank assumption for questioner-specific representations, while theoretically motivated, may not hold across all personality dimensions or conversational contexts
- The MQDialog dataset represents interactions with a limited set of 12 responders, potentially limiting generalizability to scenarios with larger responder pools

## Confidence
**High Confidence**: The dual-tower architecture design and the overall methodology for combining general and specific encoders are well-grounded. The empirical results showing consistent improvements across multiple metrics (ROUGE-L scores, winning rates) provide strong evidence for the approach's effectiveness.

**Medium Confidence**: The theoretical justification for low-rank decomposition in the specific encoder is plausible but not extensively validated. The clustering approach's impact on contrastive learning quality is demonstrated empirically but the robustness across different clustering strategies remains untested.

**Low Confidence**: The scalability of the approach to larger numbers of questioners/responders and different types of personalization tasks beyond dialogue response generation has not been established.

## Next Checks
1. **Ablation on Low-Rank Constraint**: Systematically test different ranks for the specific encoder to determine the optimal tradeoff between parameter efficiency and expressiveness, and validate whether the low-rank assumption holds across different personality dimensions.

2. **Clustering Robustness Analysis**: Evaluate the approach's performance using different clustering algorithms (k-means, hierarchical clustering, semantic similarity-based) and varying numbers of clusters to assess the sensitivity to clustering quality and parameter choices.

3. **Cross-Dataset Generalization**: Test the trained model on a different personalized dialogue dataset with different responder pools and conversational styles to evaluate generalizability beyond the MQDialog dataset, particularly for scenarios with more than 12 responders or different personality archetypes.