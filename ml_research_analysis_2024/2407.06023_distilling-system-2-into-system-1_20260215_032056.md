---
ver: rpa2
title: Distilling System 2 into System 1
arxiv_id: '2407.06023'
source_url: https://arxiv.org/abs/2407.06023
tags:
- system
- response
- question
- assistant
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work explores whether the benefits of System 2 reasoning\
  \ in large language models can be distilled into standard, faster System 1 inference\
  \ without intermediate reasoning steps. Using four System 2 methods\u2014Rephrase\
  \ and Respond, System 2 Attention, Branch-Solve-Merge, and Chain-of-Thought\u2014\
  the authors generate high-quality outputs on unlabeled data and then fine-tune a\
  \ System 1 model to replicate these outputs without generating intermediate tokens."
---

# Distilling System 2 into System 1

## Quick Facts
- arXiv ID: 2407.06023
- Source URL: https://arxiv.org/abs/2407.06023
- Authors: Ping Yu; Jing Xu; Jason Weston; Ilia Kulikov
- Reference count: 28
- Key outcome: System 2 reasoning methods (RaR, S2A, BSM, CoT) can be distilled into System 1 inference for bias reduction, instruction clarification, and evaluation judgment tasks, yielding improved performance over System 1 baselines at reduced computational cost, but complex mathematical reasoning tasks remain challenging to distill.

## Executive Summary
This work explores whether the benefits of System 2 reasoning in large language models can be distilled into standard, faster System 1 inference without intermediate reasoning steps. Using four System 2 methods—Rephrase and Respond, System 2 Attention, Branch-Solve-Merge, and Chain-of-Thought—the authors generate high-quality outputs on unlabeled data and then fine-tune a System 1 model to replicate these outputs without generating intermediate tokens. Distillation succeeds for tasks involving bias reduction, instruction clarification, and evaluation judgment, yielding improved performance over System 1 baselines at a fraction of the computational cost. However, complex mathematical reasoning tasks requiring Chain-of-Thought were not successfully distilled, suggesting limits to this approach.

## Method Summary
The authors propose a distillation framework where System 2 methods generate high-quality outputs on unlabeled data, which are then used to fine-tune a System 1 model. The process involves generating System 2 outputs using four methods (Rephrase and Respond, System 2 Attention, Branch-Solve-Merge, Chain-of-Thought), applying self-consistency filtering to identify high-quality examples, and fine-tuning the System 1 model on the filtered dataset using standard supervised learning with cross-entropy loss. The goal is to compress System 2 reasoning patterns into the System 1 model's parameters, enabling faster inference while maintaining System 2-level performance for suitable tasks.

## Key Results
- System 2 distillation successfully improves performance on bias reduction, instruction clarification, and evaluation judgment tasks
- Distilled models achieve System 2-level accuracy with System 1 inference cost (reduced token generation)
- Chain-of-Thought distillation fails for complex mathematical reasoning tasks (GSM8k)
- Self-consistency filtering is crucial for distillation quality, with multiple sampling improving results
- The approach works for pattern-matching and judgment tasks but not for complex symbolic reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The distilled model can learn procedural patterns from System 2 outputs without needing intermediate reasoning tokens.
- Mechanism: The model learns to associate input patterns directly with high-quality System 2 outputs during fine-tuning, bypassing the need to generate intermediate reasoning steps.
- Core assumption: System 2 methods produce outputs that are sufficiently consistent and high-quality that they can serve as reliable targets for fine-tuning.
- Evidence anchors:
  - [abstract]: "Distilling System 2 into System 1" - The paper explicitly frames this as compiling System 2 reasoning into System 1 outputs
  - [section 3.2]: "The aim of System 2 Distillation is to distill all the reasoning from SII back into SI so that the direct outputs from the language model pθ(x) are improved"
  - [corpus]: Weak evidence - The corpus shows related work on CoT distillation but doesn't directly support this specific mechanism
- Break condition: If System 2 outputs are inconsistent or low-quality, the distilled model will learn incorrect patterns

### Mechanism 2
- Claim: Self-consistency filtering improves the quality of distillation targets by removing noisy or incorrect System 2 outputs.
- Mechanism: Multiple samples are generated and only examples where outputs agree are kept, ensuring the distilled model learns from reliable targets.
- Core assumption: Consistent outputs across multiple samples indicate higher quality and correctness.
- Evidence anchors:
  - [section 4.2.1]: "we conduct eight sampling iterations for the last letter task and eight for each stage of the coin flip task. We then apply a majority vote to determine the final output"
  - [section 4.3]: "we sample 20 generations and then use the Llama-70B-chat model with a USC prompt to compose a self-consistent (majority) final answer that is used as the distillation target"
  - [corpus]: Weak evidence - The corpus mentions self-consistency but doesn't provide specific evidence for its effectiveness in distillation
- Break condition: If the task inherently has multiple valid answers or if the model's sampling is biased, consistency filtering may remove valid examples

### Mechanism 3
- Claim: The computational efficiency gains come from eliminating intermediate token generation while maintaining System 2-level performance.
- Mechanism: By fine-tuning on System 2 outputs, the model learns to produce similar quality responses in a single forward pass, reducing inference time.
- Core assumption: The model can compress the System 2 reasoning process into its parameters without explicitly generating intermediate steps.
- Evidence anchors:
  - [abstract]: "resulting in improved results compared to the original System 1 performance, and with less inference cost than System 2"
  - [section 4.1]: "#Tokens metric which measures the average number of tokens generated per input across the evaluation set"
  - [corpus]: Moderate evidence - The corpus shows related work on efficient reasoning but doesn't directly address this specific efficiency claim
- Break condition: If the System 2 reasoning process involves complex symbolic manipulation that cannot be compressed into distributed representations

## Foundational Learning

- Concept: Self-consistency and majority voting
  - Why needed here: Used to filter high-quality training examples by identifying consistent outputs across multiple samples
  - Quick check question: What happens if we use self-consistency on a task where there are multiple equally valid answers?

- Concept: Knowledge distillation
  - Why needed here: The core technique for transferring knowledge from System 2 to System 1, where the student model learns to mimic the teacher's outputs
  - Quick check question: How is this different from standard teacher-student distillation where the models have different architectures?

- Concept: Chain-of-thought reasoning
  - Why needed here: Understanding why CoT is harder to distill helps explain the limitations of the approach and guides future research
  - Quick check question: Why might symbolic reasoning tasks be more difficult to distill than tasks involving bias reduction or evaluation?

## Architecture Onboarding

- Component map:
  System 2 methods (RaR, S2A, BSM, CoT) -> Consistency filtering pipeline -> Fine-tuning loop for distillation -> Evaluation framework for System 1 vs System 2 performance

- Critical path:
  1. Generate System 2 outputs on unlabeled data
  2. Apply consistency filtering to create distillation dataset
  3. Fine-tune System 1 model on filtered dataset
  4. Evaluate distilled model against baselines

- Design tradeoffs:
  - Filtering quality vs. dataset size (stricter filtering produces smaller but higher-quality datasets)
  - Inference speed vs. reasoning capability (System 1 is faster but may sacrifice some reasoning depth)
  - Task complexity vs. distillability (simple pattern-matching tasks distill better than complex symbolic reasoning)

- Failure signatures:
  - Distilled model performs worse than zero-shot baseline
  - No improvement in consistency metrics after distillation
  - Large gap between System 2 and distilled System 1 performance
  - Training loss plateaus early, indicating insufficient learning signal

- First 3 experiments:
  1. Test distillation on a simple pattern-matching task (like last letter concatenation) with and without consistency filtering
  2. Compare System 1, System 2, and distilled model performance on a bias-reduction task
  3. Attempt CoT distillation on a simple math problem to understand limitations before scaling to complex reasoning tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can System 2 distillation be successfully applied to Chain-of-Thought reasoning for complex mathematical problems?
- Basis in paper: Inferred from the conclusion that "Chain-of-Thought for complex reasoning being a challenging counterexample" and "the GSM8k task (math problems) requires a very different kind of reasoning compared to other tasks we considered"
- Why unresolved: The paper explicitly states that their distillation method did not work well for CoT on the GSM8k dataset, with "poor performance across various decoding hyper-parameters" and that this highlights "the non-trivial aspect of System 2 distillation: the proposed distillation algorithm works in many cases but not always"
- What evidence would resolve it: Demonstrating successful distillation of CoT reasoning on complex mathematical tasks using alternative distillation techniques, or providing theoretical analysis explaining why CoT is fundamentally different from other System 2 methods in terms of distillability

### Open Question 2
- Question: What is the optimal balance between data quality and quantity when applying self-consistency filtering for System 2 distillation?
- Basis in paper: Explicit - "Due to the self-supervised nature of these methods, model performance relies on the specific filters applied" and the observation that "using USC for distillation is important for overall results" but also that "we observed that sampling just once for the rephrase stage yielded suboptimal results"
- Why unresolved: The paper only explores one filtering approach (self-consistency) with fixed parameters, and mentions that "there are multiple alternative strategies to enhance data quality in self-supervised learning, these were not explored in our research"
- What evidence would resolve it: Systematic experiments varying the number of samples for self-consistency, comparing different filtering criteria, and measuring the trade-off between filter strictness and final model performance across multiple tasks

### Open Question 3
- Question: How does System 2 distillation performance generalize to other System 2 methods beyond the four tested (RaR, S2A, BSM, CoT)?
- Basis in paper: Inferred - The paper states "We consider two variants of the approach: (i) few-shot CoT, whereby multiple [question, CoT, answer] examples from the training set are provided" but only tests four methods, and mentions "many other system 2 approaches" in related work
- Why unresolved: The experiments are limited to four specific System 2 methods, and the paper only states that "our method works in many cases but not always" without characterizing which types of System 2 methods are more amenable to distillation
- What evidence would resolve it: Applying the System 2 distillation framework to a diverse set of System 2 methods (e.g., Tree-of-Thoughts, Graph-of-Thoughts, self-verification methods) and measuring distillation success rates, or developing a taxonomy of System 2 methods based on their distillability characteristics

## Limitations

- The approach fails for complex mathematical reasoning tasks requiring Chain-of-Thought, suggesting fundamental limits to distillation for symbolic manipulation
- Self-consistency filtering parameters are not systematically explored, potentially leaving performance on the table
- Results are limited to specific tasks without comprehensive ablation studies across reasoning types

## Confidence

**High Confidence**: The core claim that System 2 methods can be distilled into System 1 inference for tasks involving pattern matching, bias correction, and judgment tasks. The experimental results clearly show improved performance over System 1 baselines with reduced inference cost for the tasks where distillation succeeded.

**Medium Confidence**: The claim that computational efficiency gains come from eliminating intermediate token generation while maintaining System 2-level performance. While the paper shows reduced token counts, the actual inference time measurements are not provided, and the relationship between token count and wall-clock time depends on implementation details.

**Low Confidence**: The claim that this approach represents a general solution for compressing System 2 reasoning into System 1. The failure on mathematical reasoning tasks and lack of systematic exploration across reasoning types suggest the approach may be limited to specific categories of tasks rather than representing a universal distillation technique.

## Next Checks

1. **Systematic Task Classification Study**: Test the distillation approach across a broader taxonomy of reasoning tasks (pattern matching, commonsense reasoning, symbolic manipulation, mathematical reasoning, causal reasoning) with systematic ablation of self-consistency parameters to identify which task characteristics predict distillability versus which require explicit reasoning steps.

2. **Transfer and Generalization Analysis**: Evaluate whether distilled models retain the ability to generalize to unseen examples within their task domain, and test cross-task transfer where a model distilled on bias reduction can be fine-tuned on evaluation judgment tasks without catastrophic forgetting.

3. **Efficiency Validation Under Load**: Measure actual wall-clock inference times for System 1, System 2, and distilled models under realistic deployment conditions with batching and concurrent requests to verify that the theoretical token reduction translates to meaningful computational savings in practice.