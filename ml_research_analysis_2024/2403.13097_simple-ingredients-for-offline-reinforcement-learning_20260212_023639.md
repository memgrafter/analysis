---
ver: rpa2
title: Simple Ingredients for Offline Reinforcement Learning
arxiv_id: '2403.13097'
source_url: https://arxiv.org/abs/2403.13097
tags:
- simple
- modern
- data
- task
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the failure of offline reinforcement learning
  algorithms when trained on diverse datasets. The authors introduce MOOD, a new benchmark
  based on the DeepMind Control Suite, where trajectories come from heterogeneous
  sources such as agents trained for different tasks or with exploration-focused objectives.
---

# Simple Ingredients for Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2403.13097
- Source URL: https://arxiv.org/abs/2403.13097
- Reference count: 40
- Key outcome: Large network architectures significantly improve offline RL performance on diverse datasets, outperforming complex algorithmic approaches

## Executive Summary
This paper investigates why offline reinforcement learning algorithms fail when trained on diverse datasets with heterogeneous trajectories. The authors introduce MOOD, a new benchmark based on the DeepMind Control Suite, where trajectories come from agents trained for different tasks or with exploration-focused objectives. Through extensive empirical analysis, they demonstrate that simple methods like AWAC and IQL with increased network sizes outperform prior state-of-the-art algorithms, suggesting that scale is more important than algorithmic complexity for offline RL. The study challenges the prevailing focus on sophisticated algorithms by showing that larger networks can bridge the performance gap with same-objective datasets.

## Method Summary
The authors develop MOOD benchmark by collecting data from agents trained on different tasks in DeepMind Control Suite environments, then merging and relabeling this data for target tasks. They implement and test TD3+BC, AWAC, and IQL algorithms with modifications including larger network architectures (3 hidden layers of 256 units for critics, 5 hidden layers of 1024 units for actors), ensemble of critics (5 critics), evaluation sampling (M=50 samples), and advantage sampling. The evaluation uses normalized cumulative return as the metric, comparing performance across same-objective and mixed-objective datasets. The study systematically varies network sizes and algorithmic components to identify key factors affecting performance.

## Key Results
- Scale is the dominant factor: Larger network architectures consistently improve performance on both MOOD and D4RL benchmarks
- Simple algorithms win: AWAC and IQL with increased network sizes outperform complex prior state-of-the-art algorithms
- Evaluation sampling helps selectively: ES improves performance on some tasks but hurts it on others
- Ensemble critics provide marginal gains: 5 critics show slight improvements over 2 critics but at higher computational cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scale (network size) is the dominant factor for improving performance on diverse offline RL datasets
- Mechanism: Larger networks provide sufficient representational capacity to model complex state-action distributions arising from heterogeneous data sources, preventing underfitting that leads to performance collapse
- Core assumption: The increased data diversity from mixed-objective datasets requires more parameters to accurately represent the value functions and policies
- Evidence anchors:
  - [abstract] "scale, more than algorithmic considerations, is the key factor influencing performance"
  - [section 5] "we consistently observe that all algorithms bridge the performance gap with same-objective datasets simply by using a larger architecture"
  - [corpus] Weak - no direct evidence, only related papers mention scaling
- Break condition: When the increased model capacity leads to overfitting on the specific offline dataset, or when the computational cost becomes prohibitive relative to performance gains

### Mechanism 2
- Claim: Evaluation sampling (ES) mitigates over-conservatism by selecting high-value actions at test time
- Mechanism: By sampling multiple actions from the learned policy and choosing the one with the highest Q-value, ES circumvents the conservative bias introduced during training, allowing better exploitation of the data support
- Core assumption: The learned policy distribution is sufficiently within the data support that Q-value extrapolation is reliable for action selection
- Evidence anchors:
  - [abstract] "Evaluation sampling (ES)... performing a non-parametric step of unconstrained policy improvement"
  - [section 4] "If over-conservatism is really an issue, we propose to address it entirely at test time by sampling M actions from the learned policy and selecting the one with the highest Q-value"
  - [corpus] Weak - no direct evidence, only related papers mention sampling
- Break condition: When the policy distribution extends beyond the data support, making Q-value extrapolation unreliable and leading to poor action selection

### Mechanism 3
- Claim: Advantage sampling (ASAC) reduces bias and variance in the policy improvement objective compared to weighted importance sampling
- Mechanism: By directly sampling from the target distribution B⋆(s, a) ∝ B(s, a) exp(Aϕ(s, a)/β), ASAC avoids the need for normalized weights, reducing both estimator bias and variance
- Core assumption: The advantage-weighted distribution can be efficiently sampled using a logsumexp-tree data structure
- Evidence anchors:
  - [abstract] "we can directly and tractably sample from the desired target distribution by avoiding altogether the need for weights"
  - [section 4] "This estimator, which aims at directly projecting πθ onto π⋆B, is unbiased for the AWAC objective"
  - [corpus] Weak - no direct evidence, only related papers mention sampling
- Break condition: When the advantage function is noisy or unstable, making the sampling distribution unreliable and degrading performance

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The paper is built on RL algorithms that operate within the MDP framework, requiring understanding of states, actions, transitions, rewards, and policies
  - Quick check question: What is the Bellman equation for the action-value function Qπ(s,a) in an MDP?

- Concept: Temporal Difference (TD) Learning
  - Why needed here: The algorithms in the paper use TD learning to update Q-function estimates based on bootstrapped targets
  - Quick check question: How does the TD target y = r + γQ(s',a') differ from the Monte Carlo return?

- Concept: Policy Gradient Methods
  - Why needed here: The actor-critic algorithms optimize policies using gradient-based methods, requiring understanding of policy gradients and advantage functions
  - Quick check question: What is the policy gradient theorem, and how does it relate to the REINFORCE algorithm?

## Architecture Onboarding

- Component map:
  - Actor network: Maps states to action distributions
  - Critic networks: Ensemble of Q-functions estimating action values
  - Value network (IQL only): Learns state values via expectile regression
  - Logsumexp-tree: Efficient data structure for sampling from advantage-weighted distributions

- Critical path:
  1. Initialize actor and critic networks
  2. Sample batch from offline dataset
  3. Update critics via TD learning
  4. Update actor to maximize expected Q-values (with regularization)
  5. Repeat steps 2-4 for specified training steps

- Design tradeoffs:
  - Network size vs. computational cost: Larger networks improve performance but increase training time and memory usage
  - Number of critics vs. estimation accuracy: More critics reduce overestimation bias but increase computation
  - ES sample size vs. exploration-exploitation balance: Larger sample sizes improve action selection but increase inference time

- Failure signatures:
  - Performance collapse on mixed-objective datasets: Indicates insufficient model capacity or over-conservatism
  - High variance in Q-value estimates: Suggests need for more critics or better regularization
  - Slow learning or convergence to suboptimal policies: May indicate issues with advantage weighting or initialization

- First 3 experiments:
  1. Train TD3+BC on same-objective dataset with small architecture to establish baseline
  2. Train TD3+BC on mixed-objective dataset with small architecture to observe performance drop
  3. Train TD3+BC on mixed-objective dataset with large architecture to verify scaling effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of network architecture (simple vs. modern) interact with the data diversity in offline RL?
- Basis in paper: [explicit] The authors compare large simple MLPs and modern architectures (Bjorck et al., 2021) on both MOOD and D4RL datasets, finding that the modern architecture does not provide advantages over the simple one
- Why unresolved: The paper only tests two specific architectures (large simple and modern) without exploring a wider range of architectural choices or investigating why the modern architecture does not provide benefits in this context
- What evidence would resolve it: Testing a broader range of architectures (e.g., transformers, graph neural networks) and conducting ablation studies to identify the specific components of the modern architecture that are beneficial or detrimental

### Open Question 2
- Question: What is the optimal ensemble size for Q-functions in offline RL with diverse data?
- Basis in paper: [explicit] The authors test ensembles of 2 and 5 critics, observing marginal improvements with larger ensembles but noting the added computational cost
- Why unresolved: The paper does not explore ensembles beyond 5 critics or systematically investigate the trade-off between ensemble size, performance, and computational cost
- What evidence would resolve it: Conducting a thorough analysis of ensemble sizes (e.g., 2, 5, 10, 20) on both MOOD and D4RL datasets to determine the point of diminishing returns and the optimal balance between performance and efficiency

### Open Question 3
- Question: How does the evaluation sampling (ES) strategy affect the learning dynamics and final performance in offline RL?
- Basis in paper: [explicit] The authors observe that ES improves performance on some tasks (e.g., quadruped in MOOD) but hurts it on others (e.g., humanoid in MOOD and locomotion tasks in D4RL)
- Why unresolved: The paper does not investigate the reasons behind the varying effectiveness of ES across different tasks and datasets or explore alternative sampling strategies
- What evidence would resolve it: Analyzing the data distributions and task characteristics that make ES beneficial or detrimental, and developing more sophisticated sampling strategies that adapt to the specific characteristics of the data and task

### Open Question 4
- Question: What are the limitations of using advantage weighting in offline RL with diverse data?
- Basis in paper: [explicit] The authors hypothesize that the bias and variance of the advantage weighting estimator increase with data complexity, leading to performance degradation
- Why unresolved: The paper does not provide a rigorous analysis of the bias and variance of the advantage weighting estimator or explore alternative weighting schemes
- What evidence would resolve it: Deriving theoretical bounds on the bias and variance of the advantage weighting estimator for different data distributions and comparing its performance to alternative weighting schemes (e.g., based on returns or state densities)

## Limitations
- Focus on continuous control tasks in DeepMind Control Suite limits generalizability to other domains
- All experiments use the same data collection methodology, which may not represent full diversity of potential offline datasets
- Computational cost of larger networks is not thoroughly analyzed, raising questions about practical deployment

## Confidence
- High confidence in the observation that scale matters: Multiple experiments consistently show performance improvements with larger networks
- Medium confidence in the mechanism: While correlation is strong, the paper doesn't definitively prove that increased representational capacity is the causal mechanism
- Low confidence in the proposed solutions' generality: Evaluation sampling and advantage sampling may not transfer well to other offline RL settings

## Next Checks
1. Test the scaling hypothesis on more diverse offline datasets, including image-based environments and real-world robot datasets
2. Conduct ablation studies to isolate the effects of network depth vs. width vs. training procedure
3. Analyze the computational trade-offs by measuring training time and inference latency for different network sizes