---
ver: rpa2
title: 'Target word activity detector: An approach to obtain ASR word boundaries without
  lexicon'
arxiv_id: '2409.13913'
source_url: https://arxiv.org/abs/2409.13913
tags:
- word
- speech
- baseline
- information
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of obtaining word-level timestamp
  information from end-to-end ASR models, particularly for multilingual models where
  scalability and lexicon dependency are major challenges. The proposed solution is
  a "Target Word Activity Detector" (TWAD) that leverages word embeddings derived
  from sub-word tokens and a pretrained ASR encoder to predict word activity probabilities
  at each frame, enabling lexicon-free word boundary estimation.
---

# Target word activity detector: An approach to obtain ASR word boundaries without lexicon

## Quick Facts
- arXiv ID: 2409.13913
- Source URL: https://arxiv.org/abs/2409.13913
- Authors: Sunit Sivasankaran; Eric Sun; Jinyu Li; Yan Huang; Jing Pan
- Reference count: 32
- Primary result: TWAD achieves lexicon-free word boundary estimation with comparable accuracy to lexicon-based baselines in multilingual ASR

## Executive Summary
This paper addresses the challenge of obtaining word-level timestamp information from end-to-end ASR models, particularly for multilingual scenarios where traditional lexicon-based approaches face scalability issues. The authors propose a Target Word Activity Detector (TWAD) that leverages word embeddings derived from sub-word tokens and a pretrained ASR encoder to predict word activity probabilities at each frame. This enables lexicon-free word boundary estimation while maintaining accuracy comparable to strong baseline methods that rely on lexicon-based phone alignment.

## Method Summary
The proposed TWAD approach uses a two-stage pipeline: first, it extracts word embeddings from sub-word token sequences generated by an ASR model, and second, it applies these embeddings through a pretrained ASR encoder to predict word activity probabilities for each frame. The method was validated on a multilingual ASR model trained on five languages (English, French, Spanish, Italian, German), demonstrating that TWAD can achieve word timing accuracy comparable to lexicon-based baselines without requiring language-specific pronunciation lexicons.

## Key Results
- TWAD achieved comparable word timing accuracy to lexicon-based phone alignment baselines
- Average start/end deltas were approximately 45-60 ms across all five tested languages
- p95 values were in acceptable ranges for downstream tasks like speaker diarization
- The method demonstrated superior scalability by eliminating lexicon dependency while maintaining multilingual performance

## Why This Works (Mechanism)
The approach works by leveraging the inherent linguistic information captured in sub-word token embeddings and combining it with the contextual understanding from a pretrained ASR encoder. By predicting word activity probabilities at the frame level, TWAD can identify word boundaries without requiring explicit phonetic transcription knowledge. This circumvents the scalability limitations of lexicon-based approaches while maintaining accuracy through the powerful representation learning capabilities of modern ASR encoders.

## Foundational Learning

**Sub-word tokenization** - Why needed: Enables handling of morphologically rich languages and out-of-vocabulary words without exploding vocabulary size
Quick check: Can the model segment words into sub-word units that preserve meaningful linguistic units?

**Word embeddings from sub-words** - Why needed: Captures semantic and syntactic relationships between words and their constituent sub-word components
Quick check: Do similar words produce similar embedding representations?

**Frame-level probability prediction** - Why needed: Provides fine-grained temporal resolution necessary for accurate word boundary detection
Quick check: Can the system distinguish between frames where a word is active versus inactive?

**Multilingual ASR encoder pretraining** - Why needed: Provides robust language-agnostic representations that transfer across multiple language families
Quick check: Does the encoder maintain consistent performance across diverse linguistic structures?

## Architecture Onboarding

Component map: ASR model -> Sub-word tokenization -> Word embedding extraction -> ASR encoder -> TWAD probability predictor

Critical path: The pipeline processes audio through the ASR model to obtain sub-word tokens, converts these to word embeddings, passes them through the pretrained ASR encoder for contextual representation, and finally predicts word activity probabilities at each frame. The critical path is the sequence from word embedding extraction through the ASR encoder to the probability predictor.

Design tradeoffs: The method trades potential precision from explicit phonetic knowledge (lexicon-based) for scalability and language-agnostic operation. This represents a shift from knowledge-based to representation-based approaches to word boundary detection.

Failure signatures: Performance degradation would likely manifest as increased timing errors when the ASR model produces incorrect sub-word sequences, when word embeddings fail to capture sufficient linguistic information, or when the ASR encoder cannot provide adequate contextual representations for boundary detection.

First experiments:
1. Verify that sub-word tokenization produces consistent and linguistically meaningful units across all five test languages
2. Test word embedding quality by clustering similar words and measuring semantic coherence
3. Validate that TWAD probability predictions align with ground truth word boundaries on a held-out validation set

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond those addressed in the study.

## Limitations
- Dependency on a pretrained ASR encoder that may not generalize well to languages or domains outside the training set
- Potential impact of ASR error rates on TWAD accuracy, as the approach assumes correct word predictions from the ASR model
- Performance on languages with significantly different phonotactic structures or those with limited training data remains unverified

## Confidence

High confidence:
- TWAD's ability to achieve comparable word timing accuracy to lexicon-based baselines in multilingual settings
- Elimination of lexicon dependency for improved scalability

Medium confidence:
- Generalizability of TWAD to languages outside the five tested (EN, FR, ES, IT, DE)
- Robustness in handling spontaneous or disfluent speech

Low confidence:
- Impact of ASR error rates on TWAD performance
- Computational efficiency compared to traditional approaches

## Next Checks

1. Test TWAD on low-resource languages and languages with non-Latin scripts to evaluate its cross-linguistic applicability and robustness
2. Quantify the computational overhead of TWAD in terms of inference time and memory usage compared to lexicon-based methods, particularly in resource-constrained scenarios
3. Evaluate TWAD's performance on spontaneous speech datasets with varying speaking rates and disfluencies to assess its robustness in real-world applications