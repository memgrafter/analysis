---
ver: rpa2
title: Presto! Distilling Steps and Layers for Accelerating Music Generation
arxiv_id: '2410.05167'
source_url: https://arxiv.org/abs/2410.05167
tags:
- distillation
- score
- noise
- step
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of accelerating inference for
  score-based diffusion transformers in music generation, which are slow due to iterative
  denoising. The authors introduce Presto!, a dual-faceted distillation approach that
  reduces both the number of sampling steps and the cost per step.
---

# Presto! Distilling Steps and Layers for Accelerating Music Generation

## Quick Facts
- **arXiv ID**: 2410.05167
- **Source URL**: https://arxiv.org/abs/2410.05167
- **Reference count**: 40
- **Key outcome**: Introduces Presto!, a dual-faceted distillation approach that accelerates music generation by 10-18x while improving diversity through combined step and layer distillation

## Executive Summary
This paper addresses the fundamental challenge of slow inference in score-based diffusion transformers for music generation. The authors introduce Presto!, a dual-faceted distillation approach that reduces both the number of sampling steps and the cost per step. The method includes Presto-S for distribution matching distillation, Presto-L for layer distillation with variance preservation, and Presto-LS for combined layer-step distillation. The authors evaluate their methods independently and in combination, showing that Presto-LS accelerates the base model by 10-18x while improving diversity over step-only distillation, making it the fastest high-quality text-to-music model to date.

## Method Summary
The authors develop Presto! as a comprehensive solution to accelerate score-based diffusion transformers for music generation. Presto-S introduces a new distribution matching distillation method specifically designed for continuous-time models, training a generator to minimize reverse KL divergence using an online fake score model. Presto-L improves layer distillation by preserving hidden state variance through shifted layer dropping schedules, adding budget conditioning, and using self-teacher loss. Presto-LS combines both approaches in a staged manner, first performing layer distillation then step distillation while maintaining stability through proper initialization and reduced dropping budgets. The method is evaluated on a 3.6K hour dataset of mono 44.1kHz licensed instrumental music.

## Key Results
- Presto-LS achieves 10-18x acceleration (230/435ms latency for 32 second mono/stereo 44.1kHz)
- Presto-LS is 15x faster than the comparable state-of-the-art MusicLM model
- The combined approach improves diversity metrics over step-only distillation methods
- Maintains high audio quality with competitive Frechet Audio Distance scores

## Why This Works (Mechanism)

### Mechanism 1: Online Distribution Matching for Continuous-Time Models
Presto-S accelerates music generation by distilling continuous-time score-based diffusion models into a few-step generator using online distribution matching distillation. The method trains a generator Gϕ to minimize reverse KL divergence between the real distribution and the generator's distribution. It simultaneously trains a fake score model µψ to estimate the generator's distribution score online, enabling the calculation of the distribution matching gradient without requiring expensive offline data generation. The core assumption is that the fake score model can accurately estimate the generator's distribution score at each gradient step, enabling stable online distribution matching distillation.

### Mechanism 2: Variance-Preserving Layer Distillation
Presto-L improves inference speed by dropping interior layers while preserving hidden state variance and adding budget conditioning. The method shifts layer dropping to start from the second-to-last layer (rather than the last) to preserve the final layer's unique behavior, adds explicit budget conditioning to help the model adapt computation to different noise levels, and uses self-teacher loss to align layer-dropped outputs with full model outputs. The core assumption is that the final layer has unique variance characteristics that should be preserved, and budget conditioning helps the model adapt to different computational budgets.

### Mechanism 3: Staged Combined Layer-Step Distillation
Presto-LS achieves 10-18x acceleration by combining layer and step distillation in a staged approach that maintains stability. The method first performs layer distillation to reduce cost per step, then performs step distillation on the layer-distilled model. It keeps the real and fake score models initialized from the original score model rather than the layer-distilled model to provide regularization and stability, and uses a reduced dropping budget during layer distillation. The core assumption is that layer and step distillation can be combined without interfering with each other if done in a staged approach with proper initialization.

## Foundational Learning

- **Continuous-time score-based diffusion models**: These models use continuous noise schedules rather than discrete timesteps, requiring specialized distillation approaches. Understanding the difference between training and inference noise distributions is crucial for Presto-S.
- **Distribution matching distillation**: This technique distills the base model into a few-step generator without requiring expensive offline data generation. It differs from consistency-based methods by directly matching distributions rather than enforcing consistency between different steps.
- **Layer distillation and early exiting**: These techniques reduce the cost per sampling step by dropping layers in transformers. The main challenge when applying layer dropping to diffusion models is preserving the unique behavior of the final layer, which has notably higher variance.

## Architecture Onboarding

- **Component map**: Generator (Gϕ) → Fake Score Model (µψ) → Discriminator (Dψ) for Presto-S; Score Model (µθ) with modified forward pass for Presto-L; Combined approach for Presto-LS
- **Critical path**: Real audio → Generator (Presto-S) or Score Model (Presto-L) → Generated audio → Discriminator evaluation (Presto-S) or Output (Presto-L)
- **Design tradeoffs**: Step distillation vs layer distillation - step distillation reduces total steps but each step still has full cost, while layer distillation reduces cost per step but may require more steps
- **Failure signatures**: Generator collapse (near-perfect discriminator accuracy on real samples), high variance in distribution matching gradient, performance degradation compared to base model
- **First 3 experiments**:
  1. Implement Presto-S with continuous-time conditioning and compare against discrete-time version
  2. Implement Presto-L with shifted layer dropping schedule and compare against standard ASE
  3. Combine Presto-L and Presto-S in staged approach and verify stability compared to joint training

## Open Questions the Paper Calls Out

### Open Question 1
How does the variance preservation technique in Presto-L affect the quality of generated audio for longer or more complex musical compositions compared to shorter or simpler pieces? The experiments focus on 32-second audio chunks without analysis of longer or more complex compositions. What evidence would resolve it: Conducting experiments with longer audio samples or more complex musical pieces and comparing the quality metrics (FAD, MMD, CLAP) between Presto-L and other methods.

### Open Question 2
What is the impact of using different noise distributions for the forward diffusion processes (pgen, pDMD, pDSM, pGAN) on the final audio quality and diversity in Presto-S? The paper only evaluates a specific combination of noise distributions without exploring the full range of possible combinations. What evidence would resolve it: Systematically testing different combinations of noise distributions for pgen, pDMD, pDSM, and pGAN, and measuring the resulting audio quality (FAD, MMD) and diversity (Recall, Coverage).

### Open Question 3
How does the adaptive step schedule, controlled by the ρ parameter, influence the diversity and coherence of generated music in Presto-LS? The paper provides a qualitative analysis but does not quantify the impact on diversity and coherence. What evidence would resolve it: Conducting experiments with different ρ values and measuring diversity (Recall, Coverage) and coherence using metrics that evaluate the structural consistency of the generated music.

## Limitations
- The variance preservation mechanism in Presto-L is demonstrated only on the specific DiT-XL architecture without validation on other transformer architectures
- The paper lacks human perceptual studies to validate that the 10-18x acceleration doesn't compromise perceived musical quality
- The claim about eliminating offline distillation costs with Presto-S is not fully validated with direct comparisons against offline methods

## Confidence

- **High Confidence**: The 10-18x acceleration claim is well-supported by empirical measurements and clear latency comparisons across different model variants
- **Medium Confidence**: The effectiveness of the combined Presto-LS approach is supported by experimental results, but the staged training methodology could benefit from more ablation studies
- **Low Confidence**: The claim about eliminating offline distillation costs with Presto-S is not fully validated, as the paper doesn't provide direct comparisons against offline methods or detailed analysis of the online training overhead

## Next Checks

1. **Ablation Study on Layer Distillation Schedule**: Systematically vary the layer dropping schedule in Presto-L to determine the optimal balance between acceleration and performance, particularly examining the impact of different starting points for layer dropping beyond the second-to-last layer.

2. **Direct Comparison with Offline Distillation**: Implement and compare Presto-S against traditional offline distribution matching distillation to quantify the actual computational savings and any trade-offs in final model quality.

3. **Cross-Architecture Validation**: Apply the Presto-L variance preservation technique to a different transformer architecture (e.g., standard ViT or BERT-based diffusion model) to test the generalizability of the layer distillation approach beyond the specific DiT-XL implementation.