---
ver: rpa2
title: Emotion-Driven Melody Harmonization via Melodic Variation and Functional Representation
arxiv_id: '2407.20176'
source_url: https://arxiv.org/abs/2407.20176
tags:
- melody
- music
- emotion
- chord
- keys
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a functional representation for symbolic
  music that incorporates musical keys to better control emotional valence in melody
  harmonization. Unlike prior approaches that ignore keys or transpose all music to
  a single key, this method represents melody notes and chords using Roman numerals
  relative to their key, enabling key-aware harmonization and melodic variation.
---

# Emotion-Driven Melody Harmonization via Melodic Variation and Functional Representation

## Quick Facts
- arXiv ID: 2407.20176
- Source URL: https://arxiv.org/abs/2407.20176
- Authors: Jingyue Huang; Yi-Hsuan Yang
- Reference count: 0
- Primary result: Functional representation with Roman numeral encoding significantly improves emotional valence control in melody harmonization compared to REMI encoding

## Executive Summary
This paper introduces a functional representation for symbolic music that incorporates musical keys to better control emotional valence in melody harmonization. Unlike prior approaches that ignore keys or transpose all music to a single key, this method represents melody notes and chords using Roman numerals relative to their key, enabling key-aware harmonization and melodic variation. A Transformer model is trained to harmonize melodies conditioned on valence (positive/negative), with keys determined either by rule-based transposition to parallel major/minor or predicted by the model. Objective evaluations show that this representation significantly improves harmonicity metrics (CTnCTR, PCS, MCTD) and better captures key-related patterns (RR, NR) compared to conventional REMI encoding. Subjective listening tests confirm that the method effectively conveys desired valence, especially when combined with rule-based key determination, though controlling positive emotion remains more challenging than negative.

## Method Summary
The method employs a Transformer-based sequence-to-sequence model that processes melodies and chords represented as Roman numerals relative to their musical keys. The model is trained on two datasets: HookTheory (18,206 lead sheets for pretraining) and EMOPIA (1,071 emotion-labeled piano clips for finetuning). During inference, the model generates chord progressions conditioned on target valence (positive/negative), with keys determined either by rule-based transposition to parallel major/minor keys or by the model itself. The functional representation enables cross-key pattern sharing, addressing data scarcity by allowing melodies from different keys to share similar representations.

## Key Results
- Functional representation significantly improves harmonicity metrics (CTnCTR, PCS, MCTD) compared to REMI encoding
- Better captures key-related patterns (RR, NR) indicating improved key modeling
- Subjective evaluations confirm effective valence control, especially with rule-based key determination
- Positive emotion control remains more challenging than negative emotion control

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The functional representation improves emotional valence control by explicitly modeling musical keys.
- Mechanism: By representing melody notes and chords as Roman numerals relative to their key, the model can directly incorporate key information into harmonization decisions. This allows the model to understand how the same chord progression functions differently in different keys, enabling more precise emotional manipulation through key-aware harmonization.
- Core assumption: Major/minor tonality is the primary driver of perceived valence in music, and explicit key modeling allows better control of this emotional dimension.
- Evidence anchors:
  - [abstract] "Unlike prior approaches that ignore keys or transpose all music to a single key, this method represents melody notes and chords using Roman numerals relative to their key, enabling key-aware harmonization and melodic variation."
  - [section] "Observing the above, we propose a novel functional representation designed as an alternative to REMI [1], a popular event representation that uses note pitch values and chord names to encode symbolic music. Our method represents both melody notes and chords with Roman numerals relative to musical keys..."
  - [corpus] Weak - corpus neighbors don't directly address key modeling for emotion control.

### Mechanism 2
- Claim: Functional representation addresses data scarcity by enabling cross-key pattern sharing.
- Mechanism: Since melodies in different keys but with similar scale degrees are represented by the same set of Roman numeral symbols, the model can learn from more diverse examples of similar melodic-harmonic patterns across different keys. This increases the effective training data for emotion-specific harmonization patterns.
- Core assumption: Similar melodic patterns in different keys should have similar harmonization preferences for expressing particular emotions.
- Evidence anchors:
  - [abstract] "The work addresses data scarcity by allowing melodies from different keys to share similar representations, enabling better emotion modeling."
  - [section] "Additionally, as melodies across various scales are designed to be represented by the same set of symbols for twelve scale degrees, the likelihood of encountering two melodies with similar representations increases, and their accompanying chord progressions and emotion labels can be regarded as paired samples..."
  - [corpus] Missing - corpus doesn't address data scarcity solutions.

### Mechanism 3
- Claim: Rule-based key determination outperforms model-based approaches for emotion control.
- Mechanism: By enforcing parallel major/minor key changes based on target valence (major for positive, minor for negative), the model can more reliably achieve desired emotional outcomes. This provides stronger emotional signals than letting the model predict keys from emotion conditions alone.
- Core assumption: Parallel key changes (e.g., C major to c minor) are more emotionally salient than other key changes for controlling valence.
- Evidence anchors:
  - [section] "At inference time, given an emotion condition e, a key event k could be determined in rule-based or model-based manner. In the former approach, we enforce the original key to its parallel major key for positive emotion or parallel minor key otherwise, inspired by Fig. 1."
  - [section] "When diving into user responses from different musical backgrounds, those with longer years of music-related experiences seem to favor the REMI-generated samples, while people with less experiences choose the functional representation."
  - [corpus] Weak - corpus doesn't address rule-based vs model-based key determination.

## Foundational Learning

- Concept: Musical key and tonality theory
  - Why needed here: Understanding how major/minor keys and chord functions create emotional character is essential for grasping why the functional representation helps with valence control.
  - Quick check question: Can you explain why a I-IV-V-I progression in C major might convey different emotional information than the same progression in c minor?

- Concept: Roman numeral analysis and functional harmony
  - Why needed here: The core innovation relies on representing chords and melody notes as scale-degree-relative symbols rather than absolute pitches, which requires understanding functional harmony concepts.
  - Quick check question: Given a melody in G major, what Roman numeral would represent the note B (the third scale degree)?

- Concept: Symbolic music representation formats
  - Why needed here: To understand the innovation, one must grasp how REMI encoding works and how the functional representation differs from conventional approaches.
  - Quick check question: How does representing a chord as "I" in functional notation differ from representing it as "C major" in REMI, and why might this matter for emotion modeling?

## Architecture Onboarding

- Component map: Transformer model with Performer attention -> Functional representation encoder -> Key prediction component (rule-based or model-based) -> Emotion conditioning -> Sequence generation -> Output decoding

- Critical path: 1) Input melody encoded functionally, 2) Emotion condition processed, 3) Key determined (rule or model), 4) Melody transposed if needed, 5) Transformer generates chord progression conditioned on all inputs, 6) Output decoded back to conventional notation

- Design tradeoffs: Functional representation reduces vocabulary size and enables cross-key pattern learning but requires additional conversion steps. Rule-based key determination provides stronger emotional signals but less flexibility than model-based approaches. Pretraining on large lead sheet dataset without emotion labels provides general harmonization knowledge but requires careful finetuning to learn emotion-specific patterns.

- Failure signatures: Poor harmonicity metrics (CTnCTR, PCS, MCTD) indicate the model isn't learning effective melody-chord relationships. Low RR and NR values suggest the model isn't effectively incorporating key information. Subjective evaluations showing no valence change from original indicate the model isn't successfully manipulating emotional character.

- First 3 experiments:
  1. Test functional representation on a simple harmonization task without emotion conditioning to verify it learns basic melody-chord relationships as well as REMI.
  2. Compare rule-based vs model-based key determination on a validation set to quantify which approach better achieves target valence.
  3. Analyze generated harmonies for a single melody with different valence conditions to verify the model produces meaningfully different emotional outputs.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but leaves several important avenues for future research unaddressed.

## Limitations

- Small emotion-labeled dataset (1,071 samples) may limit the model's ability to learn nuanced emotional patterns
- Reliance on rule-based key determination suggests the model may not fully capture complex relationships between musical features and emotional valence
- Only evaluates positive vs. negative valence, omitting arousal dimensions crucial for comprehensive emotion modeling

## Confidence

- High Confidence: The claim that functional representation improves harmonicity metrics (CTnCTR, PCS, MCTD) compared to REMI encoding is well-supported by objective evaluations showing consistent improvements across all metrics.
- Medium Confidence: The assertion that rule-based key determination outperforms model-based approaches for emotion control is supported by subjective evaluations, though the advantage is not overwhelming and shows variation based on participants' musical background.
- Low Confidence: The claim that the functional representation effectively addresses data scarcity by enabling cross-key pattern sharing lacks direct empirical validation in the paper, as no ablation study isolates this specific benefit.

## Next Checks

1. Conduct ablation study comparing model performance with and without functional representation on datasets of varying sizes to quantify actual benefit for data efficiency.

2. Replicate subjective evaluations with participants from diverse cultural backgrounds to assess whether key-based valence manipulation generalizes across different musical traditions and listening experiences.

3. Extend the model to condition on both valence and arousal dimensions, evaluating whether the functional representation provides similar benefits for controlling the full emotional spectrum in music.