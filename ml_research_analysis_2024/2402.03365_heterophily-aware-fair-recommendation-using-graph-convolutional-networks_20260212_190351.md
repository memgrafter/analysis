---
ver: rpa2
title: Heterophily-Aware Fair Recommendation using Graph Convolutional Networks
arxiv_id: '2402.03365'
source_url: https://arxiv.org/abs/2402.03365
tags:
- uni00000013
- items
- graph
- uni00000011
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses popularity bias in GNN-based recommender
  systems, where popular items are over-recommended at the expense of less popular,
  long-tail items. The proposed HetroFair model introduces two key innovations: (1)
  fairness-aware attention, which incorporates dot products in normalization to reduce
  the impact of node degrees, and (2) heterophily-aware feature weighting, which assigns
  distinct weights to different item features during aggregation.'
---

# Heterophily-Aware Fair Recommendation using Graph Convolutional Networks

## Quick Facts
- arXiv ID: 2402.03365
- Source URL: https://arxiv.org/abs/2402.03365
- Reference count: 40
- Primary result: HetroFair achieves up to 0.0777 NDCG improvement while reducing fairness metrics closer to zero, indicating more equitable treatment of long-tail items

## Executive Summary
This paper addresses popularity bias in GNN-based recommender systems, where popular items are over-recommended at the expense of less popular, long-tail items. The proposed HetroFair model introduces two key innovations: (1) fairness-aware attention, which incorporates dot products in normalization to reduce the impact of node degrees, and (2) heterophily-aware feature weighting, which assigns distinct weights to different item features during aggregation. Extensive experiments on six real-world datasets demonstrate that HetroFair significantly improves both accuracy and fairness compared to state-of-the-art methods, effectively balancing accuracy and fairness particularly excelling in datasets with low to moderate homophily.

## Method Summary
HetroFair is a GNN-based recommender system that addresses popularity bias through two mechanisms: fairness-aware attention that incorporates dot products in the normalization process to decrease the effect of node degrees, and heterophily-aware feature weighting that assigns distinct weights to different features during aggregation. The model is trained using pairwise Bayesian Personalized Ranking (BPR) loss with early stopping after 15 epochs without improvement. It operates on user-item bipartite graphs with initial node features, using multiple propagation layers where attention weights are computed from dot products transformed through sigmoid functions, and feature-specific weights are learned adaptively during training. The final representation combines embeddings from all layers to produce node embeddings for ranking.

## Key Results
- HetroFair achieves up to 0.0777 NDCG improvement compared to LightGCN across datasets
- Fairness metrics (PRU and PRI) are reduced closer to zero, indicating more equitable treatment of long-tail items
- The model outperforms state-of-the-art baselines including BNS, FairRec, and LightGCN on both accuracy and fairness metrics
- Performance is particularly strong on datasets with low to moderate homophily (e.g., Movies with 0.14 homophily rate)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating the dot product from the previous layer into the normalization term reduces the impact of node degrees, especially for long-tail items
- Mechanism: The sigmoid of the dot product similarity acts as a dynamic weight that is higher for items with strong past similarity and lower for long-tail items. This term is included in the denominator of the normalization, effectively shrinking embeddings of high-degree nodes more than low-degree ones, counteracting the popularity bias amplification seen in standard GCNs
- Core assumption: Higher-degree nodes produce larger embeddings and higher dot product similarities, and this amplification needs to be dampened to promote fairness
- Evidence anchors: Theorem 1 and Corollary 1 demonstrate the destructive effect of symmetric square root normalization on long-tail items, proposing this modification to balance degree and similarity effects

### Mechanism 2
- Claim: Assigning feature-specific, trainable weights to the dot product enables the model to adaptively capture heterophilous relationships between users and items
- Mechanism: Instead of a single fixed weight for the similarity between a user and an item, each feature dimension is weighted by a learned parameter w(k)_ui. This allows the model to emphasize different aspects of the interaction depending on the item's characteristics, improving fairness and accuracy for heterophilic datasets
- Core assumption: Different features capture different aspects of an item and should be weighted differently during aggregation to better model diverse user preferences
- Evidence anchors: The authors propose transforming the dot product into a trainable similarity function with these weights being learned adaptively during training

### Mechanism 3
- Claim: Using a weighted sum of embeddings from multiple layers prevents over-smoothing and maintains useful signal for both accuracy and fairness
- Mechanism: The final representation Z[v] is computed as a simple average of embeddings from all layers. This combination preserves both shallow (local) and deep (global) information, helping to maintain accuracy while reducing the over-amplification of popular items seen in deeper layers alone
- Core assumption: Combining embeddings from multiple layers can capture both local and global graph structure without losing important signal
- Evidence anchors: The paper combines embeddings from different layers to obtain the final representations of items/users

## Foundational Learning

- Concept: Graph Neural Networks and message passing
  - Why needed here: HetroFair is built on top of LightGCN's message passing framework; understanding how node embeddings are updated is essential to grasp the proposed fairness-aware modifications
  - Quick check question: In a bipartite user-item graph, how does the normalization term 1/√du√di in LightGCN affect the relative influence of high-degree versus low-degree nodes?

- Concept: Popularity bias and long-tail items in recommender systems
  - Why needed here: The motivation for HetroFair is to address unfairness toward long-tail items caused by popularity bias in standard GNN-based recommenders
  - Quick check question: Why do popular items tend to dominate recommendations in standard GNN models, and how does this affect long-tail item visibility?

- Concept: Homophily vs. heterophily in graphs
  - Why needed here: HetroFair is explicitly designed to perform well on heterophilic datasets by learning feature-specific weights; understanding the difference is key to interpreting the ablation study and dataset analysis
  - Quick check question: How does the homophily rate of a dataset influence the expected performance of a fairness-aware model like HetroFair?

## Architecture Onboarding

- Component map: User-item bipartite graph (V, E) -> Initial embeddings X -> Fairness-aware attention + heterophily feature weighting -> Multiple propagation layers -> Combined embeddings Z -> BPR loss -> Final recommendations

- Critical path:
  1. Initialize embeddings and feature weights W(k)
  2. For each layer k: Compute attention weights s = h(k-1)_v · h(k-1)_i, transform to feature weights w(k)_vi = δ × σ(s × W(k)), aggregate neighbor embeddings with normalization and Hadamard division by w(k)_vi
  3. Combine embeddings from all layers to get final Z
  4. Compute BPR loss and backpropagate

- Design tradeoffs:
  - Adding feature-specific weights increases model capacity and adaptability but also adds memory and compute overhead
  - Using sigmoid on dot products bounds the influence of attention weights, preventing extreme values but potentially limiting expressiveness
  - Averaging across layers balances shallow and deep information but may dilute strong signals if over-smoothed

- Failure signatures:
  - If PRU and PRI do not decrease (stay far from zero), the fairness mechanism may not be learning useful attention
  - If NDCG drops significantly compared to LightGCN, over-smoothing or poor weight initialization may be an issue
  - If training loss plateaus early, the feature weighting may be stuck in a local minimum or the learning rate is too low

- First 3 experiments:
  1. Run HetroFair with K=2 layers on a small, dense dataset (e.g., Beauty) to verify training stability and initial fairness gains
  2. Compare HetroFair vs. w/o Hetro (disable feature weighting) on a heterophilic dataset (e.g., Movies) to isolate the effect of feature weights
  3. Sweep the δ parameter (0.1 to 0.9) on Electronics and plot NDCG, PRU, PRI to identify the optimal trade-off point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's performance change with different types of graph homophily in real-world datasets?
- Basis in paper: The authors note that their model performs best on datasets with low to moderate homophily, such as Movies and Electronics
- Why unresolved: The study only tested six datasets with varying homophily rates, but did not systematically explore the performance across a broader range of homophily levels
- What evidence would resolve it: Testing the model on a larger set of datasets with varying homophily levels and comparing performance metrics would clarify the impact of homophily on model effectiveness

### Open Question 2
- Question: How does the model's performance scale with larger datasets or more complex graph structures?
- Basis in paper: The authors discuss the model's time and memory complexity but do not provide empirical evidence of its performance on larger datasets
- Why unresolved: The study focused on six datasets, and the scalability of the model to larger datasets or more complex graph structures remains untested
- What evidence would resolve it: Conducting experiments on larger datasets and analyzing performance metrics and resource usage would provide insights into the model's scalability

### Open Question 3
- Question: How does the model handle dynamic graphs where user-item interactions change over time?
- Basis in paper: The study assumes a static graph structure, but real-world recommendation systems often deal with dynamic interactions
- Why unresolved: The model's ability to adapt to changing user-item interactions over time was not explored in the study
- What evidence would resolve it: Testing the model on dynamic datasets where interactions change over time and evaluating its performance in such scenarios would clarify its adaptability to real-world conditions

## Limitations
- The analysis is confined to six specific datasets with varying homophily rates, limiting generalizability to other domains
- Computational overhead from feature-specific weights and sigmoid-based attention may impact scalability for very large graphs
- Fixed layer combination strategy (simple averaging) may not be optimal for all graph structures, potentially missing opportunities for adaptive layer weighting

## Confidence
- **High confidence**: The core fairness improvements (PRU/PRI reduction) and accuracy gains (NDCG improvement) are well-supported by the experimental results across multiple datasets
- **Medium confidence**: The mechanism explanations for how the attention and feature weighting work are theoretically sound but lack direct ablation evidence showing their individual contributions
- **Medium confidence**: The comparison with baseline methods is comprehensive, but the absence of certain modern fairness approaches (like PBiLoss) limits the completeness of the state-of-the-art evaluation

## Next Checks
1. Conduct ablation studies to isolate the individual contributions of fairness-aware attention versus heterophily feature weighting on datasets with varying homophily rates
2. Test the model's performance on additional datasets with extreme homophily (near 0 or 1) to evaluate robustness beyond the studied range
3. Implement and compare against PBiLoss and Graph-Structured Dual Adaptation to establish relative effectiveness among the most recent fairness approaches