---
ver: rpa2
title: Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free
arxiv_id: '2410.10814'
source_url: https://arxiv.org/abs/2410.10814
tags:
- embedding
- moee
- prompteol
- routing
- hidden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs), particularly
  Mixture-of-Experts (MoE) architectures, can serve as effective embedding models
  without further fine-tuning. While LLMs excel at generation tasks, their decoder-only
  architecture often limits their ability to produce high-quality embeddings.
---

# Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free

## Quick Facts
- arXiv ID: 2410.10814
- Source URL: https://arxiv.org/abs/2410.10814
- Authors: Ziyue Li; Tianyi Zhou
- Reference count: 8
- Key outcome: MoE router weights can serve as effective embeddings complementary to hidden states

## Executive Summary
This paper investigates whether large language models (LLMs), particularly Mixture-of-Experts (MoE) architectures, can serve as effective embedding models without further fine-tuning. While LLMs excel at generation tasks, their decoder-only architecture often limits their ability to produce high-quality embeddings. The authors propose that the expert routers in MoE models can act as an off-the-shelf embedding model, producing routing weights (RW) that are complementary to the widely used hidden states (HS). The RW embedding is more robust to prompt variations and focuses on high-level semantics, capturing different aspects of input data than HS.

## Method Summary
The authors analyze how MoE models, specifically those using router-based expert selection, can generate embeddings through both hidden states (HS) and routing weights (RW). They propose MoE Embedding (MOEE), which combines RW and HS through concatenation or a weighted sum of their similarity scores. The method leverages the observation that RW captures different semantic information than HS, with RW being more robust to prompt variations and focusing on high-level semantics. MOEE is evaluated across six embedding tasks from the Massive Text Embedding Benchmark (MTEB).

## Key Results
- MOEE consistently outperforms both RW and HS individually across six embedding tasks from MTEB
- The best MOEE variant, using a weighted sum, achieves significant gains on classification, clustering, re-ranking, and semantic textual similarity tasks
- RW embeddings are more robust to prompt variations and capture different semantic aspects than HS embeddings

## Why This Works (Mechanism)
The mechanism behind MOEE's effectiveness lies in the complementary nature of routing weights and hidden states. Router weights in MoE models represent the model's decision about which experts to activate for a given input, capturing high-level semantic decisions. These routing weights provide a different perspective on the input than the hidden states, which capture more granular linguistic features. By combining both representations, MOEE can leverage the strengths of each: the semantic richness of RW and the linguistic detail of HS.

## Foundational Learning
- **Mixture-of-Experts (MoE)**: A neural network architecture that activates different sub-networks (experts) based on input characteristics, improving efficiency and specialization
  - *Why needed*: Enables scaling model capacity without proportional computational cost
  - *Quick check*: Verify expert specialization through activation pattern analysis

- **Routing weights**: The probability distribution over experts determined by the router network, indicating expert selection importance
  - *Why needed*: Provides semantic information about input relevance to different expert domains
  - *Quick check*: Analyze routing weight entropy across different input types

- **Hidden states**: The internal representations produced by transformer layers, typically used for embeddings
  - *Why needed*: Capture linguistic and contextual information from input text
  - *Quick check*: Compare hidden state similarity across semantically related inputs

## Architecture Onboarding

**Component map**: Input Text -> Router Network -> Expert Selection -> Hidden States + Routing Weights -> MOEE Combination

**Critical path**: Input processing through MoE layers → Router weight computation → Hidden state generation → MOEE combination → Embedding output

**Design tradeoffs**: The paper trades computational efficiency for embedding quality by combining two embedding sources, increasing both computation and storage requirements compared to using HS alone.

**Failure signatures**: 
- RW embeddings may be less effective for tasks requiring fine-grained linguistic detail
- Performance gains may be marginal for certain tasks where HS alone is already highly effective
- The combination method may not generalize well to non-MoE architectures

**3 first experiments**:
1. Compare RW and HS embeddings on a semantic textual similarity task to establish baseline complementarity
2. Test different combination methods (concatenation vs. weighted sum) to determine optimal MOEE configuration
3. Evaluate MOEE performance across tasks with varying semantic complexity to understand capability boundaries

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis is primarily empirical without deeper theoretical justification for why router weights capture complementary semantic information
- Performance gains, while statistically significant, are sometimes modest (e.g., 1-2% improvements on certain tasks)
- The study focuses on a limited set of MoE architectures and may not generalize to all MoE implementations

## Confidence

| Claim Cluster | Confidence Level |
|---------------|------------------|
| MoE router weights as effective embeddings | Medium |
| MOEE consistently outperforms individual embeddings | Medium |
| RW captures different semantic aspects than HS | Medium |

## Next Checks
1. Conduct ablation studies varying the number of experts and mixture weights to determine how MoE architecture choices affect embedding quality, establishing architectural sensitivity.

2. Test MOEE on additional benchmarks beyond MTEB, particularly domain-specific embedding tasks in areas like medical, legal, or technical domains to assess generalizability.

3. Implement and evaluate the computational and storage trade-offs of MOEE in production scenarios, including latency measurements and memory requirements compared to using HS alone.