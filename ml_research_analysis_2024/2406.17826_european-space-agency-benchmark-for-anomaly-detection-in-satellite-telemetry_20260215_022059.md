---
ver: rpa2
title: European Space Agency Benchmark for Anomaly Detection in Satellite Telemetry
arxiv_id: '2406.17826'
source_url: https://arxiv.org/abs/2406.17826
tags:
- channels
- anomaly
- anomalies
- detection
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The ESA-ADB is a comprehensive benchmark for anomaly detection
  in satellite telemetry, addressing the lack of standardized datasets and evaluation
  methods in the field. It consists of a large, annotated dataset of real satellite
  telemetry from ESA missions, an evaluation pipeline tailored to spacecraft operations
  needs, and benchmarking results of various anomaly detection algorithms.
---

# European Space Agency Benchmark for Anomaly Detection in Satellite Telemetry

## Quick Facts
- **arXiv ID:** 2406.17826
- **Source URL:** https://arxiv.org/abs/2406.17826
- **Reference count:** 0
- **Primary result:** Introduces ESA-ADB, a comprehensive benchmark with real satellite telemetry data and evaluation pipeline for anomaly detection.

## Executive Summary
The ESA Anomalies Dataset (ESA-ADB) addresses the critical need for standardized datasets and evaluation methods in satellite telemetry anomaly detection. It provides a large, annotated dataset from two ESA missions containing over 1.5 billion data points with manual annotations by spacecraft operations engineers and ML experts. The benchmark includes a tailored evaluation pipeline with hierarchical metrics that reflect operational priorities, and comprehensive benchmarking results showing significant challenges for existing anomaly detection algorithms.

## Method Summary
The ESA-ADB consists of a fully anonymized dataset with telemetry channels, telecommands, and ground truth labels, along with an evaluation pipeline that extends the TimeEval framework. The dataset covers two missions with varying sampling rates (0.033-0.056 Hz) and includes over 76 and 100 channels respectively. The evaluation uses nine preselected algorithms (5 unsupervised, 3 semi-supervised) in an online streaming manner, with preprocessing that includes resampling to uniform frequency, standardization, and telecommand encoding. The hierarchical evaluation prioritizes no false alarms and anomaly existence, followed by timing and range considerations.

## Key Results
- None of the tested algorithms offer a perfect solution for both missions across complete channel sets
- Telemanom-ESA-Pruned achieved the best performance with corrected event-wise F0.5-scores of 0.968 (Mission1) and 0.842 (Mission2)
- The benchmark reveals significant challenges for typical anomaly detection algorithms on real satellite telemetry data
- Hierarchical evaluation shows that prioritizing false alarm minimization significantly impacts algorithm rankings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-channel telemetry anomaly detection is possible without prior knowledge of physical units or channel names
- Mechanism: The dataset is fully anonymized—channel names, subsystem names, physical units, and mission identifiers are replaced with consistent numeric IDs. This forces algorithms to rely solely on signal characteristics rather than domain heuristics.
- Core assumption: Signal patterns and anomalies are preserved after anonymization; numeric IDs do not introduce false correlations
- Evidence anchors:
  - [abstract] "The anonymisation does not affect the data integrity and it was verified that algorithms produce the same results as before anonymisation"
  - [section] "The anonymisation process was carefully designed to maintain data integrity, so the results are independent of the anonymisation"
- Break condition: If anonymization alters statistical properties of signals (e.g., scaling changes correlations), detection performance would degrade

### Mechanism 2
- Claim: Hierarchical evaluation better reflects operational priorities than single aggregated metrics
- Mechanism: Metrics are grouped into five priority levels (primary: no false alarms, anomaly existence; secondary: timing, range). Algorithms are compared stage-by-stage; only if tied on a higher priority do we evaluate the next
- Core assumption: Mission control prioritizes false alarm minimization over detection recall; timing and range are less critical
- Evidence anchors:
  - [section] "The highest priority aspect relates to the proper identification of anomalous events , but with a strong emphasis on avoiding false alarms"
  - [section] "The second highest priority for SOEs is to have the information about subsystems … affected by anomalies"
- Break condition: If operators actually value early detection over false alarm rates, the hierarchy would misalign with needs

### Mechanism 3
- Claim: Real-time performance is achievable even with deep learning models on large telemetry datasets
- Mechanism: Zero-order hold resampling preserves binary/categorical channel semantics; lightweight subsets enable rapid prototyping; models are evaluated on fixed hardware limits (≤5 days training, ≤RAM/VRAM constraints)
- Core assumption: Telemetry sampling rates (0.033–0.056 Hz) are low enough that model inference time is negligible compared to data arrival
- Evidence anchors:
  - [section] "The total execution time (including resampling) for the full Mission1 test set is 3.5h which is just 0.02% of the test set duration"
  - [section] "real-time execution should be possible even for sampling rates higher than 30 Hz"
- Break condition: If future missions use much higher sampling rates or onboard compute is more constrained, real-time constraints may be violated

## Foundational Learning

- Concept: Time-series anomaly detection taxonomy (point vs subsequence, univariate vs multivariate, global vs local)
  - Why needed here: ESA-ADB annotates events with all three attributes; algorithms must be evaluated against the correct type
  - Quick check question: What distinguishes a local subsequence anomaly from a global one in terms of value ranges?
- Concept: Resampling with zero-order hold interpolation
  - Why needed here: Most algorithms require uniform sampling; zero-order hold preserves binary/categorical semantics and avoids future-sample leakage
  - Quick check question: Why is linear interpolation inappropriate for binary telecommand channels?
- Concept: Event-wise vs sample-wise evaluation
  - Why needed here: Event-wise metrics avoid penalizing algorithms for partial overlap and prevent trivial "always detect" solutions
  - Quick check question: How does the corrected event-wise F-score penalize constant detection of every sample?

## Architecture Onboarding

- Component map: ESA-AD dataset -> channels/ (Pickled DataFrames) + telecommands/ + labels.csv + metadata CSVs -> TimeEval framework (extended) -> 9 TSAD algorithms (5 unsupervised, 3 semi-supervised) -> Evaluation pipeline -> 5 priority metrics -> Hierarchical comparison
- Critical path: 1. Load dataset (channels + telecommands + labels) 2. Preprocess: resample → standardize → encode TCs 3. Train algorithm on training set 4. Detect on test set 5. Compute metrics per hierarchy 6. Store results
- Design tradeoffs:
  - Full anonymization vs domain-informed features (forces generic models)
  - Lightweight subsets vs full channel sets (speed vs completeness)
  - Semi-supervised vs unsupervised (requires anomaly presence in training)
- Failure signatures:
  - Out-of-memory → algorithm too heavy for dataset size
  - All metrics zero → algorithm never fires detections
  - High false positive rate → poor thresholding/postprocessing
- First 3 experiments:
  1. Run GlobalSTD (STD3) on lightweight subset; verify baseline performance
  2. Train DC-VAE-ESA on full set; compare lightweight vs full test results
  3. Execute Telemanom-ESA-Pruned; analyze timing quality curve vs other methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal anomaly detection algorithm for satellite telemetry that balances accuracy, computational efficiency, and real-time capability?
- Basis in paper: [explicit] The paper states that "none of them offer a perfect solution for both missions, especially for complete sets of channels" and highlights the need for new approaches
- Why unresolved: The paper presents benchmarking results showing limitations of existing algorithms but does not identify a clear winner or propose a novel solution
- What evidence would resolve it: A new algorithm or significant improvement to existing algorithms that achieves high accuracy across both missions while meeting real-time computational constraints

### Open Question 2
- Question: How can anomaly detection algorithms be improved to distinguish between anomalies and rare nominal events in satellite telemetry?
- Basis in paper: [explicit] The paper emphasizes the challenge of distinguishing between anomalies and rare nominal events, noting that "it is usually impossible to distinguish between novel rare nominal events and anomalies without additional a priori expert knowledge"
- Why unresolved: Existing algorithms in the benchmark do not explicitly support learning rare nominal events, and the paper suggests this as a key area for future research
- What evidence would resolve it: Development of algorithms that can accurately identify and remember rare nominal events, preventing false alarms for subsequent occurrences

### Open Question 3
- Question: What is the minimum training data required for effective anomaly detection in satellite telemetry across different mission phases?
- Basis in paper: [explicit] The paper analyzes the effect of training set size on algorithm performance, showing that "the longest training sets do not always ensure the best results" and suggesting this may be related to concept drift
- Why unresolved: While the paper provides some insights into training set length effects, it does not determine optimal training durations for different mission phases or account for concept drift
- What evidence would resolve it: Systematic studies of algorithm performance across various training set sizes and mission phases, including methods to handle concept drift

## Limitations

- Unknown impact of anonymization on algorithm performance - claimed but not independently verified
- Dataset represents only two missions, limiting generalizability across different spacecraft architectures
- Hierarchical evaluation prioritizes false alarm minimization which may not align with all operational requirements

## Confidence

- **High Confidence**: The dataset construction methodology and anonymization process are well-documented and technically sound
- **Medium Confidence**: Benchmark results are reproducible given the provided code and documentation, though hardware dependencies may affect exact timings
- **Medium Confidence**: The claim that anonymization preserves algorithm performance, as this is stated but not independently validated

## Next Checks

1. Verify anonymization preservation by running selected algorithms on both raw and anonymized versions of the dataset to confirm identical performance metrics
2. Test algorithm performance on a third, independent satellite telemetry dataset to assess generalizability beyond the two ESA missions
3. Conduct sensitivity analysis on the hierarchical evaluation weights to determine how changing priority assumptions affects algorithm rankings