---
ver: rpa2
title: 'Two-Step SPLADE: Simple, Efficient and Effective Approximation of SPLADE'
arxiv_id: '2404.13357'
source_url: https://arxiv.org/abs/2404.13357
tags:
- splade
- retrieval
- bm25
- pruning
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Two-Step SPLADE addresses the efficiency limitations of learned
  sparse retrieval models like SPLADE when using classical inverted index data structures.
  The method splits first-stage retrieval into two steps: (1) an approximate step
  using pruned and reweighted sparse vectors, and (2) a rescoring step using original
  SPLADE vectors on a small document subset retrieved in the first stage.'
---

# Two-Step SPLADE: Simple, Efficient and Effective Approximation of SPLADE

## Quick Facts
- arXiv ID: 2404.13357
- Source URL: https://arxiv.org/abs/2404.13357
- Authors: Carlos Lassance; Hervé Dejean; Stéphane Clinchant; Nicola Tonellotto
- Reference count: 39
- Primary result: 12x-40x faster mean and tail response times compared to original SPLADE while maintaining statistical significance in effectiveness for 60% of datasets

## Executive Summary
Two-Step SPLADE addresses the efficiency limitations of learned sparse retrieval models like SPLADE when using classical inverted index data structures. The method splits first-stage retrieval into two steps: (1) an approximate step using pruned and reweighted sparse vectors, and (2) a rescoring step using original SPLADE vectors on a small document subset retrieved in the first stage. The approximation uses aggressive document and query pruning combined with BM25-style term reweighting to improve dynamic pruning efficiency. Experiments on 30 datasets show the method achieves 12x-40x faster mean and tail response times compared to original SPLADE while maintaining statistical significance in effectiveness for 60% of datasets. The approach provides a flexible, production-ready solution that works across in-domain and out-of-domain datasets without requiring changes to existing search infrastructure.

## Method Summary
Two-Step SPLADE implements a cascade approach where first-stage retrieval uses approximate sparse vectors created through aggressive document pruning (reducing to ~50 tokens), query pruning (to ~5 tokens for MSMARCO), and BM25-style term reweighting with saturation parameter k1=100. The approximate index uses these reweighted vectors to retrieve top-100 documents, which are then rescored using the original SPLADE vectors in a second step. This approach leverages the efficiency of BM25-style weighting for dynamic pruning while preserving the effectiveness of learned sparse representations for final ranking. The method is implemented using the PISA search engine and tested across 30 datasets including MSMARCO dev, TREC-DL 19/20, BEIR, and LoTTe collections.

## Key Results
- 12x-40x faster mean and tail response times compared to original SPLADE
- Statistical significance in effectiveness (nDCG@10) maintained for 60% of datasets
- Intersection with full SPLADE results remains above 90% for top-10 documents
- Works across 30 datasets including in-domain (MSMARCO, TREC-DL) and out-of-domain (BEIR, LoTTe) collections
- No infrastructure changes required, compatible with classical inverted index systems

## Why This Works (Mechanism)
Two-Step SPLADE works by addressing the fundamental mismatch between learned sparse retrieval models and classical inverted index data structures. The original SPLADE model generates sparse vectors with complex term weighting that is inefficient for dynamic pruning algorithms used in traditional search systems. By creating an approximate index with BM25-style reweighting and aggressive pruning, the method enables efficient dynamic pruning while the second rescoring step with original SPLADE vectors ensures high-quality ranking. The k1=100 saturation parameter provides optimal balance between pruning efficiency and effectiveness preservation.

## Foundational Learning

**Inverted Index Structure**: Why needed - Understanding how documents and queries are mapped to posting lists; Quick check - Verify that pruned vectors correctly update posting list lengths and frequencies.

**Dynamic Pruning Algorithms**: Why needed - Core mechanism for efficient retrieval in first step; Quick check - Confirm that WAND or similar algorithms can process the reweighted vectors efficiently.

**Term Weighting Schemes**: Why needed - BM25-style reweighting is crucial for pruning efficiency; Quick check - Validate that term saturation at k1=100 provides intended efficiency gains without excessive effectiveness loss.

**Query Pruning Techniques**: Why needed - Reduces query complexity for faster processing; Quick check - Ensure top-pooling to 5 tokens maintains sufficient query information for retrieval.

**Document Pruning Strategies**: Why needed - Limits posting list sizes for efficiency; Quick check - Verify that reducing documents to 50 tokens preserves key semantic content.

## Architecture Onboarding

Component Map: Document Collection -> Document Pruner -> Approximate Index -> First-Stage Retrieval -> Top-100 Documents -> Original SPLADE Index -> Second-Stage Rescoring -> Final Results

Critical Path: Query -> Query Pruner -> Approximate Index Search -> Top-100 Documents -> Original SPLADE Search -> Final Ranking

Design Tradeoffs: Aggressive pruning improves efficiency but risks effectiveness loss; BM25 reweighting improves pruning but may reduce semantic precision; Two-step approach adds complexity but maintains flexibility.

Failure Signatures: Low intersection rates (<90%) indicate excessive pruning; High latency suggests inefficient reweighting or indexing; Effectiveness drops signal loss of critical information during approximation.

First Experiments:
1. Test varying pruning thresholds (40, 50, 60 tokens for documents; 3, 5, 7 tokens for queries) to find optimal balance
2. Evaluate different k1 saturation values (50, 100, 200) to optimize efficiency-effectiveness tradeoff
3. Measure intersection rates between approximate and full SPLADE results across query types

## Open Questions the Paper Calls Out

**Open Question 1**: How does Two-Step SPLADE compare to sketching and clustering approaches like [3,4] in terms of effectiveness, efficiency, and storage requirements?
- Basis in paper: [explicit] The paper mentions sketching and clustering works but does not directly compare them to Two-Step SPLADE
- Why unresolved: The paper focuses on comparing Two-Step SPLADE to other sparse retrieval methods and does not provide a direct comparison with sketching and clustering approaches
- What evidence would resolve it: A direct experimental comparison of Two-Step SPLADE with sketching and clustering approaches on the same datasets, measuring effectiveness, efficiency, and storage requirements

**Open Question 2**: How does the effectiveness of Two-Step SPLADE vary with different query encoder models?
- Basis in paper: [inferred] The paper mentions that the query encoder cost is not considered in the efficiency evaluation and leaves it as future work
- Why unresolved: The paper does not provide experiments with different query encoder models, so the impact on effectiveness is unknown
- What evidence would resolve it: Experiments with Two-Step SPLADE using different query encoder models, measuring the effectiveness on various datasets

**Open Question 3**: How does Two-Step SPLADE perform on datasets with different annotation granularities?
- Basis in paper: [explicit] The paper mentions that nDCG is preferred over Recall for easier use across datasets with different annotation granularities
- Why unresolved: The paper does not provide a detailed analysis of Two-Step SPLADE's performance on datasets with varying annotation granularities
- What evidence would resolve it: Experiments with Two-Step SPLADE on datasets with different annotation granularities, comparing its effectiveness using both nDCG and Recall metrics

## Limitations

- Specific SPLADE-v3 model checkpoints and training parameters not fully specified, limiting exact replication
- Implementation details of term reweighting function and saturation mechanism remain somewhat unclear
- Focus on response time improvements without extensive exploration of potential trade-offs in other performance metrics

## Confidence

High confidence: The latency improvements (12x-40x faster response times) and the general effectiveness maintenance (60% of datasets show statistical significance) are well-supported by the experimental methodology and multiple dataset evaluations.

Medium confidence: The specific pruning thresholds and term reweighting mechanisms, while theoretically sound, lack complete implementation details that would enable perfect replication.

Medium confidence: The claim that the approach works "without requiring changes to existing search infrastructure" assumes compatibility with PISA and similar systems but may not generalize to all inverted index implementations.

## Next Checks

1. **Implementation Verification**: Replicate the pruning and term reweighting mechanisms with varying k1 parameters (e.g., 50, 100, 200) to confirm that k1=100 provides optimal balance between efficiency and effectiveness as claimed.

2. **Cross-Infrastructure Testing**: Test the Two-Step SPLADE approach with alternative search engines beyond PISA (e.g., Elasticsearch, Lucene) to verify the claim of infrastructure compatibility.

3. **Long-Tail Performance Analysis**: Conduct detailed analysis of the tail response time distribution across different query types and document collection characteristics to better understand the factors affecting the 12x-40x improvement range.