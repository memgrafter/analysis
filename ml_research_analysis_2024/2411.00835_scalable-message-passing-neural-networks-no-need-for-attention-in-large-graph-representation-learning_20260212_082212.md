---
ver: rpa2
title: 'Scalable Message Passing Neural Networks: No Need for Attention in Large Graph
  Representation Learning'
arxiv_id: '2411.00835'
source_url: https://arxiv.org/abs/2411.00835
tags:
- graph
- attention
- which
- nodes
- residual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SMPNNs, a scalable message-passing graph neural
  network architecture that integrates standard convolutional message passing into
  a Pre-Layer Normalization Transformer-style block. The key innovation is replacing
  attention mechanisms with efficient message-passing convolutions, achieving competitive
  performance with state-of-the-art Graph Transformers while maintaining O(E) computational
  complexity.
---

# Scalable Message Passing Neural Networks: No Need for Attention in Large Graph Representation Learning

## Quick Facts
- **arXiv ID**: 2411.00835
- **Source URL**: https://arxiv.org/abs/2411.00835
- **Authors**: Haitz Sáez de Ocáriz Borde; Artem Lukoianov; Anastasis Kratsinos; Michael Bronstein; Xiaowen Dong
- **Reference count**: 40
- **Key outcome**: SMPNNs achieve competitive performance with state-of-the-art Graph Transformers while maintaining O(E) computational complexity through attention-free message passing

## Executive Summary
This paper introduces SMPNNs (Scalable Message Passing Neural Networks), an architecture that achieves competitive performance with Graph Transformers on large-scale graph representation learning tasks without using attention mechanisms. By integrating standard convolutional message passing into a Pre-Layer Normalization Transformer-style block, SMPNNs maintain linear computational complexity while preserving the universal approximation properties essential for deep graph neural networks. The architecture demonstrates significant scalability advantages, handling graphs with up to 100 million nodes while using GPU memory that scales linearly with the number of edges.

## Method Summary
SMPNNs replace attention mechanisms with standard graph convolutional message passing in a Pre-Layer Normalization Transformer block structure. The core operation uses degree-normalized adjacency matrices for local information aggregation, followed by SiLU activation, residual connections, and layer normalization. The architecture includes learnable scaling factors (α₁, α₂) for the residual connections, initialized at 1e-6. The model is trained using Adam optimizer with learning rate 1e-3 and dropout 0.1, with subgraph sampling for large graphs and full graph processing for smaller datasets.

## Key Results
- SMPNNs outperform recent Graph Transformers on large-scale datasets: ogbn-proteins (83.15 ROC-AUC), pokec (79.76 accuracy), ogbn-arxiv (73.75 accuracy), ogbn-products (90.61 accuracy)
- Achieves 66.21 accuracy on the massive 100M-node ogbn-papers-100M dataset
- GPU memory usage scales linearly with the number of edges, demonstrating practical scalability
- Ablation studies show residual connections are essential for maintaining universal approximation properties

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing attention with standard message-passing convolutions in a Pre-LN Transformer block maintains competitive performance while reducing computational complexity from O(N²) to O(E).
- Mechanism: The standard GCN layer aggregates information from neighboring nodes using degree-normalized adjacency matrices, preserving the locality inductive bias of graphs while achieving linear complexity in the number of edges.
- Core assumption: The locality inductive bias is sufficient for large graph transductive learning tasks, and global attention is not essential for performance.
- Evidence anchors: [abstract] "by integrating standard convolutional message passing into a Pre-Layer Normalization Transformer-style block instead of attention", [section 3.3] "Our model's graph convolution layer inherits the computational cost of GCNs, O(E)"
- Break condition: When graph structure becomes less informative than global patterns, or when nodes have very high degree making local aggregation inefficient.

### Mechanism 2
- Claim: Residual connections after graph convolutions preserve universal approximation properties that would otherwise be lost.
- Mechanism: The residual connection allows the composite model MLP ◦ Lconv+r to maintain injectivity, which is necessary for universal approximation according to [48, Theorem 3.4].
- Core assumption: The graph convolution layer without residual connections is not injective, particularly for complete graphs where all rows of the normalized adjacency matrix are identical.
- Evidence anchors: [section 4.2] "we demonstrate that residual connections... are essential for preserving the universal approximation properties", [section 4.2] "Lconv G,W is not injective" for complete graphs
- Break condition: When the graph structure is highly irregular or when the weight matrix W is initialized such that injectivity is lost despite residual connections.

### Mechanism 3
- Claim: The SMPNN architecture enables deep message-passing networks without suffering from oversmoothing, unlike traditional GNNs.
- Mechanism: The combination of residual connections, layer normalization, and the specific architectural packaging prevents the low-frequency dominance that typically occurs in deep GNNs.
- Core assumption: Oversmoothing is primarily caused by the lack of architectural components that maintain frequency diversity in deep networks.
- Evidence anchors: [abstract] "enables deep message-passing GNNs without suffering from oversmoothing", [section 3.1] "residual connections are essential for maintaining the universal approximation properties"
- Break condition: When the graph has very specific spectral properties that cause frequency dominance regardless of architectural choices.

## Foundational Learning

- **Graph Convolutional Networks (GCNs) and message passing**: SMPNNs build directly on GCN operations as the core communication mechanism. *Quick check: What is the computational complexity of a single GCN layer on a sparse graph with E edges?*
- **Layer Normalization and Pre-LN Transformers**: The Pre-LN Transformer block structure is the foundation for SMPNN architecture. *Quick check: Where is layer normalization applied in a Pre-LN Transformer compared to Post-LN?*
- **Universal Approximation Theory**: The theoretical justification for residual connections relies on understanding when composite models can approximate any continuous function. *Quick check: What mathematical property must a transformation have to preserve the universal approximation property of a downstream learner?*

## Architecture Onboarding

- **Component map**: Input → LayerNorm → GCN → Residual → LayerNorm → Feedforward → Residual
- **Critical path**: LayerNorm → GCN → Residual → LayerNorm → Feedforward → Residual
- **Design tradeoffs**:
  - Attention vs. GCN: Attention provides global context but O(N²) complexity; GCN provides local context with O(E) complexity
  - Depth vs. Over-smoothing: Deeper networks without proper architecture suffer from feature homogenization; SMPNNs maintain diversity through architectural design
  - Expressiveness vs. Efficiency: Adding attention improves performance marginally at significant computational cost
- **Failure signatures**:
  - GPU memory overflow: Indicates insufficient subgraph sampling or too many nodes in batch
  - Degraded performance with depth: Suggests missing residual connections or improper normalization
  - Poor generalization: May indicate insufficient training data or suboptimal hyperparameter tuning
- **First 3 experiments**:
  1. Ablation study: Remove residual connections after GCN layer to verify performance degradation (expect ~15-20% accuracy drop)
  2. Depth scaling: Incrementally increase layers from 2 to 12 on ogbn-arxiv to observe performance plateau and verify deep architecture capability
  3. Attention augmentation: Add linear global attention to baseline SMPNN to measure marginal performance improvement (<1% expected)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of SMPNNs without attention extend to heterogeneous graphs with diverse node and edge types?
- Basis in paper: [inferred] The paper focuses on homogeneous graphs and mentions that attention may not be necessary due to high interconnectivity, but does not test heterogeneous graphs.
- Why unresolved: The experimental validation is limited to homogeneous graphs, leaving the applicability to heterogeneous graphs untested.
- What evidence would resolve it: Performance comparison of SMPNNs with and without attention on heterogeneous graph datasets like ogbn-mag or DBLP.

### Open Question 2
- Question: How do SMPNNs perform when the graph structure contains significant bottlenecks or low MaxSCC ratios?
- Basis in paper: [explicit] The authors note that high MaxSCC ratios in their datasets may explain why attention is not critical, but do not test graphs with bottlenecks.
- Why unresolved: The experimental datasets all have high MaxSCC ratios, so the model's performance on graphs with bottlenecks remains unknown.
- What evidence would resolve it: Testing SMPNNs on graphs with known bottlenecks or low MaxSCC ratios and comparing performance to attention-based models.

### Open Question 3
- Question: What is the theoretical relationship between the residual scaling factors (α₁, α₂) and the universal approximation properties of SMPNNs?
- Basis in paper: [explicit] The paper introduces learnable scaling factors but only provides empirical ablation studies without theoretical analysis.
- Why unresolved: The ablation studies show performance impacts but do not explain the theoretical mechanism behind the scaling factors' importance.
- What evidence would resolve it: Mathematical analysis connecting the scaling factors to the preservation of universal approximation properties or to the control of frequency dominance in the network.

## Limitations

- The theoretical claims about universal approximation are limited to specific graph structures (complete graphs) and may not generalize to all graph types
- The empirical evaluation does not explore performance on graphs with different structural properties where attention might be more beneficial
- The claim that locality bias is sufficient for large graph transductive learning is asserted but not rigorously tested across diverse graph domains

## Confidence

- **High confidence**: Computational complexity claims (O(E) vs O(N²)) and empirical scalability results
- **Medium confidence**: Performance comparisons with Graph Transformers on tested datasets
- **Medium confidence**: Theoretical claims about residual connections preserving universal approximation
- **Low confidence**: Generalizability of results to graph types not tested (heterogeneous graphs, dynamic graphs)

## Next Checks

1. **Structural robustness test**: Evaluate SMPNNs on heterogeneous graph datasets to verify performance claims extend beyond homogeneous graphs used in experiments
2. **Attention necessity ablation**: Systematically compare SMPNNs against variants with different levels of attention (none, linear attention, full attention) on graphs where global patterns are known to be important
3. **Over-smoothing verification**: Conduct controlled experiments varying graph spectral properties to identify exact conditions where SMPNNs maintain performance advantages over traditional deep GNNs