---
ver: rpa2
title: An archaeological Catalog Collection Method Based on Large Vision-Language
  Models
arxiv_id: '2412.20088'
source_url: https://arxiv.org/abs/2412.20088
tags: []
core_contribution: 'The paper addresses the challenge of automated archaeological
  catalog collection, where existing large vision-language models struggle with accurate
  image detection and modal matching. The proposed method introduces a three-module
  pipeline: document localization using open-set object detection, block comprehension
  to convert visual and textual information into structured attributes, and block
  matching using foreign key and distance-based bipartite matching.'
---

# An archaeological Catalog Collection Method Based on Large Vision-Language Models

## Quick Facts
- arXiv ID: 2412.20088
- Source URL: https://arxiv.org/abs/2412.20088
- Reference count: 18
- Primary result: 35.4% average precision with 33.8% improvement over baseline

## Executive Summary
This paper addresses the challenge of automated archaeological catalog collection, where existing large vision-language models struggle with accurate image detection and modal matching. The proposed method introduces a three-module pipeline that combines open-set object detection for document localization, vision-language model comprehension for structured attribute extraction, and a hybrid matching strategy using foreign keys and distance-based bipartite matching. Experiments on pottery catalogs from Dabagou and Miaozigou sites demonstrate significant improvements over baseline approaches, successfully processing 2,301 data pairs from 302 catalog pages.

## Method Summary
The method employs a three-module pipeline: document localization using open-set object detection to segment catalog pages into image and text blocks, block comprehension using VLMs to convert visual and textual information into structured attribute dictionaries, and block matching that combines foreign key linkages with bipartite graph matching to align corresponding blocks. The approach processes archaeological catalog PDFs by first detecting regions containing images and text, then extracting structured attributes from each region, and finally matching image blocks with their corresponding text descriptions using a two-stage strategy that handles cases with multiple images sharing identical foreign keys.

## Key Results
- Achieved 35.4% average precision on matching archaeological catalog images with text descriptions
- Showed 33.8% improvement over using Qwen-VL alone for the same task
- Successfully processed 2,301 data pairs from 302 catalog pages across two archaeological sites

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Open-set object detection enables accurate segmentation of mixed content regions in archaeological catalog pages.
- Mechanism: The method uses an open-set object detection model with specialized prompts to identify and separate image blocks and text blocks, overcoming the limitations of closed-set detectors that cannot handle the diverse layout of catalog pages.
- Core assumption: The open-set detector can generalize to the specific visual patterns of archaeological catalog layouts without requiring extensive retraining on catalog data.
- Evidence anchors:
  - [abstract] "We employ open-set object detection models to localize and segment document blocks"
  - [section] "we leverage an open-set object detection model with specifically designed prompts"
  - [corpus] Weak corpus evidence - the related papers focus on different domains (astronomical images, product catalogs) without specific evidence for archaeological catalog detection
- Break condition: If the catalog layout varies significantly across different publications, the open-set detector may fail to generalize without additional training data.

### Mechanism 2
- Claim: Foreign key matching combined with distance-based bipartite matching effectively aligns multi-modal information from different blocks.
- Mechanism: The method first uses foreign key attributes (like catalog numbers) to establish initial correspondences between image and text blocks, then refines these matches using spatial distance in a bipartite graph when multiple blocks share identical foreign keys.
- Evidence anchors:
  - [abstract] "Finally, we implement matching rules based on foreign key linkages and bipartite graph matching to complete modal matching"
  - [section] "For each element ùëêùëû in image region comprehension results and text region comprehension results, we define its foreign key set... When ùëÄ (ùëêùëé, ùëêùëè ) = 1, indicating complete foreign key matching between two blocks, we consider these blocks to be corresponding"
  - [corpus] Weak corpus evidence - the related papers focus on different types of matching (astronomical images, product recommendations) without specific evidence for archaeological catalog matching
- Break condition: If foreign key attributes are missing or inconsistent across blocks, the matching accuracy will degrade significantly.

### Mechanism 3
- Claim: VLM comprehension of localized blocks converts visual and textual information into structured attributes for downstream processing.
- Mechanism: After localization, the method uses a VLM to analyze each detected region and generate structured attribute dictionaries containing key-value pairs that represent the information in a machine-readable format.
- Core assumption: The VLM can accurately interpret both the visual content (artifact images) and textual content (descriptions) in their specific archaeological context.
- Evidence anchors:
  - [abstract] "Then, we process these document blocks for comprehension and describe them in terms of attributes"
  - [section] "For each detected region, we employ a VLM to generate structured attributes... ùëêùëû = {(ùëò1 : ùë£1), (ùëò2 : ùë£2), ..., (ùëòùëù : ùë£ùëù )}"
  - [corpus] Weak corpus evidence - the related papers focus on different types of comprehension (metadata generation, porcelain classification) without specific evidence for archaeological catalog comprehension
- Break condition: If the VLM lacks domain-specific knowledge about archaeological terminology, it may misinterpret the content.

## Foundational Learning

- Concept: Object detection and localization
  - Why needed here: The method requires accurate segmentation of catalog pages into distinct image and text regions before processing can occur
  - Quick check question: What is the difference between closed-set and open-set object detection, and why is open-set detection preferred for this application?

- Concept: Bipartite graph matching
  - Why needed here: The method uses bipartite matching to resolve cases where multiple blocks share identical foreign keys, ensuring correct alignment of multi-modal information
  - Quick check question: How does the Hungarian algorithm work for bipartite matching, and what is the time complexity?

- Concept: Vision-Language Models (VLMs) and structured output generation
  - Why needed here: The method relies on VLMs to convert visual and textual information into structured attribute dictionaries that can be processed by downstream matching algorithms
  - Quick check question: What are the key architectural differences between VLMs and traditional computer vision or NLP models?

## Architecture Onboarding

- Component map:
  - Input: Archaeological catalog PDF pages
  - Document Localization Module: Open-set object detection with prompts
  - Block Comprehension Module: VLM processing with structured output
  - Block Matching Module: Foreign key matching + distance-based bipartite matching
  - Output: Structured data pairs (image, attributes)

- Critical path: PDF ‚Üí Document Localization ‚Üí Block Comprehension ‚Üí Block Matching ‚Üí Structured Output

- Design tradeoffs:
  - Open-set vs closed-set detection: Open-set provides better generalization but may have lower precision on known classes
  - Foreign key vs distance matching: Foreign key is more reliable when available but distance matching handles complex cases
  - VLM choice: Claude 3.5 Sonnet performs best but may be more expensive than alternatives

- Failure signatures:
  - Poor localization: Visible misalignment between detected boxes and actual content regions
  - Incorrect comprehension: Attributes contain irrelevant or nonsensical information
  - Failed matching: Blocks that should be paired remain unpaired in the output

- First 3 experiments:
  1. Test document localization on a catalog page with varying layouts to verify the open-set detector's generalization
  2. Test block comprehension on individual regions to ensure the VLM generates correct structured attributes
  3. Test the full pipeline on a small catalog with known ground truth to verify end-to-end accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method scale to catalogs with different layouts and structures beyond pottery artifacts?
- Basis in paper: [inferred] The authors demonstrate effectiveness on pottery catalogs but do not test on other artifact types or catalog formats.
- Why unresolved: The paper only validates the approach on pottery catalogs from two specific sites, leaving uncertainty about generalizability to other archaeological materials and document layouts.
- What evidence would resolve it: Experiments applying the method to diverse artifact types (bronze, jade, bone tools) and catalog formats (different publication styles, languages, or archaeological periods).

### Open Question 2
- Question: What is the optimal balance between foreign key matching and distance-based bipartite matching for different catalog types?
- Basis in paper: [explicit] The authors propose a two-stage matching strategy but do not explore parameter tuning or adaptive weighting between the two stages.
- Why unresolved: The paper presents a fixed two-stage approach without investigating how different catalog characteristics might benefit from adjusted matching strategies.
- What evidence would resolve it: Comparative experiments varying the matching strategy parameters across different catalog types and analyzing which combinations work best for specific scenarios.

### Open Question 3
- Question: How can the system be made more robust to low-quality images and degraded catalog pages?
- Basis in paper: [inferred] The evaluation uses clean catalog data, but archaeological documents often suffer from physical degradation, poor digitization quality, or unclear images.
- Why unresolved: The current experiments do not address real-world challenges of working with historical documents that may have stains, fading, or scanning artifacts.
- What evidence would resolve it: Testing the method on intentionally degraded or low-quality catalog images and measuring performance degradation compared to high-quality versions.

## Limitations
- Experimental validation is limited to only two archaeological sites, raising questions about generalizability to other catalog formats and artifact types
- The reported 35.4% average precision, while showing 33.8% improvement, still indicates substantial room for improvement in accuracy
- The method relies heavily on the quality and consistency of foreign key attributes, which may not be available across different archaeological catalogs

## Confidence
**High Confidence:** The three-module pipeline architecture is clearly defined and logically sound. The basic approach of combining open-set detection with VLM comprehension and structured matching follows established patterns in multimodal processing.

**Medium Confidence:** The reported performance improvements (33.8% over baseline) are methodologically sound, but the absolute precision of 35.4% suggests the method is not yet production-ready. The improvement is statistically meaningful but practically limited.

**Low Confidence:** The generalizability of the approach to different archaeological contexts, catalog formats, and artifact types remains unproven. The method's robustness to variations in catalog design and content structure has not been demonstrated beyond the two studied sites.

## Next Checks
1. **Cross-Site Validation Test:** Apply the method to archaeological catalogs from at least three additional sites with different catalog formats and artifact types to assess generalization capability and identify format-specific failure modes.

2. **Ablation Study on Matching Components:** Systematically disable the foreign key matching and distance-based bipartite matching components separately to quantify their individual contributions to the 35.4% AP and identify whether the method is overly dependent on one component.

3. **Error Analysis Classification:** Manually categorize all false positives and false negatives from the current dataset to determine whether errors are primarily due to localization, comprehension, or matching failures, guiding targeted improvements.