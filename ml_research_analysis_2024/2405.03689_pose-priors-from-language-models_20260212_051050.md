---
ver: rpa2
title: Pose Priors from Language Models
arxiv_id: '2405.03689'
source_url: https://arxiv.org/abs/2405.03689
tags:
- pose
- contact
- person
- which
- back
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that large multimodal models can infer contact
  points between body parts in images, which can be converted into loss functions
  to improve 3D pose estimation. The authors propose a framework that prompts an LMM
  to generate structured contact constraints, converts them into a distance-based
  loss, and optimizes it alongside standard pose losses.
---

# Pose Priors from Language Models

## Quick Facts
- arXiv ID: 2405.03689
- Source URL: https://arxiv.org/abs/2405.03689
- Reference count: 40
- This work demonstrates that large multimodal models can infer contact points between body parts in images, which can be converted into loss functions to improve 3D pose estimation.

## Executive Summary
This work demonstrates that large multimodal models (LMMs) can infer contact points between body parts in images, which can be converted into loss functions to improve 3D pose estimation. The authors propose a framework that prompts an LMM to generate structured contact constraints, converts them into a distance-based loss, and optimizes it alongside standard pose losses. On two-person interaction datasets (Hi4D, FlickrCI3D, CHI3D), their method reduces PA-MPJPE by up to 85% relative to heuristic baselines and improves contact accuracy (PCC) by over a third compared to non-supervised methods. In single-person yoga pose refinement (MOYO), it captures ground-truth self-contact better than baselines. The approach requires no training data with contact annotations and leverages LMMs' implicit understanding of physical interaction, showing their potential as scalable priors for contact-aware pose estimation.

## Method Summary
The method extracts contact constraints from a large multimodal model (GPT4-V) without fine-tuning, converts these constraints into distance-based loss functions, and optimizes them jointly with standard pose estimation losses. For each input image, the method generates N constraint sets from the LMM, filters them based on confidence, maps body regions to vertices on a 3D mesh, computes minimum distances as losses, and refines initial pose estimates through joint optimization. The approach works for both one-person (self-contact) and two-person (inter-person contact) scenarios, requiring only bounding boxes as additional input.

## Key Results
- On two-person datasets (Hi4D, FlickrCI3D, CHI3D), ProsePose reduces PA-MPJPE by up to 85% relative to heuristic baselines
- Contact accuracy (PCC) improves by over a third compared to non-supervised methods across two-person datasets
- In single-person yoga poses (MOYO), the method better captures ground-truth self-contact than baselines
- Aggregating 20 LMM samples significantly improves performance by mitigating hallucination effects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LMMs can infer semantic contact constraints between body parts in images
- Mechanism: LMMs trained on large multimodal datasets learn implicit representations of physical interactions between body parts. When prompted with an image, they can output structured descriptions of which body parts are touching.
- Core assumption: LMMs have learned enough about human physical interactions through their training data to accurately identify contact points
- Evidence anchors:
  - [abstract] "Our approach extracts contact-relevant descriptors from an LMM and translates them into tractable losses to constrain 3D human pose optimization"
  - [section 3.2] "Our key insight is to leverage an LMM to identify regions of contact between different body parts on the human body surface"
  - [corpus] Weak evidence - corpus contains related work on pose estimation and contact but no direct evidence LMMs can infer contact constraints
- Break condition: If LMM training data lacks sufficient examples of physical contact descriptions, or if the LMM's multimodal training failed to capture spatial relationships between body parts

### Mechanism 2
- Claim: Aggregating multiple LMM samples reduces hallucination effects
- Mechanism: Since LMMs are prone to hallucination, taking N samples and averaging the resulting loss functions mitigates individual prediction errors by finding consensus across samples
- Core assumption: LMM hallucinations are random and uncorrelated across samples, so averaging will reduce noise while preserving true signal
- Evidence anchors:
  - [section 3.2] "Each constraint set sampled from the LMM is likely to contain noise or hallucination. To mitigate this issue, we average over all N losses corresponding to each constraint set"
  - [abstract] "Despite its simplicity, our method produces compelling reconstructions for both two-person interactions and self-contact scenarios"
  - [corpus] Weak evidence - corpus mentions hallucination as a known LMM issue but provides no specific evidence averaging helps
- Break condition: If LMM hallucinations are systematic or correlated (e.g., consistent misidentifying left/right limbs), averaging will not help

### Mechanism 3
- Claim: Structured loss functions derived from LMM constraints improve pose optimization
- Mechanism: Contact constraints are converted into distance-based loss functions that pull corresponding body parts together, which are optimized alongside standard pose losses
- Core assumption: Distance minimization between predicted contact points will lead to more physically plausible poses
- Evidence anchors:
  - [section 3.2] "We compute a loss for each constraint by mapping the relevant regions to sets of vertices and calculating the minimum distance between vertices in the two sets"
  - [section 3.3] "We jointly optimize the generated loss function with several other pre-defined loss terms"
  - [corpus] Moderate evidence - corpus shows related work on contact-aware pose estimation using similar optimization approaches
- Break condition: If LMM constraints are incorrect or if the loss function formulation doesn't properly capture the desired contact relationships

## Foundational Learning

- Concept: 3D human body models and mesh representations
  - Why needed here: The method converts LMM contact constraints into vertex distances on a 3D mesh representation of human bodies
  - Quick check question: What are the key parameters of the SMPL-X body model used in this work?

- Concept: Multimodal model prompting and structured output extraction
  - Why needed here: The method requires carefully crafted prompts to elicit structured contact constraint information from LMMs
  - Quick check question: How does the prompt instruction to avoid left/right labels affect the subsequent chirality assignment process?

- Concept: Optimization-based pose estimation with multiple loss terms
  - Why needed here: The method jointly optimizes LMM-derived contact losses with other pose estimation losses to refine initial estimates
  - Quick check question: What is the role of the 2D keypoint loss in the overall optimization objective?

## Architecture Onboarding

- Component map: Image → Pose regressor (BEV/HMR2) → Initial 3D pose estimates → LMM sampling (N times) → Constraint extraction and loss generation → Joint optimization with multiple losses → Refined 3D poses
- Critical path: The most critical path is LMM sampling → constraint extraction → loss generation → optimization. Any failure here directly impacts final pose quality.
- Design tradeoffs: Using coarse regions instead of fine-grained ones trades precision for LMM comprehension; aggregating N samples trades computation time for hallucination reduction
- Failure signatures: Poor PCC scores indicate LMM constraints don't match ground truth; high PA-MPJPE indicates optimization issues; consistently empty constraint sets indicate LMM confidence problems
- First 3 experiments:
  1. Run with N=1 sample to establish baseline performance without hallucination mitigation
  2. Run with different values of f (constraint filtering threshold) to find optimal balance between constraint quantity and quality
  3. Run ablation study removing LLMM loss to quantify its contribution versus other losses

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided content.

## Limitations
- Heavy dependence on LMM quality - if GPT4-V's training data lacks sufficient contact examples, constraints will be unreliable
- Assumes LMM hallucinations are random and uncorrelated across samples, which may not hold true
- Coarse region granularity introduces precision trade-offs that could limit effectiveness in scenarios requiring detailed contact information

## Confidence
- Confidence in LMM hallucination mitigation: Medium
- Confidence in generalizability to new datasets: Low
- Confidence in method's effectiveness: Medium

## Next Checks
1. **Ablation on LMM samples (N):** Run experiments with varying values of N (e.g., N=1, N=5, N=10) to quantify the impact of hallucination mitigation through sample aggregation and determine the optimal trade-off between performance and computational cost.

2. **Alternative LMM prompting strategies:** Test different prompt formulations (e.g., including left/right labels, using different visual descriptions) to assess the robustness of the method to prompt variations and identify the most effective prompting strategy.

3. **Generalization to new datasets:** Evaluate the method on additional datasets with different contact scenarios (e.g., sports interactions, crowded scenes) to assess its ability to generalize beyond the four datasets used in the paper.