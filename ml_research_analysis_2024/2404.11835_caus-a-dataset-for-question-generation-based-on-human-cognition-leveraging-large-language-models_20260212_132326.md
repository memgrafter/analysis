---
ver: rpa2
title: 'CAUS: A Dataset for Question Generation based on Human Cognition Leveraging
  Large Language Models'
arxiv_id: '2404.11835'
source_url: https://arxiv.org/abs/2404.11835
tags:
- questions
- question
- scene
- uncertainty
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents the CAUS dataset for question generation based
  on human cognition using large language models (LLMs). The dataset contains 1K scene
  descriptions, 1K reasoning sentences, and 5K questions generated by GPT-4 to address
  uncertainties in the scenes.
---

# CAUS: A Dataset for Question Generation based on Human Cognition Leveraging Large Language Models

## Quick Facts
- arXiv ID: 2404.11835
- Source URL: https://arxiv.org/abs/2404.11835
- Authors: Minjung Shin; Donghyun Kim; Jeh-Kwang Ryu
- Reference count: 16
- Key outcome: LLMs can effectively generate pertinent questions about uncertainties in scene descriptions, with 83.2% K-type and 83.6% Q-type classification accuracy

## Executive Summary
This paper presents CAUS, a dataset for question generation based on human cognitive processes for resolving uncertainties. The dataset contains 1K scene descriptions, 1K reasoning sentences, and 5K questions generated by GPT-4 to address uncertainties embedded in the scenes. Questions are classified along two dimensions: knowledge type (K-type) and question type (Q-type). The study demonstrates that LLMs can effectively generate pertinent questions and grasp their nuances when given appropriate context and instructions, suggesting the potential of incorporating human-like questioning into AI models to improve their ability to manage uncertainties.

## Method Summary
The CAUS dataset was created through a multi-step process using GPT-4. First, scene descriptions containing intentional inconsistencies were generated using zero-shot instructions. These descriptions were then curated by human annotators to remove biases and ensure realistic uncertainty. Next, GPT-4 generated reasoning sentences that identified the uncertainties in each scene. Finally, questions addressing these uncertainties were generated and classified into two dimensions: knowledge type (K-type) and question type (Q-type). The dataset was evaluated through human annotation, with 83.2% and 83.6% match between model and human ground truth for K-type and Q-type classifications respectively.

## Key Results
- Over 90% of reasoning sentences accurately pointed out uncertainties in scene descriptions
- Question classification achieved 83.2% and 83.6% match with human-generated ground truth for K-type and Q-type questions respectively
- The dataset contains 1K scenes, 1K reasoning sentences, and 5K questions addressing uncertainties

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CAUS dataset leverages scene descriptions embedded with uncertainties to stimulate LLM reasoning and question generation.
- Mechanism: LLMs are provided structured context (scene descriptions) and tasked with identifying uncertain elements through reasoning sentences, then generating questions that target these uncertainties.
- Core assumption: LLMs can perform multi-step reasoning when given clear task framing and appropriate context.
- Evidence anchors: [abstract] "Our approach involves providing scene descriptions embedded with uncertainties to stimulate the generation of reasoning and queries." [section] "Scene Description texts as the starting point for question generations... We crafted scene description texts containing intentional inconsistencies."
- Break condition: If LLM fails to identify uncertain elements from descriptions or generates off-topic questions.

### Mechanism 2
- Claim: Two-dimensional question classification (K-type and Q-type) improves evaluation precision and captures question nuance.
- Mechanism: Questions are classified by knowledge type (what information is missing) and question type (how inquiry is expressed), enabling granular analysis of question properties.
- Core assumption: Structured classification of question types correlates with human judgment and captures meaningful distinctions.
- Evidence anchors: [section] "Alongside the query generation, we conducted 2-dimensional classifications of the generated questions referring to our prior work (Shin et al., 2023)." [section] "Evaluation on Question Classification... There was an 83.2% match (416 questions) between the model and human ground truth for the K-type questions. Similarly, for Q-type questions, an 83.6% match (418 questions) was observed."
- Break condition: If classification accuracy drops below 70% or human evaluators disagree significantly with automated classification.

### Mechanism 3
- Claim: Iterative prompt refinement and human-in-the-loop curation improve dataset quality and reduce bias.
- Mechanism: Researchers review and modify generated scenes, remove duplicates, and ensure uncertainty is well-formed before final dataset creation.
- Core assumption: Human oversight is necessary to ensure LLM-generated content meets quality standards and research objectives.
- Evidence anchors: [section] "After gaining excessive sentences, we checked for similar sentences using the cosine similarity algorithm, and two researchers reviewed them to remove duplicates." [section] "The filtered list was then finalized by a third researcher to establish a list of scenes that contained uncertainty."
- Break condition: If human review becomes bottlenecked or introduces significant bias in scene selection.

## Foundational Learning

- Concept: Uncertainty representation in narrative contexts
  - Why needed here: Understanding how to embed and identify uncertainty in text is fundamental to creating effective scene descriptions
  - Quick check question: What makes a scene description contain uncertainty rather than just being surprising or unusual?

- Concept: Multi-dimensional question classification
  - Why needed here: Proper classification enables nuanced analysis of question types and their effectiveness
  - Quick check question: How do K-type and Q-type classifications differ in what they capture about a question?

- Concept: Prompt engineering for LLM task specification
  - Why needed here: Precise prompts are critical for getting LLMs to generate reasoning sentences and questions as intended
  - Quick check question: What temperature settings would you use for generating diverse versus consistent LLM outputs?

## Architecture Onboarding

- Component map: Scene Description → Reasoning Generation → Question Generation → Two-dimensional Classification → Human Evaluation → Dataset Finalization
- Critical path: Human curation of scene descriptions → LLM reasoning generation → LLM question generation → Classification validation → Final dataset assembly
- Design tradeoffs: Model performance vs. human effort in curation; question diversity vs. classification consistency; dataset size vs. quality control
- Failure signatures: Low classification accuracy (below 70%), reasoning sentences that don't point to specific uncertainties, questions that are off-topic or malformed
- First 3 experiments:
  1. Generate 100 scene descriptions with temperature 0.7, manually review for uncertainty quality and diversity
  2. Run reasoning generation with temperature 0, measure accuracy of uncertainty identification on 20% sample
  3. Generate questions for same scenes, classify with both LLM and human annotators, compare agreement rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop model-agnostic prompting techniques that work consistently across different large language models for question generation tasks?
- Basis in paper: Explicit - The paper acknowledges that their approach is highly sensitive to specific prompting and may not yield consistent results with different models.
- Why unresolved: Different LLMs have varying architectures, training data, and response patterns, making it challenging to create universal prompts that work effectively across all models.
- What evidence would resolve it: Empirical studies demonstrating consistent performance of question generation across multiple LLM architectures using the same prompting strategy.

### Open Question 2
- Question: How can we incorporate social interaction elements and desires into question generation models to better emulate human-like questioning behavior?
- Basis in paper: Explicit - The paper mentions this as a limitation, stating that their work does not address the role of desires and social interactions in human questioning.
- Why unresolved: Current models focus on information-seeking questions, but human questioning is influenced by social dynamics, emotions, and personal motivations that are not yet integrated into AI systems.
- What evidence would resolve it: Development and evaluation of question generation models that successfully incorporate social and motivational factors, demonstrated through improved performance in real-world interaction scenarios.

### Open Question 3
- Question: What metrics can be developed to evaluate the quality of generated questions beyond classification accuracy, particularly in terms of their usefulness in resolving uncertainty?
- Basis in paper: Explicit - The paper notes that evaluation criteria for question quality tend to be either mechanical (similarity-based) or subjective (human ranking), and suggests this as an area for improvement.
- Why unresolved: Current evaluation methods focus on classification accuracy and surface-level matching with ground truth, but don't capture the practical utility of questions in helping resolve uncertainty.
- What evidence would resolve it: Development of comprehensive evaluation frameworks that include measures of question utility, effectiveness in uncertainty resolution, and real-world applicability, validated through user studies and practical applications.

## Limitations
- Dataset size (1K scenes, 5K questions) is relatively small for a benchmark dataset
- Evaluation relies heavily on human annotation without specifying inter-rater reliability metrics
- Prompts and instructions for GPT-4 generation are not fully detailed, affecting reproducibility

## Confidence
- **High**: The core methodology of using scene descriptions with embedded uncertainties to generate questions is sound and well-executed
- **Medium**: The evaluation results are internally consistent but limited by sample size and lack of external validation
- **Low**: Generalizability of the approach to other domains and the long-term stability of LLM-generated datasets

## Next Checks
1. Test the dataset with a different LLM model (not GPT-4) to verify consistency of question generation and classification performance
2. Conduct inter-rater reliability analysis on the human annotations to establish measurement precision
3. Evaluate question quality and classification accuracy on a subset of scenes with different levels of uncertainty complexity