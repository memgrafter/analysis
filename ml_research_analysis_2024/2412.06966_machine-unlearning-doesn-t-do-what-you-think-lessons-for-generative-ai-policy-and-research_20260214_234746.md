---
ver: rpa2
title: 'Machine Unlearning Doesn''t Do What You Think: Lessons for Generative AI Policy
  and Research'
arxiv_id: '2412.06966'
source_url: https://arxiv.org/abs/2412.06966
tags:
- information
- unlearning
- data
- section
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes machine unlearning for generative AI, highlighting
  fundamental mismatches between technical methods and policy goals. It argues that
  while unlearning aims to remove unwanted information from AI models, the technical
  implementation deviates from intuitive understandings of "removal." The paper distinguishes
  between observed information (explicitly in training data), latent information (derived
  from learned patterns), and higher-order concepts (complex abstractions).
---

# Machine Unlearning Doesn't Do What You Think: Lessons for Generative AI Policy and Research

## Quick Facts
- arXiv ID: 2412.06966
- Source URL: https://arxiv.org/abs/2412.06966
- Reference count: 40
- One-line primary result: Machine unlearning methods fail to meet policy goals due to fundamental mismatches between technical implementation and intuitive understanding of information removal

## Executive Summary
This paper analyzes machine unlearning for generative AI, highlighting fundamental mismatches between technical methods and policy goals. It argues that while unlearning aims to remove unwanted information from AI models, the technical implementation deviates from intuitive understandings of "removal." The paper distinguishes between observed information (explicitly in training data), latent information (derived from learned patterns), and higher-order concepts (complex abstractions). It demonstrates that removing observed information doesn't guarantee effective output suppression, as models can still generate similar content through generalization. The analysis applies to privacy (data deletion requests), copyright (substantial similarity concerns), and safety (dual-use risks) contexts, concluding that unlearning is not a general-purpose solution for constraining generative AI behavior and requires careful consideration of specific policy goals and reasonable expectations for technical methods.

## Method Summary
This paper provides a conceptual analysis rather than empirical experiments. The method involves mapping different types of information targets (observed, latent, higher-order concepts) to four identified mismatches between unlearning methods and policy goals. The analysis then applies these mismatches to privacy, copyright, and safety contexts to demonstrate how technical limitations affect real-world policy objectives. The approach synthesizes existing literature on machine learning generalization, privacy engineering, and copyright law to create a framework for understanding when and why unlearning may or may not be appropriate for specific policy goals.

## Key Results
- Removing observed information from training data does not guarantee removal from model outputs due to generalization
- Output suppression methods don't actually remove information from model parameters, only modify generation behavior
- Models and their outputs are fundamentally different artifacts with different properties
- Unlearning methods face fundamental limitations in distinguishing between transformative fair uses and non-transformative superseding uses
- The paper identifies four fundamental mismatches between unlearning methods and policy goals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing observed information from training data does not guarantee removal from model outputs
- Mechanism: During training, data transforms into patterns encoded in parameters; these patterns cannot be cleanly targeted for deletion
- Core assumption: Model parameters store learned patterns rather than discrete pieces of information
- Evidence anchors:
  - [abstract] "removing observed information doesn't guarantee effective output suppression, as models can still generate similar content through generalization"
  - [section 2.1] "data are transformed into patterns that get encoded in the model's parameters—patterns that are not directly or easily interpretable"
  - [corpus] Strong evidence from multiple papers discussing generalization and memorization

### Mechanism 2
- Claim: Output suppression methods don't actually remove information from model parameters
- Mechanism: These methods modify generation behavior without changing what the model has learned
- Core assumption: Generation-time interventions can prevent certain outputs without affecting underlying knowledge
- Evidence anchors:
  - [abstract] "output suppression" is distinguished from removal of information from model parameters
  - [section 4.2] "output-suppression methods leave the trained generative-AI model unchanged and instead take effect in the generative-AI system"
  - [corpus] Consistent distinction between parameter modification and output filtering across literature

### Mechanism 3
- Claim: Models and their outputs are fundamentally different artifacts with different properties
- Mechanism: A model is a parameterized function; outputs are specific instantiations of that function
- Core assumption: The relationship between parameters and outputs is not one-to-one
- Evidence anchors:
  - [abstract] "Models are not equivalent to their outputs" (Mismatch 3)
  - [section 5] "it is typical to evaluate the success of an unlearning method not by examining changes in the model's parameters, but by prompting the model"
  - [corpus] Literature consistently treats models and outputs as separate evaluation targets

## Foundational Learning

- Concept: Training data transformation into model parameters
  - Why needed here: Understanding why information cannot be "deleted" like from a database
  - Quick check question: If you remove an image from training data, can you guarantee the model never generates anything similar? Why or why not?

- Concept: Generalization vs memorization in machine learning
  - Why needed here: Explains why removing specific examples doesn't prevent similar outputs
  - Quick check question: What's the difference between a model memorizing "555-123-4567" exactly versus learning patterns about phone numbers?

- Concept: Back-end vs front-end distinctions in generative AI systems
  - Why needed here: Clarifies where different unlearning methods operate
  - Quick check question: If you add an output filter to a system, have you changed the underlying model? What has changed?

## Architecture Onboarding

- Component map:
  - Training pipeline: data → preprocessing → model architecture → training algorithm → parameters
  - Generation pipeline: parameters → inference engine → prompt processing → output filtering → user-facing output
  - Unlearning methods: either modify training pipeline (removal) or generation pipeline (suppression)

- Critical path:
  1. Identify target information (observed, latent, or higher-order concept)
  2. Choose appropriate unlearning method (removal vs suppression)
  3. Implement method at correct pipeline stage
  4. Evaluate effectiveness on both parameters and outputs

- Design tradeoffs:
  - Removal: stronger guarantees but computationally expensive, limited to observed information
  - Suppression: computationally efficient but weaker guarantees, applies to all information types
  - System-level filters: easy to implement but require maintaining potentially sensitive information

- Failure signatures:
  - Model still generates target content despite removal attempts
  - Output filters miss content they should catch
  - Over-broad removal degrades model utility significantly
  - Unlearning introduces new failure modes in unrelated capabilities

- First 3 experiments:
  1. Train model A on dataset X, remove specific example from dataset, retrain from scratch to create model B, compare outputs
  2. Apply output suppression to model A, compare filtered vs unfiltered outputs for target content
  3. Remove example from training data, apply both removal and suppression methods, evaluate which better prevents generation of similar content

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can unlearning methods effectively prevent the generation of substantially similar copyrighted content while preserving transformative fair use capabilities?
- Basis in paper: [explicit] The paper discusses challenges in distinguishing between transformative fair uses and non-transformative superseding uses, noting that current unlearning methods cannot make this distinction.
- Why unresolved: Courts struggle with this distinction in copyright law, and technical methods lack the contextual understanding to determine whether generated content constitutes fair use.
- What evidence would resolve it: Empirical studies comparing generations from unlearned models against both copyrighted works and their fair-use derivatives, coupled with legal analysis of whether the distinctions made by technical methods align with fair use doctrine.

### Open Question 2
- Question: What constitutes "reasonable best efforts" for output suppression in different legal and policy contexts?
- Basis in paper: [explicit] The paper argues that policymakers and judges will need to set reasonable expectations for imperfect outcomes of best-effort implementations of unlearning methods.
- Why unresolved: Different legal domains have varying standards for what constitutes sufficient compliance, and the technical limitations of suppression methods make it unclear what level of imperfection is acceptable.
- What evidence would resolve it: Case studies of enforcement actions against model developers, comparative analysis of different jurisdictions' approaches to evaluating suppression effectiveness, and empirical data on the relationship between suppression effort and output control.

### Open Question 3
- Question: How can machine unlearning methods address latent information and higher-order concepts without being overly broad or under-inclusive?
- Basis in paper: [explicit] The paper highlights the challenge of determining which observed information contributes to latent information and higher-order concepts, and how to set appropriate boundaries for removal.
- Why unresolved: The relationship between observed training data and the complex abstractions models learn is not well understood, making it difficult to target specific concepts without affecting related legitimate content.
- What evidence would resolve it: Mechanistic interpretability studies mapping specific model behaviors to training data, ablation studies showing the effects of removing different sets of training examples, and evaluation frameworks for measuring the precision of concept targeting.

## Limitations
- The paper does not provide empirical validation of its claims, relying instead on theoretical arguments
- Potential oversimplification of complex unlearning methods that may achieve more than the paper suggests
- Does not explore future technical advances that could narrow identified gaps between policy goals and technical capabilities

## Confidence
- High: Core claim that machine unlearning doesn't do what you think is supported by well-established properties of machine learning
- Medium: Practical implications for specific policy contexts may vary based on implementation details not fully explored
- Low: Confidence in specific numerical predictions or quantitative effectiveness measures

## Next Checks
1. Conduct controlled experiments comparing different unlearning methods (removal vs suppression) on identical models and datasets to quantify their effectiveness at preventing generation of similar content
2. Survey practitioners implementing unlearning to identify which specific mismatches they encounter most frequently in real-world applications
3. Analyze the computational and utility costs of various unlearning approaches to better understand the tradeoffs between privacy/safety goals and model performance degradation