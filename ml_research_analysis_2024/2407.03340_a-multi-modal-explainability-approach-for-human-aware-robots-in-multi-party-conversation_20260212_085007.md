---
ver: rpa2
title: A Multi-Modal Explainability Approach for Human-Aware Robots in Multi-Party
  Conversation
arxiv_id: '2407.03340'
source_url: https://arxiv.org/abs/2407.03340
tags:
- robot
- ulness
- rust
- verbal
- addressee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work proposes a novel attention-based explainable addressee\
  \ estimation (AE) model for multi-party conversation, deployed in a modular robotic\
  \ architecture for human-robot interaction. The model combines face and pose inputs\
  \ via attention mechanisms to estimate the addressee\u2019s direction (robot, left,\
  \ right) while generating inherent explanations, including image saliency maps,\
  \ modality importance scores, and temporal frame contributions."
---

# A Multi-Modal Explainability Approach for Human-Aware Robots in Multi-Party Conversation

## Quick Facts
- arXiv ID: 2407.03340
- Source URL: https://arxiv.org/abs/2407.03340
- Reference count: 21
- Primary result: 79.40% F1-score on addressee estimation, surpassing prior SOTA by 4.45%

## Executive Summary
This work introduces an attention-based explainable addressee estimation (AE) model for multi-party conversation, implemented in a modular robotic architecture for human-robot interaction. The model combines face and pose inputs via attention mechanisms to estimate the addressee’s direction (robot, left, right) while generating inherent explanations, including image saliency maps, modality importance scores, and temporal frame contributions. Evaluated on the Vernissage dataset, the model achieved 79.40% F1-score, surpassing the previous state-of-the-art by 4.45%. Deployed on an iCub robot, it maintained strong performance (85.56% F1) in real-time multi-party interactions, with spatial memory corrections improving accuracy to 86.09%. A user study (n=60) revealed that verbal and graphical explanations were more satisfying and useful than embodied ones, with usefulness positively correlating with both affective and cognitive trust. The approach demonstrates how explainable AI enhances transparency and trust in social robots.

## Method Summary
The model employs a multi-modal architecture combining vision transformers (ViT) for face embeddings and MLPs for pose embeddings, fused via learned attention weights. A GRU with attention processes the combined embeddings over time to estimate the addressee per utterance. The system is deployed in a modular cognitive architecture with a spatial memory module that corrects estimates based on real-time person tracking. Explanations are generated from attention maps, modality importance scores, and temporal contributions, and are delivered via verbal, embodied, and graphical modalities. The model was trained on the Vernissage dataset and evaluated both offline and in real-time robot deployments.

## Key Results
- Achieved 79.40% F1-score on Vernissage dataset, a 4.45% improvement over prior state-of-the-art
- Real-time deployment on iCub robot yielded 85.56% F1-score, with spatial memory corrections improving accuracy to 86.09%
- User study (n=60) showed verbal and graphical explanations were more satisfying and useful than embodied ones, with usefulness positively correlating with trust

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention-based architecture yields inherently explainable intermediate representations that can be visualized in real time without post-hoc processing.
- Mechanism: The model employs vision transformers and gated recurrent units with attention mechanisms. At each stage—image patch attention, modality fusion scores, and temporal attention—the weights directly reflect the model's decision process and can be extracted during the forward pass.
- Core assumption: Attention scores correlate meaningfully with human-understandable importance in the task.
- Evidence anchors:
  - [abstract] "modify this model to include inherently explainable attention-based segments"
  - [section] "we alter the penultimate processing step using a tailored attention mechanism in the recurrent neural network"
  - [corpus] Weak; no neighbor papers explicitly report real-time explainable AI in multi-party HRI.
- Break condition: If attention weights are dominated by noise or do not align with salient visual cues, the explanations become misleading.

### Mechanism 2
- Claim: Multi-modal fusion with learned attention weights allows the model to prioritize either face or pose cues per frame, improving classification robustness.
- Mechanism: Face embeddings and pose vectors are scored separately using learned linear projections, then combined via weighted addition. This permits dynamic emphasis on either modality based on context.
- Core assumption: Different frames benefit from different modality emphasis, and the network can learn this pattern.
- Evidence anchors:
  - [abstract] "combines face and pose inputs via attention mechanisms"
  - [section] "calculate their relative contributions sf t spt...using their corresponding weights"
  - [corpus] Weak; neighbors focus on fusion but not on per-frame modality attention.
- Break condition: If modality importance scores are uniform or irrelevant, the fusion adds no value.

### Mechanism 3
- Claim: Deploying the model in a modular architecture with short-term spatial memory allows corrections to addressee estimates based on real-time environmental context, improving real-world accuracy.
- Mechanism: The XAE model outputs estimates per utterance; the Spatial Memory module cross-checks these against remembered person locations and corrects misclassifications when context contradicts the estimate.
- Core assumption: The robot's egocentric spatial model is accurate enough to inform corrections.
- Evidence anchors:
  - [abstract] "implemented the explainable addressee estimation as part of a modular cognitive architecture"
  - [section] "spatiotemporal information of the environment, following a more cognitively-inspired approach"
  - [corpus] Weak; no direct neighbor evidence for memory-corrected addressee estimation.
- Break condition: If spatial memory is noisy or out of date, corrections may degrade accuracy.

## Foundational Learning

- Concept: Vision Transformer (ViT) architecture and multi-head self-attention
  - Why needed here: ViT replaces CNN to allow direct extraction of attention maps for explainability.
  - Quick check question: Can you describe how a ViT splits an image into patches and applies self-attention?

- Concept: Attention-based modality fusion
  - Why needed here: Enables dynamic weighting between face and pose data streams per time frame.
  - Quick check question: What is the difference between simple concatenation and learned attention-based fusion?

- Concept: Recurrent neural networks with attention (e.g., GRU with attention)
  - Why needed here: Captures temporal dependencies in conversational utterances and produces per-frame importance scores.
  - Quick check question: How does attention over time steps differ from attention over image patches?

## Architecture Onboarding

- Component map:
  - Face detection → Face crop
  - Pose extraction → Pose vector
  - ViT forward pass → Face embedding
  - MLP forward pass → Pose embedding
  - Fusion attention → Combined embedding
  - GRU with attention → Utterance embedding
  - Final FC layer → Addressee class
  - Spatial Memory check → Optional correction
  - Explanation generation → User-facing output

- Critical path:
  1. Face detection → face crop
  2. Pose extraction → pose vector
  3. ViT forward pass → face embedding
  4. MLP forward pass → pose embedding
  5. Fusion attention → combined embedding
  6. GRU with attention → utterance embedding
  7. Final FC layer → addressee class
  8. Spatial Memory check → optional correction
  9. Explanation generation → user-facing output

- Design tradeoffs:
  - Attention-based vs. simple concatenation: Attention adds explainability but increases complexity.
  - Single vs. multiple modality streams: More streams improve robustness but raise data requirements.
  - Real-time correction vs. final estimate only: Corrections improve accuracy but require extra computation.

- Failure signatures:
  - Attention maps show no clear saliency → attention mechanism not learning useful patterns.
  - Confusion matrix dominated by "robot" class → model biased toward self-addressing.
  - Spatial memory shows outdated positions → correction mechanism may introduce errors.

- First 3 experiments:
  1. Ablation: Remove attention from fusion step and compare accuracy and explanation quality.
  2. Modality importance: Visualize face vs. pose attention weights across utterances.
  3. Memory correction: Measure F1-score before and after applying spatial memory corrections.

## Open Questions the Paper Calls Out
None

## Limitations

- Claims about explanation effectiveness and trust enhancement rely on self-reported satisfaction without objective comprehension metrics or controlled comparison to baseline systems.
- The model's real-world applicability is limited by evaluation on a single dataset (Vernissage) and a single robot platform (iCub).
- The user study lacks details on demographic diversity, controlled conditions, or long-term interaction assessment.

## Confidence

- **High**: Model achieves SOTA on Vernissage dataset (79.40% F1, +4.45% over prior work).
- **Medium**: Real-time deployment performance (85.56% F1) reflects dataset transfer but may not generalize to other environments or robots.
- **Low**: Claims about explanation effectiveness and trust enhancement rely on self-reported satisfaction without objective comprehension metrics or controlled comparison to baseline systems.

## Next Checks

1. **Generalization Test**: Evaluate the model on a different multi-party conversation dataset (e.g., Friends-MMC) to confirm cross-dataset robustness.
2. **Explanation Comprehension**: Conduct a controlled study comparing user understanding and trust with and without explanations, using objective comprehension questions.
3. **Memory Correction Validation**: Systematically vary the accuracy of the spatial memory module to quantify its impact on addressee estimation performance and identify failure modes.