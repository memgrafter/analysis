---
ver: rpa2
title: Unfamiliar Finetuning Examples Control How Language Models Hallucinate
arxiv_id: '2403.05612'
source_url: https://arxiv.org/abs/2403.05612
tags:
- reward
- finetuning
- unfamiliar
- examples
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how language models hallucinate by analyzing\
  \ their behavior on unfamiliar queries\u2014those involving concepts beyond the\
  \ model's pretraining knowledge. The authors find that a model's hallucinated responses\
  \ tend to mimic the responses associated with its unfamiliar finetuning examples."
---

# Unfamiliar Finetuning Examples Control How Language Models Hallucinate

## Quick Facts
- arXiv ID: 2403.05612
- Source URL: https://arxiv.org/abs/2403.05612
- Reference count: 40
- Models' hallucinated responses mirror the distribution of their unfamiliar finetuning examples

## Executive Summary
This paper investigates how language models hallucinate by analyzing their behavior on unfamiliar queries—those involving concepts beyond the model's pretraining knowledge. The authors find that a model's hallucinated responses tend to mimic the responses associated with its unfamiliar finetuning examples. This suggests that by controlling how unfamiliar examples are supervised during finetuning, we can influence a model's behavior on unfamiliar queries.

The paper validates this observation through controlled experiments on multiple-choice (MMLU) and short-form (TriviaQA) question answering tasks using supervised finetuning, reinforcement learning, and reward model finetuning. The results show that model predictions indeed default toward the distribution of responses in their unfamiliar finetuning examples as test inputs become more unfamiliar.

Building on these insights, the authors develop a method for learning more reliable reward models for RL factuality finetuning of long-form generations. They propose "conservative reward models" that avoid overestimating rewards for unfamiliar inputs, which can undermine the effectiveness of RL finetuning. Experiments on biography and book/movie plot generation tasks show that using conservative reward models significantly reduces hallucinations and improves the factuality of model generations compared to standard supervised finetuning and RL with standard reward models.

## Method Summary
The paper uses Llama2 7B as the pretrained model and conducts controlled experiments involving supervised finetuning (SFT), reinforcement learning (RL), and reward model finetuning on MMLU and TriviaQA datasets. For long-form generation, they use WikiBios and WikiPlots datasets with both standard and conservative reward models. The RL finetuning uses the PPO algorithm, while reward models are trained to predict scalar rewards. Conservative reward models are trained using responses from the same pretrained model to avoid overestimated rewards.

## Key Results
- Model predictions default toward the distribution of responses in their unfamiliar finetuning examples as test inputs become more unfamiliar
- Conservative reward models significantly reduce hallucinations in RL finetuning by avoiding overestimated rewards
- RL with conservative reward models improves factuality of long-form generations compared to standard SFT and RL with standard reward models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unfamiliar finetuning examples shape how language models hallucinate during test time
- Mechanism: During finetuning, the model minimizes loss by predicting responses that match the distribution of its unfamiliar finetuning examples (Punf(y)). At test time, for unfamiliar inputs, the model defaults to this learned distribution.
- Core assumption: The model treats unfamiliar examples as input-agnostic during finetuning and learns a constant prediction (Punf(y)) that minimizes aggregate loss over these examples
- Evidence anchors:
  - [abstract]: "we find that an LLM's hallucinated predictions tend to mirror the responses associated with its unfamiliar finetuning examples"
  - [section]: "we hypothesize that LLMs learn to predict this intelligent 'blind guess' (Punf(y)) for unfamiliar examples during finetuning, and that they default to this prediction when faced with unfamiliar queries at test time"
  - [corpus]: Weak - corpus doesn't directly address the mechanism of unfamiliar examples shaping hallucinations
- Break condition: If the model learns to always output "I don't know" for all unfamiliar examples, it won't exhibit this default behavior

### Mechanism 2
- Claim: Conservative reward models reduce hallucinations in RL finetuning by avoiding overestimated rewards
- Mechanism: Conservative reward models are trained on data collected from the same pretrained model, ensuring that unfamiliar examples in the training data have low rewards. This causes the reward model to predict low rewards for unfamiliar inputs at test time, preventing overestimation.
- Core assumption: When the reward model and data-collection model share the same knowledge base, unfamiliar queries produce factually incorrect responses with low rewards
- Evidence anchors:
  - [abstract]: "we find that overestimated reward predictions tend to be more harmful than underestimated reward predictions"
  - [section]: "by strategically configuring the model's unfamiliar finetuning examples to consist of only low rewards, the model will learn to produce low rewards for unfamiliar inputs at test time"
  - [corpus]: Weak - corpus doesn't specifically address conservative reward models
- Break condition: If the data-collection model produces factually correct responses to unfamiliar queries, the conservative reward model may still overestimate rewards

### Mechanism 3
- Claim: Reward model hallucinations can undermine RL factuality finetuning effectiveness
- Mechanism: If a reward model overestimates rewards for unfamiliar inputs, RL finetuning may encourage the model to generate more incorrect information instead of avoiding inaccuracies.
- Core assumption: The reward function decomposes responses into facts and assigns higher rewards for correct facts and lower rewards for incorrect facts
- Evidence anchors:
  - [abstract]: "reward models themselves can suffer from hallucinations in the face of unfamiliar inputs, which can diminish the efficacy of RL factuality finetuning"
  - [section]: "If, however, a reward model mistakenly labels the incorrect fact as true and favors the incorrect response instead, RL finetuning may unintentionally encourage the model to generate even more incorrect information"
  - [corpus]: Weak - corpus doesn't specifically address reward model hallucinations in RL finetuning
- Break condition: If the reward model consistently underestimates rewards for all inputs, RL finetuning may not learn effectively

## Foundational Learning

- Concept: Distribution shift and out-of-distribution (OOD) behavior
  - Why needed here: Understanding how models behave on unfamiliar inputs (OOD) is central to the paper's investigation of hallucinations
  - Quick check question: What happens to a neural network's predictions as inputs become more OOD from its training distribution?

- Concept: Reinforcement Learning from Human Feedback (RLHF) and reward modeling
  - Why needed here: The paper investigates RL finetuning strategies and the role of reward models in controlling hallucinations
  - Quick check question: How does a reward model provide supervision for RL finetuning when ground truth rewards are unavailable?

- Concept: Factuality evaluation metrics (e.g., FActScore)
  - Why needed here: The paper uses automated factuality evaluation to measure the effectiveness of hallucination reduction techniques
  - Quick check question: What is FActScore and how does it decompose responses into atomic facts for factuality evaluation?

## Architecture Onboarding

- Component map: Pretrained LLM (e.g., Llama2 7B) → Supervised Finetuning (SFT) → Reward Model → RL Finetuning (PPO) → Factuality Evaluation (FActScore)
- Critical path: Pretrained LLM → Reward Model → RL Finetuning → Factuality Evaluation
  - This path is critical because the reward model's behavior directly impacts the effectiveness of RL finetuning in reducing hallucinations
- Design tradeoffs:
  - Conservative vs. standard reward models: Conservative models require more data collection but reduce overestimation, while standard models are easier to train but may overestimate
  - SFT vs. RL finetuning: SFT is simpler but may require expensive custom target responses, while RL uses rewards but is more complex and sensitive to reward model behavior
- Failure signatures:
  - Reward model overestimates rewards → RL finetuning generates more incorrect information
  - Reward model underestimates rewards → RL finetuning learns slowly or not at all
  - SFT on unfamiliar examples → Model generates detailed but incorrect responses
- First 3 experiments:
  1. Verify that model predictions default to Punf(y) for unfamiliar inputs using MMLU with different reward distributions
  2. Compare standard vs. conservative reward models on held-out samples from SFT model
  3. Evaluate RL with standard vs. conservative reward models on biography and plot generation tasks using FActScore

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do language models behave in the "middle ground" between fully familiar and fully unfamiliar queries, and can this behavior be characterized and predicted?
- Basis in paper: The authors mention that "our conceptual model explains a model's behavior for entirely unfamiliar examples, many real-world queries fall within a spectrum of partial familiarity."
- Why unresolved: The paper focuses on understanding model behavior for completely unfamiliar queries, but real-world scenarios often involve queries with varying degrees of familiarity.
- What evidence would resolve it: Experiments characterizing model predictions and accuracy across a range of unfamiliarity scores, not just the extremes. Analyzing how prediction distributions shift as queries become partially familiar.

### Open Question 2
- Question: Can the insights from this paper be extended to improve factuality in more general prompted generation tasks beyond the specific applications studied (biography and plot generation)?
- Basis in paper: The authors state "our experiments focused on models finetuned for specific applications (e.g., biography generation). Extending factuality finetuning to more general prompted generation tasks would be useful."
- Why unresolved: The paper's experiments are limited to specific long-form generation tasks, leaving open the question of generalizability to other tasks.
- What evidence would resolve it: Applying the conservative reward model approach to other generation tasks like summarization, dialogue, or creative writing, and measuring the impact on factuality and other metrics.

### Open Question 3
- Question: Are there alternative strategies for learning conservative reward models that are more effective or efficient than the one proposed in the paper?
- Basis in paper: The authors mention "While we focus on this particular strategy for our experiments, there may be a number of other strategies that can also be effective for learning conservative reward models."
- Why unresolved: The paper presents one specific method for learning conservative reward models but does not explore other potential approaches.
- What evidence would resolve it: Comparing the performance of the proposed method with other strategies, such as using different data collection models, incorporating external knowledge, or employing different reward modeling architectures.

## Limitations
- Unfamiliarity definition granularity: The paper's unfamiliarity measure aggregates over entire input examples but doesn't capture which specific parts of a query make it unfamiliar
- Reward model training data assumptions: The conservative reward model approach assumes that responses from the same pretrained model to unfamiliar queries will consistently receive low rewards
- Generalization to other domains: While the paper demonstrates effectiveness on biography and plot generation tasks, the approach's performance on other long-form generation domains remains unverified

## Confidence
- High confidence in the core observation that model behavior on unfamiliar inputs correlates with the distribution of responses in unfamiliar finetuning examples
- Medium confidence in the conservative reward model approach for RL factuality finetuning
- Medium confidence in the mechanism explaining why unfamiliar examples control hallucinations

## Next Checks
1. Design experiments that systematically vary the degree of familiarity within single inputs to test whether the model's hallucination patterns correlate with specific unfamiliar components
2. Evaluate whether conservative reward models trained on responses from one pretrained model generalize to reward unfamiliar inputs from different models with similar pretraining corpora
3. Apply the conservative reward model approach to at least two additional long-form generation domains to assess generalizability and identify domain-specific considerations for the method