---
ver: rpa2
title: 'Align Attention Heads Before Merging Them: An Effective Way for Converting
  MHA to GQA'
arxiv_id: '2412.20677'
source_url: https://arxiv.org/abs/2412.20677
tags:
- heads
- arxiv
- attention
- pruning
- grouping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method for converting multi-head attention
  (MHA) to grouped-query attention (GQA) in large language models (LLMs) to reduce
  key-value cache memory usage. The approach uses Procrustes analysis to align attention
  heads within groups, maximizing their similarity while preserving computational
  invariance, followed by L0 regularization to prune redundant parameters.
---

# Align Attention Heads Before Merging Them: An Effective Way for Converting MHA to GQA
## Quick Facts
- **arXiv ID:** 2412.20677
- **Source URL:** https://arxiv.org/abs/2412.20677
- **Reference count:** 23
- **Primary result:** Achieves up to 87.5% compression of KV heads in LLMs while maintaining acceptable performance degradation

## Executive Summary
This paper addresses the challenge of reducing key-value cache memory consumption in large language models by converting multi-head attention (MHA) to grouped-query attention (GQA). The authors propose a method that first aligns attention heads within groups using Procrustes analysis to maximize their similarity while preserving computational invariance, then applies L0 regularization to prune redundant parameters. Experiments on LLaMA2-7B and Sheared-LLaMA-1.3B demonstrate successful conversion with significant compression ratios while maintaining model performance across multiple benchmarks.

## Method Summary
The proposed approach converts MHA to GQA through a two-stage process. First, it employs Procrustes analysis to align attention heads within groups, maximizing their similarity while maintaining computational invariance. This alignment ensures that heads within the same group produce similar outputs, making them candidates for sharing key-value pairs. Second, L0 regularization is applied to prune redundant parameters, effectively reducing the number of unique KV heads needed. The method is designed to be general and cost-effective, allowing for arbitrary compression ratios while maintaining acceptable performance degradation. The approach is validated on LLaMA2-7B and Sheared-LLaMA-1.3B models across four benchmarks.

## Key Results
- Achieves up to 87.5% compression of KV heads while maintaining acceptable performance degradation
- Successfully converts MHA to GQA in LLaMA2-7B and Sheared-LLaMA-1.3B models
- Maintains computational invariance throughout the conversion process
- Provides a general solution adaptable to standard GQA frameworks

## Why This Works (Mechanism)
The method works by exploiting the similarity between attention heads within groups. Procrustes analysis provides a mathematically rigorous way to align heads while preserving their functional behavior, creating a basis for sharing KV parameters. The L0 regularization then identifies and removes redundant parameters that don't significantly impact model performance. This combination allows for aggressive compression while maintaining the essential information processing capabilities of the attention mechanism.

## Foundational Learning
**Multi-Head Attention (MHA)**: Allows models to jointly attend to information from different representation subspaces at different positions. Why needed: Forms the baseline architecture being converted. Quick check: Understand how attention weights are computed and how multiple heads are combined.

**Grouped-Query Attention (GQA)**: Reduces KV cache memory by sharing key-value pairs across multiple query heads. Why needed: Target architecture that enables memory efficiency. Quick check: Verify understanding of how query heads map to shared KV parameters.

**Procrustes Analysis**: A method for aligning matrices through rotation and scaling while preserving distances. Why needed: Core alignment technique for maximizing head similarity. Quick check: Confirm understanding of how Procrustes maintains computational invariance.

**L0 Regularization**: Promotes sparsity by penalizing the number of non-zero parameters. Why needed: Enables parameter pruning after alignment. Quick check: Understand the difference between L0 and other regularization approaches.

**Key-Value Cache Compression**: Technique for reducing memory footprint during autoregressive generation. Why needed: Primary motivation for the conversion. Quick check: Verify understanding of memory implications in long-sequence generation.

## Architecture Onboarding
**Component Map:** Input sequences -> Multi-head attention layers -> Procrustes alignment -> L0 regularization -> Pruned GQA layers -> Output sequences

**Critical Path:** Attention computation → Alignment via Procrustes → Parameter pruning via L0 regularization → Inference with reduced KV cache

**Design Tradeoffs:** The method balances compression ratio against performance degradation, with the ability to adjust compression levels based on application requirements. Procrustes alignment adds computational overhead during training but reduces memory usage during inference.

**Failure Signatures:** Significant performance drops may indicate insufficient alignment quality or over-aggressive pruning. Models may struggle with tasks requiring diverse attention patterns if compression is too aggressive.

**First Experiments:** 1) Test alignment quality by measuring cosine similarity between aligned heads, 2) Evaluate performance degradation across different compression ratios, 3) Measure inference latency overhead introduced by alignment computations

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation limited to two model architectures (LLaMA2-7B and Sheared-LLaMA-1.3B) and four benchmarks
- The "arbitrary ratio" claim should be qualified by the 87.5% practical limit tested
- Assumption that Procrustes alignment preserves model behavior across all tasks requires broader empirical validation
- Computational overhead from alignment operations during inference is not quantified

## Confidence
- **Core methodology:** High - based on mathematically rigorous Procrustes analysis with proven invariance properties
- **Practical deployment:** Medium - pending broader testing across diverse model families and tasks
- **Generalizability:** Medium - limited by current experimental scope

## Next Checks
1. Evaluate the method across diverse model families (including decoder-only and encoder-decoder architectures) to assess architectural robustness
2. Conduct ablation studies isolating the impact of Procrustes alignment versus L0 regularization on final performance
3. Measure and report the latency overhead introduced by alignment computations in production inference scenarios