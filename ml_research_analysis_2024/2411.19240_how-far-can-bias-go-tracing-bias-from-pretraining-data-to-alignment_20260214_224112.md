---
ver: rpa2
title: How far can bias go? -- Tracing bias from pretraining data to alignment
arxiv_id: '2411.19240'
source_url: https://arxiv.org/abs/2411.19240
tags:
- bias
- olmo
- data
- gender
- occupation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates gender-occupation bias in the Dolma pre-training
  dataset and its transfer to OLMo LLM outputs. Using zero-shot prompting and token
  co-occurrence analyses, it finds that women are underrepresented in the Dolma dataset
  compared to real-world statistics, with only 28 out of 220 occupations more frequently
  associated with female tokens.
---

# How far can bias go? -- Tracing bias from pretraining data to alignment

## Quick Facts
- **arXiv ID**: 2411.19240
- **Source URL**: https://arxiv.org/abs/2411.19240
- **Reference count**: 32
- **Primary result**: Gender-occupation bias in pre-training data transfers to base models and is partially mitigated in instruction-tuned LLMs

## Executive Summary
This study investigates gender-occupation bias from pre-training data through to aligned language models, focusing on the Dolma dataset and OLMo LLM variants. Using zero-shot prompting and token co-occurrence analysis, the research finds women are underrepresented in Dolma compared to real-world statistics, with only 28 out of 220 occupations more frequently associated with female tokens. This bias is amplified in the base OLMo 7B model outputs, which further underrepresent women. However, instruction-tuned versions (OLMo 7B SFT and OLMo 7B Instruct) partially mitigate representational bias by increasing female representation, though stereotypical gender associations persist. The study demonstrates that bias is strongly correlated between pre-training data and base model outputs but shows only moderate correlation for instruction-tuned models. Changes in hyperparameters and prompts have minimal impact on bias expression across all models.

## Method Summary
The study analyzes gender-occupation bias through multiple stages: first examining the Dolma pre-training dataset for gender representation and occupational associations, then evaluating three OLMo model variants (base, SFT, and Instruct) using zero-shot prompting with 200 occupations. Token co-occurrence analysis measures the association strength between gendered tokens and occupational terms. The research compares model outputs against real-world occupational statistics to quantify representational bias. Correlation analysis tracks how bias patterns transfer from data to base models and instruction-tuned variants. The study also tests the impact of different prompts and hyperparameters on bias expression, finding minimal effects across configurations.

## Key Results
- Women are underrepresented in Dolma dataset (28/220 occupations more female-associated) compared to real-world statistics
- Base OLMo 7B model amplifies underrepresentation of women beyond the pre-training data bias
- Instruction-tuned models (OLMo 7B SFT and Instruct) partially mitigate representational bias by increasing female representation, though stereotypical associations persist

## Why This Works (Mechanism)
The study demonstrates that pre-training data bias systematically transfers to model behavior through token frequency distributions and co-occurrence patterns. Base models learn and amplify statistical biases present in their training data, while instruction-tuning can partially mitigate representation bias through supervised fine-tuning on curated datasets. However, stereotypical associations are more resistant to mitigation because they are deeply embedded in the learned representations and reinforced through both pre-training and fine-tuning processes. The minimal impact of hyperparameters and prompts suggests that bias is primarily determined by the data distribution rather than training configuration or input framing.

## Foundational Learning
- **Token co-occurrence analysis**: Measures how often tokens appear together to quantify associations between concepts
  - *Why needed*: Provides quantitative measure of bias strength between gender and occupation terms
  - *Quick check*: Verify correlation coefficients between gendered and occupational tokens are statistically significant

- **Zero-shot prompting**: Evaluating model outputs without task-specific training examples
  - *Why needed*: Isolates model biases from fine-tuning effects on specific tasks
  - *Quick check*: Ensure prompts are neutral and don't introduce additional bias

- **Representational bias measurement**: Comparing model output distributions against real-world statistics
  - *Why needed*: Quantifies how accurately models reflect actual gender distribution in occupations
  - *Quick check*: Calculate KL divergence between model and real-world distributions

## Architecture Onboarding
- **Component map**: Pre-training data (Dolma) -> Base model (OLMo 7B) -> SFT fine-tuning -> Instruction tuning (OLMo 7B Instruct)
- **Critical path**: Data bias → Model learning → Output generation → Bias measurement
- **Design tradeoffs**: Instruction tuning improves representation but cannot fully eliminate stereotypical associations; base models amplify rather than correct data biases
- **Failure signatures**: Persistent underrepresentation of women, stereotypical gender associations in occupational contexts, minimal hyperparameter sensitivity
- **3 first experiments**:
  1. Test bias transfer with different occupational categories (STEM vs humanities)
  2. Measure bias sensitivity to different prompt formulations
  3. Compare bias expression across different model scales (1B vs 7B vs 13B)

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses only on binary gender representation, missing intersectional biases across multiple identity dimensions
- Static occupational categories may not capture nuanced contextual biases that emerge in conversational scenarios
- Limited hyperparameter testing makes claims about training configuration effects uncertain

## Confidence
- **High confidence**: Demonstrating bias transfer from data to base models through correlation analysis
- **Medium confidence**: Claims about instruction-tuning mitigation effectiveness, given persistent stereotypical associations
- **Low confidence**: Assertions that hyperparameter changes minimally affect bias expression due to limited configuration testing

## Next Checks
1. Replicate analysis using intersectional identity categories to assess bias amplification across combined gender, ethnicity, and age dimensions
2. Conduct systematic ablation studies varying learning rates, batch sizes, and training duration to quantify impact on bias expression
3. Test model outputs with dynamic, context-rich prompts including qualifiers like "typically" or "historically" to determine if linguistic framing reduces stereotypical associations