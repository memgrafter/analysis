---
ver: rpa2
title: The impact of data set similarity and diversity on transfer learning success
  in time series forecasting
arxiv_id: '2404.06198'
source_url: https://arxiv.org/abs/2404.06198
tags:
- data
- source
- time
- sets
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how source-target data set similarity and
  source diversity impact transfer learning success in time series forecasting. It
  systematically evaluates pre-trained neural networks across five public source datasets
  applied to five target datasets, including real-world wholesale data.
---

# The impact of data set similarity and diversity on transfer learning success in time series forecasting

## Quick Facts
- arXiv ID: 2404.06198
- Source URL: https://arxiv.org/abs/2404.06198
- Reference count: 20
- Key outcome: This study investigates how source-target data set similarity and source diversity impact transfer learning success in time series forecasting. It systematically evaluates pre-trained neural networks across five public source datasets applied to five target datasets, including real-world wholesale data. Two feature-based similarity measures (tsfresh and catch22) and two diversity measures are identified. Results show that higher source-target similarity in tsfresh features reduces forecasting bias, while greater source diversity in catch22 features improves accuracy and uncertainty estimation but increases bias. These relationships are stronger for zero-shot than fine-tuned forecasting. Fine-tuning generally enhances performance, making source data characteristics less critical. The study provides actionable insights for selecting source data in transfer learning applications.

## Executive Summary
This study systematically evaluates how source-target data set similarity and source diversity impact transfer learning success in time series forecasting. The authors pre-train neural networks on five public source datasets and apply them to five target datasets, measuring performance through accuracy, bias, and uncertainty estimation metrics. Two feature-based similarity measures (tsfresh and catch22) and two diversity measures are identified and analyzed. Results show that higher source-target similarity in tsfresh features reduces forecasting bias, while greater source diversity in catch22 features improves accuracy and uncertainty estimation but increases bias. These relationships are stronger for zero-shot than fine-tuned forecasting. Fine-tuning generally enhances performance, making source data characteristics less critical.

## Method Summary
The study pre-trains DeepAR models on five public source datasets (M5, M4, Electricity, NN5, Exchange Rate) and their concatenation (Multisource), then applies them zero-shot and after fine-tuning to five target datasets (Kaggle Wiki, Traffic, Solar Energy, Wholesaler1, Wholesaler2). Feature-based similarity (tsfresh and catch22 features) and diversity measures are computed to analyze their relationship with forecasting performance. The authors evaluate three performance aspects: accuracy (AvgRMSSE), bias (Mean Error), and uncertainty estimation (MSIS). Linear regression analysis is used to quantify the strength of relationships between similarity/diversity measures and performance metrics across different transfer learning scenarios.

## Key Results
- Higher source-target similarity in tsfresh features reduces forecasting bias, especially in zero-shot transfer learning
- Greater source diversity in catch22 features improves zero-shot accuracy and uncertainty estimation but increases bias
- Fine-tuning generally enhances performance across all metrics, making source data characteristics less critical for success
- The relationships between similarity/diversity and performance are stronger for zero-shot than fine-tuned forecasting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Source-target similarity in tsfresh features reduces forecasting bias in zero-shot transfer learning.
- **Mechanism**: When source and target time series share similar statistical characteristics (mean, variance, kurtosis, etc.), the pre-trained model's learned parameters align better with the target data distribution, leading to more accurate median forecasts and thus reduced systematic error (bias).
- **Core assumption**: tsfresh features capture the relevant distributional and shape characteristics that directly influence the model's ability to generalize from source to target.
- **Evidence anchors**:
  - [abstract]: "source-target similarity reduces forecasting bias"
  - [section]: "higher source-target similarity in tsfresh features reduces forecasting bias"
  - [corpus]: Weak—no direct match, but conceptually similar to "selecting subsets of source data for transfer learning".
- **Break condition**: If the relevant forecasting characteristics are not captured by tsfresh features, or if the source data's tsfresh profile is misleading due to domain shift.

### Mechanism 2
- **Claim**: Greater source diversity in catch22 features improves zero-shot accuracy and uncertainty estimation.
- **Mechanism**: A diverse source dataset exposes the model to a wider range of temporal dynamics (autocorrelation, entropy, high-frequency fluctuations, etc.). This richer representation allows the model to better handle varied target time series, improving both point forecast accuracy and the reliability of uncertainty intervals.
- **Core assumption**: catch22 features effectively encode temporal and non-linear characteristics critical for forecasting generalization.
- **Evidence anchors**:
  - [abstract]: "source diversity in catch22 features improves forecasting accuracy and uncertainty estimation"
  - [section]: "more diversity in catch22 features is favorable for zero-shot accuracy and uncertainty estimation"
  - [corpus]: Weak—no direct match, but aligns with the general concept of "multi-source transfer learning".
- **Break condition**: If the diversity in catch22 features does not correspond to meaningful forecasting-relevant patterns, or if over-diversification causes noise that degrades model stability.

### Mechanism 3
- **Claim**: Fine-tuning generally improves transfer learning performance, making source data characteristics less critical.
- **Mechanism**: Fine-tuning adapts the pre-trained model's parameters to the specific statistical properties of the target data, effectively overriding the initial inductive biases from the source data. This adaptation reduces dependence on source-target similarity or diversity for achieving good performance.
- **Core assumption**: The model architecture (DeepAR) is flexible enough to adapt effectively during fine-tuning without overfitting, even with small target datasets.
- **Evidence anchors**:
  - [abstract]: "Fine-tuning generally enhances performance, making source data characteristics less critical"
  - [section]: "fine-tuning enhances the performance in general" and "fine-tuning smoothens the relations between similarity, diversity and forecasting performance"
  - [corpus]: Weak—no direct match, but conceptually aligns with general fine-tuning benefits in transfer learning.
- **Break condition**: If the target dataset is too small or noisy, fine-tuning could lead to overfitting, negating the benefits and making source data characteristics more important again.

## Foundational Learning

- **Concept**: Time series feature extraction (statistical and temporal).
  - **Why needed here**: The study relies on tsfresh and catch22 feature sets to quantify source-target similarity and source diversity. Understanding what these features measure is essential to interpret the results and design new experiments.
  - **Quick check question**: What are the key differences between tsfresh (statistical/shape) and catch22 (temporal/non-linear) feature sets, and why might each relate differently to forecasting performance?

- **Concept**: Transfer learning modes (zero-shot vs. fine-tuning).
  - **Why needed here**: The paper explicitly compares zero-shot (no target data seen) and fine-tuned (target data used for adaptation) forecasting. Knowing how each mode works explains why source data characteristics matter more in zero-shot cases.
  - **Quick check question**: In zero-shot transfer learning, how does the model generate forecasts without any exposure to the target data?

- **Concept**: Evaluation metrics for forecasting (accuracy, bias, uncertainty).
  - **Why needed here**: The study uses AvgRMSSE for accuracy, Mean Error for bias, and MSIS for uncertainty estimation. Understanding these metrics is necessary to assess model performance and interpret the trade-offs observed.
  - **Quick check question**: How does the MSIS metric penalize both over- and under-estimation differently from traditional accuracy measures?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Feature extraction (tsfresh + catch22) -> Similarity/Diversity calculation -> Model training (DeepAR pre-training) -> Transfer learning (zero-shot/fine-tuning) -> Evaluation (accuracy, bias, uncertainty)
- **Critical path**: Pre-training on source data -> Calculating feature-based similarity/diversity -> Zero-shot forecasting -> Fine-tuning on target -> Final evaluation
- **Design tradeoffs**:
  - Using tsfresh vs. catch22: tsfresh is interpretable but may miss temporal dynamics; catch22 captures complex temporal patterns but is less interpretable
  - Fine-tuning vs. zero-shot: Fine-tuning improves performance but requires target data and computational resources; zero-shot is faster but more sensitive to source data choice
- **Failure signatures**:
  - Poor zero-shot performance: Likely due to low source-target similarity in relevant features or insufficient source diversity
  - Overfitting during fine-tuning: Target data too small or noisy relative to model complexity
  - Inconsistent results across metrics: Feature sets may not align well with the forecasting task's true requirements
- **First 3 experiments**:
  1. **Replicate similarity calculation**: Compute tsfresh and catch22 feature distances between a known similar and dissimilar source-target pair to verify the method
  2. **Test zero-shot bias sensitivity**: Pre-train on a source highly similar in tsfresh features to the target and measure bias reduction versus a dissimilar source
  3. **Evaluate diversity impact**: Pre-train on a diverse source (high catch22 variance) and compare zero-shot accuracy and uncertainty against a less diverse source

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do the identified feature-based similarity measures (tsfresh and catch22) relate to domain-specific transfer learning performance across different frequency granularities?
- **Basis in paper**: [explicit] The paper identifies two feature-based similarity measures (tsfresh and catch22) but only evaluates them for weekly frequency data
- **Why unresolved**: The study restricts analysis to weekly frequency data and doesn't explore how these measures perform across different temporal granularities
- **What evidence would resolve it**: Experiments testing the same similarity measures across multiple frequencies (daily, hourly, monthly) with consistent transfer learning performance patterns

### Open Question 2
- **Question**: What is the relationship between the diversity of source data sets in terms of their time series characteristics and the generalization capability of foundation models on completely unseen domains?
- **Basis in paper**: [inferred] The paper discusses source diversity measures and their impact on transfer learning success, but doesn't test generalization to completely new domains
- **Why unresolved**: The target data sets, while from different domains, still share some characteristics with source data sets; the paper doesn't test transfer to entirely novel domains
- **What evidence would resolve it**: Testing foundation models trained on the source data sets against completely unrelated domains (e.g., medical, financial, environmental) with systematic performance evaluation

### Open Question 3
- **Question**: How do the statistical significance levels of the linear regression relationships between similarity/diversity measures and forecasting performance change with larger sample sizes?
- **Basis in paper**: [explicit] The paper notes that many p-values from linear regression fits are not statistically significant, suggesting this might be due to limited data points
- **Why unresolved**: The study uses a fixed number of source and target data sets (5 each), limiting statistical power for regression analysis
- **What evidence would resolve it**: Repeating the experiments with a much larger set of source and target data sets (e.g., 20+ each) while maintaining the same methodology to assess changes in statistical significance

## Limitations
- The study's findings are based on specific feature extraction methods (tsfresh and catch22) and a single forecasting model (DeepAR), which may limit generalizability to other time series domains or model architectures
- The observed relationships between source-target similarity/diversity and forecasting performance may not hold for datasets with fundamentally different temporal structures or noise characteristics
- The study does not explore the impact of varying the number of source datasets or their relative weighting during pre-training, which could affect transfer learning outcomes

## Confidence

- **High confidence**: The positive impact of fine-tuning on overall performance and its ability to reduce dependence on source data characteristics is well-established and aligns with general transfer learning literature
- **Medium confidence**: The specific relationships between tsfresh similarity and bias reduction, and catch22 diversity and accuracy/uncertainty estimation, are supported by the results but may be sensitive to the choice of feature sets and evaluation metrics
- **Low confidence**: The claim that these relationships are stronger in zero-shot than fine-tuned forecasting requires further validation across diverse time series domains and model architectures

## Next Checks
1. **Replicate similarity calculation**: Compute tsfresh and catch22 feature distances between a known similar and dissimilar source-target pair to verify the method's sensitivity and consistency
2. **Test alternative feature sets**: Apply additional time series feature extraction methods (e.g., tsfeatures, hctsa) to assess whether the observed relationships hold across different feature representations
3. **Evaluate model generalization**: Pre-train DeepAR on the same source datasets but with different random seeds or hyperparameters to assess the robustness of the transfer learning performance to model initialization and training variations