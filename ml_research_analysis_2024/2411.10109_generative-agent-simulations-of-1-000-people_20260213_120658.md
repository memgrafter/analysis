---
ver: rpa2
title: Generative Agent Simulations of 1,000 People
arxiv_id: '2411.10109'
source_url: https://arxiv.org/abs/2411.10109
tags:
- affiliation
- correl
- page
- what
- p-value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a novel generative agent architecture that simulates
  the attitudes and behaviors of 1,052 real individuals using two-hour qualitative
  interviews and large language models. The generative agents predict participants'
  responses on the General Social Survey with 85% of the accuracy participants show
  in replicating their own answers two weeks later.
---

# Generative Agent Simulations of 1,000 People

## Quick Facts
- arXiv ID: 2411.10109
- Source URL: https://arxiv.org/abs/2411.10109
- Reference count: 40
- Primary result: Generative agents predict survey responses with 85% of the accuracy participants show in replicating their own answers two weeks later.

## Executive Summary
This work presents a novel generative agent architecture that simulates the attitudes and behaviors of 1,052 real individuals using two-hour qualitative interviews and large language models. The agents predict responses on the General Social Survey with 85% of the accuracy participants show in replicating their own answers two weeks later. They also achieve high correlation in predicting personality traits (normalized correlation of 0.80) and outcomes in behavioral economic games and social science experiments (normalized correlation of 0.66). The interview-based approach reduces accuracy biases across racial and ideological groups compared to agents using demographic information.

## Method Summary
The study recruited 1,052 U.S. adults for two-hour qualitative interviews about their lives, followed by responses to surveys (GSS, Big Five), economic games, and social science experiments. Interview transcripts were used to prompt large language models to generate predictions for each participant's responses. Predictions were evaluated against participants' actual responses and self-consistency over two-week follow-ups. Domain-expert reflections were generated to infer latent insights from transcripts before prompting the language model.

## Key Results
- Generative agents predict GSS responses with 85% of the accuracy participants show in replicating their own answers two weeks later
- Agents achieve normalized correlation of 0.80 for personality trait prediction and 0.66 for behavioral economic game outcomes
- Interview-based agents consistently reduce demographic biases compared to demographic-based agents across all prediction tasks

## Why This Works (Mechanism)

### Mechanism 1
Generative agents achieve high fidelity by conditioning language models on detailed interview transcripts. The architecture injects full interview transcripts into model prompts, enabling the model to role-play the participant by retrieving personal context during inference. This works because language models can retain and apply individual-specific context when provided with complete interview data. Break condition: If transcripts are too sparse, contain contradictory information, or are not representative of true attitudes, model fidelity will degrade.

### Mechanism 2
Interview-based agents reduce demographic bias compared to agents using only demographic descriptors. By using rich interview data instead of limited demographic attributes, agents avoid reinforcing stereotypes associated with demographic groups. This works because interview data captures individual variation that demographic attributes alone cannot. Break condition: If interviews inadvertently encode societal stereotypes, or if demographic attributes contain predictive information not captured in interviews, bias reduction may not hold.

### Mechanism 3
Generative agents can predict treatment effects in behavioral experiments by simulating human-like decision processes. Agents use interview-derived context and reflection modules to reason about experimental scenarios and produce behavior consistent with their source participants. This works because interview context is sufficient to predict individual choices in structured experimental settings. Break condition: If experimental scenarios require knowledge not covered in interviews, or if agents overfit to interview language without capturing underlying decision logic, prediction accuracy will fall.

## Foundational Learning

- **Conditional probability and predictive accuracy**: Understanding how agents' predictions are evaluated against human self-consistency requires familiarity with probability normalization. Quick check: If an agent predicts 8 out of 10 responses correctly, and the human replicates 9 out of 10, what is the normalized accuracy?
- **Experimental design and treatment effects**: The replication studies rely on comparing control vs. treatment conditions, so understanding this structure is essential. Quick check: In a two-group experiment with 500 participants per group, what statistical test would you use to determine if a difference in means is significant?
- **Bias metrics and fairness**: Evaluating demographic parity differences requires understanding how to quantify and compare subgroup performance. Quick check: If Group A has 90% accuracy and Group B has 70%, what is the Demographic Parity Difference?

## Architecture Onboarding

- **Component map**: Data layer (interview transcripts, survey responses, experimental data) -> Core model (LLM conditioned on transcripts) -> Reflection module (domain-expert persona prompts) -> Prediction interface (prompt templates for different task types) -> Evaluation module (accuracy, MAE, correlation calculations plus self-consistency normalization)
- **Critical path**: 1. Collect and preprocess interview transcript, 2. Generate expert reflections (psychologist, economist, etc.), 3. Prompt language model with transcript + relevant reflections, 4. Generate prediction, 5. Evaluate against human data
- **Design tradeoffs**: Full transcript vs. summary (full preserves nuance but increases token costs; summaries reduce context but may lose linguistic cues), Expert reflection vs. direct prompting (reflections add interpretability but require additional model calls), Open vs. fixed task queries (open offers flexibility but raises privacy risks; fixed simplifies evaluation but limits scope)
- **Failure signatures**: Low normalized accuracy (model fails to capture individual differences), High bias DPD (demographic descriptors reintroduce stereotypes), Inconsistent experimental predictions (agents overfit interview language without modeling underlying decision logic)
- **First 3 experiments**: 1. Predict responses to 5 GSS questions using only interview data; compare to demographic-only predictions, 2. Simulate a trust game for a participant; compare agent prediction to actual choice, 3. Remove 50% of interview content and measure accuracy drop to test information efficiency

## Open Questions the Paper Calls Out

### Open Question 1
What is the impact of interview length on the predictive accuracy of generative agents? The paper mentions a robustness analysis where portions of the interview were randomly removed to assess the impact of interview content volume. This remains unresolved because while the paper indicates that performance declines linearly as interview length is reduced, it does not specify the exact threshold at which the interview becomes too sparse to retain valuable information. Conduct experiments with varying interview lengths and measure the predictive accuracy of generative agents to identify the point at which accuracy starts to significantly decline.

### Open Question 2
How do generative agents perform when predicting responses to open-ended questions or tasks? The paper mentions that the agent bank will offer restricted access to individualized responses on open tasks, suggesting that evaluating agents' performance on open-ended tasks is an area of interest. This remains unresolved because the paper primarily focuses on the predictive performance of generative agents on structured surveys and experiments, leaving the performance on open-ended tasks unexplored. Conduct experiments where generative agents are asked to respond to open-ended questions or tasks and evaluate their responses using appropriate metrics such as coherence, relevance, and creativity.

### Open Question 3
What is the role of linguistic features in enhancing the predictive accuracy of generative agents? The paper discusses the creation of "summary agents" where interview transcripts are converted into bullet-pointed summaries, suggesting an investigation into the importance of linguistic features. This remains unresolved because the paper indicates that summary agents perform slightly below interview-based agents, but it does not delve into the specific linguistic features that contribute to improved predictive accuracy. Analyze the linguistic features present in interview transcripts and correlate them with the predictive accuracy of generative agents. Identify the most influential features and explore ways to incorporate them into the agent architecture.

## Limitations

- Only partial interview protocols are provided, limiting exact reproduction of the data collection process
- Results may not generalize to populations outside the U.S. or different time periods
- Privacy concerns restrict sharing of full interview transcripts, limiting external validation

## Confidence

- **High confidence**: High normalized correlation (0.80) for personality traits and self-consistency (85%) for GSS predictions
- **Medium confidence**: Bias reduction relative to demographic-only agents
- **Low confidence**: Predictive accuracy in behavioral economic games (0.66 correlation)

## Next Checks

1. Cross-cultural replication: Repeat the agent generation process with interview and experimental data from a non-U.S. population to test robustness of accuracy and bias claims
2. Ablation on interview length: Systematically reduce interview length (e.g., 15, 30, 60 minutes) and measure impact on prediction accuracy to quantify information efficiency
3. Demographic descriptor re-introduction: Reintroduce selective demographic attributes into the agent conditioning process to isolate which specific traits contribute to bias reduction