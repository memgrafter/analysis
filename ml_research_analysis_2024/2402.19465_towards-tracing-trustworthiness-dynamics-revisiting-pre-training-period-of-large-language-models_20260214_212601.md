---
ver: rpa2
title: 'Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of
  Large Language Models'
arxiv_id: '2402.19465'
source_url: https://arxiv.org/abs/2402.19465
tags:
- pre-training
- llms
- trustworthiness
- layer
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores the trustworthiness dynamics of large language
  models (LLMs) during pre-training, a phase often overlooked in existing research.
  The authors investigate five key trustworthiness dimensions: reliability, privacy,
  toxicity, fairness, and robustness.'
---

# Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models

## Quick Facts
- arXiv ID: 2402.19465
- Source URL: https://arxiv.org/abs/2402.19465
- Reference count: 40
- Primary result: Pre-training checkpoints of LLMs contain untapped potential for enhancing trustworthiness, with middle-layer representations developing linearly separable patterns early in training.

## Executive Summary
This paper explores the trustworthiness dynamics of large language models (LLMs) during the often-overlooked pre-training phase. The authors investigate five key trustworthiness dimensions—reliability, privacy, toxicity, fairness, and robustness—across 360 pre-training checkpoints. They discover that middle-layer representations develop linearly separable patterns for trustworthiness concepts early in training, enabling effective linear probing. The study also demonstrates that steering vectors extracted from pre-training checkpoints can enhance the trustworthiness of fine-tuned models, sometimes outperforming vectors extracted directly from the fine-tuned model. Additionally, mutual information analysis reveals two distinct phases in pre-training dynamics: fitting and compression, providing new insights into how LLMs learn trustworthiness concepts.

## Method Summary
The authors apply linear probing to LLM representations across 360 pre-training checkpoints from LLM360, using five trustworthiness datasets (TruthfulQA, ToxiGen, ConfAIde, StereoSet, perturbed SST-2). They extract steering vectors from high-accuracy checkpoints and apply them to intervene in a supervised fine-tuned (SFT) model, measuring trustworthiness enhancement. Mutual information between representations and labels is estimated using HSIC to analyze pre-training dynamics, revealing fitting and compression phases. The study evaluates both trustworthiness and general capabilities using multiple benchmarks.

## Key Results
- Middle-layer representations in LLMs develop linearly separable patterns for trustworthiness concepts early in pre-training, as evidenced by high linear probing accuracy
- Steering vectors extracted from pre-training checkpoints can enhance the trustworthiness of SFT models, matching or exceeding vectors extracted from the SFT model itself
- Mutual information analysis reveals two distinct phases during pre-training: fitting (both I(T,X) and I(T,Y) increase) followed by compression (I(T,X) decreases, I(T,Y) increases)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Middle-layer representations in LLMs develop linearly separable patterns for trustworthiness concepts early in pre-training, enabling the use of linear probing to detect these patterns.
- **Mechanism**: Linear probing measures the ability of a linear classifier to distinguish between opposing concepts (e.g., true vs. false statements) using intermediate layer activations. High probing accuracy indicates that the representations are already encoding these concepts in a linearly separable manner.
- **Core assumption**: The linear representation hypothesis holds, meaning that key trustworthiness concepts can be represented as linearly separable directions in the latent space of LLMs.
- **Evidence anchors**:
  - [abstract] "high probing accuracy suggests that LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension"
  - [section 2.3] "middle layer representations of LLMs have already developed linearly separable patterns about trustworthiness"
  - [corpus] Weak - corpus neighbors focus on probing in general, not specifically trustworthiness.

### Mechanism 2
- **Claim**: Steering vectors extracted from pre-training checkpoints can enhance the trustworthiness of a fine-tuned SFT model by intervening in its activation space.
- **Mechanism**: Steering vectors are computed as the difference between activations of positive and negative examples for a given trustworthiness dimension. Applying these vectors to the SFT model's activations during inference shifts its behavior towards more trustworthy outputs.
- **Core assumption**: The latent space of the SFT model is sufficiently aligned with that of the pre-training checkpoints, allowing steering vectors to transfer across checkpoints.
- **Evidence anchors**:
  - [abstract] "steering vectors extracted from pre-training checkpoints...match or even exceed the performance of vectors extracted from the SFT model itself"
  - [section 3.1] "we extract steering vectors from a LLM's pre-training checkpoints to enhance the LLM's trustworthiness"
  - [corpus] Weak - corpus neighbors focus on activation engineering in general, not specifically trustworthiness or pre-training checkpoints.

### Mechanism 3
- **Claim**: Mutual information between representations and labels reveals two phases of pre-training dynamics for trustworthiness: a fitting phase (both I(T,X) and I(T,Y) increase) followed by a compression phase (I(T,X) decreases, I(T,Y) increases).
- **Mechanism**: Mutual information measures the dependence between representations and labels. The fitting phase captures learning of the dataset, while the compression phase reflects selective retention of label-relevant information.
- **Core assumption**: Mutual information estimation via HSIC is accurate and captures meaningful dependencies between representations and labels.
- **Evidence anchors**:
  - [abstract] "mutual information estimation is bounded by linear probing accuracy"
  - [section 4.3] "we are the first to notice that during the pre-training period of LLMs, there exist two distinct phases regarding trustworthiness: fitting and compression"
  - [corpus] Weak - corpus neighbors focus on mutual information in general, not specifically trustworthiness or pre-training dynamics.

## Foundational Learning

- **Concept**: Linear probing
  - **Why needed here**: To assess whether LLM representations encode trustworthiness concepts in a linearly separable manner, enabling steering vector interventions.
  - **Quick check question**: If linear probing achieves high accuracy on a trustworthiness dataset, what does this indicate about the LLM's representations?

- **Concept**: Mutual information
  - **Why needed here**: To analyze the dynamics of representation learning during pre-training, revealing fitting and compression phases for trustworthiness concepts.
  - **Quick check question**: What does a decrease in I(T,X) coupled with an increase in I(T,Y) indicate about the model's learning dynamics?

- **Concept**: Activation intervention
  - **Why needed here**: To apply steering vectors extracted from pre-training checkpoints to enhance the trustworthiness of a fine-tuned SFT model.
  - **Quick check question**: How does adding a steering vector to an activation during inference alter the model's output?

## Architecture Onboarding

- **Component map**:
  Pre-training checkpoints (360 from LLM360) -> Linear probing pipeline -> Steering vector extraction -> SFT model (AmberChat) -> Trustworthiness evaluation
  Mutual information estimation pipeline -> Analysis of fitting/compression phases

- **Critical path**:
  1. Collect activations from pre-training checkpoints on trustworthiness datasets
  2. Train linear classifiers and evaluate probing accuracy
  3. Extract steering vectors from high-accuracy checkpoints
  4. Apply steering vectors to AmberChat for trustworthiness enhancement
  5. Estimate mutual information across checkpoints and analyze dynamics

- **Design tradeoffs**:
  - Using middle layers vs. all layers for probing
  - Single checkpoint vs. multiple checkpoints for steering vector extraction
  - Gaussian kernel vs. other kernels for HSIC estimation

- **Failure signatures**:
  - Low linear probing accuracy → concepts not linearly separable
  - Steering vector intervention degrades general capabilities → latent spaces misaligned
  - Mutual information trends inconsistent → estimation inaccurate

- **First 3 experiments**:
  1. Linear probing on a single checkpoint and layer to verify the mechanism
  2. Steering vector intervention on SFT model with a single trustworthiness dimension
  3. Mutual information estimation on a single checkpoint to validate the fitting/compression characterization

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- The linear separability assumption for trustworthiness concepts may not hold uniformly across all dimensions or datasets
- The transfer effectiveness of steering vectors across pre-training checkpoints and SFT models is not fully characterized
- Mutual information estimation via HSIC may be sensitive to kernel choice and checkpoint selection

## Confidence
- **High**: The existence of two distinct phases (fitting and compression) in pre-training dynamics, as evidenced by mutual information trends and supported by prior research on DNNs
- **Medium**: The effectiveness of steering vectors extracted from pre-training checkpoints for enhancing trustworthiness in SFT models, given the observed performance gains but limited ablation studies
- **Low**: The generalizability of findings to other LLM architectures and pre-training objectives beyond LLM360 and the specific trustworthiness dimensions studied

## Next Checks
1. Conduct controlled experiments varying the number of pre-training checkpoints used for steering vector extraction to assess the impact on transfer effectiveness
2. Perform cross-architecture validation by applying steering vectors from one LLM family to another to test latent space alignment
3. Implement alternative mutual information estimation methods (e.g., MINE) to verify the robustness of the fitting/compression phase characterization