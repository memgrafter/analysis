---
ver: rpa2
title: 'RadDQN: a Deep Q Learning-based Architecture for Finding Time-efficient Minimum
  Radiation Exposure Pathway'
arxiv_id: '2402.00468'
source_url: https://arxiv.org/abs/2402.00468
tags:
- radiation
- training
- algorithm
- path
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RadDQN introduces a deep Q-learning-based architecture for finding
  time-efficient minimum radiation exposure pathways in radiation-contaminated zones.
  The core innovation is a radiation-aware reward function that accounts for the proximity
  to radiation sources, their strength, and distance to the exit, enabling the agent
  to prioritize both radiation minimization and time efficiency.
---

# RadDQN: a Deep Q Learning-based Architecture for Finding Time-efficient Minimum Radiation Exposure Pathway

## Quick Facts
- arXiv ID: 2402.00468
- Source URL: https://arxiv.org/abs/2402.00468
- Authors: Biswajit Sadhu; Trijit Sadhu; S. Anand
- Reference count: 40
- Primary result: Deep Q-learning architecture with radiation-aware reward function achieves superior convergence and stability in finding time-efficient minimum radiation exposure pathways in 2D grid environments

## Executive Summary
RadDQN introduces a deep Q-learning-based architecture for finding time-efficient minimum radiation exposure pathways in radiation-contaminated zones. The core innovation is a radiation-aware reward function that accounts for the proximity to radiation sources, their strength, and distance to the exit, enabling the agent to prioritize both radiation minimization and time efficiency. Additionally, the method employs novel exploration strategies that convert random actions to model-directed ones based on future radiation exposure and training performance, improving convergence and stability. In experiments with simulated 2D grid environments containing 2-3 radioactive sources of varying strength, RadDQN achieved superior convergence rates and higher training stability compared to vanilla DQN.

## Method Summary
RadDQN uses a deep Q-learning agent to navigate 2D grid environments with radioactive sources, optimizing for minimum radiation exposure and time efficiency. The agent employs a radiation-aware reward function that incorporates inverse-square law radiation intensity and exit proximity. Novel exploration strategies restrict random exploration in high-radiation states, while an adaptive target network update frequency improves training stability. The method was tested on 10x10 grid environments with 2-3 sources of varying strengths, comparing convergence rates and training stability against vanilla DQN.

## Key Results
- RadDQN achieved superior convergence rates and higher training stability compared to vanilla DQN
- Lower moving variance in cumulative reward indicates more consistent path optimization across diverse radiation field distributions
- The agent successfully found time-efficient minimum radiation exposure pathways in 2D grid environments with 2-3 radioactive sources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Radiation-aware reward function enables the agent to balance time efficiency and radiation minimization by incorporating distance to exit and proximity to radiation sources.
- Mechanism: The reward function uses inverse-square law for radiation intensity and adds a positive term for proximity to the exit, encouraging the agent to reach the exit quickly while avoiding high radiation zones.
- Core assumption: Radiation exposure is accurately modeled by inverse-square law and the agent can learn to balance competing objectives through reward shaping.
- Evidence anchors:
  - [abstract] "The core innovation is a radiation-aware reward function that accounts for the proximity to radiation sources, their strength, and distance to the exit"
  - [section] "We formulate an efficient reward structure which is aware of the presence of multiple radiation sources and their radiation strength of the sources, the duration of exposure and radial distance of the agent from all the radiation hot-spots and the destination point"
- Break condition: If the inverse-square law approximation is invalid for the radiation field distribution or the agent cannot learn the trade-off between time and exposure.

### Mechanism 2
- Claim: Novel exploration strategies that convert random actions to model-directed ones based on future radiation exposure and training performance improve convergence and stability.
- Mechanism: The exploration strategies (exppr and expr) restrict blind exploration by allowing the agent to peek at future rewards and convert random actions to model-directed ones if they lead to higher radiation exposure or based on winning ratio.
- Core assumption: Restricting random exploration in high-radiation states leads to more efficient learning and better convergence.
- Evidence anchors:
  - [abstract] "the method employs novel exploration strategies that convert random actions to model-directed ones based on future radiation exposure and training performance"
  - [section] "We propose unique strategies on controlling the exploration/exploitation based on simulation environment to achieve superior time-efficient convergence with reduced variance"
- Break condition: If the exploration restrictions lead to short-sightedness and prevent the agent from finding optimal paths in complex environments.

### Mechanism 3
- Claim: Adaptive update frequency of target network based on agent performance improves training stability and convergence.
- Mechanism: The update frequency of the target network is adjusted based on the winning ratio and the change in the moving average of steps, allowing faster updates when performance improves and slower updates when it worsens.
- Core assumption: Adaptive update frequency helps the agent learn more efficiently by transferring recent improvements to the target network.
- Evidence anchors:
  - [abstract] "RadDQN achieved superior convergence rates and higher training stability compared to vanilla DQN, as evidenced by lower moving variance in cumulative reward and more consistent path optimization"
  - [section] "Update frequency is a variable that can be only optimized based on many trial-and-errors... Inspired by this, we make this variable dependent on two factors, namely winning ratio of all played episodes and change in the moving average of number of steps during last k winning episodes"
- Break condition: If the adaptive update frequency leads to instability or slow convergence in certain environments.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The agent learns optimal policies through interaction with the environment, which is modeled as an MDP.
  - Quick check question: What are the key components of an MDP and how do they relate to the agent's learning process?

- Concept: Deep Q-Network (DQN)
  - Why needed here: DQN is used to approximate the Q-function, enabling the agent to handle high-dimensional state spaces.
  - Quick check question: How does DQN differ from traditional Q-learning and what are its key components?

- Concept: Exploration vs. Exploitation
  - Why needed here: The agent must balance between exploring new actions and exploiting known good actions to learn optimal policies.
  - Quick check question: What are the trade-offs between exploration and exploitation and how do different exploration strategies impact learning?

## Architecture Onboarding

- Component map:
  - Environment -> Agent -> Reward Function -> Exploration Strategies -> Update Frequency
  - 2D grid world with radioactive sources -> Deep Q-learning agent with neural network -> Radiation-aware reward function -> Vanilla, restricted, and partially-restricted exploration -> Adaptive target network update

- Critical path:
  - Initialize environment and agent
  - Train agent using DQN with radiation-aware reward function and exploration strategies
  - Evaluate agent performance on test scenarios

- Design tradeoffs:
  - Exploration strategies: Balance between exploration and exploitation
  - Reward function: Trade-off between time efficiency and radiation minimization
  - Update frequency: Stability vs. convergence speed

- Failure signatures:
  - High variance in cumulative reward during training
  - Slow convergence or failure to converge
  - Suboptimal paths or failure to find optimal paths

- First 3 experiments:
  1. Train agent on simple scenario with two radiation sources and evaluate performance
  2. Vary the number and strength of radiation sources and observe agent's ability to adapt
  3. Compare performance of different exploration strategies and update frequencies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does dynamically varying the positions of radiation sources during training affect the policy learned by RadDQN?
- Basis in paper: [inferred] The paper mentions that training is performed in a static environment and notes this as a limitation, suggesting future work to investigate dynamic source positioning.
- Why unresolved: The current study does not test training with moving or variably positioned radiation sources, leaving the impact on learned policies unknown.
- What evidence would resolve it: Experimental results comparing RadDQN performance and policy adaptability when radiation sources are moved during training versus kept static.

### Open Question 2
- Question: What is the effect of the parameter 'n' in the radiation-aware reward function on the trade-off between minimizing radiation exposure and reaching the exit quickly?
- Basis in paper: [explicit] The paper discusses that 'n' controls the agent's urge to reach the exit versus focusing on dose distribution, but does not empirically analyze its impact.
- Why unresolved: The study uses a fixed value of n=1 without exploring how different values affect path optimization and training outcomes.
- What evidence would resolve it: Systematic experiments varying 'n' and analyzing resulting paths, training convergence, and radiation exposure/time efficiency trade-offs.

### Open Question 3
- Question: How does RadDQN perform in real-world radiation mapping scenarios compared to simulated environments?
- Basis in paper: [inferred] The paper acknowledges that field experiments with real-time radiation measurements are planned, indicating this has not yet been tested.
- Why unresolved: The study relies entirely on simulated 2D grid environments without validation in physical, real-world radiation-contaminated areas.
- What evidence would resolve it: Comparative performance data between RadDQN predictions and actual radiation exposure measurements in real contaminated zones.

## Limitations
- The exploration strategies (exppr, expr) lack precise implementation details for "future radiation exposure" calculation, making faithful reproduction challenging
- Adaptive target network update frequency mechanism requires trial-and-error tuning, with no clear analytical guidance for optimal parameter selection
- The inverse-square law assumption may not hold for complex radiation field distributions or varying atmospheric conditions

## Confidence
- High confidence: The radiation-aware reward function concept and its core components (inverse-square law, exit proximity term) are well-specified and theoretically sound
- Medium confidence: The general approach to adaptive exploration and target network updates is described, but implementation specifics are ambiguous
- Low confidence: Performance claims relative to vanilla DQN lack statistical significance testing and error bars in reported results

## Next Checks
1. Implement controlled experiments comparing RadDQN with baseline methods (vanilla DQN, heuristic pathfinding) across multiple random seeds to establish statistical significance of performance improvements
2. Test RadDQN's generalization to environments with non-inverse-square radiation fields (e.g., linear attenuation, scattering effects) to validate the robustness of the reward function assumptions
3. Conduct ablation studies isolating the contributions of individual components (radiation-aware reward, exploration strategies, adaptive updates) to identify which mechanisms drive performance gains