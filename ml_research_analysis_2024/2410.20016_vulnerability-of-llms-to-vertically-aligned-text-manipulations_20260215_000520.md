---
ver: rpa2
title: Vulnerability of LLMs to Vertically Aligned Text Manipulations
arxiv_id: '2410.20016'
source_url: https://arxiv.org/abs/2410.20016
tags:
- uni00000013
- text
- uni00000011
- llms
- uni00000044
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the vulnerability of large language models
  (LLMs) to vertically formatted text inputs, where key words are arranged vertically
  instead of horizontally. The authors find that such formatting significantly degrades
  LLM performance across multiple text classification tasks, including sentiment analysis
  and toxic content detection.
---

# Vulnerability of LLMs to Vertically Aligned Text Manipulations

## Quick Facts
- arXiv ID: 2410.20016
- Source URL: https://arxiv.org/abs/2410.20016
- Reference count: 16
- Primary result: Vertical text formatting significantly degrades LLM accuracy across multiple classification tasks

## Executive Summary
This paper reveals that large language models (LLMs) are vulnerable to vertically formatted text inputs, where key words are arranged vertically instead of horizontally. The authors demonstrate that this simple manipulation can substantially lower LLM performance across five text classification tasks, including sentiment analysis and toxic content detection. Their analysis shows that tokenization and attention mechanisms fail to properly process vertically formatted words, leading to misclassification. While Chain-of-Thought reasoning does not mitigate this vulnerability, few-shot learning with manual analysis proves effective in improving model robustness.

## Method Summary
The authors evaluate five text classification datasets (SST-2, CoLA, QNLI, Rotten Tomatoes, Jigsaw Toxicity) using both original horizontal text and vertically formatted versions where key words are arranged vertically. They employ zero-shot prediction to compare performance, analyze tokenization patterns and attention matrices to understand failure mechanisms, and test few-shot learning with manual analysis as a mitigation strategy. The word selection uses prompt-based extraction, and vertical transformation converts key words into vertical arrangements with added spaces and line breaks.

## Key Results
- Vertical formatting causes accuracy drops of 15-20% across all five text classification datasets
- Tokenization increases token count significantly (e.g., 5-letter word becomes 15 tokens when vertical)
- Attention weights between vertically split tokens and class labels drop sharply, explaining misclassification
- Few-shot learning with manual analysis effectively improves robustness, while Chain-of-Thought reasoning does not help

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vertical formatting disrupts the natural left-to-right, top-to-bottom tokenization sequence expected by LLMs
- Mechanism: The tokenizer splits vertically formatted text into many more tokens due to added spaces and line breaks, breaking the semantic cohesion of key words
- Core assumption: Tokenization is order-sensitive and expects contiguous text in natural reading order
- Evidence anchors:
  - "modifying input formats, such as vertically aligning words... can substantially lower accuracy in text classification tasks"
  - "The tokenizer used by LLMs encodes input text in this sequence... when input vertically, it converts into a token sequence of length 15 due to the multiple spaces and line breaks"
- Break condition: If the tokenizer is explicitly trained to handle vertical formatting, the increased token count would not degrade meaning

### Mechanism 2
- Claim: Attention weights linking vertically split tokens to their semantic meaning are lost or severely weakened
- Mechanism: In the attention matrix, the original token (e.g., "miserable") had strong attention to the "negative" class; when split into many vertical tokens, the attention between these tokens and the class label drops, causing misclassification
- Core assumption: Attention matrices encode semantic relationships that rely on coherent token sequences
- Evidence anchors:
  - "our analysis of tokenization and attention patterns provides insight into the underlying reasons for LLM performance degradation when handling vertical input"
  - "when the word 'miserable' is split and input in a vertical format, its components fail to establish a strong connection with 'negative'"
- Break condition: If vertical tokens are reassembled by a preprocessing step, attention links could be restored

### Mechanism 3
- Claim: Standard LLM training data lacks examples of vertically formatted text, so models never learn to parse it
- Mechanism: Pre-training corpora contain only horizontal text; vertical formatting is an out-of-distribution pattern the model has not encountered, leading to poor generalization
- Core assumption: Models cannot infer structure from unseen formatting patterns without explicit training
- Evidence anchors:
  - "This vulnerability, where straightforward text can substantially deceive language models, raises concerns..."
  - "the tokenization conventions of LLMs... and the lack of relevant pre-training data have impaired their ability to understand vertical text"
- Break condition: If the model is fine-tuned on vertically formatted examples, it could learn to parse them correctly

## Foundational Learning

- Concept: Tokenization mechanics in transformers
  - Why needed here: Understanding how text is split into tokens and why vertical formatting increases token count is central to diagnosing the failure
  - Quick check question: If a 5-letter word becomes 15 tokens when written vertically, what caused the increase?

- Concept: Attention matrix interpretation
  - Why needed here: Attention weights reveal how the model connects tokens to semantic classes; seeing their degradation explains misclassification
  - Quick check question: In an attention matrix, what would a strong weight from a token to the "toxic" class indicate about that token's role?

- Concept: Out-of-distribution generalization limits
  - Why needed here: LLMs perform poorly on unseen text formats because they lack relevant training examples; this frames why vertical text is a vulnerability
  - Quick check question: If a model never saw vertically formatted text during training, what is its expected performance on such input?

## Architecture Onboarding

- Component map: Input text -> Tokenizer -> Token embeddings -> Self-attention -> Contextualized representations -> Classification head
- Critical path: Input text → Tokenizer → Token embeddings → Self-attention → Contextualized representations → Classification head
- Design tradeoffs: Full tokenization vs. preprocessing to preserve word integrity; training on more diverse formats vs. computational cost
- Failure signatures: Accuracy drop proportional to number of vertically formatted key words; attention weights between key tokens and class tokens drop sharply
- First 3 experiments:
  1. Measure token count and sequence length difference between horizontal and vertical formatting for a fixed word
  2. Visualize attention weight heatmaps for a key token vs. class token in both formats
  3. Train a small transformer on vertically formatted text and compare its robustness to the baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would fine-tuning be compared to few-shot learning in mitigating LLM vulnerability to vertically formatted text?
- Basis in paper: The paper mentions that few-shot learning with manual analysis is effective, but does not explore fine-tuning due to dataset and GPU limitations
- Why unresolved: The paper explicitly states they did not assess fine-tuning due to resource constraints, leaving open whether this approach would be more effective than few-shot learning
- What evidence would resolve it: Comparative experiments measuring LLM performance on vertically formatted text before and after fine-tuning versus few-shot learning approaches

### Open Question 2
- Question: Does vertical text formatting affect LLM performance in text generation tasks, and if so, to what extent?
- Basis in paper: The limitations section explicitly states they did not assess the impact on text generation tasks
- Why unresolved: The paper focuses exclusively on text classification tasks, leaving the impact on generation tasks unexplored
- What evidence would resolve it: Systematic experiments measuring LLM generation quality, coherence, and task completion when vertical formatting is applied to prompts

### Open Question 3
- Question: What specific training data characteristics make LLMs vulnerable to vertical text formatting?
- Basis in paper: The paper notes that the vulnerability arises from pre-training data and tokenization mechanisms but does not analyze specific data characteristics
- Why unresolved: The paper identifies tokenization and attention matrix issues but does not investigate the underlying data distribution or content that contributes to this vulnerability
- What evidence would resolve it: Analysis of pre-training corpora to identify frequency and patterns of vertically formatted text, and correlation with model vulnerability levels

## Limitations
- Small sample size (100 test samples per dataset) may not capture full variability in LLM performance
- Word selection method using prompt-based extraction could introduce selection bias
- Focus on English text classification limits generalizability to other languages or multimodal scenarios
- Vertical formatting implementation details not fully specified, affecting reproducibility

## Confidence

**High Confidence**: The core finding that vertically formatted text degrades LLM performance across multiple classification tasks is well-supported by empirical evidence and analysis of tokenization and attention mechanisms.

**Medium Confidence**: The effectiveness of few-shot learning with manual analysis in improving model robustness is demonstrated but could benefit from more systematic evaluation across different manual analysis approaches.

**Low Confidence**: The assertion that Chain-of-Thought reasoning does not mitigate the vulnerability is based on limited experimentation and may not hold across different CoT prompting strategies or model architectures.

## Next Checks

1. **Tokenization Analysis Validation**: Replicate the token count comparison between horizontal and vertical formatting for a diverse set of 50 key words across different languages and scripts, measuring the correlation between token count increase and accuracy degradation.

2. **Attention Mechanism Verification**: Generate attention weight heatmaps for 20 vertically formatted samples from each dataset, quantifying the drop in attention weights between key tokens and their corresponding class tokens, and compare against horizontal formatting baselines.

3. **Robustness Improvement Testing**: Implement and evaluate three different few-shot learning approaches (manual analysis, automated template generation, and synthetic data augmentation) across all five datasets, measuring which method provides the most consistent accuracy improvement for vertically formatted inputs.