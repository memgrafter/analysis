---
ver: rpa2
title: 'MLPs Compass: What is learned when MLPs are combined with PLMs?'
arxiv_id: '2401.01667'
source_url: https://arxiv.org/abs/2401.01667
tags:
- mlps
- probing
- tasks
- bert
- plms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether adding simple Multilayer Perceptrons
  (MLPs) to pre-trained language models (PLMs) like BERT can enhance their ability
  to capture linguistic structures. The authors design a probing framework that freezes
  the PLM layers and inserts MLP components, then evaluates performance on 10 linguistic
  tasks across surface, syntactic, and semantic levels.
---

# MLPs Compass: What is learned when MLPs are combined with PLMs?

## Quick Facts
- arXiv ID: 2401.01667
- Source URL: https://arxiv.org/abs/2401.01667
- Authors: Li Zhou; Wenyu Chen; Yong Cao; Dingyi Zeng; Wanlong Liu; Hong Qu
- Reference count: 30
- Primary result: Adding simple MLPs to pre-trained language models consistently improves linguistic structure comprehension across surface, syntactic, and semantic levels.

## Executive Summary
This paper investigates whether adding simple Multilayer Perceptrons (MLPs) to pre-trained language models (PLMs) like BERT can enhance their ability to capture linguistic structures. The authors design a probing framework that freezes the PLM layers and inserts MLP components, then evaluates performance on 10 linguistic tasks across surface, syntactic, and semantic levels. Results show that MLPs consistently improve PLM performance, especially on syntactic and semantic tasks, and that improvements are more pronounced in higher PLM layers. The study concludes that even basic MLPs can strengthen PLMs' language structure comprehension, offering insights for designing PLM variants that emphasize diverse linguistic features.

## Method Summary
The study freezes BERT encoder weights and adds MLP blocks between the PLM and probe classifier. The MLP block includes residual connections and is trained while keeping the PLM weights frozen. Experiments use 10 linguistic probing tasks (surface: SentLen, WC; syntactic: TreeDepth, TopConst, BShift; semantic: Tense, SubjNum, ObjNum, SOMO, CoordInv) with 100k training and 10k test sentences per task. The MLP architecture consists of two linear layers with residual connections, and models are trained using Adam optimizer with early stopping.

## Key Results
- MLPs consistently improve PLM performance on linguistic probing tasks across all three levels (surface, syntactic, semantic)
- Improvements are more pronounced in higher PLM layers compared to lower layers
- MLPs show particular strength in capturing syntactic and semantic information rather than surface features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLPs enhance PLMs by capturing linguistic structure beyond the attention mechanism
- Mechanism: MLPs operate as a separate module that extracts syntactic and semantic features from PLM hidden states without altering the original representations
- Core assumption: PLM layers encode linguistic information that can be better organized and made more accessible through MLP transformations
- Evidence anchors:
  - [abstract] "MLPs can indeed enhance the comprehension of linguistic structure by PLMs"
  - [section] "MLPs are a foundational neural network component in model design, playing a crucial role in various adaptations"
  - [corpus] Weak - no direct corpus evidence about MLPs enhancing linguistic structure
- Break condition: If MLP layers become too deep or complex, they may overfit to probing tasks rather than generalize across linguistic structures

### Mechanism 2
- Claim: Higher PLM layers benefit more from MLP enhancement than lower layers
- Mechanism: Higher layers contain more abstract linguistic representations that MLPs can reorganize more effectively
- Core assumption: Abstract linguistic features in upper layers are more amenable to MLP transformation than surface-level features in lower layers
- Evidence anchors:
  - [abstract] "improvements are more pronounced in higher PLM layers"
  - [section] "it is easier to show consistent improvements based on high-layer representations"
  - [corpus] Weak - no corpus evidence about layer-specific MLP benefits
- Break condition: If MLP enhancement disrupts the natural hierarchical progression of linguistic abstraction in PLMs

### Mechanism 3
- Claim: MLPs excel at capturing syntactic and semantic information compared to surface information
- Mechanism: MLP transformations preserve and enhance hierarchical linguistic structures better than linear surface features
- Core assumption: Syntactic and semantic relationships have inherent hierarchical structure that MLPs can model more effectively
- Evidence anchors:
  - [abstract] "MLPs are better at capturing both syntactic and semantic information"
  - [section] "MLPs' sensitivity to different layers is relatively moderate" suggesting focused improvement
  - [corpus] Weak - no corpus evidence about MLP performance on different linguistic levels
- Break condition: If MLP architecture doesn't preserve the compositional nature of linguistic structures

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanism
  - Why needed here: Understanding how PLMs encode linguistic information is crucial for interpreting MLP enhancements
  - Quick check question: What is the difference between how attention and MLPs process information in PLMs?

- Concept: Probing methodology and linguistic task categories
  - Why needed here: The paper uses specific probing tasks to measure linguistic knowledge capture
  - Quick check question: What are the three linguistic levels tested and how do they differ in terms of required linguistic knowledge?

- Concept: Layer-wise analysis and frozen encoder weights
  - Why needed here: The methodology involves analyzing each PLM layer separately while keeping weights frozen
  - Quick check question: Why might keeping encoder weights frozen be important for probing linguistic knowledge?

## Architecture Onboarding

- Component map: PLM layers (frozen) → MLP block → Residual connection → Probing classifier
- Critical path: Input sentence → PLM encoding → MLP transformation → Classifier output
- Design tradeoffs: Adding MLPs increases model complexity but may improve linguistic understanding; freezing PLM layers ensures probing doesn't adapt representations
- Failure signatures: No improvement in probing tasks, performance degradation in specific layers, overfitting to probing tasks
- First 3 experiments:
  1. Implement MLP block with residual connection on BERT-base and test on surface-level tasks
  2. Compare probing results across all 12 layers with and without MLPs
  3. Evaluate clustering performance using NMI to measure syntactic vs semantic capture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different MLP architectures (e.g., depth, width, activation functions) impact their ability to enhance PLMs' linguistic structure comprehension?
- Basis in paper: [explicit] The paper mentions "basic MLPs" and uses two-layer MLPs, but does not explore architectural variations.
- Why unresolved: The study uses a fixed MLP architecture and does not compare different MLP designs or their effects on linguistic structure capture.
- What evidence would resolve it: Systematic experiments comparing different MLP architectures (e.g., varying number of layers, hidden dimensions, activation functions) while keeping other factors constant, measuring their impact on linguistic probing tasks.

### Open Question 2
- Question: Do MLPs enhance PLMs' linguistic structure comprehension differently for various language families or linguistic phenomena?
- Basis in paper: [inferred] The study uses English BERT and probes for general linguistic structures, but does not investigate cross-linguistic differences.
- Why unresolved: The experiments are limited to English language and general linguistic structures without examining how MLPs might perform across different languages or specific linguistic phenomena.
- What evidence would resolve it: Comparative studies using PLMs pre-trained on different languages and probing for language-specific linguistic phenomena, measuring MLP-enhanced performance across language families.

### Open Question 3
- Question: What is the computational efficiency trade-off when adding MLPs to PLMs for enhanced linguistic structure comprehension?
- Basis in paper: [explicit] The paper notes that MLPs can be more efficient than GNNs but does not analyze the computational costs of adding MLPs to PLMs.
- Why unresolved: While the paper demonstrates improved performance, it does not quantify the additional computational resources required by the MLP components.
- What evidence would resolve it: Detailed analysis of training and inference time, memory usage, and parameter count for MLP-enhanced PLMs compared to baseline PLMs, across various hardware configurations.

## Limitations
- MLP architecture details (depth, width, activation functions) are not specified, making exact reproduction difficult
- Experiments limited to English BERT only, restricting generalizability to other PLMs or languages
- Frozen encoder approach may not reflect realistic scenarios where both PLM and MLP components are jointly trained

## Confidence
- High Confidence: The empirical observation that MLPs improve performance on linguistic probing tasks (ACC scores consistently higher with MLP additions)
- Medium Confidence: The claim that improvements are more pronounced in higher PLM layers
- Low Confidence: The mechanism explanation that MLPs specifically enhance syntactic and semantic capture through hierarchical reorganization

## Next Checks
1. **Architecture Sensitivity Analysis**: Systematically vary MLP depth (1-3 layers) and hidden dimensions (64, 128, 256) to determine optimal configurations and assess whether improvements are robust to architectural choices.

2. **Cross-Lingual Validation**: Replicate experiments with multilingual PLMs (e.g., mBERT or XLM-R) and non-English probing datasets to test generalizability beyond English BERT.

3. **Fine-tuning vs. Frozen Comparison**: Conduct controlled experiments where both PLM and MLP components are jointly fine-tuned, comparing against the frozen encoder approach to assess whether MLP benefits persist in realistic deployment scenarios.