---
ver: rpa2
title: Evolutionary Reinforcement Learning via Cooperative Coevolution
arxiv_id: '2404.14763'
source_url: https://arxiv.org/abs/2404.14763
tags:
- coerl
- policy
- learning
- cooperative
- subproblem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the poor scalability of evolutionary reinforcement
  learning (ERL) due to genetic operators when optimizing high-dimensional neural
  networks. The proposed CoERL algorithm uses cooperative coevolution to periodically
  decompose the policy optimization problem into subproblems and evolves populations
  of neural networks for each subproblem.
---

# Evolutionary Reinforcement Learning via Cooperative Coevolution

## Quick Facts
- arXiv ID: 2404.14763
- Source URL: https://arxiv.org/abs/2404.14763
- Authors: Chengpeng Hu; Jialin Liu; Xin Yao
- Reference count: 40
- Key outcome: CoERL outperforms seven state-of-the-art algorithms on six benchmark locomotion tasks, achieving highest average rank of 1.67.

## Executive Summary
This paper addresses the scalability limitations of evolutionary reinforcement learning (ERL) when optimizing high-dimensional neural networks. The proposed CoERL algorithm uses cooperative coevolution to decompose the policy optimization problem into subproblems, evolving populations of neural networks for each subproblem. Instead of traditional genetic operators, CoERL searches for partial gradients to update policies while maintaining consistency between parent and offspring behavior spaces. Experiments on six benchmark locomotion tasks demonstrate that CoERL outperforms seven state-of-the-art algorithms and baselines.

## Method Summary
CoERL periodically and adaptively decomposes the policy optimization problem into multiple subproblems using cooperative coevolution. For each subproblem, a population of neural networks is evolved, and partial gradients are searched instead of using genetic operators. This approach maintains consistency between behavior spaces of parents and offspring across generations. The algorithm also leverages experiences collected during evolution to improve sample efficiency through an additional MDP-based RL loop using the Soft Actor-Critic algorithm.

## Key Results
- CoERL achieves highest average rank of 1.67 across six benchmark locomotion tasks
- Outperforms seven state-of-the-art algorithms and baselines
- Ablation study verifies unique contributions of CoERL's core ingredients
- Effectively addresses scalability issues while improving sample efficiency during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing the high-dimensional policy optimization problem into subproblems improves scalability by reducing the complexity of each optimization task.
- Mechanism: The policy optimization problem is decomposed into m subproblems by grouping the neural network parameters. Each subproblem maintains its own population, and only the parameters within that subproblem are optimized, while the remaining parameters act as a "memory buffer".
- Core assumption: The neural network parameters can be grouped in a way that allows for effective optimization of each subproblem independently, while still maintaining overall policy performance.
- Evidence anchors:
  - [abstract] "CoERL periodically and adaptively decomposes the policy optimisation problem into multiple subproblems and evolves a population of neural networks for each of the subproblems."
  - [section 3.1] "CoERL decomposes the policy optimisation problem into m subproblems in the CC loop. The subproblem j of policy optimisation, θ⟨Ij⟩ indexed by parameters indices ⟨Ij⟩, and the quotient ˆθ⟨Ij⟩ = θ/θ⟨Ij⟩ constitute the entire parameter space..."
  - [corpus] Weak - no direct evidence found in corpus, but related work on cooperative coevolution suggests this approach can be effective for large-scale optimization.
- Break condition: If the parameter grouping is poor, leading to subproblems that are too interdependent or poorly defined, the overall policy performance may suffer.

### Mechanism 2
- Claim: Searching for partial gradients instead of using genetic operators maintains consistency between the behavior spaces of parents and offspring.
- Mechanism: Instead of traditional genetic operators (mutation, crossover), CoERL searches for partial gradients within each subproblem. This gradient-based approach ensures that the behavior of offspring remains consistent with their parents, as the updates are made in a proximal area of the parameter space.
- Core assumption: The partial gradients found within each subproblem are meaningful and lead to effective policy updates that improve performance.
- Evidence anchors:
  - [abstract] "Instead of using genetic operators, CoERL directly searches for partial gradients to update the policy. Updating policy with partial gradients maintains consistency between the behaviour spaces of parents and offspring across generations."
  - [section 3.2] "CoERL searches for partial gradients to update the policy. This decomposition, coupled with the use of partial gradients, ensures consistency between the behaviour spaces of parents and offspring at a reduced cost."
  - [corpus] Weak - no direct evidence found in corpus, but the paper argues this approach avoids the inconsistency issues of traditional genetic operators.
- Break condition: If the partial gradients are not meaningful or do not lead to effective policy updates, the overall performance may not improve.

### Mechanism 3
- Claim: Leveraging temporal information from collected experiences improves sample efficiency during training.
- Mechanism: CoERL maintains an additional MDP-based RL loop that uses the experiences collected by the actors in the evolution loop. This allows for better utilization of temporal information and improves sample efficiency compared to traditional evolution-based approaches that discard experiences.
- Core assumption: The experiences collected during evolution contain valuable temporal information that can be used to improve the policy through MDP-based RL.
- Evidence anchors:
  - [abstract] "The experiences collected by the population are then used to improve the entire policy, which enhances the sampling efficiency."
  - [section 3.3] "To fully utilise the collected experiences, we choose soft actor-critic (SAC) as the base algorithm in the RL loop... Experiences such as transitions produced by the individuals during the cooperative coevolution loop are collected in a replay buffer."
  - [corpus] Weak - no direct evidence found in corpus, but the paper argues this approach improves sample efficiency by leveraging temporal information.
- Break condition: If the experiences collected during evolution do not contain useful temporal information, or if the MDP-based RL loop does not effectively utilize this information, the sample efficiency gains may not materialize.

## Foundational Learning

- Markov Decision Process (MDP):
  - Why needed here: Understanding MDPs is crucial for grasping the reinforcement learning aspect of CoERL, as it defines the problem setting and the goal of maximizing cumulative reward.
  - Quick check question: What are the key components of an MDP, and how do they relate to the reinforcement learning problem?

- Cooperative Coevolution:
  - Why needed here: Cooperative coevolution is the core technique used by CoERL to decompose the policy optimization problem into subproblems and maintain populations for each subproblem.
  - Quick check question: How does cooperative coevolution differ from traditional evolutionary algorithms, and what are its advantages for large-scale optimization?

- Policy Gradient Methods:
  - Why needed here: Policy gradient methods, such as the Soft Actor-Critic used in CoERL's RL loop, are essential for understanding how the policy is updated using the collected experiences.
  - Quick check question: What is the basic idea behind policy gradient methods, and how do they differ from value-based methods in reinforcement learning?

## Architecture Onboarding

- Component map:
  - Cooperative Coevolution (CC) loop -> decomposes policy into subproblems, maintains populations, searches for partial gradients
  - Reinforcement Learning (RL) loop -> uses Soft Actor-Critic to improve policy using collected experiences
  - Neural Network -> represents policy being optimized, with parameters grouped into subproblems

- Critical path:
  1. Initialize the policy and neural network
  2. Decompose the policy into subproblems
  3. For each subproblem, sample a population and search for partial gradients to update the policy
  4. Collect experiences from the CC loop
  5. Use the collected experiences to improve the policy via the RL loop
  6. Repeat steps 2-5 for a number of generations

- Design tradeoffs:
  - Decomposition granularity: The number of subproblems affects the complexity of each optimization task but may also impact the overall policy performance if the subproblems are too interdependent
  - Population size: Larger populations may lead to better exploration but also increase computational cost
  - Partial gradient estimation: The method used to estimate partial gradients (e.g., using a factored Gaussian distribution) affects the accuracy and efficiency of the updates

- Failure signatures:
  - Poor policy performance: May indicate issues with the decomposition, partial gradient estimation, or the integration between the CC and RL loops
  - High variance in results: Could suggest instability in the optimization process or sensitivity to hyperparameters
  - Slow convergence: May point to inefficient exploration or suboptimal utilization of collected experiences

- First 3 experiments:
  1. Run CoERL on a simple continuous control task (e.g., Pendulum-v1) with a small neural network to verify basic functionality and identify any implementation issues
  2. Compare CoERL's performance to a baseline algorithm (e.g., SAC) on a more complex task (e.g., HalfCheetah-v2) to assess the effectiveness of the cooperative coevolution approach
  3. Perform an ablation study by removing the RL loop or changing the decomposition strategy to understand the impact of these components on overall performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can CoERL's performance in Swimmer-v2 be improved to match its success in other benchmark tasks?
- Basis in paper: [explicit] The paper notes that CoERL does not achieve outstanding performance in Swimmer-v2, despite improvements from its base components, and attributes this to potential issues with the base SAC algorithm and local optima.
- Why unresolved: The paper identifies the issue but does not provide a clear solution or experimental verification of potential fixes. The complexity of balancing direct and indirect coordination between the CC and RL loops is also mentioned as a challenge.
- What evidence would resolve it: Empirical results showing improved performance in Swimmer-v2 after implementing specific modifications, such as alternative decomposition strategies or enhanced coordination mechanisms between the CC and RL loops.

### Open Question 2
- Question: How can knowledge-based grouping techniques improve CoERL's decomposition strategy compared to random grouping?
- Basis in paper: [explicit] The paper suggests that knowledge-based grouping techniques, such as those that learn problem structure and variable dependency, could potentially improve CoERL's decomposition strategy over the current random grouping approach.
- Why unresolved: The paper does not implement or compare knowledge-based grouping techniques, leaving their potential benefits and practical implementation details unexplored.
- What evidence would resolve it: Comparative experimental results demonstrating the performance differences between random grouping and knowledge-based grouping techniques in CoERL across various benchmark tasks.

### Open Question 3
- Question: What is the impact of hyperparameter sensitivity on CoERL's performance, and how can it be systematically analyzed?
- Basis in paper: [explicit] The paper mentions that CoERL's hyperparameters are arbitrarily set and remain the same across all tasks, suggesting a need for systematic hyperparameter sensitivity analysis.
- Why unresolved: The paper does not provide a detailed hyperparameter sensitivity analysis or explore the impact of different hyperparameter settings on CoERL's performance across tasks.
- What evidence would resolve it: Results from a comprehensive hyperparameter sensitivity analysis, including performance metrics across different hyperparameter settings and their impact on task-specific outcomes.

## Limitations

- The choice of grouping strategy and collaboration mechanism (random grouping with single best) appears suboptimal compared to alternative configurations tested in supplementary materials
- Performance in Swimmer-v2 remains problematic, suggesting limitations when inheriting base SAC algorithm issues
- Hyperparameters are arbitrarily set and remain the same across all tasks, raising questions about task-specific optimization

## Confidence

- Claims about improved scalability and sample efficiency: Medium - supported by benchmark results but dependent on specific hyperparameter choices
- Claims about maintaining behavior space consistency: Low-Medium - theoretical justification exists but empirical validation is limited
- Claims about the unique contribution of each component: Low-Medium - ablation study shows effects but doesn't isolate relative importance

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary the population size, learning rates, and noise strength across tasks to identify whether the "arbitrarily set" parameters significantly impact performance and generalization.

2. **Alternative grouping strategies**: Implement and compare the frequency-based and position-based grouping methods mentioned in the supplementary materials against the random grouping used in the main experiments to quantify their impact on performance.

3. **Behavior space consistency validation**: Design experiments to empirically measure the consistency of behavior spaces between parents and offspring across generations, comparing the partial gradient approach against traditional genetic operators on the same tasks.