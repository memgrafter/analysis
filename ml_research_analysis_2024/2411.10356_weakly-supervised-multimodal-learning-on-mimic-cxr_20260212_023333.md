---
ver: rpa2
title: Weakly-Supervised Multimodal Learning on MIMIC-CXR
arxiv_id: '2411.10356'
source_url: https://arxiv.org/abs/2411.10356
tags:
- multimodal
- mmvm
- learning
- vaes
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the Multimodal Variational Mixture-of-Experts
  (MMVM) VAE on the MIMIC-CXR dataset for multimodal medical image analysis. The MMVM
  VAE introduces a soft-sharing mechanism allowing each modality to contribute flexibly
  to a shared posterior, improving latent representations.
---

# Weakly-Supervised Multimodal Learning on MIMIC-CXR

## Quick Facts
- arXiv ID: 2411.10356
- Source URL: https://arxiv.org/abs/2411.10356
- Reference count: 19
- Primary result: MMVM VAE achieves 73.3% AUROC on frontal views and 73.0% on lateral views with only 10% labeled data

## Executive Summary
This paper evaluates the Multimodal Variational Mixture-of-Experts (MMVM) VAE on the MIMIC-CXR dataset for multimodal medical image analysis. The MMVM VAE introduces a soft-sharing mechanism allowing each modality to contribute flexibly to a shared posterior, improving latent representations. Experiments compare the MMVM VAE against various multimodal VAEs and fully supervised classifiers on the task of diagnostic label prediction from chest X-rays. The MMVM VAE consistently outperforms baselines, achieving an average AUROC of 73.3% on frontal views and 73.0% on lateral views, and shows greater stability across varying levels of label availability. Notably, it matches or surpasses fully supervised methods while requiring only 10% of labeled data. These results demonstrate the MMVM VAE's strong potential for weakly-supervised multimodal learning in medical applications where labeled data is scarce.

## Method Summary
The MMVM VAE employs a mixture-of-experts architecture with soft-sharing mechanisms to learn robust multimodal representations from chest X-ray images. The model uses a variational approach where each modality (frontal and lateral X-ray views) contributes to a shared posterior distribution through expert networks. The soft-sharing mechanism allows modalities to contribute at different levels depending on their relevance and quality, rather than forcing equal contributions. This architecture is trained using variational inference objectives that maximize the evidence lower bound while incorporating label information through a weakly-supervised learning framework. The model is evaluated on the MIMIC-CXR dataset, comparing performance across different levels of label availability against both multimodal VAE baselines and fully supervised classifiers.

## Key Results
- MMVM VAE achieves 73.3% average AUROC on frontal chest X-rays and 73.0% on lateral views
- Outperforms all multimodal VAE baselines across all label availability scenarios
- Matches or exceeds fully supervised methods while using only 10% of labeled data
- Demonstrates superior stability and performance consistency compared to baseline models

## Why This Works (Mechanism)
The soft-sharing mechanism in MMVM VAE allows each modality to contribute flexibly to the shared posterior based on its quality and relevance. This adaptive contribution prevents poor-quality modalities from degrading overall representation learning, while still leveraging complementary information when available. The mixture-of-experts architecture enables specialized processing of each modality before integration, capturing modality-specific patterns that improve diagnostic accuracy. The weakly-supervised learning framework allows the model to learn from limited labeled data by leveraging unlabeled samples to regularize the latent space and improve generalization.

## Foundational Learning
- Variational Inference: Understanding the evidence lower bound and how it guides learning in latent variable models
  - Why needed: Forms the theoretical foundation for training VAEs and optimizing the trade-off between reconstruction and regularization
  - Quick check: Can you explain the ELBO formula and its components?

- Multimodal Learning: Principles of integrating information from multiple data sources or modalities
  - Why needed: Essential for combining frontal and lateral X-ray views to capture comprehensive diagnostic information
  - Quick check: What are the key challenges in multimodal learning compared to single-modal approaches?

- Mixture-of-Experts: Architecture where different "experts" handle different aspects of the input space
  - Why needed: Allows specialized processing of each modality while maintaining coordinated learning
  - Quick check: How does the mixture-of-experts differ from traditional ensemble methods?

## Architecture Onboarding

Component Map: Input Images -> Expert Networks -> Soft-Sharing Mechanism -> Shared Posterior -> Decoder -> Output

Critical Path: Image Encoder → Expert Networks → Soft-Sharing Layer → Latent Space → Decoder → Diagnostic Prediction

Design Tradeoffs: The soft-sharing mechanism trades off perfect independence of modalities for better integration of complementary information, while the mixture-of-experts architecture increases parameter complexity but enables more specialized feature extraction compared to simple concatenation approaches.

Failure Signatures: Poor-quality input modalities may still contribute too much if the soft-sharing weights aren't properly calibrated; over-reliance on one modality could occur if the sharing mechanism becomes too asymmetric; training instability may arise from the complex interaction between multiple expert networks.

First Experiments:
1. Train MMVM VAE on MIMIC-CXR with 10% labeled data and evaluate AUROC on both frontal and lateral views
2. Compare MMVM VAE performance against single-modal VAE baselines using only frontal or lateral views
3. Test model sensitivity to label noise by evaluating performance with artificially corrupted labels

## Open Questions the Paper Calls Out
None

## Limitations
- Lacks direct comparison with state-of-the-art fully supervised methods on identical data constraints
- Absence of ablation studies specifically isolating the soft-sharing mechanism's contribution
- Single-dataset evaluation limits generalizability to other medical imaging contexts
- Evaluation metrics limited to AUROC without precision-recall or F1-score analysis

## Confidence
- Performance claims (High): The reported AUROC scores of 73.3% (frontal) and 73.0% (lateral) are presented with sufficient methodological detail and comparison to multiple baselines
- Weakly-supervised superiority claims (Medium): While the paper demonstrates strong performance with limited labeled data, the comparison to fully supervised methods is indirect
- Generalizability claims (Low): Given the single-dataset evaluation and absence of cross-domain validation, confidence in broader applicability claims remains low

## Next Checks
1. Conduct head-to-head comparison between MMVM VAE and fully supervised classifiers using identical 10% labeled data subsets to directly validate relative performance claims
2. Implement ablation studies removing the soft-sharing mechanism to isolate its specific contribution to performance improvements
3. Evaluate the model on additional multimodal medical datasets (e.g., radiology reports paired with imaging) to assess generalizability across different medical domains