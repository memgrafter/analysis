---
ver: rpa2
title: Does Faithfulness Conflict with Plausibility? An Empirical Study in Explainable
  AI across NLP Tasks
arxiv_id: '2404.00140'
source_url: https://arxiv.org/abs/2404.00140
tags:
- explanations
- faithfulness
- gpt-4
- methods
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the relationship between faithfulness and
  plausibility in explainable AI methods for NLP tasks. Using GPT-4 to generate expert-level
  explanations, the authors compare traditional explainability methods (Shapley value,
  LIME, attention-based, and gradient-based) against these human-like interpretations
  across sentiment analysis, intent detection, and topic labeling tasks.
---

# Does Faithfulness Conflict with Plausibility? An Empirical Study in Explainable AI across NLP Tasks

## Quick Facts
- arXiv ID: 2404.00140
- Source URL: https://arxiv.org/abs/2404.00140
- Authors: Xiaolei Lu; Jianghong Ma
- Reference count: 40
- Key outcome: Shapley value and LIME achieve both high faithfulness and significant overlap with GPT-4's explanations, outperforming attention-based and gradient-based methods

## Executive Summary
This study investigates whether faithfulness and plausibility in explainable AI methods for NLP tasks are conflicting objectives. Using GPT-4 to generate expert-level explanations, the authors compare traditional explainability methods (Shapley value, LIME, attention-based, and gradient-based) against human-like interpretations across sentiment analysis, intent detection, and topic labeling tasks. The primary finding is that perturbation-based methods like Shapley value and LIME can simultaneously optimize for both faithfulness and plausibility, challenging the common assumption that these objectives are inherently in tension.

## Method Summary
The authors fine-tune BERT-base and RoBERTa-base models on three NLP tasks (SST-2, SNIPS, 20Newsgroups) and generate explanations using four different attribution methods: Shapley value, LIME, attention-based, and gradient-based. GPT-4 serves as a proxy for expert explanations, generating benchmark interpretations that are compared against traditional methods. The study evaluates explanations using faithfulness metrics (Log-odds Ratio, Sensitivity Frequency, Coverage Metric) and plausibility metrics (Rank Correlation, Overlap Rate), finding that perturbation-based methods outperform gradient-based and attention-based approaches in achieving both high faithfulness and plausibility.

## Key Results
- Shapley value and LIME methods achieve high faithfulness scores across all three NLP tasks
- These perturbation-based methods show significant overlap with GPT-4-generated explanations
- Attention-based and gradient-based methods consistently underperform on both faithfulness and plausibility metrics
- Rank Correlation scores between methods and GPT-4 range from 0.71 to 0.77 for BERT and RoBERTa respectively on SST-2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perturbation-based methods like Shapley value and LIME can simultaneously optimize faithfulness and plausibility.
- Mechanism: These methods generate explanations by measuring how feature changes affect model outputs through controlled perturbations, creating explanations that reflect both model behavior and human-understandable patterns.
- Core assumption: Perturbation-based methods inherently capture both model-specific inference patterns and human-interpretable feature importance.
- Evidence anchors:
  - [abstract] "traditional perturbation-based methods Shapley value and LIME could attain greater faithfulness and plausibility"
  - [section] "SV , LIME and GPT-4 outperform the selected gradient-based and attention-based methods"
  - [corpus] "weak corpus signal for perturbation-based methods specifically, though related explainability benchmarks exist"
- Break condition: If perturbation methods fail to capture complex non-linear relationships or if human understanding diverges significantly from model behavior.

### Mechanism 2
- Claim: GPT-4 can serve as a reliable proxy for expert-level explanations in evaluating plausibility.
- Mechanism: GPT-4's strong performance on consistency verification tasks makes it suitable for generating benchmark explanations that capture domain expert reasoning.
- Core assumption: Large language models can reliably simulate expert-level reasoning across diverse NLP tasks.
- Evidence anchors:
  - [section] "Rank Correlation scores are 0.71 and 0.77 for BERT and RoBERTa, respectively, on SST-2"
  - [section] "demonstrate the quality of GPT-4 in the expert role"
  - [corpus] "weak corpus signal for GPT-4 as explanation benchmark, though related works exist on LLM evaluation"
- Break condition: If GPT-4's explanations consistently diverge from actual human expert reasoning or fail on specialized domains.

### Mechanism 3
- Claim: Faithfulness and plausibility are complementary rather than conflicting objectives in explanation methods.
- Mechanism: Explanation methods that accurately reflect model behavior (faithfulness) tend to align with human understanding patterns (plausibility) when properly designed.
- Core assumption: Human interpretability patterns often align with actual model decision processes in NLP tasks.
- Evidence anchors:
  - [abstract] "our findings suggest that rather than optimizing for one dimension at the expense of the other"
  - [section] "The explainability method could achieve a high overlap rate in identifying influential features"
  - [corpus] "moderate corpus support for complementary relationship between faithfulness and plausibility"
- Break condition: If explanations that are highly faithful to model behavior consistently produce implausible human interpretations.

## Foundational Learning

- Concept: Feature attribution methods
  - Why needed here: Understanding how different explanation methods quantify feature importance is crucial for interpreting the comparative results
  - Quick check question: What's the key difference between perturbation-based and gradient-based attribution methods?

- Concept: Faithfulness vs plausibility evaluation
  - Why needed here: The study's core contribution hinges on understanding how these two dimensions interact and can be measured
  - Quick check question: How does the Log-odds metric capture faithfulness in explanation methods?

- Concept: NLP task characteristics
  - Why needed here: Different NLP tasks (sentiment analysis, intent detection, topic labeling) may require different explanation approaches
  - Quick check question: Why might perturbation-based methods perform differently across these three task types?

## Architecture Onboarding

- Component map: Dataset loading -> Tokenization -> Model inference -> Explanation generation -> Faithfulness evaluation -> Plausibility evaluation -> Comparison
- Critical path: Dataset -> Model inference -> Explanation generation -> Faithfulness evaluation -> Plausibility evaluation -> Comparison
- Design tradeoffs:
  - Computational cost vs explanation quality (perturbation methods vs gradient-based)
  - Human evaluation vs automated metrics (GPT-4 as proxy)
  - Task specificity vs generalizability of findings
- Failure signatures:
  - Low overlap between methods and GPT-4 explanations
  - Inconsistent faithfulness scores across different metrics
  - Poor performance of perturbation methods on certain tasks
- First 3 experiments:
  1. Run Shapley value and LIME on SST-2 task, compare faithfulness scores
  2. Generate GPT-4 explanations for the same instances, measure RC and OR
  3. Test attention-based methods on SNIPS task to verify performance differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can perturbation-based methods be optimized to achieve high performance in both faithfulness and plausibility across diverse NLP tasks?
- Basis in paper: explicit
- Why unresolved: The paper demonstrates that Shapley value and LIME can achieve both high faithfulness and significant overlap with GPT-4's explanations, but it doesn't explore optimization techniques for these methods across a wider range of NLP tasks.
- What evidence would resolve it: Systematic experiments comparing the performance of Shapley value and LIME on a diverse set of NLP tasks, along with optimization techniques to improve their faithfulness and plausibility.

### Open Question 2
- Question: What is the underlying reason for the weak correlation between the explainability methods and GPT-4's explanations?
- Basis in paper: inferred
- Why unresolved: The paper observes a weak correlation between the explainability methods and GPT-4's explanations but doesn't provide a detailed analysis of the reasons behind this observation.
- What evidence would resolve it: A thorough investigation into the factors contributing to the weak correlation, such as differences in the underlying reasoning processes or the limitations of the evaluation metrics used.

### Open Question 3
- Question: How can explainability algorithms be optimized to simultaneously achieve high levels of accuracy and user accessibility in their explanations?
- Basis in paper: explicit
- Why unresolved: The paper suggests that explainability algorithms can be optimized for dual objectives of faithfulness and plausibility but doesn't provide specific optimization techniques or guidelines.
- What evidence would resolve it: Research studies exploring various optimization techniques and their impact on the faithfulness and plausibility of explanations, along with user studies to assess the accessibility of the generated explanations.

## Limitations
- Reliance on GPT-4 as a proxy for expert explanations may introduce systematic biases
- Limited generalizability due to testing only BERT and RoBERTa on three specific NLP tasks
- Perturbation-based methods' performance may be sensitive to unoptimized hyperparameters

## Confidence

- **High confidence**: The complementary relationship between faithfulness and plausibility for perturbation-based methods (Shapley value, LIME)
- **Medium confidence**: GPT-4's effectiveness as a proxy for expert explanations, given the strong correlation results but potential for systematic biases
- **Medium confidence**: The superiority of perturbation-based methods over attention-based and gradient-based approaches, based on the specific experimental setup

## Next Checks
1. Test the same methodology on additional NLP tasks (e.g., named entity recognition, question answering) to verify the generalizability of the findings
2. Compare GPT-4-generated explanations against human expert annotations on a subset of instances to validate the proxy assumption
3. Perform ablation studies on perturbation method hyperparameters to determine sensitivity and optimal configurations