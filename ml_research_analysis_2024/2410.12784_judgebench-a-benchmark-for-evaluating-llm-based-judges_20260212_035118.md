---
ver: rpa2
title: 'JudgeBench: A Benchmark for Evaluating LLM-based Judges'
arxiv_id: '2410.12784'
source_url: https://arxiv.org/abs/2410.12784
tags:
- judges
- response
- responses
- arxiv
- llm-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JudgeBench, a novel benchmark for evaluating
  LLM-based judges that focuses on assessing factual and logical correctness rather
  than stylistic preferences. Unlike existing benchmarks that primarily measure alignment
  with human preferences, JudgeBench specifically tests judges' ability to distinguish
  objectively correct from incorrect responses to challenging questions across knowledge,
  reasoning, math, and coding domains.
---

# JudgeBench: A Benchmark for Evaluating LLM-based Judges

## Quick Facts
- arXiv ID: 2410.12784
- Source URL: https://arxiv.org/abs/2410.12784
- Reference count: 40
- Key outcome: JudgeBench evaluates LLM-based judges on distinguishing factually correct from incorrect responses to challenging questions across knowledge, reasoning, math, and coding domains.

## Executive Summary
JudgeBench introduces a novel benchmark specifically designed to evaluate LLM-based judges on their ability to distinguish factually and logically correct responses from subtly incorrect ones. Unlike existing benchmarks that measure alignment with human preferences, JudgeBench focuses on objective correctness by transforming challenging datasets with ground truth labels into response pairs. The benchmark reveals that even strong models like GPT-4o struggle significantly with this task, performing only slightly better than random guessing (56.57% accuracy), while many fine-tuned judges score below the 50% random baseline. This highlights fundamental limitations in current LLM-based evaluation systems and the need for improved reasoning capabilities.

## Method Summary
JudgeBench creates evaluation pairs by sampling multiple responses from a strong LLM for questions from challenging datasets with ground truth labels, then constructing pairs containing one correct and one subtly incorrect response. The benchmark uses a two-trial evaluation methodology where each response pair is evaluated twice with swapped order to mitigate positional bias, aggregating results to determine final verdicts. The pipeline leverages existing difficult datasets from MMLU-Pro, LiveBench, and LiveCodeBench, transforming them into 350 challenging response pairs across knowledge, reasoning, math, and coding domains.

## Key Results
- GPT-4o achieves only 56.57% accuracy, barely above random guessing baseline
- Many fine-tuned judges score below 50%, performing worse than random
- Strong model separability demonstrated with performance gaps of 6-12% between top and bottom performers
- Larger models generally outperform smaller ones, though gains diminish beyond certain sizes
- Judge performance correlates with model reasoning capabilities rather than pattern matching

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JudgeBench creates challenging evaluation pairs by leveraging existing datasets with ground truth labels and transforming them into response pairs where one answer is objectively correct and one contains subtle errors.
- Mechanism: The pipeline samples multiple responses from a strong LLM (e.g., GPT-4o) for each question, then constructs pairs only when there's at least one correct and one incorrect response. This ensures objective ground truth labels while creating difficult-to-distinguish pairs.
- Core assumption: If a model struggles to consistently generate correct responses to challenging questions, it will also struggle to differentiate between its correct and incorrect responses.
- Evidence anchors:
  - [abstract] "JudgeBench leverages a novel pipeline for converting existing difficult datasets into challenging response pairs with preference labels reflecting objective correctness."
  - [section 3] "Our main idea to achieve this objective is to leverage an existing challenging dataset with ground truth labels and develop a pipeline to transform it into a set of response pairs."
- Break condition: The mechanism breaks if the base model used for response generation is significantly weaker than the judges being evaluated, as this would create artificially difficult pairs that don't represent genuine reasoning challenges.

### Mechanism 2
- Claim: JudgeBench mitigates positional bias by evaluating each response pair twice with swapped order and aggregating the results.
- Mechanism: Each judge evaluates response pairs in both orders (A vs B, then B vs A), and the final verdict is determined by aggregating these judgments. Inconsistent decisions or ties in both trials are deemed incorrect.
- Core assumption: LLM-based judges exhibit positional bias where the order of presentation influences their decision-making.
- Evidence anchors:
  - [section 4] "Since our response pairs contain an objectively correct and incorrect response, the only valid decisions are A > B and A < B. However, in practice, some judges support a tie option: A = B."
  - [abstract] "To mitigate this, we evaluate the LLM-based judge twice, swapping the order of the response pairs in the second trial."
- Break condition: The mechanism breaks if judges consistently change their decisions based on response order, indicating severe positional bias that cannot be resolved through aggregation.

### Mechanism 3
- Claim: JudgeBench effectively separates models based on reasoning capabilities by using challenging problems that require advanced knowledge and logical thinking.
- Mechanism: The benchmark uses datasets from MMLU-Pro, LiveBench, and LiveCodeBench that are inherently difficult for both humans and models, ensuring that performance differences reflect actual reasoning ability rather than surface-level patterns.
- Core assumption: As AI models surpass human capabilities, their responses become harder for both human and LLM-based judges to assess, creating a need for more rigorous evaluation frameworks.
- Evidence anchors:
  - [abstract] "Comprehensive evaluation shows JudgeBench poses a significantly greater challenge than previous benchmarks, with many strong models (e.g., GPT-4o) performing just slightly better than random guessing."
  - [section 4.2] "Our evaluation on JudgeBench highlights the limitations of current LLM-based judges in distinguishing between challenging response pairs."
- Break condition: The mechanism breaks if judges can achieve high accuracy through pattern matching or other non-reasoning strategies rather than genuine understanding of the content.

## Foundational Learning

- Concept: Objective ground truth evaluation
  - Why needed here: JudgeBench relies on having verifiable correct/incorrect responses to create meaningful evaluation pairs, unlike preference-based benchmarks that depend on subjective human judgment.
  - Quick check question: How does JudgeBench ensure that response pairs have objective ground truth labels rather than subjective preferences?

- Concept: Positional bias in pairwise evaluation
  - Why needed here: The benchmark's evaluation methodology explicitly addresses positional bias by evaluating pairs in both orders, which is crucial for obtaining reliable measurements.
  - Quick check question: Why does JudgeBench evaluate each response pair twice with swapped order?

- Concept: Dataset transformation for evaluation
  - Why needed here: The core innovation involves converting existing challenging datasets into evaluation pairs suitable for LLM judges, requiring understanding of both the source data and the transformation pipeline.
  - Quick check question: What is the key insight behind JudgeBench's pipeline for creating challenging response pairs?

## Architecture Onboarding

- Component map: JudgeBench consists of (1) source dataset selection, (2) response generation pipeline using a strong base model, (3) correctness verification system, (4) pair construction algorithm, (5) evaluation framework with positional bias mitigation, and (6) analysis tools for interpreting results.
- Critical path: The most critical path is generating challenging response pairs that maintain objective ground truth labels while being difficult to distinguish, as this directly determines the benchmark's effectiveness.
- Design tradeoffs: Using a single strong model for response generation creates consistency but introduces bias against that model's weaknesses; using multiple models would increase diversity but complicate the evaluation process and introduce self-enhancement bias.
- Failure signatures: Low model separability (all models performing similarly), high rates of inconsistent judgments between trials, or judges achieving near-random accuracy would indicate fundamental problems with the benchmark construction.
- First 3 experiments:
  1. Evaluate the pipeline's ability to generate challenging pairs by measuring the agreement between the base model's responses and human judges on the same pairs.
  2. Test positional bias mitigation by comparing judge performance on single-order vs double-order evaluation across different judge types.
  3. Analyze the correlation between base model strength and pair difficulty by generating pairs with models of varying capabilities and measuring judge performance on each set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal model size and architecture for LLM-based judges to balance accuracy and computational efficiency?
- Basis in paper: [inferred]
- Why unresolved: The paper shows that larger models generally perform better but does not explore the trade-offs between model size, reasoning ability, and computational costs.
- What evidence would resolve it: Systematic evaluation of judges across different model sizes and architectures, measuring both accuracy and inference time/compute costs.

### Open Question 2
- Question: How does the self-enhancement bias vary across different LLM-based judge architectures and prompting strategies?
- Basis in paper: [explicit]
- Why unresolved: While the paper mentions self-enhancement bias and attempts to mitigate it, it does not provide quantitative analysis of how this bias manifests across different judge types.
- What evidence would resolve it: Empirical measurement of bias magnitude across various judge architectures, including controlled experiments where judges evaluate responses from the same vs. different models.

### Open Question 3
- Question: What is the relationship between a judge's ability to verify solutions and its ability to solve problems independently?
- Basis in paper: [explicit]
- Why unresolved: The ablation study shows a correlation but does not establish causation or explore whether verification can be enhanced independently of problem-solving ability.
- What evidence would resolve it: Comparative analysis of judges trained specifically for verification vs. general-purpose models, measuring performance on JudgeBench and problem-solving benchmarks.

### Open Question 4
- Question: How does JudgeBench performance correlate with real-world LLM evaluation tasks across different domains?
- Basis in paper: [inferred]
- Why unresolved: The paper establishes JudgeBench's difficulty but does not validate its correlation with practical evaluation scenarios or downstream task performance.
- What evidence would resolve it: Validation studies comparing JudgeBench performance with human evaluation outcomes on real-world LLM outputs across multiple application domains.

## Limitations

- JudgeBench's effectiveness depends heavily on the strength and consistency of the base model used for response generation, potentially introducing bias
- The benchmark may not fully capture whether judges are using genuine reasoning versus pattern matching to achieve their scores
- As AI capabilities advance beyond human-level reasoning, even expert human verification of response correctness may become increasingly unreliable

## Confidence

**High Confidence**: The claim that JudgeBench creates more challenging evaluation pairs than existing benchmarks is well-supported by the reported performance of strong models like GPT-4o achieving only slightly better than random guessing accuracy. The methodology for positional bias mitigation through double evaluation is clearly described and reproducible.

**Medium Confidence**: The assertion that JudgeBench effectively separates models based on reasoning capabilities is supported by the reported performance gaps, but the paper doesn't provide extensive analysis of whether these differences reflect genuine reasoning improvements versus other factors like prompt engineering or model familiarity with the evaluation format.

**Low Confidence**: The claim that current LLM-based judges are fundamentally limited in their ability to distinguish challenging response pairs is based on performance results, but doesn't explore whether these limitations could be addressed through alternative evaluation approaches or training methods.

## Next Checks

1. **Base Model Sensitivity Analysis**: Systematically vary the strength of the base model used for response generation and measure how this affects judge performance across different model families to determine whether the benchmark's difficulty scales appropriately with base model capability.

2. **Pattern Exploitation Audit**: Analyze whether high-performing judges on JudgeBench are exploiting identifiable patterns in the response pairs rather than genuine reasoning, using techniques like adversarial pair generation and ablation studies on different pair components.

3. **Human Expert Validation Study**: Conduct a controlled study with domain experts to verify the objective correctness of response pairs across different domains and difficulty levels, establishing ground truth reliability and identifying any systematic errors in the pair construction pipeline.