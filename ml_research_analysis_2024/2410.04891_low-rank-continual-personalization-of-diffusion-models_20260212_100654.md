---
ver: rpa2
title: Low-Rank Continual Personalization of Diffusion Models
arxiv_id: '2410.04891'
source_url: https://arxiv.org/abs/2410.04891
tags:
- task
- continual
- merging
- tasks
- adapters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of catastrophic forgetting in
  continual personalization of diffusion models using LoRA adapters. The authors evaluate
  four approaches: naive fine-tuning, merging with standard initialization, merging
  with orthogonal initialization, and magnitude-based weight selection.'
---

# Low-Rank Continual Personalization of Diffusion Models

## Quick Facts
- arXiv ID: 2410.04891
- Source URL: https://arxiv.org/abs/2410.04891
- Reference count: 40
- Primary result: Merge & initialization approach achieves best balance between plasticity and stability in continual personalization of diffusion models

## Executive Summary
This paper addresses catastrophic forgetting in continual personalization of diffusion models using LoRA adapters. The authors evaluate four approaches: naive fine-tuning, merging with standard initialization, merging with orthogonal initialization, and magnitude-based weight selection. Their results demonstrate that naive fine-tuning leads to severe forgetting, while all three merging-based methods effectively mitigate this issue. The merge & initialization approach achieves the best balance between plasticity and stability, with average CLIP-I alignment scores of 0.675 (objects) and 0.385 (styles), and average forgetting metrics of 0.026 (objects) and 0.131 (styles).

## Method Summary
The paper evaluates four approaches for continual personalization of diffusion models using LoRA adapters across 10 consecutive personalization tasks. The methods include: (1) naive fine-tuning without any forgetting mitigation, (2) merging LoRAs with standard initialization (reinitializing A and B matrices for each new task), (3) merging with orthogonal initialization (using SVD to orthogonalize weight updates), and (4) magnitude-based weight selection (retaining only highest magnitude parameters). The approach uses Stable Diffusion XL with LoRA rank 64, trained for 850 steps per task with learning rate 1e-5. Evaluation uses CLIP-I, DINO, and CSD alignment metrics across object and style personalization datasets.

## Key Results
- Naive fine-tuning causes catastrophic forgetting, with average CLIP-I scores dropping to 0.488 (objects) and 0.257 (styles)
- Merge & initialization achieves highest average scores: 0.675 (objects) and 0.385 (styles) with minimal forgetting (0.026 and 0.131)
- Orthogonal initialization shows high plasticity but poor stability, with forgetting scores of 0.218 (objects) and 0.242 (styles)
- Magnitude-based selection reduces forgetting but at significant performance cost, achieving scores of 0.503 (objects) and 0.280 (styles)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthogonal initialization of LoRA adapters reduces interference between tasks by directing weight updates into non-conflicting parameter subspaces.
- Mechanism: SVD is used to orthogonalize the columns of the A matrix at each new task, ensuring that the low-rank updates explore directions not previously utilized by earlier tasks.
- Core assumption: Task-specific weight updates occupy independent low-dimensional subspaces; orthogonality prevents overlap and interference.
- Evidence anchors: [abstract] "merging LoRAs initialized with orthogonal weights", [section] "we further extend the merging and reinitialization approach...by initializing...At weights' columns as orthogonal to columns of A1...t−1 matrix using Singular Value Decomposition (SVD)"

### Mechanism 2
- Claim: Reinitializing LoRA weights before each new task enables independent exploration of low-rank subspaces, reducing cross-task interference.
- Mechanism: Standard normal initialization of A and zero initialization of B create a fresh, task-specific adapter that fine-tunes from scratch, preventing accumulation of conflicting updates.
- Core assumption: Each task can be learned effectively from a random low-rank starting point without catastrophic interference from prior tasks.
- Evidence anchors: [abstract] "merging LoRAs initialized with orthogonal weights" and "merging through a selection of weights with the highest magnitude", [section] "we create a new LoRA adapter for each task with a standard initialization technique (A ∼ N (0, I), B = 0)"

### Mechanism 3
- Claim: Magnitude-based selection retains only the most salient parameters from each adapter, limiting interference while preserving key task features.
- Mechanism: After each task, weights with the highest magnitude across all LoRA matrices are kept, discarding weaker contributions that are more likely to cause interference.
- Core assumption: High-magnitude weights encode the most critical task-specific features; low-magnitude weights contribute noise or interference.
- Evidence anchors: [abstract] "merging through a selection of weights with the highest magnitude for the task", [section] "we run the MAGMAX selection of parameters with the highest magnitude by comparing the sum of already merged adapters"

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Continual fine-tuning without mitigation overwrites parameters needed for previous tasks, causing performance degradation.
  - Quick check question: What happens to model performance on earlier tasks after training on several new tasks without any regularization or merging?

- Concept: Low-rank adaptation (LoRA)
  - Why needed here: LoRA reduces the number of trainable parameters, making it feasible to store and merge multiple adapters while still allowing task-specific modifications.
  - Quick check question: How does LoRA modify the weight matrix during fine-tuning, and why does this help reduce memory usage?

- Concept: Orthogonalization via SVD
  - Why needed here: Ensures new task adapters explore parameter subspaces not used by previous adapters, reducing interference.
  - Quick check question: What property of the SVD decomposition is exploited to make new adapter updates orthogonal to previous ones?

## Architecture Onboarding

- Component map: Base diffusion model (frozen) -> LoRA adapter matrices (A and B per task) -> Merging strategy module (standard/orthogonal/magnitude) -> Evaluation pipeline (CLIP-I, DINO, CSD)

- Critical path: 1. Initialize base model and first task adapter, 2. Fine-tune adapter on task data, 3. Merge adapter into base (or store), 4. Reinitialize adapter for next task, 5. Repeat 2-4 for all tasks, 6. Evaluate final model on all tasks

- Design tradeoffs:
  - Reinitialization vs. continual adaptation: fresh exploration vs. preserving useful overlap
  - Orthogonalization vs. plasticity: reduced interference vs. possible loss of relevant directions
  - Magnitude selection vs. completeness: interference reduction vs. risk of losing subtle features

- Failure signatures:
  - Naïve continual fine-tuning: rapid degradation to baseline performance across all tasks
  - Orthogonal initialization: high plasticity but poor stability (high forgetting in style tasks)
  - Magnitude-based merging: low forgetting but reduced average scores due to limited adaptation

- First 3 experiments:
  1. Compare naïve continual fine-tuning vs. merge & initialization on a small 3-task sequence (objects only) to observe forgetting patterns
  2. Test orthogonal initialization on aligned vs. diverse task sets to quantify when orthogonality helps/hurts
  3. Measure magnitude-based selection sensitivity by varying the threshold for weight retention and observing impact on average score vs. forgetting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the rank of LoRA adapters (currently fixed at 64) affect catastrophic forgetting in continual personalization across different numbers of tasks?
- Basis in paper: [inferred] The paper uses rank 64 but doesn't explore how varying rank impacts plasticity vs stability tradeoff across the four evaluated methods.
- Why unresolved: The authors only tested a single rank value (64) as recommended by Hu et al. (2021) for self-attention layers, but didn't investigate whether lower/higher ranks might better balance task interference.
- What evidence would resolve it: Systematic experiments varying LoRA rank (e.g., 32, 64, 128) across the four methods while measuring average forgetting and average score metrics would reveal optimal rank for continual personalization.

### Open Question 2
- Question: Does the magnitude-based merging approach have potential when applied differently than sequential fine-tuning with M AGMAX selection?
- Basis in paper: [explicit] The authors note that "selecting a limited number of weights hinders the precise merging of new generative traits" but this is based on their specific implementation where they merge after each task.
- Why unresolved: The paper only tested one variant of magnitude-based selection - merging after each task. Other variants (e.g., accumulating all adapters then selecting once, or using different selection thresholds) might perform better.
- What evidence would resolve it: Testing alternative magnitude-based selection strategies such as delayed selection after all tasks, or adaptive thresholds based on task difficulty, would determine if the poor performance is method-specific or inherent to magnitude-based selection.

### Open Question 3
- Question: What is the theoretical relationship between orthogonal initialization of LoRA adapters and task interference in high-dimensional weight spaces?
- Basis in paper: [explicit] The authors observe that orthogonal initialization leads to "high plasticity but low stability" and hypothesize this is due to "task vectors being more aligned in style personalization."
- Why unresolved: The paper provides empirical observations but lacks theoretical analysis of why orthogonalization causes higher forgetting in some domains while improving plasticity in others.
- What evidence would resolve it: Mathematical analysis of the dot products between task vectors in orthogonalized space, combined with empirical correlation between vector alignment and forgetting metrics across multiple domains, would clarify when orthogonal initialization helps vs harms.

## Limitations

- The magnitude-based selection method's exact implementation details are not fully specified, particularly how the MAGMAX selection is applied in the continual learning context.
- Evaluation relies heavily on alignment metrics (CLIP-I, DINO, CSD) without extensive qualitative validation of the generated images.
- The study focuses on two specific datasets (objects and styles) but does not explore how these findings generalize to other personalization domains or more complex task relationships.

## Confidence

- **High confidence**: The core finding that naive fine-tuning leads to catastrophic forgetting is well-established and consistently observed across both object and style personalization tasks. The superiority of merge & initialization approach over other methods is strongly supported by quantitative metrics.
- **Medium confidence**: The orthogonal initialization mechanism's effectiveness appears context-dependent, showing high plasticity but poor stability for style tasks. This suggests the approach may work well for some task types but not others, requiring careful consideration of task characteristics.
- **Low confidence**: The magnitude-based selection method shows promising forgetting reduction but at the cost of overall performance. The claim that high-magnitude weights always encode the most critical features needs further validation, as subtle but important features might be discarded.

## Next Checks

1. **Cross-dataset generalization**: Test the four methods on additional personalization domains (e.g., faces, artistic styles) to verify whether merge & initialization consistently outperforms other approaches across diverse task types.
2. **Ablation of magnitude threshold**: Systematically vary the threshold for weight retention in the magnitude-based selection method to identify optimal trade-offs between forgetting reduction and performance maintenance.
3. **Qualitative evaluation**: Conduct human perceptual studies to validate whether the alignment metrics (CLIP-I, DINO, CSD) accurately reflect image quality and personalization fidelity across all four methods.