---
ver: rpa2
title: A Good Score Does not Lead to A Good Generative Model
arxiv_id: '2401.04856'
source_url: https://arxiv.org/abs/2401.04856
tags:
- score
- function
- distribution
- samples
- empirical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper shows that even when a score-based generative model\
  \ (SGM) learns the score function well, it can fail to generate truly new samples\
  \ and instead produce outputs that are essentially blurred versions of training\
  \ data\u2014similar to kernel density estimation (KDE). Theoretically, they prove\
  \ that with the empirical optimal score function, SGM behaves like a KDE, leading\
  \ to strong memorization."
---

# A Good Score Does not Lead to A Good Generative Model

## Quick Facts
- arXiv ID: 2401.04856
- Source URL: https://arxiv.org/abs/2401.04856
- Reference count: 40
- Even with good score approximation, SGMs can produce only blurred versions of training data

## Executive Summary
This paper reveals a fundamental limitation of score-based generative models (SGMs): learning the score function well does not guarantee the generation of novel samples. The authors prove that with the empirical optimal score function, SGMs behave like kernel density estimation (KDE), leading to strong memorization of training data. This occurs because the optimal score function under conditional score matching is mathematically equivalent to the score function of a Gaussian KDE. The findings challenge current evaluation criteria for generative models and suggest the need for new theoretical frameworks that can assess true generative creativity beyond mere imitation ability.

## Method Summary
The paper analyzes score-based generative models by comparing the empirical optimal score function (learned through conditional score matching) with the true score function. They prove that the empirical optimal score function is equivalent to the score function of a Gaussian KDE, leading to generation of blurred training data. The analysis covers both cases with and without early stopping, showing that early stopping only produces copies of training data. Empirical validation is performed on synthetic Gaussian data and CIFAR10 images to demonstrate the memorization effects.

## Key Results
- SGMs with empirical optimal score function perform KDE, generating blurred versions of training data
- Sample complexity analysis shows that the empirical optimal score function achieves good L2 approximation but leads to memorization
- Early stopping produces only copies of training data, not novel samples
- The phenomenon holds for both synthetic and real image datasets

## Why This Works (Mechanism)

### Mechanism 1
SGMs with empirical optimal score function perform KDE on training data. The empirical optimal score function derived from minimizing conditional score matching is mathematically equivalent to the KDE score function. When used in reverse diffusion, this drives samples toward blurred training data versions. Core assumption: learned score function is global minimizer in sufficiently large hypothesis space. Evidence: Proposition 4.2 proves empirical optimal score equals KDE score function gradient.

### Mechanism 2
Good L2 score approximation doesn't guarantee novel samples. Theorem 2.3 provides total variation distance bounds ensuring distribution closeness but not sample novelty. The empirical optimal score achieves good L2 approximation (Theorem 3.1) but results in KDE behavior (Theorem 4.3). Core assumption: target distribution has bounded second moment and score can be approximated in L2 sense. Break condition: if evaluation metric changes to capture sample novelty.

### Mechanism 3
Forward/backward diffusion processes can be interpreted as KDE when initialized with empirical distribution. Proposition 4.1 shows forward diffusion PDE solution with empirical distribution is KDE. Backward process with empirical optimal score is equivalent to running SGM on this KDE. Core assumption: training samples satisfy ∥yi∥2 ≤ d. Evidence: Proposition 4.1 bounds total variation between backward process and KDE.

## Foundational Learning

- Concept: Kernel Density Estimation (KDE)
  - Why needed here: KDE is the key mechanism by which SGMs with empirical optimal score function fail to generate novel samples
  - Quick check question: What is the difference between sampling from the true distribution and sampling from its KDE?

- Concept: Score Matching and Conditional Score Matching
  - Why needed here: These are the methods used to learn the score function in SGMs and are crucial to understanding how the empirical optimal score function is derived
  - Quick check question: How does conditional score matching differ from regular score matching, and why is it used in practice?

- Concept: Stochastic Differential Equations (SDEs) and Partial Differential Equations (PDEs)
  - Why needed here: The diffusion process in SGMs is modeled by SDEs, which are related to PDEs. Understanding this relationship is crucial to understanding how the score function drives the generation process
  - Quick check question: How are the forward and backward diffusion processes related to the PDEs (1) and (2) in the paper?

## Architecture Onboarding

- Component map: Forward diffusion -> Score network -> Score matching -> Reverse diffusion -> Early stopping
- Critical path: Train score network using conditional score matching on noisy data, use learned score function in reverse diffusion to generate samples, evaluate generated samples for quality and novelty
- Design tradeoffs: Hypothesis space size vs. generalization (larger spaces may lead to better score approximation but increased overfitting risk), early stopping time δ (smaller δ reduces discretization error but increases singularity risk)
- Failure signatures: Generated samples are blurred versions of training data, generated samples have high concentration around training data points, score function approximation error doesn't decrease with increased training data
- First 3 experiments:
  1. Generate samples using empirical optimal score function on 2D Gaussian dataset and compare to KDE samples
  2. Vary early stopping time δ and observe effect on quality and novelty of generated samples
  3. Train score network on subset of training data and evaluate memorization effect on generated samples

## Open Questions the Paper Calls Out

### Open Question 1
What specific alternative theoretical frameworks could be developed to evaluate generative models beyond imitation ability, and how would these frameworks incorporate creativity metrics? The paper identifies the limitation of current evaluation criteria but doesn't propose specific alternatives. Evidence needed: Development and empirical validation of new evaluation metrics that distinguish between models that replicate training data versus those producing truly novel outputs.

### Open Question 2
How does sample complexity of SGMs with empirical optimal score function scale with dimensionality in non-isotropic Gaussian target distributions? The theoretical analysis focuses on simplified cases (isotropic Gaussian and bounded support), leaving open how results generalize to practical high-dimensional datasets. Evidence needed: Rigorous mathematical analysis showing sample complexity scaling with dimensionality and distribution complexity, supported by empirical validation.

### Open Question 3
Can architectural modifications to neural networks used in SGMs mitigate the memorization effects observed with empirical optimal score function? The paper demonstrates the problem but doesn't investigate whether alternative network designs could learn more generalizable score functions. Evidence needed: Systematic comparison of different neural network architectures and training strategies in terms of their ability to generate novel samples versus memorizing training data.

## Limitations
- Theoretical analysis assumes sufficiently large hypothesis space that may not hold in practical neural network implementations
- Early stopping mechanism's impact on preventing singularities is discussed but not extensively validated across different model architectures
- Practical implications for improving SGM training and evaluation criteria are suggested but not fully explored

## Confidence

**High Confidence:** The theoretical framework demonstrating equivalence between SGMs with empirical optimal score functions and KDE is well-established through rigorous mathematical proofs.

**Medium Confidence:** Empirical validation on synthetic and CIFAR10 data supports theoretical claims, but could benefit from more diverse datasets and model architectures.

**Low Confidence:** Practical implications for improving SGM training and evaluation criteria are suggested but not fully explored.

## Next Checks

1. Cross-architecture validation: Test KDE equivalence on different neural network architectures (U-Net variants, residual networks) to verify architecture-agnostic phenomenon.

2. Extended dataset evaluation: Apply analysis to additional datasets with varying complexity (CelebA, LSUN) to assess if memorization effects scale with data diversity and dimensionality.

3. Alternative evaluation metrics: Implement and test alternative evaluation metrics that capture sample novelty beyond standard FID and Inception scores, such as novelty-based metrics or reconstruction-based measures.