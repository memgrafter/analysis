---
ver: rpa2
title: 'StoryTTS: A Highly Expressive Text-to-Speech Dataset with Rich Textual Expressiveness
  Annotations'
arxiv_id: '2404.14946'
source_url: https://arxiv.org/abs/2404.14946
tags:
- speech
- expressiveness
- textual
- text
- storytts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces StoryTTS, a new text-to-speech (TTS) dataset
  focused on expressive speech. Derived from a Mandarin storytelling show, StoryTTS
  contains 61 hours of high-quality speech with rich annotations for textual expressiveness
  across five dimensions: rhetorical devices, sentence patterns, scenes, imitated
  characters, and emotional colors.'
---

# StoryTTS: A Highly Expressive Text-to-Speech Dataset with Rich Textual Expressiveness Annotations

## Quick Facts
- arXiv ID: 2404.14946
- Source URL: https://arxiv.org/abs/2404.14946
- Reference count: 0
- Primary result: TTS models with expressiveness annotations significantly outperform baselines in speech expressiveness

## Executive Summary
StoryTTS is a new expressive Mandarin text-to-speech dataset derived from a storytelling show, containing 61 hours of high-quality speech with rich annotations across five expressiveness dimensions: rhetorical devices, sentence patterns, scenes, imitated characters, and emotional colors. These annotations were generated using large language models with few-shot prompting, enabling scalable and detailed labeling without prohibitive manual costs. Experiments demonstrate that incorporating these textual expressiveness labels into TTS models leads to significantly more expressive synthesized speech compared to baseline models, with the best results achieved by fusing all five label types.

## Method Summary
The StoryTTS dataset was created by collecting and preprocessing Mandarin storytelling speech, aligning transcripts to audio using Whisper ASR and Aeneas, and then annotating each sentence with five types of expressiveness labels via LLM few-shot prompting. These labels were encoded using separate learnable embedding tables and integrated into the VQTTS acoustic model through cross-attention mechanisms. The model was trained for 300 epochs (acoustic) and 100 epochs (vocoder), with expressiveness embeddings providing structured linguistic and emotional context to guide prosody generation. Evaluation used MOS for expressiveness and speech quality, and MCD and log-F0 RMSE for objective acoustic metrics.

## Key Results
- TTS models with expressiveness labels significantly outperform baseline models in both objective and subjective expressiveness metrics
- Fusing all five expressiveness label types yields the greatest improvement in synthesized speech expressiveness
- Imputed characters have the most significant individual effect on expressiveness, followed by scenes and emotional colors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Textual expressiveness annotations improve TTS expressiveness more than acoustic features alone.
- Mechanism: The expressiveness encoder learns distributed embeddings for sentence patterns, scenes, rhetorical devices, emotional colors, and imitated characters, providing the acoustic model with structured linguistic and emotional context that guides prosody generation.
- Core assumption: Expressive text contains predictable, codifiable cues that correlate with acoustic patterns and can be modeled by differentiable embeddings.
- Evidence anchors:
  - [abstract] "Experiments show that TTS models incorporating these expressiveness labels significantly improve synthesized speech expressiveness compared to baseline models"
  - [section 4.1.2] "We employed four separate learnable embedding tables to supply information to the model for the four labels: sentence pattern, scene, rhetorical method, and imitated character"
  - [corpus] Weak: No direct citation count or h-index available for this dataset's peer-reviewed evaluation
- Break condition: If the embeddings collapse to a degenerate representation or if the correlation between textual expressiveness categories and acoustic output is spurious, expressiveness gains will not materialize.

### Mechanism 2
- Claim: Few-shot prompting with large language models yields accurate, scalable annotations for expressive speech datasets.
- Mechanism: LLMs are prompted with curated examples to infer categorical labels (sentence patterns, scenes, etc.) and multi-word emotional summaries, automating annotation without manual per-sentence labeling.
- Core assumption: LLMs generalize the annotation task from few examples and maintain consistency across a large corpus.
- Evidence anchors:
  - [abstract] "These annotations were generated using large language models with few-shot prompting"
  - [section 3.2] "we transitioned to the few-shot setting, where we provided the model with comprehensive and diverse labeled text and explained the rationale behind each labeling decision"
  - [corpus] Weak: No quantitative accuracy or inter-annotator agreement metrics provided
- Break condition: If LLM output lacks consistency or fails to capture nuanced expressiveness cues, downstream TTS performance will degrade.

### Mechanism 3
- Claim: Fusing all five expressiveness label types yields greater TTS expressiveness than any single label alone.
- Mechanism: Each label captures a distinct dimension of expressiveness; combining them gives the model complementary cues about character, scene, and emotion that jointly shape prosody.
- Core assumption: The expressiveness dimensions are largely orthogonal and capture non-redundant information about how a sentence should be spoken.
- Evidence anchors:
  - [section 4.3.2] "the fusion of all expressive labels provides the most significant enhancement" and "outperforms other setups significantly in both objective and subjective metrics"
  - [section 4.3.2] "supplying the model with increasingly accurate information about imitated characters and scenes"
  - [corpus] Weak: Only relative performance shown; no ablation of interaction effects between labels
- Break condition: If labels are highly correlated or some contribute noise, the fusion may not outperform the best single label.

## Foundational Learning

- Concept: Large language model few-shot prompting
  - Why needed here: Enables scalable annotation of a 61-hour corpus without prohibitive manual labeling costs
  - Quick check question: What prompt format yields consistent categorical labels for a new expressiveness dimension?

- Concept: Prosodic-acoustic alignment in TTS
  - Why needed here: TTS models map linguistic features (phonemes, expressiveness) to acoustic features (pitch, duration); understanding this mapping is critical for evaluating expressiveness gains
  - Quick check question: How does an expressiveness embedding affect the predicted pitch contour in the acoustic model?

- Concept: Speech recognition and forced alignment
  - Why needed here: Accurate phoneme-level timing is required to align expressiveness labels with acoustic frames
  - Quick check question: Why might Whisper's ASR errors propagate into downstream TTS quality issues?

## Architecture Onboarding

- Component map: Whisper ASR -> Aeneas text-speech alignment -> manual correction -> LLM annotation -> expressiveness encoder (5 embedding tables + BERT + SBERT cross-attention) -> VQTTS acoustic model (t2v + v2w)
- Critical path: Data preparation -> ASR alignment -> LLM annotation -> model training -> evaluation
- Design tradeoffs: LLM annotation accuracy vs. manual labeling cost; embedding dimensionality vs. overfitting risk; fusion of all labels vs. simpler single-label models
- Failure signatures: ASR alignment errors -> mismatched text-phoneme pairs; LLM annotation drift -> inconsistent expressiveness cues; embedding collapse -> loss of distinctiveness between label types
- First 3 experiments:
  1. Verify that the ASR alignment produces phoneme durations within 5% of ground truth on a held-out subset.
  2. Test LLM annotation consistency by sampling 50 sentences and comparing categorical labels to manual annotations.
  3. Train the expressiveness encoder with only one label type (e.g., emotional color) and measure objective metrics to confirm that expressiveness information is being learned.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the different expressiveness annotations (sentence patterns, scenes, rhetorical devices, emotional colors, and imitated characters) individually contribute to the naturalness and expressiveness of synthesized speech in StoryTTS?
- Basis in paper: [explicit] The paper mentions that each type of expressiveness label has a different impact on the synthesized speech, with imitated characters having the most significant effect. However, the specific contributions of each label type are not thoroughly analyzed.
- Why unresolved: The paper provides an overall assessment of the impact of all labels together but lacks a detailed analysis of the individual contributions of each label type.
- What evidence would resolve it: Detailed experiments isolating the effect of each expressiveness label on the synthesized speech, possibly through ablation studies or separate model training with each label type.

### Open Question 2
- Question: How does the accuracy and consistency of the LLM-generated expressiveness annotations compare to manual annotations, and what is the potential impact of annotation errors on the synthesized speech quality?
- Basis in paper: [inferred] The paper mentions using LLMs for batch annotation with few-shot learning but does not discuss the accuracy or consistency of these annotations compared to manual ones.
- Why unresolved: The paper does not provide a comparison between LLM-generated annotations and manual annotations, nor does it discuss the potential impact of annotation errors on the final synthesized speech.
- What evidence would resolve it: A comparative study between LLM-generated annotations and manual annotations, including an analysis of annotation errors and their impact on synthesized speech quality.

### Open Question 3
- Question: What are the limitations of using LLMs for expressiveness annotation in StoryTTS, and how might these limitations affect the scalability and generalizability of the annotation approach to other languages or storytelling styles?
- Basis in paper: [explicit] The paper mentions using LLMs for batch annotation but does not discuss the limitations of this approach or its scalability to other languages or storytelling styles.
- Why unresolved: The paper does not explore the potential limitations of using LLMs for expressiveness annotation, such as the need for large amounts of annotated data for training or the challenges of adapting the approach to other languages or storytelling styles.
- What evidence would resolve it: A discussion of the limitations of using LLMs for expressiveness annotation, including potential challenges in scaling the approach to other languages or storytelling styles, and possible solutions or workarounds.

## Limitations
- LLM-generated annotations lack independent verification for accuracy and consistency; no inter-annotator agreement metrics provided
- Dataset limited to Mandarin storytelling, which may restrict generalizability to other languages or speaking styles
- No rigorous ablation or interaction analysis of the five expressiveness label types; fusion superiority assumed but not proven

## Confidence

- **High confidence**: Expressive speech synthesis benefits from structured textual annotations; StoryTTS provides a large-scale, well-aligned Mandarin dataset with rich expressiveness labels
- **Medium confidence**: Few-shot LLM prompting is a scalable method for annotating expressive speech datasets, though accuracy and consistency are not quantified
- **Medium confidence**: Fusing multiple expressiveness label types improves TTS expressiveness more than any single label, but the relative contribution and interaction effects of each label are not fully characterized

## Next Checks

1. **LLM annotation consistency check**: Sample 50 sentences from StoryTTS and manually annotate the five expressiveness dimensions, then compute inter-annotator agreement and compare against LLM-generated labels to quantify annotation quality
2. **ASR alignment verification**: Measure phoneme duration alignment error on a held-out subset by comparing Whisper ASR alignments to ground truth transcripts, and report the proportion of sentences with errors exceeding 5%
3. **Label ablation and interaction study**: Train separate TTS models using each expressiveness label type individually and in pairs, then perform statistical tests to determine if the fusion of all five labels significantly outperforms the best single or paired label configuration