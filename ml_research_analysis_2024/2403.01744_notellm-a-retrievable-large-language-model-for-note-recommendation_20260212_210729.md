---
ver: rpa2
title: 'NoteLLM: A Retrievable Large Language Model for Note Recommendation'
arxiv_id: '2403.01744'
source_url: https://arxiv.org/abs/2403.01744
tags:
- note
- recommendation
- arxiv
- notes
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NoteLLM introduces a large language model-based approach for item-to-item
  note recommendation that jointly learns note embeddings and hashtag/category generation.
  The method uses a unified compression prompt to transform note content into a special
  token, then applies generative-contrastive learning to capture collaborative signals
  between related notes, while collaborative supervised fine-tuning enables hashtag/category
  generation from the same model.
---

# NoteLLM: A Retrievable Large Language Model for Note Recommendation

## Quick Facts
- arXiv ID: 2403.01744
- Source URL: https://arxiv.org/abs/2403.01744
- Reference count: 40
- Key result: Achieves 94.66% average recall, outperforming BERT-based baselines (88.63%)

## Executive Summary
NoteLLM introduces a novel large language model-based approach for item-to-item note recommendation that jointly learns note embeddings and hashtag/category generation. The method uses a unified compression prompt to transform note content into a special token, then applies generative-contrastive learning to capture collaborative signals between related notes, while collaborative supervised fine-tuning enables hashtag/category generation from the same model. Experiments on Xiaohongshu data show NoteLLM achieves state-of-the-art performance, with 94.66% average recall across multiple cutoffs, outperforming BERT-based baselines (88.63%) and other LLM approaches. The model demonstrates strong performance on both popular and low-exposure notes, with online deployment improving click-through rate by 16.20% and increasing comments on new notes by 3.58%.

## Method Summary
NoteLLM employs a unified Note Compression Prompt that instructs the LLM to compress note content into a special token for embeddings while simultaneously generating hashtags/categories. The method uses Generative-Contrastive Learning (GCL) that leverages co-occurrence scores from user behavior to construct related note pairs, training the LLM to distinguish related notes from in-batch negatives using the compressed token's hidden state as the embedding. Collaborative Supervised Fine-tuning (CSFT) is applied to train hashtag/category generation. The model is trained with a combined loss function that balances GCL and CSFT objectives, using LLaMA 2 as the base LLM with distributed training on 8 × 80GB Nvidia A100 GPUs.

## Key Results
- Achieves 94.66% average recall across multiple cutoffs, outperforming BERT-based baselines (88.63%) and other LLM approaches
- Online deployment improves click-through rate by 16.20% and increases comments on new notes by 3.58%
- Demonstrates strong performance on both popular notes (>75,000 exposures) and low-exposure notes (<1,500 exposures)

## Why This Works (Mechanism)

### Mechanism 1: Joint Learning Transfer
Both hashtag/category generation and note embedding tasks compress note content into limited representations, forcing the model to distill key concepts. Learning to generate hashtags/categories transfers this summarization skill to the embedding compression task, enhancing recommendation quality.

### Mechanism 2: Generative-Contrastive Learning
GCL integrates collaborative signals into LLM embeddings by using co-occurrence scores from user behavior to construct related note pairs. The LLM is trained to distinguish related notes from in-batch negatives using the compressed token's hidden state, bringing related notes closer while pushing unrelated notes apart.

### Mechanism 3: Unified Prompt Design
The unified Note Compression Prompt enables flexible management of both I2I recommendation and generation tasks within a single model. This prompt template instructs the LLM to both compress the note into a special token and generate hashtags/categories, allowing multi-task learning from the same forward pass.

## Foundational Learning

- **Contrastive learning for representation learning**: GCL uses contrastive learning to train embeddings that bring related notes closer while pushing unrelated notes apart. Quick check: What loss function is typically used in contrastive learning to achieve this objective?

- **Multi-task learning and knowledge transfer**: NoteLLM learns both recommendation and generation tasks simultaneously, with the assumption that skills transfer between tasks. Quick check: What is the primary benefit of multi-task learning in this context?

- **Prompt engineering for LLM control**: The unified Note Compression Prompt controls how the LLM processes notes for both embedding and generation tasks. Quick check: What are the key components of the Note Compression Prompt template?

## Architecture Onboarding

- **Component map**: Note content → Note Compression Prompt → LLM → Compressed token + Generation → GCL/CSFT training → Updated embeddings and generation capability

- **Critical path**: Note content flows through the unified prompt into the LLM, producing both compressed tokens for embeddings and generated hashtags/categories, which are then used in GCL and CSFT training to update the model.

- **Design tradeoffs**: Single prompt vs. separate prompts (unified prompt reduces complexity but may create task interference); Generative vs. discriminative approach (using LLM's generation ability vs. treating it as an encoder only); Co-occurrence vs. other collaborative signals (simplicity and scalability vs. potentially richer signals)

- **Failure signatures**: Poor recall performance (GCL not capturing collaborative signals effectively); Low generation quality (CSFT not transferring summarization skills); Model collapse (conflicting objectives in multi-task learning)

- **First 3 experiments**: 1) Test co-occurrence score calculation and related note pair construction with sample data; 2) Verify prompt generation produces expected outputs for both embedding and generation tasks; 3) Run a small-scale training run to confirm both GCL and CSFT losses decrease as expected

## Open Questions the Paper Calls Out

1. **Task Performance Trade-off**: The paper observes a seesaw phenomenon between hashtag/category generation and I2I recommendation performance, but doesn't quantify the precise mathematical relationship or establish optimal balance between the two tasks.

2. **Cross-Platform Generalizability**: While the paper claims NoteLLM can be seamlessly adapted to diverse platforms, this remains largely theoretical without empirical validation on datasets beyond Xiaohongshu.

3. **Prompt Engineering Sensitivity**: The authors use specific prompt templates but don't conduct ablation studies on prompt variations or explore alternative formulations that might yield better performance.

## Limitations

- Evaluated exclusively on Xiaohongshu data, limiting generalizability to other platforms with different user interaction patterns
- Online performance attribution unclear - doesn't isolate whether improvements stem from better recommendations, improved hashtag generation, or both
- Cold start scenario for completely new notes without any user interactions is not explicitly addressed

## Confidence

**High Confidence Claims**:
- NoteLLM achieves 94.66% average recall, outperforming BERT-based baselines (88.63%)
- Online deployment improves click-through rate by 16.20%
- Strong performance on both popular and low-exposure notes

**Medium Confidence Claims**:
- Joint learning enhances recommendation quality through semantic compression
- GCL effectively integrates collaborative signals into LLM embeddings
- Unified prompt enables flexible management of both tasks

**Low Confidence Claims**:
- Seamless adaptation to diverse platforms beyond Xiaohongshu
- Significant transfer of summarization skills between generation and embedding tasks
- Compressed token captures sufficient collaborative signal for effective recommendations

## Next Checks

1. **Cross-Platform Validation**: Evaluate NoteLLM on a different social media platform (e.g., Twitter, Reddit, or another short-text recommendation dataset) to assess generalizability and validate whether the co-occurrence mechanism works beyond Xiaohongshu's specific user behavior patterns.

2. **Ablation Study for Joint Learning**: Conduct an ablation study comparing three variants: (a) NoteLLM with both GCL and CSFT, (b) NoteLLM with only GCL, and (c) NoteLLM with only CSFT to quantify the actual contribution of joint learning versus individual task performance.

3. **Cold Start Scenario Testing**: Design a cold start evaluation where new notes with zero user interactions are tested, comparing NoteLLM's performance against baseline methods that rely solely on content-based features to isolate the contribution of collaborative signals versus content understanding.