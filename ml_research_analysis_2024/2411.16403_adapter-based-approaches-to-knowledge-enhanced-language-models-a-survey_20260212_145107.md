---
ver: rpa2
title: Adapter-based Approaches to Knowledge-enhanced Language Models -- A Survey
arxiv_id: '2411.16403'
source_url: https://arxiv.org/abs/2411.16403
tags:
- knowledge
- language
- adapter
- adapters
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews adapter-based approaches for
  knowledge-enhanced language models (KELMs), addressing the gap between large language
  models and domain-specific knowledge. It provides a comprehensive analysis of existing
  methodologies, focusing on adapter architectures like Pfeiffer and Houlsby adapters,
  and their applications in tasks such as reading comprehension, named entity recognition,
  and question answering.
---

# Adapter-based Approaches to Knowledge-enhanced Language Models -- A Survey

## Quick Facts
- arXiv ID: 2411.16403
- Source URL: https://arxiv.org/abs/2411.16403
- Reference count: 18
- This survey systematically reviews adapter-based approaches for knowledge-enhanced language models (KELMs), addressing the gap between large language models and domain-specific knowledge.

## Executive Summary
This survey provides a comprehensive analysis of adapter-based approaches for enhancing language models with external knowledge sources. The authors systematically review 76 papers from major NLP databases, examining how adapters can inject domain-specific knowledge into language models while maintaining efficiency and avoiding catastrophic forgetting. The study focuses on adapter architectures like Pfeiffer and Houlsby adapters and their applications across various domains, particularly biomedical, legal, and financial text processing. Key findings highlight the versatility of adapter-based methods in reducing computational load while improving performance on knowledge-intensive tasks.

## Method Summary
The authors conducted a systematic literature review following Kitchenham et al. (2009) methodology, searching IEEE Xplore, ACM Digital Library, and ACL Anthology databases using the query "(adapter OR adapter-based) AND (language model OR nlp OR natural language processing) AND (injection OR knowledge)". Papers were screened through abstract and full-text phases, with quantitative analysis of adapter types, domains, tasks, and sources, along with qualitative analysis of trends and methodologies. The review includes 76 papers plus 3 additional papers, focusing on adapter-based approaches for knowledge enhancement in language models.

## Key Results
- Pfeiffer and Houlsby adapters are the most popular adapter architectures, with Pfeiffer showing particular effectiveness in knowledge injection tasks
- Biomedical domain represents the most prevalent closed-domain approach, with models like MoP and KEBLM showing significant performance improvements
- Adapter-based methods demonstrate versatility across various NLP tasks including reading comprehension, named entity recognition, and question answering while reducing computational load

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adapters enable efficient knowledge injection without catastrophic forgetting.
- Mechanism: Adapters are lightweight bottleneck layers inserted into transformer layers, allowing new knowledge to be learned while freezing base model weights. This preserves original model representations while adding task/domain-specific parameters.
- Core assumption: Base model frozen weights continue to provide stable representations during adapter training.
- Evidence anchors:
  - [abstract] "They are frequently combined with adapter modules to reduce the computational load and risk of catastrophic forgetting."
  - [section] "Adapters can avoid catastrophic forgetting (the issue when an LLM suddenly deteriorates in performance after fine-tuning) by introducing new task-specific parameters"
  - [corpus] Weak - corpus neighbors discuss adapters for robustness but not specifically for catastrophic forgetting in knowledge injection.

### Mechanism 2
- Claim: Adapter fusion enables knowledge composition from multiple specialized adapters.
- Mechanism: After training individual adapters for different tasks/knowledge sources, adapter fusion learns to combine their outputs using learned weighting parameters, enabling transfer of composite knowledge.
- Core assumption: Individual adapters capture distinct, complementary knowledge that can be meaningfully combined.
- Evidence anchors:
  - [section] "Pfeiffer et al. (2020a) introduce Adapter Fusion, a two-stage algorithm that addresses the sharing of information encapsulated in adapters trained on different tasks."
  - [abstract] "adapter-based approaches have been frequently explored along with various adapter architectures"
  - [corpus] Weak - corpus neighbors mention adapters for continual learning but don't specifically address fusion for knowledge composition.

### Mechanism 3
- Claim: Domain-specific KGs provide rich semantic relationships that enhance factual accuracy.
- Mechanism: Knowledge graphs encode structured relationships between entities, which adapters can learn to integrate into language model representations, improving performance on knowledge-intensive tasks.
- Core assumption: KGs contain accurate, relevant knowledge that aligns with language model pretraining distribution.
- Evidence anchors:
  - [abstract] "KELMs can achieve higher factual accuracy and mitigate hallucinations by leveraging knowledge graphs (KGs)"
  - [section] "Knowledge graphs (KGs) are a structured representation of world knowledge and have seen a rising prominence in NLP research"
  - [corpus] Weak - corpus neighbors discuss knowledge graphs for recommendation and image generation, not specifically for language model enhancement.

## Foundational Learning

- Concept: Transformer architecture fundamentals
  - Why needed here: Understanding where adapters fit in transformer layers is crucial for implementation
  - Quick check question: Where exactly are adapter modules inserted in a transformer layer - after attention, after feed-forward, or both?

- Concept: Knowledge graph representation and embedding
  - Why needed here: KGs provide the structured knowledge that adapters inject into language models
  - Quick check question: What are the common formats for KG triplets (head, relation, tail) and how are entities/relations typically embedded?

- Concept: Adapter architectures (Houlsby, Pfeiffer, K-Adapter)
  - Why needed here: Different adapter types have different parameter efficiencies and knowledge integration capabilities
  - Quick check question: What's the key architectural difference between Pfeiffer adapters (single module) and Houlsby adapters (dual modules)?

## Architecture Onboarding

- Component map: Base transformer model (frozen) → Adapter modules (trainable) → Adapter fusion layer (optional) → Task-specific head
- Critical path: Load base model → Insert adapter modules → Load knowledge graph embeddings → Train adapters on KG-task data → (Optional) Train adapter fusion
- Design tradeoffs: Parameter efficiency vs. knowledge capacity (more adapters = more parameters but better knowledge integration), single adapter per task vs. adapter fusion for knowledge composition
- Failure signatures: Base model degradation (catastrophic forgetting), adapter overfitting to training KG, knowledge conflicts in adapter fusion
- First 3 experiments:
  1. Insert Pfeiffer adapters into base model and train on single KG source for simple QA task
  2. Compare Houlsby vs Pfeiffer adapters on same task for parameter efficiency analysis
  3. Train two adapters on different KG sources, then implement adapter fusion for composite knowledge integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the long-term scalability of adapter-based knowledge enhancement when dealing with ever-growing knowledge graphs and larger language models?
- Basis in paper: [explicit] The paper mentions that knowledge graphs like UMLS can be several gigabytes large and are expensive to train on in their entirety, with MoP proposing to split KGs into subgraphs. It also notes the rising popularity of adapter-based approaches and predicts continued growth.
- Why unresolved: While MoP and similar approaches address immediate computational concerns, the fundamental question of how to efficiently scale knowledge enhancement to massive, continuously expanding KGs and LLMs with trillions of parameters remains unanswered.
- What evidence would resolve it: Comparative studies of adapter-based approaches on increasingly large KGs and LLMs over time, demonstrating sustained performance gains and manageable computational costs.

### Open Question 2
- Question: How do different adapter architectures (e.g., Pfeiffer, Houlsby, K-Adapter) perform across diverse domains and tasks beyond the biomedical domain?
- Basis in paper: [explicit] The paper analyzes adapter type distribution, showing Pfeiffer and Houlsby as most popular, but notes that biomedical is the most prevalent closed-domain approach. It also highlights the versatility of adapter-based approaches across various NLP tasks and domains.
- Why unresolved: While the paper provides performance comparisons in the biomedical domain, it does not offer a comprehensive analysis of how different adapter architectures perform across a wide range of domains and tasks. This limits the ability to provide precise guidelines on method selection.
- What evidence would resolve it: Systematic benchmarking studies comparing the performance of different adapter architectures across a diverse set of domains (e.g., legal, financial, social media) and tasks (e.g., question answering, sentiment analysis, summarization).

### Open Question 3
- Question: Can adapter-based knowledge enhancement improve the factuality and informativeness of generated text in generative tasks beyond dialogue modeling?
- Basis in paper: [explicit] The paper mentions that generative tasks, other than dialogue modeling, are rather unexplored. It also envisions a future use case where knowledge enhancement could improve the factuality and informativeness of generated text.
- Why unresolved: While the potential for knowledge enhancement in generative tasks is recognized, there is limited research exploring this area. The effectiveness of adapter-based approaches in improving the quality of generated text across various domains and applications remains to be investigated.
- What evidence would resolve it: Empirical studies evaluating the impact of adapter-based knowledge enhancement on the factuality and informativeness of generated text in tasks such as summarization, creative writing, and machine translation.

## Limitations
- Systematic review methodology may have missed relevant papers due to database coverage limitations or search string implementation variations
- Analysis focuses primarily on methodological aspects rather than providing extensive empirical comparisons across different adapter architectures and domains
- Limited evaluation of adapter-based KELMs' generalization capabilities to out-of-domain knowledge tasks

## Confidence

**High Confidence**: The fundamental mechanisms of adapter-based knowledge injection (catastrophic forgetting prevention, parameter efficiency) are well-established and supported by multiple papers in the survey.

**Medium Confidence**: The effectiveness of adapter fusion for knowledge composition is supported by the literature but lacks extensive empirical validation across diverse knowledge sources and domains.

**Medium Confidence**: Claims about adapter-based KELMs' performance improvements in specialized domains (biomedical, legal, financial) are based on reported results but may not generalize to all use cases.

## Next Checks

1. **Reproducibility Verification**: Implement the core adapter architectures (Pfeiffer, Houlsby) on a standardized benchmark task to verify claimed parameter efficiency and performance metrics.

2. **Knowledge Integration Quality**: Conduct experiments comparing knowledge injection quality across different adapter types using standard knowledge-intensive benchmarks like Natural Questions or HotpotQA.

3. **Cross-Domain Generalization**: Test adapter-based KELMs trained on one domain (e.g., biomedical) on out-of-domain knowledge tasks to assess generalization capabilities and potential domain-specific limitations.