---
ver: rpa2
title: Collaboratively adding new knowledge to an LLM
arxiv_id: '2410.14753'
source_url: https://arxiv.org/abs/2410.14753
tags:
- training
- lora
- knowledge
- sequential
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares different strategies for adding new knowledge
  to a large language model (LLM) while retaining previously learned knowledge. The
  authors consider two settings: semi-collaborative (datasets not available after
  training) and fully-collaborative (datasets remain available).'
---

# Collaboratively adding new knowledge to an LLM

## Quick Facts
- arXiv ID: 2410.14753
- Source URL: https://arxiv.org/abs/2410.14753
- Reference count: 20
- Primary result: LoRA outperforms full fine-tuning for adding new knowledge while preserving old knowledge across multiple tasks

## Executive Summary
This paper systematically compares different strategies for extending a large language model's knowledge base while maintaining previously learned capabilities. The authors evaluate both semi-collaborative (datasets unavailable after training) and fully-collaborative (datasets remain available) settings using Llama 3 8B. They find that parameter-efficient fine-tuning with LoRA generally outperforms full fine-tuning in preserving old knowledge while still acquiring new knowledge, with LoRA-based orthogonal subspace learning and MOE model mixing showing particular promise in the semi-collaborative setting.

## Method Summary
The paper compares full fine-tuning (FFT) versus LoRA parameter-efficient fine-tuning across two collaborative settings. In the semi-collaborative setting, methods include sequential training with LoRA (with and without orthogonality constraints), model merging (simple averaging and TIES-DARE), and MOE model mixing. In the fully-collaborative setting, methods include joint training, sequential training with replay, and FFT vs LoRA comparisons. Experiments use the TRACE benchmark datasets and evaluate performance on new knowledge acquisition, recent knowledge retention, and old knowledge retention using accuracy-based metrics.

## Key Results
- LoRA consistently outperforms full fine-tuning in balancing new knowledge acquisition with old knowledge retention
- In semi-collaborative setting, MOE model mixing and model merging show strong performance, with LoRA-based orthogonal subspace learning also effective with small orthogonality weights
- In fully-collaborative setting, both joint training and sequential training with replay are effective, with LoRA generally preferable to FFT
- Sequential training with replay using only 10% replay rate outperforms joint training in the fully-collaborative setting

## Why This Works (Mechanism)

### Mechanism 1
LoRA updates only a small subset of low-rank adapter parameters rather than all model weights, limiting interference with previously learned representations and reducing catastrophic forgetting.

### Mechanism 2
Orthogonal LoRA adds an orthogonality constraint to the loss function, pushing optimization to find adapter parameters orthogonal to previously learned adapters, reducing interference between successive adapters.

### Mechanism 3
MOE model mixing combines multiple individually trained models by creating expert modules with a routing mechanism that dynamically selects which expert(s) to use for each input token, leveraging synergies across tasks.

## Foundational Learning

- Concept: Catastrophic Forgetting
  - Why needed here: Understanding why LoRA and other techniques are effective requires understanding the problem they are trying to solve
  - Quick check question: What is the primary cause of catastrophic forgetting in neural networks, and how do techniques like LoRA attempt to mitigate it?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: LoRA is a core technique used in the paper for parameter-efficient fine-tuning
  - Quick check question: How does LoRA achieve parameter efficiency, and what are the trade-offs between LoRA and full fine-tuning?

- Concept: Orthogonal Subspace Learning
  - Why needed here: Orthogonal LoRA is a specific variant explored in the paper
  - Quick check question: How does orthogonal subspace learning reduce interference between successive adapters?

## Architecture Onboarding

- Component map: Base LLM (Llama 3) -> LoRA adapters (one per task) -> Orthogonal LoRA adapters (with orthogonality constraint) -> MOE model mixing (expert modules + routing) -> Model merging (parameter averaging/heuristics)

- Critical path:
  1. Train base LLM on pre-training data
  2. Fine-tune base LLM on each task individually using LoRA, storing adapters
  3. For semi-collaborative: Combine adapters using MOE mixing, model merging, or sequential LoRA training
  4. For fully-collaborative: Perform joint training or sequential training with replay

- Design tradeoffs:
  - LoRA vs full fine-tuning: LoRA is more parameter-efficient and better at preserving old knowledge but may be slightly less effective at acquiring new knowledge
  - Orthogonal LoRA vs standard LoRA: Orthogonal LoRA reduces interference but may hinder new knowledge acquisition if orthogonality constraint is too strong
  - MOE model mixing vs model merging: MOE mixing leverages synergies but is more complex; model merging is simpler but may not capture task-specific nuances

- Failure signatures:
  - Significant degradation in old knowledge performance after training on new tasks
  - Inability to effectively learn new tasks even with LoRA or other parameter-efficient methods
  - Poor performance of MOE model mixing or model merging, indicating tasks are too dissimilar

- First 3 experiments:
  1. Compare LoRA and full fine-tuning on a single task, measuring both new knowledge acquisition and old knowledge retention
  2. Perform sequential LoRA training on multiple tasks with and without orthogonality constraint
  3. Combine individually trained LoRA adapters using MOE model mixing or model merging

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal orthogonality weight (λ) for o-LoRA in semi-collaborative settings, and how does it vary with task characteristics?

### Open Question 2
How do different model merging methods (TIES, DARE, simple averaging) compare in terms of computational efficiency and knowledge retention across diverse task types?

### Open Question 3
What is the optimal replay percentage for sequential training with replay in fully-collaborative settings, and how does it interact with task ordering and difficulty?

## Limitations

- Experiments conducted on a specific set of 8 datasets from TRACE benchmark and MetaMath, limiting generalizability
- Evaluation focuses primarily on accuracy-based metrics without considering computational efficiency or inference latency
- Semi-collaborative setting assumes datasets are unavailable after initial training, which may not reflect all real-world scenarios
- Paper does not thoroughly explore impact of model scale on relative performance of different methods

## Confidence

**High Confidence**: LoRA outperforms full fine-tuning for preserving old knowledge while acquiring new knowledge is well-supported by experimental results.

**Medium Confidence**: Effectiveness of orthogonal LoRA and MOE model mixing shows more variability across tasks and is sensitive to implementation details.

**Low Confidence**: Comparison between joint training and sequential training with replay lacks statistical significance testing and may be influenced by specific task ordering.

## Next Checks

1. **Cross-Domain Generalization Test**: Replicate main experiments using more diverse datasets from different domains (biomedical, legal, technical) to validate performance patterns across different knowledge types.

2. **Parameter Sensitivity Analysis**: Systematically vary LoRA rank, α parameters, and orthogonality weight λ across wider range to identify optimal configurations for different task types.

3. **Longitudinal Training Stability**: Extend sequential training experiments to include more than 7 tasks to evaluate long-term stability and identify when catastrophic forgetting becomes prohibitive for different approaches.