---
ver: rpa2
title: Counterfactual Explanations for Medical Image Classification and Regression
  using Diffusion Autoencoder
arxiv_id: '2408.01571'
source_url: https://arxiv.org/abs/2408.01571
tags:
- image
- latent
- images
- https
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of generating interpretable counterfactual
  explanations (CEs) for medical image classification and regression tasks. The authors
  propose a novel method that operates directly on the latent space of a Diffusion
  Autoencoder (DAE), enabling inherent CE generation and continuous visualization
  of the model's internal representation across decision boundaries.
---

# Counterfactual Explanations for Medical Image Classification and Regression using Diffusion Autoencoder

## Quick Facts
- arXiv ID: 2408.01571
- Source URL: https://arxiv.org/abs/2408.01571
- Reference count: 33
- Generates interpretable counterfactual explanations for medical image tasks using Diffusion Autoencoder latent space

## Executive Summary
This work introduces a novel method for generating counterfactual explanations in medical image classification and regression by leveraging the latent space of a Diffusion Autoencoder (DAE). Unlike existing approaches that require labeled data or separate feature extraction models, this method operates directly on the semantically rich latent space learned through unsupervised training. The DAE's linear manifold enables meaningful interpolation and manipulation, allowing for both binary classification counterfactuals and ordinal regression explanations. Experiments across vertebral compression fractures, intervertebral disc degeneration, diabetic retinopathy, and peritumoral edema demonstrate the method's advantages in interpretability and versatility while maintaining medical realism.

## Method Summary
The approach uses a Diffusion Autoencoder to learn a semantically rich latent representation from unlabeled medical images. This DAE consists of a semantic encoder that maps images to a latent space capturing high-level semantic information, combined with a conditional DDIM decoder for reconstruction. Linear classifiers (SVM or linear regression) are trained on this latent space using available labels. Counterfactual examples are generated by manipulating latent vectors along decision boundaries and reconstructing them through the DDIM decoder. This eliminates the need for external classifiers or feature extraction models, enabling inherent interpretability through continuous visualization of the model's internal representation across decision boundaries.

## Key Results
- Achieves competitive classification performance with ROC-AUC of 0.86 and F1 score of 0.76 for vertebral compression fracture detection
- Generates medically realistic counterfactuals that visualize progression across diabetic retinopathy severity grades
- Outperforms traditional methods in interpretability while maintaining reconstruction quality with LPIPS scores below 0.1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Diffusion Autoencoder (DAE) provides a semantically rich latent space that enables meaningful counterfactual generation without external classifiers.
- Mechanism: DAE's dual latent space structure (semantic latent zsem + noise latent xT) captures high-level image semantics during reconstruction, creating a linear manifold in zsem where counterfactuals can be generated by simple vector arithmetic along decision boundaries.
- Core assumption: The semantic latent space of DAE maintains a linear data manifold that preserves semantic relationships between images, enabling meaningful interpolation and manipulation.
- Evidence anchors:
  - [abstract] "Our method leverages the DAE's ability to encode images into a semantically rich latent space in an unsupervised manner"
  - [section] "DAE distinguishes itself by incorporating a semantic encoder, designed to transform input images into a semantic latent space zsem, that captures semantically meaningful information"
  - [corpus] Weak evidence - related works focus on diffusion models but not specifically on DAE's semantic latent space for counterfactuals
- Break condition: If the latent space is not truly linear or loses semantic meaning during reconstruction, counterfactual generation would produce unrealistic or nonsensical images.

### Mechanism 2
- Claim: Linear classifiers on DAE latent space can effectively separate medical conditions and generate interpretable decision boundaries.
- Mechanism: By training linear models (SVM or linear regression) on zsem representations, the method creates hyperplanes that capture decision boundaries in a way that enables both binary classification and ordinal regression through distance metrics.
- Core assumption: The DAE's semantic latent space preserves discriminative features necessary for classification tasks, making linear separation feasible.
- Evidence anchors:
  - [abstract] "The linear manifold of the DAE's latent space allows for meaningful interpolation and manipulation"
  - [section] "We obtain the semantic latent representation zsem for a subset of training samples for which labels are available and train linear classifiers (linear regression and SVM) to predict the existence of the target pathology"
  - [corpus] Moderate evidence - several related works use linear methods on latent spaces, but specific validation on DAE is limited
- Break condition: If the latent space doesn't preserve sufficient discriminative information, linear classifiers would fail to achieve adequate separation performance.

### Mechanism 3
- Claim: Counterfactual examples generated in latent space maintain medical realism while illustrating necessary changes for different pathology grades.
- Mechanism: By reflecting latents across decision boundaries and interpolating along semantic directions, the method generates realistic images that visualize both binary and ordinal counterfactuals within the learned data distribution.
- Core assumption: The conditional DDIM decoder in DAE can faithfully reconstruct images from manipulated latents while maintaining medical plausibility and anatomical consistency.
- Evidence anchors:
  - [abstract] "This approach offers inherent interpretability by enabling the generation of CEs and the continuous visualization of the model's internal representation across decision boundaries"
  - [section] "To generate CE images, the semantic latent code can be changed in the direction n⃗ and together with the original stochastic latent decoded by the conditional DDIM to a new image"
  - [corpus] Limited evidence - related works mention diffusion-based counterfactuals but don't validate medical realism specifically
- Break condition: If the decoder introduces artifacts or hallucinations when reconstructing manipulated latents, the generated counterfactuals would lose medical validity.

## Foundational Learning

- Diffusion Models: Why needed here: Understanding how diffusion models work is crucial for grasping DAE's reconstruction process and why the latent space has desirable properties for counterfactual generation.
  - Quick check question: What are the two main components of DAE's latent space and how do they differ in function?

- Latent Space Manipulation: Why needed here: The method relies on vector arithmetic in latent space to generate counterfactuals, requiring understanding of how semantic directions correspond to meaningful image changes.
  - Quick check question: How does the method calculate the counterfactual latent vector for a given input?

- Counterfactual Explanations: Why needed here: The overall goal is to generate counterfactual explanations, so understanding what makes a good counterfactual (minimal, realistic, class-changing) is essential.
  - Quick check question: What distinguishes counterfactual explanations from other interpretability methods like saliency maps?

## Architecture Onboarding

- Component map: DAE (semantic encoder + conditional DDIM) → Latent Space → Linear Classifier (SVM/Linear Regression) → Counterfactual Generator → Image Decoder
- Critical path: Image → DAE Encoder → zsem → Classifier → Decision Boundary → Counterfactual Calculation → zce → Conditional DDIM → Counterfactual Image
- Design tradeoffs: Unsupervised pretraining provides rich features but may capture irrelevant variations; linear classifiers are interpretable but may underperform on complex boundaries; diffusion reconstruction ensures quality but is computationally expensive
- Failure signatures: Poor reconstruction quality (high LPIPS/FID), classifier performance below baseline, counterfactuals showing artifacts or anatomical inconsistencies, or counterfactuals not meaningfully changing the predicted class
- First 3 experiments:
  1. Train DAE on unlabeled data and verify reconstruction quality using LPIPS and FID metrics
  2. Train linear classifier on labeled subset and evaluate AUC/F1 compared to supervised baseline
  3. Generate binary counterfactuals for misclassified samples and visually inspect for medical realism and class change

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the DAE approach scale to three-dimensional medical imaging data compared to 2D slices?
- Basis in paper: [explicit] The authors mention in the limitations section that "the well-calibrated samples in Fig. 5 show impressive visual results, the VCF grading metrics in Table 3 reveal the limitations of our method. The linear separability of classes in DAE’s semantic latent space using 2D slices cannot compete with the fully supervised, end-to-end baseline and 3D methods, motivating an extension to three dimensions."
- Why unresolved: The paper only evaluates the method on 2D slices of 3D medical imaging data, leaving the performance on full 3D volumes unexplored.
- What evidence would resolve it: Experimental results comparing the DAE approach on 3D volumes versus 2D slices for various medical imaging tasks, particularly for vertebral compression fracture detection and grading.

### Open Question 2
- Question: Can the DAE model be explicitly guided to focus on specific anatomical features, such as the central vertebral body, to improve the accuracy of fracture detection and grading?
- Basis in paper: [inferred] The authors discuss a limitation where "the failure to disentangle the fracture of adjacent vertebrae from the central vertebral body that is classified" occurs. They suggest that "the network could be explicitly guided to attend to the central vertebrae."
- Why unresolved: While the authors propose this as a potential solution, they do not provide experimental results demonstrating its effectiveness.
- What evidence would resolve it: Experimental results showing improved performance in fracture detection and grading when the DAE model is explicitly guided to focus on the central vertebral body.

### Open Question 3
- Question: How does the performance of the DAE approach compare to fully supervised end-to-end deep learning models for various medical imaging tasks?
- Basis in paper: [explicit] The authors state that "the linear separability of classes in DAE’s semantic latent space using 2D slices cannot compete with the fully supervised, end-to-end baseline and 3D methods."
- Why unresolved: While the authors acknowledge this limitation, they do not provide a comprehensive comparison of the DAE approach to fully supervised models across multiple tasks.
- What evidence would resolve it: A thorough comparison of the DAE approach to fully supervised deep learning models for various medical imaging tasks, including classification, regression, and counterfactual explanation generation.

## Limitations
- Performance of linear classifiers in latent space may not capture complex decision boundaries, particularly for highly nonlinear pathologies
- Medical realism of generated counterfactuals requires expert validation beyond standard image quality metrics
- Method shows limitations in grading tasks where linear separability cannot compete with fully supervised end-to-end baselines

## Confidence

**High Confidence:** The core mechanism of using DAE's semantic latent space for counterfactual generation is well-supported by the paper's experiments and related diffusion model literature.

**Medium Confidence:** The effectiveness of linear classifiers on the latent space representations is moderately supported, though performance varies across datasets and tasks.

**Low Confidence:** The medical plausibility and clinical utility of generated counterfactuals require further validation, as current evaluation focuses primarily on image quality metrics rather than expert medical assessment.

## Next Checks

1. **Expert Medical Review:** Have radiologists or domain experts evaluate the generated counterfactuals for anatomical consistency and clinical meaningfulness across different pathology grades.

2. **Robustness Testing:** Evaluate the method's performance on out-of-distribution samples and adversarial examples to assess generalization capabilities.

3. **Ablation Study:** Systematically vary the latent space dimensionality and classifier complexity to determine optimal configurations for different medical tasks.