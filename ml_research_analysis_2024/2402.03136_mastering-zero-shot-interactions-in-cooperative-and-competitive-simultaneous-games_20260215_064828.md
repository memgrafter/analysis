---
ver: rpa2
title: Mastering Zero-Shot Interactions in Cooperative and Competitive Simultaneous
  Games
arxiv_id: '2402.03136'
source_url: https://arxiv.org/abs/2402.03136
tags:
- agents
- games
- albatross
- agent
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of zero-shot interactions in simultaneous
  games, where agents must adapt to unseen opponents without prior knowledge. The
  core idea is Albatross, which learns to play Smooth Best Response Logit Equilibria
  (SBRLE) by combining self-play and planning.
---

# Mastering Zero-Shot Interactions in Cooperative and Competitive Simultaneous Games

## Quick Facts
- arXiv ID: 2402.03136
- Source URL: https://arxiv.org/abs/2402.03136
- Authors: Yannik Mahlau; Frederik Schubert; Bodo Rosenhahn
- Reference count: 40
- One-line primary result: Albatross outperforms state-of-the-art by 37.6% in cooperative Overcooked and can exploit weak agents in competitive Battlesnake through temperature-based opponent modeling.

## Executive Summary
This paper addresses the challenge of zero-shot interactions in simultaneous games where agents must adapt to unseen opponents without prior knowledge. The authors introduce Albatross, a method that learns to play Smooth Best Response Logit Equilibria (SBRLE) by combining self-play and planning. By modeling bounded rationality through a scalar temperature parameter, Albatross can cooperate with agents of varying strengths and exploit weaker opponents. The approach uses two neural networks - a proxy model that learns Logit equilibria at different temperatures and a response model that exploits this knowledge to respond optimally.

## Method Summary
Albatross learns to play Smooth Best Response Logit Equilibria by combining self-play and planning with two neural networks. The proxy model learns Logit equilibria at different temperatures using fixed-depth search with Logit equilibrium backup. The response model is trained to exploit the proxy model based on estimated temperatures. During evaluation, opponent rationality is estimated online using maximum likelihood estimation within a single episode. The method is evaluated on cooperative Overcooked and competitive Battlesnake benchmarks.

## Key Results
- Achieves 37.6% improvement over state-of-the-art methods in cooperative Overcooked benchmark
- Successfully exploits weak agents in competitive Battlesnake game across multiple game modes
- Demonstrates ability to estimate opponent rationality within a single episode with high accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Albatross models bounded rationality using a scalar temperature parameter for smooth best response equilibrium computation
- Mechanism: Learns proxy model approximating Logit equilibria at different temperatures and response model exploiting it via smooth best responses, trained through self-play and planning with adapted tree search
- Core assumption: Bounded rationality of other agents can be parameterized by a scalar temperature and learned from interaction data
- Evidence anchors: Abstract mentions SBRLE enables cooperation/competition with agents of any playing strength; section describes temperature parameter τ controlling inverse strength of regularization; corpus indicates related papers focus on tree search without bounded rationality modeling
- Break condition: If bounded rationality cannot be accurately captured by single scalar parameter

### Mechanism 2
- Claim: Albatross estimates opponent rationality within single episode using maximum likelihood estimation
- Mechanism: Uses MLE algorithm to estimate temperature parameter of other agents based on observed actions and corresponding optimal policies for online adaptation
- Core assumption: Opponent rationality can be estimated from observed actions and optimal policies within single episode
- Evidence anchors: Section describes setup where rational agent estimates temperatures of weak agents from K observations; Figure 8 shows MLE converging to true temperature estimate after few time steps; corpus indicates related papers don't address opponent rationality estimation
- Break condition: If rationality cannot be accurately estimated from single episode of data

### Mechanism 3
- Claim: Albatross cooperates with agents of varying playing strengths by adapting behavior based on temperature estimation
- Mechanism: Uses estimated temperature to condition response model's policy, allowing cooperation with rational agents and self-reliance with weak agents through combination of self-play and planning
- Core assumption: Method can effectively cooperate with agents of varying playing strengths through temperature-based behavior adaptation
- Evidence anchors: Abstract mentions 37.6% improvement and ability to exploit weak agents; Figure 6 shows proxy policy entropy decreases with rising temperature; Figure shows cooperation with agents of different playing strength visible at τ = 0; corpus indicates related papers don't address cooperative behavior adaptation
- Break condition: If method cannot effectively cooperate with agents of varying playing strengths

## Foundational Learning

- Concept: Game Theory - Normal-form games, Nash equilibria, and Logit equilibria
  - Why needed here: Understanding game theoretic foundations is crucial for grasping equilibrium concepts used in Albatross like SBRLE
  - Quick check question: What is the difference between a Nash equilibrium and a Logit equilibrium?

- Concept: Reinforcement Learning - Self-play and planning
  - Why needed here: Albatross uses combination of self-play and planning similar to AlphaZero to learn SBRLE
  - Quick check question: How does self-play differ from training with fixed opponent in reinforcement learning?

- Concept: Opponent Modeling - Temperature parameter and maximum likelihood estimation
  - Why needed here: Albatross models bounded rationality using scalar temperature parameter and estimates it using maximum likelihood estimation
  - Quick check question: How does temperature parameter in Albatross relate to concept of bounded rationality in game theory?

## Architecture Onboarding

- Component map: Proxy Model -> Temperature Estimation -> Response Model
- Critical path:
  1. Train proxy model to approximate Logit equilibria at different temperatures
  2. Train response model to exploit proxy model based on estimated temperatures
  3. Estimate opponent rationality using maximum likelihood estimation
  4. Use estimated temperatures to condition response model's policy
- Design tradeoffs: Using scalar temperature parameter simplifies approach but may not capture all aspects of opponent behavior; MLE requires interaction data which may be limited
- Failure signatures: Poor cooperation performance indicates proxy model issues; failure to adapt suggests temperature estimation or response model problems
- First 3 experiments:
  1. Evaluate proxy model's ability to approximate Logit equilibria at different temperatures on small game
  2. Test temperature estimation algorithm on simple opponent with known rationality
  3. Assess cooperation performance of response model with proxy model at different temperatures

## Open Questions the Paper Calls Out
- How does Albatross perform in environments with imperfect information such as poker or hidden information games?
- Can Albatross effectively handle games with large joint action space or unknown environment dynamics?
- How does choice of temperature distribution during training affect performance across different game types?

## Limitations
- Scalar temperature parameter may not capture complex strategic behaviors beyond simple bounded rationality
- Method relies on accurate temperature estimation within single episodes which could face challenges with highly stochastic opponents
- Performance validation limited to two specific game domains (Overcooked and Battlesnake)

## Confidence
- Temperature estimation accuracy: Medium
- Zero-shot adaptation capability: Medium  
- Cross-game generalization: Low

## Next Checks
1. Test temperature estimation accuracy when playing against opponents with context-dependent rationality or mixed strategy types
2. Evaluate performance degradation when temperature estimation is perturbed by noise in observations
3. Benchmark against alternative opponent modeling approaches that use richer representations beyond scalar parameters