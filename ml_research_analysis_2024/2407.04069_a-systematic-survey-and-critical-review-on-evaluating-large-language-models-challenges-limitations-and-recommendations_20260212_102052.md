---
ver: rpa2
title: 'A Systematic Survey and Critical Review on Evaluating Large Language Models:
  Challenges, Limitations, and Recommendations'
arxiv_id: '2407.04069'
source_url: https://arxiv.org/abs/2407.04069
tags:
- arxiv
- evaluation
- preprint
- llms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper provides a systematic survey and critical review of the
  challenges, limitations, and recommendations for evaluating large language models
  (LLMs). It identifies key issues in reproducibility, reliability, and robustness
  across the LLM evaluation pipeline, including benchmark selection, model choice,
  prompt design, decoding strategies, and evaluation methods.
---

# A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations

## Quick Facts
- arXiv ID: 2407.04069
- Source URL: https://arxiv.org/abs/2407.04069
- Reference count: 40
- Key outcome: Systematic survey and critical review of LLM evaluation challenges, limitations, and recommendations

## Executive Summary
This paper provides a comprehensive systematic survey and critical review of the challenges, limitations, and recommendations for evaluating large language models (LLMs). The authors identify key issues in reproducibility, reliability, and robustness across the entire LLM evaluation pipeline, including benchmark selection, model choice, prompt design, decoding strategies, and evaluation methods. Through analysis of 212 evaluation papers, the study reveals significant inconsistencies in evaluation setups that undermine fair comparisons between models. The paper proposes best practices and implementation suggestions to ensure consistent, fair, and comprehensive evaluations of LLMs, addressing the critical need for standardized evaluation protocols in the rapidly evolving LLM landscape.

## Method Summary
The study conducted a systematic analysis of 212 LLM evaluation papers from Balloccu et al. (2024) to identify common practices and limitations across the evaluation pipeline. The authors performed a critical review of existing literature, examining challenges in benchmark selection, model choice, prompt design, decoding strategies, and evaluation methods. Through this analysis, they identified reproducibility issues stemming from inconsistent documentation, reliability concerns due to data contamination and inappropriate evaluation methods, and robustness challenges arising from model sensitivity to prompts and parameters. The methodology combines literature review with identification of specific failure modes and recommendations for improving evaluation practices across all stages of the pipeline.

## Key Results
- Lack of comprehensive documentation across evaluation pipeline stages causes significant reproducibility issues
- Minor variations in prompt design and decoding parameters can lead to substantial performance differences between models
- Automated evaluation methods alone can misclassify up to 10% of correct answers in generative tasks without human validation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Standardizing evaluation protocols across all pipeline stages increases reproducibility and reliability of LLM comparisons.
- **Mechanism**: By documenting and releasing details such as data subsets, model versions, prompts, decoding parameters, parsing scripts, and evaluation outputs, researchers enable exact replication of experiments, reducing inconsistencies caused by hidden variations.
- **Core assumption**: Variations in any step of the evaluation pipeline can significantly impact results, and transparency in these steps allows for fair, consistent comparisons.
- **Evidence anchors**:
  - [abstract]: "varied evaluation setups, causing inconsistencies in findings and interpretations"
  - [section]: "The primary challenge is the lack of comprehensive documentation for each part of the evaluation cycle"
  - [corpus]: Weak - corpus focuses on surveys of LLM use cases rather than specific reproducibility mechanisms.
- **Break condition**: If models are continuously updated with deprecation of older versions, reproducibility is still compromised despite standardized documentation.

### Mechanism 2
- **Claim**: Using multiple diverse prompts and tuned decoding parameters improves the robustness of LLM evaluations.
- **Mechanism**: Testing models across a range of prompts (e.g., zero-shot, few-shot, chain-of-thought) and parameter settings (temperature, top-k, beam size) reveals performance variations that single-prompt evaluations miss, ensuring evaluations reflect real-world usage diversity.
- **Core assumption**: LLMs are sensitive to prompt phrasing and decoding configurations, and their performance can vary significantly under different conditions.
- **Evidence anchors**:
  - [section]: "minor prompt variations can lead to diverse outcomes for different models" and "extensive tuning of decoding parameters could improve the performance during inference"
  - [abstract]: "varied evaluation setups, causing inconsistencies in findings"
  - [corpus]: Weak - surveys focus on general evaluation practices, not specific robustness experiments.
- **Break condition**: If computational resources are insufficient to test multiple prompts/parameters, robustness may be sacrificed for feasibility.

### Mechanism 3
- **Claim**: Combining automated parsing-based evaluation with human-in-the-loop validation enhances the reliability of generative task assessments.
- **Mechanism**: Automated parsing scripts extract target answers from LLM outputs for metric computation, but human review resolves ambiguities (e.g., synonyms, outdated references, or parsing errors), preventing misclassations that could bias results.
- **Core assumption**: Automated metrics alone can misclassify correct but non-matching answers, and human oversight corrects these errors.
- **Evidence anchors**:
  - [section]: "Laskar et al. (2023a) observed that evaluating LLMs entirely with an automated approach... may lead to an error of up to more than 10% difference in many tasks"
  - [abstract]: "parsing scripts are often necessary to extract target labels before applying evaluation metrics"
  - [corpus]: Weak - corpus does not discuss hybrid evaluation methods.
- **Break condition**: If human review is too costly or slow, reliability gains may not justify the resource investment.

## Foundational Learning

- **Concept**: Data contamination in LLM evaluation
  - Why needed here: Contamination undermines the validity of zero-shot evaluations by exposing models to test data during training, leading to inflated performance metrics.
  - Quick check question: How can researchers detect whether an LLM has been exposed to a benchmark dataset during training?

- **Concept**: Chain-of-thought prompting
  - Why needed here: CoT prompting improves reasoning task performance by encouraging intermediate reasoning steps, which can significantly impact evaluation outcomes.
  - Quick check question: What is the difference in performance between standard prompting and chain-of-thought prompting on complex reasoning tasks?

- **Concept**: Elo rating system for LLM comparison
  - Why needed here: Elo ratings provide a relative performance ranking across models via pairwise comparisons, but they can be sensitive to comparison order and subject to manipulation.
  - Quick check question: Why might two models have a higher win rate but a lower Elo rating in pairwise comparisons?

## Architecture Onboarding

- **Component map**: Evaluation pipeline → Benchmark selection → Model selection → Prompt design → Response generation → Parsing script design → Evaluation methodology → Output validation
- **Critical path**: Prompt design → Response generation → Parsing script design → Evaluation methodology; these steps directly affect metric computation and must be standardized
- **Design tradeoffs**: Automated vs. human evaluation (speed vs. reliability), open vs. closed-source model access (transparency vs. convenience), comprehensive vs. resource-constrained testing (robustness vs. feasibility)
- **Failure signatures**: Inconsistent results across similar setups, high variance in model rankings with minor prompt changes, parsing errors leading to metric misclassification
- **First 3 experiments**:
  1. Replicate a published LLM evaluation using exact data subsets, prompts, and decoding parameters to test reproducibility
  2. Compare model performance across 3-5 diverse prompts and parameter settings to assess robustness
  3. Apply human-in-the-loop validation on a subset of generative task outputs to measure reliability gains over pure automation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a standardized and comprehensive evaluation protocol that addresses the identified limitations across all stages of the LLM evaluation pipeline (benchmark selection, model choice, prompt design, decoding strategies, and evaluation methods)?
- Basis in paper: [explicit] The paper identifies significant challenges and limitations in LLM evaluation across reproducibility, reliability, and robustness dimensions, highlighting the need for a standardized approach.
- Why unresolved: While the paper provides recommendations for each stage, it does not offer a concrete, unified protocol that addresses all limitations simultaneously and can be widely adopted.
- What evidence would resolve it: Development and validation of a standardized evaluation framework that demonstrably improves consistency, fairness, and comprehensiveness of LLM evaluations across diverse tasks and model types.

### Open Question 2
- Question: What are the most effective methods for detecting and mitigating data contamination in LLM evaluation, especially considering the vast and constantly growing training data of modern LLMs?
- Basis in paper: [explicit] The paper discusses data contamination as a major reliability concern, noting that many LLMs may have encountered evaluation data during pre-training, undermining the validity of zero-shot evaluations.
- Why unresolved: Current contamination detection techniques are limited, and the paper acknowledges the difficulty of ensuring fair evaluation when LLMs are pre-trained on massive, uncurated datasets.
- What evidence would resolve it: Development of robust contamination detection methods with high precision and recall, along with techniques to effectively mitigate contamination's impact on evaluation results.

### Open Question 3
- Question: How can we ensure the generalizability and correlation of evaluation metrics with human judgment across diverse LLM tasks and domains, particularly for generative tasks where traditional string-based metrics may be insufficient?
- Basis in paper: [explicit] The paper highlights the shortcomings of string-based metrics like ROUGE for generative tasks and questions the correlation between automated evaluations and human judgments, especially for LLM-as-a-judge evaluations.
- Why unresolved: The paper notes the limitations of existing metrics and the lack of alignment with human preferences, but does not provide a definitive solution for robust evaluation across all task types.
- What evidence would resolve it: Development and validation of evaluation metrics that demonstrate strong correlation with human judgments across a wide range of LLM tasks, including generative tasks, and can effectively capture nuanced aspects of model performance.

## Limitations

- The paper relies predominantly on literature review rather than empirical validation of the proposed recommendations
- The corpus analysis provides weak evidence anchors for specific evaluation mechanisms
- The study does not address practical feasibility constraints of implementing comprehensive evaluation protocols in resource-limited settings

## Confidence

- **High confidence**: Claims about evaluation inconsistencies due to varied protocols and need for documentation standardization are well-supported by literature
- **Medium confidence**: Recommendations for multi-prompt testing and hybrid evaluation are logically sound but lack direct empirical validation
- **Low confidence**: Specific performance thresholds like "10% difference" in automated evaluation appear to reference single studies without broader corroboration

## Next Checks

1. Conduct controlled experiments comparing single-prompt versus multi-prompt evaluation approaches on identical models and benchmarks to quantify performance variance
2. Implement and test the proposed documentation template across 5-10 diverse LLM evaluation studies to assess practical utility and adoption barriers
3. Perform a reproducibility audit by attempting to replicate key findings from 3-5 recent LLM benchmark papers using only publicly available documentation and released artifacts