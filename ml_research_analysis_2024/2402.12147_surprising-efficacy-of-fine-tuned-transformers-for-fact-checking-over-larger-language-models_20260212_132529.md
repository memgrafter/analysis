---
ver: rpa2
title: Surprising Efficacy of Fine-Tuned Transformers for Fact-Checking over Larger
  Language Models
arxiv_id: '2402.12147'
source_url: https://arxiv.org/abs/2402.12147
tags:
- fact-checking
- claim
- claims
- llms
- fine-tuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper compares fine-tuned Transformer models against large
  language models (LLMs) for multilingual fact-checking. Fine-tuned XLM-RoBERTa-Large
  models outperform GPT-4, GPT-3.5-Turbo, and Mistral-7b on claim detection and veracity
  prediction tasks, achieving 0.743 Macro-F1 for claim detection versus 0.624 for
  GPT-4.
---

# Surprising Efficacy of Fine-Tuned Transformers for Fact-Checking over Larger Language Models

## Quick Facts
- arXiv ID: 2402.12147
- Source URL: https://arxiv.org/abs/2402.12147
- Reference count: 22
- Fine-tuned XLM-RoBERTa-Large outperforms GPT-4 (0.743 vs 0.624 Macro-F1) on claim detection

## Executive Summary
This paper challenges the assumption that large language models (LLMs) are superior for fact-checking tasks by demonstrating that fine-tuned Transformer models achieve better performance on classification tasks like claim detection and veracity prediction. The study compares fine-tuned XLM-RoBERTa-Large models against GPT-4, GPT-3.5-Turbo, and Mistral-7b across 114 languages, finding that fine-tuned models outperform LLMs in 37 of 46 tested languages for claim detection. While LLMs excel at generative tasks such as question decomposition for evidence retrieval, the results suggest a hybrid approach where fine-tuned models handle classification while LLMs assist with generation in fact-checking pipelines.

## Method Summary
The study fine-tuned XLM-RoBERTa-Large on ClaimBuster and CLEF CheckThat Lab data for claim detection, and on FEVER data plus Factisearch for veracity prediction using natural language inference. A separate FinQA-RoBERTa-Large model was fine-tuned for numerical claims. LLMs (GPT-4, GPT-3.5-Turbo, Mistral-7b) were evaluated using prompt engineering with temperature set to 0.2 and 3 random seeds. The Factiverse production dataset provided 3,066 English claims translated to 114 languages for evaluation, with Macro-F1 and Micro-F1 scores reported across 46 languages for claim detection.

## Key Results
- Fine-tuned XLM-RoBERTa-Large achieves 0.743 Macro-F1 for claim detection versus 0.624 for GPT-4
- Fine-tuned models exceed LLMs in 37 of 46 tested languages for multilingual claim detection
- GPT-3.5-Turbo achieves 0.741 Macro-F1 for question decomposition, outperforming fine-tuned T5-3b
- FinQA-RoBERTa-Large achieves 0.781 Macro-F1 for numerical claims, outperforming all LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned XLM-RoBERTa-Large models outperform LLMs on claim detection and veracity prediction tasks in multilingual fact-checking.
- Mechanism: Fine-tuning adapts the model architecture to the specific classification task, optimizing parameters for the fact-checking domain and enabling better performance on multilingual datasets.
- Core assumption: Task-specific fine-tuning provides better performance than general-purpose LLMs for classification tasks in the fact-checking domain.
- Evidence anchors:
  - [abstract] "Our real-world experimental benchmarks demonstrate that fine-tuning Transformer models specifically for fact-checking tasks, such as claim detection and veracity prediction, provide superior performance over large language models (LLMs) like GPT-4, GPT-3.5-Turbo, and Mistral-7b."
  - [section] "As shown in Figure 2, the fine-tuned XLM-RoBERTa-Large outperforms both OpenAI and Mistral models in most languages."
  - [corpus] Weak corpus evidence for this specific claim. Only found related work on AFC and LLMs but no direct comparison of fine-tuned models vs. LLMs for fact-checking tasks.

### Mechanism 2
- Claim: LLMs excel at generative tasks such as question decomposition for evidence retrieval in fact-checking pipelines.
- Mechanism: LLMs are better at generating diverse and relevant questions for evidence retrieval due to their ability to understand context and generate natural language.
- Core assumption: Generative tasks like question decomposition benefit more from the language understanding capabilities of LLMs compared to fine-tuned models.
- Evidence anchors:
  - [abstract] "However, we illustrate that LLMs excel in generative tasks such as question decomposition for evidence retrieval."
  - [section] "Table 2 shows that GPT-3.5-Turbo outperforms other question decomposition methods, including GPT-4, with the fine-tuned T5-3b model ranking lowest."
  - [corpus] Weak corpus evidence for this specific claim. Only found related work on AFC and LLMs but no direct comparison of generative tasks.

### Mechanism 3
- Claim: Fine-tuned models are superior for classification tasks while LLMs are better for generative subtasks in fact-checking pipelines.
- Mechanism: The strengths of fine-tuned models and LLMs complement each other in different parts of the fact-checking pipeline, with fine-tuned models excelling at classification and LLMs at generation.
- Core assumption: Different tasks in the fact-checking pipeline benefit from different model architectures and training approaches.
- Evidence anchors:
  - [abstract] "The results suggest fine-tuned models are superior for classification tasks while LLMs are better for generative subtasks in fact-checking pipelines."
  - [section] "Table 3 shows the average Macro-F1 and Micro-F1 scores for all four models. This suggests that a fine-tuned model is significantly better for claim detection compared to using LLMs with prompt engineering."
  - [corpus] Weak corpus evidence for this specific claim. Only found related work on AFC and LLMs but no direct comparison of the two approaches.

## Foundational Learning

- Concept: Multilingual Transformers
  - Why needed here: The fact-checking pipeline needs to handle claims in over 90 languages, requiring models that can understand and process multilingual text.
  - Quick check question: What are the key differences between multilingual and monolingual transformers, and how do they impact performance on multilingual tasks?

- Concept: Prompt Engineering
  - Why needed here: LLMs require well-crafted prompts to perform tasks effectively, and prompt engineering is crucial for optimizing their performance in the fact-checking pipeline.
  - Quick check question: What are the best practices for prompt engineering with LLMs, and how can they be applied to improve the performance of generative tasks in the fact-checking pipeline?

- Concept: Natural Language Inference (NLI)
  - Why needed here: The veracity prediction stage of the fact-checking pipeline relies on NLI to determine if evidence supports or refutes a claim, requiring a strong understanding of NLI models and techniques.
  - Quick check question: How do different NLI models and approaches impact the performance of veracity prediction in the fact-checking pipeline, and what are the key considerations for selecting and fine-tuning an NLI model?

## Architecture Onboarding

- Component map: Front-end React app -> Python FastAPI backend -> ML models (claim detection, evidence search, veracity prediction) -> Search engines (Google, Bing, You.com, Wikipedia, Semantic Scholar, Factiverse's fact-checking collection) -> Kubernetes cluster on Google Cloud Platform
- Critical path: Claim detection → Evidence retrieval → Veracity prediction
- Design tradeoffs:
  - Fine-tuned models vs. LLMs for classification tasks (tradeoff between task-specific performance and generalization)
  - Multilingual transformers vs. language-specific models (tradeoff between coverage and performance)
  - Self-hosted vs. third-party LLMs (tradeoff between privacy and ease of use)
- Failure signatures:
  - Low recall in claim detection stage (missing check-worthy claims)
  - Irrelevant evidence retrieval (poor question decomposition or search results)
  - Inaccurate veracity prediction (misclassification of claims)
- First 3 experiments:
  1. Compare the performance of fine-tuned XLM-RoBERTa-Large and LLMs on a held-out test set for claim detection.
  2. Evaluate the effectiveness of different question decomposition methods for evidence retrieval using a subset of the test data.
  3. Assess the accuracy of veracity prediction using different NLI models and evidence aggregation strategies on a sample of claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of fine-tuned models compare to LLMs when scaling to a larger number of languages beyond the 114 tested?
- Basis in paper: [inferred] The paper tested 114 languages but acknowledges the need for further studies to confirm findings at a larger scale.
- Why unresolved: The current study's dataset and scope are limited, and the paper explicitly states the need for larger scale experiments.
- What evidence would resolve it: Testing the same models across 200+ languages and comparing performance metrics.

### Open Question 2
- Question: What are the specific architectural modifications needed to improve Mistral-7b's performance in claim detection tasks?
- Basis in paper: [explicit] The paper notes Mistral-7b performs poorly at claim detection and hallucinates more than other models.
- Why unresolved: The paper identifies the problem but doesn't explore architectural solutions or fine-tuning strategies specific to Mistral-7b.
- What evidence would resolve it: Comparative experiments testing different fine-tuning approaches, prompt engineering techniques, or architectural modifications for Mistral-7b.

### Open Question 3
- Question: How does the quality of evidence retrieval (measured by IR metrics like NDCG) correlate with final veracity prediction accuracy?
- Basis in paper: [inferred] The paper mentions they will leave evidence retrieval evaluation for future work and only report effects on veracity prediction.
- Why unresolved: The paper only reports the downstream effect on veracity prediction without analyzing the intermediate evidence retrieval quality.
- What evidence would resolve it: Experiments measuring both IR metrics (NDCG, MRR) for retrieved evidence and correlating them with veracity prediction accuracy across different decomposition methods.

## Limitations

- The comparison is limited to three specific LLM models (GPT-4, GPT-3.5-Turbo, Mistral-7b) with undisclosed prompt templates
- The multilingual coverage claims are based on 46 languages from the translated dataset, which may not represent true multilingual capabilities
- The study doesn't address computational costs or latency differences between fine-tuned models and API-based LLMs

## Confidence

- **High confidence**: The core finding that fine-tuned XLM-RoBERTa-Large models achieve 0.743 Macro-F1 for claim detection versus 0.624 for GPT-4 is well-supported by the experimental results and aligns with established research on task-specific fine-tuning benefits.
- **Medium confidence**: The claim about LLMs excelling at generative tasks like question decomposition is supported by the 0.741 Macro-F1 for GPT-3.5-Turbo, but the comparison lacks ablation studies on prompt engineering variations and doesn't explore whether fine-tuned generative models could achieve similar results.
- **Medium confidence**: The numerical claims results showing FinQA-RoBERTa-Large at 0.781 Macro-F1 versus lower LLM performance are promising but based on only 100 test samples, which may not be statistically robust.

## Next Checks

1. **Prompt engineering ablation**: Test multiple prompt variants for GPT-4 and GPT-3.5-Turbo on claim detection task to determine if the 0.624 Macro-F1 result represents a lower bound or if optimized prompts could close the gap with fine-tuned models.

2. **Cross-lingual robustness test**: Evaluate model performance across language families (e.g., Romance vs. non-Indo-European languages) to identify whether the 37/46 languages advantage is uniform or concentrated in specific language groups.

3. **Computational efficiency comparison**: Measure inference time and cost per prediction for fine-tuned XLM-RoBERTa-Large versus API-based LLMs on the same hardware/infrastructure to assess practical deployment tradeoffs beyond accuracy metrics.