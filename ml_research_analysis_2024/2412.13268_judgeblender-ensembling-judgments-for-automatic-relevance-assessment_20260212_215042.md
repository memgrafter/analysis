---
ver: rpa2
title: 'JudgeBlender: Ensembling Judgments for Automatic Relevance Assessment'
arxiv_id: '2412.13268'
source_url: https://arxiv.org/abs/2412.13268
tags:
- relevance
- judgments
- methods
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: JudgeBlender addresses the high cost and potential bias of using
  large language models (LLMs) for automated relevance judgment in information retrieval
  by introducing an ensemble-based framework. The framework combines multiple LLMs
  or prompts to generate more robust and accurate relevance judgments.
---

# JudgeBlender: Ensembling Judgments for Automatic Relevance Assessment

## Quick Facts
- arXiv ID: 2412.13268
- Source URL: https://arxiv.org/abs/2412.13268
- Authors: Hossein A. Rahmani; Emine Yilmaz; Nick Craswell; Bhaskar Mitra
- Reference count: 26
- One-line primary result: Ensemble methods combining multiple LLMs or prompts achieve strong correlation with human judgments while reducing bias and cost compared to single large models

## Executive Summary
JudgeBlender addresses the high cost and potential bias of using large language models (LLMs) for automated relevance judgment in information retrieval by introducing an ensemble-based framework. The framework combines multiple LLMs or prompts to generate more robust and accurate relevance judgments. It includes two variants: PromptBlender, which uses diverse prompts with a single LLM, and LLMBlender, which employs multiple LLMs with different prompts. The study evaluated JudgeBlender on the LLMJudge benchmark, demonstrating that both variants achieve strong correlation with human judgments and outperform single-model approaches.

## Method Summary
JudgeBlender introduces an ensemble framework for automated relevance judgment that combines multiple LLM outputs to generate more reliable assessments. The method has two variants: PromptBlender uses a single LLM with diverse prompts, while LLMBlender employs multiple different LLMs, each with distinct prompts. Both variants aggregate outputs using majority voting or average voting functions. The approach is evaluated on the LLMJudge challenge dataset based on TREC DL 2023 passage ranking task, using Cohen's kappa and Krippendorff's alpha to measure agreement with human judgments, along with Kendall's tau and Spearman's rho for system ranking correlation.

## Key Results
- LLMBlender-MV(Avg.) showed particularly strong performance, maintaining consistent agreement across all relevance levels
- Both PromptBlender and LLMBlender variants achieved strong correlation with human judgments and outperformed single-model approaches
- The method effectively reduces bias toward specific system types and provides a cost-effective alternative to relying on large, expensive models for relevance assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining multiple LLM judgments reduces individual model bias
- Mechanism: Multiple models or prompts provide diverse perspectives on relevance, compensating for each model's specific biases
- Core assumption: Different LLMs or prompts will make different types of errors that are complementary
- Evidence anchors:
  - [abstract]: "incorporates multiple LLMs or prompts to generate more robust and accurate relevance judgments"
  - [section 2]: "by aggregating their outputs, we aim to achieve a more balanced and comprehensive assessment"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.527
- Break condition: If models share similar training data or architectures, they may make correlated errors rather than complementary ones

### Mechanism 2
- Claim: Ensemble methods improve correlation with human judgments
- Mechanism: Aggregating multiple relevance scores through voting or averaging produces final scores that better match human assessment patterns
- Core assumption: Human judgments contain inherent uncertainty that multiple models can capture through ensemble averaging
- Evidence anchors:
  - [section 4.4]: "Both metrics are used to evaluate the agreement between human labels and the relevance scores generated by our LLM-based methods"
  - [section 5.1]: "Table 2 presents the correlation between scores produced by different evaluator methods and human judgments"
  - [corpus]: Average neighbor citations=0.0
- Break condition: If ensemble aggregation method is poorly chosen, it may amplify noise rather than reduce it

### Mechanism 3
- Claim: Smaller open-source models can match larger commercial models when ensembled
- Mechanism: Multiple smaller models provide sufficient diversity and computational efficiency while maintaining quality
- Core assumption: Model diversity matters more than individual model size for this task
- Evidence anchors:
  - [abstract]: "demonstrates that very large models are often unnecessary for reliable relevance assessments"
  - [section 1]: "employing smaller, open-source models to provide relevance judgments"
  - [section 4.3]: "We use open-source, small language models for both variants of JudgeBlender"
- Break condition: If the task requires very high reasoning complexity that smaller models cannot handle

## Foundational Learning

- Concept: Ensemble methods in machine learning
  - Why needed here: Understanding how combining multiple models improves robustness and reduces variance
  - Quick check question: Why does averaging predictions from multiple models typically reduce error compared to a single model?

- Concept: Relevance judgment scales in information retrieval
  - Why needed here: Understanding the four-point scale (perfectly relevant, highly relevant, related, irrelevant) used in TREC DL
  - Quick check question: What are the four levels of relevance in the TREC DL judgment scale and what numerical values are assigned to each?

- Concept: Inter-annotator agreement metrics
  - Why needed here: Understanding Cohen's kappa and Krippendorff's alpha for measuring agreement with human judgments
  - Quick check question: What threshold value for Cohen's kappa indicates strong agreement between annotators?

## Architecture Onboarding

- Component map:
  - Input layer: Query and passage pairs
  - Judge layer: Multiple LLM evaluators (either different models or different prompts)
  - Aggregation layer: Voting or averaging functions
  - Output layer: Final relevance score

- Critical path: Query/Passage → Multiple LLM evaluations → Aggregator function → Final score

- Design tradeoffs:
  - Number of models vs computational cost
  - Voting method (majority vs average) vs accuracy
  - Open-source vs commercial models vs reproducibility

- Failure signatures:
  - Poor correlation with human judgments indicates aggregation method issues
  - Consistent bias toward certain system types suggests model selection problems
  - High computational cost may indicate inefficient model choices

- First 3 experiments:
  1. Test PromptBlender with different numbers of prompts (2 vs 3 vs 4) to find optimal balance
  2. Compare different aggregation methods (MV with different tie-breakers vs AV) on same model pool
  3. Evaluate bias across different system categories (GPT, T5, GPT+T5, Others) to identify systematic errors

## Open Questions the Paper Calls Out
The paper suggests several directions for future research: exploring more advanced aggregation strategies beyond simple averaging and majority voting, such as leveraging other LLMs to make the final decision; optimizing panel selection and prompt strategies to balance quality and cost; and investigating JudgeBlender's performance across diverse datasets beyond the TREC DL 2023 benchmark.

## Limitations
- The evaluation is confined to the TREC DL 2023 dataset, which may not represent all information retrieval scenarios
- The study uses specific open-source models and custom prompts, making it unclear whether the approach generalizes to other model families or domains
- The computational efficiency gains are not fully quantified, and the impact of ensemble size on performance remains unexplored

## Confidence
- High confidence: The ensemble approach improves correlation with human judgments compared to single models
- Medium confidence: Smaller models can match larger models when ensembled, as this requires assumptions about task complexity
- Medium confidence: The method effectively reduces bias toward specific system types, though the analysis is limited to four categories

## Next Checks
1. Test JudgeBlender on multiple information retrieval datasets beyond TREC DL 2023 to assess generalizability
2. Experiment with different ensemble sizes to determine the optimal balance between performance and computational cost
3. Conduct ablation studies comparing different aggregation methods (MV vs AV) across various model combinations to identify the most effective configuration