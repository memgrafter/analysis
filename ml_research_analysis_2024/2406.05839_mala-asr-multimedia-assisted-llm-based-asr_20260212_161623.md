---
ver: rpa2
title: 'MaLa-ASR: Multimedia-Assisted LLM-Based ASR'
arxiv_id: '2406.05839'
source_url: https://arxiv.org/abs/2406.05839
tags:
- speech
- recognition
- information
- mala-asr
- keywords
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes MaLa-ASR, a multi-modal large language model
  (LLM)-based automatic speech recognition (ASR) system that leverages textual keywords
  extracted from presentation slides to improve recognition of conference content.
  The core idea is to incorporate slide keywords into the input prompt fed to an LLM
  decoder, which is connected to a speech encoder via a linear projector.
---

# MaLa-ASR: Multimedia-Assisted LLM-Based ASR

## Quick Facts
- arXiv ID: 2406.05839
- Source URL: https://arxiv.org/abs/2406.05839
- Reference count: 0
- The paper proposes MaLa-ASR, a multi-modal LLM-based ASR system that leverages slide keywords to improve recognition of conference content, achieving significant WER reductions of 27.9% and 44.7% compared to baseline on SlideSpeech dataset.

## Executive Summary
MaLa-ASR is a multimedia-assisted LLM-based ASR system that incorporates textual keywords from presentation slides into the input prompt to improve recognition of conference content. The system uses a frozen WavLM speech encoder and Vicuna LLM decoder connected via a lightweight linear projector, with only the projector being trained. The approach achieves significant improvements in both overall word error rate and biased word error rate on the SlideSpeech corpus, demonstrating the effectiveness of keyword-assisted prompting in ASR.

## Method Summary
MaLa-ASR uses WavLM Large as a frozen speech encoder to extract audio embeddings, which are then mapped to LLM token space through a linear projector. The Vicuna 7B LLM decoder generates transcriptions based on concatenated speech embeddings and keyword-enhanced prompts. The model is trained only on the projector weights using cross-entropy loss, while the speech encoder and LLM remain frozen. Training is performed on the SlideSpeech corpus with 473 hours (L95) and 161 hours (S95) of conference presentation audio paired with slide-extracted keywords and transcriptions.

## Key Results
- Achieved WER reductions of 27.9% and 44.7% compared to baseline on L95 and S95 subsets respectively
- B-WER reduced by 46.0% and 44.2% when keywords were added to input prompts
- Demonstrated robustness by maintaining comparable performance when keywords were unavailable during inference
- Established new state-of-the-art results on SlideSpeech dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based ASR with slide keywords improves recognition of conference content
- Mechanism: Textual keywords from slides are injected into LLM prompts, allowing the model to bias recognition toward domain-specific vocabulary and named entities present in the slides
- Core assumption: The LLM can effectively leverage keyword context in its autoregressive decoding without additional architectural changes beyond prompt engineering
- Evidence anchors:
  - By adding keywords to the input prompt, the biased word error rate (B-WER) reduces relatively by 46.0% and 44.2%
  - MaLa-ASR achieves average WERs of 9.4% and 11.7%, demonstrating a significant relative WER reduction of 27.9% and 44.7% compared to the contextual ASR baseline model proposed in SlideSpeech
- Break condition: If keyword relevance to spoken content is low or if keywords are noisy or misleading, B-WER improvement may reverse

### Mechanism 2
- Claim: Freezing speech encoder and LLM while training only a lightweight projector reduces training cost and complexity
- Mechanism: The speech encoder extracts fixed audio embeddings, the projector maps these to LLM token space, and the frozen LLM handles autoregressive generation; only the projector is updated during training
- Core assumption: The frozen LLM's pre-trained generative ability is sufficient for ASR when paired with appropriate speech embeddings via the projector
- Evidence anchors:
  - We only train the lightweight projector and freeze the rest of the model
  - An adaptor is necessary for aligning the speech feature space with the LLM text token space...we use a simple-structured linear projector
- Break condition: If the fixed LLM lacks sufficient ASR capability or projector capacity is too low, performance will plateau regardless of prompt content

### Mechanism 3
- Claim: Incorporating slide keywords does not significantly harm performance when keywords are unavailable in inference
- Mechanism: The model trained with keyword prompts can still operate effectively when no keywords are provided, indicating robustness to missing auxiliary information
- Core assumption: The LLM can generalize beyond keyword-augmented prompts and maintain ASR quality in their absence
- Evidence anchors:
  - Our ablation experiments...reveal that the WER result of Model S3, trained with keywords but inferred without, is comparable to that of Model S1
  - This demonstrates the model's robustness and suggests its practicality in real-life settings where keywords are unavailable
- Break condition: If the model overfits to keyword prompts during training, performance may degrade when keywords are absent

## Foundational Learning

- Concept: Prompt engineering for multimodal LLM integration
  - Why needed here: The LLM accepts text prompts; keywords must be embedded in a way the LLM can leverage during decoding
  - Quick check question: How are keywords inserted into the prompt template in the training data?

- Concept: ASR evaluation metrics (WER, B-WER, U-WER, Recall)
  - Why needed here: Performance is measured on both overall word error rate and keyword-specific error rates; understanding these is critical to interpret results
  - Quick check question: What is the difference between B-WER and U-WER in this context?

- Concept: Projector adaptation in speech-LLM pipelines
  - Why needed here: The projector maps audio embeddings to LLM token space; its design and training directly impact recognition quality
  - Quick check question: What is the dimensionality of the projected embeddings before being fed to the LLM?

## Architecture Onboarding

- Component map:
  WavLM Large (frozen) -> speech encoder -> 1-D convolution -> linear layers -> projector -> LLM token space
  Vicuna 7B (frozen) -> LLM decoder
  Prompt encoder -> text token space
  Concatenated embeddings -> LLM input -> autoregressive transcription output

- Critical path:
  Speech encoder -> Projector -> LLM -> Transcription
  (Prompt + Keywords) -> LLM (parallel path)

- Design tradeoffs:
  - Freezing encoder/LLM reduces trainable parameters but limits fine-tuning flexibility
  - Simple linear projector is lightweight but may underfit complex audio-LLM mapping
  - Prompt-based keyword integration is simple but relies on LLM's in-context learning ability

- Failure signatures:
  - High WER despite low projector parameter count -> projector capacity issue
  - No B-WER improvement despite keyword injection -> prompt engineering or keyword relevance problem
  - Performance collapse when keywords missing -> overfitting to keyword prompts

- First 3 experiments:
  1. Train baseline without keywords, measure WER
  2. Add keyword prompts, measure B-WER improvement
  3. Train with keywords but test without them, verify robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can historical context be effectively incorporated into LLM-based ASR models to improve performance on long-content speech recognition?
- Basis in paper: The authors explicitly state that incorporating historical context into prompts did not yield significant gains, despite various prompt designs and context lengths being tested
- Why unresolved: The paper suggests that keywords are more directly linked to speech content, while preceding information aligns more with background and theme. The effectiveness of historical context in this context remains unclear
- What evidence would resolve it: Experimental results showing improved WER when historical context is effectively integrated, possibly through different architectural approaches or prompt engineering techniques

### Open Question 2
- Question: What is the optimal way to extract and utilize visual information from slides to assist in speech recognition?
- Basis in paper: The authors mention the potential of using visual encoders to extract features from slides to capture structural information, but this approach is not explored in the current study
- Why unresolved: The paper focuses on textual keywords from slides but does not investigate the potential benefits of visual features extracted from slides
- What evidence would resolve it: Comparative studies showing improved ASR performance when visual features from slides are incorporated alongside textual keywords

### Open Question 3
- Question: How can the memory and computational efficiency of LLM-based ASR models be improved without sacrificing performance?
- Basis in paper: The authors acknowledge the limitations of large language models, including high memory utilization and reduced decoding speeds due to autoregressive decoding
- Why unresolved: The paper does not propose specific solutions to address these efficiency issues, focusing instead on the model's performance benefits
- What evidence would resolve it: Experimental results demonstrating improved efficiency (e.g., faster inference, lower memory usage) while maintaining or improving ASR accuracy

## Limitations

- The system's effectiveness fundamentally depends on the quality and relevance of slide-extracted keywords to the spoken content
- MaLa-ASR is trained and evaluated on academic conference presentations, limiting generalizability to other ASR domains
- The choice to freeze both speech encoder and LLM may limit the model's ability to learn complex audio-text alignments

## Confidence

- High Confidence: The core architectural design of using a frozen LLM decoder with a trainable projector for ASR tasks is sound and supported by results
- Medium Confidence: The claim that keyword injection improves B-WER by 46.0% and 44.2% is well-supported by experimental results, but the mechanism's robustness across different keyword qualities and domains is uncertain
- Low Confidence: The assertion that the model is practical in real-life settings where keywords are unavailable, while supported by ablation studies, lacks broader validation across diverse scenarios

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate MaLa-ASR on ASR datasets from different domains (e.g., Switchboard, Common Voice) without domain-specific fine-tuning to assess whether keyword-prompting benefits transfer beyond conference presentations

2. **Keyword Quality Sensitivity Analysis**: Systematically vary keyword quality (relevant, partially relevant, irrelevant) in the input prompts and measure B-WER and WER degradation to quantify the system's sensitivity to keyword relevance

3. **Ablation of Projector Complexity**: Replace the linear projector with progressively more complex architectures (e.g., small MLP, attention-based) while freezing the encoder and LLM to determine if the simple linear design is optimal or if increased capacity yields better performance