---
ver: rpa2
title: Efficient Model Compression Techniques with FishLeg
arxiv_id: '2412.02328'
source_url: https://arxiv.org/abs/2412.02328
tags:
- pruning
- fishleg
- loss
- fisher
- inverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FishLeg Surgeon (FLS), a second-order pruning
  method that uses the FishLeg optimizer to efficiently estimate the inverse Fisher
  Information Matrix (FIM) during gradual pruning. Unlike traditional methods that
  recompute the FIM from scratch after each pruning step, FLS updates the inverse
  FIM approximation online between pruning steps by minimizing an auxiliary loss.
---

# Efficient Model Compression Techniques with FishLeg

## Quick Facts
- arXiv ID: 2412.02328
- Source URL: https://arxiv.org/abs/2412.02328
- Authors: Jamie McGowan, Wei Sheng Lai, Weibin Chen, Henry Aldridge, Jools Clarke, Jezabel Garcia, Rui Xia, Yilei Liang, Guillaume Hennequin, Alberto Bernacchia
- Reference count: 24
- Primary result: Achieves 84% accuracy at 95% sparsity on CIFAR-10 compared to 60% for OBS, and 53% accuracy at 80% sparsity on TinyIM compared to 48% for OBS

## Executive Summary
FishLeg Surgeon (FLS) introduces a second-order pruning method that efficiently estimates the inverse Fisher Information Matrix (FIM) during gradual pruning using the FishLeg optimizer. Unlike traditional methods that recompute the FIM from scratch after each pruning step, FLS updates the inverse FIM approximation online between pruning steps by minimizing an auxiliary loss. This approach, combined with memory-efficient block-diagonal parameterization using tensor factorization techniques, enables scalable second-order pruning without the memory explosion typical of full second-order methods. Experiments on ResNet18 with CIFAR-10 and TinyIM datasets demonstrate significant improvements over magnitude pruning and OBS baselines at high sparsity levels.

## Method Summary
FLS uses the FishLeg optimizer to directly estimate the inverse Fisher Information Matrix Q(λ) through auxiliary loss minimization, avoiding the expensive two-step process of estimating Fisher then inverting it. The method employs a block-diagonal parameterization with tensor factorization to handle large models efficiently, and introduces a preconditioner to accelerate convergence when the FIM is ill-conditioned. During gradual pruning, FLS alternates between pruning steps (selecting weights based on inverse Fisher diagonal) and fine-tuning steps (updating remaining weights while preserving sparsity), with auxiliary loss minimization running in parallel to update the inverse Fisher estimate. The approach amortizes inverse Fisher computation through meta-learning, making second-order pruning scalable to modern architectures.

## Key Results
- Achieves 84% accuracy at 95% sparsity on CIFAR-10 (vs 60% for OBS baseline)
- Achieves 53% accuracy at 80% sparsity on TinyIM (vs 48% for OBS baseline)
- Demonstrates significant improvements over magnitude pruning and OBS baselines at high sparsity levels

## Why This Works (Mechanism)

### Mechanism 1
FLS avoids expensive Fisher matrix recomputation by amortizing the inverse Fisher estimate through meta-learning. Instead of recomputing the full Fisher matrix from scratch after each pruning step, FLS updates the inverse Fisher approximation online between pruning steps using an auxiliary loss. This is possible because the FishLeg optimizer directly estimates the inverse Fisher matrix Q(λ) rather than estimating Fisher and then inverting it. The core assumption is that the inverse Fisher matrix changes gradually enough between pruning steps that a meta-learned update is sufficient rather than full recomputation.

### Mechanism 2
Direct estimation of inverse Fisher reduces sensitivity to gradient noise compared to estimating Fisher then inverting. FLS learns Q(λ) to approximate F⁻¹ directly through auxiliary loss minimization, which is less sensitive to stochastic noise in gradient estimates than the two-step process of estimating Fisher then inverting it. The auxiliary loss formulation provides unbiased gradients that converge to the true inverse Fisher despite noisy estimates.

### Mechanism 3
Block-diagonal parameterization with tensor factorization enables scalable second-order pruning without sacrificing accuracy. FLS uses memory-efficient block-diagonal parameterization of the inverse Fisher matrix with tensor factorization techniques, allowing it to handle large models without the memory explosion of full second-order methods. The block-diagonal structure with appropriate tensor factorization captures enough of the Fisher structure for effective pruning while remaining computationally tractable.

## Foundational Learning

- Concept: Fisher Information Matrix (FIM) and its role in second-order optimization
  - Why needed here: The paper's core innovation relies on understanding what the Fisher matrix represents and why its inverse is valuable for determining parameter importance during pruning
  - Quick check question: What information does the Fisher matrix capture about a model's parameters, and why is its inverse useful for determining parameter importance during pruning?

- Concept: Kronecker-factored curvature approximations (KFAC)
  - Why needed here: FLS builds on KFAC concepts but extends them with more flexible parameterization
  - Quick check question: How does KFAC approximate the Fisher matrix using Kronecker products, and what are its computational advantages over full second-order methods?

- Concept: Meta-learning and amortized inference
  - Why needed here: FLS uses meta-learning to amortize the computation of inverse Fisher products rather than computing them explicitly each time
  - Quick check question: What is the key idea behind using meta-learning to amortize computationally expensive operations, and how does this apply to inverse Fisher matrix estimation?

## Architecture Onboarding

- Component map: Pruning module -> Fine-tuning module -> Auxiliary loss minimization module -> Tensor factorization module
- Critical path: The main training loop alternates between pruning steps (selecting and removing weights) and fine-tuning steps (updating remaining weights while preserving sparsity), with auxiliary loss minimization running in parallel to update the inverse Fisher estimate
- Design tradeoffs: FLS trades off between expressiveness of the inverse Fisher approximation (larger Q parameterization) and memory/compute efficiency (smaller, more structured Q), as well as between update frequency of the inverse Fisher and computational overhead
- Failure signatures: Poor pruning performance may indicate inadequate inverse Fisher approximation (check auxiliary loss convergence), while memory issues suggest Q parameterization is too large, and slow convergence may indicate need for better preconditioning or initialization
- First 3 experiments:
  1. Verify that FLS can maintain accuracy during unstructured pruning on a small network (e.g., LeNet on MNIST) compared to magnitude pruning
  2. Test the effect of different Q parameterizations (diagonal vs Kronecker vs full) on pruning performance and memory usage
  3. Evaluate the importance of preconditioning by comparing auxiliary loss convergence with and without the proposed preconditioner

## Open Questions the Paper Calls Out

### Open Question 1
How well would FishLeg Surgeon scale to transformer architectures with attention mechanisms and larger parameter counts? The paper demonstrates FishLeg on ResNet18 with CIFAR-10 and TinyIM datasets but does not test on transformer architectures or larger models. Empirical results showing FishLeg performance on transformer models (BERT, GPT variants) at various scales would be needed to resolve this.

### Open Question 2
What is the optimal trade-off between block size in the block-diagonal parameterization and memory/computational efficiency for different network architectures? The authors discuss their block-diagonal parameterization approach but do not systematically explore the optimal size. Systematic experiments varying block sizes across different architectures measuring both pruning performance and resource usage would identify Pareto-optimal configurations.

### Open Question 3
How does the choice of damping parameter γ affect FishLeg's performance across different pruning scenarios and model scales? The authors use γ values of 10^-3 for CIFAR-10 and 1.0 for TinyIM but do not explore how varying this parameter affects performance. Comprehensive sensitivity analysis showing FishLeg performance across a range of γ values would provide guidance for parameter selection.

## Limitations

- Limited evaluation scope: Only tested on ResNet18 with two image classification datasets (CIFAR-10 and TinyIM), lacking validation on larger models, different architectures, or diverse tasks
- Scalability claims unverified: While the paper provides theoretical arguments for scalability, empirical evidence is limited to ResNet18 without testing on architectures like ResNet50, Vision Transformers, or language models
- Incomplete comparison: Claims of universal superiority are premature without comparison to recent state-of-the-art methods like SWORD, RigL, or movement pruning on the same datasets

## Confidence

- High confidence in core mechanism claims: The FishLeg optimizer's approach to directly estimating inverse Fisher matrices through auxiliary loss minimization is technically sound and well-supported
- Medium confidence in scalability claims: Theoretical arguments exist but empirical evidence is limited to ResNet18
- Low confidence in universal superiority claim: Only compares against magnitude pruning and OBS on two datasets without testing recent state-of-the-art methods

## Next Checks

1. Test FLS on ResNet50 and Vision Transformer (ViT) architectures to verify that the block-diagonal approximation and tensor factorization techniques scale effectively without accuracy degradation or memory issues.

2. Evaluate FLS on non-vision tasks including language modeling (BERT fine-tuning) and speech recognition to determine if the second-order pruning approach generalizes beyond image classification.

3. Benchmark FLS against recent state-of-the-art pruning methods like SWORD, RigL, and movement pruning on the same datasets to establish its relative performance in the current landscape.