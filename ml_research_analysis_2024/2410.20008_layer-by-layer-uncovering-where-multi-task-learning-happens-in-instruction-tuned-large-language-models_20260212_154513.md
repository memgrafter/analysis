---
ver: rpa2
title: 'Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned
  Large Language Models'
arxiv_id: '2410.20008'
source_url: https://arxiv.org/abs/2410.20008
tags:
- llama
- layer
- layers
- tasks
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how instruction tuning affects the representations
  learned by large language models (LLMs) across over 60 NLP tasks. Using a matrix
  analysis framework (MOSSA) and the Center Kernel Alignment (CKA) metric, we compare
  pre-trained and instruction-tuned LLMs to identify where task-specific information
  is encoded.
---

# Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models

## Quick Facts
- **arXiv ID**: 2410.20008
- **Source URL**: https://arxiv.org/abs/2410.20008
- **Reference count**: 33
- **Primary result**: Instruction tuning creates three functional layer groups in LLMs: shared layers (1-9) for general representations, transition layers (10-15) for intensifying task-specific information, and refinement layers (16-32) for task-specific refinement

## Executive Summary
This study investigates how instruction tuning affects the representations learned by large language models across over 60 NLP tasks. Using a matrix analysis framework (MOSSA) and the Center Kernel Alignment (CKA) metric, we compare pre-trained and instruction-tuned LLMs to identify where task-specific information is encoded. We find that pre-trained LLMs already encode some tasks well, while others benefit significantly from instruction tuning. The instruction-tuned model exhibits three distinct functional layer groups: shared layers (1-9) that learn general representations across tasks, transition layers (10-15) that intensify task-specific information, and refinement layers (16-32) that continue refining representations toward specific tasks. This mapping extends our understanding of LLM mechanisms and has implications for parameter-efficient fine-tuning and multi-task learning. Our findings also reveal that instruction-tuned models generalize better to unseen tasks due to their ability to produce more general lower-layer representations while maintaining task-specific information in deeper layers.

## Method Summary
The study uses Llama 2-7B as the base model and fine-tunes it using LoRA with rank 8 on over 60 NLP tasks from the Flan 2021 dataset. Control models are fine-tuned on individual tasks. The MOSSA framework computes CKA similarities between representations at different layers of pre-trained, instruction-tuned, and control models. The analysis identifies three functional layer groups and examines how instruction tuning affects task representation across these layers. The study also evaluates generalization to unseen tasks and correlates CKA similarities with reading difficulty measures.

## Key Results
- Pre-trained LLMs already encode some tasks well, while others benefit significantly from instruction tuning
- Instruction-tuned models exhibit three distinct functional layer groups: shared layers (1-9), transition layers (10-15), and refinement layers (16-32)
- Instruction-tuned models generalize better to unseen tasks due to more general lower-layer representations while maintaining task-specific information in deeper layers
- Tasks requiring specialized knowledge benefit more from instruction tuning than tasks relying on general linguistic understanding

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Instruction tuning transforms the representational landscape of LLMs, enabling them to learn more general representations in lower layers and task-specific representations in middle and higher layers.
- **Mechanism**: Instruction tuning on multiple tasks encourages the model to diverge from individual task representations in lower layers, creating shared representations that generalize across tasks. In middle layers (transition layers), the model intensifies task-specific information, and in higher layers (refinement layers), it continues refining representations toward specific tasks.
- **Core assumption**: The representational changes induced by instruction tuning are hierarchical and layer-specific, with different layers serving distinct functional purposes.
- **Evidence anchors**:
  - [abstract]: "We find that pre-trained LLMs already encode some tasks well, while others benefit significantly from instruction tuning. The instruction-tuned model exhibits three distinct functional layer groups: shared layers (1-9) that learn general representations across tasks, transition layers (10-15) that intensify task-specific information, and refinement layers (16-32) that continue refining representations toward specific tasks."
  - [section 4.2]: "In the early layers (1 to 9), we observe that for many tasks, the CKA scores are lower for Llama 2-SFT compared to Llama 2, indicating that Llama 2-SFT representations diverge from those of the control models, which were fine-tuned on individual tasks (thus specializing in them)."
  - [corpus]: Weak - related papers discuss layer scaling and task arithmetic but don't directly address the hierarchical transformation mechanism described here.
- **Break condition**: If CKA similarities between instruction-tuned and control models remain consistently high across all layers, indicating no hierarchical transformation occurred.

### Mechanism 2
- **Claim**: Instruction-tuned models generalize better to unseen tasks due to their ability to produce more general lower-layer representations while maintaining task-specific information in deeper layers.
- **Mechanism**: The shared representations learned in lower layers of instruction-tuned models are more general and transferable, allowing the model to perform well on unseen tasks. The task-specific information in deeper layers provides the necessary specialization for task-specific performance.
- **Core assumption**: General representations in lower layers are beneficial for generalization to unseen tasks, while task-specific representations in deeper layers are necessary for task-specific performance.
- **Evidence anchors**:
  - [abstract]: "Our findings also reveal that instruction-tuned models generalize better to unseen tasks due to their ability to produce more general lower-layer representations while maintaining task-specific information in deeper layers."
  - [section 4.5]: "However, as we move to the middle and higher layers responsible for encoding more specialized, task-specific knowledge, the Llama 2-SFT model began matching and ultimately surpassing the CKA similarities of the Llama 2 model."
  - [corpus]: Weak - related papers discuss multi-task learning and task-specific neurons but don't directly address the generalization mechanism to unseen tasks described here.
- **Break condition**: If instruction-tuned models perform worse than pre-trained models on unseen tasks, indicating the general representations are not beneficial for generalization.

### Mechanism 3
- **Claim**: Tasks requiring specialized, structured, or domain-specific knowledge benefit more from instruction tuning than tasks relying on general linguistic and semantic understanding.
- **Mechanism**: Instruction tuning adapts the model's internal structure to meet the specific demands of tasks requiring specialized knowledge, such as structured data to text generation and translation, while tasks relying on general understanding are already well-encoded in pre-trained models.
- **Core assumption**: The pre-training objective of next token prediction is insufficient for encoding tasks requiring specialized knowledge, necessitating instruction tuning for these tasks.
- **Evidence anchors**:
  - [section 4.1]: "Conversely, for clusters like coreference resolution, reading comprehension, structured data to text generation, summarization, and translation, which require specialized, structured, or domain-specific knowledge involving complex transformations or extended context management, the CKA similarities are low, suggesting that next token prediction at pre-training is insufficient for encoding these tasks."
  - [section 4.2]: "For tasks that are not well encoded in the pre-trained Llama 2 (e.g., structured data to text generation, translation), the CKA similarities from the instruction-tuned Llama 2-SFT remained high throughout all transition and refinement layers."
  - [corpus]: Weak - related papers discuss task-specific neurons and multi-task learning but don't directly address the differential benefits of instruction tuning for different task types.
- **Break condition**: If all tasks benefit equally from instruction tuning, indicating no differential effect based on task type.

## Foundational Learning

- **Concept**: Center Kernel Alignment (CKA)
  - **Why needed here**: CKA is used to measure the similarity between representations in different layers of pre-trained and instruction-tuned models, providing a robust measure of similarity that is insensitive to scaling and centering.
  - **Quick check question**: How does CKA differ from other similarity metrics like SVCCA, and why is it more suitable for this analysis?

- **Concept**: Multi-task learning (MTL)
  - **Why needed here**: The study investigates how instruction tuning affects the representations learned by LLMs across multiple tasks, which is a form of multi-task learning.
  - **Quick check question**: What are the key differences between instruction tuning and traditional multi-task learning approaches?

- **Concept**: Task clusters
  - **Why needed here**: The Flan dataset organizes tasks into task clusters, allowing the analysis to investigate how different types of tasks are affected by instruction tuning.
  - **Quick check question**: How are tasks grouped into clusters, and what are the implications of this grouping for the analysis?

## Architecture Onboarding

- **Component map**: Llama 2-7B model -> LoRA fine-tuning -> Control models -> MOSSA framework -> CKA metric -> Functional layer analysis

- **Critical path**:
  1. Train control models on individual tasks
  2. Train instruction-tuned model on multiple tasks
  3. Extract representations from both models at different layers
  4. Compute CKA similarities between representations
  5. Analyze CKA similarities to identify functional layer groups

- **Design tradeoffs**:
  - Using LoRA for instruction tuning vs. full fine-tuning (computational efficiency vs. potential representational changes)
  - Focusing on Llama 2 model vs. exploring multiple models (control over experimental conditions vs. generalizability of findings)
  - Using CKA vs. other similarity metrics (robustness to scaling and centering vs. potential limitations in certain scenarios)

- **Failure signatures**:
  - Consistently high CKA similarities across all layers between pre-trained and instruction-tuned models (indicating no transformational effect of instruction tuning)
  - No clear distinction between functional layer groups in instruction-tuned models (indicating lack of hierarchical representational changes)
  - Poor generalization of instruction-tuned models to unseen tasks (indicating general representations are not beneficial for transfer)

- **First 3 experiments**:
  1. Train control models on a subset of tasks and compute CKA similarities with pre-trained model to establish baseline representational differences
  2. Train instruction-tuned model on the same subset of tasks and compute CKA similarities to identify functional layer groups
  3. Evaluate instruction-tuned model on unseen tasks to assess generalization capabilities

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the layer-by-layer functional mapping (shared, transition, refinement) discovered in Llama 2-SFT generalize to other LLM architectures like BERT or GPT-3? The study only examined Llama 2-7B due to computational constraints, limiting generalizability across architectures. Systematic CKA analysis across multiple LLM architectures would provide a more comprehensive understanding.

- **Open Question 2**: Does instruction tuning create a universal task-agnostic shared representation layer that works across domains, or are these representations domain-specific? The study only tested NLP tasks and did not explore whether the shared representations would transfer to non-NLP domains like code or multimodal tasks. Testing instruction-tuned models on cross-domain tasks would measure performance using the shared layer representations.

- **Open Question 3**: What is the precise mechanism by which instruction tuning enables better generalization to unseen tasks compared to pre-trained models? While the paper observed improved generalization, it did not identify the specific architectural or representational changes responsible. Detailed ablation studies comparing attention patterns, residual connections, and activation distributions would identify the responsible mechanisms.

## Limitations

- Findings are primarily based on Llama 2-7B and may not generalize to larger or smaller models
- Analysis focuses on representation similarities rather than actual task performance, which could reveal different patterns
- CKA metric may not capture all aspects of representational differences, particularly for complex transformations

## Confidence

- **High Confidence**: The identification of three distinct functional layer groups in instruction-tuned models and the differential benefits of instruction tuning for different task types.
- **Medium Confidence**: The generalization mechanism to unseen tasks and the hierarchical transformation mechanism, as these rely on correlational evidence and could benefit from additional ablation studies.
- **Low Confidence**: The precise boundaries between functional layer groups may vary depending on the model size, architecture, or instruction tuning approach used.

## Next Checks

1. Conduct ablation studies varying the instruction tuning strategy (e.g., different mixing rates, template qualities) to assess their impact on the identified functional layer groups and generalization capabilities.
2. Extend the analysis to other model architectures (e.g., OPT, GPT models) and sizes to evaluate the generalizability of the findings.
3. Perform direct task performance evaluations on both seen and unseen tasks to validate the representational findings and assess the practical implications of the identified layer-wise mechanisms.