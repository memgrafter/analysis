---
ver: rpa2
title: 'Deeploy: Enabling Energy-Efficient Deployment of Small Language Models On
  Heterogeneous Microcontrollers'
arxiv_id: '2408.04413'
source_url: https://arxiv.org/abs/2408.04413
tags:
- memory
- deeploy
- inference
- deployment
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Deeploy, a novel DNN compiler designed to
  enable energy-efficient deployment of Small Language Models (SLMs) on heterogeneous
  microcontrollers. The compiler addresses the challenge of end-to-end SLM deployment
  on memory-constrained MCU-class devices without external memory access.
---

# Deeploy: Enabling Energy-Efficient Deployment of Small Language Models On Heterogeneous Microcontrollers

## Quick Facts
- arXiv ID: 2408.04413
- Source URL: https://arxiv.org/abs/2408.04413
- Reference count: 40
- Key result: First end-to-end deployment of TinyStories-class SLM on MCU without external memory, achieving 490 µJ/token and 340 tokens/second

## Executive Summary
This paper introduces Deeploy, a DNN compiler that enables energy-efficient deployment of Small Language Models on memory-constrained heterogeneous microcontrollers. The compiler addresses the challenge of end-to-end SLM deployment on MCU-class devices without external memory access through a novel bottom-up compilation approach. Deeploy generates optimized C code that fully exploits heterogeneous compute resources, including multicore RISC-V cores with ML instruction extensions and a neural processing unit (NPU). When applied to deploy a TinyStories-class SLM on the Siracusa MCU, Deeploy achieves leading-edge energy efficiency of 490 µJ per token and throughput of 340 tokens per second.

## Method Summary
Deeploy employs a bottom-up compilation approach where user-provided C kernels are composed and extended with code generation passes for tiling and memory allocation. The compiler targets heterogeneous microcontroller architectures with multicore RISC-V cores and neural processing units. A novel algorithm co-optimizes tiling and static memory allocation using constraint programming to solve the 2D bin packing problem for multi-level software-managed caches. The approach leverages existing optimized kernel libraries and supports easy integration of novel operators through kernel composition rather than top-down lowering.

## Key Results
- First successful end-to-end deployment of TinyStories-class SLM (125M parameters) on MCU without external memory
- Achieves energy efficiency of 490 µJ per token and throughput of 340 tokens per second
- Demonstrates 25× speedup for linear layers using NPU compared to baseline
- Shows effective utilization of heterogeneous compute resources through kernel fusion and memory optimization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Bottom-up compilation approach enables reuse of hand-optimized kernel libraries and efficient integration of custom operators
- **Mechanism:** Deeploy composes user-provided C kernels and extends them with code generation passes for tiling and memory allocation, supporting reuse of existing optimized kernels
- **Core assumption:** Expert-optimized kernel libraries are available for most ISAs and accelerators and can be composed to implement complex networks
- **Evidence anchors:** Abstract and section III describe the bottom-up approach using user-provided C kernels with code generation passes
- **Break condition:** If expert-optimized kernel libraries are not available or cannot be composed, the approach loses its advantage

### Mechanism 2
- **Claim:** Novel algorithm co-optimizes tiling and static memory allocation, addressing complex buffer lifetime requirements in Transformers
- **Mechanism:** Deeploy uses constraint programming to solve a 2D bin packing problem that couples tiling constraints with memory allocation, considering tensor lifetimes across inference rounds
- **Core assumption:** Constraint programming formulation can efficiently find solutions satisfying both tiling and memory allocation constraints for complex Transformers
- **Evidence anchors:** Abstract and section III describe the constraint programming approach for solving tiling and memory allocation problems
- **Break condition:** If the constraint programming problem becomes too large or complex to solve efficiently, or if buffer lifetime assumptions are violated

### Mechanism 3
- **Claim:** Integration with Siracusa's heterogeneous hardware achieves leading-edge energy efficiency and throughput
- **Mechanism:** Deeploy leverages NPU's mixed-weight-precision bit-serial datapath for linear layers and compute cluster's SIMD capabilities while managing memory transfers and tiling
- **Core assumption:** Hardware platform has necessary compute resources and memory hierarchy to support efficient SLM deployment, and Deeploy can effectively orchestrate their use
- **Evidence anchors:** Abstract and section V describe Deeploy's integration with Siracusa's heterogeneous architecture including NPU and compute cluster
- **Break condition:** If hardware platform lacks necessary resources or memory hierarchy is not properly managed

## Foundational Learning

- **Concept:** Deep Neural Network (DNN) compilation and optimization
  - **Why needed here:** Understanding how DNNs are compiled and optimized for deployment on resource-constrained devices is crucial for grasping Deeploy's approach
  - **Quick check question:** What are the key challenges in compiling DNNs for deployment on microcontrollers with limited memory and compute resources?

- **Concept:** Transformer models and their computational requirements
  - **Why needed here:** Deeploy targets deployment of Small Language Models based on Transformer architectures; understanding their computational requirements is essential for evaluating effectiveness
  - **Quick check question:** What are the main computational bottlenecks in Transformer models, and how do they impact deployment on resource-constrained devices?

- **Concept:** Hardware-aware compilation and optimization
  - **Why needed here:** Deeploy leverages specific features of the target hardware platform to achieve high performance; understanding hardware-aware compilation is key to understanding the approach
  - **Quick check question:** How can compiler optimizations be tailored to exploit the specific features of a heterogeneous hardware platform like Siracusa?

## Architecture Onboarding

- **Component map:** Frontend -> Graph transformation, operator mapping, kernel selection; Midend -> Tiling & Memory Scheduling, Memory Level Annotation; Backend -> Code generation, closure generation, tiling code generation; Deployment Platform -> Hardware abstraction, kernel templates, code generation passes

- **Critical path:** Frontend -> Midend -> Backend; Midend's Tiling & Memory Scheduling step is critical for performance as it directly impacts memory usage and data movement

- **Design tradeoffs:** Compilation time vs. runtime performance (co-optimization algorithm may increase compilation time but improve runtime performance); Hardware utilization vs. code complexity (offloading to specialized hardware improves performance but may increase code complexity)

- **Failure signatures:** Memory allocation failures (if static memory allocation algorithm fails to find valid solution, deployment will fail); Performance degradation (if tiling and memory scheduling optimizations are not effective, performance may be suboptimal)

- **First 3 experiments:** 1) Deploy simple CNN on Siracusa using Deeploy and measure performance; 2) Vary sequence length in SLM and measure impact on performance; 3) Compare performance of Deeploy-generated code with other compilers (e.g., Dory) on same hardware platform

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum model size (in parameters) that can be deployed on Siracusa MCU using Deeploy without external memory access?
- Basis in paper: [explicit] Paper demonstrates deployment of TinyStories-class SLM with 125M parameters but doesn't specify theoretical maximum
- Why unresolved: Paper focuses on specific model size as proof-of-concept rather than exploring deployment framework limits
- What evidence would resolve it: Systematic scaling experiments varying model size and measuring when performance or memory constraints become prohibitive

### Open Question 2
- Question: How does Deeploy's performance scale when deploying models with encoder-decoder architectures (like BART or T5) compared to decoder-only models?
- Basis in paper: [inferred] Paper focuses exclusively on decoder-only Transformers but bottom-up compilation approach suggests it could handle encoder-decoder models
- Why unresolved: Paper doesn't evaluate Deeploy on encoder-decoder architectures, leaving open questions about applicability to other Transformer variants
- What evidence would resolve it: Benchmarking Deeploy's performance and memory usage when deploying encoder-decoder models with varying numbers of encoder and decoder layers

### Open Question 3
- Question: What is the energy efficiency trade-off between using NPU versus octa-core cluster for different types of Transformer operations?
- Basis in paper: [explicit] Paper shows NPU provides 25× speedup for linear layers but only 19% for autoregressive mode, suggesting operation-specific efficiency differences
- Why unresolved: Paper provides aggregate performance numbers but doesn't systematically analyze which operations benefit most from NPU acceleration
- What evidence would resolve it: Detailed per-operator energy and latency measurements comparing NPU versus cluster execution across various Transformer architectures and sequence lengths

## Limitations
- Evaluation focuses exclusively on single model architecture (TinyStories) and hardware platform (Siracusa MCU), limiting generalizability
- Lacks detailed ablation studies on individual optimization techniques to isolate specific contributions
- Energy measurements reported only for end-to-end inference without breakdown of individual component costs
- Static memory allocation approach may struggle with more complex model architectures or larger sequence lengths

## Confidence

- **High confidence**: Bottom-up compilation approach and use of heterogeneous hardware resources (NPU, RISC-V cluster) are well-established techniques with clear implementation details and empirical validation
- **Medium confidence**: Effectiveness of co-optimization algorithm for tiling and memory allocation is supported by reported results but lacks comprehensive ablation studies or comparison with alternatives
- **Low confidence**: Generalizability of reported energy efficiency and throughput metrics to other SLM architectures, sequence lengths, or heterogeneous MCU platforms remains uncertain due to limited experimental scope

## Next Checks

1. **Ablation study on optimization components**: Measure performance and energy consumption when disabling individual optimization passes (tiling, memory scheduling, kernel fusion) to quantify their individual contributions to reported results

2. **Cross-platform validation**: Deploy same TinyStories model using Deeploy on different heterogeneous MCU platform (e.g., Arm Cortex-M with CMSIS-NN) to assess portability and identify platform-specific bottlenecks

3. **Memory pressure analysis**: Systematically vary sequence length and model size to identify breaking point where static memory allocation fails, and evaluate impact on compilation time and runtime performance