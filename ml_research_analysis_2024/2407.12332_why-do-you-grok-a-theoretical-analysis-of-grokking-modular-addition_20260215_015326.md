---
ver: rpa2
title: Why Do You Grok? A Theoretical Analysis of Grokking Modular Addition
arxiv_id: '2407.12332'
source_url: https://arxiv.org/abs/2407.12332
tags:
- kernel
- training
- where
- loss
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present a theoretical explanation for grokking in modular addition,
  where models generalize long after overfitting. Our analysis shows that early in
  training, when networks behave like kernel methods, they require nearly all possible
  data points to generalize due to inherent symmetries in the problem.
---

# Why Do You Grok? A Theoretical Analysis of Grokking Modular Addition

## Quick Facts
- **arXiv ID**: 2407.12332
- **Source URL**: https://arxiv.org/abs/2407.12332
- **Reference count**: 40
- **Primary result**: Theoretical explanation for grokking in modular addition showing early kernel regime overfitting transitions to generalization via ℓ∞ regularization

## Executive Summary
This paper provides a theoretical explanation for grokking in modular addition tasks, where neural networks generalize long after overfitting. The authors show that early in training, networks behave like kernel methods and cannot generalize due to inherent symmetries in the problem. However, with tiny ℓ∞ regularization (approximating AdamW's implicit bias), networks escape the kernel regime and learn generalizing features. The analysis proves that while kernel methods require nearly all data points to generalize, ℓ∞-regularized networks can generalize with far fewer samples (O(1/p) for regression, O(1/p^{1/3}) for classification).

## Method Summary
The paper analyzes a two-layer neural network with quadratic activations trained on modular addition tasks. For regression, the network takes triples (a,b,c) and outputs a scalar indicating if a+b≡c (mod p). For classification, it takes pairs (a,b) and outputs a p-way classifier for c. The training uses gradient descent with tiny ℓ∞ regularization (10^{-4} for regression, 10^{-20} for classification) to approximate AdamW's implicit bias. The analysis combines empirical NTK calculations, Rademacher complexity bounds, and PAC-Bayesian frameworks to prove generalization guarantees for networks that achieve zero training loss while maintaining bounded ℓ∞ norm.

## Key Results
- Early in training, kernel methods require nearly all possible data points to generalize on modular addition due to permutation symmetries
- Tiny ℓ∞ regularization causes networks to leave the kernel regime and learn generalizing features
- Networks with bounded ℓ∞ norm can generalize with O(1/p) samples for regression and O(1/p^{1/3}) samples for classification
- Gradient descent with appropriate ℓ∞ regularization can find these generalizing solutions despite initial overfitting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early in training, neural networks behave like kernel methods which cannot generalize on modular addition without seeing nearly all data points
- Mechanism: Networks in the kernel regime approximate linear combinations of fixed feature maps determined by the initial parameters. The modular addition task has inherent permutation symmetries that make it impossible for any permutation-equivariant kernel method to generalize without access to a constant fraction of all possible data points
- Core assumption: The neural tangent kernel (NTK) remains approximately constant during early training, making the network behave like a kernel method
- Evidence anchors:
  - [abstract] "early in training, when the 'kernel regime' approximately holds, no permutation-equivariant model can achieve small population error on modular addition unless it sees at least a constant fraction of all possible data points"
  - [section 3.1] "kernel regression with the empirical neural tangent kernel of our quadratic network achieves zero training loss if the network is mildly wide"
  - [corpus] Weak evidence - corpus papers don't directly address kernel regime limitations
- Break condition: When the NTK changes significantly (indicated by large changes in ∥Θt - Θ0∥F), the network leaves the kernel regime and begins learning features

### Mechanism 2
- Claim: Weak ℓ∞ regularization causes networks to leave the kernel regime and learn generalizing features
- Mechanism: ℓ∞ regularization, which approximates AdamW's implicit bias, constrains parameter growth and forces the network to escape the kernel regime where it can then learn representations that generalize with far fewer samples (O(1/p) for regression, O(1/p^(1/3)) for classification)
- Core assumption: Small ℓ∞ regularization effectively implements the implicit bias of AdamW, which Zhang et al. (2024) show converges to max-margin solutions with respect to ℓ∞ norm
- Evidence anchors:
  - [abstract] "we prove that with appropriate regularization (ℓ∞ norm bounds), networks can generalize with far fewer samples"
  - [section 3.2] "networks with small ℓ∞ norm in the regression setting (Section 3.2) and large ℓ∞-normalized margin in the classification setting (Section 4.2) can both provably generalize"
  - [corpus] Weak evidence - corpus papers don't discuss ℓ∞ regularization as a mechanism for leaving kernel regime
- Break condition: When ℓ∞ regularization becomes too strong, it prevents the network from fitting the training data at all

### Mechanism 3
- Claim: Networks that achieve zero training loss with bounded ℓ∞ norm will generalize well
- Mechanism: The Rademacher complexity and PAC-Bayesian frameworks show that networks with bounded ℓ∞ norm have controlled generalization error. The authors prove such networks exist and gradient descent with small ℓ∞ regularization can find them
- Core assumption: The network can achieve zero training loss while maintaining small ℓ∞ norm
- Evidence anchors:
  - [section 3.2] "we prove that two-layer quadratic networks that achieve zero training loss with bounded ℓ∞ norm generalize well with substantially fewer training points"
  - [section 4.2] "networks with bounded ℓ∞ parameter norm can both provably generalize with far fewer samples"
  - [corpus] Weak evidence - corpus papers don't discuss Rademacher complexity bounds for modular addition
- Break condition: When the network width is insufficient (h < 8p for regression) or when the problem dimension p becomes too large relative to available data

## Foundational Learning

- Concept: Permutation-equivariance and its impact on generalization
  - Why needed here: The modular addition task is invariant under permutations of inputs and outputs, which creates fundamental limitations for any learning algorithm that respects this symmetry
  - Quick check question: If you permute the inputs to a modular addition problem, will the same network weights work for the permuted problem?

- Concept: Neural tangent kernel and lazy training regime
  - Why needed here: Understanding when networks behave like kernel methods is crucial for explaining why they initially overfit without generalizing
  - Quick check question: What happens to the neural tangent kernel as training progresses - does it stay constant or change significantly?

- Concept: Implicit regularization and its relationship to optimization dynamics
  - Why needed here: The transition from kernel to rich regime is driven by implicit regularization effects, particularly ℓ∞ regularization approximating AdamW's behavior
  - Quick check question: How does ℓ∞ regularization differ from L2 regularization in terms of its effect on parameter norms?

## Architecture Onboarding

- Component map:
  - Input: Pairs (a,b) for classification or triples (a,b,c) for regression, represented as one-hot vectors
  - Hidden layer: Quadratic activation applied element-wise after linear transformation
  - Output: For classification, p-dimensional vector; for regression, scalar (or p-dimensional for the construction)
  - Loss: Cross-entropy for classification, squared loss for regression
  - Regularization: Tiny ℓ∞ norm constraint to approximate AdamW's implicit bias

- Critical path: Data → One-hot encoding → Linear transformation → Quadratic activation → Linear transformation → Loss computation → Gradient descent with ℓ∞ regularization

- Design tradeoffs:
  - Width vs. generalization: Minimum width of 8p required for theoretical guarantees
  - Initialization scale: Smaller scales delay kernel regime departure but slow training
  - Regularization strength: Must be tiny enough to allow fitting but large enough to constrain ℓ∞ norm

- Failure signatures:
  - No generalization despite zero training loss: Network stuck in kernel regime
  - Very slow convergence: Initialization scale too small
  - Oscillating training: Regularization strength too large
  - Perfect training but poor test: Insufficient width or too few training samples

- First 3 experiments:
  1. Train with varying initialization scales (α = 10, 1, 0.1, 0.01) and observe gap between train/test accuracy
  2. Compare performance with and without tiny ℓ∞ regularization (strength = 1e-4 for regression, 1e-20 for classification)
  3. Vary network width (h = 4p, 8p, 16p) and training set size (n = p^2, 2p^2, 4p^2) to find sample complexity threshold

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we prove a classification error lower bound for permutation-equivariant kernel methods, analogous to the ℓ² loss lower bounds we established?
- Basis in paper: [explicit] The paper states "It remains open to prove a classification error lower bound for permutation-equivariant kernel methods" in Section 4.1.
- Why unresolved: The proof technique used for ℓ² loss lower bounds (based on showing kernel methods cannot approximate the modular addition function well) doesn't directly translate to 0-1 classification error. High ℓ² loss doesn't necessarily imply high classification error.
- What evidence would resolve it: A formal proof showing that permutation-equivariant kernel methods require Ω(p²) training samples to achieve small classification error on modular addition, or a counterexample demonstrating they can achieve good classification accuracy with fewer samples.

### Open Question 2
- Question: What training techniques could enable faster generalization on modular addition, beyond the initialization scale modification explored in the paper?
- Basis in paper: [explicit] The paper states in Section 6 "we only study the cause of grokking in these settings, but do not analyze possible training techniques to enable quick generalization on this task" and notes that while changing initialization scale eliminates grokking, it "actually slows down the time to final generalization."
- Why unresolved: The paper focuses on explaining why grokking occurs (transition from kernel to rich regime) but doesn't explore how to prevent or accelerate it.
- What evidence would resolve it: Empirical results showing a training method (e.g., specific curriculum, regularization scheme, or optimization algorithm) that achieves good generalization on modular addition with substantially fewer total training steps than standard gradient descent.

### Open Question 3
- Question: Does the kernel regime overfitting phenomenon generalize to other algorithmic tasks beyond modular addition, or is it specific to this problem's symmetries?
- Basis in paper: [inferred] The paper extensively analyzes grokking in modular addition but only briefly mentions other tasks (sparse parities, image classifiers, GCD, matrix completion) in the introduction without theoretical analysis.
- Why unresolved: The theoretical framework developed is specific to modular addition's permutation symmetries. It's unclear whether other algorithmic tasks share similar properties that cause kernel regime overfitting.
- What evidence would resolve it: Either a theoretical framework extending the permutation-equivariance analysis to other algorithmic tasks, or empirical studies showing kernel regime overfitting on multiple distinct algorithmic problems.

## Limitations
- Theoretical bounds rely on idealized assumptions about network width and regularization strength that may not hold in practice
- Analysis focuses on quadratic networks which may not capture the behavior of deeper architectures where grokking is typically observed
- Kernel regime analysis assumes perfect permutation-equivariance, but real networks may have slight asymmetries that affect generalization

## Confidence
- **High Confidence**: The impossibility result for kernel methods on modular addition (Mechanism 1) is rigorously proven and well-supported
- **Medium Confidence**: The theoretical generalization bounds for ℓ∞-regularized networks (Mechanism 2) are mathematically sound but depend on idealized assumptions about network width and data distribution
- **Medium Confidence**: The empirical demonstration that tiny ℓ∞ regularization causes networks to leave the kernel regime and generalize (Mechanism 3) is supported by experiments but the connection to AdamW's implicit bias remains somewhat indirect

## Next Checks
1. Test alternative regularization schemes: Verify that ℓ∞ regularization specifically, rather than any regularization, is necessary for escaping the kernel regime by comparing with L2 regularization and other norms
2. Probe deeper architectures: Extend the theoretical analysis to deeper networks (3+ layers) to determine if the same kernel-to-rich regime transition explains grokking in more realistic settings
3. Validate width requirements empirically: Test whether the theoretical width lower bound (h ≥ 8p) is tight by systematically varying width and measuring the impact on sample complexity and generalization timing