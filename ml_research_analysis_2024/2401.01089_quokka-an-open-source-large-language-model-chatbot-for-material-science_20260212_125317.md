---
ver: rpa2
title: 'Quokka: An Open-source Large Language Model ChatBot for Material Science'
arxiv_id: '2401.01089'
source_url: https://arxiv.org/abs/2401.01089
tags:
- arxiv
- materials
- science
- language
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Quokka, a family of open-source large language
  models (LLMs) specialized for materials science. The models are built by continuing
  pre-training Llama-2 models on over one million materials science academic articles
  from the S2ORC dataset, followed by instruction tuning.
---

# Quokka: An Open-source Large Language Model ChatBot for Material Science

## Quick Facts
- arXiv ID: 2401.01089
- Source URL: https://arxiv.org/abs/2401.01089
- Reference count: 9
- Primary result: Open-source LLMs specialized for materials science through continuing pre-training on S2ORC corpus

## Executive Summary
This paper introduces Quokka, a family of open-source large language models specialized for materials science. The models are created by continuing pre-training Llama-2 models on over one million materials science academic articles from the S2ORC dataset, followed by instruction tuning. The resulting models include both base versions for text processing tasks and chat variants optimized for dialogue in answering materials science questions. The authors report training losses and perplexity scores demonstrating the effectiveness of their approach, and showcase the models' capabilities through case studies including general question answering, safety refusals, and research article summarization.

## Method Summary
The Quokka models are developed through a two-stage process: first, continuing pre-training Llama-2 models on the S2ORC materials science corpus (mixed with 10% general domain data to prevent catastrophic forgetting), then instruction tuning on a combination of general and materials science-specific instructions. The models come in two sizes (7B and 13B parameters) and two variants (base models and chat-optimized versions). Training is performed on A100 GPUs, with pre-training taking 25-56 hours and instruction tuning requiring 4.5-8 hours depending on model size.

## Key Results
- Quokka models achieve improved materials science understanding through domain-specific pre-training on S2ORC corpus
- Instruction tuning enables the models to effectively respond to materials science queries and follow domain-specific instructions
- Case studies demonstrate capabilities in answering general questions, refusing harmful requests, and summarizing research articles
- The models show reduced perplexity scores compared to baseline Llama-2 models on materials science text

## Why This Works (Mechanism)

### Mechanism 1
Continuing pre-training on domain-specific corpus injects materials science knowledge into the model. The model starts with a general-purpose LLM (Llama-2) and further trains it on over one million materials science academic articles from the S2ORC dataset, allowing the model to learn specialized vocabulary, concepts, and reasoning patterns relevant to materials science.

### Mechanism 2
Instruction tuning on materials science instructions enables the model to follow domain-specific user intents. After continuing pre-training, the model is fine-tuned on a combination of general instructions and materials science-specific instructions, teaching it to interpret and respond to domain-relevant queries.

### Mechanism 3
Mixing materials science corpus with general domain data prevents catastrophic forgetting of foundational knowledge. The training pipeline mixes 10% of general RedPajama-Data-1T-Sample dataset with the materials science corpus during pre-training, helping the model retain general reasoning and language capabilities while acquiring domain expertise.

## Foundational Learning

- Concept: Continuing pre-training vs. fine-tuning
  - Why needed here: Continuing pre-training adapts the model to a new domain while preserving general capabilities, whereas fine-tuning typically focuses on specific tasks
  - Quick check question: What is the difference between continuing pre-training and fine-tuning, and why did the authors choose continuing pre-training for this materials science application?

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding how to prevent the model from losing general knowledge while acquiring domain-specific expertise
  - Quick check question: What strategies can prevent catastrophic forgetting when adapting a general-purpose LLM to a specialized domain?

- Concept: Instruction tuning methodology
  - Why needed here: Understanding how to teach the model to follow user instructions and format responses appropriately
  - Quick check question: How does instruction tuning differ from traditional supervised fine-tuning, and what are its key benefits for building chatbots?

## Architecture Onboarding

- Component map: Llama-2 base model → Continuing pre-training on S2ORC materials science corpus → Instruction tuning on materials science instructions → Quokka models (7B/13B and 7B-Chat/13B-Chat variants)
- Critical path: Data preparation → Continuing pre-training (8 A100 GPUs, 25-56 hours) → Instruction tuning (4 A100 GPUs, 4.5-8 hours) → Model evaluation and deployment
- Design tradeoffs: Larger models (13B) achieve lower perplexity but require more computational resources; mixing general and domain data prevents forgetting but adds complexity; using open-source datasets enables reproducibility but may limit proprietary advantages
- Failure signatures: High perplexity during pre-training (model not learning domain knowledge), instruction tuning loss not converging (poor instruction quality or model capacity issues), safety failures (model providing harmful information)
- First 3 experiments:
  1. Run perplexity calculation on held-out validation set after pre-training to verify domain knowledge acquisition
  2. Test zero-shot response quality on sample materials science queries to evaluate instruction tuning effectiveness
  3. Verify safety responses by testing with potentially harmful queries to ensure proper refusal mechanisms

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Quokka models compare to other specialized LLMs in materials science tasks? The paper does not provide direct comparisons with other specialized LLMs in materials science, focusing instead on introducing Quokka and demonstrating its capabilities through case studies.

### Open Question 2
What is the impact of the instruction-tuning dataset size and diversity on the performance of Quokka models? The paper does not discuss how different sizes and diversities of instruction-tuning datasets affect the performance of Quokka models.

### Open Question 3
How does the Quokka model handle out-of-domain queries or queries that require knowledge outside of materials science? The paper does not provide information on the model's performance or behavior when faced with out-of-domain queries.

## Limitations

- Evaluation relies heavily on qualitative case studies rather than comprehensive quantitative benchmarks
- Safety evaluation is superficial, consisting of only one example where the model refuses to provide a drug synthesis recipe
- No discussion of potential biases in the S2ORC corpus or how these might affect the model's outputs

## Confidence

**High Confidence**: The core technical approach of continuing pre-training followed by instruction tuning is well-established in the literature, and the authors clearly describe their methodology. The reported training losses and perplexity scores show the models successfully learned from the materials science corpus.

**Medium Confidence**: The claims about the model's materials science capabilities are supported by case studies, but the lack of comprehensive benchmarking on established materials science datasets makes it difficult to assess true