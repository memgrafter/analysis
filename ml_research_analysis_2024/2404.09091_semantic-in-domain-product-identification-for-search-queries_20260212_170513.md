---
ver: rpa2
title: Semantic In-Domain Product Identification for Search Queries
arxiv_id: '2404.09091'
source_url: https://arxiv.org/abs/2404.09091
tags:
- product
- adobe
- queries
- products
- cards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurately identifying Adobe
  products in user search queries, which is critical for enhancing user experience
  on Adobe's platforms. The authors propose a novel approach using a domain-specific
  language model (DeBERTa v3) pretrained on Adobe HelpX documents to understand Adobe
  product vocabulary and features.
---

# Semantic In-Domain Product Identification for Search Queries

## Quick Facts
- arXiv ID: 2404.09091
- Source URL: https://arxiv.org/abs/2404.09091
- Authors: Sanat Sharma; Jayant Kumar; Twisha Naik; Zhaoyu Lu; Arvind Srikantan; Tracy Holloway King
- Reference count: 11
- Key outcome: 0.949 F1 score on explicit product queries, 93.1% accuracy on implicit product queries, >25% relative CTR improvement in production

## Executive Summary
This paper addresses the challenge of accurately identifying Adobe products in user search queries, which is critical for enhancing user experience on Adobe's platforms. The authors propose a novel approach using a domain-specific language model (DeBERTa v3) pretrained on Adobe HelpX documents to understand Adobe product vocabulary and features. They then train a multi-label classifier on top of this LM using four datasets: Adobe HelpX behavioral data, HelpX document titles, product NER explicit data, and Adobe Express queries. The model achieves strong performance metrics and demonstrates significant production impact, including a 2x increase in app cards surfaced and >50% decrease in null rate.

## Method Summary
The authors pretrain a DeBERTa v3 language model on Adobe HelpX documents to capture domain-specific vocabulary and product features. They then train a multi-label classifier with a 2-layer MLP head on top of the pretrained model using Weighted Binary Cross Entropy loss. The training data combines four sources: Adobe HelpX behavioral logs, HelpX document titles, product NER explicit data, and Adobe Express queries. The model is evaluated using both quantitative metrics (precision, recall, F1) and qualitative manual annotation, and has been deployed in production with significant improvements to click-through rate and app card surfacing.

## Key Results
- Achieved F1 score of 0.949 on explicit product queries
- 93.1% accuracy on implicit product queries in qualitative evaluation
- >25% relative improvement in click-through rate across deployed surfaces
- 2x increase in app cards surfaced and >50% decrease in null rate in production

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The DeBERTa v3 language model pretrained on Adobe HelpX documents captures Adobe-specific product vocabulary and features, enabling better query understanding than general-purpose models.
- Mechanism: By pretraining on domain-specific HelpX documents, the LM learns embeddings for Adobe product features (e.g., "crop," "generative fill") and disambiguates similar product names (e.g., Premiere Pro vs. Premiere Rush).
- Core assumption: Adobe HelpX documents contain sufficient domain-specific vocabulary and product usage patterns to train an effective LM.
- Evidence anchors:
  - [abstract]: "Our semantic model led to >25% relative improvement in CTR (click through rate) across the deployed surfaces"
  - [section 4.1]: "We found open-source language models (LMs) like BERT [2] to be inadequate for Adobe user queries" and "Pretraining the LM on HelpX data causes a 14% improvement in downstream classification accuracy compared to using a pretrained LM"
  - [corpus]: Weak evidence - corpus does not directly address domain-specific pretraining benefits
- Break condition: If HelpX documents do not cover sufficient product vocabulary or if product naming conventions change significantly, the LM may fail to capture necessary domain knowledge.

### Mechanism 2
- Claim: The multi-label classification approach effectively handles queries that can be associated with multiple products, both explicit and implicit.
- Mechanism: By treating product identification as a multi-label problem, the model can assign probability scores to multiple products for a single query, capturing cases where users may be interested in related products.
- Core assumption: Users' search behavior and query patterns can be effectively modeled as multiple product associations.
- Evidence anchors:
  - [abstract]: "Accurate explicit and implicit product identification in search queries is critical"
  - [section 4.2]: "We use Weighted Binary Cross Entropy loss function for our training and leverage the relevancy weights...to pay more attention to more important examples during training"
  - [section 5.1]: "We compute per-product and per-source metrics...precision and recall are well balanced and result in an F1 score of .949"
- Break condition: If query-product associations become too complex or if user intent patterns shift dramatically, the multi-label approach may become less effective.

### Mechanism 3
- Claim: The combination of behavioral data, curated document titles, and explicit NER data creates a robust training dataset that captures both common and rare product queries.
- Mechanism: By leveraging multiple data sources with different characteristics (noisy behavioral data, high-quality curated data, explicit product mentions), the model learns to handle a wide range of query types and product associations.
- Core assumption: The diverse data sources provide complementary information that improves overall model performance.
- Evidence anchors:
  - [section 3]: Describes the four datasets used and their characteristics
  - [section 5.2]: "We utilized a set of 2700 production CC queries for evaluation...accuracy results for the 2700 queries in the qualitative evaluation" showing 93.1% accuracy on implicit queries
  - [corpus]: Weak evidence - corpus does not directly address the multi-source training approach
- Break condition: If the quality or distribution of any data source changes significantly, it may impact the model's ability to handle certain types of queries.

## Foundational Learning

- Concept: Multi-label classification
  - Why needed here: Product queries can be associated with multiple products, requiring the model to predict multiple labels simultaneously
  - Quick check question: Why can't we use single-label classification for this task?

- Concept: Domain-specific language model pretraining
  - Why needed here: General-purpose LMs lack knowledge of Adobe product vocabulary and features, leading to poor performance on domain-specific queries
  - Quick check question: What are the key differences between domain-specific and general-purpose LMs?

- Concept: Data augmentation and weighting
  - Why needed here: Different data sources have varying quality and importance, requiring appropriate weighting during training
  - Quick check question: How does the log click ratio weighting help improve model performance?

## Architecture Onboarding

- Component map:
  - Data Ingestion -> Pretraining -> Classification -> Evaluation -> Deployment

- Critical path:
  1. Data collection and preprocessing
  2. LM pretraining on HelpX documents
  3. Classifier training with multi-source data
  4. Offline evaluation and tuning
  5. AB testing and production deployment

- Design tradeoffs:
  - Using DeBERTa v3 vs. other LMs: Better performance on domain-specific tasks but potentially higher computational requirements
  - Multi-label vs. multi-class classification: More flexible for handling queries with multiple product associations but potentially more complex to train and evaluate
  - Data source weighting: Balancing between high-quality curated data and large-scale behavioral data

- Failure signatures:
  - Poor performance on new product launches or product name changes
  - Inability to handle complex implicit queries that weren't well-represented in training data
  - Degradation in performance for non-English locales despite English model training

- First 3 experiments:
  1. Compare model performance using different LM backbones (e.g., BERT vs. DeBERTa v3) on a held-out test set
  2. Evaluate the impact of different data source combinations on model performance
  3. Test the model's ability to handle new product queries not present in the training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model perform on non-English queries with implicit product intent, and what is the impact of multilingual training on its accuracy?
- Basis in paper: [explicit] The paper mentions that non-English queries with implicit product intent, such as "images gratuites" (French: 'free pictures'), are associated with the Adobe Stock image marketplace. It also states that future work includes training a multilingual version of the model to better support non-English queries.
- Why unresolved: The current model is trained on English queries, and its performance on non-English queries with implicit product intent is not evaluated or reported.
- What evidence would resolve it: Results from a multilingual evaluation showing accuracy, precision, and recall metrics for non-English queries with implicit product intent.

### Open Question 2
- Question: How does the model handle long user queries, and what improvements are needed for better long prompt understanding in product disambiguation?
- Basis in paper: [explicit] The paper mentions that future work includes experimenting with better long prompt understanding for product disambiguation, particularly for RAG (Retrieval Augmented Generation) based systems dealing with retrieval for long prompts.
- Why unresolved: The current model's performance on long user queries is not evaluated, and the specific challenges or improvements needed for long prompt understanding are not detailed.
- What evidence would resolve it: Evaluation results comparing the model's performance on short vs. long queries, along with insights into the specific challenges and proposed improvements for handling long prompts.

### Open Question 3
- Question: What are the limitations of using behavioral data for training the product classifier, and how does it affect the model's generalization to new or less popular products?
- Basis in paper: [inferred] The paper uses behavioral data from user clicks on HelpX articles and other sources to train the model. However, it does not discuss the limitations of this approach or how it might impact the model's ability to generalize to new or less popular products.
- Why unresolved: The paper does not provide an analysis of the limitations of using behavioral data or its impact on the model's generalization capabilities.
- What evidence would resolve it: A study comparing the model's performance on popular vs. less popular products, along with an analysis of how the training data's distribution affects its generalization.

## Limitations

- The evaluation relies heavily on offline metrics and qualitative annotation rather than comprehensive A/B testing data
- The model's performance on non-English locales is not evaluated despite being deployed across surfaces
- The long-term stability of the deployed model and its performance on emerging product names or features is not discussed

## Confidence

**High Confidence**: The mechanism of using domain-specific pretraining to improve performance on Adobe product queries is well-supported by the 14% improvement in downstream classification accuracy compared to general LMs, and the >25% relative improvement in CTR in production. The multi-label classification approach is appropriate for the task and is validated by strong F1 scores on explicit queries.

**Medium Confidence**: The claim that the combination of four diverse data sources creates a robust training dataset is supported by the model's strong performance on both explicit (F1 0.949) and implicit (93.1% accuracy) queries. However, the specific contribution of each data source to overall performance is not quantified.

**Low Confidence**: The long-term effectiveness and scalability of the model in production, particularly for new product launches and non-English locales, cannot be assessed from the available evidence. The paper does not provide detailed error analysis or failure case studies.

## Next Checks

1. **Generalization Testing**: Evaluate the model's performance on queries containing newly launched Adobe products or products with recently changed names that were not present in the training data. This would validate the model's ability to generalize beyond its training distribution.

2. **Cross-Locale Evaluation**: Test the deployed model on non-English search queries to assess its performance across different locales. This is critical given the claim of deployment across surfaces but only English model training being discussed.

3. **Long-term Stability Analysis**: Conduct a longitudinal study of the model's performance over 6-12 months in production to identify any degradation in accuracy or changes in error patterns, particularly following Adobe product updates or naming changes.