---
ver: rpa2
title: Temporal Cross-Attention for Dynamic Embedding and Tokenization of Multimodal
  Electronic Health Records
arxiv_id: '2403.04012'
source_url: https://arxiv.org/abs/2403.04012
tags:
- time
- clinical
- series
- embedding
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a dynamic embedding and tokenization framework
  for multimodal clinical time series, addressing challenges such as high dimensionality,
  sparsity, multimodality, irregular recording frequencies, and timestamp duplication.
  The framework incorporates flexible positional encoding, learnable time embedding,
  and variable-specific encoding to capture distinct characteristics and relationships
  between temporal variables.
---

# Temporal Cross-Attention for Dynamic Embedding and Tokenization of Multimodal Electronic Health Records

## Quick Facts
- arXiv ID: 2403.04012
- Source URL: https://arxiv.org/abs/2403.04012
- Authors: Yingbo Ma; Suraj Kolla; Dhruv Kaliraman; Victoria Nolan; Zhenhong Hu; Ziyuan Guan; Yuanfang Ren; Brooke Armfield; Tezcan Ozrazgat-Baslanti; Tyler J. Loftus; Parisa Rashidi; Azra Bihorac; Benjamin Shickel
- Reference count: 9
- Primary result: Mean AUROC of 0.801 for predicting nine postoperative complications from multimodal EHR data

## Executive Summary
This paper introduces a dynamic embedding and tokenization framework for multimodal clinical time series that addresses key challenges in EHR data: high dimensionality, sparsity, multimodality, irregular recording frequencies, and timestamp duplication. The framework incorporates flexible positional encoding, learnable time embedding, and variable-specific encoding to capture distinct characteristics and relationships between temporal variables. When integrated into a multitask transformer classifier with sliding window attention, the framework outperformed baseline approaches on predicting nine postoperative complications from multimodal data of more than 120,000 major inpatient surgeries across three hospitals.

## Method Summary
The framework uses variable-specific encoders for each clinical variable, Time2Vec for learnable time embedding, and flexible positional encoding to handle irregular sampling. A cross-attention-based approach fuses structured EHR time series with unstructured clinical notes. The model employs a Longformer with sliding window attention for long sequence processing, outputting predictions for nine postoperative complications. The approach dynamically tokenizes multimodal data while preserving temporal patterns and inter-variable relationships.

## Key Results
- Achieved mean AUROC of 0.801 across nine postoperative complication predictions
- Outperformed baseline approaches on multimodal EHR classification task
- Demonstrated effectiveness across 113,953 patients and 124,777 surgeries from three hospitals

## Why This Works (Mechanism)

### Mechanism 1
Variable-specific encoders preserve distinct temporal dynamics of different clinical variables by assigning separate encoders to each variable type, allowing the model to learn intra-variable temporal patterns without interference. This assumes different clinical variables have fundamentally different statistical distributions and temporal behaviors. The approach addresses the challenge that multivariate clinical time series include different categories of health variables with distinct characteristics and numerical ranges.

### Mechanism 2
Cross-attention effectively fuses structured time series and unstructured clinical notes by learning relevance between modalities through attention scores. This assumes clinical notes contain complementary information to structured data that can be discovered through attention mechanisms. The approach generates enriched feature sequences by searching relevant information between modalities rather than simply concatenating them.

### Mechanism 3
Learnable time embedding (Time2Vec) captures temporal patterns better than positional embeddings alone by generating both periodic and non-periodic representations of time. This assumes the relative timing between measurements carries important clinical information beyond sequence position. Time2Vec encodes relative time between events rather than just sequence order, generating one non-periodic and one periodic time-dependent vector.

## Foundational Learning

- **Concept**: Transformer attention mechanisms
  - **Why needed here**: The paper builds on transformer architectures to handle sequential EHR data, requiring understanding of self-attention and cross-attention operations.
  - **Quick check question**: What is the difference between self-attention and cross-attention in the context of multimodal learning?

- **Concept**: Irregular time series processing
  - **Why needed here**: EHR data has variable sampling rates and missing values, requiring techniques beyond standard fixed-interval processing.
  - **Quick check question**: How does flexible positional encoding differ from traditional fixed-interval resampling?

- **Concept**: Multimodal representation learning
  - **Why needed here**: The framework combines structured numerical data with unstructured text, requiring knowledge of how to fuse different data modalities effectively.
  - **Quick check question**: What are the advantages of cross-attention over simple concatenation for multimodal fusion?

## Architecture Onboarding

- **Component map**: Input layer → Variable-specific encoders → Time embedding layer → Positional encoding → Cross-attention module → Transformer encoder → Output layer
- **Critical path**: Time series → Variable-specific encoders → Time embedding → Cross-attention → Transformer → Classification
- **Design tradeoffs**: Separate encoders increase parameter count but improve variable-specific learning; cross-attention adds computational cost but enables effective multimodal fusion; Time2Vec increases model complexity but captures richer temporal patterns
- **Failure signatures**: Poor performance on tasks requiring temporal reasoning suggests issues with time embedding; inconsistent predictions across similar clinical scenarios may indicate cross-attention problems; overfitting on variable-specific patterns suggests need for regularization
- **First 3 experiments**:
  1. **Ablation of variable-specific encoders**: Replace with single shared encoder to quantify benefit
  2. **Cross-attention variants**: Test concatenation vs. cross-attention for multimodal fusion
  3. **Time embedding comparison**: Compare Time2Vec against positional embeddings only

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several areas remain unexplored based on the methodology and results presented.

## Limitations
- Limited generalizability due to evaluation on single dataset from three hospitals only
- Significant computational overhead from variable-specific encoders and cross-attention may limit real-world deployment
- Insufficient clinical interpretability and validation for practical clinical adoption
- Missing value handling strategies not detailed despite addressing irregular sampling

## Confidence
- **High confidence**: The core architectural components (variable-specific encoders, Time2Vec, cross-attention) are technically sound and well-implemented
- **Medium confidence**: The superiority claims over baseline approaches are supported by experimental results, but baselines may not represent state-of-the-art multimodal EHR methods
- **Low confidence**: Claims about clinical utility and practical deployment readiness, as these aspects were not rigorously evaluated

## Next Checks
1. **External validation across healthcare systems**: Test the framework on EHR data from hospitals with different electronic health record systems, patient demographics, and clinical practices to assess generalizability

2. **Computational efficiency benchmarking**: Measure and compare the computational overhead of variable-specific encoders and cross-attention against simpler alternatives, including inference latency and memory requirements

3. **Interpretability and clinical validation**: Implement attention visualization techniques to show clinicians how the model combines clinical notes and time series data, then validate whether these attention patterns align with clinical reasoning through expert review