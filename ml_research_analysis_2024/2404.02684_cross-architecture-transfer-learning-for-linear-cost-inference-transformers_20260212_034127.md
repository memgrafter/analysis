---
ver: rpa2
title: Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers
arxiv_id: '2404.02684'
source_url: https://arxiv.org/abs/2404.02684
tags:
- weights
- attention
- performance
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training efficient linear-cost
  inference (LCI) transformer architectures, which often require expensive full pretraining
  from scratch. To overcome this, the author proposes Cross-Architecture Transfer
  Learning (XATL), a method that transfers weights of shared components (e.g., layernorms,
  MLPs, embeddings) from pretrained self-attention transformers to LCI models.
---

# Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers

## Quick Facts
- arXiv ID: 2404.02684
- Source URL: https://arxiv.org/abs/2404.02684
- Authors: Sehyun Choi
- Reference count: 6
- Key outcome: XATL reduces training time up to 2.5x and improves performance up to 2.6% on LM benchmarks for LCI models.

## Executive Summary
This paper addresses the challenge of training efficient Linear-Cost Inference (LCI) transformer architectures, which typically require expensive full pretraining from scratch. The author proposes Cross-Architecture Transfer Learning (XATL), a method that transfers weights of shared components (layernorms, MLPs, embeddings) from pretrained self-attention transformers to LCI models. Experiments on RetNet and StripedMamba architectures demonstrate that XATL significantly reduces training time while improving performance within the same compute budget, enabling LCI models to achieve performance on par with or exceeding similar-sized self-attention transformers.

## Method Summary
XATL transfers weights of shared components between pretrained transformers and LCI models, specifically Ei (input embeddings), Eo (output embeddings), FFN (feed-forward networks), and WO (output projection) layers. The method uses AdamW optimizer with learning rate 3e-4, cosine annealing, warmup of 2000 steps, and weight decay of 0.1. Models are trained on the Pile dataset with batch size 1024 and sequence length 2048 for 150B tokens (400M models) or 100B tokens (1B models). A Loss Improvement Threshold (LIT) method unfreezes transferred weights when validation loss improvement falls below 1% for one patience period.

## Key Results
- XATL reduces training time by up to 2.5x compared to training from scratch
- Performance improves by up to 2.6% on language modeling and commonsense benchmarks
- LCI models achieve performance on par with or exceeding similar-sized self-attention transformers when trained with transferred weights

## Why This Works (Mechanism)

### Mechanism 1
Cross-Architecture Transfer Learning (XATL) leverages weight transfer from pretrained transformers to significantly accelerate training and improve performance of LCI models. XATL transfers weights of shared components (layernorms, MLPs, embeddings) from pretrained transformers to LCI models, providing a strong initialization and reducing the need for expensive full pretraining from scratch. Core assumption: LCI and transformer architectures share sufficient structural components to enable effective weight transfer. Evidence anchors: [abstract] "Cross-Architecture Transfer Learning (XATL), in which the weights of the shared components between LCI and self-attention-based transformers, such as layernorms, MLPs, input/output embeddings, are directly transferred to the new architecture from already pre-trained model parameters." [section] "Most LCI and Transformer variants share core components, such as layernorms, residual connections, and stack of interleaved time-mixing (Attention) + channel-mixing (FFN) blocks." Break condition: If the structural differences between LCI and transformer architectures are too significant, the transferred weights may not provide a meaningful initialization or could even hinder training.

### Mechanism 2
Transferring FFN layers provides a significant performance boost by transferring learned memory representations. The FFN layers in transformers are interpreted as key-value memory storage (Geva et al., 2021). Transferring these layers to LCI models equips them with pre-learned memory representations, accelerating training and improving performance. Core assumption: The FFN layers in transformers capture generalizable memory representations that are beneficial for LCI models. Evidence anchors: [section] "Transferring these layers from a PTLM can be conceptually thought of as transferring the memory of the PTLMs to the new models." [section] "The FFN layers has been interpreted to serve as the key-value memory storage for the models (Geva et al., 2021)." Break condition: If the FFN layers in transformers are highly task-specific or if the LCI architectures process information in a fundamentally different way, transferring FFN weights may not provide a meaningful benefit.

### Mechanism 3
XATL enables LCI models to achieve performance on par with or exceeding similar-sized self-attention transformers within the same compute budget. By providing a strong initialization through weight transfer, XATL reduces the training time required for LCI models to reach comparable performance levels, effectively lowering the compute cost barrier. Core assumption: The transferred weights provide a sufficient initialization for LCI models to converge faster and achieve comparable or better performance. Evidence anchors: [abstract] "XATL significantly reduces training time (up to 2.5x) and improves performance (up to 2.6% better on LM benchmarks) within the same compute budget." [section] "Our experiments on popular LCI architectures and model sizes show that XATL can achieve the same level of performance with 2.5x less compute and reach up to 2.6% higher performance on language modeling and commonsense benchmarks with the same compute budget." Break condition: If the LCI models require a fundamentally different training regime or if the performance gains from weight transfer are not sufficient to overcome the architectural differences, XATL may not achieve the claimed performance improvements.

## Foundational Learning

- Concept: Transformer Architecture and Attention Mechanisms
  - Why needed here: Understanding the transformer architecture and its components (self-attention, FFN, layernorms) is crucial for grasping how XATL transfers weights and how LCI models differ.
  - Quick check question: What are the main components of a transformer block, and how do they contribute to the model's functionality?

- Concept: Linear-Cost Inference (LCI) Architectures
  - Why needed here: Familiarity with LCI architectures (RetNet, Mamba, etc.) and their design principles is essential for understanding the motivation behind XATL and its potential benefits.
  - Quick check question: What are the key differences between LCI architectures and traditional transformers, and how do these differences impact inference efficiency?

- Concept: Knowledge Transfer and Transfer Learning
  - Why needed here: Understanding the concepts of knowledge transfer and transfer learning is crucial for grasping the motivation and mechanism behind XATL.
  - Quick check question: How does transfer learning work in the context of neural networks, and what are the potential benefits and challenges of transferring weights between different architectures?

## Architecture Onboarding

- Component map:
  Transformer components: self-attention, FFN, layernorms, embeddings
  LCI components: state-space models (RetNet, Mamba), linear attention kernels
  Shared components: layernorms, FFN, embeddings

- Critical path:
  1. Pretrain a transformer model on a large dataset.
  2. Identify the shared components between the transformer and the target LCI architecture.
  3. Transfer the weights of the shared components from the pretrained transformer to the LCI model.
  4. Train the LCI model with the transferred weights on the target task.

- Design tradeoffs:
  - Transfer vs. train from scratch: XATL reduces training time and compute cost but may not fully leverage the unique capabilities of LCI architectures.
  - Which components to transfer: Transferring FFN layers provides the most significant benefit, but transferring embeddings or attention output projections may also be beneficial depending on the specific LCI architecture.

- Failure signatures:
  - Poor performance: If the transferred weights do not provide a meaningful initialization or if the LCI architecture is too different from the transformer, the model may not converge or may achieve poor performance.
  - Overfitting: If the transferred weights are too specific to the pretraining task, the model may overfit to the pretraining data and not generalize well to the target task.

- First 3 experiments:
  1. Train a RetNet model with XATL using weights transferred from a pretrained transformer. Compare its performance to a RetNet model trained from scratch.
  2. Experiment with transferring different components (embeddings, FFN, attention output projections) and evaluate their impact on performance.
  3. Train a hybrid model with both attention and LCI components, using XATL to transfer weights from a pretrained transformer. Evaluate its performance compared to pure attention or pure LCI models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific characteristics of the loss improvement threshold that lead to optimal unfreezing of transferred weights?
- Basis in paper: [explicit] The paper mentions using a 1% threshold and 1 patience parameter for the Loss Improvement Threshold (LIT) method, but notes that further experimentation with different thresholds and patience values could be beneficial.
- Why unresolved: The paper only tested one set of parameters for the LIT method, leaving the optimal configuration for this heuristic unexplored.
- What evidence would resolve it: Systematic experimentation with varying threshold and patience parameters to determine the configuration that consistently yields the best performance across different model architectures and sizes.

### Open Question 2
- Question: How does the performance of Cross-Architecture Transfer Learning (XATL) scale with model size beyond 1 billion parameters?
- Basis in paper: [inferred] The paper mentions that larger models (3B and above) are an interesting direction for future work, as they may exhibit different performance characteristics and phase changes compared to smaller models.
- Why unresolved: The experiments were limited to models up to 1 billion parameters due to computational constraints, leaving the behavior of XATL at larger scales unknown.
- What evidence would resolve it: Training and evaluating XATL models with 3 billion parameters or more, comparing their performance to both scratch-trained models and smaller XATL models to identify any scaling trends or phase changes.

### Open Question 3
- Question: What is the impact of Cross-Architecture Transfer Learning on the inference efficiency and memory usage of LCI models?
- Basis in paper: [inferred] The paper focuses on the training benefits of XATL but does not explore how weight transfer affects inference performance, which is a key motivation for using LCI architectures.
- Why unresolved: The paper does not provide any analysis of inference time, memory usage, or other efficiency metrics for XATL-trained models compared to scratch-trained ones.
- What evidence would resolve it: Benchmarking XATL-trained LCI models on inference tasks, measuring their memory usage, generation speed, and KV-cache size compared to equivalent models trained from scratch.

## Limitations

- Experiments only validate on two specific LCI architectures (RetNet and StripedMamba) and relatively small model sizes (400M and 1B parameters)
- Theoretical interpretation of FFN transfer as memory representation transfer is plausible but not empirically validated within this work
- No analysis of downstream task performance or zero-shot capabilities, limiting generalizability assessment

## Confidence

**High Confidence**: The core mechanism of transferring shared components (layernorms, FFN, embeddings) between pretrained transformers and LCI models is well-established and technically sound.

**Medium Confidence**: The specific performance improvements (2.5x speedup, 2.6% accuracy gains) are demonstrated on the tested architectures but may not generalize to all LCI models or tasks.

**Low Confidence**: The interpretation that FFN transfer works primarily through memory representation transfer is theoretically plausible but not directly validated.

## Next Checks

1. **Cross-Architecture Generalization Test**: Implement XATL on additional LCI architectures (RWKV, Griffin, or different SSM variants) to verify that the 2.5x speedup claim holds beyond RetNet and StripedMamba.

2. **Component Ablation Study**: Systematically test different weight transfer combinations (FFN only, embeddings only, all three components) across multiple random seeds and model initializations.

3. **Downstream Task Transferability**: Evaluate XATL-transferred models on a diverse set of downstream tasks (question answering, summarization, code generation) and measure zero-shot performance.