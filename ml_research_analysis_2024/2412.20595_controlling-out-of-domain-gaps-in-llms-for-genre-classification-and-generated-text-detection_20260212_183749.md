---
ver: rpa2
title: Controlling Out-of-Domain Gaps in LLMs for Genre Classification and Generated
  Text Detection
arxiv_id: '2412.20595'
source_url: https://arxiv.org/abs/2412.20595
tags:
- classification
- genre
- texts
- text
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) struggle with domain transfer in non-topical
  classification tasks like genre classification and generated text detection. When
  demonstration examples come from one domain and testing occurs in another, performance
  declines significantly.
---

# Controlling Out-of-Domain Gaps in LLMs for Genre Classification and Generated Text Detection

## Quick Facts
- arXiv ID: 2412.20595
- Source URL: https://arxiv.org/abs/2412.20595
- Authors: Dmitri Roussinov; Serge Sharoff; Nadezhda Puchnina
- Reference count: 26
- Primary result: LLMs struggle with domain transfer in non-topical classification, showing up to 20 percentage point performance gaps when examples and testing occur in different domains.

## Executive Summary
This paper addresses a critical limitation in large language models: their struggle with domain transfer in non-topical classification tasks. When LLMs are demonstrated with examples from one domain and tested in another, performance declines significantly. The authors introduce a method that controls which predictive indicators LLMs use during classification, specifically guiding them to focus on stylistic rather than topical attributes. This approach reduces out-of-domain performance gaps by up to 20 percentage points in few-shot settings, demonstrating that simple Chain-of-Thought methods are insufficient and that detailed control over classification criteria consistently improves domain transfer performance.

## Method Summary
The method uses In-Context Learning (ICL) with three prompt types: Basic (baseline Chain-of-Thought), Simple Control (adds instruction to avoid topical content), and Detailed Control (specifies which stylistic features to prioritize and which topical indicators to ignore). The approach works by explicitly guiding LLMs to focus on domain-independent stylistic features rather than domain-dependent topical content. This is achieved through carefully constructed prompts that demonstrate both on-topic and off-topic examples, explicitly prohibiting the use of topical content or text length as classification criteria while emphasizing universally applicable features like formality, tone, and structure.

## Key Results
- Out-of-domain performance gaps of 5-20 percentage points occur when demonstration examples and testing occur in different domains
- Detailed control prompts reduce these gaps by up to 20 percentage points compared to baseline methods
- The approach works across different model families (GPT-4, Claude) and tasks (genre classification, generated text detection)
- Simple Chain-of-Thought methods prove insufficient for addressing domain transfer issues

## Why This Works (Mechanism)

### Mechanism 1
Controlling which predictive indicators LLMs use during classification reduces reliance on topical features and improves domain transfer. The method guides the model to focus on stylistic and structural features (e.g., tone, sentence structure, purpose) while explicitly excluding topical content as classification criteria. This shifts the model's attention from domain-specific content to more universal writing patterns. The core assumption is that topical features are domain-dependent and cause performance degradation when transferring between domains, while stylistic features are more domain-independent.

### Mechanism 2
Detailed control over classification criteria produces better performance than simple control or baseline Chain-of-Thought methods. By providing explicit examples of which features to prioritize (formality, tone, structure) and which to avoid (topics, length), the model receives clearer guidance than with generic instructions or no guidance at all. The core assumption is that LLMs need explicit instruction on feature selection for optimal few-shot classification performance.

### Mechanism 3
The method works across different model families (GPT-4, Claude) and tasks (genre classification, generated text detection). The approach of controlling feature selection is model-agnostic and task-agnostic, focusing on the fundamental problem of domain transfer through feature selection. The core assumption is that different LLMs share similar limitations regarding domain transfer and benefit from similar interventions.

## Foundational Learning

- **Domain transfer and out-of-domain performance gaps**: Why needed here - The paper addresses the core problem of why LLMs perform worse when testing domain differs from training/demonstration domain. Quick check - What causes LLMs to rely on topical features that don't transfer well between domains?

- **Feature selection and indicator control in few-shot learning**: Why needed here - The method works by explicitly controlling which features the model uses for classification rather than letting it discover them automatically. Quick check - How does controlling which indicators the model uses differ from traditional few-shot learning approaches?

- **Stylistic vs. topical features in text classification**: Why needed here - The method specifically shifts focus from topical (domain-dependent) to stylistic (domain-independent) features to enable better domain transfer. Quick check - Why would focusing on writing style rather than content improve performance across different topics?

## Architecture Onboarding

- **Component map**: Data preparation (corpus with genre/topic annotations) -> Prompt engineering (three prompt types) -> Model interfaces (GPT-4o, GPT-3.5, Claude 3) -> Evaluation (binary classification accuracy, McNemar test) -> Analysis (ablation studies, performance comparisons)

- **Critical path**: 1) Prepare off-topic and on-topic demonstration examples, 2) Generate test texts from on-topic data, 3) Construct prompts with appropriate control level, 4) Run classification through LLM APIs, 5) Compare performance between off-topic and on-topic settings, 6) Analyze which features models actually used

- **Design tradeoffs**: Token cost vs. performance (more examples improve performance but increase cost), Prompt specificity vs. generality (detailed prompts work better but are less flexible), Model capability vs. accessibility (more powerful models perform better but are more expensive), Binary vs. multi-class classification (binary reduces prompt size but may miss nuanced distinctions)

- **Failure signatures**: No performance difference between off-topic and on-topic settings (model ignores domain differences), Performance decreases with detailed control (over-constraining the model), Inconsistent results across model families (approach not truly model-agnostic), High sensitivity to prompt phrasing variations (lack of robustness)

- **First 3 experiments**: 1) Replicate basic prompt baseline comparison between off-topic and on-topic settings to confirm OOD gap exists, 2) Implement simple control prompt to verify basic feature control improves performance, 3) Test detailed control prompt on genre classification first, then generated text detection to confirm cross-task applicability

## Open Questions the Paper Calls Out

### Open Question 1
How would the proposed method perform on non-English texts across different linguistic structures and genre conventions? The paper acknowledges that the study is limited to English texts and notes that exploring non-English applications would require a large multilingual corpus with sufficient genre annotation and topical diversity. Experiments applying the method to a large multilingual corpus spanning diverse genres and topics, with performance metrics compared to English results, would resolve this question.

### Open Question 2
What is the impact of prompt variations on classification accuracy, and how sensitive is the method to specific wording? The paper mentions that prompt variations were tested using GPT-4o to paraphrase prompts, and while minor variations occurred, overall trends remained stable. However, it acknowledges that exhaustively testing all possible configurations is not feasible. A systematic study varying prompt wording and structure, measuring the impact on classification accuracy across multiple models and tasks, would resolve this question.

### Open Question 3
Can the proposed method be extended to other non-topical classification tasks such as sentiment analysis, author identification, or stylistic categorization? The paper states that the methodology is flexible and can be adapted to other non-topical classification tasks, but it only demonstrates the method on genre classification and generated text detection. Experiments applying the method to tasks like sentiment analysis, author identification, or stylistic categorization, with performance metrics compared to existing approaches, would resolve this question.

## Limitations

- The effectiveness of the control method depends heavily on the task's susceptibility to domain transfer issues, and it may not apply or could degrade performance for tasks requiring topical understanding.
- The method shows substantial variation in effectiveness between model families (Claude 3 Haiku showing 20-point improvements while GPT-3.5 shows only 1 point), suggesting it may not be equally effective for all LLMs.
- The paper doesn't extensively explore failure cases where the approach might backfire, particularly for tasks requiring domain-specific knowledge.

## Confidence

- **High Confidence**: The core claim that LLMs experience significant performance degradation when transferring between domains in non-topical classification tasks is well-supported with consistent OOD gaps of 5-20 percentage points across multiple models and tasks.
- **Medium Confidence**: The claim that detailed control over classification criteria produces better domain transfer than simple control or baseline methods is supported but could benefit from more extensive ablation studies.
- **Low Confidence**: The claim that the method is universally applicable across all model families and tasks is the weakest, given the substantial variation in effectiveness between models.

## Next Checks

1. Test the detailed control method on additional non-topical classification tasks beyond genre classification and generated text detection, such as authorship attribution or writing style analysis, to verify the approach generalizes to other domains where domain transfer is problematic.

2. Systematically test the method on tasks where topical understanding is essential to determine the boundary conditions where the approach succeeds versus fails, creating a taxonomy of tasks suitable for this intervention.

3. Use model interpretability tools to verify that LLMs are actually shifting from topical to stylistic feature reliance when using detailed control prompts, rather than finding alternative mechanisms for improved performance.