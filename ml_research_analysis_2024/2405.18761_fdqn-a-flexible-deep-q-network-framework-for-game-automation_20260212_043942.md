---
ver: rpa2
title: 'FDQN: A Flexible Deep Q-Network Framework for Game Automation'
arxiv_id: '2405.18761'
source_url: https://arxiv.org/abs/2405.18761
tags:
- game
- learning
- games
- framework
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the Flexible Deep Q-Network (FDQN) framework
  for automating web-based games using reinforcement learning. FDQN addresses the
  challenge of high-dimensional, real-time decision-making in dynamic environments
  by combining a CNN for processing visual input with a modular architecture that
  adapts to varying action spaces.
---

# FDQN: A Flexible Deep Q-Network Framework for Game Automation

## Quick Facts
- arXiv ID: 2405.18761
- Source URL: https://arxiv.org/abs/2405.18761
- Reference count: 8
- Primary result: FDQN outperforms baseline models on Atari games and Chrome Dino, achieving performance comparable to or exceeding human averages

## Executive Summary
This paper introduces FDQN, a Flexible Deep Q-Network framework designed for automating web-based games using reinforcement learning. The framework combines a CNN-based feature extractor with a modular architecture that adapts to varying action spaces, enabling effective handling of high-dimensional visual inputs in real-time environments. FDQN employs an epsilon-greedy policy for exploration-exploitation balance and utilizes a large replay buffer to stabilize training. Experimental results on Atari games and Chrome Dino demonstrate that FDQN outperforms baseline models, with performance on par with or exceeding human averages. The modular design allows easy adaptation to other HTML-based games, and future work includes extending FDQN to multi-agent systems and more complex domains.

## Method Summary
FDQN is a reinforcement learning framework that automates web