---
ver: rpa2
title: Mining Frequent Structures in Conceptual Models
arxiv_id: '2406.07129'
source_url: https://arxiv.org/abs/2406.07129
tags:
- patterns
- conceptual
- modeling
- mining
- pattern
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an interactive approach for discovering frequent
  structural patterns in conceptual models using frequent subgraph mining and graph
  manipulation techniques. The method transforms conceptual models into labeled property
  graphs, applies gSpan for mining, and offers interactive filtering, clustering,
  and visualization of discovered patterns.
---

# Mining Frequent Structures in Conceptual Models

## Quick Facts
- **arXiv ID**: 2406.07129
- **Source URL**: https://arxiv.org/abs/2406.07129
- **Reference count**: 40
- **Key outcome**: Approach achieves 100% accuracy in discovering known patterns in 143 OntoUML models using frequent subgraph mining and interactive visualization

## Executive Summary
This paper presents an interactive approach for discovering frequent structural patterns in conceptual models using frequent subgraph mining and graph manipulation techniques. The method transforms conceptual models into labeled property graphs, applies gSpan for mining, and offers interactive filtering, clustering, and visualization of discovered patterns. Tested on 143 OntoUML models, the approach achieves 100% accuracy in discovering known patterns, with mining times ranging from 1-360 seconds depending on parameters, and clustering accuracy of 93%. The tool enables language engineers to identify both good and bad modeling practices, supporting the evolution of conceptual modeling languages through automated pattern discovery.

## Method Summary
The approach transforms conceptual models into labeled property graphs as input for the gSpan frequent subgraph mining algorithm. Users can interactively filter patterns, adjust minimum support thresholds, and cluster similar structures using cosine similarity on extracted features. The system provides visualization of discovered patterns and allows comparison with known patterns to identify novel structures. The method was implemented in Python using NetworkX, Grandiso, and gSpan libraries, and tested on a corpus of 143 OntoUML models from various domains.

## Key Results
- 100% accuracy in discovering known modeling patterns from 143 OntoUML models
- Mining times range from 1-360 seconds depending on parameters and model complexity
- Clustering accuracy of 93% when using similarity thresholds of 0.6 or 0.61
- Interactive filtering enables discovery of unexpected patterns beyond frequent ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The approach reliably identifies known modeling patterns with high accuracy.
- Mechanism: gSpan frequent subgraph mining algorithm discovers recurring structures in labeled property graphs derived from conceptual models, while interactive filtering allows users to remove known patterns and focus on novel ones.
- Core assumption: Conceptual models can be accurately transformed into labeled property graphs without losing semantic information, and known patterns are explicitly representable as subgraphs.
- Evidence anchors:
  - [abstract] "Tested on 143 OntoUML models, the approach achieves 100% accuracy in discovering known patterns"
  - [section] "Experiment 1 demonstrated that already-known structures can be discovered accounting for all the constructs of which they are composed"
  - [corpus] Weak corpus evidence; related papers focus on general graph mining but not conceptual modeling patterns specifically

### Mechanism 2
- Claim: Clustering improves pattern assessment by grouping similar structures together.
- Mechanism: Feature extraction from graph patterns (node count, edge count, adjacency matrix, labels) combined with cosine similarity clustering groups structurally similar patterns, reducing cognitive load for language engineers.
- Core assumption: Structural similarity correlates with functional similarity in modeling patterns, and extracted features adequately capture pattern essence.
- Evidence anchors:
  - [abstract] "clustering accuracy of 93%" 
  - [section] "the clustering component we employ in the pipeline achieves 0.93 accuracy when the adopted similarity threshold is 0.6 or 0.61"
  - [corpus] No direct corpus evidence for clustering accuracy in conceptual modeling context

### Mechanism 3
- Claim: Interactive customization enables discovery of unexpected patterns beyond frequent ones.
- Mechanism: Users can adjust minimum support thresholds, node counts, and construct filters to shift focus from highly frequent to potentially novel patterns that may reveal modeling issues or opportunities.
- Core assumption: Unexpected patterns exist in the data and are discoverable when frequency constraints are relaxed, and users can effectively identify interesting patterns among the noise.
- Evidence anchors:
  - [abstract] "enables language engineers to identify both good and bad modeling practices"
  - [section] "To what extent is the approach useful for discovering new interesting structures?"
  - [corpus] Weak corpus evidence; related papers focus on general pattern mining but not on discovering "unexpected" patterns in conceptual models

## Foundational Learning

- **Concept**: Labeled Property Graphs (LPGs)
  - Why needed here: The approach transforms conceptual models into LPGs as input for the gSpan algorithm, requiring understanding of graph data structures with labeled nodes and edges
  - Quick check question: How does a labeled property graph differ from a standard graph, and why is this distinction important for pattern mining?

- **Concept**: Frequent Subgraph Mining (FSM)
  - Why needed here: The core discovery mechanism uses FSM algorithms like gSpan to find recurring patterns, requiring understanding of support thresholds and subgraph isomorphism
  - Quick check question: What determines whether a subgraph is considered "frequent" in the context of this approach?

- **Concept**: Cosine Similarity and Clustering
  - Why needed here: The clustering component uses cosine similarity on feature vectors to group similar patterns, requiring understanding of vector similarity measures
  - Quick check question: How does cosine similarity work when comparing feature vectors extracted from graph patterns?

## Architecture Onboarding

- **Component map**: Importing → Filtering → Mining → Deepening → Clustering → Visualization, with interactive parameter adjustment at each stage
- **Critical path**: Importing → Mining → Visualization (core discovery workflow), with Filtering and Deepening as optional enhancements
- **Design tradeoffs**: High accuracy vs. performance (lower support thresholds increase pattern count but mining time), comprehensive vs. focused analysis (broad filters vs. targeted searches)
- **Failure signatures**: Zero patterns found (thresholds too high), excessive patterns (thresholds too low), poor clustering results (similarity threshold misconfigured), missing patterns (import transformation errors)
- **First 3 experiments**:
  1. Run with minimum support 3, minimum nodes 5 on synthetic dataset to verify known pattern discovery
  2. Run with minimum support 20, minimum nodes 5 on OntoUML catalog to test performance scaling
  3. Run with varying similarity thresholds (0.5, 0.6, 0.7) on clustered patterns to optimize grouping accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the clustering approach be improved to achieve consistent optimal performance across different datasets and similarity thresholds?
- Basis in paper: [explicit] The paper mentions that the optimal similarity threshold for clustering varies across different cases and suggests that the approach is not yet optimal.
- Why unresolved: The paper does not provide a definitive solution or guideline for determining the optimal threshold, indicating that this is an area for further research.
- What evidence would resolve it: Developing a method to predict the optimal similarity threshold based on dataset characteristics or providing a range of thresholds that consistently yield good clustering results.

### Open Question 2
- Question: What is the impact of increasing the size and complexity of the dataset on the performance of the approach, and how can it be optimized for large-scale data?
- Basis in paper: [inferred] The paper discusses the performance of the approach on a dataset of 143 models but does not explore the limits of scalability or optimization strategies for larger datasets.
- Why unresolved: The paper does not provide insights into the scalability challenges or solutions for handling significantly larger datasets, which is crucial for real-world applications.
- What evidence would resolve it: Conducting experiments with larger datasets and analyzing the performance metrics to identify bottlenecks and potential optimizations.

### Open Question 3
- Question: How can the approach be extended to support other conceptual modeling languages beyond OntoUML, such as BPMN or ArchiMate?
- Basis in paper: [explicit] The paper mentions the goal of supporting multiple conceptual modeling languages but does not provide a detailed implementation or evaluation for languages other than OntoUML.
- Why unresolved: The paper focuses on OntoUML as a proof of concept but does not explore the adaptability and performance of the approach for other languages.
- What evidence would resolve it: Implementing the approach for additional languages and evaluating its effectiveness in discovering patterns specific to those languages.

## Limitations
- Corpus Representativeness: Evaluation uses only 143 OntoUML models, limiting generalizability to other modeling languages or domains
- Threshold Sensitivity: High accuracy at specific parameter settings but performance may degrade significantly with different thresholds
- Semantic Completeness: Transformation to labeled property graphs may not capture all semantic nuances of conceptual models

## Confidence
**High Confidence** (3 claims):
- The gSpan algorithm can discover frequent subgraphs in labeled property graphs derived from conceptual models
- Interactive filtering and visualization components function as described
- The approach successfully identifies known patterns in controlled test conditions

**Medium Confidence** (2 claims):
- Clustering accuracy of 93% generalizes across different parameter settings
- Unexpected pattern discovery works effectively with relaxed frequency thresholds
- Performance times (1-360 seconds) scale predictably with model complexity

## Next Checks
1. **Parameter Sensitivity Test**: Systematically vary minimum support thresholds (2-50) and similarity thresholds (0.5-0.9) to identify robustness boundaries and optimal operating ranges.
2. **Cross-Language Generalization**: Apply the approach to conceptual models from different modeling languages (UML class diagrams, ER diagrams) to assess language independence and identify any language-specific limitations.
3. **Semantic Depth Evaluation**: Test pattern discovery on models with rich semantic annotations and constraints to determine whether LPG transformation preserves sufficient semantic information for meaningful pattern mining.