---
ver: rpa2
title: Compute-Constrained Data Selection
arxiv_id: '2410.16208'
source_url: https://arxiv.org/abs/2410.16208
tags:
- data
- selection
- compute
- training
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies compute-constrained data selection for finetuning
  large language models. While prior data selection methods reduce training dataset
  size, their utility depends heavily on their compute cost.
---

# Compute-Constrained Data Selection

## Quick Facts
- arXiv ID: 2410.16208
- Source URL: https://arxiv.org/abs/2410.16208
- Reference count: 40
- Key outcome: Simpler data selection methods (BM25, embedding) are more compute-optimal than complex methods (perplexity, gradient-based) for finetuning LLMs under compute constraints

## Executive Summary
This paper studies compute-constrained data selection for finetuning large language models, addressing a gap where prior data selection methods reduced dataset size but ignored their own computational costs. The authors formalize a compute-aware data selection objective and conduct extensive experiments with 600+ models ranging from 7B to 70B parameters across 6 data selection methods and 3 tasks. They find that while complex methods like perplexity and gradient-based selection provide better performance at the same data budget, they are rarely compute-optimal due to their high computational costs. Simpler methods like BM25 and embedding-based selection dominate under compute constraints, with perplexity-based methods requiring a 5x training-to-selection model size ratio and gradient-based methods requiring 10x to become compute-optimal.

## Method Summary
The authors evaluate five data selection methods (Random, BM25, Embed, PPL, and LESS) across multiple model sizes (7B, 13B, 70B) and three tasks (MMLU, BBH, IFEval). They use LoRA parameter-efficient finetuning with fixed data budgets (2.5%-100% of total tokens) and AdamW optimizer with specific learning rates. The compute cost for each method is calculated based on theoretical FLOPs, and a parametric model P(k) = (P̄ - P0) × (1 - exp(-λC(k)/C(|D|))) + P0 is used to fit the relationship between compute allocation and performance. Experiments are conducted with multiple runs increasing compute allocated to either larger models or more expensive selection methods.

## Key Results
- Complex data selection methods (perplexity, gradient-based) are rarely compute-optimal despite providing better performance at the same data budget
- BM25 and embedding-based methods significantly outperform perplexity and gradient-based methods for 7B models under tight compute constraints
- Perplexity-based data selection requires a 5x training-to-selection model size ratio to become compute-optimal (around 35B parameters)
- Gradient-based data selection requires a 10x training-to-selection model size ratio to become compute-optimal (around 70B parameters)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compute-optimal data selection depends on the ratio between training model size and data selection model size.
- Mechanism: As the training model becomes much larger than the selection model, the fixed cost of complex data selection methods becomes relatively small compared to the training cost, making them more efficient despite their high absolute cost.
- Core assumption: The utility of data selection methods increases with model size at different rates.
- Evidence anchors:
  - [abstract]: "For compute-optimal finetuning, we find that perplexity and gradient data selection require training-to-selection model size ratios of 5x and 10x, respectively."
  - [section 7]: "For perplexity-based data selection, our extrapolation suggests that the method becomes compute-optimal when the training model is 5x larger than the data selection model—around 35B parameters. For gradient-based data selection, our extrapolation indicates that the training model needs to be approximately 10x larger than the data selection model to be compute-optimal—around 70B parameters."

### Mechanism 2
- Claim: Simpler data selection methods (BM25, embedding-based) are more compute-optimal for smaller models and tight compute budgets.
- Mechanism: These methods have very low absolute computational costs that don't scale with model size, making them Pareto-optimal when the total compute budget is constrained.
- Core assumption: The utility gain from more complex methods doesn't outweigh their computational cost when the budget is tight.
- Evidence anchors:
  - [section 6]: "The main 7B results in Figure 2 (A, D) and Figure 5 (b), show that cheap lexicon-based methods (BM25) and embedding-based methods (Embed) significantly outperform perplexity-based (PPL) and gradient-based methods (LESS)."
  - [section 7]: "At the 70B model size, shown in Figure 2(C, F), PPL and LESS outperform both BM25 and Embed finetuning tokens for the first time. This suggests that at very large compute budgets, more sophisticated and costly methods can gain a greater advantage compared to lexical and embedding methods."

### Mechanism 3
- Claim: Data selection methods that use model information (perplexity, gradient) provide better performance at the same data budget but are not compute-optimal.
- Mechanism: These methods can identify more relevant data points, leading to better performance per data point, but their high computational cost means they're not the most efficient use of limited compute.
- Core assumption: Better performance per data point translates to better overall performance when compute is constrained.
- Evidence anchors:
  - [section 7]: "While PPL and LESS achieve better performance at the same data budget compared to these methods (see Figure 5), they are not compute optimal under the same compute budget due to the high FLOPs required for data selection."

## Foundational Learning

- Concept: Compute scaling laws for language models
  - Why needed here: The paper builds on established scaling laws to understand how model size, training data, and compute budget interact in the context of data selection.
  - Quick check question: What is the relationship between model size and loss according to scaling laws, and how does this relate to the compute-optimal data selection findings?

- Concept: Data selection as a coreset selection problem
  - Why needed here: Understanding data selection as selecting a subset of data that approximates the full dataset is fundamental to why different methods are being compared.
  - Quick check question: How does the coreset selection framework apply to the data selection methods being evaluated (BM25, embedding, perplexity, gradient)?

- Concept: Pareto optimality in resource allocation
  - Why needed here: The concept of Pareto optimality is used to determine which data selection methods provide the best performance for a given compute budget.
  - Quick check question: How is Pareto optimality defined in the context of compute-constrained data selection, and why is it a useful metric for comparing methods?

## Architecture Onboarding

- Component map:
  - Data selection methods: Random -> BM25 -> Embed -> PPL -> LESS
  - Model sizes: 7B -> 13B -> 70B parameters
  - Tasks: MMLU -> BBH -> IFEval
  - Compute budget allocation: Model size vs Data selection method
  - Performance evaluation: Task accuracy

- Critical path:
  1. Select data selection method and compute budget
  2. Train selection model (if needed)
  3. Apply data selection to get subset
  4. Train target model on selected subset
  5. Evaluate performance on target task
  6. Record compute usage and performance

- Design tradeoffs:
  - Simpler methods (BM25, Embed) have lower compute cost but potentially lower performance per data point
  - Complex methods (PPL, LESS) have higher compute cost but potentially better data selection
  - Model size affects both training cost and data selection method efficiency
  - Multiple task setting allows amortizing data selection cost

- Failure signatures:
  - Complex methods failing to outperform simpler ones despite higher cost
  - Performance not improving with increased model size as expected
  - Data selection methods not scaling as predicted with model size ratios
  - Compute budget allocation not yielding expected performance improvements

- First 3 experiments:
  1. Replicate the 7B model results with BM25 vs PPL vs LESS to verify simpler methods outperform complex ones under tight compute constraints
  2. Test the 5x and 10x model size ratio hypothesis by training a 35B model with 7B PPL selection and a 70B model with 7B LESS selection
  3. Compare multiple task setting (5 tasks) with single task to verify gradient information amortization benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact compute-optimal ratio between training model size and selection model size for perplexity-based and gradient-based methods across different tasks and datasets?
- Basis in paper: [explicit] The paper extrapolates that perplexity-based methods require a 5x training-to-selection model size ratio while gradient-based methods require 10x, but these are based on parametric fits from limited experiments
- Why unresolved: The extrapolation relies on parametric models fitted to a specific set of experiments with LLaMA models on three tasks, and may not generalize to other model families or tasks
- What evidence would resolve it: Empirical validation with a wider range of model sizes (particularly larger than 70B), different model architectures, and diverse tasks would establish more robust scaling relationships

### Open Question 2
- Question: How do data selection methods perform in multi-epoch training scenarios where data can be reused selectively?
- Basis in paper: [inferred] The paper notes this as a limitation, acknowledging that their analysis focuses on single-epoch finetuning and that multi-epoch settings could provide further training speedup
- Why unresolved: The study only examines single-pass training, while practical scenarios often involve multiple epochs with potential for intelligent data reuse
- What evidence would resolve it: Comparative experiments testing various data selection methods across multiple epochs, measuring both computational efficiency and final performance

### Open Question 3
- Question: What is the impact of validation set size on the effectiveness of different data selection methods?
- Basis in paper: [explicit] The paper mentions that validation sets with more than 50 examples are "sufficient" for BM25 and Embed, but doesn't systematically explore how validation set size affects performance
- Why unresolved: The experiments use fixed validation set sizes without exploring the sensitivity of results to this hyperparameter, which could significantly affect method performance
- What evidence would resolve it: Controlled experiments varying validation set sizes across different data selection methods while measuring both selection quality and computational costs

## Limitations

- The compute-optimal ratios (5x, 10x) are extrapolated from only three model sizes and may not generalize to the full range of model sizes
- Compute cost calculations are based on theoretical FLOPs rather than measured wall-clock times, which could differ significantly in practice
- The analysis focuses on single-epoch finetuning and doesn't explore how data selection methods perform in multi-epoch scenarios

## Confidence

- **High confidence**: Simpler methods (BM25, embedding) dominate under tight compute constraints - directly observed across multiple experiments with 7B models and different tasks
- **Medium confidence**: The 5x and 10x model size ratio thresholds for compute-optimality - based on extrapolation with only three data points, requiring validation at intermediate model sizes
- **Medium confidence**: The superiority of complex methods at large compute budgets - observed at 70B but not systematically tested across a broader range of compute budgets

## Next Checks

1. Validate the 5x and 10x ratio hypothesis by training intermediate model sizes (15B, 25B, 35B) with corresponding selection model sizes to test the transition points empirically
2. Measure actual wall-clock time and energy consumption for data selection methods on the same hardware used for training, rather than relying on theoretical FLOPs calculations
3. Test the model size scaling hypothesis with additional model families (not just LLaMA) to verify that the compute-optimal ratios are architecture-independent