---
ver: rpa2
title: 'GigaSpeech 2: An Evolving, Large-Scale and Multi-domain ASR Corpus for Low-Resource
  Languages with Automated Crawling, Transcription and Refinement'
arxiv_id: '2406.11546'
source_url: https://arxiv.org/abs/2406.11546
tags:
- speech
- gigaspeech
- data
- languages
- thai
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GigaSpeech 2, a large-scale, multi-domain,
  multilingual ASR corpus for low-resource Southeast Asian languages, including Thai,
  Indonesian, and Vietnamese. It comprises about 30,000 hours of automatically transcribed
  speech from unlabeled YouTube videos.
---

# GigaSpeech 2: An Evolving, Large-Scale and Multi-domain ASR Corpus for Low-Resource Languages with Automated Crawling, Transcription and Refinement

## Quick Facts
- arXiv ID: 2406.11546
- Source URL: https://arxiv.org/abs/2406.11546
- Reference count: 40
- Primary result: ~30,000 hours of multilingual ASR data for Thai, Indonesian, Vietnamese; models reduce WER by 25-40% vs Whisper large-v3 with 10% of parameters

## Executive Summary
GigaSpeech 2 is a large-scale, multilingual ASR corpus targeting low-resource Southeast Asian languages (Thai, Indonesian, Vietnamese). It comprises about 30,000 hours of automatically transcribed speech from unlabeled YouTube videos, enabling scalable dataset construction without paired text supervision. An automated pipeline handles data crawling, transcription, forced alignment, and filtering, followed by iterative Noisy Student Training (NST) to refine pseudo-labels. Models trained on the refined corpus achieve strong performance with significantly fewer parameters than Whisper large-v3, making it a valuable resource for low-resource ASR research.

## Method Summary
The approach uses YouTube audio crawling and automatic transcription with Whisper, followed by TorchAudio forced alignment and multi-dimensional filtering (charset, language confidence, duration, balancing). A modified Noisy Student Training method iteratively relabels, filters, and retrains models to progressively improve data quality and model performance. Zipformer Transducer and Conformer models are trained using the refined pseudo-labels, achieving strong ASR results despite the low-resource setting.

## Key Results
- ~30,000 hours of Thai, Indonesian, and Vietnamese speech from YouTube
- 25-40% WER reduction compared to Whisper large-v3 with only 10% of the model parameters
- Iterative NST improves performance by progressively refining pseudo-labels
- Corpus and pipeline will be released for research use

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Iterative Noisy Student Training with relabeling and filtering significantly improves ASR performance on low-resource languages.
- **Mechanism:** The process begins by training a teacher model on a subset of pseudo-labeled data. The teacher then relabels and filters a larger subset of raw pseudo-labels, which is used to train a student model with noise augmentation. This student becomes the new teacher in the next iteration, gradually refining the data quality.
- **Core assumption:** Initial pseudo-labels from Whisper are imperfect but contain enough correct information to bootstrap a teacher model.
- **Evidence anchors:**
  - [abstract] "A modified Noisy Student Training is developed to further refine flawed pseudo labels iteratively, thereby enhancing model performance."
  - [section 4.1] "Experimental results... confirm our corpus's high quality and broad applicability."
  - [section 3.2] "A modified NST method... starts by training a teacher model on a subset of flawed pseudo labels and iteratively expands the training set, generates new pseudo labels, and filters them."
- **Break condition:** If initial pseudo-labels are too noisy or the filtering thresholds are too strict, the iterative process may fail to bootstrap meaningful improvements.

### Mechanism 2
- **Claim:** Using unlabeled YouTube audio data without relying on paired text allows scalable dataset construction for low-resource languages.
- **Mechanism:** The pipeline focuses solely on audio content, bypassing the need for high-quality paired text. YouTube videos are crawled based on topic and format diversity, transcribed with Whisper, and aligned with TorchAudio. Multi-dimensional filtering removes low-quality segments.
- **Core assumption:** YouTube contains sufficient audio content in target languages to build a representative dataset without text supervision.
- **Evidence anchors:**
  - [abstract] "It is designed for low-resource languages and does not rely on paired speech and text data."
  - [section 3.1] "Our dataset is collected with a focus solely on the audio content, irrespective of the existence or quality of corresponding text pairs."
  - [section 2] "YODAS attempts to crawl audio from YouTube, but neither manual nor automatic subtitles accurately reflect the speech content, resulting in unguaranteed quality."
- **Break condition:** If the YouTube content in target languages is too sparse or dominated by non-linguistic audio, the corpus will lack coverage.

### Mechanism 3
- **Claim:** Multi-dimensional filtering based on charset, language confidence, duration, and balancing ensures high data quality despite using automatic transcriptions.
- **Mechanism:** After initial transcription, segments are filtered by allowed character sets, language identification confidence scores, duration constraints, and duplication balancing. This reduces noise from automatic transcription and alignment errors.
- **Core assumption:** Even imperfect automatic transcriptions contain enough correct tokens to allow filtering rules to remove the worst segments.
- **Evidence anchors:**
  - [section 3.1] "A series of heuristic filtering rules across text and audio modalities are implemented to exclude relatively poor-quality samples."
  - [section 4.2] "Our model achieves such impressive performance with nearly one-tenth of the parameters compared to Whisper large-v3."
  - [corpus] Limited direct evidence; the filtering effectiveness is inferred from downstream ASR performance.
- **Break condition:** If filtering rules are too aggressive, they may remove too much data; if too lenient, residual noise may degrade model training.

## Foundational Learning

- **Concept:** Automatic speech recognition (ASR) model architecture and training
  - **Why needed here:** Understanding how Zipformer Transducer and Conformer models work is essential to grasp the experimental setup and performance claims.
  - **Quick check question:** What are the main components of a transducer-based ASR model, and how do they differ from encoder-decoder architectures?

- **Concept:** Self-supervised learning and pseudo-labeling in low-resource settings
  - **Why needed here:** The paper relies on generating and refining pseudo-labels from unlabeled data; knowing how these techniques work is key to understanding the pipeline.
  - **Quick check question:** How does Noisy Student Training use teacher-student dynamics to improve model performance with unlabeled data?

- **Concept:** Data filtering and quality assurance in speech corpora
  - **Why needed here:** The pipeline's effectiveness hinges on filtering out low-quality segments; understanding these techniques is crucial for evaluating the approach.
  - **Quick check question:** What types of filters are typically applied to speech data, and how do they improve downstream model performance?

## Architecture Onboarding

- **Component map:** YouTube crawling -> audio preprocessing -> TRAIN/DEV/TEST splitting -> Whisper transcription -> TorchAudio forced alignment -> text normalization -> charset, language confidence, duration, balancing filtering -> NST iterations -> Zipformer Transducer (Icefall) / Conformer (ESPNet) training -> evaluation
- **Critical path:** Crawling -> Transcription -> Forced Alignment -> Filtering -> NST Iterations -> Model Training -> Evaluation
- **Design tradeoffs:**
  - Using automatic transcriptions enables scalability but introduces noise, mitigated by iterative NST and filtering.
  - Focusing on audio-only data avoids dependency on paired text but may include domain mismatches.
  - Balancing between data quantity and quality: aggressive filtering reduces data size but improves quality.
- **Failure signatures:**
  - ASR performance plateaus or degrades after NST iterations -> possible overfitting or filtering too much data.
  - High deletion/substitution rates in evaluation -> alignment or transcription errors not caught by filters.
  - Model underperforms on public test sets -> domain mismatch between YouTube and evaluation data.
- **First 3 experiments:**
  1. Run the full pipeline on a small subset of YouTube data for one language; verify transcription, alignment, and filtering outputs.
  2. Train an initial teacher model on filtered pseudo-labels; evaluate on a held-out manual transcription set.
  3. Execute one NST iteration (relabeling + filtering + student training) and compare performance to the teacher model.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of GigaSpeech 2 models scale with more iterations of Noisy Student Training beyond the tested 3-4 iterations?
- **Basis in paper:** [explicit] The paper mentions "Due to time constraints, we only tested 3-4 iterations of the proposed NST model. We are optimistic that more iterations will yield even better results."
- **Why unresolved:** The paper did not have the resources to test more than 3-4 iterations of NST.
- **What evidence would resolve it:** Running GigaSpeech 2 models with 5, 6, 7+ iterations of NST and comparing WER reduction to the 3-4 iteration models.

### Open Question 2
- **Question:** How does the performance of GigaSpeech 2 models change if trained on larger amounts of data for Indonesian and Vietnamese to match the Thai dataset size?
- **Basis in paper:** [explicit] The paper states "The Indonesian and Vietnamese datasets are not as extensive as the Thai datasets, so we plan to add more data for these two languages."
- **Why unresolved:** The current GigaSpeech 2 datasets for Indonesian and Vietnamese are smaller than the Thai dataset. The impact of increasing their size is unknown.
- **What evidence would resolve it:** Creating a larger GigaSpeech 2 dataset for Indonesian and Vietnamese, and training models on it to measure the change in WER compared to the current models.

### Open Question 3
- **Question:** How much does language model fusion improve the performance of GigaSpeech 2 models, and how difficult is it to create high-quality in-domain text data for low-resource languages?
- **Basis in paper:** [explicit] The paper mentions "In addition, we did not perform language model fusion to further boost performance since there is a lack of high-quality and in-domain text data for low-resource languages."
- **Why unresolved:** The paper did not have access to high-quality in-domain text data for low-resource languages to perform LM fusion.
- **What evidence would resolve it:** Creating high-quality in-domain text data for low-resource languages, and training GigaSpeech 2 models with LM fusion to measure the WER improvement.

## Limitations

- Reliance on automatic transcriptions and filtering introduces uncertainty about data quality; effectiveness is inferred from downstream ASR performance rather than directly measured.
- Filtering thresholds (CER/WER cutoffs, language confidence thresholds) are not disclosed, making it difficult to assess strictness or potential over-filtering.
- Exact YouTube channel list and category definitions for data crawling are not specified, affecting reproducibility and domain coverage assessment.
- Focus on three Southeast Asian languages; generalization to other low-resource languages remains an open question.

## Confidence

- **Iterative Noisy Student Training improves ASR performance:** High confidence. The mechanism is well-established in the literature, and the paper provides ablation studies comparing WER before and after NST iterations.
- **Automatic transcription and filtering suffice for building a high-quality ASR corpus:** Medium confidence. While downstream ASR performance is strong, direct evidence of filtering quality is limited; effectiveness is inferred rather than directly measured.
- **10% of Whisper parameters achieves 25-40% WER reduction:** High confidence. The paper reports clear comparisons between their models and Whisper large-v3, with specific WER and CER metrics provided.

## Next Checks

1. **Filter Quality Validation:** Manually inspect a random sample of segments that were filtered out vs. kept. Calculate the percentage of true positives (correctly removed) and false positives (incorrectly removed) to assess filter effectiveness.
2. **Domain Generalization Test:** Evaluate the trained ASR models on an external, manually transcribed speech corpus in the same languages (e.g., TED talks or news broadcasts) to measure domain transfer and robustness.
3. **Ablation of Filtering Thresholds:** Systematically vary the filtering thresholds (e.g., language confidence cutoff, duration limits) and measure the impact on both data quantity and ASR performance to find the optimal tradeoff.