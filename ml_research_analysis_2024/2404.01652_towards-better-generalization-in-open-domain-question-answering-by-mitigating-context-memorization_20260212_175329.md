---
ver: rpa2
title: Towards Better Generalization in Open-Domain Question Answering by Mitigating
  Context Memorization
arxiv_id: '2404.01652'
source_url: https://arxiv.org/abs/2404.01652
tags:
- knowledge
- corpus
- reader
- training
- openqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving generalization
  in open-domain question answering (OpenQA) models when dealing with evolving knowledge
  corpora and unseen domains. The core issue stems from the reader's tendency to over-memorize
  retrieved contexts, which hinders its ability to adapt to new information.
---

# Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization

## Quick Facts
- arXiv ID: 2404.01652
- Source URL: https://arxiv.org/abs/2404.01652
- Reference count: 19
- Models with Corpus-Invariant Tuning (CIT) achieve up to 2.1% absolute gain in exact match (EM) scores for cross-corpus and cross-domain generalization in OpenQA

## Executive Summary
This paper addresses the challenge of improving generalization in open-domain question answering (OpenQA) models when dealing with evolving knowledge corpora and unseen domains. The core issue stems from the reader's tendency to over-memorize retrieved contexts, which hinders its ability to adapt to new information. To mitigate this, the authors propose Corpus-Invariant Tuning (CIT), a training strategy that controls the likelihood of retrieved documents to prevent over-memorization. Extensive experiments on multiple OpenQA benchmarks show that CIT significantly improves model generalization across different corpus versions and knowledge domains, achieving up to 2.1% absolute gain in exact match (EM) scores, without compromising performance in the original corpus and domain.

## Method Summary
The authors introduce Corpus-Invariant Tuning (CIT), a training strategy that adds a regularization loss term to prevent the reader model from over-memorizing retrieved contexts. The CIT loss computes the squared difference between the log-likelihood of retrieved documents under current and initial reader parameters, constraining the reader from assigning higher probabilities to retrieved contexts. This forces the reader to depend more on the retriever during training, which in turn improves the retriever's performance. The method is tested on OpenQA benchmarks using the Atlas-XL model architecture (Contriever retriever with Fusion-in-Decoder reader) across different corpus versions (Wikipedia 2017 vs 2018) and knowledge domains (general vs biomedical).

## Key Results
- CIT achieves up to 2.1% absolute improvement in exact match scores for cross-corpus generalization (Wiki-2017 to Wiki-2018)
- Models trained with CIT show better cross-domain generalization on RobustQA benchmark compared to baseline models
- CIT improves retriever performance by forcing the reader to rely more on retrieved documents during training
- The proposed method maintains performance on the original corpus while significantly improving generalization to new corpora and domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CIT reduces reader's reliance on memorizing retrieved contexts by penalizing increases in the likelihood of retrieved documents during training.
- Mechanism: The CIT loss term (Equation 1) computes the squared difference between the log-likelihood of retrieved documents under current and initial reader parameters. This constrains the reader from assigning higher probabilities to retrieved contexts, forcing it to depend more on the retriever.
- Core assumption: The reader's over-memorization of retrieved contexts is a major cause of poor generalization across corpus versions and domains.
- Evidence anchors:
  - [abstract] "We introduce Corpus-Invariant Tuning (CIT), a simple but effective training strategy, to mitigate the knowledge over-memorization by controlling the likelihood of retrieved contexts during training."
  - [section 3] "To solve this problem, we propose Corpus-Invariant Tuning (CIT), a straightforward but effective method to temper the reader’s tendency to over-memorize the contents of externally retrieved documents..."
- Break condition: If the retriever consistently retrieves irrelevant documents, penalizing likelihood increases may degrade performance because the reader needs some memorization to compensate.

### Mechanism 2
- Claim: CIT improves retriever performance by forcing the reader to rely more on retrieved documents during training.
- Mechanism: When the reader is discouraged from memorizing corpus content, it becomes more dependent on the retriever for information. This increased reliance during training encourages the retriever to improve its document selection quality.
- Core assumption: The reader and retriever are jointly trained, so changes in reader behavior influence retriever learning.
- Evidence anchors:
  - [section 4.5] "The proposed CIT training loss reduces the reader model’s memorization tendency, leading to greater reliance on the documents retrieved. This enhanced dependency during training on the retrieved documents appears to enhance the retriever’s performance..."
  - [section 3] "Such a phenomenon can be caused by the exposure bias problem as discussed in (Yu and Ji, 2023)."
- Break condition: If the retriever's capacity is severely limited, forcing the reader to depend on it more may not yield improvements.

### Mechanism 3
- Claim: CIT prevents "hard-coding" of outdated knowledge by maintaining invariant likelihood of retrieved contexts across training updates.
- Mechanism: By constraining how much the reader's probability distribution over retrieved documents can change, CIT prevents the model from committing too strongly to specific knowledge facts, allowing easier adaptation to new information.
- Core assumption: Models that commit too strongly to specific knowledge representations struggle to adapt when that knowledge becomes outdated.
- Evidence anchors:
  - [section 2.2.1] "When directly transitioning to an updated corpus, there is a noticeable performance decline. Even additional tuning on the newer version doesn’t achieve the same effectiveness as training from scratch..."
  - [section 3] "Given a question Who is the prime minister of the UK?, if a model has already hard-coded an outdated answer Boris Johnson into its parameters (while being trained on an old corpus), it is harder to change its response even if the new information Rishi Sunak from an updated corpus is available."
- Break condition: If the corpus updates are minimal, the constraint on likelihood changes may unnecessarily limit learning.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG) architecture
  - Why needed here: Understanding how retrievers and readers interact is crucial for grasping why CIT works - the paper addresses limitations in the standard RAG training approach.
  - Quick check question: In a RAG system, what are the two main components and how do they typically interact during training?

- Concept: Masked Language Modeling (MLM) probability as likelihood measure
  - Why needed here: CIT uses MLM probability from T5 to measure the reader's likelihood of retrieved documents, so understanding this concept is essential for implementing CIT.
  - Quick check question: How does masked language modeling probability differ from standard language model probability, and why might it be used to measure document likelihood?

- Concept: Knowledge memorization vs. retrieval dependency trade-off
  - Why needed here: The paper's core argument is that excessive memorization harms generalization, so understanding this trade-off is fundamental to the work.
  - Quick check question: What are the potential advantages and disadvantages of a reader model that relies heavily on memorization versus one that relies heavily on retrieval?

## Architecture Onboarding

- Component map: Question -> Retriever (Contriever) -> Documents -> Reader (FiD) -> Answer -> QA Loss + CIT Loss -> Model Parameters

- Critical path:
  1. Question and corpus → Retriever selects relevant documents
  2. Documents + question → Reader generates answer
  3. Answer + ground truth → Compute QA loss
  4. Retrieved documents → Compute CIT loss
  5. Combined loss → Update model parameters

- Design tradeoffs:
  - Strength of CIT loss (α): Too weak provides little benefit; too strong may prevent useful memorization
  - Masked span percentage: Higher percentages give more stable likelihood estimates but may miss important document structure
  - Retriever dropout: Balancing between stable retrieval and preventing over-reliance on specific documents

- Failure signatures:
  - Performance degradation on original corpus when CIT is too strong
  - Retriever performance drops significantly when reader is forced to rely on it more
  - Training instability when CIT loss overwhelms QA loss

- First 3 experiments:
  1. Train Atlas-XL on NQ with Wiki-2017, test on Wiki-2018 (zero-shot) to establish baseline generalization gap
  2. Add CIT loss with α=0.2, repeat experiment to measure improvement in cross-corpus generalization
  3. Vary α from 0.1 to 0.5, measure trade-off between original corpus performance and cross-corpus generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the optimal level of memorization mitigation be automatically determined by the model, rather than relying on human-selected hyperparameters?
- Basis in paper: [inferred] The paper mentions that the extent of memorization mitigation depends on a hyper-parameter (α) which is chosen by humans, and suggests this is a limitation that could be improved by automatic determination.
- Why unresolved: Current models require manual tuning of the CIT strength parameter α to balance between memorization and retrieval reliance. The paper does not propose or evaluate methods for automatic determination of this parameter.
- What evidence would resolve it: Experiments showing a method that can automatically adapt α during training to achieve optimal performance across different domains and corpus versions, compared to fixed human-selected values.

### Open Question 2
- Question: Does CIT's effectiveness extend to larger foundation models like GPT-4, and how does it scale with model size?
- Basis in paper: [explicit] The paper mentions that CIT is theoretically applicable to larger-scale auto-regressive foundation models and provides preliminary experiments with LLaMA-2-7b as a case study.
- Why unresolved: The paper only tests CIT with Atlas-XL (3B parameters) and LLaMA-2-7b (7B parameters). The scaling behavior and effectiveness on much larger models remains unexplored.
- What evidence would resolve it: Comprehensive experiments testing CIT across a range of model sizes from 1B to 175B+ parameters, showing performance improvements and computational efficiency trade-offs.

### Open Question 3
- Question: What is the impact of CIT on the model's ability to handle questions that require synthesizing information from multiple retrieved documents?
- Basis in paper: [inferred] While CIT improves retrieval performance and document coverage, the paper does not specifically analyze how it affects multi-document reasoning capabilities.
- Why unresolved: The paper focuses on retrieval performance metrics but doesn't examine whether CIT's encouragement of retrieval reliance improves or hinders complex reasoning tasks that require integrating information across multiple documents.
- What evidence would resolve it: Detailed analysis of model performance on multi-hop QA tasks and questions requiring synthesis of information from multiple sources, comparing CIT-trained models with baseline models.

## Limitations

- The paper focuses primarily on one retriever-reader architecture (Atlas with Contriever retriever and FiD reader), raising questions about generalizability to other OpenQA systems
- The mechanism by which CIT improves retriever performance is inferred rather than directly validated through ablation studies
- The analysis of when CIT might fail (break conditions) is theoretical rather than empirically tested

## Confidence

- **High Confidence**: The core claim that CIT improves cross-corpus and cross-domain generalization (up to 2.1% EM gain) is well-supported by experimental results across multiple benchmarks. The mechanism of constraining document likelihood to prevent over-memorization is clearly demonstrated.
- **Medium Confidence**: The explanation of how CIT improves retriever performance is plausible but indirect. While experimental results show retriever improvements, the causal mechanism linking reader behavior changes to retriever learning is inferred rather than proven through ablation studies.
- **Low Confidence**: The break conditions identified for when CIT might fail are speculative. The paper doesn't provide empirical evidence for scenarios where the retriever consistently retrieves irrelevant documents or when corpus updates are minimal.

## Next Checks

1. **Ablation Study on Retriever Dependency**: Conduct controlled experiments that decouple reader and retriever training to definitively establish whether the observed retriever improvements are a direct consequence of CIT's effect on reader behavior, or if they arise from other factors in the joint training setup.

2. **Cross-Architecture Generalization**: Test CIT with alternative retriever-reader combinations (e.g., DPR retriever with FiD reader, or different dense retrieval models) to verify whether the memorization-mitigation benefits extend beyond the specific Atlas-Contriever-FiD configuration used in the paper.

3. **Dynamic CIT Loss Scheduling**: Experiment with adaptive scheduling of the CIT loss weight (α) that adjusts based on training progress or retrieval quality metrics, rather than using a fixed coefficient, to determine if this approach can better balance memorization and retrieval dependency throughout training.