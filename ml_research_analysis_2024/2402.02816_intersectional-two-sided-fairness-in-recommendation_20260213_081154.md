---
ver: rpa2
title: Intersectional Two-sided Fairness in Recommendation
arxiv_id: '2402.02816'
source_url: https://arxiv.org/abs/2402.02816
tags:
- fairness
- intersectional
- two-sided
- groups
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the overlooked problem of intersectional two-sided
  unfairness in recommendation systems, where certain intersectional groups (combinations
  of user and item groups) experience systematic discrimination even when two-sided
  fairness is achieved. The authors propose ITFR, a method that simultaneously balances
  the ability to distinguish positives from negatives across groups and ensures fair
  treatment of positives within different intersectional groups.
---

# Intersectional Two-sided Fairness in Recommendation

## Quick Facts
- arXiv ID: 2402.02816
- Source URL: https://arxiv.org/abs/2402.02816
- Reference count: 40
- Primary result: ITFR significantly reduces intersectional unfairness (e.g., CV@20 drops by 43.5% on Movielens) while maintaining or improving accuracy

## Executive Summary
This paper addresses the overlooked problem of intersectional two-sided unfairness in recommendation systems, where certain intersectional groups (combinations of user and item groups) experience systematic discrimination even when two-sided fairness is achieved. The authors propose ITFR, a method that simultaneously balances the ability to distinguish positives from negatives across groups and ensures fair treatment of positives within different intersectional groups. ITFR employs sharpness-aware loss to detect disadvantaged groups, collaborative loss balance to fairly reweight training losses, and predicted score normalization to align positive predictions. Experiments on three public datasets show ITFR significantly reduces intersectional unfairness while maintaining or improving accuracy.

## Method Summary
ITFR targets intersectional two-sided fairness by improving both the ability to distinguish positives from negatives across groups and ensuring fair treatment of positives within different intersectional groups. The method employs three key components: (1) Sharpness-aware loss to detect disadvantaged groups by considering worst-case loss within a parameter neighborhood, (2) Collaborative loss balance that uses inter-group gradient contributions to fairly reweight training losses, and (3) Predicted score normalization to align positive prediction scores across groups. The method is compatible with existing recommendation backbones like matrix factorization and LightGCN, and shows significant improvements in fairness metrics while maintaining or improving accuracy.

## Key Results
- ITFR reduces intersectional unfairness by 43.5% on Movielens dataset (CV@20)
- Method maintains or improves accuracy metrics (P@K, R@K, N@K) while improving fairness
- ITFR is compatible with existing reranking algorithms and improves their fairness performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sharpness-aware loss improves detection of disadvantaged intersectional groups by considering worst-case loss within a parameter neighborhood, not just point loss.
- Mechanism: Instead of using standard BPR loss directly, the method computes a worst-case loss over a bounded region around current parameters. This accounts for the "sharpness" of the loss landscapeâ€”if the loss curve is sharp, small perturbations can cause large test performance drops, so the worst-case loss is a better proxy for true distinguishing ability.
- Core assumption: The worst-case loss within a bounded region is a more reliable indicator of a group's distinguishing ability on test data than point-wise loss, because it captures loss landscape geometry.
- Evidence anchors:
  - [abstract] "Our method utilizes a sharpness-aware loss to perceive disadvantaged groups"
  - [section] "Considering training losses on only a single point ðœƒ is vulnerable to random perturbations if the loss curve is sharp, which may lead to an ineffective detection of discriminated groups."
  - [corpus] No direct corpus evidence on sharpness-aware methods in fairness recommendation; assumption is from general ML literature.
- Break condition: If the loss landscape is relatively flat for all groups, worst-case loss differences may be negligible and sharpness-aware loss offers little benefit over point loss.

### Mechanism 2
- Claim: Collaborative loss balance uses inter-group gradient contributions to fairly reweight training losses, accounting for group dependencies.
- Mechanism: The method models how updating parameters based on one group's loss impacts other groups' losses (via gradient contributions). Groups are reweighted not just by their own loss but by their total contribution to all groups, weighted by group difficulty. This encourages balanced learning across related intersectional groups.
- Core assumption: Gradients of different intersectional groups are related due to shared users/items, so reweighting based on isolated group losses ignores these dependencies and leads to suboptimal fairness.
- Evidence anchors:
  - [abstract] "uses collaborative loss balance to develop consistent distinguishing abilities for different intersectional groups"
  - [section] "Different intersectional groups are related to each other as they may share similar users and items. Therefore, optimizing the loss of one group can also strongly impact the losses of other groups."
  - [corpus] No direct corpus evidence on collaborative loss balance for fairness; this is a novel contribution.
- Break condition: If groups are truly independent (no shared users/items), collaborative reweighting provides no benefit over standard group reweighting.

### Mechanism 3
- Claim: Predicted score normalization bounds positive prediction scores, indirectly reducing systematic bias between intersectional groups while maintaining accuracy.
- Mechanism: After improving loss balance, the method normalizes predicted scores (e.g., via embedding normalization) to bound them within a fixed range. This limits systematic score biases between groups that can persist even with balanced losses, since BPR only optimizes relative distances, not absolute scores.
- Core assumption: Even with balanced losses, systematic bias in positive prediction scores can exist across groups, and bounding these scores helps mitigate this bias without severely impacting accuracy.
- Evidence anchors:
  - [abstract] "predicted score normalization is leveraged to align positive predicted scores to fairly treat positives in different intersectional groups"
  - [section] "Even if the training loss is similar between different intersectional groups, the predicted score for positive samples in different intersectional groups may be systematically biased as the commonly used recommendation loss (e.g., BPR) only optimizes the distance between positives and negatives and does not constrain the absolute value of the predicted scores."
  - [corpus] No direct corpus evidence on predicted score normalization for fairness; this is a novel approach.
- Break condition: If the normalization significantly distorts the relative ranking quality or causes accuracy loss, it may not be worth the fairness gain.

## Foundational Learning

- Concept: Intersectional fairness in recommendation
  - Why needed here: The paper addresses a specific fairness problem where combinations of user and item groups face discrimination even when individual user/item fairness is achieved. Understanding this concept is crucial to grasp why standard fairness methods fail.
  - Quick check question: Why might a recommender system be fair to all male users and all romance movie fans individually, yet unfair to male users who like romance movies?

- Concept: Sharpness-aware optimization
  - Why needed here: The method uses sharpness-aware loss to detect disadvantaged groups. Understanding this optimization technique is key to why this approach improves group detection.
  - Quick check question: How does considering the worst-case loss in a parameter neighborhood differ from using just the current loss value?

- Concept: Gradient-based reweighting and collaborative optimization
  - Why needed here: The method uses collaborative loss balance based on gradient contributions between groups. Understanding how gradients from one group affect others is essential to grasp this mechanism.
  - Quick check question: Why might reweighting training losses based solely on individual group losses be suboptimal when groups share users or items?

## Architecture Onboarding

- Component map:
  Input data -> Embedding-based recommender -> Sharpness-aware loss computation -> Collaborative loss balance -> Predicted score normalization -> Fairness-aware recommendations

- Critical path:
  1. Forward pass to compute predictions and BPR loss for each intersectional group
  2. Compute sharpness-aware loss by approximating worst-case loss (gradient ascent step)
  3. Compute gradient contributions between groups (using cumulative gradients)
  4. Update group weights based on collaborative contributions
  5. Apply weighted sharpness-aware loss for backward pass
  6. Apply predicted score normalization to embeddings before final predictions

- Design tradeoffs:
  - Sharpness-aware loss vs. point loss: More robust group detection but higher computational cost
  - Collaborative vs. independent reweighting: Better fairness at potential cost of convergence stability
  - Score normalization: Reduces systematic bias but may affect ranking quality if over-applied

- Failure signatures:
  - Poor convergence or unstable training: May indicate issues with collaborative weight updates or sharpness-aware loss computation
  - Accuracy degradation: Could suggest score normalization is too aggressive
  - Minimal fairness improvement: Might indicate groups are too sparse or the method isn't capturing the right unfairness patterns

- First 3 experiments:
  1. Run on Movielens-1M with gender-user and genre-item groups to verify intersectional unfairness exists (compare CV@20 before/after)
  2. Compare ITFR vs. GroupDRO (intersectional version) on the same dataset to isolate benefit of sharpness-aware loss
  3. Test score normalization alone on a baseline to measure its direct impact on fairness and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed method generalize effectively to settings with multiple user and item attributes (e.g., user gender Ã— age Ã— location, item genre Ã— rating Ã— popularity)?
- Basis in paper: [explicit] The authors note their method can handle multiple single-sided attributes using Cartesian products, and show preliminary results on gender Ã— age combinations.
- Why unresolved: Only a single dataset (Movielens) with two user attributes was tested, and no experiments with three or more attributes per side were conducted.
- What evidence would resolve it: Experiments on datasets with more complex attribute combinations showing consistent performance improvements and fairness gains across varying numbers of attributes.

### Open Question 2
- Question: How does the method perform when applied to non-matrix factorization recommendation backbones like neural collaborative filtering or transformer-based models?
- Basis in paper: [explicit] The authors tested their method with LightGCN as an alternative backbone and showed positive results.
- Why unresolved: Only two types of recommendation models were tested (MF and LightGCN), leaving uncertainty about performance with more complex or state-of-the-art architectures.
- What evidence would resolve it: Experiments demonstrating fairness and accuracy improvements when ITFR is integrated with diverse recommendation architectures including neural CF, transformers, and sequential models.

### Open Question 3
- Question: What is the computational overhead of the collaborative loss balance component compared to standard GroupDRO, especially with increasing numbers of intersectional groups?
- Basis in paper: [inferred] The authors mention that collaborative loss balance models relationships between intersectional groups, but note that their approximation reduces computational cost.
- Why unresolved: The paper provides limited details on actual computational complexity or runtime comparisons with standard GroupDRO across different dataset sizes and group granularities.
- What evidence would resolve it: Systematic benchmarking of training time, memory usage, and convergence speed comparing ITFR with and without collaborative loss balance across datasets with varying numbers of intersectional groups.

## Limitations

- The effectiveness of sharpness-aware loss depends on the assumption that loss landscape geometry reliably indicates test performance, which may not hold in all scenarios
- Collaborative loss balance assumes significant gradient dependencies between intersectional groups, which may be less pronounced in sparse datasets or when groups have minimal overlap
- Predicted score normalization lacks theoretical grounding on how it affects ranking quality and may introduce unintended bias in certain cases

## Confidence

- **High Confidence**: The existence of intersectional unfairness in recommendation systems (supported by empirical evidence across three datasets)
- **Medium Confidence**: The three proposed mechanisms (sharpness-aware loss, collaborative reweighting, score normalization) improve fairness based on experimental results
- **Low Confidence**: The theoretical justification for why sharpness-aware loss specifically helps with fairness detection, as this extends beyond standard sharpness-aware optimization literature

## Next Checks

1. **Ablation Study on Loss Components**: Test ITFR with only the collaborative loss balance (no sharpness-aware loss, no score normalization) to isolate whether the fairness gains primarily come from the reweighting mechanism rather than the worst-case loss approximation.

2. **Robustness to Group Definition**: Vary the number and composition of user/item groups (e.g., use different demographic attributes or item categories) to assess whether ITFR's performance degrades significantly when group definitions change, which would indicate overfitting to specific group structures.

3. **Scalability Analysis**: Evaluate ITFR's computational overhead on larger datasets (beyond the ~1M interactions in current experiments) and measure training time, memory usage, and convergence behavior compared to standard BPR training to determine practical deployment constraints.