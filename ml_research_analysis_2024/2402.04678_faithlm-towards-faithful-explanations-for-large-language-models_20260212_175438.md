---
ver: rpa2
title: 'FaithLM: Towards Faithful Explanations for Large Language Models'
arxiv_id: '2402.04678'
source_url: https://arxiv.org/abs/2402.04678
tags:
- explanation
- fidelity
- explanations
- faithlm
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FaithLM introduces a model-agnostic framework that improves the\
  \ faithfulness of LLM-generated explanations through a novel intervention-based\
  \ fidelity measure. It treats explanation faithfulness as a causal property: if\
  \ an explanation truly captures model reasoning, contradicting its content should\
  \ shift the model\u2019s prediction."
---

# FaithLM: Towards Faithful Explanations for Large Language Models

## Quick Facts
- arXiv ID: 2402.04678
- Source URL: https://arxiv.org/abs/2402.04678
- Reference count: 23
- Key outcome: FaithLM improves explanation faithfulness through intervention-based fidelity measures and iterative optimization

## Executive Summary
FaithLM introduces a model-agnostic framework that enhances the faithfulness of LLM-generated explanations through a novel intervention-based fidelity measure. The method treats explanation faithfulness as a causal property: if an explanation truly captures model reasoning, contradicting its content should shift the model's prediction. FaithLM operationalizes this by generating contrary hints with opposite semantics and measuring prediction shifts, then iteratively refining both explanations and prompts to maximize fidelity scores. Experiments across three multi-domain datasets with multiple LLM backbones demonstrate consistently higher fidelity and truthfulness compared to baselines.

## Method Summary
FaithLM employs a two-phase iterative optimization approach to generate faithful explanations. First, it refines explanations by maximizing a contrary-hint score that measures prediction shift when explanations are contradicted. Second, it optimizes the explanation trigger prompt itself to improve overall fidelity. The framework uses trajectory-based learning where high-fidelity explanations from previous iterations guide future generation through in-context learning. The method is model-agnostic, avoids token masking, and works across different LLM architectures while maintaining computational efficiency through controlled optimization steps.

## Key Results
- FaithLM achieves consistently higher fidelity scores across ECQA, TriviaQA-Long, and COPA datasets compared to baseline methods
- Optimized trigger prompts show significant improvement in explanation faithfulness over human-crafted prompts
- Explanations generated by FaithLM demonstrate better alignment with ground-truth rationales while maintaining high fidelity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Faithful explanations should change model predictions when their content is contradicted.
- Mechanism: The intervention-based fidelity measure constructs a "contrary hint" that expresses opposite semantics to the explanation and measures the resulting prediction shift. A large, directionally consistent change indicates faithful explanation.
- Core assumption: If an explanation captures information the model actually uses, then intervening with contradictory information should change the model's output distribution.
- Evidence anchors: [abstract] "a faithful explanation should yield a prediction shift when its content is contradicted"; [section] "FaithLM formalizes explanation faithfulness as an intervention property: a faithful explanation should yield a prediction shift when its content is contradicted"

### Mechanism 2
- Claim: Iterative optimization of explanations using trajectory-based learning improves fidelity over multiple rounds.
- Mechanism: FaithLM collects explanations with their fidelity scores in a trajectory, then generates new explanations conditioned on this trajectory to maximize future fidelity scores through in-context learning.
- Core assumption: The trajectory of high-fidelity explanations provides useful context that guides the explainer LLM to generate increasingly faithful explanations.
- Evidence anchors: [abstract] "Building on this principle, FaithLM iteratively refines both the elicitation prompt and the explanation to maximize the measured score"; [section] "FaithLM iteratively refines both the elicitation prompt and the explanation to maximize the measured score"

### Mechanism 3
- Claim: Trigger prompt optimization improves explanation quality by finding optimal prompts that elicit faithful explanations.
- Mechanism: FaithLM optimizes the explanation trigger prompt itself by measuring average fidelity scores across hold-out samples and iteratively updating the prompt to maximize these scores.
- Core assumption: The quality of the explanation trigger prompt significantly impacts the faithfulness of generated explanations, and this can be optimized separately from the explanations themselves.
- Evidence anchors: [abstract] "Building on this principle, FaithLM iteratively refines both the elicitation prompt and the explanation to maximize the measured score"; [section] "Despite the success of enhancing fidelity in Section 3.2, the low quality of the explanation trigger prompts PE may still hinder the optimization process"

## Foundational Learning

- Concept: Causal intervention testing
  - Why needed here: FaithLM's core innovation relies on testing faithfulness through controlled semantic interventions (contrary hints) rather than traditional token-level masking or heuristics
  - Quick check question: How does FaithLM's contrary hint intervention differ from traditional fidelity measurement methods that mask tokens or modify outputs?

- Concept: In-context learning and trajectory optimization
  - Why needed here: The iterative refinement process relies on using previously generated high-fidelity explanations as context to guide future generation
  - Quick check question: What role does the trajectory T play in guiding the explainer LLM toward higher-fidelity explanations in subsequent iterations?

- Concept: Model-agnostic evaluation frameworks
  - Why needed here: FaithLM works across different LLM backbones without requiring access to model internals or task-specific heuristics
  - Quick check question: How does FaithLM maintain model-agnosticism while still providing effective fidelity measurement across different LLM architectures?

## Architecture Onboarding

- Component map: Input → Targeted LLM → Explainer LLM → Explanation → Contrary Hint → Fidelity Evaluation → Trajectory Update → New Explanation

- Critical path: The system processes input through the targeted LLM, generates explanations via the explainer LLM, evaluates fidelity through contrary hint intervention, and uses trajectory-based optimization to refine both explanations and prompts iteratively.

- Design tradeoffs:
  - Model-agnostic vs. task-specific optimization: FaithLM sacrifices some potential performance gains for broad applicability
  - Computational cost vs. fidelity: Multiple iterations increase computation but improve explanation quality
  - Simple contrary hints vs. nuanced semantic analysis: Binary contradiction may miss complex faithfulness issues

- Failure signatures:
  - No prediction shift despite contradictory contrary hint indicates non-faithful explanation
  - Optimization plateaus suggest trigger prompt limitations or explainer LLM constraints
  - High variance in fidelity scores across iterations may indicate unstable contrary hint generation

- First 3 experiments:
  1. Test contrary hint generation quality on sample explanations using NLI classifiers to verify semantic opposition
  2. Run single iteration of FaithLM on ECQA dataset to verify basic fidelity measurement functionality
  3. Compare baseline self-explanation methods against FaithLM on a small dataset to establish baseline performance gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FaithLM's contrary-hint score compare to other faithfulness evaluation methods like simulatability when applied to the same dataset?
- Basis in paper: [explicit] "We additionally report a small-scale comparison where we compute (i) simulatability scores and (ii) SE on the same instances"
- Why unresolved: The paper mentions this comparison but doesn't provide the actual results or detailed analysis of when the two methods agree or disagree
- What evidence would resolve it: Quantitative comparison showing correlation coefficients and case studies of examples where the two methods produce different faithfulness assessments

### Open Question 2
- Question: What is the relationship between the quality of contrary hints and the fidelity scores they produce, and how can we systematically improve contrary hint generation?
- Basis in paper: [explicit] "The quality of contrary hints ¬ENL determines the efficacy of FaithLM" and ablation studies show NLI classifiers achieve 86% and 82% accuracy in identifying dissimilar meanings
- Why unresolved: While the paper demonstrates contrary hints are mostly semantically opposite, it doesn't explore the full impact of hint quality on fidelity scores or methods to optimize hint generation
- What evidence would resolve it: Controlled experiments varying contrary hint quality and measuring corresponding fidelity score changes, plus methods to generate more effective contrary hints

### Open Question 3
- Question: How does FaithLM's performance scale with larger models and more complex reasoning tasks beyond the three datasets studied?
- Basis in paper: [inferred] Experiments use Vicuna-7B and Phi-2 as target models on ECQA, TriviaQA-Long, and COPA datasets
- Why unresolved: The paper demonstrates effectiveness on relatively simple tasks with medium-sized models but doesn't explore performance on larger models or more complex reasoning tasks
- What evidence would resolve it: Experiments with larger models (Llama, GPT-4) on more complex reasoning benchmarks (MMLU, BigBench) showing fidelity and truthfulness scores

### Open Question 4
- Question: What is the optimal number of optimization steps for FaithLM across different task types and model sizes?
- Basis in paper: [explicit] "20 rounds of optimization are sufficient to converge" and "The iteration process terminates at a predetermined 20 step"
- Why unresolved: The paper uses fixed optimization steps (20 for explanations, 50 for trigger prompts) but doesn't explore whether different tasks or model sizes require different optimization durations
- What evidence would resolve it: Analysis showing fidelity score trends across varying optimization steps for different task types and model sizes, identifying optimal stopping points

### Open Question 5
- Question: How does FaithLM perform in specialized domains like healthcare where transparency is critical, and what are the practical deployment challenges?
- Basis in paper: [explicit] "For future work, we plan to extend FaithLM in healthcare, where the needs for transparency is critical"
- Why unresolved: The paper mentions healthcare as a future direction but doesn't provide any empirical results or analysis of domain-specific challenges
- What evidence would resolve it: Case studies applying FaithLM to medical QA datasets with evaluation of both technical performance and practical deployment considerations like latency and computational costs

## Limitations

- The contrary hint generation mechanism may not capture nuanced forms of non-faithfulness that go beyond simple semantic opposition
- Iterative optimization increases computational costs, potentially limiting practical deployment in resource-constrained settings
- The framework's effectiveness depends heavily on the quality of initial explanations and contrary hint generation, which may vary across domains

## Confidence

- High confidence: The causal intervention principle (contrary hints measuring faithfulness) is theoretically sound and well-supported by the abstract and methodology sections
- Medium confidence: The iterative optimization approach will consistently improve fidelity across diverse tasks, as results show improvement but may not generalize to all domains
- Low confidence: The model-agnostic nature will perform equally well across all LLM architectures without task-specific tuning, given limited cross-model validation in the presented experiments

## Next Checks

1. **Semantic Opposition Verification**: Validate contrary hint quality by testing a sample of generated contrary hints against a separate NLI classifier to ensure they truly express opposite semantics to the explanations, addressing potential weaknesses in the core intervention mechanism.

2. **Cross-Model Transferability**: Test FaithLM's performance across three different LLM pairs (explainer-predictor combinations) on the same task to verify the claimed model-agnostic benefits and identify any architecture-specific limitations.

3. **Long-Term Optimization Stability**: Run FaithLM for extended iterations (beyond the reported 20 steps) to determine if fidelity improvements plateau, decline, or continue improving, which would reveal the true limits of the trajectory-based optimization approach.