---
ver: rpa2
title: Can Language Model Understand Word Semantics as A Chatbot? An Empirical Study
  of Language Model Internal External Mismatch
arxiv_id: '2409.13972'
source_url: https://arxiv.org/abs/2409.13972
tags:
- word
- language
- association
- linguistics
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the internal-external mismatch in language
  models, focusing on word semantics understanding. It compares the performance of
  probing (internal representations) and querying (model outputs) across three tasks:
  word similarity, structured prediction, and analogy.'
---

# Can Language Model Understand Word Semantics as A Chatbot? An Empirical Study of Language Model Internal External Mismatch

## Quick Facts
- arXiv ID: 2409.13972
- Source URL: https://arxiv.org/abs/2409.13972
- Authors: Jinman Zhao; Xueyan Zhang; Xingyu Yue; Weizhe Chen; Zifan Qian; Ruiyu Wang
- Reference count: 17
- One-line primary result: Language models show significant internal-external mismatch in word semantics understanding, with probing consistently outperforming querying across all tasks and models.

## Executive Summary
This paper investigates the internal-external mismatch in language models, focusing on word semantics understanding. The study compares probing (internal representations) and querying (model outputs) across three tasks: word similarity, structured prediction, and analogy. Using BERT, GPT-2, and T5 models, the authors find a consistent gap where probing outperforms querying, indicating that models possess semantic knowledge in their internal representations that fails to fully manifest in outputs. While finetuning and calibration can reduce this discrepancy, they cannot eliminate it entirely.

## Method Summary
The study evaluates three model architectures (BERT, GPT-2, T5) on three semantic tasks using two evaluation methods. Probing accesses internal token representations through linear classification on frozen models, while querying requires full inference to generate outputs. The tasks include WiC for word similarity, CoNLL2003 for named entity recognition (structured prediction), and BATS for analogy. The authors systematically compare probing and querying performance, then investigate whether finetuning on task-specific data or calibration techniques can reduce the observed internal-external mismatch.

## Key Results
- Probing consistently outperforms querying across all three models (BERT, GPT-2, T5) and all three tasks (WiC, NER, Analogy)
- The internal-external gap persists even after finetuning, with probing accuracy remaining superior to query accuracy
- Calibration improves confidence-accuracy alignment but cannot eliminate the fundamental mismatch between internal knowledge and external outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Internal hidden representations contain richer word semantic information than model outputs.
- Mechanism: Probing directly accesses internal token representations through linear classification, while querying requires information to propagate through the full inference pipeline to output tokens.
- Core assumption: The model's internal representations encode word semantics that are not fully utilized in the final output distribution.
- Evidence anchors:
  - [abstract] "Results show a significant gap between probing and querying, with probing consistently outperforming querying in all tasks and models."
  - [section 3.1] "We found noticeable differences between probe and query in terms of word semantic capturing. This gap is evident across all models and all benchmarks."
  - [corpus] Weak evidence - no direct corpus support for this mechanism, only indirect support from related work on model honesty.

### Mechanism 2
- Claim: Finetuning on task-specific data improves alignment between internal representations and outputs.
- Mechanism: Fine-tuning adjusts model parameters to better propagate semantic knowledge from internal representations to output distributions, reducing the internal-external mismatch.
- Core assumption: The knowledge exists in internal representations but requires additional training to properly manifest in outputs.
- Evidence anchors:
  - [section 3.2] "Flan T5 outperforms the T5 in terms of query accuracy, proving that finetuning indeed enhances model's ability to direct the knowledge to the output."
  - [section 3.2] "However, although the accuracy is boosted from 50% to 59%, probing still shows a better performance."
  - [corpus] Weak evidence - no direct corpus support for this mechanism, only indirect support from related work on model calibration.

### Mechanism 3
- Claim: Calibration techniques improve the alignment between model confidence and accuracy, indirectly reducing internal-external mismatch.
- Mechanism: Better calibration ensures that the model's confidence in its outputs better reflects its actual knowledge, which may help propagate internal semantic knowledge more effectively.
- Core assumption: Poor calibration contributes to the internal-external mismatch by causing the model to express uncertainty inappropriately.
- Evidence anchors:
  - [section 3.3] "We demonstrate the confidence and accuracy of three models on the WIC task in Figure 1; probe are better calibrated than queries."
  - [abstract] "Finetuning and calibration can improve the discrepancy but not eliminate it."
  - [corpus] Weak evidence - no direct corpus support for this mechanism, only indirect support from related work on model calibration.

## Foundational Learning

- Concept: Linear probing as an evaluation technique
  - Why needed here: The paper uses linear probing to access internal representations without modifying model parameters, providing a baseline for semantic understanding.
  - Quick check question: What is the key difference between probing and finetuning in terms of parameter modification?

- Concept: Word embeddings and contextual meaning
  - Why needed here: The study evaluates word semantic understanding across different tasks, requiring understanding of how word meanings vary by context.
  - Quick check question: How do contextual embeddings differ from static word embeddings in representing word meaning?

- Concept: Calibration in machine learning
  - Why needed here: The paper investigates calibration as a potential solution to internal-external mismatch, requiring understanding of confidence-accuracy alignment.
  - Quick check question: What does it mean for a model to be well-calibrated in terms of confidence and accuracy?

## Architecture Onboarding

- Component map: BERT (Encoder-only) -> GPT-2 (Decoder-only) -> T5 (Encoder-Decoder) across WiC -> CoNLL2003 -> BATS tasks
- Critical path: Internal representation → Linear classifier (probing) vs. Full inference → Output distribution (querying)
- Design tradeoffs: Probing provides direct access to internal knowledge but may not reflect actual model usage; querying reflects real usage but may underutilize internal knowledge
- Failure signatures: Large gaps between probing and querying accuracy across multiple tasks and models indicate systematic internal-external mismatch
- First 3 experiments:
  1. Replicate probing vs querying comparison on WiC task to verify internal-external mismatch
  2. Apply calibration techniques to see if confidence-accuracy alignment improves
  3. Test finetuning on task-specific data to measure impact on internal-external gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size of language models affect the internal-external mismatch in word semantics understanding?
- Basis in paper: [inferred] The paper mentions that "Other factors like model size also contribute to the discrepancy" but does not explore this in detail.
- Why unresolved: The paper does not investigate the relationship between model size and the internal-external mismatch, leaving this as an open question.
- What evidence would resolve it: Conducting experiments with models of varying sizes to measure the internal-external mismatch across different model scales.

### Open Question 2
- Question: Can alternative probing methods reduce the internal-external mismatch more effectively than linear probing?
- Basis in paper: [explicit] The paper uses linear probing and notes that finetuning or calibration help to improve the accuracy to some extent, but not on par with probe accuracy.
- Why unresolved: The paper does not explore other probing methods that might better align internal representations with external outputs.
- What evidence would resolve it: Comparing the effectiveness of different probing techniques (e.g., non-linear probes) in reducing the internal-external mismatch.

### Open Question 3
- Question: How do different types of prompts affect the internal-external mismatch in word semantics understanding?
- Basis in paper: [explicit] The paper uses various prompts for different tasks and notes that there is a significant gap between probes and queries.
- Why unresolved: The paper does not systematically investigate how variations in prompt design might influence the internal-external mismatch.
- What evidence would resolve it: Conducting experiments with a wide range of prompt designs to assess their impact on the internal-external mismatch across different tasks.

## Limitations
- Probing methodology relies on frozen models with linear classifiers, which may not fully capture how internal representations encode semantic knowledge
- Querying approach uses hand-crafted prompts, introducing variability based on prompt engineering quality
- Study focuses on word-level semantics rather than broader semantic understanding, limiting generalizability to more complex linguistic phenomena

## Confidence

- **High Confidence**: The core finding that probing consistently outperforms querying across all tested models and tasks
- **Medium Confidence**: The mechanism explanation that internal representations contain richer semantic information than what propagates to outputs
- **Medium Confidence**: The effectiveness of finetuning and calibration in reducing but not eliminating the internal-external gap

## Next Checks
1. Cross-dataset validation: Test the internal-external mismatch hypothesis on additional semantic tasks beyond the three studied to determine if the pattern generalizes to other forms of semantic understanding.

2. Architectural ablation study: Compare the internal-external gap across different model sizes and training objectives within each architecture family to isolate whether the mismatch stems from architectural constraints or optimization limitations.

3. Prompt sensitivity analysis: Systematically vary the querying prompts across multiple semantic tasks to quantify how much of the probing-querying gap can be attributed to prompt quality versus fundamental information propagation limitations.