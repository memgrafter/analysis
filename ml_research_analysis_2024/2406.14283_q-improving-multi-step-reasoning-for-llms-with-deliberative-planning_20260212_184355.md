---
ver: rpa2
title: 'Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning'
arxiv_id: '2406.14283'
source_url: https://arxiv.org/abs/2406.14283
tags:
- reasoning
- arxiv
- llms
- math
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Q, a general framework for improving multi-step
  reasoning in large language models (LLMs) by leveraging deliberative planning. The
  core idea is to cast the multi-step reasoning process of LLMs as a Markov Decision
  Process (MDP) and use A search with learned Q-value models as heuristics to guide
  the LLM's decoding process.
---

# Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning

## Quick Facts
- arXiv ID: 2406.14283
- Source URL: https://arxiv.org/abs/2406.14283
- Reference count: 40
- Q* helps Llama-2-7b achieve 80.8% accuracy on GSM8K, surpassing ChatGPT-turbo

## Executive Summary
Q* is a general framework that improves multi-step reasoning in large language models by leveraging deliberative planning. It frames the reasoning process as a Markov Decision Process and uses A* search with learned Q-value models as heuristics to guide the LLM's decoding process. The approach avoids fine-tuning the LLM for specific tasks, reducing computational overhead and the risk of performance degradation on other tasks. Extensive experiments demonstrate Q*'s effectiveness across multiple reasoning datasets.

## Method Summary
Q* casts multi-step reasoning as a Markov Decision Process where states are partial reasoning traces and actions are next reasoning steps. It uses A* search with learned Q-value models as heuristics to guide the LLM's decoding process. The Q-value models are trained using offline reinforcement learning or by completing trajectories with stronger LLMs. Process-based rewards provide intermediate feedback on reasoning quality rather than just rewarding final correct answers. This approach avoids the need for fine-tuning the LLM for specific tasks.

## Key Results
- Q* helps Llama-2-7b achieve 80.8% accuracy on GSM8K, surpassing ChatGPT-turbo
- The framework shows consistent improvements across GSM8K, MATH, and MBPP datasets
- Q* provides computational efficiency by using single-step expansion compared to MCTS rollouts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Q* improves multi-step reasoning by framing it as an MDP and using A* search with learned Q-values as heuristics
- Mechanism: The framework converts reasoning into states and actions, using A* search where f-value combines accumulated utility and learned heuristic estimating future rewards
- Core assumption: Optimal Q-value can be approximated using offline reinforcement learning, rollouts, or stronger LLMs
- Evidence anchors: Abstract mentions casting reasoning as heuristic search problem and learning plug-and-play Q-value models; section formalizes reasoning as MDP and casts it as heuristic search
- Break condition: Learned Q-value model becomes a poor heuristic, causing A* to select suboptimal reasoning paths

### Mechanism 2
- Claim: Process-based rewards guide reasoning quality better than outcome-only rewards
- Mechanism: Q* uses process-based reward functions that provide intermediate feedback on reasoning steps, helping A* distinguish promising early traces from those that will fail later
- Core assumption: Process-based rewards provide meaningful intermediate signals correlating with final answer quality
- Evidence anchors: Section proposes using process-based reward function encoding prior knowledge; abstract mentions avoiding fine-tuning to prevent performance degeneration
- Break condition: Process-based rewards become noisy or misaligned with actual reasoning quality

### Mechanism 3
- Claim: Using single step expansion in A* is computationally efficient compared to MCTS rollouts
- Mechanism: Q* only needs to expand one step at a time using the Q-value heuristic, dramatically reducing computational overhead while maintaining search quality
- Core assumption: Q-value heuristic provides sufficient guidance for A* to find good reasoning paths without extensive rollouts
- Evidence anchors: Section states Q* considers only one single step when performing deliberation; abstract mentions effectively guiding LLMs without fine-tuning
- Break condition: Single-step expansion becomes insufficient for complex reasoning tasks with long-term dependencies

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation
  - Why needed here: The paper explicitly frames multi-step reasoning as an MDP where states are reasoning traces and actions are next steps
  - Quick check question: In the Q* framework, what constitutes a "state" and what constitutes an "action" in the MDP formulation?

- Concept: A* search algorithm with heuristic functions
  - Why needed here: Q* builds directly on A* search, using learned Q-values as the heuristic function
  - Quick check question: How does Q* modify the standard A* f-value calculation, and what are the two components being combined?

- Concept: Reinforcement learning and Q-learning
  - Why needed here: Q* estimates optimal Q-values using techniques from reinforcement learning
  - Quick check question: What are the three methods Q* uses to estimate optimal Q-values, and how do they differ in terms of data requirements?

## Architecture Onboarding

- Component map: LLM policy πθ (frozen base model) → Q-value model ˆQ (learned heuristic) → A* search planner → Best trajectory selector
- Critical path: State expansion → Q-value computation → f-value calculation → Best-first selection → Terminal state detection
- Design tradeoffs: Q* trades computational efficiency (single-step expansion vs MCTS rollouts) for potentially reduced exploration; trades fine-tuning flexibility for complexity of learning accurate Q-value heuristics
- Failure signatures: Poor reasoning performance indicates either Q-value model inaccuracy, inappropriate process rewards, or insufficient search breadth
- First 3 experiments:
  1. Implement basic A* with constant heuristic on GSM8K to verify search framework works before adding Q-values
  2. Train Q-value model using offline RL on small dataset and test if it improves over random search
  3. Compare single-step A* expansion vs MCTS rollout on simple reasoning task to validate computational efficiency claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Q* scale with increasing reasoning depth and complexity compared to other methods like Best-of-N or PPO-based approaches?
- Basis in paper: The paper mentions Q* is designed to handle multi-step reasoning problems effectively but lacks extensive analysis on scaling behavior
- Why unresolved: Paper primarily focuses on demonstrating effectiveness rather than comparative analysis on scaling
- What evidence would resolve it: Experiments on tasks with varying reasoning depths comparing Q* with Best-of-N, PPO, or MCTS-based approaches

### Open Question 2
- Question: How does the choice of aggregation function (min, max, sum, etc.) impact Q*'s performance across different reasoning tasks?
- Basis in paper: Paper mentions using different aggregation functions but doesn't provide comprehensive analysis of their impact
- Why unresolved: While briefly mentioning aggregation functions, paper doesn't explore effects in detail or provide guidelines
- What evidence would resolve it: Experiments using different aggregation functions on various reasoning tasks analyzing their impact

### Open Question 3
- Question: How does Q* perform when the underlying LLM policy πθ is suboptimal or biased towards certain reasoning patterns?
- Basis in paper: Paper discusses learning Q-values from potentially suboptimal LLM policy but doesn't extensively explore performance with significantly suboptimal or biased policies
- Why unresolved: Paper assumes availability of reasonable LLM policy but doesn't investigate highly suboptimal or biased scenarios
- What evidence would resolve it: Experiments using LLM policies with varying degrees of suboptimality or bias evaluating Q*'s performance

## Limitations
- Computational efficiency gains over MCTS aren't quantified with runtime comparisons
- Limited ablation studies on impact of different reward functions and Q-value estimation methods
- No analysis of failure cases or conditions under which Q* framework degrades

## Confidence
- **High confidence**: MDP formulation and A* search framework are well-established methods correctly applied
- **Medium confidence**: Three methods for estimating Q-values are plausible but relative effectiveness isn't systematically compared
- **Low confidence**: Scalability analysis is limited - strong results on 7B parameter models but doesn't demonstrate effectiveness on larger models or more complex tasks

## Next Checks
1. Conduct systematic ablation studies comparing the three Q-value estimation methods (offline RL, rollouts, stronger LLM completion) on the same tasks
2. Measure and report actual inference time comparisons between Q* and MCTS approaches across different reasoning task complexities
3. Test the framework's robustness by applying it to reasoning tasks outside math/code domains (e.g., commonsense reasoning or multi-step planning) to assess generalizability