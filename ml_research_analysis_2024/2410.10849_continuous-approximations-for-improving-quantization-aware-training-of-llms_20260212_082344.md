---
ver: rpa2
title: Continuous Approximations for Improving Quantization Aware Training of LLMs
arxiv_id: '2410.10849'
source_url: https://arxiv.org/abs/2410.10849
tags:
- arxiv
- quantization
- preprint
- training
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of improving Quantization-Aware
  Training (QAT) for Large Language Models (LLMs) to reduce their energy consumption
  and computational requirements. The authors introduce two continuous approximations
  to enhance QAT: Sigmoid STE for the rounding function and SoftClamp for the clamping
  function.'
---

# Continuous Approximations for Improving Quantization Aware Training of LLMs

## Quick Facts
- arXiv ID: 2410.10849
- Source URL: https://arxiv.org/abs/2410.10849
- Reference count: 28
- 4-bit quantized LLaMA-3-8B achieves 9.0815 PPL on WikiText-v2 using Sigmoid STE and SoftClamp

## Executive Summary
This paper addresses the challenge of improving Quantization-Aware Training (QAT) for Large Language Models (LLMs) by introducing two continuous approximations: Sigmoid STE for the rounding function and SoftClamp for the clamping function. These methods replace the traditional Straight-Through Estimator (STE) and hard clamping with smooth, differentiable alternatives that enable better gradient flow during training. The approach achieves significant improvements in perplexity on WikiText-v2 and downstream task performance on BoolQ and MMLU datasets, while maintaining the same precision, model size, and training setup as baseline QAT.

## Method Summary
The authors introduce two continuous approximations to enhance QAT: Sigmoid STE for the rounding function and SoftClamp for the clamping function. Sigmoid STE uses a sigmoid-based approximation to replace the hard rounding operation, controlled by a temperature parameter T that determines the approximation sharpness. SoftClamp employs a smooth, non-linear function based on SwiGLU activation to replace hard clamping, enabling better learning of the scaling factor s. By applying both methods together, the quantized model achieves improved perplexity and downstream task performance through more effective learning of both weights and scaling factors during the QAT process.

## Key Results
- WikiText-v2 perplexity improves from 9.9621 (baseline) to 9.0815 with combined Sigmoid STE and SoftClamp
- BoolQ accuracy improves by 2.76% compared to baseline QAT
- MMLU performance improves by 5.47% on average across tasks
- Performance improves with increasing T parameter, from 9.7194 (T=0) to 9.0815 (T=100)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sigmoid STE improves QAT by providing a smooth gradient approximation for rounding, reducing gradient error
- Mechanism: Replaces STE's hard rounding with sigmoid-based approximation, allowing non-zero gradients across function domain. Temperature T controls sharpness, with higher values providing better rounding function approximation.
- Core assumption: STE gradient error negatively impacts model performance and can be mitigated through continuous approximations
- Evidence anchors: Abstract mentions introducing continuous approximations to STE; section discusses Sigmoid STE as continuous rounding approximation with experimental support [14]
- Break condition: When T is too low, approximation becomes too smooth and loses quantization effect; when T is too high, numerical instability may occur

### Mechanism 2
- Claim: SoftClamp provides smooth, differentiable alternative to hard clamping, enabling better learning of scaling factor s
- Mechanism: Replaces hard min/max clamping with smooth function based on SwiGLU activation, creating continuous gradients for scaling factor s and preventing gradient vanishing
- Core assumption: Hard clamping creates zero gradients for scaling factor in certain regions, preventing optimal learning of s
- Evidence anchors: Abstract mentions clamping function as key target; section provides SoftClamp formulation and shows gradient vanishing under conventional method
- Break condition: When SwiGLU activation is too smooth, it may not effectively constrain values to quantization range

### Mechanism 3
- Claim: Combining Sigmoid STE and SoftClamp creates complementary improvements enabling more effective learning of weights and scaling factors
- Mechanism: Two methods work synergistically - Sigmoid STE improves gradient flow through rounding while SoftClamp improves gradient flow through clamping, creating more complete gradient landscape
- Core assumption: Two continuous approximations address orthogonal aspects of quantization process, providing multiplicative benefits
- Evidence anchors: Abstract shows PPL improvement from 9.9621 to 9.0815; section demonstrates PPL decrease with increasing T; section explains efficient adaptation of step size and weights
- Break condition: When combined approximation becomes too smooth, it may lose discrete nature of quantization, potentially degrading performance

## Foundational Learning

- Concept: Quantization-aware training (QAT) fundamentals
  - Why needed here: Understanding how QAT simulates quantization during training and challenges of non-differentiable operations is essential to grasp why continuous approximations help
  - Quick check question: What is the purpose of STE in QAT, and why is it necessary?

- Concept: Activation functions and their derivatives
  - Why needed here: SoftClamp uses SwiGLU activation, and understanding activation function properties is crucial for implementing and tuning approach
  - Quick check question: How does derivative of smooth activation function differ from hard threshold function, and why does this matter for gradient-based learning?

- Concept: Scaling factor selection in quantization
  - Why needed here: Paper discusses how different scaling factor selection methods affect quantization performance, and SoftClamp specifically targets improving scaling factor learning
  - Quick check question: What is relationship between scaling factor s and trade-off between quantization range and precision?

## Architecture Onboarding

- Component map: Input preprocessing → Quantization simulation (Sigmoid STE + SoftClamp) → Forward pass → Loss computation → Backward pass (through continuous approximations) → Weight updates
- Critical path: Forward pass with quantization simulation → Loss computation → Backward pass through continuous approximations → Scaling factor and weight updates
- Design tradeoffs: Smoothness vs. quantization fidelity (more continuous approximations improve gradient flow but may reduce discrete nature); Parameter T tuning (higher T provides better approximation but may cause numerical instability); Computational overhead (continuous approximations add computation vs standard STE)
- Failure signatures: Training instability or divergence (may indicate T is too high or approximations are too smooth); No improvement over baseline (may indicate improper implementation or model too small to benefit); Gradient vanishing (may indicate SoftClamp not properly implemented or activation function too smooth)
- First 3 experiments: 1) Implement Sigmoid STE alone with T=5 and compare perplexity to baseline on WikiText-v2 to verify rounding approximation improves performance; 2) Implement SoftClamp alone and verify gradients for scaling factor s are non-zero when inputs within quantization range; 3) Combine both methods with T=10 and T=100 to find optimal balance between approximation quality and numerical stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do proposed continuous approximations perform on larger datasets or with longer training times, given current experiments limited by hardware constraints?
- Basis in paper: Explicit - Paper mentions hardware and time limits resulted in relatively small dataset and unexplored precision combinations
- Why unresolved: Experiments constrained by available resources, preventing comprehensive evaluation across diverse datasets and training durations
- What evidence would resolve it: Conducting experiments with larger datasets, extended training times, and varying precision combinations to compare performance metrics

### Open Question 2
- Question: What is impact of different activation functions used in SoftClamp (e.g., GeLU, SiLU, SwiGLU) on overall performance of quantization-aware training?
- Basis in paper: Explicit - Paper mentions using SwiGLU with β = 1 but does not explore other activation functions
- Why unresolved: Study only tested one specific activation function in SoftClamp mechanism, leaving effects of alternative functions unexamined
- What evidence would resolve it: Systematically testing various activation functions within SoftClamp and analyzing effects on model performance and stability during QAT

### Open Question 3
- Question: How does choice of scaling factor selection method (e.g., MinMax, learned scaling factor) influence effectiveness of continuous approximations in QAT?
- Basis in paper: Explicit - Paper discusses selection of scaling factors but does not investigate how different methods affect proposed continuous approximations
- Why unresolved: Study did not explore interaction between different scaling factor selection methods and continuous approximations introduced
- What evidence would resolve it: Experimenting with various scaling factor selection methods in conjunction with Sigmoid STE and SoftClamp to determine impact on quantization accuracy and model performance

### Open Question 4
- Question: Are there potential improvements in energy efficiency and computational requirements when applying these continuous approximations to larger, more complex LLMs beyond LLaMA-3-8B model tested?
- Basis in paper: Inferred - Paper discusses contributions to energy-efficient LLM technology but only tested on single model
- Why unresolved: Experiments limited to LLaMA-3-8B model, so generalizability of improvements to larger models is unknown
- What evidence would resolve it: Applying continuous approximations to range of LLMs with varying sizes and complexities, then measuring changes in energy consumption and computational efficiency

## Limitations

- Evaluation scope limited to single model architecture (LLaMA-3-8B) and specific datasets, raising questions about generalization to other models and domains
- Critical implementation details underspecified, particularly temperature parameter T for Sigmoid STE and exact SoftClamp configuration
- Training dynamics and sensitivity to hyperparameters not thoroughly analyzed, leaving questions about stability and optimal settings

## Confidence

**High confidence**: Core claim that continuous approximations improve gradient flow in QAT is well-supported by theoretical reasoning and experimental evidence; mechanism descriptions for Sigmoid STE and SoftClamp are mathematically sound and align with established principles of differentiable approximations

**Medium confidence**: Specific performance improvements (9.08 PPL vs 9.96 baseline, 2.76% BoolQ improvement, 5.47% MMLU improvement) are well-documented for tested configuration but may not generalize across different models and tasks; synergistic effect of combining both methods is supported but could benefit from more extensive ablation studies

**Low confidence**: Claims about energy efficiency improvements and computational requirements are not directly measured or validated; paper mentions these benefits as motivations but doesn't provide quantitative evidence or comparisons to alternative quantization approaches

## Next Checks

- Implement Sigmoid STE with T=5, T=20, and T=50 on LLaMA-3-8B and measure perplexity on WikiText-v2 to determine optimal temperature parameter and verify relationship between T and performance improvements
- Test combined approach (Sigmoid STE + SoftClamp) on different model architecture (e.g., Mistral-7B) and dataset (e.g., C4) to assess generalization beyond original evaluation setup
- Measure training time per epoch and memory consumption with and without continuous approximations to empirically validate claims about computational overhead and energy efficiency impacts