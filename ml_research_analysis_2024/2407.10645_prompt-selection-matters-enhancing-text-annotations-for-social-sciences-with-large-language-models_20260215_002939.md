---
ver: rpa2
title: 'Prompt Selection Matters: Enhancing Text Annotations for Social Sciences with
  Large Language Models'
arxiv_id: '2407.10645'
source_url: https://arxiv.org/abs/2407.10645
tags:
- prompt
- prompts
- text
- have
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that the choice of prompt significantly
  affects the accuracy of large language models in social science text annotation
  tasks. Automatic prompt optimization was applied to systematically generate high-quality
  prompts, outperforming hand-crafted prompts on most tested tasks.
---

# Prompt Selection Matters: Enhancing Text Annotations for Social Sciences with Large Language Models

## Quick Facts
- arXiv ID: 2407.10645
- Source URL: https://arxiv.org/abs/2407.10645
- Reference count: 15
- Key outcome: Automatic prompt optimization systematically outperforms hand-crafted prompts in social science text annotation tasks, with accuracy differences up to 47% between prompts.

## Executive Summary
This study investigates how prompt selection affects large language model performance in social science text annotation tasks. The authors systematically compare hand-crafted prompts with automatically optimized prompts across multiple classification tasks including hate speech detection, sentiment analysis, and political bias identification. Automatic prompt optimization (APO) was found to consistently outperform hand-crafted prompts on most tasks, demonstrating that prompt choice significantly impacts accuracy. The study also highlights challenges including potential training data overlap and the need for careful evaluation methodology.

## Method Summary
The research compares five hand-crafted prompt formats (Simple, Explanations, Examples, Roleplay, Chain of Thoughts) against automatically optimized prompts using GPT-3.5 Turbo API. The automatic optimization method iteratively generates and evaluates prompt variations on a validation subset, selecting those yielding highest performance. Datasets include TweetEval (hate, emotion, sentiment, offensive), Tweet Sentiment Multilingual, Article Bias Prediction, and Liberals vs Conservatives on Reddit. Performance is measured using accuracy and F1 scores across test sets.

## Key Results
- Automatic prompt optimization consistently outperformed hand-crafted prompts on most tasks
- Accuracy varied widely between prompts, with differences up to 47% in error rates
- Automatic method achieved high performance without manual tuning
- Performance discrepancies between training and test sets suggest potential training data overlap

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The choice of prompt significantly affects LLM annotation accuracy in social science tasks.
- Mechanism: LLMs rely on prompt phrasing to interpret the task; slight changes in wording or structure alter the model's internal reasoning path, leading to large variance in output accuracy.
- Core assumption: The LLM's output is highly sensitive to the semantic and syntactic structure of the prompt.
- Evidence anchors:
  - [abstract] "performance greatly varies between prompts" and "accuracy varied widely between prompts, with differences up to 47% in error rates"
  - [section] "we observe that apparently similar prompts yield greatly varied accuracy levels"
  - [corpus] weak: only general prompt-tuning literature exists, no specific causal analysis cited
- Break condition: If LLM architecture shifts to ignore prompt context (e.g., direct task inference), sensitivity to prompt phrasing would drop.

### Mechanism 2
- Claim: Automatic prompt optimization systematically improves accuracy by exploring prompt variations.
- Mechanism: The method generates and evaluates multiple prompt variants on a validation subset, selecting those that yield highest performance; this iterative process refines prompt wording toward optimal phrasing.
- Core assumption: Performance variance across prompt variants is measurable and optimizable on a subset of labeled data.
- Evidence anchors:
  - [abstract] "automatic prompt optimization was applied to systematically generate high-quality prompts, outperforming hand-crafted prompts"
  - [section] "automatic prompt optimization yields consistently good performance and beats prompt-crafting heuristics"
  - [corpus] weak: no quantitative comparison of APO vs hand-crafted prompts from independent studies
- Break condition: If the optimization loop converges to local minima or if validation subset is not representative, gains may plateau or degrade.

### Mechanism 3
- Claim: Performance differences between training and test sets may arise from training data overlap, not model capability.
- Mechanism: LLMs trained on web data may have seen training set examples during pre-training, causing inflated accuracy when tested on those examples.
- Core assumption: The training set used in experiments is included in the LLM's pre-training corpus.
- Evidence anchors:
  - [section] "all prompts result in much higher accuracy when tested on the training set of the TE-hate dataset than on its test set"
  - [section] "we suspect that some of the most impressive accuracy scores...might be partially explained by this"
  - [corpus] weak: no public transparency into exact training data composition
- Break condition: If the model is trained on truly out-of-distribution data or if training/test splits are temporally disjoint, this effect would diminish.

## Foundational Learning

- Concept: Prompt engineering and semantic framing
  - Why needed here: The core finding is that prompt phrasing directly impacts model accuracy; understanding how wording affects LLM behavior is critical.
  - Quick check question: If you rephrase "Is this tweet offensive?" to "Does this tweet contain offensive language?", will accuracy necessarily change? Why or why not?

- Concept: Cross-validation and bias in evaluation
  - Why needed here: The paper uses separate validation subsets to avoid bias; understanding this prevents over-optimistic performance estimates.
  - Quick check question: What could go wrong if you use the same data for both prompt optimization and final accuracy measurement?

- Concept: Domain adaptation of LLMs without fine-tuning
  - Why needed here: The study applies general LLMs to social science tasks without retraining; knowing the limits of zero-shot learning is key.
  - Quick check question: Why might a general-purpose LLM perform worse on niche social science tasks compared to a fine-tuned model?

## Architecture Onboarding

- Component map:
  - Prompt generation engine (APO loop) -> Evaluation subset (labeled validation data) -> LLM inference API (ChatGPT/OpenAI) -> Result aggregation and accuracy calculation -> Web UI for manual testing and optimization

- Critical path:
  1. Upload labeled dataset
  2. Define initial prompt
  3. Run APO iterations (prompt generation → evaluation → selection)
  4. Deploy best prompt on full dataset
  5. Compute final accuracy

- Design tradeoffs:
  - APO increases API calls and cost but reduces manual prompt crafting effort
  - Using frozen LLM versions improves replicability but may limit access to newer capabilities
  - Browser-based UI simplifies adoption but may limit scalability for very large datasets

- Failure signatures:
  - Accuracy plateaus quickly during APO (possible local optimum)
  - High variance between validation and final test sets (data leakage or non-representative split)
  - Consistently low accuracy across all prompts (task may be out of distribution for the LLM)

- First 3 experiments:
  1. Run APO on a small labeled subset of an easy task (e.g., TE-sent) to observe prompt variance.
  2. Compare APO-generated prompt vs a hand-crafted prompt on the same task to verify performance gain.
  3. Test the same prompt on training vs test sets to detect possible training data leakage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum dataset size required for automatic prompt optimization to consistently outperform hand-crafted prompts?
- Basis in paper: [explicit] The paper mentions using "a few hundred or a few thousand messages" for the optimization subset, but doesn't specify the minimum effective size.
- Why unresolved: The paper tested with 400 samples for prompt evaluation during optimization but didn't systematically vary the subset size to determine the threshold for reliable performance.
- What evidence would resolve it: Controlled experiments varying the labeled subset size (e.g., 100, 200, 400, 800 samples) and measuring the consistency and quality of optimized prompts across different dataset sizes.

### Open Question 2
- Question: How do different LLM architectures (beyond GPT-3.5 Turbo) affect the performance of automatic prompt optimization in social science text annotation?
- Basis in paper: [inferred] The paper only tested GPT-3.5 Turbo, though it mentions various LLMs like GPT-4 exist and have different training characteristics.
- Why unresolved: The paper doesn't compare performance across different LLM architectures or consider how model-specific training data might influence optimization outcomes.
- What evidence would resolve it: Comparative experiments using multiple LLM architectures (GPT-4, Claude, LLaMA variants) on the same tasks, measuring both raw accuracy and optimization effectiveness.

### Open Question 3
- Question: Can the automatic prompt optimization process be made more efficient by incorporating meta-learning or transfer learning approaches?
- Basis in paper: [explicit] The paper notes the current method requires "increased number of calls to the chatbot API" as a downside.
- Why unresolved: The paper presents a straightforward evolutionary approach but doesn't explore whether prior knowledge from similar tasks could accelerate optimization.
- What evidence would resolve it: Experiments comparing standard optimization with variants that leverage prompts from similar tasks or use meta-learning to initialize the search, measuring convergence speed and final performance.

### Open Question 4
- Question: How do systematic biases in LLM training data manifest in automatic text annotation for different social science domains?
- Basis in paper: [explicit] The paper discusses potential political biases and mentions that LLMs are known to suffer from certain biases, but doesn't systematically analyze bias patterns across tasks.
- Why unresolved: While the paper mentions bias as a concern, it doesn't provide empirical analysis of how different types of bias (political, cultural, demographic) affect annotation accuracy across different social science domains.
- What evidence would resolve it: Comprehensive bias audits using diverse test datasets designed to probe different bias dimensions, measuring both overall accuracy and systematic error patterns across demographic and topical dimensions.

## Limitations
- Limited transparency into exact training data composition makes it difficult to determine if accuracy gains stem from genuine prompt effectiveness or training data overlap
- Automatic optimization method lacks detailed methodological specification, preventing exact replication
- Performance gains are task-dependent, with automatic optimization underperforming on TE-hate and AS-pol tasks despite working well on others

## Confidence
- **High confidence**: The observation that prompt selection significantly affects LLM accuracy (supported by up to 47% error rate differences)
- **Medium confidence**: That automatic prompt optimization consistently outperforms hand-crafted prompts (limited by lack of methodological transparency and mixed results across tasks)
- **Low confidence**: The claim that automatic optimization eliminates the need for manual prompt engineering (contradicted by instances where manual prompts performed comparably or better)

## Next Checks
1. **Training data contamination verification**: Compare model performance on training versus test sets for each task to quantify potential accuracy inflation from pre-training data overlap, particularly for high-performing prompts.

2. **APO algorithm specification**: Request or reconstruct the exact implementation details of the automatic prompt optimization method, including the macroprompt structure and iteration criteria, to enable reproducible experiments.

3. **Cross-LLM validation**: Test the same prompt optimization approach across different LLM architectures (e.g., Claude, LLaMA) to determine whether observed prompt sensitivity is model-specific or a general phenomenon.