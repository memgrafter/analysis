---
ver: rpa2
title: Reinforcement Learning applied to Insurance Portfolio Pursuit
arxiv_id: '2408.00713'
source_url: https://arxiv.org/abs/2408.00713
tags:
- portfolio
- customer
- value
- problem
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the portfolio pursuit problem in insurance:
  how to modulate individual customer offers to achieve a desired portfolio composition
  over time. The authors formulate this as a Markov Decision Process and develop a
  novel reinforcement learning algorithm to solve it.'
---

# Reinforcement Learning applied to Insurance Portfolio Pursuit

## Quick Facts
- arXiv ID: 2408.00713
- Source URL: https://arxiv.org/abs/2408.00713
- Authors: Edward James Young; Alistair Rogers; Elliott Tong; James Jordon
- Reference count: 4
- One-line primary result: RL algorithm generates 7% more profit (£472 on average) while maintaining comparable portfolio quality

## Executive Summary
This paper addresses the portfolio pursuit problem in insurance: how to modulate individual customer offers to achieve a desired portfolio composition over time. The authors formulate this as a Markov Decision Process and develop a novel reinforcement learning algorithm to solve it. Their approach uses a dynamic programming method with function approximation, learning both a policy that modulates offers based on current portfolio composition and a value function that estimates the long-term benefit of different portfolios. The algorithm trains offline using a model-based approach, making it safer for deployment.

## Method Summary
The method formulates portfolio pursuit as a Markov Decision Process where the insurer's actions at each time step affect future states and rewards. The algorithm uses dynamic programming with function approximation to learn a policy that modulates customer prices based on current portfolio composition. It trains offline using a model-based approach by first learning auxiliary models (market model, conversion model, action model) from historical data, then using these models to simulate the environment and train the value function through backward induction. At inference time, the algorithm computes a k-value representing the trade-off between immediate profit and portfolio composition, then uses this k-value along with market variables to determine the optimal offer price.

## Key Results
- RL algorithm outperforms baseline mimicking current industry practices by generating 7% more profit (£472 on average over 1000 interactions)
- Maintains comparable portfolio quality while achieving higher profit
- Method can be integrated naturally with existing machine learning pipelines for insurance pricing

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The RL algorithm learns to modulate individual customer offers to achieve a desired portfolio composition over time by dynamically adjusting prices based on current portfolio state.
- **Mechanism:** The algorithm uses a dynamic programming approach with function approximation to learn both a policy that modulates offers based on current portfolio composition and a value function that estimates the long-term benefit of different portfolios. At each time step, the algorithm computes a k-value that represents the trade-off between immediate profit and portfolio composition, then uses this k-value along with market variables to determine the optimal offer price.
- **Core assumption:** The portfolio value function can be accurately approximated as a function of portfolio composition and time step, and the optimal policy depends only on market variables and k-value rather than the full state.
- **Evidence anchors:**
  - [abstract] "Their approach uses a dynamic programming method with function approximation, learning both a policy that modulates offers based on current portfolio composition and a value function that estimates the long-term benefit of different portfolios."
  - [section] "Our reinforcement learning algorithm uses a dynamic programming approach with function approximation to provide a policy which modulates customer prices according to the current portfolio possessed by the insurer."
  - [corpus] No direct corpus evidence found for this specific mechanism.
- **Break condition:** The approximation of the value function breaks down if the portfolio state space becomes too complex or if customer behavior changes significantly over time in ways not captured by the model.

### Mechanism 2
- **Claim:** The model-based approach allows the algorithm to be trained entirely offline, making it safer for deployment in real insurance markets.
- **Mechanism:** The algorithm trains offline using a model-based approach by first learning auxiliary models (market model, conversion model, action model) from historical data, then using these models to simulate the environment and train the value function through backward induction. This avoids the need for online exploration in the real market.
- **Core assumption:** The auxiliary models learned from historical data accurately represent the true market dynamics and customer behavior.
- **Evidence anchors:**
  - [abstract] "The algorithm trains offline using a model-based approach, making it safer for deployment."
  - [section] "We take a model-based approach, allowing our algorithm to be trained entirely offline, thereby protecting against risk in deployment."
  - [corpus] No direct corpus evidence found for this specific mechanism.
- **Break condition:** The model-based approach fails if the historical data becomes stale or if market conditions change significantly after the model is trained.

### Mechanism 3
- **Claim:** The algorithm outperforms baseline methods by generating more profit while maintaining comparable portfolio quality through better optimization of the trade-off between immediate profit and long-term portfolio goals.
- **Mechanism:** The RL algorithm explicitly optimizes the total reward (profit minus portfolio loss) over the entire time horizon, whereas baseline methods use heuristic approaches to modulate prices. By learning the optimal trade-off through dynamic programming, the algorithm can make more informed decisions about when to accept lower profits to achieve better portfolio composition.
- **Core assumption:** The reward function (profit minus portfolio loss) accurately captures the true business objectives of the insurance firm.
- **Evidence anchors:**
  - [abstract] "Experiments in a complex synthetic market environment show that their method outperforms a baseline mimicking current industry practices, generating 7% more profit (£472 on average over 1000 interactions) while maintaining comparable portfolio quality."
  - [section] "We see that our method outperforms the baseline, achieving higher reward on average over the trials."
  - [corpus] No direct corpus evidence found for this specific mechanism.
- **Break condition:** The algorithm's performance degrades if the portfolio loss function does not accurately reflect the true business objectives or if the market environment differs significantly from the synthetic environment used for testing.

## Foundational Learning

- **Concept:** Markov Decision Processes (MDPs)
  - Why needed here: The portfolio pursuit problem is formulated as a sequential decision-making problem where the insurer's actions at each time step affect future states and rewards, which is naturally modeled as an MDP.
  - Quick check question: What are the key components of an MDP and how do they relate to the insurance portfolio pursuit problem?

- **Concept:** Dynamic Programming with Function Approximation
  - Why needed here: Exact dynamic programming is intractable for this problem due to the large state space, so function approximation is used to estimate the value function and policy.
  - Quick check question: How does function approximation enable dynamic programming to scale to problems with large state spaces?

- **Concept:** Model-Based Reinforcement Learning
  - Why needed here: The algorithm uses a model-based approach to train offline using historical data, which is safer for deployment in real insurance markets compared to model-free approaches.
  - Quick check question: What are the advantages and disadvantages of model-based vs. model-free reinforcement learning approaches?

## Architecture Onboarding

- **Component map:** Customer features -> Market Model -> Market Variables -> Conversion Model + Action Model -> Acceptance Probability + Offer Price -> Customer Reply -> Portfolio State
- **Critical path:**
  1. Train auxiliary models (market, conversion, action) on historical data
  2. Initialize value function and next-step value function
  3. Iterate backward through time to estimate portfolio values
  4. Fit value function to estimated values
  5. At inference time, compute k-value and use policy to determine optimal action

- **Design tradeoffs:**
  - Model-based vs. model-free: Model-based allows offline training but requires accurate auxiliary models
  - Function approximation vs. tabular methods: Function approximation scales better but may introduce approximation errors
  - On-policy vs. off-policy: Off-policy sampling is necessary due to lack of on-policy data but may introduce bias

- **Failure signatures:**
  - Poor performance if auxiliary models are inaccurate
  - Instability if portfolio sampling is not diverse enough
  - Suboptimal results if the reward function does not align with business objectives

- **First 3 experiments:**
  1. Test the algorithm on a simplified version of the problem with a small number of customer features and portfolio categories
  2. Compare the performance of different function approximators for the value function (e.g., linear models vs. MLPs)
  3. Evaluate the sensitivity of the algorithm to the loss coefficient λ by testing different values and measuring the trade-off between profit and portfolio quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the reinforcement learning algorithm perform when scaling to larger time horizons (e.g., months instead of 1000 steps)?
- Basis in paper: [explicit] "We believe that our work presents an important first step in the application of ideas from reinforcement learning to the portfolio pursuit problem. However, our work has several limitations... In our investigation, we limited the time span over which insurers pursued a particular portfolio to only T = 1000 time-steps... This is a relatively short length of time for a large insurer, who may see orders of magnitude more customers per day... Accordingly, we believe that the framework and methods we have develop above could be integrated naturally with RL methods which attempt to perform temporal abstraction, e.g., goal-conditioned policies or the options framework."
- Why unresolved: The paper only tested on 1000 steps due to computational constraints, and explicitly states this is a limitation that needs future work.
- What evidence would resolve it: Experimental results showing performance on larger time horizons (e.g., 10,000+ steps), or theoretical analysis demonstrating scalability properties of the algorithm.

### Open Question 2
- Question: How can the algorithm be adapted to handle customers leaving the portfolio?
- Basis in paper: [explicit] "In our framework, we have also made the simplifying assumption that customers do not leave the portfolio once they have entered... However, for longer time-scales, it may begin to become necessary to model the exit of customers from the portfolio. In App. B we expand our analytic derivation to consider this case, and provide some preliminary suggestions for how to integrate this into our algorithm."
- Why unresolved: The paper only provides preliminary suggestions in Appendix B and acknowledges this as an important direction for future research, without providing a complete solution.
- What evidence would resolve it: A complete implementation and experimental validation of the algorithm with customer exit dynamics, including performance comparisons with and without exit modeling.

### Open Question 3
- Question: How can model uncertainty and non-stationarity be incorporated into the algorithm?
- Basis in paper: [explicit] "Lastly, our method is model-based, with optimal actions and values computed using a conversion model which estimates the probability of customers accepting any given offer. Consequently, if the conversion model is incorrect, our bidding strategy will accordingly be sub-optimal... Model uncertainty could be incorporated into the algorithm by explicitly penalising the value of actions about which the insurer is uncertain... An effective pricing strategy will need to precisely trade-off between profit and the value of the information generated by making particular offers."
- Why unresolved: The paper identifies this as a limitation and suggests potential directions (explicit uncertainty penalization, online updates) but does not implement or test these approaches.
- What evidence would resolve it: Implementation and experimental results showing improved performance when incorporating model uncertainty or online learning in non-stationary environments.

## Limitations
- Performance validation only in synthetic market environment, not real-world insurance markets
- Assumes customers don't leave the portfolio once acquired, limiting applicability to longer time horizons
- Algorithm performance heavily dependent on accuracy of auxiliary models trained on historical data

## Confidence

| Claim | Confidence |
|-------|------------|
| RL algorithm generates 7% more profit than baseline | Medium |
| Model-based approach is safer for deployment | High |
| Results generalize to different insurance markets | Low |

## Next Checks

1. **Real-world pilot test**: Deploy the algorithm in a controlled real insurance market environment with limited customer segments to validate performance outside the synthetic setting.

2. **Model sensitivity analysis**: Systematically test the algorithm's performance with varying levels of accuracy in the auxiliary models to quantify the impact of model errors on final outcomes.

3. **Cross-market validation**: Test the algorithm in multiple synthetic market environments with different customer distributions, competitive structures, and pricing dynamics to assess generalizability.