---
ver: rpa2
title: The I/O Complexity of Attention, or How Optimal is Flash Attention?
arxiv_id: '2402.07443'
source_url: https://arxiv.org/abs/2402.07443
tags:
- complexity
- matrix
- cache
- attention
- lower
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the I/O complexity of self-attention, the key\
  \ component of the popular Transformer architecture, which is known to have quadratic\
  \ time and memory complexity. The authors show that the breakthrough FlashAttention\
  \ algorithm is optimal in terms of I/O complexity for all values of cache size M\
  \ \u2265 d\xB2, where N is the dimension of the attention matrix, d is the head-dimension,\
  \ and M is the cache size."
---

# The I/O Complexity of Attention, or How Optimal is Flash Attention?

## Quick Facts
- arXiv ID: 2402.07443
- Source URL: https://arxiv.org/abs/2402.07443
- Authors: Barna Saha; Christopher Ye
- Reference count: 8
- Key outcome: This paper studies the I/O complexity of self-attention, the key component of the popular Transformer architecture, which is known to have quadratic time and memory complexity. The authors show that the breakthrough FlashAttention algorithm is optimal in terms of I/O complexity for all values of cache size M ≥ d², where N is the dimension of the attention matrix, d is the head-dimension, and M is the cache size. They establish a connection between communication complexity and I/O complexity to prove their lower bounds, introducing a new communication complexity protocol for matrix compression. For M < d², the authors give a better algorithm than FlashAttention and show that it is optimal as well. They also show that their lower bounds hold even when using fast matrix multiplication, not just combinatorial matrix multiplication.

## Executive Summary
This paper analyzes the I/O complexity of self-attention, a key component of the Transformer architecture. The authors prove that FlashAttention is optimal for cache sizes M ≥ d² and show that for smaller cache sizes, attention is equivalent to rectangular matrix multiplication in I/O complexity. They establish a novel connection between communication complexity and I/O complexity to prove lower bounds that hold even when using fast matrix multiplication algorithms. The paper provides a comprehensive characterization of attention's I/O complexity across different cache size regimes.

## Method Summary
The authors analyze the I/O complexity of attention computation using the red-blue pebble game to model two-level memory hierarchies. They establish a connection between communication complexity and I/O complexity by introducing a matrix compression problem. This connection allows them to prove lower bounds that apply even when fast matrix multiplication algorithms are used. For different cache size regimes (M ≥ d² and M < d²), they provide matching upper and lower bounds on I/O complexity, demonstrating optimality of their algorithms.

## Key Results
- FlashAttention achieves optimal I/O complexity O(N²d²/M) for cache sizes M ≥ d²
- For cache sizes M < d², attention computation has I/O complexity Θ(N²d/√M), matching rectangular matrix multiplication
- The lower bounds hold even when using fast matrix multiplication algorithms, not just standard combinatorial matrix multiplication
- A new communication complexity protocol for matrix compression is introduced to bridge communication and I/O complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FlashAttention achieves optimal I/O complexity for cache sizes M ≥ d².
- Mechanism: FlashAttention exploits cache locality by processing the attention matrix in blocks, reducing memory accesses to O(N²d²/M). The authors prove this bound is tight by showing any algorithm must access at least this many elements to compute the full attention matrix.
- Core assumption: The computational graph for attention is fixed (standard matrix multiplication for QKᵀ).
- Evidence anchors:
  - [abstract] "The authors show that the breakthrough FlashAttention algorithm is optimal in terms of I/O complexity for all values of cache size M ≥ d²"
  - [section 3.1] "For large M = Ω(d2), we show that the following result of [DFE+22] is optimal in terms of I/O complexity"
- Break condition: If the computational graph changes (e.g., using fast matrix multiplication), the I/O complexity may be different.

### Mechanism 2
- Claim: For cache sizes M < d², attention is equivalent to rectangular matrix multiplication in I/O complexity.
- Mechanism: When M < d², the algorithm must write the full N×N attention matrix A to memory, making the I/O complexity Θ(N²d/√M). This matches the I/O complexity of multiplying two N×d matrices.
- Core assumption: M < d² implies N²d/√M > N², allowing explicit storage of A.
- Evidence anchors:
  - [section 3.2] "When M < d 2, we show that attention is equivalent to matrix multiplication, establishing a Θ(N 2d√M ) bound on the I/O complexity"
- Break condition: If M ≥ d², the equivalence breaks down as the algorithm can keep more data in cache.

### Mechanism 3
- Claim: Even with fast matrix multiplication, the I/O complexity cannot be improved.
- Mechanism: The authors introduce a matrix compression problem and show it has high communication complexity. Any algorithm that computes many entries of QKᵀ in a single epoch (batch of M I/O operations) would give an efficient communication protocol, contradicting the lower bound.
- Core assumption: The matrix compression problem has high communication complexity regardless of the matrix multiplication algorithm used.
- Evidence anchors:
  - [section 4.3] "We introduce the B-entry matrix compression problem... We then complete the argument by giving a lower bound on the communication complexity of the matrix compression problem"
- Break condition: If the communication complexity of the matrix compression problem can be reduced, the I/O lower bound may not hold.

## Foundational Learning

- Concept: Red-blue pebble game for modeling I/O complexity
  - Why needed here: The authors use this game to model computations on a two-level memory hierarchy and prove lower bounds on I/O complexity
  - Quick check question: How does the red-blue pebble game model cache and memory accesses?

- Concept: Communication complexity and its connection to I/O complexity
  - Why needed here: The authors establish a novel connection between communication complexity and I/O complexity to prove lower bounds for fast matrix multiplication algorithms
  - Quick check question: How does an efficient compression protocol for QKᵀ entries imply low communication complexity?

- Concept: Matrix compression problem
  - Why needed here: The authors introduce this problem to bridge communication complexity and I/O complexity, showing it has high communication complexity
  - Quick check question: What is the relationship between the B-entry matrix compression problem and computing QKᵀ entries?

## Architecture Onboarding

- Component map:
  - Input matrices: Q, K, V (all N×d)
  - Attention matrix: A = softmax(QKᵀ) (N×N)
  - Output: D⁻¹AV (N×d)
  - Cache: Fast memory of size M
  - Memory: Slow memory (unbounded)

- Critical path:
  1. Compute QKᵀ (N×N matrix)
  2. Apply softmax to QKᵀ to get A
  3. Compute D = diag(A · 1)
  4. Compute D⁻¹AV
  - The I/O complexity is dominated by steps 1 and 4

- Design tradeoffs:
  - Cache size M vs. I/O complexity
  - Standard matrix multiplication vs. fast matrix multiplication
  - Block size for processing QKᵀ

- Failure signatures:
  - I/O complexity higher than theoretical lower bound
  - Cache thrashing due to poor data locality
  - Incorrect computation of QKᵀ or softmax

- First 3 experiments:
  1. Verify the I/O complexity of FlashAttention for M ≥ d² matches the theoretical lower bound
  2. Test the equivalence between attention and matrix multiplication for M < d²
  3. Measure the I/O complexity when using fast matrix multiplication algorithms for QKᵀ

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is FlashAttention optimal for all cache sizes M when considering algorithms that use both standard and fast matrix multiplication?
- Basis in paper: [explicit] The paper shows FlashAttention is optimal for M ≥ d² using standard matrix multiplication and extends this to fast matrix multiplication, but doesn't explicitly address optimality for all M values.
- Why unresolved: The paper establishes optimality for M ≥ d² but doesn't fully explore the small cache regime (M < d²) with fast matrix multiplication.
- What evidence would resolve it: A complete characterization of I/O complexity for attention algorithms using fast matrix multiplication across all cache sizes M.

### Open Question 2
- Question: Can the connection between communication complexity and I/O complexity be extended to other computational problems beyond attention?
- Basis in paper: [explicit] The authors believe their connection between communication and I/O complexity "could be of independent interest and will find many more applications in proving I/O complexity lower bounds in the future."
- Why unresolved: The paper only applies this connection to attention and matrix multiplication, leaving broader applicability unexplored.
- What evidence would resolve it: Successful application of communication complexity techniques to derive I/O lower bounds for other fundamental algorithms.

### Open Question 3
- Question: What is the exact I/O complexity of attention in the small cache regime (M ≤ d²)?
- Basis in paper: [explicit] The authors state this as an "interesting open problem" in their conclusion, having only established equivalence to matrix multiplication.
- Why unresolved: While the paper shows attention is equivalent to matrix multiplication in this regime, it doesn't provide tight bounds for the I/O complexity of either operation.
- What evidence would resolve it: A tight lower bound matching the upper bound for I/O complexity of attention or matrix multiplication in the small cache regime.

## Limitations

- The lower bounds apply to the standard attention computation and may not hold for alternative formulations or approximations
- The analysis assumes a two-level memory hierarchy and may not capture more complex memory systems
- While the paper proves optimality for the standard attention computation, practical implementations may have different performance characteristics due to hardware-specific optimizations

## Confidence

**High:** The optimality of FlashAttention for M ≥ d² and the equivalence between attention and matrix multiplication for M < d²

**Medium:** The extension of lower bounds to fast matrix multiplication algorithms

**Medium:** The tightness of the communication complexity lower bound for the matrix compression problem

## Next Checks

1. Verify the tightness of the communication complexity lower bound for the matrix compression problem by attempting to construct more efficient communication protocols.

2. Test the I/O complexity of attention computation on hardware with more than two memory levels to validate the theoretical analysis.

3. Investigate the performance of attention computation using alternative formulations or approximations to see if the lower bounds still apply.