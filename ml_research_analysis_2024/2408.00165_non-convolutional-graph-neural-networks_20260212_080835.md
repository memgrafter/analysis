---
ver: rpa2
title: Non-convolutional Graph Neural Networks
arxiv_id: '2408.00165'
source_url: https://arxiv.org/abs/2408.00165
tags:
- graph
- graphs
- networks
- node
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RUM, a convolution-free graph neural network
  that uses random walks and recurrent neural networks to unify semantic and topological
  graph features. RUM addresses limitations of traditional convolutional GNNs, including
  limited expressiveness, over-smoothing, and over-squashing.
---

# Non-convolutional Graph Neural Networks

## Quick Facts
- arXiv ID: 2408.00165
- Source URL: https://arxiv.org/abs/2408.00165
- Reference count: 40
- Primary result: RUM is a convolution-free GNN using random walks and RNNs that unifies semantic and topological features while addressing over-smoothing, over-squashing, and limited expressiveness

## Executive Summary
This paper introduces RUM (Random Walk with RNNs), a novel graph neural network architecture that eliminates convolution operations entirely. Instead of using message-passing convolutions, RUM employs random walks for feature propagation combined with recurrent neural networks for feature aggregation. The architecture aims to unify semantic and topological graph features while addressing fundamental limitations of traditional convolutional GNNs including limited expressiveness, over-smoothing, and over-squashing. The theoretical analysis demonstrates that RUM is more expressive than the Weisfeiler-Lehman isomorphism test, and experimental results show competitive performance across various node- and graph-level tasks with notably faster GPU inference compared to simple convolutional GNNs.

## Method Summary
RUM replaces traditional graph convolutions with a random walk-based approach where features are propagated through the graph via multiple random walks, and then aggregated using recurrent neural networks. This design choice eliminates the fixed neighborhood aggregation pattern of convolutional GNNs, allowing for more flexible and expressive feature propagation. The architecture uses random walks to explore graph topology, capturing both local and global structural information, while RNNs process the walk sequences to maintain temporal dependencies and avoid the aggregation artifacts common in standard GNNs. This approach theoretically enables RUM to capture more complex graph patterns and relationships than models constrained by the Weisfeiler-Lehman isomorphism test.

## Key Results
- RUM achieves competitive performance on node- and graph-level tasks compared to state-of-the-art convolutional GNNs
- The model demonstrates robustness to adversarial attacks that typically affect traditional GNNs
- RUM shows faster inference speeds on GPUs compared to simple convolutional GNN architectures
- Theoretical analysis proves RUM's expressiveness exceeds the Weisfeiler-Lehman isomorphism test
- The architecture successfully mitigates over-smoothing and over-squashing pathologies common in message-passing GNNs

## Why This Works (Mechanism)
RUM's effectiveness stems from replacing fixed-radius neighborhood aggregation with adaptive random walk exploration combined with RNN-based sequence processing. This mechanism allows the model to capture long-range dependencies without the exponential growth in receptive field that causes over-squashing, while the RNN's sequential processing prevents the uniform averaging that leads to over-smoothing. The random walk strategy naturally balances local and global information gathering, and the absence of convolutional operations eliminates the rigid neighborhood constraints that limit expressiveness in traditional GNNs.

## Foundational Learning
- Graph Neural Networks: Essential for understanding the limitations being addressed; check by identifying common GNN architectures and their message-passing mechanisms
- Random Walks on Graphs: Critical for grasping the propagation strategy; verify by explaining how random walks explore graph structure differently from fixed neighborhoods
- Recurrent Neural Networks: Important for understanding the aggregation mechanism; confirm by describing how RNNs process sequential data
- Weisfeiler-Lehman Isomorphism Test: Key theoretical benchmark for expressiveness; validate by explaining why exceeding this test matters for GNN capability
- Over-smoothing and Over-squashing: Central pathologies being addressed; assess by identifying how these phenomena manifest in deep GNNs
- GPU Acceleration for GNNs: Relevant for understanding the computational advantages; check by comparing inference speeds across different GNN architectures

## Architecture Onboarding
Component Map: Random Walk Generator -> RNN Processor -> Feature Aggregator -> Output Layer
Critical Path: Feature extraction from node embeddings → Random walk-based propagation through graph → RNN processing of walk sequences → Aggregation and final prediction
Design Tradeoffs: Eliminates convolutional operations (simpler theoretical guarantees, faster inference) vs. increased complexity in random walk generation and RNN processing
Failure Signatures: Poor performance on graphs with high homophily, failure to capture local structure, computational bottlenecks with very large graphs
First Experiments: 1) Node classification on Cora citation network, 2) Graph classification on MUTAG dataset, 3) Link prediction on Facebook social network

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical proofs for expressiveness and pathology alleviation are referenced but not included in the paper
- Experimental validation is limited to standard benchmark datasets without exploration of diverse graph types
- Computational complexity analysis is incomplete, particularly for scaling to very large graphs
- Ablation studies focus primarily on GCN comparisons rather than a broader range of modern GNN architectures

## Confidence
- Theoretical expressiveness claims: Medium
- Experimental performance claims: Medium
- Pathology alleviation claims: Medium
- GPU inference speed claims: High

## Next Checks
1. Request and review the complete theoretical proofs for expressiveness beyond Weisfeiler-Lehman test and the analysis of over-smoothing/over-squashing alleviation
2. Conduct experiments on additional diverse datasets including those with different graph properties (heterophilic graphs, dynamic graphs, large-scale graphs)
3. Perform comprehensive ablation studies comparing RUM against a wider range of GNN architectures including GAT, GraphSAGE, and modern attention-based models across varying graph sizes and densities