---
ver: rpa2
title: 'FTF-ER: Feature-Topology Fusion-Based Experience Replay Method for Continual
  Graph Learning'
arxiv_id: '2407.19429'
source_url: https://arxiv.org/abs/2407.19429
tags:
- graph
- information
- learning
- node
- ftf-er
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FTF-ER, a novel experience replay method
  for continual graph learning that integrates feature and global topological information
  to mitigate catastrophic forgetting. The core idea is to combine Gradient Norm Score
  (GraNd) for feature-level importance with Hodge Potential Score (HPS) for global
  topological importance, fused via weighted averaging.
---

# FTF-ER: Feature-Topology Fusion-Based Experience Replay Method for Continual Graph Learning

## Quick Facts
- arXiv ID: 2407.19429
- Source URL: https://arxiv.org/abs/2407.19429
- Reference count: 40
- One-line primary result: FTF-ER improves AA by up to 3.6% and AF by 7.1% over existing methods on OGB-Arxiv while maintaining comparable buffer storage and reducing training time.

## Executive Summary
This paper introduces FTF-ER, a novel experience replay method for continual graph learning that integrates feature and global topological information to mitigate catastrophic forgetting. The core idea is to combine Gradient Norm Score (GraNd) for feature-level importance with Hodge Potential Score (HPS) for global topological importance, fused via weighted averaging. HPS leverages Hodge decomposition on graphs to derive a global ranking of nodes without adding neighbors to the replay buffer, reducing memory overhead. Experiments on four public datasets show FTF-ER achieves state-of-the-art performance, improving AA by up to 3.6% and AF by 7.1% over existing methods on OGB-Arxiv, while maintaining comparable buffer storage to topology-agnostic approaches and reducing training time.

## Method Summary
FTF-ER combines Gradient Norm Score (GraNd) for feature importance and Hodge Potential Score (HPS) for global topological importance, fused via weighted averaging (parameter ùõΩ). It uses experience replay with subgraph sampling per class, leveraging GCN, GAT, or GIN backbones. The method addresses continual graph learning with class-incremental learning setting, focusing on node classification tasks across sequences of disjoint tasks. It trains on four public graph datasets (Amazon Computers, Corafull, OGB-Arxiv, Reddit) with node features, edge lists, and labels, split into 6:2:2 training/validation/test ratio per class. The model optimizes for Average Accuracy (AA) and Average Forgetting (AF) across all tasks, using Adam optimizer with lr=0.005 for 200 epochs. Buffer size per class: 60, 60, 400, 100 for the four datasets respectively.

## Key Results
- FTF-ER improves Average Accuracy (AA) by up to 3.6% over existing methods on OGB-Arxiv.
- FTF-ER reduces Average Forgetting (AF) by 7.1% compared to baselines on OGB-Arxiv.
- Maintains comparable buffer storage to topology-agnostic approaches while reducing training time.

## Why This Works (Mechanism)
FTF-ER addresses catastrophic forgetting in continual graph learning by intelligently selecting which nodes to retain in the replay buffer. The method combines two complementary importance scores: GraNd captures feature-level importance through gradient norms, while HPS captures global topological importance via Hodge decomposition. This dual-perspective selection ensures both locally informative features and globally significant nodes are preserved. The weighted fusion allows flexible control over the balance between feature and topology importance. By sampling subgraphs rather than individual nodes, FTF-ER maintains the relational structure of the graph in the buffer, providing more effective replay samples for training.

## Foundational Learning
- **Continual Graph Learning (CGL)**: Learning on evolving graph data where new tasks arrive sequentially. Needed because real-world graphs grow and change over time. Quick check: Can you explain catastrophic forgetting in this context?
- **Experience Replay**: Storing and reusing past experiences to mitigate forgetting. Needed to maintain performance on previous tasks when learning new ones. Quick check: What's the trade-off between buffer size and forgetting?
- **Hodge Decomposition**: A mathematical framework for analyzing flows on graphs by decomposing them into gradient, curl, and harmonic components. Needed to derive global topological importance scores. Quick check: Can you describe the relationship between Hodge decomposition and the graph Laplacian?

## Architecture Onboarding

**Component Map:**
Data Preparation -> Score Computation (GraNd + HPS) -> Buffer Management -> Model Training (Current Task + Replay) -> Update Scores

**Critical Path:**
For each new task: compute node importance scores ‚Üí sample nodes for buffer ‚Üí create subgraphs ‚Üí train model on current task + replay from buffer ‚Üí update scores for next task

**Design Tradeoffs:**
- GraNd vs HPS: Feature importance vs topological importance
- Buffer size vs computational efficiency
- Deterministic vs probabilistic sampling strategies
- Computational cost of Hodge decomposition vs topological awareness

**Failure Signatures:**
- Numerical instability in Hodge decomposition leading to incorrect HPS scores
- Buffer overflow or incorrect subgraph sampling
- Poor balance between GraNd and HPS leading to suboptimal node selection
- Forgetting on previous tasks despite replay mechanism

**3 First Experiments:**
1. Verify correct computation of GraNd scores by comparing gradient norms across different node importance levels
2. Test HPS computation on small graphs where manual verification is possible
3. Validate buffer management by logging buffer sizes and checking subgraph correctness

## Open Questions the Paper Calls Out
None

## Limitations
- Unknown optimal ùõΩ hyperparameter values for each dataset, requiring additional validation runs
- Potential numerical instability in Hodge decomposition for large or sparse graphs
- Computational overhead of HPS computation compared to feature-only methods

## Confidence

**High confidence**: Overall framework design and experimental methodology are clearly described with appropriate metrics and datasets.

**Medium confidence**: Reproducibility of HPS computation due to potential numerical instability in Hodge decomposition for large graphs.

**Low confidence**: Exact performance improvements without knowing specific ùõΩ values used per dataset, as these hyperparameters are crucial for score fusion.

## Next Checks
1. Implement numerical stability check for Hodge decomposition by verifying eigenvalues of graph Laplacian and applying regularization if needed.
2. Conduct hyperparameter sweep over ùõΩ values for each dataset to identify optimal configuration.
3. Validate buffer management by logging buffer sizes per class and confirming correct subgraph induction from sampled nodes.