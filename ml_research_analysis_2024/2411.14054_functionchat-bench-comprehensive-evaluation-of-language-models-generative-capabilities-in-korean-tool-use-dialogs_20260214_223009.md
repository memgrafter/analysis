---
ver: rpa2
title: 'FunctionChat-Bench: Comprehensive Evaluation of Language Models'' Generative
  Capabilities in Korean Tool-use Dialogs'
arxiv_id: '2411.14054'
source_url: https://arxiv.org/abs/2411.14054
tags:
- function
- tool
- call
- user
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces FunctionChat-Bench, a benchmark for evaluating
  language models'' generative capabilities in Korean tool-use dialogs. The study
  categorizes model outputs into four types: Tool Call, Answer Completion, Slot Question,
  and Relevance Detection, and uses an LLM judge to assess performance across 700
  evaluation items.'
---

# FunctionChat-Bench: Comprehensive Evaluation of Language Models' Generative Capabilities in Korean Tool-use Dialogs

## Quick Facts
- arXiv ID: 2411.14054
- Source URL: https://arxiv.org/abs/2411.14054
- Reference count: 4
- Primary result: High single-turn tool call accuracy does not guarantee strong multi-turn performance in Korean tool-use dialogs

## Executive Summary
This paper introduces FunctionChat-Bench, a benchmark for evaluating language models' generative capabilities in Korean tool-use dialogs. The study categorizes model outputs into four types: Tool Call, Answer Completion, Slot Question, and Relevance Detection, and uses an LLM judge to assess performance across 700 evaluation items. Experiments with eight function-calling models reveal that high accuracy in single-turn Tool Call scenarios does not guarantee superior performance in multi-turn environments. The findings emphasize the importance of evaluating both tool call and conversational outputs to accurately reflect real-world user interactions.

## Method Summary
The study employs an automated evaluation framework using an LLM judge (gpt-4-0125-preview) to assess model outputs across four distinct types in Korean tool-use dialogs. The benchmark includes 700 evaluation items (500 single-call, 200 dialog) and defines specific pass/fail criteria for each output type. Models generate responses that are evaluated based on semantic appropriateness rather than exact syntactic match. The evaluation process includes qualitative human review of LLM judge decisions and detailed error analysis. The methodology addresses limitations of traditional exact match evaluation by incorporating nuanced conversational quality assessment through the LLM judge.

## Key Results
- High accuracy in single-turn Tool Call scenarios does not guarantee superior performance in multi-turn environments
- Performance evaluation requires assessment of both tool call accuracy and conversational appropriateness
- Korean language handling presents specific challenges for models, including token processing errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FunctionChat-Bench evaluates not just tool call accuracy but also conversational appropriateness, which better reflects real-world performance.
- Mechanism: The benchmark defines four output types (Tool Call, Answer Completion, Slot Question, Relevance Detection) and uses an LLM judge to assess each, capturing both tool interaction and user engagement quality.
- Core assumption: Real-world user satisfaction depends on both correct tool usage and natural conversational responses, not just JSON correctness.
- Evidence anchors:
  - [abstract]: "we categorize the models' outputs in tool-use dialogs into four distinct types... which serve as aspects for evaluation"
  - [section]: "we introduce FunctionChat-Bench, comprising 700 evaluation items and automated assessment programs"
- Break condition: If the LLM judge's criteria don't align with actual user preferences, or if conversational outputs are too context-dependent to evaluate reliably.

### Mechanism 2
- Claim: Multi-turn dialog performance is a better predictor of real-world capability than single-turn tool call accuracy.
- Mechanism: The benchmark includes both single-call and dialog datasets; experiments show that high single-turn Tool Call accuracy doesn't guarantee strong multi-turn performance.
- Core assumption: Tool use in practice involves context, history, and conversational flow, not isolated queries.
- Evidence anchors:
  - [abstract]: "high accuracy in single-turn Tool Call scenarios does not guarantee superior performance in multi-turn environments"
  - [section]: "While single-turn environments might show high accuracy in Tool Call, this does not necessarily imply superior overall generative capabilities in multi-turn environments"
- Break condition: If single-turn accuracy correlates strongly with multi-turn performance in some model families, or if multi-turn evaluation is too noisy to measure reliably.

### Mechanism 3
- Claim: LLM-as-judge provides more nuanced evaluation than exact match or cosine similarity, especially for conversational outputs and non-English languages.
- Mechanism: The evaluation uses a powerful LLM judge with detailed rubrics to assess semantic appropriateness rather than syntactic match.
- Core assumption: Semantic equivalence and conversational quality are better captured by a model that understands context than by rigid matching rules.
- Evidence anchors:
  - [section]: "To address these limitations, we chose a method using a powerful large language model (LLM) as a judge"
  - [section]: "Our expectation for the LLM judge is to determine whether each turn generated by language models is a successful output or a failure"
- Break condition: If the LLM judge introduces significant bias or inconsistency, or if exact match metrics are sufficient for the task.

## Foundational Learning

- Concept: Function calling in LLMs
  - Why needed here: The entire benchmark evaluates how well models generate tool calls and handle function arguments
  - Quick check question: What JSON structure do models need to output for a function call, and what fields are required?

- Concept: Evaluation methodology design
  - Why needed here: The paper describes a novel evaluation approach using LLM judges and multiple output types
  - Quick check question: How does using an LLM judge differ from traditional exact match evaluation, and what are the trade-offs?

- Concept: Korean language processing
  - Why needed here: The dataset is in Korean, requiring understanding of how models handle non-English inputs and outputs
  - Quick check question: What specific challenges arise when evaluating tool use in Korean versus English, particularly for argument extraction?

## Architecture Onboarding

- Component map: Dataset generation -> Model output collection -> LLM judge evaluation -> Human review -> Final scoring
- Critical path: Dataset generation → Model evaluation → LLM judge assessment → Human review → Final scoring
- Design tradeoffs: LLM judge provides nuanced evaluation but introduces potential bias; exact match is objective but misses conversational quality
- Failure signatures: LLM judge misclassifications (false positives/negatives), dataset bias toward certain function types, Korean language handling issues
- First 3 experiments:
  1. Run single-call evaluation on a small set of functions to verify LLM judge criteria alignment
  2. Test dialog evaluation with models known to have strong vs weak conversational abilities
  3. Compare LLM judge results against human evaluation on a subset to measure alignment rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of language models vary when evaluating multi-turn dialogues versus single-turn utterances in tool-use scenarios?
- Basis in paper: [explicit] The paper states that high accuracy in single-turn Tool Call scenarios does not guarantee superior performance in multi-turn environments.
- Why unresolved: The study highlights the discrepancy but does not provide a detailed comparative analysis of model performance across single-turn and multi-turn contexts.
- What evidence would resolve it: Conducting a comprehensive study comparing model performance metrics in single-turn versus multi-turn dialogues, focusing on different output types like Tool Call, Answer Completion, Slot Question, and Relevance Detection.

### Open Question 2
- Question: What are the specific challenges faced by language models in handling Korean tokens, and how do these challenges affect the accuracy of tool-use dialogs?
- Basis in paper: [explicit] The paper mentions errors related to the handling of Korean tokens, such as adding or removing spaces and newline characters when extracting long strings.
- Why unresolved: While the paper identifies these issues, it does not delve into the underlying causes or propose solutions to improve Korean token handling.
- What evidence would resolve it: Investigating the linguistic and technical aspects of Korean token processing in language models, and developing methods to enhance accuracy in handling Korean text.

### Open Question 3
- Question: How can the FunctionChat-Bench be expanded to include scenarios involving parallel function calling and complex planning sequences?
- Basis in paper: [inferred] The paper acknowledges limitations in the current dataset, noting it does not include scenarios involving multiple function calls or complex planning sequences.
- Why unresolved: The current benchmark focuses on single function calls, limiting its ability to assess advanced capabilities of language models in handling more complex tool-use scenarios.
- What evidence would resolve it: Designing and implementing new evaluation items within the FunctionChat-Bench that simulate multi-function call scenarios and require strategic planning sequences.

## Limitations
- The evaluation relies heavily on an LLM judge, introducing potential subjectivity and bias that may not align with actual user preferences
- Korean-specific nature of the benchmark limits generalizability to other languages
- Dataset size (700 items) may not capture the full diversity of real-world tool use scenarios

## Confidence

- **High confidence**: The finding that single-turn tool call accuracy does not guarantee multi-turn performance - supported by direct experimental evidence
- **Medium confidence**: The claim that LLM-as-judge provides more nuanced evaluation than exact match - logical but limited empirical evidence
- **Medium confidence**: The assertion that real-world user satisfaction depends on both tool usage and conversational appropriateness - reasonable assumption but not directly validated

## Next Checks

1. Conduct a validation study comparing LLM judge assessments against human evaluations on a random sample of 100+ outputs to measure agreement rates and identify systematic biases.

2. Perform cross-lingual validation by translating a subset of the Korean benchmark to English and evaluating the same models to assess whether performance patterns hold across languages.

3. Expand the benchmark with additional tool types and dialog scenarios to test whether the observed performance gaps persist across a broader range of use cases and whether current evaluation criteria capture all relevant failure modes.