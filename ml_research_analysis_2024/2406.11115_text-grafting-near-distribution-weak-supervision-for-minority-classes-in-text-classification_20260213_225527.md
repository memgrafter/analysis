---
ver: rpa2
title: 'Text Grafting: Near-Distribution Weak Supervision for Minority Classes in
  Text Classification'
arxiv_id: '2406.11115'
source_url: https://arxiv.org/abs/2406.11115
tags:
- text
- grafting
- data
- texts
- synthesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of extremely weak-supervised
  text classification for minority classes, where traditional methods like text mining
  and data synthesis have limitations. Text mining struggles with low precision on
  minority classes, while data synthesis often generates out-of-distribution texts.
---

# Text Grafting: Near-Distribution Weak Supervision for Minority Classes in Text Classification

## Quick Facts
- arXiv ID: 2406.11115
- Source URL: https://arxiv.org/abs/2406.11115
- Reference count: 32
- Primary result: Text grafting improves minority class F1 scores from ~15-20 to ~27-32 by combining text mining with LLM-based synthesis

## Executive Summary
Text grafting addresses the challenge of extremely weak-supervised text classification for minority classes by combining the strengths of text mining and data synthesis approaches. Traditional text mining struggles with low precision on minority classes, while data synthesis often produces out-of-distribution texts. The proposed method first uses LLM-based logits to mine potential text templates from the raw corpus, then fills these templates with state-of-the-art LLMs to synthesize near-distribution, in-class texts. Experiments across multiple datasets demonstrate significant improvements over existing methods, with F1 scores increasing from around 15-20 to 27-32.

## Method Summary
Text grafting operates through a three-stage pipeline: (1) LLM-based potential text mining that identifies words contributing to minority class content using regularized logits from prompts with and without class names, (2) template creation by masking low-potential words while preserving structural patterns, and (3) template filling with strong LLMs to generate in-class, near-distribution texts. The method is particularly robust for zero-occurrence minority classes and reduces the need for negative sample synthesis. The approach is evaluated on multiple datasets with RoBERTa-Large classifiers, showing consistent improvements over baselines like text mining, direct synthesis, and positive-unlabeled learning methods.

## Key Results
- F1 scores increase from approximately 15-20 to 27-32 across multiple datasets
- Outperforms both pure text mining and pure data synthesis approaches
- Demonstrates robustness to extreme cases where minority classes have zero occurrences in the raw corpus
- Reduces need for negative sample synthesis while maintaining classification performance

## Why This Works (Mechanism)

### Mechanism 1
Text grafting achieves near-distribution synthetic texts by preserving raw corpus structure while filling minority class content. The method masks low-potential words (based on LLM logit differences) to create templates that retain structural patterns of the original corpus, then uses a strong LLM to fill these templates with minority class content. This works under the assumption that words with small logit differences between class-specific and generic prompts are structurally important but semantically neutral, thus preserving distribution.

### Mechanism 2
Template filling with strong LLMs generates high-quality in-class texts that outperform both pure mining and pure synthesis. Strong LLMs (GPT-4o) fill carefully selected templates to produce texts that are both semantically correct for the minority class and stylistically consistent with the original corpus. This relies on the assumption that strong LLMs can understand template structure and fill it appropriately while maintaining class semantics.

### Mechanism 3
Text grafting is robust to zero-occurrence minority classes by not requiring ground truth examples in the raw corpus. By using LLM-based potential scoring rather than direct matching, the method can identify and utilize text patterns that could be transformed into minority class content, even when no examples exist. This assumes that LLM logits can identify potential minority class patterns even without direct examples.

## Foundational Learning

- **Regularized LLM logits for word potential scoring**: Needed to distinguish words that are structurally important from those that are class-indicative, enabling effective template creation. Quick check: What is the formula for calculating word potential in text grafting?

- **Template-based data synthesis vs. direct generation**: Explains why filling templates produces more near-distribution texts than direct generation from class descriptions. Quick check: How does template filling differ from in-context generation in terms of distribution preservation?

- **Minority class classification challenges**: Provides context for why standard mining fails and why synthesis needs distribution preservation. Quick check: What are the two main limitations of text mining and data synthesis approaches for minority classes?

## Architecture Onboarding

- **Component map**: LLM-based potential scorer (Gemma 1.1-7B) → Template creator → Template ranker → Strong LLM filler (GPT-4o) → Classifier trainer

- **Critical path**: Potential scoring → Template creation → Template filling → Classifier training

- **Design tradeoffs**: 
  - Template mining rate (K%) vs. template quality: Higher rates capture more patterns but increase noise
  - Number of templates (N%) vs. synthesis cost: More templates improve coverage but increase LLM API calls
  - Mask ratio vs. template usefulness: Higher ratios preserve more structure but may limit semantic flexibility

- **Failure signatures**:
  - Low classifier F1 scores despite successful LLM calls: Templates may not capture relevant patterns
  - Generated texts that don't match class semantics: LLM filling may not understand template intent
  - Excessive noise in synthetic data: Potential scoring may not effectively distinguish useful patterns

- **First 3 experiments**:
  1. Verify potential scoring works by comparing logit differences for known class-indicative vs. neutral words
  2. Test template creation by examining masked templates to ensure structural preservation
  3. Validate template filling by checking if generated texts match both class semantics and corpus style

## Open Questions the Paper Calls Out

- **Optimal balance between mask ratio and number of templates**: While the paper identifies optimal values for specific datasets, the generalizability to other minority classes and datasets is not fully explored. Conducting experiments across diverse datasets would determine if these values hold or need adjustment.

- **Impact of LLM choice on performance**: The paper uses specific LLMs but doesn't explore how different LLMs for mining and filling affect results. Different LLMs have varying capabilities and biases that could influence template quality and synthesis, requiring experiments with different combinations.

- **Extension to other NLP tasks**: The paper mentions future work on extending to tasks like information extraction but provides no experimental results. Implementing text grafting for various NLP tasks and comparing performance would reveal its versatility and limitations.

## Limitations

- **Prompt template specificity**: Exact LLM prompts for each stage are referenced but not fully detailed, creating uncertainty about faithful reproduction
- **Zero-example robustness evidence**: Claims of effectiveness for zero-occurrence classes rely on indirect evidence and LLM understanding without examples
- **Distribution preservation verification**: "Near-distribution" synthesis claims lack direct distributional analysis through text embeddings or statistical properties

## Confidence

- **High Confidence**: Framework architecture and three-stage pipeline are clearly specified and experimentally validated with documented F1 improvements
- **Medium Confidence**: LLM logits mechanism for identifying potential words is theoretically sound but lacks detailed implementation specifics
- **Low Confidence**: Zero-occurrence class handling and no-ground-truth-examples claims are weakly supported with limited evidence

## Next Checks

1. **Distribution Analysis**: Perform t-SNE or PCA visualization comparing embeddings of raw corpus texts, text-mined texts, synthesized texts, and text-grafted texts to verify the "near-distribution" claim

2. **Prompt Sensitivity Study**: Test how variations in LLM prompts for template creation and filling affect the quality and class alignment of generated texts, particularly for minority classes

3. **Zero-Example Stress Test**: Systematically evaluate text grafting performance on minority classes with varying levels of corpus presence (0%, 0.1%, 1%) to quantify degradation as examples decrease