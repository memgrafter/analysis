---
ver: rpa2
title: Evaluating authenticity and quality of image captions via sentiment and semantic
  analyses
arxiv_id: '2409.09560'
source_url: https://arxiv.org/abs/2409.09560
tags:
- captions
- sentiment
- image
- semantic
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents an evaluation framework for assessing the authenticity
  and quality of image captions by analyzing sentiment and semantic variability. The
  approach was applied to the COCO-MS dataset, which contains approximately 150,000
  images with crowd-sourced captions.
---

# Evaluating authenticity and quality of image captions via sentiment and semantic analyses

## Quick Facts
- arXiv ID: 2409.09560
- Source URL: https://arxiv.org/abs/2409.09560
- Reference count: 17
- Primary result: Image captions contain approximately 6% strong sentiment influenced by object categories, with semantic variability remaining low and uncorrelated with categories

## Executive Summary
This study presents an evaluation framework for assessing the authenticity and quality of image captions by analyzing sentiment and semantic variability in the COCO-MS dataset. Using pre-trained language models, the researchers quantified sentiment scores and semantic variability across captions, revealing that while most captions were neutral, about 6% exhibited strong sentiment influenced by specific object categories. The findings highlight the importance of evaluating crowd-sourced captions to ensure objectivity and diversity, offering insights into potential improvements for training generative language models.

## Method Summary
The study employed pre-trained models (Twitter-RoBERTa-base and BERT-base) to extract sentiment scores and variability of semantic embeddings from captions in the COCO-MS dataset. Sentiment analysis was performed using Twitter-roBERTa-base to compute scores between -1 and 1, while semantic variability was measured using BERT embeddings and cosine similarity. Multiple linear regression was used to examine the relationship between sentiment scores, semantic variability, and object categories. The analysis focused on identifying strong sentiments and their correlation with object categories, as well as assessing the semantic diversity of captions.

## Key Results
- Approximately 6% of human-generated captions exhibited strong sentiment (scores above 0.5 or below -0.5), influenced by specific object categories
- Multiple linear regression showed 24% of sentiment variance in human captions explained by object categories, compared to 13% for model-generated captions
- Semantic variability within-image captions remained low and uncorrelated with object categories
- Model-generated captions showed less than 1.5% strong sentiment, not influenced by object categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained language models can extract sentiment scores from image captions that reflect human perception.
- Mechanism: Twitter-roBERTa-base and BERT-base models convert captions into embeddings, which are processed through sentiment analysis layers to produce a continuous score between -1 and 1, where higher values indicate more positive sentiment.
- Core assumption: Sentiment scores from large language models align with human sentiment perception in image captions.
- Evidence anchors:
  - [abstract] "We employed pre-trained models (Twitter-RoBERTa-base and BERT-base) to extract sentiment scores and variability of semantic embeddings from captions."
  - [section] "Sentiment score between -1 and 1 was computed as a difference between positive and negative confidence values [7]."
  - [corpus] Weak evidence - no direct comparison with human annotations provided.
- Break condition: Sentiment model fails when captions contain sarcasm, negation, or domain-specific language not well represented in pre-training data.

### Mechanism 2
- Claim: Semantic variability within image captions can be quantified using cosine similarity of BERT embeddings.
- Mechanism: BERT embeddings for each caption are compared pairwise using cosine similarity. The standard deviation of these similarities across all caption pairs for an image provides a measure of semantic variability.
- Core assumption: Lower cosine similarity between captions indicates greater semantic diversity in descriptions.
- Evidence anchors:
  - [abstract] "Variability was defined as the standard deviation of the cosine-similarity metric across the embeddings of the same image."
  - [section] "Variability was defined as the standard deviation of the cosine-similarity metric across the embeddings of the same image. To approximate that value without explicitly defining the mean in the cosine space, pairwise cosine similarities Sc(ai, aj) for i-th and j-th captions were used..."
  - [corpus] Moderate evidence - method is described but validation against human diversity judgments is not shown.
- Break condition: High variability could indicate poor caption quality rather than richness, or captions might use different sentence structures that artificially lower cosine similarity.

### Mechanism 3
- Claim: Object categories in images influence the sentiment expressed in human-generated captions but not in model-generated captions.
- Mechanism: Multiple linear regression was used to examine how the presence/absence of object categories affects sentiment scores. For human captions, 24% of sentiment variance was explained by object categories, while for model captions only 13% was explained.
- Core assumption: The sentiment expressed in captions is influenced by the presence of specific object categories in the image.
- Evidence anchors:
  - [abstract] "Results indicate that while most captions were neutral, about 6% of the captions exhibited strong sentiment influenced by specific object categories."
  - [section] "Influence of one-hot-encoded category presence on sentiment score was assessed via multiple linear regression (MLR) with first-order terms."
  - [corpus] Moderate evidence - the claim is supported but the corpus shows only 5 related papers, none directly addressing this specific relationship.
- Break condition: The relationship breaks if object categories don't capture the visual features that actually influence sentiment, or if captions focus on non-categorized elements.

## Foundational Learning

- Concept: Multiple Linear Regression (MLR)
  - Why needed here: To quantify how the presence of different object categories influences sentiment scores in captions.
  - Quick check question: What assumptions must be met for MLR to provide valid results when analyzing categorical predictors like object presence?

- Concept: Cosine Similarity and Embeddings
  - Why needed here: To measure semantic similarity between different captions of the same image using their BERT embeddings.
  - Quick check question: Why might cosine similarity be preferred over Euclidean distance when comparing sentence embeddings?

- Concept: Pre-trained Language Models for Sentiment Analysis
  - Why needed here: To automatically extract sentiment scores from text captions without requiring manual annotation.
  - Quick check question: What are the limitations of using pre-trained sentiment models on domain-specific text like image captions?

## Architecture Onboarding

- Component map: COCO-MS dataset -> Preprocessing -> Sentiment analysis (Twitter-roBERTa-base) -> Semantic analysis (BERT-base embeddings + cosine similarity) -> Statistical analysis (MLR) -> Visualization and interpretation
- Critical path: The sentiment extraction and semantic variability computation must complete before the regression analysis can be performed
- Design tradeoffs: Using pre-trained models enables rapid analysis but may not capture caption-specific nuances; manual annotation would be more accurate but prohibitively expensive for 150K images
- Failure signatures: Low variance explained in regression (less than 10%) suggests poor relationship between predictors and sentiment; high semantic variability might indicate inconsistent captions or poor quality control
- First 3 experiments:
  1. Test sentiment analysis on a small subset of captions with known sentiment to validate model performance
  2. Compare semantic variability across images with different numbers of captions to establish baseline expectations
  3. Run MLR on a stratified sample of captions to verify the relationship between object categories and sentiment before full-scale analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific object categories most strongly influence the sentiment of image captions?
- Basis in paper: [explicit] The paper found that certain object categories significantly affected sentiment scores in captions, but did not specify which categories were most influential.
- Why unresolved: The study identified that object categories influenced sentiment but did not rank or detail which categories had the strongest impact.
- What evidence would resolve it: A detailed analysis ranking object categories by their impact on sentiment scores would clarify which categories are most influential.

### Open Question 2
- Question: How does the sentiment of human-generated captions compare to model-generated captions in terms of objectivity?
- Basis in paper: [explicit] The study compared sentiment scores between human-generated and model-generated captions, noting differences in sentiment distribution.
- Why unresolved: While differences were noted, the study did not explore the implications of these differences for objectivity.
- What evidence would resolve it: A comparative study focusing on the objectivity of sentiments in both human and model-generated captions would provide insights.

### Open Question 3
- Question: What strategies can be employed to enhance semantic diversity in image captions while maintaining objectivity?
- Basis in paper: [inferred] The paper suggests that low semantic diversity might limit model variation, indicating a need for strategies to enhance diversity.
- Why unresolved: The study highlights the issue but does not propose specific strategies to address it.
- What evidence would resolve it: Research into methods that increase semantic diversity without compromising objectivity would be beneficial.

## Limitations
- The study lacks direct human validation for sentiment analysis results, leaving alignment with human perception unverified
- Semantic variability metric may conflate meaningful diversity with inconsistent or poor-quality captions
- Corpus analysis reveals weak connections to prior work, with only 5 related papers, suggesting limited validation against existing research

## Confidence

| Claim | Confidence |
|-------|------------|
| Methodology for extracting sentiment scores and computing semantic variability is clearly specified and follows established NLP practices | High |
| Object categories influence sentiment in human captions is supported by regression results, but causal mechanism remains unclear | Medium |
| Model-generated captions show less category-influenced sentiment based on indirect evidence without clear validation | Low |

## Next Checks

1. Conduct a small-scale human annotation study to validate whether the sentiment scores from pre-trained models align with human judgments for a representative sample of captions.
2. Test the semantic variability metric by comparing it with human diversity ratings across a subset of images to ensure it captures meaningful differences rather than noise.
3. Perform ablation studies by removing specific object categories from the regression analysis to identify which categories most strongly influence sentiment scores.