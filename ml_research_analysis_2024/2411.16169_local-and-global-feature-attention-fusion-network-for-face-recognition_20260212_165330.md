---
ver: rpa2
title: Local and Global Feature Attention Fusion Network for Face Recognition
arxiv_id: '2411.16169'
source_url: https://arxiv.org/abs/2411.16169
tags:
- feature
- local
- face
- global
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of recognizing low-quality face
  images by introducing a Local and Global Feature Attention Fusion (LGAF) network
  that adaptively allocates attention between local and global features based on feature
  quality. The method employs a Multi-Head Multi-Scale Local Feature Extraction (MHMS)
  module to capture fine-grained local information at various scales, while a Global
  Feature Extraction (GFE) module extracts global features.
---

# Local and Global Feature Attention Fusion Network for Face Recognition

## Quick Facts
- arXiv ID: 2411.16169
- Source URL: https://arxiv.org/abs/2411.16169
- Reference count: 40
- Key outcome: LGAF achieves state-of-the-art performance on TinyFace and SCFace datasets with 96.67% average recognition rate across four high-resolution validation sets

## Executive Summary
This paper addresses the challenge of recognizing low-quality face images by proposing a Local and Global Feature Attention Fusion (LGAF) network. The method dynamically allocates attention between local and global features based on feature quality assessment using feature norm as a proxy. The network combines a Multi-Head Multi-Scale Local Feature Extraction module with a Global Feature Extraction module, fused through an adaptive Local and Global Feature Fusion module. Experiments demonstrate superior performance on both low-resolution and high-resolution face recognition benchmarks.

## Method Summary
LGAF uses ResNet100 as backbone to extract initial features, then processes them through three main components: the Multi-Head Multi-Scale Local Feature Extraction (MHMS) module captures fine-grained local information at multiple scales with spatial and channel attention, the Global Feature Extraction (GFE) module projects global features to compact representations, and the Local and Global Feature Fusion (LGF) module dynamically balances these features based on their computed feature norms. The network is trained on MS1MV2 and WebFace4M datasets using ArcFace and CosFace losses with SGD optimization and data augmentation including random rectangle cropping.

## Key Results
- Achieves state-of-the-art performance on TinyFace and SCFace datasets
- Average recognition rate of 96.67% across four high-resolution validation sets (CFP-FP, CPLFW, AgeDB, and CALFW)
- Outperforms existing methods in handling low-quality face images with missing regions and deformations

## Why This Works (Mechanism)

### Mechanism 1
The LGF module uses feature norm as a quality proxy to adaptively allocate attention between local and global features. It computes L2 norms for both feature types, normalizes using batch statistics, and emphasizes the feature type with higher quality based on the relative norm values.

### Mechanism 2
The MHMS module employs multi-scale convolution operations to capture local features distributed across different scales and spatial positions. Spatial attention via LANet and channel attention via SE module work together to emphasize important regions while suppressing noise and redundancy.

### Mechanism 3
Local and global features exhibit complementary strengths for different degradation types: local features are more robust to missing facial regions while global features better handle facial deformations. The adaptive fusion exploits this complementarity by selecting the most reliable feature type for each input.

## Foundational Learning

- **Feature norm as quality proxy**: Understanding how feature norm correlates with feature quality is essential for implementing the LGF module's adaptive attention mechanism.
  - Quick check: How does feature norm behave differently for local versus global features when face images are degraded by missing regions versus deformations?

- **Multi-scale feature extraction with attention**: The MHMS module relies on extracting features at different scales and applying spatial/channel attention to enhance discriminative information.
  - Quick check: What is the difference between spatial attention and channel attention, and how do they complement each other in multi-scale feature extraction?

- **Local vs global feature complementarity**: Understanding when local features outperform global features (and vice versa) is crucial for designing effective fusion strategies.
  - Quick check: In what scenarios would focusing on local features be detrimental to face recognition performance, and why?

## Architecture Onboarding

- **Component map**: Input → Backbone → MHMS (local features) + GFE (global features) → LGF (fusion with attention) → Classification

- **Critical path**: Input passes through ResNet100 backbone, then splits to MHMS and GFE modules, whose outputs are fused by LGF before classification

- **Design tradeoffs**: Local features provide robustness to missing regions but are deformation-sensitive; global features handle deformations but are sensitive to missing regions; multi-scale extraction increases computational cost but captures comprehensive information; feature norm-based attention is lightweight but assumes norm reliably indicates quality

- **Failure signatures**: Performance degradation on specific degradation types; unstable batch statistics in LGF leading to erratic attention weights; excessive computational overhead without performance gains; overfitting when adaptive fusion doesn't generalize

- **First 3 experiments**:
  1. Ablation study comparing performance with only local features, only global features, and combined without adaptive attention
  2. Parameter sensitivity testing different values of hyperparameter b (number of heads in MHMS)
  3. Cross-dataset validation testing trained model on multiple validation sets (CFP-FP, CPLFW, AgeDB, CALFW)

## Open Questions the Paper Calls Out

### Open Question 1
How does the LGAF network perform when trained on datasets with significantly different distributions from MS1MV2 and WebFace4M?
- Basis: The paper evaluates on relatively similar datasets without exploring performance across vastly different distributions
- Why unresolved: Experiments focus on datasets with similar face recognition challenges
- Evidence needed: Testing on datasets with different demographic distributions, environmental conditions, or face recognition challenges

### Open Question 2
What is the impact of varying the number of heads in the MHMS module on performance and computational efficiency?
- Basis: The paper mentions parameter b affects performance but only tests limited range
- Why unresolved: Ablation study tests only limited values for b
- Evidence needed: Experiments with wider range of b values analyzing performance vs computational cost trade-off

### Open Question 3
How does the LGAF network handle extreme occlusions not present in training data?
- Basis: The paper doesn't address network's ability to handle significantly different occlusions from training
- Why unresolved: Evaluation focuses on common occlusion scenarios, not rare or extreme ones
- Evidence needed: Evaluating on datasets with novel or extreme occlusion patterns

## Limitations
- Feature norm as quality proxy lacks strong empirical validation across diverse degradation types
- Implementation details of LANet and SE modules are not fully specified, affecting reproducibility
- Adaptive fusion strategy performance on extreme degradation scenarios (severe missing regions combined with significant deformations) remains untested

## Confidence

- **High confidence**: Experimental results showing state-of-the-art performance on TinyFace and SCFace datasets
- **Medium confidence**: Theoretical framework for why local/global feature fusion improves recognition performance
- **Low confidence**: Claim that feature norm is universally reliable proxy for feature quality across all low-quality face images

## Next Checks

1. **Ablation study on feature fusion**: Compare LGAF with configurations using only local features, only global features, and simple concatenation without adaptive attention to quantify LGF module contribution

2. **Cross-degradation validation**: Test on synthetic low-quality images with controlled combinations of missing regions and deformations to verify adaptive fusion correctly identifies and responds to different degradation types

3. **Feature norm correlation analysis**: Systematically analyze relationship between feature norm values and actual recognition accuracy across different degradation types to validate feature norm as quality proxy