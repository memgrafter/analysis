---
ver: rpa2
title: 'EmPO: Emotion Grounding for Empathetic Response Generation through Preference
  Optimization'
arxiv_id: '2406.19071'
source_url: https://arxiv.org/abs/2406.19071
tags:
- language
- dataset
- empathetic
- empathy
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel approach for empathetic response generation
  (ERG) by constructing theory-driven preference datasets based on emotion grounding
  and aligning large language models (LLMs) with preference optimization algorithms.
  The method leverages the EmpatheticDialogues dataset, pairing preferred responses
  with those of opposite emotions derived from Plutchik's wheel.
---

# EmPO: Emotion Grounding for Empathetic Response Generation through Preference Optimization

## Quick Facts
- arXiv ID: 2406.19071
- Source URL: https://arxiv.org/abs/2406.19071
- Reference count: 17
- The paper proposes a novel approach for empathetic response generation by constructing theory-driven preference datasets based on emotion grounding and aligning LLMs with preference optimization algorithms.

## Executive Summary
This paper introduces EmPO (Emotion Preference Optimization), a novel approach for empathetic response generation that constructs preference datasets using emotion grounding theory and aligns LLMs through preference optimization. The method leverages the EmpatheticDialogues dataset, pairing preferred responses with those of opposite emotions derived from Plutchik's wheel. By using preference optimization algorithms like DPO, the approach improves empathetic response generation while retaining the model's general language understanding capabilities, as validated by multiple evaluation metrics.

## Method Summary
The approach constructs preference datasets by pairing ground truth responses with randomly sampled responses labeled with polar opposite emotions using Plutchik's wheel. The method employs LoRA-based fine-tuning with SFT followed by DPO training to align the LLM with the constructed preference dataset. The LoRA configuration uses a large rank (r=1024) and low alpha (α=256) to balance between retaining general performance and adapting for empathy. The preference dataset is constructed such that each prompt is paired with preferred (ground truth) and non-preferred (opposite emotion) completions.

## Key Results
- Training LLMs with the constructed preference dataset improves empathetic response generation (diff-Epitome metrics) while retaining generalization performance (MMLU benchmark)
- The approach achieves a diff-Epitome score of 0.1235 on the EmpathyBench test set, outperforming the base model (0.1215)
- Using LoRA with r=1024 and α=256 effectively mitigates overfitting to the EmpatheticDialogues dataset while improving empathy metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Emotion grounding via Plutchik's wheel enables construction of preference datasets where rejected completions have opposite emotional valence, creating a contrastive learning signal for empathy alignment.
- Mechanism: By pairing each preferred response with a randomly sampled response labeled with the polar opposite emotion, the model learns to distinguish empathetic from non-empathetic completions.
- Core assumption: Emotional polarity in EmpatheticDialogues is reliable enough to use opposite emotions as a proxy for non-empathetic responses.
- Evidence anchors:
  - [abstract] "We leverage this property to extract responses from the corpus of the polar opposite emotion label using Plutchik's wheel (Plutchik, 2001)"
  - [section] "For the preferred completion, we take the ground truth - the original response. For its rejected counterpart, we use Plutchik's wheel of emotions"
- Break condition: If emotion labels are unreliable or opposite emotions don't consistently map to non-empathetic responses, the preference dataset construction would fail.

### Mechanism 2
- Claim: Preference optimization algorithms like DPO can align LLMs for empathetic response generation while retaining general language understanding capabilities.
- Mechanism: DPO uses the constructed preference dataset to fine-tune the LLM, optimizing for generating responses that align with preferred (empathetic) completions rather than rejected (non-empathetic) ones.
- Core assumption: DPO can effectively learn from synthetic preference data constructed from emotion opposites rather than requiring human-labeled preferences.
- Evidence anchors:
  - [abstract] "Experiments using diff-Epitome and BERTscore metrics show that training LLMs with the constructed preference dataset improves ERG while retaining generalization performance"
  - [section] "For DPO, we build a preference dataset from the EmpatheticDialogues consisting of preferred/rejected completions"
- Break condition: If DPO training becomes unstable due to random selection of rejected completions or if the model overfits to the preference dataset, alignment would fail.

### Mechanism 3
- Claim: LoRA-based fine-tuning with appropriate hyperparameter configuration can mitigate overfitting to the EmpatheticDialogues dataset while improving empathy metrics.
- Mechanism: Using LoRA adapters with specific rank and alpha values (r=1024, α=256) allows efficient fine-tuning that preserves the model's general capabilities while adapting for empathetic response generation.
- Core assumption: LoRA with large rank and low alpha can balance between retaining general performance and adapting for empathy.
- Evidence anchors:
  - [section] "We observed that increasing the rank above 1024 brings diminishing returns... The large α supports retention of the original model's abilities"
  - [section] "The key to reducing over-fitting was using a large lora_rank (r) and low lora_alpha (α) relative to the rank"
- Break condition: If the LoRA configuration is suboptimal, the model would either overfit or fail to improve empathy.

## Foundational Learning

- Concept: Emotion theory and Plutchik's wheel of emotions
  - Why needed here: Understanding how opposite emotions are defined is crucial for constructing the preference dataset where rejected completions are sampled from polar opposite emotion labels.
  - Quick check question: What are the eight basic emotions in Plutchik's wheel and how are opposite emotions defined?

- Concept: Preference optimization and DPO algorithm
  - Why needed here: The method relies on Direct Preference Optimization to align the LLM using the constructed preference dataset, requiring understanding of how DPO works with pairs of preferred/rejected completions.
  - Quick check question: How does DPO use preference pairs to update model weights without requiring human feedback during training?

- Concept: LoRA (Low-Rank Adaptation) for efficient fine-tuning
  - Why needed here: The approach uses LoRA adapters with specific hyperparameters (rank and alpha) to fine-tune the LLM while mitigating overfitting and preserving general capabilities.
  - Quick check question: What is the relationship between LoRA rank, alpha, and the balance between retaining original model capabilities versus adapting to new tasks?

## Architecture Onboarding

- Component map: EmpatheticDialogues dataset -> Plutchik's wheel + dyads -> LoRA adapters -> DPO algorithm -> diff-Epitome and BERTscore metrics -> MMLU benchmark

- Critical path:
  1. Load EmpatheticDialogues dataset with emotion labels
  2. Construct preference dataset using emotion opposites
  3. Perform SFT with LoRA to initialize in-domain weights
  4. Apply DPO training with constructed preference pairs
  5. Evaluate empathy (diff-Epitome, BERTscore) and generalization (MMLU)

- Design tradeoffs:
  - Random selection of rejected completions vs. stability (solved by re-drawing per epoch)
  - LoRA rank/alpha balance between adaptation and preservation
  - Number of DPO epochs vs. overfitting risk
  - Emotion grounding approach vs. potential noise in emotion labels

- Failure signatures:
  - High variance in diff-Epitome scores across training runs (training instability)
  - Significant drop in MMLU scores (overfitting/generalization loss)
  - No improvement in BERTscore despite diff-Epitome gains (metric correlation issues)

- First 3 experiments:
  1. Run SFT with different LoRA configurations (vary rank and alpha) and measure empathy vs. MMLU tradeoff
  2. Test DPO with different beta values (0.025 vs 0.05 vs 0.1) to find stability sweet spot
  3. Compare diff-Epitome scores when using emotion opposites vs. random non-ground truth responses as rejected completions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the random selection of rejected completions impact the stability and effectiveness of the preference optimization process, and what alternative methods could be used to select these completions?
- Basis in paper: [explicit] The paper mentions that the stability can suffer from random selection of rejected completions from the group of dialogues labeled with polar opposite emotions, and they solved it by training for more epochs and re-drawing new randomly selected rejected completions for each epoch.
- Why unresolved: While the paper describes a method to mitigate the stability issues, it does not explore alternative methods for selecting rejected completions or provide a detailed analysis of how the random selection impacts the overall effectiveness of the preference optimization.
- What evidence would resolve it: Conducting experiments with different methods of selecting rejected completions (e.g., using a fixed set, clustering similar responses, or using a different metric to determine opposition) and comparing the results in terms of stability, empathy scores, and generalization performance.

### Open Question 2
- Question: How does the performance of the proposed method scale with different foundational LLMs, and what are the limitations of using smaller models like Zephyr-7B?
- Basis in paper: [inferred] The paper uses Zephyr-7B, a 7 billion parameter model, and mentions that it matches Llama-2-70B on many NLP benchmarks. However, it does not explore the performance of the method with larger or smaller models.
- Why unresolved: The paper does not provide a comparative analysis of the method's performance across different sizes of foundational LLMs, which could reveal limitations or advantages of using smaller models.
- What evidence would resolve it: Conducting experiments with various foundational LLMs of different sizes and comparing their performance in terms of empathy generation, generalization, and computational efficiency.

### Open Question 3
- Question: How does the proposed method handle the nuances of different cultural contexts in empathetic response generation, and what are the implications for global deployment?
- Basis in paper: [inferred] The paper does not address the cultural aspects of empathetic response generation, which is crucial for global deployment of conversational agents.
- Why unresolved: The paper focuses on the technical aspects of empathetic response generation but does not consider the cultural nuances that may affect the perception and effectiveness of empathy in different contexts.
- What evidence would resolve it: Conducting cross-cultural studies to evaluate the effectiveness of the generated empathetic responses in different cultural contexts and analyzing the need for culturally specific datasets or adaptations in the model.

## Limitations
- The approach relies on emotion opposites from Plutchik's wheel, assuming consistent emotional polarity in EmpatheticDialogues without validation
- The effectiveness of synthetic preference data versus human-labeled preferences for DPO training is not empirically compared
- The specific LoRA configuration (r=1024, α=256) lacks theoretical justification and corpus validation

## Confidence
- **High confidence**: The overall framework of using emotion grounding to construct preference datasets for DPO training is sound and well-motivated
- **Medium confidence**: The specific implementation details (LoRA hyperparameters, beta value) appear reasonable but lack comprehensive validation
- **Low confidence**: The assumption that opposite emotions reliably indicate non-empathetic responses is the weakest link in the methodology

## Next Checks
1. Conduct ablation studies comparing diff-Epitome performance when using emotion opposites versus random non-ground truth responses as rejected completions to validate the emotion grounding approach
2. Test the sensitivity of results to different LoRA configurations (varying rank and alpha) to establish optimal hyperparameter settings
3. Evaluate the stability of training by running multiple trials with different random seeds and measuring variance in empathy metrics to assess robustness of the approach