---
ver: rpa2
title: Automated Machine Learning in Insurance
arxiv_id: '2408.14331'
source_url: https://arxiv.org/abs/2408.14331
tags:
- automl
- data
- ensemble
- insurance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an automated machine learning (AutoML) framework
  tailored for the insurance domain, addressing challenges like imbalanced datasets
  and data preprocessing. The proposed AutoML integrates data encoding, imputation,
  balancing, scaling, and feature selection with model selection and hyperparameter
  optimization using a combined algorithm selection and hyperparameter optimization
  (CASH) approach.
---

# Automated Machine Learning in Insurance

## Quick Facts
- arXiv ID: 2408.14331
- Source URL: https://arxiv.org/abs/2408.14331
- Authors: Panyi Dong; Zhiyu Quan
- Reference count: 40
- One-line primary result: AutoML framework outperforms traditional models like GLM on real-world insurance datasets with imbalanced data

## Executive Summary
This paper introduces an automated machine learning (AutoML) framework tailored for the insurance domain, addressing challenges like imbalanced datasets and data preprocessing. The proposed AutoML integrates data encoding, imputation, balancing, scaling, and feature selection with model selection and hyperparameter optimization using a combined algorithm selection and hyperparameter optimization (CASH) approach. It incorporates ensemble learning to enhance performance on imbalanced data. Experiments on real-world insurance datasets demonstrate superior performance compared to traditional models like GLM and other actuarial literature, with AutoML achieving better results in claim frequency, occurrence, and amount predictions. The framework is user-friendly, flexible, and scalable, offering a benchmark for future ML innovations in insurance.

## Method Summary
The framework implements a Combined Algorithm Selection and Hyperparameter Optimization (CASH) approach that treats model selection as a hyperparameter within a unified search space. This space includes five classes of preprocessing techniques (encoding, imputation, balancing, scaling, feature selection) combined with various ML models and their hyperparameters. The framework uses Ray Tune to coordinate the optimization, supporting multiple search algorithms like HyperOpt and Optuna. Ensemble learning is integrated through pipeline-level ensembles using stacking, bagging, and boosting strategies. The system automatically searches for optimal pipeline configurations that best handle insurance-specific challenges like imbalanced claim/non-claim data distributions.

## Key Results
- AutoML outperforms traditional GLM models on French Motor Third-Party Liability dataset with Poisson deviance improvements
- Framework achieves superior R² scores on Wisconsin Local Government Property Insurance Fund dataset with transformed response variables
- AutoML demonstrates better AUC scores on Australian Automobile Insurance claim occurrence classification tasks
- Ensemble learning strategies effectively address class imbalance while maintaining state-of-the-art performance
- Comprehensive preprocessing automation extracts maximum value from low-quality insurance datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The combined algorithm selection and hyperparameter optimization (CASH) approach eliminates the need for manual model selection and hyperparameter tuning.
- Mechanism: CASH treats the choice of ML model as a hyperparameter within a unified search space that includes both preprocessing techniques and model architectures. The framework searches this combined space to find the optimal pipeline configuration.
- Core assumption: A single global search over model and preprocessing space can find better solutions than sequential model selection followed by hyperparameter optimization.
- Evidence anchors:
  - [abstract]: "It incorporates ensemble learning to enhance performance on imbalanced data" and "Experiments on real-world insurance datasets demonstrate superior performance compared to traditional models like GLM"
  - [section 2]: "we adopt a similar approach to Auto-WEKA, proposed by Thornton et al. (2013). This approach merges the problems of Model Selection and HPO into a combined algorithm selection and hyperparameter optimization (CASH) problem"
  - [corpus]: Weak - related papers focus on general AutoML frameworks rather than insurance-specific implementations
- Break condition: The combined search space becomes computationally intractable, or the search algorithm fails to explore the space effectively due to its high dimensionality.

### Mechanism 2
- Claim: Ensemble learning addresses the imbalance problem inherent in insurance datasets by combining multiple models' predictions.
- Mechanism: The framework creates pipeline-level ensembles using stacking, bagging, and boosting strategies. Multiple trained pipelines are combined through voting mechanisms to improve minority class prediction accuracy.
- Core assumption: Individual models that struggle with imbalanced data can collectively produce better minority class predictions when their outputs are aggregated appropriately.
- Evidence anchors:
  - [abstract]: "incorporates ensemble learning to enhance performance on imbalanced data"
  - [section 3.3]: "ensemble models combine multiple trained models into a single model by aggregating their predictions, potentially addressing the imbalance problem"
  - [section 3.3]: "As summarized by Sagi and Rokach (2018), ensemble learning not only achieves state-of-the-art performance, but also has the potential to address the imbalance problem effectively"
  - [corpus]: Weak - related papers mention ensemble selection but don't specifically address insurance domain imbalance issues
- Break condition: Ensemble voting mechanisms fail to properly weight minority class predictions, or the computational cost of maintaining multiple models outweighs performance gains.

### Mechanism 3
- Claim: Comprehensive data preprocessing automation extracts maximum value from low-quality insurance data.
- Mechanism: The framework includes five classes of preprocessing techniques (encoding, imputation, balancing, scaling, feature selection) that are automatically optimized as part of the pipeline search. This addresses data quality issues common in insurance datasets.
- Core assumption: Systematic exploration of preprocessing combinations can compensate for data quality issues that would otherwise limit model performance.
- Evidence anchors:
  - [section 3]: "Our AutoML incorporates comprehensive data preprocessing techniques to extract as much valuable information as possible through automated trial and error"
  - [section 3.1]: Detailed description of the five preprocessing components and their insurance-specific relevance
  - [corpus]: Weak - related papers discuss AutoML preprocessing generally but not specifically for insurance domain challenges
- Break condition: Preprocessing optimization fails to improve data quality sufficiently, or certain preprocessing steps introduce artifacts that degrade model performance.

## Foundational Learning

- Concept: Imbalanced learning techniques (oversampling, undersampling, cost-sensitive methods)
  - Why needed here: Insurance datasets typically have rare claim events (minority class) versus non-claim events (majority class), requiring specialized handling to avoid model bias toward the majority class
  - Quick check question: What's the typical imbalance ratio in insurance claim datasets, and how does this affect standard ML models?

- Concept: Ensemble learning strategies (stacking, bagging, boosting)
  - Why needed here: Multiple models can be combined to improve minority class prediction while achieving state-of-the-art performance, addressing both imbalance and general accuracy concerns
  - Quick check question: How do the voting mechanisms differ between regression and classification tasks in ensemble models?

- Concept: Hyperparameter optimization algorithms (Bayesian optimization, random search, grid search)
  - Why needed here: The framework must efficiently search a high-dimensional space combining preprocessing methods, models, and their hyperparameters within computational constraints
  - Quick check question: What are the tradeoffs between exploration and exploitation in different hyperparameter search algorithms?

## Architecture Onboarding

- Component map: Dataset -> Search Space Definition -> Search Algorithm Execution -> Pipeline Evaluation -> Ensemble Construction -> Final Model Output
- Critical path: Dataset → Search Space Definition → Search Algorithm Execution → Pipeline Evaluation → Ensemble Construction → Final Model Output
- Design tradeoffs:
  - Computational cost vs. search space coverage: Larger search spaces provide better solutions but require more resources
  - Ensemble complexity vs. interpretability: More complex ensembles improve performance but reduce model transparency
  - Preprocessing automation vs. domain knowledge: Full automation vs. allowing expert intervention
- Failure signatures:
  - Performance plateaus despite increased computational budget (search space too limited or poorly explored)
  - Large gap between train and test metrics (overfitting or insufficient regularization)
  - Ensemble models underperform individual pipelines (voting mechanisms not well-suited to the task)
- First 3 experiments:
  1. French Motor Third-Party Liability dataset (regression, Poisson deviance): Simple test of framework on standard insurance dataset
  2. Wisconsin Local Government Property Insurance Fund dataset (regression, R²): Tests handling of transformed response variables and coverage features
  3. Australian Automobile Insurance claim occurrence (classification, AUC): Tests binary classification with cross-validation on imbalanced data

Each experiment should start with default parameters and gradually increase evaluation budget to observe performance scaling. Monitor runtime vs. performance improvements to identify optimal resource allocation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AutoML compare to manual feature engineering and model selection by human experts in the insurance domain?
- Basis in paper: [explicit] The paper mentions that AutoML can serve as a benchmark for evaluating future ML innovations in the insurance domain and can provide insights that users may have overlooked.
- Why unresolved: The paper primarily focuses on comparing AutoML's performance to traditional models like GLM and other actuarial literature. It does not directly compare AutoML to manual feature engineering and model selection by human experts.
- What evidence would resolve it: A study comparing the performance of AutoML to manually engineered models by human experts on the same insurance datasets would provide evidence to answer this question.

### Open Question 2
- Question: What is the impact of different search algorithms on the performance and runtime of AutoML in the insurance domain?
- Basis in paper: [explicit] The paper mentions that the selection of an appropriate search algorithm significantly influences the attainment of the optimal solution and efficiency.
- Why unresolved: The paper uses different search algorithms (HyperOpt, Optuna) in the experiments but does not explicitly analyze the impact of different search algorithms on performance and runtime.
- What evidence would resolve it: A study comparing the performance and runtime of AutoML using different search algorithms on the same insurance datasets would provide evidence to answer this question.

### Open Question 3
- Question: How does the ensemble strategy (stacking, bagging, boosting) impact the performance of AutoML on imbalanced insurance datasets?
- Basis in paper: [explicit] The paper mentions that ensemble learning offers alternative solutions to the imbalance problem and integrates ensemble learning into the AutoML to further address the imbalance problem.
- Why unresolved: The paper does not provide a detailed analysis of how different ensemble strategies impact the performance of AutoML on imbalanced insurance datasets.
- What evidence would resolve it: A study comparing the performance of AutoML using different ensemble strategies (stacking, bagging, boosting) on imbalanced insurance datasets would provide evidence to answer this question.

## Limitations
- Lack of detailed computational resource specifications and runtime measurements for different evaluation budgets
- Limited systematic benchmarking against traditional GLM models across diverse insurance scenarios
- Scalability validation needed with larger, more complex insurance datasets reflecting real-world production environments
- Specific implementation details of ensemble voting mechanisms and insurance domain adaptations not fully detailed

## Confidence
- **High confidence**: The core CASH approach combining model selection and hyperparameter optimization is well-established in AutoML literature and the mechanism is technically sound
- **Medium confidence**: Ensemble learning effectiveness for imbalanced insurance data, as the specific voting mechanisms and their insurance domain adaptation are not fully detailed
- **Medium confidence**: Data preprocessing automation claims, since while comprehensive, the insurance-specific optimizations are not extensively validated across diverse data quality scenarios

## Next Checks
1. **Computational Scaling Test**: Measure runtime and performance as evaluation budget increases from 10 to 1000 pipeline evaluations on the French Motor dataset to identify optimal resource allocation points
2. **Ensemble Voting Mechanism Validation**: Compare different voting strategies (weighted averaging, stacking meta-learners) on the Wisconsin property insurance dataset to determine which best handles transformed response variables
3. **Cross-Domain Generalization**: Apply the trained framework from French motor insurance to Australian automobile claims data without retraining to test domain transferability and identify preprocessing gaps