---
ver: rpa2
title: An Initial Investigation of Language Adaptation for TTS Systems under Low-resource
  Scenarios
arxiv_id: '2406.08911'
source_url: https://arxiv.org/abs/2406.08911
tags:
- language
- languages
- speech
- data
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of language adaptation
  for a self-supervised learning-based multilingual TTS system (ZMM-TTS) across 12
  diverse languages in low-resource scenarios. Experiments show that phonetic similarity
  and language category (tonal vs non-tonal) significantly impact adaptation performance.
---

# An Initial Investigation of Language Adaptation for TTS Systems under Low-resource Scenarios

## Quick Facts
- arXiv ID: 2406.08911
- Source URL: https://arxiv.org/abs/2406.08911
- Reference count: 0
- Key outcome: Language adaptation effectiveness for ZMM-TTS across 12 languages depends on phonetic similarity, language category, and fine-tuning data configuration.

## Executive Summary
This paper investigates language adaptation for a self-supervised learning-based multilingual TTS system (ZMM-TTS) across 12 diverse languages under low-resource scenarios. The study systematically evaluates adaptation performance using various fine-tuning configurations (paired data, audio-only, zero-shot) with different dataset sizes and speaker numbers. Key findings include the significant impact of phonetic similarity between pre-training and target languages on adaptation success, and surprisingly, that audio-only fine-tuning often outperforms paired data fine-tuning when extremely limited data is available. The research provides valuable insights into the complexities of adapting TTS systems to diverse languages with limited resources.

## Method Summary
The study adapts ZMM-TTS, a multilingual TTS system built on self-supervised learning representations, from six pre-training languages (English, French, German, Portuguese, Spanish, Swedish) to twelve target languages (Bulgarian, Croatian, Czech, Dutch, Italian, Japanese, Korean, Chinese, Polish, Russian, Turkish, Vietnamese). Fine-tuning is performed using different configurations: zero-shot, paired data (text + audio), and audio-only. The model is evaluated across multiple metrics including character error rate (CER), language identification probability (LI), speaker encoder cosine similarity (SECS), and predicted mean opinion score (UT-MOS). The study systematically varies fine-tuning dataset sizes and speaker diversity to understand their impact on adaptation performance.

## Key Results
- Phonetic similarity between pre-training and target languages significantly impacts adaptation performance, with higher angular similarity of phone frequencies (ASPF) correlating with lower CER.
- Audio-only fine-tuning often outperforms paired data fine-tuning with extremely small datasets (20 samples), likely due to reduced overfitting risk.
- Increasing fine-tuning dataset size and speaker diversity improves adaptation performance, with speaker diversity particularly beneficial for speaker similarity generalization.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Phonetic similarity between pre-training and target languages improves TTS adaptation performance.
- Mechanism: Angular similarity of phone frequencies (ASPF) captures how closely the phonological systems align. Higher ASPF values indicate that the target language shares more phonemes with the pre-training languages, allowing the model to better generalize pronunciation rules.
- Core assumption: The model's phonetic knowledge transfers effectively across similar phonological inventories.
- Evidence anchors:
  - [abstract] "We demonstrate that the similarity in phonetics between the pre-training and target languages, as well as the language category, affects the target language's adaptation performance."
  - [section] "Figure 1 also shows the most similar language along with their ASPF values. Firstly, we found strong correlations (r = −0.630, p = 0.028), using Pearson correlation coefficient (PCC), between the ASPF and the CER values."
  - [corpus] Weak. Corpus evidence focuses on low-resource settings but does not directly test phonetic similarity transfer. Only one related paper mentions low-resource languages without phonetic analysis.
- Break condition: Break occurs when target language has phonologically distant inventory (e.g., tonal vs non-tonal) with minimal overlap in phoneme set, causing ASR/CER to degrade despite fine-tuning.

### Mechanism 2
- Claim: Audio-only fine-tuning can outperform paired data fine-tuning in extremely low-resource scenarios.
- Mechanism: With very few paired samples, the model overfits to the limited text-audio alignment, harming generalization. Audio-only fine-tuning updates the acoustic decoder while keeping the linguistic encoder frozen, reducing overfitting risk.
- Core assumption: The linguistic encoder already captures robust phoneme representations from pre-training; acoustic modeling benefits more from audio-only adaptation.
- Evidence anchors:
  - [abstract] "Surprisingly, we also observed that using paired data for fine-tuning is not always optimal compared to audio-only data."
  - [section] "With limited data, fine-tuning both of txt2vec and vec2wav is not always the best option. Using an extremely small amount of data, such as only 20 paired samples, can make the model prone to overfitting, compromising the adaptability of the pre-trained model."
  - [corpus] Weak. Corpus contains papers on low-resource adaptation but no direct comparison of audio-only vs paired fine-tuning.
- Break condition: Break when paired data size increases beyond ~100 samples, at which point alignment supervision becomes beneficial and audio-only lag behind.

### Mechanism 3
- Claim: Increasing speaker diversity in fine-tuning data improves speaker similarity generalization.
- Mechanism: Training on multiple speakers forces the model to learn speaker-invariant acoustic features, improving its ability to synthesize unseen speakers.
- Core assumption: Speaker embeddings and acoustic patterns are decoupled in the architecture, so more speakers improve generalization without harming language-specific adaptation.
- Evidence anchors:
  - [section] "For speaker similarity, fine-tuning with data from the target speaker improves speaker similarity of synthesized audio compared to unseen speakers. Furthermore, with total training data unchanged, increasing the number of utterances from the target speaker in training data is more effective in enhancing the similarity than increasing the number of speakers."
  - [abstract] "Additionally, we find that the fine-tuning dataset size and number of speakers influence adaptability."
  - [corpus] Weak. No corpus evidence directly addresses speaker diversity impact; most papers focus on few-shot or multilingual TTS without explicit speaker generalization analysis.
- Break condition: Break when speaker diversity is increased at the expense of language sample size, leading to worse intelligibility due to insufficient language coverage.

## Foundational Learning

- Concept: Self-supervised learning (SSL) representations in multilingual speech models.
  - Why needed here: ZMM-TTS relies on XLSR-derived discrete speech representations; understanding SSL helps explain why pre-training on six languages enables adaptation to twelve others.
  - Quick check question: What is the main advantage of using SSL representations versus supervised multilingual embeddings in low-resource TTS?

- Concept: Angular similarity of phone frequencies (ASPF).
  - Why needed here: ASPF quantifies phonetic similarity between languages, directly explaining why some languages adapt better than others.
  - Quick check question: If two languages have ASPF = 0.9, what does that imply about their phoneme inventories?

- Concept: Overfitting in low-resource fine-tuning.
  - Why needed here: Explains why paired data fine-tuning with very few samples can harm performance, motivating audio-only fine-tuning as an alternative.
  - Quick check question: What is the typical sign that a model is overfitting when fine-tuning on a tiny dataset?

## Architecture Onboarding

- Component map: Input text -> XPhoneBERT -> phoneme embeddings -> txt2vec encoder -> discrete SSL tokens -> txt2vec decoder -> target discrete tokens -> vec2wav encoder -> acoustic features -> vec2wav decoder -> waveform

- Critical path:
  1. Input text → XPhoneBERT → phoneme embeddings.
  2. Phoneme embeddings → txt2vec encoder → discrete SSL tokens.
  3. Discrete tokens + durations → txt2vec decoder → target discrete tokens.
  4. Target discrete tokens → vec2wav encoder → acoustic features.
  5. Acoustic features → vec2wav decoder → waveform.

- Design tradeoffs:
  - Paired vs audio-only fine-tuning: Paired offers alignment supervision but risks overfitting; audio-only is safer with tiny data but slower convergence.
  - Speaker diversity vs sample size: More speakers improve generalization but reduce per-speaker samples, potentially hurting speaker similarity.
  - ASR-based CER vs subjective MOS: ASR is cheap and scalable but less reliable for low-resource languages; MOS is accurate but expensive.

- Failure signatures:
  - CER remains high (>40%) even after fine-tuning → phonetic mismatch or tonal language challenge.
  - SECS for unseen speakers plateaus despite more data → insufficient speaker diversity in pre-training.
  - UT-MOS predicts highest MOS for zero-shot → model overfits to pre-training naturalness, not target language intelligibility.

- First 3 experiments:
  1. Run zero-shot inference on a target language to establish baseline CER and SECS.
  2. Fine-tune with 20 paired samples; measure CER, SECS, LI; compare to zero-shot.
  3. Fine-tune with 20 audio-only samples; measure same metrics; compare to paired result.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the quality of self-supervised representations (e.g., from XLSR) vary significantly across different low-resource languages, and how does this impact downstream TTS performance?
- Basis in paper: [explicit] The paper notes that 5 languages (Bulgarian, Croatian, Czech, Japanese, Korean) are not covered in XLSR, and their adaptation performance may be affected by this absence.
- Why unresolved: The paper does not directly compare adaptation performance between languages covered and not covered by XLSR, leaving the impact of representation quality on TTS performance unclear.
- What evidence would resolve it: Comparative experiments measuring TTS performance (e.g., CER, speaker similarity) between languages covered and not covered by XLSR would clarify the impact of representation quality.

### Open Question 2
- Question: How do different text representations (e.g., characters, phonemes, pre-trained phoneme representations) affect the performance of low-resource language TTS when using self-supervised learning?
- Basis in paper: [explicit] The paper mentions that ZMM-TTS tested three different text representations but focuses only on pre-trained phoneme representations, stating their advantages in low-resource scenarios.
- Why unresolved: The paper does not provide a direct comparison of the three text representation methods in the current study, leaving their relative effectiveness unexplored.
- What evidence would resolve it: Experiments comparing TTS performance (e.g., intelligibility, naturalness) using different text representations for the same low-resource languages would reveal their relative effectiveness.

### Open Question 3
- Question: What is the optimal balance between fine-tuning dataset size and speaker diversity for achieving the best TTS performance in low-resource scenarios?
- Basis in paper: [explicit] The paper finds that increasing the amount of data in the training set enhances fine-tuning performance and that increasing the number of utterances from the target speaker is more effective than increasing the number of speakers.
- Why unresolved: The paper does not explore the interaction between dataset size and speaker diversity, nor does it determine the optimal balance for maximizing TTS performance.
- What evidence would resolve it: Systematic experiments varying both dataset size and speaker diversity to measure their combined effect on TTS performance metrics would identify the optimal balance.

## Limitations
- Evaluation relies heavily on automatic metrics rather than human perceptual studies, potentially missing subjective quality differences in low-resource languages.
- ASR-based CER evaluation may be unreliable for low-resource languages due to lack of robust ASR systems, potentially biasing results.
- The study does not systematically explore the impact of text normalization differences across languages on adaptation performance.

## Confidence

**High Confidence:** The correlation between phonetic similarity (ASPF) and adaptation performance is well-supported by the data, with Pearson correlation coefficient showing r = -0.630 (p = 0.028) between ASPF and CER values. The finding that audio-only fine-tuning outperforms paired data fine-tuning with extremely small datasets (20 samples) is also strongly supported by the experimental results and provides a clear mechanism (reduced overfitting risk).

**Medium Confidence:** The claim that increasing speaker diversity improves speaker similarity generalization is supported but the evidence is weaker, with only directional results and no controlled experiments isolating speaker diversity effects from sample size effects. The observation that performance improves with increased fine-tuning data size is expected and moderately well-supported, though the specific thresholds where different approaches become optimal are not precisely characterized.

**Low Confidence:** The interpretation of automatic MOS predictions (UT-MOS) as reliable quality indicators is questionable given the paper's own observation that UT-MOS sometimes predicts highest MOS for zero-shot outputs, contradicting intelligibility improvements. The generalisability of findings across all low-resource languages is uncertain due to the specific language set chosen and potential sampling bias.

## Next Checks

1. **Human Evaluation Validation:** Conduct comprehensive human perceptual studies measuring intelligibility, naturalness, and speaker similarity across the same language pairs and fine-tuning conditions to verify whether automatic metrics accurately predict human preferences, particularly for low-resource languages where ASR-based CER may be unreliable.

2. **Controlled Speaker Diversity Experiment:** Design an experiment that isolates speaker diversity from sample size by keeping total utterances constant while systematically varying the number of speakers and utterances per speaker, then measure the trade-off between speaker generalization and language-specific adaptation quality.

3. **Phonetic Distance Threshold Analysis:** Perform a more granular analysis of phonetic similarity by testing adaptation performance across different ASPF value ranges to identify the threshold below which adaptation becomes ineffective, and test whether this threshold varies by language family or tonal category.