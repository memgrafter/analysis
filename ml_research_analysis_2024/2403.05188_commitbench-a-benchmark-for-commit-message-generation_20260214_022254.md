---
ver: rpa2
title: 'CommitBench: A Benchmark for Commit Message Generation'
arxiv_id: '2403.05188'
source_url: https://arxiv.org/abs/2403.05188
tags:
- commit
- dataset
- message
- data
- messages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CommitBench, a large-scale, high-quality
  dataset for commit message generation that addresses privacy, license, and reproducibility
  issues in previous datasets. The authors compiled commits from 72k repositories
  across 6 programming languages, applying extensive filtering to ensure quality.
---

# CommitBench: A Benchmark for Commit Message Generation

## Quick Facts
- arXiv ID: 2403.05188
- Source URL: https://arxiv.org/abs/2403.05188
- Reference count: 40
- Key outcome: Introduces CommitBench, a high-quality dataset for commit message generation that addresses privacy and license issues, with models trained on it achieving higher C-GOOD scores and producing more diverse outputs than those trained on previous datasets.

## Executive Summary
CommitBench is a new large-scale dataset for commit message generation that addresses key limitations in existing datasets, including privacy concerns, license issues, and lack of diversity. The dataset contains commits from 72,000 repositories across 6 programming languages, with extensive filtering to ensure quality. The authors demonstrate that models trained on CommitBench outperform those trained on previous datasets, with multi-language training providing additional benefits. The repository-based splitting strategy has minimal impact, highlighting the dataset's diversity.

## Method Summary
The authors compiled commits from 72,000 repositories across 6 programming languages, applying multiple filtering stages including removal of bot commits, binary changes, trivial messages, and non-English/non-code changes. They used Transformer-based models (T5 and CodeTrans) pretrained on source code, fine-tuned on the CommitBench dataset. The dataset was split using both random and repository-based approaches to evaluate diversity. Models were evaluated using standard metrics (BLEU, ROUGE-L, METEOR) plus C-GOOD for semantic relevance.

## Key Results
- Models trained on CommitBench achieve significantly higher C-GOOD scores than those trained on previous datasets
- Repository-based splitting has minimal impact on CommitBench, demonstrating its high diversity
- Multi-language training consistently outperforms monolingual training across all evaluated models
- CommitBench models produce more diverse outputs compared to models trained on datasets with lower quality messages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CommitBench's extensive filtering significantly improves dataset quality compared to previous commit message generation datasets.
- Mechanism: Multiple filtering stages eliminate noise including bot commits, binary changes, trivial messages, and non-English/non-code changes that would mislead model training.
- Core assumption: Removing low-quality commits and ensuring commits contain actual source code changes improves the signal-to-noise ratio for training models.
- Evidence anchors: Abstract states filtering improves generated message quality; filtering methods section details specific criteria like removing commits containing "bot" word.

### Mechanism 2
- Claim: Repository-based splitting has minimal impact on CommitBench due to its high diversity across repositories.
- Mechanism: Sourcing from 72,000 repositories instead of 500 ensures training and test sets contain commits from different repositories, reducing code overlap.
- Core assumption: Greater repository diversity means less code overlap between training and test sets, making random and repository-based splits similarly effective.
- Evidence anchors: Abstract highlights minimal repository-based split impact; section notes CommitBench uses 150x more repositories than MCMD.

### Mechanism 3
- Claim: Multi-language training outperforms monolingual training for commit message generation.
- Mechanism: Training on multiple programming languages exposes models to broader syntactic and semantic patterns, improving generalization across different codebases.
- Core assumption: Learning from diverse programming languages teaches models fundamental principles common across languages, enhancing overall performance.
- Evidence anchors: Abstract states multilingual training outperforms monolingual; section reports multilingual setup consistently outperforms monolingual.

## Foundational Learning

- Concept: Sequence-to-sequence modeling
  - Why needed here: Commit message generation translates code diffs (input sequence) into commit messages (output sequence), requiring sequence modeling understanding.
  - Quick check question: What is the typical relationship between input (diff) and output (commit message) sequence lengths in commit message generation?

- Concept: Tokenization and sequence length limitations
  - Why needed here: Models have maximum sequence lengths they can process, so handling long diffs and commit messages is crucial for dataset preprocessing and model architecture.
  - Quick check question: Why does the paper use 512-token maximum for diffs and 128-token maximum for messages, and what happens to longer sequences?

- Concept: Evaluation metrics for natural language generation
  - Why needed here: Assessing generated commit message quality requires appropriate metrics beyond simple token overlap, such as C-GOOD which evaluates semantic relevance.
  - Quick check question: How does C-GOOD differ from traditional metrics like BLEU or ROUGE-L in evaluating commit message quality?

## Architecture Onboarding

- Component map: Repository selection (CodeSearchNet) -> Data extraction (commit message, diff, metadata) -> Filtering (bot removal, binary changes, trivial messages, etc.) -> Deduplication -> Privacy handling -> Splitting (train/validation/test) -> Release
- Critical path: Repository selection → Data extraction → Filtering → Deduplication → Privacy handling → Splitting → Release
- Design tradeoffs: Extensive filtering improves quality but may reduce dataset size; multi-language training improves generalization but requires handling diverse syntax; privacy measures protect individuals but require careful implementation.
- Failure signatures: Models performing well on validation but poorly on real-world data suggests overfitting or insufficient diversity; low output diversity indicates models are exploiting dataset patterns rather than learning meaningful representations.
- First 3 experiments:
  1. Train a simple Transformer model on CommitBench and evaluate using standard metrics (BLEU, ROUGE-L, METEOR, C-GOOD) to establish baseline performance.
  2. Compare model performance when trained on random vs. repository-based splits to verify diversity claims.
  3. Train separate models for each programming language versus a single multi-language model to validate multi-language training benefits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of commit message generation models vary when evaluated on multi-line commit messages compared to single-line messages?
- Basis in paper: [explicit] The paper mentions that the majority of human-written messages are single-line, and some approaches specifically target this format. It also notes that many generated messages are short and less informative, particularly when trained on datasets with low-quality messages.
- Why unresolved: The paper does not directly compare model performance on single-line vs. multi-line messages, focusing instead on overall diversity and quality metrics.
- What evidence would resolve it: Conducting a detailed analysis comparing model performance metrics (BLEU, ROUGE-L, C-GOOD) on single-line vs. multi-line commit messages would provide insights into how message length affects generation quality.

### Open Question 2
- Question: What is the impact of including contextual information (e.g., issue IDs, related documentation) in commit message generation models, and how does it affect the informativeness and relevance of generated messages?
- Basis in paper: [inferred] The paper discusses replacing irrelevant information like issue IDs and URLs with unique tokens, but does not explore the potential benefits of incorporating such context into the generation process. It also mentions the importance of understanding the broader context of a commit for effective message generation.
- Why unresolved: The paper focuses on improving dataset quality and model performance through filtering and deduplication, but does not investigate the role of contextual information in enhancing commit message generation.
- What evidence would resolve it: Training and evaluating models with and without access to contextual information (issue descriptions, related documentation) would demonstrate the impact of context on message informativeness and relevance.

### Open Question 3
- Question: How do different programming languages influence the syntactic and semantic patterns in commit messages, and can this knowledge be leveraged to improve cross-language commit message generation?
- Basis in paper: [explicit] The paper discusses challenges of commit message generation across different programming languages and presents a multilingual training approach. It also notes that some languages' syntax may better align with that of programming languages, potentially affecting commit message patterns.
- Why unresolved: While the paper explores multilingual training and its benefits, it does not delve into specific syntactic and semantic differences between programming languages and how they impact commit message generation.
- What evidence would resolve it: Conducting a detailed analysis of commit message patterns across different programming languages, identifying language-specific syntactic and semantic features, and evaluating their impact on cross-language generation models would provide insights into leveraging language knowledge for improved performance.

## Limitations
- Dataset filtering criteria may exclude legitimate commit patterns that models should learn to handle
- 512-token limit for diffs and 128-token limit for messages may exclude substantial portions of longer, more complex changes
- Limited evaluation of how well models perform on real-world, unseen repositories outside the training distribution

## Confidence

**High Confidence**:
- Dataset construction methodology and filtering approach are sound and address known issues in previous commit message generation datasets
- Transformer-based models pretrained on source code consistently outperform other approaches across multiple datasets
- CommitBench produces more diverse outputs than models trained on previous datasets

**Medium Confidence**:
- Repository-based splitting has minimal impact due to CommitBench's diversity (based on dataset statistics but requires real-world validation)
- Multi-language training outperforms monolingual training (supported by experimental results but could vary with different model architectures)

**Low Confidence**:
- The claim that C-GOOD scores are "significantly higher" for CommitBench models compared to previous datasets (limited comparison scope)
- The long-term effectiveness of privacy measures (no systematic evaluation of privacy protection)

## Next Checks

1. **Real-World Deployment Test**: Deploy CommitBench-trained models on commits from repositories explicitly excluded from the dataset (popular open-source projects not in CodeSearchNet) and evaluate performance drop to validate whether claimed diversity translates to real-world generalization.

2. **Filter Impact Analysis**: Train models with different filtering configurations (aggressive, moderate, minimal) on CommitBench and compare performance to quantify how much filtering improves results versus simply reducing dataset size, determining if useful patterns are being excluded.

3. **Long-Diff Handling Study**: Evaluate model performance specifically on diffs exceeding 512 tokens using truncation or sliding window approaches to understand how sequence length limitations affect real-world applicability, as many production changes involve multiple files or extensive modifications.