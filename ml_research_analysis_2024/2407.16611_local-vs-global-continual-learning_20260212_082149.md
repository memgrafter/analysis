---
ver: rpa2
title: Local vs Global continual learning
arxiv_id: '2407.16611'
source_url: https://arxiv.org/abs/2407.16611
tags:
- learning
- task
- loss
- local
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work distinguishes between local and global approximations
  in continual learning algorithms, classifying them based on whether the task loss
  approximation relies on the current network state or the task solution. Local approximations,
  which depend on information from each task solution, require the restrictive locality
  assumption that all task solutions remain close in parameter space.
---

# Local vs Global continual learning

## Quick Facts
- arXiv ID: 2407.16611
- Source URL: https://arxiv.org/abs/2407.16611
- Authors: Giulia Lanzillotta; Sidak Pal Singh; Benjamin F. Grewe; Thomas Hofmann
- Reference count: 40
- Primary result: Local approximations in continual learning require restrictive locality assumptions that cause higher forgetting when violated by larger learning rates

## Executive Summary
This paper provides a theoretical framework for classifying continual learning algorithms based on whether their task loss approximations are local (dependent on current network state) or global (independent of task solution). Local approximations require the restrictive locality assumption that all task solutions remain close in parameter space, while global approximations remain robust regardless of parameter distances. Through theoretical analysis and extensive experiments on standard benchmarks, the authors demonstrate that local algorithms like Orthogonal Gradient Descent and Elastic Weight Consolidation suffer significantly higher forgetting when this assumption is violated, while global algorithms like Experience Replay and Synaptic Intelligence maintain stable performance.

## Method Summary
The paper analyzes continual learning algorithms through the lens of task loss approximations, classifying them as local or global based on their dependence on the current network state. Local approximations use information from the task solution and require all solutions to remain close in parameter space, while global approximations are independent of θt. The authors derive optimal learning objectives for local quadratic approximations and show that Orthogonal Gradient Descent implements this objective. Experiments on Split CIFAR-10, Split Tiny-ImageNet, and Rotated-MNIST compare forgetting across algorithms at different learning rates, with local algorithms showing significantly higher forgetting when learning rates increase.

## Key Results
- Local algorithms (OGD, EWC) exhibit significantly higher forgetting when learning rates increase beyond the locality assumption's validity
- Global algorithms (ER, SI) maintain stable performance regardless of learning rate, though with slightly lower average accuracy
- The locality assumption is critical for local algorithm performance and can be empirically tested by varying learning rates
- Orthogonal Gradient Descent implements the optimal objective for local quadratic approximations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local algorithms depend on task solutions staying close in parameter space to maintain low forgetting
- Mechanism: Local approximations use information from the current network state to approximate previous task losses. The accuracy of this approximation degrades as the distance between task solutions increases, causing higher forgetting.
- Core assumption: The locality assumption - all task solutions remain within an epsilon-region of each other in parameter space
- Evidence anchors:
  - [abstract] "Local approximations, which depend on information from each task solution, require the restrictive locality assumption that all task solutions remain close in parameter space"
  - [section] "We say that the task loss approximation ˆLt(θ) is local when it uses information about the function at the task solution θt and it is accurate in a neighborhood of θt"
  - [corpus] Weak - related papers focus on optimization and approximation methods but don't directly address continual learning locality
- Break condition: When learning rates are too high, causing task solutions to drift far apart in parameter space

### Mechanism 2
- Claim: Global algorithms maintain stable performance regardless of parameter space distance between tasks
- Mechanism: Global approximations don't rely on the current network state after learning each task. They use either external memory (experience replay) or constrain parameter updates to not affect previous tasks.
- Core assumption: The approximation used doesn't depend on θt
- Evidence anchors:
  - [abstract] "global algorithms (e.g., Experience Replay, Synaptic Intelligence) maintain stable performance"
  - [section] "We say that the approximation is global when it is independent of θt, meaning that modifying θt would not change the approximation"
  - [corpus] Weak - optimization papers discuss global vs local approximations but not in continual learning context
- Break condition: When approximation error becomes too high due to poor buffer sampling or insufficient regularization strength

### Mechanism 3
- Claim: Orthogonal Gradient Descent implements the optimal objective for local quadratic approximations
- Mechanism: OGD enforces orthogonality between parameter updates and previous task output gradients, which is equivalent to constraining updates to lie in the null space of the average Hessian matrix
- Core assumption: Quadratic approximation is accurate enough around task minima
- Evidence anchors:
  - [abstract] "we show formally that orthogonal gradient descent (Farajtabar et al., 2020) implements the optimal objective for local quadratic approximations derived in Section 4"
  - [section] "OGD avoids catastrophic forgetting by enforcing orthogonality between the parameter update and the previous tasks output-gradients"
  - [corpus] Weak - related papers discuss optimization with approximate Hessians but not OGD specifically
- Break condition: When the quadratic approximation breaks down due to large parameter updates or non-convex loss landscapes

## Foundational Learning

- Concept: Taylor expansion for loss approximation
  - Why needed here: The paper uses Taylor expansions to derive optimal learning objectives for local polynomial approximations
  - Quick check question: What is the error bound for a p-th order Taylor approximation around θt?

- Concept: Hessian matrix properties in deep networks
  - Why needed here: The paper analyzes how the Hessian matrix rank affects the size of the null space where forgetting is zero
  - Quick check question: Why does a low-rank Hessian matrix make it easier to find parameter updates that don't cause forgetting?

- Concept: Mutual information for classifying approximations
  - Why needed here: The paper formally defines local vs global approximations using mutual information between the approximation and task solution
  - Quick check question: What does I(ˆLt(θ); θt) > 0 ∀ θ ∈ Θ mean in terms of information flow?

## Architecture Onboarding

- Component map:
  - Loss approximation layer: Determines whether algorithm uses local or global approximation
  - Task memory management: For global algorithms using experience replay or gradient buffers
  - Optimization constraint module: For algorithms like OGD that enforce orthogonality constraints
  - Hessian computation subsystem: For second-order regularization methods

- Critical path: Task learning → Loss approximation → Parameter update → Forgetting measurement → Performance evaluation

- Design tradeoffs:
  - Local algorithms: Lower computational cost but sensitive to learning rate and task solution distance
  - Global algorithms: More robust but require external memory or complex constraint enforcement
  - Second-order methods: More accurate forgetting prevention but expensive Hessian computations

- Failure signatures:
  - Local algorithms: Sudden increase in forgetting when learning rate increases
  - Global algorithms: Degraded performance when buffer size is too small
  - All algorithms: Poor performance when task distributions are too different

- First 3 experiments:
  1. Compare forgetting vs learning rate for local vs global algorithms on Split CIFAR-10
  2. Measure Hessian rank evolution across tasks to understand null space size
  3. Test iCarl with random vs herding buffer selection to validate locality assumption

## Open Questions the Paper Calls Out

None

## Limitations

- The locality assumption's validity across different network architectures and loss landscapes remains uncertain
- The quadratic approximation may break down for deeper networks with more complex optimization dynamics
- Standard benchmarks may not fully capture real-world complexity and task distribution shifts

## Confidence

- Local vs Global classification mechanism: High
- Locality assumption impact on forgetting: Medium
- Optimal OGD objective derivation: High
- Empirical validation across datasets: Medium

## Next Checks

1. Test the locality assumption on more diverse architectures (Transformers, RNNs) and non-image tasks (language, reinforcement learning) to assess generalizability
2. Implement and compare alternative local approximations beyond quadratic (cubic, exponential) to test if the locality principle extends to other approximation families
3. Conduct ablation studies varying task similarity and data distribution shifts to understand when the locality assumption breaks down in practice