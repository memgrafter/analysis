---
ver: rpa2
title: 'From Lab to Pocket: A Novel Continual Learning-based Mobile Application for
  Screening COVID-19'
arxiv_id: '2410.12589'
source_url: https://arxiv.org/abs/2410.12589
tags:
- learning
- covid-19
- continual
- x-ray
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a continual learning-based mobile application
  for COVID-19 screening from chest X-ray images. The approach uses deep learning
  architectures to create a foundation model, then applies continual learning strategies
  to enable the model to adapt to new data without retraining from scratch.
---

# From Lab to Pocket: A Novel Continual Learning-based Mobile Application for Screening COVID-19

## Quick Facts
- **arXiv ID**: 2410.12589
- **Source URL**: https://arxiv.org/abs/2410.12589
- **Reference count**: 40
- **Primary result**: Developed mobile app using DenseNet161 foundation model (96.87% accuracy) with LwF continual learning (71.99% overall performance)

## Executive Summary
This study presents a mobile application for COVID-19 screening using chest X-ray images with continual learning capabilities. The approach combines deep learning architectures with continual learning strategies, enabling the model to adapt to new data without retraining from scratch. DenseNet161 emerged as the optimal foundation model, while Learning without Forgetting (LwF) proved most effective for continual learning. The mobile app integrates this system on a cloud server, allowing real-time adaptation while maintaining low inference latency suitable for clinical use.

## Method Summary
The method involves developing a continual learning-based mobile application for COVID-19 screening from chest X-ray images. It uses DenseNet161 as a foundation model pre-trained on ImageNet, achieving 96.87% accuracy. The continual learning component employs Learning without Forgetting (LwF) regularization to adapt to new data while preserving previously learned knowledge. The mobile application incorporates this model on a cloud server, enabling real-time learning from new chest X-ray submissions. The system was designed for both patient and doctor use, with local validation for immediate feedback and cloud-based processing for continual updates.

## Key Results
- DenseNet161 foundation model achieved 96.87% accuracy on chest X-ray classification
- LwF continual learning method achieved 71.99% overall performance
- Cloud model inference time averaged 16.03 milliseconds with 2.28ms standard deviation
- Cloud training time averaged 980.46 milliseconds per image
- Mobile app successfully integrated with cloud-based continual learning for real-time adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continual learning enables the model to adapt to evolving datasets without retraining from scratch.
- Mechanism: The model incrementally updates its parameters using new data while preserving previously learned knowledge, preventing catastrophic forgetting through regularization or memory-based strategies.
- Core assumption: New data distributions (e.g., different virus strains or clinical presentations) can be integrated incrementally without requiring access to the original training data.
- Evidence anchors:
  - [abstract] "Our approach demonstrates the ability to adapt to evolving datasets, including data collected from different locations or hospitals, varying virus strains, and diverse clinical presentations, without retraining from scratch."
  - [section 2.1] "Continual learning... allows models to adapt continuously to changes in the clinical picture, symptoms, and infection rates associated with new COVID-19 strains."
  - [corpus] Weak evidence - corpus papers focus on general app testing and screening, not specifically on continual learning mechanisms for medical imaging adaptation.
- Break condition: If the distribution shift between new and old data is too large, or if catastrophic forgetting cannot be effectively mitigated, the model's performance will degrade despite incremental updates.

### Mechanism 2
- Claim: Using a well-established foundation model (DenseNet161) provides superior baseline performance for continual learning.
- Mechanism: DenseNet161's architecture with dense connections enables effective feature reuse and gradient flow, making it robust when adapted through continual learning strategies like LwF.
- Core assumption: Pre-trained weights from ImageNet transfer effectively to COVID-19 chest X-ray classification, and the architecture remains stable under incremental updates.
- Evidence anchors:
  - [section 5.2] "DenseNet161 emerged as the best foundation model with 96.87% accuracy, and Learning without Forgetting (LwF) was the top continual learning method with an overall performance of 71.99%."
  - [section 4.1] "DenseNet161 has a favorable size compared to VGG16 and EfficientNetB7, but MobileNetV3 had the smallest size at 19.7 MB and almost achieved the performance of EfficientNetB7."
  - [corpus] Weak evidence - corpus papers discuss app testing and screening but do not address architectural choices for continual learning in medical imaging.
- Break condition: If the foundation model's architecture proves too rigid or sensitive to incremental updates, or if transfer learning from ImageNet proves ineffective for this specific medical domain.

### Mechanism 3
- Claim: Mobile app integration with cloud-based continual learning enables real-time adaptation while maintaining low inference latency.
- Mechanism: The app uses a local validation classifier for immediate feedback, while the cloud server performs continual learning updates, balancing responsiveness with adaptability.
- Core assumption: Network connectivity is generally available for cloud communication, and the computational overhead of continual learning updates is manageable on cloud infrastructure.
- Evidence anchors:
  - [section 6.1] "The app design considers both patient and doctor perspectives. It incorporates the continual learning DenseNet161 LwF model on a cloud server, enabling the model to learn from new instances of chest X-rays and their classifications as they are submitted."
  - [section 6.5.1] "The continual learning cloud model's average inference time was 16.03 milliseconds with a standard deviation of 2.28 milliseconds."
  - [corpus] Weak evidence - corpus papers discuss general app testing but not the specific integration of cloud-based continual learning with mobile inference.
- Break condition: If network connectivity is unreliable, if cloud server latency exceeds acceptable thresholds, or if continual learning updates consume excessive cloud resources.

## Foundational Learning

- Concept: Transfer Learning vs. Continual Learning distinction
  - Why needed here: Understanding why pre-training on ImageNet plus incremental updates is different from simple transfer learning is crucial for implementing the correct approach
  - Quick check question: If I just fine-tune DenseNet161 on new COVID-19 data each time, am I doing continual learning or transfer learning?

- Concept: Catastrophic Forgetting and its mitigation strategies
  - Why needed here: The core challenge in this system is preventing performance degradation on previously learned classes while adapting to new data
  - Quick check question: What happens to the model's COVID-19 classification accuracy if I train it on pneumonia data without any forgetting mitigation?

- Concept: Supervised Learning paradigm for medical image classification
  - Why needed here: The system relies on expert-labeled data for training, requiring understanding of how labeled medical data flows through the learning pipeline
  - Quick check question: How does the system ensure that new training data comes with proper expert labels before updating the model?

## Architecture Onboarding

- Component map:
  - Mobile App (Android) -> Local validation classifier -> Firebase API client
  - Cloud Server -> Continual learning model (DenseNet161 + LwF) -> Database API client
  - Database -> User profiles, doctor-patient pairings, chest X-ray submissions, classification results

- Critical path:
  1. Patient captures/upload chest X-ray → local validation classifier → Firebase submission
  2. Cloud server receives submission → inference with continual learning model → stores result
  3. Doctor reviews result → provides expert confirmation → triggers model update
  4. Continual learning model retrains on confirmed data → updates deployed model

- Design tradeoffs:
  - Local validation vs. cloud-only inference: Local validation provides immediate feedback but adds complexity; cloud-only is simpler but slower
  - LwF regularization vs. memory-based methods: LwF preserves knowledge better for this dataset size but may be slower; memory methods are faster but require storing samples
  - Real-time updates vs. batch updates: Real-time provides immediate adaptation but higher computational load; batch is more efficient but slower to adapt

- Failure signatures:
  - High inference latency (>50ms) indicates model complexity issues or network problems
  - Degrading accuracy on COVID-19 class suggests catastrophic forgetting or distribution shift
  - Database write failures suggest connectivity or schema issues
  - Model update failures suggest training data quality or computational resource issues

- First 3 experiments:
  1. Test local validation classifier accuracy on 100 chest X-ray images to ensure it correctly filters non-X-ray inputs before cloud processing
  2. Measure cloud inference latency with 200 images to verify the 16ms target is consistently met across different network conditions
  3. Simulate model update cycle with 50 labeled expert confirmations to verify LwF regularization maintains baseline accuracy while improving on new data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the continual learning model performance compare when applied to CT scans versus chest X-rays for COVID-19 detection?
- Basis in paper: [explicit] The paper suggests implementing and expanding the continual learning approach for CT scans as a potential future research direction.
- Why unresolved: The study focused exclusively on chest X-ray images and did not evaluate the model's performance on CT scan data.
- What evidence would resolve it: Comparative experiments applying the continual learning model to both chest X-rays and CT scans, measuring accuracy, adaptability, and resource consumption for each modality.

### Open Question 2
- Question: What is the impact of different memory buffer sizes (k) on the performance of the GDUMB continual learning method for COVID-19 detection?
- Basis in paper: [explicit] The paper tested multiple memory buffer sizes (k=200, 1280, 2560, 5120) for GDUMB but only identified k=1280 as optimal.
- Why unresolved: While k=1280 was found to be optimal, the paper did not thoroughly investigate the performance trade-offs across the full range of tested buffer sizes or beyond.
- What evidence would resolve it: Comprehensive experiments varying memory buffer sizes beyond those tested, analyzing performance metrics and resource usage to identify the optimal range and limitations.

### Open Question 3
- Question: How does the mobile application's continual learning model perform in a real-world clinical environment with live patient data?
- Basis in paper: [inferred] The paper mentions conducting clinical tests in a live environment as a potential future research direction to refine the application.
- Why unresolved: The study evaluated the app's performance in controlled conditions but did not deploy it in actual clinical settings with real patient data.
- What evidence would resolve it: Clinical trials deploying the mobile application in hospitals or clinics, collecting data on diagnostic accuracy, user feedback, and integration with existing healthcare workflows.

## Limitations
- The LwF continual learning method achieved only 71.99% overall performance, significantly lower than the 96.87% foundation model accuracy, indicating persistent challenges with catastrophic forgetting
- Critical hyperparameters for LwF implementation, particularly regularization strength and memory buffer management, were not specified, limiting reproducibility
- The evaluation focuses on static dataset performance rather than true continual learning scenarios with evolving data distributions over time

## Confidence
- **High Confidence**: Foundation model selection (DenseNet161 with 96.87% accuracy) - well-supported by experimental results and established in literature
- **Medium Confidence**: LwF continual learning implementation - methodology described but critical hyperparameters unspecified
- **Low Confidence**: Real-world adaptation claims - evaluation doesn't demonstrate true continual learning with evolving distributions

## Next Checks
1. Implement and test different LwF regularization strengths on a validation set to identify optimal hyperparameters that balance knowledge retention with new learning
2. Simulate distribution shift by training on one hospital's data, then testing and updating on another hospital's data to measure catastrophic forgetting and adaptation performance
3. Deploy the mobile application with the continual learning model in a controlled clinical setting for one month, measuring inference latency, update frequency, and classification accuracy on real patient submissions