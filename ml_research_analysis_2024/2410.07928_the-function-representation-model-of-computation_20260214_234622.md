---
ver: rpa2
title: The Function-Representation Model of Computation
arxiv_id: '2410.07928'
source_url: https://arxiv.org/abs/2410.07928
tags:
- function
- cognitive
- knowledge
- output
- thus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel model of computation for Cognitive Architectures,
  where memory and program are unified into a single entity called a Function-Representation.
  This addresses the fundamental problem of knowledge retrieval heuristics in traditional
  architectures that separate memory and program.
---

# The Function-Representation Model of Computation

## Quick Facts
- arXiv ID: 2410.07928
- Source URL: https://arxiv.org/abs/2410.07928
- Reference count: 40
- The paper proposes a novel model of computation for Cognitive Architectures where memory and program are unified into a single entity called a Function-Representation.

## Executive Summary
This paper addresses the fundamental problem of knowledge retrieval in Cognitive Architectures by proposing a novel model of computation where memory and program are unified into a single entity called a Function-Representation. The authors prove that this unified approach can contain knowledge, access it without complex heuristics, and produce emergent behavior when multiple Function-Representations are connected. The theoretical framework distinguishes between additive and associative functions, exploring their respective capabilities and limitations for cognitive architecture development.

## Method Summary
The paper presents a theoretical framework with mathematical proofs establishing that functions can contain knowledge when they produce non-zero entropy, and that connections between Function-Representations can produce emergent behavior with non-linear functions. The authors explore case studies of sequential (Artificial Neural Networks), parallel, and hybrid implementations as thought experiments. The approach relies on abstract mathematical reasoning rather than empirical implementation, examining function properties and their suitability for cognitive architecture development.

## Key Results
- Function-Representations unify memory and program, eliminating the need for complex knowledge retrieval heuristics
- Mathematical proofs establish that functions contain knowledge when producing non-zero entropy
- Connected Function-Representations can produce emergent behavior that solves overall tasks
- The choice between additive and associative functions determines architecture capabilities and limitations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Function-Representation model solves the knowledge retrieval problem by unifying memory and program into a single entity.
- **Mechanism:** Instead of storing knowledge separately from processing functions, Function-Representations encode knowledge directly within their parameters and functions, allowing direct processing without complex retrieval heuristics.
- **Core assumption:** The function parameter can process any input of its data type and produce an output, with knowledge contained in the function's parameters.
- **Evidence anchors:** [abstract] "memory and program are unified into a single entity called a Function-Representation"; [section III] "the representation has to be the function... a representation that can be used as a function needs to fulfil a fundamental requisite: that it can be used to process an input and produce an output"
- **Break condition:** If the function cannot process arbitrary inputs of its data type, or if knowledge cannot be effectively encoded in the parameters.

### Mechanism 2
- **Claim:** Emergent behavior from connected Function-Representations can solve the knowledge retrieval problem.
- **Mechanism:** When multiple Function-Representations are connected in sequence or parallel, their individual processing functions combine to create higher-level behavior that can identify inputs and associate them with appropriate knowledge for processing.
- **Core assumption:** The composition of individual functions can produce emergent behavior that solves the overall task.
- **Evidence anchors:** [section IV] "we can build a setup in which the function is the one that leads the knowledge retrieval and processing... the function should produce, at the same time, the effect of a knowledge retrieval function... and also the effect of the processing function"; [section III] "from the connections between different processing units will emerge the intelligent behaviour"
- **Break condition:** If the composition of functions does not produce emergent behavior, or if the emergent behavior cannot identify appropriate knowledge for processing.

### Mechanism 3
- **Claim:** The choice between additive and associative functions determines the capabilities and limitations of the architecture.
- **Mechanism:** Additive functions (non-bijective) can overcome their limitations through emergence when connected, while associative functions (bijective) cannot, determining what kinds of problems the architecture can solve.
- **Core assumption:** The function's bijectivity determines whether it loses information and whether connections can overcome limitations.
- **Evidence anchors:** [section VI] "Additive functions will be those that keep intact the associations between the inputs... Associative functions will be those that keep intact the associations between the inputs, and thus are able to retrieve those associations also from the output"; [section VI] "Artificial Neural Networks use an additive function in their Function-Representations... they can overcome this limitation by connecting multiple Function-Representations with non-linear functions"
- **Break condition:** If additive functions cannot overcome their limitations through emergence, or if associative functions cannot represent certain types of knowledge.

## Foundational Learning

- **Concept:** Shannon's Information Theory and entropy
  - Why needed here: Used to define information content of outputs and prove that functions contain knowledge when they produce non-zero entropy
  - Quick check question: Given a random variable with uniform distribution over a set of 4 elements, what is its entropy in bits?

- **Concept:** Function composition and linearity
  - Why needed here: Essential for proving that connections between Function-Representations can produce emergent behavior only with non-linear functions
  - Quick check question: If f(x) = 2x and g(x) = x + 3, what is (g ∘ f)(x)?

- **Concept:** Bijectivity and information preservation
  - Why needed here: Used to distinguish between additive and associative functions and their capabilities for knowledge representation
  - Quick check question: Is the function f(x) = x³ bijective over the real numbers? Why or why not?

## Architecture Onboarding

- **Component map:** Input → Function-Representation processing → Connection propagation → Emergent behavior → Output selection
- **Critical path:** Input → Function-Representation processing → Connection propagation → Emergent behavior → Output selection
- **Design tradeoffs:**
  - Function complexity vs. emergent capability: Simpler functions require more connections for complex behavior
  - Parameter expressiveness vs. generalization: More parameters can encode more knowledge but may overfit
  - Parallel vs. sequential organization: Parallel offers speed but requires selection mechanisms; sequential offers depth but is slower
- **Failure signatures:**
  - No emergent behavior: Connected functions produce linear combinations instead of new capabilities
  - Knowledge retrieval failure: Function-Representations cannot identify relevant knowledge for given inputs
  - Catastrophic forgetting: Learning updates destroy previously encoded knowledge
- **First 3 experiments:**
  1. Implement a single Function-Representation with simple arithmetic function and test input-output behavior
  2. Connect two Function-Representations sequentially and verify emergent behavior (e.g., can produce outputs not possible with single function)
  3. Create parallel Function-Representations with different parameters and implement selection mechanism based on input similarity

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Function-Representations with additive functions be designed to overcome their limitations through emergence and specifically built architectures?
- **Basis in paper:** [explicit] The paper states "the most promising type of functions are the additive ones, that are the ones that can overcome its limitations through emergence, and thus have the potential to produce complex behaviours."
- **Why unresolved:** While the paper suggests additive functions can overcome limitations through emergence, it does not provide concrete examples or empirical evidence of such architectures.
- **What evidence would resolve it:** A working cognitive architecture implemented using Function-Representations with additive functions that demonstrates complex behaviors and overcomes the limitations of not being able to represent bijective functions.

### Open Question 2
- **Question:** How can the Function-Representation framework be empirically validated as a solution to the knowledge retrieval problem in cognitive architectures?
- **Basis in paper:** [explicit] The paper acknowledges that "building a whole Cognitive Architecture with this approach was not the purpose of this paper" and that empirical validation is needed.
- **Why unresolved:** The paper is purely theoretical and relies on thought experiments and analogies to Artificial Neural Networks, but lacks empirical validation of the framework's effectiveness in solving the knowledge retrieval problem.
- **What evidence would resolve it:** Implementation and testing of a cognitive architecture based on Function-Representations that demonstrates superior performance in knowledge retrieval compared to traditional architectures.

### Open Question 3
- **Question:** What are the specific properties that define which functions can be used as effective Function-Representations for building cognitive architectures?
- **Basis in paper:** [explicit] The paper explores "different alternatives, maybe more suited for Cognitive Architectures" but does not provide a definitive answer on what properties make a function suitable.
- **Why unresolved:** While the paper discusses additive vs associative functions and their limitations, it does not provide a comprehensive framework for evaluating and selecting functions for cognitive architecture development.
- **What evidence would resolve it:** A set of criteria or a decision framework for evaluating functions based on their properties (e.g., linearity, bijectivity, information preservation) and their suitability for different cognitive architecture tasks.

## Limitations
- Theoretical framework lacks empirical validation and real-world implementation evidence
- No concrete guidance for choosing appropriate function types for different cognitive tasks
- Case studies are conceptual rather than implemented, leaving practical applicability questions unanswered
- Unclear how the framework scales to complex, real-world cognitive architectures

## Confidence

- **High confidence**: The mathematical proofs that functions can contain knowledge when they produce non-zero entropy, and that connections between Function-Representations can produce emergent behavior with non-linear functions.
- **Medium confidence**: The theoretical framework for unifying memory and program, and the distinction between additive and associative functions.
- **Low confidence**: Practical implementation details, empirical validation of knowledge retrieval superiority, and guidance for choosing appropriate function types.

## Next Checks

1. **Implement a benchmark comparison**: Create a controlled experiment comparing knowledge retrieval performance between a traditional separated memory/program architecture and a Function-Representation implementation on a standard cognitive task (e.g., question answering or pattern recognition). Measure retrieval time, accuracy, and scalability.

2. **Function type experimentation**: Systematically test different function types (linear, polynomial, neural network, etc.) as Function-Representations to identify which provide the best balance of knowledge storage, retrieval efficiency, and emergent capability. Document the relationship between function complexity and emergent behavior.

3. **Scaling analysis**: Implement a small-scale Function-Representation system and gradually increase the number of connected units while monitoring emergent behavior emergence, computational complexity, and knowledge retention. Identify the point at which the architecture breaks down or becomes inefficient.