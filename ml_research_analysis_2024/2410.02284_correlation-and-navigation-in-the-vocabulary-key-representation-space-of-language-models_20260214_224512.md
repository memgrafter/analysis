---
ver: rpa2
title: Correlation and Navigation in the Vocabulary Key Representation Space of Language
  Models
arxiv_id: '2410.02284'
source_url: https://arxiv.org/abs/2410.02284
tags:
- query
- tokens
- which
- probing
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates spurious correlations in neural language
  model decoding caused by fixed vocabulary key representations. It shows that middle-ranked
  predictions are biased toward tokens distributionally similar to top predictions,
  harming sampling diversity.
---

# Correlation and Navigation in the Vocabulary Key Representation Space of Language Models

## Quick Facts
- arXiv ID: 2410.02284
- Source URL: https://arxiv.org/abs/2410.02284
- Reference count: 40
- Key outcome: Fixed vocabulary key representations cause spurious correlations in neural language model decoding, biasing middle-ranked predictions toward distributionally similar tokens and harming sampling diversity.

## Executive Summary
This paper investigates spurious correlations in neural language model decoding caused by fixed vocabulary key representations. It shows that middle-ranked predictions are biased toward tokens distributionally similar to top predictions, harming sampling diversity. To address this, the authors propose in-context navigation (ICN), an iterative method that appends explored answers to the prompt and instructs the model to generate something else, pushing the query representation away from explored keys. Experiments on knowledge probing show ICN improves precision significantly over vanilla decoding and simple rephrasing. Extending to open-ended and chain-of-thought generation, ICN increases diversity and improves self-consistency voting accuracy. The authors also reveal that key spaces converge early and remain static during fine-tuning, leading to generalization toward similar (but incorrect) keys rather than other correct ones. They suggest adding reranking or contextualization layers as potential remedies.

## Method Summary
The paper proposes in-context navigation (ICN) as an iterative decoding method to address spurious correlations in neural language model decoding. ICN works by appending explored answers to the prompt and instructing the model to generate something else, which encourages the encoder to produce a query representation with low dot products with explored keys. The method is evaluated on knowledge probing tasks using the ProbeSet with 60 categories, open-ended generation on three datasets (SST-2, AG-News, Emotion), and chain-of-thought generation on three math reasoning datasets (GSM8K, SVAMP, AQuA). The approach is tested on three language models: llama-3-8b-instruct, olmo-7b-instruct-hf, and GPT-4o for evaluation.

## Key Results
- ICN improves MAP@50 and precision in knowledge probing compared to vanilla decoding and simple rephrasing
- ICN increases unique n-gram diversity and improves self-consistency voting accuracy in open-ended and chain-of-thought generation tasks
- Key space analysis reveals that vocabulary embeddings converge early during pre-training and remain static during fine-tuning
- In-cluster tokens benefit more from fine-tuning than correct tokens, suggesting spurious correlation in generalization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The fixed vocabulary key representations introduce spurious correlations in next-token prediction (NTP) because key similarity is context-agnostic.
- **Mechanism:** Vocabulary keys are fixed vectors in the embedding space. Tokens that are distributionally similar (e.g., "P" and "Q") have high cosine similarity in this space, regardless of semantic relevance. During NTP, the query (encoded context) is compared against all keys via dot product, and high similarity between keys causes middle-ranked predictions to be biased toward tokens distributionally similar to top predictions, even if they lead to incorrect decoding paths.
- **Core assumption:** The vocabulary embedding space contains clusters of tokens with high similarity that are not semantically related in most contexts (e.g., capitalized letters, numbers, or subwords).
- **Evidence anchors:**
  - [abstract]: "However, the middle-ranked prediction is highly biased towards the tokens that are distributionally (not necessarily semantically) similar to these top ones."
  - [section 4.2]: Visualization shows incorrect tokens with high similarity to correct top tokens in the key space.
  - [corpus]: Weak—corpus mentions related work on key-space analysis but no direct evidence for this specific claim.
- **Break condition:** If the vocabulary embedding space is recontextualized or dynamically adjusted per input, or if the similarity between unrelated tokens is minimized.

### Mechanism 2
- **Claim:** In-context navigation (ICN) pushes the query representation away from explored keys by appending them to the prompt and instructing the model to generate something else.
- **Mechanism:** When explored answers are appended to the prompt (e.g., "A computer scientist other than Alan Turing is..."), the model's next-token prediction distribution eliminates or reduces the probability of those explored tokens. This encourages the encoder to produce a query representation with low dot product values with the explored keys, effectively navigating to a new region of the key space that contains unexplored, potentially correct tokens.
- **Core assumption:** The model's instruction-following capability is strong enough to suppress explored tokens when explicitly told to generate "something else."
- **Evidence anchors:**
  - [abstract]: "Specifically, we include the explored decoding results in the context and prompt the LM to generate something else, which encourages the LM to produce a query representation that has small dot products with explored keys."
  - [section 5.1]: Experiments show ICN produces queries dissimilar from explored keys compared to simple rephrasing.
  - [corpus]: Weak—corpus mentions navigation and retrieval but no direct evidence for this ICN mechanism.
- **Break condition:** If the model fails to suppress explored tokens despite the instruction, or if the appended context overwhelms the original prompt.

### Mechanism 3
- **Claim:** Early-converged key space remains static during fine-tuning, leading to generalization toward similar (but incorrect) keys rather than other correct ones.
- **Mechanism:** The key representations (vocabulary embeddings) converge early during pre-training and remain unchanged even after extensive fine-tuning (SFT and RLHF). Fine-tuning only adjusts the query encoder to push queries toward certain keys. This means when a correct token is used for fine-tuning, the model generalizes by increasing the probability of tokens in the same cluster (by similarity) rather than other correct tokens, reinforcing spurious correlations.
- **Core assumption:** The embedding layer (key space) is shallow and cannot capture complex query-key interactions, so it converges early and stays static.
- **Evidence anchors:**
  - [abstract]: "We observe the key space is almost unchanged, indicating the key representations have converged at the very early stage."
  - [section 6]: Experiments show in-cluster tokens benefit more from fine-tuning than correct tokens.
  - [corpus]: Weak—corpus mentions related work on key-space analysis but no direct evidence for this training claim.
- **Break condition:** If the key space is made dynamic or contextualized per input, or if fine-tuning also adjusts the key representations.

## Foundational Learning

- **Concept:** Next-token prediction (NTP) as a retrieval problem.
  - Why needed here: Understanding NTP as a retrieval task (query-key matching) is essential to grasp how vocabulary keys influence predictions and why spurious correlations arise.
  - Quick check question: In NTP, what are the "query" and "key" representations, and how are they used to produce the next-token probability distribution?

- **Concept:** Vector similarity and clustering in high-dimensional spaces.
  - Why needed here: The spurious correlation mechanism relies on understanding how tokens cluster in the embedding space based on distributional similarity, regardless of semantic meaning.
  - Quick check question: Why might capitalized letters like "P" and "Q" have high similarity in the vocabulary embedding space, even if they are not semantically related in most contexts?

- **Concept:** In-context learning and instruction following in LLMs.
  - Why needed here: ICN relies on the model's ability to understand and follow instructions in the prompt to suppress explored tokens and generate new ones.
  - Quick check question: How does appending "something else" to the prompt help the model navigate away from explored keys in the embedding space?

## Architecture Onboarding

- **Component map:** Input prompt → Encoder (transforms context to query vector) → Vocabulary key matrix (fixed embeddings) → Dot product → Softmax → Next-token probability distribution. ICN adds a preprocessing step: Append explored answers to prompt → Re-encode → New query → Repeat.

- **Critical path:**
  1. Encode context to query.
  2. Compute dot products with all vocabulary keys.
  3. Apply softmax to get probabilities.
  4. For ICN: Append explored answers, re-encode, repeat.

- **Design tradeoffs:**
  - Fixed keys: Efficient (no per-token computation) but prone to spurious correlations.
  - Dynamic keys: Could reduce spurious correlations but increase computational cost.
  - ICN: Simple and effective but requires multiple encodings, reducing efficiency.

- **Failure signatures:**
  - ICN fails if the model cannot suppress explored tokens despite the instruction.
  - Spurious correlations persist if the embedding space cannot be adjusted or recontextualized.
  - Fine-tuning does not improve generalization if the key space remains static.

- **First 3 experiments:**
  1. Reproduce the knowledge probing experiment: Probe top-100 first-tokens for a category, cluster keys, and measure accuracy of in-top-cluster vs. out-of-top-cluster tokens.
  2. Test ICN navigation: Compare query similarity with original query and with top keys before and after ICN.
  3. Validate key space convergence: Compare vocabulary embeddings before and after fine-tuning using cosine similarity and rank correlation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the in-context navigation (ICN) method maintain its effectiveness when applied to smaller language models that lack extensive supervised fine-tuning (SFT) or reinforcement learning with human feedback (RLHF)?
- Basis in paper: The paper explicitly mentions that ICN "might not be applicable to weaker models, especially for language models that are not trained by supervised fine-tuning (e.g., GPT-2)."
- Why unresolved: The authors note that the effectiveness of ICN may depend on the generation ability of the language model itself, but they do not provide experimental results to validate this claim across models of varying sizes or training regimes.
- What evidence would resolve it: Testing ICN on a range of models from small (e.g., GPT-2) to large (e.g., Llama-3) with varying levels of SFT/RLHF would demonstrate whether ICN's performance is contingent on model size or training quality.

### Open Question 2
- Question: How does the quality of in-context examples (i.e., the explored answers appended to the prompt) affect the navigation performance of ICN, and can incorrect or noisy examples mislead the model?
- Basis in paper: The paper discusses that "ICN requires encoding contexts to queries for multiple times, which will reduce the generation efficiency" and hints at potential issues with appending incorrect results.
- Why unresolved: While the paper acknowledges the risk of appending incorrect results, it does not explore how the quality of these examples impacts ICN's effectiveness or whether the model can recover from poor-quality examples.
- What evidence would resolve it: Experiments varying the quality of appended examples (e.g., correct vs. incorrect vs. noisy) and measuring their impact on ICN's navigation accuracy and efficiency would clarify this dependency.

### Open Question 3
- Question: Can the spurious correlation issue in the vocabulary key space be mitigated during training by adjusting the key representations or incorporating contextualization layers, and what are the trade-offs?
- Basis in paper: The authors propose adding a contextualization layer for vocabularies as a potential solution, but note that it "requires multiple times of interactions between them and the input context," suggesting computational trade-offs.
- Why unresolved: The paper discusses the theoretical benefits of contextualizing key representations but does not provide empirical evidence of their effectiveness or analyze the computational costs.
- What evidence would resolve it: Implementing and benchmarking a contextualization layer during training, comparing its performance and efficiency against baseline models, would determine its viability as a solution.

### Open Question 4
- Question: Does the spurious correlation in the key space affect the interpretability of language models, and can it be used to diagnose or improve model transparency?
- Basis in paper: The paper highlights that spurious correlation leads to "incorrect tokens into the prediction by their context-agnostic similarity," which could obscure the model's reasoning process.
- Why unresolved: While the paper identifies spurious correlation as a problem, it does not explore its implications for model interpretability or whether addressing it could enhance transparency.
- What evidence would resolve it: Analyzing the interpretability of models with and without spurious correlation (e.g., via attention visualization or feature attribution) would reveal its impact on transparency and potential diagnostic uses.

### Open Question 5
- Question: How does the early convergence of the key space during pre-training influence the generalization ability of fine-tuned models, and can this be leveraged to improve few-shot or zero-shot learning?
- Basis in paper: The authors observe that the key space "converges at the very early stage" and remains static during fine-tuning, raising questions about how this affects generalization to new tasks or domains.
- Why unresolved: The paper discusses the implications of fixed key spaces for knowledge storage but does not investigate how this property could be exploited for few-shot or zero-shot learning.
- What evidence would resolve it: Experiments testing the performance of fine-tuned models on few-shot or zero-shot tasks, comparing models with fixed vs. dynamic key spaces, would clarify the relationship between key space convergence and generalization.

## Limitations
- The key space convergence analysis relies on visualizations and similarity metrics without ablation studies showing how dynamic keys would perform.
- The ICN mechanism assumes strong instruction-following capability without testing model-specific variations in this ability.
- The proposed solutions (reranking, contextualization layers) are only briefly mentioned without experimental validation.

## Confidence
- **High Confidence:** The existence of spurious correlations due to fixed vocabulary keys
- **Medium Confidence:** The effectiveness of ICN in reducing spurious correlations and improving decoding diversity
- **Low Confidence:** The claim about early-converged key spaces causing clustering-based generalization during fine-tuning

## Next Checks
1. **Ablation Study on Key Space Dynamics:** Implement a version of the model where vocabulary keys are either (a) dynamically generated per input or (b) fine-tuned along with the query encoder, and compare spurious correlation metrics and downstream task performance against the fixed-key baseline.

2. **ICN Robustness Testing:** Test ICN across different instruction-following capabilities by comparing performance on models with varying instruction-tuning quality, and measure the correlation between instruction-following strength and ICN effectiveness in suppressing explored tokens.

3. **Generalization Impact Analysis:** Design an experiment where fine-tuning explicitly rewards diversity in correct predictions rather than clustering around similar tokens, and measure whether this improves performance on tasks where multiple correct answers exist but are distributionally distant in the embedding space.