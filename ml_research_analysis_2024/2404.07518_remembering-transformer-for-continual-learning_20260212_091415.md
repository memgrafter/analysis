---
ver: rpa2
title: Remembering Transformer for Continual Learning
arxiv_id: '2404.07518'
source_url: https://arxiv.org/abs/2404.07518
tags:
- task
- adapter
- learning
- tasks
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in continual learning
  by proposing the Remembering Transformer, which employs a mixture-of-adapters architecture
  with generative model-based novelty detection. The method dynamically routes task
  data to the most relevant adapter using low-rank autoencoders, enabling efficient
  parameter sharing.
---

# Remembering Transformer for Continual Learning

## Quick Facts
- arXiv ID: 2404.07518
- Source URL: https://arxiv.org/abs/2404.07518
- Reference count: 34
- Outperforms second-best method by 15.90% in split tasks while reducing memory footprint from 11.18M to 0.22M in CIFAR10 task

## Executive Summary
This paper addresses catastrophic forgetting in continual learning by proposing the Remembering Transformer, which employs a mixture-of-adapters architecture with generative model-based novelty detection. The method dynamically routes task data to the most relevant adapter using low-rank autoencoders, enabling efficient parameter sharing. Experiments on class-incremental split tasks and permutation tasks demonstrate state-of-the-art performance, surpassing the second-best method by 15.90% in split tasks while significantly reducing memory footprint.

## Method Summary
The method introduces a mixture-of-adapters architecture where task-specific low-rank adapter matrices are added to each attention layer of a pretrained Vision Transformer. Each adapter is sparsely activated based on novelty detection using low-rank autoencoders that encode task-specific embeddings. When adapter capacity is reached, knowledge distillation transfers relevant knowledge from the most similar old adapter to a new one, which is then trained on both new task data and replayed old task samples. This approach enables independent learning for different tasks while preserving base model parameters and achieving parameter efficiency under capacity constraints.

## Key Results
- Achieves 86.68% average accuracy on CIFAR10/5 split task compared to 70.78% for the second-best method
- Maintains competitive performance on CIFAR100/10 (61.47%) and CIFAR100/20 (47.87%) split tasks
- Reduces memory footprint from 11.18M to 0.22M trainable parameters in CIFAR10 task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixture-of-adapters architecture enables task-specific adaptation without disrupting base model knowledge
- Mechanism: Task-specific low-rank adapter matrices are added to each attention layer, sparsely activated based on novelty detection
- Core assumption: Base Vision Transformer has sufficient generalizable representation capacity
- Evidence anchors: [abstract], [section 3.2], [corpus]
- Break condition: Insufficient base model representation capacity for diverse tasks

### Mechanism 2
- Claim: Generative model-based novelty detection enables adaptive routing without task identity information
- Mechanism: Low-rank autoencoders encode task-specific embeddings; reconstruction losses indicate task similarity
- Core assumption: Reconstruction loss reliably proxies for task similarity in embedding space
- Evidence anchors: [abstract], [section 3.3], [section 4.4]
- Break condition: Overlapping but distinct task characteristics may confuse routing signals

### Mechanism 3
- Claim: Adapter fusion with knowledge distillation enables parameter-efficient learning under capacity constraints
- Mechanism: Knowledge from most relevant old adapter is distilled into new adapter using new and replayed old task samples
- Core assumption: Knowledge distillation can effectively transfer relevant task knowledge
- Evidence anchors: [abstract], [section 3.4], [section 4.3]
- Break condition: Low similarity between old and new tasks may degrade performance

## Foundational Learning

- Concept: Catastrophic Forgetting in Neural Networks
  - Why needed here: Understanding why traditional neural networks fail at continual learning is essential to appreciate the proposed solution
  - Quick check question: What happens to a neural network's performance on old tasks when it's trained on new tasks without any special mechanism?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: The method relies on LoRA for efficient adapter implementation, so understanding its mechanics is crucial
  - Quick check question: How does LoRA achieve parameter efficiency compared to full fine-tuning of all model weights?

- Concept: Mixture-of-Experts (MoE) Architecture
  - Why needed here: The method is inspired by MoE principles, so understanding how expert routing works is important
  - Quick check question: What are the key differences between traditional MoE and the mixture-of-adapters approach used here?

## Architecture Onboarding

- Component map: Input -> Autoencoder reconstruction loss computation -> Adapter routing decision -> Adapter-based attention modification -> Output prediction
- Critical path: Input → Autoencoder reconstruction loss computation → Adapter routing decision → Adapter-based attention modification → Output prediction
- Design tradeoffs:
  - Parameter efficiency vs. model capacity (limited number of adapters)
  - Routing accuracy vs. computational overhead (autoencoder complexity)
  - Knowledge preservation vs. adaptation capability (knowledge distillation balance)
- Failure signatures:
  - Degraded performance on both old and new tasks (routing errors or poor adapter initialization)
  - Memory bloat (failure to properly fuse adapters when capacity is reached)
  - Catastrophic forgetting (inadequate replay memory or poor knowledge distillation)
- First 3 experiments:
  1. Test adapter routing accuracy: Feed inputs from different tasks and verify the correct adapter is selected based on reconstruction loss
  2. Validate knowledge distillation: Train a new adapter with fusion, then test performance on both old and new tasks
  3. Measure parameter efficiency: Compare total trainable parameters with and without adapter fusion under capacity constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the hierarchical representation within adapters affect the efficiency of knowledge reuse in longer task sequences?
- Basis in paper: [explicit] The paper mentions that addressing longer task sequences necessitates learning hierarchical representations within adapters to facilitate more flexible and efficient knowledge reuse.
- Why unresolved: The paper does not provide experimental results or theoretical analysis on the impact of hierarchical representations in adapters for longer task sequences.
- What evidence would resolve it: Empirical studies comparing the performance of Remembering Transformer with and without hierarchical representations in adapters on datasets with longer task sequences.

### Open Question 2
- Question: What is the impact of the number of autoencoders on the accuracy of novelty detection in the generative routing mechanism?
- Basis in paper: [inferred] The paper uses autoencoders for novelty detection, but it does not explore the effect of varying the number of autoencoders on the accuracy of detecting novel tasks.
- Why unresolved: The paper does not provide an ablation study on the number of autoencoders used in the generative routing mechanism.
- What evidence would resolve it: Experiments varying the number of autoencoders and measuring the accuracy of novelty detection and overall model performance.

### Open Question 3
- Question: How does the performance of Remembering Transformer compare to other continual learning methods in real-world scenarios with non-stationary data distributions?
- Basis in paper: [explicit] The paper focuses on class-incremental and permutation tasks but does not evaluate performance in scenarios with non-stationary data distributions.
- Why unresolved: The paper does not include experiments or analysis on the performance of Remembering Transformer in real-world scenarios with changing data distributions.
- What evidence would resolve it: Comparative studies of Remembering Transformer against other methods in real-world datasets with non-stationary distributions, such as video streams or sensor data.

## Limitations

- Unknown hyperparameters for grid search (learning rates, batch sizes, weight decay, loss function coefficients) create reproducibility challenges
- Implementation details for novelty detection mechanism and herding method for replay sample selection are not fully specified
- Performance in real-world scenarios with non-stationary data distributions remains unexplored

## Confidence

- High Confidence: Mixture-of-adapters architecture for task-specific adaptation
- Medium Confidence: Generative routing mechanism using reconstruction loss
- Medium Confidence: Adapter fusion approach based on knowledge distillation

## Next Checks

1. Implement a controlled experiment testing adapter routing accuracy with overlapping task characteristics to validate the robustness of the reconstruction loss-based routing mechanism
2. Conduct ablation studies comparing the proposed method against standard LoRA fine-tuning and traditional rehearsal methods to quantify the actual contribution of each component
3. Perform stress testing by training with more tasks than available adapters to verify the effectiveness of the adapter fusion mechanism under capacity constraints