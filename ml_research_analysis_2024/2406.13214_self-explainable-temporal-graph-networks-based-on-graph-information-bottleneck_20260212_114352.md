---
ver: rpa2
title: Self-Explainable Temporal Graph Networks based on Graph Information Bottleneck
arxiv_id: '2406.13214'
source_url: https://arxiv.org/abs/2406.13214
tags:
- graph
- temporal
- explanation
- information
- tgib
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TGIB, a self-explainable temporal graph neural
  network that performs prediction and explanation in an end-to-end manner. TGIB addresses
  the limitations of post-hoc explanation methods for temporal graphs by introducing
  stochasticity in candidate events based on the Graph Information Bottleneck principle,
  enabling it to detect important past events that explain target event occurrences.
---

# Self-Explainable Temporal Graph Networks based on Graph Information Bottleneck

## Quick Facts
- arXiv ID: 2406.13214
- Source URL: https://arxiv.org/abs/2406.13214
- Authors: Sangwoo Seo; Sungwon Kim; Jihyeong Jung; Yoonho Lee; Chanyoung Park
- Reference count: 40
- Primary result: Achieves 99.28% AP on Wikipedia dataset for link prediction while providing self-explanations

## Executive Summary
This paper introduces TGIB, a self-explainable temporal graph neural network that performs prediction and explanation in an end-to-end manner. The model addresses the limitations of post-hoc explanation methods for temporal graphs by introducing stochasticity in candidate events based on the Graph Information Bottleneck principle. TGIB detects important past events that explain target event occurrences while maintaining state-of-the-art predictive performance across six real-world temporal graph datasets.

## Method Summary
TGIB is a temporal graph neural network that integrates the Graph Information Bottleneck principle to provide self-explanations. The model uses time-aware event representations with self-attention and temporal encoding to capture dynamic dependencies. It introduces stochasticity through Bernoulli distributions for candidate events, allowing the model to identify which past events are causally important for explaining target events. The architecture optimizes an objective that balances prediction accuracy with explanation quality by minimizing mutual information between predictions and the full graph while maximizing mutual information between predictions and the explanation graph.

## Key Results
- Achieves state-of-the-art link prediction performance with 99.28% AP on Wikipedia dataset
- Outperforms existing methods in capturing temporal relationships across six real-world datasets
- Successfully performs both prediction and explanation in an end-to-end manner
- Theoretically proves ability to remove spurious correlations while maintaining predictive performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TGIB simultaneously performs prediction and explanation in an end-to-end manner by introducing stochasticity in each temporal event based on the Information Bottleneck (IB) theory.
- Mechanism: The model injects stochasticity into candidate events through a Bernoulli distribution, allowing it to learn which past events are important for explaining target event occurrences. During training, stochasticity for label-relevant components decreases while maintaining stochasticity for label-irrelevant components, creating an intrinsic explanation mechanism.
- Core assumption: Past events that are stochastically selected with higher probability are causally important for explaining target event occurrences rather than being spurious correlations.
- Evidence anchors:
  - [abstract]: "TGIB provides explanations for event occurrences by introducing stochasticity in each temporal event based on the Information Bottleneck theory"
  - [section 4.1]: "TGIB considers the interaction between the target event and candidate events to extract important candidate events, eventually predicting the occurrence of the target event"
  - [corpus]: Weak - neighboring papers mention IB but don't specifically validate the stochastic selection mechanism for explanations
- Break condition: If stochastic selection does not correlate with actual causal importance, the model will produce spurious explanations.

### Mechanism 2
- Claim: Time-aware event representations capture temporal dependencies that static graph explanation methods cannot capture.
- Mechanism: The model uses self-attention with temporal encoding to create event representations that incorporate both structural information and temporal spans between events. The temporal encoding Œ¶ùëëùëá measures time distance rather than absolute time values.
- Core assumption: Recent events have greater influence on target events than distant events, and this temporal relationship can be captured through time-aware representations.
- Evidence anchors:
  - [section 4.2]: "We use the representations of neighbors, attributes of their interactions, and their temporal information as the input to the self-attention layer"
  - [section 4.2]: "We use the time span since time encoding is designed to measure the temporal distance between nodes, which is more important than the absolute value of time"
  - [corpus]: Weak - neighboring papers mention temporal dependencies but don't specifically validate the time-aware representation approach
- Break condition: If temporal encoding fails to properly measure temporal distance or if the relationship between time spans and event importance is not consistent.

### Mechanism 3
- Claim: The mutual information objective removes spurious correlations while maintaining predictive performance.
- Mechanism: The model optimizes an objective that minimizes mutual information between predictions and the full graph while maximizing mutual information between predictions and the explanation graph. This creates a bottleneck that forces the model to focus on relevant information.
- Core assumption: The explanation graph Rùëò contains only the causally relevant information for prediction, while Gùëò may contain spurious correlations.
- Evidence anchors:
  - [section 4.5]: "TGIB eliminates spurious correlations within the input data and ensures interpretability"
  - [section 4.5]: "If there is a correspondence between the explanation of event occurrence Rùëò‚àó and the label Y, we can prove that Rùëò‚àó is the optimal solution for the objective function"
  - [corpus]: Weak - neighboring papers mention spurious correlations but don't specifically validate the mutual information approach for their removal
- Break condition: If the explanation graph Rùëò does not actually contain all relevant information or if spurious correlations are not properly filtered out.

## Foundational Learning

- Concept: Information Bottleneck Principle
  - Why needed here: The IB principle provides the theoretical foundation for extracting minimal sufficient information from temporal graphs while maintaining predictive accuracy.
  - Quick check question: What is the trade-off between minimizing mutual information with the input data versus maximizing mutual information with the label in the IB framework?

- Concept: Temporal Graph Neural Networks
  - Why needed here: Understanding how TGNNs capture both spatial and temporal dependencies is crucial for understanding how TGIB extends this to include explanation capabilities.
  - Quick check question: How do TGNNs differ from static GNNs in handling dynamic interactions over time?

- Concept: Mutual Information and Kullback-Leibler Divergence
  - Why needed here: These information-theoretic concepts are central to the optimization objectives and theoretical guarantees of TGIB.
  - Quick check question: How does minimizing KL divergence between the stochastic selection distribution and the Bernoulli prior enforce sparsity in the explanation graph?

## Architecture Onboarding

- Component map: Time-aware representation layer -> Bernoulli stochastic selection -> Mutual information computation -> Predictor -> Readout function
- Critical path: Event representation ‚Üí Stochastic selection ‚Üí Mutual information computation ‚Üí Prediction ‚Üí Explanation extraction
- Design tradeoffs:
  - Stochastic selection vs deterministic selection: stochastic approach allows gradient flow but introduces variance
  - Mutual information weighting: balancing prediction accuracy vs explanation quality
  - Temporal encoding vs absolute timestamps: measuring distance vs exact timing
- Failure signatures:
  - High mutual information loss indicates poor explanation quality
  - Low prediction accuracy suggests insufficient information retention
  - High variance in stochastic selection suggests unstable explanations
- First 3 experiments:
  1. Verify time-aware representations capture temporal dependencies by comparing with static representations
  2. Test stochastic selection stability across multiple runs
  3. Validate explanation quality by measuring prediction accuracy using only explanation graphs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model perform when the temporal encoding dimension (d_T) is varied significantly from the raw feature dimensions?
- Basis in paper: [inferred] The paper states "We set the dimensions of the node embeddings and time encodings to be identical to the raw features of the events" but does not explore variations.
- Why unresolved: The authors only tested one configuration where temporal encoding dimension matches raw feature dimensions, leaving the impact of different dimensionalities unexplored.
- What evidence would resolve it: Experiments showing link prediction and explanation performance across different temporal encoding dimensions relative to raw feature dimensions.

### Open Question 2
- Question: What is the effect of different sparsity levels in the Bernoulli distribution (r) used for the variational approximation q(R_k) on both prediction accuracy and explanation quality?
- Basis in paper: [explicit] The paper mentions "We sample ùõº ‚Ä≤ùëí ‚àº ùêµùëíùëüùëõ (ùëü ), where ùëü ‚àà [0, 1] is a predefined hyperparameter" but does not report sensitivity analysis for this parameter.
- Why unresolved: The hyperparameter r is critical for the mutual information term but the paper does not explore its sensitivity or optimal range.
- What evidence would resolve it: Sensitivity analysis showing how varying r affects both LMI loss and final model performance metrics across datasets.

### Open Question 3
- Question: How does the model's performance degrade when the L-hop computational graph radius is reduced below 2 or increased above 2?
- Basis in paper: [explicit] The paper states "For the ùêø-hop computational graph, we set ùêø as 2" without justification or exploration of other values.
- Why unresolved: The choice of L=2 appears arbitrary and the paper does not investigate how local vs. more global context affects performance.
- What evidence would resolve it: Comparative results showing link prediction and explanation performance with L=1, L=2, L=3, and L=4 on representative datasets.

## Limitations
- Stochastic selection mechanism correlation with actual causal importance remains empirically unverified
- Computational overhead from mutual information estimation and stochastic sampling may limit scalability
- Generalizability to temporal graph domains beyond the six tested datasets remains untested

## Confidence
- High Confidence: TGIB achieves state-of-the-art link prediction performance on the six tested datasets; the model successfully performs both prediction and explanation in an end-to-end manner; time-aware event representations improve temporal dependency capture compared to static approaches
- Medium Confidence: The stochastic selection mechanism provides meaningful explanations for event occurrences; the mutual information objective effectively removes spurious correlations while maintaining predictive performance; the theoretical guarantees for optimal explanation selection hold under practical conditions
- Low Confidence: TGIB is universally applicable across all temporal graph domains; the computational overhead is negligible compared to existing methods; the explanation quality is always interpretable to human users

## Next Checks
1. **Causal Validation Study**: Design an experiment to test whether events selected with higher stochastic probability actually have causal relationships with target events, rather than merely correlational relationships. This could involve synthetic datasets with known causal structures.

2. **Scalability Analysis**: Evaluate TGIB's performance and computational efficiency on large-scale temporal graphs with millions of events and edges to determine practical limitations and identify optimization opportunities.

3. **Cross-Domain Generalization**: Test TGIB on temporal graphs from domains not represented in the current study (e.g., financial transaction networks, IoT sensor networks) to assess generalizability and identify domain-specific challenges.