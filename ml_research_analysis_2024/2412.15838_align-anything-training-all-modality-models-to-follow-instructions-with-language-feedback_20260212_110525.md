---
ver: rpa2
title: 'Align Anything: Training All-Modality Models to Follow Instructions with Language
  Feedback'
arxiv_id: '2412.15838'
source_url: https://arxiv.org/abs/2412.15838
tags:
- arxiv
- modalities
- language
- feedback
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first large-scale all-modality human
  preference dataset (align-anything-200k) spanning text, image, audio, and video
  modalities. It addresses the challenge of aligning all-modality models with human
  intentions by proposing a unified language feedback approach that significantly
  improves instruction-following across modalities compared to traditional binary
  feedback.
---

# Align Anything: Training All-Modality Models to Follow Instructions with Language Feedback

## Quick Facts
- arXiv ID: 2412.15838
- Source URL: https://arxiv.org/abs/2412.15838
- Reference count: 40
- This paper introduces the first large-scale all-modality human preference dataset (align-anything-200k) and shows that learning from language feedback achieves an average 5.83× improvement over baseline RLHF across 5 modalities, 5 open-source models, and 7 benchmarks.

## Executive Summary
This paper addresses the challenge of aligning all-modality models (models that can handle any combination of text, image, audio, and video inputs/outputs) with human intentions by introducing align-anything-200k, the first large-scale human preference dataset spanning all four modalities. The key innovation is Learning from Language Feedback (LLF), which unifies human preference modeling across modalities by using detailed language feedback rather than binary preferences. This approach captures complex, modality-specific human preferences more effectively and enables efficient preference data synthesis. The authors also introduce eval-anything, a comprehensive evaluation framework that assesses not just instruction-following within each modality but also modality selection and synergy—capabilities unique to all-modality models.

## Method Summary
The method involves a two-stage pipeline: first, training a feedback model using supervised fine-tuning on the align-anything-200k dataset, which contains 200k all-modality human preference data with language feedback (critique and refinement). Second, using this feedback model to synthesize preference pairs by having the initial model generate responses conditioned on feedback. The preference pairs are then used to train the final model using Direct Preference Optimization (DPO). This contrasts with traditional approaches that rely on binary feedback and pairwise comparisons. The approach is evaluated using eval-anything, a framework that assesses all-modality understanding (AMU) and generation (AMG) across eight subtasks, with special attention to modality selection and synergy.

## Key Results
- Learning from language feedback achieves an average 5.83× improvement over baseline RLHF across 5 modalities, 5 open-source models, and 7 benchmarks
- LLF unifies human preference modeling across modalities more effectively than binary feedback by capturing modality-specific details
- The eval-anything framework successfully evaluates unique all-modality capabilities like modality selection and synergy that traditional single-modality evaluation pipelines miss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning from language feedback (LLF) unifies human preference modeling across modalities better than binary feedback.
- Mechanism: Language feedback provides fine-grained critique and refinement for each response, capturing modality-specific details that binary preferences miss. This allows models to learn genuine human preference beyond simple pairwise comparisons.
- Core assumption: Language feedback can effectively capture and convey complex human preferences across different modalities.
- Evidence anchors:
  - [abstract] "we introduce an alignment method that learns from unified language feedback, effectively capturing complex modality-specific human preferences and enhancing the model's instruction-following capabilities."
  - [section 2.2] "language feedback... offers targeted and fine-grained human preference information, higher consistency is achieved across all modalities."
  - [corpus] FMR score of 0.5636 for "Self-play with Execution Feedback" suggests related work on feedback-based instruction following exists.
- Break condition: If language feedback becomes too vague or fails to capture modality-specific nuances, the unification advantage disappears.

### Mechanism 2
- Claim: LLF enables more efficient preference data synthesis than binary feedback.
- Mechanism: The feedback model generates language feedback for initial responses, which the initial model then uses to refine its outputs. This creates more learnable preference pairs than random pairwise comparisons.
- Core assumption: Online sampling with language feedback generates more informative preference pairs than traditional pairwise comparisons.
- Evidence anchors:
  - [section 3.3] "LLF provides richer information and supports efficient preference data synthesis... despite having only a smaller amount of language feedback, DPO+LLF outperforms DPO with binary feedback."
  - [section 3.2] "Based on online sampling, it often enables y* and y to exhibit significant differences in certain aspects... thereby generating more learnable preference pairs."
  - [corpus] FMR score of 0.5428 for "Scaling Instructable Agents" indicates relevance to efficient instruction following methods.
- Break condition: If the feedback model generates poor quality feedback or the refinement step fails to create meaningful differences, synthesis efficiency drops.

### Mechanism 3
- Claim: The eval-anything framework addresses evaluation gaps for all-modality models by assessing modality selection and synergy.
- Mechanism: Eval-anything evaluates models on three dimensions: instruction following per modality, modality selection based on human preferences, and modality synergy through a trained judge model.
- Core assumption: A comprehensive evaluation framework can accurately assess all-modality capabilities beyond simple modality-specific benchmarks.
- Evidence anchors:
  - [abstract] "we construct a challenging all-modality capability evaluation framework – eval-anything... not only meticulously constructs evaluation tasks across various modalities but also emphasizes assessing the unique features of all-modality models, such as modality selection and synergy."
  - [section 4.1.2] "All-modality models uniquely select the appropriate modalities based on user queries, enabling seamless cross-modal synergy, a capability that traditional single-modality evaluation pipelines fail to capture fully."
  - [corpus] FMR score of 0.5561 for "Distilling Instruction-following Abilities" shows related work on instruction following evaluation.
- Break condition: If the judge model cannot accurately assess modality synergy or human preferences for modality selection are too inconsistent, evaluation reliability suffers.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF provides the foundation for aligning models with human preferences, which is extended to all modalities in this work.
  - Quick check question: What are the two main stages of RLHF and how do they differ from direct preference optimization?

- Concept: Multimodal representation learning
  - Why needed here: All-modality models must process and generate multiple modalities, requiring understanding of how different modalities can be represented and combined.
  - Quick check question: How do models typically handle non-text modalities when extending language models to multimodal tasks?

- Concept: Preference modeling and reward learning
  - Why needed here: Both the alignment algorithm and evaluation framework rely on modeling human preferences, whether through binary comparisons or language feedback.
  - Quick check question: What are the key differences between binary preference modeling and language-based preference modeling?

## Architecture Onboarding

- Component map:
  - align-anything-200k dataset (8 subtasks across text, image, audio, video)
  -> Feedback model training (SFT on language feedback)
  -> Preference pair synthesis (feedback-conditioned responses)
  -> DPO training (with synthesized preferences)
  -> eval-anything evaluation (AMU + AMG tasks)

- Critical path:
  1. Collect and annotate preference data with language feedback
  2. Train feedback model on language feedback
  3. Use feedback model to synthesize preference pairs
  4. Apply DPO/PPO with synthesized preferences
  5. Evaluate with eval-anything framework

- Design tradeoffs:
  - Language feedback vs binary preferences: Richer information but more expensive to collect
  - Judge model vs human evaluation: Scalable but may miss nuances
  - Modality-specific vs modality-agnostic dimensions: Balanced evaluation vs complexity

- Failure signatures:
  - Poor feedback quality → ineffective preference synthesis
  - Judge model overfitting → unreliable synergy scores
  - Inconsistent human annotations → unreliable modality selection metrics

- First 3 experiments:
  1. Train feedback model on subset of align-anything-200k and evaluate on holdout set
  2. Compare DPO+LLF vs DPO+binary on single modality task
  3. Test judge model on synthetic data before full evaluation

## Open Questions the Paper Calls Out
The paper identifies several limitations and open questions, including the need for more diverse annotator demographics to ensure the representativeness of human preferences, the challenge of evaluating all-modality models when current models still fall far behind true all-modality capabilities, and the complexity of annotating multimodal responses that requires careful prompt design and quality control.

## Limitations
- The evaluation framework relies heavily on judge models for assessing modality synergy, which may not fully capture human judgment of cross-modal interactions
- Language feedback collection requires significant human annotation effort, and feedback quality directly impacts downstream performance
- Claims about "first" or "novel" aspects are difficult to verify given the rapid pace of all-modality research

## Confidence
- **High confidence**: The 5.83× improvement over baseline RLHF across multiple models and benchmarks (supported by experimental results in Table 1 and Figure 4)
- **Medium confidence**: Claims about LLF unifying human preference modeling across modalities (supported by ablation studies but dependent on feedback quality)
- **Medium confidence**: Evaluation framework comprehensiveness (the judge model approach is reasonable but not independently validated)

## Next Checks
1. **Judge model reliability**: Conduct human evaluation studies to validate the judge model's assessment of modality synergy against human preferences, measuring inter-annotator agreement and model-human correlation.
2. **Feedback quality impact**: Systematically vary the quality of language feedback (using automated metrics and human assessment) to quantify how feedback quality affects downstream preference synthesis and model performance.
3. **Cross-dataset generalization**: Test the trained all-modality models on external datasets not seen during training to evaluate generalization beyond the align-anything-200k distribution.