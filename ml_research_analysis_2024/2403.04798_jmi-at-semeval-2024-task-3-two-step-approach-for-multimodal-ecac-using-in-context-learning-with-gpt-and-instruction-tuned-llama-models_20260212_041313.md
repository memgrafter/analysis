---
ver: rpa2
title: 'JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using in-context
  learning with GPT and instruction-tuned Llama models'
arxiv_id: '2403.04798'
source_url: https://arxiv.org/abs/2403.04798
tags:
- emotion
- cause
- llama
- prompt
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents two distinct approaches for the SemEval-2024
  Task 3 Multimodal Emotion Cause Analysis in Conversations. The first approach involves
  instruction-tuning two separate Llama 2 models for emotion and cause prediction,
  while the second leverages GPT-4V for video captioning and in-context learning with
  GPT-3.5 for emotion and cause prediction.
---

# JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using in-context learning with GPT and instruction-tuned Llama models

## Quick Facts
- arXiv ID: 2403.04798
- Source URL: https://arxiv.org/abs/2403.04798
- Reference count: 24
- Weighted F1 score of 0.2816 on evaluation set, ranking 4th among 25+ teams

## Executive Summary
This paper presents two distinct approaches for the SemEval-2024 Task 3 Multimodal Emotion Cause Analysis in Conversations. The first approach involves instruction-tuning two separate Llama 2 models for emotion and cause prediction, while the second leverages GPT-4V for video captioning and in-context learning with GPT-3.5 for emotion and cause prediction. The proposed two-step framework decomposes the task into emotion prediction followed by cause extraction guided by the predicted emotions. The system achieved a weighted F1 score of 0.2816 on the evaluation set, ranking 4th among 25+ teams. Ablation studies demonstrate the effectiveness of the proposed solutions, with significant performance gains observed when incorporating context, instruction-tuning, and in-context learning.

## Method Summary
The authors propose a two-step framework for emotion-cause pair extraction in multimodal conversations. Approach 1 uses instruction-tuned Llama 2-13b models for emotion and cause prediction, where the models are fine-tuned with task-specific prompts on 25,000 training samples. Approach 2 employs GPT-4V for video captioning and in-context learning with GPT-3.5, where semantically similar conversations from the training set are retrieved and used as demonstration examples. Both approaches incorporate conversational context by including speaker information and previous utterances. The system processes video frames in 3x3 grids, generates captions using GPT-4V, and adds self-causes when emotions can be self-caused based on the dataset statistics.

## Key Results
- Weighted F1 score of 0.2816 on evaluation set, ranking 4th among 25+ teams
- Instruction-tuned Llama outperforms zero-shot Llama (0.364 F1 vs 0.222 F1 with self-causes)
- In-context learning GPT outperforms zero-shot GPT (0.336 F1 vs 0.189 F1 with self-causes)
- Context incorporation improves performance across both approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing emotion-cause pair extraction into two sequential steps improves performance by providing guided context for the cause prediction model
- Mechanism: Emotion prediction generates labels that are incorporated into cause prediction prompts, helping the model focus on relevant utterances that could trigger specific emotions
- Core assumption: Emotion labels provide meaningful guidance that narrows the search space for potential causes
- Evidence anchors:
  - [abstract] "The proposed two-step framework decomposes the task into emotion prediction followed by cause extraction guided by the predicted emotions."
  - [section 3.1] "In the first step, we predict the emotion of each utterance... In the next step, we utilize these emotion labels to guide cause extraction."
  - [corpus] Weak - No direct ablation comparing one-step vs two-step approaches

### Mechanism 2
- Claim: Instruction-tuning LLMs with task-oriented prompts improves domain-specific task performance
- Mechanism: Llama 2 models are fine-tuned on task-specific data using carefully crafted instructions that guide the model to output the correct format
- Core assumption: LLMs can be effectively adapted to specific tasks through instruction-tuning
- Evidence anchors:
  - [abstract] "In Approach 1, we employ instruction-tuning with two separate Llama 2 models for emotion and cause prediction."
  - [section 3.2.1] "Using this prompt as the input and the corresponding true emotion label ye j, we perform supervised fine-tuning of a Llama 2-13b model."
  - [corpus] Moderate - Shows instruction-tuned Llama outperforms zero-shot Llama (0.364 F1 vs 0.222 F1)

### Mechanism 3
- Claim: In-context learning with demonstration examples from semantically similar conversations helps GPT models learn task-specific patterns
- Mechanism: For each test conversation, the system retrieves a semantically similar conversation from the training set with annotated emotions or causes
- Core assumption: Semantic similarity implies similar emotional patterns and causal relationships
- Evidence anchors:
  - [abstract] "In Approach 2, we use GPT-4V for conversation-level video description and employ in-context learning with annotated conversation using GPT 3.5."
  - [section 3.3.2] "To guide and control the process, we leverage in-context learning (ICL) by retrieving a conversation from the training set whose emotions are already annotated."
  - [corpus] Moderate - Shows ICL-GPT outperforms zero-shot GPT (0.336 F1 vs 0.189 F1)

## Foundational Learning

- Concept: Multimodal emotion analysis combining text, audio, and video cues
  - Why needed here: The task requires understanding emotions from multiple modalities simultaneously
  - Quick check question: Can a system relying only on text modality capture the full emotional context of conversations that include audio and video cues?

- Concept: Emotion-cause pair extraction as a structured prediction problem
  - Why needed here: The task requires identifying specific relationships between emotion utterances and their causes
  - Quick check question: How does the system differentiate between self-caused emotions and externally-caused emotions?

- Concept: In-context learning and few-shot prompting techniques
  - Why needed here: The system uses GPT models with in-context learning rather than fine-tuning
  - Quick check question: What happens to GPT's performance when the number of demonstration examples is reduced?

## Architecture Onboarding

- Component map: Video frames → GPT-4V captioning → Context embedding retrieval → Emotion prediction (ICL-GPT or fine-tuned Llama) → Cause prediction (ICL-GPT or fine-tuned Llama) → Post-processing (self-cause addition)
- Critical path: Video frames → captions → emotion prediction → cause prediction → final output
- Design tradeoffs: Fine-tuned Llama offers better performance but requires more resources; ICL-GPT is cheaper but depends on retrieval quality; two-step approach adds complexity but provides guidance
- Failure signatures: Poor emotion predictions leading to incorrect cause identification; retrieval failures resulting in irrelevant demonstration examples; self-cause addition introducing noise
- First 3 experiments:
  1. Compare zero-shot vs instruction-tuned Llama on emotion recognition to validate the fine-tuning approach
  2. Test different context window sizes for cause prediction to find optimal bounds
  3. Evaluate the impact of self-cause addition by comparing results with and without this post-processing step

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed multimodal ECA system vary when using larger Llama 2 models (e.g., 70 billion parameters) instead of the 13 billion parameter model?
- Basis in paper: [explicit] The paper mentions using a 13 billion parameter Llama 2 model due to resource constraints and suggests that larger models could be a follow-up investigation
- Why unresolved: The authors did not have access to larger models during their experimentation due to resource limitations
- What evidence would resolve it: Comparing the performance of the system using different sizes of Llama 2 models (e.g., 13 billion vs. 70 billion parameters) on the same dataset and evaluation metrics

### Open Question 2
- Question: How does the incorporation of video captions impact the performance of the ECA system, and what are the reasons behind the observed decrease in performance when using individual utterance captions?
- Basis in paper: [explicit] The paper states that using video captions generated by GPT-4V for individual utterances led to a decrease in performance due to noisy descriptions and confusion caused by multiple emotions in the captions
- Why unresolved: The authors did not further investigate the reasons behind the performance decrease or explore alternative ways of incorporating video information
- What evidence would resolve it: Conducting experiments to compare the performance of the system with and without video captions, and analyzing the quality and relevance of the generated captions to identify potential issues

### Open Question 3
- Question: How can the proposed ECA system be improved to handle class imbalance in the dataset, particularly for emotions like disgust and fear that have lower support?
- Basis in paper: [explicit] The paper mentions that the dataset is imbalanced, with disgust and fear constituting only 3% and 2.7% of the emotions, respectively, and that the performance on these emotions is low due to this imbalance
- Why unresolved: The authors did not propose any specific techniques to address the class imbalance issue in their experiments
- What evidence would resolve it: Implementing and evaluating techniques such as data augmentation, class weighting, or oversampling for the underrepresented emotions, and comparing the performance of the system before and after applying these techniques

## Limitations
- Performance claims based on test set with unavailable true labels, relying on Kaggle's scoring system
- Instruction-tuned Llama models fine-tuned on only 25,000 samples (50% of training data)
- In-context learning heavily dependent on quality of retrieved semantically similar conversations
- Multimodal approach constrained by GPT-4V cost and rate limits
- Self-cause addition heuristics may not generalize across different conversation domains

## Confidence
- **High Confidence:** Two-step framework decomposition approach is well-supported by ablation studies showing consistent performance improvements
- **Medium Confidence:** In-context learning approach shows promising results but relies heavily on retrieval quality with limited failure analysis
- **Low Confidence:** Overall system performance claims difficult to verify due to lack of accessible test labels and limited failure mode discussion

## Next Checks
1. **Retrieval Quality Analysis:** Conduct ablation study varying the quality and relevance of retrieved demonstration examples for in-context learning, measuring impact on emotion and cause prediction accuracy
2. **Context Window Impact:** Experiment with different context window sizes for cause prediction to determine optimal range and understand how much conversational history is beneficial versus confusing
3. **Self-Cause Heuristic Validation:** Create controlled test set with manually labeled self-caused/externally-caused utterances to evaluate heuristic performance and false positive/negative rates