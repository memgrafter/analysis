---
ver: rpa2
title: 'Beyond KV Caching: Shared Attention for Efficient LLMs'
arxiv_id: '2407.12866'
source_url: https://arxiv.org/abs/2407.12866
tags:
- attention
- layers
- across
- weights
- shared
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of computational and memory
  inefficiency in large language models (LLMs) by introducing a novel Shared Attention
  (SA) mechanism. The core idea is to exploit the isotropic tendencies of attention
  distributions across layers in pretrained LLMs by directly sharing computed attention
  weights between multiple layers, rather than recalculating them.
---

# Beyond KV Caching: Shared Attention for Efficient LLMs

## Quick Facts
- arXiv ID: 2407.12866
- Source URL: https://arxiv.org/abs/2407.12866
- Authors: Bingli Liao; Danilo Vasconcellos Vargas
- Reference count: 4
- One-line primary result: Shared attention mechanism reduces computational FLOPs and KV cache size while maintaining model accuracy on standard benchmarks.

## Executive Summary
This paper introduces Shared Attention (SA), a novel mechanism that exploits isotropic attention patterns across layers in pretrained LLMs to improve inference efficiency. Unlike traditional KV cache optimization methods that share intermediate Key-Value matrices, SA shares computed attention weights directly between layers, eliminating redundant softmax calculations. The approach significantly reduces both computational overhead and memory requirements while maintaining competitive performance on standard benchmarks.

## Method Summary
The method introduces a Shared Attention mechanism that shares computed attention weights across multiple layers in pretrained LLMs. By observing that attention distributions become isotropic (similar) across layers post-pretraining, the approach stores a single softmax result and reuses it for multiple layers, avoiding redundant calculations. The implementation modifies standard attention layers to share weights within specified layer ranges, particularly effective in later layers where attention patterns stabilize.

## Key Results
- SA maintains stable performance on Llama2-7B across GLUE, GSM8k, HellaSwag, and MMLU benchmarks
- Some configurations achieved higher accuracy than baseline on GLUE tasks
- Significant computational savings through reduced FLOPs and smaller KV cache requirements
- Performance degradation only observed in early layers with high attention variance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sharing attention weights across layers reduces computational overhead by avoiding redundant softmax calculations.
- Mechanism: Self-attention computes softmax(QKᵀ/√d)V at each layer. SA stores a single softmax result and reuses it across multiple layers where attention patterns are similar.
- Core assumption: Attention weight distributions across layers are sufficiently similar to allow reuse without significant accuracy loss.
- Evidence anchors: [abstract] isotropic tendencies of attention distributions; [section] cosine similarity reveals high degree of similarity across most layers; [corpus] No direct corpus evidence found.

### Mechanism 2
- Claim: Reducing KV cache size by sharing attention weights decreases memory requirements during inference.
- Mechanism: Instead of caching separate K and V matrices for each layer, SA shares computed attention weights (softmax results) across layers while maintaining separate value matrices.
- Core assumption: Value matrices can remain separate while sharing only attention weights for memory efficiency.
- Evidence anchors: [abstract] reduces KV cache size; [section] fundamentally different paradigm from previous KV sharing methods; [corpus] No direct corpus evidence found.

### Mechanism 3
- Claim: Attention mechanism stabilizes and becomes more uniform across layers as pretraining progresses.
- Mechanism: Analysis of pretraining checkpoints shows attention similarity within Group 3 layers increases from 0.8 to 0.9 as the model processes more tokens.
- Core assumption: Isotropic attention patterns observed in fully pretrained models develop progressively during pretraining.
- Evidence anchors: [section] similarity within Group 3 improves from 0.8 to 0.9; [section] similarity continues to enhance after alignment stage; [corpus] No direct corpus evidence found.

## Foundational Learning

- Concept: Self-attention mechanism in transformers
  - Why needed here: Understanding how attention weights are computed (softmax(QKᵀ/√d)) is essential to grasp why sharing them across layers is beneficial.
  - Quick check question: What is the computational complexity of the softmax operation in self-attention, and why is it expensive?

- Concept: KV cache optimization techniques
  - Why needed here: The paper builds on existing KV cache optimization methods (MQA, GQA, CLA) but proposes a novel approach that shares attention weights instead of KV pairs.
  - Quick check question: How do Multi-Query Attention and Grouped-Query Attention reduce memory requirements compared to standard multi-head attention?

- Concept: Cosine similarity for comparing attention matrices
  - Why needed here: The paper uses cosine similarity to measure similarity of attention weight distributions across layers, which is the empirical basis for the shared attention approach.
  - Quick check question: Why is cosine similarity an appropriate metric for comparing attention weight distributions, and what does a high cosine similarity value indicate?

## Architecture Onboarding

- Component map:
  Input tokens → Query, Key, Value matrices (Q, K, V) → Attention weight computation: softmax(QKᵀ/√d) → Shared attention storage: single attention weight matrix reused across multiple layers → Output computation: attention weights × value matrices

- Critical path:
  1. Compute initial attention weights for first layer in shared segment
  2. Store and reuse these weights for subsequent layers in the segment
  3. Apply shared attention to compute outputs using respective value matrices
  4. Continue processing remaining layers with standard attention

- Design tradeoffs:
  - Memory vs. accuracy: Sharing attention weights reduces memory but may slightly degrade accuracy, especially in early layers with high attention variance
  - Computational efficiency vs. flexibility: Avoids redundant softmax calculations but assumes attention patterns are stable enough to share
  - Layer selection: Best applied to later layers (Group 3) where attention patterns are most isotropic

- Failure signatures:
  - Performance degradation in tasks requiring precise attention (e.g., GSM8K arithmetic reasoning)
  - Perplexity explosion when applied to early layers with high attention variance
  - Inconsistency between layers when attention patterns are not sufficiently similar

- First 3 experiments:
  1. Apply shared attention to layers 27-30 in Llama2-7B and measure accuracy drop on GLUE benchmark compared to baseline
  2. Compare computational FLOPs and memory usage between shared attention and standard attention implementations
  3. Test shared attention on Llama3-8B with layers 23-30 and evaluate performance on MMLU knowledge benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Shared Attention (SA) compare when integrated from the pretraining phase versus applied to pre-trained models?
- Basis in paper: [explicit] The paper mentions that integrating SA from the pretraining phase could allow models to better adapt to the streamlined attention mechanism, potentially improving performance and efficiency.
- Why unresolved: The paper did not have the computational resources to pretrain an LLM from scratch incorporating SA, so direct comparisons are unavailable.
- What evidence would resolve it: Pretraining an LLM from scratch with SA and comparing its performance to a standard pre-trained model with SA applied post-training on various benchmarks.

### Open Question 2
- Question: What are the potential benefits and drawbacks of combining Shared Attention with other attention-sharing strategies like Cross-Layer Attention (CLA)?
- Basis in paper: [explicit] The paper suggests exploring combinations between SA and other attention-sharing strategies like CLA as a promising research direction.
- Why unresolved: The paper did not conduct experiments combining SA with other attention-sharing strategies, leaving their potential benefits and drawbacks unexplored.
- What evidence would resolve it: Experimental results comparing models using SA alone, CLA alone, and a combination of both on various benchmarks and efficiency metrics.

### Open Question 3
- Question: How does the attention variance in early layers impact the effectiveness of Shared Attention, and can this variance be reduced through architectural modifications?
- Basis in paper: [explicit] The paper found that early layers exhibited significantly higher weighted variances compared to latter layers, suggesting a stabilization of attention mechanisms in the latter layers.
- Why unresolved: The paper did not explore methods to reduce attention variance in early layers or investigate how this variance impacts the effectiveness of SA.
- What evidence would resolve it: Experiments comparing the effectiveness of SA in models with and without architectural modifications aimed at reducing attention variance in early layers.

## Limitations

- The approach lacks ablation studies showing how sensitive performance is to the threshold of similarity required for sharing attention weights.
- No analysis is provided showing how attention patterns vary across different tasks or domains, leaving open questions about generalization beyond tested benchmarks.
- The assertion that shared attention outperforms baseline on certain GLUE tasks is not well-explained mechanistically and could be due to random variation.

## Confidence

**High Confidence:** The mechanism of sharing attention weights to reduce computational FLOPs is well-established and empirical results on Llama2-7B and Llama3-8B show consistent performance with modest improvements on some GLUE tasks.

**Medium Confidence:** The claim about progressive stabilization of attention patterns during pretraining is supported by presented similarity metrics but lacks independent validation and doesn't explore whether this phenomenon holds across different model architectures or training regimes.

**Low Confidence:** The assertion that shared attention outperforms the baseline on certain GLUE tasks is not well-explained mechanistically and could be due to random variation or specific dataset characteristics rather than a fundamental advantage of the approach.

## Next Checks

1. **Ablation Study on Similarity Thresholds:** Systematically vary the cosine similarity threshold for sharing attention weights and measure the resulting accuracy-FLOPs tradeoff curve to identify optimal operating points.

2. **Cross-Model Generalization Test:** Apply the shared attention approach to models with different architectures (e.g., GPT-style, Mamba) and training objectives to determine whether the observed isotropic attention patterns are universal or Llama-specific.

3. **Task-Specific Performance Analysis:** Conduct detailed analysis of which GLUE tasks show performance improvements and investigate whether this correlates with specific attention pattern characteristics or dataset properties.