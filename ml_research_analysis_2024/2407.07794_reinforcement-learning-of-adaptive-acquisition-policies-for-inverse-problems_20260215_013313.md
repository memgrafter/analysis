---
ver: rpa2
title: Reinforcement Learning of Adaptive Acquisition Policies for Inverse Problems
arxiv_id: '2407.07794'
source_url: https://arxiv.org/abs/2407.07794
tags:
- adaptive
- acquisition
- measurements
- learning
- sensing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a reinforcement learning framework for adaptive
  acquisition in compressed sensing, aiming to reduce the number of measurements needed
  to reconstruct high-dimensional signals. The method jointly learns a reconstruction
  network and an acquisition policy network, with the policy selecting measurement
  actions based on reconstruction quality.
---

# Reinforcement Learning of Adaptive Acquisition Policies for Inverse Problems

## Quick Facts
- arXiv ID: 2407.07794
- Source URL: https://arxiv.org/abs/2407.07794
- Reference count: 40
- This paper proposes a reinforcement learning framework for adaptive acquisition in compressed sensing, aiming to reduce the number of measurements needed to reconstruct high-dimensional signals.

## Executive Summary
This paper introduces a reinforcement learning framework for adaptive acquisition policies in compressed sensing inverse problems. The approach jointly learns a reconstruction network and an acquisition policy network, where the policy selects measurement actions based on reconstruction quality. Experiments on MNIST and Mayo datasets with Gaussian and Radon measurements show that adaptive acquisition outperforms random acquisition, especially with limited measurement budgets. The probabilistic variational formulation further improves results by learning a belief distribution over the latent space.

## Method Summary
The method frames compressed sensing as a sequential decision problem where at each step, a policy network selects measurement actions based on the current latent representation of the signal. The framework jointly trains a reconstruction network (GRU encoder + convolutional decoder) and an acquisition policy network (CNN or MLP) using reinforcement learning with reward defined as reconstruction quality improvement. A variational formulation is also introduced to learn a belief distribution over the latent space, providing structured uncertainty representation for better policy guidance. The system is trained end-to-end with policy gradients, though gradients from the policy don't backpropagate through the encoder.

## Key Results
- Adaptive acquisition achieves up to 85% SSIM on MNIST and 66% on Mayo datasets for short measurement horizons
- Adaptive strategies significantly outperform random acquisition baselines, especially with limited measurement budgets
- The variational formulation with β = 0.01 consistently improves performance over the deterministic version
- End-to-end training of both reconstruction and acquisition networks outperforms sequential training approaches

## Why This Works (Mechanism)

### Mechanism 1
Adaptive acquisition improves signal reconstruction by selecting measurements that maximize immediate reconstruction improvement. The policy network uses the latent representation from the reconstruction encoder to predict the next action that maximizes the reward, defined as the improvement in reconstruction quality between consecutive steps. The core assumption is that the latent space encodes sufficient information about the current reconstruction quality and uncertainty to guide optimal measurement selection.

### Mechanism 2
The variational formulation provides structured uncertainty representation that improves policy guidance and reconstruction quality. The encoder outputs parameters of a belief distribution over the latent space, and the policy selects actions based on samples from this distribution, while the decoder reconstructs from latent samples. Modeling uncertainty in the latent space provides useful signals for the policy to distinguish between high and low uncertainty regimes.

### Mechanism 3
Joint training of reconstruction and acquisition networks enables better adaptation to the measurement model compared to separate training. Both networks are trained end-to-end with gradients flowing from the reconstruction loss and policy gradient, but gradients from the policy don't backpropagate through the encoder. Simultaneous optimization of both components allows the system to find a better solution space than sequential training.

## Foundational Learning

- **Concept**: Reinforcement Learning with continuous action spaces
  - Why needed here: The acquisition policy must select continuous measurement vectors (Gaussian) or angles (Radon) rather than discrete actions.
  - Quick check question: Can you explain the difference between discrete and continuous action spaces in RL, and why policy gradient methods are typically used for continuous spaces?

- **Concept**: Variational Autoencoders and latent space uncertainty
  - Why needed here: The variational formulation models uncertainty in the latent space to guide adaptive measurement selection.
  - Quick check question: What is the role of the KL divergence term in VAE training, and how does the β parameter control the trade-off between reconstruction quality and latent space disentanglement?

- **Concept**: Compressed sensing theory and measurement bounds
  - Why needed here: Understanding the theoretical limits helps interpret when adaptive strategies should provide benefits versus when random measurements are optimal.
  - Quick check question: What is the relationship between the number of measurements, signal sparsity, and reconstruction error in compressed sensing, and how do adaptive strategies potentially improve this relationship?

## Architecture Onboarding

- **Component map**: Measurement selection → Observation collection → Latent encoding → Reconstruction → Reward computation → Policy update
- **Critical path**: The policy must quickly learn to select informative measurements based on reconstruction feedback
- **Design tradeoffs**: 
  - End-to-end vs. two-stage training: Joint training enables better coordination but requires careful gradient management
  - Deterministic vs. variational: Variational adds uncertainty modeling but increases complexity and training time
  - Reward definition: Per-step vs. final reward affects learning stability and policy behavior
- **Failure signatures**:
  - Policy converging to uniform or random behavior: Indicates poor reward signal or inadequate latent representation
  - Reconstruction quality degrading with more measurements (adaptive): Suggests policy selecting redundant or uninformative measurements
  - Training instability: May require gradient clipping, learning rate adjustment, or modified reward shaping
- **First 3 experiments**:
  1. **Random vs. Adaptive Baseline**: Train AE-R and AE-E2E on MNIST with Gaussian measurements, compare SSIM after 20 measurements to establish baseline performance gap.
  2. **Discount Factor Sensitivity**: Test AE-E2E with γ = 0, 0.9, 0.99, 1.0 on MNIST to identify optimal temporal credit assignment for policy learning.
  3. **Variational vs. Deterministic**: Compare AE-E2E and V AE-E2E with β = 0.01 on MNIST to evaluate whether uncertainty modeling improves adaptive measurement selection.

## Open Questions the Paper Calls Out

### Open Question 1
How can we design more efficient policy architectures to handle high-dimensional action spaces in adaptive acquisition, especially for Gaussian measurements? The paper notes that the dimensionality of the action space scales quadratically with the image dimension for Gaussian measurements, leading to performance degradation in adaptive strategies.

### Open Question 2
What is the optimal trade-off between latent space disentanglement and reconstruction quality when tuning the β parameter in β-VAEs for adaptive acquisition? The paper mentions that a high value of β leads to high disentanglement but can harm reconstruction performance, while a low value can improve reconstruction quality.

### Open Question 3
How do adaptive acquisition strategies compare to random baselines when the acquisition budget is large enough to theoretically achieve optimal recovery? The paper states that adaptive strategies are particularly relevant for limited acquisition budgets and that random measurements are theoretically optimal with sufficient measurements.

## Limitations
- Limited ablation studies on policy network architecture variations and reward shaping strategies
- Unclear sensitivity to hyperparameter choices (learning rates, β values, network depths)
- No theoretical analysis of measurement complexity or convergence guarantees for the adaptive policy
- Results primarily shown for two datasets with relatively simple signal structures

## Confidence
- **High**: Adaptive acquisition outperforms random acquisition in terms of SSIM
- **Medium**: Variational formulation with β = 0.01 provides consistent improvements over deterministic version
- **Low**: The mechanism of why specific measurement selections lead to optimal reconstruction (lack of interpretability analysis)

## Next Checks
1. **Architecture Ablation**: Systematically vary the policy network architecture (CNN vs MLP) and reconstruction encoder depth to identify optimal design choices
2. **Reward Function Sensitivity**: Test alternative reward definitions (per-step vs final, normalized vs raw SSIM) to understand their impact on policy learning and stability
3. **Generalization Testing**: Evaluate the trained policy on out-of-distribution signals (different datasets or noise levels) to assess robustness beyond the training distribution