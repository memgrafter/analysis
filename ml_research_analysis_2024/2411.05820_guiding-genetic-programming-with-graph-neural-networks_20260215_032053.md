---
ver: rpa2
title: Guiding Genetic Programming with Graph Neural Networks
arxiv_id: '2411.05820'
source_url: https://arxiv.org/abs/2411.05820
tags:
- search
- graph
- nudge
- evonudge
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EvoNUDGE, a hybrid method that combines graph
  neural networks (GNNs) with genetic programming (GP) for symbolic regression. The
  approach addresses the limitation of conventional GP by pre-computing a library
  of subprograms using a GNN before an evolutionary run.
---

# Guiding Genetic Programming with Graph Neural Networks

## Quick Facts
- arXiv ID: 2411.05820
- Source URL: https://arxiv.org/abs/2411.05820
- Reference count: 29
- EvoNUDGE significantly outperforms both conventional GP and purely neural approaches in symbolic regression, achieving up to 10 percentage points improvement in success rates.

## Executive Summary
This paper introduces EvoNUDGE, a hybrid method combining graph neural networks (GNNs) with genetic programming (GP) for symbolic regression. The approach pre-computes a library of subprograms using a GNN before an evolutionary run, addressing the limitation of conventional GP in maintaining and updating problem-specific knowledge. The method was evaluated on 510 synthetic symbolic regression problems and 97 problems from the AI Feynman suite, demonstrating significant performance improvements over both pure GP and pure neural approaches.

## Method Summary
EvoNUDGE combines GNN-guided graph construction with library-based GP operators. First, a GNN is trained on SR problems and then used to build a graph of symbolic expressions bottom-up from input variables and operations, guided by attention weights. This graph is converted into a library of subprograms that seeds the initial population and biases search operators in the subsequent GP phase. The hybrid approach aims to leverage the problem-specific guidance of the GNN while maintaining the evolutionary search's ability to explore and recombine solutions.

## Key Results
- EvoNUDGE achieved success rates (MSE < 10^-10) of 30-43% across different library heights, compared to 28-31% for conventional GP
- The hybrid approach demonstrated up to 10 percentage points improvement over GP
- EvoNUDGE showed better out-of-sample performance on the AI Feynman suite
- Computational overhead was modest, with total runtimes being less than double that of GP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EvoNUDGE outperforms both pure GP and pure neural approaches by combining the problem-specific guidance of a GNN with the evolutionary search's ability to explore and recombine solutions.
- Mechanism: The GNN first builds a graph of symbolic expressions bottom-up from input variables and operations, using attention weights to guide which application nodes to expand. This graph is then converted into a library of subprograms that seeds the initial population and biases search operators in the subsequent GP phase.
- Core assumption: The GNN can effectively identify useful subprograms that will improve the evolutionary search, and that these subprograms can be meaningfully recombined by GP operators.
- Evidence anchors:
  - [abstract]: "The network is queried on the problem before an evolutionary run to produce a library of subprograms, which is subsequently used to seed the initial population and bias the actions of search operators."
  - [section]: "By hybridizing NUDGE with tree-based GP, we expect them to complement each other."
- Break condition: If the GNN fails to identify useful subprograms, or if GP cannot effectively recombine them, the hybrid approach would perform no better than either component alone.

### Mechanism 2
- Claim: The graph representation allows NUDGE to maintain and update problem-specific knowledge more effectively than GP's population-based approach.
- Mechanism: NUDGE builds multiple SR expression trees in parallel, representing them together as a graph that forms the working search state. This provides an integrated, coherent view of problem-specific knowledge collected so far, unlike GP where knowledge is distributed across the population.
- Core assumption: Representing solutions as a graph rather than as individual trees allows for more efficient knowledge maintenance and transfer.
- Evidence anchors:
  - [section]: "NUDGE builds therefore multiple SR expression trees in parallel, representing them together as a graph that forms the working search state of the algorithm, alike the population in GP â€“ however in a more compact and integrated manner."
- Break condition: If the graph representation doesn't provide significant advantages over the population-based approach, the benefits of NUDGE would be minimal.

### Mechanism 3
- Claim: The hybrid approach benefits from both the problem-specific bias of the GNN and a method bias from the library construction process.
- Mechanism: The libraries disallow semantic duplicates (expressions with the same semantics are collapsed to the same value node), changing the distribution of subtrees used by search operators in a way that might be favorable for search efficiency.
- Core assumption: The method bias introduced by the library construction process contributes positively to search performance.
- Evidence anchors:
  - [section]: "We hypothesize that the method bias has two main constituents. Firstly, all training and test sets come from the same distribution... Secondly, recall that the libraries disallow semantic duplicates..."
- Break condition: If the method bias doesn't contribute to performance, or if it introduces harmful biases, the hybrid approach would perform no better than pure GP with the same operators.

## Foundational Learning

- Graph Neural Networks
  - Why needed here: GNNs are used to build a graph of symbolic expressions and guide which nodes to expand, requiring understanding of graph representation and message passing.
  - Quick check question: How does a GNN differ from a standard neural network in handling graph-structured data?

- Symbolic Regression
  - Why needed here: The problem domain involves constructing mathematical expressions that map input variables to a dependent variable, requiring understanding of program synthesis and tree-based representations.
  - Quick check question: What are the key differences between symbolic regression and standard regression?

- Genetic Programming
  - Why needed here: The evolutionary search component uses tree-based representations and genetic operators like crossover and mutation.
  - Quick check question: How do genetic operators in GP differ from those in standard genetic algorithms?

## Architecture Onboarding

- Component map:
  GNN component -> Graph construction -> Library generation -> GP component -> Search pipeline

- Critical path:
  1. Query GNN on training data to build expression graph
  2. Generate library of subprograms from final graph
  3. Use library to seed initial population and bias mutation operators
  4. Run GP with these modified operators

- Design tradeoffs:
  - GNN-guided vs random expansion: GNN provides problem-specific guidance but adds computational overhead
  - Library-based vs standard operators: Library-based operators can be more effective but may reduce diversity
  - Graph vs tree representation: Graph allows more compact representation but may be more complex to implement

- Failure signatures:
  - Poor GNN performance: Library contains irrelevant or redundant subprograms
  - GP convergence issues: Population fails to evolve beyond initial library
  - Computational bottlenecks: GNN queries become too expensive as graph grows

- First 3 experiments:
  1. Compare EvoNUDGE with and without GNN guidance on a simple synthetic dataset
  2. Test different library heights (h=1,2,3) to find optimal balance between guidance and diversity
  3. Compare EvoNUDGE performance on out-of-sample problems from AI Feynman suite

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of EvoNUDGE scale with increasing problem complexity, particularly for expressions requiring heights greater than 6?
- Basis in paper: [explicit] The paper notes that NUDGE struggles to scale when consecutive expansions become increasingly costly due to querying the GNN on larger graphs, causing its success rate to stagnate for heights greater than 3.
- Why unresolved: The experiments were limited to expression heights up to 6, and the paper mentions that further graph expansions do not significantly improve NUDGE's performance due to computational overhead.
- What evidence would resolve it: Conducting experiments with expression heights greater than 6 and measuring the success rates and computational overhead of EvoNUDGE would provide evidence on its scalability.

### Open Question 2
- Question: What is the impact of using saliency values in constructing the libraries on the performance of EvoNUDGE?
- Basis in paper: [inferred] The paper suggests that saliency values could convey more nuanced information about the usefulness of particular subexpressions and mentions that this is the subject of ongoing work.
- Why unresolved: The current implementation ignores saliency values when constructing libraries, and the potential benefits of incorporating them have not been explored.
- What evidence would resolve it: Comparing the performance of EvoNUDGE with and without using saliency values in library construction would demonstrate the impact of this feature.

### Open Question 3
- Question: How would training the GNN in combination with GP using reinforcement learning affect the performance of EvoNUDGE?
- Basis in paper: [explicit] The paper proposes treating the GNN's choice of the library as an action and the resulting outcome of the GP run using that library as a reward for that action.
- Why unresolved: This approach has not been implemented or tested, and its potential benefits are speculative.
- What evidence would resolve it: Implementing and evaluating EvoNUDGE with a GNN trained using reinforcement learning would provide empirical evidence on its effectiveness.

## Limitations
- Evaluation is primarily focused on synthetic benchmark problems, with limited real-world validation
- Computational overhead of GNN-guided library generation doubles runtime in some configurations
- Method bias introduced by library construction and impact of different library heights requires further investigation

## Confidence

- High confidence in EvoNUDGE's superiority over pure GP on synthetic benchmarks (success rate improvements of 10 percentage points)
- Medium confidence in the mechanism explanations, as the exact contribution of method bias versus problem-specific bias is not fully isolated
- Medium confidence in the scalability claims, given that NUDGE's graph growth could become problematic for more complex problems

## Next Checks

1. Test EvoNUDGE on larger, more complex real-world datasets to verify scalability beyond the current synthetic and AI Feynman benchmarks
2. Conduct ablation studies to isolate the contribution of the GNN guidance versus the library-based operators by comparing with random library generation
3. Investigate the impact of different library heights (h>3) on performance and computational efficiency to find optimal balance between guidance and diversity