---
ver: rpa2
title: Data value estimation on private gradients
arxiv_id: '2412.17008'
source_url: https://arxiv.org/abs/2412.17008
tags:
- data
- noise
- where
- estimation
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of data valuation in gradient-based
  machine learning under differential privacy (DP). Existing data valuation methods
  fail when DP is enforced via i.i.d.
---

# Data value estimation on private gradients

## Quick Facts
- arXiv ID: 2412.17008
- Source URL: https://arxiv.org/abs/2412.17008
- Reference count: 40
- Key outcome: Novel method uses correlated noise instead of i.i.d. noise to achieve constant estimation uncertainty for data valuation under DP, outperforming baseline on multiple datasets and tasks.

## Executive Summary
This paper addresses the fundamental challenge of data valuation in gradient-based machine learning when differential privacy is enforced. Traditional data valuation methods fail under DP because i.i.d. Gaussian noise perturbations cause estimation uncertainty to scale linearly with the evaluation budget, producing unreliable results. The authors propose a novel approach that injects carefully correlated noise into gradients instead of i.i.d. noise, provably controlling estimation uncertainty to a constant regardless of budget size. Their method successfully enables meaningful data valuation tasks including data selection and noisy label detection while maintaining privacy guarantees.

## Method Summary
The authors develop a data valuation framework that replaces standard i.i.d. Gaussian noise with carefully structured correlated noise when enforcing differential privacy. The key insight is that by controlling the correlation structure of the noise, they can bound the estimation uncertainty of data values to O(1) instead of the Ω(k) scaling seen with i.i.d. noise. The method works by first computing gradients on private data, then applying a noise correlation matrix that ensures the estimation uncertainty remains constant regardless of the evaluation budget k. This allows the Shapley value-based data valuation approach to function effectively even under strong privacy constraints. The framework is general enough to apply to various machine learning tasks including dataset valuation, federated learning, and defending against membership inference attacks.

## Key Results
- Reduces estimation uncertainty from Ω(k) to O(1) compared to i.i.d. noise approach
- Outperforms baseline on data selection tasks across diabetes, Covertype, MNIST, and CIFAR10 datasets
- Successfully detects noisy labels while maintaining DP guarantees
- Defends against membership inference attacks while providing meaningful data value estimates

## Why This Works (Mechanism)
The method works by replacing i.i.d. Gaussian noise with carefully structured correlated noise in the gradient perturbation process. When gradients are perturbed with i.i.d. noise, the variance of the data value estimates scales linearly with the number of samples k, making the estimates essentially random for large k. By introducing correlation between noise terms, the authors can control how uncertainty propagates through the valuation calculation. The correlated noise structure ensures that the estimation variance remains bounded regardless of k, allowing meaningful data values to be extracted. This correlation is carefully designed to maintain differential privacy guarantees while dramatically improving the signal-to-noise ratio in the data valuation process.

## Foundational Learning
**Differential Privacy (DP)**: A mathematical framework ensuring individual data points cannot be inferred from model outputs. Needed to provide rigorous privacy guarantees when training on sensitive data. Quick check: ε-DP ensures privacy loss is bounded by ε for any individual record.

**Shapley Values**: A game-theoretic approach for fairly distributing value among players (data points) based on their marginal contributions. Needed for principled data valuation that fairly attributes model performance to individual data points. Quick check: Shapley value satisfies efficiency, symmetry, dummy, and additivity properties.

**Gradient Perturbation**: Adding noise to model gradients during training to achieve DP. Needed as the standard mechanism for applying DP to gradient-based learning. Quick check: sensitivity of gradients determines required noise scale for DP.

**Correlation Structure**: The mathematical relationship between different noise components. Needed to control how uncertainty propagates through the valuation calculation. Quick check: positive correlation can reduce variance in certain estimation problems.

**Estimation Uncertainty**: The variance or confidence interval of parameter estimates. Needed to quantify reliability of data value estimates. Quick check: estimation uncertainty should decrease with more samples or better noise control.

## Architecture Onboarding
**Component Map**: Data → Gradient Computation → Noise Correlation Matrix → DP Perturbation → Shapley Value Calculation → Data Values

**Critical Path**: The most timing-sensitive components are gradient computation and noise correlation application, as these directly impact both privacy guarantees and valuation accuracy.

**Design Tradeoffs**: The method trades computational complexity (requiring careful noise correlation matrix computation) for improved estimation accuracy. The correlation structure must balance privacy requirements with valuation utility.

**Failure Signatures**: Poor correlation structure leads to estimation uncertainty scaling with k. Incorrect sensitivity estimation breaks DP guarantees. Overly strong correlation can reduce model utility.

**First 3 Experiments**: 1) Verify O(1) vs Ω(k) uncertainty scaling on synthetic data with known ground truth. 2) Test data selection performance on diabetes dataset with varying privacy budgets. 3) Evaluate noisy label detection accuracy on MNIST with different noise correlation structures.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical claims rely on idealized assumptions about gradient smoothness that may not hold for complex deep neural networks
- Constant O(1) uncertainty bound is asymptotic and may not translate to practical improvements for smaller datasets
- Method requires careful tuning of correlation structure parameters without comprehensive exploration of hyperparameter sensitivity
- Privacy evaluation focuses on membership inference defense without testing other attack vectors

## Confidence
High confidence in core theoretical insight that correlated noise reduces estimation uncertainty compared to i.i.d. noise.
Medium confidence in empirical results due to limited ablation studies on noise correlation parameters.
Medium confidence in privacy claims as membership inference is demonstrated but other attacks are not explored.

## Next Checks
1. Conduct ablation studies varying correlation structure parameters to identify optimal configurations and robustness to hyperparameter choices.
2. Evaluate method against additional privacy attacks beyond membership inference, including attribute inference and reconstruction attacks.
3. Test approach on larger-scale datasets and more diverse model architectures to verify scalability and generalizability of results.