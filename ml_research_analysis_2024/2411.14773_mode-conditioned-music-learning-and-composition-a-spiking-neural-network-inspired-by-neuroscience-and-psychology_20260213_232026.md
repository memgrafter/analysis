---
ver: rpa2
title: 'Mode-conditioned music learning and composition: a spiking neural network
  inspired by neuroscience and psychology'
arxiv_id: '2411.14773'
source_url: https://arxiv.org/abs/2411.14773
tags:
- music
- pitch
- learning
- bach
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a spiking neural network model inspired by
  brain mechanisms to represent and generate music with tonality features. The model
  consists of a musical theory subsystem that encodes Western modes and keys hierarchically,
  and a sequential memory subsystem that learns and stores ordered musical notes.
---

# Mode-conditioned music learning and composition: a spiking neural network inspired by neuroscience and psychology

## Quick Facts
- arXiv ID: 2411.14773
- Source URL: https://arxiv.org/abs/2411.14773
- Authors: Qian Liang; Yi Zeng; Menghaoran Tang
- Reference count: 10
- This paper introduces a spiking neural network model inspired by brain mechanisms to represent and generate music with tonality features.

## Executive Summary
This paper presents a spiking neural network (SNN) model for mode-conditioned music composition that draws inspiration from neuroscience and psychology. The model features a hierarchical architecture with a Musical Theory Subsystem that encodes Western modes and keys, and a Sequential Memory Subsystem that learns ordered musical sequences. The system employs Spike-Timing-Dependent Plasticity (STDP) learning rules and synaptic creation mechanisms to enable dynamic neural circuit evolution. Experiments demonstrate that the model's architecture closely resembles the Krumhansl-Schmuckler key perception model and can generate music pieces exhibiting specified mode and key characteristics.

## Method Summary
The method employs a spiking neural network with two main subsystems: the Musical Theory Subsystem (MTS) for encoding musical knowledge and the Sequential Memory Subsystem (SMS) for learning musical sequences. The MTS uses a hierarchical structure with mode and key clusters to represent musical theory, while the SMS partitions musical information into four segments (one per track) with pitch and duration subnetworks. The model uses Izhikevich neurons with STDP learning rules and synaptic creation mechanisms. During generation, the system uses seed notes and winner-takes-all selection to produce mode-conditioned music. The model is trained on Bach chorales and harmony textbook exercises, with evaluations comparing its performance to baseline models across multiple musical features.

## Key Results
- The model's connection architecture closely resembles the Krumhansl-Schmuckler key perception model
- Generated music pieces exhibit characteristics of specified modes and keys
- Quantitative assessments show improved pitch diversity, diatonic consistency, and tonal coherence compared to baseline models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hierarchical structure of the Musical Theory Subsystem (MTS) enables the model to encode Western modes and keys in a cognitively plausible manner, mirroring human key perception processes.
- Mechanism: The MTS consists of two layers: a mode cluster encoding major and minor modes with 12 neurons each, and a key cluster with 24 neural groups (12 major and 12 minor keys). Synaptic connections project from mode cluster neurons to key cluster neurons based on scale degree relationships. This hierarchical architecture mirrors the Krumhansl-Schmuckler (KS) model's key profile structure.
- Core assumption: Hierarchical neural organization can effectively capture the tonal hierarchies and relationships that humans use to perceive musical keys and modes.
- Evidence anchors:
  - [abstract] "The model is designed with multiple collaborated subsystems inspired by the structures and functions of corresponding brain regions"
  - [section] "The music theory subsystem(MTS) is structured hierarchically to encode modes as musical prior knowledge... The second layer called the key cluster is consisted of twenty-four neural groups encoding twelve major keys and twelve minor keys"
- Break condition: If the hierarchical structure fails to produce key profiles similar to the KS model, or if the synaptic connections between mode and key clusters don't effectively capture tonal hierarchies.

### Mechanism 2
- Claim: The sequential memory subsystem with its dual-network configuration (pitch and duration subnetworks) enables the model to learn and store ordered musical notes with their temporal relationships, essential for generating coherent musical pieces.
- Mechanism: The sequential memory subsystem is partitioned into four segments with pitch and duration subnetworks in each. Each subnetwork consists of functional minicolumns representing different pitches or durations. STDP learning rules update synaptic weights based on spike timing, with oscillatory patterns triggering synaptic creation between neurons.
- Core assumption: Spike-based learning rules and minicolumn organization can effectively capture the temporal structure and sequential relationships in music.
- Evidence anchors:
  - [section] "The sequential memory subsystem(SMS) is partitioned into four segments to encode and store the sequential order and temporal relationships of the notes... Each minicolumn comprises numerous neurons, all of which share a common preference for a specific MIDI pitch index"
  - [section] "Synaptic plasticity and transmission delay plays a crucial role in the learning process... When the oscillatory times o ≥ 5 of these neuron i and j, a new synaptic connection is formed"
- Break condition: If the spiking dynamics fail to capture temporal dependencies, or if the STDP learning rule doesn't effectively encode sequential musical patterns.

### Mechanism 3
- Claim: The collaborative learning between the Musical Theory Subsystem and Sequential Memory Subsystem through neural circuit evolution enables the model to generate music that exhibits both tonality characteristics and melodic adaptability.
- Mechanism: During learning, notes activate corresponding neurons in both subsystems, creating new synaptic connections based on oscillatory patterns. During generation, activated key cluster neurons guide the sequential memory subsystem, with connection architecture determining which pitches are more likely to be generated. Winner-Takes-All selection incorporates both tonic-dominant relationships and chromatic possibilities.
- Core assumption: Collaborative learning between knowledge representation and sequential memory subsystems can produce musically coherent and tonally consistent outputs.
- Evidence anchors:
  - [abstract] "We incorporate mechanisms for neural circuit evolutionary learning that enable the network to learn and generate mode-related features in music"
  - [section] "The establishment of novel neural pathways mainly occurs between the musical theory subsystem and the sequential memory system... When no connections link two oscillating neurons, novel connections are established spontaneously"
  - [section] "The model requires not only the mode and key but also a set of seed notes to initiate the creative process... The generated notes are G4, Eb4, C4, C3 in respective part"
- Break condition: If the collaborative learning fails to produce musically coherent outputs, or if the generated music doesn't reflect the specified modes and keys.

## Foundational Learning

- Concept: Spike-Timing-Dependent Plasticity (STDP)
  - Why needed here: STDP is crucial for learning temporal relationships in music, allowing the model to capture the sequential nature of musical notes and their timing dependencies
  - Quick check question: How does STDP modify synaptic weights based on the relative timing of pre- and post-synaptic spikes, and why is this important for learning musical sequences?

- Concept: Hierarchical neural organization and key profiles
  - Why needed here: Understanding how hierarchical structures can represent musical knowledge (like the KS model's key profiles) is essential for grasping how the MTS encodes modes and keys
  - Quick check question: What is the relationship between the hierarchical organization of the MTS and the Krumhansl-Schmuckler model's key profiles, and how does this enable mode-conditioned music generation?

- Concept: Winner-Takes-All (WTA) competition in neural networks
  - Why needed here: WTA is used during music generation to select the most appropriate notes based on neural activation patterns, ensuring coherent musical output
  - Quick check question: How does the Winner-Takes-All mechanism work in the context of this model's music generation process, and what role does it play in maintaining tonal consistency?

## Architecture Onboarding

- Component map: MTS (mode cluster -> key cluster) -> SMS (pitch subnetwork, duration subnetwork) -> STDP learning -> neural circuit evolution -> generation module (WTA + key activation)
- Critical path: Input musical data → MTS encoding (mode/key) → SMS learning (pitch/duration) with STDP → neural circuit evolution → generation (seed notes + key activation) → WTA selection → output music
- Design tradeoffs:
  - Hierarchical vs. flat representation: Hierarchical MTS provides cognitive plausibility but adds complexity
  - Spiking vs. rate-based neurons: Spiking neurons better model temporal dynamics but are computationally more expensive
  - Synaptic creation vs. fixed connections: Dynamic circuit evolution allows adaptive learning but increases model complexity
- Failure signatures:
  - Generated music doesn't match specified modes/keys (MTS learning failure)
  - Poor melodic coherence or temporal structure (SMS learning failure)
  - Model doesn't converge during training (STDP learning issues)
  - Generated music lacks variety or expressiveness (WTA selection too restrictive)
- First 3 experiments:
  1. Test MTS alone: Feed simple melodies in different modes/keys and verify that the key cluster neurons activate appropriately, comparing the learned pitch profiles to KS model expectations
  2. Test SMS alone: Use pre-trained MTS connections and test if SMS can learn simple sequential patterns, verifying that STDP effectively captures temporal relationships
  3. End-to-end test: Train on simple four-part chorales and generate music in specified keys, evaluating whether generated pieces show appropriate tonality and mode characteristics using pitch class histograms and diatonic pitch rates

## Open Questions the Paper Calls Out

- How does the model's performance generalize across different musical genres beyond the Western classical tradition?
- What is the impact of varying the number of minicolumns in the pitch and duration subnetworks on the model's learning and generation capabilities?
- How does the model handle polyrhythms or complex rhythmic structures that deviate from the standard note durations it was trained on?
- Can the model be extended to incorporate real-time interactive music generation, allowing for dynamic user input during the composition process?

## Limitations

- The model's generalization to diverse musical styles beyond Bach and SHTE datasets remains untested
- Specific Izhikevich neuron parameters and exact synaptic creation thresholds are not fully specified
- Computational complexity of spiking neuron implementation may limit practical applications

## Confidence

- High confidence for architectural design
- Medium confidence for learning and generation mechanisms
- Low confidence in computational efficiency for real-world deployment

## Next Checks

1. Parameter Sensitivity Analysis: Systematically vary the Izhikevich neuron parameters (a, b, c, d, Vth) and synaptic creation thresholds to assess their impact on model performance and stability.

2. Cross-dataset Evaluation: Test the model on diverse musical datasets (e.g., jazz, pop, contemporary classical) to evaluate its ability to generalize across different musical styles and tonalities.

3. Human Evaluation Study: Conduct a listening test with trained musicians to assess the perceptual quality, musicality, and stylistic appropriateness of the generated pieces compared to human-composed music in the same modes and keys.