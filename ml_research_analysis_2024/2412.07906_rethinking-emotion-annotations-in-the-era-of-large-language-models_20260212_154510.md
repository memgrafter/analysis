---
ver: rpa2
title: Rethinking Emotion Annotations in the Era of Large Language Models
arxiv_id: '2412.07906'
source_url: https://arxiv.org/abs/2412.07906
tags:
- emotion
- human
- gpt-4
- labels
- annotation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of Large Language Models (LLMs),
  specifically GPT-4, for emotion annotation in affective computing. The authors argue
  that human annotations are expensive, subjective, and difficult to quality control,
  while LLMs show promise for automated text annotation.
---

# Rethinking Emotion Annotations in the Era of Large Language Models

## Quick Facts
- arXiv ID: 2412.07906
- Source URL: https://arxiv.org/abs/2412.07906
- Reference count: 40
- GPT-4 shows promise for emotion annotation tasks, outperforming or matching human annotators in preference studies

## Executive Summary
This paper investigates the use of Large Language Models (LLMs), specifically GPT-4, for emotion annotation in affective computing. The authors argue that human annotations are expensive, subjective, and difficult to quality control, while LLMs show promise for automated text annotation. They conduct a human evaluation study comparing GPT-4 and human annotations across three datasets with varying label space complexities, finding that GPT-4 generally aligns with human annotations and is often preferred by evaluators. The paper also explores two methods for integrating GPT-4 into annotation pipelines: pre-filtering labels to reduce cognitive load and post-filtering to flag low-quality samples. Overall, the results highlight the potential of LLMs to improve emotion annotation processes while acknowledging the need for careful consideration of label space design and evaluation metrics.

## Method Summary
The authors conduct a human evaluation study comparing GPT-4 and human annotations across three emotion classification datasets (ISEAR, SemEval, GoEmotions). They use zero-shot emotion classification with GPT-4 API and instruction-driven prompts. Human evaluators rate perceived accuracy and preference between GPT-4 and human annotations on 500 samples from each dataset's test split. The study also implements pre-filtering and post-filtering approaches using GPT-4 to reduce cognitive load and identify low-quality annotations. Logistic regression analysis examines text features affecting GPT-4 performance. Models are trained on filtered and unfiltered annotations to evaluate the impact on emotion recognition performance.

## Key Results
- GPT-4 annotations are often preferred over human annotations in human evaluation studies, particularly in larger label spaces
- Pre-filtering approach reduces options by over 70% while retaining over 90% of human-selected labels
- Post-filtering leads to better model performance with fewer training samples, suggesting GPT-4 can help identify and remove low-quality annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 annotations are often preferred over human annotations in emotion recognition tasks, even when automatic metrics suggest comparable performance.
- Mechanism: LLMs leverage extensive training data to capture nuanced emotional patterns that may be missed by individual human annotators, especially in larger label spaces.
- Core assumption: Human evaluators can detect and prefer more nuanced and contextually appropriate emotion labels generated by LLMs over simpler human annotations.
- Evidence anchors:
  - [abstract] "GPT-4 achieves high ratings in a human evaluation study, painting a more positive picture than previous work, in which human labels served as the only ground truth."
  - [section 4.4.2] "GPT-4 demonstrates stronger performance in identifying emotions deemed totally accurate by evaluators, indicating good comprehension of the complexity of emotion labels and subtlety of emotion expressions."
- Break condition: If the LLM's training data does not adequately represent the diversity of emotional expressions in the target dataset, or if the prompt is not properly tuned to the specific task.

### Mechanism 2
- Claim: GPT-4 can effectively reduce cognitive load for human annotators by pre-filtering emotion labels.
- Mechanism: By dynamically suggesting a smaller set of likely emotion labels for each text sample, GPT-4 reduces the number of options human annotators need to consider, while preserving label diversity.
- Core assumption: Human annotators can more efficiently and accurately select from a smaller, GPT-4 filtered set of emotion labels compared to a larger unfiltered set.
- Evidence anchors:
  - [section 5.1.2] "Annotators spent an average of 17.41 seconds on the Pre-filter set and 18.02 seconds on the Small set, while much longer (25.04 seconds) on the Large set."
  - [section 5.1.2] "The Pre-filtered set can match the advantages of both the Small and the Large sets: it maintains a low cognitive load and shorter time to completion... while allowing more descriptive accuracy and confidence of the annotators."
- Break condition: If GPT-4's pre-filtering is too aggressive and excludes potentially correct emotion labels, leading to a decrease in annotation accuracy.

### Mechanism 3
- Claim: GPT-4 can improve model training efficiency by filtering out low-quality human annotations.
- Mechanism: By comparing GPT-4 and human annotations, samples with significant disagreement can be flagged as potentially low-quality and removed from the training set, leading to better model performance with fewer samples.
- Core assumption: Samples where GPT-4 and human annotations disagree are more likely to contain annotation errors, and removing them improves the overall quality of the training data.
- Evidence anchors:
  - [section 5.2.2] "The filtered set, despite comprising less than 40% of the samples in the full set, consistently led to better model performance across models (both BERT and DistilBERT) and both the full and filtered test sets."
  - [section 5.2] "What's more, on annotated datasets, GPT-4 can act as a post-annotation sample filter to flag potentially low-quality labels. Models trained on the filtered dataset, although much smaller in training data size, achieved better performance than the original full set with human annotations."
- Break condition: If GPT-4 and human annotators have systematic differences in emotion perception that are not due to annotation errors, leading to the removal of valid samples.

## Foundational Learning

- Concept: Emotion recognition and annotation
  - Why needed here: The paper focuses on improving emotion annotation processes using LLMs, so a solid understanding of emotion recognition concepts and challenges is crucial.
  - Quick check question: What are the main challenges in obtaining reliable human emotion annotations, and how do they impact the quality of emotion recognition models?

- Concept: Large Language Models (LLMs) and their capabilities
  - Why needed here: The paper explores the use of GPT-4, a specific LLM, for emotion annotation. Understanding LLM capabilities and limitations is essential for evaluating their potential in this context.
  - Quick check question: How do LLMs like GPT-4 differ from traditional supervised models in terms of their ability to understand and generate text, and what are the implications for emotion annotation?

- Concept: Human evaluation methods and metrics
  - Why needed here: The paper relies on human evaluation studies to assess the performance of GPT-4 in emotion annotation. Understanding how to design and interpret human evaluation studies is crucial for drawing valid conclusions.
  - Quick check question: What are the key considerations when designing a human evaluation study for comparing LLM and human annotations, and how can potential biases be mitigated?

## Architecture Onboarding

- Component map:
  GPT-4 API -> Human evaluation interface -> Pre-filtering module -> Post-filtering module -> Emotion recognition models

- Critical path:
  1. Query GPT-4 for emotion annotations on text samples.
  2. Conduct human evaluation study to compare GPT-4 and human annotations.
  3. Implement pre-filtering and post-filtering modules based on evaluation results.
  4. Train emotion recognition models on filtered or unfiltered annotations.
  5. Evaluate model performance on test sets.

- Design tradeoffs:
  - Accuracy vs. efficiency: Using GPT-4 for pre-filtering reduces cognitive load but may exclude some correct labels. Post-filtering improves model performance but reduces training data size.
  - Human vs. LLM annotations: Human annotations are more reliable but expensive, while LLM annotations are cheaper but may have systematic biases.

- Failure signatures:
  - GPT-4 consistently underperforms human annotations in human evaluations.
  - Pre-filtering leads to a significant decrease in annotation accuracy.
  - Post-filtering results in a large reduction in training data size without a corresponding improvement in model performance.

- First 3 experiments:
  1. Conduct a human evaluation study comparing GPT-4 and human annotations on a small subset of text samples to assess initial performance and identify potential issues.
  2. Implement and test the pre-filtering module on a larger set of text samples to evaluate its impact on cognitive load and annotation accuracy.
  3. Apply the post-filtering module to an existing annotated dataset and train emotion recognition models on the filtered and unfiltered sets to compare performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does GPT-4's emotion perception systematically differ from humans in predictable ways, and if so, can these differences be characterized across datasets, emotion categories, and annotation processes?
- Basis in paper: [explicit] The authors note "differences between human and GPT-4 emotion perception" and observe "asymmetric" confusion matrices, but state these differences "do not generalize across datasets" and suggest "potential perspective differences" that require further investigation.
- Why unresolved: The paper identifies these differences but doesn't explore their underlying causes or develop methods to characterize them systematically.
- What evidence would resolve it: A systematic study examining GPT-4's performance across multiple datasets with controlled variables, analyzing patterns in misclassification and identifying factors (dataset characteristics, emotion categories, annotation contexts) that contribute to consistent differences.

### Open Question 2
- Question: Can LLMs be effectively used for dimensional emotion annotation (valence/activation) rather than just categorical classification, and what evaluation methods would best capture their performance?
- Basis in paper: [inferred] The authors note they limited discussion to classification tasks and that preliminary experiments with dimensional labels "raised more questions for evaluation," indicating this is an unexplored direction.
- Why unresolved: The paper focuses exclusively on categorical emotion classification, leaving the feasibility and best practices for dimensional annotation unexplored.
- What evidence would resolve it: Comparative studies evaluating LLM performance on dimensional annotation tasks using appropriate metrics, potentially developing new evaluation frameworks that better capture the continuous nature of dimensional emotions.

### Open Question 3
- Question: What is the optimal balance between human oversight and LLM automation in emotion annotation pipelines, and how does this balance vary across different annotation contexts and quality requirements?
- Basis in paper: [explicit] The authors propose pre-filtering and post-filtering methods but acknowledge these are "preliminary demonstrations" and suggest "future work could incorporate more refined approaches."
- Why unresolved: While the paper demonstrates the feasibility of LLM-assisted annotation, it doesn't establish guidelines for determining when and how much human oversight is needed in different scenarios.
- What evidence would resolve it: Empirical studies comparing annotation quality, efficiency, and cost across various human-LLM collaboration models, identifying thresholds and best practices for different use cases.

## Limitations
- The evaluation relies on human preference rather than objective accuracy measures, which may introduce subjective bias
- All experiments were conducted using English datasets, limiting generalizability to other languages and cultural contexts
- The study does not account for potential demographic differences between human annotators and LLM training data, which could affect annotation patterns

## Confidence

- High confidence: GPT-4's ability to reduce cognitive load in pre-filtering experiments (supported by quantitative time measurements)
- Medium confidence: GPT-4's superiority in human preference studies (based on human evaluation but with acknowledged subjectivity)
- Low confidence: Generalizability to non-English languages and different cultural contexts (not tested in this study)

## Next Checks

1. **Cross-cultural validation**: Test GPT-4's emotion annotation performance on non-English datasets from diverse cultural contexts to verify if the observed advantages extend beyond Western emotional expression patterns.

2. **Long-term stability analysis**: Evaluate whether GPT-4's emotion annotation quality remains consistent across different versions of the model and over time, as LLMs are known to have version-dependent performance variations.

3. **Bias auditing**: Conduct a systematic analysis of potential demographic biases in GPT-4's emotion annotations by comparing its outputs against annotated datasets with known demographic information about authors and annotators.