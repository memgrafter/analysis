---
ver: rpa2
title: 'Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings'
arxiv_id: '2401.06112'
source_url: https://arxiv.org/abs/2401.06112
tags:
- axis
- tour
- embeddings
- axes
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Axis Tour optimizes the order of axes in ICA-transformed word embeddings
  to improve interpretability. By maximizing semantic continuity between adjacent
  axes using Word Tour, it produces more interpretable low-dimensional projections
  than PCA or ICA.
---

# Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings

## Quick Facts
- **arXiv ID**: 2401.06112
- **Source URL**: https://arxiv.org/abs/2401.06112
- **Reference count**: 40
- **Primary result**: Axis Tour optimizes ICA axis ordering to improve interpretability and achieves better or comparable performance on analogy, word similarity, and categorization tasks compared to PCA and ICA.

## Executive Summary
Axis Tour addresses the interpretability problem in ICA-transformed word embeddings by reordering axes to maximize semantic continuity. Inspired by Word Tour's TSP-based word ordering, it constructs axis embeddings from top-k words on each ICA axis and optimizes their order using TSP. The method then performs dimensionality reduction by merging consecutive axes weighted by skewness. Experiments demonstrate that Axis Tour produces more interpretable low-dimensional embeddings than PCA or ICA while maintaining or improving performance on downstream tasks.

## Method Summary
Axis Tour transforms word embeddings using ICA, then defines axis embeddings as the average of normalized top-k word embeddings for each axis. It applies Word Tour's TSP optimization using cosine similarity as the cost function to reorder axes for maximum semantic continuity. For dimensionality reduction, consecutive axes are merged into intervals weighted by skewness (raised to power α), with projections computed using normalized interval-specific vectors. The method is evaluated on 300-dimensional GloVe embeddings using FastICA with 10,000 iterations and the LKH solver for TSP optimization.

## Key Results
- Axis Tour achieves higher semantic continuity than Skewness Sort, with average cosine similarity between adjacent axes of 0.269 versus 0.185
- On downstream tasks (analogy, word similarity, categorization), Axis Tour yields better or comparable performance to PCA and ICA
- The method successfully orders ICA axes to reveal more interpretable semantic relationships between dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Axis Tour improves interpretability by reordering ICA-transformed axes to maximize semantic continuity.
- Mechanism: Word Tour TSP optimization is applied to axis embeddings (v_ℓ), defined as the average of the top-k word embeddings on each ICA axis. By minimizing the sum of pairwise distances (or maximizing cosine similarity), adjacent axes in the resulting order have similar meanings, making the embedding space more interpretable.
- Core assumption: The top-k words on each ICA axis are representative of the axis's meaning, and axis embeddings defined this way preserve semantic similarity.
- Evidence anchors:
  - [abstract]: "Inspired by Word Tour, a one-dimensional word embedding method, we aim to improve the clarity of the word embedding space by maximizing the semantic continuity of the axes."
  - [section 4.1]: "We then define the ℓ-th axis embedding v_ℓ for S as follows: v_ℓ := 1/k Σ_{i∈Top_k^ℓ} ŝ_i ∈ R^d. As we saw in Fig. 1, since the meaning of an axis can be interpreted from the top words, v_ℓ can be considered the embedding that represents the meaning of the ℓ-th axis of S."
  - [corpus]: Found 25 related papers; average neighbor FMR=0.541, but no direct experimental comparisons to ICA axis ordering.
- Break Condition: If the top-k words are not semantically representative of the axis, or if the TSP optimization fails to converge to a meaningful order.

### Mechanism 2
- Claim: Axis Tour's dimensionality reduction via merging consecutive axes yields better or comparable performance on downstream tasks compared to PCA and ICA.
- Mechanism: After ordering the axes, the embeddings are reduced by projecting onto subspaces defined by merging consecutive axes, weighted by skewness. This preserves semantic continuity and improves downstream task performance.
- Core assumption: Consecutive axes with similar meanings can be merged without significant loss of information, and the weighting by skewness appropriately reflects axis importance.
- Evidence anchors:
  - [abstract]: "Furthermore, we show through experiments on downstream tasks that Axis Tour yields better or comparable low-dimensional embeddings compared to both PCA and ICA."
  - [section 4.3]: "First, we consider reducing the dimensionality of T along I_r, r = 1, ..., p. To do this, we define a unit vector f_r := (f_r^(ℓ))_{ℓ=1}^d ∈ R^{d≥0} for each I_r as follows: f_r^(ℓ) = (γ_ℓ^α / sqrt(Σ_{m∈I_r} γ_m^(2α))) for ℓ ∈ I_r, 0 otherwise, where α ∈ R≥0."
  - [corpus]: No direct experimental evidence for merging consecutive axes based on semantic similarity; assumed from general dimensionality reduction literature.
- Break Condition: If merging axes leads to significant loss of discriminative information, or if the skewness weighting is not optimal.

### Mechanism 3
- Claim: The semantic continuity of axes in Axis Tour embeddings is higher than in Skewness Sort embeddings.
- Mechanism: Axis Tour optimizes the order of axes to maximize the sum of cosine similarities between adjacent axis embeddings, resulting in higher semantic continuity.
- Core assumption: Cosine similarity between axis embeddings is a valid measure of semantic similarity between axes.
- Evidence anchors:
  - [section 5.2.1]: "The semantic continuity of these axes is assessed by calculating the average cosine similarity between adjacent axis embeddings. For Axis Tour, the average cosine similarity is 0.269, but it decreases to 0.185 when these axes are rearranged by skewness, confirming the higher semantic continuity of the axes in Axis Tour."
  - [section 5.2.2]: "As shown in Fig. 3, Axis Tour has a greater number of related axes compared to Skewness Sort for each model, implying more continuous changes in axis meanings."
  - [corpus]: No direct experimental evidence for the validity of cosine similarity as a measure of semantic similarity between axes; assumed from general embedding literature.
- Break Condition: If cosine similarity does not accurately reflect semantic similarity between axes, or if the GPT models are not reliable judges of semantic relatedness.

## Foundational Learning

- Concept: Independent Component Analysis (ICA)
  - Why needed here: ICA is the core transformation used to reveal interpretable semantic axes in word embeddings. Understanding ICA is crucial for understanding the problem Axis Tour addresses (arbitrary axis ordering in ICA-transformed embeddings).
  - Quick check question: What is the main goal of ICA in the context of word embeddings, and how does it differ from PCA?

- Concept: Word Tour
  - Why needed here: Axis Tour is inspired by Word Tour, a one-dimensional word embedding method that uses TSP to order words based on semantic similarity. Understanding Word Tour is essential for understanding the optimization problem Axis Tour solves.
  - Quick check question: How does Word Tour use the TSP to order words, and what is the resulting property of the word order?

- Concept: Dimensionality Reduction
  - Why needed here: Axis Tour includes a dimensionality reduction method that merges consecutive axes. Understanding dimensionality reduction techniques is crucial for evaluating the effectiveness of Axis Tour's reduction method.
  - Quick check question: What are the common goals of dimensionality reduction in the context of word embeddings, and how do PCA and ICA differ in their approach?

## Architecture Onboarding

- Component map: ICA Transformation -> Axis Embedding -> Word Tour Optimization -> Dimensionality Reduction -> Evaluation

- Critical path:
  1. Apply ICA to word embeddings.
  2. Define axis embeddings as the average of top-k word embeddings on each ICA axis.
  3. Apply Word Tour TSP optimization to axis embeddings to reorder axes.
  4. Reduce dimensionality by merging consecutive axes based on semantic similarity and skewness weighting.
  5. Evaluate semantic continuity and downstream task performance.

- Design tradeoffs:
  - Axis Embedding: Choosing k (number of top words) affects the representativeness of the axis embedding.
  - Dimensionality Reduction: Choosing α (skewness weighting) and the method of merging axes affects the quality of the reduced embeddings.
  - Evaluation: Choosing appropriate metrics and tasks to assess semantic continuity and downstream performance.

- Failure signatures:
  - Low semantic continuity: Axis Tour fails to reorder axes to maximize semantic similarity.
  - Poor downstream performance: Axis Tour's dimensionality reduction method leads to significant loss of information.
  - High computational cost: Word Tour TSP optimization becomes intractable for high-dimensional embeddings.

- First 3 experiments:
  1. Apply Axis Tour to a small word embedding dataset and qualitatively assess the semantic continuity of the reordered axes.
  2. Compare the performance of Axis Tour's dimensionality reduction method with PCA and ICA on a downstream task (e.g., word similarity).
  3. Vary the hyperparameter k in the axis embedding definition and assess its impact on semantic continuity and downstream performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of the hyperparameter k for axis embedding in Axis Tour, and how does it vary across different datasets and embedding dimensions?
- Basis in paper: [explicit] The paper explores different values of k (1, 10, 100, 1000) and finds that k = 100 works well for their experiments with 300-dimensional GloVe embeddings. However, they acknowledge that the optimal k may depend on the specific dataset and embedding dimension.
- Why unresolved: The paper only experiments with a limited set of k values and one specific dataset (300-dimensional GloVe). The optimal k for other datasets and embedding dimensions remains unknown.
- What evidence would resolve it: Conduct extensive experiments with different values of k across various datasets and embedding dimensions to determine the optimal k for each case. Analyze the relationship between k, dataset characteristics, and embedding dimensions to derive guidelines for choosing k.

### Open Question 2
- Question: How does the performance of Axis Tour compare to other dimensionality reduction methods like t-SNE or UMAP when applied to ICA-transformed embeddings?
- Basis in paper: [inferred] The paper compares Axis Tour to PCA and ICA for dimensionality reduction but does not explore other popular methods like t-SNE or UMAP. Given the focus on interpretability, it would be valuable to see how Axis Tour compares to these non-linear methods.
- Why unresolved: The paper does not include experiments with t-SNE or UMAP, leaving the comparison to these methods open.
- What evidence would resolve it: Perform experiments comparing Axis Tour to t-SNE and UMAP on ICA-transformed embeddings. Evaluate the interpretability and performance of the resulting low-dimensional embeddings on downstream tasks.

### Open Question 3
- Question: Can Axis Tour be extended to dynamic embeddings like BERT, and how does it perform compared to static embeddings like GloVe?
- Basis in paper: [explicit] The paper mentions that BERT embeddings are dynamic and different for identical tokens, making evaluation challenging. They only evaluate Axis Tour on static embeddings like GloVe and word2vec.
- Why unresolved: The paper does not explore the application of Axis Tour to dynamic embeddings like BERT, leaving its effectiveness on such embeddings unknown.
- What evidence would resolve it: Adapt Axis Tour to handle dynamic embeddings and evaluate its performance on BERT embeddings. Compare the interpretability and downstream task performance to static embeddings like GloVe. Analyze the differences in axis ordering and semantic continuity between static and dynamic embeddings.

## Limitations
- Relies heavily on GPT-4's semantic assessment rather than human judgment or established semantic benchmarks
- Assumes axis embeddings derived from top-k words are representative without empirical validation
- Downstream evaluation covers only analogy, word similarity, and categorization, missing other important NLP tasks
- Choice of k=100 and α=0.8 are not justified through sensitivity analysis

## Confidence
- **High confidence**: The core mechanism of using TSP optimization to order ICA axes based on semantic similarity (Mechanism 1) - this is well-defined and technically sound.
- **Medium confidence**: The downstream task performance claims (Mechanism 2) - results show improvement but the evaluation scope is limited and GPT-based semantic assessment introduces uncertainty.
- **Low confidence**: The validity of cosine similarity as a measure of semantic continuity between axes (Mechanism 3) - this assumption is not empirically validated.

## Next Checks
1. **Validate axis embedding representativeness**: Conduct a human evaluation study where annotators judge whether the top-k words truly represent the semantic meaning of each ICA axis, and assess how k affects axis interpretability.

2. **Expand downstream task evaluation**: Test Axis Tour on additional tasks including semantic textual similarity benchmarks, sentence classification, and contextualized embedding scenarios to verify generalization beyond the current evaluation.

3. **Benchmark against alternative ordering methods**: Compare Axis Tour with other axis ordering approaches including variance-based PCA ordering, random ordering, and alternative semantic similarity measures (e.g., supervised classifiers rather than cosine similarity).