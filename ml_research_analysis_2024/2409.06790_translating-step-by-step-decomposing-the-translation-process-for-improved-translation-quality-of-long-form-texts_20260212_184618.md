---
ver: rpa2
title: 'Translating Step-by-Step: Decomposing the Translation Process for Improved
  Translation Quality of Long-Form Texts'
arxiv_id: '2409.06790'
source_url: https://arxiv.org/abs/2409.06790
tags:
- translation
- zero-shot
- step-by-step
- source
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a step-by-step translation approach for
  long-form texts using large language models, inspired by human translation processes.
  The method breaks down translation into four distinct stages: pre-translation research,
  drafting, refinement, and proofreading, implemented as a multi-turn interaction
  with Gemini 1.5 Pro.'
---

# Translating Step-by-Step: Decomposing the Translation Process for Improved Translation Quality of Long-Form Texts

## Quick Facts
- **arXiv ID:** 2409.06790
- **Source URL:** https://arxiv.org/abs/2409.06790
- **Reference count:** 21
- **Primary result:** Multi-stage translation framework achieves state-of-the-art results on WMT 2024, outperforming zero-shot translation across 10 language pairs

## Executive Summary
This paper introduces a step-by-step translation approach for long-form texts using large language models, inspired by human translation processes. The method breaks down translation into four distinct stages: pre-translation research, drafting, refinement, and proofreading, implemented as a multi-turn interaction with Gemini 1.5 Pro. Extensive automatic evaluations on WMT 2023 and 2024 datasets across ten language pairs show that translating step-by-step consistently outperforms zero-shot translation, achieving state-of-the-art results on WMT 2024. The framework demonstrates robust improvements across all domains and languages studied, with each stage contributing to overall translation quality.

## Method Summary
The method implements a multi-turn interaction with Gemini 1.5 Pro, decomposing document-level translation into four sequential stages. First, the pre-translation research stage identifies potential translation challenges using source text and target language knowledge. Second, the drafting stage generates an initial faithful translation. Third, the refinement stage improves fluency through micro-level edits. Finally, the proofreading stage provides a final polish. The approach processes entire documents (up to 250 tokens) rather than segments, and uses only parametric knowledge without external resources. The framework was evaluated on WMT 2023 development and WMT 2024 test datasets across ten language pairs using MetricX-23 metrics with statistical significance testing.

## Key Results
- Translating step-by-step consistently outperforms zero-shot translation across all ten language pairs tested
- The framework achieves state-of-the-art results on WMT 2024, outperforming other systems
- Each stage contributes meaningfully to translation quality, with ablation studies showing performance degradation when stages are removed
- Improvements are robust across all domains and language pairs, with consistent gains in both reference-based and QE-based MetricX-23 variants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Breaking translation into staged interactions allows the model to focus on distinct subtasks sequentially
- Mechanism: The step-by-step framework decomposes translation into pre-translation research, drafting, refinement, and proofreading. Each stage uses a dedicated prompt that guides the LLM's attention to specific aspects (e.g., research identifies challenges, drafting prioritizes faithfulness, refinement focuses on fluency).
- Core assumption: LLMs can maintain coherent context across multi-turn interactions and benefit from task decomposition rather than monolithic generation.
- Evidence anchors:
  - [abstract]: "engages language models in a multi-turn interaction, encompassing pre-translation research, drafting, refining, and proofreading"
  - [section 3]: "a multi-turn interaction with an LLM where each prompt guides the model's next action"
  - [corpus]: Weak - only general mentions of CoT and translation studies, no direct evidence that staged interactions improve translation quality specifically

### Mechanism 2
- Claim: Pre-translation research identifies translation challenges that inform better initial drafts
- Mechanism: The research stage prompts the model to identify phrases that cannot be directly translated word-for-word, using source text and target language knowledge to anticipate difficulties before drafting.
- Core assumption: Identifying translation challenges upfront improves the quality of subsequent translation stages.
- Evidence anchors:
  - [section 3]: "pre-translation research stage... focuses on using the source text to identify potential translation challenges"
  - [section 4]: "Some phrases that appeared idiomatic or difficult at the segment level disappeared, as their translations became clear with context"
  - [corpus]: Weak - mentions of He et al. (2023) and Li et al. (2024) exploring pre-translation research, but no strong evidence that this specifically improves translation quality

### Mechanism 3
- Claim: Refinement and proofreading stages progressively improve fluency and polish without external resources
- Mechanism: The refinement stage improves draft fluency through micro-level edits, while proofreading provides a final polish. Both stages use only the LLM's parametric knowledge rather than external feedback.
- Core assumption: LLMs can effectively self-improve their translations through iterative refinement using only internal knowledge.
- Evidence anchors:
  - [abstract]: "This draft is then revised in subsequent turns, ensuring a polished final translation"
  - [section 5.1]: "the refinement step consistently improves the translation quality... regardless of the initial translation it processes"
  - [corpus]: Weak - mentions of Chen et al. (2023) showing LLMs can refine outputs using parametric knowledge, but no direct evidence for multi-stage refinement specifically

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: The step-by-step approach is essentially applying CoT to translation by breaking it into reasoning steps
  - Quick check question: How does decomposing a complex task into smaller reasoning steps help LLMs generate better outputs?

- Concept: Translation studies methodology
  - Why needed here: The framework draws on established human translation processes to inform LLM prompting strategies
  - Quick check question: What are the key stages in professional human translation workflows that could inform machine translation?

- Concept: Document-level context in translation
  - Why needed here: The approach works with full documents rather than segments to capture broader context
  - Quick check question: Why might translating at the document level produce better results than segment-by-segment translation?

## Architecture Onboarding

- Component map: Input document -> Research stage -> Drafting stage -> Refinement stage -> Proofreading stage -> Final translation
- Critical path: Research → Drafting → Refinement → Proofreading
- Design tradeoffs:
  - Multiple model calls increase latency but improve quality
  - Using only parametric knowledge simplifies implementation but may miss external knowledge benefits
  - Document-level processing requires more context window but captures better meaning
- Failure signatures:
  - Degradation in quality when stages are ablated (Table 3)
  - Hallucinations introduced during refinement stage
  - Context loss between multi-turn interactions
- First 3 experiments:
  1. A/B test single zero-shot translation vs. research+draft stages on WMT 2023
  2. Measure impact of refinement stage alone on draft quality
  3. Test document-level vs. segment-level processing with same prompt structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the step-by-step translation approach vary across different language families (e.g., Indo-European vs. Sino-Tibetan)?
- Basis in paper: [inferred] The paper tests the approach on ten language pairs but does not analyze performance differences across language families.
- Why unresolved: The paper does not provide a detailed breakdown of results by language family, making it difficult to assess whether the approach's effectiveness is consistent across different linguistic structures.
- What evidence would resolve it: A comparative analysis of translation quality improvements for languages from different families, using both reference-based and QE-based metrics.

### Open Question 2
- Question: What is the impact of document length on the effectiveness of the step-by-step translation approach?
- Basis in paper: [explicit] The paper mentions that the approach was tested on documents with an average length of 178 tokens for WMT 2023 and 130 tokens for WMT 2024, but does not explore how performance scales with document length.
- Why unresolved: The paper does not provide results for documents of varying lengths, leaving uncertainty about the approach's scalability and effectiveness for longer texts.
- What evidence would resolve it: An analysis of translation quality improvements across a range of document lengths, using both reference-based and QE-based metrics.

### Open Question 3
- Question: How does the step-by-step approach compare to other LLM-based translation methods that use external resources (e.g., dictionaries, knowledge bases)?
- Basis in paper: [explicit] The paper compares its approach to a baseline that uses an LLM to analyze the source text and select the best translation from candidates generated with different knowledge types, but does not explore other methods that rely on external resources.
- Why unresolved: The paper does not provide a comprehensive comparison with all LLM-based translation methods that use external resources, making it difficult to assess the relative advantages and disadvantages of the step-by-step approach.
- What evidence would resolve it: A detailed comparison of translation quality improvements between the step-by-step approach and other LLM-based methods that use external resources, using both reference-based and QE-based metrics.

## Limitations
- The framework does not compare against other strong multi-stage translation approaches like iterative decoding, back-translation, or model ensembles
- Reliance on automatic metrics without human evaluation, which is critical for assessing long-form translation quality where fluency and cultural appropriateness matter
- The approach uses only parametric knowledge without external resources that professional translators commonly use (dictionaries, translation memories, web search)

## Confidence

**High Confidence:** The core finding that multi-stage translation (research + draft + refinement + proofreading) consistently outperforms single-stage zero-shot translation across all tested language pairs and domains. The ablation study results (Table 3) provide strong evidence that each stage contributes meaningfully to the final translation quality.

**Medium Confidence:** The claim of achieving state-of-the-art results on WMT 2024, as this depends heavily on the specific evaluation setup and the choice of automatic metrics. Without human evaluation and comparison to other SOTA systems using the same metrics, these claims remain provisional.

**Low Confidence:** The assertion that the framework's improvements are solely due to task decomposition and human-inspired reasoning. The paper does not adequately control for the effects of additional model inference steps, temperature settings, or the specific prompt engineering used in each stage.

## Next Checks

1. Conduct human evaluation studies comparing step-by-step translations with zero-shot translations for fluency, adequacy, and overall quality on the same document sets, using professional translators or bilingual speakers.

2. Implement an ablation study comparing the step-by-step framework against alternative multi-stage approaches such as iterative decoding, back-translation, or model ensemble methods using identical evaluation metrics and datasets.

3. Test the framework's performance when incorporating external knowledge sources (translation memories, dictionaries, or web search) during the research and refinement stages to determine whether parametric knowledge alone is optimal for long-form translation.