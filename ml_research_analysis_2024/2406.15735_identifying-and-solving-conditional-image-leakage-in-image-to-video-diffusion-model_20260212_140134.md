---
ver: rpa2
title: Identifying and Solving Conditional Image Leakage in Image-to-Video Diffusion
  Model
arxiv_id: '2406.15735'
source_url: https://arxiv.org/abs/2406.15735
tags:
- image
- motion
- arxiv
- diffusion
- conditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies conditional image leakage in image-to-video
  diffusion models, where models over-rely on the static conditional image at large
  time steps, resulting in videos with less motion than expected. The authors propose
  two strategies to address this issue: an inference strategy that starts generation
  from an earlier time step with an analytically derived optimal initial noise distribution
  (Analytic-Init), and a training strategy that uses a time-dependent noise distribution
  (TimeNoise) for the conditional image to reduce model dependency on it.'
---

# Identifying and Solving Conditional Image Leakage in Image-to-Video Diffusion Model

## Quick Facts
- arXiv ID: 2406.15735
- Source URL: https://arxiv.org/abs/2406.15735
- Reference count: 40
- Authors: Min Zhao; Hongzhou Zhu; Chendong Xiang; Kaiwen Zheng; Chongxuan Li; Jun Zhu
- Primary result: The paper identifies conditional image leakage in I2V-DMs and proposes two strategies that achieve state-of-the-art performance on UCF101 with FVD of 272.4 and IS of 25.18.

## Executive Summary
This paper identifies a critical issue in image-to-video diffusion models called "conditional image leakage," where models over-rely on the static conditional image at large time steps during generation, resulting in videos with insufficient motion. The authors propose two complementary strategies to address this: an inference strategy (Analytic-Init) that starts generation from an earlier time step with an optimal initial noise distribution, and a training strategy (TimeNoise) that uses time-dependent noise for the conditional image to reduce model dependency on it. Experimental results on multiple I2V-DMs demonstrate significant improvements in motion scores while maintaining image alignment and temporal consistency, achieving state-of-the-art performance on UCF101.

## Method Summary
The authors propose two strategies to mitigate conditional image leakage in image-to-video diffusion models. The inference strategy (Analytic-Init) starts generation from an earlier time step M < T, avoiding the unreliable large-time-step regime where conditional image leakage is strongest. This requires analytically deriving the optimal initial noise distribution through KL divergence minimization. The training strategy (TimeNoise) introduces a time-dependent noise distribution for the conditional image, with higher noise levels at larger time steps to disrupt the conditional image's information and force the model to rely more on the noisy input for motion. The authors validate these strategies on three I2V-DMs (VideoCrafter1, DynamiCrafter, and SVD) using the WebVid-2M dataset and evaluate on UCF101 with metrics including FVD, IS, motion scores, and user studies.

## Key Results
- Achieved state-of-the-art performance on UCF101 with FVD of 272.4 and IS of 25.18 for SVD
- Produced higher motion scores with lower motion score errors while maintaining image alignment
- Validated effectiveness across multiple I2V-DMs including VideoCrafter1, DynamiCrafter, and SVD
- Demonstrated that both inference and training strategies effectively mitigate conditional image leakage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Over-reliance on conditional image at large time steps causes motion degradation.
- Mechanism: As the diffusion process progresses, the noisy input becomes heavily corrupted while the conditional image retains fine detail, biasing the model to depend on the static conditional image rather than the motion signal in the noisy input.
- Core assumption: The conditional image contains more high-frequency detail than the noisy input at large time steps.
- Evidence anchors:
  - [abstract] "as the diffusion process progresses—especially at large time steps, the noisy input becomes heavily corrupted, while the conditional image preserves extensive detail of the target video."
  - [section] "At large time steps, the noisy input Xt becomes increasingly corrupted, while the conditional image y0 retains significant information of the target video."

### Mechanism 2
- Claim: Starting generation from an earlier time step avoids the unreliable large-time-step regime.
- Mechanism: By initializing generation at time M < T, the model avoids the point where conditional image leakage is strongest, improving motion without sacrificing quality when paired with proper initial noise.
- Core assumption: The training-inference gap at early steps is smaller than at late steps.
- Evidence anchors:
  - [section] "a straightforward solution is to start the generation process from an earlier time step M ∈ (0, T), thus avoiding the unreliable later stages of I2V-DMs."
  - [section] "However, a smaller M value (e.g., M = 0.8T) results in poor visual quality due to the training-inference discrepancy."

### Mechanism 3
- Claim: Time-dependent noise on the conditional image reduces model dependency on it.
- Mechanism: Higher noise levels at larger time steps disrupt the conditional image's information, forcing the model to rely more on the noisy input for motion, while lower noise at earlier steps preserves image alignment.
- Core assumption: A logit-normal distribution can flexibly schedule noise over time.
- Evidence anchors:
  - [section] "we propose a time-dependent noise distribution (TimeNoise) that increases noise levels at larger time steps, effectively disrupting the conditional image and reducing model dependency on it."
  - [section] "the key principle is to favor high noise levels at large time steps to sufficiently disrupt y0, shifting towards lower noise levels as the time step decreases."

## Foundational Learning

- Concept: KL divergence and optimal normal distribution matching.
  - Why needed here: Used to derive Analytic-Init by minimizing KL between initial noise and true marginal.
  - Quick check question: If qM(XM) is normal with mean µq and variance Σq, what is the optimal pM(XM) in terms of µp and σ2p?

- Concept: Diffusion process forward and reverse transitions.
  - Why needed here: The forward transition defines how noise is added; the reverse step is what the model learns to invert.
  - Quick check question: Given Xt = αtX0 + σtϵ, what is the distribution of Xt conditioned on X0?

- Concept: Logit-normal distribution and time scheduling.
  - Why needed here: TimeNoise uses a logit-normal to vary noise over time steps.
  - Quick check question: How does the logit transform βs/βm ensure the noise level stays within [0, βm]?

## Architecture Onboarding

- Component map: Conditional image + noisy video → U-Net backbone → output video
- Critical path: Training: (X0, y0) → forward diffusion → TimeNoise(βs) → U-Net → loss. Inference: y0 + XM (Analytic-Init) → reverse diffusion steps → X0
- Design tradeoffs: TimeNoise vs image alignment vs motion; M vs quality vs leakage; noise schedule complexity vs simplicity
- Failure signatures: Motion scores much lower than input motion; poor image alignment with conditional image; excessive noise causing visual artifacts
- First 3 experiments:
  1. Compare motion scores and image alignment for M = 0.96T vs M = 0.8T without Analytic-Init.
  2. Validate TimeNoise with βm = 25 vs βm = 100 on VideoCrafter1 to find sweet spot for alignment vs motion.
  3. Test Analytic-Init vs FrameInit [41] and FreeInit [61] on SVD to measure FVD, IS, and runtime trade-offs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal noise distribution for the conditional image in a scratch-trained image-to-video diffusion model?
- Basis in paper: [inferred] The authors demonstrate the effectiveness of their time-dependent noise distribution (TimeNoise) for mitigating conditional image leakage in existing models, but note that they do not provide a definitive noise distribution choice for a scratch-trained model.
- Why unresolved: The paper focuses on adapting existing image-to-video diffusion models (I2V-DMs) rather than training new models from scratch. The optimal noise distribution may depend on the specific architecture and training data of the model.
- What evidence would resolve it: Training and evaluating various scratch-trained I2V-DMs with different noise distributions for the conditional image, comparing their performance on standard benchmarks like UCF101.

### Open Question 2
- Question: How does the choice of noise schedule (e.g., VP-SDE vs. VE-SDE) affect the severity of conditional image leakage and the effectiveness of mitigation strategies?
- Basis in paper: [explicit] The authors mention that recent techniques adjusting the noise schedule towards higher noise levels may exacerbate conditional image leakage. They validate their strategies on both VP-SDE and VE-SDE frameworks.
- Why unresolved: The paper does not explicitly compare the effects of different noise schedules on conditional image leakage or the performance of their mitigation strategies.
- What evidence would resolve it: Conducting experiments with I2V-DMs using different noise schedules (VP-SDE and VE-SDE) and evaluating the severity of conditional image leakage and the effectiveness of mitigation strategies like TimeNoise and Analytic-Init.

### Open Question 3
- Question: Can the proposed inference and training strategies be extended to other conditional generation tasks beyond image-to-video, such as text-to-image or text-to-video generation?
- Basis in paper: [inferred] The authors identify conditional image leakage as a general issue in diffusion models where the model over-relies on a static conditional input at large time steps. This issue may arise in other conditional generation tasks.
- Why unresolved: The paper focuses specifically on image-to-video generation and does not explore the applicability of their strategies to other conditional generation tasks.
- What evidence would resolve it: Applying the inference strategy (starting from an earlier time step) and training strategy (time-dependent noise distribution for the conditional input) to other conditional generation tasks like text-to-image or text-to-video and evaluating their effectiveness in mitigating similar issues.

## Limitations
- Implementation details for TimeNoise hyperparameters (βm, noise scheduling function) are not fully specified
- Validation is primarily limited to three I2V-DM architectures, which may not generalize to other models
- User study uses 100 images but selection criteria and diversity metrics are not fully specified

## Confidence

**High Confidence**: The existence of conditional image leakage as a phenomenon is well-supported by the theoretical analysis of the diffusion process at large time steps. The mathematical formulation of Analytic-Init via KL divergence minimization is rigorous.

**Medium Confidence**: The TimeNoise mechanism is theoretically sound, but the optimal scheduling of noise levels over time is presented as empirical rather than theoretically derived. The claimed improvements on SVD (FVD 272.4, IS 25.18) are impressive but need independent verification.

**Medium Confidence**: The inference strategy with early start (M = 0.96T) is supported by the analysis, but the sensitivity to different M values and the exact impact on different quality metrics could be more thoroughly explored.

## Next Checks
1. **Cross-Architecture Validation**: Test both proposed strategies (Analytic-Init and TimeNoise) on at least two additional I2V-DM architectures not mentioned in the paper (e.g., LTX-Video, CogVideo) to assess generalizability.

2. **Hyperparameter Sensitivity Analysis**: Systematically vary the TimeNoise parameters (βm, noise scheduling function) across a wider range and measure their impact on FVD, IS, motion scores, and image alignment to identify optimal settings for different model families.

3. **Long Video Generation Test**: Evaluate the methods on video generation tasks with longer sequences (beyond 16 frames) to test whether conditional image leakage becomes more pronounced and whether the proposed solutions scale effectively.