---
ver: rpa2
title: 'Dual Process Learning: Controlling Use of In-Context vs. In-Weights Strategies
  with Weight Forgetting'
arxiv_id: '2406.00053'
source_url: https://arxiv.org/abs/2406.00053
tags:
- structural
- forgetting
- tokens
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how language models shift from flexible in-context
  learning to rigid memorization as training progresses. The authors define "structural
  in-context learning" as the ability to generalize to completely novel tokens based
  on context and structure alone.
---

# Dual Process Learning: Controlling Use of In-Context vs. In-Weights Strategies with Weight Forgetting

## Quick Facts
- arXiv ID: 2406.00053
- Source URL: https://arxiv.org/abs/2406.00053
- Authors: Suraj Anand; Michael A. Lepori; Jack Merullo; Ellie Pavlick
- Reference count: 32
- Primary result: Temporary forgetting maintains structural in-context learning for rare tokens while allowing memorization for frequent ones, creating dual process learning

## Executive Summary
This paper investigates how language models shift from flexible in-context learning to rigid memorization during training, identifying that structural in-context learning (ICL) capability emerges early but disappears as models overfit to their training distribution. The authors define "structural ICL" as the ability to generalize to completely novel tokens based on context and structure alone. Using both large pretrained models (MultiBERTs) and small synthetic masked language models, they find this capability emerges early in training but quickly disappears as models overfit to their training distribution.

To address this limitation, the authors adapt active forgetting techniques to maintain structural ICL while introducing "temporary forgetting" to selectively preserve in-context strategies for rare tokens while allowing memorization for common ones. This creates a dual-process learning system where the model uses contextual generalization for unseen tokens and weight-based memorization for frequent tokens. The approach works across various data distributions and extends to autoregressive transformers, with a proof-of-concept demonstrating inducing structural ICL in pretrained GPT-2 via probabilistic temporary forgetting.

## Method Summary
The authors develop and test active forgetting (reinitializing embeddings every k steps) and temporary forgetting (active forgetting for first N steps, then normal training) techniques to control the balance between in-context learning (ICL) and in-weights learning (IWL). They use synthetic masked language modeling tasks with controlled vocabulary sizes, ambiguity levels, and sampling distributions, along with MultiBERT checkpoints for POS probing evaluation. For autoregressive transformers, they modify Chan et al. (2022b) tasks to use learned token embeddings. The temporary forgetting approach enables selective preservation of ICL for rare tokens while maintaining IWL for frequent tokens, creating a dual-process learning system.

## Key Results
- Structural ICL appears early in training but quickly disappears as models overfit to training distribution
- Active forgetting preserves structural ICL by preventing embedding layer memorization through periodic reinitialization
- Temporary forgetting enables dual process learning by allowing head tokens to become memorized while tail tokens remain generalized
- Probabilistic temporary forgetting induces structural ICL in pretrained GPT-2, enabling logical reasoning on unseen tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structural ICL disappears because models shift from using contextual information to relying on memorized token embeddings in the weights.
- Mechanism: Early in training, models use contextualization layers to infer properties like part-of-speech from surrounding words. As training progresses, the embedding layer becomes enriched with information, allowing the model to bypass contextualization and directly read properties from the embeddings. This transition happens faster for common tokens (head) than rare tokens (tail).
- Core assumption: The embedding layer can encode sufficient information to replace contextualization for determining token properties.
- Evidence anchors:
  - [abstract] "we find that structural ICL appears before quickly disappearing early in LM pretraining"
  - [section 3.2] "We find that the benefit of contextualization fades for both Head and Tail datasets, but dissipates more quickly for the head of the distribution than the tail"
  - [corpus] Weak - related work discusses ICL transience but not specifically the contextual-to-embedding shift mechanism
- Break condition: If embeddings cannot encode sufficient information to replace contextualization, or if the model architecture prevents embedding enrichment from being used effectively.

### Mechanism 2
- Claim: Active forgetting maintains structural ICL by preventing embedding layer memorization.
- Mechanism: By reinitializing the embedding matrix every k steps, active forgetting forces the model to rely on contextual information rather than stored token properties. Since embeddings are constantly reset, the model cannot memorize properties in the weights and must use contextual generalization.
- Core assumption: Reinitializing embeddings removes stored information faster than the model can rebuild contextual generalization capabilities.
- Evidence anchors:
  - [abstract] "Building on Chen et al. (2024)'s active forgetting method, we introduce pretraining and finetuning methods that can modulate the preference for structural ICL and IWL"
  - [section 5] "When training a model using active forgetting, we re-initialize the embedding matrix every k steps during training"
  - [corpus] Moderate - Chen et al. (2024) shows active forgetting helps with linguistic plasticity, supporting the general approach
- Break condition: If the model develops alternative memorization strategies that don't rely on embeddings, or if forgetting frequency is too low to prevent memorization.

### Mechanism 3
- Claim: Temporary forgetting enables dual process learning by allowing different strategies for head vs tail tokens.
- Mechanism: By performing active forgetting only for the first N steps, temporary forgetting allows head tokens to become memorized (IWL) while tail tokens remain generalized (ICL). After N steps, forgetting stops and the model retains both strategies. The parameter N controls the balance between memorization and generalization.
- Core assumption: Early forgetting creates a window where tail tokens cannot be memorized, while head tokens accumulate sufficient training after forgetting stops.
- Evidence anchors:
  - [abstract] "temporary forgetting to selectively preserve in-context strategies for rare tokens while allowing memorization for common ones"
  - [section 6] "We modify the paradigm of active forgetting in order to induce a bias for structural in-context strategies for the tail of the distribution while preserving in-weights strategies for frequently-observed tokens"
  - [corpus] Moderate - Dual process learning concepts exist in cognitive science literature, supporting the general framework
- Break condition: If N is too small (insufficient time for head memorization) or too large (tail tokens also become memorized), or if the forgetting schedule doesn't align with token frequency distributions.

## Foundational Learning

- Concept: The distinction between in-context learning (ICL) and in-weights learning (IWL)
  - Why needed here: Understanding this distinction is fundamental to grasping why models lose structural ICL and how the forgetting techniques work
  - Quick check question: Can you explain the difference between a model using contextual information vs. memorized information to solve a task?

- Concept: The concept of token frequency distributions (head vs tail)
  - Why needed here: The dual process learning approach relies on treating frequent and rare tokens differently based on their frequency
  - Quick check question: If a vocabulary has 10,000 tokens and you're using the top/bottom 10%, how many tokens are in the head and tail respectively?

- Concept: How transformer embeddings and contextualization layers interact
  - Why needed here: The mechanisms depend on understanding how information flows from embeddings through attention layers
  - Quick check question: In a standard transformer, what layers have access to contextual information beyond what's stored in token embeddings?

## Architecture Onboarding

- Component map: Embedding matrix -> Attention layers -> Output projection. The embedding matrix stores token representations, attention layers provide contextualization, and the output projection maps representations to predictions. The forgetting techniques primarily target the embedding matrix.

- Critical path: The critical path for implementing temporary forgetting is: 1) Set up training loop with embedding matrix access, 2) Implement forgetting mechanism (reinitialization every k steps for first N steps), 3) Implement evaluation pipeline for both head and tail tokens, 4) Tune k and N hyperparameters based on validation performance.

- Design tradeoffs: Higher k values mean less frequent forgetting but longer training stability; lower N values preserve more memorization but may lose structural ICL for tail tokens. The tradeoff is between maintaining generalization capabilities and efficient memorization for common inputs.

- Failure signatures: If temporary forgetting fails, you'll see either: 1) Poor performance on tail tokens (if forgetting is insufficient), 2) Poor performance on head tokens (if forgetting is too aggressive), or 3) No improvement over vanilla training (if k/N parameters are poorly chosen).

- First 3 experiments:
  1. Implement active forgetting with various k values on a synthetic POS task to verify structural ICL preservation
  2. Implement temporary forgetting with different N values to find the sweet spot for dual process learning
  3. Test probabilistic temporary forgetting on a pretrained model (like GPT-2) to verify downstream applicability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can temporary forgetting be effectively scaled to large language models while maintaining computational efficiency?
- Basis in paper: [explicit] The paper states "A critical next step involves determining whether temporary forgetting can be effectively incorporated into large-scale pretraining curricula, which would establish the method's broader impact" and mentions investigating integration with parameter-efficient fine-tuning methods.
- Why unresolved: The current experiments focus on small toy models and medium-sized models like GPT-2. Scaling to models with billions of parameters while maintaining computational feasibility and effectiveness remains untested.
- What evidence would resolve it: Successful implementation and validation of temporary forgetting on large-scale language models (e.g., GPT-3, LLaMA) demonstrating maintained dual process learning capabilities with reasonable computational overhead.

### Open Question 2
- Question: What is the optimal timing and frequency for temporary forgetting to balance between maintaining structural ICL and efficient memorization?
- Basis in paper: [explicit] The paper mentions that "we can control the preference for IWL versus ICL on observed tokens by modifying N" but notes "we did not exhaustively search over parameters" and "we use k = 1000, N = 8000 because these parameters worked well in initial experiments."
- Why unresolved: The experiments use fixed hyperparameters for