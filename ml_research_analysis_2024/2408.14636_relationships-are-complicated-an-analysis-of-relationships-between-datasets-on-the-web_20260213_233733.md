---
ver: rpa2
title: Relationships are Complicated! An Analysis of Relationships Between Datasets
  on the Web
arxiv_id: '2408.14636'
source_url: https://arxiv.org/abs/2408.14636
tags:
- datasets
- dataset
- relationships
- data
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes dataset relationships from the perspective
  of users who discover, use, and share datasets on the Web. The authors present a
  comprehensive taxonomy of relationships between datasets and map these relationships
  to user tasks performed during dataset discovery.
---

# Relationships are Complicated! An Analysis of Relationships Between Datasets on the Web

## Quick Facts
- arXiv ID: 2408.14636
- Source URL: https://arxiv.org/abs/2408.14636
- Authors: Kate Lin; Tarfah Alrashed; Natasha Noy
- Reference count: 40
- Key outcome: Machine learning methods achieve 90% multi-class classification accuracy for dataset relationship identification

## Executive Summary
This paper presents a comprehensive analysis of dataset relationships from the perspective of users who discover, use, and share datasets on the Web. The authors develop a taxonomy of dataset relationships grounded in user discovery tasks and evaluate multiple methods for identifying these relationships at scale. Their findings reveal that 20% of datasets have at least one relationship with another dataset, with machine learning approaches significantly outperforming schema.org and heuristics-based methods. The study highlights critical gaps in semantic markup for datasets and provides a benchmark for future research in dataset relationship identification.

## Method Summary
The authors collected 2.7 million dataset metadata entries from Web pages with schema.org markup and created a manually annotated ground truth of 2,178 dataset pairs. They implemented and compared four methods: schema.org extraction, heuristics-based rules, gradient boosted decision trees (GBDT), and a fine-tuned T5.1.1 large language model. The evaluation used precision, recall, and F1 scores to assess performance across multiple relationship types including replicas, versions, subsets, derivations, and variants. The machine learning approaches achieved 90% multi-class classification accuracy, outperforming the other methods.

## Key Results
- Machine learning methods achieve 90% multi-class classification accuracy for dataset relationship identification
- 20% of datasets have at least one relationship with another dataset
- Schema.org and heuristics methods show significantly lower precision and recall compared to ML approaches
- Only 2 relationship types (replica, derivation) have schema.org support, despite many more being identified

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machine-learning models using dataset metadata outperform schema.org and heuristics for identifying dataset relationships.
- Mechanism: The ML models (GBDT and T5) learn complex patterns from multiple metadata fields that simple rule-based systems miss, achieving 90% multi-class classification accuracy.
- Core assumption: Metadata contains sufficient signal to distinguish between different relationship types.
- Evidence anchors:
  - [abstract]: "We demonstrate that machine-learning based methods that use dataset metadata achieve multi-class classification accuracy of 90%."
  - [section 5]: Comparison of four methods shows ML approaches outperform others.
  - [corpus]: Ground truth labeled pairs used for training show diverse relationship patterns.
- Break condition: If metadata fields become too sparse or noisy, the signal-to-noise ratio would drop below the threshold needed for accurate classification.

### Mechanism 2
- Claim: Relationships between datasets are fundamentally grounded in user tasks during dataset discovery.
- Mechanism: By mapping dataset relationships to specific user tasks (finding datasets, evaluating trustworthiness, citing, curating), the taxonomy becomes more practical and actionable than abstract relationship definitions.
- Core assumption: Users' actual needs during dataset discovery drive which relationships are most valuable to identify.
- Evidence anchors:
  - [section 3]: "We base our categorization of dataset relationships on the analysis of user tasks in dataset discovery"
  - [abstract]: "We study dataset relationships from the perspective of users who discover, use, and share datasets"
  - [corpus]: Manual labeling process considered user task context when categorizing relationships.
- Break condition: If user tasks evolve significantly or new discovery paradigms emerge (e.g., AI-driven discovery), the current task-to-relationship mapping might become outdated.

### Mechanism 3
- Claim: The 20% of datasets with relationships represents a meaningful but incomplete picture due to markup incompleteness.
- Mechanism: Schema.org markup is extremely incomplete for relationships, so even though 20% have relationships identified through metadata analysis, the actual percentage is likely higher.
- Core assumption: Many relationships exist but are not captured in current semantic markup.
- Evidence anchors:
  - [abstract]: "highlight gaps in available semantic markup for datasets"
  - [section 7]: "our analysis found that over 20% of datasets have at least one relationship with another dataset. These relationships are not captured by schema.org metadata"
  - [corpus]: Only 2 relationships (replica, derivation) have schema.org support, yet many more are identified.
- Break condition: If metadata quality improves dramatically across the web, the gap between identified and actual relationships would shrink, changing the interpretation of the 20% figure.

## Foundational Learning

- Concept: Multi-class classification in ML
  - Why needed here: The paper uses ML to classify dataset pairs into multiple relationship categories simultaneously.
  - Quick check question: How does multi-class classification differ from binary classification, and why is it more appropriate for this problem?

- Concept: Metadata schema and structure
  - Why needed here: Understanding schema.org and other metadata standards is crucial for implementing the methods described.
  - Quick check question: What are the key metadata fields typically available for datasets, and which ones proved most predictive for relationship identification?

- Concept: Embedding spaces for semantic similarity
  - Why needed here: The paper uses NewsEmbed embedding space to find similar datasets before relationship classification.
  - Quick check question: How do embedding spaces capture semantic similarity, and why might NewsEmbed be particularly effective for datasets?

## Architecture Onboarding

- Component map: Data pipeline → Metadata extraction → Embedding generation → Relationship classification → Corpus analysis
- Critical path: Metadata extraction → ML model inference → Relationship output
- Design tradeoffs: Schema.org vs heuristics vs ML approaches (accuracy vs interpretability vs computational cost)
- Failure signatures: Low precision/recall indicates metadata quality issues; skewed class distribution suggests need for sampling adjustments
- First 3 experiments:
  1. Run classification on a small subset of dataset pairs and manually verify accuracy to establish baseline
  2. Test different embedding spaces (NewsEmbed vs others) to measure impact on nearest neighbor quality
  3. Implement a simple web interface to visualize relationship clusters and validate the 20% finding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we accurately identify non-provenance-based relationships (like topical similarity and task similarity) between datasets when they depend heavily on user context?
- Basis in paper: [explicit] The authors note that "Topical and task similarity relationships depend on the user context" and acknowledge this as a limitation.
- Why unresolved: The paper focuses on provenance-based relationships that can be inferred from metadata, but acknowledges that non-provenance relationships require understanding user context, which is challenging to formalize and automate.
- What evidence would resolve it: A method that can reliably identify topical and task similarity relationships between datasets without requiring explicit user input, validated on a diverse set of real-world datasets and user tasks.

### Open Question 2
- Question: What are the specific factors that contribute to the low quality and incompleteness of schema.org metadata for datasets, and how can these be addressed?
- Basis in paper: [explicit] The authors note that "schema.org metadata is not always reliable" and that "the markup for dataset relationships is extremely incomplete," citing previous research on this issue.
- Why unresolved: While the paper identifies the problem, it doesn't deeply investigate the root causes of poor schema.org metadata quality or propose comprehensive solutions to improve it.
- What evidence would resolve it: A systematic analysis of schema.org metadata quality issues across a large corpus of datasets, identifying specific patterns of incompleteness or errors, along with proposed solutions that demonstrate improved metadata quality in practice.

### Open Question 3
- Question: How do dataset relationships differ between general Web datasets and those in specific repositories (like Figshare, Zenodo) or the Linked Open Data cloud?
- Basis in paper: [explicit] The authors note that "these relationships likely paint a different picture for datasets in specific dataset repositories" and "among the datasets that constitute the linked open data (LOD) cloud," suggesting this as future work.
- Why unresolved: The paper's analysis is based on a general Web corpus, and the authors acknowledge that relationships may differ in specialized contexts, but don't investigate this difference.
- What evidence would resolve it: A comparative analysis of dataset relationships in general Web datasets versus those in specific repositories or the LOD cloud, identifying key differences in relationship patterns and their implications for dataset discovery and use.

## Limitations
- The 20% relationship prevalence figure likely represents a lower bound due to incomplete metadata coverage
- The dataset corpus may not fully represent the diversity of datasets available on the Web
- Manual annotation process may contain human bias in relationship classification

## Confidence
- High: Comparative effectiveness of ML methods over schema.org and heuristics approaches (90% accuracy)
- Medium: Generalizability of the taxonomy across different domains
- Low: Completeness of identified relationships given acknowledged gaps in semantic markup

## Next Checks
1. **External Corpus Validation**: Test the ML models on a completely independent dataset corpus from a different domain (e.g., biomedical datasets) to assess cross-domain generalizability of the 90% accuracy claim.

2. **Metadata Completeness Analysis**: Conduct a systematic audit of metadata availability across different dataset providers to quantify the impact of missing metadata fields on relationship detection performance.

3. **Temporal Stability Test**: Re-run relationship identification on the same corpus after a 6-12 month interval to measure stability and identify whether relationships evolve over time, which would affect the practical utility of the taxonomy.