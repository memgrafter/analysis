---
ver: rpa2
title: 'Explainable Predictive Maintenance: A Survey of Current Methods, Challenges
  and Opportunities'
arxiv_id: '2401.07871'
source_url: https://arxiv.org/abs/2401.07871
tags:
- they
- data
- methods
- maintenance
- explainable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys explainable artificial intelligence (XAI) and
  interpretable machine learning (iML) methods as applied to predictive maintenance
  (PdM), following PRISMA 2020 guidelines. The study identified 102 relevant articles,
  categorizing methods into model-agnostic (e.g., SHAP, LIME), model-specific (e.g.,
  CAM, GradCAM, DIFFI), and interpretable architectures.
---

# Explainable Predictive Maintenance: A Survey of Current Methods, Challenges and Opportunities

## Quick Facts
- arXiv ID: 2401.07871
- Source URL: https://arxiv.org/abs/2401.07871
- Authors: Logan Cummins; Alex Sommers; Somayeh Bakhtiari Ramezani; Sudip Mittal; Joseph Jabour; Maria Seale; Shahram Rahimi
- Reference count: 40
- Primary result: Survey of 102 papers on XAI/iML methods for predictive maintenance using PRISMA 2020 methodology

## Executive Summary
This survey systematically reviews explainable artificial intelligence (XAI) and interpretable machine learning (iML) methods applied to predictive maintenance (PdM), following PRISMA 2020 guidelines to identify 102 relevant articles. The study categorizes methods into model-agnostic (e.g., SHAP, LIME), model-specific (e.g., CAM, GradCAM, DIFFI), and interpretable architectures, with SHAP emerging as the most frequently used XAI method. The authors highlight the growing importance of explainability in PdM driven by increasing adoption of complex models and the need for trust in high-stakes applications, while identifying key challenges including stakeholder-specific explanation design and robust evaluation metrics.

## Method Summary
The survey employed PRISMA 2020 systematic review methodology, conducting database searches across IEEE Xplore, ACM Digital Library, ScienceDirect, and Scopus with predefined search terms related to predictive maintenance and explainability. Papers underwent title/abstract screening followed by full-text assessment against inclusion/exclusion criteria. The 102 selected papers were categorized by XAI/iML method type (model-agnostic, model-specific, interpretable architectures) and PdM task (anomaly detection, fault diagnosis, prognosis). The survey also identified challenges in explanation evaluation and proposed incorporating human feedback into evaluation processes as a future direction.

## Key Results
- SHAP was the most frequently applied XAI method across PdM applications
- Methods categorized into model-agnostic (SHAP, LIME), model-specific (CAM, GradCAM, DIFFI), and interpretable architectures
- Three main PdM tasks identified: anomaly detection, fault diagnosis, and prognosis
- Key challenges include stakeholder-specific explanation design and lack of robust evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The survey's systematic approach using PRISMA 2020 guidelines ensures reproducibility and comprehensive coverage of XAI/iML methods applied to PdM.
- Mechanism: By following a predefined search strategy across multiple databases and applying clear inclusion/exclusion criteria, the authors minimize selection bias and maximize relevance of identified papers.
- Core assumption: Database coverage and keyword selection are sufficiently broad to capture the majority of relevant literature in this niche intersection.
- Evidence anchors:
  - [abstract] "following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020 guidelines"
  - [section] "We utilized the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020 statement [31], [32] to layout a systemized methodology of performing a literature review"
  - [corpus] Weak evidence - neighbor papers focus on human-centered XAI design, not systematic review methodology
- Break condition: If key databases are omitted or search terms miss major terminology variations, the coverage becomes incomplete.

### Mechanism 2
- Claim: Categorizing methods into model-agnostic, model-specific, and interpretable architectures provides a clear taxonomy that aligns with existing XAI literature.
- Mechanism: The classification scheme mirrors established XAI frameworks while being tailored to PdM applications, allowing readers to map familiar methods to PdM contexts.
- Core assumption: The existing XAI literature provides sufficiently stable categorization schemes that can be directly applied to PdM without significant modification.
- Evidence anchors:
  - [abstract] "categorizing methods into model-agnostic (e.g., SHAP, LIME), model-specific (e.g., CAM, GradCAM, DIFFI), and interpretable architectures"
  - [section] "These methods are colloquially known as model-agnostic explainable methods [149]. These methods found in this section can be applied to any architecture"
  - [corpus] Weak evidence - neighbor papers focus on human-centered design, not methodological categorization
- Break condition: If new XAI methods emerge that don't fit these categories or if the taxonomy becomes too rigid for emerging approaches.

### Mechanism 3
- Claim: Identifying the growing importance of XAI in PdM addresses the trust and interpretability challenges in high-stakes applications.
- Mechanism: By linking XAI adoption to the increasing use of complex models in PdM and the need for human trust, the survey contextualizes technical methods within practical deployment challenges.
- Core assumption: Trust and interpretability are primary barriers to XAI adoption in PdM, and addressing these through methodological survey will accelerate adoption.
- Evidence anchors:
  - [abstract] "As these methods are adopted for more serious and potentially life-threatening applications, the human operators need trust the predictive system"
  - [section] "Many articles discuss the importance of increasing the trust of the users in the model while decreasing the bias in black-box models [33]–[36]"
  - [corpus] Weak evidence - neighbor papers focus on human-centered design evaluation, not trust barriers in PdM
- Break condition: If trust is not actually a primary barrier or if other factors (like regulatory requirements) are more important.

## Foundational Learning

- Concept: PRISMA systematic review methodology
  - Why needed here: Provides the rigorous framework that ensures the survey's findings are comprehensive and reproducible
  - Quick check question: What are the key steps in PRISMA 2020 that differentiate it from previous versions?

- Concept: XAI method categorization (model-agnostic vs. model-specific)
  - Why needed here: Understanding these categories helps readers place the surveyed methods in the broader XAI landscape
  - Quick check question: What is the fundamental difference between how SHAP and GradCAM generate explanations?

- Concept: PdM task classification (anomaly detection, fault diagnosis, prognosis)
  - Why needed here: These tasks have different XAI requirements and challenges, which the survey addresses separately
- Quick check question: How does the explanation need differ between binary classification (anomaly detection) and regression (prognosis) tasks?

## Architecture Onboarding

- Component map:
  - Database search layer (IEEE Xplore, ACM Digital Library, ScienceDirect, Scopus)
  - Screening pipeline (title/abstract review → full-text assessment → eligibility check)
  - Categorization engine (method taxonomy + PdM task classification)
  - Challenge identification module (explanation evaluation + human involvement)
  - Future directions synthesizer

- Critical path: Search → Screening → Categorization → Challenge Analysis → Future Directions
- Design tradeoffs: Broad keyword search vs. precision (more results but more screening work), multiple databases vs. consistency (different indexing systems)
- Failure signatures: Low precision in initial screening (too many irrelevant papers), high inter-rater disagreement in eligibility assessment, missing key methods due to database limitations
- First 3 experiments:
  1. Test search strategy on a small sample to validate keyword coverage and refine inclusion criteria
  2. Pilot screening process with 20 papers to calibrate inter-rater reliability and adjust exclusion criteria
  3. Categorize first 50 papers to validate taxonomy alignment and identify any classification gaps early

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What evaluation metrics for explainable AI (XAI) methods are most effective for predictive maintenance applications?
- Basis in paper: [explicit] The paper identifies a lack of consensus on how to evaluate explanations in predictive maintenance, despite numerous metrics being proposed in the literature.
- Why unresolved: There is no agreement on which metrics best capture the quality and usefulness of explanations for diverse stakeholders in predictive maintenance.
- What evidence would resolve it: Comparative studies applying multiple evaluation metrics to the same XAI methods in predictive maintenance, showing which metrics best correlate with user trust, understanding, and task performance.

### Open Question 2
- Question: How do different types of explanations (e.g., global vs. local, model-agnostic vs. model-specific) impact user trust and decision-making in predictive maintenance?
- Basis in paper: [inferred] The paper discusses the importance of explainability for building trust in predictive maintenance systems, but does not provide empirical evidence on how different explanation types affect user behavior.
- Why unresolved: While various XAI methods are described, their relative effectiveness in real-world predictive maintenance scenarios is not established.
- What evidence would resolve it: User studies comparing the impact of different explanation types on trust, understanding, and decision-making accuracy in predictive maintenance tasks.

### Open Question 3
- Question: What is the optimal balance between model performance and interpretability in predictive maintenance systems?
- Basis in paper: [explicit] The paper notes that complex, high-performing models often require post-hoc explainability methods, while simpler interpretable models may sacrifice performance.
- Why unresolved: The trade-off between model accuracy and interpretability is not quantified, and the ideal balance may vary depending on the specific predictive maintenance application and stakeholder needs.
- What evidence would resolve it: Systematic studies comparing the performance-interpretability trade-off across various predictive maintenance tasks, identifying scenarios where simpler interpretable models suffice versus when complex models with explanations are necessary.

## Limitations
- The survey's systematic methodology appears sound, but the actual execution quality cannot be fully verified from the paper alone
- Claims about specific usage frequencies of methods and the relative importance of identified challenges depend heavily on the search strategy's effectiveness
- Potential database coverage gaps and keyword selection bias may affect the comprehensiveness of identified literature

## Confidence
- **High confidence**: The survey successfully identifies and categorizes existing XAI methods applied to PdM, providing a useful taxonomy and highlighting key challenges
- **Medium confidence**: The systematic methodology (PRISMA 2020) provides appropriate framework, though execution quality is uncertain
- **Low confidence**: Claims about specific usage frequencies of methods and the relative importance of identified challenges, as these depend heavily on the search strategy's effectiveness

## Next Checks
1. **Database coverage verification**: Test the search strategy by checking whether key papers identified through other means (citation chasing, expert recommendations) appear in the search results
2. **Taxonomy validation**: Survey a sample of XAI researchers not involved in PdM to assess whether the method categorization aligns with broader XAI understanding
3. **Coverage timeline assessment**: Compare the survey's identified methods with more recent XAI publications (post-2023) to determine what proportion of current methods are captured