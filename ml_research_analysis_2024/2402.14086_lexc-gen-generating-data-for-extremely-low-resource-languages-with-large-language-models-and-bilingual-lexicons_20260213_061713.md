---
ver: rpa2
title: 'LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large
  Language Models and Bilingual Lexicons'
arxiv_id: '2402.14086'
source_url: https://arxiv.org/abs/2402.14086
tags:
- data
- task
- lexc-gen
- languages
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LexC-Gen generates synthetic data for extremely low-resource languages\
  \ by training large language models (LLMs) to produce lexicon-compatible task data\
  \ in high-resource languages, then translating it word-by-word using bilingual lexicons.\
  \ This approach addresses the data-lexicon mismatch problem, achieving competitive\
  \ performance with expert-translated gold data\u2014improving accuracy by 5.6 points\
  \ on sentiment analysis and 8.9 points on topic classification over direct word\
  \ translation baselines across 17 extremely low-resource languages."
---

# LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons

## Quick Facts
- **arXiv ID**: 2402.14086
- **Source URL**: https://arxiv.org/abs/2402.14086
- **Reference count**: 38
- **Primary result**: Achieves competitive performance with expert-translated gold data on sentiment analysis and topic classification across 17 extremely low-resource languages.

## Executive Summary
LexC-Gen addresses the data scarcity problem for extremely low-resource languages by generating synthetic task data through a novel approach that combines lexicon-conditioned text generation with word-to-word translation. The method trains a large language model to produce task data using words from bilingual lexicons, then translates this data word-by-word to create labeled datasets for low-resource languages. Experiments show LexC-Gen improves accuracy by 5.6 points on sentiment analysis and 8.9 points on topic classification over direct word translation baselines, effectively closing the performance gap between open-source multilingual models and commercial models like GPT-4o.

## Method Summary
LexC-Gen generates synthetic labeled data for extremely low-resource languages through a three-step process: first, it performs controlled text generation (CTG) training on an LLM using existing task data with lexicon-conditioned prompts that include random subsets of bilingual lexicon words; second, it generates lexicon-compatible task data in English and filters it using input-label consistency checking; third, it translates the filtered data word-by-word using the bilingual lexicon to create labeled datasets in target low-resource languages. The approach addresses the data-lexicon mismatch problem by ensuring high lexical overlap between generated data and available translation resources, enabling effective utilization of bilingual lexicons for low-resource language tasks.

## Key Results
- Improves accuracy by 5.6 points on sentiment analysis and 8.9 points on topic classification over direct word translation baselines across 17 extremely low-resource languages
- Closes performance gap between open-source multilingual models (BLOOMZ, Aya-101) and commercial models (GPT-4o) on low-resource-language tasks
- Scaling up generated data increases lexicon utilization rate, with larger datasets incorporating more vocabulary from bilingual lexicons
- Lexicon-conditioning is essential for strong performance, as standard generation without conditioning shows significantly worse results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lexicon-conditioning improves word translation coverage by aligning generated task data with bilingual lexicon vocabulary
- Mechanism: LexC-Gen generates task data using words from the bilingual lexicon, ensuring high lexical overlap that increases the proportion of words that can be translated via word-for-word substitution
- Core assumption: The bilingual lexicon contains enough task-relevant semantic information for effective classification in low-resource languages
- Evidence anchors: [abstract] "lexicon-conditioned data generation... generates low-resource-language classification task data at scale... uses high-resource-language words from bilingual lexicons to generate lexicon-compatible task data" [section] "we argue that it is ineffective because of data-lexicon mismatch... words in the existing task data... have low lexical overlap with the words in the task-agnostic bilingual lexicons"

### Mechanism 2
- Claim: Scaling generated data increases lexicon utilization rate, providing more semantic context for classifiers
- Mechanism: As more task data instances are generated, they incorporate a larger proportion of the bilingual lexicon's vocabulary, enriching the semantic information available for training
- Core assumption: Semantic information from lexicon words is sufficient for classification tasks like sentiment analysis and topic classification
- Evidence anchors: [abstract] "scaling up generated data increases lexicon utilization rate" [section] "Scaling is enabled by the generative nature of LexC-Gen, as opposed to previous approaches constrained to the quantity of labeled task data"

### Mechanism 3
- Claim: Quality control via input-label consistency filtering reduces training noise and improves model performance
- Mechanism: A classifier trained on existing task data relabels generated data, and instances with mismatched labels are filtered out, removing labeling errors
- Core assumption: The classifier used for filtering accurately predicts labels for generated data instances
- Evidence anchors: [abstract] "we propose a quality-control method that checks for input-label consistency to filter out poor-quality generated data" [section] "we apply an input-label consistency filter after data generation to reduce training noise from labeling errors"

## Foundational Learning

- **Controlled Text Generation (CTG) training**: Why needed here: Standard LLMs lack the ability to generate text conditioned on specific word sets, which is essential for lexicon-conditioned generation. Quick check question: How does CTG training differ from standard instruction tuning, and why is it necessary for LexC-Gen?

- **Word-to-word translation using bilingual lexicons**: Why needed here: Low-resource languages lack translation models/APIs, so word substitution with bilingual lexicons is the primary translation method. Quick check question: What are the limitations of word-to-word translation compared to neural machine translation, and how does LexC-Gen mitigate these limitations?

- **Quality control in synthetic data generation**: Why needed here: Generated data may contain labeling errors that harm classifier performance, requiring filtering mechanisms. Quick check question: How does input-label consistency filtering differ from other quality control methods like label distillation, and why is it more effective for LexC-Gen?

## Architecture Onboarding

- **Component map**: CTG-trained LLM (BLOOMZ-7.1B) -> Bilingual lexicon (Gatitos) -> Input-label consistency filter -> Task classifier (mBERT/XLMR) -> Evaluation
- **Critical path**: 1. CTG training: Existing task data → CTG-trained LLM 2. Data generation: Lexicon words + class labels → LLM → filtered data 3. Translation: Generated data → word-to-word translation → low-resource task data 4. Classifier training: Translated data → task classifier → evaluation
- **Design tradeoffs**: Lexicon size vs. quality: Larger lexicons provide more vocabulary but may include noisy translations; Generated data quantity vs. quality: More data increases lexicon utilization but requires effective filtering; Language family proximity: Using source language more related to target language may improve generation quality
- **Failure signatures**: Low word translation coverage: Indicates poor lexical overlap between generated data and bilingual lexicon; Poor task performance: May indicate insufficient semantic information in lexicon words or inadequate filtering; High filtering rate: Suggests LLM struggles to generate data matching provided labels
- **First 3 experiments**: 1. Ablation study: Compare lexicon-conditioned generation vs. standard generation with and without filtering 2. Scaling analysis: Measure lexicon utilization rate and task performance across different generated data sizes 3. Quality control comparison: Compare input-label consistency filtering vs. label distillation for LexC-Gen generated data

## Open Questions the Paper Calls Out
- **How does performance change on semantic tasks requiring high sensitivity to sentence-level meaning?** The authors acknowledge that future work should explore LexC-Gen's potential and limitations on tasks requiring sensitivity to semantic complexity at the sentence level, such as common sense reasoning and natural language inference. This remains unresolved as the paper only evaluates on sentiment analysis and topic classification.

- **How does performance change with bilingual lexicons including linguistic information?** The authors note that word-to-word translation may introduce errors due to word ambiguity, as low-resource-language words in lexicons lack linguistic information or context necessary for word sense disambiguation. This remains unresolved as the paper uses bilingual lexicons without linguistic information.

- **How does performance change on languages with syntactic structures significantly different from English?** The authors acknowledge that word-to-word translation suffers from inherent limitations due to unchanged syntax, and future work should explore syntactical transformation of synthetic data to better align with low-resource languages for tasks relying heavily on syntactic information.

## Limitations
- The exact prompt template for CTG training is not fully specified, making precise replication difficult
- Key hyperparameters for CTG training (learning rate, batch size, training steps) are unspecified
- The approach relies on high-quality bilingual lexicons, but performance impact of lexicon noise is not thoroughly analyzed
- Word-to-word translation limitations (word ambiguity, unchanged syntax) are acknowledged but not fully addressed

## Confidence

**High Confidence**: The core claim that lexicon-conditioning improves word translation coverage is well-supported by results showing 5.6-point accuracy improvements on sentiment analysis and 8.9-point improvements on topic classification over word translation baselines across 17 languages.

**Medium Confidence**: The claim that scaling generated data increases lexicon utilization rate is supported by experimental results but lacks detailed analysis of the relationship between data quantity and coverage quality.

**Low Confidence**: The quality control mechanism's effectiveness is asserted but not thoroughly validated, with insufficient error analysis of filtered instances or comparison to alternative quality control methods.

## Next Checks
1. **Lexicon Quality Analysis**: Systematically evaluate how varying levels of bilingual lexicon quality (noisy vs. clean translations) affect LexC-Gen performance across different language pairs to validate robustness to lexicon imperfections.

2. **Prompt Template Reconstruction**: Reconstruct the CTG prompt template through systematic experimentation with different formulations to identify critical components that enable effective lexicon-conditioned generation.

3. **Quality Control Ablation Study**: Conduct comprehensive ablation study comparing input-label consistency filtering against alternative quality control methods (label distillation, human validation, no filtering) across multiple low-resource languages to quantify true impact on final performance.