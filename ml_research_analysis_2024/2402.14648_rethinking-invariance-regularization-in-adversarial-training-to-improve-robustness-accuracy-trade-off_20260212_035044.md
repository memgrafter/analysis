---
ver: rpa2
title: Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy
  Trade-off
arxiv_id: '2402.14648'
source_url: https://arxiv.org/abs/2402.14648
tags:
- adversarial
- invariance
- clean
- regularization
- ar-at
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the robustness-accuracy trade-off in adversarial
  training by focusing on invariance regularization. The authors identify two key
  issues: (1) a "gradient conflict" between invariance and classification objectives
  leading to suboptimal convergence, and (2) the mixture distribution problem in BatchNorm
  layers when using the same statistics for clean and adversarial inputs.'
---

# Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off

## Quick Facts
- arXiv ID: 2402.14648
- Source URL: https://arxiv.org/abs/2402.14648
- Reference count: 40
- This paper addresses the robustness-accuracy trade-off in adversarial training by focusing on invariance regularization.

## Executive Summary
This paper identifies two key issues in invariance regularization-based adversarial training: gradient conflict between invariance and classification objectives leading to suboptimal convergence, and the mixture distribution problem in BatchNorm layers when using the same statistics for clean and adversarial inputs. To address these issues, the authors propose Asymmetric Representation-regularized Adversarial Training (ARAT), which incorporates asymmetric invariance regularization with stop-gradient operation and a predictor, along with a split-BatchNorm structure. Experimental results show that ARAT outperforms existing methods across various settings, achieving higher clean and robust accuracy compared to state-of-the-art invariance regularization-based defenses like TRADES, MART, and LBGAT.

## Method Summary
The paper proposes Asymmetric Representation-regularized Adversarial Training (ARAT) to improve the robustness-accuracy trade-off in adversarial training. ARAT uses asymmetric invariance regularization with a stop-gradient operation to prevent gradient conflict between classification and invariance objectives, and a split-BatchNorm structure to address the mixture distribution problem. Additionally, a predictor MLP head is introduced to stabilize training by handling variations in adversarial representations. The method is evaluated on CIFAR-10 and CIFAR-100 with ResNet-18 and WideResNet-34-10 architectures.

## Key Results
- ARAT outperforms existing invariance regularization-based defenses (TRADES, MART, LBGAT) on CIFAR-10 and CIFAR-100
- Achieves higher clean accuracy and robust accuracy against AutoAttack and PGD-20 attacks
- Resolves identified issues of gradient conflict and mixture distribution problem in BatchNorm layers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Stop-gradient resolves "gradient conflict" by eliminating gradient flow from adversarial to clean representations.
- **Mechanism**: The stop-gradient operation treats the clean representation z as constant when computing the invariance loss with respect to the adversarial representation z'. This prevents the adversarial loss from corrupting the clean representation's gradient direction.
- **Core assumption**: The second term of the symmetric invariance loss (Dist(sg(z'), z)) actively conflicts with classification objectives by pushing clean representations toward corrupted adversarial representations.
- **Evidence anchors**:
  - [abstract] Identifies "gradient conflict" between invariance and classification objectives leading to suboptimal convergence.
  - [section] "Minimizing the second term, Dist(sg(z'), z), attempts to bring the clean representation z closer to the potentially corrupted adversarial representation z', encouraging the 'corruption' of representations."
  - [corpus] Weak evidence - no direct corpus citations support this specific mechanism.
- **Break condition**: If adversarial representations do not significantly corrupt clean representations, or if the model architecture naturally prevents gradient conflict through other mechanisms.

### Mechanism 2
- **Claim**: Split-BatchNorm resolves the "mixture distribution problem" by using separate statistics for clean and adversarial inputs.
- **Mechanism**: By maintaining separate batch normalization statistics for clean and adversarial inputs, the model avoids the suboptimal estimation that occurs when both distributions are mixed during training.
- **Core assumption**: Clean and adversarial inputs exhibit sufficiently diverged distributions that shared batch normalization statistics become harmful.
- **Evidence anchors**:
  - [abstract] Identifies "mixture distribution problem arising from diverged distributions between clean and adversarial inputs."
  - [section] "Since clean and adversarial inputs exhibit diverged distributions, using the same BNs on a mixture of these inputs can be suboptimal for both inputs."
  - [corpus] No direct corpus evidence found for this specific mechanism.
- **Break condition**: If clean and adversarial distributions are similar enough that shared batch normalization is beneficial, or if the model architecture doesn't rely heavily on batch normalization.

### Mechanism 3
- **Claim**: Predictor MLP head stabilizes training by handling variations in adversarial representations.
- **Mechanism**: The predictor MLP acts as a "stabilizer" that smooths the optimization of invariance regularization by preventing large fluctuations in classifier parameters caused by variations in adversarial representations.
- **Core assumption**: Adversarial representations vary significantly between training iterations due to the randomness inherent in the attack process.
- **Evidence anchors**:
  - [section] "Therefore, introducing an additional predictor head may help stabilize the training process by handling the variations of adversarial representation through updates of the predictor."
  - [section] "Fig. 4a shows the cosine distance between the adversarial representations at epochs t and t + 1, where the predictor MLP stabilizes the updates of the representations where invariance loss is applied."
  - [corpus] No direct corpus evidence found for this specific mechanism.
- **Break condition**: If adversarial representations are stable across training iterations, or if the model architecture already provides sufficient stability without the predictor.

## Foundational Learning

- **Concept: Adversarial Training**
  - Why needed here: Understanding how adversarial training works is fundamental to grasping why the robustness-accuracy trade-off exists and how ARAT addresses it.
  - Quick check question: What is the core optimization problem solved by standard adversarial training?

- **Concept: Batch Normalization**
  - Why needed here: The split-BatchNorm mechanism relies on understanding how batch normalization works and why mixing clean and adversarial statistics is problematic.
  - Quick check question: How do batch normalization layers compute their running statistics during training?

- **Concept: Gradient Conflict**
  - Why needed here: The stop-gradient mechanism specifically targets gradient conflict, so understanding what gradient conflict means in multi-objective optimization is essential.
  - Quick check question: What does it mean when two loss functions have negative cosine similarity between their gradients?

## Architecture Onboarding

- **Component map**: Standard classification head with adversarial loss -> auxiliary branch with clean loss -> stop-gradient operation -> predictor MLP for latent projection -> split BatchNorm layers
- **Critical path**: The most important components are the stop-gradient operation and split BatchNorm, as they directly address the identified issues. The predictor MLP provides additional stability but is less critical.
- **Design tradeoffs**: ARAT trades increased memory usage (separate BN statistics) and computational overhead (extra forward pass for clean branch) for improved robustness-accuracy trade-off. The stop-gradient operation eliminates useful gradient information but prevents harmful corruption.
- **Failure signatures**: If gradient conflict persists (high cosine similarity between classification and invariance gradients), if batch statistics become unstable (high variance in running means), or if the predictor MLP causes overfitting to adversarial perturbations.
- **First 3 experiments**:
  1. Implement the stop-gradient operation alone on a simple invariance regularization baseline and measure gradient similarity reduction.
  2. Add split BatchNorm to the previous setup and measure changes in batch statistics stability and accuracy-robustness trade-off.
  3. Add the predictor MLP and measure training stability improvements (gradient norm reduction, representation similarity stability).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the stop-gradient operation's effectiveness generalize beyond representation-based regularization to other forms of adversarial training?
- Basis in paper: [explicit] The paper demonstrates stop-gradient resolves gradient conflict in representation-based AR-AT and logit-based Asym-TRADES, but notes this may not extend to all adversarial training methods.
- Why unresolved: The paper only tests stop-gradient on representation-based and logit-based regularization. Other adversarial training methods (e.g., those using different loss functions or architectures) might have different gradient dynamics.
- What evidence would resolve it: Systematic testing of stop-gradient across diverse adversarial training methods (different loss functions, architectures, and regularization strategies) to identify conditions where it consistently improves robustness-accuracy trade-offs.

### Open Question 2
- Question: How does the split-BatchNorm structure perform in architectures without Batch Normalization, such as Transformer-based models?
- Basis in paper: [inferred] The paper identifies mixture distribution problems specifically in BatchNorm layers and proposes split-BatchNorm as a solution, but acknowledges this may not apply to architectures using LayerNorm or other normalization techniques.
- Why unresolved: The paper focuses exclusively on CNNs with BatchNorm. The behavior of adversarial training with mixed clean/adversarial inputs in Transformer architectures remains unexplored.
- What evidence would resolve it: Empirical comparison of adversarial training performance with and without split normalization (or analogous mechanisms) in Transformer architectures, measuring robustness-accuracy trade-offs across different normalization schemes.

### Open Question 3
- Question: What is the optimal strategy for selecting layers to regularize in representation-based invariance regularization for novel model architectures?
- Basis in paper: [explicit] The paper shows that regularizing multiple layers in the later stage of networks improves performance, but notes that determining appropriate layers remains ambiguous, especially for novel architectures.
- Why unresolved: The paper provides empirical guidance (regularize later-stage layers) but doesn't establish a principled method for layer selection that generalizes across architectures.
- What evidence would resolve it: A theoretical framework or empirical study identifying architectural features that predict optimal regularization layers, validated across diverse network families beyond ResNet and WideResNet.

### Open Question 4
- Question: How does the predictor MLP head's effectiveness vary with different predictor architectures and training strategies?
- Basis in paper: [explicit] The paper uses a simple two-layer MLP predictor following SimSiam, but doesn't explore how different predictor designs affect training stability and performance.
- Why unresolved: The paper adopts a specific predictor architecture without systematic exploration of alternatives or analysis of how predictor design choices impact the stability and effectiveness of invariance regularization.
- What evidence would resolve it: Comparative analysis of different predictor architectures (depth, width, activation functions) and training strategies (initialization, learning rate schedules) measuring their impact on training stability and robustness-accuracy trade-offs.

## Limitations
- The paper lacks ablation studies isolating the individual contributions of stop-gradient, split-BatchNorm, and predictor MLP
- Claims about knowledge distillation defenses resolving the same issues remain speculative without rigorous analysis
- Empirical validation of proposed mechanisms is limited to correlation analysis rather than controlled experiments

## Confidence
- **Mechanism 1 (Stop-gradient)**: Medium confidence - The theoretical motivation is clear, but the empirical demonstration is limited to correlation analysis rather than controlled ablation.
- **Mechanism 2 (Split-BatchNorm)**: Medium confidence - The issue is well-identified, but the paper doesn't explore the degree to which clean and adversarial distributions diverge, nor does it compare against alternative normalization strategies.
- **Mechanism 3 (Predictor MLP)**: Low confidence - The stabilizing effect is inferred from cosine distance metrics, but the paper doesn't demonstrate that this is the primary mechanism for improved performance, nor does it explore simpler alternatives.

## Next Checks
1. **Ablation study design**: Conduct controlled experiments removing each component (stop-gradient, split-BN, predictor) individually to quantify their independent contributions to the robustness-accuracy trade-off.
2. **Distribution analysis**: Measure and visualize the divergence between clean and adversarial input distributions across different network layers to quantify the severity of the mixture distribution problem.
3. **Gradient analysis**: Perform detailed gradient analysis comparing the cosine similarity between classification and invariance gradients in ARAT versus standard invariance regularization methods, and correlate these metrics with final performance.