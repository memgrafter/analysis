---
ver: rpa2
title: 'Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy'
arxiv_id: '2411.03513'
source_url: https://arxiv.org/abs/2411.03513
tags:
- slicing
- layers
- layer
- slicegpt
- dynamic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a dynamic layer-specific pruning approach for
  large language models (LLMs) that improves upon the constant slicing method of SliceGPT.
  The key innovation is the Layer Redundancy (LR) score, which measures how much each
  layer changes its input using cosine similarity between inputs and outputs.
---

# Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy

## Quick Facts
- arXiv ID: 2411.03513
- Source URL: https://arxiv.org/abs/2411.03513
- Reference count: 32
- Primary result: Dynamic layer pruning using Layer Redundancy scores achieves up to 5% better accuracy and 7% lower perplexity than constant slicing across multiple benchmarks

## Executive Summary
This paper presents a dynamic layer-specific pruning approach for large language models (LLMs) that improves upon the constant slicing method of SliceGPT. The key innovation is the Layer Redundancy (LR) score, which measures how much each layer changes its input using cosine similarity between inputs and outputs. This score guides variable pruning of individual layers while maintaining a fixed average pruning percentage across all layers. Experiments on Llama3-8B and Mistral-7B demonstrate that dynamic slicing outperforms constant slicing, achieving up to 5% better accuracy and 7% lower perplexity across multiple benchmarks. The method shows that adaptive layer pruning based on redundancy leads to better performance than uniform pruning approaches.

## Method Summary
The method computes Layer Redundancy (LR) scores for each layer by measuring cosine similarity between layer inputs and outputs using validation data. These raw scores are then scaled to achieve a target average pruning percentage while preserving a base percentage for all layers. The scaled scores determine per-layer slicing percentages, which are applied using SliceGPT's PCA-based compression method. The approach maintains fixed average pruning across all layers while allowing variable pruning percentages based on layer redundancy, enabling more aggressive pruning of less important layers while preserving critical ones.

## Key Results
- Dynamic slicing achieves up to 5% better accuracy compared to constant slicing across four classification benchmarks
- Perplexity decreases by as much as 1.4 points compared to constant slicing
- The method works effectively on both Llama3-8B and Mistral-7B architectures with similar performance improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer redundancy scores enable dynamic layer pruning that adapts to individual layer contributions rather than applying uniform pruning across all layers.
- Mechanism: The Layer Redundancy (LR) score measures cosine similarity between layer inputs and outputs, identifying which layers change their input less (more redundant). Higher redundancy layers can be pruned more aggressively while preserving critical layers, maintaining performance with less overall pruning.
- Core assumption: Layers with higher cosine similarity between input and output are more redundant and can be pruned more without significant performance loss.
- Evidence anchors:
  - [abstract] "The key innovation is the Layer Redundancy (LR) score, which measures how much each layer changes its input using cosine similarity between inputs and outputs."
  - [section 3.1] "We define this as the Layer Redundancy (LR) score: LR(Li) = LI_i Â· LO_i / (||LI_i|| ||LO_i||)"
  - [corpus] Weak evidence - only one related paper mentions "redundant nature" of layers but doesn't discuss layer redundancy scoring
- Break condition: If layers that appear redundant based on cosine similarity actually perform critical functions not captured by input-output similarity, or if cosine similarity doesn't correlate with functional importance.

### Mechanism 2
- Claim: Dynamic slicing maintains fixed average pruning percentage while allowing variable layer-specific pruning percentages.
- Mechanism: The method scales LR scores to achieve a target average pruning percentage (SP) while preserving a base percentage (SB) for all layers. This creates per-layer slicing percentages that sum to the desired total while allowing more aggressive pruning of redundant layers.
- Core assumption: It's possible to maintain performance while varying pruning percentages across layers as long as the average remains fixed.
- Evidence anchors:
  - [section 3.2] "Our goal is to have a function that slices variable sized parts of each layers based on their LR score while keeping the overall average of sliced out parts across all layers to be a fixed percentage"
  - [abstract] "This score guides variable pruning of individual layers while maintaining a fixed average pruning percentage across all layers"
  - [corpus] No direct evidence - corpus papers don't discuss fixed average pruning with variable per-layer percentages
- Break condition: If the assumption that fixed average pruning equals performance is incorrect, or if maintaining the fixed average constraint prevents optimal layer-specific pruning.

### Mechanism 3
- Claim: Layer redundancy pruning outperforms constant slicing methods by preserving functionality in critical layers.
- Mechanism: By pruning more from redundant layers and less from important layers based on LR scores, the method maintains better model accuracy and lower perplexity compared to uniform pruning approaches like SliceGPT.
- Core assumption: Dynamic allocation of pruning based on layer importance preserves more model functionality than uniform pruning.
- Evidence anchors:
  - [abstract] "Experiments on Llama3-8B and Mistral-7B demonstrate that dynamic slicing outperforms constant slicing, achieving up to 5% better accuracy and 7% lower perplexity"
  - [section 5] "Figure 2 shows the behaviors of Llama3-8b and Mistral-7b... the accuracy is improved across all of the 5 evaluated datasets, and the perplexity decreases by as much as 1.4"
  - [corpus] Weak evidence - corpus papers mention "redundant nature" but don't compare dynamic vs constant slicing performance
- Break condition: If the performance gains are due to factors other than the dynamic allocation (e.g., different hyperparameters, data splits, or implementation details).

## Foundational Learning

- Concept: Cosine similarity and its interpretation in neural network layers
  - Why needed here: The LR score fundamentally relies on measuring cosine similarity between layer inputs and outputs to quantify redundancy
  - Quick check question: If layer input vector is [1, 2, 3] and output vector is [2, 4, 6], what is their cosine similarity and what does this imply about layer redundancy?

- Concept: Principal Component Analysis (PCA) for dimensionality reduction
  - Why needed here: The slicing method uses a specialized version of PCA to compress layer representations by selectively omitting components
  - Quick check question: In PCA-based compression, if we have 100 principal components and keep only the top 10, what percentage of the original dimensionality are we preserving?

- Concept: Model pruning and its impact on model performance
  - Why needed here: Understanding how removing parameters affects accuracy and perplexity is crucial for interpreting the experimental results
  - Quick check question: If a model has 7 billion parameters and we prune 30%, how many parameters remain, and what typical performance degradation might we expect based on literature?

## Architecture Onboarding

- Component map:
  - LR score calculator: Computes cosine similarity between layer inputs/outputs using validation data
  - LR scaler: Transforms raw LR scores to achieve target average pruning percentage
  - Layer slicer: Applies variable PCA-based compression to each layer based on computed slicing percentages
  - Evaluation pipeline: Measures accuracy and perplexity across multiple benchmarks
  - Configuration manager: Handles different model architectures (Llama3-8B, Mistral-7B) and pruning ratios

- Critical path:
  1. Compute LR scores for all layers using validation data
  2. Scale LR scores to achieve target average pruning percentage
  3. Apply variable layer-specific slicing using PCA-based compression
  4. Evaluate pruned model on benchmarks
  5. Compare against baseline constant slicing

- Design tradeoffs:
  - Fixed average pruning vs. optimal layer-specific pruning: Maintaining fixed average enables fair comparison but may limit optimal performance
  - Validation data choice: Using pg-19 validation set for LR computation vs. task-specific data
  - Computational cost: Computing LR scores requires full validation pass but only needs to be done once per model

- Failure signatures:
  - Performance worse than constant slicing: Indicates LR scores don't correlate with actual layer importance
  - Extremely uneven pruning distribution: Suggests LR score scaling needs adjustment
  - High perplexity despite good accuracy: Indicates text generation quality degradation

- First 3 experiments:
  1. Run constant slicing (SB=SP) as baseline to verify implementation matches SliceGPT results
  2. Implement LR score computation and verify cosine similarity values make intuitive sense (high for redundant layers, low for important layers)
  3. Test dynamic slicing with SP=30%, SB=10% on Llama3-8B and compare accuracy/perplexity against baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Layer Redundancy (LR) score correlate with other layer importance metrics such as spectral analysis or gradient-based methods?
- Basis in paper: [explicit] The paper mentions that SliceGPT's initial attempts to implement dynamic pruning through spectral analysis did not yield a reliable method, and that the LR score is a new metric introduced in this work.
- Why unresolved: The paper does not compare the LR score with other established layer importance metrics, leaving open the question of whether LR is more effective or if it captures different aspects of layer importance.
- What evidence would resolve it: Empirical studies comparing LR scores with spectral analysis, gradient-based methods, or other importance metrics on the same models and datasets, showing which metric better predicts layer redundancy and pruning performance.

### Open Question 2
- Question: Can the dynamic slicing method be effectively extended to hybrid approaches that combine layer removal with partial layer pruning, such as removing the least important layers completely and applying dynamic slicing to moderately important layers?
- Basis in paper: [explicit] The paper suggests that variable slicing scaled from layer importance can be easily extended and merged with techniques like ShortGPT by removing the least important layers completely and slicing moderately important layers using the proposed approach.
- Why unresolved: The paper does not provide experimental results or analysis of such hybrid methods, leaving open the question of whether combining layer removal with partial pruning would enhance model efficiency and performance.
- What evidence would resolve it: Experimental results comparing hybrid methods (combining layer removal and partial pruning) with the current dynamic slicing approach, showing improvements in accuracy, perplexity, and computational efficiency.

### Open Question 3
- Question: What is the optimal method for selecting the Slice Base (SB) value, beyond using the minimum perplexity on a calibration dataset like Wikitextv2?
- Basis in paper: [explicit] The paper notes that they only experiment with one method to choose the SB value (that gives lowest perplexity) and there can be other methods for estimating SB which they leave for future work.
- Why unresolved: The paper does not explore alternative methods for selecting SB, such as using task-specific validation sets or cross-validation, leaving open the question of whether there are better ways to determine SB that could further improve model performance.
- What evidence would resolve it: Comparative studies using different SB selection methods (e.g., task-specific validation, cross-validation, or learning-based approaches) to determine which method consistently yields the best performance across various models and tasks.

## Limitations

- The method relies on the assumption that cosine similarity between layer inputs and outputs accurately reflects layer redundancy, which may not hold for all architectures or tasks
- Performance gains are modest (up to 5% accuracy, 7% perplexity) and the method requires careful tuning of the base slicing percentage (SB)
- The approach is evaluated only on two specific model architectures (Llama3-8B and Mistral-7B) and five benchmarks, limiting generalizability claims

## Confidence

**High Confidence Claims:**
- The Layer Redundancy (LR) score calculation using cosine similarity is correctly implemented and mathematically sound
- The scaling mechanism to achieve fixed average pruning percentage while allowing variable per-layer percentages is technically valid
- The experimental methodology using standard benchmarks (PiQa, Hellaswag, Winogrande, Arc Easy, Wikitextv2) is appropriate

**Medium Confidence Claims:**
- The dynamic slicing approach outperforms constant slicing by the reported margins (5% accuracy, 7% perplexity)
- The observed performance gains are primarily attributable to the adaptive layer pruning strategy rather than implementation artifacts
- The method generalizes across different model architectures (Llama3-8B, Mistral-7B)

**Low Confidence Claims:**
- The interpretation that cosine similarity directly measures layer redundancy and importance
- The assertion that maintaining fixed average pruning percentage is optimal for performance
- The claim that this approach represents the best method for layer-specific pruning without comparison to other dynamic pruning strategies

## Next Checks

1. **Ablation Study on LR Score Calculation**: Run experiments with random LR scores versus computed LR scores to verify that the performance gains are specifically due to the redundancy-based allocation rather than the variable pruning mechanism itself.

2. **Cross-Dataset Generalization Test**: Evaluate the same dynamically pruned models on completely different datasets not used in LR score computation or original evaluation to test if the approach generalizes beyond the specific benchmarks reported.

3. **Layer-wise Performance Analysis**: Analyze accuracy and perplexity contributions from individual layers in both dynamically and constantly sliced models to verify that the dynamic approach is indeed preserving more functionality in high-importance layers as claimed.