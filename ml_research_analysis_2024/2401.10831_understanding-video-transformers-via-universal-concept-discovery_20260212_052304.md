---
ver: rpa2
title: Understanding Video Transformers via Universal Concept Discovery
arxiv_id: '2401.10831'
source_url: https://arxiv.org/abs/2401.10831
tags:
- concepts
- video
- concept
- vtcd
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of interpreting the inner workings
  of video transformer models by introducing the first Video Transformer Concept Discovery
  (VTCD) algorithm. VTCD decomposes video transformer representations into interpretable,
  high-level spatiotemporal concepts that are automatically discovered without labeled
  data.
---

# Understanding Video Transformers via Universal Concept Discovery

## Quick Facts
- arXiv ID: 2401.10831
- Source URL: https://arxiv.org/abs/2401.10831
- Reference count: 40
- Primary result: First method to discover interpretable spatiotemporal concepts in video transformers without labeled data, revealing universal mechanisms across diverse architectures

## Executive Summary
This paper introduces Video Transformer Concept Discovery (VTCD), the first algorithm to decompose video transformer representations into interpretable, high-level spatiotemporal concepts without requiring labeled data. The method generates spatiotemporal tubelet proposals in feature space and clusters them to discover concepts, then quantifies their importance to model predictions using a noise-robust Concept Randomized Importance Sampling (CRIS) approach. VTCD is applied to study four diverse video transformer models, revealing universal mechanisms across architectures: early layers encode spatiotemporal positional information, middle layers develop object-centric representations, and later layers capture fine-grained spatiotemporal events. The discovered concepts can be used for video object segmentation, achieving strong performance on the DAVIS'16 benchmark even for self-supervised representations, and pruning unimportant heads based on VTCD importance rankings improves both accuracy and efficiency for action classification tasks.

## Method Summary
VTCD operates by first generating spatiotemporal tubelets through SLIC clustering in the feature space of video transformers, then clustering these tubelets into high-level concepts using Convex Non-Negative Matrix Factorization (CNMF). The method quantifies concept importance through Concept Randomized Importance Sampling (CRIS), which masks multiple concepts simultaneously to overcome transformer robustness to perturbations. VTCD discovers Rosetta concepts - shared concepts across different models with high similarity scores based on mean Intersection over Union (mIoU) of support masks. The approach is validated on four video transformer models (TCOW, VideoMAE, InternVideo) across synthetic and real video datasets, with applications including video object segmentation and model pruning.

## Key Results
- VTCD discovers universal mechanisms across video transformer models: early layers encode positional information, middle layers develop object-centric representations, and later layers capture fine-grained spatiotemporal events
- Object-centric concepts discovered by VTCD achieve strong performance on DAVIS'16 video object segmentation benchmark, even for self-supervised representations
- Pruning unimportant heads based on VTCD importance rankings improves action classification accuracy by 4.3% while reducing FLOPS by 33%
- CRIS importance estimation dramatically outperforms single-concept occlusion and gradient-based baselines in attribution curves

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Video Transformer Concept Discovery (VTCD) can decompose video transformer representations into interpretable spatiotemporal concepts without labeled data.
- Mechanism: VTCD uses SLIC clustering in feature space to generate spatiotemporal tubelets, then applies Convex Non-Negative Matrix Factorization (CNMF) to cluster these tubelets into high-level concepts across videos.
- Core assumption: Feature space clustering captures meaningful spatiotemporal patterns that humans can interpret as concepts.
- Evidence anchors:
  - [abstract] "VTCD decomposes video transformer representations into interpretable, high-level spatiotemporal concepts that are automatically discovered without labeled data"
  - [section 3.1.1] "We produce tubelets per-video via Simple Linear Iterative Clustering [1] (SLIC) on the spatiotemporal features"
  - [corpus] Weak evidence - corpus neighbors discuss concept-based approaches but don't specifically validate VTCD's clustering approach.
- Break condition: If the SLIC clustering parameters (number of segments, compactness) are poorly chosen, the resulting tubelets may not capture meaningful spatiotemporal patterns.

### Mechanism 2
- Claim: VTCD's Concept Randomized Importance Sampling (CRIS) can accurately rank concept importance for transformer models, even with redundant attention heads.
- Mechanism: CRIS randomly samples K concept sets and masks them in parallel across all layers, then measures performance drop to estimate importance, averaging over thousands of samples.
- Core assumption: Transformers' robustness to minor perturbations in self-attention layers requires masking multiple concepts simultaneously to detect importance.
- Evidence anchors:
  - [abstract] "quantifies their importance to model predictions using a noise-robust Concept Randomized Importance Sampling (CRIS) approach"
  - [section 3.2] "single concept masking has little effect on performance... Instead, we mask a high percentage of sampled concepts in parallel"
  - [section 5.1] "CRIS produces a more viable importance ranking, dramatically outperforming both random ordering and the occlusion baseline"
- Break condition: If K is too small, the random sampling may not adequately capture concept importance; if too large, computational cost becomes prohibitive.

### Mechanism 3
- Claim: VTCD can discover universal mechanisms across video transformer models trained for different tasks (supervised, self-supervised, video-language).
- Mechanism: VTCD identifies Rosetta concepts - concepts shared between models with high similarity scores based on mean Intersection over Union (mIoU) of support masks.
- Core assumption: Different video transformer architectures learn similar spatiotemporal representations despite different training objectives.
- Evidence anchors:
  - [abstract] "discover universal mechanisms: early layers encode spatiotemporal positional information, middle layers develop object-centric representations, and later layers capture fine-grained spatiotemporal events"
  - [section 4.1] "We then mine Rosetta concepts following Section 4.1, with δ = 0.15 and ϵ = 15% in all experiments"
  - [section 5.3] "Interestingly, we find object-centric representations in all the models"
- Break condition: If the similarity threshold (δ) is set too high, genuine universal concepts may be missed; if too low, spurious matches may be included.

## Foundational Learning

- Concept: Spatiotemporal tubelet proposals
  - Why needed here: Video transformers process 4D tensors (batch, channels, time, space), requiring a way to segment these into interpretable units that capture both spatial and temporal relationships.
  - Quick check question: Why does VTCD generate proposals in feature space rather than pixel space, and what advantage does this provide?

- Concept: Convex Non-Negative Matrix Factorization (CNMF)
  - Why needed here: Standard NMF cannot handle negative activations from GeLU non-linearities in transformers, requiring an extension that works with both positive and negative values.
  - Quick check question: How does CNMF's constraint that columns of W are convex combinations of columns of T differ from standard NMF, and why is this necessary for video transformer features?

- Concept: Concept Randomized Importance Sampling (CRIS)
  - Why needed here: Transformers' robustness to minor perturbations means single-concept masking has little effect, requiring a sampling-based approach to detect importance.
  - Quick check question: Why does CRIS mask multiple concepts simultaneously rather than individually, and how does this overcome the transformer's robustness to perturbations?

## Architecture Onboarding

- Component map: Tubelet generation (SLIC) -> Concept clustering (CNMF) -> Importance estimation (CRIS) -> Rosetta mining -> Downstream application
- Critical path: Feature extraction → Tubelet generation (SLIC) → Concept clustering (CNMF) → Importance estimation (CRIS) → Rosetta mining → Downstream application
- Design tradeoffs: SLIC compactness parameter trades off between capturing fine-grained vs. coarse concepts; CNMF number of clusters balances concept granularity vs. interpretability; CRIS sample size trades off between accuracy and computational cost.
- Failure signatures: Poor SLIC parameters result in noisy tubelets that don't align with human-interpretable concepts; too few CNMF clusters lead to overly broad concepts; CRIS with insufficient samples produces unreliable importance rankings.
- First 3 experiments:
  1. Generate tubelets with different SLIC compactness values on a small video sample and visualize the resulting proposals to find the optimal setting for your target model.
  2. Apply CNMF clustering to the tubelets with varying cluster counts and qualitatively assess which produces the most interpretable concepts.
  3. Run CRIS with different sample sizes (K) on a known important concept to verify that larger K produces more stable importance scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the universal concepts discovered by VTCD compare across different architectural variants of transformers (e.g., Swin, DeiT, ViViT) beyond the four models studied?
- Basis in paper: [explicit] The authors mention analyzing four diverse models (TCOW, VideoMAE variants, InternVideo) and finding shared concepts, suggesting potential for broader architectural comparison
- Why unresolved: The paper only examines four specific models, leaving open whether similar universal concepts exist across different transformer architectures designed for video
- What evidence would resolve it: Testing VTCD on a broader range of transformer architectures and comparing the discovered concept sets and their importance rankings across architectures

### Open Question 2
- Question: Can VTCD be adapted to discover concepts in multimodal transformers that process both video and audio or text alongside visual information?
- Basis in paper: [inferred] The paper discusses VTCD's potential for video-language models (mentioning InternVideo) and mentions multimodal transformers in related work, but doesn't explore cross-modal concept discovery
- Why unresolved: While VTCD handles video transformers well, extending it to discover concepts that bridge visual and non-visual modalities remains unexplored
- What evidence would resolve it: Applying VTCD to multimodal models and analyzing whether it can discover concepts that integrate information across different input modalities

### Open Question 3
- Question: How does the choice of SLIC hyperparameters (number of segments, compactness) affect the quality and interpretability of discovered concepts across different video datasets?
- Basis in paper: [explicit] The authors mention manually tuning the compactness parameter for each model and dataset, indicating sensitivity to these choices
- Why unresolved: While the paper demonstrates VTCD's effectiveness, it doesn't systematically study how different SLIC settings impact concept discovery quality or whether optimal parameters generalize across datasets
- What evidence would resolve it: Conducting systematic ablation studies varying SLIC parameters across multiple video datasets and measuring concept interpretability through human studies or downstream task performance

### Open Question 4
- Question: What is the relationship between the universal concepts discovered by VTCD and the emerging field of intuitive physics - can video transformers learn physics-like reasoning without explicit physics supervision?
- Basis in paper: [explicit] The authors discuss discovering spatiotemporal events and object-container relationships in self-supervised models, and question whether intuitive physics can be learned via large-scale video representation training
- Why unresolved: While the paper shows self-supervised models develop physics-relevant concepts, it doesn't explicitly test whether these concepts capture physical laws or could be used for physics reasoning tasks
- What evidence would resolve it: Testing discovered concepts on physics prediction tasks or analyzing their behavior when applied to videos with counterfactual physical scenarios

## Limitations

- The method's effectiveness across different video transformer architectures beyond the four tested models remains unproven, limiting generalizability claims.
- Computational cost of CRIS (requiring thousands of samples) may limit practical applicability for large-scale models.
- The assumption that SLIC clustering in feature space will consistently capture human-interpretable concepts across diverse video domains is untested.

## Confidence

- High confidence: The basic VTCD pipeline (SLIC → CNMF → CRIS) works as described and produces interpretable concepts that correlate with model performance.
- Medium confidence: The discovery of universal mechanisms across different video transformer models is supported but may not generalize to architectures outside the tested set.
- Low confidence: The effectiveness of VTCD for downstream applications (like video object segmentation) without additional fine-tuning on the target dataset.

## Next Checks

1. Test VTCD on a video transformer architecture not included in the original study (e.g., TimeSformer or MViT) to verify universal mechanism discovery claims.
2. Compare VTCD-discovered concepts with human-annotated semantic segments on the DAVIS'16 dataset to quantify alignment between machine-discovered and human-interpretable concepts.
3. Evaluate computational efficiency improvements when applying VTCD-based pruning to a larger set of video transformer models beyond the initial action classification task.