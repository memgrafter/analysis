---
ver: rpa2
title: A comprehensive study of on-device NLP applications -- VQA, automated Form
  filling, Smart Replies for Linguistic Codeswitching
arxiv_id: '2409.19010'
source_url: https://arxiv.org/abs/2409.19010
tags:
- screen
- which
- question
- data
- smart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work explores three novel on-device NLP applications: visual
  question answering (VQA) on screen content, automated form filling using previous
  screen context, and smart replies for multilingual speakers with code-switching.
  The proposed solutions leverage recent advancements in document AI, particularly
  LayoutLMv3, to interpret screen text and layout.'
---

# A comprehensive study of on-device NLP applications -- VQA, automated Form filling, Smart Replies for Linguistic Codeswitching

## Quick Facts
- arXiv ID: 2409.19010
- Source URL: https://arxiv.org/abs/2409.19010
- Reference count: 3
- One-line primary result: Three novel on-device NLP applications (VQA, form filling, code-switch smart replies) using LayoutLMv3 and multilingual BERT with auxiliary translation task

## Executive Summary
This work proposes three novel on-device NLP applications leveraging recent document AI advancements, particularly LayoutLMv3. The applications include visual question answering on screen content, automated form filling using previous screen context, and smart replies for multilingual speakers with code-switching. The proposed solutions address key challenges in on-device NLP, such as high latency and small-scale experiments, while demonstrating promising results across all three tasks.

## Method Summary
The study employs LayoutLMv3 for VQA and form filling tasks, using a two-stage approach with rule-based label generation and fine-tuning on DocVQA and internal app datasets. For code-switching smart replies, a multilingual BERT-based bi-encoder with auxiliary translation tasks is introduced. The models are trained on synthetic code-switched data generated from English-Hindi pairs and evaluated using standard metrics like F1, recall, and MRR.

## Key Results
- VQA task achieves 74.5 F1 for question extraction and 63.87 F1 for answer extraction
- Automated form filling uses LayoutLMv3 to extract relevant questions and answers between current and previous screens
- Code-switching smart replies achieve 0.526 MRR on code-switch data using multilingual BERT with auxiliary translation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual Question Answering (VQA) on screen content is enabled by LayoutLMv3's ability to jointly model text and image patches.
- Mechanism: LayoutLMv3 projects word tokens and image patches into a shared latent space, applies multimodal transformer layers, and uses masked language/image modeling objectives to learn alignment between text and layout. This allows the model to interpret structured screen content (e.g., forms, tables, titles) when answering natural language queries.
- Core assumption: The visual structure of the screen (e.g., spatial proximity of labels and values) contains sufficient information for answering questions without full image reasoning.
- Evidence anchors:
  - [abstract] "proposed solutions leverage recent advancements in document AI, particularly LayoutLMv3, to interpret screen text and layout."
  - [section 2.2] "The main idea for LayoutLM family is to do multiple pretraining tasks which closely align the image and text level tokens."
  - [corpus] Weak: neighbor papers mention LayoutLM-like models for document UI understanding but not this exact VQA pipeline.
- Break condition: If LayoutLMv3 fails to learn text-image alignment (e.g., due to poor OCR or noisy bounding boxes), VQA accuracy will drop sharply.

### Mechanism 2
- Claim: Automated form filling is enabled by cross-screen contextual linking using LayoutLMv3's representations.
- Mechanism: Screen representations from previous and current views are concatenated and fed into a two-head architecture: a question extraction head identifies which form fields can be auto-filled, and an answer extraction head predicts the corresponding values from the previous screen. This reduces the need to recompute representations per field.
- Core assumption: The same LayoutLMv3 instance can meaningfully represent both "form screen" and "info screen" such that a cross-attention head can align questions with answers.
- Evidence anchors:
  - [section 4.2] "We use the same LayoutLMv3 model instance to extract representations for both form screen and info screen... passed to respective question or answer extraction head."
  - [section 3.7] "To make it work on device in real time, we have to shrink its runtime to less than 50M parameters."
  - [corpus] Weak: form-filling benchmarks exist but not this specific LayoutLMv3-based cross-screen approach.
- Break condition: If the concatenated representations do not preserve cross-screen semantic links, question extraction will fail and auto-fill suggestions will be irrelevant.

### Mechanism 3
- Claim: Code-switching smart replies are enabled by multilingual BERT with auxiliary translation task to bridge language gaps.
- Mechanism: A multilingual BERT encoder is trained on code-switched message-reply pairs with an auxiliary translation objective that maps replies back to the original message language, improving cross-lingual semantic matching in the shared embedding space.
- Core assumption: Adding a translation head to the bi-encoder improves retrieval of code-switched responses by forcing the encoder to internalize cross-lingual mappings.
- Evidence anchors:
  - [section 6.2] "apart from the normal cosine similarity... we add additional auxiliary tasks by adding a translation head and task using the learned embedding to make the model work better."
  - [section 6.3] "Using this value, we can compute the Mean Reciprocal Rank... multilingual BERT trained on Code-Switch m-r data performed the best in retrieving the correct response."
  - [corpus] Weak: neighbor papers do not cite this exact multilingual BERT + translation auxiliary setup for smart replies.
- Break condition: If the auxiliary translation task introduces conflicting gradients or overfits to synthetic code-switch data, MRR will plateau or degrade.

## Foundational Learning

- Concept: Document AI and LayoutLM family architectures
  - Why needed here: Understanding how LayoutLMv3 fuses text and image patches is essential to reason about why it works for VQA and form-filling.
  - Quick check question: What are the three pretraining objectives of LayoutLMv3 and why do they help document understanding?

- Concept: Multimodal transformer pretraining (masked language modeling, masked image modeling, word-patch alignment)
  - Why needed here: These objectives directly determine the quality of cross-modal representations used for all three tasks.
  - Quick check question: How does word-patch alignment differ from masked image modeling in LayoutLMv3?

- Concept: Smart reply ranking with bi-encoders and MRR evaluation
  - Why needed here: The code-switch smart reply experiment uses a bi-encoder with MRR metric; understanding this helps debug ranking failures.
  - Quick check question: Why is MRR a suitable metric for evaluating ranked response lists in smart reply systems?

## Architecture Onboarding

- Component map:
  - LayoutLMv3 backbone (text + image patch encoder) -> Question extraction head (3-layer FF + cross-attention) -> Answer extraction head (3-layer FF + cross-attention) -> Multilingual BERT encoder (code-switch smart replies) -> Auxiliary translation head (code-switch task) -> Data pipelines: rule-based label gen, DocVQA finetune, code-switch synthetic corpus

- Critical path:
  1. Load LayoutLMv3 checkpoint -> preprocess screen image + OCR -> extract features
  2. Feed features to downstream heads (Q&A or form-filling)
  3. For code-switch: encode m-r pairs -> compute similarity + translation loss -> update encoder
  4. Output ranked responses or extracted answers

- Design tradeoffs:
  - Using LayoutLMv3 vs. smaller models: higher accuracy but 450 ms latency vs. sub-50M params target
  - Rule-based labeling vs. manual annotation: fast but noisy bounding boxes degrade fine-tuning
  - Synthetic code-switch data vs. real code-switched corpora: scalable but may lack naturalness

- Failure signatures:
  - VQA: low recall on non-title fields -> likely bounding box misalignment
  - Form filling: high question extraction F1 but low answer extraction F1 -> head misalignment or feature collapse
  - Code-switch: marginal MRR gain over English-only -> translation head may be ineffective or data distribution mismatch

- First 3 experiments:
  1. Validate LayoutLMv3 QA head on DocVQA -> measure baseline F1 before fine-tuning on internal data.
  2. Run ablation: remove auxiliary translation head in code-switch model -> compare MRR drop.
  3. Stress test latency: run LayoutLMv3 on-device with model quantization -> confirm <50M param target.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design a scalable and diverse dataset for on-device screen understanding tasks (VQA, form filling) that covers a wide range of real-world applications?
- Basis in paper: [explicit] The paper highlights the lack of datasets and evaluation benchmarks for on-device screen understanding tasks, and mentions that constructing such datasets requires large-scale collections from user habits and special annotations.
- Why unresolved: Collecting large-scale user data for screen understanding is challenging due to privacy concerns and the diversity of app interfaces. Additionally, creating annotated datasets requires significant manual effort and domain expertise.
- What evidence would resolve it: A comprehensive study evaluating different data collection and annotation strategies, including crowdsourcing, synthetic data generation, and privacy-preserving techniques, would provide insights into creating scalable and diverse datasets for on-device screen understanding tasks.

### Open Question 2
- Question: How can we reduce the latency of LayoutLMv3 and other document AI models to enable real-time on-device processing for screen understanding tasks?
- Basis in paper: [explicit] The paper mentions that the latency of LayoutLMv3 is around 450ms, which is too high for real-time on-device applications. It also suggests that the model needs to be shrunk to less than 50M parameters for on-device deployment.
- Why unresolved: Reducing the latency of large language models while maintaining their performance is an active area of research. It involves techniques like model compression, quantization, and efficient architectures, which are not straightforward to implement and may require trade-offs in accuracy.
- What evidence would resolve it: Comparative studies evaluating different model compression techniques, quantization methods, and efficient architectures for on-device document AI models would provide insights into reducing latency while maintaining performance.

### Open Question 3
- Question: How can we improve the accuracy of VQA models for on-screen content by incorporating visual understanding of images and infographics?
- Basis in paper: [explicit] The paper mentions that the proposed VQA model cannot understand and reason about images, as understanding images was not part of its training objective. It suggests that supporting infographics and charts understanding is a future direction.
- Why unresolved: Incorporating visual understanding of images into VQA models requires additional training data and techniques for image encoding and reasoning. It also involves challenges in aligning text and image representations for effective cross-modal reasoning.
- What evidence would resolve it: Experiments comparing the performance of VQA models with and without image understanding capabilities, using datasets that include images and infographics, would provide insights into the benefits and challenges of incorporating visual understanding in on-screen VQA.

## Limitations

- Limited sample sizes (150 form-filling samples, internal VQA dataset of unknown size beyond "100,000+ app screenshots")
- High latency for LayoutLMv3 on-device deployment (450 ms)
- Reliance on rule-based label generation which may introduce noisy bounding boxes

## Confidence

- **High Confidence**: The core mechanisms of using LayoutLMv3 for document understanding and the two-stage VQA approach are well-grounded in existing document AI literature. The F1 scores for question extraction (74.5) and answer extraction (63.87) are internally consistent with the reported methodology.

- **Medium Confidence**: The cross-screen form filling approach using concatenated LayoutLMv3 representations is novel but relies on assumptions about semantic alignment that are not extensively validated. The translation auxiliary task for code-switching smart replies shows measurable improvement (MRR 0.526) but the ablation study is not reported.

- **Low Confidence**: The scalability of the proposed solutions to diverse screen layouts and natural code-switching patterns is uncertain due to limited sample sizes and synthetic data generation methods.

## Next Checks

1. **Latency Optimization Validation**: Profile the LayoutLMv3 model on target on-device hardware to confirm whether quantization or pruning can achieve the <50M parameter target while maintaining acceptable accuracy.

2. **Cross-Screen Alignment Test**: Run an ablation study removing the cross-attention mechanism in the form filling pipeline to quantify how much performance depends on the concatenated representation approach versus simple nearest-neighbor matching.

3. **Code-Switching Naturalness Check**: Compare the MRR performance of the multilingual BERT model on synthetic code-switched data versus a small set of human-annotated code-switched message-reply pairs to assess whether the translation auxiliary task generalizes to natural patterns.