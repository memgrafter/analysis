---
ver: rpa2
title: Partially Stochastic Infinitely Deep Bayesian Neural Networks
arxiv_id: '2402.03495'
source_url: https://arxiv.org/abs/2402.03495
tags:
- neural
- networks
- stochastic
- bayesian
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Partially Stochastic Infinitely Deep Bayesian\
  \ Neural Networks (PSDE-BNNs), a novel architecture that selectively applies stochasticity\
  \ within the weights of infinitely deep Bayesian neural networks. By partitioning\
  \ weights into stochastic and deterministic subsets, PSDE-BNNs retain the benefits\
  \ of full stochasticity\u2014such as robustness, uncertainty quantification, and\
  \ memory efficiency\u2014while improving computational efficiency at training and\
  \ inference time."
---

# Partially Stochastic Infinitely Deep Bayesian Neural Networks

## Quick Facts
- arXiv ID: 2402.03495
- Source URL: https://arxiv.org/abs/2402.03495
- Reference count: 40
- Primary result: Partially stochastic infinitely deep BNNs improve computational efficiency while maintaining uncertainty quantification

## Executive Summary
This paper introduces Partially Stochastic Infinitely Deep Bayesian Neural Networks (PSDE-BNNs), a novel architecture that selectively applies stochasticity within infinitely deep Bayesian neural networks. By partitioning weights into stochastic and deterministic subsets, PSDE-BNNs retain the benefits of full stochasticity—such as robustness, uncertainty quantification, and memory efficiency—while improving computational efficiency at training and inference time. The authors provide a mathematical framework for incorporating partial stochasticity, prove that their models qualify as Universal Conditional Distribution Approximators (UCDAs) under specific conditions, and demonstrate empirically that PSDE-BNNs outperform fully stochastic networks in downstream tasks and uncertainty quantification while being significantly more efficient.

## Method Summary
The paper proposes PSDE-BNNs that combine infinitely deep Bayesian neural networks with partial stochasticity by separating weights into stochastic and deterministic components. Three architectural configurations are introduced: ODEFirst (stochastic weights in later layers), SDEFirst (stochastic weights in earlier layers), and Horizontal Cut (horizontal separation of weights in each layer). The model uses Ornstein-Uhlenbeck processes as priors and is trained via variational inference with the evidence lower bound (ELBO). The key innovation is maintaining universal conditional distribution approximation while reducing computational burden by limiting stochasticity to only a subset of weights.

## Key Results
- PSDE-BNNs achieve better accuracy and calibration than fully stochastic infinitely deep BNNs while requiring less computation
- The method maintains universal conditional distribution approximation properties while improving training and inference efficiency
- Ablation studies show robust performance across different stochasticity ratios and hyperparameter configurations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Partial stochasticity preserves universal conditional distribution approximation while improving computational efficiency
- **Mechanism:** Selective stochasticity reduces KL divergence variance during training while maintaining approximation capabilities through noise outsourcing
- **Core assumption:** Stochastic weights are sufficient to capture posterior uncertainty while deterministic weights handle efficient feature extraction
- **Evidence anchors:** Theoretical proof of UCDA properties in Theorem 4.2; computational efficiency claims in abstract
- **Break condition:** If stochastic subset is too small to capture essential posterior uncertainty

### Mechanism 2
- **Claim:** Vertical and horizontal weight separation strategies offer flexible control over stochasticity application
- **Mechanism:** Different separation strategies allow tailoring stochasticity to specific computational and accuracy requirements
- **Core assumption:** Separation strategy choice doesn't fundamentally alter expressivity
- **Evidence anchors:** Architectural descriptions in Section 4; comparison with Sharma et al. (2023)
- **Break condition:** Mismatch between stochastic and deterministic components degrading performance

### Mechanism 3
- **Claim:** Ornstein-Uhlenbeck prior stabilizes stochastic weight dynamics
- **Mechanism:** Mean-reverting OU process constrains weights within reasonable ranges, preventing divergence
- **Core assumption:** OU process is suitable prior for weight dynamics in BNNs
- **Evidence anchors:** Prior choice justification in Section 5
- **Break condition:** OU process too restrictive, limiting complex posterior capture

## Foundational Learning

- **Concept:** Universal Conditional Distribution Approximation (UCDA)
  - **Why needed here:** PSDE-BNNs claim UCDA properties, justifying their expressivity
  - **Quick check question:** What conditions must a neural network satisfy to be a UCDA?

- **Concept:** Stochastic Differential Equations (SDEs)
  - **Why needed here:** SDEs model weight dynamics in PSDE-BNNs
  - **Quick check question:** How do SDE solutions differ from ODE solutions in terms of properties?

- **Concept:** Variational Inference and ELBO
  - **Why needed here:** PSDE-BNNs use variational inference to approximate posterior distributions
  - **Quick check question:** What is the relationship between ELBO and KL divergence?

## Architecture Onboarding

- **Component map:** Input preprocessing (a(x)) -> Hidden state dynamics (fh) -> Weight dynamics (fq, gp) -> Prior process (OU) -> Variational inference (ELBO optimization)
- **Critical path:** Initialize weights (w0) -> Evolve weights via SDE/ODE -> Compute hidden state dynamics -> Evaluate likelihood/KL -> Update parameters via gradient descent
- **Design tradeoffs:** Stochasticity ratio (accuracy vs efficiency); time intervals (expressivity vs efficiency); prior choice (stability vs generalization)
- **Failure signatures:** Numerical instability; poor uncertainty quantification; slow convergence; degraded downstream performance
- **First 3 experiments:** 1) Implement PSDE-BNN with vertical separation on MNIST vs fully stochastic BNN; 2) Vary stochasticity ratio and measure efficiency/uncertainty trade-offs; 3) Test different time intervals for stochasticity and analyze performance effects

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal ratio of stochastic to deterministic weights for different task types?
- **Basis in paper:** [explicit] Ablation studies on CIFAR-10 show no correlation between ratio and performance, but only tested one dataset and task type
- **Why unresolved:** Limited to classification on CIFAR-10; optimal ratio may vary across domains and problem complexities
- **What evidence would resolve it:** Systematic experiments across diverse datasets (image, text, tabular, time series), tasks (classification, regression, generation), and problem complexities showing optimal ratios

### Open Question 2
- **Question:** How does choice between vertical and horizontal partitioning affect representational capacity and efficiency?
- **Basis in paper:** [explicit] Introduces both methods but only provides theoretical analysis for vertical cuts and limited empirical comparison
- **Why unresolved:** Lacks direct empirical comparison of both methods on identical tasks
- **What evidence would resolve it:** Head-to-head comparison measuring accuracy, uncertainty quantification, training time, inference time, and memory usage

### Open Question 3
- **Question:** What are the theoretical limits of PSDE-BNNs' approximation capabilities vs fully stochastic infinitely deep BNNs?
- **Basis in paper:** [explicit] Proves PSDE-BNNs are UCDAs but doesn't establish bounds or compare efficiency
- **Why unresolved:** Shows approximation capability but doesn't quantify required stochastic weights or compare efficiency
- **What evidence would resolve it:** Formal bounds on approximation error vs stochasticity ratio, comparison with fully stochastic networks, empirical validation of trade-offs

## Limitations
- Theoretical guarantees rely on assumptions about stochastic weight configurations that may not hold in practice
- Relationship between stochasticity ratio and performance needs more thorough exploration across diverse tasks
- Focus primarily on image classification limits generalizability to other domains

## Confidence
- **High confidence**: Computational efficiency improvements vs fully stochastic architectures
- **Medium confidence**: Theoretical UCDA claims pending further validation
- **Medium confidence**: Uncertainty quantification improvements primarily validated on calibration metrics

## Next Checks
1. Test PSDE-BNNs on regression tasks with heteroscedastic noise to evaluate uncertainty quantification beyond classification
2. Conduct ablation studies varying stochasticity ratio across multiple network depths to understand scaling properties
3. Compare against alternative efficient BNN approaches (MC dropout, ensemble methods) on identical benchmarks