---
ver: rpa2
title: Zero-Shot Clinical Trial Patient Matching with LLMs
arxiv_id: '2402.05125'
source_url: https://arxiv.org/abs/2402.05125
tags:
- patient
- clinical
- criteria
- trial
- matching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of automatically matching patients
  to clinical trials using large language models (LLMs). The authors propose a zero-shot
  approach that leverages LLMs to evaluate patient eligibility based on unstructured
  clinical text and free-text inclusion criteria.
---

# Zero-Shot Clinical Trial Patient Matching with LLMs

## Quick Facts
- arXiv ID: 2402.05125
- Source URL: https://arxiv.org/abs/2402.05125
- Reference count: 40
- Primary result: GPT-4-based zero-shot system achieves state-of-the-art performance with Macro-F1 0.81 and Micro-F1 0.93 on clinical trial matching task

## Executive Summary
This paper presents a zero-shot approach for matching patients to clinical trials using large language models (LLMs). The authors develop a system that evaluates patient eligibility by processing unstructured clinical text against free-text inclusion criteria, achieving state-of-the-art performance on the n2c2 2018 cohort selection benchmark. They investigate different prompting strategies and implement a two-stage retrieval pipeline to improve efficiency, demonstrating that GPT-4 can effectively understand clinical context without requiring structured data transformation.

## Method Summary
The system uses GPT-4 to evaluate patient eligibility for clinical trials by processing unstructured clinical notes against free-text eligibility criteria. Four prompting strategies (ACAN, ACIN, ICAN, ICIN) are tested, where the system processes either all criteria at once or individually. A two-stage retrieval pipeline embeds clinical notes and retrieves the top-k most relevant chunks before sending them to GPT-4 for assessment. The approach is evaluated on the n2c2 2018 cohort selection dataset using Macro-F1 and Micro-F1 scores.

## Key Results
- GPT-4-based system achieves state-of-the-art Macro-F1 score of 0.81 and Micro-F1 score of 0.93
- Two-stage retrieval pipeline reduces token usage by up to one-third while maintaining high performance
- System can output coherent justifications for 97% of correct decisions and 75% of incorrect ones
- ACIN prompting strategy balances performance and cost better than other approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 outperforms prior rule-based systems by understanding unstructured clinical text
- Mechanism: The model processes free-text eligibility criteria and patient notes directly without requiring structured data transformation
- Core assumption: GPT-4's language understanding is sufficient to capture clinical context in unstructured text
- Evidence anchors: [abstract] "Our zero-shot GPT-4-based system achieves state-of-the art scores... with an overall Macro-F1 score of 0.81 and Micro-F1 score of 0.93"; [section] "Traditional natural language processing (NLP) approaches have seen limited success with clinical text due to its idiosyncratic grammar and terminology"

### Mechanism 2
- Claim: Two-stage retrieval pipeline reduces token usage while maintaining performance
- Mechanism: Embedding model pre-filters relevant clinical notes, feeding only top-k chunks to the expensive assessment model
- Core assumption: Relevant information is concentrated in a small subset of clinical notes
- Evidence anchors: [abstract] "develop a two-stage retrieval pipeline that reduces the number of tokens processed by up to a third while retaining high performance"; [section] "This retrieval-based approach would allow our system to scale to real-world health systems with millions of notes"

### Mechanism 3
- Claim: Specific prompt engineering significantly improves model performance
- Mechanism: Providing detailed, tailored definitions for each criterion helps the model make more accurate assessments
- Core assumption: LLMs can leverage more specific instructions to improve task performance
- Evidence anchors: [section] "we find in Table 2 that increasing the specificity of criteria is an essential first step in using an LLM"; [abstract] "We investigate different prompting strategies"

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: Allows system to work on arbitrary trials without retraining
  - Quick check question: Can the model make accurate predictions without any examples of the specific task?

- Concept: Token efficiency
  - Why needed here: Critical for scaling to large health systems with millions of notes
  - Quick check question: How does token usage change when using retrieval vs. full notes?

- Concept: Clinical trial eligibility criteria
  - Why needed here: Understanding what makes a patient eligible is the core task
  - Quick check question: What's the difference between inclusion and exclusion criteria in trial design?

## Architecture Onboarding

- Component map: Embedding model (BGE/MiniLM) → Vector database → Top-k retrieval → Assessment model (GPT-4) → JSON output
- Critical path: Patient notes → Embedding → Top-k retrieval → Prompt generation → LLM assessment → Output parsing
- Design tradeoffs: Retrieval reduces tokens but may miss information → Tradeoff between efficiency and completeness; ACIN strategy balances performance and cost better than other prompting strategies; Open-source models (Llama-2, Mixtral) cheaper but less accurate than GPT-4
- Failure signatures: Performance drops when k is too small (missing relevant information); Increased costs when using individual criteria prompting (13x more LLM calls); Reduced accuracy when criteria definitions are too vague
- First 3 experiments: 1) Compare Macro-F1 scores across different k values (1, 3, 5, 10) to find optimal retrieval cutoff; 2) Test different prompting strategies (ACAN vs ACIN vs ICAN vs ICIN) to identify cost-performance tradeoff; 3) Evaluate open-source models (Llama-2, Mixtral) against GPT-4 to assess cost-quality tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of the LLM-based system change when applied to real-world clinical trial data that includes exclusion criteria and a broader range of patient demographics?
- Basis in paper: [explicit] The paper acknowledges that the n2c2 dataset used for evaluation only includes inclusion criteria and is a simplified representation of actual trial matching. It also mentions that the patient cohort is limited to diabetics from specific hospitals.
- Why unresolved: The current evaluation is based on a synthetic dataset that doesn't fully capture the complexity of real-world clinical trials, which often have more stringent exclusion criteria and diverse patient populations.
- What evidence would resolve it: Testing the system on real-world clinical trial data that includes exclusion criteria and a more diverse patient population would provide evidence of its generalizability and performance in practical settings.

### Open Question 2
- Question: What is the optimal balance between prompt specificity and generalizability for different types of clinical trial eligibility criteria?
- Basis in paper: [explicit] The paper demonstrates that increasing the specificity of criteria definitions significantly improves LLM performance, but also raises concerns about generalization to different trials.
- Why unresolved: While more specific prompts improve accuracy, they may reduce the system's ability to adapt to new, unseen criteria without additional engineering.
- What evidence would resolve it: A systematic study comparing the performance of the system across various trials with different levels of prompt specificity would help determine the optimal balance.

### Open Question 3
- Question: How can the retrieval-based approach be further optimized to minimize the performance gap between retrieval-based and non-retrieval-based methods?
- Basis in paper: [inferred] The paper notes that while the retrieval-based approach reduces token usage, it doesn't fully match the performance of the non-retrieval-based method on Micro-F1.
- Why unresolved: The retrieval pipeline introduces a performance trade-off that needs to be addressed to fully leverage its efficiency benefits.
- What evidence would resolve it: Experimenting with different embedding models, chunk sizes, and retrieval strategies could help close the performance gap and optimize the retrieval-based approach.

## Limitations

- System relies heavily on GPT-4, making it expensive and potentially inaccessible for widespread clinical deployment
- Evaluation based on single dataset (n2c2 2018) with 13 criteria and 288 patients may not generalize to full complexity of real-world clinical trial matching
- Prompt templates are not fully specified, making exact reproduction challenging
- System only evaluated on inclusion criteria, not the more complex exclusion criteria typically found in real clinical trials

## Confidence

**High confidence**: The overall performance metrics (Macro-F1 0.81, Micro-F1 0.93) are well-supported by the n2c2 benchmark results. The two-stage retrieval pipeline's efficiency gains are clearly demonstrated with specific token reduction figures.

**Medium confidence**: The generalizability to other clinical trial domains and patient populations. While the system performs well on the n2c2 dataset, real-world clinical notes often contain more complex narratives, abbreviations, and non-standard terminology.

**Medium confidence**: The cost estimates are reasonable but depend on API pricing that may change. The comparison with open-source models (Llama-2, Mixtral) shows they underperform GPT-4, but the absolute performance gap may vary with different implementations.

## Next Checks

1. **Cross-domain validation**: Test the system on a different clinical trial dataset with varying complexity and domain specificity to assess generalizability beyond the n2c2 2018 cohort selection task.

2. **Prompt template specification**: Request and validate the exact prompt templates used for each of the four strategies (ACAN, ACIN, ICAN, ICIN) to enable exact reproduction and assess sensitivity to prompt variations.

3. **Longitudinal performance evaluation**: Evaluate system performance across multiple iterations of the same clinical trial matching task to assess consistency and identify any performance degradation over time with different patient cohorts.