---
ver: rpa2
title: Learning Dynamics of LLM Finetuning
arxiv_id: '2407.10490'
source_url: https://arxiv.org/abs/2407.10490
tags:
- learning
- epochs
- different
- number
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified framework to analyze the learning
  dynamics of large language models (LLMs) during finetuning, decomposing parameter
  updates into interpretable terms that capture how training examples influence predictions.
  By analyzing both supervised and preference-based finetuning, the framework explains
  phenomena like response repetition and hallucination amplification, and reveals
  a "squeezing effect" in off-policy direct preference optimization (DPO) that concentrates
  probability mass on peaky predictions and degrades alignment.
---

# Learning Dynamics of LLM Finetuning

## Quick Facts
- arXiv ID: 2407.10490
- Source URL: https://arxiv.org/abs/2407.10490
- Authors: Yi Ren; Danica J. Sutherland
- Reference count: 40
- This paper proposes a unified framework to analyze the learning dynamics of large language models (LLMs) during finetuning, decomposing parameter updates into interpretable terms that capture how training examples influence predictions.

## Executive Summary
This paper introduces a unified framework to analyze the learning dynamics of large language models during finetuning, decomposing parameter updates into interpretable terms that explain how training examples influence predictions. By analyzing both supervised and preference-based finetuning, the framework explains phenomena like response repetition and hallucination amplification, and reveals a "squeezing effect" in off-policy direct preference optimization (DPO) that concentrates probability mass on peaky predictions and degrades alignment. The analysis explains why on-policy methods outperform off-policy ones and inspires a simple dataset extension method—training with both preferred and rejected responses during supervised finetuning—that mitigates the squeezing effect and improves alignment performance, achieving up to 69% win rate against baseline DPO in human evaluation.

## Method Summary
The framework analyzes LLM finetuning by decomposing parameter updates into three interpretable terms: a function of current predictions, a similarity kernel between training and observation examples, and a gradient-based residual term. For SFT, the method trains on [x, y+] pairs, while DPO uses [x, y+, y-] triples. The dataset extension method adds [x, y-] pairs to SFT training data. The framework tracks log-likelihood changes during training using probing datasets with various response types and evaluates alignment quality through win-rate comparisons using GPT-3.5-Turbo and Claude-3-Haiku.

## Key Results
- The framework decomposes parameter updates into three interpretable terms that explain how training examples influence predictions
- The "squeezing effect" explains why off-policy DPO degrades alignment over time by concentrating probability mass on peaky predictions
- Dataset extension during SFT (training on both preferred and rejected responses) mitigates the squeezing effect and improves alignment performance, achieving up to 69% win rate against baseline DPO

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework decomposes parameter updates into three interpretable terms that explain how training examples influence predictions.
- Mechanism: The decomposition expresses the change in model predictions as a function of the current prediction state, a similarity kernel between training and observation examples, and a gradient-based residual term that captures the energy and direction of adaptation.
- Core assumption: The relative influence of learning one example on other examples remains stable throughout training (i.e., the neural tangent kernel's relative structure is preserved).
- Evidence anchors:
  - [abstract] "by analyzing the step-wise decomposition of how influence accumulates among different potential responses"
  - [section 2] "The analysis in this paper relies on the following assumption: During the training, the relative influence of learning xu on all other different xo is relatively stable."
  - [corpus] No direct corpus evidence for this specific claim; framework relies on empirical verification in experiments.
- Break condition: If the neural tangent kernel changes drastically during training (e.g., due to large learning rates or architecture changes), the decomposition loses interpretability.

### Mechanism 2
- Claim: The "squeezing effect" explains why off-policy DPO degrades alignment over time.
- Mechanism: Negative gradients in cross-entropy loss with softmax output cause probability mass to concentrate on the most likely tokens, degrading diversity and alignment quality.
- Core assumption: When negative gradients are applied to already-unlikely predictions, the redistribution of probability mass disproportionately benefits the most confident tokens.
- Evidence anchors:
  - [abstract] "highlights a unique 'squeezing effect' to explain a previously observed phenomenon in off-policy direct preference optimization (DPO), where running DPO for too long makes even the desired outputs less likely"
  - [section 3.3] Detailed mathematical analysis showing how negative gradients lead to peakier distributions
  - [corpus] No corpus evidence specifically for the squeezing effect; theoretical analysis is original to this paper.
- Break condition: If the model's initial distribution is uniform or if regularization prevents extreme peakiness, the squeezing effect may be mitigated.

### Mechanism 3
- Claim: Dataset extension during SFT (training on both preferred and rejected responses) mitigates the squeezing effect.
- Mechanism: By exposing the model to rejected responses during SFT, the region containing these responses is "pulled up," making the negative gradient in DPO less extreme and reducing probability mass concentration.
- Core assumption: Both preferred and rejected responses contain valid information about the task, so exposing both during SFT improves alignment without overfitting to rejected responses.
- Evidence anchors:
  - [abstract] "inspires a simple dataset extension method—training with both preferred and rejected responses during supervised finetuning—that mitigates the squeezing effect"
  - [section 4.3] Experimental results showing improved win rates against baseline DPO
  - [corpus] No corpus evidence for this specific dataset extension method; method is proposed and validated in this paper.
- Break condition: If rejected responses contain harmful or contradictory information, including them in SFT could degrade rather than improve alignment.

## Foundational Learning

- Concept: Learning dynamics and influence decomposition
  - Why needed here: Provides the mathematical framework for understanding how individual training examples affect model predictions across different training scenarios
  - Quick check question: Can you explain in your own words how the three-term decomposition relates changes in predictions to changes in parameters?

- Concept: Neural tangent kernel (NTK) and its stability
  - Why needed here: The NTK determines the similarity between training and observation examples, which scales the influence of parameter updates on predictions
  - Quick check question: Why is the assumption of NTK stability important for interpreting the decomposition terms?

- Concept: Softmax output behavior and probability concentration
  - Why needed here: Understanding how cross-entropy loss with softmax leads to probability mass redistribution is crucial for explaining the squeezing effect
  - Quick check question: What happens to the model's output distribution when a large negative gradient is applied to an already-unlikely token?

## Architecture Onboarding

- Component map: NTK computation -> Gradient calculation -> Prediction state update
- Critical path: First compute the NTK between training and observation examples, then calculate the residual gradient from the loss function, finally combine these with the current prediction state to get the influence on predictions.
- Design tradeoffs: The framework trades computational complexity (requires NTK computation) for interpretability and insight into training dynamics. Alternative approaches might sacrifice some interpretability for efficiency.
- Failure signatures: If the NTK assumption breaks down, the decomposition terms may become uninterpretable. If the model's distribution becomes too peaky, the framework may not capture the full dynamics.
- First 3 experiments:
  1. Verify NTK stability by tracking the relative influence of training examples on different observation examples across training epochs
  2. Demonstrate the squeezing effect by applying large negative gradients to unlikely predictions and observing probability mass redistribution
  3. Test the dataset extension method by comparing alignment performance with and without including rejected responses during SFT

## Open Questions the Paper Calls Out
None

## Limitations

- NTK Stability Assumption: The framework's interpretability relies heavily on the assumption that the neural tangent kernel's relative structure remains stable during training, which may break down with large learning rates or architectural changes.
- Theoretical vs. Empirical Evidence: While the mathematical framework for the "squeezing effect" is sound, the paper lacks direct empirical validation of the underlying mechanism linking it to performance degradation.
- Dataset Extension Generalizability: The effectiveness of including rejected responses in SFT may be task-dependent and assumes rejected responses contain useful information, which may not hold for all alignment tasks.

## Confidence

- **High Confidence**: The mathematical framework for decomposing parameter updates is well-founded and provides clear insights into how training examples influence predictions. The analysis of cross-entropy loss behavior with softmax outputs is rigorous and well-established.
- **Medium Confidence**: The explanation of the "squeezing effect" in off-policy DPO is plausible and mathematically coherent, but lacks direct causal evidence linking the mechanism to observed performance degradation. The dataset extension method shows empirical success but may not generalize across all alignment scenarios.
- **Low Confidence**: The assumption of NTK stability throughout training is a critical but unverified claim that could undermine the framework's interpretability if violated. The paper doesn't provide extensive empirical validation of this assumption.

## Next Checks

1. **NTK Stability Tracking**: Conduct systematic experiments tracking the relative influence of training examples on different observation examples across multiple training epochs and learning rates to empirically validate the NTK stability assumption.

2. **Ablation Study of Squeezing Effect**: Design controlled experiments that isolate the probability concentration mechanism by varying the degree of negative gradient application and measuring direct impacts on prediction diversity and alignment quality.

3. **Cross-Domain Dataset Extension Testing**: Test the rejected-response inclusion method across diverse alignment tasks (safety, helpfulness, style consistency) to determine its generalizability and identify scenarios where it may be counterproductive.