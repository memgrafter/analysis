---
ver: rpa2
title: Multilingual Pretraining and Instruction Tuning Improve Cross-Lingual Knowledge
  Alignment, But Only Shallowly
arxiv_id: '2404.04659'
source_url: https://arxiv.org/abs/2404.04659
tags:
- knowledge
- language
- languages
- multilingual
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study evaluates cross-lingual knowledge alignment in large
  language models (LLMs) across three levels: Performance, Consistency, and Conductivity.
  Using a new framework, CLiKA, the authors tested popular multilingual LLMs like
  BLOOM, LLaMA, and ChatGPT on ten languages.'
---

# Multilingual Pretraining and Instruction Tuning Improve Cross-Lingual Knowledge Alignment, But Only Shallowly

## Quick Facts
- arXiv ID: 2404.04659
- Source URL: https://arxiv.org/abs/2404.04659
- Reference count: 20
- Key outcome: Multilingual pretraining and instruction tuning improve cross-lingual knowledge alignment at the performance and consistency levels, but not at the conductivity level.

## Executive Summary
This study evaluates cross-lingual knowledge alignment in large language models (LLMs) across three levels: Performance, Consistency, and Conductivity. Using a new framework, CLiKA, the authors tested popular multilingual LLMs like BLOOM, LLaMA, and ChatGPT on ten languages. Results show that while multilingual pretraining and instruction tuning improve basic language abilities and performance consistency, they fail to enhance knowledge conductivityâ€”the ability to retrieve knowledge learned in one language using another. This indicates that current multilingual training methods lead to shallow alignment, relying on overlapping data rather than true knowledge transfer.

## Method Summary
The study evaluates cross-lingual knowledge alignment in LLMs using the CLiKA framework across three levels: Performance (RA), Consistency (en-CO), and Conductivity (XRR). Ten languages are tested using datasets covering Basic, Factual, and Fictional knowledge. Representative multilingual LLMs are evaluated, including BLOOM, LLaMA, ChatGPT, and their variants. The study compares the effects of multilingual pretraining and instruction tuning on these alignment levels.

## Key Results
- Multilingual pretraining and instruction tuning improve basic language abilities and performance consistency across languages
- Knowledge conductivity remains low across all tested models and languages
- Mixed pretraining outperforms continued pretraining for cross-lingual alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual pretraining and instruction tuning improve cross-lingual knowledge alignment at the performance and consistency levels, but not at the conductivity level.
- Mechanism: Multilingual pretraining and instruction tuning improve basic language abilities and knowledge consistency across languages, but do not enable effective knowledge transfer from one language to another. This is evidenced by high consistency scores (en-CO) but low conductivity scores (XRR) across tested languages.
- Core assumption: The high consistency between non-English and English answers is due to overlapping training data rather than true knowledge transfer.
- Evidence anchors:
  - [abstract] "Results show that: while both multilingual pretraining and instruction tuning are beneficial for cross-lingual knowledge alignment, the training strategy needs to be carefully designed... the overall cross-lingual knowledge alignment, especially in the conductivity level, is unsatisfactory for all tested LLMs"
  - [section 5.1] "However, the en-CO scores are quite high in all languages, suggesting that the right answers given in non-English languages are very likely the same as English answers... Table 4 shows the XRR scores are low across all non-English languages"
  - [corpus] Weak evidence for this mechanism in the corpus - only one related paper mentions conductivity, and none discuss the specific finding of high consistency with low conductivity.

- Break condition: If the models showed high XRR scores across all languages, this mechanism would be invalidated, suggesting true knowledge transfer rather than overlapping training data.

### Mechanism 2
- Claim: Mixed multilingual pretraining improves basic abilities, performance, and consistency across all languages, while continued pretraining only improves the target language at the cost of others.
- Mechanism: Mixed pretraining involves training on data from multiple languages from the beginning, allowing the model to develop balanced language abilities. Continued pretraining involves further training on a specific language, which can improve performance in that language but may degrade performance in others due to overfitting.
- Core assumption: The model's ability to learn multiple languages is affected by the training strategy - mixed pretraining allows for balanced learning while continued pretraining leads to language-specific specialization.
- Evidence anchors:
  - [section 5.2.1] "Comparing models with and without continued and mixed Chinese pretraining, one can see that mixed pretraining improves the models' basic language abilities in all languages, while continued pretraining has negative effect on them"
  - [section 5.2.1] "Mixed pretraining improves the performance in all languages, as well as enhancing the English consistency of non-English languages. However, continued pretraining in Chinese only improves the Chinese performance, at the cost of lowering performance in other languages"
  - [corpus] No direct evidence in the corpus for this specific mechanism of mixed vs. continued pretraining effects.

- Break condition: If continued pretraining showed similar improvements across all languages as mixed pretraining, this mechanism would be invalid.

### Mechanism 3
- Claim: Multilingual instruction tuning improves basic language abilities in the target language and reduces performance drops in factual knowledge, but does not improve consistency or conductivity alignment.
- Mechanism: Instruction tuning on multilingual data improves the model's ability to follow instructions in the target language and reduces the negative impact of instruction tuning on factual knowledge performance. However, it does not lead to better knowledge consistency or transfer between languages.
- Core assumption: The benefits of multilingual instruction tuning are limited to improving language-specific instruction following and mitigating performance drops, rather than enhancing cross-lingual knowledge alignment.
- Evidence anchors:
  - [section 5.2.2] "Compared with English-only instruction tuning, adding Chinese data in the tuning process significantly improves the RA scores in Chinese, while not hurting the English performance"
  - [section 5.2.2] "Compared with English-only tuning, multilingual tuning causes less damage to the factual knowledge performance"
  - [section 5.2.2] "Table 11 and 12 shows the en-CO scores on the Factual knowledge and the XRR scores on the Fictional knowledge. One can see that the changes in the two scores brought by English-only and multilingual instructiong tuning are both minor"
  - [corpus] No direct evidence in the corpus for this specific mechanism of multilingual instruction tuning effects.

- Break condition: If multilingual instruction tuning showed significant improvements in consistency (en-CO) or conductivity (XRR) scores, this mechanism would be invalid.

## Foundational Learning

- Concept: Cross-lingual knowledge alignment
  - Why needed here: Understanding the concept of cross-lingual knowledge alignment is crucial for interpreting the study's findings on how well models can retrieve knowledge across different languages.
  - Quick check question: What are the three levels of cross-lingual knowledge alignment measured in this study?

- Concept: Multilingual pretraining vs. instruction tuning
  - Why needed here: Distinguishing between multilingual pretraining and instruction tuning is important for understanding the different approaches to improving multilingual performance and their effects on knowledge alignment.
  - Quick check question: How does multilingual pretraining differ from multilingual instruction tuning in terms of their goals and methods?

- Concept: Knowledge conductivity vs. consistency
  - Why needed here: Understanding the difference between knowledge conductivity (ability to retrieve knowledge learned in one language using another) and consistency (generating the same output for the same input in different languages) is crucial for interpreting the study's findings on the limitations of current multilingual models.
  - Quick check question: Why might a model show high consistency but low conductivity in cross-lingual knowledge alignment?

## Architecture Onboarding

- Component map:
  - Data collection and preprocessing: Constructing the Basic, Factual, and Fictional knowledge datasets in 10 languages.
  - Model selection: Choosing representative multilingual LLMs for evaluation (BLOOM, LLaMA, ChatGPT, and their variants).
  - Evaluation framework: Implementing the CLiKA framework with metrics for Performance (RA), Consistency (en-CO), and Conductivity (XRR).
  - Experiment design: Setting up experiments to compare the effects of multilingual pretraining and instruction tuning on knowledge alignment.

- Critical path:
  1. Construct the three knowledge datasets in 10 languages.
  2. Select and load the representative multilingual LLMs.
  3. Implement the CLiKA evaluation framework.
  4. Run experiments to measure Performance, Consistency, and Conductivity.
  5. Analyze the results to compare the effects of different training strategies.

- Design tradeoffs:
  - Dataset construction: Balancing the need for parallel data across languages with the availability and quality of existing datasets.
  - Model selection: Choosing a diverse set of models that represent different training strategies and language families.
  - Evaluation metrics: Designing metrics that accurately capture the three levels of knowledge alignment while being computationally feasible.

- Failure signatures:
  - Low Performance (RA) scores: Indicates poor basic language abilities or knowledge retrieval in certain languages.
  - High Consistency (en-CO) but low Conductivity (XRR): Suggests that the model relies on overlapping training data rather than true knowledge transfer.
  - Negative effects of continued pretraining: May indicate overfitting to the target language at the expense of others.

- First 3 experiments:
  1. Evaluate the basic language abilities (RA) of the selected models on the Basic knowledge dataset.
  2. Measure the consistency (en-CO) and conductivity (XRR) of the models on the Factual and Fictional knowledge datasets.
  3. Compare the effects of mixed vs. continued pretraining on the models' Performance, Consistency, and Conductivity scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms in multilingual pretraining and instruction tuning could be designed to improve cross-lingual knowledge conductivity (CD) rather than just consistency (CT) and performance (PF)?
- Basis in paper: [explicit] The paper concludes that neither multilingual pretraining nor instruction tuning can improve knowledge conductivity, suggesting the need for novel strategies.
- Why unresolved: The study identifies the limitation but does not propose or test specific new mechanisms that could address the shallow alignment issue.
- What evidence would resolve it: Experimental results demonstrating that a new training approach (e.g., incorporating explicit cross-lingual knowledge alignment objectives or parallel data augmentation) significantly improves CD scores without sacrificing PF or CT.

### Open Question 2
- Question: How does the depth of cross-lingual knowledge alignment vary across different language families or typological features (e.g., morphological complexity, writing system differences)?
- Basis in paper: [inferred] The paper observes lower performance and conductivity in non-Latin and non-Indo-European languages (e.g., Arabic, Hebrew, Japanese, Chinese), implying potential typological influences.
- Why unresolved: The study focuses on a limited set of languages and does not systematically analyze alignment quality across diverse linguistic features.
- What evidence would resolve it: Comparative analysis of cross-lingual alignment metrics (PF, CT, CD) across a broader and more typologically diverse set of languages, revealing patterns correlated with specific linguistic properties.

### Open Question 3
- Question: To what extent does the observed high cross-lingual consistency (CT) in LLMs stem from overlapping training data versus true knowledge transfer, and how can this be accurately measured?
- Basis in paper: [explicit] The paper hypothesizes that high CT is more likely due to overlapping training data rather than knowledge conduction, supported by low CD scores.
- Why unresolved: The study infers this from low CD but does not directly test or quantify the contribution of overlapping data versus knowledge transfer to CT.
- What evidence would resolve it: Experiments isolating the effects of data overlap (e.g., by controlling for parallel data in training) on CT scores, or developing new metrics that distinguish between consistency from overlap and consistency from knowledge transfer.

## Limitations

- Limited theoretical foundation: The corpus analysis reveals minimal evidence supporting the mechanism linking high consistency to overlapping training data.
- Narrow linguistic scope: Testing only ten languages, primarily from Indo-European families, may not capture full cross-linguistic variation.
- Limited intermediate measures: The framework focuses on three levels of alignment without exploring potential intermediate mechanisms.

## Confidence

**High Confidence**: The finding that multilingual pretraining and instruction tuning improve basic language abilities and performance consistency across languages is well-supported by the experimental results.

**Medium Confidence**: The claim that current multilingual training methods lead to shallow alignment relies on the conductivity measurements, but the theoretical explanation could be strengthened with additional evidence.

**Low Confidence**: The specific mechanisms proposed for why mixed pretraining outperforms continued pretraining, and why multilingual instruction tuning doesn't improve conductivity, are based on limited experimental comparisons without strong theoretical grounding.

## Next Checks

1. **Dataset Overlap Analysis**: Conduct a systematic analysis of the training data overlap between English and non-English corpora to directly test the hypothesis that high consistency scores result from overlapping training data rather than true knowledge transfer.

2. **Intermediate Alignment Measures**: Design and implement additional metrics that could capture potential intermediate stages of cross-lingual knowledge alignment between consistency and conductivity.

3. **Cross-Lingual Transfer Learning Experiments**: Design controlled experiments that explicitly test knowledge transfer by training models on specific facts in one language and testing retrieval capabilities in another, systematically varying factors like linguistic distance, script differences, and domain specificity.