---
ver: rpa2
title: 'LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs'
arxiv_id: '2409.02076'
source_url: https://arxiv.org/abs/2409.02076
tags:
- task
- instruction
- tasks
- generation
- floor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LongGenBench, the first benchmark to evaluate
  long-form text generation in long-context LLMs. It tests models' ability to generate
  coherent, instruction-following text up to 32K tokens across four real-world scenarios
  (diary, menu, skyscraper, urban planning).
---

# LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs

## Quick Facts
- arXiv ID: 2409.02076
- Source URL: https://arxiv.org/abs/2409.02076
- Reference count: 40
- Key outcome: First benchmark showing LLMs struggle with coherent long-form generation beyond 4K tokens, with best models achieving only ~27% STIC-2 scores at 16K tokens

## Executive Summary
LongGenBench is the first benchmark designed to evaluate long-form text generation in long-context language models. It tests models' ability to generate coherent, instruction-following text up to 32K tokens across four real-world scenarios. The evaluation reveals that while models excel at short outputs, all tested LLMs (7B-72B parameters) struggle with long-form generation, with accuracy dropping significantly beyond 4K tokens. Performance gaps between input handling and output generation suggest these tasks require different skills, highlighting the need for architectural improvements to enhance long-context generation capabilities.

## Method Summary
LongGenBench uses template-based synthetic data generation to create controlled tasks requiring specific events or constraints within generated text. The benchmark includes four scenarios (diary, menu, skyscraper, urban planning) with three instruction types (Single, Range, Periodic) evaluated at 16K and 32K tokens. Model outputs are evaluated using Llama 3.1-8B as a judge with three complementary metrics: Main Task Completion (CR), Specific Task Instruction Completion-1 (STIC-1), and Specific Task Instruction Completion-2 (STIC-2). The evaluation pipeline involves generating task instructions, feeding them to LLMs, splitting outputs into subtasks, matching against verification sets, and calculating performance metrics.

## Key Results
- All tested LLMs show clear performance degradation beyond 4K tokens, with STIC-2 scores dropping from ~55% at 4K to ~27% at 16K tokens
- GPT-4o and Qwen2-72B achieve the highest performance (~27% STIC-2 at 16K tokens), while most models complete <60% of tasks at 32K tokens
- Significant performance gap between input handling (Ruler dataset) and output generation, with Pearson correlation coefficients of 0.51 at 16K and 0.66 at 32K tokens
- Open-source models show larger performance gaps than closed-source models, suggesting architectural differences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LongGenBench exposes fundamental architectural limitations in LLMs - inability to maintain coherent generation over extended sequences
- Mechanism: Transformer self-attention mechanisms struggle to maintain meaningful context relationships as sequence length increases, causing progressive degradation
- Core assumption: Self-attention's quadratic complexity creates a bottleneck for long-form generation
- Evidence anchors: Performance degradation beyond 4K tokens; gap between Ruler input handling and LongGenBench output generation
- Break condition: If models maintain consistent STIC-2 scores across all sequence lengths or if sparse attention eliminates degradation

### Mechanism 2
- Claim: LongGenBench reveals instruction-following generation requires different capabilities than long-context input understanding
- Mechanism: Input handling tests retrieval/comprehension skills while output generation demands sustained reasoning, memory retention, and coherent narrative construction
- Core assumption: These are distinct cognitive capabilities that don't transfer between input and output tasks
- Evidence anchors: Performance gaps between Ruler and LongGenBench; Pearson correlation 0.51-0.66 suggesting tasks aren't equivalent
- Break condition: If correlation approaches 1.0 or if Ruler performers also excel at LongGenBench

### Mechanism 3
- Claim: LongGenBench's synthetic task design effectively isolates instruction-following capabilities from confounding factors
- Mechanism: Controlled templates and verification sets create tasks where success depends primarily on instruction-following rather than knowledge or creativity
- Core assumption: Synthetic tasks provide cleaner evaluation signals than real-world data
- Evidence anchors: Carefully chosen scenarios with temporal and spatial tasks; active research area in related benchmarks
- Break condition: If real-world evaluation shows different patterns or if models can game synthetic tasks

## Foundational Learning

- Concept: Instruction-following vs. retrieval comprehension
  - Why needed here: LongGenBench demonstrates these are distinct capabilities - models can excel at understanding long inputs but fail at generating coherent long outputs
  - Quick check question: If a model scores 90% on Ruler but only 20% on LongGenBench, what does this reveal about its capabilities?

- Concept: Token vs. word count measurement
  - Why needed here: The paper uses tokens for task specification but words for evaluation output, affecting interpretation of performance
  - Quick check question: If a task requires 16K tokens of output, approximately how many words should we expect the model to generate?

- Concept: Template-based synthetic data generation
  - Why needed here: LongGenBench relies on template-based generation to create controlled, verifiable tasks
  - Quick check question: What are the advantages and limitations of using template-based synthetic data for evaluating long-form generation?

## Architecture Onboarding

- Component map: Task generation engine -> LLM generation -> Output splitting -> Subtask matching -> Llama 3.1-8B evaluation -> Metric calculation
- Critical path: 1) Generate task instructions using templates 2) Feed to LLM for generation 3) Split output into subtasks 4) Match subtasks against verification set 5) Evaluate using Llama 3.1-8B judge 6) Calculate metrics (CR, STIC-1, STIC-2)
- Design tradeoffs: Synthetic vs. real data (control vs. complexity); Binary vs. continuous evaluation (simplicity vs. nuance); Token vs. word measurement (standardization vs. practical assessment)
- Failure signatures: STIC-2 >> STIC-1 with low CR (incomplete responses); STIC-1 ≈ STIC-2 but both low (struggles with instruction-following); Performance degradation beyond 4K tokens (attention limitation)
- First 3 experiments: 1) Replicate Ruler benchmark results for baseline 2) Run LongGenBench with 8K tokens to identify degradation point 3) Compare open-source vs closed-source models on identical tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can long-context LLMs be designed to maintain content richness and avoid repetitive outputs over extended sequences?
- Basis in paper: Model outputs tend to converge as output volume increases, leading to homogenization of recorded events and repetitive outputs
- Why unresolved: Adjusting parameters like repetition penalty during inference has shown limited success, suggesting more advanced techniques are needed
- What evidence would resolve it: Research demonstrating successful architectural modifications that enable diverse, non-repetitive content across long outputs

### Open Question 2
- Question: What training data strategies could improve long-form text generation by addressing the lack of extended instructional content in current datasets?
- Basis in paper: Most instruction-tuning datasets are brief (under 200 tokens) and lack extended instructional content necessary for generating longer outputs
- Why unresolved: Identifies significant performance gap but doesn't provide concrete solutions for creating longer instructional content
- What evidence would resolve it: Successful experiments showing improved performance after incorporating longer, more comprehensive instructional examples

### Open Question 3
- Question: How can evaluation metrics be refined to better capture nuances of instruction-following and content coherence in long-form text generation?
- Basis in paper: Introduces STIC-1 and STIC-2 metrics but notes STIC-2 provides more granular assessment and considers simplifying to use only STIC-2
- Why unresolved: Acknowledges limitations in current evaluation approaches and suggests potential improvements without implementing or validating them
- What evidence would resolve it: Comparative studies showing refined evaluation metrics provide more accurate, interpretable assessments of long-form generation quality

## Limitations
- Synthetic task generalizability to real-world scenarios remains uncertain
- Evaluation using single judge (Llama 3.1-8B) may introduce bias
- Substantial computational requirements (8× NVIDIA A800 GPUs) may limit reproducibility

## Confidence
- High: Models consistently show performance degradation beyond 4K tokens; performance gaps between input handling and output generation are consistently observed; correlation between input and output performance increases with sequence length but remains below 0.7
- Medium: Synthetic tasks effectively isolate instruction-following capabilities; the three proposed metrics provide comprehensive evaluation coverage; open-source models show larger performance gaps than closed-source models
- Low: Long-term generalization of synthetic task results to real-world applications; binary evaluation approach captures all nuances of instruction adherence; current transformer architectures are fundamentally incapable of improving long-form generation

## Next Checks
1. Apply LongGenBench evaluation methodology to real-world long-form generation tasks to assess synthetic task generalizability
2. Implement an ensemble of different evaluation models to assess robustness and potential bias of the binary classification approach
3. Test modified transformer architectures with sparse attention patterns to determine if performance degradation can be mitigated or eliminated