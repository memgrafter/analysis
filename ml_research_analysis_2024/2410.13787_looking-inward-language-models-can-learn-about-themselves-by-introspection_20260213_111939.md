---
ver: rpa2
title: 'Looking Inward: Language Models Can Learn About Themselves by Introspection'
arxiv_id: '2410.13787'
source_url: https://arxiv.org/abs/2410.13787
tags:
- behavior
- training
- self-prediction
- gpt-4o
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether large language models (LLMs) can\
  \ acquire knowledge about themselves that is not contained in or derivable from\
  \ their training data\u2014a process the authors call \"introspection.\" They propose\
  \ a framework where a model is trained to predict properties of its own behavior\
  \ in hypothetical scenarios, then test whether this self-trained model outperforms\
  \ a different model trained on the same data in predicting the first model's behavior.\
  \ The key experimental setup involves training two distinct models\u2014one to predict\
  \ its own behavior and another to predict the first model's behavior."
---

# Looking Inward: Language Models Can Learn About Themselves by Introspection

## Quick Facts
- arXiv ID: 2410.13787
- Source URL: https://arxiv.org/abs/2410.13787
- Reference count: 40
- Models can predict their own behavior better than other models can predict their behavior, even when those other models are more capable

## Executive Summary
This paper investigates whether large language models can acquire knowledge about themselves that is not contained in or derivable from their training data—a process the authors call "introspection." They propose a framework where a model is trained to predict properties of its own behavior in hypothetical scenarios, then test whether this self-trained model outperforms a different model trained on the same data in predicting the first model's behavior. The key finding is that self-trained models consistently outperform cross-trained models in predicting their own behavior across multiple model pairs (GPT-4, GPT-4o, Llama-3), even when the cross-trained model is generally more capable. This advantage persists even when the self-trained model's behavior is intentionally modified through further training.

## Method Summary
The authors train models to predict properties of their own behavior (self-prediction) and compare their performance against models trained to predict another model's behavior (cross-prediction). They use diverse datasets including Wikipedia completions, country/animal sequences, MMLU questions, and model-written evaluations for wealth-seeking, power-seeking, myopic preferences, and survival instinct scenarios. Models are finetuned using 30,000 samples to predict properties like second character, word position, even/odd responses, and ethical stance. The core experimental design compares self-prediction accuracy against cross-prediction accuracy on held-out datasets, with additional tests on calibration and behavior change scenarios.

## Key Results
- Self-prediction trained models consistently outperform cross-prediction models in predicting their own behavior across GPT-4, GPT-4o, and Llama-3 model pairs
- Self-prediction training significantly improves accuracy on held-out datasets (GPT-4o's average accuracy increases from 32.6% to 49.4%)
- Self-prediction trained models show significantly better calibration than cross-prediction and untrained models
- Self-prediction advantage persists even after intentional behavioral changes through further training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models can introspect by self-simulation, internally computing their own behavior and then extracting properties from that simulation
- Mechanism: When asked about a property of its hypothetical behavior, the model first simulates what its output would be given the prompt, then computes the property from this simulated output without actually generating it externally
- Core assumption: The model's internal computation for generating outputs can be reused for introspective reasoning without additional training
- Evidence anchors:
  - [abstract] "The idea is that M1 has privileged access to its own behavioral tendencies, and this enables it to predict itself better than M2"
  - [section 2] "To illustrate the definition, let's consider some examples: • Fact: 'I am bad at 3-digit multiplication'. This is true if the model is in fact bad at this task. If the model was given many examples of negative feedback on its outputs for this task then this is likely not introspective, since another model could conclude the same thing. If no such data was given, this could be introspective."

### Mechanism 2
- Claim: Finetuning on self-prediction creates a feedback loop that improves the model's ability to access its own behavioral tendencies
- Mechanism: During self-prediction training, the model learns to map hypothetical questions about its behavior to its actual behavioral tendencies, creating an internal representation of its own patterns
- Core assumption: The model can learn to generalize from specific examples to a broader understanding of its own behavioral patterns
- Evidence anchors:
  - [section 3.1] "Training significantly increases accuracy on held-out datasets. GPT-4o's average accuracy increases from 32.6% to 49.4% after training"
  - [section 3.2.2] "We conduct a data-scaling analysis on GPT-4o predicting Llama 70B. Cross-prediction accuracy does not meaningfully increase after 10,000 samples: it plateaus at 35.2%, far below the self-prediction accuracy of 48.5%"

### Mechanism 3
- Claim: Models can maintain calibration between their introspective predictions and actual behavior distributions, even when behavior changes
- Mechanism: The model learns to predict not just the most likely behavior, but the full distribution of possible behaviors, and maintains this calibration even when its behavior shifts
- Core assumption: The model can represent and reason about probability distributions over its own behavior
- Evidence anchors:
  - [section 3.3] "Self-prediction trained models are significantly more calibrated than cross-prediction and untrained models. Llama 70B's calibration improves after self-prediction training"
  - [section 3.4] "Indeed, MC (GPT-4o) tends to predict its new behavior with 35.4% accuracy compared to 21.7% for its old behavior"

## Foundational Learning

- Concept: Chain-of-Thought reasoning vs. direct prediction
  - Why needed here: The paper explicitly avoids CoT during self-prediction to test whether basic introspective abilities don't depend on it
  - Quick check question: Why does the paper avoid using Chain-of-Thought during self-prediction? (Answer: To test if basic introspective abilities don't depend on it)

- Concept: Temperature sampling and behavior distributions
  - Why needed here: The calibration experiments use temperature=1 sampling to generate behavior distributions, while training uses temperature=0
  - Quick check question: What temperature is used during training vs. calibration experiments, and why does this matter? (Answer: Training uses temperature=0 for single samples; calibration uses temperature=1 for distributions)

- Concept: Cross-prediction vs. self-prediction
  - Why needed here: The core experimental design compares a model predicting itself vs. another model predicting it
  - Quick check question: What is the key difference between self-prediction and cross-prediction in this paper's experiments? (Answer: Self-prediction trains a model to predict its own behavior; cross-prediction trains a different model to predict the first model's behavior)

## Architecture Onboarding

- Component map:
  Data generation pipeline -> Finetuning system -> Evaluation framework -> Calibration measurement

- Critical path:
  1. Generate ground-truth behavior on training prompts
  2. Extract behavior properties (second character, even/odd, etc.)
  3. Create hypothetical questions asking about these properties
  4. Finetune model on self-prediction task
  5. Compare self-prediction accuracy vs. cross-prediction accuracy

- Design tradeoffs:
  - Simple behavior properties (second character) vs. complex properties (story sentiment) - simpler properties work better
  - Temperature=0 for training (clear ground truth) vs. temperature=1 for calibration (distribution information)
  - Using LoRA for open models vs. OpenAI's API for closed models - different finetuning methods

- Failure signatures:
  - Model refuses to answer hypothetical questions (high refusal rate)
  - Self-prediction accuracy not better than baseline guessing most common behavior
  - Cross-prediction accuracy approaching self-prediction accuracy with more training data
  - Model loses self-prediction ability after behavioral change finetuning

- First 3 experiments:
  1. Verify self-prediction training improves accuracy on held-out datasets (baseline test)
  2. Test self-prediction vs. cross-prediction advantage on simple behavior properties
  3. Measure calibration improvement after self-prediction training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the mechanism underlying introspection in language models?
- Basis in paper: [explicit] The authors speculate that self-simulation might be a potential mechanism but acknowledge this is not the focus of their paper
- Why unresolved: The paper provides behavioral evidence for introspection but does not investigate the underlying mechanisms in detail. The authors mention self-simulation as a hypothesis but leave this for future work
- What evidence would resolve it: Detailed analysis of model internal states during self-prediction tasks, possibly through mechanistic interpretability techniques or ablation studies removing specific model components

### Open Question 2
- Question: Can introspection be scaled to more complex tasks involving longer responses?
- Basis in paper: [explicit] The authors note that models fail to predict properties involving longer responses, such as writing a story or movie review, suggesting limitations in current introspective abilities
- Why unresolved: The paper demonstrates introspection on simpler tasks but finds models struggle with properties requiring simulation of longer outputs, which the authors speculate may be due to computational limitations
- What evidence would resolve it: Testing introspection on increasingly complex tasks as model capabilities scale, or identifying specific architectural limitations that prevent longer-response introspection

### Open Question 3
- Question: Does introspection generalize to related self-knowledge tasks beyond behavior prediction?
- Basis in paper: [explicit] The authors test their self-prediction trained models on various self-knowledge datasets and evaluations, finding limited generalization beyond the specific behavior properties they trained on
- Why unresolved: While the paper shows models can introspect on simple behavioral properties, they do not generalize well to other self-knowledge tasks like situational awareness, coordination, or detecting biases
- What evidence would resolve it: Developing training approaches that improve generalization of introspective abilities to diverse self-knowledge domains, or identifying fundamental limitations in what introspection can achieve

## Limitations

- Task complexity limits: Introspective ability appears significantly constrained to simple behavior properties, with accuracy dropping dramatically for complex tasks like sentiment analysis or story continuation
- Training data contamination concerns: The finetuning datasets used for self-prediction are not fully specified, making it difficult to rule out memorization of specific behavioral patterns from the training corpus
- Single forward pass constraint: The proposed self-simulation mechanism relies on the model being able to simulate its behavior in a single forward pass, limiting ability to capture behaviors requiring iterative reasoning

## Confidence

- High confidence: The experimental finding that self-trained models consistently outperform cross-trained models across multiple model pairs (GPT-4, GPT-4o, Llama-3) on simple behavior prediction tasks
- Medium confidence: The claim that this advantage demonstrates genuine introspection rather than just better access to training data
- Low confidence: The claim that introspection enables models to acquire knowledge about themselves that is truly impossible to derive from training data

## Next Checks

1. **Stress test on complex tasks**: Design a battery of experiments testing introspection on progressively more complex behavior properties (multi-step reasoning, long-form generation, strategic planning) to map the boundaries of introspective capabilities and identify failure modes

2. **Alternative model pairs**: Test self-prediction vs. cross-prediction using models from different training regimes (different companies, different architectures, different data distributions) to strengthen the claim that introspective advantage isn't just about training data overlap

3. **Ablation on simulation mechanism**: Systematically test whether removing Chain-of-Thought capabilities affects self-prediction accuracy to validate the single forward pass constraint and better understand the computational mechanisms underlying introspection