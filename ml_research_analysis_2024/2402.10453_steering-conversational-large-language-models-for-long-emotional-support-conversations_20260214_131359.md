---
ver: rpa2
title: Steering Conversational Large Language Models for Long Emotional Support Conversations
arxiv_id: '2402.10453'
source_url: https://arxiv.org/abs/2402.10453
tags:
- strategy
- support
- emotional
- responses
- conversation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of enabling large language models
  (LLMs) to consistently adhere to emotional support strategies in extended conversations.
  The authors introduce the Strategy Relevant Attention (SRA) metric to quantify the
  model's adherence to prompted strategies through attention maps.
---

# Steering Conversational Large Language Models for Long Emotional Support Conversations

## Quick Facts
- arXiv ID: 2402.10453
- Source URL: https://arxiv.org/abs/2402.10453
- Authors: Navid Madani; Sougata Saha; Rohini Srihari
- Reference count: 19
- Primary result: Fine-tuned Llama2-7B-chat achieves 78.9% improvement in strategy adherence accuracy over base model

## Executive Summary
This study addresses the challenge of enabling large language models to consistently adhere to emotional support strategies in extended conversations. The authors introduce the Strategy Relevant Attention (SRA) metric to quantify model adherence to prompted strategies through attention maps. They create a strategy-conditioned synthetic conversational dataset derived from ESConv and propose various prompting baselines informed by the SRA metric. Through fine-tuning Llama2-7B-chat and Llama3-8B-instruct models, they achieve significant improvements in steerability while maintaining conversational quality, with SRA showing strong correlation (0.94 Pearson) with strategy adherence.

## Method Summary
The authors develop the Strategy Relevant Attention (SRA) metric to measure how well models maintain attention on strategy-relevant tokens. They create an extended ESConv dataset using a 13B parameter model with high-SRA prompts, then fine-tune Llama2-7B-chat and Llama3-8B-instruct using LoRA on strategy-conditioned continuations. The approach focuses on the last utterance while maintaining conversational context. They evaluate using both automatic strategy classifiers and human evaluation to assess improvements in strategy adherence and conversational quality.

## Key Results
- Fine-tuned models achieve 78.9% improvement in strategy adherence accuracy over Llama2 base model
- SRA metric shows 0.94 Pearson correlation with strategy adherence, validating its effectiveness
- Fine-tuned models maintain naturalness and coherence while improving steerability
- Llama3-8B-instruct fine-tuned model shows 37.6% improvement over base model

## Why This Works (Mechanism)

### Mechanism 1
The SRA metric captures how well a model maintains attention on strategy-relevant tokens throughout a conversation. SRA aggregates attention weights from all layers and heads of the LLM for response tokens over strategy-relevant prompt tokens. Higher SRA indicates stronger focus on the strategy. Core assumption: Attention weight distribution correlates with the model's adherence to the prompted strategy.

### Mechanism 2
Prompt structure significantly affects the model's attention to strategy tokens, especially in longer conversations. Different prompt templates (e.g., moving conversation history to system message) can maintain higher SRA and strategy adherence by keeping strategy tokens more prominent in the input sequence. Core assumption: The position and prominence of strategy tokens in the prompt directly influences the model's attention allocation.

### Mechanism 3
Fine-tuning on a strategy-conditioned synthetic dataset improves the model's steerability while maintaining conversational quality. LoRA fine-tuning on strategy-conditioned continuations from the extended ESConv dataset, focusing on the last utterance, enhances the model's attention to system prompts and strategy adherence. Core assumption: Synthetic data generated with high-SRA prompts provides effective training signals for improving steerability.

## Foundational Learning

- **Attention mechanisms in Transformers**: Understanding how SRA is calculated and why it correlates with strategy adherence. Quick check: How do attention weights in Transformers relate to the model's focus on different parts of the input sequence?

- **Prompt engineering and template design**: To understand how different prompt structures affect SRA and strategy adherence. Quick check: What are the key differences between the standard prompt and the c1_hf prompt, and how might these affect the model's attention?

- **Fine-tuning techniques (LoRA)**: To understand how the model is adapted to improve steerability without extensive retraining. Quick check: How does LoRA fine-tuning differ from full fine-tuning, and what are its advantages for this task?

## Architecture Onboarding

- **Component map**: Extended ESConv Dataset -> Fine-tuning (LoRA) -> Llama2-7B-chat/Llama3-8B-instruct -> Strategy Classifier -> SRA Metric -> Strategy Adherence Evaluation

- **Critical path**: 1. Generate extended dataset using high-SRA prompts 2. Fine-tune model on strategy-conditioned continuations 3. Evaluate steerability using SRA metric and strategy adherence accuracy 4. Validate conversational quality through human and model-based evaluation

- **Design tradeoffs**: Synthetic vs. real data (controlled generation vs. real-world diversity), prompt complexity vs. effectiveness (more complex prompts may improve SRA but harder to implement), fine-tuning scope (focusing only on last utterance may miss context but reduces computational cost)

- **Failure signatures**: Low SRA scores despite high strategy adherence accuracy, high SRA scores but poor strategy adherence, degradation in conversational quality after fine-tuning, overfitting to synthetic data leading to poor performance on real conversations

- **First 3 experiments**: 1. Compare SRA scores across different prompt templates using base Llama model 2. Fine-tune Llama2-7B-chat on extended dataset and evaluate strategy adherence vs. base model 3. Perform human evaluation comparing responses from fine-tuned and base models for conversational quality

## Open Questions the Paper Calls Out

### Open Question 1
Does the SRA metric maintain its strong correlation (0.94 Pearson) with strategy adherence when applied to emotional support conversations in different languages or cultural contexts? The paper establishes high correlation in English conversations but doesn't explore cross-linguistic or cross-cultural applications.

### Open Question 2
How does the SRA metric's predictive power change when applied to emotional support conversations with non-LLM participants (e.g., human-to-human or human-to-rule-based-bot interactions)? The metric was developed and tested specifically for steering LLMs.

### Open Question 3
Does the relationship between SRA and strategy adherence vary depending on the specific emotional support strategy being followed? The authors present aggregate results across all 15 strategies but don't analyze strategy-specific differences.

## Limitations
- Synthetic dataset may not fully capture real emotional support conversation diversity and complexity
- SRA metric remains a proxy measure based on attention weights, not direct evaluation of strategy adherence
- Fine-tuning focuses on last utterance, potentially limiting ability to maintain strategy adherence across entire conversation histories

## Confidence

**High Confidence**: Correlation between SRA metric and strategy adherence accuracy (0.94 Pearson correlation), and quantitative improvements in strategy adherence after fine-tuning (78.9% for Llama2, 37.6% for Llama3)

**Medium Confidence**: SRA effectively measures model performance and prompt structure significantly affects attention to strategy tokens

**Low Confidence**: Fine-tuning on synthetic data maintains conversational quality while improving steerability, particularly regarding long-term generalization and performance on real emotional support conversations

## Next Checks

1. Test fine-tuned models on emotional support conversations from different domains (e.g., crisis intervention, peer support) to assess generalization beyond ESConv dataset

2. Conduct ablation study to determine individual contributions of attention heads and layers to SRA metric, and validate whether SRA remains predictive across different model architectures

3. Deploy fine-tuned models in controlled real-world setting with actual emotional support providers to evaluate performance, safety, and user satisfaction compared to baseline models