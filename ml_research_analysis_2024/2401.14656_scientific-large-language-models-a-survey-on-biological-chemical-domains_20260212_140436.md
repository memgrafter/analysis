---
ver: rpa2
title: 'Scientific Large Language Models: A Survey on Biological & Chemical Domains'
arxiv_id: '2401.14656'
source_url: https://arxiv.org/abs/2401.14656
tags:
- protein
- language
- molecular
- data
- scientific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive overview of Scientific Large
  Language Models (Sci-LLMs), focusing on their applications in biological and chemical
  domains. The paper systematically categorizes Sci-LLMs into textual, molecular,
  protein, genomic, and multimodal models, highlighting their architectures, capabilities,
  datasets, and evaluation methods.
---

# Scientific Large Language Models: A Survey on Biological & Chemical Domains

## Quick Facts
- arXiv ID: 2401.14656
- Source URL: https://arxiv.org/abs/2401.14656
- Reference count: 40
- Primary result: Comprehensive survey of Scientific Large Language Models (Sci-LLMs) in biological and chemical domains, categorizing models by architecture and highlighting key challenges including dataset scale, 3D structural integration, and evaluation methods.

## Executive Summary
This survey provides a systematic review of Scientific Large Language Models (Sci-LLMs) across biological and chemical domains. The paper categorizes Sci-LLMs into textual, molecular, protein, genomic, and multimodal models, examining their architectures, capabilities, datasets, and evaluation methods. A key contribution is the introduction of Sci-LLMs tailored to handle scientific languages such as SMILES for molecules, amino acid sequences for proteins, and genomic sequences for DNA/RNA. The survey identifies critical challenges including the need for larger-scale training datasets, integration of 3D structural information, and improved evaluation metrics. Future directions include constructing cross-modal datasets, incorporating 3D and temporal data, and enhancing Sci-LLMs with external knowledge sources and specialized tools.

## Method Summary
The survey reviews existing Sci-LLMs through systematic literature analysis, categorizing them into encoder-only, decoder-only, and encoder-decoder architectures. It examines training datasets and benchmarks across molecular, protein, and genomic languages, including ZINC, PubChem, UniRef, and PDB. The paper introduces a new framework for evaluating Sci-LLMs based on pre-college, college, and post-college levels of scientific knowledge. While comprehensive in scope, the survey relies on existing literature without providing new experimental data or empirical validation of claims.

## Key Results
- Systematic categorization of Sci-LLMs into encoder-only, decoder-only, and encoder-decoder architectures based on task requirements
- Identification of critical challenges including dataset scale/quality, 3D structural integration, and evaluation methods
- Introduction of a three-tier evaluation framework for assessing scientific knowledge capabilities
- Comprehensive overview of datasets and benchmarks across biological and chemical domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The survey categorizes Sci-LLMs into encoder-only, decoder-only, and encoder-decoder architectures, enabling targeted architectural selection based on task type.
- Mechanism: Encoder-only models are used for understanding and interpreting input molecules, proteins, or genomes, making them well-suited for tasks that require deep comprehension of molecular structures and properties. Decoder-only models are used for generating sequences or other forms of output data based on given inputs, such as generating novel protein sequences. Encoder-decoder models are used for sequence-to-sequence transformations, such as translating between protein sequences and structures.
- Core assumption: Different architectural designs are optimal for different types of scientific language tasks (e.g., understanding vs. generation).
- Evidence anchors:
  - [abstract]: "We divide them into three distinct types based on their specific architectures: encoder-only, decoder-only, and encoder-decoder models, as shown in Table 3."
  - [section]: "In the realm of encoder-only architecture, significant models such as SpliceBERT [ 419], DNABERT [143], DNABERT-2 [426], iEnhancer-BERT [213], scBERT [378], BioSeq-BLM [180], PLPMpro [189], GPN [21],..."
  - [corpus]: Weak or missing. The corpus does not provide explicit evidence of architectural categorization or task-specific suitability.
- Break condition: If a single architecture could effectively handle all types of scientific language tasks (understanding, generation, and transformation), the need for distinct architectural categories would diminish.

### Mechanism 2
- Claim: The survey emphasizes the importance of training data scale and quality, highlighting the need for larger pre-training datasets and higher-quality fine-tuning datasets for Sci-LLMs.
- Mechanism: Sci-LLMs require extensive pre-training on unlabeled data to learn general scientific language patterns, followed by fine-tuning on labeled data for specific tasks. The scale and quality of these datasets directly impact the model's performance and ability to generalize.
- Core assumption: Larger and higher-quality datasets lead to better-performing Sci-LLMs.
- Evidence anchors:
  - [abstract]: "A key contribution is the introduction of Sci-LLMs, which are tailored to handle scientific languages such as SMILES for molecules, amino acid sequences for proteins, and genomic sequences for DNA/RNA."
  - [section]: "Data stands as the foundation of AI model development. In exploring Sci-LLMs, we focus on key factors that influence their growth and effectiveness."
  - [corpus]: Weak or missing. The corpus does not provide explicit evidence of the relationship between dataset scale/quality and model performance.
- Break condition: If smaller or lower-quality datasets could achieve comparable performance to larger or higher-quality datasets, the emphasis on dataset scale and quality would be less critical.

### Mechanism 3
- Claim: The survey identifies the need for incorporating 3D structural and temporal information into scientific language systems, recognizing the importance of 3D structures in determining biological and chemical functions.
- Mechanism: Current Sci-LLMs primarily focus on sequence data, but 3D structural information is crucial for understanding the functions and properties of molecules, proteins, and genomes. Incorporating this information would enhance the models' ability to capture the dynamic nature of biological and chemical systems.
- Core assumption: 3D structural and temporal information is essential for understanding biological and chemical functions.
- Evidence anchors:
  - [abstract]: "The function of a protein is intricately linked to its structure, another reasonable approach for protein representation would involve the integration of its 3D conformation."
  - [section]: "The inherent nature of biological and chemical data is rooted in the three-dimensional physical world, where the functionalities are primarily determined by their constantly changing 3D structures."
  - [corpus]: Weak or missing. The corpus does not provide explicit evidence of the relationship between 3D structural information and model performance.
- Break condition: If sequence data alone could adequately capture biological and chemical functions, the need for incorporating 3D structural and temporal information would be less critical.

## Foundational Learning

- Concept: Scientific Languages
  - Why needed here: Understanding the different types of scientific languages (textual, molecular, protein, genomic) is crucial for developing and applying Sci-LLMs effectively.
  - Quick check question: What are the key differences between textual, molecular, protein, and genomic languages in terms of vocabulary and grammatical rules?

- Concept: Transformer Architectures
  - Why needed here: Familiarity with Transformer architectures (encoder-only, decoder-only, encoder-decoder) is essential for understanding the design choices and capabilities of Sci-LLMs.
  - Quick check question: What are the primary differences between encoder-only, decoder-only, and encoder-decoder Transformer architectures, and how do these differences impact their suitability for different scientific language tasks?

- Concept: Self-supervised Learning
  - Why needed here: Self-supervised learning techniques, such as masked language modeling, are widely used in training Sci-LLMs to learn from unlabeled scientific data.
  - Quick check question: How does masked language modeling work in the context of Sci-LLMs, and what are its advantages for learning from scientific language data?

## Architecture Onboarding

- Component map:
  - Encoder-only models -> Understanding and interpreting input data (property prediction, function prediction)
  - Decoder-only models -> Generating sequences or output data (molecule generation, protein design)
  - Encoder-decoder models -> Sequence-to-sequence transformations (protein sequence to structure translation)

- Critical path:
  1. Identify the specific scientific language task (e.g., property prediction, molecule generation, protein design)
  2. Select the appropriate architectural design (encoder-only, decoder-only, or encoder-decoder) based on the task requirements
  3. Choose or develop a suitable training dataset (pre-training and fine-tuning) that aligns with the task and architectural design
  4. Train and evaluate the Sci-LLM using the selected architecture and dataset
  5. Iterate and refine the model based on performance and evaluation results

- Design tradeoffs:
  - Encoder-only models: Better for understanding and interpretation but may struggle with generation tasks
  - Decoder-only models: Better for generation tasks but may lack deep understanding of input data
  - Encoder-decoder models: Offer a balance between understanding and generation but may be more complex to implement and train

- Failure signatures:
  - Poor performance on understanding tasks: May indicate the need for a more powerful encoder or a different architectural design
  - Poor performance on generation tasks: May indicate the need for a more powerful decoder or a different architectural design
  - Overfitting or underfitting: May indicate issues with dataset size, quality, or model complexity

- First 3 experiments:
  1. Train an encoder-only Sci-LLM on a property prediction task using a dataset like MoleculeNet or ProteinGym. Evaluate its performance on a held-out test set.
  2. Train a decoder-only Sci-LLM on a molecule generation task using a dataset like ZINC or ChEMBL. Evaluate its performance on metrics like validity, uniqueness, and novelty.
  3. Train an encoder-decoder Sci-LLM on a protein structure prediction task using a dataset like CASP or PDB. Evaluate its performance on metrics like RMSD or GDT.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can 3D structural and temporal information be effectively incorporated into scientific language systems to improve model performance?
- Basis in paper: [explicit] The paper discusses the need for incorporating 3D structural information and temporal dynamics into scientific language models, as current models primarily focus on sequence data and lack explicit representation of 3D structures.
- Why unresolved: While the paper acknowledges the importance of 3D structural information and suggests using structural motifs, it does not provide a concrete solution for integrating this information into language models.
- What evidence would resolve it: A proposed method for representing 3D structures as tokens within the language model, along with experimental results demonstrating improved performance in tasks like protein structure prediction or molecular property prediction.

### Open Question 2
- Question: What are the most effective ways to evaluate the performance of generative scientific language models, particularly for tasks involving the creation of novel molecules or proteins?
- Basis in paper: [explicit] The paper highlights the difficulty in evaluating the performance of generative scientific language models, especially for tasks involving the creation of novel molecules or proteins. It mentions the limitations of computational metrics and the need for wet-lab experiments for validation.
- Why unresolved: The paper does not propose a specific solution for evaluating the performance of generative scientific language models beyond mentioning the need for more robust computational evaluation metrics and benchmarks.
- What evidence would resolve it: A set of proposed evaluation metrics and benchmarks that can accurately assess the quality and validity of generated molecules or proteins, validated through comparison with wet-lab experimental results.

### Open Question 3
- Question: How can scientific language models be effectively aligned with external knowledge sources, such as domain-specific knowledge graphs, to improve their performance and reduce hallucinations?
- Basis in paper: [explicit] The paper discusses the potential benefits of integrating scientific language models with external knowledge sources, such as domain-specific knowledge graphs, to enhance their capabilities and reduce hallucinations.
- Why unresolved: The paper does not provide a detailed explanation of how to effectively align scientific language models with external knowledge sources or discuss the challenges involved in this process.
- What evidence would resolve it: A proposed method for integrating external knowledge sources into scientific language models, along with experimental results demonstrating improved performance in tasks like protein function prediction or drug discovery.

## Limitations
- The survey relies heavily on existing literature without providing new experimental data or empirical validation of claims
- Some datasets and evaluation methods are mentioned but not fully specified, potentially limiting reproducibility
- The relationship between architectural choices and specific task performance is described but not empirically demonstrated

## Confidence
- Claims about architectural categorization and their general suitability: High
- Claims about dataset scale and quality impact on performance: Medium
- Claims about the importance of 3D structural information: Medium

## Next Checks
1. Conduct controlled experiments comparing encoder-only, decoder-only, and encoder-decoder architectures on specific scientific language tasks to validate claimed architectural suitability
2. Perform ablation studies to quantify the impact of dataset scale and quality on Sci-LLM performance across different scientific domains
3. Develop and test methods for incorporating 3D structural information into Sci-LLMs, measuring performance improvements on tasks that benefit from spatial context