---
ver: rpa2
title: On the Principles of ReLU Networks with One Hidden Layer
arxiv_id: '2411.06728'
source_url: https://arxiv.org/abs/2411.06728
tags:
- linear
- equation
- function
- figure
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically studies the mechanism of solutions derived
  from the back-propagation algorithm for two-layer ReLU networks. The main idea is
  to construct universal function-approximation solutions based on the principles
  of one-sided bases of splines, multiple strict partial orders, and continuity restriction.
---

# On the Principles of ReLU Networks with One Hidden Layer

## Quick Facts
- arXiv ID: 2411.06728
- Source URL: https://arxiv.org/abs/2411.06728
- Authors: Changcun Huang
- Reference count: 8
- One-line primary result: A two-layer ReLU network can approximate any continuous piecewise linear function with ζ linear pieces using Θ ≥ ζ + n units

## Executive Summary
This paper provides a systematic theoretical framework for understanding the solutions obtained by back-propagation in two-layer ReLU networks. The key insight is that the training solution can be explained through three fundamental principles: one-sided bases of splines, multiple strict partial orders of knots, and continuity restriction. For one-dimensional inputs, the paper establishes that any continuous linear spline with ζ linear pieces can be realized by a two-layer ReLU network with at least Θ ≥ ζ + 1 units. The framework reveals that different solution patterns emerge from different combinations of these basic principles, explaining why the same function can be implemented by various network configurations.

## Method Summary
The paper constructs theoretical solutions for two-layer ReLU networks by arranging knots in strict partial order and using recurrence relations to build piecewise linear functions. The method relies on the observation that the output of a two-layer ReLU network is always continuous, which allows for recursive construction of adjacent linear pieces. The training procedure uses standard back-propagation with uniform initialization U(-1,1), learning rate 0.002, and 10000 steps. The theoretical framework is validated through experiments on synthetic piecewise linear functions, comparing the learned solutions against the predicted patterns based on one-sided and two-sided bases of splines.

## Key Results
- Any continuous linear spline with ζ linear pieces can be realized by a two-layer ReLU network with at least Θ ≥ ζ + n units
- The training solution obtained by back-propagation follows predictable patterns based on one-sided and two-sided bases of splines
- Multiple strict partial orders can be combined to cover the entire input space when a single order is insufficient
- Continuity at knots allows automatic determination of linear functions on boundary regions, creating new mathematical phenomena

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A two-layer ReLU network can approximate any continuous piecewise linear function with ζ linear pieces using Θ ≥ ζ + n units, where n is input dimensionality.
- Mechanism: The network constructs the approximation by arranging knots in a strict partial order and using one-sided bases of splines. Each linear piece is determined by a single parameter that relates it to its adjacent piece via continuity at the knot.
- Core assumption: The strict partial order ensures each region has its own distinguished unit that shapes its linear function, and continuity at knots allows recursive construction.
- Evidence anchors:
  - [abstract] states "any continuous linear spline with ζ linear pieces can be realized by a two-layer ReLU network with at least Θ ≥ ζ + n units"
  - [section 2.2] shows the recurrence formula sν(x) = sν−1(x) + λνσ(wTx + b) for constructing adjacent linear pieces
  - [corpus] provides related work on approximation theory but doesn't directly contradict this mechanism
- Break condition: If the strict partial order cannot be maintained or if knots are not properly arranged, the construction fails.

### Mechanism 2
- Claim: The continuity restriction principle allows realization of piecewise linear functions even when some regions don't belong to any strict partial order.
- Mechanism: If the function values on a boundary region and its adjacent regions are implemented, the linear function on the boundary region is automatically determined through continuity constraints.
- Core assumption: The output of a two-layer ReLU network is always continuous (corollary 1), and this continuity propagates through adjacent regions.
- Evidence anchors:
  - [section 7.1] states "if the function values on Lκν ∪ Lκµ have been implemented by N, the linear function on Rκ is also simultaneously produced"
  - [section 7.3] discusses boundary-determination problem as a new mathematical phenomenon
  - [corpus] shows related work on universal approximation but doesn't address this specific continuity mechanism
- Break condition: If the continuity property is violated or if regions are not properly adjacent, the automatic determination fails.

### Mechanism 3
- Claim: Multiple strict partial orders can be combined to cover the entire input space when a single order is insufficient.
- Mechanism: Different strict partial orders are arranged such that their initial regions are either the same or derived from previously constructed functions, allowing recursive construction across the entire space.
- Core assumption: The ordered regions from different partial orders can be arranged without interference, and their union covers the universal set.
- Evidence anchors:
  - [section 6.2] describes "a set H of n−1-dimensional hyperplanes divides U = [0,1]n into M regions" and shows how multiple orders work together
  - [section 6.3] explains how two-sided bases can be formed by changing hyperplanes into negative forms
  - [corpus] includes related work on universal approximation using ridge functions, which is different from this approach
- Break condition: If the conditions for non-overlapping ordered regions or proper arrangement of initial regions are not met, the combination fails.

## Foundational Learning

- Concept: Strict partial order of knots
  - Why needed here: Establishes the geometric framework for arranging hyperplanes in higher dimensions analogous to knots on a line
  - Quick check question: What conditions must be satisfied for a set of hyperplanes to form a strict partial order?

- Concept: Continuity at knots
  - Why needed here: Ensures the piecewise linear function constructed by the network has no discontinuities, which is essential for approximation
  - Quick check question: How does the continuity condition at a knot relate to the output weight parameter λ in the recurrence formula?

- Concept: Linear-output matrix rank condition
  - Why needed here: Determines when a set of units can realize an arbitrary linear function on a region
  - Quick check question: What is the minimum rank required for the linear-output matrix to ensure any linear function can be realized?

## Architecture Onboarding

- Component map: Hidden layer units → hyperplanes in input space → regions; Output layer → weighted sum of activated units
- Critical path: Initialize units → arrange in strict partial order → determine output weights via continuity constraints → verify approximation
- Design tradeoffs: More units provide finer approximation but increase complexity; bidirectional knots add flexibility but require more parameters
- Failure signatures: Discontinuities in the output function; inability to cover all regions; insufficient number of units for required linear pieces
- First 3 experiments:
  1. Implement a two-layer ReLU network to approximate a simple piecewise linear function with 3 linear pieces using 4 units
  2. Test continuity restriction by constructing a function where some regions are not in any strict partial order
  3. Verify multiple strict partial orders by dividing [0,1]² into regions using different sets of hyperplanes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum number of distinct solution patterns for a two-layer ReLU network approximating a given continuous function, considering the combination of global units, multiple strict partial orders, and continuity restriction?
- Basis in paper: [explicit] Section 9, item VI discusses the combination of different basic principles leading to various concrete solutions
- Why unresolved: The paper demonstrates the existence of multiple solution patterns but does not provide a comprehensive enumeration or upper bound on the total number of distinct patterns
- What evidence would resolve it: A systematic classification of all possible combinations of the three basic principles for different dimensional inputs and function types

### Open Question 2
- Question: Can the boundary-determination principle be extended to higher-order piecewise functions (e.g., piecewise quadratic or cubic) realized by two-layer ReLU networks?
- Basis in paper: [explicit] Section 7.4 introduces the boundary-determination problem for piecewise linear functions
- Why unresolved: The paper focuses exclusively on piecewise linear functions and does not investigate whether similar boundary-determination properties hold for higher-order piecewise polynomials
- What evidence would resolve it: Mathematical proof or counterexample showing whether boundary-determination holds for piecewise polynomials of degree > 1 in two-layer ReLU networks

### Open Question 3
- Question: How does the initialization of weights and biases affect the probability of obtaining a specific solution pattern in the back-propagation algorithm for two-layer ReLU networks?
- Basis in paper: [inferred] Section 8.1 mentions that weights and biases are initialized by uniform distribution U(-1,1), but does not analyze the impact of initialization on solution patterns
- Why unresolved: The paper demonstrates that various solution patterns exist but does not investigate the relationship between initialization and the likelihood of obtaining particular patterns
- What evidence would resolve it: Empirical study showing the distribution of solution patterns across different initialization schemes and parameter ranges

### Open Question 4
- Question: What is the minimum number of units required in the hidden layer of a two-layer ReLU network to achieve universal approximation for a given function class with specified error tolerance?
- Basis in paper: [explicit] Section 7.2 provides lower bounds Θ ≥ ζ^(1/n)n + 1 for universal approximation
- Why unresolved: The paper establishes lower bounds but does not determine the exact minimum number of units required for specific function classes and error tolerances
- What evidence would resolve it: Analytical derivation or experimental validation of tight upper and lower bounds on the minimum number of units for various function classes and error tolerances

## Limitations
- The theoretical framework is rigorously established only for one-dimensional inputs, with higher-dimensional extensions lacking complete proofs
- The paper focuses on function approximation rather than learning from data, assuming target function structure
- Experimental validation is limited to relatively simple cases, with complex one-dimensional and higher-dimensional examples remaining open problems

## Confidence

- **High confidence**: The universal approximation theorem for one-dimensional inputs with Θ ≥ ζ + 1 units, supported by rigorous mathematical proofs and experimental validation
- **Medium confidence**: The extension to higher-dimensional cases, which follows similar principles but lacks complete theoretical justification
- **Medium confidence**: The characterization of back-propagation solutions through the constructed frameworks, based on experimental evidence but without formal proofs of convergence

## Next Checks

1. **Theoretical extension**: Formally prove the higher-dimensional approximation theorem for general n, including rigorous treatment of multiple strict partial orders and their combination

2. **Empirical robustness**: Test the framework on real-world datasets with one-dimensional inputs to verify that back-propagation solutions consistently follow the predicted patterns across diverse function classes

3. **Boundary case analysis**: Investigate scenarios where strict partial orders cannot be formed or where continuity constraints fail, to identify conditions under which the theoretical framework breaks down