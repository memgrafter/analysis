---
ver: rpa2
title: Selectively Answering Visual Questions
arxiv_id: '2406.00980'
source_url: https://arxiv.org/abs/2406.00980
tags:
- answer
- question
- answers
- methods
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of calibrating visual question
  answering (VQA) models, specifically focusing on their ability to provide reliable
  confidence estimates for their predictions. The authors conduct the first in-depth
  analysis of calibration methods and metrics for VQA systems using in-context learning
  with large multimodal models (LMMs).
---

# Selectively Answering Visual Questions

## Quick Facts
- arXiv ID: 2406.00980
- Source URL: https://arxiv.org/abs/2406.00980
- Authors: Julian Martin Eisenschlos; Hernán Maina; Guido Ivetta; Luciana Benotti
- Reference count: 24
- This paper conducts the first in-depth analysis of calibration methods for visual question answering systems, proposing AVG BLEU which improves coverage by 5-8 points.

## Executive Summary
This paper addresses the challenge of calibrating visual question answering (VQA) models to provide reliable confidence estimates for their predictions. The authors conduct the first comprehensive analysis of calibration methods and metrics for VQA systems using in-context learning with large multimodal models (LMMs). They evaluate various calibration approaches across two answerability benchmarks (VizWiz-VQA and UNK-VQA) and propose AVG BLEU, a novel calibration score that significantly improves coverage metrics by combining sampling and likelihood methods.

## Method Summary
The study uses in-context learning with LMMs (LLaVA 13B, Flamingo 3B, BLIP-2) and text-only LLMs (PaLM-2 Bison, Falcon) with 4-shot prompts. Models generate 10 responses with temperature 0.7 for each image-question pair, using gold captions for LLMs. Four calibration methods are compared: likelihood-based scoring, repetition, diversity, and the proposed AVG BLEU. The evaluation uses Expected Calibration Error (ECE), ROC-AUC, and Coverage@Accuracy metrics across VizWiz-VQA (4k validation split) and UNK-VQA (1k validation split) datasets.

## Key Results
- Likelihood scoring is better calibrated for LMMs than LLMs, with no clear winner among sampling methods
- Sampling-based methods (repetition, diversity) are generally superior for text-only models
- AVG BLEU significantly improves coverage at 80% accuracy by 5 points for the best LMM and coverage at 70% accuracy by 8 points for the best LLM

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Likelihood scoring is better calibrated for LMMs than LLMs because visual grounding acts as a regularizer, spreading the likelihood distribution more meaningfully.
- **Mechanism:** LMMs incorporate visual grounding into their likelihood computation, which regularizes the model to produce more concentrated and semantically relevant likelihoods compared to LLMs relying solely on captions.
- **Core assumption:** Visual grounding introduces additional signal that constrains the likelihood distribution to be more discriminative between answerable and unanswerable questions.
- **Evidence anchors:** The paper shows that visually grounded models have better-calibrated likelihood scores, though the grounding regularization effect is inferred rather than explicitly proven.

### Mechanism 2
- **Claim:** Sampling-based methods are superior for LLMs because they capture uncertainty from surface form variability of answers.
- **Mechanism:** By generating multiple samples, these methods measure how much the model's outputs vary, with low variance indicating high confidence in a single answer.
- **Core assumption:** The variance among sampled answers directly reflects the model's uncertainty about the correct answer.
- **Evidence anchors:** The paper cites Cole et al. (2023a) showing that likelihood is the worst calibration metric for text-only models while repetition and diversity are superior.

### Mechanism 3
- **Claim:** AVG BLEU improves calibration by combining benefits of sampling diversity and likelihood scoring.
- **Mechanism:** It computes pairwise BLEU scores among sampled answers weighted by their likelihoods, capturing both semantic similarity and model confidence in a single metric.
- **Core assumption:** BLEU similarity between samples reflects semantic equivalence, and averaging over all pairs captures the "spread" of the answer distribution.
- **Evidence anchors:** The paper proposes AVG BLEU as a novel calibration score combining sampling and likelihood methods, though the exact implementation details are not fully specified.

## Foundational Learning

- **Concept: Expected Calibration Error (ECE)**
  - Why needed here: ECE quantifies how well predicted confidence scores match actual accuracy, a core metric for evaluating calibration.
  - Quick check question: If a model has ECE of 0.1, what does that mean in terms of calibration quality?

- **Concept: Coverage@Acc**
  - Why needed here: This metric measures how often a calibrated model can answer while maintaining a target accuracy, directly relevant to selective QA.
  - Quick check question: If C@70 = 30, what percentage of the most confident predictions are at least 70% accurate?

- **Concept: Sampling-based uncertainty estimation**
  - Why needed here: Methods like repetition and diversity rely on generating multiple outputs to estimate model uncertainty, central to the paper's comparison.
  - Quick check question: How does the repetition method compute its confidence score from sampled outputs?

## Architecture Onboarding

- **Component map:** Image + Question (or caption + question for LLMs) -> LMM/LLM -> Sampling (10 responses, temperature 0.7) -> Scoring (Likelihood/Repetition/Diversity/AVG BLEU) -> Thresholding (Compare to τ) -> Trigger/Abstain

- **Critical path:** 1) Generate multiple sampled answers, 2) Compute calibration score (AVG BLEU preferred), 3) Compare to threshold, 4) Trigger if above threshold, else abstain

- **Design tradeoffs:**
  - Sampling temperature: Higher → more diversity but noisier samples
  - Number of samples: More → better uncertainty estimate but higher latency
  - Similarity metric: BLEU is fast but may miss semantic equivalence; BEM/BLEURT are slower but more accurate

- **Failure signatures:**
  - High ECE despite good accuracy → scores poorly calibrated
  - Low coverage at target accuracy → model too conservative
  - Sampling methods outperform likelihood on LLMs → consider multimodal grounding

- **First 3 experiments:**
  1. Run LLaVa on VizWiz-VQA with likelihood scoring only, measure ECE and coverage@70
  2. Run same model with AVG BLEU scoring, compare improvements
  3. Repeat with LLM (PaLM-2) using gold captions, verify sampling superiority over likelihood

## Open Questions the Paper Calls Out
The paper highlights the need for more analysis and datasets to further research in this space, particularly regarding how calibration methods perform across different VQA scenarios and whether findings generalize beyond the specific datasets studied.

## Limitations
- The study focuses on in-context learning rather than fine-tuned approaches, limiting applicability to production systems
- Evaluation uses specific datasets (VizWiz-VQA and UNK-VQA) that may not represent all VQA scenarios
- Exact implementation details for AVG BLEU computation are underspecified, particularly regarding BLEU smoothing methods

## Confidence
- **High confidence:** LMMs are better calibrated than LLMs when using likelihood scoring
- **Medium confidence:** AVG BLEU significantly improves coverage metrics (5 points at 80% accuracy for LMMs, 8 points at 70% accuracy for LLMs)
- **Medium confidence:** Sampling-based methods are superior to likelihood for LLMs, though differences between repetition and diversity are not statistically compared

## Next Checks
1. **Reproduce the exact 4-shot prompts:** Implement the exact prompting examples shown in the paper for both LMMs and LLMs, ensuring the format and examples match precisely. Run a small-scale test on 100 validation examples and verify that the calibration scores align with reported ranges.

2. **Verify AVG BLEU implementation:** Implement AVG BLEU using the same BLEU version and smoothing method as the paper (likely SacreBLEU with standard settings). Compare its performance against likelihood and repetition methods on a single model-dataset pair to confirm the reported improvement pattern.

3. **Test on alternative datasets:** Apply the best-performing method (AVG BLEU with LMM) to a different VQA dataset like VQAv2 or GQA to verify whether the calibration improvements generalize beyond the VizWiz and UNK-VQA datasets used in the study.