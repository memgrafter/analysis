---
ver: rpa2
title: 'Quality of Answers of Generative Large Language Models vs Peer Patients for
  Interpreting Lab Test Results for Lay Patients: Evaluation Study'
arxiv_id: '2402.01693'
source_url: https://arxiv.org/abs/2402.01693
tags:
- responses
- questions
- test
- answers
- patients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the feasibility of using large language models
  (LLMs) to generate responses to patients' questions about laboratory test results.
  The researchers collected 53 question-answer pairs from Yahoo!
---

# Quality of Answers of Generative Large Language Models vs Peer Patients for Interpreting Lab Test Results for Lay Patients: Evaluation Study

## Quick Facts
- arXiv ID: 2402.01693
- Source URL: https://arxiv.org/abs/2402.01693
- Authors: Zhe He; Balu Bhasuran; Qiao Jin; Shubo Tian; Karim Hanna; Cindy Shavor; Lisbeth Garcia Arguello; Patrick Murray; Zhiyong Lu
- Reference count: 0
- Primary result: GPT-4 significantly outperformed other LLMs and human responses in generating lab test interpretation answers

## Executive Summary
This study evaluates the feasibility of using large language models (LLMs) to generate responses to patient questions about laboratory test results. Researchers compared responses from four LLMs (GPT-4, LLaMA 2, MedAlpaca, ORCA_mini) against human responses from Yahoo! Answers, using both automated evaluation metrics and medical expert review. Results showed GPT-4 achieved significantly better scores than other LLMs and human responses across four evaluation dimensions: relevance, correctness, helpfulness, and safety. However, LLM responses occasionally lacked medical context, contained incorrect statements, and lacked references, indicating areas for improvement.

## Method Summary
The study collected 53 question-answer pairs from Yahoo! Answers and used the LangChain framework to generate responses using four LLMs. Responses were evaluated using automated metrics (BLEU, METEOR, ROUGE, BERTScore) measuring similarity to reference responses, and manual evaluation by medical experts assessing relevance, correctness, helpfulness, and safety. The study compared LLM performance against human responses across these evaluation dimensions to determine the feasibility of using LLMs for lab test interpretation.

## Key Results
- GPT-4 responses achieved significantly better scores than other LLMs and human responses on all four evaluation aspects
- LLM responses occasionally lacked medical context and contained incorrect statements
- Manual expert evaluation identified safety concerns and missing references in LLM responses
- Automated metrics showed GPT-4 responses were more similar to human responses than other LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's responses are more accurate, helpful, relevant, and safer because they use more comprehensive language generation and context understanding than other LLMs.
- Mechanism: GPT-4 leverages advanced training with reinforcement learning feedback from both human and AI, allowing it to produce more coherent, contextually relevant, and grammatically correct responses. It also tends to generate longer, more detailed answers that better address patients' questions.
- Core assumption: GPT-4's larger model size and more diverse training data allow it to generate more accurate and comprehensive responses to patient questions about lab results.
- Evidence anchors:
  - [abstract]: "Results showed that GPT-4's responses achieved significantly better scores than the other LLMs and human responses on all four aspects."
  - [section]: "GPT-4 [36] is the fourth generation Generative Pre-trained Transformer (GPT) model from OpenAI. GPT-4 is a large-scale, multimodal large language model developed using reinforcement learning feedback from both human and AI."
  - [corpus]: Weak - the corpus contains papers on LLM evaluation and healthcare Q&A, but none specifically compare GPT-4 to other LLMs for lab test interpretation. One paper mentions GPT-4 provides unsafe answers to patient-posed medical questions.
- Break condition: If the patient's question requires highly specialized medical knowledge or involves complex clinical context, GPT-4's general-purpose training may not be sufficient to provide accurate responses.

### Mechanism 2
- Claim: The automated evaluation metrics (BLEU, METEOR, ROUGE, BERTScore) effectively assess the similarity between LLM responses and human responses, allowing for quantitative comparison of response quality.
- Mechanism: These metrics compute similarity scores based on n-gram overlap, string matching, or semantic similarity of token embeddings between the LLM-generated response and a reference response (either human or another LLM). Higher scores indicate greater similarity and thus higher quality.
- Core assumption: The similarity between an LLM response and a human response is a valid proxy for the quality and usefulness of the LLM response for patients.
- Evidence anchors:
  - [section]: "We first assessed the similarity of their answers using standard QA similarity-based evaluation metrics including ROUGE, BLEU, METEOR, BERTScore."
  - [section]: "All the metrics range from 0.0 to 1.0, where a higher score indicates the LLM-generated answers are similar to the ground truth whereas a lower score suggests otherwise."
  - [corpus]: Weak - the corpus contains papers on automated evaluation of AI responses to patient questions, but none specifically use these particular metrics for lab test interpretation.
- Break condition: If the human responses used as reference are of poor quality (short, incomplete, inaccurate), then a high similarity score may not indicate a high-quality LLM response.

### Mechanism 3
- Claim: Medical expert evaluation provides a reliable assessment of the relevance, correctness, helpfulness, and safety of LLM responses for patients.
- Mechanism: Medical experts manually review and rate LLM responses using a standardized rubric, providing qualitative feedback on the strengths and weaknesses of each response. This human evaluation can identify issues that automated metrics may miss, such as the presence of inaccurate information or lack of medical context.
- Core assumption: Medical experts can accurately judge the quality of LLM responses in terms of their relevance, correctness, helpfulness, and safety for patients.
- Evidence anchors:
  - [section]: "We performed a manual evaluation with medical experts for all the responses to seven selected questions on the same four aspects."
  - [section]: "The medical experts also identified inaccurate information with LLM responses."
  - [corpus]: Weak - the corpus contains papers on LLM evaluation for healthcare Q&A, but none specifically describe medical expert evaluation of lab test interpretation responses.
- Break condition: If the medical experts have limited experience with LLM-generated responses or are not representative of the target patient population, their evaluations may not accurately reflect the usefulness of the responses for patients.

## Foundational Learning

- Concept: Question classification
  - Why needed here: To identify relevant questions about lab test interpretation from a large dataset of patient questions.
  - Quick check question: How would you train a classifier to distinguish between questions about lab test results and other types of health questions?

- Concept: Prompt engineering
  - Why needed here: To craft effective prompts that elicit accurate and helpful responses from LLMs about lab test results.
  - Quick check question: What information should be included in a prompt to help an LLM generate a useful response about a patient's lab test results?

- Concept: Response evaluation
  - Why needed here: To assess the quality and safety of LLM-generated responses for patients.
  - Quick check question: What criteria would you use to evaluate whether an LLM response to a patient's question about lab test results is accurate, helpful, and safe?

## Architecture Onboarding

- Component map:
  Data collection -> Question classification -> Response generation -> Automated evaluation -> Manual evaluation

- Critical path:
  1. Collect and preprocess dataset of patient questions about lab test results
  2. Train and apply question classifiers to identify relevant questions
  3. Generate responses to selected questions using four LLMs
  4. Evaluate responses using automated metrics and manual expert review
  5. Analyze results and identify areas for improvement

- Design tradeoffs:
  - Using human responses as reference for automated evaluation may introduce bias if the human responses are of low quality
  - Manual expert evaluation is time-consuming and may not be scalable for large datasets
  - LLMs may generate responses that are too technical or verbose for patients to understand

- Failure signatures:
  - Automated evaluation metrics produce high scores but manual expert review identifies significant issues with response quality
  - LLM responses consistently miss important clinical context or provide inaccurate information
  - Manual expert evaluation reveals low inter-rater reliability, indicating unclear evaluation criteria

- First 3 experiments:
  1. Compare the performance of different question classifiers (BioBERT, ClinicalBERT, SciBERT, PubMedBERT) on a held-out test set of lab test interpretation questions.
  2. Evaluate the impact of different prompt engineering strategies (e.g., role-prompting, chain-of-thought) on the quality of LLM-generated responses.
  3. Conduct a user study to assess patient comprehension and satisfaction with LLM-generated responses compared to human responses or no response.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of LLM-generated responses vary across different types of lab tests (e.g., CBC, metabolic panel, thyroid function, lipid panel)?
- Basis in paper: [explicit] The paper mentions that responses were evaluated for different lab test panels, but does not provide detailed analysis of quality variation across test types.
- Why unresolved: The paper provides overall quality metrics but does not break down performance by specific lab test categories, which could reveal areas where LLMs perform better or worse.
- What evidence would resolve it: A detailed analysis comparing LLM performance metrics across different categories of lab tests would provide insights into where LLMs excel or struggle.

### Open Question 2
- Question: What is the optimal balance between automated and human evaluation in assessing the quality of LLM responses for lab test interpretation?
- Basis in paper: [inferred] The paper uses both automated metrics and human expert evaluation, suggesting that both methods have value but the optimal combination is not explored.
- Why unresolved: The paper demonstrates the use of both methods but does not investigate how different combinations or weighting of automated and human evaluation might affect the overall assessment.
- What evidence would resolve it: A study comparing different ratios of automated to human evaluation in assessing LLM responses could determine the most effective balance for quality assessment.

### Open Question 3
- Question: How can LLM responses be improved to provide more individualized and context-specific interpretations of lab test results?
- Basis in paper: [explicit] The paper notes that LLM responses occasionally lack individualized interpretation and medical context.
- Why unresolved: While the paper identifies this limitation, it does not explore specific strategies or approaches to enhance the personalization of LLM responses.
- What evidence would resolve it: Developing and testing methods to incorporate patient-specific data or context into LLM prompts and responses could demonstrate improvements in individualized interpretation.

## Limitations

- The study relies on human responses from Yahoo! Answers as reference points, which may contain inaccuracies or be unrepresentative of expert medical advice
- Automated evaluation metrics measure similarity to reference responses rather than actual medical accuracy, potentially overestimating LLM performance
- Manual evaluation was limited to only 7 out of 53 questions, which may not capture the full range of potential issues across all responses

## Confidence

- High confidence: GPT-4 demonstrates superior performance compared to other LLMs and human responses across all evaluation metrics
- Medium confidence: Automated evaluation metrics provide meaningful quantitative comparisons of response quality
- Medium confidence: Medical expert evaluation effectively identifies issues with response safety and medical context
- Low confidence: The study's findings generalize to all types of lab test interpretation questions and diverse patient populations

## Next Checks

1. Conduct a larger-scale study with more questions and responses evaluated by multiple medical experts to validate the consistency of findings across different clinical scenarios
2. Compare LLM responses against standardized medical guidelines rather than human responses to establish a more objective ground truth for medical accuracy
3. Implement a user study with actual patients to assess comprehension, satisfaction, and potential risks of using LLM-generated responses for lab test interpretation