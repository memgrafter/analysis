---
ver: rpa2
title: Joint Selective State Space Model and Detrending for Robust Time Series Anomaly
  Detection
arxiv_id: '2405.19823'
source_url: https://arxiv.org/abs/2405.19823
tags:
- time
- series
- anomaly
- detection
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses two key challenges in time series anomaly
  detection: modeling long-range dependencies and handling non-stationary data with
  trends. To tackle these, the authors propose a detector based on the Selective State
  Space Model (S6) for effective long-term dependency modeling and a multi-stage detrending
  mechanism to mitigate the impact of trends in non-stationary data.'
---

# Joint Selective State Space Model and Detrending for Robust Time Series Anomaly Detection

## Quick Facts
- arXiv ID: 2405.19823
- Source URL: https://arxiv.org/abs/2405.19823
- Reference count: 40
- Outperforms 12 baselines with 6.2% relative improvement in F1-AF score

## Executive Summary
This paper addresses two key challenges in time series anomaly detection: modeling long-range dependencies and handling non-stationary data with trends. The authors propose a detector based on the Selective State Space Model (S6) for effective long-term dependency modeling and a multi-stage detrending mechanism to mitigate the impact of trends in non-stationary data. The S6 model's selective nature helps discard abnormal information while the detrending mechanism stabilizes input for the sequence model. Extensive experiments on three benchmark datasets (NASA, SWaT, and SMD) demonstrate that the proposed method outperforms 12 baseline methods, achieving a 6.2% relative improvement in average F1-AF score compared to the best baseline.

## Method Summary
The proposed method combines an S6-based sequence model with a multi-stage detrending mechanism for multivariate time series anomaly detection. It first applies an HP trend filter to extract and remove trend components, then processes the detrended signal through multiple Decomposition-based Mamba (DMamba) blocks containing S6 models with Adaptive MA modules. The S6 model uses time-dependent parameters to selectively filter abnormal information while capturing long-term dependencies. The Adaptive MA module estimates local periodicity via Fourier analysis to refine detrending. The final output combines reconstructed signals with trend components for anomaly detection using MSE-based scoring and POT threshold selection.

## Key Results
- Achieves 6.2% relative improvement in average F1-AF score compared to best baseline
- Outperforms 12 baseline methods across three benchmark datasets (NASA, SWaT, SMD)
- Ablation studies confirm necessity of both HP trend filter and Adaptive MA mechanism
- Demonstrates effective handling of long-range dependencies and non-stationary trends

## Why This Works (Mechanism)

### Mechanism 1
- Claim: S6 model's selective nature discards abnormal information while capturing long-term dependencies.
- Mechanism: The S6 model uses time-dependent parameters (Δ, B, C) to selectively attend to normal context, reducing influence of anomalies during reconstruction.
- Core assumption: Anomalies are sparse enough that selective filtering improves reconstruction fidelity for normal patterns.
- Evidence anchors:
  - [abstract] states "the selective nature of the S6 model allows it to discard abnormal information and generate reliable reconstructed output."
  - [section II] contrasts S6 with other models, noting its "input-dependent parameters" that remove LTI constraints.
  - [corpus] evidence is missing direct comparisons of S6 selective filtering performance.
- Break condition: If anomalies are dense or highly structured, selective filtering may discard useful context.

### Mechanism 2
- Claim: Multi-stage detrending stabilizes input for the sequence model by removing trend components before modeling.
- Mechanism: HP trend filter extracts trend component τHP, which is subtracted from input; subsequent AMA modules adaptively estimate local periods to refine detrending.
- Core assumption: Trends dominate variance in non-stationary data and mask anomalies if not removed.
- Evidence anchors:
  - [abstract] explains "multi-stage detrending mechanism...to mitigate the prominent trend component in non-stationary data."
  - [section IV-A] describes HP trend filter extraction of τHP and sHP.
  - [corpus] lacks ablation results showing detrending impact in isolation.
- Break condition: If data has weak trends or seasonality dominates, detrending may remove meaningful signal.

### Mechanism 3
- Claim: Adaptive MA kernel size estimation aligns detrending window with local periodicity, improving anomaly detection.
- Mechanism: AMA computes kernel size k = ⌊W/n⌋ where n is peak position of power density function of SSM output.
- Core assumption: Local periodicity is stable enough for adaptive estimation to improve detrending accuracy.
- Evidence anchors:
  - [section IV-B] describes AMA using Fourier analysis to estimate peak position for kernel size.
  - [section V-B] ablation study shows performance drops without both HP filter and AMA.
  - [corpus] evidence missing direct comparison of fixed vs adaptive MA.
- Break condition: If periodicity varies rapidly or signal is non-periodic, adaptive estimation fails.

## Foundational Learning

- Concept: Time series decomposition (trend, seasonality, residual).
  - Why needed here: Enables separation of long-term patterns from anomalies; foundational to detrending strategy.
  - Quick check question: What are the three components of classical time series decomposition?

- Concept: State Space Models (SSM) and linear time invariance.
  - Why needed here: S6 extends SSM; understanding LTI vs time-variant parameters explains selective capability.
  - Quick check question: How does S6 differ from standard SSM in parameter dynamics?

- Concept: Anomaly detection metrics (precision, recall, F1-AF).
  - Why needed here: Evaluation relies on affiliation-based metrics to handle point-adjustment issues.
  - Quick check question: Why are affiliation-based metrics preferred over traditional ranking in TSAD?

## Architecture Onboarding

- Component map: HP Trend Filter → DMamba Blocks (S6 + AMA) → Output Module (reconstruction + trend fusion)
- Critical path: Input → HP detrend → Emb → DMamba stack → SSM + AMA → trend fusion → reconstruction
- Design tradeoffs:
  - S6 vs Transformer: lower memory, linear time, but requires careful detrending for non-stationary data.
  - Fixed vs adaptive MA: adaptive improves robustness but adds Fourier computation overhead.
- Failure signatures:
  - Poor anomaly localization → detrending over-removal or under-removal.
  - High false positives → insufficient selective filtering in S6.
  - Unstable training → mis-specified discretization rules in S6 discretization step.
- First 3 experiments:
  1. Ablation: HP filter only vs AMA only vs both vs none on NASA dataset.
  2. S6 vs Transformer backbone on SWaT with identical detrending.
  3. Fixed MA kernel vs adaptive MA on SMD dataset with trend variation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the S6 model's selective nature specifically improve anomaly detection compared to other sequence models like RNNs or Transformers in TSAD tasks?
- Basis in paper: [explicit] The paper states that the selective nature of the S6 model allows it to discard abnormal information and generate reliable reconstructed output for TSAD.
- Why unresolved: The paper does not provide a detailed comparison of the S6 model's selective nature with other sequence models in terms of anomaly detection performance.
- What evidence would resolve it: A comprehensive experimental comparison of the S6 model with other sequence models, focusing on their selective mechanisms and their impact on anomaly detection performance.

### Open Question 2
- Question: What is the optimal kernel size for the Adaptive MA (AMA) mechanism, and how does it vary with different time series datasets?
- Basis in paper: [explicit] The paper mentions that the kernel size is adaptively estimated by k = ⌊W/n⌋, but it does not provide specific optimal values for different datasets.
- Why unresolved: The paper does not provide a detailed analysis of how the optimal kernel size varies with different time series datasets or characteristics.
- What evidence would resolve it: A systematic study of the optimal kernel size for the AMA mechanism across various time series datasets with different characteristics, such as trend strength, seasonality, and noise levels.

### Open Question 3
- Question: How does the proposed method handle multivariate time series with complex interdependencies and correlations between different features?
- Basis in paper: [inferred] The paper mentions that the method is applied to multivariate time series datasets, but it does not provide a detailed analysis of how it handles complex interdependencies and correlations between different features.
- Why unresolved: The paper does not provide a detailed explanation of how the proposed method captures and utilizes the interdependencies and correlations between different features in multivariate time series.
- What evidence would resolve it: A comprehensive analysis of the proposed method's performance on multivariate time series datasets with varying degrees of feature interdependencies and correlations, along with a comparison to other methods that explicitly model these relationships.

## Limitations

- Claims about S6's selective filtering capability lack direct ablation evidence for individual mechanisms.
- Adaptive MA component's kernel estimation assumes stable local periodicity, which may not hold for highly irregular signals.
- Implementation details for S6 discretization rules and embedding network architecture are underspecified.
- Evaluation focuses on F1-AF metrics without reporting traditional precision/recall tradeoffs or computational efficiency comparisons.

## Confidence

- **High Confidence**: Claims about overall performance improvement (6.2% F1-AF gain) and the necessity of combining HP filter with AMA (based on ablation study showing performance drops when either component is removed).
- **Medium Confidence**: Claims about S6's selective nature improving reconstruction fidelity and the specific mechanism of AMA kernel size estimation via Fourier analysis - these have theoretical justification but limited empirical isolation studies.
- **Low Confidence**: Claims about S6's superiority over Transformer backbones (no direct comparison provided) and the generalizability of results to datasets outside the three benchmarks.

## Next Checks

1. **Mechanism Isolation Study**: Run controlled experiments on NASA dataset comparing: (a) HP filter only, (b) AMA only, (c) S6 without detrending, (d) Full proposed method - to quantify individual contributions of each component to F1-AF improvement.

2. **Cross-dataset Generalization Test**: Evaluate the method on additional non-stationary datasets (e.g., Yahoo Webscope, ECG) with varying trend characteristics to assess whether the 6.2% improvement generalizes beyond the three benchmark datasets.

3. **Computational Efficiency Benchmark**: Measure and compare training/inference time and memory usage between S6-based detector and Transformer-based alternatives on identical hardware, particularly for long sequence lengths where S6's linear complexity advantage should be most pronounced.