---
ver: rpa2
title: Biomedical Entity Linking as Multiple Choice Question Answering
arxiv_id: '2402.15189'
source_url: https://arxiv.org/abs/2402.15189
tags:
- entities
- entity
- answer
- bioelqa
- instances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenges of biomedical entity linking
  (BioEL), particularly for fine-grained and long-tailed entities. The authors propose
  BioELQA, a novel approach that treats BioEL as a multiple-choice question answering
  (MCQA) task.
---

# Biomedical Entity Linking as Multiple Choice Question Answering

## Quick Facts
- arXiv ID: 2402.15189
- Source URL: https://arxiv.org/abs/2402.15189
- Reference count: 0
- Accuracy: 93.5% on NCBI, 94.5% on BC5CDR, 85.2% on COMETA datasets

## Executive Summary
This paper addresses the challenges of biomedical entity linking (BioEL), particularly for fine-grained and long-tailed entities. The authors propose BioELQA, a novel approach that treats BioEL as a multiple-choice question answering (MCQA) task. The method first retrieves candidate entities using a fast retriever, then presents the mention and candidate entities to a generator, which outputs the predicted symbol associated with the chosen entity. This formulation enables explicit comparison of different candidate entities, capturing fine-grained interactions between mentions and entities, as well as among entities themselves. To improve generalization for long-tailed entities, the authors retrieve similar labeled training instances as clues and concatenate the input with retrieved instances for the generator.

## Method Summary
BioELQA reformulates biomedical entity linking as a multiple-choice question answering task. It uses a retriever (SapBERT-based bi-encoder) to select candidate entities from an ontology, then presents the mention and candidates to a T5-base generator in MCQA format. The generator outputs a symbol corresponding to the predicted entity. A kNN module retrieves similar training instances to provide contextual clues for long-tailed entities. The method employs data augmentation by randomly ordering answer options during training to prevent position-based memorization.

## Key Results
- Achieves 93.5% accuracy on NCBI dataset
- Achieves 94.5% accuracy on BC5CDR dataset
- Achieves 85.2% accuracy on COMETA dataset
- Outperforms state-of-the-art baselines across all benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reformulating BioEL as MCQA enables explicit comparison of candidate entities
- Mechanism: By presenting the mention and candidate entities as a multiple-choice prompt, the model can directly model both mention-entity and entity-entity interactions simultaneously
- Core assumption: The generator can effectively learn to compare entities when they are explicitly presented as answer options
- Evidence anchors:
  - [abstract]: "This formulation enables explicit comparison of different candidate entities, thus capturing fine-grained interactions between mentions and entities, as well as among entities themselves."
  - [section]: "In MCP, a mention and its symbol-enumerated candidate answers are all passed to the generator as a single prompt, explicitly modeling both mention-entity and entity-entity interactions."
- Break condition: If the model cannot effectively learn the association between symbols and answers rather than just memorizing answer positions, the explicit comparison benefit would be lost

### Mechanism 2
- Claim: kNN module improves generalization for long-tailed entities
- Mechanism: Retrieving similar training instances provides contextual clues that help the model handle rare or morphologically similar entities
- Core assumption: Similar instances from training data contain useful information that can guide predictions for unseen long-tailed entities
- Evidence anchors:
  - [abstract]: "To improve generalization for long-tailed entities, we retrieve similar labeled training instances as clues and concatenate the input with retrieved instances for the generator."
  - [section]: "we introduce a kNN module to enhance the model's generalization capabilities for long-tailed entities, enabling it to reference similar instances from the entire training corpus as prediction clues."
  - [section]: "incorporating the kNN module allows the model to leverage related instances from the training set, enabling more informative decisions by referencing these instances."
- Break condition: If retrieved instances are not sufficiently relevant or the generator cannot effectively integrate this additional context

### Mechanism 3
- Claim: Data augmentation through random answer option ordering improves model robustness
- Mechanism: By training with different orderings of answer options, the model learns to associate symbols with answers rather than memorizing positions
- Core assumption: The generator can learn the association between symbols and answers regardless of their position
- Evidence anchors:
  - [section]: "we employ a simple yet effective data augmentation strategy during training. By randomly swapping the order of O, we can construct different (T(m, O), a) training pairs."
  - [section]: "This strategy allows the model to learn the association between symbols and answers, rather than solely memorizing answer positions."
- Break condition: If the model still learns to rely on answer position despite the augmentation strategy

## Foundational Learning

- Concept: Entity linking and its challenges in biomedical domain
  - Why needed here: Understanding why BioEL is challenging (fine-grained entities, long-tailed distribution, morphological variations) is crucial for appreciating the proposed solution
  - Quick check question: What makes biomedical entity linking more challenging than general domain entity linking?

- Concept: Multiple-choice question answering formulation
  - Why needed here: The core innovation is reframing BioEL as MCQA, so understanding how MCQA works is essential
  - Quick check question: How does presenting candidate entities as answer options enable explicit comparison?

- Concept: Retrieval-augmented learning
  - Why needed here: The kNN module is a key component for handling long-tailed entities
  - Quick check question: How does retrieving similar training instances help the model generalize to unseen entities?

## Architecture Onboarding

- Component map:
  - Retriever (bi-encoder based on SapBERT) → Candidate entity selection
  - kNN module → Retrieval of similar training instances
  - Multiple-choice prompt generator → Combines mention, candidates, and similar instances
  - T5-based generator → Produces answer symbol
  - Output layer → Maps symbol to final entity

- Critical path: Retriever → kNN module → Multiple-choice prompt → Generator → Output

- Design tradeoffs:
  - Using symbol generation vs. entity name generation: Symbol generation avoids generating invalid entities but requires additional mapping step
  - Number of candidates (N) vs. accuracy: More candidates increase recall but may introduce noise
  - Number of similar instances (K) vs. generalization: More instances provide more context but may confuse the model

- Failure signatures:
  - Poor candidate retrieval → Incorrect answer options provided to generator
  - Irrelevant similar instances → Generator confused by noise in prompt
  - Model overfits to answer positions → Performance degrades with different answer orderings

- First 3 experiments:
  1. Verify retriever accuracy on a small validation set
  2. Test kNN module retrieval relevance on sample mentions
  3. Validate multiple-choice prompt format with a simple T5 model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BioELQA compare when using different generator backbones, such as BERT, RoBERTa, or other variants of T5?
- Basis in paper: [explicit] The paper mentions using T5 as the generator backbone but does not explore other options.
- Why unresolved: The paper focuses on T5 and does not provide a comparative analysis with other transformer models.
- What evidence would resolve it: Experimental results comparing the performance of BioELQA with different generator backbones on the same datasets.

### Open Question 2
- Question: What is the impact of varying the number of candidate entities (N) and similar instances (K) on the performance of BioELQA in different biomedical domains?
- Basis in paper: [explicit] The paper mentions that the accuracy initially improves and then declines as N and K increase, but does not explore domain-specific impacts.
- Why unresolved: The paper does not provide a detailed analysis of how these hyperparameters affect performance across different biomedical domains.
- What evidence would resolve it: Detailed experiments showing the performance of BioELQA with different values of N and K across various biomedical domains.

### Open Question 3
- Question: How does BioELQA handle contextual information, and can incorporating context improve its performance on ambiguous mentions?
- Basis in paper: [explicit] The paper mentions that BioELQA does not consider contextual information, which is a limitation.
- Why unresolved: The paper does not explore methods to incorporate context or evaluate the potential benefits of doing so.
- What evidence would resolve it: Experimental results comparing the performance of BioELQA with and without contextual information on datasets with ambiguous mentions.

## Limitations
- Computational overhead from retrieving both candidate entities and similar training instances
- Potential scalability issues with large ontologies
- Inherits T5's token limit constraints, which may affect performance on very long mentions

## Confidence
- BioELQA formulation effectiveness: Medium
- kNN module for long-tailed entities: High
- Data augmentation strategy effectiveness: Low

## Next Checks
1. **Ablation study on MCQA formulation**: Compare BioELQA against a direct generation baseline (generating entity names instead of symbols) while keeping all other components constant to isolate the MCQA benefit.

2. **Long-tail entity analysis**: Analyze performance stratified by entity frequency in the training data to quantify the kNN module's impact on different tail segments and identify where it provides diminishing returns.

3. **Computational efficiency profiling**: Measure the trade-off between accuracy gains and inference time when varying the number of candidate entities (N) and retrieved similar instances (K) to identify optimal configurations for different deployment scenarios.