---
ver: rpa2
title: On the Use of Audio to Improve Dialogue Policies
arxiv_id: '2410.13385'
source_url: https://arxiv.org/abs/2410.13385
tags:
- dialogue
- audio
- system
- architecture
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses improving dialogue policy performance in goal-oriented
  spoken dialogue systems by incorporating audio information. The core method idea
  is to use a Double Multi-Head Attention component to combine speech and text embeddings,
  allowing the model to capture complex relationships between the two modalities.
---

# On the Use of Audio to Improve Dialogue Policies

## Quick Facts
- arXiv ID: 2410.13385
- Source URL: https://arxiv.org/abs/2410.13385
- Authors: Daniel Roncel; Federico Costa; Javier Hernando
- Reference count: 0
- This paper addresses improving dialogue policy performance in goal-oriented spoken dialogue systems by incorporating audio information.

## Executive Summary
This paper presents a novel architecture for improving dialogue policy performance in goal-oriented spoken dialogue systems by incorporating audio information alongside text. The proposed approach uses pre-trained self-supervised models (GPT-2 for text and Wav2Vec2.0, HuBERT, UniSpeechSAT, or WavLM for speech) to extract representations, which are then fused using a Double Multi-Head Attention component. The architecture is evaluated on the DSTC2 dataset, demonstrating significant improvements over text-only baselines, particularly in capturing extralinguistic information embedded in user's speech.

## Method Summary
The proposed architecture uses pre-trained self-supervised models to extract text and audio embeddings, which are then fused using a Double Multi-Head Attention (Double MHA) component. The method employs a weighted sum over all transformer layers for both text (from GPT-2) and speech (from one of the audio models) to create optimal layer combinations. The Double MHA performs early fusion of multimodal representations, transforming them into complementary contextualized representations that are then pooled into a single vector for the linear predictor. The model is trained as a classifier to predict dialogue acts and evaluated using the User Request Score (URS) metric on the DSTC2 dataset.

## Key Results
- The proposed architecture achieves a 9.8% relative improvement in User Request Score compared to text-only baselines
- Audio embedding-aware dialogue policies outperform text-based ones, particularly in noisy transcription scenarios
- The architecture shows robust performance across different pre-trained speech models (Wav2Vec2.0, HuBERT, UniSpeechSAT, WavLM)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Double Multi-Head Attention allows the model to jointly attend to information from different representation subspaces, enabling complementary contextualized representations from speech and text.
- Mechanism: The first MHA layer takes T1 speech and T2 text representations and outputs T multimodal contextualized vectors. The second attention layer pools these into a single vector for the linear predictor.
- Core assumption: That the attention scores computed across modality-specific subspaces will effectively capture the complementary aspects of speech and text.
- Evidence anchors:
  - [abstract] "Our proposed architecture outperforms text-only architectures achieving a 9.8% User Request Score relative improvement."
  - [section] "First, a Multi-Head Attention (MHA) layer transforms multimodal representations into complementary contextualized representations."
- Break condition: If attention weights collapse to trivial values or fail to learn meaningful cross-modal relationships.

### Mechanism 2
- Claim: Weighted sum over all transformer layers (instead of selecting a subset) allows the model to learn optimal layer combinations for both text and speech representations.
- Mechanism: Instead of selecting specific transformer layers, every transformer layer is considered by performing a weighted sum. The weights are learned during training.
- Core assumption: That different layers capture different aspects of linguistic/speech information and that the model can learn optimal combinations.
- Evidence anchors:
  - [abstract] "speech and text representations are extracted using pre-trained self-supervised models."
  - [section] "Instead of selecting a subset of transformer layers, every transformer layer is considered by performing a weighted sum."
- Break condition: If layer weights converge to uniform values or the weighted sum fails to improve over simple selection.

### Mechanism 3
- Claim: Early fusion of multimodal representations through Double MHA is more effective than late fusion (concatenation) for dialogue policy tasks.
- Mechanism: Speech and text embeddings are combined at the embedding level using Double MHA, rather than being concatenated at the linear predictor input.
- Core assumption: That cross-modal interaction at the embedding level provides better information integration than simple concatenation.
- Evidence anchors:
  - [abstract] "audio embedding-aware dialogue policies outperform text-based ones, particularly in noisy transcription scenarios"
  - [section] "Speech and text representations are fused using a Double Multi-Head Attention component."
- Break condition: If early fusion doesn't improve performance over simple concatenation baselines.

## Foundational Learning

- Concept: Attention mechanisms in transformer architectures
  - Why needed here: Understanding how MHA layers compute attention scores across modalities is fundamental to grasping the proposed architecture
  - Quick check question: How does a Multi-Head Attention layer differ from a single attention head, and why is this beneficial for multimodal fusion?

- Concept: Self-supervised pre-training for speech and text
  - Why needed here: The architecture relies on pre-trained models (GPT-2, Wav2Vec2.0, HuBERT, UniSpeechSAT, WavLM) to extract representations
  - Quick check question: What are the key differences between Wav2Vec2.0, HuBERT, and WavLM, and how might these affect their suitability for dialogue policy tasks?

- Concept: Dialogue act classification and user request scoring
  - Why needed here: The model is trained as a classifier to predict dialogue acts, and performance is evaluated using User Request Score
  - Quick check question: What is the difference between slot-filling and dialogue act prediction in goal-oriented dialogue systems?

## Architecture Onboarding

- Component map:
  - Pre-trained speech model (Wav2Vec2.0/HuBERT/UniSpeechSAT/WavLM) → Audio embeddings
  - Pre-trained text model (GPT-2) → Text embeddings
  - Double Multi-Head Attention component → Fused multimodal representations
  - Linear predictor with softmax → Dialogue act probability distribution

- Critical path: Audio input → Audio embedding model → Weighted layer sum → Text input → GPT-2 → Weighted layer sum → Double MHA fusion → Linear predictor → Dialogue act prediction

- Design tradeoffs:
  - Early fusion (Double MHA) vs late fusion (concatenation): Early fusion enables cross-modal interaction but adds complexity
  - Weighted layer sum vs layer selection: Weighted sum provides flexibility but requires more parameters to learn
  - Pre-trained frozen models vs fine-tuning: Using frozen models reduces training complexity but may limit adaptation to dialogue-specific features

- Failure signatures:
  - Attention weights collapsing to uniform values indicating poor cross-modal learning
  - Performance similar to text-only baseline suggesting fusion isn't adding value
  - Overfitting to training data with poor generalization to test set
  - Layer weights converging to extreme values (all weight on single layer)

- First 3 experiments:
  1. Compare architecture with and without Double MHA component to isolate the impact of early fusion
  2. Test different combinations of speech models (Wav2Vec2.0, HuBERT, UniSpeechSAT, WavLM) to identify best performer
  3. Evaluate the impact of weighted layer sum vs selecting top-k layers for both speech and text representations

## Open Questions the Paper Calls Out
- How does the performance of the proposed architecture scale with the number of dialogue turns used as input (currently 9)?
- How robust is the proposed architecture to different types of speech noise and distortions?
- How does the proposed architecture perform on dialogue datasets in languages other than English?

## Limitations
- Lack of ablation studies that isolate the contribution of each architectural component
- Limited evaluation to a single dataset (DSTC2) raises questions about generalizability
- No detailed analysis of how the model behaves under different noise levels or with varying quality of ASR transcriptions

## Confidence
- **High Confidence**: The architectural framework using pre-trained models with weighted layer sums is well-established and the implementation approach is technically sound.
- **Medium Confidence**: The claim that Double Multi-Head Attention specifically enables better cross-modal learning is supported by results but lacks detailed analysis of attention patterns.
- **Low Confidence**: The assertion that the approach particularly benefits noisy transcription scenarios is not directly tested or quantified in the paper.

## Next Checks
1. **Ablation Study on Fusion Methods**: Compare the Double Multi-Head Attention approach against simple concatenation and other fusion strategies on the same dataset to isolate the specific contribution of the proposed fusion method.

2. **Noise Robustness Analysis**: Systematically evaluate model performance across different levels of transcription noise using synthetic noise injection or multiple ASR systems to validate the claim about improved performance in noisy scenarios.

3. **Cross-Dataset Generalization Test**: Evaluate the trained model on a different dialogue dataset (e.g., MultiWOZ or schema-guided dialogue) to assess whether the audio-aware improvements generalize beyond the DSTC2 domain.