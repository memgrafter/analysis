---
ver: rpa2
title: Hypothesis Spaces for Deep Learning
arxiv_id: '2403.03353'
source_url: https://arxiv.org/abs/2403.03353
tags:
- space
- learning
- problem
- banach
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper constructs a reproducing kernel Banach space (RKBS)
  as a hypothesis space for deep learning. The key idea is to treat a deep neural
  network as a function of both the input variable and the parameter variable (weights
  and biases), then take the weak closure of the linear span of these networks to
  form a Banach space.
---

# Hypothesis Spaces for Deep Learning

## Quick Facts
- arXiv ID: 2403.03353
- Source URL: https://arxiv.org/abs/2403.03353
- Reference count: 40
- Primary result: This paper constructs a reproducing kernel Banach space (RKBS) as a hypothesis space for deep learning by treating DNNs as functions of both input and parameter variables, proving solutions to learning problems can be expressed as finite kernel expansions.

## Executive Summary
This paper introduces a novel hypothesis space for deep learning by constructing a reproducing kernel Banach space (RKBS) through the weak* closure of the linear span of deep neural networks (DNNs). The key innovation is treating DNNs as functions of both the input variable and the parameter variable (weights and biases), then forming a Banach space by taking the weak* closure. The authors prove this space is an RKBS and explicitly construct its reproducing kernel, which depends on both input and parameter variables. This framework enables representer theorems that show solutions to learning problems - including regularized learning and minimum norm interpolation - can be expressed as finite kernel expansions based on training data, despite the infinite-dimensional nature of the RKBS.

## Method Summary
The authors construct a hypothesis space for deep learning by first defining DNNs as functions of both input variables and parameter variables, then forming the linear span BW of these networks. They take the weak* closure of BW to obtain a Banach space BN, proving it is a reproducing kernel Banach space (RKBS) with an explicitly constructed reproducing kernel K(x,θ) = N(x,θ)ρ(θ). The weight function ρ(θ) ensures the necessary continuity conditions for the RKBS. They establish representer theorems showing that solutions to learning problems in this space - including minimum norm interpolation and regularized learning - can be expressed as finite kernel expansions, effectively reducing infinite-dimensional optimization problems to finite-dimensional ones.

## Key Results
- The weak* closure of the linear span of DNNs forms a Banach space BN that is isometrically isomorphic to a quotient space M(Θ)/S⊥, where S is the pre-dual space.
- The constructed space BN is proven to be a reproducing kernel Banach space (RKBS) with reproducing kernel K(x,θ) = N(x,θ)ρ(θ).
- Representer theorems are established showing solutions to regularized learning and minimum norm interpolation problems can be expressed as finite kernel expansions based on training data.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The weak* closure of the linear span of DNNs forms a Banach space that is dense in an RKBS, enabling finite-dimensional representer theorems despite infinite-dimensional learning problems.
- Mechanism: By treating DNNs as functions of both input and parameter variables, the authors construct BW as the linear span of DNNs with parameters from a measure space Θ. Taking the weak* closure creates a Banach space BN that is isometrically isomorphic to a quotient space M(Θ)/S⊥, where S is the pre-dual space. This structure ensures that point evaluation functionals are continuous, satisfying the RKBS condition.
- Core assumption: The DNN components Nk(x,·)ρ(·) belong to C0(Θ), ensuring the weight function ρ makes all components vanish at infinity.
- Evidence anchors:
  - [abstract] "By taking the weak* closure of the linear span of this DNN set, we construct a Banach space of functions of the input variable."
  - [section] "We prove that the resulting Banach space is a reproducing kernel Banach space (RKBS) and explicitly construct its reproducing kernel."
  - [corpus] Weak evidence: Only 3 citations across 25 related papers suggest limited external validation of this specific weak* completion approach.
- Break condition: If the activation function σ grows faster than polynomially or if ρ(θ) does not decay sufficiently fast, the condition Nk(x,·)ρ(·) ∈ C0(Θ) fails, breaking the RKBS construction.

### Mechanism 2
- Claim: The asymmetric reproducing kernel K(x,θ) = N(x,θ)ρ(θ) enables solutions to learning problems to be expressed as finite kernel expansions based on training data.
- Mechanism: The kernel K(x,θ) is constructed such that for any function fµ in BN, the reproducing property fµ(x) = ⟨K(x,·),fµ⟩BN holds. This property allows solutions to learning problems to be represented as integrals over measures, which can be discretized into finite sums when solving for specific training data.
- Core assumption: The weight function ρ(θ) is chosen such that K(x,θ) ∈ S, the pre-dual space, ensuring the reproducing property can be established through the duality pairing.
- Evidence anchors:
  - [abstract] "We prove that the resulting Banach space is a reproducing kernel Banach space (RKBS) and explicitly construct its reproducing kernel."
  - [section] "The reproducing kernel is shown to be the product of the neural network with an appropriate weight function"
  - [corpus] Weak evidence: The corpus mentions "Neural reproducing kernel Banach spaces" but provides no specific validation of the asymmetric kernel construction.
- Break condition: If the duality pairing ⟨µ,K(x,·)⟩C0(Θ) cannot be established for the chosen ρ(θ), the reproducing property fails and kernel expansions become invalid.

### Mechanism 3
- Claim: The representer theorems reduce infinite-dimensional optimization problems to finite-dimensional ones by showing solutions lie in finite-dimensional manifolds spanned by kernel evaluations at training data.
- Mechanism: By establishing that BW is weak* dense in BN (Theorem 10), and using the explicit representer theorems for MNI and regularized learning, the authors show that any solution can be expressed as a finite linear combination of kernel functions K(·,θℓ) evaluated at specific parameter values θℓ determined by the data.
- Core assumption: The functionals Kk(xj,·) are linearly independent in S, ensuring the existence of solutions to the MNI problem and the validity of the dual problem formulation.
- Evidence anchors:
  - [abstract] "These theorems reveal that the solutions to these learning problems can be expressed as a finite sum of kernel expansions based on training data."
  - [section] "representer theorems for these learning models by leveraging the theory of the RKBS"
  - [corpus] Weak evidence: The corpus contains related work on representer theorems but lacks specific validation of the finite-dimensional manifold structure in RKBS.
- Break condition: If the linear independence condition fails or if the dual problem does not have a solution, the representer theorems cannot be established and the reduction to finite dimensions breaks down.

## Foundational Learning

- Concept: Weak* topology and weak* density
  - Why needed here: The weak* topology allows completing the linear span of DNNs in a way that preserves the necessary continuity properties for RKBS, while weak* density ensures that any element in the RKBS can be approximated by elements from BW.
  - Quick check question: Can you explain why the weak* topology is preferred over the norm topology when completing BW to obtain BN?

- Concept: Reproducing kernel Banach spaces (RKBS) vs reproducing kernel Hilbert spaces (RKHS)
  - Why needed here: RKBS generalize RKHS by allowing Banach space structure instead of Hilbert space, which is necessary to properly model the non-linear, non-convex structure of DNNs while maintaining the reproducing property.
  - Quick check question: What is the key difference between the reproducing property in RKBS versus RKHS, and why does this matter for deep learning?

- Concept: Duality mapping and subdifferential in Banach spaces
  - Why needed here: The representer theorems rely on characterizing solutions through subdifferentials of the norm function and duality mappings, which are more complex in Banach spaces than in Hilbert spaces due to the lack of inner products.
  - Quick check question: How does the subdifferential ∂∥·∥∞(g) in a Banach space differ from the gradient in a Hilbert space, and why is this distinction important for finding extreme points of solution sets?

## Architecture Onboarding

- Component map:
  - Input layer: Rs (input space)
  - Parameter layer: Θ (weight matrices and biases from DNNs)
  - Hypothesis space: BN (RKBS of functions from Rs to Rt)
  - Measure space: M(Θ) (dual space for representing functions in BN)
  - Kernel: K(x,θ) = N(x,θ)ρ(θ) (asymmetric reproducing kernel)
  - Learning models: MNI problem and regularized learning

- Critical path:
  1. Define DNNs as functions of input x and parameter θ
  2. Construct BW as linear span of DNNs
  3. Complete BW in weak* topology to obtain BN
  4. Verify BN is an RKBS with kernel K
  5. Establish weak* density of BW in BN
  6. Solve learning problems using representer theorems

- Design tradeoffs:
  - Choice of weight function ρ(θ): Faster decay improves regularity but may restrict the class of learnable functions
  - Activation function σ: Must be continuous and have polynomial growth rate for the RKBS construction to work
  - Layer width set W: Affects the norm structure of BN and the specific form of the reproducing kernel

- Failure signatures:
  - Learning problems have no solution: Check if functionals Kk(xj,·) are linearly independent
  - Representer theorems fail: Verify that Nk(x,·)ρ(·) ∈ C0(Θ) for all components
  - Optimization becomes unstable: Check if the regularization parameter λ is appropriate for the problem scale

- First 3 experiments:
  1. Verify RKBS properties: Test if point evaluation functionals are continuous on BN for simple DNN architectures with ReLU activation and exponential weight decay
  2. Test weak* density: Approximate elements of BN using finite linear combinations from BW and measure convergence in the BN norm
  3. Validate representer theorem: Solve a simple MNI problem with synthetic data and verify the solution can be expressed as a finite kernel expansion using the learned parameters θℓ

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of weight function ρ(θ) affect the convergence properties and generalization performance of learning models in the RKBS hypothesis space?
- Basis in paper: [explicit] The paper assumes a continuous weight function ρ: Θ → R+ and discusses its role in defining the RKBS, but does not analyze its impact on learning performance.
- Why unresolved: The paper focuses on the mathematical construction of the RKBS and representer theorems but does not investigate the empirical or theoretical consequences of different weight functions on convergence or generalization.
- What evidence would resolve it: Empirical studies comparing learning outcomes with different weight functions (e.g., polynomial decay vs. exponential decay) on benchmark datasets, along with theoretical bounds on generalization error.

### Open Question 2
- Question: Can the representer theorems established for MNI and regularized learning be extended to other learning models, such as adversarial training or transfer learning, within the RKBS framework?
- Basis in paper: [inferred] The paper establishes representer theorems for MNI and regularized learning but does not explore their applicability to other learning paradigms.
- Why unresolved: The paper focuses on specific learning models and does not address whether the RKBS framework can accommodate more complex or modern learning scenarios.
- What evidence would resolve it: Proofs or counterexamples showing whether the RKBS framework can be adapted to adversarial training or transfer learning, along with empirical validation on relevant tasks.

### Open Question 3
- Question: How does the network layer width set W influence the computational complexity and scalability of algorithms derived from the RKBS hypothesis space?
- Basis in paper: [explicit] The paper discusses the dependence of the RKBS on the network layer width set W but does not analyze its impact on computational efficiency.
- Why unresolved: While the paper establishes the mathematical properties of the RKBS, it does not address the practical challenges of implementing algorithms for large-scale or high-width networks.
- What evidence would resolve it: Complexity analyses of algorithms for solving learning problems in the RKBS as a function of network width, along with experiments demonstrating scalability on large datasets or networks.

## Limitations

- The weak* closure construction is non-constructive, making it difficult to explicitly compute or approximate elements of the RKBS BN in practice.
- The results depend on the choice of weight function ρ(θ), which introduces sensitivity - if ρ(θ) decays too quickly, the RKBS may not capture the full capacity of DNNs; if it decays too slowly, the continuity conditions for RKBS may fail.
- The paper assumes DNNs have prescribed depth and layer widths, which may limit the generality of the results for architectures used in practice.

## Confidence

- **High Confidence:** The theoretical construction of BW as the linear span of DNNs and its weak* closure to form BN follows established functional analysis principles. The reproducing kernel construction K(x,θ) = N(x,θ)ρ(θ) is mathematically sound given the assumptions.
- **Medium Confidence:** The representer theorems for MNI and regularized learning are proven under the stated assumptions, but their practical utility depends on whether the solutions in the RKBS correspond to meaningful DNN architectures.
- **Low Confidence:** The density results showing BW is weak* dense in BN, while theoretically established, may not translate to practical learning scenarios where only finite-dimensional approximations are available.

## Next Checks

1. **Empirical Verification:** Implement a simple learning problem (e.g., function approximation on a small dataset) using the RKBS framework and verify that solutions can indeed be expressed as finite kernel expansions as predicted by the representer theorems.

2. **Approximation Analysis:** For a given DNN architecture, empirically measure how well functions from BW approximate target functions in practice, and quantify the approximation error when restricting to finite-dimensional subspaces.

3. **Activation Function Sensitivity:** Test the RKBS construction with different activation functions (ReLU, tanh, sigmoid) and measure how the choice affects the continuity of point evaluation operators and the quality of the reproducing kernel.