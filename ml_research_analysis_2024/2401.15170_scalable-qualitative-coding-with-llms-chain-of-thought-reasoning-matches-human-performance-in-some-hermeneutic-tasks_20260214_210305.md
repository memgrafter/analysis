---
ver: rpa2
title: 'Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning Matches
  Human Performance in Some Hermeneutic Tasks'
arxiv_id: '2401.15170'
source_url: https://arxiv.org/abs/2401.15170
tags:
- code
- coding
- arxiv
- codes
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study presents the first evidence that large language models\
  \ (LLMs), specifically GPT-4, can perform qualitative coding tasks with human-level\
  \ accuracy. The authors adapted a codebook for socio-historical analysis and found\
  \ that GPT-4 achieved excellent intercoder reliability (Cohen\u2019s \u03BA \u2265\
  \ 0.79) for 3 of 9 codes and substantial reliability (\u03BA \u2265 0.6) for 8 of\
  \ 9 codes, matching or exceeding human performance."
---

# Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning Matches Human Performance in Some Hermeneutic Tasks

## Quick Facts
- arXiv ID: 2401.15170
- Source URL: https://arxiv.org/abs/2401.15170
- Authors: Zackary Okun Dunivin
- Reference count: 40
- Primary result: GPT-4 achieves human-level intercoder reliability on qualitative coding tasks when using chain-of-thought prompting

## Executive Summary
This study demonstrates that large language models, particularly GPT-4, can perform qualitative coding tasks with human-level accuracy when properly adapted and prompted. The research successfully adapted a codebook for socio-historical analysis and found that GPT-4 achieved excellent intercoder reliability (Cohen's κ ≥ 0.79) for 3 of 9 codes and substantial reliability (κ ≥ 0.6) for 8 of 9 codes, matching or exceeding human performance. Chain-of-thought reasoning prompting significantly improved coding fidelity compared to standard prompting approaches. The findings suggest automated qualitative coding is a viable option for many research applications, though codebook adaptation and model selection remain critical factors.

## Method Summary
The authors adapted an existing codebook designed for socio-historical analysis of South African university curriculum changes for use with large language models. They tested both GPT-4 and GPT-3.5 on coding interview transcripts from the original study, comparing LLM performance against human-coded gold standards. The study employed chain-of-thought prompting, where the LLM was asked to provide rationales for its coding decisions, which proved crucial for achieving high intercoder reliability. The evaluation measured intercoder reliability using Cohen's kappa statistic across nine different codes, assessing both overall agreement and per-code performance.

## Key Results
- GPT-4 achieved excellent intercoder reliability (κ ≥ 0.79) for 3 of 9 codes and substantial reliability (κ ≥ 0.6) for 8 of 9 codes
- Chain-of-thought prompting significantly improved coding fidelity compared to standard prompting
- GPT-3.5 underperformed on all codes, highlighting the importance of model selection

## Why This Works (Mechanism)
The success of LLMs in qualitative coding stems from their ability to understand context, identify patterns, and apply consistent reasoning across large text corpora. Chain-of-thought prompting works by forcing the model to explicitly articulate its reasoning process, making its decision-making more transparent and aligned with human coding practices. The adaptation of traditional codebooks for LLM use involves translating qualitative criteria into clear, structured prompts that leverage the model's natural language understanding capabilities while maintaining the interpretive depth required for hermeneutic analysis.

## Foundational Learning
- Qualitative coding principles: Understanding thematic analysis and code application is essential for adapting codebooks to LLM use; quick check: verify that codebook definitions align with standard qualitative research terminology
- Intercoder reliability metrics: Cohen's kappa and other reliability measures provide the framework for evaluating LLM performance; quick check: ensure agreement thresholds match established standards in the field
- Prompt engineering basics: Structured prompting techniques are necessary for optimal LLM performance; quick check: test prompt variations on a small sample before full deployment
- Hermeneutic analysis concepts: Understanding interpretive methods helps in adapting codebooks for LLM use; quick check: confirm that code definitions preserve the original interpretive intent

## Architecture Onboarding
**Component Map:** Codebook adaptation -> Chain-of-thought prompting -> LLM processing -> Reliability evaluation -> Validation
**Critical Path:** Prompt design → Model selection → Codebook adaptation → Reliability measurement → Performance validation
**Design Tradeoffs:** GPT-4 offers superior performance but higher cost versus GPT-3.5's lower cost but reduced accuracy; detailed rationales improve reliability but increase processing time
**Failure Signatures:** Poor intercoder reliability indicates inadequate codebook adaptation or insufficient prompting detail; inconsistent coding across similar texts suggests prompt ambiguity
**First Experiments:**
1. Test chain-of-thought prompting versus standard prompting on a small sample to measure improvement magnitude
2. Compare different model variants (GPT-4 vs GPT-3.5) on the same coding task to establish performance baseline
3. Evaluate codebook adaptation effectiveness by measuring intercoder reliability before and after prompting modifications

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to a single socio-historical dataset, raising questions about generalizability across different qualitative research domains
- LLM was not trained on or exposed to the same training materials as human annotators, making the comparison more capability demonstration than true equivalence
- The effort required for codebook adaptation and validation was not fully quantified, making it unclear when automation becomes more efficient than traditional approaches

## Confidence
- **High confidence**: LLMs can achieve human-level intercoder reliability on many qualitative coding tasks when properly prompted with chain-of-thought reasoning
- **Medium confidence**: Chain-of-thought prompting significantly improves coding fidelity, though the magnitude of improvement may vary by code complexity
- **Medium confidence**: Codebook adaptation is necessary but feasible, though the effort required was not fully quantified

## Next Checks
1. Test the adapted codebook and prompting approach across multiple qualitative datasets from different domains (e.g., healthcare, education, social sciences) to assess generalizability
2. Conduct head-to-head comparisons where both LLM and human coders receive identical training materials and have access to the same supporting documentation
3. Measure the time and expertise required for codebook adaptation and validation to determine when LLM coding becomes more efficient than traditional approaches