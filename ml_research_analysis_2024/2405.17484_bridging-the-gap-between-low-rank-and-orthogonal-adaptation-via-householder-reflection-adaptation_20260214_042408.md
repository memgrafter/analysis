---
ver: rpa2
title: Bridging The Gap between Low-rank and Orthogonal Adaptation via Householder
  Reflection Adaptation
arxiv_id: '2405.17484'
source_url: https://arxiv.org/abs/2405.17484
tags:
- answer
- latexit
- should
- adaptation
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper bridges the gap between low-rank and orthogonal adaptation
  techniques for efficient fine-tuning of large pre-trained models. The authors propose
  a method based on Householder reflections that allows each frozen weight matrix
  to be multiplied by an orthogonal matrix constructed from learnable Householder
  reflections.
---

# Bridging The Gap between Low-rank and Orthogonal Adaptation via Householder Reflection Adaptation

## Quick Facts
- arXiv ID: 2405.17484
- Source URL: https://arxiv.org/abs/2405.17484
- Reference count: 40
- Key outcome: Proposed HRA method achieves superior performance with fewer trainable parameters compared to state-of-the-art methods when adapting large language models and conditional image generators

## Executive Summary
This paper introduces Householder Reflection Adaptation (HRA), a novel method for efficient fine-tuning of large pre-trained models. HRA bridges the gap between low-rank and orthogonal adaptation techniques by constructing orthogonal matrices from learnable Householder reflections. The method is shown to be equivalent to an adaptive low-rank adaptation while maintaining orthogonality constraints. Experiments demonstrate that HRA achieves superior performance with fewer trainable parameters compared to state-of-the-art methods across various tasks including language modeling and image generation.

## Method Summary
HRA fine-tunes frozen weight matrices by multiplying them with orthogonal matrices constructed from a chain of learnable Householder reflections. Each reflection is parameterized by a unit vector, and the product of these reflections forms an orthogonal matrix that can be efficiently computed. The method is equivalent to adaptive low-rank adaptation (LoRA) but maintains orthogonality by construction. An orthogonality regularizer can be applied to control the trade-off between model capacity and regularity. The approach is evaluated on DeBERTaV3-base, LLaMA2-7B, and Stable Diffusion models across multiple tasks.

## Key Results
- HRA achieves superior performance with fewer trainable parameters compared to LoRA and full fine-tuning baselines
- The method demonstrates effectiveness across diverse tasks including GLUE benchmark, mathematical reasoning, and image generation
- HRA shows particular strength in tasks requiring high model capacity while maintaining regularization benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Householder reflections form a structured way to construct orthogonal matrices using minimal parameters
- Mechanism: Each Householder reflection is parameterized by a unit vector u, producing the matrix H = I - 2uuᵀ, which is orthogonal by construction. Chaining r such reflections yields a product H(r) that remains orthogonal while only requiring r unit vectors as learnable parameters.
- Core assumption: The set of orthogonal matrices is closed under matrix multiplication, so the product of orthogonal matrices remains orthogonal.
- Evidence anchors:
  - [abstract] "each frozen weight matrix with an orthogonal matrix constructed by a chain of learnable Householder reflections (HRs)"
  - [section 3.1] "the product of orthogonal matrices is also an orthogonal matrix [1]"
- Break condition: If numerical precision errors accumulate over many chained reflections, orthogonality could degrade.

### Mechanism 2
- Claim: HRA can be equivalently interpreted as an adaptive low-rank adaptation method
- Mechanism: The chain of Householder reflections H(r) can be rewritten as I + UrΓrUᵀr, where Ur contains the reflection vectors and Γr is an upper triangular matrix. This transforms the operation W·H(r) into W + WUrΓrUᵀr, matching the LoRA form with A(W,B) = WUrΓr and B = Ur.
- Core assumption: The algebraic manipulation of the Householder product into the low-rank form is mathematically valid.
- Evidence anchors:
  - [section 3.3] "We can rewrite the chain of HRs...in the following equivalent format: H(r) = ∏(I - 2uᵢuᵢᵀ) = I + UrΓrUᵀr"
  - [section 3.3] "This formulation can be viewed as an adaptive LoRA"
- Break condition: If the triangular structure of Γr is not preserved during optimization, the equivalence breaks.

### Mechanism 3
- Claim: Orthogonality regularization on reflection vectors controls the trade-off between model capacity and regularity
- Mechanism: Adding a penalty λ·||I - (Ur)ᵀUr||²F to the loss encourages Ur toward orthogonality. When λ→∞, Ur becomes strictly orthogonal (Od×r), maximizing regularity but constraining capacity. When λ=0, Ur is only column-normalized, maximizing capacity.
- Core assumption: The orthogonality of Ur directly impacts the regularity of the adaptation through the Frobenius norm penalty.
- Evidence anchors:
  - [section 3.4] "we can implement HRA with an orthogonality regularizer...encourages the orthogonality of all U(l)r's"
  - [section 3.4] "by controlling the strength of the orthogonality regularizer, we can achieve a trade-off between the model capacity and regularity"
- Break condition: If the regularization strength λ is poorly tuned, either the model becomes too rigid (λ→∞) or loses regularization benefits (λ=0).

## Foundational Learning

- Concept: Householder reflections
  - Why needed here: HRA is built entirely on Householder reflections as the mechanism to generate orthogonal matrices with minimal parameters
  - Quick check question: What is the formula for a Householder reflection matrix and why is it orthogonal?

- Concept: Low-rank matrix decomposition
  - Why needed here: HRA's equivalence to LoRA requires understanding how W + AB with A∈Rdout×r and B∈Rr×d reduces parameters while maintaining adaptation capacity
  - Quick check question: How does the rank r in LoRA relate to the number of parameters saved compared to full fine-tuning?

- Concept: Orthogonal matrices and Lie groups
  - Why needed here: The theoretical foundation of HRA relies on the orthogonal group Od×d being compact and closed under multiplication, ensuring the chained reflections remain orthogonal
  - Quick check question: What are the defining properties of the orthogonal group Od×d that make it suitable for parameter-efficient fine-tuning?

## Architecture Onboarding

- Component map: Pre-trained weight matrix W -> Householder adapter (r unit vectors) -> Adapted activation z

- Critical path:
  1. Initialize r unit vectors for each weight matrix to be adapted
  2. For each forward pass: compute x(j+1) = x(j) - 2⟨uᵣ₋ⱼ, x(j)⟩uᵣ₋ⱼ for j=0,...,r-1
  3. Apply adapted weight: z = W·x(r)
  4. During training: backpropagate through the reflection operations
  5. Apply orthogonality regularization if λ > 0

- Design tradeoffs:
  - Rank r vs. parameter count: Higher r increases capacity but also parameters (r×d per matrix)
  - Regularization strength λ vs. performance: Higher λ increases regularity but may reduce capacity
  - Strict orthogonality (λ=∞) vs. efficiency: Requires Gram-Schmidt orthogonalization with O(dr²) cost

- Failure signatures:
  - Poor performance with high λ: Indicates tasks need more model capacity than regularity
  - Numerical instability with large r: Accumulated floating-point errors in chained reflections
  - Slow convergence: May indicate learning rate too low for the reflection parameterization

- First 3 experiments:
  1. Verify orthogonality preservation: Check ||H(r)ᵀH(r) - I||F ≈ 0 for initialized and optimized Ur
  2. Test equivalence to LoRA: Compare adaptation patterns of HRA vs. standard LoRA on a simple task
  3. Validate computational efficiency: Benchmark training time and memory usage vs. OFT and LoRA baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of the regularization strength λ on the orthogonality of the Householder reflection matrices, and how does it affect the trade-off between model capacity and regularity in practice?
- Basis in paper: [explicit] The paper discusses the use of an orthogonality regularizer weighted by λ and analyzes the trade-off between model capacity and regularity for different values of λ.
- Why unresolved: The paper does not provide a detailed theoretical analysis of how λ influences the orthogonality of the reflection planes and the resulting model capacity and regularity.
- What evidence would resolve it: A comprehensive study analyzing the relationship between λ, the orthogonality of the reflection planes, and the resulting model capacity and regularity through both theoretical analysis and extensive experiments.

### Open Question 2
- Question: How does HRA compare to other parameter-efficient fine-tuning methods in terms of computational efficiency and memory usage when adapting extremely large language models with billions of parameters?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of HRA in adapting large language models and conditional image generators, but does not provide a detailed comparison of computational efficiency and memory usage with other methods for extremely large models.
- Why unresolved: The paper does not provide a comprehensive comparison of computational efficiency and memory usage of HRA with other methods when adapting extremely large language models.
- What evidence would resolve it: A detailed comparison of the computational efficiency and memory usage of HRA with other parameter-efficient fine-tuning methods when adapting extremely large language models, including benchmarks and resource utilization analysis.

### Open Question 3
- Question: Can HRA be extended to other types of neural network architectures beyond Transformers, such as Convolutional Neural Networks or Recurrent Neural Networks, and what would be the implications for their adaptation performance?
- Basis in paper: [inferred] The paper focuses on adapting Transformers using HRA and mentions that the principle of imposing orthogonality constraints has been applied in designing robust neural network architectures, but does not explore the extension of HRA to other architectures.
- Why unresolved: The paper does not investigate the applicability of HRA to other neural network architectures and the potential implications for their adaptation performance.
- What evidence would resolve it: An exploration of the extension of HRA to other neural network architectures, including theoretical analysis and experimental results on their adaptation performance compared to existing methods.

## Limitations

- The computational overhead of maintaining orthogonality through regularization is not quantified, particularly for strict orthogonality (λ→∞)
- The scalability of HRA to extremely large models (e.g., trillion-parameter models) remains untested
- The practical benefits of different regularization strengths λ across diverse tasks and model architectures are not thoroughly explored

## Confidence

- High confidence: The theoretical foundation of HRA and its equivalence to LoRA is mathematically rigorous and well-explained
- Medium confidence: The experimental results show promising performance improvements, but the sample size of tasks and models is relatively limited
- Low confidence: The paper's assertion that different tasks require different levels of model capacity versus regularity is based on limited empirical evidence

## Next Checks

1. Conduct extensive ablation studies varying the rank r and regularization strength λ across multiple tasks and model architectures to quantify the trade-off between model capacity and regularity
2. Measure and compare the computational overhead (training time, memory usage) of HRA with different λ values against LoRA and full fine-tuning baselines
3. Test the scalability of HRA on extremely large models (e.g., GPT-3, OPT-175B) to validate its efficiency gains at scale and identify any potential limitations or failure modes