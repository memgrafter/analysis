---
ver: rpa2
title: Model Parallel Training and Transfer Learning for Convolutional Neural Networks
  by Domain Decomposition
arxiv_id: '2408.14442'
source_url: https://arxiv.org/abs/2408.14442
tags:
- local
- training
- cnn-dnn
- cnns
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates model-parallel training strategies for
  convolutional neural networks (CNNs) applied to image classification problems. The
  authors propose a novel CNN-DNN architecture that decomposes input images into smaller
  subimages, trains local CNNs in parallel on each subimage, and aggregates the local
  classifications using a dense feedforward neural network (DNN).
---

# Model Parallel Training and Transfer Learning for Convolutional Neural Networks by Domain Decomposition

## Quick Facts
- arXiv ID: 2408.14442
- Source URL: https://arxiv.org/abs/2408.14442
- Authors: Axel Klawonn; Martin Lanser; Janine Weber
- Reference count: 13
- Primary result: Proposes CNN-DNN architecture that decomposes images, trains local CNNs in parallel, and aggregates via DNN, outperforming baseline methods.

## Executive Summary
This paper introduces a model-parallel training strategy for CNNs based on domain decomposition, where input images are split into subimages, each processed by a local CNN in parallel, and results are aggregated by a DNN. The method reduces computational cost and parameter count while maintaining or improving classification accuracy. Transfer learning, initializing a global model with pre-trained local CNNs, further boosts performance. The approach is tested on CIFAR-10, TF-Flowers, and 3D CT scan datasets with VGG9 and ResNet20 architectures.

## Method Summary
The method decomposes input images into non-overlapping subimages, trains proportionally smaller CNNs in parallel on each subimage, and aggregates the resulting local classifications using a dense feedforward neural network (DNN). The paper compares this approach with simpler aggregation methods (average probability, majority voting), a coherent CNN-DNN model trained as one architecture, and a transfer learning strategy where pre-trained local CNNs initialize the global model. Experiments are conducted on CIFAR-10 (32x32, 10 classes), TF-Flowers (180x180, 5 classes), and chest CT scans (128x128x64 voxels, binary classification), using VGG9 and ResNet20 architectures.

## Key Results
- CNN-DNN architecture outperforms average probability and majority voting baselines.
- ResNet20 benefits from coherent training due to smoother loss surfaces from skip connections; VGG9 does not.
- Transfer learning with pre-trained local CNNs improves global model accuracy across all datasets and architectures.
- Validation accuracies reach up to 0.9117 (ResNet20, CIFAR-10) and 0.9702 (ResNet20, TF-Flowers).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing images into subimages and training smaller local CNNs in parallel reduces total parameter count and computational cost while maintaining accuracy.
- Core assumption: Local CNNs trained on subimages can extract discriminative features sufficient for accurate classification when aggregated by a DNN.
- Evidence anchors:
  - [abstract] "local CNNs with a proportionally smaller number of parameters are trained in parallel and the resulting local classifications are then aggregated in a second step by a dense feedforward neural network (DNN)."
  - [section] "For each of these subimages, local CNNs with a proportionally smaller number of parameters are trained in parallel and the resulting local classifications are then aggregated in a second step by a dense feedforward neural network (DNN)."
  - [corpus] Weak evidence; corpus lacks citations or performance data for this specific decomposition strategy.
- Break Condition: If the DNN fails to learn a useful mapping from local to global probabilities, or if local features are too incomplete to distinguish classes.

### Mechanism 2
- Claim: Transfer learning with pre-trained local CNNs as initialization improves global model accuracy compared to training the coherent CNN-DNN from scratch.
- Core assumption: Feature representations learned on subimages are transferable and beneficial for the global model.
- Evidence anchors:
  - [abstract] "using a transfer learning strategy, where the parameters of the pre-trained local CNNs are used as initial values for a subsequently trained global coherent CNN-DNN model."
  - [section] "we present classification accuracies for training the CNN-DNN model from [5] as one cohesive model architecture... we further use the concept of transfer learning... we first train proportionally smaller CNNs... and subsequently use the obtained network parameters as initializations..."
  - [corpus] No direct citations; this is a novel combination of local model pre-training with global model fine-tuning.
- Break Condition: If the initialization degrades training (e.g., due to distribution mismatch between subimages and full images) or the global model diverges during fine-tuning.

### Mechanism 3
- Claim: Training the CNN-DNN as one coherent model can outperform the local-then-DNN pipeline for certain architectures (e.g., ResNet20) due to smoother loss surfaces introduced by skip connections.
- Core assumption: Skip connections improve the loss surface topology enough to outweigh the benefits of parallelization and modular training.
- Evidence anchors:
  - [section] "when considering the ResNet20 model, the quantitative behavior is reversed... the CNN-DNN-coherent shows an improved classification accuracy... the introduction of skip connections usually results in smoother loss surfaces... and enhanced training properties."
  - [corpus] No corpus evidence; this is a hypothesis based on known ResNet properties.
- Break Condition: If the coherent model becomes too complex to optimize effectively or if parallelization benefits outweigh smoother loss surfaces.

## Foundational Learning

- Concept: Domain decomposition in numerical methods
  - Why needed here: The method is inspired by domain decomposition, where a problem is split into subproblems solved in parallel, then coupled. Understanding this analogy helps grasp why local CNNs and a global DNN are used.
  - Quick check question: How does domain decomposition in PDEs relate to splitting an image into subimages for parallel CNN training?

- Concept: Transfer learning and model initialization
  - Why needed here: The paper uses pre-trained local CNNs to initialize a global model. Knowing how transfer learning works and why good initialization matters is key to understanding the accuracy gains.
  - Quick check question: What are the benefits and risks of using pre-trained submodels as initialization for a larger model?

- Concept: Loss surface geometry and optimization in deep learning
  - Why needed here: The paper hypothesizes that skip connections in ResNet20 smooth the loss surface, aiding coherent training. Understanding loss landscapes explains why some architectures benefit more from joint training.
  - Quick check question: How do skip connections in residual networks affect the loss surface compared to plain networks?

## Architecture Onboarding

- Component map:
  Input image -> decomposition into N non-overlapping subimages -> N local CNNs (smaller, parallel, trained on subimages) -> DNN (trained on concatenated local probability vectors) -> (Optional) Coherent CNN-DNN (single model, functional API) -> (Optional) Transfer learning path (local CNNs -> global initialization)

- Critical path:
  1. Decompose input images
  2. Train local CNNs in parallel
  3. Collect local probability outputs
  4. Train DNN to map local probs -> global labels
  5. (For coherent model) Build and train unified architecture
  6. (For transfer) Initialize coherent model with local CNN weights, then fine-tune

- Design tradeoffs:
  - Parallelism vs. model complexity: More subimages -> more parallel models but more DNN parameters to learn
  - Accuracy vs. training time: Transfer learning can improve accuracy but requires extra pre-training step
  - Flexibility vs. simplicity: Local-then-DNN is modular; coherent model is simpler to deploy but harder to scale

- Failure signatures:
  - DNN fails to generalize from local to global probabilities (overfitting on training data)
  - Coherent model training diverges or plateaus (initialization or optimization issue)
  - Accuracy drops when increasing number of subimages (loss of global context)

- First 3 experiments:
  1. Implement the local CNN + DNN pipeline on a small dataset (e.g., CIFAR-10) with 2x2 decomposition; compare accuracy to global CNN baseline.
  2. Test the coherent CNN-DNN variant on the same dataset; compare training curves and final accuracy to the local-then-DNN approach.
  3. Apply transfer learning: pre-train local CNNs, initialize coherent model, and fine-tune; measure accuracy improvement over random initialization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the complexity of the loss surface differ between CNN-DNN-coherent and locally trained CNN models for VGG9 and ResNet20 architectures?
- Basis in paper: [explicit] The paper suggests that the CNN-DNN-coherent model might have a more complex loss function and loss surface than the locally trained smaller CNNs and the relatively small DNN, potentially explaining the different performance between VGG9 and ResNet20 models.
- Why unresolved: The paper only hypothesizes about the loss surface complexity without providing concrete evidence or analysis of the loss landscapes for the different models.
- What evidence would resolve it: Detailed analysis and visualization of the loss landscapes for both CNN-DNN-coherent and locally trained CNN models for VGG9 and ResNet20 architectures, comparing their complexity and smoothness.

### Open Question 2
- Question: What is the required training time for the CNN-DNN-transfer approach compared to other training strategies?
- Basis in paper: [explicit] The paper mentions that a detailed investigation of the required training times of the transfer learning strategy for the proposed model architecture is a potential topic for future research.
- Why unresolved: The paper does not provide any comparison of training times between the CNN-DNN-transfer approach and other training strategies.
- What evidence would resolve it: Comparative analysis of training times for CNN-DNN-transfer, CNN-DNN, CNN-DNN-coherent, and global CNN approaches across different datasets and architectures.

### Open Question 3
- Question: How does the performance of the CNN-DNN architecture change with different decomposition strategies beyond the "type A" decomposition?
- Basis in paper: [explicit] The paper states that it only considers decompositions of input images into rectangular subimages without overlap (type A decomposition) due to space limitations, and refers to a previous work for more general and overlapping decompositions.
- Why unresolved: The paper does not explore the performance impact of different decomposition strategies on the CNN-DNN architecture.
- What evidence would resolve it: Comparative analysis of classification accuracies for CNN-DNN models using various decomposition strategies, including overlapping decompositions and non-rectangular subimages, across different datasets and architectures.

## Limitations

- The paper lacks detailed hyperparameter specifications, making exact reproduction difficult.
- Experimental validation is limited to two architectures (VGG9, ResNet20) and three datasets, with no ablation studies or sensitivity analysis.
- Claims about loss surface complexity and the benefits of transfer learning are not robustly supported by empirical evidence or comparison to alternative initialization strategies.

## Confidence

- **Domain decomposition and parallel training idea**: High
- **CNN-DNN architecture and its advantages**: Medium
- **Transfer learning benefits**: Low-Medium

## Next Checks

1. Implement ablation studies: compare CNN-DNN with alternative aggregation methods (e.g., attention, gating) and test different numbers of subimages to find optimal decomposition.
2. Conduct hyperparameter sensitivity analysis: systematically vary learning rates, batch sizes, and DNN architectures to assess robustness of the proposed method.
3. Test transfer learning against other initialization strategies: compare pre-trained local CNNs with ImageNet pre-training or self-supervised features to isolate the benefit of the domain decomposition approach.