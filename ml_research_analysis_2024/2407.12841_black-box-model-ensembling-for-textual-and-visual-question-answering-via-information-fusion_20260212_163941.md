---
ver: rpa2
title: Black-box Model Ensembling for Textual and Visual Question Answering via Information
  Fusion
arxiv_id: '2407.12841'
source_url: https://arxiv.org/abs/2407.12841
tags:
- data
- training
- base
- infosel
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InfoSel is a data-efficient ensemble method for selecting the best-performing
  black-box model (LLM or VQA model) for each input in textual and visual question
  answering tasks. Unlike traditional ensemble methods, InfoSel does not rely on prediction
  probabilities or confidences, which are typically unavailable for black-box models.
---

# Black-box Model Ensembling for Textual and Visual Question Answering via Information Fusion

## Quick Facts
- **arXiv ID**: 2407.12841
- **Source URL**: https://arxiv.org/abs/2407.12841
- **Reference count**: 40
- **Primary result**: Data-efficient ensemble method that selects best black-box model per input without requiring model confidences, achieving up to +5.19% F1-score improvement for textual QA and +31.63% accuracy improvement for visual QA with only 1K training instances

## Executive Summary
InfoSel is a data-efficient ensemble method for selecting the best-performing black-box model (LLM or VQA model) for each input in textual and visual question answering tasks. Unlike traditional ensemble methods, InfoSel does not rely on prediction probabilities or confidences, which are typically unavailable for black-box models. It trains a small ensemble model to dynamically identify the most accurate base model for a given input by designing a meta-level classification task considering all base models as labels. InfoSel incorporates task-specific optimization, allowing it to adapt easily to different datasets with varying inputs and predicted answers.

## Method Summary
InfoSel reframes black-box model ensembling as a meta-classification problem where each base model becomes a label. The method generates answers from multiple black-box models, evaluates their F1 or accuracy against ground truth, and uses these as supervision to train a lightweight transformer to predict which base model will perform best for each input. For visual QA, InfoSel-MT fuses multimodal inputs using a multimodal transformer encoder. An optional extension, InfoSel*, fine-tunes lightweight models on task-specific data to handle unseen labels and ensembles them with InfoSel predictions. The approach is designed to work with minimal training data (as few as 10-1K samples) while achieving significant performance improvements over standalone base models.

## Key Results
- Achieves up to +5.19% improvement in F1-score for textual QA compared to standalone base models
- Achieves up to +31.63% improvement in accuracy for visual QA compared to standalone base models
- More data-efficient than state-of-the-art baseline ensemble methods, using only 1K training instances
- Surpasses performance of leading base models with as few as 10 training samples on one benchmark dataset

## Why This Works (Mechanism)

### Mechanism 1
InfoSel avoids dependency on model confidences by reframing ensembling as a meta-classification task where base models are treated as labels. The method generates answers from multiple black-box models, evaluates their F1 or accuracy against ground truth, and uses these as supervision to train a lightweight transformer to predict which base model will perform best for each input. Core assumption: The relative performance of base models is consistent enough across inputs to be learned from a small labeled sample.

### Mechanism 2
InfoSel-MT fuses multimodal inputs (image, question, answers) into a single contextual representation before selection, improving performance over text-only methods. The multimodal transformer encodes the visual features from R-CNN along with the joint question-answer strings, producing a fused representation that is then used by a dense layer to rank base model predictions. Core assumption: Fusing visual and textual information before ranking base models yields more discriminative features than concatenating embeddings.

### Mechanism 3
InfoSel* improves robustness to unseen labels by fine-tuning lightweight models on task-specific data and ensembling them with InfoSel. FT-TT/FT-MT are trained to classify or generate answers directly on the task dataset, capturing labels the base models have not seen. InfoSel* then learns to choose between these fine-tuned models and the original InfoSel predictions. Core assumption: Even a small fine-tuned model can learn new labels that black-box models cannot produce, and combining them improves coverage.

## Foundational Learning

- **Meta-level classification and label space construction**: Why needed: InfoSel reframes the problem from aggregating predictions to selecting the best predictor per instance, requiring a label space where each class is a base model. Quick check: How do you transform a set of base model outputs into a classification problem where each class is a model?

- **Multimodal fusion strategies (concatenation vs. attention-based fusion)**: Why needed: InfoSel-MT must decide how to combine image features and text embeddings before ranking; fusion quality directly impacts selection accuracy. Quick check: What is the difference between simply concatenating embeddings and using cross-modal attention for fusion?

- **Data efficiency in few-shot learning**: Why needed: InfoSel is designed to work with <1% of training data; understanding how to extract maximum signal from minimal samples is critical. Quick check: How can you evaluate whether a model is learning meaningful patterns with only 10-1000 training examples?

## Architecture Onboarding

- **Component map**: Data preparation pipeline → base model inference → score computation → input construction (question + answers) → transformer encoder → dense selector → (optional) fine-tuned model inference → second selector
- **Critical path**: Input → base model generation → score computation → transformer encoding → dense layer → winner selection
- **Design tradeoffs**: Using full black-box outputs vs. just scores (preserves more information but increases input size); separate FT models vs. joint fine-tuning (modularity vs. tighter integration); single-stage vs. two-stage ensemble (simplicity vs. robustness to unseen labels)
- **Failure signatures**: Selector always picks the same model (meta-classifier not learning); performance worse than random (feature representation or training signal broken); degradation on multimodal tasks (fusion or multimodal encoder issues)
- **First 3 experiments**: 1) Run base models on a small dev set, compute F1/accuracy, verify scores are usable. 2) Train InfoSel-TT on 10 samples, check if it selects different models per input. 3) Add FT-TT, train InfoSel* -TT, compare selection patterns and accuracy gains.

## Open Questions the Paper Calls Out

### Open Question 1
How does InfoSel's performance scale with increasingly large numbers of base models beyond the three used in the experiments? The paper demonstrates effectiveness with three models but doesn't establish the upper limits or optimal number of base models for InfoSel's performance.

### Open Question 2
What is the impact of different pre-training datasets on the base models' performance when used with InfoSel? While the paper acknowledges differences in pre-training data, it doesn't investigate how varying the base models' pre-training data affects InfoSel's ability to select the best model.

### Open Question 3
How does InfoSel handle cases where multiple base models provide equally correct but differently phrased answers? The paper focuses on selecting a single "winner" model but doesn't address scenarios where multiple base models give semantically equivalent but syntactically different correct answers.

## Limitations

- Generalizability across domains is limited to four specific QA datasets; effectiveness for other reasoning tasks remains unknown
- Performance depends on diversity among base model predictions; highly correlated models would diminish gains
- Data efficiency claims lack comprehensive baseline comparisons against other ensemble methods

## Confidence

- **High confidence**: The core mechanism of reframing ensemble selection as a meta-classification problem is technically sound and well-explained
- **Medium confidence**: Quantitative improvements are specific and verifiable, but practical significance depends on baseline performance levels which aren't fully contextualized
- **Low confidence**: Effectiveness of InfoSel* for handling unseen labels is demonstrated on only one dataset, making generalizability uncertain

## Next Checks

1. **Diversity analysis of base model predictions**: Analyze correlation and agreement patterns among base model predictions across all four datasets to quantify the actual diversity that InfoSel exploits

2. **Ablation study on training data requirements**: Systematically evaluate InfoSel performance with varying training set sizes (10, 50, 100, 500, 1000) on each dataset, comparing against random selection and majority voting baselines

3. **Cross-dataset transfer validation**: Train InfoSel on one dataset (e.g., SQuAD-V2) and evaluate on another (e.g., NQ-Open) without fine-tuning to test how much dataset-specific training is actually required