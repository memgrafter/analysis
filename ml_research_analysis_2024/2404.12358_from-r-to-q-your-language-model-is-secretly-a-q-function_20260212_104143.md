---
ver: rpa2
title: 'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function'
arxiv_id: '2404.12358'
source_url: https://arxiv.org/abs/2404.12358
tags:
- reward
- which
- learning
- have
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper connects Direct Preference Optimization (DPO) to reinforcement
  learning by showing that DPO implicitly learns a token-level Q-function for a language
  model, enabling per-token credit assignment. The authors prove that DPO can represent
  any dense reward function within the token-level MDP and derive connections between
  likelihood-based search and reward-guided decoding.
---

# From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function

## Quick Facts
- **arXiv ID:** 2404.12358
- **Source URL:** https://arxiv.org/abs/2404.12358
- **Authors:** Rafael Rafailov; Joey Hejna; Ryan Park; Chelsea Finn
- **Reference count:** 36
- **Primary result:** DPO implicitly learns a token-level Q-function for a language model, enabling per-token credit assignment and unifying DPO with RL.

## Executive Summary
This paper establishes a theoretical connection between Direct Preference Optimization (DPO) and reinforcement learning by showing that DPO implicitly learns a token-level Q-function for a language model. Through this lens, DPO can perform credit assignment at the token level and represent any dense reward function within the token MDP. The authors prove that classical search-based algorithms like MCTS are equivalent to likelihood-based search on a DPO policy, unifying decoding and search-based methods. Empirically, they demonstrate DPO's credit assignment capability and show that simple beam search over DPO policies yields performance improvements comparable to reward-guided search.

## Method Summary
The paper reinterprets DPO in the context of a token-level Markov Decision Process (MDP), showing that DPO's optimization corresponds to inverse Q-learning where the language model logits directly encode the optimal Q-function. The method involves training a DPO policy using preference data while treating the reference policy (typically an SFT model) as a baseline. The training maximizes the likelihood of preferred responses relative to rejected ones, which implicitly learns an advantage function and thus a reward function. The theoretical framework is validated through experiments on text summarization, demonstrating credit assignment capabilities and the equivalence between likelihood search and reward-guided search methods.

## Key Results
- DPO implicitly learns a token-level Q-function, enabling per-token credit assignment from preference feedback.
- DPO can represent any dense reward function within the token-level MDP, not just sparse terminal rewards.
- Likelihood search over DPO policies is mathematically equivalent to reward-guided search, unifying decoding and search-based algorithms.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DPO implicitly learns a token-level Q-function for a language model, enabling per-token credit assignment.
- **Mechanism:** The paper shows that DPO's optimization, when cast into the token-level MDP, learns an optimal Q-function Q* that represents the expected total future reward. Since the policy π is derived from Q* via a softmax, the model's logits lθ directly encode Q*. This allows each token to be assigned an implicit reward r(st,at) = βlogπ(a|s) - βlogπref(a|s), enabling credit assignment for individual tokens based on preference feedback.
- **Core assumption:** The token-level MDP is deterministic and structured such that there is a bijection between reward functions r and optimal Q-functions Q* (Lemma 1).
- **Evidence anchors:** [abstract], [section 4.1], [corpus]
- **Break condition:** If the token-level MDP assumption fails (e.g., non-deterministic dynamics or states that can be revisited), the bijection between r and Q* breaks, invalidating the implicit reward interpretation.

### Mechanism 2
- **Claim:** DPO can represent any dense reward function within the token-level MDP.
- **Mechanism:** The paper proves that DPO's parameterization βlogπ(a|s) - βlogπref(a|s) covers all reward classes consistent with the Plackett-Luce (and Bradley-Terry) models. This means that any dense reward function expressible in the token MDP can be learned by DPO, not just sparse terminal rewards.
- **Core assumption:** The reward functions must be expressible as optimal advantage functions A*(s,a) = Q*(s,a) - V*(s), which are in the same equivalence class as the original rewards under potential shaping.
- **Evidence anchors:** [abstract], [section 4.2], [corpus]
- **Break condition:** If the preference model deviates from Plackett-Luce/Bradley-Terry assumptions, or if the reward function lies outside the optimal advantage class, DPO cannot represent it.

### Mechanism 3
- **Claim:** Likelihood search over DPO policies is equivalent to reward-guided search, unifying decoding and search-based algorithms.
- **Mechanism:** The paper shows that search algorithms maximizing a reward function (e.g., MCTS) in the token MDP are mathematically equivalent to likelihood search over the DPO policy π*. This equivalence arises because the reward function can be expressed as an advantage function of π*, and maximizing it is the same as maximizing the log-likelihood of π*.
- **Core assumption:** The reward function used in search is in the same equivalence class as the implicit reward learned by DPO (i.e., it's an optimal advantage function).
- **Evidence anchors:** [abstract], [section 5.2], [corpus]
- **Break condition:** If the search reward is not in the optimal advantage class, the equivalence breaks, and likelihood search may not optimize the intended objective.

## Foundational Learning

- **Concept:** Reinforcement Learning from Human Feedback (RLHF)
  - **Why needed here:** RLHF is the foundational paradigm that DPO builds upon. Understanding how RLHF uses policy gradients and reward modeling is essential to grasp DPO's simplification and theoretical connections.
  - **Quick check question:** What is the key difference between classical RLHF and DPO in terms of the optimization loop and reward modeling?

- **Concept:** Contextual Bandits vs. Markov Decision Processes (MDPs)
  - **Why needed here:** DPO is originally derived as a contextual bandit (single-step), while the paper reinterprets it in the token-level MDP (multi-step). Understanding the distinction and the conditions under which bandit methods can be extended to MDPs is crucial.
  - **Quick check question:** Under what conditions can a contextual bandit algorithm like DPO be validly applied to a sequential decision problem like language generation?

- **Concept:** Inverse Reinforcement Learning (IRL) and Q-Learning
  - **Why needed here:** The paper's theoretical contribution relies on the bijection between reward functions and Q-functions (Lemma 1) and the use of inverse Q-learning to derive DPO. Familiarity with IRL and Q-learning is necessary to follow the proofs and implications.
  - **Quick check question:** What is the relationship between an optimal policy π* and its corresponding Q-function Q* in maximum entropy RL, and how does this enable the reward-to-Q bijection?

## Architecture Onboarding

- **Component map:** Reference policy (πref) -> DPO policy (πθ) -> Preference dataset (D) -> Training loop
- **Critical path:**
  1. Initialize πθ (e.g., from SFT).
  2. For each batch of preference data, compute the DPO loss using the ratio of log-probabilities under πθ and πref.
  3. Update πθ to maximize the likelihood of preferred responses relative to rejected ones.
  4. The updated πθ implicitly learns a better Q-function and thus a better reward function.

- **Design tradeoffs:**
  - **Choice of πref:** SFT on all data vs. SFT on only chosen responses affects the trajectory of implicit rewards and potential overfitting.
  - **Temperature β:** Controls the sharpness of the policy; higher β leads to more deterministic policies but may reduce exploration.
  - **Reference policy stability:** Frequent changes to πref during training can destabilize learning; fixing πref after SFT is common.

- **Failure signatures:**
  - **Decreasing likelihood of chosen responses:** Expected under MaxEnt RL framing when using SFT on chosen responses as πref; not a failure but a theoretical prediction.
  - **Over-optimization with beam search:** Increasing beam size beyond a point leads to longer, lower-quality responses due to reward over-optimization.
  - **Poor credit assignment:** If the preference data is too sparse or noisy, the implicit reward may not accurately reflect token-level quality.

- **First 3 experiments:**
  1. **Credit assignment validation:** Generate summaries with and without introduced errors, color tokens by their implicit reward, and verify that erroneous tokens receive higher (worse) rewards.
  2. **Beam search vs. baseline:** Compare win rates and sample lengths for 1, 2, 5, and 10 beams on a held-out set; observe over-optimization trends.
  3. **SFT pre-training effect:** Train DPO with and without SFT pre-training; compare the evolution of implicit rewards for chosen vs. rejected responses to confirm the theoretical prediction about decreasing likelihood.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can DPO be effectively applied to multi-turn conversational systems, and what modifications would be needed? (The paper mentions "information elicitation in multi-turn dialogue" as a potential application but provides no empirical evidence.)
- **Open Question 2:** How does the choice of reference policy affect the implicit reward trajectory during DPO training, and can this be optimized? (The paper provides theoretical analysis but lacks empirical exploration of different reference policy choices.)
- **Open Question 3:** Can the theoretical connection between DPO and Q-learning be extended to other generative AI domains beyond text, such as image or video generation? (The paper provides preliminary treatment of diffusion models in Appendix B without empirical validation.)
- **Open Question 4:** What is the relationship between beam search performance and reward over-optimization in DPO, and how can this be mitigated? (The paper identifies this phenomenon but does not provide solutions or deeper analysis.)
- **Open Question 5:** How does DPO's credit assignment capability compare to traditional RLHF methods in compositional tasks like code generation or reasoning? (The paper only provides qualitative evidence on a summarization task without empirical comparison to RLHF.)

## Limitations

- The theoretical claims rely on strong assumptions about the token-level MDP being deterministic and states not being revisited, which may not hold in practice.
- Empirical validation is based on a single dataset (TL;DR) and model size (2.8B), limiting generalizability to other domains and larger models.
- The explanation for decreasing likelihoods during training is theoretically plausible but not extensively validated with ablation studies comparing different reference policy constructions.

## Confidence

- **High Confidence:** The derivation of DPO as an inverse Q-learning algorithm in the token-level MDP and the proof that DPO can represent any dense reward function within the token MDP are mathematically rigorous and well-supported.
- **Medium Confidence:** The empirical validation of credit assignment and the equivalence between beam search and reward-guided search are convincing but based on limited datasets and model sizes.
- **Low Confidence:** The explanation for decreasing likelihoods is theoretically plausible but not directly validated with extensive ablation studies or controlled experiments.

## Next Checks

1. **Stress-test the MDP assumptions:** Design an experiment where the token-level MDP is intentionally made non-deterministic (e.g., by injecting noise or allowing state revisits). Measure whether the implicit reward function learned by DPO still satisfies the Bellman equation and whether the bijection between r and Q* breaks down.

2. **Ablation on reference policy construction:** Train DPO models with different reference policies (e.g., SFT on all data, SFT on chosen responses, random policy) and track the evolution of implicit rewards for chosen vs. rejected responses. Verify that the decreasing likelihood of chosen responses is a consistent outcome only when using SFT on chosen responses as πref.

3. **Generalization across tasks and model sizes:** Replicate the credit assignment and beam search experiments on at least two additional datasets (e.g., summarization, dialogue) and model sizes (e.g., 7B, 13B). Use a larger held-out set (e.g., 1000 samples) and report win rates, sample lengths, and human evaluations to assess the robustness of the findings.