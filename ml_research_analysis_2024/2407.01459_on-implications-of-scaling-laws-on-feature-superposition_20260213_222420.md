---
ver: rpa2
title: On Implications of Scaling Laws on Feature Superposition
arxiv_id: '2407.01459'
source_url: https://arxiv.org/abs/2407.01459
tags:
- number
- features
- neurons
- superposition
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This theoretical note uses scaling laws to show that two widely
  held assumptions in interpretability cannot both be true: () the superposition hypothesis
  (where sparse features are linearly represented across a layer) is a complete theory
  of feature representation, and () features are universal (meaning models trained
  on the same data with equal performance learn identical features). The argument
  is based on the observation that a model''s computational capacity scales with parameters
  while its representational capacity scales with neurons.'
---

# On Implications of Scaling Laws on Feature Superposition

## Quick Facts
- arXiv ID: 2407.01459
- Source URL: https://arxiv.org/abs/2407.01459
- Authors: Pavan Katta
- Reference count: 2
- One-line primary result: Superposition hypothesis and feature universality cannot both be true under current scaling laws

## Executive Summary
This theoretical note examines the relationship between neural network scaling laws and feature representation through the lens of the superposition hypothesis. The author argues that two widely held assumptions in interpretability - that sparse features are linearly represented across layers (superposition hypothesis) and that features are universal across models with equal performance - cannot both be true. The argument is based on the observation that while computational capacity scales with parameters, representational capacity scales with neurons. By comparing models with different aspect ratios but equal parameters, the author demonstrates that maintaining equal performance would require features to have different sparsity levels depending on the model architecture, contradicting the assumption that sparsity is a property of the data itself.

## Method Summary
The paper presents a theoretical argument using scaling laws and compressed sensing bounds to examine the implications of feature superposition on model architecture. It compares two models with equal parameters but different aspect ratios (neurons per layer vs. layers) and applies the superposition hypothesis to show that maintaining equal performance would require features to have different sparsity levels. The argument relies on established scaling laws showing performance invariance across aspect ratios, the mathematical relationship between superposition and sparsity from compressed sensing theory, and the assumption that sparsity is a property of the data rather than the model architecture.

## Key Results
- Models with equal parameters but different aspect ratios achieve equal performance according to scaling laws
- Under superposition hypothesis, the degree of superposition must be constant for the same features across models
- This leads to a contradiction where the same features must have different sparsity levels depending on model architecture, violating feature universality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature universality cannot hold if superposition is the complete theory of feature representation
- Mechanism: Two models with equal parameters but different aspect ratios achieve equal performance. Under superposition, the degree of superposition (features per neuron) must be constant for the same features across models. However, models with different aspect ratios have different total neurons for the same parameter count, leading to different features per neuron. This forces features to have different sparsity levels, contradicting the assumption that sparsity is a property of the data itself.
- Core assumption: Scaling laws predict equal performance for a range of aspect ratios with fixed parameters
- Evidence anchors:
  - [abstract] The argument is based on the observation that a model's computational capacity scales with parameters while its representational capacity scales with neurons
  - [section] Model B, with higher degree of superposition, should have sparser features compared to Model A. But, sparsity of a feature is a property of the data itself, and the same feature can't be sparser in Model B if both models are trained on the same data
- Break condition: If scaling laws do not predict equal performance across aspect ratios, or if sparsity is not purely a property of the data but also depends on model architecture

### Mechanism 2
- Claim: The number of features a model can represent is constrained by the number of neurons, not parameters
- Mechanism: A model's computational capacity (FLOPs) scales with parameters (2N), but its representational capacity (number of features it can represent) scales with neurons. When comparing models with equal parameters but different aspect ratios, the total number of neurons differs. This leads to different numbers of features per neuron, which under superposition requires different sparsity levels for the same features
- Core assumption: The superposition hypothesis states that features can be linearly represented in each layer, and the number of features scales with the degree of superposition
- Evidence anchors:
  - [section] Let's apply the superposition hypothesis, which states that features can be linearly represented in each layer. Since both models achieve equal loss on the same dataset, it's reasonable to assume that they have learned the same features
  - [section] So Model B, with higher degree of superposition, should have sparser features compared to Model A
- Break condition: If features can be represented across multiple layers (cross-layer superposition) or if the relationship between neurons and features is not linear

### Mechanism 3
- Claim: The inconsistency between superposition and feature universality arises from the model's aspect ratio affecting the degree of superposition
- Mechanism: For two models with equal parameters but different aspect ratios, the degree of superposition (features per neuron) differs. Under superposition, a higher degree of superposition requires sparser features to avoid interference. However, if the models learn the same features, the sparsity should be constant, leading to a contradiction
- Core assumption: The degree of superposition is inversely related to feature sparsity (Equation 4: n/m = 1/((1-S)log(1-S)))
- Evidence anchors:
  - [section] Elhage et al. show that, using lower bounds of compressed sensing, if we want to recover n features compressed in m neurons (where n > m), the bound is m = Î©(-n(1-S)log(1-S)), where 1-S is the sparsity of the features
  - [section] So Model B, with higher degree of superposition, should have sparser features compared to Model A
- Break condition: If the relationship between superposition and sparsity is not as described by compressed sensing bounds, or if models can achieve equal performance with different feature sets

## Foundational Learning

- Concept: Scaling laws for neural networks
  - Why needed here: The argument relies on the observation that model performance is invariant over a wide range of aspect ratios with fixed parameters
  - Quick check question: What is the relationship between a model's parameters, data size, and loss according to scaling laws?

- Concept: Feature superposition hypothesis
  - Why needed here: The argument assumes that superposition is a complete theory of feature representation, meaning features are linearly represented across a layer
  - Quick check question: What is the superposition hypothesis, and how does it allow models to represent more features than neurons?

- Concept: Feature universality
  - Why needed here: The argument assumes that features are universal, meaning models trained on the same data with equal performance learn identical features
  - Quick check question: What does feature universality mean, and why is it relevant to comparing models with different architectures?

## Architecture Onboarding

- Component map:
  Transformer models with varying aspect ratios (number of layers vs. neurons per layer) -> Superposition hypothesis (features linearly represented across layers) -> Scaling laws (relationship between parameters, data size, and performance) -> Compressed sensing bounds (relationship between features, neurons, and sparsity)

- Critical path:
  1. Understand scaling laws and their implications for model performance across aspect ratios
  2. Grasp the superposition hypothesis and how it allows models to represent more features than neurons
  3. Recognize the contradiction that arises when models with different aspect ratios learn the same features under superposition
  4. Explore potential solutions, such as alternative compression schemes or cross-layer superposition

- Design tradeoffs:
  - Superposition allows for efficient feature representation but introduces interference between features
  - Higher degree of superposition requires sparser features to mitigate interference
  - Cross-layer superposition could alleviate the inconsistency but requires features to grow superlinearly with the number of neurons

- Failure signatures:
  - If scaling laws do not predict equal performance across aspect ratios, the argument falls apart
  - If the relationship between superposition and sparsity is not as described by compressed sensing bounds, the contradiction may not exist
  - If feature universality is not a valid assumption, the argument may not hold

- First 3 experiments:
  1. Train two transformer models with equal parameters but different aspect ratios on the same dataset and compare their learned features
  2. Vary the aspect ratio of a transformer model while keeping parameters constant and measure the degree of superposition and feature sparsity
  3. Explore alternative compression schemes that are not based on superposition and compare their performance to traditional superposition-based models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can cross-layer superposition schemes maintain both scaling law predictions and feature universality while accounting for different aspect ratios?
- Basis in paper: [explicit] The author proposes cross-layer superposition as a potential solution, noting that features represented across multiple layers should grow superlinearly with neurons
- Why unresolved: No mathematical framework has been developed to quantify how features would scale across layers under this scheme, and it's unclear if such a scheme could maintain both the computational capacity-parameter relationship and feature universality
- What evidence would resolve it: A formal mathematical proof showing how cross-layer superposition can scale with parameters while maintaining equal performance across different aspect ratios, or experimental results from models trained with different architectures demonstrating feature universality

### Open Question 2
- Question: What alternative compression schemes beyond simple superposition could maintain feature universality while respecting scaling laws?
- Basis in paper: [explicit] The author speculates about needing "completely new compression scheme compared to superposition" and mentions that "methods such as Dictionary learning which disentangle features assuming superposition hypothesis have been successful"
- Why unresolved: While dictionary learning is mentioned as a current approach, the paper doesn't propose or analyze specific alternative compression schemes that could resolve the tension between scaling laws and feature universality
- What evidence would resolve it: Development and experimental validation of a new compression scheme that can represent features as a function of parameters rather than neurons, while maintaining equal performance across different model architectures

### Open Question 3
- Question: Does the relationship between computational capacity and representational capacity differ for architectures beyond standard transformers?
- Basis in paper: [inferred] The argument relies on transformer scaling laws and assumes the computational capacity-parameter relationship holds across different aspect ratios, but this hasn't been tested for other architectures
- Why unresolved: The paper focuses specifically on transformers and their scaling laws, but other architectures might have different relationships between parameters, neurons, and computational capacity
- What evidence would resolve it: Empirical studies comparing scaling laws and feature representation across multiple neural network architectures (CNNs, RNNs, transformers with different attention mechanisms, etc.)

## Limitations

- The argument relies on the strong assumption that model performance is truly invariant across aspect ratios with fixed parameters, which may only hold approximately
- Compressed sensing bounds provide theoretical limits but their exact applicability to neural network feature representation remains uncertain
- The assumption that sparsity is purely a property of the data rather than being influenced by model architecture may be oversimplified

## Confidence

**High Confidence:** The core mathematical structure of the argument is sound - if scaling laws hold as stated and superposition is the complete theory of feature representation, then the inconsistency with feature universality follows logically. The relationship between parameters, neurons, and computational capacity is well-established.

**Medium Confidence:** The applicability of compressed sensing bounds to neural network feature representation. While the mathematical relationship is correct, whether these bounds accurately describe how neural networks actually represent features remains uncertain.

**Low Confidence:** The assumption that model performance is truly invariant across aspect ratios with fixed parameters. Scaling laws show approximate invariance, but the exact nature of this relationship and its implications for feature representation may be more nuanced than the argument suggests.

## Next Checks

1. **Empirical validation of aspect ratio invariance:** Train multiple transformer models with equal parameters but systematically varied aspect ratios on the same dataset. Measure both performance and feature similarity using techniques like centered kernel alignment (CKA) or canonical correlation analysis to verify whether models truly learn identical features.

2. **Testing compressed sensing bounds in practice:** Implement feature reconstruction experiments where known sparse features are compressed into fewer neurons, then attempt recovery. Compare the empirical relationship between number of features, neurons, and required sparsity against the theoretical bounds to verify their applicability.

3. **Exploring cross-layer superposition:** Design a modified training setup where features can be represented across multiple layers rather than within single layers. Test whether this approach can reconcile the tension between equal performance across aspect ratios and feature universality by allowing superlinear growth in representable features.