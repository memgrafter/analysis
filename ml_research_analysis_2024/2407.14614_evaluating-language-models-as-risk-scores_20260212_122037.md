---
ver: rpa2
title: Evaluating language models as risk scores
arxiv_id: '2407.14614'
source_url: https://arxiv.org/abs/2407.14614
tags:
- risk
- score
- scores
- calibration
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces folktexts, a software package for evaluating
  language models (LLMs) as risk scores using real-world US Census data. The key idea
  is to map tabular data features into natural language prompts and extract risk scores
  from LLM output probabilities, enabling evaluation of calibration and predictive
  performance on underspecified prediction tasks.
---

# Evaluating language models as risk scores

## Quick Facts
- arXiv ID: 2407.14614
- Source URL: https://arxiv.org/abs/2407.14614
- Authors: André F. Cruz; Moritz Hardt; Celestine Mendler-Dünner
- Reference count: 40
- Primary result: LLMs show strong predictive signal (high AUC) but are widely miscalibrated when used as risk scores on census data

## Executive Summary
This paper introduces folktexts, a software package that evaluates language models as risk scores using real-world US Census data. The key innovation maps tabular data features into natural language prompts and extracts risk scores from LLM output probabilities, enabling evaluation of calibration and predictive performance on underspecified prediction tasks. The authors evaluate 17 recent LLMs across five benchmark tasks, finding that while LLMs have strong predictive signal (high AUC), they are widely miscalibrated. Base models tend to overestimate uncertainty while instruction-tuned models underestimate it, showing polarized score distributions regardless of true data uncertainty.

## Method Summary
The folktexts package converts structured census features into natural language descriptions and formulates binary questions. Models are queried using multiple-choice or verbalized numeric prompts, with token probabilities serving as risk scores. The package provides a systematic framework for translating between natural language interfaces and standard machine learning type signatures, supporting both local and web-hosted models. Evaluation metrics include calibration (ECE), AUC, accuracy, Brier score, and subgroup calibration across five benchmark tasks using ACS PUMS data from 3.2M individuals.

## Key Results
- LLMs achieve high AUC scores across all benchmark tasks, demonstrating strong predictive signal
- Instruction-tuned models produce overconfident risk scores with polarized distributions near 0 or 1
- Verbalized numeric prompting substantially improves calibration compared to multiple-choice formats
- LLMs underperform supervised baselines on certain tasks despite high AUC scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can be transformed into risk scorers by mapping tabular data to natural language prompts and extracting token probabilities.
- Mechanism: The folktexts package converts structured census features into natural language descriptions and formulates binary questions. The model's next-token probabilities for answer choices serve as calibrated risk scores.
- Core assumption: The LLM's probability distribution over tokens reflects its confidence in the answer, and this can be mapped to a risk score.
- Evidence anchors:
  - [abstract] "The folktexts package offers a systematic way to translate between the natural language interface and the standard machine learning type signature."
  - [section 3.2] "We use a standard multiple-choice prompting format to elicit outcome predictions from LLMs. The model's confidence on a given answer is given by the next token probabilities for A and B."
  - [corpus] Weak - no direct corpus evidence for this specific mechanism.
- Break condition: If the LLM's token probabilities are not calibrated or if the prompt construction introduces bias.

### Mechanism 2
- Claim: Instruction-tuned models produce overconfident risk scores due to polarized answer distributions.
- Mechanism: Fine-tuning on curated datasets leads to sharp decision boundaries and high-confidence predictions even when underlying data uncertainty is high.
- Core assumption: The instruction-tuning process emphasizes confident, definitive answers over calibrated uncertainty.
- Evidence anchors:
  - [abstract] "Base models consistently overestimate outcome uncertainty, while instruction-tuned models underestimate uncertainty and produce over-confident risk scores."
  - [section 4.2] "instruction-tuned models often output scores near 0 or 1... while base models consistently produce low-variance distributions centered around 0.5."
  - [corpus] Weak - no direct corpus evidence for this specific mechanism.
- Break condition: If base models also show overconfidence or if numeric prompting fails to improve calibration.

### Mechanism 3
- Claim: Verbalized numeric prompting improves calibration by directly eliciting probability estimates.
- Mechanism: Instead of choosing between discrete options, the model is asked to produce a numeric value, which better captures uncertainty.
- Core assumption: LLMs can more accurately express uncertainty when prompted for continuous values rather than discrete choices.
- Evidence anchors:
  - [abstract] "A separate experiment using verbalized chat-style risk queries yields substantially improved calibration across instruction-tuned models."
  - [section 3.2] "The above income query would be: 'Question: What is the probability that this person's yearly income is above $50,000? Answer (between 0 and 1):'"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism.
- Break condition: If numeric prompting introduces bias or if the model defaults to tied scores.

## Foundational Learning

- Concept: Calibration and Expected Calibration Error (ECE)
  - Why needed here: The paper evaluates models based on how well their predicted probabilities match true outcome frequencies.
  - Quick check question: What does it mean for a model to be calibrated, and how is ECE calculated?

- Concept: Area Under the Curve (AUC) as a rank-based metric
  - Why needed here: AUC measures predictive signal independently of calibration, allowing separation of discrimination from calibration.
  - Quick check question: How does AUC differ from accuracy in evaluating model performance?

- Concept: Survey data as a source of natural uncertainty
  - Why needed here: Census data contains inherent outcome uncertainty, making it suitable for evaluating risk scoring.
  - Quick check question: Why is natural outcome uncertainty important for evaluating risk scores?

## Architecture Onboarding

- Component map:
  - folktexts package -> Provides datasets, prompt templates, and evaluation tools
  - LLMClassifier -> Handles model inference and risk score extraction
  - Benchmark -> Runs experiments and computes metrics
  - TaskMetadata -> Defines feature-target mappings for each prediction task

- Critical path:
  1. Load census data and define prediction task
  2. Convert tabular rows to natural language prompts
  3. Query LLM and extract token probabilities
  4. Compute risk scores and evaluate calibration/AUC

- Design tradeoffs:
  - Multiple-choice vs. numeric prompting: Discrete options vs. continuous probability estimates
  - Base vs. instruction-tuned models: Calibration vs. accuracy tradeoffs
  - Feature selection: More features increase predictive power but may reduce uncertainty

- Failure signatures:
  - High ECE with low AUC: Model lacks predictive signal
  - Low ECE with high AUC: Model may be overfitting or task is unrealizable
  - Polarized score distributions: Instruction-tuning may be causing overconfidence

- First 3 experiments:
  1. Run ACSIncome benchmark with Llama 3 8B using multiple-choice prompting
  2. Repeat with numeric prompting to compare calibration
  3. Compare base vs. instruction-tuned versions of the same model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does instruction-tuning affect uncertainty quantification across different types of tasks beyond the benchmark tasks studied?
- Basis in paper: [explicit] The paper shows instruction-tuning leads to worse calibration on multiple-choice prompts but improved calibration on verbalized numeric prompts for ACS tasks.
- Why unresolved: The study only evaluates 5 specific benchmark tasks. It's unclear whether these findings generalize to other types of tasks or domains.
- What evidence would resolve it: Systematic evaluation of instruction-tuned models across a diverse set of tasks (e.g., medical diagnosis, legal reasoning, creative writing) using both multiple-choice and verbalized numeric prompting.

### Open Question 2
- Question: What specific aspects of instruction-tuning cause the polarization of score distributions observed in the study?
- Basis in paper: [explicit] The paper observes that instruction-tuned models produce polarized score distributions regardless of true data uncertainty, but doesn't identify the underlying cause.
- Why unresolved: The study doesn't investigate the training process or architectural differences between base and instruction-tuned models that might explain this phenomenon.
- What evidence would resolve it: Detailed analysis of the instruction-tuning process, including ablation studies on different instruction formats, dataset sizes, and model architectures to identify which factors contribute to score polarization.

### Open Question 3
- Question: How do different feature selection strategies impact the calibration and predictive performance of language models?
- Basis in paper: [inferred] The paper shows that adding features generally increases predictive signal but has inconsistent effects on calibration. It also presents feature importance analysis showing differences between models and supervised learning baselines.
- Why unresolved: The study uses a fixed feature set for each task and doesn't explore how different feature selection strategies might affect model performance or calibration.
- What evidence would resolve it: Systematic experiments varying feature selection methods (e.g., correlation-based, wrapper methods, domain knowledge) and measuring their impact on calibration, AUC, and feature importance across multiple tasks.

## Limitations

- Prompt engineering dependency creates sensitivity to how tabular data is verbalized
- Task coverage limited to census-derived features may not generalize to other risk scoring domains
- Closed-model API dependencies create reproducibility barriers and potential cost issues

## Confidence

**High Confidence**:
- LLMs can extract predictive signal from census data (demonstrated by consistently high AUC scores across models)
- Instruction-tuned models produce more polarized score distributions than base models
- The folktexts package provides a functional framework for evaluating LLMs as risk scorers

**Medium Confidence**:
- Base models systematically overestimate uncertainty while instruction-tuned models underestimate it
- Numeric prompting improves calibration compared to multiple-choice formats
- LLMs underperform supervised baselines on certain tasks despite high AUC

**Low Confidence**:
- The specific calibration issues are inherent to instruction-tuning methodology rather than implementation details
- The magnitude of calibration differences between model families is consistent across all tasks
- Current LLMs cannot be effectively calibrated for risk scoring without architectural modifications

## Next Checks

1. **Prompt Ablation Study**: Systematically vary prompt formulations (feature ordering, descriptive language, question framing) across a subset of models and tasks to quantify sensitivity of calibration and AUC scores to prompt engineering choices.

2. **Uncertainty Ground