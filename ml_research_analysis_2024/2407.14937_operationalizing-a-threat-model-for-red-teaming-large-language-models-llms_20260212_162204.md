---
ver: rpa2
title: Operationalizing a Threat Model for Red-Teaming Large Language Models (LLMs)
arxiv_id: '2407.14937'
source_url: https://arxiv.org/abs/2407.14937
tags:
- cited
- semanticscholar
- corpusid
- attacks
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive threat model and taxonomy
  for red-teaming large language models (LLMs), organizing attacks based on entry
  points across the LLM development and deployment lifecycle. It categorizes attacks
  into five main types: Direct Attacks (jailbreak, inversion, side-channel), Infusion
  Attacks (indirect prompt injection), Inference Attacks (activation engineering),
  Training Attacks (data poisoning, weight tampering), and Compound Systems Attacks.'
---

# Operationalizing a Threat Model for Red-Teaming Large Language Models (LLMs)

## Quick Facts
- arXiv ID: 2407.14937
- Source URL: https://arxiv.org/abs/2407.14937
- Reference count: 40
- Authors: Apurv Verma; Satyapriya Krishna; Sebastian Gehrmann; Madhavan Seshadri; Anu Pradhan; Tom Ault; Leslie Barrett; David Rabinowitz; John Doucette; NhatHai Phan

## Executive Summary
This paper develops a comprehensive threat model and taxonomy for red-teaming large language models by organizing attacks based on entry points across the LLM development and deployment lifecycle. The framework categorizes attacks into five main types based on required access levels: Direct Attacks, Infusion Attacks, Inference Attacks, Training Attacks, and Compound Systems Attacks. The work surveys defense strategies and provides practical recommendations for practitioners to improve LLM security through systematic red-teaming exercises.

## Method Summary
The paper develops a threat model based on user interaction through prompting, application layers beyond LLMs, and model internals/training data. It organizes attacks into five main types (Direct, Infusion, Inference, Training, Compound Systems) based on access required, ranging from simple application input to full model weights and training data. The methodology involves reviewing previous research on LLM attacks and defenses, risk taxonomies from various sources, and mapping attacks to specific system components and access levels.

## Key Results
- Taxonomy organizes attacks by access level from application input → API parameters → in-context data → model activations → training artifacts
- Manual attacks require only application interface access, while automated attacks need programmatic API access
- Certification methods like "erase-and-check"