---
ver: rpa2
title: On The Global Convergence Of Online RLHF With Neural Parametrization
arxiv_id: '2410.15610'
source_url: https://arxiv.org/abs/2410.15610
tags:
- equation
- rlhf
- gradient
- neural
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes the first global convergence analysis for
  online reinforcement learning from human feedback (RLHF) with neural network parameterization.
  The authors tackle the distribution shift problem inherent in RLHF by formulating
  it as a bilevel optimization problem, introducing a novel first-order algorithm
  to solve it.
---

# On The Global Convergence Of Online RLHF With Neural Parametrization

## Quick Facts
- arXiv ID: 2410.15610
- Source URL: https://arxiv.org/abs/2410.15610
- Reference count: 40
- Primary result: First global convergence analysis for online RLHF with neural networks, sample complexity of $\epsilon^{-7/2}$ under weak gradient domination

## Executive Summary
This paper establishes the first global convergence analysis for online reinforcement learning from human feedback (RLHF) with neural network parameterization. The authors tackle the distribution shift problem inherent in RLHF by formulating it as a bilevel optimization problem and introducing a novel first-order algorithm to solve it. The key contribution is proving sample complexity of $\epsilon^{-7/2}$ under the weak gradient domination assumption, which is weaker than previous assumptions and allows for general nonlinear reward functions rather than just linear ones.

## Method Summary
The method formulates RLHF as a bilevel optimization problem where the upper level optimizes a reward function and the lower level optimizes a policy. The algorithm uses a first-order approach with DQN critic estimation to solve the lower-level policy optimization problem at each iteration, while updating the reward function using gradient-based methods. The key innovation is using a proxy objective that eliminates the need for computing hyper-gradients while still approximating the true bilevel gradient.

## Key Results
- Establishes sample complexity of $\epsilon^{-7/2}$ for online RLHF with neural parameterization
- Introduces weak gradient domination assumption, weaker than previous convexity requirements
- Provides first global convergence guarantee for RLHF with neural networks
- Sample complexity is slightly worse than vanilla actor-critic but accounts for reward function estimation from preference data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weak gradient domination allows convergence analysis beyond linear reward functions
- Mechanism: Instead of requiring strong convexity, weak gradient domination provides a looser bound on the relationship between gradient norm and optimality gap, enabling analysis of general nonlinear reward functions
- Core assumption: √µ₁(Φ* - Φ(φ)) ≤ ||∇Φ(φ)|| where µ₁ > 0 is the weak gradient domination constant
- Evidence anchors:
  - [abstract]: "sample complexity of ϵ^{-7/2} under the weak gradient domination assumption, which is weaker than previous assumptions and allows for general nonlinear reward functions rather than just linear ones"
  - [section 6.1]: Formal statement of Assumption 1(a) with weak gradient domination property
  - [corpus]: No direct evidence in neighbors, but related to convergence conditions in "Global Convergence and Rich Feature Learning" which discusses gradient conditions

### Mechanism 2
- Claim: Bilevel formulation addresses distribution shift by incorporating policy dependence into reward optimization
- Mechanism: By formulating RLHF as max_φ Φ(φ) = G(φ, λ*(φ)) where λ*(φ) = argmax_λ J(λ, φ), the algorithm ensures the reward function is optimized for the policy that actually generates the data
- Core assumption: The optimal policy λ*(φ) for reward φ is well-defined and depends continuously on φ
- Evidence anchors:
  - [section 4]: "In order to account for the distribution drift, Chakraborty et al. (2024c); Ding et al. (2024a) proposed that the RLHF problem is inherently bilevel in nature given by max_φ Φ(φ) = G(φ, λ*(φ)) where λ* ∈ argmax_λ J(λ, φ)"
  - [section 5]: Detailed description of how the gradient of the upper-level objective depends on both reward and policy components

### Mechanism 3
- Claim: Proxy objective Φ_σ eliminates the need for hyper-gradient computation
- Mechanism: Instead of computing the gradient of λ*(φ) with respect to φ, the proxy objective Φ_σ(φ) = max_λ (G(φ, λ) + J(λ*) - J(λ, φ)/σ) allows first-order methods to approximate the bilevel gradient without explicit hyper-gradient calculation
- Core assumption: ∇Φ_σ(φ) ≈ ∇Φ(φ) when σ is appropriately chosen
- Evidence anchors:
  - [section 4]: "The key advantage of this formulation is that we do not have to calculate the gradient of λ* with respect to φ"
  - [section 5]: Algorithm 1 explicitly uses the proxy objective formulation without computing hyper-gradients

## Foundational Learning

- Concept: Bilevel optimization
  - Why needed here: RLHF naturally forms a bilevel problem where reward learning depends on policy optimization, and their interdependence causes distribution shift
  - Quick check question: Can you explain why treating reward learning and policy optimization as separate single-level problems leads to distribution shift?

- Concept: Policy gradient theorem
  - Why needed here: The algorithm requires computing ∇_λ J(λ, φ) = E_(s,a)∼d_π_λ_ν [∇_λ log π_λ Q_λ^φ], which is the foundation for updating policy parameters in the inner loop
  - Quick check question: What is the policy gradient theorem formula and how does it relate to the expected return?

- Concept: Bellman operator and Q-function approximation
  - Why needed here: The algorithm uses neural network parameterization for both policy and Q-function, requiring understanding of how the Bellman operator T^π acts on value functions and how to approximate it with neural networks
  - Quick check question: How does the Bellman operator T^π relate to the optimal Q-function Q^* and what challenges arise when using neural networks to approximate it?

## Architecture Onboarding

- Component map: Data generation -> Critic update (Algorithm 2) -> Policy update (inner loop) -> Reward update (outer loop) -> Convergence check
- Critical path: Data generation → Critic update → Policy update → Reward update → Convergence check
- Design tradeoffs:
  - Batch size B vs. variance: Larger B reduces gradient variance but increases computational cost
  - σ parameter: Balances approximation accuracy of proxy objective vs. computational tractability
  - Neural network architecture depth vs. approximation error: Deeper networks may reduce ǫ_approx but increase training complexity
- Failure signatures:
  - High variance in gradient estimates (σ_B large) → Unstable training
  - Policy collapse (λ updates diverging) → Incorrect learning rate or critic failure
  - Reward function not learning (φ updates near zero) → Insufficient preference data or poor initialization
- First 3 experiments:
  1. Validate critic convergence on a simple MDP (GridWorld) with known Q-values
  2. Test policy gradient updates on a bandit problem with linear rewards
  3. Run full algorithm on a small RLHF problem (e.g., preference-based CartPole) with tabular policies to verify bilevel structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sample complexity of O(ε^{-7/2}) for online RLHF with neural parameterization compare to the practical sample efficiency observed in real-world implementations?
- Basis in paper: [explicit] The paper establishes a theoretical sample complexity of O(ε^{-7/2}) for their algorithm, noting this is slightly worse than the O(ε^{-3}) for vanilla actor-critic but attributes this to the reward function needing estimation from preference data
- Why unresolved: The theoretical bound provides worst-case guarantees but doesn't reflect empirical performance. The paper focuses on theoretical analysis without presenting empirical validation or comparing theoretical vs. practical sample requirements
- What evidence would resolve it: Empirical experiments comparing sample efficiency of the proposed algorithm against baseline methods on standard RLHF benchmarks, along with statistical analysis of how theoretical vs. observed sample complexity varies across problem instances

### Open Question 2
- Question: What is the impact of the weak gradient domination assumption on the algorithm's performance when applied to non-linear reward functions in practice?
- Basis in paper: [explicit] The authors claim their weak gradient domination assumption is "weaker than previous assumptions" and allows for "general nonlinear reward functions rather than just linear ones"
- Why unresolved: The paper proves convergence under this assumption but doesn't demonstrate through experiments how the algorithm performs when this assumption is only approximately satisfied or violated, particularly for highly non-linear reward landscapes
- What evidence would resolve it: Systematic experiments varying the degree of non-linearity in reward functions, measuring performance degradation as the weak gradient domination assumption becomes less valid, and comparing against methods with stronger assumptions

### Open Question 3
- Question: How sensitive is the algorithm to the choice of hyperparameters (σ, β', τ, τ', η) and what are principled approaches for tuning them in practice?
- Basis in paper: [inferred] The theoretical analysis requires specific settings (σ^2 = O(ε), B = O(ε)^{-1}, etc.) but the paper doesn't discuss practical hyperparameter tuning strategies or the sensitivity of performance to deviations from these settings
- Why unresolved: While the authors derive optimal theoretical settings, real-world applications require practical guidance on hyperparameter selection when problem parameters are unknown, and the stability of performance across different hyperparameter regimes
- What evidence would resolve it: Sensitivity analysis showing performance across hyperparameter ranges, automated tuning procedures that work without prior knowledge of problem parameters, and empirical validation that performance degrades gracefully when hyperparameters deviate from theoretical optima

## Limitations
- Theoretical analysis relies heavily on the weak gradient domination assumption, which may not hold for complex neural network architectures
- Sample complexity bound of $\epsilon^{-7/2}$ is derived under idealized conditions and may not reflect practical performance
- Proxy objective formulation introduces approximation error whose impact on convergence is not fully characterized

## Confidence

**High confidence**: The bilevel optimization formulation for RLHF is well-established and the connection to distribution shift is clearly articulated. The mechanism of using a proxy objective to avoid hyper-gradient computation is sound.

**Medium confidence**: The weak gradient domination assumption provides a reasonable theoretical foundation, but its applicability to deep neural networks in practical RLHF settings remains uncertain. The sample complexity analysis is rigorous but may be conservative.

**Low confidence**: The empirical validation of theoretical bounds is limited, and the performance of the proposed algorithm in real-world RLHF scenarios is not demonstrated.

## Next Checks

1. Test the weak gradient domination assumption empirically across different neural network architectures and RLHF tasks to assess its practical validity
2. Conduct ablation studies comparing the proxy objective approach with exact hyper-gradient methods to quantify the approximation error
3. Implement the algorithm on a standard RLHF benchmark (e.g., preference-based Humanoid locomotion) to verify whether the theoretical sample complexity bounds are achievable in practice