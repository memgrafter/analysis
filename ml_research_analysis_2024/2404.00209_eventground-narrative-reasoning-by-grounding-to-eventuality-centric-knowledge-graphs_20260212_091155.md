---
ver: rpa2
title: 'EventGround: Narrative Reasoning by Grounding to Eventuality-centric Knowledge
  Graphs'
arxiv_id: '2404.00209'
source_url: https://arxiv.org/abs/2404.00209
tags:
- event
- knowledge
- events
- language
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of narrative reasoning by grounding
  free-texts to eventuality-centric knowledge graphs (KGs). The authors identify two
  critical problems: event representation and sparsity.'
---

# EventGround: Narrative Reasoning by Grounding to Eventuality-centric Knowledge Graphs

## Quick Facts
- arXiv ID: 2404.00209
- Source URL: https://arxiv.org/abs/2404.00209
- Reference count: 0
- EventGround improves event matching performance by 86.6% through normalization and partial information extraction

## Executive Summary
EventGround addresses narrative reasoning by grounding free-texts to eventuality-centric knowledge graphs (KGs). The framework tackles two critical problems: event representation and sparsity. It employs semantic parsing-based event extraction with event normalization to preserve co-reference information, and partial information extraction to address event sparsity. The system grounds partial events to KGs to obtain joint reasoning subgraphs, which are then used by graph neural network (GNN) or large language model (LLM) based graph reasoning models. Experimental results show that EventGround consistently outperforms baseline models and achieves state-of-the-art performance on three narrative reasoning tasks while providing interpretable evidence.

## Method Summary
EventGround uses semantic role labeling (SRL) to extract events from text, followed by event normalization that replaces personal words with special tokens ([P0], [P1]) to preserve co-reference information. The framework then applies partial information extraction (PIE) to create multi-level abstractions of events by systematically dropping event arguments in order of semantic importance. These partial events are matched to the ASER eventuality-centric KG using semantic similarity (SBERT) with a threshold of 0.65. The system retrieves joint reasoning subgraphs containing both context events and KG anchor events within 3 hops, and performs reasoning using either GNN-based (RGCN) or LLM-based (ChatGPT) approaches.

## Key Results
- Achieves state-of-the-art performance on three narrative reasoning tasks (SCT-v1.0, SCT-v1.5, MCNC)
- Improves event matching performance by 86.6% through normalization and partial information extraction
- Consistently outperforms baseline models across all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1: Event Normalization with Personal Word Substitution
- Replaces personal words (e.g., "Tom", "he") with special tokens ([P0], [P1]) during semantic parsing
- Preserves co-reference information while enabling events to be matched across different contexts
- Core assumption: Personal words are the primary source of event sparsity in eventuality-centric KGs
- Break condition: Normalization may introduce ambiguity when different people share the same role or co-reference chains become too complex

### Mechanism 2: Partial Information Extraction (PIE)
- Creates multi-level abstractions of events by systematically dropping event arguments (ARGM → ARG2,3,4 → ARG1 → ARG0)
- Addresses event sparsity by increasing matching probability with KG entries through higher-level abstractions
- Core assumption: Higher-level abstractions are more likely to be present in KGs, compensating for loss of semantic detail
- Break condition: Abstraction process may remove too much semantic content, making matches too generic to be useful

### Mechanism 3: Joint Subgraph Construction
- Combines original context events with retrieved KG subgraphs to create a reasoning graph
- Preserves both narrative context and background knowledge relationships
- Core assumption: Graph neural networks can effectively reason over this hybrid structure to leverage both context and external knowledge
- Break condition: Graph may become too dense or disconnected, making GNN reasoning ineffective

## Foundational Learning

- **Semantic role labeling (SRL) and argument importance hierarchy**
  - Why needed: Event extraction relies on identifying verb-centric events with their semantic arguments, and PIE requires understanding which arguments are most semantically important
  - Quick check: What are the typical roles in PropBank SRL annotation, and which are considered most important for semantic content?

- **Graph neural networks for relational reasoning**
  - Why needed: The framework uses GNNs to perform reasoning over joint knowledge subgraphs, requiring understanding of how GNNs handle different edge types and node representations
  - Quick check: How do relational graph convolutional networks (RGCNs) differ from standard GCNs in handling multi-relational data?

- **Semantic similarity vs token-level similarity**
  - Why needed: Event matching requires choosing between semantic similarity (using sentence transformers) and token-level approaches (like TF-IDF), with significant performance implications
  - Quick check: Why does semantic similarity outperform token-level similarity for event matching in this context?

## Architecture Onboarding

- **Component map**: Event Extraction -> Event Normalization -> Partial Information Extraction -> Event Matching -> Subgraph Retrieval -> Graph Construction -> Reasoning
- **Critical path**: Event Extraction → Normalization → PIE → Matching → Subgraph Retrieval → Graph Construction → Reasoning
- **Design tradeoffs**:
  - Depth of PIE abstraction vs matching accuracy
  - Subgraph retrieval radius (3 hops) vs computational efficiency
  - Semantic vs token-level matching accuracy
  - GNN complexity vs reasoning performance
- **Failure signatures**:
  - Low hit rate in event matching → Check normalization and PIE configuration
  - Poor reasoning performance → Verify subgraph connectivity and GNN training
  - High computational cost → Optimize event matching with Faiss indexing
- **First 3 experiments**:
  1. Baseline comparison: Run without normalization and PIE to establish performance drop
  2. Matching threshold tuning: Vary the L2 distance threshold to optimize hit rate vs precision
  3. PIE level ablation: Test different abstraction levels to find optimal argument dropping strategy

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains primarily demonstrated through ablation studies rather than direct comparisons with strong baseline models
- Evaluation focuses on three narrative reasoning datasets, which may not represent real-world application diversity
- Claims of interpretability benefits lack quantitative metrics for substantiation

## Confidence
- **High confidence**: Core methodology of using event normalization and partial information extraction to address sparsity is well-articulated and logically sound
- **Medium confidence**: 86.6% improvement claim supported by ablation studies but lacks comparison with alternative normalization approaches
- **Low confidence**: Interpretability claims not substantiated with quantitative metrics; potential limitations of semantic similarity-based event matching not addressed

## Next Validation Checks
1. **Event Normalization Ablation**: Compare EventGround's normalization approach with alternative strategies (entity linking, coreference resolution without substitution) to isolate the contribution of personal word substitution
2. **Cross-Dataset Generalization**: Evaluate EventGround on additional narrative reasoning datasets beyond the three currently used to assess robustness across different domains
3. **Interpretability Quantification**: Develop and apply quantitative metrics for evaluating the interpretability of EventGround's reasoning process, such as clarity of evidence paths or ability to explain reasoning steps to human evaluators