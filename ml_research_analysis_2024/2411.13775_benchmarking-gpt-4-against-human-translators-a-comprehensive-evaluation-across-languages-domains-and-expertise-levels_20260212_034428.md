---
ver: rpa2
title: 'Benchmarking GPT-4 against Human Translators: A Comprehensive Evaluation Across
  Languages, Domains, and Expertise Levels'
arxiv_id: '2411.13775'
source_url: https://arxiv.org/abs/2411.13775
tags:
- human
- translation
- translators
- gpt-4
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks GPT-4's translation quality against human
  translators of varying expertise levels across three language pairs and domains.
  Using systematic human evaluation with the MQM schema, GPT-4 achieves performance
  comparable to junior/medium-level translators in terms of total errors (17.35 major
  errors vs 18.19/3.27 for junior/medium), while lagging behind senior translators
  (1.83 major errors).
---

# Benchmarking GPT-4 against Human Translators: A Comprehensive Evaluation Across Languages, Domains, and Expertise Levels

## Quick Facts
- arXiv ID: 2411.13775
- Source URL: https://arxiv.org/abs/2411.13775
- Authors: Jianhao Yan; Pingchuan Yan; Yulong Chen; Jing Li; Xianchao Zhu; Yue Zhang
- Reference count: 40
- Key outcome: GPT-4 achieves performance comparable to junior/medium human translators across languages and domains, while lagging behind senior translators.

## Executive Summary
This study benchmarks GPT-4's translation quality against human translators of varying expertise levels across three language pairs and domains. Using systematic human evaluation with the MQM schema, GPT-4 achieves performance comparable to junior/medium-level translators in terms of total errors (17.35 major errors vs 18.19/3.27 for junior/medium), while lagging behind senior translators (1.83 major errors). Unlike traditional NMT systems, GPT-4 maintains consistent quality across resource-rich and resource-poor language pairs. Qualitative analysis reveals GPT-4's limitations in literal translation and lexical inconsistency, while human translators exhibit context over-interpretation and hallucination. The results demonstrate GPT-4's potential to replace junior/medium translators, marking a significant milestone in neural machine translation.

## Method Summary
The study employs a mixed-method approach combining automatic and human evaluation. Researchers selected three language pairs (English→Chinese, Chinese→English, English→German) and two domains (patent and literary). They translated 50 sentences per language pair using GPT-4 and collected human translations from junior, medium, and senior translators at Youdao Translation Company. Both automatic metrics (COMET, BERTScore) and human evaluation using the Multidimensional Quality Metrics (MQM) schema were applied. Human evaluators with professional translation backgrounds assessed translations on 13 error types across accuracy, fluency, and style dimensions.

## Key Results
- GPT-4's average error rate (17.35 major errors) falls between junior (18.19) and medium (3.27) translators, while senior translators achieve the lowest error rate (1.83)
- GPT-4 shows consistent performance across resource-rich (English→German) and resource-poor (Chinese→English) language pairs
- Qualitative analysis identifies GPT-4's strengths in literal translation and weaknesses in lexical consistency and context over-interpretation
- Human translators exhibit more instances of hallucination and context over-interpretation compared to GPT-4

## Why This Works (Mechanism)
The study demonstrates that GPT-4's underlying language understanding capabilities, trained on diverse multilingual corpora, enable it to perform translation tasks with consistency across different language pairs. The model's ability to maintain context and generate fluent translations stems from its transformer architecture and large-scale pretraining on web data, which provides broad linguistic knowledge beyond what traditional NMT systems achieve through parallel corpus training alone.

## Foundational Learning
- **MQM Schema**: A hierarchical error taxonomy for translation quality assessment; needed for standardized human evaluation across different translators and systems
- **Translationese**: The phenomenon where translations differ systematically from original target language text; quick check: compare fluency scores between human and machine translations
- **Resource-rich vs Resource-poor languages**: Classification based on availability of parallel training data; quick check: evaluate performance consistency across language pairs with varying data availability

## Architecture Onboarding
- **Component Map**: Web-scale pretraining -> Fine-tuning for translation -> Human evaluation
- **Critical Path**: Pretraining provides multilingual knowledge -> Fine-tuning adapts to translation task -> Human evaluation validates quality
- **Design Tradeoffs**: GPT-4 sacrifices some domain-specific accuracy for broader language coverage and consistency
- **Failure Signatures**: Lexical inconsistency, literal translation preference, context over-interpretation
- **First Experiments**: 1) Test GPT-4 on specialized domain translations (legal, medical) 2) Evaluate translation speed and cost-effectiveness 3) Compare performance across different GPT-4 model versions

## Open Questions the Paper Calls Out
None

## Limitations
- Study limited to three language pairs and two domains, potentially limiting generalizability
- Human evaluation relied on a small pool of translators from a single company, introducing potential selection bias
- Error analysis using MQM schema is subjective and may vary between evaluators
- Study did not account for translation speed or cost-effectiveness

## Confidence
- High confidence in GPT-4's comparable performance to junior/medium translators and its consistency across language pairs
- Medium confidence in specific error rate comparisons due to subjective nature of error classification
- Low confidence in generalizability to other languages, domains, or professional contexts not covered in the study

## Next Checks
1. Conduct a follow-up study with a larger and more diverse pool of human translators across multiple translation companies to validate the error rate comparisons and identify potential biases
2. Expand the evaluation to include additional language pairs, particularly those with varying levels of resource availability, and test GPT-4's performance in specialized domains such as legal, medical, or technical translations
3. Implement a longitudinal study to assess GPT-4's performance over time, as the model may be updated or fine-tuned, potentially impacting its translation quality and consistency