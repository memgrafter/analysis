---
ver: rpa2
title: Improving Dialogue Agents by Decomposing One Global Explicit Annotation with
  Local Implicit Multimodal Feedback
arxiv_id: '2403.11330'
source_url: https://arxiv.org/abs/2403.11330
tags:
- reward
- which
- human
- local
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GELI, a method for aligning LLM-based dialogue
  agents using global rewards at the session level while incorporating multimodal
  signals. GELI decomposes the single global explicit reward into local turn-level
  rewards, guided by local implicit multimodal feedback.
---

# Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback

## Quick Facts
- arXiv ID: 2403.11330
- Source URL: https://arxiv.org/abs/2403.11330
- Authors: Dong Won Lee; Hae Won Park; Yoon Kim; Cynthia Breazeal; Louis-Philippe Morency
- Reference count: 15
- One-line primary result: GELI improves dialogue agents by decomposing global rewards using multimodal feedback, achieving +9% emotional connection and +18% positivity in human evaluations.

## Executive Summary
This paper introduces GELI, a method for aligning LLM-based dialogue agents using global rewards at the session level while incorporating multimodal signals. GELI decomposes the single global explicit reward into local turn-level rewards, guided by local implicit multimodal feedback. The method employs randomized return decomposition for the global reward and crossmodal knowledge distillation for the local implicit rewards. Human evaluations show GELI consistently improves conversational metrics like emotional connection (+9%), positivity (+18%), and user inclination (+11%) compared to baselines. The approach generalizes across datasets and outperforms base LLAMA-2 models in long-term dialogue scenarios.

## Method Summary
GELI addresses the challenge of aligning dialogue agents using global session-level rewards by decomposing these rewards into turn-level signals guided by local implicit multimodal feedback. The method combines randomized return decomposition (RRD) for global explicit rewards with crossmodal knowledge distillation from multimodal signals (e.g., facial affect) to shape local implicit rewards. The framework trains a reward function that captures both global and local signals, then uses this reward function in RLHF to adapt LLAMA-2 dialogue agents. The approach is evaluated through human studies measuring conversational quality metrics.

## Key Results
- GELI improves emotional connection by +9%, positivity by +18%, and user inclination by +11% compared to baseline LLAMA-2
- Performance generalizes across datasets, showing improvements on both CANDOR and SODA datasets
- Human evaluations demonstrate consistent improvements over GE-only and LI-only approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Global explicit rewards can be decomposed into turn-level signals using multimodal local implicit feedback.
- Mechanism: The global session-level reward (RGE) is redistributed to individual turns via sum decomposition. The decomposition is shaped by local implicit multimodal signals (e.g., facial affect) through crossmodal knowledge distillation, creating turn-level rewards aligned with session-level outcomes.
- Core assumption: Local implicit multimodal signals are correlated with the global explicit reward and can guide decomposition.
- Evidence anchors:
  - [abstract] "learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session-level reward, using Local Implicit (LI) multimodal reward signals to crossmodally shape the reward decomposition step."
  - [section 4.2] "we design the shaped reward rLI(smm_at) to capture this intuition... we utilize the implicit visual feedback from a facial affect classifier as a way to encourage a decomposition informed by visual affective signals."
  - [corpus] Weak - no direct corpus evidence for this decomposition mechanism.

### Mechanism 2
- Claim: Combining global explicit decomposition with local implicit shaping produces better reward signals than either alone.
- Mechanism: Global explicit decomposition methods (RRD, IRCR, RUDDER) provide accurate session-level reward redistribution, while local implicit shaping (visual affect, language sentiment) adds turn-level alignment to human perception. The combination creates a reward function that is both globally accurate and locally meaningful.
- Core assumption: Global explicit decomposition and local implicit shaping capture complementary aspects of conversational quality.
- Evidence anchors:
  - [section 6.1] "We find that after the GE decomposition methods without any LI feedback training is unable to discern between positive and non-positive facial affect" and "the improvements are often worse than that of the base LLAMA-2 model (3 out of 8 evaluation measures are worse), this leads to the conclusion that the reward signals in GE, and LI separately do not contain enough reward signals."
  - [table 2] "LLAMA2 + GELI refers to our proposed approach, the reward function trained with both global explicit decomposition shaped by local implicit rewards. For dialogue, we find the local implicit rewards (LLAMA2+LI) perform better than that of LLAMA2+GE, where we observe up to a 10% performance boost."
  - [corpus] Weak - no direct corpus evidence for this complementarity.

### Mechanism 3
- Claim: Crossmodal knowledge distillation from multimodal signals to text-only reward functions preserves multimodal information while maintaining text-only interface compatibility.
- Mechanism: The multimodal signal (e.g., facial affect) is processed by a classifier to create proxy rewards. These proxy rewards are then used as labels to train a text-only reward function via knowledge distillation, allowing the reward function to capture multimodal information while only requiring text inputs.
- Core assumption: Crossmodal knowledge distillation can effectively transfer information from multimodal to unimodal representations.
- Evidence anchors:
  - [section 4.2] "we can formulate this problem set up as a form of crossmodal knowledge distillation (KD) for reward shaping" and "we implement an indicator function where we assign a score of 1 if the facial affect of the listener is positive and 0 otherwise."
  - [table 1] "LL: Visual Affect (V A) 1546.17 0.256" shows the LI baseline with visual affect achieves better ∆rLI than language sentiment.
  - [corpus] Weak - no direct corpus evidence for this crossmodal knowledge distillation approach.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The decomposed reward functions are used to adapt LLMs via RLHF, requiring understanding of how to optimize language models with learned reward signals.
  - Quick check question: How does the KL divergence term in RLHF prevent the policy from deviating too far from the original model?

- Concept: Reward Shaping
  - Why needed here: Local implicit multimodal signals are used as reward shaping to guide the decomposition of global rewards, requiring understanding of how shaped rewards affect learning.
  - Quick check question: What is the difference between potential-based reward shaping and the crossmodal knowledge distillation approach used in GELI?

- Concept: Crossmodal Knowledge Distillation
  - Why needed here: The method transfers information from multimodal signals to text-only reward functions, requiring understanding of how to perform knowledge distillation across modalities.
  - Quick check question: How does the choice of score function Γ affect the effectiveness of crossmodal knowledge distillation?

## Architecture Onboarding

- Component map: Global Explicit Reward Decomposition -> Local Implicit Multimodal Signals -> Crossmodal Knowledge Distillation -> RLHF Training Pipeline
- Critical path: Collect global explicit rewards and multimodal signals → Train reward function with GELI → Use reward function in RLHF to adapt LLM → Evaluate adapted LLM with human studies
- Design tradeoffs:
  - Using RRD vs. other decomposition methods (computational efficiency vs. accuracy)
  - Choice of multimodal signals (relevance vs. availability)
  - Crossmodal distillation vs. multimodal reward function (simplicity vs. expressiveness)
  - Human evaluation vs. automatic metrics (validity vs. scalability)
- Failure signatures:
  - Reward function shows low variance in predictions (overly conservative decomposition)
  - RLHF training shows high KL divergence (policy deviating too far from base model)
  - Human evaluations show no improvement over baseline (reward signals not aligned with preferences)
- First 3 experiments:
  1. Implement GELI with RRD and visual affect, train reward function, evaluate decomposition accuracy (LGE) and visual affect alignment (∆rLI)
  2. Use trained GELI reward function in RLHF to adapt LLM, compare human evaluation results with baseline LLAMA-2
  3. Test generalizability by evaluating adapted LLM on unseen dataset (SODA), compare with GPT-3.5 and other baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of score function Γ in GELI impact the overall performance of the dialogue agent?
- Basis in paper: [explicit] The paper mentions that the design of the score function Γ is one of many ways to capture the relationship between local multimodal signals and the single global explicit reward, leaving exciting research opportunities.
- Why unresolved: The paper uses a simple indicator function based on positive facial affect, but does not explore other potential score functions or their impact on performance.
- What evidence would resolve it: Comparative experiments using different score functions (e.g., continuous values based on affect intensity, multi-class affect categories) and their impact on various conversational metrics.

### Open Question 2
- Question: How does GELI perform when using other types of local implicit feedback beyond facial affect and language sentiment?
- Basis in paper: [inferred] The paper mentions that GELI uses facial affect as local implicit feedback, but does not explore other modalities like prosody, body language, or contextual information.
- Why unresolved: The paper focuses on facial affect and language sentiment, leaving the potential of other multimodal signals unexplored.
- What evidence would resolve it: Experiments incorporating additional modalities (e.g., prosodic features, body posture, contextual information) and their impact on GELI's performance.

### Open Question 3
- Question: How does the performance of GELI change with different levels of global explicit reward granularity (e.g., session-level vs. multi-session trends)?
- Basis in paper: [explicit] The paper uses session-level global explicit rewards but mentions the potential for extending the framework to cases where human preference labels are only available at the session level.
- Why unresolved: The paper does not explore the impact of different reward granularity levels on GELI's performance.
- What evidence would resolve it: Experiments comparing GELI's performance using different levels of global explicit reward granularity (e.g., session-level, multi-session trends, long-term user satisfaction) and their impact on conversational metrics.

## Limitations
- The paper lacks statistical significance testing for reported improvements in human evaluations
- Limited exploration of alternative decomposition methods beyond RRD
- Crossmodal knowledge distillation effectiveness is not empirically validated

## Confidence
**High Confidence:** The GELI framework architecture and training procedure are well-defined and implementable based on the paper's descriptions.

**Medium Confidence:** The claim that combining global explicit decomposition with local implicit shaping produces better reward signals than either alone is supported by ablation studies but could benefit from more rigorous comparison across different decomposition methods.

**Low Confidence:** The assertion that GELI generalizes across datasets and outperforms base LLAMA-2 models in long-term dialogue scenarios is based on limited evaluation and lacks statistical significance testing.

## Next Checks
1. **Ablation Study on Decomposition Methods:** Compare GELI's performance when using different global explicit reward decomposition methods (RRD vs. IRCR vs. RUDDER) to determine if RRD's choice is optimal or if the improvements are method-agnostic.

2. **Statistical Significance Testing:** Perform proper statistical analysis on the human evaluation results to determine if the reported improvements in conversational metrics are statistically significant and not due to chance variation.

3. **Multimodal Information Preservation Analysis:** Conduct experiments to verify that the crossmodal knowledge distillation actually preserves multimodal information by testing whether the distilled reward function can distinguish between conversations with positive vs. negative visual affect when only given text input.