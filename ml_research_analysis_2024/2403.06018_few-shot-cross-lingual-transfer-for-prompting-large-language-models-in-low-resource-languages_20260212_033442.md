---
ver: rpa2
title: Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in Low-Resource
  Languages
arxiv_id: '2403.06018'
source_url: https://arxiv.org/abs/2403.06018
tags:
- language
- prompting
- prompt
- languages
- laft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work evaluates three methods for adapting a monolingual large
  language model to low-resource languages for prompting: few-shot prompting, translating
  the prompt before prompting, and language-adaptive fine-tuning before prompting.
  Experiments on Kinyarwanda, Hausa, and Luganda using NER, summarization, and classification
  tasks show that pure few-shot prompting outperforms the other methods with statistical
  significance.'
---

# Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in Low-Resource Languages

## Quick Facts
- arXiv ID: 2403.06018
- Source URL: https://arxiv.org/abs/2403.06018
- Authors: Christopher Toukmaji
- Reference count: 31
- Primary result: Few-shot prompting in target languages outperforms translation and language-adaptive fine-tuning methods for low-resource language prompting

## Executive Summary
This paper investigates three methods for adapting monolingual large language models to low-resource languages for prompting: few-shot prompting in the target language, translating the prompt before prompting, and language-adaptive fine-tuning before prompting. Experiments across three languages (Kinyarwanda, Hausa, Luganda) and three tasks (NER, summarization, classification) demonstrate that few-shot prompting consistently outperforms the other methods with statistical significance. While language-adaptive fine-tuning requires significantly more compute, it does not yield better results than direct prompting, and translation-based approaches introduce noise that degrades performance.

## Method Summary
The study evaluates three adaptation methods for prompting a LLaMa 7B model in low-resource languages. Few-shot prompting involves directly prompting in the target language with k examples (1, 2, or 4 shots). The translation method translates prompts to the target language using Google Cloud Translate API, then translates outputs back. Language-adaptive fine-tuning involves further pre-training the model on target language corpora (Hausa mC4, CommonVoice for Kinyarwanda and Luganda) for 3 epochs before prompting. Evaluation uses MasakhaNER for NER, XL-Sum for Hausa summarization, and KinNEWS for Kinyarwanda topic classification, with statistical significance determined via two-sided t-tests across 5 seeds.

## Key Results
- Few-shot prompting in target languages outperforms both translation and LAFT methods with statistical significance
- Language-adaptive fine-tuning requires more compute but doesn't consistently yield better results than direct prompting
- Translation-based approaches are more compute-efficient but introduce noise that degrades performance
- Results are task and language dependent, but prompting is best on average across all tasks and languages

## Why This Works (Mechanism)

### Mechanism 1
Few-shot prompting works best because it preserves the instruction-following capabilities of PLMs without introducing translation noise. When directly prompting in the target language, the PLM can leverage its pre-existing language understanding and task comprehension abilities without interference from translation errors.

### Mechanism 2
Language-adaptive fine-tuning can introduce unwanted noise and degrade the PLM's general capabilities. LAFT updates all parameters of the PLM, potentially altering its instruction-following abilities and introducing language-specific biases that may not generalize well to other tasks.

### Mechanism 3
Translation-based approaches introduce noise due to limitations in NMT systems for low-resource languages. The forward and backward translations through NMT systems can introduce errors and inconsistencies, particularly for complex tasks like summarization where nuanced understanding is crucial.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: The entire premise of the research is based on the ability of large language models to learn from examples within the prompt itself, without parameter updates.
  - Quick check question: How does the number of shots in the prompt affect the model's performance, and why does this relationship exist?

- Concept: Cross-lingual transfer
  - Why needed here: The research is investigating how to adapt a monolingual model to perform well in low-resource languages, which is a cross-lingual transfer problem.
  - Quick check question: What are the main challenges in cross-lingual transfer for low-resource languages, and how do they differ from high-resource language scenarios?

- Concept: Language-adaptive fine-tuning
  - Why needed here: One of the methods being evaluated is LAFT, which involves further pre-training the model on a corpus in the target language.
  - Quick check question: How does LAFT differ from traditional fine-tuning, and what are its potential benefits and drawbacks for adapting PLMs to low-resource languages?

## Architecture Onboarding

- Component map: LLaMa 7B -> Three languages (Kinyarwanda, Hausa, Luganda) -> Three tasks (NER, summarization, classification) -> Three methods (few-shot, translation, LAFT) -> Evaluation metrics (F1, accuracy, ROUGE)

- Critical path:
  1. Load the LLaMa model and prepare it for inference
  2. Prepare few-shot prompts for each language and task combination
  3. For LAFT method: Perform language-adaptive fine-tuning on target language corpora
  4. For translation method: Set up translation pipeline using Google Cloud Translate API
  5. Run experiments for each combination of method, language, and task
  6. Aggregate and analyze results

- Design tradeoffs:
  - Few-shot prompting vs. LAFT: Balancing between preserving general capabilities and adapting to specific languages
  - Translation accuracy vs. computational efficiency: Using NMT services for translation adds cost and potential noise
  - Model size vs. performance: Using a 7B parameter model instead of larger models for computational efficiency

- Failure signatures:
  - Poor performance across all methods for a particular language-task combination
  - Significant degradation in performance as the number of shots increases (indicating truncation issues)
  - LAFT model producing "psychic" outputs that include information not present in the input

- First 3 experiments:
  1. Few-shot prompting in Kinyarwanda for Named-Entity Recognition
  2. Language-adaptive fine-tuning for Hausa, followed by few-shot prompting for abstractive summarization
  3. Translation-based approach for Luganda topic classification, using Google Cloud Translate API for both forward and backward translation

## Open Questions the Paper Calls Out

- How does the quality of neural machine translation systems impact the performance of the translation method for prompting in low-resource languages?
- What is the impact of data contamination on the performance of language-adaptive fine-tuning (LAFT) for prompting in low-resource languages?
- How do parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA) and Quantization, affect the performance of LAFT for prompting in low-resource languages?

## Limitations

- Evaluation is constrained to three low-resource languages and three specific tasks, limiting generalizability
- The use of Google Cloud Translate API introduces variability in translation quality and cost
- The study does not explore the effects of varying shot counts beyond 1, 2, and 4
- LAFT introduces potential data contamination issues that affect summarization outputs

## Confidence

- High Confidence: Few-shot prompting outperforms translation and LAFT methods on average
- Medium Confidence: LAFT introduces unwanted noise and degrades instruction-following capabilities
- Low Confidence: Translation noise is the primary reason for poor translation-based performance

## Next Checks

1. **Cross-lingual generalization test**: Evaluate the same methods on a broader set of low-resource languages and tasks to determine if the few-shot prompting advantage holds across different linguistic families and task types.

2. **Translation quality analysis**: Implement a controlled study comparing different NMT systems (including open-source models) for the target languages to quantify the impact of translation noise on downstream performance.

3. **Parameter-efficient adaptation comparison**: Compare LAFT with parameter-efficient fine-tuning methods like LoRA or prefix tuning to determine if the degradation in instruction-following capabilities is inherent to fine-tuning or specific to full LAFT.