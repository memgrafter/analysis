---
ver: rpa2
title: How Does Overparameterization Affect Features?
arxiv_id: '2407.00968'
source_url: https://arxiv.org/abs/2407.00968
tags:
- networks
- features
- overparameterized
- test
- low-width
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how overparameterization affects learned
  features by comparing models with identical architectures but varying widths. The
  authors introduce the Feature Span Error (FSE) metric to measure how well one model's
  features can represent another's, and the Feature Performance (FP) metric to assess
  feature accuracy on classification tasks.
---

# How Does Overparameterization Affect Features?

## Quick Facts
- arXiv ID: 2407.00968
- Source URL: https://arxiv.org/abs/2407.00968
- Reference count: 40
- This paper investigates how overparameterization affects learned features by comparing models with identical architectures but varying widths

## Executive Summary
This paper investigates how overparameterization affects learned features by comparing models with identical architectures but varying widths. The authors introduce the Feature Span Error (FSE) metric to measure how well one model's features can represent another's, and the Feature Performance (FP) metric to assess feature accuracy on classification tasks. Their experiments with VGG-16, ResNet18, and Transformer models reveal that overparameterized networks consistently outperform concatenated underparameterized networks even when they have similar total parameters, and that the uncaptured portions of overparameterized features ("residuals") are crucial for performance.

## Method Summary
The authors introduce two key metrics: Feature Span Error (FSE) measures how well one model's features can represent another's features, while Feature Performance (FP) assesses feature accuracy on classification tasks. They conduct experiments across three architectures (VGG-16, ResNet18, and Transformer) with varying widths, comparing overparameterized networks against concatenated underparameterized networks with similar total parameters. The study also includes a toy setting demonstrating theoretical insights about feature learning in overparameterized versus underparameterized networks.

## Key Results
- Concatenated features from multiple underparameterized networks cannot fully capture overparameterized features
- Overparameterized networks consistently outperform concatenated underparameterized networks even with similar total parameters
- The uncaptured portions of overparameterized features ("residuals") are crucial for performance

## Why This Works (Mechanism)
Overparameterization enables networks to learn features that cannot be represented as linear combinations of features from underparameterized networks. The increased capacity allows the network to discover and encode additional signal components that are essential for optimal performance. These additional features are not simply redundant copies but represent genuinely distinct information that contributes to classification accuracy.

## Foundational Learning
- **Feature Span Error (FSE)**: A metric measuring how well one model's features can represent another's features through linear combinations. Needed to quantify representational capacity differences between models.
- **Feature Performance (FP)**: A metric assessing how well features perform on classification tasks when used independently. Needed to evaluate the practical utility of learned features.
- **Residual Features**: Components of overparameterized features that cannot be captured by underparameterized networks. Needed to understand what unique information overparameterization provides.
- **Concatenated Networks**: Multiple underparameterized networks combined to match parameter count of overparameterized networks. Needed as a controlled comparison baseline.
- **Linear Representational Capacity**: The ability to express features as linear combinations of other features. Needed to formalize the concept of feature coverage.

## Architecture Onboarding
**Component Map**: Input -> Feature Extraction Layers -> Feature Representation -> Classification Head -> Output

**Critical Path**: Feature extraction layers are the critical path, as they determine what information is available for classification and directly relate to the paper's investigation of how overparameterization affects feature learning.

**Design Tradeoffs**: Width expansion vs depth expansion for overparameterization; the paper focuses on width to isolate the effect of overparameterization while keeping architecture structure constant.

**Failure Signatures**: If overparameterization provided no benefit, concatenated underparameterized networks would match or exceed overparameterized network performance. The failure to capture residuals would indicate missing critical information.

**First Experiments**:
1. Replicate the basic FSE and FP measurements for a simple architecture (e.g., MLP) to verify metric implementation
2. Compare a single overparameterized network against two concatenated underparameterized networks on a simple classification task
3. Visualize feature representations from overparameterized vs concatenated networks to qualitatively assess differences

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis is limited to specific architectures (VGG-16, ResNet18, Transformer) which may not generalize to other model families
- FSE metric assumes linear separability which may not capture all relevant feature relationships
- Toy setting simplifies complex real-world interactions and may not fully represent practical overparameterization effects

## Confidence
High: Empirical findings comparing overparameterized vs concatenated underparameterized networks are robust within tested architectures and datasets. Mathematical framework for FSE and FP metrics is sound.
Medium: Generalization to broader model families requires additional validation. Interpretation of residuals as crucial for performance needs more direct causal evidence.
Low: Extent to which toy setting insights translate to practical deep learning scenarios is uncertain. FSE may not capture all aspects of feature representational capacity.

## Next Checks
1. Replicate experiments with additional architectures (Vision Transformers, MLPs) and diverse datasets to assess generalizability across model families and data distributions
2. Conduct ablation studies on FSE metric by testing alternative feature comparison methods (nonlinear kernel-based approaches) to validate conclusions under different similarity measures
3. Implement feature evolution tracking during training to understand how overparameterized and underparameterized networks develop feature representations over time, particularly focusing on when and how residual features emerge