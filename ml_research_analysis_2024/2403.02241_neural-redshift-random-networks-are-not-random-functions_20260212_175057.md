---
ver: rpa2
title: 'Neural Redshift: Random Networks are not Random Functions'
arxiv_id: '2403.02241'
source_url: https://arxiv.org/abs/2403.02241
tags:
- complexity
- networks
- bias
- functions
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that neural networks' generalization ability
  comes primarily from their architecture's inductive biases, not from gradient descent
  optimization. The authors demonstrate that random-weight neural networks overwhelmingly
  implement simple functions, regardless of depth or weight magnitude, when using
  common components like ReLU activations, residual connections, and layer normalizations.
---

# Neural Redshift: Random Networks are not Random Functions

## Quick Facts
- **arXiv ID**: 2403.02241
- **Source URL**: https://arxiv.org/abs/2403.02241
- **Reference count**: 40
- **Primary result**: Random-weight neural networks overwhelmingly implement simple functions due to architectural inductive biases, not gradient descent optimization.

## Executive Summary
This paper challenges the prevailing view that gradient descent's implicit biases are the primary source of neural networks' generalization capabilities. Through extensive analysis of random-weight networks, the authors demonstrate that common architectural components like ReLU activations, residual connections, and layer normalizations create a strong "simplicity bias" - random networks overwhelmingly implement simple functions regardless of depth or weight magnitude. This bias is not inherent to neural networks but depends on specific architectural choices, and architectures can be designed to prefer any level of complexity. The findings suggest that generalization occurs when the architecture's preferred complexity matches the target function's complexity, and that architectural design may be more important than optimization methods for achieving good generalization.

## Method Summary
The authors analyze random-weight neural networks to characterize their inductive biases. They sample random weights from uniform distributions, evaluate networks on a 2D input grid (64x64 points), and apply three complexity measures: Fourier frequency analysis, polynomial order decomposition (Chebyshev/Legendre), and LZ compressibility. The study systematically varies architectural components including activation functions (ReLU, GELU, Swish, TanH, etc.), depth, weight magnitude, and the presence of layer normalization and residual connections. They also extend the analysis to transformers by examining their building blocks. The approach reveals how different architectural choices create different biases in the function space, enabling the design of architectures with controlled complexity preferences.

## Key Results
- Random-weight neural networks with ReLU, residual connections, and layer normalization overwhelmingly implement simple functions, regardless of depth or weight magnitude
- The simplicity bias is not universal - architectures using TanH or sine activations show no such preference and can implement complex functions
- Complexity measures (Fourier frequency, polynomial order, compressibility) correlate strongly across architectures, providing consistent quantification of function complexity
- Architectures can be designed to prefer any level of complexity, enabling better generalization on complex tasks where standard simplicity bias is suboptimal

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random-weight neural networks overwhelmingly implement simple functions due to architectural inductive biases
- Mechanism: The parameter space of neural networks is not uniformly distributed across all possible functions. Instead, it is biased toward functions with low complexity (measured by Fourier frequency, polynomial order, and compressibility). When weights are sampled uniformly from their initialization distributions, they fall into regions of parameter space that correspond to simple functions.
- Core assumption: The mapping from parameter space to function space is highly non-uniform, with simple functions occupying a much larger volume than complex ones
- Evidence anchors:
  - [abstract] "random-weight neural networks overwhelmingly implement simple functions"
  - [section 3] "Even simple MLPs show strong inductive biases: uniform sampling in weight space yields a very biased distribution of functions in terms of complexity"
  - [corpus] Weak - the related papers discuss generalization and bias but don't directly address the non-uniformity of function space
- Break condition: If the architecture uses components that don't create this bias (like certain activations or parametrizations), or if the weight distribution is not uniform

### Mechanism 2
- Claim: The simplicity bias is not inherent to neural networks but depends on specific architectural components
- Mechanism: Different architectural components create different biases in the function space. ReLU activations, residual connections, and layer normalizations create a strong bias toward low-complexity functions. Other components like TanH or sine activations create different biases, including toward higher complexity. The overall bias is the combination of all these components.
- Core assumption: Architectural components can be classified by whether they increase, decrease, or don't affect complexity
- Evidence anchors:
  - [section 3] "Unlike common wisdom, NNs do not have an inherent 'simplicity bias'. This property depends on components such as ReLUs, residual connections, and layer normalizations"
  - [section 3] Table 1 showing which components bias toward lower/higher complexity
  - [corpus] Weak - related papers discuss different aspects of bias but not this specific component-based analysis
- Break condition: If architectural components are combined in ways that cancel out their individual biases, or if new components are introduced that create unexpected biases

### Mechanism 3
- Claim: Generalization occurs when the architecture's preferred complexity matches the target function's complexity
- Mechanism: An architecture can only learn functions that lie within its preferred complexity range. If the target function is simpler than the architecture's preference, the architecture will learn it well. If the target function is more complex, the architecture will struggle to generalize beyond training data. This explains why certain architectures work better for certain tasks.
- Core assumption: The "preferred complexity" of an architecture is observable in both random-weight networks and trained networks
- Evidence anchors:
  - [abstract] "Generalization is enabled by popular components like ReLUs setting this bias to a low complexity that often aligns with the target function"
  - [section 4] "We will also see that unusual architectures with a bias towards high complexity can improve generalization on tasks where the standard 'simplicity bias' is suboptimal"
  - [corpus] Weak - related papers discuss generalization but not this specific mechanism of complexity matching
- Break condition: If the relationship between initialization complexity and trained model complexity breaks down, or if gradient descent finds solutions outside the preferred complexity range

## Foundational Learning

- Concept: Fourier decomposition and frequency analysis
  - Why needed here: The paper uses Fourier frequency as one measure of function complexity, so understanding how functions can be decomposed into frequency components is essential
  - Quick check question: If a function has large coefficients for high-frequency components in its Fourier decomposition, would it be considered simple or complex according to this paper's framework?

- Concept: Polynomial decomposition and order
  - Why needed here: The paper uses polynomial order as another measure of complexity, so understanding how functions can be approximated by polynomials of different orders is important
  - Quick check question: A linear function would have what kind of polynomial order - low or high?

- Concept: Kolmogorov complexity and compressibility
  - Why needed here: The paper uses compressibility as a proxy for Kolmogorov complexity, so understanding how simple patterns can be compressed more than complex ones is relevant
  - Quick check question: Would a sequence with many repeating patterns be highly compressible or not very compressible?

## Architecture Onboarding

- Component map:
  - Input layer → Hidden layers (activation function, layer normalization, residual connections) → Output layer
  - Key architectural decisions: activation function choice, use of residual connections, layer normalization placement, network depth, width
  - Critical components: activation functions (ReLU, GELU, TanH, etc.), layer normalizations, residual connections, multiplicative interactions

- Critical path: Activation function selection → Layer normalization placement → Residual connection implementation → Weight initialization
  - Why: These choices directly determine the complexity bias of the network
  - Dependencies: Activation function choice affects how other components work; layer normalization interacts with activation functions; residual connections interact with depth

- Design tradeoffs:
  - ReLU vs other activations: ReLU gives strong simplicity bias but may struggle with complex functions; other activations offer more flexibility but less bias
  - Depth vs simplicity: Deeper networks with residual connections can maintain simplicity bias while increasing capacity
  - Layer normalization: Helps with training stability but can affect complexity bias

- Failure signatures:
  - Model consistently underfits complex data: Likely too strong simplicity bias (too many ReLU-like components)
  - Model overfits simple data: Likely too weak simplicity bias (too many TanH-like components)
  - Training instability: Layer normalization or residual connections may be misconfigured

- First 3 experiments:
  1. Test complexity bias: Create a 2-layer MLP with ReLU, GELU, and TanH activations. Generate random weights and measure Fourier complexity. Compare results to see how different activations create different biases.
  2. Test component effects: Take a base MLP and systematically add layer normalization, residual connections, and gating. Measure how each component changes the complexity bias.
  3. Test generalization matching: Create simple and complex synthetic datasets. Train MLPs with different activation functions on both. Measure generalization to see which architectures match which data complexities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific architectural components like ReLU activations, residual connections, and layer normalization contribute to the simplicity bias observed in neural networks?
- Basis in paper: [explicit] The paper identifies these components as critical in determining the complexity of functions implemented by neural networks.
- Why unresolved: The exact mechanisms by which these components influence the simplicity bias are not fully understood.
- What evidence would resolve it: Detailed analysis of how these components affect the distribution of weight magnitudes and the structure of learned functions.

### Open Question 2
- Question: Can the simplicity bias observed in neural networks be quantified and controlled to improve generalization on complex tasks?
- Basis in paper: [explicit] The paper demonstrates that the simplicity bias can be modulated with the architecture, enabling better generalization on complex tasks.
- Why unresolved: The relationship between the simplicity bias and generalization is not fully characterized.
- What evidence would resolve it: Experiments showing how adjusting the simplicity bias affects the performance of neural networks on a variety of tasks.

### Open Question 3
- Question: How do the inductive biases of neural networks impact their ability to learn complex functions, and can these biases be leveraged to improve learning efficiency?
- Basis in paper: [explicit] The paper suggests that the inductive biases of neural networks play a crucial role in their ability to learn complex functions.
- Why unresolved: The exact nature of the relationship between inductive biases and learning efficiency is not fully understood.
- What evidence would resolve it: Studies comparing the learning efficiency of neural networks with different inductive biases on complex tasks.

## Limitations

- The analysis is based on random-weight networks rather than trained models, which may not fully capture the complexities of actual learning dynamics
- The complexity measures used (Fourier, polynomial, compressibility) are proxies that may not perfectly capture functional complexity across all domains
- The study focuses on specific architectural components and may not generalize to all possible neural network designs

## Confidence

- **High Confidence**: The observation that random-weight networks with common components (ReLU, residual connections, layer normalization) implement predominantly simple functions is well-supported by the empirical evidence presented.
- **Medium Confidence**: The claim that this simplicity bias is primarily due to architectural components rather than gradient descent is plausible but requires more direct comparison between random and trained networks across diverse tasks.
- **Medium Confidence**: The assertion that matching architecture bias to target function complexity optimizes generalization is theoretically sound but needs more extensive empirical validation across real-world tasks.

## Next Checks

1. **Training vs Random Comparison**: Train MLPs with different activation functions on synthetic datasets of varying complexity, then compare the complexity distributions of random-weight vs trained networks to directly test if training preserves or modifies the architectural bias.

2. **Real-World Task Validation**: Apply the complexity-matching principle to real vision or language tasks by designing architectures with controlled complexity biases (using alternative activations or modified residual connections) and measuring their generalization performance against standard architectures.

3. **Gradient Descent Interaction**: Investigate how gradient descent interacts with architectural biases by tracking complexity evolution during training across different architectures and initialization scales, particularly focusing on whether training amplifies, reduces, or preserves the initial bias.