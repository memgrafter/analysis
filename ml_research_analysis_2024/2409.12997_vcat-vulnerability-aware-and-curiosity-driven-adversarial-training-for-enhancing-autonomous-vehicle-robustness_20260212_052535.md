---
ver: rpa2
title: 'VCAT: Vulnerability-aware and Curiosity-driven Adversarial Training for Enhancing
  Autonomous Vehicle Robustness'
arxiv_id: '2409.12997'
source_url: https://arxiv.org/abs/2409.12997
tags:
- adversarial
- training
- victim
- attack
- attacker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving autonomous vehicle
  (AV) robustness against malicious attacks in complex traffic environments. The proposed
  Vulnerability-aware and Curiosity-driven Adversarial Training (VCAT) framework enhances
  AV robustness by training both an attacker and defender.
---

# VCAT: Vulnerability-aware and Curiosity-driven Adversarial Training for Enhancing Autonomous Vehicle Robustness

## Quick Facts
- arXiv ID: 2409.12997
- Source URL: https://arxiv.org/abs/2409.12997
- Reference count: 31
- One-line primary result: VCAT significantly improves autonomous vehicle robustness against malicious attacks, achieving up to 83.4% crash rate during attack training and 99.1% non-crash rate during validation.

## Executive Summary
This paper presents VCAT, a framework that enhances autonomous vehicle (AV) robustness against malicious attacks in complex traffic environments. VCAT employs a two-stage adversarial training approach where an attacker, equipped with vulnerability-aware and curiosity-driven modules, learns to generate safety-critical scenarios by exploiting the victim AV's weaknesses. During defense training, the AV learns to handle these critical scenarios, resulting in improved robustness. Experiments across three traffic scenarios demonstrate VCAT's superiority over baseline methods, with significant improvements in both crash rates during attack training and non-crash rates during validation.

## Method Summary
VCAT uses a two-stage adversarial training framework where a traffic vehicle (attacker) is trained to generate safety-critical scenarios against a target AV (victim). The attacker employs a vulnerability-aware module that approximates the victim's value function using a surrogate network, allowing efficient exploitation of vulnerabilities. Additionally, curiosity-driven exploration via Random Network Distillation (RND) guides the attacker to discover novel attack scenarios and avoid mode collapse. During defense training, the victim AV learns to defend against attacks generated by the pretrained attacker. The framework is implemented using reinforcement learning algorithms (PPO) and evaluated in three intersection scenarios from the highway-env simulator.

## Key Results
- VCAT achieves up to 83.4% crash rate during attack training, significantly outperforming baseline methods (MC: 31.3%, PPO: 46.6%, SAC: 59.7%)
- During validation, VCAT maintains non-crash rates up to 99.1%, demonstrating superior robustness compared to baselines
- t-SNE visualizations show VCAT generates more diverse and novel attack scenarios, with improved action space coverage
- VCAT exhibits higher effectiveness against various attack types including white-box, black-box, and adaptive attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The vulnerability-aware module improves attacker efficiency by approximating the victim's value function.
- Mechanism: A surrogate network (parameterized by θυ) fits the state-value function of the victim AV, providing dense information about the victim's inherent vulnerabilities. This allows the attacker to explicitly identify unfavorable states of the black-box victim.
- Core assumption: The value function approximation accurately reflects the victim's vulnerabilities and can guide the attacker to exploit these weaknesses.
- Evidence anchors:
  - [abstract]: "a surrogate network is employed to fit the value function of the AV victim, providing dense information about the victim's inherent vulnerabilities"
  - [section]: "We use an approximation network (parameterized by θυ) to fit the state-value function of υ, which aids in the explicit formulation of Eq.1"
- Break condition: If the value function approximation is inaccurate or fails to capture the true vulnerabilities of the victim, the attacker's efficiency will be severely compromised.

### Mechanism 2
- Claim: Curiosity-driven exploration helps the attacker discover novel attack scenarios and avoid mode collapse.
- Mechanism: Random Network Distillation (RND) is used to characterize the novelty of the environment. Two networks are constructed: a stationary target network and a dynamic predictor network. The prediction error between these networks serves as an intrinsic reward, guiding the attacker to explore unfamiliar states.
- Core assumption: The prediction error from RND accurately reflects the novelty of the environment and can effectively guide the attacker's exploration.
- Evidence anchors:
  - [abstract]: "Subsequently, random network distillation is used to characterize the novelty of the environment, constructing an intrinsic reward to guide the attacker in exploring unexplored territories"
  - [section]: "When ˆϱ encounters a fresh state, the prediction error will be high, resulting in a high intrinsic reward output"
- Break condition: If the intrinsic reward fails to properly incentivize exploration or leads the attacker astray, the attacker may still get stuck in mode collapse or fail to discover novel attack scenarios.

### Mechanism 3
- Claim: The two-stage adversarial training framework enhances the victim AV's robustness against malicious attacks.
- Mechanism: The framework consists of an adversarial attack phase and an adversarial defense phase. In the attack phase, the attacker learns to generate safety-critical scenarios by exploiting the victim's vulnerabilities. In the defense phase, the victim AV learns to defend against these attacks by training on the critical scenarios generated by the pretrained attacker.
- Core assumption: The critical scenarios generated by the attacker are diverse and representative of real-world safety-critical situations, and the victim AV can effectively learn to handle these scenarios during defense training.
- Evidence anchors:
  - [abstract]: "During defense training, the AV learns to handle critical scenarios generated by the pretrained attacker"
  - [section]: "In the victim defense training phase, the AV is trained in critical scenarios in which the pretrained attacker is positioned around the victim to generate attack behaviors"
- Break condition: If the generated scenarios are not diverse or realistic enough, or if the victim AV fails to learn effective defensive strategies, the adversarial training framework will not enhance the AV's robustness.

## Foundational Learning

- Concept: Markov Games (MGs)
  - Why needed here: The adversarial training problem is modeled as a two-player Markov Game, where the attacker and victim AV are the two players with opposite interests.
  - Quick check question: What is the key difference between a Markov Decision Process (MDP) and a Markov Game (MG)?

- Concept: Reinforcement Learning (RL) and Proximal Policy Optimization (PPO)
  - Why needed here: Both the attacker and victim AV are trained using RL algorithms, specifically PPO, to learn optimal policies for generating attacks and defending against them, respectively.
  - Quick check question: What is the main advantage of using PPO over other RL algorithms in this context?

- Concept: Random Network Distillation (RND) and Intrinsic Motivation
  - Why needed here: RND is used to generate an intrinsic reward that guides the attacker's exploration towards novel and unfamiliar states, preventing mode collapse and improving attack efficiency.
  - Quick check question: How does the intrinsic reward generated by RND differ from the extrinsic reward typically used in RL?

## Architecture Onboarding

- Component map:
  - Victim AV (target) -> Attacker (traffic vehicle) -> Vulnerability-aware module (surrogate network) -> Curiosity-driven module (RND) -> Adversarial training framework (attack and defense phases)

- Critical path:
  1. Train the victim AV on standard datasets to establish baseline capabilities.
  2. Freeze the victim AV's parameters and train the attacker using the vulnerability-aware and curiosity-driven modules.
  3. Once the attacker is sufficiently trained, freeze its parameters and thaw the victim AV's parameters.
  4. Train the victim AV to defend against the attacks generated by the pretrained attacker.

- Design tradeoffs:
  - Balancing exploitation and exploration: The curiosity-driven module must strike a balance between exploiting known vulnerabilities and exploring novel attack scenarios.
  - Computational complexity: The surrogate network and RND add computational overhead to the training process, which must be considered in the overall system design.

- Failure signatures:
  - Mode collapse: If the attacker gets stuck in a loop of overexploiting established vulnerabilities, the victim AV's robustness will not improve significantly.
  - Inaccurate value function approximation: If the surrogate network fails to accurately capture the victim AV's vulnerabilities, the attacker's efficiency will be compromised.

- First 3 experiments:
  1. Train the victim AV on a simple traffic scenario and evaluate its baseline performance.
  2. Train the attacker using the vulnerability-aware and curiosity-driven modules and measure its attack efficiency.
  3. Conduct adversarial training by alternating between the attack and defense phases and evaluate the victim AV's robustness against various attack methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the VCAT framework perform when applied to real-world autonomous driving data instead of simulated environments?
- Basis in paper: [explicit] The paper mentions future work will focus on incorporating real-world data into the training process, suggesting this has not yet been tested.
- Why unresolved: The current experiments are conducted in a simulated environment (highway-env), and the authors explicitly state that extending to real-world data is a future direction.
- What evidence would resolve it: Comparative results showing VCAT's performance on real-world autonomous driving datasets versus simulation, including metrics like crash rates and robustness measures.

### Open Question 2
- Question: What is the optimal balance between vulnerability exploitation and curiosity-driven exploration in the attacker policy for different traffic scenarios?
- Basis in paper: [inferred] The paper discusses balancing exploitation and exploration through the hyperparameter λ, but doesn't provide a systematic analysis of optimal values across different scenarios.
- Why unresolved: While the paper mentions λ=0.2 as a reference value, it doesn't explore how this parameter should be tuned for different traffic scenarios or what the optimal balance might be.
- What evidence would resolve it: A comprehensive sensitivity analysis showing how different λ values affect attack success rates and AV robustness across various traffic scenarios.

### Open Question 3
- Question: How does VCAT perform against adaptive adversaries that can learn and respond to the victim's defense strategies?
- Basis in paper: [explicit] The authors mention future work on strengthening the system's resilience against adaptive adversaries, indicating this scenario hasn't been tested.
- Why unresolved: The current experiments focus on static attackers with pretrained policies, not on adaptive adversaries that can modify their strategies based on the victim's responses.
- What evidence would resolve it: Experiments showing VCAT's performance against adaptive adversaries that can update their strategies during the defense phase, comparing robustness metrics to the current static attacker approach.

## Limitations

- The evaluation is constrained to only three intersection scenarios from a single simulator, limiting generalizability to diverse real-world conditions.
- The study focuses exclusively on white-box attacks where the attacker has full knowledge of the victim AV, potentially overestimating robustness in practical black-box scenarios.
- The evaluation metrics heavily emphasize crash rates without quantifying safety-critical near-miss scenarios or the computational overhead introduced by the vulnerability-aware and curiosity-driven modules.

## Confidence

- **High Confidence**: The two-stage adversarial training framework and the use of RND for curiosity-driven exploration are well-established concepts with clear implementation paths.
- **Medium Confidence**: The vulnerability-aware module's effectiveness depends on the accuracy of the surrogate network's value function approximation, which may vary across different AV architectures.
- **Medium Confidence**: The claim of superior robustness enhancement is supported by quantitative results but requires validation across more diverse traffic scenarios and attack types.

## Next Checks

1. **Cross-scenario robustness**: Evaluate VCAT's performance on additional traffic scenarios (highway merging, pedestrian crossings) to assess generalizability beyond intersection cases.
2. **Black-box attack evaluation**: Test the trained attacker against victim AVs with different architectures or hyperparameters to verify effectiveness in realistic black-box conditions.
3. **Safety-critical near-miss analysis**: Implement metrics to quantify the frequency and severity of near-miss scenarios during validation, providing a more comprehensive safety assessment beyond binary crash outcomes.