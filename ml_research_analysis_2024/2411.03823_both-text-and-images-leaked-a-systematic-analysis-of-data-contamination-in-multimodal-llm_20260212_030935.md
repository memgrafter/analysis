---
ver: rpa2
title: Both Text and Images Leaked! A Systematic Analysis of Data Contamination in
  Multimodal LLM
arxiv_id: '2411.03823'
source_url: https://arxiv.org/abs/2411.03823
tags:
- contamination
- data
- multimodal
- mllms
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper systematically analyzes multimodal data contamination\
  \ in MLLMs by introducing a detection framework (MM-Detect) that identifies two\
  \ contamination types: unimodal (text-only leakage) and cross-modal (image-text\
  \ pair leakage). The framework employs Option Order Sensitivity Test for multiple-choice\
  \ tasks and Slot Guessing for Perturbed Captions for caption-based tasks, measuring\
  \ contamination via performance degradation (\u2206) and instance-level leakage\
  \ (\u03A6)."
---

# Both Text and Images Leaked! A Systematic Analysis of Data Contamination in Multimodal LLM

## Quick Facts
- arXiv ID: 2411.03823
- Source URL: https://arxiv.org/abs/2411.03823
- Reference count: 38
- Primary result: Introduces MM-Detect framework revealing significant multimodal data contamination, often originating from unimodal pre-training rather than multimodal fine-tuning

## Executive Summary
This paper introduces MM-Detect, a framework for detecting data contamination in multimodal large language models (MLLMs). The framework employs two perturbation methods - Option Order Sensitivity Test for multiple-choice tasks and Slot Guessing for Perturbed Captions for caption-based tasks - to measure performance degradation as an indicator of contamination. Evaluations across twelve MLLMs and five benchmarks reveal significant contamination levels, particularly in proprietary models and older datasets. Critically, the analysis shows that contamination often originates during unimodal pre-training rather than solely from multimodal fine-tuning, challenging existing assumptions about contamination sources.

## Method Summary
MM-Detect is a black-box framework that detects multimodal data contamination by measuring performance degradation when benchmark data is perturbed. The framework uses two methods: Option Order Sensitivity Test, which shuffles answer choices in multiple-choice tasks, and Slot Guessing for Perturbed Captions, which masks keywords in captions using back-translation. Contamination severity is quantified using dataset-level metric ∆ (performance reduction) and instance-level leakage Φ (count of correct-to-incorrect transitions). The framework evaluates 12 MLLMs across 5 benchmarks, providing both contamination detection and diagnosis of evaluation bias.

## Key Results
- Significant contamination detected across all evaluated MLLMs, with proprietary models showing higher levels than open-source alternatives
- Contamination often originates from unimodal pre-training stages rather than multimodal fine-tuning
- Dataset-level contamination severity (∆) ranges from minor to severe, with instance-level leakage (Φ) identifying specific contaminated examples
- Contamination patterns vary by benchmark age and model training data transparency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models trained on benchmark data show performance degradation when answer order or key words are semantically perturbed
- Mechanism: If a model has memorized the original benchmark, it will struggle to generalize to perturbed variants that maintain semantic content but differ in surface form
- Core assumption: Semantic-preserving perturbations (shuffled options, back-translated captions) are sufficiently different to expose memorization while remaining valid inputs
- Evidence anchors:
  - [abstract] "incorporates two methods, Option Order Sensitivity Test and Slot Guessing for Perturbed Captions, designed to handle two common types of Visual Question Answering tasks"
  - [section 3.1] "if the model's performance is highly sensitive to the order of the options... it indicates potential contamination"
- Break condition: If model generalizes robustly to semantic perturbations, this mechanism will underestimate contamination

### Mechanism 2
- Claim: Contamination can originate from unimodal pre-training, not just multimodal fine-tuning
- Mechanism: Text from multimodal benchmarks can leak into unimodal corpora used for pre-training, leading to implicit memorization before any multimodal training
- Core assumption: Unimodal pre-training data overlaps with textual components of multimodal benchmarks
- Evidence anchors:
  - [abstract] "Crucially, contamination sometimes originates during unimodal pre-training rather than solely from multimodal fine-tuning"
  - [section 6.1] "if an LLM can correctly answer an image-required question without the image... it may indicate the leakage of that instance"
- Break condition: If unimodal and multimodal training data are completely disjoint, this mechanism cannot detect contamination

### Mechanism 3
- Claim: The framework provides both dataset-level and instance-level contamination metrics (∆ and Φ)
- Mechanism: Dataset-level ∆ measures overall performance drop; instance-level Φ identifies specific leaked examples by counting correct→incorrect transitions
- Core assumption: Performance degradation correlates with contamination severity, and instance-level tracking can isolate specific leaked examples
- Evidence anchors:
  - [abstract] "quantifies contamination severity across multiple-choice and caption-based Visual Question Answering tasks"
  - [section 3.3] "evaluate the reduction in atomic metrics, denoted as ∆" and "compute X, the count of cases where the model provided correct answers before perturbation but incorrect answers after"
- Break condition: If model answers perturbed questions correctly due to generalization, this mechanism will underestimate contamination

## Foundational Learning

- Concept: Semantic-preserving perturbation techniques
  - Why needed here: To create test variants that expose memorization without invalidating the task
  - Quick check question: How does back-translation preserve meaning while changing surface form?

- Concept: Multimodal training pipeline analysis
  - Why needed here: To understand where contamination can enter (pre-training vs fine-tuning)
  - Quick check question: What are the three distinct training stages where contamination could occur?

- Concept: Contamination metrics interpretation
  - Why needed here: To distinguish between dataset-level trends and instance-level leakage
  - Quick check question: How do ∆ and Φ differ in what they reveal about contamination?

## Architecture Onboarding

- Component map: MM-DETECT framework consists of perturbation generation (Option Order Sensitivity Test, Slot Guessing for Perturbed Captions) → metric calculation (CR, PCR, ∆, Φ) → contamination degree analysis
- Critical path: Generate perturbed dataset → run model on original and perturbed → compute metrics → classify contamination severity
- Design tradeoffs: Black-box approach (no model internals needed) vs. potential underestimation if model generalizes to perturbations
- Failure signatures: High Φ with low ∆ suggests instance-level leakage; low Φ with high ∆ suggests dataset-level contamination
- First 3 experiments:
  1. Run Option Order Sensitivity Test on a known clean model to establish baseline ∆
  2. Test Slot Guessing on a model with known caption contamination to validate method
  3. Compare contamination levels across models with different training data disclosure levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of MM-DETECT vary across different multimodal task types beyond multiple-choice and caption-based VQA?
- Basis in paper: [inferred] The paper focuses on multiple-choice and caption-based VQA tasks, but mentions potential applicability to other image-feature-analyzing benchmarks without testing them
- Why unresolved: The framework's performance on other multimodal tasks (e.g., open-ended generation, visual reasoning, video understanding) remains unexplored, limiting generalizability claims
- What evidence would resolve it: Systematic testing of MM-DETECT across diverse multimodal benchmark types with varying input-output structures and perturbation strategies

### Open Question 2
- Question: Can MM-DETECT distinguish between contamination from intentional training on benchmark data versus unintentional overlap in large-scale web datasets?
- Basis in paper: [explicit] The paper notes that some models train on large-scale interleaved image-text datasets (e.g., OBELICS) or in-house data without full disclosure, potentially causing overlooked contamination
- Why unresolved: Current framework detects contamination presence but cannot differentiate between deliberate benchmark training and incidental data overlap in massive web corpora
- What evidence would resolve it: Analysis of contamination patterns showing distinct signatures between intentional benchmark training versus incidental web dataset overlap, possibly through correlation with known data sources

### Open Question 3
- Question: What is the relationship between contamination detection metrics (∆ and Φ) and downstream model performance degradation when evaluated on clean benchmarks?
- Basis in paper: [inferred] The paper establishes contamination detection metrics but doesn't investigate how these metrics correlate with actual performance drops when models are evaluated on uncontaminated test sets
- Why unresolved: Understanding whether high contamination scores predict significant performance degradation would validate the practical importance of contamination detection
- What evidence would resolve it: Empirical studies correlating ∆ and Φ values with performance drops across multiple benchmark evaluations and model architectures

## Limitations
- Perturbation generalizability: The semantic-preserving perturbations may not fully capture all forms of memorization, potentially underestimating contamination in models with robust generalization capabilities
- Proprietary model verification: Contamination findings in proprietary models rely on indirect evidence rather than access to training data, limiting verification
- Unimodal contamination detection: While the framework identifies contamination during unimodal pre-training, it cannot definitively prove the source (e.g., could be from other text datasets rather than the multimodal benchmark)

## Confidence
- Framework effectiveness in detecting contamination: High
- Unimodal pre-training as contamination source: Medium
- Significant contamination in proprietary models: Low-Medium

## Next Checks
1. Cross-dataset contamination mapping: Test whether models show contamination on multiple benchmarks with known data overlap to establish contamination spread patterns
2. Perturbation robustness testing: Evaluate model performance on systematically varied perturbations (different back-translation services, multiple shuffling patterns) to verify detection consistency
3. Training data reconciliation: For open-source models with available training logs, cross-reference detected contamination instances with documented data sources to validate origin claims