---
ver: rpa2
title: Docling Technical Report
arxiv_id: '2408.09869'
source_url: https://arxiv.org/abs/2408.09869
tags:
- document
- docling
- page
- table
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Docling is an open-source, MIT-licensed Python library for PDF
  document conversion, leveraging state-of-the-art AI models for layout analysis (DocLayNet)
  and table structure recognition (TableFormer). It converts PDFs to JSON or Markdown
  formats with high accuracy and efficiency, running locally on commodity hardware.
---

# Docling Technical Report

## Quick Facts
- arXiv ID: 2408.09869
- Source URL: https://arxiv.org/abs/2408.09869
- Reference count: 34
- Docling is an open-source Python library for PDF document conversion using AI models

## Executive Summary
Docling is an open-source Python library designed to convert PDF documents into structured JSON or Markdown formats with high accuracy. The tool leverages state-of-the-art AI models including DocLayNet for layout analysis and TableFormer for table structure recognition. It runs efficiently on commodity hardware, processing PDFs at 0.6-2.45 pages per second with 2.4-6.2 GB memory usage depending on configuration.

The library provides detailed page layout understanding, metadata extraction, optional OCR support, and extensibility for custom models. Docling is particularly suited for downstream applications like document search, knowledge extraction, and LLM workflows. The project includes extensive documentation, examples, and a container deployment option, making it accessible for both individual users and enterprise deployments.

## Method Summary
Docling processes PDF documents through a modular pipeline that combines PDF parsing, AI model inference, and output assembly. The system uses DocLayNet for layout analysis to detect and classify document elements, followed by TableFormer for recovering table structures. The pipeline supports multiple PDF backends, optional OCR processing, and configurable resource usage to optimize performance across different hardware configurations. Output can be serialized in either JSON or Markdown formats, with the former providing richer structured data and the latter offering better human readability.

## Key Results
- Processes PDFs at 0.6-2.45 pages per second on commodity hardware
- Uses 2.4-6.2 GB memory depending on configuration and document complexity
- Delivers richly structured output suitable for document search and knowledge extraction
- Provides accurate table structure recognition with support for complex table layouts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Docling achieves high accuracy in document conversion by combining specialized AI models for layout analysis and table structure recognition.
- Mechanism: The system uses DocLayNet for layout analysis to detect and classify document elements (text, tables, figures, etc.) and TableFormer for recovering table structures. These models are applied sequentially on each page, with layout analysis feeding bounding box information to table structure recognition.
- Core assumption: Specialized models trained on large, diverse datasets (DocLayNet with 80,863 pages) can accurately segment document layouts better than general-purpose models.
- Evidence anchors:
  - [abstract]: "powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer)"
  - [section]: "Our layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13]"
  - [corpus]: Weak evidence - corpus shows related work but no direct validation of DocLayNet accuracy claims
- Break condition: Performance degrades significantly when encountering document layouts not represented in training data, or when PDF parsing quality is poor due to complex font encodings or embedded bitmaps.

### Mechanism 2
- Claim: Docling's modular pipeline architecture enables extensibility and customization.
- Mechanism: The processing pipeline consists of independent stages (PDF parsing, model pipeline, assembly/post-processing) that can be customized by subclassing the BaseModelPipeline interface. This allows users to add, replace, or modify models and pipeline parameters.
- Core assumption: The linear pipeline design with well-defined interfaces between stages allows for flexible modification without breaking core functionality.
- Evidence anchors:
  - [abstract]: "The code interface allows for easy extensibility and addition of new features and models"
  - [section]: "Docling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly"
  - [corpus]: Weak evidence - corpus shows related work but no direct validation of extensibility claims
- Break condition: Extensibility breaks when custom models don't implement the required pythonCallable interface correctly, or when pipeline modifications introduce incompatible data structures between stages.

### Mechanism 3
- Claim: Docling achieves efficient processing on commodity hardware through optimized model implementations and configurable resource usage.
- Mechanism: The system uses ONNX Runtime for layout analysis (sub-second latency on CPU) and PyTorch for TableFormer, with configurable thread budgets and support for various accelerators (GPU, MPS). Memory usage ranges from 2.4-6.2 GB depending on configuration.
- Core assumption: Model optimizations and resource configuration options can maintain acceptable performance across different hardware configurations without requiring specialized infrastructure.
- Evidence anchors:
  - [abstract]: "runs efficiently on commodity hardware in a small resource budget"
  - [section]: "For inference, our implementation relies on the onnxruntime [5]. The Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency"
  - [corpus]: Weak evidence - corpus shows related work but no direct validation of performance claims on commodity hardware
- Break condition: Efficiency degrades when running on very low-resource environments (e.g., less than 2GB RAM) or when OCR is enabled with the current EasyOCR backend.

## Foundational Learning

- Concept: PDF document structure and parsing challenges
  - Why needed here: Understanding how PDFs store text and layout information is crucial for comprehending Docling's PDF backend requirements and limitations
  - Quick check question: What are the two basic requirements Docling's PDF backend must satisfy to process documents effectively?

- Concept: Object detection and computer vision fundamentals
  - Why needed here: DocLayNet uses object detection (RT-DETR architecture) to identify document elements, so understanding bounding boxes, confidence scores, and detection metrics is essential
  - Quick check question: How does the DocLayNet model convert image-based page analysis into structured text and layout elements?

- Concept: Table structure recognition techniques
  - Why needed here: TableFormer's ability to recover logical table structures from visual information is a key differentiator, requiring understanding of vision transformers and cell hierarchy
  - Quick check question: What specific table characteristics does TableFormer handle that earlier approaches couldn't?

## Architecture Onboarding

- Component map: PDF backends (docling-parse default, pypdfium alternative) -> Model pipeline (layout analysis → table structure recognition → optional OCR) -> Assembly and post-processing (metadata detection, reading order inference) -> Output serialization (JSON, Markdown) -> Core data models (PagePredictions, Document object)

- Critical path: 1. PDF parsing (text extraction + page rendering) 2. Layout analysis (DocLayNet object detection) 3. Table structure recognition (TableFormer for detected tables) 4. OCR (optional, EasyOCR backend) 5. Post-processing (language detection, reading order, metadata) 6. Output assembly (JSON/Markdown serialization)

- Design tradeoffs: Multiple PDF backends provide flexibility but increase complexity; CPU-optimized models ensure broad compatibility but may limit performance on complex documents; Optional OCR support adds functionality but significantly impacts speed; Markdown vs JSON output formats serve different use cases (readability vs machine processing)

- Failure signatures: Poor table structure recovery: Check if pypdfium backend is being used instead of docling-parse; Slow processing: Verify OCR is disabled for non-scanned documents; check thread budget configuration; Missing metadata: Confirm PDF contains extractable metadata; check language detection accuracy; Layout segmentation errors: Review training data coverage for document type; verify PDF parsing quality

- First 3 experiments: 1. Convert a simple PDF document (e.g., academic paper) using default settings and compare JSON vs Markdown output 2. Process a document with complex tables using both PDF backends and compare table structure recognition quality 3. Configure and run batch conversion mode with thread budget optimization to measure throughput differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Docling's layout analysis accuracy compare to human annotators across different document categories?
- Basis in paper: [explicit] The paper mentions that layout predictions fall approximately 10% behind the inter-annotator agreement on DocLayNet dataset, but doesn't provide detailed breakdowns by document category
- Why unresolved: The paper only provides aggregate performance metrics and doesn't break down accuracy by document type (financial reports, patents, scientific articles, etc.)
- What evidence would resolve it: Detailed performance metrics showing Docling's accuracy on each document category from the DocLayNet dataset

### Open Question 2
- Question: What is the impact of GPU acceleration on Docling's processing speed and memory usage?
- Basis in paper: [explicit] The paper states "Establishing GPU acceleration support for the AI models is currently work-in-progress and largely untested"
- Why unresolved: The paper provides CPU-only performance benchmarks but doesn't test or report on GPU acceleration capabilities
- What evidence would resolve it: Comparative benchmarks showing processing speed and memory usage with GPU acceleration enabled versus CPU-only processing

### Open Question 3
- Question: How does Docling handle documents with complex nested table structures and irregular formatting?
- Basis in paper: [explicit] The paper mentions TableFormer can handle "partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy" but doesn't provide specific examples or metrics
- Why unresolved: The paper doesn't provide concrete examples or quantitative metrics for handling complex table structures
- What evidence would resolve it: Detailed case studies or metrics showing Docling's performance on documents with nested tables, irregular formatting, and other complex table structures

### Open Question 4
- Question: What is the impact of different OCR backends on Docling's overall performance and accuracy?
- Basis in paper: [explicit] The paper mentions they "are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends"
- Why unresolved: The paper only mentions EasyOCR as the default OCR backend and doesn't compare it with other OCR solutions
- What evidence would resolve it: Comparative analysis of different OCR backends in terms of speed, accuracy, and resource usage when integrated with Docling

### Open Question 5
- Question: How does Docling's performance scale with larger documents and higher page counts?
- Basis in paper: [inferred] The paper only tests on a dataset of 225 pages, which may not represent larger document processing scenarios
- Why unresolved: The paper doesn't provide performance data for larger documents or document collections
- What evidence would resolve it: Performance benchmarks showing how processing time, memory usage, and accuracy change as document size and page count increase

## Limitations

- Model performance uncertainty: No quantitative performance metrics provided for DocLayNet or TableFormer models
- Hardware performance variability: Performance claims lack context about document complexity and specific hardware configurations
- PDF parsing limitations: Trade-offs between accuracy and performance across different document types and PDF encodings not quantified

## Confidence

**High Confidence**: Modular pipeline architecture with clear interfaces for extensibility; ONNX Runtime integration for CPU-optimized layout analysis; Sequential processing pipeline from PDF parsing through output assembly; Open-source licensing and community contribution model

**Medium Confidence**: State-of-the-art AI model integration (DocLayNet, TableFormer); Commodity hardware compatibility claims; Memory usage estimates; Table structure recognition capabilities

**Low Confidence**: Processing speed claims; Quality of layout analysis compared to alternatives; OCR performance and accuracy; Batch processing throughput optimization

## Next Checks

1. **Benchmark Performance Testing**: Process a diverse corpus of 50+ documents (including academic papers, technical reports, and scanned documents) with both PDF backends, measuring actual throughput, memory usage, and accuracy metrics for layout analysis and table structure recognition.

2. **Model Comparison Study**: Compare Docling's table structure recognition output against established tools (Camelot, Tabula) on standardized datasets like PubTabNet, quantifying improvements in cell hierarchy detection and structural accuracy.

3. **Hardware Compatibility Assessment**: Test Docling across a range of hardware configurations (from low-end laptops to high-end workstations) to validate commodity hardware claims and identify performance bottlenecks in real-world usage scenarios.