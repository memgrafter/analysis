---
ver: rpa2
title: 'DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning
  Robustness of Vision Language Models'
arxiv_id: '2411.00836'
source_url: https://arxiv.org/abs/2411.00836
tags:
- variant
- reasoning
- function
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DynaMath, a dynamic visual math benchmark
  designed to evaluate the robustness of Vision-Language Models (VLMs) in mathematical
  reasoning. Unlike existing static benchmarks, DynaMath uses Python programs to generate
  diverse variants of 501 seed questions, creating 5,010 concrete problems with variations
  in numerical values, function types, geometric transformations, and more.
---

# DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models

## Quick Facts
- arXiv ID: 2411.00836
- Source URL: https://arxiv.org/abs/2411.00836
- Reference count: 40
- Primary result: DynaMath reveals significant robustness gaps in VLMs, with worst-case accuracy often less than 50% of average-case accuracy

## Executive Summary
This paper introduces DynaMath, a dynamic visual math benchmark designed to evaluate the robustness of Vision-Language Models (VLMs) in mathematical reasoning. Unlike existing static benchmarks, DynaMath uses Python programs to generate diverse variants of 501 seed questions, creating 5,010 concrete problems with variations in numerical values, function types, geometric transformations, and more. Experiments with 14 state-of-the-art VLMs reveal significant gaps between average-case and worst-case accuracy, with worst-case accuracy often less than 50% of average-case performance. Many models also show high repetition consistency, indicating consistent failures on specific variants rather than random errors. These findings highlight the need for more robust VLMs in mathematical reasoning tasks and provide valuable insights for future model development.

## Method Summary
DynaMath generates 5,010 mathematical reasoning questions from 501 seed questions encoded as Python programs. Each seed question produces 10 concrete variants through controlled modifications including numerical changes, function type substitutions, geometric transformations, and coordinate system alterations. The benchmark covers nine mathematical topics across elementary to undergraduate difficulty levels. Evaluation uses average-case accuracy, worst-case accuracy (performance on the single worst variant per seed), and repetition consistency to measure inherent randomness. 14 state-of-the-art VLMs are evaluated using zero-shot and few-shot CoT prompts with JSON-formatted answer extraction.

## Key Results
- VLMs show significant robustness gaps, with worst-case accuracy often less than 50% of average-case accuracy
- Many models demonstrate high repetition consistency, indicating systematic reasoning failures rather than random errors
- Closed-source models (GPT-4o, Claude-3.5) outperform open-source models (Qwen2-VL, InternVL2, LLaVA, DeepSeek-VL) on both average and worst-case metrics
- Scaling trends observed in open-source models, with performance improving as model size increases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Dynamic question generation through Python programs exposes robustness gaps in VLMs that static benchmarks miss.
- **Mechanism**: Each seed question is encoded as a Python program that generates multiple concrete variants with controlled variations (numerical values, function types, geometric transformations, etc.). This systematic variation tests whether VLMs can apply reasoning consistently across similar problems.
- **Core assumption**: VLMs should perform consistently across mathematically equivalent problem variants, similar to human reasoning patterns.
- **Evidence anchors**:
  - [abstract]: "Unlike existing static benchmarks, DynaMath uses Python programs to generate diverse variants of 501 seed questions, creating 5,010 concrete problems"
  - [section]: "DynaMath allows us to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question"
  - [corpus]: Weak - DynaMath is unique in its dynamic generation approach; no direct corpus comparison found

### Mechanism 2
- **Claim**: Worst-case accuracy metric reveals fundamental reasoning weaknesses that average-case accuracy masks.
- **Mechanism**: By measuring performance on the single worst-performing variant per seed question, the benchmark identifies consistent failure modes where VLMs fail systematically on specific problem types.
- **Core assumption**: Models that perform well on average but poorly on worst cases have unreliable reasoning capabilities.
- **Evidence anchors**:
  - [abstract]: "Our results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy"
  - [section]: "The worst-case accuracy is defined as the percentage of correctly answered seed problems in all 10 variants"
  - [corpus]: Weak - This specific worst-case analysis approach appears novel in the multimodal reasoning literature

### Mechanism 3
- **Claim**: Repetition consistency measurement distinguishes inherent model randomness from systematic reasoning failures.
- **Mechanism**: By generating multiple responses to identical questions, the benchmark measures whether failures are due to inherent randomness or consistent inability to handle specific problem variants.
- **Core assumption**: High repetition consistency with low worst-case accuracy indicates systematic reasoning failures rather than random errors.
- **Evidence anchors**:
  - [abstract]: "many models show high consistency in answering these questions – the incorrectness of a certain variant of a seed question is not due to inherent randomness"
  - [section]: "We define repetition consistency as: RC(i, j) = 1/K ∑k=1K I[Ansk(i, j) = Ans(i, j)]"
  - [corpus]: Weak - While consistency measurement exists in LLM literature, the specific application to multimodal mathematical reasoning appears novel

## Foundational Learning

- **Concept: Dynamic benchmarking vs static benchmarking**
  - Why needed here: Static benchmarks can be memorized by models, masking true reasoning capabilities. Dynamic generation creates infinite variations that test genuine understanding.
  - Quick check question: What advantage does generating 5,010 questions from 501 seed questions provide over a static benchmark of 501 questions?

- **Concept: Worst-case vs average-case performance metrics**
  - Why needed here: Average accuracy can hide systematic failures. Worst-case accuracy reveals whether models can handle all variants of a problem type.
  - Quick check question: If a model scores 80% average accuracy but only 40% worst-case accuracy, what does this reveal about its reasoning robustness?

- **Concept: Repetition consistency in model evaluation**
  - Why needed here: VLMs can exhibit randomness in their outputs. Consistency measurement determines whether failures are systematic or random.
  - Quick check question: How would you interpret a VLM that scores low worst-case accuracy but high repetition consistency?

## Architecture Onboarding

- **Component map**: Seed question collection (501 questions) -> Python program generation (each seed → program) -> Dynamic generation engine (produces concrete variants) -> Answer extraction pipeline (JSON formatting) -> Evaluation metrics (average accuracy, worst-case accuracy, repetition consistency)

- **Critical path**: Seed question → Python program → 10 variants → VLM evaluation → metric calculation → analysis

- **Design tradeoffs**:
  - Static vs dynamic generation: Static is simpler but risks memorization; dynamic is more complex but tests true reasoning
  - Number of variants per seed: More variants provide better worst-case measurement but increase evaluation cost
  - Program complexity: More sophisticated programs enable richer variations but require more human effort

- **Failure signatures**:
  - Low worst-case accuracy with high repetition consistency: Systematic reasoning failures
  - Low worst-case accuracy with low repetition consistency: Random errors or unstable outputs
  - High average accuracy with low worst-case accuracy: Memorization without robust understanding

- **First 3 experiments**:
  1. **Baseline comparison**: Run DynaMath on existing static benchmarks to quantify the gap between static and dynamic evaluation
  2. **Varying variant count**: Test whether 5, 10, or 20 variants per seed question provides optimal worst-case measurement
  3. **Adversarial generation**: Implement an adversarial program generator that specifically targets model weaknesses identified in initial runs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can adversarial training on DynaMath significantly improve the mathematical reasoning robustness of vision-language models?
- **Basis in paper**: [explicit] The paper mentions that an intriguing approach to enhance VLM robustness involves leveraging adversarial training (Zhou et al., 2024; Schlarmann et al., 2024) on DynaMath.
- **Why unresolved**: While the paper suggests this approach, it does not actually conduct experiments to test its effectiveness. The authors only propose it as a potential future direction.
- **What evidence would resolve it**: Conducting experiments where vision-language models are trained using adversarial examples generated from DynaMath, then evaluating their performance on the benchmark to measure improvements in worst-case accuracy.

### Open Question 2
- **Question**: How does the mathematical reasoning robustness of vision-language models scale with model size across different mathematical domains?
- **Basis in paper**: [explicit] The paper observes a clear scaling trend in open-source models, noting that higher performance is observed as model sizes increase (e.g., Qwen2-VL boosts its score from 42.1% to 55.1% when scaling from 7B to 72B parameters).
- **Why unresolved**: The paper provides preliminary evidence of scaling trends but does not systematically analyze how robustness scales across different mathematical domains or at what point diminishing returns occur.
- **What evidence would resolve it**: A comprehensive study evaluating models of various sizes (e.g., 7B, 34B, 72B, 90B parameters) across all nine mathematical topics in DynaMath, measuring both average-case and worst-case accuracy to identify scaling patterns and domain-specific differences.

### Open Question 3
- **Question**: What are the specific visual perception limitations that cause vision-language models to consistently fail on certain mathematical problem variants?
- **Basis in paper**: [explicit] The paper identifies consistent failure cases where models can solve some variants but consistently fail on others, suggesting perception issues rather than reasoning deficiencies.
- **Why unresolved**: While the paper identifies these consistent failures, it does not conduct a detailed analysis of the specific visual perception limitations (e.g., coordinate reading, angle measurement, shape recognition) that lead to these failures.
- **What evidence would resolve it**: A detailed error analysis categorizing the types of visual perception errors across different mathematical domains, potentially using techniques like saliency maps or controlled experiments with systematically modified visual inputs to isolate the specific perception challenges.

## Limitations
- The benchmark's dynamic generation relies on human-crafted Python programs for 501 seed questions, which may not fully capture the space of mathematical reasoning problems
- Evaluation results depend heavily on the quality and consistency of JSON-formatted answer extraction, which may not work equally well across all VLMs
- The benchmark focuses on visual mathematical reasoning but may not generalize to broader multimodal reasoning tasks

## Confidence
- **High confidence**: The fundamental insight that dynamic generation reveals robustness gaps missed by static benchmarks
- **Medium confidence**: The specific quantitative results showing worst-case accuracy being 50% of average-case accuracy, as these may vary with different model versions and generation parameters
- **Medium confidence**: The conclusion that systematic reasoning failures (high repetition consistency) are more concerning than random errors, as this interpretation depends on assumptions about ideal model behavior

## Next Checks
1. **Benchmark generalization test**: Apply DynaMath's dynamic generation approach to non-mathematical visual reasoning tasks to validate whether the methodology generalizes beyond mathematical domains

2. **Model improvement validation**: Train or fine-tune VLMs specifically to improve worst-case performance on DynaMath and measure whether this closes the gap between average and worst-case accuracy

3. **Adversarial variant generation**: Implement an automated system that generates increasingly difficult variants for each seed question and measure the point at which all models fail, establishing upper bounds on model reasoning capabilities