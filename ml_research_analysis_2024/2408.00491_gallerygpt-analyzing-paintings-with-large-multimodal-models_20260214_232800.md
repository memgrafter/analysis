---
ver: rpa2
title: 'GalleryGPT: Analyzing Paintings with Large Multimodal Models'
arxiv_id: '2408.00491'
source_url: https://arxiv.org/abs/2408.00491
tags:
- formal
- gallerygpt
- painting
- paintings
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GalleryGPT, a large multimodal model designed
  to generate formal analyses of paintings by focusing on visual characteristics rather
  than relying on pre-existing knowledge. The authors collect a dataset of 19k paintings
  paired with 50k analysis paragraphs, annotated by powerful LLMs based solely on
  visual elements, to train GalleryGPT using a fine-tuned LLaVA architecture.
---

# GalleryGPT: Analyzing Paintings with Large Multimodal Models

## Quick Facts
- arXiv ID: 2408.00491
- Source URL: https://arxiv.org/abs/2408.00491
- Reference count: 40
- Primary result: GalleryGPT significantly outperforms baseline models in generating formal painting analyses while demonstrating strong generalization to other art analysis tasks

## Executive Summary
GalleryGPT is a large multimodal model designed to generate formal analyses of paintings by focusing on visual characteristics rather than relying on pre-existing knowledge. The authors collect a dataset of 19k paintings paired with 50k analysis paragraphs, annotated by powerful LLMs based solely on visual elements, to train GalleryGPT using a fine-tuned LLaVA architecture. Experiments show that GalleryGPT significantly outperforms baseline models in generating formal analyses (BLEU: 21.23, GLEU: 21.68, METEOR: 37.62, ROUGE: 31.34) and demonstrates strong generalization to other tasks like visual question answering and style classification, while maintaining superior multimodal understanding capabilities.

## Method Summary
The method involves fine-tuning a pre-trained LLaVA model using a specialized dataset of paintings and corresponding formal analyses. The key innovation is using LLM-generated annotations that focus exclusively on visual characteristics, avoiding references to artist names or titles. LoRA modules are added to learn specific analyzing patterns of paintings while keeping the base visual encoder frozen to retain its superb visual perception ability. The model is trained using supervised learning with a two-stage annotation process: first generating overall formal analysis, then generating aspect-specific analyses covering composition, color, light/shadow, and other visual elements.

## Key Results
- GalleryGPT outperforms baseline models with BLEU score of 21.23, GLEU of 21.68, METEOR of 37.62, and ROUGE of 31.34
- The model demonstrates strong generalization to visual question answering and style classification tasks
- GalleryGPT maintains superior multimodal understanding capabilities compared to models that rely on pre-trained knowledge

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning LLaVA with painting-analysis pairs shifts the model's focus from recognition to visual analysis. The PaintingForm dataset contains only visual descriptions without identifying information, forcing the model to analyze visual characteristics rather than retrieve memorized knowledge. Core assumption: Removing identifying information prevents "LLM-biased visual hallucination" where models rely on pre-trained knowledge. Evidence: [abstract] "do not mention the title and artist in the formal analysis annotation" and [section 3.3] "we ask the LLMs do not mention the title and artist name in the generated formal analysis."

### Mechanism 2
LoRA modules enable task-specific adaptation without losing general multimodal capabilities. LoRA (Low-Rank Adaptation) adds small trainable matrices to the frozen LLaVA weights, allowing specialized learning for formal analysis while preserving the base model's visual understanding. Core assumption: The base ShareGPT4V-7B retains strong visual perception that can be leveraged for specialized tasks. Evidence: [abstract] "we add a LoRA component to LLM to learn the specific analyzing patterns of paintings" and [section 4.1] "we add several LoRA modules to learn the formal analysis specific patterns and freeze the LLM in ShareGPT4V-7B."

### Mechanism 3
Two-stage LLM annotation (overall + aspect-specific) creates diverse, comprehensive training data. First stage generates overall formal analysis, second stage generates analysis focusing on specific visual aspects (composition, color, light/shadow, etc.), creating richer training signals. Core assumption: Different aspects of visual analysis require different perceptual and descriptive capabilities. Evidence: [section 3.3] "we ask the LLMs to generate two-level formal analyses: 1) overall formal analysis, and 2) formal analysis from a certain perspective" and [figure 4] Statistics showing distribution of different analytical perspectives.

## Foundational Learning

- **Supervised fine-tuning (SFT)**: Why needed here - Standard pre-trained LMMs lack domain-specific knowledge for formal art analysis. Quick check: What is the key difference between SFT and continued pre-training in terms of parameter updates?
- **Multimodal alignment through contrastive learning**: Why needed here - GalleryGPT needs to understand how visual features map to detailed descriptive text. Quick check: How does CLIP-style training differ from instruction tuning in multimodal models?
- **LoRA (Low-Rank Adaptation)**: Why needed here - Enables efficient adaptation without full fine-tuning, preserving base capabilities. Quick check: What is the mathematical relationship between rank and parameter count in LoRA?

## Architecture Onboarding

- **Component map**: Image → Visual encoder (CLIP-Large ViT) → Projector (2-layer MLP) → LLM with LoRA → Formal analysis output
- **Critical path**: Visual features flow through frozen visual encoder and projector, then through adapted LLM to generate text analysis
- **Design tradeoffs**: Freezing base model vs. full fine-tuning (efficiency vs. adaptation), dataset size vs. annotation quality (19k images vs. 50k analyses), LoRA rank vs. adaptation capacity (128 rank used)
- **Failure signatures**: Recognition bias (outputs describe known paintings instead of visual analysis), hallucination (generates plausible but incorrect visual descriptions), aspect coverage (misses key visual elements in analysis)
- **First 3 experiments**: 1) Test with paintings the model "knows" vs. unknown paintings to verify recognition bias is eliminated, 2) Evaluate aspect coverage by checking if all 10 formal analysis elements are addressed, 3) Compare with baseline LMMs on the same test set to verify performance improvements

## Open Questions the Paper Calls Out

### Open Question 1
How does the "LLM-biased visual hallucination" phenomenon specifically manifest in formal analysis tasks, and what are the key factors that trigger this behavior in current LMMs? Basis: [explicit] The paper identifies this phenomenon through examples where GPT-4V and Gemini misrecognized a nameless painting and generated analysis based on recalled knowledge rather than visual content. Why unresolved: The paper observes this behavior but doesn't provide systematic analysis of when and why it occurs. What evidence would resolve it: Controlled experiments varying painting obscurity, prompt specificity, and visual complexity.

### Open Question 2
What is the optimal balance between freezing visual encoders versus fine-tuning them for specialized art analysis tasks? Basis: [explicit] The authors chose to freeze the visual encoder from ShareGPT4V-7B while only adding LoRA modules to the LLM. Why unresolved: The paper doesn't empirically compare performance when fine-tuning the visual encoder versus keeping it frozen. What evidence would resolve it: Comparative experiments testing multiple configurations across various art analysis tasks.

### Open Question 3
How well does GalleryGPT generalize to different types of artwork beyond paintings, such as sculptures, ceramics, or digital art, and what architectural modifications would be needed for these domains? Basis: [explicit] The authors acknowledge this limitation, stating "we only collect paintings in this work, while artworks consist of multiple types." Why unresolved: The current dataset and model are explicitly limited to paintings. What evidence would resolve it: Testing GalleryGPT on non-painting artwork datasets and evaluating performance.

## Limitations
- Dataset construction relies heavily on LLM-generated annotations without human verification
- Model evaluation uses automated metrics alone, which are insufficient for subjective art analysis quality assessment
- Limited generalization testing to only two additional tasks beyond formal analysis

## Confidence
- **High confidence**: The technical implementation of GalleryGPT using LLaVA architecture with LoRA modules is sound and follows established practices
- **Medium confidence**: The dataset construction methodology is reasonable but relies heavily on LLM annotations without human verification
- **Low confidence**: The evaluation methodology using automated metrics alone is insufficient for assessing subjective art analysis quality

## Next Checks
1. **Human Expert Evaluation**: Conduct blind evaluations where art experts rate GalleryGPT's formal analyses alongside human-generated analyses, assessing both accuracy and artistic insight quality across different painting styles and periods
2. **Recognition Bias Testing**: Systematically test GalleryGPT on paintings from the same period/artist that appear in its training data versus completely novel works to quantify any remaining "LLM-biased visual hallucination" effects
3. **Cross-Cultural Generalization**: Evaluate GalleryGPT's performance on non-Western art traditions (Chinese landscape painting, Islamic geometric patterns, etc.) to assess whether the model's training on predominantly Western art datasets limits its analytical capabilities across diverse aesthetic traditions