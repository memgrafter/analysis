---
ver: rpa2
title: Disentangling Dense Embeddings with Sparse Autoencoders
arxiv_id: '2408.00657'
source_url: https://arxiv.org/abs/2408.00657
tags:
- feature
- features
- saes
- similarity
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces sparse autoencoders (SAEs) to extract interpretable
  features from dense text embeddings, addressing the lack of interpretability in
  large language model representations. The authors train SAEs on embeddings from
  over 420,000 scientific paper abstracts and demonstrate that SAEs effectively disentangle
  semantic concepts while maintaining semantic fidelity.
---

# Disentangling Dense Embeddings with Sparse Autoencoders

## Quick Facts
- arXiv ID: 2408.00657
- Source URL: https://arxiv.org/abs/2408.00657
- Authors: Charles O'Neill; Christine Ye; Kartheik Iyer; John F. Wu
- Reference count: 40
- Introduces sparse autoencoders to extract interpretable features from dense text embeddings

## Executive Summary
This paper addresses the critical interpretability gap in large language model representations by introducing sparse autoencoders (SAEs) to disentangle dense text embeddings into semantically meaningful, sparse feature activations. The authors train SAEs on over 420,000 scientific paper abstracts from Semantic Scholar, demonstrating that the resulting feature decompositions preserve semantic fidelity while enabling precise semantic search interventions that outperform traditional query rewriting methods. The approach successfully bridges the semantic richness of dense embeddings with the interpretability of sparse representations through the introduction of "feature families" that capture hierarchical relationships between related features.

## Method Summary
The authors develop a sparse autoencoder architecture that learns to map dense embeddings from pre-trained language models into sparse, interpretable feature representations. The SAE consists of an encoder that produces sparse activations and a decoder that reconstructs the original embeddings, trained with L1 regularization to encourage sparsity. They introduce "feature families" to group related features based on correlation patterns, enabling hierarchical interpretation of the learned representations. The trained SAEs are then applied to scientific paper abstracts, and the resulting sparse features are evaluated for semantic interpretability through human assessment and quantitative correlation metrics. The authors also demonstrate how SAE-based feature interventions can precisely steer semantic search results, providing a practical application of the interpretable features.

## Key Results
- Achieves median Pearson correlations of 0.65-0.71 (cs.LG) and 0.85-0.98 (astro-ph) between predicted and actual feature activations
- SAE-based semantic search interventions outperform traditional query rewriting methods
- Successfully captures hierarchical feature relationships through "feature families" mechanism
- Maintains semantic fidelity while providing interpretable sparse representations

## Why This Works (Mechanism)
The SAE architecture works by learning a sparse latent representation that captures the most salient semantic features of dense embeddings. During training, the L1 regularization penalty forces the encoder to activate only a small subset of features for each input, creating a compressed representation that highlights semantically meaningful dimensions. The decoder then learns to reconstruct the original embedding from this sparse representation, ensuring that the activated features contain sufficient information to preserve semantic content. This process effectively disentangles the dense embedding space into interpretable semantic components, where each active feature corresponds to a specific concept or relationship. The feature family mechanism further organizes these individual features into hierarchical groups based on their activation patterns, enabling multi-level interpretation of the semantic space.

## Foundational Learning

**Sparse Autoencoders**: Neural networks trained to produce sparse latent representations through L1 regularization, enabling interpretable feature extraction from dense embeddings
- Why needed: Dense embeddings lack interpretability despite containing rich semantic information
- Quick check: Verify that SAE activations are indeed sparse (most values near zero) after training

**Feature Families**: Hierarchical groupings of related SAE features based on activation correlation patterns
- Why needed: Individual features may be too granular; families capture broader semantic concepts
- Quick check: Confirm that correlated features within families activate together across similar inputs

**Semantic Fidelity**: The degree to which reconstructed embeddings preserve the original semantic meaning
- Why needed: Interpretability is useless if the compressed representation loses critical semantic information
- Quick check: Measure reconstruction error and semantic similarity between original and reconstructed embeddings

**Correlation-based Feature Organization**: Using Pearson correlation to identify relationships between SAE features
- Why needed: Helps identify which features encode related semantic concepts
- Quick check: Verify that correlation thresholds meaningfully separate distinct semantic categories

**Search Steering via Feature Intervention**: Modifying query embeddings by manipulating specific SAE features to control search results
- Why needed: Demonstrates practical utility of interpretable features beyond passive analysis
- Quick check: Test whether feature interventions produce predictable changes in search result relevance

## Architecture Onboarding

**Component Map**: Dense Embeddings -> SAE Encoder -> Sparse Features -> SAE Decoder -> Reconstructed Embeddings
- Input embeddings flow through encoder to produce sparse activations
- Sparse features are decoded back to dense space for reconstruction
- Feature families organize sparse features hierarchically

**Critical Path**: Input Embeddings → SAE Encoder → Sparse Feature Extraction → Feature Family Organization → Semantic Analysis/Intervention
- The encoding process is most critical as it determines feature quality
- Feature family identification adds interpretability layer
- Search intervention demonstrates practical application

**Design Tradeoffs**: 
- Higher sparsity improves interpretability but may reduce semantic fidelity
- More features enable finer-grained interpretation but increase complexity
- Correlation thresholds for feature families balance granularity vs. coherence

**Failure Signatures**: 
- High reconstruction error indicates poor feature selection
- Non-sparse activations suggest insufficient regularization
- Uninterpretable features indicate poor alignment with semantic concepts
- Feature families that don't correspond to meaningful semantic groupings

**3 First Experiments**:
1. Train SAE on a small subset of embeddings and visualize activation patterns
2. Test reconstruction quality across different sparsity levels
3. Apply feature interventions to sample queries and observe search result changes

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on scientific abstracts limits generalizability to other text domains
- Computational cost of training and deploying SAEs at scale is not discussed
- Evaluation uses a relatively small dataset (100 queries) for search steering
- Feature family identification relies on correlation thresholds that may miss nuanced relationships

## Confidence
- High: Feature disentanglement effectiveness, semantic fidelity preservation, search steering precision
- Medium: Generalizability across domains, scalability to different embedding architectures
- Low: Real-world deployment feasibility, long-term robustness

## Next Checks
1. Evaluate SAE performance on non-scientific text domains (social media, conversational dialogue, code documentation) to assess generalizability
2. Test scalability by applying the approach to embeddings from different model architectures (BERT, GPT, T5 variants) with varying dimensionalities
3. Conduct a longitudinal study measuring feature stability and interpretability across model updates and embedding distribution shifts