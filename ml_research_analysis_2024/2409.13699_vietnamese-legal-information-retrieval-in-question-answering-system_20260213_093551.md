---
ver: rpa2
title: Vietnamese Legal Information Retrieval in Question-Answering System
arxiv_id: '2409.13699'
source_url: https://arxiv.org/abs/2409.13699
tags:
- retrieval
- search
- documents
- data
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of Vietnamese legal information
  retrieval in Question Answering (QA) systems, focusing on overcoming language-specific
  limitations such as excessive token length and inefficient data processing. The
  authors propose three main modifications: practical data processing approaches to
  handle embedding model constraints, an enhanced Reciprocal Rank Fusion method for
  combining keyword and vector search results, and a meticulous re-ranking process
  using Active Retrieval to improve answer accuracy.'
---

# Vietnamese Legal Information Retrieval in Question-Answering System

## Quick Facts
- arXiv ID: 2409.13699
- Source URL: https://arxiv.org/abs/2409.13699
- Reference count: 0
- One-line primary result: Improved recall metrics with up to 0.98 recall on processed datasets using hybrid BM25-vector search and LLM re-ranking

## Executive Summary
This paper addresses Vietnamese legal information retrieval challenges in QA systems, focusing on overcoming language-specific limitations like excessive token length and inefficient data processing. The authors propose a comprehensive approach combining practical data processing, enhanced Reciprocal Rank Fusion for keyword and vector search combination, and LLM-powered re-ranking using Active Retrieval. The system significantly improves performance and reliability, achieving up to 0.98 recall on processed datasets while outperforming traditional methods.

## Method Summary
The approach involves three main modifications: (1) practical data processing to handle embedding model constraints through chunking legal articles into sub-256-token segments with overlap and metadata enrichment, (2) enhanced Reciprocal Rank Fusion combining BM25 and vector search results with normalized scores, and (3) meticulous re-ranking using Active Retrieval with LLM-generated answers as queries. These techniques are integrated into a comprehensive QA system that processes Vietnamese legal documents and QA pairs from the "Legal Library" website.

## Key Results
- Achieves up to 0.98 recall on processed Vietnamese legal datasets
- Demonstrates effectiveness of combining keyword and semantic search approaches
- Shows LLM-powered re-ranking improves answer accuracy and user experience
- Outperforms traditional retrieval methods on Vietnamese legal information

## Why This Works (Mechanism)

### Mechanism 1
Chunking legal articles into sub-256-token segments while preserving context improves embedding quality and retrieval accuracy. The chunking process uses section-based splits when possible, otherwise employs overlapping chunks to retain related information across boundaries. Metadata enrichment adds contextual cues to each chunk. The Vietnamese legal embedding model (BKAI Bi-Encoder) has a 256-token input limit, and semantic meaning degrades beyond this threshold.

### Mechanism 2
Reciprocal Rank Fusion with normalized scores effectively combines keyword and semantic search results to improve retrieval performance. BM25 and Bi-Encoder scores are normalized to [0,1] range using min-max normalization, then weighted equally (0.5 each) to produce combined rankings. Keyword and semantic search capture complementary aspects of relevance - BM25 handles exact term matching while Bi-Encoder captures semantic similarity.

### Mechanism 3
LLM-powered re-ranking using the generated answer as a query outperforms traditional cross-encoder approaches. Retrieved documents are re-ranked by comparing them to the LLM-generated answer using combined BM25-Vector scores, creating a feedback loop that improves document ordering. The LLM-generated answer captures the essential information need more accurately than the original query, enabling better document ranking.

## Foundational Learning

- **Concept**: Tokenization and embedding limitations in Vietnamese language processing
  - Why needed here: Vietnamese is a monosyllabic language where segmentation significantly affects model performance, and embedding models have strict token limits
  - Quick check question: Why is Vietnamese segmentation particularly important for embedding quality compared to English?

- **Concept**: Information retrieval evaluation metrics (Recall@K, MAP@K)
  - Why needed here: The system performance is evaluated using recall and mean average precision metrics at different cutoff points
  - Quick check question: What does Recall@10 of 0.98 indicate about the system's performance?

- **Concept**: Reciprocal Rank Fusion and normalization techniques
  - Why needed here: The system combines BM25 and vector search results using RRF with normalized scores
  - Quick check question: How does min-max normalization affect the combination of scores from different retrieval methods?

## Architecture Onboarding

- **Component map**: Query Rewriting (LLM) → BM25 Search → Vector Search → Score Normalization → RRF Combination → Active Retrieval → Answer Generation (LLM) → LLM-based Re-ranking
- **Data Pipeline**: Legal Articles → Preprocessing → Chunking → Embedding → Vector Database → Retrieval → Answer Generation
- **Critical path**: Query → BM25 Search → Vector Search → RRF → LLM Answer → LLM Re-ranking → Final Answer
- **Design tradeoffs**: Equal weighting (0.5/0.5) for BM25 and Vector scores vs. learned weights; Fixed 256-token chunk size vs. adaptive chunking based on content complexity; LLM-based re-ranking vs. traditional cross-encoder approaches
- **Failure signatures**: Low Recall@1 despite high Recall@10 suggests poor ranking quality; High variance in BM25 vs. Vector scores indicates potential imbalance in search methods; LLM re-ranking doesn't improve over initial ranking suggests answer generation quality issues
- **First 3 experiments**: 
  1. Test BM25-only vs. Vector-only retrieval on processed dataset to establish baseline performance gap
  2. Evaluate different chunk sizes (128, 256, 512 tokens) on embedding quality and retrieval accuracy
  3. Compare equal weighting (0.5/0.5) vs. learned weights for BM25-Vector combination on validation set

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed LLM-powered re-ranking method compare to traditional cross-encoder re-ranking in terms of computational efficiency and accuracy, especially for large-scale datasets? The authors suggest their method can serve as an alternative to cross-encoders but don't provide direct comparisons on computational efficiency or accuracy metrics.

### Open Question 2
What are the long-term effects of using overlapping chunking techniques on the retrieval accuracy and user experience in Vietnamese legal information retrieval systems? The paper focuses on immediate improvements without discussing potential long-term impacts of overlapping chunking on system performance and user satisfaction.

### Open Question 3
How does the performance of the proposed Vietnamese legal information retrieval system vary with different sizes and complexities of legal datasets? The paper evaluates on a specific dataset but doesn't explore how performance changes with different dataset sizes or complexities.

## Limitations
- Chunk size of 256 tokens appears to be a hard model constraint rather than an optimization based on content complexity
- Equal weighting for BM25 and vector search lacks justification or sensitivity analysis
- LLM-based re-ranking assumes generated answers are accurate without analyzing hallucination rates
- Evaluation uses only recall and MAP metrics without precision analysis or user studies

## Confidence
- **High Confidence**: Chunking methodology to handle embedding model constraints; BM25 and vector search combination through RRF
- **Medium Confidence**: Enhanced Reciprocal Rank Fusion normalization method; LLM-powered re-ranking implementation details
- **Low Confidence**: Claimed performance improvements (up to 0.98 recall) without baseline comparisons or statistical significance testing

## Next Checks
1. Conduct chunk size sensitivity analysis varying chunk sizes (128, 256, 512 tokens) to empirically determine optimal chunk size for Vietnamese legal documents
2. Replace fixed 0.5/0.5 weighting with learned weights optimized on validation data and compare performance
3. Measure LLM answer quality and hallucination rates, then analyze correlation with re-ranking effectiveness