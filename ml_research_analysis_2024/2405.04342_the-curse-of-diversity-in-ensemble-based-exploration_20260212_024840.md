---
ver: rpa2
title: The Curse of Diversity in Ensemble-Based Exploration
arxiv_id: '2405.04342'
source_url: https://arxiv.org/abs/2405.04342
tags:
- bootstrapped
- ensemble
- learning
- cerl
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Individual ensemble members in data-sharing ensembles underperform
  single-agent baselines due to low self-generated data and inefficient learning from
  off-policy data (the "curse of diversity"). Intuitively increasing replay buffer
  size or reducing ensemble size helps inconsistently and undermines aggregation benefits.
---

# The Curse of Diversity in Ensemble-Based Exploration

## Quick Facts
- arXiv ID: 2405.04342
- Source URL: https://arxiv.org/abs/2405.04342
- Reference count: 40
- Key outcome: Individual ensemble members in data-sharing ensembles underperform single-agent baselines due to low self-generated data and inefficient learning from off-policy data; CERL mitigates this by training each member to learn all others' value functions as an auxiliary task.

## Executive Summary
Ensemble-based exploration methods like Bootstrapped DQN suffer from a "curse of diversity" where individual ensemble members underperform single-agent baselines despite data sharing. This occurs because each member generates only a fraction of the training data but must learn from all ensemble members' data, creating highly off-policy learning conditions. The authors propose Cross-Ensemble Representation Learning (CERL) to address this by training each member's encoder to represent all ensemble members' value functions, improving both individual and aggregated performance across Atari and MuJoCo domains.

## Method Summary
The paper investigates ensemble-based exploration using Bootstrapped DQN and SAC with N=10 ensemble members sharing a single replay buffer. Each member generates 1/N of the data but learns from all data, creating the curse of diversity. CERL mitigates this by adding N×N value function heads where each member learns all others' value functions as auxiliary tasks, with encoder gradients scaled by 1/N. The method is tested on 55 Atari games and 4 MuJoCo tasks, comparing individual member performance against single-agent baselines and aggregated policy performance.

## Key Results
- Individual ensemble members underperform single-agent baselines by 20-30% on Atari due to low self-generated data proportion
- CERL consistently improves both individual and aggregated policy performance across all tested domains
- Diversity-reduction techniques (smaller N, more shared layers) improve individual performance but reduce aggregation benefits
- p%-tandem experiments confirm that performance correlates with self-generated data proportion

## Why This Works (Mechanism)

### Mechanism 1
Low self-generated data in ensemble training causes poor individual performance. When an ensemble member trains on mostly other members' data, it faces high off-policy error, leading to inaccurate value estimates and suboptimal policies. The core assumption is that Q-learning performance degrades significantly when training data distribution diverges from the current policy's state-action distribution.

### Mechanism 2
Representation learning via auxiliary tasks improves off-policy learning efficiency. CERL forces each ensemble member's encoder to represent all value functions, creating shared features that generalize better across different data distributions from ensemble members. The core assumption is that better representations enable more accurate value function approximation when training data comes from multiple, diverse policies.

### Mechanism 3
Diversity-reduction techniques trade individual performance for aggregation benefits. Reducing ensemble diversity (smaller N, more shared layers) makes training data more on-policy for each member but also reduces the benefit of majority voting. The core assumption is that the benefit of policy aggregation is proportional to ensemble diversity.

## Foundational Learning

- Concept: Off-policy learning challenges in Q-learning
  - Why needed here: The curse of diversity fundamentally stems from learning Q-values from data generated by other policies
  - Quick check question: Why does training on data from a different policy cause Q-learning to fail, and what techniques exist to mitigate this?

- Concept: Representation learning in deep RL
  - Why needed here: CERL's effectiveness relies on the idea that better representations can improve learning efficiency, especially in off-policy settings
  - Quick check question: How do auxiliary tasks like CURL, SPR, or CERL improve representation quality, and why might this help with off-policy learning?

- Concept: Ensemble methods and diversity
  - Why needed here: Understanding the trade-off between ensemble diversity (for exploration/better aggregation) and individual performance is central to this work
  - Quick check question: What are the theoretical benefits of ensemble diversity in RL, and how do they manifest in practice versus single-agent methods?

## Architecture Onboarding

- Component map: Environment -> Ensemble member interaction -> Shared replay buffer -> N×N Q-value heads training -> Policy updates -> Target network updates
- Critical path: 1) Sample one ensemble member per episode, collect transition 2) Store transition in shared replay buffer 3) Sample batch, update all N×N Q-value heads in parallel 4) Use main heads to update policies 5) Periodically update target networks
- Design tradeoffs: Number of ensemble members (N) vs individual performance, shared layers (L) vs diversity, auxiliary task design vs stability, replay buffer size vs off-policy data
- Failure signatures: Individual members consistently underperform single-agent baselines, policy aggregation fails to compensate, CERL causes training instability, performance improvement only in individual policies
- First 3 experiments: 1) Implement p%-tandem experiment to verify self-generated data proportion connection 2) Test CERL with different auxiliary task designs to identify stability issues 3) Compare CERL against other representation learning methods to isolate effects

## Open Questions the Paper Calls Out

### Open Question 1
Does the "curse of diversity" generalize to ensemble-based exploration methods that do not use data-sharing? The paper focuses on data-sharing ensembles and does not test non-data-sharing variants that maintain separate replay buffers for each member.

### Open Question 2
Is there an optimal ensemble size that balances the benefits of policy aggregation with the costs of the "curse of diversity"? The authors show that reducing ensemble size mitigates the curse but reduces aggregation benefits without identifying an optimal point.

### Open Question 3
Can the "curse of diversity" be mitigated through alternative representation learning approaches beyond CERL? The paper only tests two specific alternative methods (MICo, MH) and concludes this is preliminary investigation.

## Limitations
- Experimental scope limited to DQN and SAC algorithm families with specific hyperparameter settings
- The paper doesn't provide theoretical justification for why learning all value functions as auxiliary tasks specifically addresses the curse of diversity
- Claims about CERL generalization across diverse RL domains need broader validation in continuous control tasks

## Confidence

| Claim | Confidence |
|-------|------------|
| Low self-generated data causes poor individual performance | Medium |
| CERL's representation learning mechanism effectively addresses the curse | Medium |
| Diversity-reduction tradeoff analysis is conceptually sound | Low |

## Next Checks

1. Systematically vary the proportion of self-generated data in p%-tandem experiments to identify the off-policy threshold where performance degradation begins
2. Test CERL with alternative representation learning methods (MICo, MH) to isolate whether the auxiliary task design or representation learning itself drives improvements
3. Implement a diversity sweep experiment varying N and shared layers to map the individual-vs-aggregated performance tradeoff surface