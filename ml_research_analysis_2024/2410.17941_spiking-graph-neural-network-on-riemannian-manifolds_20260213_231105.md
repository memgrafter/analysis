---
ver: rpa2
title: Spiking Graph Neural Network on Riemannian Manifolds
arxiv_id: '2410.17941'
source_url: https://arxiv.org/abs/2410.17941
tags:
- manifold
- spiking
- space
- graph
- riemannian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces Manifold-valued Spiking Graph Neural Networks
  (MSG), the first spiking GNN designed for Riemannian manifolds. MSG addresses two
  key issues in existing spiking GNNs: ignoring graph geometry by operating in Euclidean
  space, and high latency due to Back-Propagation-Through-Time (BPTT) with surrogate
  gradients.'
---

# Spiking Graph Neural Network on Riemannian Manifolds

## Quick Facts
- arXiv ID: 2410.17941
- Source URL: https://arxiv.org/abs/2410.17941
- Authors: Li Sun; Zhenhao Huang; Qiqi Wan; Hao Peng; Philip S. Yu
- Reference count: 40
- Primary result: Introduces Manifold-valued Spiking Graph Neural Networks (MSG), achieving up to 20x less energy consumption than Riemannian baselines

## Executive Summary
This paper introduces Manifold-valued Spiking Graph Neural Networks (MSG), the first spiking GNN designed for Riemannian manifolds. MSG addresses two key issues in existing spiking GNNs: ignoring graph geometry by operating in Euclidean space, and high latency due to Back-Propagation-Through-Time (BPTT) with surrogate gradients. The core innovation is a Manifold Spiking Layer that performs parallel forwarding of spike trains and manifold representations using a new spiking neuron with diffeomorphism. Extensive experiments on four benchmark datasets demonstrate MSG achieves superior performance to previous spiking GNNs and energy efficiency to conventional GNNs.

## Method Summary
MSG combines spiking neural networks with Riemannian geometry by introducing a Manifold Spiking Layer that simultaneously processes spike trains and manifold representations. The key innovation is a spiking neuron that uses exponential maps to create diffeomorphisms between tangent spaces and manifolds, allowing spikes to push manifold representations along geodesics. Instead of BPTT through spikes, MSG uses a novel Differentiation via Manifold (DvM) algorithm that differentiates through manifold representations using pullback properties. The method theoretically approximates a solver of manifold ordinary differential equations and is evaluated on four benchmark datasets.

## Key Results
- MSG achieves superior performance to previous spiking GNNs on node classification and link prediction tasks
- MSG demonstrates up to 20x less energy consumption than Riemannian baseline methods
- The method successfully approximates manifold ODE solvers while maintaining training efficiency
- MSG outperforms conventional GNNs in both accuracy and energy efficiency across all benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The manifold spiking neuron emits spike trains and relates them to manifold representations simultaneously through diffeomorphism
- Mechanism: The neuron uses exponential map to create a diffeomorphism between spike-derived Euclidean vectors in tangent space and points on the manifold, allowing spikes to push manifold representations along geodesics
- Core assumption: The diffeomorphism between tangent space and manifold is smooth and invertible
- Evidence anchors:
  - [abstract]: "design a new spiking neuron on geodesically complete manifolds with the diffeomorphism"
  - [section]: "We propose to place the Euclidean v, a representation of the spikes, in the tangent space TzM of the point z. In MSG, we choose the exponential map to act as the diffeomorphism between the tangent space and manifold"
  - [corpus]: "Geometry-Aware Spiking Graph Neural Network" (related work on geometric spiking GNNs)

### Mechanism 2
- Claim: Differentiation via Manifold (DvM) eliminates the need for surrogate gradients and reduces training latency
- Mechanism: Instead of differentiating through spikes using BPTT, DvM differentiates through the manifold representation using pullback properties, enabling recurrence-free gradient computation
- Core assumption: The manifold operations are differentiable and the pullback between dual spaces can be computed
- Evidence anchors:
  - [abstract]: "BPTT regarding the spikes is replaced by the proposed differentiation via manifold"
  - [section]: "In MSG, we decouple the forward pass and backward pass, and propose Differentiation via Manifold (DvM) to avoid the high latency in differentiation via spikes"
  - [corpus]: "SGNNBench: A Holistic Evaluation of Spiking Graph Neural Network on Large-scale Graph" (evaluation of spiking GNNs, but no mention of DvM approach)

### Mechanism 3
- Claim: MSG approximates a solver of manifold ordinary differential equations through dynamic chart construction
- Mechanism: Each spiking layer corresponds to a chart defined by logarithmic map, and the layer-by-layer forwarding solves the manifold ODE by connecting successive charts
- Core assumption: The manifold ODE can be solved by finite collection of successive charts with sufficient accuracy
- Evidence anchors:
  - [abstract]: "Theoretically, we show that MSG approximates a solver of the manifold ordinary differential equation"
  - [section]: "MSG approximates a dynamic chart solver of manifold ODE (Theorem 5.2)"
  - [corpus]: "Riemannian Liquid Spatio-Temporal Graph Network" (related work on continuous-time GNNs on manifolds)

## Foundational Learning

- Concept: Riemannian manifolds and tangent spaces
  - Why needed here: The manifold structure provides the geometric framework where graph representations live, and tangent spaces provide the Euclidean-like domains where spike-derived vectors operate
  - Quick check question: Why can't we directly place spike trains in hyperbolic space without using tangent spaces?

- Concept: Geodesically complete manifolds and exponential/logarithmic maps
  - Why needed here: Geodesic completeness ensures the exponential map is defined everywhere, allowing any point on the manifold to be reached from any other point along a geodesic
  - Quick check question: What property of hyperbolic and hyperspherical spaces makes them suitable for MSG?

- Concept: Pullback and dual spaces in differential geometry
  - Why needed here: The pullback allows gradient computation in the dual space of the tangent bundle, enabling DvM to avoid differentiating through spikes
  - Quick check question: How does the pullback operation differ from standard backpropagation in Euclidean space?

## Architecture Onboarding

- Component map: Input features → Graph convolution layer → Manifold spiking neuron (spike generation + manifold update) → Pooling → Manifold representation → Loss computation
- Critical path: Forward pass: Input → GCN → MSNeuron (spikes + manifold) → Output; Backward pass: Loss → Dual space gradient → Layer-wise update via DvM
- Design tradeoffs: Using tangent spaces adds computational overhead but enables spike-manifold alignment; DvM reduces latency but requires complex differential geometry implementation
- Failure signatures: Gradient vanishing/explosion in DvM (checked via gradient norm monitoring), poor manifold representation quality, spikes not effectively encoding structural information
- First 3 experiments:
  1. Implement basic manifold spiking neuron with exponential map on sphere (S²) with synthetic graph data
  2. Add DvM training and verify gradient flow without surrogate gradients
  3. Compare energy consumption of MSG vs Euclidean spiking GNN on benchmark dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MSG perform on dynamic graphs compared to static graphs?
- Basis in paper: [inferred] The paper focuses on static graphs and mentions that dynamic graph models are out of scope, but doesn't explore MSG's performance on dynamic graphs.
- Why unresolved: The authors deliberately excluded dynamic graphs from their study to maintain focus, leaving this comparison unexplored.
- What evidence would resolve it: Experimental results comparing MSG's performance on both static and dynamic graph datasets, showing accuracy and efficiency metrics for each scenario.

### Open Question 2
- Question: Can MSG be extended to handle heterophilous graphs where connected nodes may have dissimilar features?
- Basis in paper: [inferred] The paper mentions that existing SNN-based GNNs consider undirected, homophilous graphs, but doesn't investigate MSG's behavior on heterophilous graphs.
- Why unresolved: The authors acknowledge this as an open problem but don't attempt to address it in their current work.
- What evidence would resolve it: Experimental results showing MSG's performance on heterophilous graph datasets compared to existing methods, along with any modifications needed to handle non-homophilous relationships.

### Open Question 3
- Question: How does the choice of manifold (e.g., hyperbolic vs. spherical) affect MSG's performance across different graph types?
- Basis in paper: [explicit] The authors mention that MSG can be instantiated in different manifolds and study the impact of representation space in the ablation study.
- Why unresolved: While the ablation study compares different manifolds, it doesn't systematically analyze which manifold types work best for specific graph structures or properties.
- What evidence would resolve it: A comprehensive study mapping graph characteristics (e.g., hierarchical structure, cyclical patterns) to optimal manifold choices, with performance comparisons across multiple graph types.

## Limitations

- Theoretical guarantees rely heavily on manifold smoothness assumptions that may not hold for all graph data, particularly when node features are sparse or noisy
- Computational complexity of DvM scales with manifold dimension, potentially limiting scalability to high-dimensional spaces
- Evaluation focuses on specific manifold types (hyperbolic, hyperspherical) without exploring other geodesically complete manifolds that might better suit certain graph structures

## Confidence

- High: MSG's energy efficiency advantage over conventional GNNs (supported by clear FLOPs analysis)
- Medium: Performance superiority on benchmark datasets (requires independent replication)
- Low: Theoretical approximation quality (Theorem 5.2 depends on idealized conditions)

## Next Checks

1. Verify gradient flow stability across different manifold dimensions and graph sizes through ablation studies
2. Test MSG on graphs with heterogeneous feature distributions to assess diffeomorphism robustness
3. Compare MSG's ODE approximation error against ground truth manifold ODE solvers on synthetic manifolds