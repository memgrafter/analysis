---
ver: rpa2
title: Towards Optimizing and Evaluating a Retrieval Augmented QA Chatbot using LLMs
  with Human in the Loop
arxiv_id: '2407.05925'
source_url: https://arxiv.org/abs/2407.05925
tags:
- evaluation
- dataset
- metrics
- question
- retriever
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a human-in-the-loop approach to developing
  and evaluating an HR chatbot using LLMs. The authors worked with SAP SE to create
  a chatbot addressing employee inquiries, incorporating domain experts throughout
  development cycles including dataset collection, prompt optimization, and evaluation.
---

# Towards Optimizing and Evaluating a Retrieval Augmented QA Chatbot using LLMs with LLMs with Human in the Loop

## Quick Facts
- arXiv ID: 2407.05925
- Source URL: https://arxiv.org/abs/2407.05925
- Authors: Anum Afzal; Alexander Kowsik; Rajna Fani; Florian Matthes
- Reference count: 32
- Primary result: Human-in-the-loop approach improves HR chatbot development with LLM-based evaluation showing high correlation to human judgment

## Executive Summary
This paper presents a human-in-the-loop methodology for developing and evaluating an HR chatbot using large language models, developed in collaboration with SAP SE. The approach integrates domain experts throughout the development cycle, including dataset collection, prompt optimization, and evaluation phases. The system employs a retrieval-augmented generation pipeline with optimizations to both the retriever and model prompts. The study demonstrates that GPT-4 outperforms other models in generation capabilities and that LLM-based evaluation metrics like G-Eval and Prometheus provide reliable assessments closely aligned with human evaluation, particularly for truthfulness.

## Method Summary
The methodology involves a comprehensive human-in-the-loop approach to chatbot development, beginning with dataset collection from SAP SE's HR department and continuing through multiple development cycles. The system uses a retrieval-augmented generation pipeline where the retriever first searches a domain-specific corpus, followed by prompt optimization techniques to improve generation quality. The evaluation framework incorporates both automated metrics (G-Eval and Prometheus) and human judgment, with particular attention to truthfulness assessment. The approach includes iterative refinement cycles where domain experts provide feedback to optimize both the retrieval component and the language model prompts.

## Key Results
- GPT-4 demonstrates superior performance in language generation capabilities compared to fine-tuned LongT5 and other models
- LLM-based evaluation metrics (G-Eval and Prometheus) show high correlation (Pearson coefficient of 0.82) with human judgments
- Despite optimization efforts, retrieval accuracy remains low at 40% due to corpus limitations and cleaning approaches
- Human evaluation proves essential for domain-specific applications where automated metrics may not capture all nuances

## Why This Works (Mechanism)
The human-in-the-loop approach succeeds because domain experts can identify context-specific nuances and edge cases that automated systems miss, particularly in specialized domains like HR where accuracy and truthfulness are critical. The iterative refinement process allows for continuous improvement of both retrieval accuracy and generation quality by incorporating expert feedback at each development stage. The combination of retrieval-augmented generation with optimized prompts enables the system to ground responses in verified information while maintaining conversational quality.

## Foundational Learning
- **Retrieval-augmented generation**: Combines information retrieval with language model generation to improve factual accuracy - needed to ground responses in verified domain knowledge; quick check: verify retrieved documents are relevant to the query
- **Prompt optimization techniques**: Strategic engineering of input prompts to improve model outputs - needed to maximize performance of base LLMs for specific tasks; quick check: compare outputs across different prompt formulations
- **LLM-based evaluation metrics**: Using language models to assess other models' outputs - needed for scalable, consistent evaluation beyond human judgment alone; quick check: correlate metric scores with human evaluation
- **Human-in-the-loop development**: Iterative process incorporating expert feedback throughout development - needed for domain-specific applications requiring high accuracy; quick check: track improvement metrics across development cycles
- **Retrieval accuracy measurement**: Quantitative assessment of how well the retriever finds relevant documents - needed to identify and address corpus limitations; quick check: calculate precision and recall on test queries
- **Truthfulness assessment**: Evaluation of whether generated responses are factually correct - needed for HR applications where misinformation could have serious consequences; quick check: verify responses against source documents

## Architecture Onboarding

**Component Map:**
Dataset Collection -> Corpus Cleaning -> Retriever Optimization -> Prompt Engineering -> Generation Model -> Evaluation (Automated + Human)

**Critical Path:**
The critical path for successful response generation involves: (1) query processing, (2) document retrieval from cleaned corpus, (3) prompt formulation with retrieved context, (4) generation using LLM, and (5) evaluation against human standards. Bottlenecks typically occur at the retrieval stage due to corpus limitations.

**Design Tradeoffs:**
The system trades retrieval comprehensiveness for response quality - a smaller, well-cleaned corpus with lower retrieval recall can produce more accurate responses than a larger, noisier corpus. Similarly, the choice of GPT-4 over open-source alternatives prioritizes generation quality over deployment flexibility and cost considerations.

**Failure Signatures:**
Primary failure modes include: (1) unanswerable queries due to corpus gaps, (2) retrieval of irrelevant documents leading to hallucinated responses, (3) prompt optimization that inadvertently biases responses, and (4) automated evaluation metrics missing subtle context errors that humans would catch.

**First 3 Experiments:**
1. Conduct A/B testing of different corpus cleaning approaches to determine optimal balance between recall and precision
2. Implement controlled experiments varying prompt formulations to identify optimal prompt structures for HR queries
3. Run comparative analysis of different evaluation metrics against human judgment on a diverse sample of responses

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset collection from SAP SE's HR department results in significant retrieval challenges, with only 40% of test questions answerable from the corpus
- Automated evaluation metrics, while highly correlated with human judgment, lack detailed analysis of potential biases across different query types
- Direct performance comparisons between GPT-4 and LongT5 are limited by different evaluation methodologies applied to each model

## Confidence
- **High confidence**: GPT-4's superior performance in language generation capabilities and the reliability of LLM-based evaluation metrics in assessing truthfulness
- **Medium confidence**: The effectiveness of the human-in-the-loop approach for domain-specific optimization, given the limited scope to a single HR domain
- **Low confidence**: Claims about retrieval accuracy improvements, as the study shows persistently low retrieval performance despite optimization attempts

## Next Checks
1. Conduct a systematic analysis of the unanswerable questions to identify specific gaps in the corpus and evaluate whether different cleaning approaches or corpus expansion would improve retrieval performance
2. Perform cross-domain validation by applying the same methodology to at least two additional domains (e.g., technical support and customer service) to assess generalizability
3. Implement a controlled experiment comparing automated evaluation metrics against a larger sample of human evaluations across different query types to better understand metric reliability and potential biases