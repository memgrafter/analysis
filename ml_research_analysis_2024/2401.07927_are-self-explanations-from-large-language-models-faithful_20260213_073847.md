---
ver: rpa2
title: Are self-explanations from Large Language Models faithful?
arxiv_id: '2401.07927'
source_url: https://arxiv.org/abs/2401.07927
tags:
- paragraph
- answer
- redacted
- words
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to evaluate the faithfulness of self-explanations
  from large language models (LLMs) using self-consistency checks. The core idea is
  to ask the LLM to generate explanations and then re-evaluate the model's behavior
  with modified inputs based on those explanations.
---

# Are self-explanations from Large Language Models faithful?

## Quick Facts
- arXiv ID: 2401.07927
- Source URL: https://arxiv.org/abs/2401.07927
- Authors: Andreas Madsen; Sarath Chandar; Siva Reddy
- Reference count: 40
- Primary result: Self-explanation faithfulness is highly model-dependent, with different explanation types working better for different models and tasks

## Executive Summary
This paper introduces a method to evaluate the faithfulness of self-explanations from large language models using self-consistency checks. The core approach involves asking models to generate explanations and then re-evaluating their behavior with modified inputs based on those explanations. If the model's behavior changes as expected, the explanation is considered faithful. The authors apply this methodology to counterfactual, feature attribution, and redaction explanations across sentiment classification, multi-choice QA, and entailment tasks. The key finding is that self-explanation faithfulness varies significantly across models, explanation types, and tasks, suggesting that self-explanations should not be trusted in general without validation.

## Method Summary
The methodology generates explanations (counterfactual, feature attribution, or redaction) from LLMs for given inputs, then checks faithfulness by modifying inputs based on explanations and observing if model behavior changes as predicted. Evaluation uses separate inference sessions to prevent context leakage, with discrete "yes"/"no"/"unknown" outputs rather than confidence scores. The faithfulness ratio is calculated as the proportion of observations where self-consistency checks pass. The approach is applied across multiple datasets (IMDB, bAbI-1, MCTest, RTE) and three open-weight models (Llama2-70B, Falcon-40B/7B, Mistral-7B).

## Key Results
- Self-explanation faithfulness is highly model-dependent: counterfactuals work better for Llama2, feature attribution for Mistral, and redaction for Falcon 40B
- Overall, self-explanations are not faithful in general, with varying performance across tasks and explanation types
- Using separate inference sessions is crucial to prevent context leakage that could artificially inflate faithfulness scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-consistency checks validate faithfulness by re-evaluating predictions after explanation-guided input modifications
- Mechanism: If an explanation claims certain input features are important, removing or altering them should change the model's prediction. If the prediction changes as expected, the explanation is faithful
- Core assumption: The model's behavior is consistent across separate inference sessions and modifications directly impact predictions as claimed
- Evidence anchors: [abstract] "if an LLM says a set of words is important for making a prediction, then it should not be able to make its prediction without these words"

### Mechanism 2
- Claim: Separate inference sessions prevent context leakage that could artificially inflate faithfulness scores
- Mechanism: By evaluating explanations and consistency checks in separate chat sessions, we ensure the model cannot use prior conversation history to predict expected changes
- Core assumption: Instruction-tuned LLMs are sensitive to conversation context and may predict expected answers based on chat history
- Evidence anchors: [abstract] "it's important to perform the re-evaluation using a new chat session. Otherwise, the chat model may predict the opposite label only because it was previously prompted to do so within the chat history"

### Mechanism 3
- Claim: Using discrete classification outputs ("yes"/"no"/"unknown") rather than confidence scores provides more reliable faithfulness evaluation
- Mechanism: Discrete outputs avoid calibration issues and allow clear determination of whether predictions are possible after explanation-guided modifications
- Core assumption: Models can reliably produce discrete outputs and understand "unknown" as a valid response when information is missing
- Evidence anchors: [abstract] "we propose that future work on developing instruction-tuned LLMs also evaluate the self-explanation faithfulness using the methodology presented in this paper"

## Foundational Learning

- Concept: Self-consistency checks
  - Why needed here: Core evaluation mechanism for measuring faithfulness of self-explanations
  - Quick check question: How does re-evaluating predictions after explanation-guided modifications validate faithfulness?

- Concept: Counterfactual explanations
  - Why needed here: One of three explanation types tested, provides contrastive examples by modifying inputs to get opposite predictions
  - Quick check question: What makes counterfactual explanations particularly suitable for self-consistency evaluation?

- Concept: Discrete classification with "unknown" class
  - Why needed here: Avoids confidence score calibration issues and handles cases where modifications prevent classification
  - Quick check question: Why is allowing "unknown" responses important for faithful explanation evaluation?

## Architecture Onboarding

- Component map: Prompt generation → Explanation generation → Input modification → Classification → Faithfulness determination → Aggregation
- Critical path: Prompt generation → Explanation generation → Input modification → Classification (each step must succeed for faithfulness evaluation)
- Design tradeoffs: Separate sessions ensure validity but increase computational cost; discrete outputs are more reliable but may miss nuanced changes
- Failure signatures: High "unknown" rates indicate explanations may be too aggressive; inconsistent results across prompt variations suggest model sensitivity
- First 3 experiments:
  1. Test baseline classification accuracy to establish which observations can be evaluated
  2. Evaluate a single explanation type (counterfactual) on one dataset to verify methodology
  3. Test prompt variation robustness by running same experiment with different persona instructions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning large language models improve their faithfulness in generating self-explanations across different tasks and explanation types?
- Basis in paper: [explicit] The paper suggests in the Future Work section that fine-tuning LLMs towards greater faithfulness might be possible, referencing Kadavath et al. (2022) who showed it's possible to fine-tune a model to improve its self-modeling capabilities
- Why unresolved: The authors did not attempt to fine-tune any models in this study, and the model from Kadavath et al. (2022) is not publicly available for evaluation

### Open Question 2
- Question: How does the faithfulness of self-explanations vary with the complexity and domain specificity of tasks?
- Basis in paper: [inferred] The paper demonstrates that faithfulness is task-dependent, showing different results for sentiment classification, multi-choice Q&A, and entailment tasks, but does not explore a wide range of task complexities or domain-specific applications
- Why unresolved: The study only evaluates three types of tasks (sentiment classification, multi-choice Q&A, and entailment) and does not explore tasks with varying levels of complexity or domain specificity

### Open Question 3
- Question: To what extent does the size and architecture of large language models impact the faithfulness of their self-explanations?
- Basis in paper: [explicit] The paper shows that model size affects faithfulness, with larger models generally performing better, but also notes exceptions (e.g., Falcon 40B performing worse than Falcon 7B in some cases)
- Why unresolved: While the paper compares different model sizes and types, it does not systematically analyze how architectural differences or scaling laws specifically influence explanation faithfulness

## Limitations
- The methodology shows high model-specific variability, making results difficult to generalize across different model architectures
- The discrete classification approach may miss nuanced cases where predictions are possible but less confident
- The study evaluates only three specific models and three task types, limiting generalizability

## Confidence
- Confidence Level: Low for generalizability across different model architectures and training regimes
- Confidence Level: Medium in the self-consistency methodology itself
- Confidence Level: Medium in the discrete classification approach

## Next Checks
1. Run the same explanation generation and consistency checks across multiple independent session pairs to measure variance in faithfulness scores
2. Compare discrete "yes"/"no"/"unknown" faithfulness evaluation against continuous confidence score approaches for a subset of observations
3. Test the same explanation types on models with different architectural properties to understand which architectural features contribute to explanation faithfulness