---
ver: rpa2
title: Monotonic Paraphrasing Improves Generalization of Language Model Prompting
arxiv_id: '2403.16038'
source_url: https://arxiv.org/abs/2403.16038
tags:
- prompt
- linguistics
- perplexity
- decoding
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes monotonic paraphrasing (MonoPara), a decoding
  strategy that paraphrases prompts into lower-perplexity versions while maintaining
  semantic meaning. MonoPara uses an ensemble of a paraphrase language model and a
  target language model to generate paraphrases that are more familiar to the target
  model, improving zero-shot prompting performance.
---

# Monotonic Paraphrasing Improves Generalization of Language Model Prompting

## Quick Facts
- arXiv ID: 2403.16038
- Source URL: https://arxiv.org/abs/2403.16038
- Reference count: 30
- Primary result: Monotonic paraphrasing reduces prompt perplexity and improves zero-shot prompting accuracy across diverse NLP tasks

## Executive Summary
The paper proposes monotonic paraphrasing (MonoPara), a decoding strategy that paraphrases prompts into lower-perplexity versions while maintaining semantic meaning. MonoPara uses an ensemble of a paraphrase language model and a target language model to generate paraphrases that are more familiar to the target model, improving zero-shot prompting performance. The paper explores two decoding schemes: greedy and search-based. Experiments on diverse NLP tasks show that MonoPara consistently reduces prompt perplexity and improves accuracy compared to original prompts and direct paraphrasing. It also enhances model robustness against instruction perturbations. The results demonstrate that monotonic paraphrasing is an effective approach for improving the generalization of language model prompting without requiring any training.

## Method Summary
MonoPara paraphrases original prompts into lower-perplexity versions by combining a paraphrase model (Ppara) with the target model (Ptar) using ensemble decoding. The method employs either greedy decoding, which selects tokens based on a weighted combination of probabilities from both models, or search-based decoding, which maintains k candidate sequences and expands them using look-ahead scoring. The ensemble weights semantic fidelity against perplexity reduction, with the target model's perplexity serving as the optimization objective. This approach improves zero-shot prompting performance by generating prompts that are both semantically equivalent and linguistically familiar to the target model.

## Key Results
- MonoPara consistently reduces prompt perplexity compared to original prompts and direct paraphrasing across all tested tasks
- Accuracy improvements range from 1-4% on classification tasks when using MonoPara-generated paraphrases
- Search-based decoding provides systematic advantages over greedy decoding in finding lower-perplexity paraphrases
- MonoPara enhances model robustness against instruction perturbations, maintaining performance across various perturbation types

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Ensemble-based decoding combines paraphrase model fluency with target model perplexity awareness.
- **Mechanism:** The algorithm jointly samples tokens from two models—Ppara provides semantically faithful paraphrases, while Ptar enforces low-perplexity constraints. The weighted sum of log probabilities (α · log Ptar + (1-α) · log Ppara) balances semantic fidelity against linguistic familiarity.
- **Core assumption:** Perplexity measured by the target model correlates with task performance.
- **Evidence anchors:**
  - [abstract] "The ensemble decoding process can efficiently paraphrase the original prompt without altering its semantic meaning, while monotonically decrease the perplexity of each generation as calculated by the target LM."
  - [section] "Based on the intuition delivered in §2.1, MONO PARA decodes the paraphrase iteratively, and the next token is selected based on the combination of predicted probabilities from the two LMs"
  - [corpus] Weak - no direct neighbor evidence, though related work on ensemble decoding (Huang et al., 2023) supports the general approach.
- **Break condition:** If the two models' probability distributions are too dissimilar, the weighted combination may produce incoherent outputs or fail to improve perplexity.

### Mechanism 2
- **Claim:** Search-based decoding explores broader token sequences to find globally optimal low-perplexity paraphrases.
- **Mechanism:** Uses look-ahead decoding that maintains k candidate sequences at each step, expanding them with the top tokens from Ppara and scoring each with Ptar's perplexity function. This avoids greedy myopia by considering future token impacts.
- **Core assumption:** Low-perplexity local choices lead to globally low-perplexity sequences.
- **Evidence anchors:**
  - [abstract] "We explore in detail both greedy and search-based decoding as two alternative decoding schemes of MONO PARA."
  - [section] "To further enhance the efficiency of decoding prompts of lower perplexity, we define the search-based decoding strategy for MONO PARA... each step of our search-based decoding consists of (i) expanding a set of candidate next-tokens, (ii) scoring each candidate, and (iii) selecting the k best candidates"
  - [corpus] Weak - related work (Lu et al., 2022a) validates look-ahead decoding for constrained generation but doesn't specifically address perplexity optimization.
- **Break condition:** If k is too small, search space is insufficient; if k is too large, computational cost explodes without proportional gains.

### Mechanism 3
- **Claim:** Perplexity inversely correlates with instruction-following performance, especially under perturbations.
- **Mechanism:** By reducing prompt perplexity through monotonic paraphrasing, the target model encounters more familiar language patterns, improving both accuracy and robustness to instruction variations.
- **Core assumption:** Training data frequency of prompt patterns affects zero-shot generalization.
- **Evidence anchors:**
  - [abstract] "One consensus reached by recent studies is the inverse relationship between a prompt's perplexity and its task performance"
  - [section] "However, simply paraphrasing the original prompt with a paraphrase model Ppara does not necessarily result in a low-perplexity counterpart. This discrepancy arises from a mismatch between the perplexity of the output xpara from the paraphrase model and the perplexity observed when using xpara as the input prompt for the target model."
  - [corpus] Weak - no direct neighbor evidence, though related work (Gonen et al., 2023) supports perplexity as a heuristic for prompt selection.
- **Break condition:** If the target model has strong instruction-tuning that overrides perplexity signals, or if the task requires novel language patterns not in the training data.

## Foundational Learning

- **Concept:** Perplexity as a proxy for language model familiarity
  - Why needed here: The entire method relies on perplexity as the optimization target for prompt refinement.
  - Quick check question: What does perplexity measure, and why might lower perplexity indicate better task performance?

- **Concept:** Ensemble decoding strategies
  - Why needed here: The greedy variant requires understanding how to combine probability distributions from multiple models.
  - Quick check question: How does weighting different models' outputs affect generation quality and diversity?

- **Concept:** Look-ahead decoding and beam search
  - Why needed here: The search-based variant builds on these techniques but applies them to perplexity optimization rather than standard NLL objectives.
  - Quick check question: What's the difference between greedy decoding, beam search, and look-ahead decoding in terms of search space exploration?

## Architecture Onboarding

- **Component map:** Input prompt -> Ppara generates candidates -> Ptar scores by perplexity -> Ensemble controller selects token -> Low-perplexity paraphrase -> Ptar executes task

- **Critical path:**
  1. Input prompt xori and system prompt xsys
  2. Ppara generates next-token candidates
  3. Ptar scores candidates by perplexity
  4. Ensemble controller selects token based on weighted combination (greedy) or look-ahead scoring (search-based)
  5. Output low-perplexity paraphrase xpara
  6. Ptar executes task with xpara

- **Design tradeoffs:**
  - Greedy vs search-based: Greedy is faster but potentially suboptimal; search-based is more thorough but computationally expensive
  - α parameter: Higher α prioritizes perplexity over semantic fidelity; lower α may preserve meaning but sacrifice perplexity gains
  - Model choice: Using the target model as both Ppara and Ptar is convenient but may limit paraphrase diversity

- **Failure signatures:**
  - Perplexity increases despite iterations (ensemble weights misaligned)
  - Semantic drift from original prompt (α too high or Ppara too weak)
  - Excessive computational cost without performance gains (k too large in search-based variant)
  - Inconsistent results across runs (insufficient random seed control)

- **First 3 experiments:**
  1. Run greedy decoding with α=0.5 on a simple classification task, measure perplexity reduction and accuracy improvement
  2. Compare greedy vs search-based decoding on the same task with k=5, measure computational cost and performance tradeoff
  3. Test instruction perturbation robustness by applying MonoPara to perturbed instructions and measuring accuracy stability across perturbation types

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of monotonic paraphrasing vary with different paraphrasing models (e.g., specialized paraphrase models vs. the target model itself)?
- **Basis in paper:** [explicit] The paper mentions that using the target model as the paraphrase model is a natural choice, but also suggests that examining other paraphrasing models, especially those customized for paraphrasing, may lead to better performance.
- **Why unresolved:** The paper only uses the target model as the paraphrase model and does not explore the use of other paraphrasing models.
- **What evidence would resolve it:** Experiments comparing the performance of monotonic paraphrasing using different paraphrase models (e.g., specialized paraphrase models, the target model, and other LMs) on a variety of tasks and datasets.

### Open Question 2
- **Question:** How does the choice of coefficient α in the ensemble-based decoding scheme affect the quality and diversity of the generated paraphrases?
- **Basis in paper:** [explicit] The paper discusses the effect of coefficient α on the ensemble-based decoding variant (Mono-E) and provides a case study showing how different values of α impact the perplexity and BERTScore of the generated paraphrases.
- **Why unresolved:** While the paper provides insights into the effect of α, it does not explore the full range of possible α values or investigate how α affects the diversity of the generated paraphrases.
- **What evidence would resolve it:** A comprehensive analysis of the impact of α on the quality and diversity of paraphrases, including experiments with a wider range of α values and measures of paraphrase diversity.

### Open Question 3
- **Question:** How does the performance of monotonic paraphrasing scale with longer prompts and more complex tasks?
- **Basis in paper:** [inferred] The paper mentions that the current experiments are limited to relatively short prompts and tasks, and suggests that experiments on tasks with longer inputs and outputs may provide additional evidence of MONO PARA's effectiveness.
- **Why unresolved:** The paper does not provide evidence of how monotonic paraphrasing performs on longer prompts or more complex tasks.
- **What evidence would resolve it:** Experiments evaluating the performance of monotonic paraphrasing on tasks with longer prompts, more complex instructions, and more diverse output formats (e.g., generation tasks, summarization tasks).

## Limitations

- Evaluation scope limited to relatively simple classification and sentiment analysis tasks
- Method's effectiveness depends heavily on the relationship between paraphrase and target models
- Computational cost of search-based variant not fully quantified against performance gains

## Confidence

**High Confidence:**
- Ensemble decoding can reduce prompt perplexity while maintaining semantic meaning
- Lower-perplexity prompts generally correlate with improved task performance
- The inverse relationship between perplexity and performance is empirically validated

**Medium Confidence:**
- Search-based decoding provides systematic advantages over greedy decoding
- MonoPara improves model robustness to instruction perturbations
- The method generalizes across different model architectures

**Low Confidence:**
- MonoPara's effectiveness extends to highly complex reasoning tasks
- The method's benefits scale linearly with model size
- Perplexity optimization is the optimal objective for prompt refinement

## Next Checks

1. **Cross-Architecture Generalization Test:** Evaluate MonoPara on a broader range of model architectures including both encoder-decoder (like T5) and decoder-only (like GPT) models across different parameter scales (1B, 8B, 30B). Measure whether perplexity reduction consistently translates to performance gains across this architectural diversity.

2. **Complex Task Transfer Study:** Apply MonoPara to multi-step reasoning tasks (like GSM8K, BigBench) and open-ended generation tasks (like summarization, story completion). Compare performance against prompt engineering baselines and fine-tuned models to determine if perplexity optimization generalizes beyond classification tasks.

3. **Real-World Perturbation Analysis:** Instead of synthetic instruction perturbations, collect naturally occurring instruction variations from different datasets, user prompts, or adversarial examples. Measure MonoPara's effectiveness at maintaining performance across these authentic perturbations and analyze failure cases to understand limitations.