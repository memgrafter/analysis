---
ver: rpa2
title: 'ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools'
arxiv_id: '2406.12793'
source_url: https://arxiv.org/abs/2406.12793
tags:
- glm-4
- gpt-4
- language
- zhang
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChatGLM is a family of large language models developed by Zhipu
  AI and Tsinghua University, ranging from GLM-130B to GLM-4 All Tools. The models
  are pre-trained on approximately ten trillion tokens of Chinese and English data,
  with context lengths up to 1 million tokens.
---

# ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools

## Quick Facts
- arXiv ID: 2406.12793
- Source URL: https://arxiv.org/abs/2406.12793
- Reference count: 40
- Key outcome: GLM-4 models closely rival or outperform GPT-4 on benchmarks like MMLU, GSM8K, and HumanEval, while matching GPT-4 Turbo on instruction following and outperforming in Chinese alignment tasks.

## Executive Summary
ChatGLM is a family of large language models developed by Zhipu AI and Tsinghua University, ranging from GLM-130B to GLM-4 All Tools. These models are pre-trained on approximately ten trillion tokens of Chinese and English data, with context lengths up to 1 million tokens. The GLM-4 series achieves state-of-the-art performance on multiple benchmarks, closely rivaling or outperforming GPT-4 and GPT-4 Turbo across various tasks, including instruction following, Chinese alignment, and long-context understanding. The family includes specialized variants like GLM-4 All Tools, which extends capabilities with autonomous tool use including web browsing and Python execution.

## Method Summary
ChatGLM models are pre-trained on approximately 10 trillion tokens of multilingual data using a transformer-based GLM architecture with optimizations including Group Query Attention, RMSNorm, and SwiGLU activation. The models employ byte-level BPE tokenization with 150k vocabulary. Post-training involves a multi-stage process combining supervised fine-tuning with human-annotated prompt-response pairs and reinforcement learning from human feedback to achieve high-quality alignment. Context lengths are extended to 128K tokens through positional encoding extension and long-context alignment techniques. The GLM-4 All Tools variant further incorporates tool use capabilities through specialized training.

## Key Results
- GLM-4 closely rivals or outperforms GPT-4 on benchmarks like MMLU, GSM8K, and HumanEval
- GLM-4 matches GPT-4 Turbo on instruction following (IFEval) and outperforms in Chinese alignment tasks
- GLM-4 outperforms GPT-4 Turbo and Claude 3 on long-context tasks (LongBench-Chat)
- GLM-4 All Tools model extends capabilities with autonomous tool use including web browsing and Python execution
- Open models like GLM-4-9B and GLM-4V-9B have attracted over 10 million downloads on Hugging Face

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GLM-4's performance improvement stems from the combination of increased pre-training tokens and advanced architectural optimizations.
- Mechanism: The model was pre-trained on approximately ten trillion tokens of multilingual data, with architectural enhancements like Group Query Attention (GQA) and RMSNorm, which improve training efficiency and model capacity.
- Core assumption: Scaling model size and training data, combined with architectural improvements, directly leads to better downstream task performance.
- Evidence anchors:
  - [abstract] "The GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English..."
  - [section] "The recent GLM-4 model adopts the following architecture design choices... Group Query Attention (GQA)... RMSNorm and SwiGLU..."
  - [corpus] Weak corpus evidence for token count; no direct citations for architectural claims.
- Break condition: If the scaling laws do not hold (e.g., performance plateaus despite more tokens), or if architectural changes degrade performance.

### Mechanism 2
- Claim: Multi-stage post-training (SFT + RLHF) is critical for aligning GLM-4 with human preferences and improving instruction following.
- Mechanism: Supervised fine-tuning with high-quality prompt-response pairs followed by reinforcement learning from human feedback refines the model's responses to be more helpful, safe, and aligned with user intent.
- Core assumption: Human feedback and preference alignment are essential for practical usability beyond base model capabilities.
- Evidence anchors:
  - [abstract] "The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback."
  - [section] "For GLM-4, the alignment is mostly achieved with supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF)."
  - [corpus] No direct corpus evidence for SFT or RLHF effectiveness.
- Break condition: If the human feedback data is noisy or unrepresentative, leading to misalignment or degraded performance.

### Mechanism 3
- Claim: Long context handling (up to 128K tokens) is enabled by a combination of position encoding extension and long-context alignment techniques.
- Mechanism: Extensions to the RoPE positional encoding and continual training on long texts, followed by targeted alignment, allow the model to effectively process and reason over extended contexts.
- Core assumption: Position encoding can be extended beyond original training context lengths without loss of accuracy.
- Evidence anchors:
  - [section] "The context length of our models was extended from 2K... to 128K... These expansions were achieved not only through context extension—position encoding extension... but also long context alignment..."
  - [section] "To extend LLMs’ context window size, we proposed LongAlign—a comprehensive recipe for long context alignment."
  - [corpus] No direct corpus evidence for LongAlign's effectiveness.
- Break condition: If extended position encodings introduce errors or if long-context alignment does not generalize across tasks.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: GLM-4 is built on Transformer foundations; understanding attention, multi-head attention, and positional encoding is essential for grasping how GLM-4 processes information.
  - Quick check question: What is the role of positional encoding in Transformer models, and how does it differ from recurrence?
- Concept: Supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF)
  - Why needed here: These are the core alignment techniques used to make GLM-4 responsive to human instructions and preferences.
  - Quick check question: How does SFT differ from RLHF in terms of data and objectives?
- Concept: Benchmarking and evaluation metrics (e.g., MMLU, GSM8K, IFEval)
  - Why needed here: Performance is assessed across diverse academic and practical benchmarks; understanding these is necessary to interpret GLM-4's capabilities.
  - Quick check question: What does MMLU measure, and why is it widely used for LLM evaluation?

## Architecture Onboarding

- Component map: Input -> Tokenizer (BPE, 150k vocab) -> Embedding + Positional Encoding -> Transformer blocks (RMSNorm, SwiGLU, GQA, RoPE) -> Output projection
- Critical path: Data -> Pre-training (10T tokens) -> SFT (human prompts/responses) -> RLHF (preference learning) -> Tool use alignment -> Deployment
- Design tradeoffs: Larger context (128K) increases KV cache size and memory usage; GQA reduces parameters but may slightly impact accuracy; RMSNorm and SwiGLU improve speed but may alter training dynamics.
- Failure signatures: Degraded performance on long-context tasks suggests position encoding issues; poor instruction following indicates SFT/RLHF misalignment; slow inference points to architectural inefficiencies.
- First 3 experiments:
  1. Evaluate model on a small set of MMLU and GSM8K to verify base capability before alignment.
  2. Test instruction following on IFEval subset to assess SFT/RLHF effectiveness.
  3. Measure inference speed and memory usage with GQA and RMSNorm enabled/disabled.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the fundamental principles guiding data collection, cleaning, and selection for large language model pre-training?
- Basis in paper: [explicit] The paper states: "we have to date yet to identify a fundamental principle that could guide the processes of data collection, cleaning, and selection"
- Why unresolved: Despite recognizing the importance of data quality and diversity, the authors acknowledge that current practices lack a guiding theoretical framework for optimal data curation.
- What evidence would resolve it: A formal theoretical framework or mathematical model that optimizes data selection criteria based on downstream task performance, validated through empirical studies across multiple model architectures.

### Open Question 2
- Question: How does increasing model size and training tokens affect emergent abilities beyond the loss threshold identified in the paper?
- Basis in paper: [explicit] The paper discusses emergent abilities appearing when pre-training loss falls below certain thresholds, but questions remain about scaling beyond these points.
- Why unresolved: While the paper establishes a relationship between loss and emergent abilities, it doesn't fully explore how scaling affects abilities that emerge at different loss thresholds or whether new abilities emerge at higher scales.
- What evidence would resolve it: Systematic scaling studies examining performance across multiple orders of magnitude in model size and training compute, mapping ability emergence to specific loss thresholds.

### Open Question 3
- Question: What training strategies and data curation methods could further improve GLM-4's mathematics performance to match or exceed GPT-4 Turbo?
- Basis in paper: [explicit] The paper notes that GLM-4 lags behind GPT-4 Turbo in mathematics and mentions ongoing work with techniques like self-critique to enhance math reasoning.
- Why unresolved: Despite implementing specific techniques for math improvement, the performance gap persists, suggesting current methods may be insufficient or that different approaches are needed.
- What evidence would resolve it: Comparative studies of alternative training methodologies (curriculum learning, specialized math-focused pre-training, different alignment techniques) showing measurable improvements in mathematical reasoning benchmarks.

## Limitations

- Lack of direct empirical evidence for architectural optimizations (GQA, RMSNorm, SwiGLU) and alignment techniques (SFT, RLHF)
- Absence of ablation studies makes it difficult to attribute performance gains to specific components
- Training corpus details are general without specific composition and quality metrics
- RLHF implementation details (reward model architecture, PPO hyperparameters, preference dataset composition) are not specified

## Confidence

**High Confidence**: GLM-4's strong performance on established benchmarks (MMLU, GSM8K, HumanEval) is verifiable through public leaderboards and reproducible results. The architectural components (Group Query Attention, RMSNorm, SwiGLU) are well-documented in literature and their implementation is straightforward.

**Medium Confidence**: Claims about GLM-4 matching or exceeding GPT-4 Turbo and Claude 3 on instruction following and long-context tasks are supported by benchmark results but lack ablation studies showing which specific architectural or alignment choices drive these improvements.

**Low Confidence**: The effectiveness of the multi-stage post-training process (SFT + RLHF) and long-context alignment techniques is asserted but not empirically validated through comparative studies or detailed implementation specifications.

## Next Checks

1. **Ablation study on architectural components**: Test GLM-4 variants with GQA, RMSNorm, and SwiGLU individually disabled to quantify their contribution to overall performance on MMLU and GSM8K benchmarks.

2. **Alignment technique isolation**: Compare GLM-4 performance on IFEval and SafetyBench with and without SFT/RLHF stages to measure alignment improvement, and test long-context performance with position encoding extensions disabled.

3. **Reproducibility audit**: Replicate model training on a 1% sample of the claimed 10T token corpus with documented architectural choices to verify scaling claims and identify potential overfitting or data quality issues.