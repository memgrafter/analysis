---
ver: rpa2
title: Autonomous Vehicle Controllers From End-to-End Differentiable Simulation
arxiv_id: '2409.07965'
source_url: https://arxiv.org/abs/2409.07965
tags:
- policy
- trajectory
- learning
- dynamics
- gradients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of learning robust and accurate
  controllers for autonomous vehicles by leveraging differentiable simulation. The
  authors propose using analytic policy gradients (APG) within the Waymax simulator,
  enabling end-to-end training of policies from expert trajectories without requiring
  expert actions.
---

# Autonomous Vehicle Controllers From End-to-End Differentiable Simulation

## Quick Facts
- arXiv ID: 2409.07965
- Source URL: https://arxiv.org/abs/2409.07965
- Reference count: 40
- One-line primary result: Analytic policy gradients in differentiable simulation outperform behavioral cloning for autonomous vehicle control, yielding more accurate and robust trajectories.

## Executive Summary
This work introduces a novel framework for training autonomous vehicle controllers using end-to-end differentiable simulation. By leveraging analytic policy gradients (APG) within the Waymax simulator, the authors enable direct optimization of vehicle policies from expert trajectories without requiring expert actions. This approach integrates simulator dynamics into the training loop, allowing gradients to flow back through the environment and facilitating more grounded and efficient learning compared to traditional methods like behavioral cloning or reinforcement learning.

The method employs a recurrent architecture to propagate temporal information across long simulated trajectories and incorporates gradient detachment for efficient, off-policy training. Experiments on the Waymo Open Motion Dataset demonstrate that APG achieves superior performance in terms of accuracy, robustness to noise, and collision/offroad rates, while also producing more realistic, human-like driving behavior.

## Method Summary
The authors propose using analytic policy gradients (APG) within a differentiable simulator to train autonomous vehicle policies from expert trajectories. The simulator is treated as a differentiable function, allowing gradients of future states with respect to actions to be computed and used to update the policy. Gradient detachment at each timestep enables off-policy training and prevents gradient explosion in long sequences. Incremental training with periodic state resets improves sample efficiency and stability when using stochastic policies. The approach uses a recurrent architecture to propagate temporal information and is evaluated on the Waymo Open Motion Dataset.

## Key Results
- APG outperforms behavioral cloning in terms of accuracy, robustness to noise, and collision/offroad rates.
- The method produces more realistic, human-like driving behavior compared to baselines.
- APG demonstrates improved performance with fewer modes and is robust to shorter training sequences.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Analytic policy gradients (APG) can directly optimize autonomous vehicle policies by backpropagating through a differentiable simulator's dynamics.
- Mechanism: The simulator is treated as a differentiable function where gradients of future states with respect to actions are available. These gradients are composed with policy gradients during backpropagation to update the policy parameters. This avoids the need for reward estimation and enables end-to-end training on expert trajectories.
- Core assumption: The simulator's dynamics are smooth, differentiable, and accurately reflect real-world vehicle behavior, and the policy is differentiable.
- Evidence anchors:
  - [abstract] "Our proposed framework brings the differentiable simulator into an end-to-end training loop, where gradients of the environment dynamics serve as a useful prior to help the agent learn a more grounded policy."
  - [section] "In a differentiable environment one can optimize the policy directly using gradient descent, just via supervision from expert agent trajectories."
- Break condition: If the simulator dynamics are non-differentiable (e.g., discrete events like collisions) or if the policy introduces non-differentiable operations, the gradient flow will break.

### Mechanism 2
- Claim: Gradient detachment at each timestep enables off-policy training and prevents gradient explosion in long sequences.
- Mechanism: After each simulator step, gradients are detached before proceeding to the next timestep. This breaks the chain of gradient accumulation across timesteps, effectively splitting the trajectory into independent (state, action, next state) transitions. This allows training with expert trajectories that differ from the current policy's rollout.
- Core assumption: The value of each timestep's gradient signal is sufficient to update the policy without needing the full unrolled gradient path.
- Evidence anchors:
  - [section] "When we obtain s_t, we calculate the loss and backpropagate through the environment dynamics obtaining ∂s_t/∂a_t−1 without continuing on to previous steps."
  - [section] "This allows for off-policy training – a key aspect of our setup."
- Break condition: If the optimal policy requires long-term dependencies that are lost by detaching gradients, performance may degrade.

### Mechanism 3
- Claim: Incremental training with periodic state resets improves sample efficiency and stability when using stochastic policies.
- Mechanism: During training, the simulated trajectory is periodically reset to the ground truth state. This prevents the policy from drifting too far from expert behavior and reduces the compounding noise from stochastic action sampling. The reset frequency is gradually decreased to increase difficulty.
- Core assumption: Keeping the simulated trajectory close to the ground truth trajectory provides more informative gradient signals than allowing large deviations.
- Evidence anchors:
  - [section] "We implement incremental training where we periodically 'reset' the simulated state back to the corresponding log state... This ensures that the data-collection stays around the GT trajectory, instead of far from it, increasing sample efficiency."
  - [section] "To improve sample-efficiency, as a form of curriculum, we snap back the simulated trajectory whenever it goes beyond a threshold ξ with respect to {ŝ_t}."
- Break condition: If the reset threshold is too strict or too frequent, the policy may not learn to recover from errors; if too lenient, training becomes unstable.

## Foundational Learning

- Concept: Differentiable simulation and analytic policy gradients
  - Why needed here: The core innovation relies on differentiating through the simulator to obtain action gradients for direct policy optimization.
  - Quick check question: Can you compute ∂s_t/∂a_t−1 using JAX's grad function if the simulator is implemented as a differentiable JAX function?

- Concept: Behavior cloning vs. imitation learning with simulators
  - Why needed here: The paper contrasts APG with behavior cloning, which requires expert actions and suffers from compounding errors.
  - Quick check question: Why does behavior cloning fail to reproduce trajectories even when trained on the exact expert actions?

- Concept: Gradient vanishing/explosion in unrolled sequences
  - Why needed here: Long trajectories in differentiable simulators can lead to unstable gradients; understanding this helps explain the need for gradient detachment and incremental training.
  - Quick check question: What happens to the gradient norm as you backpropagate through 100 timesteps in a typical RNN or unrolled simulator?

## Architecture Onboarding

- Component map:
  Scene encoder → RNN (temporal) → Policy head → Simulator → Loss

- Critical path:
  Scene encoder → RNN hidden state update → Policy head → Action sampling → Simulator step → State update → Loss computation

- Design tradeoffs:
  - Gradient detachment improves training stability but loses long-term credit assignment.
  - Stochastic policy enables multimodal outputs but introduces gradient noise; mitigated by incremental resets.
  - Multi-agent attention allows flexible agent counts but increases computational cost.

- Failure signatures:
  - Large gradient norms → possible explosion; check for detachment bugs.
  - Policy collapses to constant output → check for gradient flow or optimizer issues.
  - High collision/offroad rates → may indicate simulator mismatch or insufficient training diversity.

- First 3 experiments:
  1. Overfit a single trajectory using APG vs. behavior cloning to verify APG can reproduce expert behavior.
  2. Train on full dataset with gradient detachment; compare minADE, collision, and offroad rates against behavior cloning.
  3. Add noise to simulator dynamics at test time; measure robustness of APG vs. BC.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do analytic policy gradients (APG) perform compared to value-based and policy gradient RL methods in large-scale autonomous driving scenarios when both are given access to the same differentiable simulator?
- Basis in paper: [explicit] The paper contrasts APG with traditional RL methods like value-learning and policy gradients, noting that those treat the environment as a black box, while APG leverages simulator gradients directly.
- Why unresolved: The paper does not empirically compare APG to these RL baselines within the Waymax framework, only describing theoretical differences.
- What evidence would resolve it: Direct performance comparison (ADE, collision/offroad rates) between APG and value/policy gradient RL agents trained in the same differentiable simulator environment.

### Open Question 2
- Question: What is the effect of detaching gradients at each timestep versus performing full backpropagation through time in APG when training autonomous vehicle controllers?
- Basis in paper: [explicit] The authors discuss gradient detachment as a design choice to enable off-policy training and improve efficiency, but note that it prevents gradient accumulation across timesteps unlike RNNs.
- Why unresolved: The paper does not provide empirical results comparing APG with and without gradient detachment in terms of sample efficiency or final performance.
- What evidence would resolve it: Controlled experiments comparing APG performance and training speed with gradient detachment enabled/disabled, measured across multiple training runs.

### Open Question 3
- Question: How does APG's performance scale when increasing the number of agents in multi-agent autonomous driving scenarios beyond what was tested?
- Basis in paper: [inferred] The authors test APG in multi-agent settings with 32 training agents and 128 evaluation agents, showing improved performance over behavioral cloning, but do not explore scenarios with significantly more agents.
- Why unresolved: The scalability limits of APG for large numbers of interacting agents are not investigated, particularly regarding computational cost and prediction accuracy.
- What evidence would resolve it: Experiments measuring APG performance (ADE, collision/offroad rates) and computational requirements as the number of agents increases to 256, 512, or more in the Waymax simulator.

## Limitations
- The approach relies on accurate differentiable simulation, which may not capture all real-world complexities.
- Gradient detachment may limit the policy's ability to learn long-term dependencies or recover from errors far from expert trajectories.
- The method assumes expert trajectories are available, but does not require expert actions, which may limit its applicability in some scenarios.

## Confidence

- **High Confidence:** The core mechanism of using differentiable simulation for policy optimization is well-supported by the experimental results, particularly in terms of improved accuracy and robustness over behavioral cloning.
- **Medium Confidence:** The benefits of gradient detachment and incremental training are demonstrated, but the long-term effects on policy generalization and error recovery are less clear.
- **Medium Confidence:** The claims of human-like driving behavior and robustness to noise are supported by the experiments, but further validation in more diverse and challenging scenarios would strengthen these claims.

## Next Checks

1. **Simulator Fidelity:** Validate the differentiable simulator's accuracy by comparing simulated trajectories against real-world data under various conditions (e.g., different road types, weather, traffic densities).
2. **Long-Horizon Performance:** Evaluate the policy's ability to handle long, complex scenarios without frequent resets, to assess the impact of gradient detachment on long-term planning.
3. **Real-World Transfer:** Test the policy in a closed-course or simulated real-world environment to measure its performance when transitioning from the simulated training domain to real-world conditions.