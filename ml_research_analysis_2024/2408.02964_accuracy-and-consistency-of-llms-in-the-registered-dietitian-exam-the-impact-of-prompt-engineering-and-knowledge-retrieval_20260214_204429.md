---
ver: rpa2
title: 'Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The Impact
  of Prompt Engineering and Knowledge Retrieval'
arxiv_id: '2408.02964'
source_url: https://arxiv.org/abs/2408.02964
tags:
- nutrition
- care
- food
- groups
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates three leading large language models (LLMs)\u2014\
  GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro\u2014on the Registered Dietitian (RD)\
  \ exam, using 1050 multiple-choice questions. Four prompting techniques (Zero-Shot,\
  \ Chain of Thought, Chain of Thought with Self Consistency, and Retrieval Augmented\
  \ Prompting) are tested for their impact on accuracy and consistency."
---

# Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The Impact of Prompt Engineering and Knowledge Retrieval

## Quick Facts
- arXiv ID: 2408.02964
- Source URL: https://arxiv.org/abs/2408.02964
- Reference count: 40
- Primary result: GPT-4o with Chain of Thought + Self Consistency achieved ~94% accuracy on RD exam questions

## Executive Summary
This study evaluates three leading large language models (GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro) on the Registered Dietitian exam using 1050 multiple-choice questions. Four prompting techniques are tested: Zero Shot, Chain of Thought, Chain of Thought with Self Consistency, and Retrieval Augmented Prompting. Results show that GPT-4o with Chain of Thought + Self Consistency achieved the highest accuracy (~94%), while Gemini 1.5 Pro with Zero Shot showed the best consistency. Chain of Thought improved accuracy for GPT-4o and Claude 3.5 Sonnet across most proficiency levels, while Retrieval Augmented Prompting was particularly effective for GPT-4o on Expert-level questions.

## Method Summary
The study evaluated three LLMs (GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro) on 1050 RD exam questions categorized by proficiency level (Easy, Moderate, Difficult, Expert) and nutrition domain. Four prompting techniques were applied to each model: Zero Shot, Chain of Thought, Chain of Thought with Self Consistency, and Retrieval Augmented Prompting. Each question was asked five times per model per prompting technique using temperature=0 to assess consistency. Accuracy was measured as percentage of correct answers, while consistency was evaluated using Cohen's Kappa (inter-rater) and Fleiss Kappa (intra-rater).

## Key Results
- GPT-4o with Chain of Thought + Self Consistency achieved the highest accuracy (~94%)
- Gemini 1.5 Pro with Zero Shot showed the best consistency across repeated runs
- Chain of Thought improved accuracy for GPT-4o and Claude 3.5 Sonnet on Easy, Moderate, and Difficult level questions
- Retrieval Augmented Prompting was most effective for GPT-4o on Expert-level questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4o with Chain of Thought + Self Consistency consistently outperforms other models and prompting combinations in accuracy.
- Mechanism: The CoT-SC approach guides the model through multiple independent reasoning paths, then uses majority voting to select the most likely answer, reducing variance from random reasoning paths.
- Core assumption: The model's internal knowledge is sound but its reasoning path selection is noisy; ensemble averaging reduces this noise.
- Evidence anchors:
  - [abstract] "GPT-4o with CoT-SC prompting outperformed the other approaches"
  - [section] "CoT-SC achieved notably higher consistency (intra-rater agreement) compared to CoT"
- Break condition: If reasoning paths are consistently biased or the majority voting rule does not reflect true correctness, performance degrades.

### Mechanism 2
- Claim: Chain of Thought (CoT) improves accuracy for GPT-4o and Claude 3.5 Sonnet on Easy, Moderate, and Difficult level questions, but not Expert level.
- Mechanism: CoT forces explicit step-by-step reasoning, helping models identify and apply correct intermediate knowledge steps for simpler reasoning tasks.
- Core assumption: Reasoning steps help when the path from question to answer is relatively straightforward; less so when Expert-level reasoning requires integrating multiple complex concepts.
- Evidence anchors:
  - [abstract] "For GPT-4o and Claude 3.5, CoT improved the accuracy"
  - [section] "CoT notably improved the questions about D3) food service systems, which involved calculations for food cost and portion estimation/forecasting"
- Break condition: If the reasoning steps themselves contain errors or if the task requires deep integration beyond stepwise logic.

### Mechanism 3
- Claim: Retrieval Augmented Prompting (RAP) improves GPT-4o's performance on Difficult and Expert questions by supplementing internal knowledge with external nutrition references.
- Mechanism: RAP retrieves relevant chunks from a curated knowledge base and includes them in the prompt, allowing the model to leverage up-to-date or specialized information not fully captured in its training.
- Core assumption: The knowledge base contains relevant, accurate information for the questions; retrieval relevance is high enough to improve reasoning.
- Evidence anchors:
  - [abstract] "RAP was particularly effective for GPT-4o to answer Expert level questions"
  - [section] "GPT-4o effectively leveraged the retrieved information to reduce error rates, particularly for Difficult and Expert questions"
- Break condition: If retrieval yields irrelevant or conflicting information, or if the model overweights external text even when it is incorrect.

## Foundational Learning

- Concept: Prompt engineering techniques (Zero Shot, Chain of Thought, Chain of Thought with Self Consistency, Retrieval Augmented Prompting)
  - Why needed here: Different techniques have distinct effects on accuracy and consistency; selecting the right one is critical for reliable nutrition chatbot deployment.
  - Quick check question: What is the key difference between CoT and CoT-SC in how they generate answers?

- Concept: Inter-rater and intra-rater reliability (Cohen's Kappa, Fleiss Kappa)
  - Why needed here: Consistency of model responses is as important as accuracy in safety-critical nutrition applications; statistical measures quantify this.
  - Quick check question: How does Cohen's Kappa differ from Fleiss Kappa in this study's context?

- Concept: Domain-specific error analysis (proficiency levels, nutrition domains)
  - Why needed here: Errors are not uniform; understanding where and why they occur guides model and prompt selection.
  - Quick check question: Which nutrition domain saw the largest improvement from CoT-SC in GPT-4o?

## Architecture Onboarding

- Component map: RD exam question source -> Prompt engineering module (4 techniques) -> LLM (3 models) -> Answer selection -> Ground truth comparison -> Accuracy/consistency metrics
- Critical path: Question -> Prompt generation -> LLM call (temperature=0) -> Answer extraction -> Accuracy check -> Repeat 5x for consistency
- Design tradeoffs: Higher accuracy via CoT-SC at cost of multiple LLM calls; RAP improves Expert accuracy but risks over-reliance on external text; simpler prompts (ZS) yield high consistency but lower accuracy
- Failure signatures: Model fails to select a choice ("None of the above"); RAP retrieval irrelevant; CoT reasoning steps contain miscalculations; high variance across repeated runs
- First 3 experiments:
  1. Run a small subset of RD questions with all four prompting techniques on GPT-4o; compare accuracy and consistency.
  2. Test RAP on a Difficult/Expert question set to confirm retrieval relevance and answer quality.
  3. Perform intra-rater analysis (5 repeats) on one prompting technique to establish baseline consistency before scaling up.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do open-source LLMs compare to proprietary models in nutrition and diet applications when evaluated using the Registered Dietitian exam?
- Basis in paper: [explicit] The paper states this study is limited to leading proprietary LLM models and suggests future research should evaluate open-source LLMs like Llama 3, Falcon 2, and Yi-34B.
- Why unresolved: The paper only tested GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, leaving a gap in understanding how open-source alternatives perform.
- What evidence would resolve it: Comparative evaluation of open-source LLMs on the same 1050 RD exam questions using the same prompting techniques and accuracy/consistency metrics.

### Open Question 2
- Question: Which prompting techniques are most effective for improving LLM performance on expert-level nutrition questions?
- Basis in paper: [explicit] The paper notes that Chain of Thought improved accuracy for most proficiency levels but was less effective for expert-level questions, where only a few errors were corrected.
- Why unresolved: While the paper shows CoT reduced errors for easy-moderate-difficult questions, it doesn't identify optimal prompting strategies specifically for expert-level questions.
- What evidence would resolve it: Systematic testing of advanced prompting techniques (like retrieval augmented generation, multi-step reasoning, or specialized nutrition prompts) specifically on expert-level RD exam questions.

### Open Question 3
- Question: How do LLM responses vary in terms of safety, bias, and emotional support when providing nutrition advice to patients with specific medical conditions?
- Basis in paper: [inferred] The paper mentions the need to assess LLMs from perspectives like safety, bias, and emotional support, but only evaluated accuracy and consistency using standardized exam questions.
- Why unresolved: The study used multiple-choice exam questions rather than real patient scenarios, missing critical evaluation of how LLMs handle sensitive patient interactions.
- What evidence would resolve it: Evaluation framework using patient-centric questions, simulated patient conversations, and metrics for safety, bias detection, and emotional intelligence in nutrition counseling contexts.

## Limitations
- Study relies on proprietary RD exam question bank, making full replication challenging without access to exact questions and ground truth answers
- Evaluation focuses on multiple-choice questions only, which may not reflect real-world complexity of nutrition counseling or assessment tasks
- No comparison is made to human expert performance on the same question set, limiting the ability to benchmark AI against established standards
- Study does not explore model calibration or confidence scoring, which are critical for clinical applications where uncertain responses could impact patient safety

## Confidence
- **High confidence**: GPT-4o with Chain of Thought + Self Consistency achieving highest accuracy (~94%) - supported by multiple accuracy metrics and consistent across proficiency levels
- **Medium confidence**: RAP's effectiveness for Expert-level questions with GPT-4o - while improvements are documented, the mechanism (retrieval relevance vs. model capability) is not fully disentangled
- **Medium confidence**: Gemini 1.5 Pro showing highest consistency with Zero-Shot - consistency is measured but the practical significance for clinical applications requires further validation

## Next Checks
1. Cross-validation with alternative question sets: Test the prompting techniques on a separate, publicly available nutrition or medical exam to verify that observed patterns (CoT-SC accuracy gains, RAP effectiveness) generalize beyond the specific RD exam used
2. Human expert comparison: Have registered dietitians answer the same question set and compare both accuracy and consistency metrics to establish baseline human performance for context
3. Clinical scenario extension: Adapt the evaluation framework to open-ended clinical case scenarios rather than multiple-choice questions to assess real-world applicability for nutrition chatbot deployment