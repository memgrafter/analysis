---
ver: rpa2
title: 'LeanAgent: Lifelong Learning for Formal Theorem Proving'
arxiv_id: '2410.06209'
source_url: https://arxiv.org/abs/2410.06209
tags:
- leanagent
- learning
- theorem
- theorems
- lifelong
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LeanAgent is a lifelong learning framework for formal theorem proving
  that continuously generalizes to and improves on ever-expanding mathematical knowledge
  without forgetting previously learned information. It introduces a curriculum learning
  strategy that optimizes the learning trajectory in terms of mathematical difficulty,
  a dynamic database for efficient management of evolving mathematical knowledge,
  and progressive training to balance stability and plasticity.
---

# LeanAgent: Lifelong Learning for Formal Theorem Proving

## Quick Facts
- arXiv ID: 2410.06209
- Source URL: https://arxiv.org/abs/2410.06209
- Reference count: 40
- Proves 155 theorems across 23 diverse Lean repositories where formal proofs were previously missing

## Executive Summary
LeanAgent introduces a lifelong learning framework for formal theorem proving in Lean that continuously generalizes to and improves on ever-expanding mathematical knowledge without forgetting previously learned information. It achieves this through curriculum learning that optimizes the learning trajectory by mathematical difficulty, a dynamic database for efficient management of evolving mathematical knowledge, and progressive training that balances stability and plasticity. The framework successfully generates formal proofs for 155 theorems across 23 diverse Lean repositories, including challenging theorems in abstract algebra and algebraic topology.

## Method Summary
LeanAgent implements a lifelong learning framework that continuously adapts to new mathematical knowledge while preserving previously learned information. The method involves scanning 23 Lean repositories to extract theorems and proofs, calculating theorem complexity using proof step count (eS), and sorting repositories by easy theorem count. A dynamic database tracks theorem complexity, proof status, and repository metadata. The framework uses curriculum learning to train on increasingly complex mathematical repositories, with progressive training that limits each new dataset to one additional epoch to prevent overfitting while allowing adaptation to new information. LeanAgent proves "sorry" theorems using retrieval-augmented LLM provers and best-first tree search.

## Key Results
- Proves 155 theorems across 23 diverse Lean repositories where formal proofs were previously missing
- Achieves exceptional scores in stability and backward transfer metrics
- Demonstrates continuous generalizability and improvement by learning basic concepts first and advancing to complex theorems in domains like abstract algebra and algebraic topology

## Why This Works (Mechanism)

### Mechanism 1
Curriculum learning enables progressive skill building from basic to advanced mathematical concepts, leading to better generalization. LeanAgent uses proof step count (eS) as a complexity metric, allowing it to structure learning by mathematical difficulty. This structured progression helps the model build foundational skills before tackling complex theorems.

### Mechanism 2
Progressive training with one epoch per dataset balances stability and plasticity, preventing catastrophic forgetting while allowing adaptation to new mathematical domains. By training on each new repository dataset for only one additional epoch and repeating this process for each dataset, LeanAgent continuously adapts to new knowledge while preserving previously learned information.

### Mechanism 3
The dynamic database infrastructure enables efficient management of evolving mathematical knowledge across multiple repositories and domains. The custom database tracks theorem complexity, proof status, and repository metadata, allowing LeanAgent to efficiently reuse repositories in future curricula and maintain knowledge of what has been proven.

## Foundational Learning

- **Concept: Curriculum learning and its application to theorem proving**
  - Why needed here: Understanding how structured learning progression improves model performance in mathematical domains
  - Quick check question: How does curriculum learning differ from standard training approaches in formal theorem proving?

- **Concept: Lifelong learning and catastrophic forgetting**
  - Why needed here: Essential for understanding why progressive training and stability-plasticity balance are critical for continuous improvement
  - Quick check question: What are the key challenges in balancing plasticity and stability in lifelong learning systems?

- **Concept: Proof step complexity metrics and their relationship to mathematical difficulty**
  - Why needed here: Critical for understanding LeanAgent's approach to measuring and organizing mathematical complexity
  - Quick check question: Why might proof step count be a better complexity metric than theorem statement complexity in formal theorem proving?

## Architecture Onboarding

- **Component map:** Repository scanning → Dynamic database → Curriculum learning → Progressive training → Retrieval-augmented LLM prover → sorry theorem proving
- **Critical path:** Repository scanning → Dynamic database → Curriculum learning → Progressive training → sorry theorem proving
- **Design tradeoffs:**
  - Single epoch vs. multiple epochs in progressive training (stability vs. plasticity)
  - Proof step count vs. other complexity metrics (simplicity vs. accuracy)
  - Centralized database vs. distributed knowledge management (efficiency vs. scalability)
- **Failure signatures:**
  - Poor stability metrics indicate excessive plasticity and catastrophic forgetting
  - Low backward transfer suggests failure to leverage previous knowledge
  - Inability to prove basic theorems indicates insufficient foundational learning
- **First 3 experiments:**
  1. Implement curriculum learning with proof step complexity on a small set of repositories and measure stability metrics
  2. Test progressive training with varying epoch counts to find optimal stability-plasticity balance
  3. Evaluate sorry theorem proving performance across different mathematical domains to validate generalization capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the exponential scaling of proof step complexity affect the learning trajectory and performance compared to linear scaling?
- Basis in paper: The paper mentions using exponential scaling to address the combinatorial explosion of possible proof paths as proof length increases.
- Why unresolved: The paper does not provide experimental results comparing exponential scaling with linear scaling or other complexity measures.
- What evidence would resolve it: Comparative experiments showing LeanAgent's performance with different complexity scaling methods (linear, exponential, or other) would clarify the impact on learning efficiency and final performance.

### Open Question 2
- Question: Can LeanAgent's curriculum learning strategy be extended to incorporate topic-based ordering alongside complexity-based ordering?
- Basis in paper: The paper discusses why topic-based approaches were unsuitable for LeanAgent, but does not explore hybrid approaches.
- Why unresolved: The paper only considers complexity-based ordering and does not test topic-based or hybrid curriculum strategies.
- What evidence would resolve it: Experiments comparing LeanAgent's performance using topic-based, complexity-based, and hybrid curriculum strategies would show if topic-based ordering could complement complexity-based ordering.

### Open Question 3
- Question: How does LeanAgent's performance on sorry theorems change when using different retrieval-augmented language models as the base LLM?
- Basis in paper: The paper states that LeanAgent works with any LLM and uses ReProver's retriever as an example implementation.
- Why unresolved: The paper only demonstrates LeanAgent with one specific LLM (ReProver's retriever) and does not test other models.
- What evidence would resolve it: Comparative experiments using different retrieval-augmented LLMs (e.g., different model architectures or pre-trained models) would show how the choice of base LLM affects LeanAgent's theorem-proving performance.

## Limitations

- Proof step count metric may not accurately capture true mathematical difficulty for advanced concepts
- One-epoch progressive training may limit deep learning of complex mathematical structures
- Dynamic database scalability for very large mathematical corpora remains untested
- Limited evaluation on repositories from non-Western mathematical traditions and specialized domains

## Confidence

- High confidence: LeanAgent's ability to prove 155 theorems across 23 diverse repositories demonstrates successful implementation of the lifelong learning framework
- Medium confidence: The curriculum learning approach with proof step complexity shows promise but requires further validation across broader mathematical domains
- Medium confidence: Progressive training with one epoch per dataset effectively balances stability and plasticity, though optimal epoch count may vary by domain

## Next Checks

1. Test the curriculum learning approach on a larger, more diverse set of mathematical repositories including non-Western mathematical traditions to validate generalizability
2. Conduct ablation studies varying the number of training epochs per dataset to identify optimal stability-plasticity balance across different mathematical complexity levels
3. Implement cross-domain transfer experiments to evaluate whether learning in one mathematical domain (e.g., algebra) improves performance in unrelated domains (e.g., topology) beyond what's explained by shared foundational concepts