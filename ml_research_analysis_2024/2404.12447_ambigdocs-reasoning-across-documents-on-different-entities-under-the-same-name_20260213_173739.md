---
ver: rpa2
title: 'AmbigDocs: Reasoning across Documents on Different Entities under the Same
  Name'
arxiv_id: '2404.12447'
source_url: https://arxiv.org/abs/2404.12447
tags:
- answer
- answers
- question
- entity
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces AmbigDocs, a new benchmark for testing language
  models' ability to reason across documents containing different entities that share
  the same name. The authors leverage Wikipedia's disambiguation pages to create a
  dataset of 36K examples, where each instance consists of a question containing an
  ambiguous entity name and a set of documents providing answers for each disambiguated
  entity.
---

# AmbigDocs: Reasoning across Documents on Different Entities under the Same Name

## Quick Facts
- arXiv ID: 2404.12447
- Source URL: https://arxiv.org/abs/2404.12447
- Authors: Yoonsang Lee; Xi Ye; Eunsol Choi
- Reference count: 40
- Key outcome: Introduces AmbigDocs benchmark testing language models' ability to reason across documents containing different entities sharing the same name

## Executive Summary
This paper introduces AmbigDocs, a new benchmark designed to test language models' ability to reason across documents containing different entities that share the same name. The authors leverage Wikipedia's disambiguation pages to create a dataset of 36K examples, where each instance consists of a question containing an ambiguous entity name and a set of documents providing answers for each disambiguated entity. The key challenge is to generate a complete answer that correctly pairs each entity with its corresponding answer.

The authors establish an ontology categorizing five types of answers (complete, partial, ambiguous, merged, and no answer) and develop automatic evaluation metrics to identify these categories. Experimental results on state-of-the-art models reveal that they often fail to generate complete answers, instead producing ambiguous or merged responses that incorrectly combine information from different entities. However, the authors find that providing in-context examples can significantly improve model performance, with GPT-4 and Mistral-7B achieving 58% and 43% complete answer rates respectively, compared to 20% and 10% in the zero-shot setting.

## Method Summary
The authors create AmbigDocs by leveraging Wikipedia disambiguation pages to identify surface names and their corresponding disambiguated entities. They generate synthetic data using high-quality LLMs to create questions and answer sets for each entity, then evaluate model performance using custom metrics that categorize answers into five types based on whether disambiguated entity names are correctly paired with their corresponding answers. The evaluation system uses token-level recall to determine if reference answers are mentioned alongside their disambiguated entity names.

## Key Results
- State-of-the-art models often fail to generate complete answers, producing ambiguous or merged responses instead
- In-context few-shot learning significantly improves performance, with GPT-4 achieving 58% complete answer rate compared to 20% in zero-shot
- Larger models like Llama2-13b and GPT-3.5 tend to generate more ambiguous and merged answers
- The automatic evaluation metrics show strong agreement with human annotators (Cohen's Kappa 0.83)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset construction leverages Wikipedia's disambiguation pages to ensure each surface name has multiple disambiguated entities with distinct answers, creating a controlled environment for testing entity disambiguation.
- Mechanism: By mining Wikipedia disambiguation pages and generating questions with corresponding answer sets for each disambiguated entity, the dataset provides ground truth pairs of (entity, answer) that can be used to evaluate model outputs.
- Core assumption: Wikipedia's disambiguation pages are comprehensive and reliable sources for mapping surface names to distinct entities with different attributes.
- Evidence anchors:
  - [abstract]: "By leveraging Wikipedia's disambiguation pages, we identify a set of documents, belonging to different entities who share an ambiguous name."
  - [section 3.1]: "For each surface name sn, we have a set of documents D (10.6 documents on average) gathered from Wikipedia disambiguation page, each belonging to one of the disambiguated entities."
  - [corpus]: Weak evidence - the corpus only contains 8 papers, suggesting limited prior work in this specific area.

### Mechanism 2
- Claim: The automatic evaluation metrics can reliably categorize model outputs into five distinct categories (complete, partial, ambiguous, merged, no answer) by checking whether answers are paired with disambiguated entity names.
- Mechanism: The evaluation system uses token-level recall to check if each reference answer is mentioned with its disambiguated entity name, allowing classification based on which answers are correctly paired.
- Core assumption: Token-level recall is sufficient to determine if a disambiguated entity name is mentioned alongside its corresponding answer in the generated text.
- Evidence anchors:
  - [section 4.2]: "We introduce a moving threshold for recall based on the length of the target string... Then, we count the number of answers with disambiguation, denoted as cp, and the number of answers without disambiguation, denoted as cnp, for long-form answer y."
  - [section 4.2]: "The Cohen's Kappa statistic between the two label sets is 0.83, showing strong agreement, though slightly lower than the human agreement of 0.85."
  - [corpus]: No direct evidence - this appears to be a novel evaluation approach.

### Mechanism 3
- Claim: In-context few-shot learning significantly improves model performance by providing examples of complete answers that pair each disambiguated entity with its answer.
- Mechanism: By providing two examples with complete answers in the prompt, models learn the expected output format and are more likely to generate complete answers rather than ambiguous or merged responses.
- Core assumption: Models can learn the desired output pattern from a small number of examples and generalize this pattern to new questions.
- Evidence anchors:
  - [section 5.4]: "We observe performance gains across all models, except for Llama2-7b, particularly in entity recall. GPT-4 exhibits the most substantial gain among all models, nearly three times more than others."
  - [section 5.4]: "All models, except for Llama2-7b, show a significant increase in the number of Complete answers."
  - [corpus]: No direct evidence - this appears to be a novel finding from this paper.

## Foundational Learning

- Concept: Entity disambiguation and coreference resolution across multiple documents
  - Why needed here: The task requires distinguishing between different entities that share the same surface name when presented with multiple documents, each containing information about a different entity.
  - Quick check question: Given two documents about "Michael Jordan" - one about the basketball player and one about the statistician - can you identify which facts belong to which entity?

- Concept: Retrieval-augmented generation and multi-document reasoning
  - Why needed here: The task involves processing multiple documents simultaneously to generate a cohesive answer that correctly attributes information to the appropriate entity.
  - Quick check question: If you have three documents about different people named "John Smith," can you generate a single answer that correctly identifies which John Smith has which attributes?

- Concept: Automatic evaluation metrics for long-form generation
  - Why needed here: Standard QA metrics are insufficient for evaluating whether models correctly pair answers with disambiguated entity names, requiring custom metrics.
  - Quick check question: How would you evaluate whether an answer like "Michael Jordan was born in Brooklyn and earned a degree from UNC" correctly attributes birthplace and education to the right Michael Jordan?

## Architecture Onboarding

- Component map: Wikipedia disambiguation page mining -> document pair selection -> question/answer generation -> answer set expansion -> evaluation
- Critical path: 1) Identify surface names and disambiguated entities from Wikipedia 2) Select document pairs with similar topics but different answers 3) Generate questions and initial answer pairs 4) Expand answer sets for all disambiguated entities 5) Evaluate model outputs using custom metrics
- Design tradeoffs:
  - Synthetic vs. human-annotated data: Synthetic data allows for larger scale but may contain noise
  - Token-level vs. semantic matching: Token-level matching is simpler but may miss paraphrases
  - In-context learning vs. fine-tuning: In-context learning is more flexible but may be less effective for complex patterns
- Failure signatures:
  - High merged answer rate: Model is combining information from multiple entities without proper attribution
  - High ambiguous answer rate: Model is providing correct answers but not identifying which entity they belong to
  - Low entity recall: Model is missing some of the disambiguated entities entirely
- First 3 experiments:
  1. Test zero-shot performance on gold documents only to establish baseline
  2. Test in-context few-shot learning with two examples to measure improvement
  3. Test performance when retrieved documents are mixed with gold documents to measure robustness to noise

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can language models be trained or fine-tuned to improve their ability to generate complete answers when faced with ambiguous entities across multiple documents?
- Basis in paper: Explicit
- Why unresolved: The paper demonstrates that current state-of-the-art models often fail to generate complete answers, instead producing ambiguous or merged responses. While in-context examples can improve performance, the study suggests that more fundamental approaches are needed to teach models to effectively disambiguate entities across documents.
- What evidence would resolve it: Experiments comparing various training/fine-tuning approaches (e.g., contrastive learning, entity-aware pretraining, multi-task learning) on the AmbigDocs benchmark, showing significant improvements in complete answer generation.

### Open Question 2
- Question: What factors contribute most to the generation of ambiguous or merged answers by language models when dealing with ambiguous entities across multiple documents?
- Basis in paper: Inferred
- Why unresolved: The paper identifies that larger models like Llama2-13b and GPT-3.5 tend to generate more ambiguous and merged answers. However, the specific factors within the model architecture, training data, or inference process that lead to these behaviors are not explored in depth.
- What evidence would resolve it: Detailed analysis of model behaviors, including attention patterns, knowledge conflicts, and reasoning processes, to identify key factors influencing ambiguous or merged answer generation.

### Open Question 3
- Question: How does the performance of language models on the AmbigDocs benchmark generalize to real-world scenarios involving ambiguous entities across multiple documents?
- Basis in paper: Inferred
- Why unresolved: While the paper introduces a new benchmark and demonstrates model weaknesses, it does not evaluate how well these findings translate to practical applications where ambiguous entities are common, such as web search or information retrieval systems.
- What evidence would resolve it: Experiments applying models trained or fine-tuned on AmbigDocs to real-world datasets or tasks involving ambiguous entities, comparing performance improvements and generalization capabilities.

## Limitations
- The synthetic data generation pipeline may introduce artifacts that don't fully capture real-world ambiguity scenarios
- The performance gains from few-shot prompting may not generalize to all ambiguous entity types
- The dataset focuses primarily on well-documented Wikipedia entities, potentially missing more challenging real-world ambiguity cases

## Confidence
**High Confidence**: The dataset construction methodology is clearly specified and leverages established Wikipedia resources. The five-category ontology for answer classification is intuitive and empirically validated. The core finding that current models struggle with complete entity-answer pairing is consistently demonstrated across multiple model families.

**Medium Confidence**: The automatic evaluation metrics, while showing strong inter-annotator agreement, may have edge cases where token-level matching fails to capture semantic equivalence. The effectiveness of few-shot prompting is demonstrated but may vary with different prompt formulations or entity types.

**Low Confidence**: The generalizability of findings to domains beyond Wikipedia (e.g., news articles, scientific literature) remains untested. The impact of document ordering on model performance is not fully explored.

## Next Checks
1. **Semantic Equivalence Testing**: Run a small-scale human evaluation comparing the automatic categorization against human judgment for answers that use paraphrases rather than exact token matches, to quantify the false negative rate in the current evaluation system.

2. **Domain Transfer Validation**: Apply the AmbigDocs methodology to a non-Wikipedia corpus (e.g., news articles or scientific papers) to test whether the same evaluation metrics and model performance patterns hold in different domains.

3. **Prompt Sensitivity Analysis**: Systematically vary the few-shot examples (different entity types, question styles, answer formats) to determine which factors most strongly influence the improvement in complete answer generation.