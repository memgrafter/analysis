---
ver: rpa2
title: metabench -- A Sparse Benchmark of Reasoning and Knowledge in Large Language
  Models
arxiv_id: '2407.12844'
source_url: https://arxiv.org/abs/2407.12844
tags:
- items
- benchmark
- item
- latent
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces metabench, a sparse benchmark distilled from
  six prominent LLM evaluation benchmarks (ARC, GSM8K, HellaSwag, MMLU, TruthfulQA,
  WinoGrande). Using data from over 5000 LLMs, the authors apply Item Response Theory
  to identify the most informative items, reducing the total from 28,632 items to
  858 items (less than 3% of original size).
---

# metabench -- A Sparse Benchmark of Reasoning and Knowledge in Large Language Models

## Quick Facts
- arXiv ID: 2407.12844
- Source URL: https://arxiv.org/abs/2407.12844
- Authors: Alex Kipnis; Konstantinos Voudouris; Luca M. Schulze Buschoff; Eric Schulz
- Reference count: 40
- Key outcome: metabench reduces 28,632 items to 858 items (less than 3%) while reconstructing original scores with RMSE of 1.24% and grand mean with 0.58% RMSE

## Executive Summary
metabench introduces a sparse benchmark for evaluating reasoning and knowledge in large language models by distilling items from six prominent benchmarks (ARC, GSM8K, HellaSwag, MMLU, TruthfulQA, WinoGrande). Using data from over 5000 LLMs and Item Response Theory, the authors identify the most informative items, reducing the benchmark size by 97% while maintaining strong predictive accuracy. The resulting benchmark reveals a single underlying general ability factor with high correlation to total scores, and provides two disjoint versions to mitigate memorization effects.

## Method Summary
The authors collected responses from over 5000 LLMs across six established benchmarks, totaling 28,632 items. They applied Item Response Theory to identify the most informative items for distinguishing between model abilities, reducing the total to 858 items (less than 3% of original size). Factor analysis revealed a single underlying general ability factor with Spearman correlation r=0.94 to total scores. To address memorization concerns, they created two disjoint versions of the benchmark. The methodology also enables adaptive testing simulations that show potential for further reduction in test items while maintaining reconstruction accuracy.

## Key Results
- Reduces benchmark size from 28,632 items to 858 items (97% reduction) using IRT
- Reconstructs original benchmark scores with average RMSE of 1.24% and grand mean with 0.58% RMSE
- Identifies a single underlying general ability factor with Spearman correlation r=0.94 to total scores

## Why This Works (Mechanism)
metabench works by applying Item Response Theory to identify items that provide the most information about distinguishing between LLM abilities. The IRT approach estimates item difficulty and discrimination parameters from the response patterns of thousands of models, selecting items that best separate high-performing from low-performing models. This data-driven approach ensures that each retained item maximally contributes to ability estimation. The resulting sparse benchmark captures a single general ability factor that strongly correlates with total scores across all original benchmarks, demonstrating that reasoning and knowledge abilities in LLMs can be effectively measured with far fewer items than traditionally used.

## Foundational Learning
- **Item Response Theory**: Statistical framework for modeling the relationship between latent traits and item responses; needed to identify maximally informative items for ability estimation; quick check: examine item characteristic curves for discrimination parameters
- **Factor Analysis**: Statistical method for identifying underlying latent variables that explain correlations among observed variables; needed to validate the unidimensionality of the general ability factor; quick check: verify eigenvalue > 1 for first factor and scree plot
- **Benchmark Distillation**: Process of reducing item sets while preserving measurement properties; needed to create efficient evaluation protocols; quick check: compare reconstruction accuracy metrics (RMSE) between full and reduced benchmarks

## Architecture Onboarding
**Component Map**: Data Collection -> IRT Analysis -> Item Selection -> Factor Analysis -> Benchmark Construction
**Critical Path**: IRT parameter estimation from 5000+ model responses → item information calculation → top items selection → validation of reconstruction accuracy
**Design Tradeoffs**: Size reduction (858 vs 28,632 items) vs. reconstruction accuracy (RMSE 1.24% vs. original scores); computational efficiency vs. measurement precision
**Failure Signatures**: Poor reconstruction accuracy (>5% RMSE) indicates insufficient item selection; low discrimination parameters suggest items don't differentiate well between abilities
**First 3 Experiments**: 1) Validate reconstruction accuracy on held-out LLM responses 2) Test factor structure stability across different LLM families 3) Evaluate memorization resistance using disjoint benchmark versions

## Open Questions the Paper Calls Out
None

## Limitations
- Inherits biases and cultural specificity from original six benchmarks (ARC, GSM8K, HellaSwag, MMLU, TruthfulQA, WinoGrande)
- IRT assumptions of unidimensionality and local independence may not fully capture complex LLM abilities
- Validated only on specific set of 5,000+ LLMs; generalizability to future models untested

## Confidence
- High confidence: Technical methodology of IRT for item selection and statistical validation showing strong reconstruction of original benchmark scores
- Medium confidence: Claim that metabench captures single underlying general ability factor, though may oversimplify multidimensional nature
- Medium confidence: Assertion that metabench can detect overfitting and memorization, though practical effectiveness requires further validation

## Next Checks
1. Validate metabench's performance on a held-out set of recently developed LLMs not included in the original 5,000+ model dataset
2. Conduct cross-cultural validation by testing whether metabench scores correlate similarly across LLMs trained on different language corpora and cultural contexts
3. Perform ablation studies by systematically removing items from metabench to determine minimum set size maintaining acceptable reconstruction accuracy