---
ver: rpa2
title: 'Beyond Model Collapse: Scaling Up with Synthesized Data Requires Verification'
arxiv_id: '2406.07515'
source_url: https://arxiv.org/abs/2406.07515
tags:
- data
- synthesized
- training
- selection
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of "model collapse" - the degradation
  in model performance when trained on data generated by other models. The authors
  propose using verification to select high-quality synthesized data and prevent model
  collapse.
---

# Beyond Model Collapse: Scaling Up with Synthesized Data Requires Verification

## Quick Facts
- arXiv ID: 2406.07515
- Source URL: https://arxiv.org/abs/2406.07515
- Authors: Yunzhen Feng; Elvis Dohmatob; Pu Yang; Francois Charton; Julia Kempe
- Reference count: 40
- This paper addresses model collapse by using verification to select high-quality synthesized data before training

## Executive Summary
This paper tackles the problem of model collapse - performance degradation when training on data generated by other models. The authors propose using verification to select high-quality synthesized data, showing that even imperfect verifiers can prevent model collapse and recover optimal performance. They introduce a proxy function p* = 1/(1 + ψ/ϕ) that strongly correlates with final model performance. Experiments across three domains (linear classification, matrix eigenvalue prediction, and news summarization) validate the theory and demonstrate that verification is key to effectively scaling up with synthesized data.

## Method Summary
The method involves training a generator model, using it to create synthesized data, then applying a verifier to select high-quality examples based on confidence measures. The selected data is used to train a downstream model. The proxy function p* characterizes verifier effectiveness and correlates with final performance. Three experimental settings test this approach: linear classification with Gaussian mixtures, matrix eigenvalue prediction with transformers, and news summarization with LLMs. Different pruning strategies (no pruning, oracle pruning, supervised pruning) are compared, with verification consistently preventing model collapse.

## Key Results
- Verification prevents model collapse even with imperfect verifiers
- A measurable proxy function p* strongly correlates with final model performance
- A stronger model is not always a better verifier
- The approach works across diverse domains: linear classification, matrix eigenvalue prediction, and news summarization

## Why This Works (Mechanism)

### Mechanism 1
Verification-based pruning can recover optimal performance even with imperfect verifiers by selecting synthesized data with correct labels. This avoids the harmful effects of incorrect labels that cause model collapse. The verifier distinguishes high-quality from low-quality synthesized data based on confidence measures. Break condition: If verifier selection becomes uncorrelated with label quality, or if p exceeds the breakdown point p* = 1/(1 + ψ/ϕ).

### Mechanism 2
A measurable proxy function p* strongly correlates with final model performance by characterizing the verifier's ability to select high-quality data. Higher p* values indicate better selection capability, with p* = 1/(1 + ψ/ϕ) capturing verifier performance characteristics. Break condition: If the relationship between p* and performance becomes non-monotonic or breaks down in different data regimes.

### Mechanism 3
Model collapse occurs due to distributional shift from incorrect labels, not data volume. Training on unverified synthesized data leads to learning the wrong decision boundary, while verification preserves the correct distribution. The synthesized data contains sufficient information about the true distribution, just corrupted by label errors. Break condition: If synthesized data becomes too dissimilar from the true distribution, or if verification cannot recover the correct label distribution.

## Foundational Learning

- Concept: Gaussian Mixture Models
  - Why needed here: Forms the theoretical foundation for understanding how verification works in a controlled setting
  - Quick check question: In a balanced binary Gaussian mixture with means ±μ, what is the Bayes-optimal decision boundary?

- Concept: Linear Classification with Ridge Regularization
  - Why needed here: The downstream model architecture used to test the verification hypothesis
  - Quick check question: How does ridge regularization affect the convergence of logistic regression in high dimensions?

- Concept: Binary Cross-Entropy Loss
  - Why needed here: The loss function used for training the downstream model on verified synthesized data
  - Quick check question: Why is binary cross-entropy preferred over squared error for classification tasks?

## Architecture Onboarding

- Component map: Generator -> Verifier -> Downstream model, with Proxy function p* measuring verifier effectiveness
- Critical path:
  1. Generate synthesized data using trained model
  2. Apply verification/selection to filter data
  3. Train downstream model on verified data
  4. Measure performance and p* correlation
- Design tradeoffs:
  - Stronger verifier vs computational cost
  - Selection rate (12.5%, 25%, 50%) vs data efficiency
  - Perfect vs imperfect verification for practical deployment
- Failure signatures:
  - p* close to 0.5 indicates random selection, likely model collapse
  - Performance worse than generator indicates collapse occurred
  - Weak correlation between p* and performance suggests proxy breakdown
- First 3 experiments:
  1. Linear classification on Gaussian mixtures with synthetic labels and various verifiers
  2. Transformer eigenvalue prediction with beam search and noisy verifiers
  3. Llama-2 news summarization with different selection strategies and p* calculation

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of pruning strategy affect the breakdown point in practical settings with finite data? The theoretical analysis focuses on the infinite-sample limit, and the impact of finite data and different pruning strategies on the practical breakdown point remains unexplored.

### Open Question 2
Can the proposed proxy function (p*) be effectively used to select the best verifier in settings where a natural oracle is not available? The experiments rely on known ground truth to compute p*, but its ability to guide verifier selection without ground truth needs further investigation.

### Open Question 3
How does the model collapse phenomenon extend to more complex tasks and model architectures beyond the ones explored in the paper? The paper focuses on specific settings, and the generalizability of model collapse to other tasks and architectures remains unknown.

### Open Question 4
What are the limitations of using verification to prevent model collapse, and are there alternative approaches that can be equally or more effective? The paper focuses on verification benefits but does not extensively explore its limitations or compare against alternative solutions.

## Limitations

- Theoretical analysis assumes verifier selection bits are independent, which may not hold in practice with correlated data or biased verifiers
- Empirical validation is limited to three specific domains, leaving uncertainty about generalizability to other tasks and modalities
- The proxy function p* shows strong correlation in tested scenarios but its predictive power across diverse settings remains to be validated

## Confidence

- **High** for linear classification setting where theoretical guarantees are rigorously proven
- **Medium** for matrix eigenvalue prediction experiments where p* and performance correlation is demonstrated
- **Medium** for LLM summarization experiments where results are promising but less theoretically grounded
- The claim that "a stronger model is not always a better verifier" is supported but requires more systematic investigation

## Next Checks

1. **Cross-domain validation**: Test the verification approach on image generation tasks (e.g., using stable diffusion models) to assess generalizability beyond text and structured data.

2. **Verifier diversity analysis**: Systematically compare different verifier architectures (smaller models, rule-based systems, ensemble methods) to identify optimal verifier characteristics beyond raw model strength.

3. **Scaling behavior study**: Investigate how the proxy function p* and verification effectiveness change as the volume of synthesized data increases, particularly examining the regime where synthesized data exceeds original training data by orders of magnitude.