---
ver: rpa2
title: 'Choice of PEFT Technique in Continual Learning: Prompt Tuning is Not All You
  Need'
arxiv_id: '2406.03216'
source_url: https://arxiv.org/abs/2406.03216
tags:
- learning
- prompt
- methods
- lora
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper questions the common use of prompt tuning in continual
  learning (CL) methods, arguing that this choice is rarely justified or ablated.
  The authors propose LoRA-based variants of two state-of-the-art CL methods, Learning
  to Prompt and S-Prompts, by replacing prompt tuning with LoRA.
---

# Choice of PEFT Technique in Continual Learning: Prompt Tuning is Not All You Need

## Quick Facts
- arXiv ID: 2406.03216
- Source URL: https://arxiv.org/abs/2406.03216
- Reference count: 40
- One-line primary result: LoRA-based continual learning variants consistently outperform prompt tuning counterparts by 4-10% in average accuracy.

## Executive Summary
This paper challenges the common use of prompt tuning in continual learning (CL) methods, demonstrating that LoRA-based variants consistently outperform prompt-based approaches across multiple benchmarks. The authors systematically replace prompt tuning with LoRA in two state-of-the-art CL methods (Learning to Prompt and S-Prompts) and show significant accuracy improvements while maintaining competitive inference speed. Their findings suggest that unexamined architectural choices can hinder CL progress and recommend using LoRA over prompt tuning when the CL method is not intrinsically tied to prompt tuning's properties.

## Method Summary
The paper compares LoRA and prompt tuning as parameter-efficient fine-tuning techniques in continual learning by implementing LoRA variants of two state-of-the-art CL methods: Learning to Prompt (L2P) and S-Prompts. They create L2L and S-LoRA by replacing prompt tuning components with LoRA modules attached to the query and value weight matrices in the multi-head self-attention layers of a frozen Vision Transformer. The experiments are conducted across four benchmarks (CORe50, DomainNet, Split CIFAR-100, and Split Tiny ImageNet) in both class-incremental and domain-incremental settings, measuring average accuracy, inference speed, parameter efficiency, backward transfer, and forgetting.

## Key Results
- LoRA variants (L2L, S-LoRA) outperform prompt-based variants (L2P, S-Prompts) by 4-10% average accuracy across all benchmarks
- L2L improves over L2P by approximately 6.1% on average, with DomainNet showing a 10% improvement
- LoRA-based expert selection in class-incremental learning achieves significantly higher accuracy than prompt-based selection
- Using shared classifier heads in S-X variants improves class-incremental learning performance by 7-8%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LoRA-based variants of continual learning methods consistently outperform prompt tuning variants across multiple benchmarks.
- **Mechanism:** LoRA updates weight matrices via low-rank decomposition, allowing parameter-efficient adaptation while maintaining higher representational capacity than prompt tokens, which only prepend learnable embeddings.
- **Core assumption:** The added capacity and gradient flow of LoRA provide better adaptation for sequential tasks than soft tokens, without increasing memory footprint significantly.
- **Evidence anchors:**
  - [abstract] "These variants consistently achieve higher accuracy across a wide range of domain-incremental and class-incremental benchmarks, while being competitive in inference speed."
  - [section 4.1] "across all four benchmarks, we see that in the L2X family, L2L improves over L2P by on average about 6.1%, with the performance on DomainNet increasing by approximately 10%."
- **Break condition:** If the base model's weight matrices are already saturated, or if memory constraints strictly limit trainable parameters below LoRA's minimal rank, prompt tuning may become preferable.

### Mechanism 2
- **Claim:** Using a shared classifier head in S-X variants improves class-incremental learning performance.
- **Mechanism:** Sharing the classifier head forces the LoRA adapters to align outputs to a common logit space, reducing confusion between tasks and improving cross-task knowledge transfer.
- **Core assumption:** Class labels are fixed across datasets in class-incremental settings, so a shared head can generalize better than per-task heads.
- **Evidence anchors:**
  - [section 4.3] "For the class-incremental benchmarks, when using a shared head, we mask the logits for classes that are not present in the current dataset... we find this important for performance."
  - [table 4] "Whenever L2X shows better results, using a shared classifier leads to 7 − 8% higher average accuracy."
- **Break condition:** If dataset class distributions are highly disjoint or tasks are domain-incremental with fixed label sets, independent classifiers may be preferable.

### Mechanism 3
- **Claim:** LoRA-based expert selection accuracy is higher than prompt-based selection in class-incremental settings.
- **Mechanism:** LoRA modules, being attached to the base model weights, can modify feature representations more effectively for task identification, whereas prompt tokens only modify input embeddings.
- **Core assumption:** Feature extraction quality is crucial for expert selection; LoRA's direct weight modification provides better task-discriminative features.
- **Evidence anchors:**
  - [section 4.3] "Using the LoRA modules of the first dataset (S-Lo++) to extract the features gives a big boost in identifying the right expert model and hence average accuracy."
  - [figure 5] "S-LoRA++ gets a big boost in performance for the class-incremental datasets (Split CIFAR and Tiny ImageNet). This is due to the improved expert identification accuracy."
- **Break condition:** If expert selection is based on task ID or metadata rather than unsupervised feature clustering, the difference disappears.

## Foundational Learning

- **Vision Transformer (ViT) architecture**
  - Why needed here: The paper uses ViT-B/16 as the base model; understanding its layers and tokenization is essential to grasp how LoRA modules interact with MHSA layers.
  - Quick check question: In ViT, which layer outputs the CLS token embedding used as the classification feature?

- **Parameter-efficient fine-tuning (PEFT) methods**
  - Why needed here: The core contribution compares LoRA and prompt tuning as PEFT strategies; knowing how they differ in parameter usage and placement is key.
  - Quick check question: How many trainable parameters does prompt tuning add compared to LoRA of rank 1 for a ViT-B/16?

- **Continual learning scenarios (class-incremental vs. domain-incremental)**
  - Why needed here: The experiments span both scenarios; understanding label changes vs. distribution shifts explains why shared classifiers help in CIL but not DIL.
  - Quick check question: In class-incremental learning, do new datasets introduce new classes, new domains, or both?

## Architecture Onboarding

- **Component map:**
  Base ViT-B/16 (frozen weights) -> LoRA modules per task (query/value weight matrices in MHSA layers) -> Expert selection module (k-means clustering or nearest prototype) -> Shared or per-task classifier head

- **Critical path:**
  1. Input image → patch embedding → CLS token extraction
  2. LoRA modules (selected by expert ID) modify MHSA weights
  3. Forward pass through ViT
  4. Feature vector → classifier head → logits
  5. Loss computation and backpropagation to LoRA keys, LoRA weights, classifier

- **Design tradeoffs:**
  - Prompt tuning: Fewer parameters, lighter compute, but limited capacity and slower convergence.
  - LoRA: More parameters, better performance, but requires efficient LoRA implementation for speed.
  - Shared classifier: Saves parameters and encourages knowledge sharing; may hurt if tasks are very different.
  - Per-task classifier: Task isolation but higher parameter count.

- **Failure signatures:**
  - Low expert selection accuracy → wrong LoRA/prompt selected → poor performance.
  - Overfitting on prompt tokens → degraded generalization across tasks.
  - Rank too low → underfitting; rank too high → risk of forgetting or increased compute.

- **First 3 experiments:**
  1. Run L2L and L2P on Split CIFAR-100 with default hyperparameters; compare average accuracy.
  2. Measure throughput (images/sec) vs. number of trainable parameters for S-LoRA and S-Prompts on Tiny ImageNet.
  3. Evaluate expert selection accuracy for S-LoRA vs. S-Prompts on CORe50 using k-means prototypes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do other parameter-efficient fine-tuning methods (e.g., IA3, VeRA, DoRA) compare to LoRA and prompt tuning in continual learning?
- Basis in paper: [inferred] The paper mentions that other PEFT methods like IA3, VeRA, and DoRA exist and have been shown to be more parameter-efficient than LoRA, but only experiments with LoRA were conducted.
- Why unresolved: The authors explicitly state that extending the experiments to other PEFT methods is left to future work.
- What evidence would resolve it: Direct experimental comparison of these PEFT methods with LoRA and prompt tuning across multiple CL benchmarks, measuring both accuracy and parameter efficiency.

### Open Question 2
- Question: What is the optimal number of clusters (k) for the S-X family in class-incremental learning scenarios?
- Basis in paper: [explicit] The paper discusses that the choice of k is non-optimal for class-incremental settings and suggests that further increasing k could improve performance, but does not determine an optimal value.
- Why unresolved: The paper only provides preliminary results showing that k = 5 is clearly suboptimal and that performance improves with higher values, but does not exhaustively explore the parameter space.
- What evidence would resolve it: Systematic ablation studies varying k across different class-incremental benchmarks to identify the optimal number of clusters.

### Open Question 3
- Question: How does the choice of LoRA rank affect performance in continual learning, and is there a trade-off between rank and accuracy?
- Basis in paper: [explicit] The paper mentions that they used a LoRA rank of r = 1 to minimize additional parameters, but suggests that larger ranks (e.g., 16 or higher) are typically used and might perform better.
- Why unresolved: The paper only provides limited results with r = 1 and does not explore the impact of varying the rank on both accuracy and parameter efficiency.
- What evidence would resolve it: Comprehensive experiments varying the LoRA rank across multiple CL benchmarks to quantify the trade-off between rank, accuracy, and parameter efficiency.

## Limitations
- The choice of ViT-B/16 as the base model may limit generalizability to other architectures like ConvNets or larger transformers.
- The paper does not provide ablation studies isolating the impact of LoRA's rank versus prompt length, leaving open the possibility that scaling up prompt tuning could close the performance gap.
- The paper does not address catastrophic forgetting beyond average accuracy metrics, so the long-term stability of LoRA-based methods in very long task sequences is unclear.

## Confidence
- **High confidence**: The empirical finding that LoRA-based CL methods (L2L, S-LoRA) consistently outperform their prompt-based counterparts (L2P, S-Prompts) across all tested benchmarks, as evidenced by direct comparisons in tables and figures.
- **Medium confidence**: The claim that LoRA's advantage is robust across both class-incremental and domain-incremental scenarios, since the paper does not report ablations for each scenario separately or test on more diverse datasets.
- **Low confidence**: The assertion that prompt tuning is never preferable to LoRA in CL, since the paper does not explore extreme memory or speed constraints where prompt tuning's minimal parameter count could be decisive.

## Next Checks
1. **Ablation of LoRA rank (r) and prompt length (Lp)**: Systematically vary the LoRA rank and prompt token count across benchmarks to determine the minimum settings for LoRA to outperform prompt tuning, and whether prompt tuning can match LoRA if scaled up.
2. **Generalization to other architectures**: Replicate the main experiments using ConvNets (e.g., ResNet-50) or larger ViT variants (e.g., ViT-L/16) to test if LoRA's advantage holds beyond the chosen base model.
3. **Long-sequence forgetting analysis**: Evaluate L2L and L2P on a task sequence of 20+ datasets, measuring not just average accuracy but also backward/forward transfer and forgetting, to assess the long-term stability of each method.