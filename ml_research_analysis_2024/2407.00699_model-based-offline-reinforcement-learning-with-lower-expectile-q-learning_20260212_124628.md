---
ver: rpa2
title: Model-based Offline Reinforcement Learning with Lower Expectile Q-Learning
arxiv_id: '2407.00699'
source_url: https://arxiv.org/abs/2407.00699
tags:
- offline
- policy
- learning
- expectile
- model-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of inaccurate value estimation\
  \ from model rollouts in model-based offline reinforcement learning (RL). The authors\
  \ propose Lower Expectile Q-learning (LEQ), a novel method that uses expectile regression\
  \ with a small \u03C4 for both policy and Q-function training, providing a low-bias\
  \ model-based value estimation via lower expectile regression of \u03BB-returns."
---

# Model-based Offline Reinforcement Learning with Lower Expectile Q-Learning

## Quick Facts
- arXiv ID: 2407.00699
- Source URL: https://arxiv.org/abs/2407.00699
- Reference count: 40
- Key outcome: LEQ outperforms previous model-based offline RL methods on long-horizon tasks like D4RL AntMaze, matching or surpassing state-of-the-art model-free and sequence modeling approaches.

## Executive Summary
This paper addresses the challenge of inaccurate value estimation from model rollouts in model-based offline reinforcement learning. The authors propose Lower Expectile Q-learning (LEQ), which uses expectile regression with a small τ for both policy and Q-function training. This approach provides low-bias model-based value estimation via lower expectile regression of λ-returns. LEQ significantly outperforms previous model-based offline RL methods on long-horizon tasks, matching or surpassing the performance of model-free approaches and sequence modeling approaches.

## Method Summary
LEQ combines world model pretraining, conservative value estimation via lower expectile regression (τ ≤ 0.5), and λ-returns for multi-step bias reduction. The method pretrains world models and a critic on offline data, then expands the dataset with model rollouts. The critic is updated using both real and model data with λ-returns, while the policy is optimized using conservative expectile regression of model-based returns. This architecture specifically targets the overestimation bias in model-generated Q-values through asymmetric loss functions.

## Key Results
- LEQ significantly outperforms previous model-based offline RL methods on D4RL AntMaze tasks
- Matches or surpasses performance of model-free approaches on long-horizon navigation
- Achieves state-of-the-art results across state-based (NeoRL, D4RL) and pixel-based (V-D4RL) dense-reward environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lower expectile regression with τ < 0.5 yields conservative Q-value estimates from model-generated rollouts.
- Mechanism: Expectile regression replaces squared error with asymmetric weighting that penalizes underestimation more heavily when τ < 0.5, shifting Q-function predictions to lower values.
- Core assumption: Model-generated returns are inherently optimistic due to model error; conservative estimates reduce this bias.
- Evidence anchors: [abstract] "Lower Expectile Q-learning (LEQ), which provides a low-bias model-based value estimation via lower expectile regression of λ-returns." [section 4.1] "To mitigate the overestimation of ˆymodel from inaccurate H-step model rollouts, we propose to use lower expectile regression on target Q-value estimates with small τ."
- Break condition: If model error distribution is not predominantly upward, lower expectile may underestimate true values excessively.

### Mechanism 2
- Claim: λ-returns from 10-step model rollouts reduce bias from both model errors and critic estimation errors.
- Mechanism: λ-returns aggregate multi-step returns with geometric weighting, giving more weight to immediate rewards and less to distant, noisy predictions.
- Core assumption: Long-horizon tasks suffer more from compounded model errors; λ-returns act as a bias-variance tradeoff.
- Evidence anchors: [abstract] "Our empirical results show that LEQ significantly outperforms previous model-based offline RL methods on long-horizon tasks, such as the D4RL AntMaze tasks..." [section 4.2] "we use λ-return instead of 1-step return for Q-learning... allowing the policy to directly learn from low-bias multi-step returns."
- Break condition: If model errors are large and systematic, λ-returns may still propagate bias; if H is too long, model errors dominate.

### Mechanism 3
- Claim: Conservative policy updates via lower expectile of λ-returns prevent exploitation of erroneous model rollouts.
- Mechanism: Policy loss weights conservative λ-returns more heavily (via |τ - 1(·)|), encouraging maximization of less optimistic returns.
- Core assumption: Model-generated rollouts contain false optimism; conservative policy updates mitigate this.
- Evidence anchors: [section 4.3] "we maximize the lower expectile of λ-returns... analogous to our conservative critic learning..." [section 5.6] "LEQ shows better performance compared to AWR (461.8 vs 114.2) and maximizing the Q-values (461.8 vs 410.3)."
- Break condition: If critic underestimates true returns, conservative policy updates may under-explore optimal actions.

## Foundational Learning

- Concept: Expectile regression
  - Why needed here: Provides asymmetric loss for conservative Q-value estimation without requiring full return distribution estimation.
  - Quick check question: What happens to the loss when τ < 0.5 and the prediction is larger than the target?

- Concept: λ-return (TD(λ))
  - Why needed here: Combines multi-step returns to reduce bias from both model errors and critic estimation in long-horizon tasks.
  - Quick check question: How does the weighting change as λ approaches 1?

- Concept: Model-based value estimation bias
  - Why needed here: Explains why model-generated rollouts can overestimate values and why conservatism is needed.
  - Quick check question: What are the two main error sources in model-generated Q-targets?

## Architecture Onboarding

- Component map: World model ensemble -> Critic Q-network -> Policy network -> Dataset expansion -> Loss functions (LQ,model, LQ,env, LQ,EMA, LλQ,model, ˆLλπ)

- Critical path:
  1. Pretrain world models on offline data
  2. Pretrain policy and critic on offline data
  3. Expand dataset with model rollouts (R steps)
  4. Generate imaginary trajectories (H steps)
  5. Update critic with both real and model data
  6. Update policy with model data only

- Design tradeoffs:
  - H (imagination length) vs model error accumulation
  - τ (expectile) vs conservatism vs under-exploration
  - β (loss ratio) vs reliance on model vs real data
  - Ensemble size vs computation vs robustness

- Failure signatures:
  - Oscillating returns during training (optimistic model predictions)
  - Zero or near-zero performance (model collapse or excessive conservatism)
  - High variance across seeds (unstable model or policy updates)

- First 3 experiments:
  1. Run LEQ on D4RL AntMaze-umaze with default H=10, τ=0.1; verify >90% success.
  2. Vary H=5,10,15 on AntMaze-umaze; observe trade-off between bias and variance.
  3. Replace lower expectile with standard squared loss; confirm performance drop.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LEQ's performance scale with the number of world model ensemble members beyond 5?
- Basis in paper: [explicit] The paper mentions using 5 models selected from an ensemble of 7 models with the best validation scores for state-based experiments.
- Why unresolved: The paper does not explore performance improvements or diminishing returns from increasing ensemble size.
- What evidence would resolve it: Experiments comparing performance across varying ensemble sizes (e.g., 3, 5, 10, 20 models) on long-horizon tasks like AntMaze.

### Open Question 2
- Question: Can LEQ's conservative value estimation strategy be extended to handle stochastic policies effectively?
- Basis in paper: [explicit] The paper notes that stochastic policies are quickly driven to deterministic ones by LEQ's conservative penalty, and uses a small entropy bonus to partially mitigate this.
- Why unresolved: The paper only briefly explores stochastic policies and does not develop a method to fully accommodate stochasticity.
- What evidence would resolve it: An extended version of LEQ that successfully incorporates stochastic policies without collapsing to deterministic ones, validated on benchmarks.

### Open Question 3
- Question: How does LEQ's performance change when using learned termination functions instead of ground-truth termination functions?
- Basis in paper: [explicit] The paper reports a significant performance drop when using learned termination functions, especially on diverse datasets.
- Why unresolved: The paper does not explore methods to improve learned termination function accuracy.
- What evidence would resolve it: Improved learned termination functions that achieve performance close to ground-truth termination, or methods to make LEQ robust to termination prediction errors.

### Open Question 4
- Question: How does the choice of imagination length H affect LEQ's performance in very long-horizon tasks beyond AntMaze?
- Basis in paper: [explicit] The paper shows a trade-off between model error and critic robustness with different H values (5, 10, 15) in AntMaze.
- Why unresolved: The paper does not explore performance in tasks with horizons significantly longer than AntMaze.
- What evidence would resolve it: Experiments comparing LEQ performance on tasks with horizons 2x, 5x, or 10x longer than AntMaze with varying H values.

## Limitations
- Effectiveness depends critically on correct choice of τ, but systematic sensitivity analysis is lacking
- λ-return horizon (H=10) is chosen based on computational constraints rather than optimal bias-variance tradeoff
- Ensemble approach may not fully address model bias, and individual model errors are not quantified
- Performance on extremely sparse reward tasks beyond AntMaze remains unproven

## Confidence
- **High Confidence:** Claims about LEQ outperforming previous model-based methods on D4RL AntMaze tasks
- **Medium Confidence:** Claims about matching state-of-the-art model-free methods in dense-reward environments
- **Medium Confidence:** Claims about the effectiveness of lower expectile regression and λ-returns based on ablation studies

## Next Checks
1. Conduct systematic ablation studies varying τ values (0.1, 0.3, 0.5) and H values (5, 10, 15) across multiple AntMaze tasks to quantify sensitivity
2. Compare LEQ against model-free baselines on sparse-reward tasks beyond AntMaze, including continuous control tasks with delayed rewards
3. Quantify individual world model errors and analyze how model bias correlates with performance degradation across different task horizons