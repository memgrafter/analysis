---
ver: rpa2
title: 'ClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate
  Climate Disclosures'
arxiv_id: '2406.09818'
source_url: https://arxiv.org/abs/2406.09818
tags:
- question
- climate
- information
- relevant
- change
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a dataset of 8.5K labeled question-source-answer
  pairs for evaluating retrieval in RAG systems on corporate climate disclosures.
  The dataset was constructed by having expert annotators label relevant report paragraphs
  across 30 sustainability reports for 16 climate-related questions.
---

# ClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures

## Quick Facts
- arXiv ID: 2406.09818
- Source URL: https://arxiv.org/abs/2406.09818
- Reference count: 40
- Introduces a dataset of 8.5K labeled question-source-answer pairs for evaluating retrieval in RAG systems on corporate climate disclosures

## Executive Summary
This paper addresses the critical need for effective information retrieval in climate disclosure analysis by introducing ClimRetrieve, a benchmarking dataset of 8.5K labeled question-source-answer pairs. The dataset was constructed using 30 sustainability reports and 16 climate-related questions, with expert annotators labeling relevant paragraphs across these reports. The study evaluates embedding-based retrieval systems and demonstrates that expert-informed explanations significantly outperform simple question-based queries, while also highlighting the limitations of current embedding models in capturing domain expertise for knowledge-intensive tasks like climate communication.

## Method Summary
The study constructed a dataset by collecting 30 sustainability reports and 16 climate-related Yes/No questions, then having expert annotators label relevant report paragraphs with relevance scores (1-3). The dataset contains 8.5K question-source-answer pairs. For the use case experiment, the researchers implemented embedding-based retrieval using multiple models (BM25, DRAGON+, GTE-base, ColBERTv2, OpenAI's text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large) and compared different query formulations: question only, definitions, concepts, and expert-informed explanations generated using GPT-4. They evaluated retrieval performance using Recall@K, Precision@K, and F1-score metrics.

## Key Results
- Expert-informed explanations outperformed using just the question or definitions for retrieval recall
- Excluding the question from explanations yielded the best retrieval results
- Embedding models showed critical limitations in knowledge-intensive climate disclosure domains
- The dataset is available at https://github.com/tobischimanski/ClimRetrieve

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert-informed explanations improve retrieval recall over raw questions
- Mechanism: Longer, domain-specific explanations provide more detailed semantic context that better matches relevant paragraphs than short questions alone
- Core assumption: Embeddings capture semantic similarity effectively when input contains sufficient domain-specific detail
- Evidence anchors:
  - [abstract] "Results showed that expert-informed explanations outperformed using just the question or definitions"
  - [section] "using these explanations can improve retrieval"
  - [corpus] Weak - corpus doesn't directly address this mechanism
- Break condition: When embeddings fail to capture nuanced domain concepts or when explanations become too generic

### Mechanism 2
- Claim: Excluding the question from explanations yields better retrieval performance
- Mechanism: Removing the exact question text forces the model to focus on conceptual understanding rather than literal matching
- Core assumption: Semantic matching is more effective when not constrained by question wording
- Evidence anchors:
  - [abstract] "excluding the question from explanations yielded the best results"
  - [section] "using expert-informed definitions that exclude the question"
  - [corpus] Weak - corpus lacks evidence about question exclusion effects
- Break condition: When question wording contains unique terms critical for retrieval

### Mechanism 3
- Claim: Embedding models struggle with knowledge-intensive climate disclosure retrieval
- Mechanism: Current embeddings lack sufficient domain-specific training to capture complex climate adaptation concepts
- Core assumption: Climate disclosure domain requires specialized knowledge beyond general embeddings
- Evidence anchors:
  - [abstract] "we also outline the critical limitations of embeddings in knowledge-intensive downstream domains like climate change communication"
  - [section] "SOTA embedding models (on which RAG systems heavily rely) usually fail to effectively reflect domain expertise"
  - [corpus] Moderate - related papers on climate finance and ESG analysis support domain complexity
- Break condition: When embeddings are specifically trained on climate disclosure data

## Foundational Learning

- Concept: Semantic similarity in embedding spaces
  - Why needed here: Core mechanism relies on comparing question/paragraph embeddings
  - Quick check question: What happens when two semantically similar texts have low cosine similarity?

- Concept: Knowledge-intensive domains vs. general domains
  - Why needed here: Explains why standard embeddings underperform in climate disclosure context
  - Quick check question: How does domain specificity affect embedding effectiveness?

- Concept: RAG system architecture
  - Why needed here: Understanding retrieval phase importance in overall RAG pipeline
  - Quick check question: What role does retrieval quality play in final answer generation?

## Architecture Onboarding

- Component map: Question → Embedding model → Similarity scoring → Top-k retrieval → Answer generation
- Critical path: Embedding generation and similarity calculation are bottlenecks for performance
- Design tradeoffs: Generic vs. domain-specific embeddings; question-inclusion vs. exclusion in explanations
- Failure signatures: Low recall with high precision (misses relevant content); high recall with low precision (retrieves irrelevant content)
- First 3 experiments:
  1. Compare retrieval performance using questions vs. definitions vs. expert explanations
  2. Test different embedding models on the same retrieval task
  3. Evaluate impact of question inclusion vs. exclusion in explanations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can domain expertise be effectively integrated into embedding-based retrieval systems for climate disclosures?
- Basis in paper: [explicit] The paper discusses limitations of current embedding models in capturing expert knowledge and suggests this as a key area for improvement.
- Why unresolved: The paper demonstrates that expert-informed explanations outperform simple questions or definitions in retrieval tasks, but doesn't provide a concrete methodology for integrating this expertise into retrieval systems.
- What evidence would resolve it: A proposed framework or algorithm for incorporating expert knowledge into embedding models, along with experimental validation showing improved retrieval performance.

### Open Question 2
- Question: What is the optimal balance between specificity and generalization in expert-informed explanations for information retrieval?
- Basis in paper: [inferred] The paper shows that example-informed explanations improve retrieval, but also suggests potential overfitting when using too many examples.
- Why unresolved: The paper doesn't explore the trade-off between using a small number of examples versus a larger set, or between highly specific versus more general explanations.
- What evidence would resolve it: Experiments comparing retrieval performance using varying numbers of examples and different levels of specificity in explanations, identifying an optimal balance.

### Open Question 3
- Question: How can uncertainty in relevance labeling be effectively incorporated into retrieval evaluation metrics?
- Basis in paper: [explicit] The paper mentions that relevance labels of 1 (partially relevant) introduce uncertainty and suggests this affects evaluation results.
- Why unresolved: The paper uses binary relevance thresholds (2 or higher) for evaluation but doesn't explore methods to incorporate the full spectrum of relevance scores or uncertainty in labeling.
- What evidence would resolve it: Development and validation of evaluation metrics that account for partial relevance and labeling uncertainty, showing improved alignment with human judgment.

## Limitations
- Study relies on a relatively small dataset (30 reports, 16 questions, 8.5K pairs), which may limit generalizability
- Expert annotators' relevance judgments are subjective and may introduce bias in the ground truth labels
- The specific methodology for generating expert-informed explanations using GPT-4 is not fully detailed

## Confidence
- High confidence in the core finding that embedding models struggle with domain-specific climate disclosure retrieval
- Medium confidence in the superiority of expert-informed explanations over raw questions, given the small sample size
- Medium confidence in the finding that excluding questions from explanations improves performance, as this requires further validation

## Next Checks
1. Replicate the retrieval experiments using a larger, more diverse corpus of climate disclosures to test generalizability
2. Compare embedding-based retrieval with sparse retrieval methods (e.g., BM25) and hybrid approaches on the same dataset
3. Conduct ablation studies on the expert-informed explanation generation process to isolate the impact of different components (GPT-4 vs. human experts, question inclusion vs. exclusion)