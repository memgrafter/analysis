---
ver: rpa2
title: 'CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents'
arxiv_id: '2407.01511'
source_url: https://arxiv.org/abs/2407.01511
tags:
- agent
- task
- tasks
- environment
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CRAB introduces a cross-environment agent benchmark framework featuring
  a graph-based evaluation method and efficient task generation, enabling agents to
  operate across multiple platforms. Using CRAB, we developed CRAB Benchmark-v0 with
  120 tasks spanning Ubuntu desktop and Android environments, evaluated on 6 advanced
  MLMs across single and multi-agent configurations.
---

# CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents

## Quick Facts
- arXiv ID: 2407.01511
- Source URL: https://arxiv.org/abs/2407.01511
- Reference count: 40
- Key outcome: Graph-based evaluation framework enabling cross-environment task generation and execution, achieving 38.01% completion ratio with GPT-4o single-agent configuration

## Executive Summary
CRAB introduces a cross-environment agent benchmark framework featuring a graph-based evaluation method and efficient task generation, enabling agents to operate across multiple platforms. Using CRAB, we developed CRAB Benchmark-v0 with 120 tasks spanning Ubuntu desktop and Android environments, evaluated on 6 advanced MLMs across single and multi-agent configurations. Results show GPT-4o with a single-agent structure achieved the highest completion ratio of 38.01%, while multi-agent setups suffered from communication issues. The framework's fine-grained graph evaluator better captures agent performance than traditional goal-based metrics, and the modular design allows easy extension to new environments and tasks.

## Method Summary
CRAB defines a benchmark framework for evaluating multimodal language model agents across multiple environments through a graph-based evaluation approach. The framework consists of environment-specific action spaces, sub-task templates with typed inputs/outputs, and graph evaluators that track intermediate progress. Tasks are generated by composing sub-tasks whose outputs match inputs of others, automatically constructing evaluator graphs. The evaluation uses Completion Ratio (CR), Execution Efficiency (EE), Communication Efficiency (CE), and Success Rate (SR) metrics to measure both final outcomes and intermediate milestones.

## Key Results
- GPT-4o single-agent configuration achieved highest completion ratio of 38.01%
- Multi-agent setups suffered from communication issues, performing worse than single-agent configurations
- Graph evaluator provided finer-grained performance differentiation compared to goal-based metrics
- Ubuntu environment yielded higher completion rates than Android environment across all models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Graph evaluator captures intermediate progress better than goal-based metrics, enabling fine-grained performance differentiation.
- **Mechanism**: Tasks are decomposed into sub-tasks with explicit input/output dependencies forming a DAG. Each sub-task has a binary checkpoint evaluator. During execution, the evaluator updates node status based on environment state, allowing metrics like Completion Ratio (CR) to measure partial progress.
- **Core assumption**: Sub-tasks are well-defined with clear pre/post conditions, and intermediate states are verifiable via deterministic evaluators.
- **Evidence anchors**:
  - [abstract] "Our proposed evaluation framework seeks to overcome these limitations by capturing both final outcomes and intermediate milestones"
  - [section 4.3] "We propose a novel integrated approach, the Graph Evaluator, which provides fine-grained metrics and supports multiple valid paths"
  - [corpus] Weak/no direct evidence in corpus; framework appears novel in benchmarking literature
- **Break condition**: If sub-tasks have ambiguous dependencies or non-deterministic evaluators, graph progress becomes unreliable.

### Mechanism 2
- **Claim**: Cross-environment tasks force agents to handle message passing and action space switching, revealing weaknesses not exposed by single-environment benchmarks.
- **Mechanism**: Tasks combine operations across multiple platforms (e.g., Android → Ubuntu). Agents must maintain context across environment switches, use correct action spaces per environment, and transfer data/state between them.
- **Core assumption**: Real-world tasks often require multi-device coordination, and agents must handle heterogeneous interfaces and coordinate across them.
- **Evidence anchors**:
  - [abstract] "cross-environment tasks offer two main advantages... require sophisticated message processing and information transfer between environments"
  - [section 4.1] "We define a task that requires operations across multiple environments as a cross-environment task"
  - [corpus] No direct evidence; this appears to be the first benchmark targeting cross-environment scenarios
- **Break condition**: If agent communication protocols are robust or if tasks are trivial enough to be solved in a single environment, the cross-environment requirement adds no value.

### Mechanism 3
- **Claim**: Sub-task composition with graph-based task generation enables scalable, diverse task creation without manual authoring.
- **Mechanism**: Sub-tasks are defined as templates with typed inputs/outputs. Tasks are generated by composing sub-tasks whose outputs match inputs of others, automatically constructing evaluator graphs. This replaces manual task writing with systematic composition.
- **Core assumption**: Sub-task templates are sufficiently general to cover a wide range of real-world scenarios, and output/input type matching is sufficient to ensure logical task flow.
- **Evidence anchors**:
  - [section 4.2] "To generate a GDT, input attributes can be filled with either a hand-crafted value corresponding to their type or linked to a task with the same output type as the input type"
  - [section 4.2] "Task descriptions are initially generated by GPT-4 from subtask prompts and refined by human reviewers"
  - [corpus] Weak evidence; composition approach is referenced in other benchmarks but not with explicit graph-based generation
- **Break condition**: If sub-task templates are too narrow or if type-based composition fails to produce realistic task sequences, generation quality degrades.

## Foundational Learning

- **Concept**: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: Environments are defined as POMDPs (S, A, T, O) to capture uncertainty in state observation and action outcomes in GUI automation.
  - Quick check question: In a POMDP, if an agent takes action 'click(elem)' but the screenshot shows a different UI after, what could explain this discrepancy?

- **Concept**: Graph of Decomposed Tasks (GDT)
  - Why needed here: GDT structures tasks as DAGs of sub-tasks with sequential/parallel dependencies, enabling graph evaluator construction and partial progress tracking.
  - Quick check question: If sub-task B depends on output from sub-task A, what must be true about their positions in the GDT?

- **Concept**: Cross-platform task composition
  - Why needed here: Real-world scenarios often require operations across multiple devices/platforms; benchmark must reflect this to evaluate agent generalization.
  - Quick check question: Why would a task requiring both Android and Ubuntu operations be more challenging than one confined to a single platform?

## Architecture Onboarding

- **Component map**: Ubuntu VM -> Android Emulator -> Agent -> Action Space -> Environment -> Screenshot -> Evaluator Graph -> Metrics
- **Critical path**:
  1. Initialize environments with action spaces and observation mechanisms
  2. Load task dataset (JSON with sub-task compositions and adjacency lists)
  3. For each task, construct graph evaluator from sub-task templates
  4. Agent interacts via action calls, environment returns screenshots
  5. Evaluator updates node statuses, metrics computed (CR, EE, CE, SR)
  6. Log termination reasons for analysis
- **Design tradeoffs**:
  - Single vs multi-agent: Single agents simpler but may struggle with cross-platform switching; multi-agent can specialize but introduces communication overhead
  - Graph evaluator complexity vs metric granularity: More nodes = better progress tracking but higher implementation cost
  - Template-based vs manual task creation: Templates scalable but may miss edge cases; manual tasks comprehensive but not scalable
- **Failure signatures**:
  - High "Reach Step Limit" → agent gets stuck in loops or cannot find correct actions
  - High "Invalid Action" → agent hallucinates actions or uses wrong environment's action space
  - High "False Completion" → agent incorrectly assumes task complete without verification
  - Low CR but moderate SR → agent completes final step without building necessary intermediate state
- **First 3 experiments**:
  1. Run single agent (GPT-4o) on Ubuntu-only task, verify CR tracks partial progress through graph evaluator
  2. Run multi-agent by environment on cross-platform task, observe communication logs for information loss
  3. Disable evaluator graph, compare SR-only results with full metric suite to quantify value of intermediate tracking

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the graph-based evaluation method compare to trajectory-based methods in terms of scalability and computational efficiency when applied to large-scale tasks with hundreds of nodes?
- Basis in paper: [explicit] The paper mentions that the graph evaluator provides fine-grained metrics and supports multiple valid paths, unlike trajectory-based methods which compare against a predefined gold action sequence.
- Why unresolved: The paper does not provide quantitative data or experiments comparing the computational overhead or scalability of the graph-based evaluator against trajectory-based methods for tasks of varying complexity.
- What evidence would resolve it: Benchmarking experiments comparing execution time, memory usage, and accuracy of graph-based vs trajectory-based evaluators across tasks with increasing node counts (e.g., 10, 50, 100, 200 nodes).

### Open Question 2
- Question: What are the specific architectural or training differences in GPT-4 Turbo that make it perform better on Ubuntu tasks compared to Android tasks, while GPT-4o shows the opposite trend?
- Basis in paper: [explicit] The paper notes that GPT-4 Turbo exhibits superior performance on Ubuntu tasks, while GPT-4o and other models perform better on Android tasks, suggesting model-specific optimizations.
- Why unresolved: The paper does not investigate or explain the underlying reasons for these platform-specific performance differences between model variants.
- What evidence would resolve it: Ablation studies comparing model architectures, training data distributions, or fine-tuning strategies for Ubuntu vs Android tasks, along with analysis of error patterns on each platform.

### Open Question 3
- Question: How would the performance of the CRAB framework change if it incorporated real-time visual grounding improvements, such as those from groundingDINO or advanced OCR, to address icon recognition failures?
- Basis in paper: [inferred] The paper identifies icon recognition failures as a key issue where models plan correctly but fail to interact with visual elements, despite visual prompts detecting them correctly.
- Why unresolved: The paper uses existing visual grounding tools but does not experiment with enhanced or customized visual grounding models specifically trained for GUI interaction tasks.
- What evidence would resolve it: Comparative experiments replacing groundingDINO and OCR with state-of-the-art visual grounding models, measuring improvements in completion ratios and reductions in invalid action rates.

## Limitations

- Graph evaluator assumptions may give false confidence if sub-task boundaries are poorly defined or evaluators are too permissive
- Cross-environment value proposition contradicted by results showing multi-agent setups performing worse than single-agent configurations
- Generalizability of task generation limited by coverage analysis missing - whether 120 tasks represent diverse real-world use cases

## Confidence

- Graph evaluator effectiveness: Medium - Shows improved metrics but limited validation of semantic relevance
- Cross-environment difficulty claims: Low - Multi-agent setups performed worse, contradicting stated advantages
- Task generation scalability: Medium - Framework described but coverage analysis missing

## Next Checks

1. **Semantic validity test**: Have human experts rate whether intermediate milestones in the graph evaluator correspond to meaningful progress through tasks, comparing against baseline goal-only evaluation.

2. **Template coverage analysis**: Systematically enumerate all possible sub-task compositions within the template system and assess whether generated tasks cover realistic multi-step workflows beyond the current 120 examples.

3. **Communication overhead quantification**: Design controlled experiments varying communication frequency and message complexity in multi-agent setups to isolate whether communication failures or task switching overhead drives performance degradation.