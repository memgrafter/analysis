---
ver: rpa2
title: 'Reducing the Cost: Cross-Prompt Pre-Finetuning for Short Answer Scoring'
arxiv_id: '2408.13966'
source_url: https://arxiv.org/abs/2408.13966
tags:
- answer
- prompt
- data
- answers
- scoring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of automated short answer scoring
  (SAS), where models need to be trained for each new prompt due to varying rubrics
  and reference answers. The authors propose a two-phase approach: pre-finetuning
  a model on existing cross-prompt data and then finetuning it for a new target prompt.'
---

# Reducing the Cost: Cross-Prompt Pre-Finetuning for Short Answer Scoring

## Quick Facts
- **arXiv ID**: 2408.13966
- **Source URL**: https://arxiv.org/abs/2408.13966
- **Reference count**: 25
- **Primary result**: Pre-finetuning with key phrases significantly improves scoring accuracy, especially with limited training data

## Executive Summary
This paper addresses the challenge of automated short answer scoring (SAS), where models need to be trained for each new prompt due to varying rubrics and reference answers. The authors propose a two-phase approach: pre-finetuning a model on existing cross-prompt data and then finetuning it for a new target prompt. They use key phrases from rubrics as representative expressions to train the model to learn the relationship between key phrases and answers. Experiments on a Japanese SAS dataset show that pre-finetuning with key phrases significantly improves scoring accuracy, especially with limited training data. The model learns that answers containing expressions semantically close to key phrases receive higher scores. The approach reduces the need for costly training data preparation for each new prompt, making SAS more feasible in resource-limited settings like schools and online courses.

## Method Summary
The authors propose a two-phase training approach for short answer scoring. First, they pre-finetune a BERT model on cross-prompt data where inputs consist of key phrases (representative expressions from rubrics) concatenated with student answers, and outputs are normalized scores. Then, they finetune this pre-finetuned model on the target prompt's specific data. The input format is key phrases + [SEP] + answer tokens, processed through a BERT encoder with a linear layer and sigmoid output for scoring. They evaluate four settings: Baseline (no key phrases, no pre-finetuning), Key phrase (with key phrases, no pre-finetuning), Pre-finetune (no key phrases, with pre-finetuning), and Pre-finetune & key phrase (with both). The model is pre-finetuned for 5 epochs and finetuned for 10 epochs (or 30 without pre-finetuning) on 6 target prompts with varying training data sizes (10, 25, 50, 100, 200 examples), using Quadratic Weighted Kappa (QWK) as the primary metric.

## Key Results
- Pre-finetuning with key phrases significantly improves scoring accuracy, especially with limited training data
- The model learns that answers containing expressions semantically close to key phrases receive higher scores
- Pre-finetuning can reduce required training data by half while maintaining the same performance
- The approach reduces the need for costly training data preparation for each new prompt

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-finetuning on cross-prompt data with key phrases enables the model to learn a generalizable scoring principle—answers with expressions semantically close to key phrases receive higher scores.
- Mechanism: The model is trained to associate key phrases (representative expressions from rubrics) with answer content across multiple prompts. This builds a transferable scoring bias that applies even to unseen prompts.
- Core assumption: The scoring principle (semantic proximity to rubric-specified expressions correlates with score) is consistent across prompts.
- Evidence anchors:
  - [abstract]: "The model learns that answers containing expressions semantically close to key phrases receive higher scores."
  - [section 4]: "We hypothesize that one essential property a SAS model can learn in pre-finetuning is the scoring principle: an answer generally gets a high score if it contains sufficient information specified by the rubric..."
  - [corpus]: Weak—no direct citations in neighbors; related work focuses on cross-prompt scoring but not pre-finetuning with key phrases.
- Break condition: If rubrics and scoring criteria vary in structure or logic (e.g., some prompts value creativity over content match), the semantic proximity assumption fails.

### Mechanism 2
- Claim: Two-phase training (pre-finetuning → finetuning) improves performance especially when in-prompt training data is scarce.
- Mechanism: Pre-finetuning transfers generalizable scoring knowledge to the model, reducing the amount of prompt-specific data needed for good performance during finetuning.
- Core assumption: Cross-prompt data contains enough common scoring patterns to be useful for a new prompt.
- Evidence anchors:
  - [abstract]: "Pre-finetuning with key phrases significantly improves scoring accuracy, especially with limited training data."
  - [section 5.3]: "Our results indicate that pre-finetuning with key phrases can reduce the required training data by half while maintaining the same performance."
  - [corpus]: Weak—neighbors discuss cross-prompt scoring but not specifically pre-finetuning; no cited comparisons.
- Break condition: When sufficient in-prompt data is available (e.g., >200 samples), the marginal benefit of pre-finetuning disappears.

### Mechanism 3
- Claim: Key phrases act as interpretable anchors that bridge the gap between abstract rubric criteria and concrete answer evaluation.
- Mechanism: By inputting both key phrases and answers during training, the model learns to align answer content with rubric expectations, making the scoring more rubric-compliant.
- Core assumption: Key phrases are faithful, representative summaries of rubric expectations.
- Evidence anchors:
  - [abstract]: "We utilize key phrases, or representative expressions that the answer should contain to increase scores..."
  - [section 4]: "Key phrases are clearly stated in each rubrics. We use those key phrases for each prompt p from the corresponding rubric..."
  - [corpus]: Weak—no explicit mention of key phrases in neighbors; closest is rubric-assisted scoring in TRATES.
- Break condition: If key phrases are incomplete, ambiguous, or poorly aligned with rubric intent, the model cannot learn correct scoring rules.

## Foundational Learning

- Concept: Cross-prompt generalization in NLP
  - Why needed here: The paper's innovation relies on transferring scoring knowledge across prompts with different rubrics. Understanding how models generalize across domains is essential.
  - Quick check question: Why might a model trained on prompt A not automatically score prompt B well without adaptation?

- Concept: Pre-finetuning vs. standard fine-tuning
  - Why needed here: The two-phase training approach is central to the method. Knowing how pre-finetuning differs from standard fine-tuning clarifies why it helps here.
  - Quick check question: What is the difference between pre-finetuning on cross-prompt data and simply training on all prompts jointly?

- Concept: Semantic similarity and representation learning
  - Why needed here: The model's ability to judge "semantic closeness" between key phrases and answers is key to its scoring. Understanding embedding-based similarity is important.
  - Quick check question: How might a model measure whether an answer contains content "close to" a key phrase?

## Architecture Onboarding

- Component map: Input layer (key phrases + [SEP] + answer) -> BERT encoder -> CLS token representation -> Linear layer + sigmoid -> Score output

- Critical path:
  1. Prepare key phrases from rubrics
  2. Construct cross-prompt dataset (key phrases + answers + scores)
  3. Pre-finetune BERT on this dataset
  4. Load pre-finetuned model weights
  5. Finetune on target prompt data
  6. Evaluate with QWK

- Design tradeoffs:
  - Using key phrases vs. full rubrics: Key phrases are concise but may lose nuance; full rubrics are richer but noisier.
  - Pre-finetuning with many prompts vs. few: More prompts improve generalization but increase compute and may introduce conflicting patterns.
  - Score normalization: Scaling to [0,1] simplifies regression but may obscure rubric-specific score meaning.

- Failure signatures:
  - Performance doesn't improve over baseline: Likely the key phrase alignment is poor or the scoring principle isn't consistent.
  - Large variance across prompts: Some prompts may be poorly suited for cross-prompt transfer.
  - Overfitting during pre-finetuning: Model memorizes cross-prompt patterns without learning general rules.

- First 3 experiments:
  1. Baseline: Finetune BERT on target prompt without key phrases or pre-finetuning.
  2. Key phrase only: Finetune BERT on target prompt with key phrases but no pre-finetuning.
  3. Pre-finetuning effect: Pre-finetune on cross-prompt data without key phrases, then finetune on target.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the long-term effects of pre-finetuning with key phrases on the model's ability to generalize to new prompts with unseen key phrases?
- Basis in paper: [inferred] The paper mentions that the model learns the relationship between key phrases and answers, but does not explore how this affects generalization to new prompts with different key phrases.
- Why unresolved: The paper focuses on the immediate performance improvements of pre-finetuning with key phrases, but does not investigate the long-term effects on generalization to new prompts with unseen key phrases.
- What evidence would resolve it: Conducting experiments with prompts containing new key phrases not seen during pre-finetuning and evaluating the model's performance would provide evidence of its ability to generalize.

### Open Question 2
- Question: How does the diversity of key phrases in the pre-finetuning data affect the model's performance on new prompts?
- Basis in paper: [inferred] The paper mentions that increasing the number of prompts used for pre-finetuning improves performance, but does not specifically investigate the impact of key phrase diversity.
- Why unresolved: The paper does not explore how the diversity of key phrases in the pre-finetuning data affects the model's ability to learn and generalize to new prompts.
- What evidence would resolve it: Analyzing the relationship between key phrase diversity in the pre-finetuning data and the model's performance on new prompts would provide insights into the importance of key phrase diversity.

### Open Question 3
- Question: Can the model learn to identify key phrases from the rubric without explicit key phrase annotations?
- Basis in paper: [inferred] The paper uses key phrases as input to the model, but does not explore whether the model can learn to identify key phrases from the rubric without explicit annotations.
- Why unresolved: The paper relies on explicit key phrase annotations for pre-finetuning, but does not investigate whether the model can learn to identify key phrases from the rubric itself.
- What evidence would resolve it: Training the model to identify key phrases from the rubric without explicit annotations and evaluating its performance on new prompts would provide evidence of its ability to learn from the rubric directly.

## Limitations

- The paper relies on a single Japanese dataset with a specific rubric structure, limiting generalizability to other languages or assessment formats
- Key phrase extraction is manually performed without specified criteria, raising concerns about reproducibility and scalability
- The approach focuses exclusively on analytic criteria scoring, potentially limiting applicability to holistic or multi-trait scoring systems

## Confidence

- **High Confidence**: The core claim that pre-finetuning improves performance with limited data is well-supported by the experimental results, showing consistent QWK improvements across all six target prompts when training data is scarce (10-50 examples)
- **Medium Confidence**: The mechanism explaining that models learn "semantic proximity to key phrases correlates with score" is plausible but primarily supported by qualitative interpretation rather than quantitative analysis of learned representations or ablation studies on different types of key phrases
- **Medium Confidence**: The claim that pre-finetuning reduces required training data by half is supported by results but based on a single dataset; external validation across different domains or languages would strengthen this claim

## Next Checks

1. **Cross-linguistic validation**: Replicate the pre-finetuning approach on an English SAS dataset (such as ASAP or other available corpora) to test whether the mechanism generalizes beyond Japanese and whether key phrase extraction methods transfer across languages

2. **Key phrase robustness analysis**: Systematically vary the quality and completeness of key phrases (e.g., using partial phrases, semantically related but not exact phrases, or phrases from different rubric sections) to determine how sensitive the approach is to key phrase quality and whether the model truly learns the scoring principle or merely pattern-matches exact phrases

3. **Zero-shot and few-shot boundary analysis**: Conduct experiments with zero training examples for target prompts (pure zero-shot) and with varying amounts of cross-prompt data to establish the minimum viable dataset size for effective pre-finetuning, helping to quantify the practical resource requirements for different educational settings