---
ver: rpa2
title: A Benchmark Dataset with Larger Context for Non-Factoid Question Answering
  over Islamic Text
arxiv_id: '2409.09844'
source_url: https://arxiv.org/abs/2409.09844
tags:
- dataset
- evaluation
- question-answering
- answers
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a large-scale dataset of over 73,000 question-answer
  pairs for non-factoid QA over Islamic texts, including Quranic Tafsir and Hadith.
  The dataset is enriched with contextual information from these sources.
---

# A Benchmark Dataset with Larger Context for Non-Factoid Question Answering over Islamic Text

## Quick Facts
- **arXiv ID:** 2409.09844
- **Source URL:** https://arxiv.org/abs/2409.09844
- **Authors:** Faiza Qamar; Seemab Latif; Rabia Latif
- **Reference count:** 40
- **Primary result:** Introduced a large-scale dataset of over 73,000 question-answer pairs for non-factoid QA over Islamic texts, with transformer-based models fine-tuned to achieve ROUGE scores of 24.70-27.23, though human evaluation revealed low verdict consistency (11%-20%) with expert scholars.

## Executive Summary
This paper introduces a large-scale dataset for non-factoid question answering over Islamic texts, including Quranic Tafsir and Hadith, enriched with contextual information. The dataset comprises over 73,000 question-answer pairs from Islamqa.org, with context extracted from Al-Tafsir.com and Hadith collections. The authors establish baseline results using transformer-based models (T5, BART, LED, LongT5) fine-tuned on this dataset, achieving significant improvements in ROUGE scores. However, human evaluation reveals significant disparities between automatic metrics and expert assessments, highlighting the need for more nuanced evaluation approaches in specialized domains.

## Method Summary
The authors constructed a dataset of 73,000+ question-answer pairs from Islamqa.org, enriched with contextual information from Quranic Tafsir and Hadith collections. They employed LDA topic modeling to categorize questions into eight topics, reducing the search space for relevant context. Transformer-based models (T5, BART, LED, LongT5) were fine-tuned on this dataset using batch sizes of 1-6, learning rates of 0.001-2e-5, and sequence lengths of 512-4096. Evaluation was conducted using both automatic metrics (ROUGE, BERTScore) and human assessment focusing on verdict consistency with scholars and contextual understanding.

## Key Results
- ROUGE scores improved significantly after fine-tuning, ranging from 24.70 to 27.23
- Verdict consistency with expert scholars ranged between 11% to 20%
- Contextual understanding scores spanned 50% to 90%
- Automatic evaluation metrics showed substantial improvement, but human evaluation revealed significant disparities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LDA topic modeling reduces the search space for relevant context, enabling efficient answer generation without scanning the full corpus.
- **Mechanism:** The dataset is pre-segmented into eight topics (e.g., Prayer, Fasting, Hajj, Hadith) via LDA. For each question, only Ahadith and Ayahs matching the question's topic are retrieved, limiting context to a manageable subset.
- **Core assumption:** Topics assigned to questions are accurate enough to capture semantic relevance for retrieval.
- **Evidence anchors:**
  - [section] "To reduce the search space from the complete text of Quranic Tafsir and Ahadith, we employed Latent Dirichlet Allocation (LDA) topic modeling... By categorizing the dataset into these topics, we significantly reduced the search space, allowing the model to quickly identify relevant texts and generate accurate answers."
  - [section] "Table 1 presents the top words from each topic with an assigned label... By categorizing the dataset into these topics, we significantly reduced the search space, allowing the model to quickly identify relevant texts and generate accurate answers."
- **Break condition:** If topic labels are inaccurate or overly broad, retrieval will include irrelevant context, degrading answer quality.

### Mechanism 2
- **Claim:** Fine-tuning transformer-based models on the curated dataset improves ROUGE scores by aligning model outputs with the style and content of Islamic scholarly answers.
- **Mechanism:** Pre-trained models (T5, BART, LED, LongT5) are fine-tuned on over 73k question-answer-context triples, adapting their parameters to the linguistic and thematic patterns of the Islamic domain.
- **Core assumption:** The dataset contains enough diversity and quality to generalize beyond the training examples.
- **Evidence anchors:**
  - [abstract] "Baseline results were established using transformer-based models (T5, BART, LED, LongT5) fine-tuned on the dataset. ROUGE scores improved significantly after fine-tuning, ranging from 24.70 to 27.23."
  - [section] "The fine-tuning of language models on the dataset resulted in improved performance, as evident by the evaluation using ROUGE in Table 4."
- **Break condition:** If the dataset is too narrow or contains errors, fine-tuning may overfit or propagate inaccuracies.

### Mechanism 3
- **Claim:** Human evaluation reveals that automatic metrics like ROUGE and BERTScore do not reliably measure verdict consistency with expert scholars in religious domains.
- **Mechanism:** Human evaluators assess both verdict consistency (agreement with scholars) and contextual understanding (model's grasp of question context). Low consistency scores (11%-20%) versus higher contextual understanding (50%-90%) expose the gap between automatic and human assessment.
- **Core assumption:** Expert scholars can serve as a gold standard for evaluating religious QA systems.
- **Evidence anchors:**
  - [abstract] "However, human evaluation revealed significant disparities: verdict consistency with expert scholars ranged between 11% to 20%, while contextual understanding spanned 50% to 90%."
  - [section] "The human evaluation of the QA system for the Quran and Ahadith was conducted based on two crucial parameters: Verdict Consistency and Contextual Understanding."
- **Break condition:** If human evaluators disagree or if scholars' interpretations vary widely, the "ground truth" becomes ambiguous.

## Foundational Learning

- **Concept:** Latent Dirichlet Allocation (LDA) topic modeling
  - Why needed here: Reduces the computational load and improves retrieval precision by focusing on topically relevant context rather than the entire corpus.
  - Quick check question: What is the primary advantage of using LDA for topic modeling in this dataset?
    - **Answer:** It groups documents by latent topics, allowing the system to retrieve only contextually relevant texts for each question.

- **Concept:** Fine-tuning transformer models
  - Why needed here: Adapts general-purpose language models to the specific linguistic patterns, style, and domain knowledge required for Islamic QA.
  - Quick check question: Why is fine-tuning necessary instead of using pre-trained models directly?
    - **Answer:** Pre-trained models lack the specialized vocabulary, tone, and context of Islamic texts; fine-tuning aligns them with the dataset's domain.

- **Concept:** Human evaluation in specialized domains
  - Why needed here: Automatic metrics like ROUGE cannot assess the correctness of religious verdicts, which require expert interpretation and contextual judgment.
  - Quick check question: What limitation of ROUGE is highlighted by the human evaluation results?
    - **Answer:** ROUGE measures surface similarity but not whether the generated answer aligns with scholarly consensus or accurately reflects the context.

## Architecture Onboarding

- **Component map:** IslamQA.org (Q&A pairs) -> LDA topic modeling -> Context filtering (soft cosine similarity) -> Transformer models (T5, BART, LED, LongT5) -> ROUGE/BERTScore/Human evaluation
- **Critical path:**
  1. Load and clean raw data
  2. Apply LDA to assign topics
  3. Filter context by topic and similarity
  4. Fine-tune models on the dataset
  5. Evaluate using both automatic and human metrics
- **Design tradeoffs:**
  - RAM constraints led to using base model versions and limiting input lengths.
  - ROUGE prioritizes recall, which may not suit domains requiring high precision (e.g., religious verdicts).
  - Human evaluation is time-consuming but necessary for domain-specific accuracy.
- **Failure signatures:**
  - Low verdict consistency but high ROUGE scores: Model generates plausible-sounding but incorrect answers.
  - High contextual understanding but low verdict consistency: Model grasps context but misapplies it to verdicts.
  - ROUGE scores improve but human evaluation shows no gain: Overfitting to surface patterns.
- **First 3 experiments:**
  1. **Topic assignment accuracy:** Run LDA on a held-out validation set and manually verify topic labels; if accuracy < 70%, revisit preprocessing.
  2. **Context relevance filtering:** Compare retrieval results using LDA-only vs. LDA+soft cosine similarity; measure precision@3.
  3. **Model ablation:** Fine-tune one model (e.g., T5) with and without context; compare ROUGE and human evaluation to quantify context's impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of transformer-based models on non-factoid QA over Islamic texts compare to their performance on other domains, and what specific challenges arise from the religious context?
- Basis in paper: [explicit] The paper discusses the fine-tuning of transformer-based models (T5, BART, LED, LongT5) on a dataset of over 73,000 question-answer pairs for non-factoid QA over Islamic texts, and reports ROUGE scores and human evaluation results.
- Why unresolved: The paper provides initial baseline results but does not extensively compare performance across different domains or delve deeply into the unique challenges posed by the religious context.
- What evidence would resolve it: Comparative studies of model performance on non-factoid QA tasks across various domains, with a focus on identifying domain-specific challenges and proposing tailored solutions for the Islamic text domain.

### Open Question 2
- Question: What are the limitations of automatic evaluation metrics like ROUGE and BERTScore in assessing the quality of answers for non-factoid QA over Islamic texts, and how can these metrics be improved or supplemented?
- Basis in paper: [explicit] The paper highlights the limitations of automatic evaluation metrics through human evaluation, which revealed significant disparities between automatic scores and human assessments of verdict consistency and contextual understanding.
- Why unresolved: The paper identifies the limitations but does not propose specific improvements or alternative evaluation methods that could better capture the nuances of non-factoid QA in this domain.
- What evidence would resolve it: Development and validation of new evaluation metrics or frameworks that incorporate domain expertise and human judgment, with a focus on capturing the accuracy, consistency, and contextual relevance of answers.

### Open Question 3
- Question: How can the dataset of over 73,000 question-answer pairs for non-factoid QA over Islamic texts be further expanded and enriched to improve the performance of QA systems and support research in this field?
- Basis in paper: [inferred] The paper introduces a large dataset but does not discuss potential strategies for expanding or enriching it, such as incorporating multiple Tafsir books, Hadith collections, or additional Islamic scholarly resources.
- Why unresolved: The paper does not explore the potential benefits or methods for dataset expansion, which could lead to more comprehensive and accurate QA systems.
- What evidence would resolve it: Studies on the impact of dataset size, diversity, and enrichment on model performance, as well as the development of techniques for automatically incorporating and aligning additional Islamic texts and resources into the dataset.

## Limitations

- **Significant gap between automatic and human evaluation:** ROUGE scores show improvement, but human evaluation reveals low verdict consistency (11%-20%) with expert scholars, suggesting automatic metrics may not adequately capture answer quality in religious domains.
- **Dependence on source data quality:** The dataset's accuracy relies entirely on the correctness of Islamqa.org content, which was not independently verified.
- **Potential topic modeling bias:** LDA topic assignments may be inaccurate or overly broad, leading to retrieval of irrelevant context and degraded answer quality.

## Confidence

- **High confidence:** The dataset construction methodology (73k+ QA pairs with contextual enrichment), the fine-tuning procedure for transformer models, and the improvement in ROUGE scores after fine-tuning are well-documented and reproducible.
- **Medium confidence:** The human evaluation results and their interpretation, particularly the stark contrast between verdict consistency and contextual understanding scores, are credible but may depend on evaluator expertise and subjective judgment.
- **Low confidence:** The generalizability of these results to other religious or specialized domains remains uncertain, as does the long-term stability of fine-tuned models when applied to evolving or nuanced religious questions.

## Next Checks

1. **Topic Assignment Validation:** Conduct an independent audit of the LDA topic assignments by having domain experts manually verify topic labels for a random sample of 100+ questions. Calculate precision and recall against the expert judgments to quantify topic model accuracy.

2. **Context Retrieval Relevance:** Perform an ablation study comparing retrieval performance using only LDA-based topic filtering versus the full LDA + soft cosine similarity approach. Measure precision@3 for retrieved context passages to isolate the contribution of similarity-based filtering.

3. **Longitudinal Expert Consistency:** Repeat the human evaluation with a different set of Islamic scholars on the same model outputs to assess inter-rater reliability and determine whether verdict consistency scores are consistent across expert panels.