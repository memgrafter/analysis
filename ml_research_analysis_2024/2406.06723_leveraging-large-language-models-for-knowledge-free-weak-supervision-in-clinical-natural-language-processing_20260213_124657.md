---
ver: rpa2
title: Leveraging Large Language Models for Knowledge-free Weak Supervision in Clinical
  Natural Language Processing
arxiv_id: '2406.06723'
source_url: https://arxiv.org/abs/2406.06723
tags:
- entity
- type
- bert
- e-06
- inst
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a method that leverages large language models
  (LLMs) to generate weak labels for clinical named entity recognition tasks. The
  method uses a fine-tuned LLM to perform in-context learning on unannotated notes,
  generating weak labels that are used to train a downstream BERT model.
---

# Leveraging Large Language Models for Knowledge-free Weak Supervision in Clinical Natural Language Processing

## Quick Facts
- **arXiv ID**: 2406.06723
- **Source URL**: https://arxiv.org/abs/2406.06723
- **Reference count**: 40
- **Primary result**: LLM-generated weak labels improve clinical NER performance by 4.7-47.9% F1 over out-of-box PubMedBERT with minimal gold data

## Executive Summary
This study proposes a method that leverages large language models (LLMs) to generate weak labels for clinical named entity recognition tasks. The approach uses a fine-tuned LLM to perform in-context learning on unannotated clinical notes, generating weak labels that are used to train a downstream BERT model. The BERT model is then further fine-tuned on small amounts of gold standard data. Evaluated on three clinical benchmarks, the method achieved F1 scores that outperformed the out-of-the-box PubMedBERT baseline by 4.7% to 47.9% when using no more than 10 gold standard notes. The method requires minimal domain knowledge and avoids the computational burden of deploying LLMs in production.

## Method Summary
The method employs a fine-tuned Llama2-13B LLM to generate weak labels for clinical NER through prompt-based in-context learning. First, a prompt template is constructed for each benchmark, and the LLM is fine-tuned on a small set of gold standard notes via supervised fine-tuning. The fine-tuned LLM then processes unannotated clinical notes sentence-by-sentence to generate JSON-formatted weak labels. These labels undergo post-processing to extract entities, which are then used to weakly supervise a PubMedBERT model. Finally, the BERT model is fine-tuned on the small gold standard set. The approach also includes a compact variant that uses Llama2 out-of-the-box without fine-tuning to reduce computational costs.

## Key Results
- Outperformed out-of-box PubMedBERT by 4.7% to 47.9% F1 with no more than 10 gold standard notes across three benchmarks
- With 50 gold standard notes, performance approached that of fully fine-tuned systems
- The compact variant using Llama2 out-of-the-box achieved improved performance across benchmarks while reducing computational burden

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fine-tuning LLMs on small gold-standard sets before in-context learning improves weak label quality
- **Mechanism**: Supervised fine-tuning adapts the LLM to the clinical domain and task, reducing the mismatch between general-domain pre-training and specific annotation patterns
- **Core assumption**: The few gold-standard examples contain representative patterns that can be generalized by the LLM to label the rest of the corpus
- **Evidence anchors**:
  - [abstract]: "Using a prompt-based approach, the LLM is used to generate weakly-labeled data for training a downstream BERT model"
  - [section]: "We first constructed a prompt template... For a given set of n gold standard notes, we fine-tune Llama2-13B via prompt-based supervised fine-tuning (SFT)"
  - [corpus]: Weak (only mentions fine-tuning in related works, not mechanism evaluation)
- **Break condition**: If gold-standard examples are too few or not representative, SFT may overfit or generalize poorly, making weak labels noisier

### Mechanism 2
- **Claim**: Weak supervision with LLM-generated labels outperforms direct fine-tuning of small models on gold data
- **Mechanism**: Weak labels provide broader coverage of the data space than the limited gold set, acting as a form of data augmentation that captures patterns missed by the small gold sample
- **Core assumption**: LLM-generated labels, despite being noisy, capture enough correct signal to improve downstream model generalization
- **Evidence anchors**:
  - [abstract]: "With no more than 10 gold standard notes, our final BERT models weakly supervised by fine-tuned Llama2-13B consistently outperformed out-of-the-box PubMedBERT by 4.7% to 47.9% in F1 scores"
  - [section]: "Our primary proposed method, Llama-SFTn-WS-BERTn consistently achieved dominant performance in most experiments across the three benchmarks"
  - [corpus]: Weak (related works mention weak supervision but not specific to LLM-generated labels)
- **Break condition**: If weak labels are too noisy or systematically biased, they may harm downstream performance more than help

### Mechanism 3
- **Claim**: Compact approach using out-of-the-box LLM for weak supervision achieves near-optimal performance with lower computational cost
- **Mechanism**: Eliminates SFT step while still leveraging LLM's pre-trained knowledge, reducing GPU hours while maintaining performance gains
- **Core assumption**: Llama2's pre-trained knowledge is sufficient for generating useful weak labels without domain-specific adaptation
- **Evidence anchors**:
  - [abstract]: "Considering the computational burden of fine-tuning LLMs, we also proposed a compact version using Llama2 out-of-the-box and achieved improved performances across the board"
  - [section]: "The compact method Llama-WS-BERTn showed improved performance in most benchmarks"
  - [corpus]: Weak (no direct evidence in related works)
- **Break condition**: If task requires domain-specific adaptation beyond general medical knowledge, out-of-the-box performance may degrade significantly

## Foundational Learning

- **Concept**: Prompt engineering and template design
  - **Why needed here**: Effective in-context learning depends on well-structured prompts that clearly communicate task requirements to the LLM
  - **Quick check question**: What are the four key sections of the prompt template used in this study, and why is each important?

- **Concept**: Weak supervision and label aggregation
  - **Why needed here**: Understanding how noisy weak labels from LLM are combined to train the downstream BERT model
  - **Quick check question**: How does the study ensure the ns gold-standard notes are representative of the training set distribution?

- **Concept**: Few-shot learning and in-context reasoning
  - **Why needed here**: The LLM performs few-shot in-context learning at sentence level to generate weak labels
  - **Quick check question**: Why did the study choose sentence-level processing instead of processing entire clinical notes in the prompt?

## Architecture Onboarding

- **Component map**: Llama2-13B (LLM) → SFT (optional) → Few-shot in-context learning → Weak label generation → Weak supervision of BERT → Fine-tuning on gold data → Production BERT model
- **Critical path**: Prompt template creation → LLM setup → SFT (if used) → Weak label generation → Weak supervision training → Gold fine-tuning → Evaluation
- **Design tradeoffs**: SFT vs out-of-box LLM (performance vs computational cost), sentence-level vs note-level processing (efficiency vs context), exact matching vs fuzzy matching in post-processing (bias vs flexibility)
- **Failure signatures**: Poor performance on any benchmark, high variance across runs, excessive post-processing failures (>1% sentences), GPU time exceeding projections
- **First 3 experiments**:
  1. Run Llama2-13B with prompt template on 3 gold-standard notes, check weak label quality and post-processing success rate
  2. Fine-tune BERT with weak labels from experiment 1, evaluate on development set
  3. Fine-tune BERT from experiment 2 with 3 gold-standard notes, compare to baseline PubMedBERT fine-tuned on same gold data

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of the proposed method scale with increasing amounts of weakly labeled data beyond the benchmarks tested?
- **Basis in paper**: [inferred] The paper mentions that with less than 800 notes, the LLM was able to generate weak labels that dramatically improved performance, but does not explore scaling to larger datasets.
- **Why unresolved**: The paper focused on benchmarks with relatively small sample sizes for computational considerations, but did not evaluate performance on larger datasets.
- **What evidence would resolve it**: Experiments evaluating the method on larger clinical datasets with millions of notes to measure performance gains and computational costs.

### Open Question 2
- **Question**: How do different prompt templates and few-shot examples affect the quality of weak labels generated by the LLM?
- **Basis in paper**: [inferred] The paper states that different settings in prompt templates and the number of few-shot examples were not evaluated, following reported best practices instead.
- **Why unresolved**: The study used a fixed prompt template design without exploring alternative templates or example quantities to optimize weak label generation.
- **What evidence would resolve it**: Systematic experiments varying prompt templates and few-shot example numbers to measure impact on weak label quality and downstream model performance.

### Open Question 3
- **Question**: How does the proposed method perform compared to other weak supervision approaches that use different labeling functions or ontologies?
- **Basis in paper**: [explicit] The paper mentions that other weak supervision studies use a large number of unlabeled notes processed by LFs, but does not compare to these approaches.
- **Why unresolved**: The study focuses on LLM-generated weak labels without benchmarking against other weak supervision methods using rule-based LFs or ontology-based approaches.
- **What evidence would resolve it**: Direct comparison experiments between the LLM-based method and other weak supervision techniques on the same clinical benchmarks.

## Limitations
- Limited data efficiency studies - doesn't explore minimal viable gold set size or performance degradation curves
- Reproducibility barriers - key implementation details including exact prompt templates and hyperparameter configurations remain unspecified
- Generalizability constraints - evaluation limited to three specific n2c2 benchmarks in English clinical text

## Confidence

- **High confidence**: The core finding that weak supervision with LLM-generated labels improves performance over direct PubMedBERT fine-tuning on small gold sets is well-supported by consistent results across three benchmarks
- **Medium confidence**: The mechanism explanation for why SFT improves weak label quality relies heavily on related work citations rather than direct ablation studies within this paper
- **Low confidence**: The generalizability claims beyond the three evaluated benchmarks and the assertion that this approach works across different clinical domains are not empirically supported in the current study

## Next Checks
1. **Ablation study on gold set size**: Systematically test performance with 1, 2, 5, and 10 gold notes to characterize the minimal viable dataset size and identify the inflection point where weak supervision stops providing benefits

2. **Cross-domain robustness test**: Apply the same methodology to a different clinical domain (e.g., radiology reports or discharge summaries) or a non-clinical domain to evaluate generalizability beyond the three n2c2 benchmarks

3. **Variance and stability analysis**: Run the complete pipeline 5+ times with different random seeds and document the variance in F1 scores across benchmarks to establish the reliability of the approach and identify potential sources of instability