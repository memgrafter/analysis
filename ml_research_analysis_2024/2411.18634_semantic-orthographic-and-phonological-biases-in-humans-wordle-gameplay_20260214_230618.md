---
ver: rpa2
title: Semantic, Orthographic, and Phonological Biases in Humans' Wordle Gameplay
arxiv_id: '2411.18634'
source_url: https://arxiv.org/abs/2411.18634
tags:
- guesses
- human
- distance
- guess
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how human players'' guesses in the game
  Wordle are influenced by semantic, orthographic, and phonological biases from previous
  guesses. Using a dataset of over 65,000 human games and comparing them to near-optimal
  model guesses generated by an entropy-based solver, the researchers measured several
  metrics: Levenshtein distance, semantic distance (using Word2Vec and GloVe embeddings),
  shared syllables, shared characters, and rhyme occurrence.'
---

# Semantic, Orthographic, and Phonological Biases in Humans' Wordle Gameplay

## Quick Facts
- arXiv ID: 2411.18634
- Source URL: https://arxiv.org/abs/2411.18634
- Reference count: 26
- Key outcome: Human Wordle gameplay is systematically biased by semantic, orthographic, and phonological factors from previous guesses, leading to suboptimal strategies compared to entropy-based models.

## Executive Summary
This study investigates how human players' Wordle guesses are influenced by biases from previous attempts. Using a dataset of over 65,000 human games and comparing them to near-optimal model guesses generated by an entropy-based solver, researchers found that humans consistently guess words that are structurally closer to previous attempts and more semantically related. These biases are particularly pronounced when players receive partial feedback, leading them to reuse characters and syllables more frequently than the model. The study demonstrates that human gameplay is constrained by cognitive shortcuts and priming effects, resulting in systematic deviations from optimal play.

## Method Summary
The researchers collected human Wordle gameplay data from Reddit posts, extracting guesses and game states using regex patterns. They cleaned the data by verifying against Wordle answers and removing invalid entries. For each human game state, they used the Doddle Wordle solver to generate near-optimal guesses. They then computed several metrics comparing each human guess to their previous guess, and similarly for model guesses. These metrics included Levenshtein distance, semantic distance using Word2Vec and GloVe embeddings, shared syllables, shared characters, and rhyme occurrence. Cohen's d and t-tests were used to assess statistical differences between human and model distributions across different game states.

## Key Results
- Human players consistently guess words with smaller Levenshtein distance to their previous attempts compared to the model.
- Humans show greater semantic similarity between consecutive guesses, especially in states with partial feedback.
- Players reuse characters and syllables more frequently than the model, particularly when receiving feedback about correct letters.
- These biases lead to suboptimal strategies, as humans prioritize familiarity and ease over the model's information-maximizing approach.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human players exhibit semantic, orthographic, and phonological biases in their Wordle gameplay, leading to systematic deviations from optimal play.
- Mechanism: Cognitive priming and familiarity-seeking behaviors cause players to select words that are structurally or semantically similar to previous guesses, especially after receiving partial feedback. This leads to suboptimal exploration of the solution space.
- Core assumption: Human decision-making in constrained environments like Wordle is influenced by cognitive shortcuts, such as priming and the desire to minimize cognitive load.
- Evidence anchors:
  - [abstract] "human players' guesses are biased to be similar to previous guesses semantically, orthographically, and morphologically."
  - [section] "Since, as we show, humans’ guesses tend to be close semantically to previous guesses, humans’ Wordle plays can be seen as being akin to word-association games."
- Break condition: If players are explicitly instructed to ignore previous guesses or if they receive no feedback, these biases may diminish or disappear.

### Mechanism 2
- Claim: Human players tend to reuse characters and syllables from previous guesses more frequently than the optimal model, particularly in states with some correct letters.
- Mechanism: After receiving partial feedback (e.g., some correct letters), humans become more conservative, focusing on retaining and reusing confirmed letters or familiar patterns rather than exploring new possibilities. This leads to higher character-level similarity between consecutive guesses.
- Core assumption: Players prioritize minimizing cognitive effort by sticking to familiar patterns and confirmed information.
- Evidence anchors:
  - [abstract] "Humans also reuse syllables and characters more frequently than the model, particularly in states with some correct letters."
  - [section] "States like 1g0y4b, 2g0y3b, 0g1y4b, and 1g1y3b exhibit high Cohen's d values (0.85 and above), indicating a significant divergence between human and model strategies."
- Break condition: If players are forced to make entirely new guesses (e.g., by a rule change) or if feedback is minimal, the tendency to reuse characters may decrease.

### Mechanism 3
- Claim: Human players' semantic choices are more closely aligned with their previous guesses than the model's, especially when substantial feedback is provided.
- Mechanism: Humans rely on semantic associations and contextual clues to select words that are conceptually or contextually similar to their previous guesses, particularly when they have received feedback that narrows down the solution space. This leads to smaller semantic distances between consecutive guesses.
- Core assumption: Human decision-making in Wordle is influenced by semantic priming and the tendency to select words that are contextually related.
- Evidence anchors:
  - [abstract] "human players' guesses are biased to be similar to previous guesses semantically."
  - [section] "In states like 3g2y0b, 0g5y0b, and 3g1y1b show minimal differences in behavior between humans and the model."
- Break condition: If players are explicitly instructed to ignore semantic relationships or if feedback is minimal, semantic biases may diminish.

## Foundational Learning

- Concept: Levenshtein distance and its role in measuring structural similarity between words.
  - Why needed here: Levenshtein distance is used to quantify how closely human guesses align with their previous attempts, providing a measure of structural similarity.
  - Quick check question: What is the maximum possible Levenshtein distance between two 5-letter words?

- Concept: Word embeddings (Word2Vec and GloVe) and their use in measuring semantic similarity.
  - Why needed here: Word embeddings are used to represent words as vectors, allowing for the calculation of semantic distance between guesses and their previous attempts.
  - Quick check question: How does cosine similarity between word vectors relate to semantic similarity?

- Concept: Cohen's d and its use in measuring effect size.
  - Why needed here: Cohen's d is used to quantify the standardized difference between human and model performance, providing a measure of the magnitude of biases.
  - Quick check question: What does a Cohen's d value of 0.8 indicate about the difference between two groups?

## Architecture Onboarding

- Component map: Data collection -> Data cleaning -> Metrics calculation -> Statistical analysis -> Visualization
- Critical path: Data collection → Data cleaning → Metrics calculation → Statistical analysis → Visualization
- Design tradeoffs:
  - Using a heuristic solver (Doddle) instead of an exact optimal solver due to computational limitations.
  - Focusing on single-word comparisons rather than all prior guesses to simplify analysis.
  - Using pre-trained word embeddings (Word2Vec, GloVe) rather than training custom embeddings for this specific task.
- Failure signatures:
  - High Cohen's d values indicate significant differences between human and model play.
  - Low p-values indicate statistically significant differences.
  - Large effect sizes in specific game states suggest cognitive biases are at play.
- First 3 experiments:
  1. Compare Levenshtein distances between human and model guesses in states with minimal feedback (e.g., 0g0y5b).
  2. Calculate semantic distances using Word2Vec and GloVe embeddings for human and model guesses in states with partial feedback (e.g., 1g0y4b).
  3. Analyze shared character and syllable reuse between human and model guesses in states with complete feedback (e.g., 3g2y0b).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do human players exhibit priming effects in Wordle gameplay beyond the immediate previous guess, and if so, how far back do these effects extend?
- Basis in paper: [explicit] The paper mentions that human guesses are compared to the previous guess, but priming effects from multiple previous guesses are not investigated.
- Why unresolved: The analysis only considers the immediate previous guess for measuring biases, leaving potential multi-step priming effects unexplored.
- What evidence would resolve it: Analyzing human guesses against multiple previous guesses to identify whether priming effects extend beyond the immediate prior attempt, and quantifying the extent of these effects.

### Open Question 2
- Question: How do individual differences in vocabulary size, linguistic background, or cognitive style affect the observed biases in human Wordle gameplay?
- Basis in paper: [inferred] The study uses aggregate data from Reddit users but does not examine individual differences or player characteristics.
- Why unresolved: The analysis treats all human players as a homogeneous group without considering variations in linguistic knowledge or cognitive strategies.
- What evidence would resolve it: Collecting data on player characteristics and correlating these with gameplay patterns to identify how individual differences influence guessing strategies.

### Open Question 3
- Question: Do the observed biases in human Wordle gameplay change as players gain experience or as the game progresses through the week?
- Basis in paper: [explicit] The paper mentions that players maintain streaks and want to win, but does not investigate how performance or biases evolve over time or across days.
- Why unresolved: The study analyzes gameplay without considering temporal factors or player experience level.
- What evidence would resolve it: Analyzing gameplay data segmented by player experience (e.g., number of games played) and by day of the week to identify changes in strategy and bias intensity over time.

## Limitations
- The study relies on observational data from Reddit, which may introduce sampling bias as participants self-select into posting their results.
- The Doddle solver, while heuristic-based and practical, may not represent true optimal play, potentially underestimating human performance.
- The use of pre-trained embeddings may not perfectly capture semantic relationships specific to Wordle's constrained vocabulary and game context.

## Confidence
- High Confidence: The existence of systematic differences between human and model gameplay, as measured by Levenshtein distance and shared character/syllable reuse.
- Medium Confidence: The interpretation of these differences as primarily driven by cognitive biases like priming and familiarity-seeking.
- Low Confidence: The specific attribution of semantic biases to priming effects, as this requires additional psychological validation beyond the game data.

## Next Checks
1. Conduct a controlled experiment where players are instructed to avoid using letters from previous guesses, then compare whether character reuse patterns change significantly.
2. Replicate the analysis using an exact optimal solver (if computationally feasible) to establish whether the Doddle heuristic systematically underperforms in certain game states.
3. Test whether the observed biases persist across different word game variants (e.g., with different word lengths or feedback mechanisms) to assess generalizability.