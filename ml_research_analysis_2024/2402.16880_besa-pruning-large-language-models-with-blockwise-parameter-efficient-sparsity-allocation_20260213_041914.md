---
ver: rpa2
title: 'BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity
  Allocation'
arxiv_id: '2402.16880'
source_url: https://arxiv.org/abs/2402.16880
tags:
- pruning
- sparsity
- besa
- weight
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes BESA, a blockwise parameter-efficient sparsity
  allocation technique for pruning large language models (LLMs). BESA addresses the
  limitations of layer-wise pruning methods, which can lead to significant performance
  degradation and require careful hyperparameter tuning.
---

# BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation

## Quick Facts
- **arXiv ID**: 2402.16880
- **Source URL**: https://arxiv.org/abs/2402.16880
- **Reference count**: 10
- **Primary result**: Achieves state-of-the-art performance in pruning LLaMA1 and LLaMA2 models with 7B to 70B parameters

## Executive Summary
This paper introduces BESA (Blockwise Parameter-Efficient Sparsity Allocation), a novel technique for pruning large language models (LLMs) that addresses limitations of layer-wise pruning methods. BESA optimizes pruning rates for each layer within transformer blocks by minimizing block-wise reconstruction error in a differentiable manner. The method achieves state-of-the-art performance in language modeling tasks and downstream tasks while demonstrating practical speedup potential in hardware simulation.

## Method Summary
BESA targets overall pruning error with respect to individual transformer blocks rather than layers, and allocates layer-specific sparsity in a differentiable manner. The method encodes weight importance relationships in a parameter-efficient way by learning pruning probabilities based on sorted weights. BESA uses a combination of block-wise reconstruction loss, parameter-efficient sparsity learning through simplex-constrained optimization, and joint quantization-pruning optimization. The algorithm processes each transformer block sequentially, sorting weights by importance using Wanda's metric, initializing learnable combination coefficients, and iteratively updating sparsity masks through gradient-based optimization.

## Key Results
- Achieves 0.16 perplexity improvement on WikiText2 compared to SparseGPT when pruning 50% of LLaMA2-70B parameters
- Establishes new state-of-the-art performance in language modeling tasks and downstream tasks
- Can efficiently prune 50% of parameters in LLaMA2-70B within five hours on a single A100 GPU
- Demonstrates practical speedup potential in hardware simulation with row-wise sparsity patterns

## Why This Works (Mechanism)

### Mechanism 1
BESA reduces layer-wise error accumulation by pruning at the transformer block level rather than individual layers. Instead of applying uniform pruning rates per layer, BESA reconstructs the output of each entire transformer block and optimizes pruning rates for each layer within that block in a differentiable manner. This approach assumes that minimizing block-wise reconstruction error leads to less overall degradation than layer-wise minimization because errors don't accumulate across layers.

### Mechanism 2
Differentiable sparsity allocation allows optimal pruning rates to be learned for each layer within a block. BESA represents sparsity as learnable combination coefficients βd over candidate pruning rates pd, enabling gradient-based optimization of per-layer sparsity through straight-through estimation of binary masks. The core assumption is that the optimal sparsity for each layer varies significantly and can be found through differentiable optimization rather than heuristic assignment.

### Mechanism 3
Parameter-efficient sparsity learning reduces optimization complexity by encoding weight importance relationships. Instead of learning binary masks for all weights, BESA learns pruning probabilities based on sorted weights where less important weights have higher pruning probability, reducing the solution space dramatically. The method assumes that weight importance can be accurately estimated using simple metrics and that this ordering remains stable during pruning.

## Foundational Learning

- **Concept**: Straight-through estimator (STE) for gradient estimation through discrete operations
  - Why needed here: BESA needs to backpropagate through binary mask generation (Mi,j = 0 or 1) which is non-differentiable
  - Quick check question: What happens to the gradient of a binary mask when using STE during backpropagation?

- **Concept**: Block reconstruction loss and its relationship to overall model performance
  - Why needed here: Understanding how minimizing block-level reconstruction error translates to preserving model capabilities
  - Quick check question: How does block-wise reconstruction error differ from layer-wise reconstruction error in terms of error propagation?

- **Concept**: Parameter-efficient learning through simplex-constrained optimization
  - Why needed here: BESA uses β coefficients constrained to a simplex to represent sparsity allocation probabilities
  - Quick check question: Why constrain the combination coefficients to a simplex rather than allowing them to take arbitrary values?

## Architecture Onboarding

- **Component map**: Input -> Pre-trained LLM weights, calibration dataset, target sparsity -> Core components -> Weight sorting (Wanda metric), parameter-efficient sparsity learning, block-wise reconstruction loss, joint quantization-pruning optimization -> Output -> Pruned LLM with layer-specific sparsity masks and optional quantization parameters -> Key hyperparameters -> Candidate pruning rates {pd}, combination coefficients {βd}, λ (sparsity penalty weight), calibration data size

- **Critical path**:
  1. Sort weights by importance using Wanda metric (done once per block)
  2. Initialize learnable β coefficients
  3. Iteratively: generate masks → compute block reconstruction loss → update β via gradients
  4. Store final masks and proceed to next block
  5. (Optional) Joint quantization optimization

- **Design tradeoffs**:
  - Row-wise vs layer-wise sparsity learning: Row-wise provides finer control but requires custom CUDA kernel; layer-wise is simpler but less precise
  - Number of candidate pruning rates: More candidates allow finer granularity but increase parameter count and optimization complexity
  - Block size: Larger blocks reduce optimization overhead but may lose fine-grained control over individual layer contributions

- **Failure signatures**:
  - If reconstruction loss plateaus early: likely issues with learning rate or insufficient calibration data
  - If final perplexity is much worse than baseline: possible issues with weight importance metric or mask generation
  - If optimization is unstable: check simplex constraint implementation or STE gradient estimation

- **First 3 experiments**:
  1. Single transformer block pruning: Apply BESA to one block with known expected behavior to verify mask generation and reconstruction loss minimization
  2. Ablation on candidate pruning rates: Test different numbers of candidates to find the optimal balance between granularity and efficiency
  3. Joint pruning-quantization: Verify that quantization parameters can be optimized alongside pruning without destabilizing the process

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of BESA compare to other state-of-the-art pruning methods on larger language models beyond LLaMA-2-70B? The paper only reports results on LLaMA models up to 70B parameters. Experimental results showing the performance of BESA on larger language models like GPT-3 or PaLM would be valuable.

### Open Question 2
What is the impact of different weight importance metrics on the performance of BESA? The paper mentions that BESA uses Wanda's weight importance metric, but also states that the method is sensitive to the choice of importance metrics. Experimental results comparing the performance of BESA using different weight importance metrics would be informative.

### Open Question 3
How does the joint optimization of pruning and quantization affect the overall compression ratio and model performance? The paper mentions that BESA can be jointly optimized with weight-only quantization, but does not provide detailed analysis of the trade-offs. Experimental results showing the compression ratio and model performance for different quantization schemes and bit-widths when jointly optimized with BESA would be valuable.

## Limitations
- Performance claims and comparisons with state-of-the-art methods would benefit from more extensive ablation studies
- The relationship between block-wise reconstruction error and final model performance needs more empirical validation across diverse tasks
- Practical speedup claims based on hardware simulation require empirical verification on actual hardware

## Confidence

- **High confidence**: The core mechanism of blockwise reconstruction error minimization and differentiable sparsity allocation is well-founded and technically sound. The use of parameter-efficient learning through simplex-constrained optimization is a valid approach with reasonable assumptions.
- **Medium confidence**: The performance claims and comparisons with state-of-the-art methods are plausible but would benefit from more extensive ablation studies. The relationship between block-wise reconstruction error and final model performance, while theoretically motivated, needs more empirical validation across diverse tasks.
- **Low confidence**: The practical speedup claims based on hardware simulation require empirical verification. The effectiveness of the joint pruning-quantization optimization and its contribution to overall performance improvements is not fully disentangled from the benefits of pruning alone.

## Next Checks

1. **Extended ablation on candidate pruning rates**: Systematically evaluate the impact of different numbers of candidate pruning rates on both performance and computational efficiency to determine the optimal trade-off.

2. **Task generalization study**: Test BESA-pruned models on a broader range of downstream tasks beyond the few mentioned to verify that the block-wise reconstruction error minimization generalizes across different application domains.

3. **Hardware implementation verification**: Implement the proposed row-wise sparsity pattern on actual GPU hardware to measure real-world speedup and compare with the simulated results, identifying any discrepancies between theoretical and practical performance.