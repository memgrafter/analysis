---
ver: rpa2
title: 'CREAM: Consistency Regularized Self-Rewarding Language Models'
arxiv_id: '2410.12735'
source_url: https://arxiv.org/abs/2410.12735
tags:
- cream
- preference
- reward
- srlm
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of self-rewarding language models,
  where the same model is used to generate responses and score them for iterative
  alignment. The core issue is that without regularization, the model can become overconfident
  in its judgments, leading to unreliable preference data and diminishing returns
  over iterations.
---

# CREAM: Consistency Regularized Self-Rewarding Language Models

## Quick Facts
- arXiv ID: 2410.12735
- Source URL: https://arxiv.org/abs/2410.12735
- Reference count: 37
- This paper proposes CREAM, which improves self-rewarding language models by using consistency regularization to prevent overconfidence in preference judgments.

## Executive Summary
This paper addresses a fundamental challenge in self-rewarding language models (SRLMs): when the same model is used for both response generation and scoring, it can become overconfident in its judgments, leading to unreliable preference data and diminishing returns over iterations. CREAM (Consistency Regularized sElf-rewarding lAnguage Model) tackles this by leveraging the consistency of rankings across iterations as a regularization signal. By comparing how a model ranks responses against its own ranking from the previous iteration, CREAM identifies and downweights noisy or unreliable preference pairs. Experiments demonstrate significant improvements in alignment performance across multiple NLP benchmarks while maintaining gains over multiple iterations.

## Method Summary
CREAM improves SRLM training by using consistency between consecutive iterations as a regularization signal. The method works by sampling multiple responses per prompt, computing rewards using intrinsic reward models, ranking responses, and then measuring the Kendall's Tau correlation between current and previous iteration rankings. This consistency rate is used to weight a mixed loss function that combines normal and reversed DPO losses, preventing the model from becoming overconfident in distinguishing responses of similar quality. The approach is tested on Llama-2 and Llama-3 (7B) models using a seed SFT dataset and iterative preference training across downstream tasks.

## Key Results
- CREAM significantly improves alignment performance over standard SRLM approaches across multiple NLP benchmarks
- The method maintains performance gains over multiple self-rewarding iterations without degradation
- CREAM outperforms both standard self-rewarding and regularization baselines in ranking consistency metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CREAM mitigates reward bias by regularizing preference pairs that are inconsistent across iterations.
- Mechanism: The model compares rankings from the current iteration (θt) against rankings from the previous iteration (θt-1). If the rankings are inconsistent, the consistency rate is low, indicating unreliable preference data. The regularization term Cλ in the loss function downweights these unreliable pairs by mixing in reversed preference pairs.
- Core assumption: The model's rankings become more consistent over time if the model is improving, and inconsistent rankings signal noise or ambiguity in the preference data.
- Evidence anchors:
  - [abstract]: "CREAM leverages the consistency of rewards across different iterations to regularize the self-rewarding training, helping the model to learn from more reliable preference data."
  - [section]: "We can recover Cλ ≈ 1 − λ = (1 + τj)/2 and we average all τj for all xj ∈ DU in Line 11."
  - [corpus]: "Self-Consistency of the Internal Reward Models Improves Self-Rewarding Language Models" (related work suggesting consistency is valuable).
- Break condition: If the model's rankings become highly unstable or erratic between iterations, the consistency signal would become unreliable.

### Mechanism 2
- Claim: CREAM prevents the model from becoming overconfident in distinguishing responses of similar quality.
- Mechanism: By using a soft-label DPO loss that mixes in reversed preference pairs (weighted by the consistency rate C), CREAM prevents the model from learning hard distinctions between responses that the model itself is uncertain about. This regularization is equivalent to label smoothing for preference learning.
- Core assumption: When two responses are of similar quality, the model should not be forced to make a hard preference judgment, as this can lead to noisy training data.
- Evidence anchors:
  - [abstract]: "The core idea behind CREAM is that we should not force the model to be overly confident in distinguishing between responses of similar quality."
  - [section]: "The regularization term LReg(θ; y, y′, x) prevents the model πθ from overconfidence in distinguishing the preference of {y, y′} of similar quality."
  - [corpus]: Weak - the corpus neighbors don't directly address overconfidence regularization, but the general theme of self-rewarding consistency is present.
- Break condition: If the consistency rate becomes too high (near 1), the regularization effect would be minimal, potentially allowing overconfidence to return.

### Mechanism 3
- Claim: CREAM uses the iterative nature of self-rewarding to create a self-supervised consistency signal without external reward models.
- Mechanism: Instead of using an external reward model to judge consistency (which would defeat the purpose of self-rewarding), CREAM uses the model from the previous iteration as a baseline. The Kendall's Tau correlation between the two models' rankings provides a consistency estimate that can be used for regularization.
- Core assumption: The model's performance generally improves over iterations, so the previous iteration's model serves as a reasonable baseline for consistency estimation.
- Evidence anchors:
  - [abstract]: "CREAM leverages the consistency of rewards across different iterations to regularize the self-rewarding training."
  - [section]: "Fortunately, we can employ the model before last update θt−1 as the baseline model θ′t (i.e., last iteration's model) for evaluating the consistency of the model θ, thanks to chances provided by iterative training manner."
  - [corpus]: "Calibrated Self-Rewarding Vision Language Models" (suggests calibration and consistency are important in self-rewarding).
- Break condition: If the model's performance degrades between iterations, the previous iteration would not be a good baseline, and the consistency signal would become misleading.

## Foundational Learning

- Concept: Kendall's Tau correlation coefficient
  - Why needed here: Used to measure the consistency between rankings from consecutive iterations, providing the core signal for CREAM's regularization.
  - Quick check question: If two ranking lists are perfectly correlated, what is the value of Kendall's Tau? (Answer: 1.0)

- Concept: Label smoothing in classification
  - Why needed here: CREAM's soft-label DPO is mathematically equivalent to label smoothing, which helps prevent overconfidence by mixing in alternative labels weighted by the consistency rate.
  - Quick check question: In traditional label smoothing, what probability is typically assigned to the correct class versus the smoothed distribution? (Answer: e.g., 0.9 for correct, 0.1 distributed among others)

- Concept: Bradley-Terry model for pairwise comparisons
  - Why needed here: The theoretical foundation for modeling preference probabilities between responses, which is used to derive the regularization term LReg.
  - Quick check question: In the Bradley-Terry model, what does the logistic function σ(log odds) represent? (Answer: The probability that one item is preferred over another)

## Architecture Onboarding

- Component map:
  Response Sampler -> Reward Calculator -> Ranker -> Consistency Estimator -> Regularization Controller -> Preference Trainer

- Critical path:
  1. Sample responses → 2. Compute rewards for both models → 3. Generate rankings → 4. Calculate consistency → 5. Update mixing weight → 6. Train with weighted loss

- Design tradeoffs:
  - Using previous iteration as baseline vs external reward model: More efficient but potentially less accurate
  - N=5 responses per prompt: Balances computational cost with ranking stability
  - Mixing normal and reversed preferences: Prevents overconfidence but may slow convergence if overused

- Failure signatures:
  - Consistency rate consistently near 0: Model is unstable or degrading between iterations
  - Consistency rate consistently near 1: Regularization is ineffective, may need stronger regularization
  - Rankings from θt and θt-1 are completely uncorrelated: Previous model is not a good baseline

- First 3 experiments:
  1. Implement the basic SRLM pipeline (sampling, ranking, DPO training) to establish baseline performance
  2. Add Kendall's Tau consistency calculation between consecutive iterations and log the values
  3. Implement the C-weighted mixing of normal and reversed DPO losses, starting with fixed C values (e.g., 0.5) before implementing dynamic C calculation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the consistency rate in CREAM evolve across multiple iterations, and what is its impact on the model's performance?
- Basis in paper: [explicit] The paper discusses the use of Kendall's Tau coefficient to measure consistency between iterations and mentions the evolution of consistency rates in Table 7.
- Why unresolved: While the paper provides initial observations, a deeper analysis of how consistency rates change over multiple iterations and their correlation with performance metrics is not fully explored.
- What evidence would resolve it: Detailed analysis of consistency rate trends across more iterations and their direct impact on performance metrics would clarify this relationship.

### Open Question 2
- Question: Can CREAM be effectively applied to larger language models beyond the 7B and 13B models tested in the study?
- Basis in paper: [explicit] The paper mentions that CREAM is theoretically applicable to models of any size but primarily tests it on 7B and 13B models due to computational constraints.
- Why unresolved: The paper does not provide experimental evidence for CREAM's effectiveness on larger models, leaving its scalability untested.
- What evidence would resolve it: Conducting experiments with larger models, such as 30B or 70B, and comparing their performance with baseline methods would demonstrate CREAM's scalability.

### Open Question 3
- Question: How does the choice of baseline reward model (BRM) affect the performance of CREAM, especially when an external reward model is unavailable?
- Basis in paper: [explicit] The paper discusses using the last iteration's model as the BRM and compares it with an external reward model in Table 3.
- Why unresolved: The paper provides initial insights but does not explore the impact of different BRMs on CREAM's performance comprehensively.
- What evidence would resolve it: Comparative experiments using various BRMs, including different iterations or models, would clarify the optimal choice for maximizing CREAM's effectiveness.

## Limitations
- The effectiveness depends on the assumption that model rankings become more consistent over time; if performance degrades between iterations, the consistency signal becomes unreliable
- The method assumes Kendall's Tau is an appropriate measure of ranking quality, which may not capture all aspects of preference reliability
- The regularization effect depends on having sufficient variability in the consistency rate; if rankings are consistently highly correlated, the regularization may be minimal

## Confidence

**High confidence**: The core mechanism of using consistency between iterations as a regularization signal is well-founded and mathematically coherent. The equivalence to label smoothing provides theoretical justification for preventing overconfidence.

**Medium confidence**: The claim that CREAM significantly improves alignment performance over multiple iterations. While the experimental results show improvements, the evaluation is limited to a specific set of benchmarks and model sizes (Llama-2/3 7B). The generalizability to other model families, sizes, and tasks remains to be established.

**Medium confidence**: The assertion that CREAM prevents the model from becoming overconfident in distinguishing responses of similar quality. This is supported by the theoretical framework and design, but empirical validation of the overconfidence mitigation specifically (rather than overall performance) would strengthen this claim.

## Next Checks
1. **Stability analysis**: Monitor ranking consistency metrics (Kendall's Tau, C values) across multiple self-rewarding iterations to verify that the model's rankings become more stable over time, not just in absolute performance.

2. **Ablation on consistency signal**: Implement variants that use different consistency baselines (e.g., external reward model, ensemble of past models) to test whether the previous iteration baseline is optimal for capturing reliable preference data.

3. **Overconfidence quantification**: Design experiments that specifically measure the model's confidence in ranking responses of similar quality (e.g., by injecting near-duplicate responses or responses with minimal quality differences) to directly validate the overconfidence prevention mechanism.