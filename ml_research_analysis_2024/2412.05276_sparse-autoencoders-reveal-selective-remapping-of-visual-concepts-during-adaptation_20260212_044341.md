---
ver: rpa2
title: Sparse autoencoders reveal selective remapping of visual concepts during adaptation
arxiv_id: '2412.05276'
source_url: https://arxiv.org/abs/2412.05276
tags:
- latent
- latents
- clip
- adaptation
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PatchSAE, a sparse autoencoder for CLIP Vision
  Transformers that extracts interpretable visual concepts with spatial attributions
  at patch-level granularity. The method trains SAE on CLIP's frozen representations
  to discover monosemantic concepts, enabling localized interpretation of what visual
  features the model recognizes and where.
---

# Sparse autoencoders reveal selective remapping of visual concepts during adaptation

## Quick Facts
- arXiv ID: 2412.05276
- Source URL: https://arxiv.org/abs/2412.05276
- Reference count: 40
- Sparse autoencoders (SAEs) extract interpretable visual concepts from CLIP ViTs and reveal that adaptation primarily remaps existing concepts to downstream classes rather than introducing new ones.

## Executive Summary
This paper introduces PatchSAE, a sparse autoencoder for CLIP Vision Transformers that extracts interpretable visual concepts with spatial attributions at patch-level granularity. The method trains SAE on CLIP's frozen representations to discover monosemantic concepts, enabling localized interpretation of what visual features the model recognizes and where. Through extensive experiments on 11 datasets, the authors demonstrate that (1) SAE latents contain class-discriminative information crucial for zero-shot classification, and (2) prompt-based adaptation methods like MaPLe primarily improve performance by remapping existing concepts to downstream classes rather than introducing new ones, with only one dataset (EuroSAT) showing notable concept addition due to domain shift.

## Method Summary
PatchSAE operates by hooking into a pretrained CLIP ViT-B/16 model and training a sparse autoencoder on its frozen token representations. The SAE uses an expansion factor of 64 to map dense activations to sparse latents, with L1 regularization (λl1=8e-5) enforcing sparsity. The method processes all image tokens (not just CLS) to enable patch-level concept attribution. For analysis, the authors compute summary statistics (sparsity, mean activation, label entropy, label standard deviation) and extract reference images for each latent. They also conduct top-k SAE latent masking experiments on both CLIP and MaPLe models across 11 downstream datasets to evaluate concept discriminativeness and adaptation mechanisms.

## Key Results
- SAE latents contain class-discriminative information essential for zero-shot classification, with top-k masking experiments showing measurable accuracy drops when masking class-specific latents
- Adaptation methods like MaPLe primarily improve performance by remapping existing concepts to downstream classes rather than introducing new concepts, with only EuroSAT showing notable concept addition due to domain shift
- Patch-level SAE latents provide spatially localized concept attribution that enables granular interpretation of visual features across different image regions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PatchSAE discovers interpretable, monosemantic visual concepts by decomposing dense vision transformer activations into sparse latent representations
- Mechanism: The sparse autoencoder maps high-dimensional, polysemantic activations from CLIP's vision transformer into a sparse latent space where each dimension corresponds to a distinct, interpretable visual concept
- Core assumption: Visual concepts in CLIP are represented in superposition and can be disentangled through sparse reconstruction with appropriate regularization
- Evidence anchors:
  - [abstract]: "SAEs map dense model representations, which are difficult to interpret because multiple unrelated concepts are entangled (polysemantic), to sparse and interpretable (monosemantic) concepts"
  - [section 3.1]: "To overcome the superposition phenomenon in neural network interpretation, sparse autoencoders (SAEs) have recently gained significant attention"
  - [corpus]: Weak evidence - related papers focus on SAEs for interpretability but don't specifically validate the superposition claim for vision models
- Break condition: If the latent space becomes dense (many latents activate for each input) or if reconstructed outputs fail to match inputs, the decomposition fails and concepts become uninterpretable

### Mechanism 2
- Claim: Adaptation primarily works by remapping existing concepts to downstream classes rather than introducing new concepts
- Mechanism: Prompt-based adaptation methods like MaPLe add learnable tokens that modify the mapping between activated SAE latents and class predictions
- Core assumption: The foundation model already contains sufficient conceptual representations for downstream tasks, and adaptation mainly adjusts the concept-to-class mapping
- Evidence anchors:
  - [abstract]: "we find that the majority of gains on common adaptation tasks can be explained with the existing concepts already present in the non-adapted foundation model"
  - [section 4.2.2]: "The first six columns of Table 1 show the normalized count of top activating SAE latents in three groups: high, high-to-low, and low-to-high... In most cases, SAE latents rarely place in off-diagonal regions"
  - [corpus]: Weak evidence - related work on concept unlearning exists but doesn't directly test this remapping hypothesis
- Break condition: If adaptation consistently introduces new high-frequency, high-activation latents across datasets, or if performance gains cannot be explained by concept remapping

### Mechanism 3
- Claim: Patch-level SAE latents provide spatially localized concept attribution that enables granular interpretation of visual features
- Mechanism: By processing individual image tokens through the SAE, the method creates patch-level activation maps that show exactly where specific concepts are recognized within an image
- Core assumption: Vision transformers represent spatial information in a way that preserves local feature relationships through token processing
- Evidence anchors:
  - [abstract]: "PatchSAE extracts interpretable concepts and their patch-wise spatial attributions, providing localized understanding of multiple visual concepts that can be simultaneously captured from different regions of a single image"
  - [section 3.2]: "The token-wise (i.e., patch-wise) investigation allows spatially localized understandings of an image"
  - [corpus]: Weak evidence - while related papers discuss SAEs for vision, none specifically demonstrate patch-level spatial attribution for concept localization
- Break condition: If patch-level activations don't correspond to meaningful spatial regions or if the aggregation process loses important spatial relationships

## Foundational Learning

- Concept: Sparse Autoencoders (SAEs) and their role in mechanistic interpretability
  - Why needed here: SAEs are the core technical innovation that enables interpretable concept extraction from dense neural network representations
  - Quick check question: How does L1 regularization in SAEs promote sparsity in the latent space, and why is this sparsity important for interpretability?

- Concept: Vision Transformer (ViT) architecture and token processing
  - Why needed here: PatchSAE operates on ViT outputs at the token level, so understanding how ViTs process spatial information into tokens is crucial
  - Quick check question: What is the relationship between ViT patches and tokens, and how does this relationship enable patch-level concept attribution?

- Concept: Foundation model adaptation and prompt-based methods
  - Why needed here: The paper's second major contribution is analyzing how adaptation works, specifically through prompt-based methods like MaPLe
  - Quick check question: How do prompt-based adaptation methods differ from fine-tuning in terms of parameter updates and computational efficiency?

## Architecture Onboarding

- Component map: CLIP ViT backbone -> SAE encoding -> Sparse latent activation -> Concept interpretation/reference images -> Aggregation for analysis -> Adaptation module (MaPLe) -> Final predictions
- Critical path: input image -> ViT hook layer output -> SAE encoding -> SAE latent activation -> concept interpretation/reference images -> aggregation for analysis
- Design tradeoffs: The expansion factor (64x) creates a very high-dimensional latent space that enables fine-grained concept discovery but increases computational cost and makes the system more prone to overcomplete representations
- Failure signatures: (1) SAE latents showing no interpretable patterns in reference images, (2) very low sparsity metrics indicating dense activations, (3) poor reconstruction accuracy preventing concept discovery, (4) adaptation showing minimal performance improvement despite concept remapping
- First 3 experiments:
  1. Train a simple SAE on frozen ViT outputs from a small dataset and verify reconstruction accuracy and sparsity metrics meet target values (MSE < 0.01, L0 < 50)
  2. Visualize reference images for top-10 latents to confirm interpretable concepts are discovered and test with dataset shift cases
  3. Implement top-k masking experiment on a single downstream task to verify SAE latents contain class-discriminative information by showing accuracy drops when masking class-specific latents

## Open Questions the Paper Calls Out

- Does the concept-level analysis reveal whether adaptation methods like MaPLe improve perceptual ability by discovering new visual concepts, or merely remap existing concepts to new classes?
- How transferable are SAE-discovered concepts across different vision transformer architectures and scales?
- What is the relationship between SAE-discovered concept granularity and task performance in fine-grained classification?

## Limitations

- The interpretation of SAE latents relies heavily on visual inspection of reference images, introducing subjectivity
- The study focuses exclusively on CLIP ViT-B/16, limiting generalizability to other architectures
- The adaptation analysis assumes MaPLe represents typical prompt-based methods, but other approaches might behave differently

## Confidence

- **High Confidence**: The empirical finding that SAE latents contain class-discriminative information (validated through top-k masking experiments showing measurable accuracy drops when masking latents)
- **Medium Confidence**: The claim that adaptation primarily remaps existing concepts rather than introducing new ones (supported by activation pattern analysis but could vary with different adaptation methods)
- **Medium Confidence**: The interpretation of reference images as evidence of monosemantic concepts (visually convincing but subject to confirmation bias)

## Next Checks

1. **Cross-Architecture Validation**: Train PatchSAE on a different vision transformer architecture (e.g., ConvNeXt) and verify that the same patterns of concept remapping during adaptation hold, addressing the architecture-specificity limitation.

2. **Controlled Concept Addition Experiment**: Design an experiment where a synthetic downstream dataset requires concepts provably absent from the foundation model, then test whether adaptation can introduce these concepts or if it remains constrained to remapping existing ones.

3. **Quantitative Interpretability Metric**: Develop and apply a quantitative metric for SAE latent interpretability (beyond reconstruction accuracy and sparsity) to objectively measure the quality of concept decomposition across different λl1 values and expansion factors.