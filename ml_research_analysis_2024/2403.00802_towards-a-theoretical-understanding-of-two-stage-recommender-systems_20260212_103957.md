---
ver: rpa2
title: Towards a Theoretical Understanding of Two-Stage Recommender Systems
arxiv_id: '2403.00802'
source_url: https://arxiv.org/abs/2403.00802
tags:
- recommender
- systems
- system
- users
- two-stage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the theoretical convergence behavior of two-stage
  recommender systems, specifically the two-tower architecture. The key insight is
  that the convergence rate of the two-tower model is influenced by the smoothness
  of the underlying true model and the intrinsic dimensionality of user/item features.
---

# Towards a Theoretical Understanding of Two-Stage Recommender Systems

## Quick Facts
- arXiv ID: 2403.00802
- Source URL: https://arxiv.org/abs/2403.00802
- Reference count: 40
- Two-tower recommender system achieves up to 81.3% RMSE improvement over traditional methods in sparse, high-dimensional settings

## Executive Summary
This paper provides a theoretical analysis of two-stage recommender systems, specifically the two-tower architecture, proving that such systems converge robustly to the optimal recommender system. The key insight is that convergence rate depends on the smoothness of the underlying true model (Hölder continuity) and the intrinsic dimensionality of user/item features. The authors demonstrate that their theoretical bounds on convergence rates are tighter than existing results, and validate these findings through experiments on both synthetic and real-world datasets (Yelp), where the two-tower approach (T2Rec) significantly outperforms traditional methods like SVD, SVD++, KNN, and co-clustering.

## Method Summary
The paper analyzes a two-tower neural network architecture where user and item features are mapped through separate deep networks into a shared low-dimensional embedding space, with predictions made via dot product of embeddings. The theoretical analysis proves convergence rates under assumptions of Hölder smoothness (exponent β) and low intrinsic Minkowski dimension for feature distributions. Empirically, the authors implement T2Rec using TensorFlow with 5-layer fully-connected networks (50 neurons per hidden layer, 30 in output), trained via SGD with learning rate decay and early stopping on both synthetic data (100k ratings, 50D features) and preprocessed Yelp datasets, comparing against baseline methods including rSVD, SVD++, Co-Ca, and KNN.

## Key Results
- Theoretical proof that two-tower model converges robustly to optimal recommender with convergence rate faster than most existing theoretical results
- Convergence rate explicitly depends on smoothness parameter β and intrinsic dimensions du, di of user/item features
- Empirical results show up to 81.3% RMSE improvement over baselines (SVD, SVD++, KNN, co-clustering) on both synthetic and Yelp datasets
- Superior performance particularly in sparse and high-dimensional settings, with cold-start mitigation through feature-based encoding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-tower model converges robustly to the optimal recommender system because it leverages the smoothness of the true model and intrinsic dimensionality of user/item features.
- Mechanism: Smoothness (Hölder continuity) allows low-dimensional approximation; intrinsic dimension controls parameter scaling; together they bound approximation and estimation errors.
- Core assumption: The true user-item preference function f* is in a Hölder space with exponent β, and the input feature distributions have low intrinsic Minkowski dimension.
- Evidence anchors:
  - [abstract] "...convergence rate of the two-tower model is influenced by the smoothness of the underlying true model and the intrinsic dimensionality of user/item features."
  - [section] Theorem 5.1 and Theorem 5.5 explicitly tie approximation error to β and intrinsic dimensions du, di.
  - [corpus] No direct corpus evidence; corpus neighbors focus on LLM/explainable recsys, not theoretical convergence.
- Break condition: If the true model is not smooth (β low) or the intrinsic dimensions are high, the convergence rate degrades toward slower polynomial rates.

### Mechanism 2
- Claim: The two-tower model achieves faster convergence than most existing theoretical results because the latent embeddings reduce the effective parameter space.
- Mechanism: User and item embeddings collapse high-dimensional sparse features into a lower-dimensional space; this reduces the covering number (entropy) of the hypothesis class, leading to a smaller estimation error.
- Core assumption: Embedding dimensions are sufficiently low relative to the raw feature space, and the embedding network can approximate the true functions within the Hölder space.
- Evidence anchors:
  - [abstract] "...convergence rate being faster than most existing theoretical results."
  - [section] Lemma 5.4 bounds the bracketing entropy using the number of parameters, showing that W = O(|Ω|dui/(2β+dui) log|Ω|) controls the complexity.
  - [corpus] No corpus support; neighbors are LLM-based, not theoretical.
- Break condition: If the embedding dimensionality p is set too high or the network capacity too large, the entropy bound increases and convergence slows.

### Mechanism 3
- Claim: The two-tower model mitigates cold-start problems by encoding new users/items via their covariate representations.
- Mechanism: The dot-product scoring between f(xu) and f̃(̃xi) can be computed even when there are no interaction histories, as long as covariates exist; this avoids reliance on user-item co-occurrence.
- Core assumption: User/item covariates are available and informative; the embedding networks generalize well to unseen users/items.
- Evidence anchors:
  - [abstract] "...two-tower model offers a significant advantage in its ability to address the well-established cold-start problem by integrating features of both users and items..."
  - [section] Experimental results show T2Rec outperforms baselines in sparse and high-dimensional settings.
  - [corpus] No corpus evidence; neighbors are LLM-based.
- Break condition: If covariate information is missing or uninformative for new entities, the cold-start benefit vanishes.

## Foundational Learning

- Concept: Hölder space and smoothness (Hölder continuity)
  - Why needed here: Provides the mathematical framework for bounding approximation error when using neural networks to model user/item functions.
  - Quick check question: If a function f has smoothness β = 2 and is defined on [0,1]^D, what does the Hölder norm ∥f∥H(β,[0,1]^D) measure?
- Concept: Intrinsic dimension and Minkowski dimension
  - Why needed here: Determines how many parameters are needed to approximate the true model; lower intrinsic dimension → faster convergence.
  - Quick check question: If Supp(µu) is a d-dimensional manifold embedded in R^Du, what is its upper Minkowski dimension?
- Concept: Bracketing entropy and empirical process theory
  - Why needed here: Used to bound the estimation error of the learned model; controls how fast the empirical risk minimizer converges.
  - Quick check question: In Lemma 5.4, what is the relationship between the bracketing entropy bound and the parameters W, L, B?

## Architecture Onboarding

- Component map:
  - User features (R^Du) -> User tower (f: deep net, ReLU, L layers) -> User embedding (R^p)
  - Item features (R^Di) -> Item tower (f̃: deep net, ReLU, L̃ layers) -> Item embedding (R^p)
  - User embedding • Item embedding -> Predicted rating
  - Observed rating + Predicted rating -> Squared error loss
  - Loss + L2 regularization -> SGD optimization
- Critical path:
  1. Embed users and items in shared low-dim space
  2. Compute dot product for rating prediction
  3. Backpropagate loss to update both towers jointly
- Design tradeoffs:
  - Width vs depth: Wider towers capture more complex feature interactions; deeper towers capture hierarchical representations. Tradeoff: wider increases parameters and entropy; deeper increases non-convexity.
  - Embedding size p: Larger p gives more expressive power but higher overfitting risk and slower convergence.
  - Regularization λ: Controls overfitting; too low → high variance; too high → underfitting.
- Failure signatures:
  - Slow convergence: High intrinsic dimension or low smoothness β.
  - Poor cold-start: Missing or noisy covariate features.
  - Overfitting: λ too low or embedding size too large relative to data.
- First 3 experiments:
  1. Vary β (simulate by controlling signal smoothness in synthetic data) and measure RMSE convergence.
  2. Vary intrinsic dimension d (by controlling how many features are actually informative) and measure RMSE.
  3. Compare T2Rec vs SVD++ on a real sparse dataset with and without cold-start splits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence rate of the two-tower recommender system scale with the number of layers (L) in the neural networks?
- Basis in paper: [explicit] The paper states that "Theorem 5.1 remains valid irrespective of the value of L, indicating that the approximation error of the two-tower model can converge to zero with any number of layers."
- Why unresolved: The paper does not provide explicit bounds or analysis on how the convergence rate specifically depends on the number of layers L. While it mentions that the approximation error can converge to zero with any number of layers, it doesn't quantify the rate of convergence as a function of L.
- What evidence would resolve it: A detailed theoretical analysis showing the explicit dependence of the convergence rate on the number of layers L, potentially including upper bounds or asymptotic expressions. Experimental results demonstrating the convergence behavior for different values of L would also be valuable.

### Open Question 2
- Question: How robust is the two-tower recommender system to noise and outliers in the user-item rating data?
- Basis in paper: [inferred] The paper assumes sub-Gaussian noise bounded by Be in the rating generation model (Equation 3), but does not explicitly analyze the robustness of the system to different types of noise or outliers.
- Why unresolved: The theoretical analysis assumes a specific noise model (sub-Gaussian with bounded variance), but real-world data often contains various types of noise and outliers. The paper does not investigate how the system performs under more general noise conditions or in the presence of outliers.
- What evidence would resolve it: A comprehensive analysis of the system's performance under different noise models (e.g., heavy-tailed distributions, heteroscedastic noise) and in the presence of outliers. This could include both theoretical bounds on performance degradation and experimental results on real-world datasets with known noise characteristics.

### Open Question 3
- Question: How does the two-tower recommender system perform in scenarios with non-linear relationships between user/item features and ratings that are not captured by the Hölder space assumption?
- Basis in paper: [explicit] The paper assumes that the true model functions f* and f~ are in the Hölder space H(β, [0,1]^D, M), which implies a certain level of smoothness in the relationship between features and ratings.
- Why unresolved: The theoretical analysis is based on the assumption of Hölder smoothness, but real-world relationships between features and ratings may exhibit more complex non-linearities that are not well-captured by this assumption. The paper does not explore the performance of the system in scenarios with more general non-linear relationships.
- What evidence would resolve it: A theoretical analysis extending the convergence results to more general function spaces or non-linear relationships between features and ratings. Experimental results demonstrating the system's performance on datasets with known non-linear relationships that violate the Hölder smoothness assumption would also be informative.

## Limitations

- Theoretical analysis relies on idealized assumptions about Hölder smoothness and low intrinsic dimensionality that may not hold in real-world recommendation scenarios
- Convergence proofs assume infinite data and perfect optimization, which may not reflect practical implementation constraints
- Experimental validation focuses primarily on sparse settings rather than truly cold-start scenarios with zero interactions

## Confidence

- High confidence: The two-tower architecture's ability to handle sparse and high-dimensional data, demonstrated by the significant RMSE improvement (up to 81.3%) on both synthetic and Yelp datasets
- Medium confidence: The theoretical convergence rate claims, as they depend on specific smoothness and dimensionality assumptions that may not generalize across all recommendation scenarios
- Medium confidence: The cold-start advantage, as the experiments show performance in sparse settings but don't specifically isolate cold-start scenarios with completely new users/items

## Next Checks

1. Test T2Rec's performance on a truly cold-start scenario where new users/items have zero interaction history but possess covariate features, comparing against strong cold-start baselines
2. Evaluate the sensitivity of convergence rates to violations of smoothness assumptions by introducing non-smooth preference patterns in synthetic data and measuring performance degradation
3. Conduct ablation studies varying the embedding dimension p and network depth/width to quantify their impact on both convergence speed and final recommendation accuracy