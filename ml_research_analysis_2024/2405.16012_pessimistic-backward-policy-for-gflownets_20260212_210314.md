---
ver: rpa2
title: Pessimistic Backward Policy for GFlowNets
arxiv_id: '2405.16012'
source_url: https://arxiv.org/abs/2405.16012
tags:
- flow
- backward
- policy
- trajectories
- observed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a key limitation in GFlowNets: they tend
  to under-exploit high-reward objects when training on insufficient trajectories,
  leading to a mismatch between estimated flow and true rewards. To address this,
  the authors propose a Pessimistic Backward Policy for GFlowNets (PBP-GFN), which
  maximizes the observed backward flow to align it more closely with the true reward.'
---

# Pessimistic Backward Policy for GFlowNets

## Quick Facts
- arXiv ID: 2405.16012
- Source URL: https://arxiv.org/abs/2405.16012
- Reference count: 40
- Key outcome: PBP-GFN consistently improves performance across eight benchmarks by enhancing discovery of high-reward objects while maintaining diversity

## Executive Summary
This paper addresses a critical limitation in GFlowNets: under-exploitation of high-reward objects when training on insufficient trajectories. The authors propose a Pessimistic Backward Policy for GFlowNets (PBP-GFN) that maximizes observed backward flow to better align with true rewards, effectively shifting unobserved backward flow into observed flow. Through extensive experiments across hyper-grid environments, molecular generation, and RNA sequence tasks, PBP-GFN demonstrates consistent performance improvements by resolving the under-exploitation problem while maintaining asymptotic optimality guarantees.

## Method Summary
PBP-GFN works by pessimistically training the backward policy to maximize the likelihood of observed trajectories stored in a replay buffer. This increases the proportion of observed backward flow for high-reward objects while preserving the total backward flow per object, thus maintaining the flow matching property. The method optimizes the backward policy to reduce unobserved backward flow, which naturally improves error bounds in estimating the Boltzmann distribution. The approach is implemented through trajectory likelihood maximization followed by standard trajectory balance training of the forward policy.

## Key Results
- PBP-GFN consistently improves mode discovery across eight benchmark tasks
- The method enhances exploitation of high-reward objects while maintaining diversity
- Outperforms existing methods in both hyper-grid environments and real-world applications like molecular and RNA sequence generation
- Demonstrates improved error bounds in estimating the target Boltzmann distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PBP-GFN resolves under-exploitation by maximizing observed backward flow to better align with true rewards
- Mechanism: The pessimistic backward policy increases the probability of backward transitions along observed trajectories, effectively shifting unobserved backward flow into observed flow
- Core assumption: Observed trajectories can be leveraged to infer the distribution of unobserved trajectories through the flow matching objective
- Evidence anchors: [abstract] "maximizes the observed flow to align closely with the true reward"; [section 3.2] "backward policy is trained to reduce the amount of unobserved backward flow"
- Break condition: If observed trajectories are too sparse or unrepresentative of the true reward distribution, maximizing their backward flow may not improve overall reward alignment

### Mechanism 2
- Claim: PBP-GFN maintains asymptotic optimality while improving exploitation
- Mechanism: By only modifying relative backward trajectory flows among trajectories inducing the same object, the algorithm preserves the flow matching property ensuring convergence to the target Boltzmann distribution
- Core assumption: Total backward flow for each object remains equal to its reward during pessimistic training
- Evidence anchors: [section 3.2] "only modifies the relative backward trajectory flows among trajectories inducing the same object"; [corpus] "Looking Backward: Retrospective Backward Synthesis for Goal-Conditioned GFlowNets"
- Break condition: If pessimistic training significantly distorts backward policy beyond just relative flows, it may break asymptotic optimality guarantee

### Mechanism 3
- Claim: PBP-GFN improves error bounds in estimating the Boltzmann distribution
- Mechanism: By maximizing observed forward and backward flows, unobserved flows are naturally minimized, leading to tighter error bounds
- Core assumption: Error in flow matching is inversely related to the proportion of observed flow
- Evidence anchors: [section 3.2] "maximizing the observed forward and backward flows naturally minimizes the unobserved forward and backward flows"; [appendix A] provides mathematical derivation
- Break condition: If observed flow is not representative of true reward structure, minimizing unobserved flows may not lead to better estimates

## Foundational Learning

- Concept: Flow matching in GFlowNets
  - Why needed here: Essential to grasp why under-exploitation occurs and how PBP-GFN addresses it
  - Quick check question: What is the relationship between forward flow, backward flow, and the target Boltzmann distribution in GFlowNets?

- Concept: Trajectory decomposition and marginalization
  - Why needed here: PBP-GFN operates by modifying backward flows over trajectories, so understanding how trajectories decompose into objects is crucial
  - Quick check question: How does the backward policy decompose the reward into backward flows over trajectories?

- Concept: Credit assignment in generative models
  - Why needed here: PBP-GFN improves credit assignment by focusing on observed trajectories, which is a key aspect of its mechanism
  - Quick check question: How does traditional credit assignment in GFlowNets differ from the approach taken by PBP-GFN?

## Architecture Onboarding

- Component map:
  Forward policy (PF) -> Replay buffer (B) -> Pessimistic backward policy trainer -> Backward policy (PB) -> Flow matching objective -> Forward policy (PF)

- Critical path:
  1. Sample trajectories using the forward policy
  2. Store trajectories in the replay buffer
  3. Train the pessimistic backward policy to maximize observed backward flow
  4. Use the updated backward policy in flow matching to train the forward policy
  5. Repeat until convergence

- Design tradeoffs:
  - Exploitation vs. exploration: PBP-GFN focuses on exploitation of observed high-reward trajectories, potentially at expense of exploring new regions
  - Computational overhead: Training pessimistic backward policy adds computational cost but can improve convergence
  - Buffer size and composition: Effectiveness depends on having representative set of observed trajectories

- Failure signatures:
  - Forward policy converges to distribution underrepresenting high-reward objects despite their presence in buffer
  - Pessimistic backward policy training leads to instability or divergence
  - Algorithm fails to discover new modes not represented in initial buffer

- First 3 experiments:
  1. Reproduce hyper-grid environment results to verify mode discovery and distribution learning
  2. Test on bag generation task to confirm improved exploitation of high-reward objects
  3. Validate on molecular generation to ensure diverse high-reward molecule discovery is maintained

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PBP-GFN performance compare when combined with other exploitation methods like local search or reward-prioritized buffers?
- Basis in paper: [explicit] Section 5.4 mentions PBP-GFN was compared with local search and reward-prioritized buffer methods
- Why unresolved: Paper provides results for one specific task (RNA-C) in Figure 11 but doesn't explore combination across all benchmarks
- What evidence would resolve it: Additional experiments showing PBP-GFN performance when combined with local search and reward-prioritized buffers across all eight benchmarks with statistical significance tests

### Open Question 2
- Question: What is the impact of varying the learning rate for the pessimistic backward policy on PBP-GFN performance?
- Basis in paper: [explicit] Section 5 mentions learning rate set to 1e-3 but doesn't explore sensitivity to this hyperparameter
- Why unresolved: Paper doesn't provide ablation study on learning rate for pessimistic backward policy
- What evidence would resolve it: Ablation study varying learning rate across different benchmarks showing impact on performance and tradeoff between exploitation and exploration

### Open Question 3
- Question: How does PBP-GFN perform in environments where exploration is more significant than exploitation?
- Basis in paper: [inferred] Section 6 mentions PBP-GFN makes trade-off between high-reward and diversified trajectories, which may not hold for environments where exploration is significant
- Why unresolved: Paper doesn't provide experimental results for environments where exploration is more important than exploitation
- What evidence would resolve it: Experiments on environments requiring significant exploration (sparse rewards, optimal policy requires exploration) comparing PBP-GFN with exploration-focused methods

## Limitations
- Reliance on observed trajectories being representative of true reward structure - may fail when replay buffer lacks coverage of high-reward regions
- Computational overhead of training pessimistic backward policy could be prohibitive for very large state spaces
- May reduce exploration if unobserved high-reward trajectory overlaps significantly with observed low-reward trajectory

## Confidence

**High confidence:** Mechanism of maximizing observed backward flow to reduce under-exploitation is well-supported by both theoretical analysis and experimental results

**Medium confidence:** Asymptotic optimality preservation claim, while theoretically justified, requires careful implementation to ensure backward policy modifications remain within specified bounds

**Medium confidence:** Error bound improvement claim depends on quality and representativeness of observed trajectories, which may vary across different domains

## Next Checks

1. Test PBP-GFN on synthetic environment with clearly separable high-reward regions to verify ability to discover modes when not well-represented in initial buffer

2. Evaluate computational overhead of PBP-GFN training compared to baseline methods across different state space sizes to quantify tradeoff between performance and resource usage

3. Conduct ablation studies removing pessimistic backward policy training to measure its specific contribution to performance improvements in various benchmark tasks