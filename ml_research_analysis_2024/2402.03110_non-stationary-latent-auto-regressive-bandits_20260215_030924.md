---
ver: rpa2
title: Non-Stationary Latent Auto-Regressive Bandits
arxiv_id: '2402.03110'
source_url: https://arxiv.org/abs/2402.03110
tags:
- latent
- algorithm
- regret
- reward
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel non-stationary multi-armed bandit
  formulation called the latent auto-regressive (AR) bandit, where reward mean changes
  are driven by an unknown, latent AR state process. The authors reduce this problem
  to a stochastic linear bandit with context corruptions, leveraging the linear structure
  of the AR process and reward function.
---

# Non-Stationary Latent Auto-Regressive Bandits

## Quick Facts
- arXiv ID: 2402.03110
- Source URL: https://arxiv.org/abs/2402.03110
- Authors: Anna L. Trella; Walter Dempsey; Asim H. Gazi; Ziping Xu; Finale Doshi-Velez; Susan A. Murphy
- Reference count: 40
- One-line primary result: LARL achieves O(k√T) regret when AR order k is known, outperforming standard UCB in non-stationary latent AR bandit environments.

## Executive Summary
This paper introduces the latent auto-regressive (AR) bandit, a non-stationary multi-armed bandit where reward means change due to an unknown latent AR state process. The authors reduce this problem to a stochastic linear bandit with context corruptions, leveraging the linear structure of the AR process. They propose LARL, a computationally efficient algorithm that implicitly predicts the latent state and learns system parameters online. LARL achieves O(k√T) regret when the AR order k is known, provided the latent state noise variance is sufficiently small relative to T. Empirical results show LARL outperforms standard UCB across various non-stationary environments, even with mis-specified AR order.

## Method Summary
The method involves modeling the latent AR bandit as a linear bandit with context corruptions by explicitly including past k rewards and actions as context. LARL maintains a ridge regression estimator with confidence sets, selects actions via UCB, and updates using observed rewards and predicted reward noises. The algorithm implicitly predicts the latent state through noise estimation, allowing it to handle non-stationary rewards without explicitly estimating the AR state.

## Key Results
- LARL achieves O(k√T) regret when AR order k is known and latent state noise is small
- LARL outperforms standard UCB across various non-stationary environments
- Performance degrades with mis-specified AR order (k0 < k or k0 >> k)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The latent AR bandit problem can be reduced to a linear bandit with context corruptions by explicitly modeling the past k rewards and actions as context and treating the unobserved noise terms as corruptions.
- Mechanism: By rewriting the reward equation to depend linearly on the last k observed rewards and actions, the algorithm can maintain a linear model over a context vector that includes these past observations. The unobserved reward noises are incorporated into the context, turning them into "corruptions" rather than noise in the linear bandit framework.
- Core assumption: The reward depends linearly on the past k rewards and actions, and the unobserved noises can be bounded or predicted with sublinear error.
- Evidence anchors:
  - [abstract]: "We instead consider the non-stationary bandit problem where the reward means change due to a latent, auto-regressive (AR) state."
  - [section 3.4.1]: "Using Lemma 3.3, we re-write the reward for a fixed action a into the linear form of a standard un-corrupted linear bandit."
  - [corpus]: Weak anchor. No direct mention of context corruption reduction.
- Break condition: If the AR order k is mis-specified or the noise prediction error is not sublinear, the corruption budget grows and the regret bound degrades.

### Mechanism 2
- Claim: The algorithm achieves sublinear regret because the noise term satisfies a martingale difference sequence with bounded conditional subgaussian norm, ensuring concentration.
- Mechanism: The noise term in the linear bandit formulation (combining current reward noise and AR state noise) is shown to be a martingale difference sequence with bounded conditional subgaussian norm. This allows the use of standard concentration inequalities to maintain a confidence set around the true parameter.
- Core assumption: The reward and AR state noises are independent and subgaussian, and the sequence of actions does not leak information about future noises.
- Evidence anchors:
  - [section 3.4.2]: "We show in Lemma 3.4 that the error term from Equation 10 satisfies Assumption 3.2."
  - [section 4.3]: "We now derive a regret bound for Algorithm 4.1."
  - [corpus]: Weak anchor. No explicit mention of martingale difference or concentration.
- Break condition: If the noise process violates independence or subgaussianity, the concentration bounds fail and the regret guarantee is lost.

### Mechanism 3
- Claim: The algorithm's performance is robust to mis-specified AR order k0 if k0 ≥ k, but degrades if k0 < k because the model is under-parameterized.
- Mechanism: When k0 ≥ k, the algorithm includes extra parameters that do not hurt performance but require more data to learn. When k0 < k, the model cannot capture the full dependence on past rewards, leading to larger context prediction errors and increased regret.
- Core assumption: The true AR order k is fixed and the algorithm uses a linear model of fixed order k0.
- Evidence anchors:
  - [section 6]: "If one chooses k0 < k , we expect Algorithm 4.1 to not perform as well because the reward model would also be mis-specified."
  - [section 6.1]: "Empirically, we see that an algorithm with either k0 too small or large leads to worse performance than the same algorithm who used a k0 close to k."
  - [corpus]: Weak anchor. No explicit discussion of model order misspecification.
- Break condition: If the true process is not AR or has a time-varying order, fixed-order modeling will fail regardless of k0.

## Foundational Learning

- Concept: Auto-regressive (AR) processes and their stability conditions (|sum of coefficients| < 1).
  - Why needed here: The latent state evolves as an AR process; stability ensures the state does not explode over time.
  - Quick check question: What condition on the AR coefficients guarantees the process is stationary?

- Concept: Linear bandit algorithms (LinUCB, ridge regression estimators, confidence sets).
  - Why needed here: The algorithm reduces the problem to a linear bandit with corruptions, so understanding confidence bounds and action selection is essential.
  - Quick check question: How does LinUCB select actions using the confidence set and estimated parameter?

- Concept: Martingale difference sequences and subgaussian noise.
  - Why needed here: The noise term must satisfy these properties to apply concentration inequalities for regret bounds.
  - Quick check question: What is the definition of a martingale difference sequence with respect to a filtration?

## Architecture Onboarding

- Component map:
  - Context builder: Constructs the 2k|A|+1 dimensional context vector from the last k rewards, actions, and noise estimates
  - Estimator: Maintains the ridge regression estimator and updates it with corrupted contexts and observed rewards
  - Action selector: Uses UCB on the confidence set to pick the next action
  - Predictor: Generates estimates of the last k reward noises to fill the context

- Critical path:
  1. At each step t, use noise estimates to form corrupted context ˆxt for all actions
  2. Update ridge regression estimator with (ˆxt, rt)
  3. Compute confidence set Ct
  4. Select action via UCB
  5. Observe reward and update noise estimates

- Design tradeoffs:
  - Larger k increases model expressiveness but also parameter dimension and data requirements
  - Noise estimation accuracy directly impacts corruption budget and regret
  - Using predictions for past noises trades off model accuracy for tractability

- Failure signatures:
  - Regret grows linearly if corruption budget is not sublinear (poor noise estimation)
  - Suboptimal performance if k0 << k (under-parameterization) or k0 >> k (over-parameterization)
  - Confidence set may not contain true parameter if noise assumptions violated

- First 3 experiments:
  1. Run with k0 = k and small σz to verify sublinear regret
  2. Run with k0 < k to observe degradation in performance
  3. Run with k0 > k to confirm slower learning but eventual convergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LARL change when the latent state process is not purely auto-regressive but includes additional non-linear dynamics?
- Basis in paper: [inferred] The paper assumes the latent state follows a linear AR process. It would be interesting to see how the algorithm performs when the underlying dynamics are more complex.
- Why unresolved: The paper focuses on linear AR processes and does not explore non-linear dynamics.
- What evidence would resolve it: Empirical results comparing LARL's performance on synthetic datasets with non-linear latent dynamics to other algorithms designed for non-linear settings.

### Open Question 2
- Question: What is the impact of the estimation procedure for the reward noise (ϵt) on the overall performance of LARL?
- Basis in paper: [explicit] The paper mentions that the algorithm uses estimates of the reward noise in the context, but leaves the exact estimator choice open. It states that any choice of estimator suffices as long as it satisfies certain assumptions.
- Why unresolved: The paper does not provide specific guidance on how to choose the reward noise estimator, leaving this as a practical consideration for future work.
- What evidence would resolve it: An empirical study comparing the performance of LARL using different reward noise estimation techniques (e.g., simple averaging, exponential smoothing, Kalman filtering) on a range of latent AR bandit environments.

### Open Question 3
- Question: How does LARL perform in settings where the latent state process is multi-dimensional or has mixed known and unknown components?
- Basis in paper: [inferred] The paper discusses extending the work to multi-dimensional latent states and states with mixed known and unknown components as future work.
- Why unresolved: The paper focuses on one-dimensional latent states and does not explore these more complex settings.
- What evidence would resolve it: Empirical results showing LARL's performance on synthetic datasets with multi-dimensional or mixed latent states, compared to other algorithms or oracle baselines.

## Limitations

- The analysis assumes the AR process noise can be accurately predicted with sublinear error, which is critical for maintaining the corruption budget but not explicitly bounded
- Performance degrades with mis-specified AR order, particularly when k0 < k (under-parameterization)
- The reduction to linear bandit with corruptions is stated but not rigorously justified in the main text

## Confidence

- High Confidence: The linear bandit reduction framework and the LARL algorithm structure are clearly defined and empirically validated
- Medium Confidence: The regret bound derivation assumes the noise process satisfies martingale difference properties, but the proof details are in supplementary material and key assumptions are not fully verified in the main text
- Medium Confidence: Empirical results show LARL outperforms UCB, but the performance gap depends strongly on the noise level σz, which is not thoroughly analyzed

## Next Checks

1. Verify the martingale difference assumption for the noise process by checking the independence and subgaussianity conditions explicitly
2. Implement LARL with different noise estimators and quantify the impact of noise prediction error on the corruption budget and regret
3. Run ablation studies with varying AR orders k0 to confirm the under/over-parameterization effects described in the paper