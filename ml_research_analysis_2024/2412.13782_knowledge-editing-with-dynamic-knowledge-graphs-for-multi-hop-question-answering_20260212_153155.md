---
ver: rpa2
title: Knowledge Editing with Dynamic Knowledge Graphs for Multi-Hop Question Answering
arxiv_id: '2412.13782'
source_url: https://arxiv.org/abs/2412.13782
tags:
- knowledge
- editing
- question
- edited
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KEDKG, a novel knowledge editing method for
  multi-hop question answering (MHQA) using dynamic knowledge graphs. The key innovation
  is constructing a dynamic knowledge graph to store edited facts and resolve conflicts
  during secondary editing, addressing limitations of existing approaches that struggle
  with inaccurate retrieval and knowledge conflicts.
---

# Knowledge Editing with Dynamic Knowledge Graphs for Multi-Hop Question Answering

## Quick Facts
- arXiv ID: 2412.13782
- Source URL: https://arxiv.org/abs/2412.13782
- Reference count: 13
- Multi-hop accuracy of 66.80% on MQUAKE-CF-3k, outperforming state-of-the-art methods

## Executive Summary
KEDKG introduces a novel approach for multi-hop question answering that addresses knowledge editing challenges through dynamic knowledge graphs. The method constructs a knowledge graph to store edited facts while resolving conflicts during secondary editing, significantly improving accuracy over existing approaches that struggle with inaccurate retrieval and knowledge conflicts. Using a fine-tuned LLM for question decomposition and entity/relation detectors for precise retrieval, KEDKG achieves substantial performance gains, particularly on complex 3-hop and 4-hop questions, demonstrating 108.1-83.9% improvements over baselines across different edit batch sizes.

## Method Summary
KEDKG addresses multi-hop question answering with knowledge editing by constructing a dynamic knowledge graph that stores edited facts and resolves conflicts during secondary editing. The approach uses a fine-tuned LLM to decompose multi-hop questions into sub-questions, then employs entity and relation detectors to retrieve relevant facts from the knowledge graph with high precision. When new edited knowledge is added, the system detects and resolves conflicts by removing old facts with matching subject and relation pairs before adding the new information. This fine-grained retrieval strategy, combined with conflict detection and modification, enables accurate answers even when knowledge bases contain edited or contradictory information.

## Key Results
- Achieves 66.80% multi-hop accuracy and 63.67% hop-wise accuracy on MQUAKE-CF-3k
- Outperforms state-of-the-art methods with 108.1-83.9% improvements across different edit batch sizes
- Particularly effective on 3-hop and 4-hop questions, maintaining strong performance even with black-box models like GPT-3.5-turbo-instruct

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic knowledge graphs enable conflict resolution during secondary editing
- **Mechanism:** When a new edited fact (s, r, o*) is introduced, the system checks for existing facts with the same subject and relation. If found, the old fact is removed before adding the new one, preventing contradictory information from co-existing
- **Core assumption:** Knowledge conflicts can be detected by matching (subject, relation) pairs
- **Evidence anchors:**
  - [abstract] "constructs a dynamic knowledge graph to store revised information while resolving potential knowledge conflicts"
  - [section] "For a new piece of edited knowledge enew = (s, r, o*), we locate the old edited knowledge eold = (s, r, o*old) based on the subject s and relation r, then remove it and add enew to the knowledge graph"

### Mechanism 2
- **Claim:** Fine-grained retrieval with entity and relation detectors improves accuracy over semantic similarity matching
- **Mechanism:** Instead of relying on semantic similarity to retrieve relevant facts, the system uses trained detectors to predict the probability that a given entity or relation matches the sub-question's requirements, only retrieving when confidence exceeds threshold α
- **Core assumption:** Entity and relation detectors can accurately identify relevant knowledge components better than semantic similarity alone
- **Evidence anchors:**
  - [abstract] "employs a fine-grained retrieval strategy coupled with an entity and relation detector to enhance the accuracy of graph retrieval"
  - [section] "We use an entity detector gϕ(qi, kj) → [0, 1] to predict the probability that entity kj is the subject of qi"
  - [section] "We use a relation detector gψ(qi, rj) → [0, 1] to detect the probability of the relation rj being relevant to the subject in q"

### Mechanism 3
- **Claim:** Question decomposition with fine-tuned LLM reduces burden on base LLM and improves reasoning reliability
- **Mechanism:** Instead of iterative decomposition, a fine-tuned LLM breaks multi-hop questions into sub-questions in one step using a template. The base LLM only handles single-hop reasoning for each sub-question
- **Core assumption:** Decoupling decomposition from the base LLM reduces complexity and improves accuracy
- **Evidence anchors:**
  - [abstract] "employs a fine-tuned LLM for question decomposition"
  - [section] "KEDKG use a fine-tuned LLM to decompose a multi-hop question into multiple sub-questions"
  - [section] "we propose an approach that utilizes a template Pdivide to guide a trained LLM to decompose a multi-hop question Q into multiple sub-questions in a single step"

## Foundational Learning

- **Concept:** Knowledge graphs and triple representation
  - **Why needed here:** The system stores edited facts as (subject, relation, object) triples in a graph structure to enable structured retrieval and conflict detection
  - **Quick check question:** How would you represent "Paris is the capital of France" as a knowledge graph triple?

- **Concept:** Entity linking and disambiguation
  - **Why needed here:** Different names referring to the same entity (e.g., "United States" vs "U.S.") need to be linked to the same graph node to maintain consistency
  - **Quick check question:** What challenges arise when linking "USA", "United States", and "America" to the same entity?

- **Concept:** Retrieval-augmented generation (RAG) systems
  - **Why needed here:** The system retrieves relevant facts from the knowledge graph to augment the LLM's generation process, similar to RAG but using structured knowledge
  - **Quick check question:** How does structured retrieval from a knowledge graph differ from semantic similarity retrieval in traditional RAG?

## Architecture Onboarding

- **Component map:** Relation extraction model -> Entity linker -> Conflict detection module -> Knowledge graph <- Question decomposition LLM -> Entity detector -> Relation detector -> Base LLM

- **Critical path:**
  1. Edit facts → Relation extraction → Entity linking → Conflict detection → Knowledge graph
  2. Multi-hop question → Question decomposition → Sub-question processing loop:
     - Entity detection → Relation detection → Knowledge graph retrieval → Base LLM answering
  3. Sub-question answers → Final answer composition

- **Design tradeoffs:**
  - Fine-grained vs coarse-grained retrieval: Better accuracy but more complex pipeline
  - Dynamic vs static knowledge graph: Supports updates but requires conflict management
  - Decoupled vs integrated decomposition: Reduces base LLM burden but adds training complexity

- **Failure signatures:**
  - Low hop-wise accuracy indicates retrieval or detection failures
  - Performance degradation with batch size suggests conflict resolution issues
  - Poor multi-hop accuracy despite good single-hop accuracy indicates decomposition problems

- **First 3 experiments:**
  1. Test conflict detection with contradictory edits (e.g., "Brazil is in South America" vs "Brazil is in Africa")
  2. Evaluate entity detector performance with ambiguous entity names (e.g., "Paris" the city vs "Paris" the person)
  3. Test decomposition accuracy on multi-hop questions of varying complexity (2-hop, 3-hop, 4-hop)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does KEDKG's performance scale with increasing complexity of secondary edits (multiple conflicting edits across different knowledge chains)?
- **Basis in paper:** [explicit] The paper mentions that KEDKG integrates a conflict detector and uses dynamic knowledge graphs to handle conflicting edits, showing improved performance in large batch edits compared to baselines. However, the paper doesn't explore the limits of this approach with highly complex conflict scenarios.
- **Why unresolved:** The paper only evaluates KEDKG on the MQUAKE-CF dataset which contains some conflicting edits, but doesn't systematically test performance with increasingly complex conflict scenarios or measure the point at which performance degrades.
- **What evidence would resolve it:** Experiments testing KEDKG on datasets with varying degrees of edit conflicts, particularly those with overlapping edits across multiple knowledge chains, would reveal the scalability limits of the conflict resolution approach.

### Open Question 2
- **Question:** What is the impact of KEDKG's entity and relation detection accuracy on downstream multi-hop QA performance?
- **Basis in paper:** [explicit] The paper demonstrates that replacing entity detector gϕ or relation detector gψ with semantic similarity models leads to performance drops (up to 31.5% decrease in multi-hop accuracy). However, it doesn't analyze how variations in detection accuracy correlate with QA performance.
- **Why unresolved:** While the paper shows that the detection modules are effective compared to baselines, it doesn't establish a quantitative relationship between detection accuracy and multi-hop QA accuracy, nor does it explore the sensitivity of overall performance to detection errors.
- **What evidence would resolve it:** A systematic analysis measuring how incremental improvements or degradations in entity and relation detection accuracy affect multi-hop QA performance would clarify the importance of these components and guide optimization priorities.

### Open Question 3
- **Question:** How does KEDKG's knowledge graph construction and maintenance scale with large-scale knowledge bases and frequent updates?
- **Basis in paper:** [inferred] The paper presents KEDKG as a solution for knowledge editing in multi-hop QA, using dynamic knowledge graphs. However, it only evaluates on the MQUAKE dataset and doesn't address scalability concerns for real-world applications with massive knowledge bases and continuous updates.
- **Why unresolved:** The paper demonstrates effectiveness on a benchmark dataset but doesn't explore computational efficiency, memory requirements, or update latency when scaling to industrial-sized knowledge bases with thousands of daily updates.
- **What evidence would resolve it:** Performance benchmarks measuring KEDKG's computational resources, update latency, and memory usage when applied to large-scale knowledge graphs (e.g., Wikidata-scale) with varying update frequencies would establish practical deployment constraints.

## Limitations
- Limited to MQUAKE dataset evaluation, raising questions about generalizability to other domains
- Conflict resolution mechanism may not capture all types of knowledge inconsistencies in complex scenarios
- Performance degradation with batch editing suggests practical limits to conflict resolution system

## Confidence
- **High confidence** in the core innovation of dynamic knowledge graphs for conflict resolution
- **Medium confidence** in the fine-grained retrieval approach
- **Medium confidence** in the overall performance improvements

## Next Checks
1. **Conflict resolution stress test**: Systematically test the conflict detection module with edge cases including circular dependencies, nested contradictions, and overlapping edits across multiple subjects
2. **Cross-dataset generalization**: Evaluate KEDKG on at least two additional multi-hop QA datasets (such as HotpotQA or ComplexWebQuestions) to assess generalizability
3. **Detector sensitivity analysis**: Vary the entity and relation detector thresholds (α) across a range of values (0.05, 0.1, 0.2, 0.5) and measure the trade-off between precision and recall