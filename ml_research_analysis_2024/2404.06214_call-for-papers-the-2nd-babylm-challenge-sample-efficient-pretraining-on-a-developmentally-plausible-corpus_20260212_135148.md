---
ver: rpa2
title: '[Call for Papers] The 2nd BabyLM Challenge: Sample-efficient pretraining on
  a developmentally plausible corpus'
arxiv_id: '2404.06214'
source_url: https://arxiv.org/abs/2404.06214
tags:
- will
- year
- dataset
- data
- track
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The 2nd BabyLM Challenge aims to study sample-efficient pretraining
  on developmentally plausible language data by relaxing dataset restrictions and
  introducing a multimodal vision-and-language track. Participants can now construct
  their own datasets within 100M or 10M word budgets, and a new multimodal track uses
  50% text-only and 50% image-text data.
---

# [Call for Papers] The 2nd BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus

## Quick Facts
- **arXiv ID**: 2404.06214
- **Source URL**: https://arxiv.org/abs/2404.06214
- **Reference count**: 7
- **Key outcome**: Second iteration of BabyLM Challenge with relaxed dataset restrictions, new multimodal track, and paper-only submissions

## Executive Summary
The 2nd BabyLM Challenge builds upon last year's competition by introducing significant changes to encourage more diverse and developmentally plausible approaches to sample-efficient language model pretraining. The challenge now allows participants to construct their own datasets within strict 100M or 10M word budgets, replaces the loose track with a dedicated multimodal vision-and-language track, and introduces a paper-only track for non-model-based contributions. The multimodal track uses a balanced corpus of 50% text-only and 50% image-text data, aiming to better simulate human language acquisition through multiple modalities.

## Method Summary
The challenge provides two main tracks: language-only (STRICT) with 100M or 10M word budgets, and multimodal (VISION) with balanced text-image data. Participants can use provided datasets or construct their own within budget constraints. Models are evaluated on language modeling log-likelihood and classification tasks, with multimodal evaluations TBD. Submissions include trained models and optional competition papers, with baselines based on last year's winners. The evaluation pipeline uses Dynabench for results submission and OpenReview for paper review, focusing on soundness rather than correctness.

## Key Results
- Relaxed dataset restrictions while maintaining word budget constraints
- Introduction of multimodal vision-and-language track with 50/50 text-image split
- Addition of paper-only track for cognitively-inspired benchmarks and analyses
- Updated text-only and multimodal datasets provided as starting points
- Evaluation tasks similar to last year plus additional multimodal assessments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relaxing dataset restrictions while maintaining word budget constraints allows participants to optimize data composition for better pretraining efficiency.
- Mechanism: By permitting participants to construct their own datasets within the 100M or 10M word budget, they can select data sources and compositions that are more developmentally plausible or task-specific, potentially improving model performance compared to using a fixed dataset.
- Core assumption: Data quality and composition significantly impact pretraining outcomes, and participants can identify better combinations than a fixed dataset.
- Evidence anchors:
  - [abstract] "we are relaxing the rules around pretraining data, and will now allow participants to construct their own datasets provided they stay within the 100M-word or 10M-word budget."
  - [section] "While we will still provide language-only and multi-modal (see below) datasets of 100M and 10M words, participants are free to construct their own datasets, provided that they stay within the 100M or 10M word budget."
- Break condition: If participants cannot identify better data compositions, or if the dataset construction process introduces biases that harm model performance.

### Mechanism 2
- Claim: Introducing a multimodal vision-and-language track with balanced text and image-text data simulates human language learning's multi-modal nature.
- Mechanism: The multimodal track provides 50% text-only and 50% image-text data, encouraging participants to develop models that learn from both linguistic and visual inputs, potentially leading to more human-like language acquisition.
- Core assumption: Human language learning is inherently multi-modal, and incorporating visual information during pretraining improves language model performance.
- Evidence anchors:
  - [abstract] "Third, we introduce a multimodal vision-and-language track, and will release a corpus of 50% text-only and 50% image-text multimodal data as a starting point for LM model training."
  - [section] "Human language learning is inherently multi-modal. To encourage more multi-modal submissions, we are replacing last year's loose track with a vision-language track."
- Break condition: If visual information does not improve language model performance, or if the image-text alignment is too weak to provide meaningful learning signals.

### Mechanism 3
- Claim: Introducing a paper-only track broadens the scope of contributions to include cognitively-inspired benchmarks and analysis techniques.
- Mechanism: The paper track allows submissions that don't involve direct competition entries, such as novel evaluation metrics or in-depth analyses of BabyLM models, expanding the research impact beyond just model performance.
- Core assumption: Valuable insights can be gained from analyses and benchmarks that don't necessarily involve competitive model submissions.
- Evidence anchors:
  - [abstract] "First, we replace the loose track with a paper track, which allows (for example) non-model-based submissions, novel cognitively-inspired benchmarks, or analysis techniques."
  - [section] "To encourage contributions that are related to the goals of the challenge, but do not involve direct competition entries, we are introducing a paper-only track."
- Break condition: If the paper track doesn't attract high-quality submissions or if the insights gained don't significantly advance the field.

## Foundational Learning

- Concept: Dataset construction and data curation
  - Why needed here: Participants need to understand how to construct effective datasets within the word budget constraints, selecting appropriate data sources and balancing different types of data.
  - Quick check question: How would you ensure that your constructed dataset stays within the 100M word budget while maximizing its developmental plausibility?

- Concept: Multimodal learning and vision-language modeling
  - Why needed here: Participants in the vision track need to understand how to integrate visual and linguistic information during pretraining, and how to evaluate multimodal models effectively.
  - Quick check question: What are the key challenges in aligning image and text data for multimodal pretraining?

- Concept: Evaluation metrics and benchmarking
  - Why needed here: Participants need to understand the evaluation pipeline and how to interpret results, especially as new evaluation tasks are introduced for the multimodal track.
  - Quick check question: How would you design a cognitively-inspired evaluation metric for assessing language model performance?

## Architecture Onboarding

- Component map: Dataset selection/construction -> Model pretraining -> Evaluation pipeline -> Results submission (models) / Paper writing -> Submission (papers)
- Critical path: For model submissions, the critical path is: dataset selection/construction → model pretraining → evaluation → results submission. For paper submissions, it's: research question formulation → methodology development → analysis → paper writing → submission.
- Design tradeoffs: Balancing data quality vs. quantity within the word budget, choosing between text-only and multimodal approaches, and deciding between competing model architectures.
- Failure signatures: Poor model performance on evaluation tasks, inability to stay within word budget constraints, misalignment between visual and textual data in multimodal models, or rejection of paper submissions due to unsound claims.
- First 3 experiments:
  1. Test dataset construction: Create a small dataset within the word budget and verify it meets the requirements.
  2. Baseline model training: Train a simple model on the provided dataset to ensure familiarity with the evaluation pipeline.
  3. Multimodal integration test: If participating in the vision track, test the alignment between a small set of images and text captions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of models trained on self-constructed datasets within the 100M/10M word budget compare to those trained on the provided datasets?
- Basis in paper: [explicit] The challenge allows participants to construct their own datasets within the word budget, but does not provide comparative results.
- Why unresolved: The paper does not include empirical comparisons between self-constructed and provided datasets.
- What evidence would resolve it: Empirical results comparing model performance across different dataset constructions within the same word budget.

### Open Question 2
- Question: What is the optimal balance between text-only and image-text data for multimodal models in the VISION track?
- Basis in paper: [explicit] The VISION track provides a 50/50 split of text-only and image-text data, but does not specify if this is optimal.
- Why unresolved: The paper does not provide evidence for why this specific ratio is chosen or if other ratios might be more effective.
- What evidence would resolve it: Comparative results of multimodal models trained with different text-image data ratios within the 100M word budget.

### Open Question 3
- Question: How do different training regimes (e.g., number of epochs, hyperparameters) affect model performance within the word budget constraints?
- Basis in paper: [explicit] The paper states that any training regime is permitted but does not provide guidance on optimal training strategies.
- Why unresolved: The paper does not include results on how different training approaches affect performance under the constraints.
- What evidence would resolve it: Empirical results showing the impact of various training parameters on model performance within the word budget.

## Limitations
- The actual impact of relaxing dataset restrictions on model performance remains untested and may vary significantly based on participants' data curation skills.
- The multimodal track's effectiveness is uncertain, with limited evidence that the 50/50 text-image split optimally simulates human language acquisition.
- The evaluation pipeline for multimodal tasks is not yet finalized, which could affect comparability across submissions.

## Confidence
- **High Confidence**: The basic challenge structure (word budget constraints, submission process via Dynabench and OpenReview, and evaluation on language modeling and classification tasks) is well-specified and actionable.
- **Medium Confidence**: The claim that relaxing dataset restrictions will lead to better pretraining efficiency has theoretical support but lacks empirical validation from this specific challenge context.
- **Medium Confidence**: The assumption that multimodal learning improves language acquisition has support from cognitive science but may not translate directly to computational models.
- **Low Confidence**: The optimal composition of multimodal data (50% text-only, 50% image-text) and its developmental plausibility are speculative without empirical validation.

## Next Checks
1. Analyze the word count verification process: Create a small test corpus and verify that the word counting methodology (excluding code, formulas, etc.) aligns with the challenge's requirements before constructing full datasets.
2. Test multimodal data alignment: Before full pretraining, validate that image-text pairs from Localized Narratives and CC3M have sufficient semantic alignment for meaningful learning signals in a small-scale experiment.
3. Evaluate evaluation task selection: Review the proposed language modeling and classification tasks for developmental plausibility and cognitive relevance, providing feedback before the final evaluation pipeline is locked.