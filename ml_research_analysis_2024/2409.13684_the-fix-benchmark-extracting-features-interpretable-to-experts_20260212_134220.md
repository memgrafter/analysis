---
ver: rpa2
title: 'The FIX Benchmark: Extracting Features Interpretable to eXperts'
arxiv_id: '2409.13684'
source_url: https://arxiv.org/abs/2409.13684
tags:
- features
- expert
- dataset
- feature
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The FIX benchmark addresses the challenge of evaluating interpretable
  features in machine learning models across diverse real-world domains. It introduces
  FIXScore, a unified metric that measures alignment between extracted features and
  expert-defined features in cosmology, psychology, and medicine.
---

# The FIX Benchmark: Extracting Features Interpretable to eXperts

## Quick Facts
- arXiv ID: 2409.13684
- Source URL: https://arxiv.org/abs/2409.13684
- Authors: Helen Jin; Shreya Havaldar; Chaehyeon Kim; Anton Xue; Weiqiu You; Helen Qu; Marco Gatti; Daniel A Hashimoto; Bhuvnesh Jain; Amin Madani; Masao Sako; Lyle Ungar; Eric Wong
- Reference count: 40
- Primary result: FIXScore reveals poor alignment between popular feature extraction methods and expert-defined features across six diverse domains

## Executive Summary
The FIX benchmark addresses the challenge of evaluating interpretable features in machine learning models across diverse real-world domains. It introduces FIXScore, a unified metric that measures alignment between extracted features and expert-defined features in cosmology, psychology, and medicine. The benchmark includes six datasets spanning vision, language, and time series modalities. Evaluation of popular feature extraction methods reveals poor alignment with expert knowledge, highlighting the need for new approaches to automatically extract interpretable features. The benchmark aims to guide the development of general-purpose methods for expert feature extraction, with applications in healthcare, scientific discovery, and social science.

## Method Summary
FIXScore is a unified evaluation metric that measures alignment between extracted features and expert-defined features across diverse domains. The benchmark includes six datasets spanning vision, language, and time series modalities in cosmology, psychology, and medicine. The metric uses an expert alignment function customized by domain experts for each dataset, then aggregates scores across all low-level features while being duplication-invariant at optimality. The benchmark evaluates both domain-specific and domain-agnostic baseline algorithms for feature extraction, including patches, quickshift, watershed, and clustering methods.

## Key Results
- Popular feature extraction methods (patches, quickshift, watershed, clustering) show poor alignment with expert-defined features across all six FIX domains
- Domain-specific approaches perform better than domain-agnostic ones, but both fall short of expert expectations
- The benchmark successfully demonstrates that current methods struggle to automatically extract features interpretable to domain experts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** FIXScore achieves domain-agnostic evaluation by abstracting expert alignment into a flexible scoring function.
- **Mechanism:** FIXScore uses a generic ExpertAlign function that domain experts define, then applies a coverage-aware aggregation (Equation 1-2) to unify diverse domains.
- **Core assumption:** Domain experts can provide a meaningful, bounded ExpertAlign function that maps extracted features to interpretable scores.
- **Evidence anchors:**
  - [abstract]: "FIXScore, a unified expert alignment measure applicable to diverse real-world settings"
  - [section 3.1]: "FIXScore includes an expert alignment function customized by experts for each domain"
  - [corpus]: Weak - no direct evidence of cross-domain consistency validation
- **Break condition:** If ExpertAlign outputs are unbounded or inconsistent across domains, the aggregation loses interpretability.

### Mechanism 2
- **Claim:** FIXScore rewards diversity of expert features while being duplication-invariant at optimality.
- **Mechanism:** The metric averages alignment scores across all low-level features, ensuring that adding redundant expert features does not increase the score beyond 1 (Theorem 1).
- **Core assumption:** Expert features have full coverage and perfect alignment corresponds to ground-truth features.
- **Evidence anchors:**
  - [section 3.1]: "If one extracts perfect expert features (i.e., FIXScore(Ĝ, x) = 1 for some Ĝ and x), the FIXScore cannot be increased further by duplicating expert features"
  - [section 3.1]: "Theorem 1. In the explicit case where G⋆ is known and has full coverage... FIXScore(G⋆, x) = 1 for all x"
  - [corpus]: Weak - no empirical evidence of duplication-invariance under imperfect conditions
- **Break condition:** If ground-truth expert features are incomplete or overlapping, the optimality condition fails.

### Mechanism 3
- **Claim:** The benchmark's domain diversity ensures the generality of methods developed on it.
- **Mechanism:** By including six datasets across vision, language, and time series in cosmology, psychology, and medicine, the benchmark forces methods to generalize beyond task-specific heuristics.
- **Core assumption:** The chosen domains are sufficiently distinct that success on all implies general-purpose capability.
- **Evidence anchors:**
  - [abstract]: "six datasets spanning vision, language, and time series modalities"
  - [section 1]: "Our goal is to guide the development of new methods to produce interpretable features by introducing a unified evaluation metric"
  - [corpus]: Weak - no ablation study removing domains to test sensitivity
- **Break condition:** If methods overfit to a subset of domains (e.g., all vision tasks), generality claim is invalid.

## Foundational Learning

- **Concept:** Feature attribution methods assume interpretability of base features
  - Why needed here: The benchmark explicitly addresses the failure mode where pixel/token-level features are uninterpretable to experts
  - Quick check question: What happens to feature attribution interpretability when input features are low-level (e.g., pixels) rather than high-level (e.g., organs)?

- **Concept:** Expert features are domain-specific semantic groupings
  - Why needed here: The core contribution is enabling automatic extraction of such groupings without manual annotation
  - Quick check question: How does the benchmark distinguish between semantically meaningful expert features and arbitrary feature groupings?

- **Concept:** Coverage-aware aggregation enables fair comparison across domains with different numbers of expert features

## Architecture Onboarding

**Component Map:** Datasets -> Feature Extraction Methods -> FIXScore Evaluation -> Expert Alignment Functions

**Critical Path:** Raw data → Feature extraction → Expert alignment scoring → FIXScore aggregation → Benchmark results

**Design Tradeoffs:**
- Domain-specific vs. domain-agnostic approaches: Domain-specific methods perform better but lack generalizability
- Simple vs. complex feature extraction: Simple methods (patches, quickshift) are easier to implement but show poor expert alignment
- Explicit vs. implicit expert alignment: Explicit requires known ground truth, implicit handles real-world scenarios but may be noisier

**Failure Signatures:**
- FIXScore values consistently below 0.5 across domains indicate fundamental challenges in automatic feature extraction
- Large variance in FIXScore across different feature extraction methods suggests method sensitivity to domain characteristics
- FIXScore approaching 1 for simple methods may indicate inadequate expert feature complexity

**First Experiments:**
1. Evaluate all baseline feature extraction methods on each FIX dataset to establish performance baselines
2. Test FIXScore sensitivity by varying the number of extracted features for a single dataset
3. Compare domain-specific vs. domain-agnostic approaches on the same dataset to quantify generalization benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can FIXScore be extended to handle cases where expert opinions conflict on what constitutes an interpretable feature?
- Basis in paper: [inferred] The paper acknowledges this as a limitation in the Conclusion section, noting that "Dealing with potential conflicting expert opinions may need a more nuanced approach, which is left for future work to address."
- Why unresolved: The current FIXScore framework assumes a single set of expert features and does not account for situations where different experts might disagree on feature importance or definition.
- What evidence would resolve it: Experimental validation showing how FIXScore performs when multiple, potentially conflicting expert feature sets are available, along with proposed modifications to handle such cases.

### Open Question 2
- Question: What is the optimal slice size for time-series feature extraction in the Supernova dataset?
- Basis in paper: [explicit] The paper states "For the Supernova time-series dataset, larger slices score yield higher expert alignment scores" but does not specify an optimal value.
- Why unresolved: While the paper shows that larger slices perform better, it does not identify a specific optimal slice size that balances computational efficiency with feature interpretability.
- What evidence would resolve it: Systematic evaluation of FIXScore across different slice sizes to identify the point of diminishing returns or optimal trade-off.

### Open Question 3
- Question: How does the choice of underlying neural network architecture affect feature interpretability across different FIX domains?
- Basis in paper: [inferred] The paper mentions that "The main benefit of using a neural approach is that it can more easily automatically discover relevant features" but does not explore how different architectures perform.
- Why unresolved: The paper evaluates domain-specific and domain-agnostic baselines but does not systematically compare different neural architectures (CNNs, transformers, etc.) for their ability to extract interpretable features.
- What evidence would resolve it: Comparative study of FIXScore across multiple neural architectures within each FIX domain to identify which architectures consistently produce more interpretable features.

## Limitations
- Weak empirical validation of cross-domain consistency: No evidence that ExpertAlign functions across domains produce comparable scales
- Limited methodological diversity in baselines: Evaluation focuses on simple feature extraction methods rather than advanced approaches
- Unclear handling of expert feature completeness: Theoretical guarantees assume full coverage which may not hold in practice

## Confidence
- **High confidence**: The benchmark construction methodology (dataset selection, FIXScore formulation) is clearly specified and reproducible
- **Medium confidence**: The claim that current feature extraction methods poorly align with expert knowledge, based on the limited baseline evaluations
- **Medium confidence**: The generality claim, as it relies on domain diversity but lacks ablation studies showing sensitivity to domain selection

## Next Checks
1. **Cross-domain calibration validation**: Test whether FIXScore produces comparable numerical ranges across domains by having experts evaluate the same set of features across multiple domains, checking if scores are meaningfully comparable
2. **Expert feature completeness study**: Conduct a systematic evaluation of how incomplete or overlapping expert features affect FIXScore behavior, particularly testing the duplication-invariance property under realistic conditions
3. **Advanced method benchmarking**: Evaluate more sophisticated feature extraction approaches (e.g., attention-based methods, self-supervised learning) on the benchmark to determine if the poor performance is method-specific or reflects deeper challenges in expert feature extraction