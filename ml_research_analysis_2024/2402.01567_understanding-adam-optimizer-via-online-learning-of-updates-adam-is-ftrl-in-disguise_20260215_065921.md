---
ver: rpa2
title: 'Understanding Adam Optimizer via Online Learning of Updates: Adam is FTRL
  in Disguise'
arxiv_id: '2402.01567'
source_url: https://arxiv.org/abs/2402.01567
tags:
- regret
- theorem
- dynamic
- ftrl
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a new perspective on the popular Adam optimizer
  by connecting it to the Follow-the-Regularized-Leader (FTRL) algorithm from online
  learning. The key insight is that Adam can be viewed as a principled dynamic FTRL
  instance well-suited for dynamic environments.
---

# Understanding Adam Optimizer via Online Learning of Updates: Adam is FTRL in Disguise

## Quick Facts
- **arXiv ID**: 2402.01567
- **Source URL**: https://arxiv.org/abs/2402.01567
- **Reference count**: 40
- **Key outcome**: Adam optimizer can be viewed as a principled Follow-the-Regularized-Leader (FTRL) instance with discounted losses, explaining its effectiveness in dynamic optimization environments

## Executive Summary
This paper provides a novel theoretical perspective on the Adam optimizer by connecting it to the Follow-the-Regularized-Leader (FTRL) algorithm from online learning. Through the framework of "online learning of updates" (OLU), the authors demonstrate that Adam's update rule corresponds exactly to a specific FTRL variant with exponentially discounted gradients. This connection reveals that Adam's momentum and discounting components are not arbitrary design choices but are fundamental for achieving good dynamic regret in non-stationary environments. The theoretical analysis suggests that Adam is particularly well-suited for optimization settings with sparse and changing gradients, providing a potential explanation for its success in training deep neural networks, especially Transformer-based models.

## Method Summary
The paper establishes a framework called "online learning of updates" (OLU) where an online learner chooses the optimizer's update direction at each iteration. Within this framework, the authors show that Adam corresponds to a Follow-the-Regularized-Leader (FTRL) algorithm with exponentially discounted losses, termed β-FTRL. The key insight is that the discounting factors in Adam (typically β₁ = 0.9 and β₂ = 0.999) are crucial for achieving sublinear dynamic regret in non-stationary environments. The paper analyzes the dynamic regret of this FTRL variant and proves that both momentum and discounting are essential components for good performance, particularly in sparse gradient settings where traditional methods like SGD or AdaGrad suffer from high dynamic regret.

## Key Results
- Adam can be mathematically derived as a specific instantiation of FTRL with discounted losses through the OLU framework
- The discounting factor β in Adam is crucial for achieving sublinear dynamic regret in non-stationary environments
- Momentum in Adam is essential for good performance in sparse gradient settings, where non-momentum methods incur large dynamic regret
- The paper provides theoretical regret bounds for β-FTRL and validates the connection through experiments on hinge loss classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Adam corresponds to FTRL with exponentially discounted gradients, making it effective for dynamic optimization environments.
- **Mechanism**: In the OLU framework, Adam's update rule matches FTRL with discounted losses, where the discounting factors serve as regularization that enables good dynamic regret bounds.
- **Core assumption**: The OLU framework correctly models the relationship between iterative optimization and online learning.
- **Evidence anchors**: The abstract states "Adam corresponds to a principled online learning framework called FTRL" and plugging discounted scale-free FTRL into OLU "would almost recover Adam."
- **Break condition**: In static environments, the benefits of discounting disappear and simpler methods may perform equally well.

### Mechanism 2
- **Claim**: The discounting factor β is crucial for achieving sublinear dynamic regret in non-stationary environments.
- **Mechanism**: Discounting allows the algorithm to "forget" older gradients, enabling adaptation to changing environments through a "discounted-to-dynamic conversion" that translates regret bounds.
- **Core assumption**: Discounting can be optimally tuned based on path length and environment characteristics.
- **Evidence anchors**: Theorems 3.1 and 3.2 show regret bounds that hold simultaneously for all loss sequences of arbitrary size.
- **Break condition**: In stationary, dense gradient settings, discounting provides no benefit and may harm performance.

### Mechanism 3
- **Claim**: Momentum is essential for sublinear dynamic regret in sparse gradient environments.
- **Mechanism**: Momentum allows accumulation of gradient information over time, which is particularly beneficial when gradients are sparse and non-stationary.
- **Core assumption**: Sparsity patterns are persistent enough that accumulated gradients provide useful information.
- **Evidence anchors**: Section 4.2 shows that "non-momentum" learners incur large dynamic regret due to gradient sparsity.
- **Break condition**: In dense, rapidly changing gradients, momentum may cause the optimizer to follow outdated directions.

## Foundational Learning

- **Concept**: Online Learning of Updates (OLU) framework
  - Why needed here: Establishes the connection between iterative optimization and online learning, enabling analysis of optimizers using online learning theory.
  - Quick check question: In OLU, what does the online learner choose - the iterates wt or the updates ∆t?

- **Concept**: Dynamic Regret in Online Learning
  - Why needed here: Measures performance against time-varying comparators, which is appropriate for analyzing optimizers in non-stationary environments.
  - Quick check question: What is the key difference between static regret and dynamic regret in online learning?

- **Concept**: Follow-the-Regularized-Leader (FTRL) algorithm
  - Why needed here: The classical online learning algorithm that Adam corresponds to when viewed through the OLU framework with discounted losses.
  - Quick check question: How does FTRL choose its updates differently from gradient descent methods?

## Architecture Onboarding

- **Component map**: Online Learning of Updates (OLU) framework -> β-FTRL algorithm -> Dynamic regret analysis -> Optimization guarantees -> Practical implications for Adam's success

- **Critical path**: OLU framework establishes connection → β-FTRL instantiation formalizes Adam → Dynamic regret bounds provide theoretical justification → Optimization guarantees explain practical effectiveness → Implications for deep learning applications

- **Design tradeoffs**: Both momentum and discounting are necessary for good performance but must be balanced; too much discounting ignores useful historical information while too little prevents adaptation to changing environments.

- **Failure signatures**: In static environments, Adam's advantages over SGD disappear; in dense, rapidly changing gradients, momentum can cause following outdated directions; poorly tuned discounting factors can lead to linear regret bounds.

- **First 3 experiments**:
  1. Replicate hinge loss classification experiment with sparse gradients to verify Adam's advantage over SGD in non-stationary environments.
  2. Test Adam vs. SGD on synthetic dataset with controlled sparsity and non-stationarity to quantify momentum and discounting impact.
  3. Implement β-FTRL algorithm directly and compare its dynamic regret against theoretical bounds on benchmark online learning problems.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the precise impact of using different discounting factors β1 and β2 in Adam?
- **Basis in paper**: The paper notes that the default choice is β1 = 0.9 and β2 = 0.999, and understanding β1 < β2 is an important future direction.
- **Why unresolved**: Analysis focuses on β1 = β2 case without exploring different values for momentum and variance discounting factors.
- **What evidence would resolve it**: Theoretical or empirical analysis comparing Adam with different β1 and β2 combinations in various optimization settings.

### Open Question 2
- **Question**: Can other practical optimizers be analyzed using the OLU framework?
- **Basis in paper**: The paper states that studying other optimizers based on this framework would build new insights.
- **Why unresolved**: While Adam-FTRL connection is established, other optimizers haven't been explored through OLU.
- **What evidence would resolve it**: Applying OLU to analyze other optimizers (e.g., SGD with momentum, RMSprop) and comparing their theoretical properties and practical performance.

### Open Question 3
- **Question**: Can β-FTRL be made adaptive to the environment without prior knowledge?
- **Basis in paper**: Developing an adaptive version of β-FTRL that automatically adapts to the environment without prior knowledge might lead to more practical algorithms.
- **Why unresolved**: Current β-FTRL analysis requires knowledge of parameters like the environment's Mβ value, which may not be available in practice.
- **What evidence would resolve it**: Theoretical analysis or empirical study demonstrating an adaptive β-FTRL variant achieving competitive performance without requiring prior environment knowledge.

### Open Question 4
- **Question**: Can OLU insights be used to design new practical optimizers?
- **Basis in paper**: Whether one can design practical optimizers based on recent advancements in dynamic online learning would be an important future direction.
- **Why unresolved**: While OLU provides new perspective on understanding Adam, it hasn't been explored for developing novel optimization algorithms.
- **What evidence would resolve it**: Proposing and evaluating new optimizer designs inspired by OLU and dynamic online learning advancements, demonstrating improved performance in various optimization tasks.

## Limitations
- Analysis assumes stochastic gradients but doesn't account for noise or variance in practical implementations.
- Optimal tuning of discounting factors β₁ and β₂ is not fully explored, using simplified assumptions for theoretical analysis.
- Connection to deep learning practice is primarily theoretical without extensive empirical validation on modern architectures.

## Confidence
- **High**: Adam can be mathematically derived as FTRL with discounted losses through OLU framework (supported by direct mathematical derivations).
- **Medium**: Theoretical analysis of dynamic regret bounds for β-FTRL (mathematical framework appears sound but requires empirical validation).
- **Low**: Explanation of Adam's success in deep learning (largely speculative based on theoretical connections rather than direct experimental evidence).

## Next Checks
1. **Empirical validation of dynamic regret bounds**: Implement β-FTRL on benchmark online learning problems with varying non-stationarity levels and compare achieved dynamic regret against theoretical bounds from Theorems 3.1 and 3.2.

2. **Ablation study of Adam's components**: Create controlled experiments isolating effects of momentum and discounting by implementing variants that disable each component separately, then measure impact on optimization performance in sparse and non-stationary gradient environments.

3. **Connection to deep learning practice**: Conduct experiments comparing Adam with β-FTRL and other optimizers on modern deep learning tasks (e.g., Transformer training) while systematically varying non-stationarity and sparsity to validate theoretical predictions about when Adam's advantages are most pronounced.