---
ver: rpa2
title: Fact Finder -- Enhancing Domain Expertise of Large Language Models by Incorporating
  Knowledge Graphs
arxiv_id: '2408.03010'
source_url: https://arxiv.org/abs/2408.03010
tags:
- graph
- query
- cypher
- knowledge
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces FactFinder, a hybrid question-answering system
  that combines Large Language Models (LLMs) with domain-specific knowledge graphs
  to improve factual accuracy. The system addresses the issue of hallucinations and
  incomplete answers in LLMs by retrieving and leveraging structured knowledge from
  medical knowledge graphs.
---

# Fact Finder -- Enhancing Domain Expertise of Large Language Models by Incorporating Knowledge Graphs

## Quick Facts
- arXiv ID: 2408.03010
- Source URL: https://arxiv.org/abs/2408.03010
- Authors: Daniel Steinigen; Roman Teucher; Timm Heine Ruland; Max Rudat; Nicolas Flores-Herr; Peter Fischer; Nikola Milosevic; Christopher Schymura; Angelo Ziletti
- Reference count: 12
- Primary result: 78% precision in retrieving correct knowledge graph nodes

## Executive Summary
This paper introduces FactFinder, a hybrid question-answering system that combines Large Language Models (LLMs) with domain-specific knowledge graphs to improve factual accuracy. The system addresses the common issues of hallucinations and incomplete answers in LLMs by retrieving and leveraging structured knowledge from medical knowledge graphs. FactFinder employs a pipeline involving pre-processing, Cypher query generation, graph retrieval, and LLM-enhanced response generation. Evaluation on a dataset of 69 samples shows the hybrid system outperforms standalone LLMs in accuracy and completeness, as verified by an LLM-as-a-Judge evaluation method.

## Method Summary
FactFinder is a hybrid QA system that integrates LLMs with medical knowledge graphs to improve factual accuracy. The pipeline includes pre-processing to handle entity detection and normalization, Cypher query generation using LLMs, query preprocessing for robustness, knowledge graph retrieval via Neo4j, and LLM-enhanced response generation incorporating graph results. The system was evaluated on a dataset of 69 manually curated medical questions using metrics including precision, recall, and correctness as judged by LLM-as-a-Judge.

## Key Results
- 78% precision in retrieving correct knowledge graph nodes
- Hybrid system outperforms standalone LLMs in accuracy and completeness
- LLM-as-a-Judge evaluation validates improvements in factual correctness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The system leverages knowledge graphs to reduce hallucinations by providing structured, verifiable facts to the LLM.
- Mechanism: Cypher queries are generated from natural language questions, retrieving precise nodes and relationships from the KG, which are then verbalized into answers.
- Core assumption: The LLM can generate valid Cypher queries that accurately reflect the intent of the question.
- Evidence anchors:
  - [abstract] The system addresses hallucinations by retrieving and leveraging structured knowledge from medical knowledge graphs.
  - [section] The pipeline includes Cypher query generation, graph retrieval, and LLM-enhanced response generation.
  - [corpus] Weak correlation (FMR=0.617) with TC-RAG, which also uses RAG for medical LLMs.
- Break condition: LLM fails to generate valid Cypher queries, leading to incorrect or incomplete graph results.

### Mechanism 2
- Claim: Pre-processing and query refinement steps improve the robustness of the system by ensuring compatibility with the KG schema.
- Mechanism: Preprocessors handle formatting, lowercasing, synonym selection, and deprecated code, mapping child nodes to parent nodes where necessary.
- Core assumption: The KG schema is consistent and well-documented, allowing for reliable preprocessing transformations.
- Evidence anchors:
  - [section] The system preprocesses Cypher queries to increase robustness, including lowercasing property values and synonym selection.
  - [corpus] No direct evidence in corpus; weak support from general KG integration literature.
- Break condition: KG schema changes invalidate preprocessing rules, causing query failures.

### Mechanism 3
- Claim: Evidence visualization (subgraphs, Cypher queries, graph responses) enhances transparency and trust in the system.
- Mechanism: The system provides intermediate results and subgraph visualizations, allowing users to verify the accuracy of the answer.
- Core assumption: Users can interpret technical evidence (Cypher, subgraphs) to assess answer quality.
- Evidence anchors:
  - [section] The system provides evidence such as Cypher query generation prompts, graph results, and subgraph visualizations.
  - [section] Users can interactively visualize the relevant subgraph, generated Cypher query, and graph response.
  - [corpus] No direct evidence; assumption based on general explainability literature.
- Break condition: Evidence is too technical for users, reducing its effectiveness in building trust.

## Foundational Learning

- Concept: Cypher query language
  - Why needed here: The system generates Cypher queries to retrieve data from the Neo4j knowledge graph.
  - Quick check question: Can you write a simple Cypher query to match nodes of type "drug" with a property "name" equal to "aspirin"?

- Concept: Knowledge graph structure
  - Why needed here: Understanding node types, relationships, and properties is essential for generating accurate Cypher queries.
  - Quick check question: What is the difference between a node and a relationship in a knowledge graph?

- Concept: Entity linking and normalization
  - Why needed here: The system maps entities in questions to preferred terms in the KG to ensure consistency.
  - Quick check question: How would you normalize the term "alcohol" to its preferred KG term "ethanol"?

## Architecture Onboarding

- Component map: Frontend (Streamlit UI) -> Question preprocessor (entity detection) -> Cypher query generator (LLM-based) -> Query preprocessor (formatting, synonym selection) -> Neo4j graph database (KG storage and retrieval) -> LLM verbalizer (answer generation from graph results) -> Evidence visualizer (subgraph, Cypher, graph response)
- Critical path: User question → Entity detection → Cypher generation → Query preprocessing → Graph retrieval → LLM verbalization → Answer + Evidence
- Design tradeoffs:
  - Using LLM for Cypher generation trades precision for flexibility; specialized parsers might be more accurate but less adaptable.
  - Preprocessing steps add latency but improve query robustness.
  - Evidence visualization increases transparency but may overwhelm non-technical users.
- Failure signatures:
  - Cypher generation errors (SCHEMA_ERROR, invalid syntax)
  - Graph retrieval timeouts or empty results
  - LLM verbalization that introduces hallucinations despite correct graph data
- First 3 experiments:
  1. Test Cypher generation with a simple question ("Which drugs treat hypertension?") and verify the query structure.
  2. Evaluate preprocessing by checking if entity normalization ("alcohol" → "ethanol") works correctly.
  3. Assess evidence visualization by running a query and confirming the subgraph displays the correct nodes and relationships.

## Open Questions the Paper Calls Out
None

## Limitations
- Relies heavily on LLM-as-a-Judge for evaluation, introducing potential circularity
- 69-sample dataset may not represent full complexity of real-world medical queries
- Performance with different KG schemas or non-medical domains remains untested

## Confidence
- Hybrid system effectiveness (accuracy/completeness improvements): **Medium**
- Cypher query generation capability: **Medium**
- Evidence visualization utility: **Low**

## Next Checks
1. Conduct human evaluation of the hybrid system's answers versus LLM-only answers using a separate panel of medical experts to validate the LLM-as-a-Judge results and assess real-world accuracy.
2. Perform systematic testing of the Cypher generation pipeline with edge cases, including questions requiring multiple joins, aggregation functions, or complex pattern matching to identify failure modes.
3. Test the system's adaptability by evaluating performance on a different domain's knowledge graph (e.g., financial or legal) to assess the generalizability of preprocessing and query generation approaches.