---
ver: rpa2
title: Chaos-based reinforcement learning with TD3
arxiv_id: '2405.09086'
source_url: https://arxiv.org/abs/2405.09086
tags:
- learning
- agent
- goal
- exploration
- reservoir
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Twin Delayed Deep Deterministic Policy Gradients
  (TD3) to chaos-based reinforcement learning (CBRL), where internal chaotic dynamics
  drives exploration instead of external random noise. By using an Echo State Network
  (ESN) with controlled chaoticity as the actor network, the agent learns to balance
  exploration and exploitation, autonomously suppressing exploration as learning progresses
  and resuming it when environmental changes occur.
---

# Chaos-based reinforcement learning with TD3

## Quick Facts
- arXiv ID: 2405.09086
- Source URL: https://arxiv.org/abs/2405.09086
- Authors: Toshitaka Matsuki; Yusuke Sakemi; Kazuyuki Aihara
- Reference count: 40
- This paper introduces Twin Delayed Deep Deterministic Policy Gradients (TD3) to chaos-based reinforcement learning (CBRL), where internal chaotic dynamics drives exploration instead of external random noise.

## Executive Summary
This paper introduces Twin Delayed Deep Deterministic Policy Gradients (TD3) to chaos-based reinforcement learning (CBRL), where internal chaotic dynamics drives exploration instead of external random noise. By using an Echo State Network (ESN) with controlled chaoticity as the actor network, the agent learns to balance exploration and exploitation, autonomously suppressing exploration as learning progresses and resuming it when environmental changes occur. Experiments on a simple goal-reaching task and more complex MuJoCo tasks demonstrate that TD3-CBRL successfully learns while adapting to dynamic environments, outperforming regular TD3 without exploration noise.

## Method Summary
The paper introduces TD3-CBRL, which uses an Echo State Network (ESN) with controlled chaoticity as the actor network instead of external exploration noise. The ESN generates spontaneous chaotic activity that drives exploration, with the spectral radius parameter g controlling the level of chaoticity. The agent learns to balance exploration and exploitation by adapting readout weights that naturally suppress exploration as learning progresses. The approach is tested on goal-reaching and MuJoCo tasks, demonstrating the ability to adapt to environmental changes and outperform regular TD3.

## Key Results
- TD3-CBRL successfully learns continuous control tasks without external exploration noise
- Agents autonomously suppress exploration as learning progresses and resume it when environmental changes occur
- Experiments show optimal performance at spectral radius g≈2.2, outperforming standard reservoir computing values of g≈1
- The approach adapts to dynamic environments and can re-learn when goals change

## Why This Works (Mechanism)

### Mechanism 1
Internal chaotic dynamics in the Echo State Network (ESN) replaces external exploration noise in TD3, enabling autonomous switching between exploration and exploitation. The chaotic reservoir generates spontaneous activity that drives exploration without random noise. As learning progresses, the readout weights adapt to suppress exploration naturally. Core assumption: Chaotic dynamics in the reservoir can be tuned via the spectral radius parameter g to balance exploration and exploitation. Evidence anchors: [abstract] "by using an Echo State Network (ESN) with controlled chaoticity as the actor network, the agent learns to balance exploration and exploitation"; [section 3.3] "we rely on exploration driven spontaneously by chaotic dynamics and use an ESN with a larger spectral radius...for the µnetwork". Break condition: If g is too large, the reservoir becomes overly chaotic and experiences are meaningless, hindering re-learning.

### Mechanism 2
Short-term memory in the reservoir retains temporal context, enabling adaptation to environmental changes. The reservoir state reflects the history of inputs, preserving the sequence of states before and after environmental changes, which helps the agent re-learn. Core assumption: The reservoir's dynamics maintain temporal correlations that are useful for policy updates. Evidence anchors: [abstract] "experiments on a simple goal-reaching task and more complex MuJoCo tasks demonstrate that TD3-CBRL successfully learns while adapting to dynamic environments"; [section 4.4] "the short-term memory capability of the reservoir allows the agent to distinguish between the state after and before reaching the initial goal area". Break condition: If g is too small, the reservoir dynamics become too convergent and cannot escape attractors to explore new solutions.

### Mechanism 3
TD3-CBRL learns tasks that require partial observability by leveraging the reservoir's time-series processing capability. The reservoir processes the history of observations and provides context to the critic network, enabling value estimation in partially observable environments. Core assumption: The Echo State Property holds when g is appropriately tuned, allowing stable temporal processing. Evidence anchors: [abstract] "experiments on a simple goal-reaching task and more complex MuJoCo tasks"; [section 4.9] "the TD3-CBRL agent can learn POMDP tasks...time-series processing by the ESN is essential for predicting state-action values". Break condition: If g is too large or too small, the reservoir loses its ability to retain meaningful temporal information.

## Foundational Learning

- **Echo State Networks (ESNs) and the Echo State Property**
  - Why needed here: ESNs provide chaotic dynamics for exploration while allowing readout training without modifying recurrent weights
  - Quick check question: What happens to reservoir dynamics when g > 1 versus g < 1?

- **Twin Delayed Deep Deterministic Policy Gradients (TD3)**
  - Why needed here: TD3 handles deterministic continuous actions and can be modified to work without external exploration noise
  - Quick check question: How does TD3's clipped double Q-learning prevent overestimation compared to standard DDPG?

- **Spectral radius and reservoir chaoticity**
  - Why needed here: The spectral radius g controls the reservoir's chaoticity, which must be tuned for optimal exploration-exploitation balance
  - Quick check question: What is the relationship between g and the edge of chaos in reservoir computing?

## Architecture Onboarding

- **Component map**: Environment → ESN reservoir (Nx=256, p=0.1) → Readout (Wout) → TD3 actor network (µ); Environment state → ESN reservoir → TD3 critic network (Q) (concatenated with environment state); Experience buffer stores (u, x, a, r, u', x')
- **Critical path**: Environment state → ESN → µ → Action → Environment → Reward → Experience buffer → Q and µ updates
- **Design tradeoffs**: Larger g increases exploration but may hinder re-learning due to chaoticity; Smaller reservoir size reduces computational cost but may limit exploration; Including reservoir states in experience replay enables adaptation but may store irrelevant information
- **Failure signatures**: No learning progress: reservoir too convergent (g too small) or exploration insufficient; Unstable learning: reservoir too chaotic (g too large) or spectral radius outside optimal range; Poor re-learning: experiences before environmental change dominate replay buffer
- **First 3 experiments**: 1) Verify that TD3-CBRL learns the simple goal task with g=2.2 and baseline hyperparameters; 2) Test learning failure when replacing ESN with MLP (no chaotic dynamics); 3) Evaluate re-learning performance when changing goal position mid-training

## Open Questions the Paper Calls Out

### Open Question 1
How does the optimal spectral radius for TD3-CBRL differ from the typical "edge of chaos" value (g≈1) in standard reservoir computing tasks, and why? Basis in paper: [explicit] The paper notes that optimal performance occurs at g≈2.2 rather than g≈1, and discusses possible reasons related to the entire system's dynamics including environment interaction. Why unresolved: The paper only hypothesizes that the reservoir needs to be more chaotic to place the entire system at the edge of chaos, but does not provide empirical validation of this hypothesis through systematic investigation of system-level dynamics. What evidence would resolve it: Experiments measuring the combined dynamics of agent-environment system across different g values, analysis of phase transitions in the joint state space, and comparison with theoretical predictions for optimal criticality in closed-loop control systems.

### Open Question 2
What specific properties of chaotic exploration provide advantages over random noise exploration for re-learning in non-stationary environments? Basis in paper: [explicit] The paper demonstrates that TD3-CBRL agents can re-learn when goals change, while regular TD3 fails, and suggests this is due to temporal correlations and memory properties of reservoir states. Why unresolved: The paper observes superior performance but does not isolate and quantify which specific properties (temporal correlation, memory, attractor dynamics, etc.) contribute to this advantage through controlled ablation studies. What evidence would resolve it: Systematic comparison of different exploration strategies (pure random, temporally correlated noise, chaotic reservoir) while controlling for other variables, measuring exploration coverage, convergence speed, and generalization across multiple non-stationary tasks.

### Open Question 3
What is the relationship between reservoir chaoticity, replay buffer size, and learning performance in CBRL, and how can this inform buffer management strategies? Basis in paper: [explicit] The paper shows that excessive reservoir chaoticity combined with large replay buffers hinders re-learning, while smaller buffers mitigate this issue, suggesting interaction between reservoir states and stored experiences. Why unresolved: The paper demonstrates this interaction but does not provide a theoretical framework for understanding when and why reservoir states become detrimental to stored experiences, or develop adaptive buffer management strategies based on reservoir dynamics. What evidence would resolve it: Analysis of reservoir state evolution in relation to buffer composition over time, development of metrics quantifying the utility of stored reservoir states, and experimental validation of adaptive buffer management strategies that balance memory retention with relevance to current environment.

## Limitations
- Results are limited to specific MuJoCo tasks and a simple goal-reaching problem, with unknown generalization to broader continuous control tasks
- Computational efficiency and sample efficiency compared to standard TD3 with noise-based exploration are not evaluated
- The exact mechanisms by which chaotic dynamics enable autonomous exploration-exploitation switching remain incompletely characterized

## Confidence
- **High confidence**: TD3-CBRL can learn standard continuous control tasks without external exploration noise
- **Medium confidence**: Chaotic reservoir dynamics enable autonomous exploration-exploitation switching
- **Medium confidence**: Short-term memory in the reservoir aids adaptation to environmental changes
- **Low confidence**: The approach generalizes effectively to complex, partially observable environments

## Next Checks
1. **Mechanism ablation study**: Test learning performance with reservoir connectivity p varied (0.05, 0.1, 0.2) to isolate the contribution of short-term memory to adaptation capability.

2. **Sample efficiency comparison**: Benchmark TD3-CBRL against standard TD3 with tuned noise parameters on the same tasks, measuring both final performance and learning speed.

3. **Transfer to novel domains**: Evaluate TD3-CBRL on a set of challenging continuous control tasks (e.g., Humanoid, Ant) not used in hyperparameter tuning to assess generalization.