---
ver: rpa2
title: 'WangLab at MEDIQA-M3G 2024: Multimodal Medical Answer Generation using Large
  Language Models'
arxiv_id: '2404.14567'
source_url: https://arxiv.org/abs/2404.14567
tags:
- image
- text
- images
- task
- solutions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents two solutions to the MEDIQA-M3G 2024 shared
  task for multimodal medical answer generation. The first approach uses two consecutive
  API calls to Claude 3 Opus to generate and refine differential diagnoses from medical
  images.
---

# WangLab at MEDIQA-M3G 2024: Multimodal Medical Answer Generation using Large Language Models

## Quick Facts
- arXiv ID: 2404.14567
- Source URL: https://arxiv.org/abs/2404.14567
- Authors: Ronald Xie; Steven Palayew; Augustin Toma; Gary Bader; Bo Wang
- Reference count: 15
- Primary result: Achieved top rankings in MEDIQA-M3G 2024 competition using two approaches: Claude 3 Opus API and CLIP-based image classification

## Executive Summary
This paper presents two competitive solutions to the MEDIQA-M3G 2024 multimodal medical answer generation task. The first approach leverages Claude 3 Opus API in a two-stage framework: generating differential diagnoses from medical images followed by extracting the most likely disease name. The second approach trains a CLIP-based joint embedding model for image-disease classification via nearest neighbor retrieval. Both methods achieved top leaderboard positions, demonstrating the effectiveness of large language models and contrastive learning for medical visual question answering, while also highlighting significant room for improvement in this challenging domain.

## Method Summary
The paper employs two distinct approaches to address multimodal medical answer generation. The first method uses two consecutive Claude 3 Opus API calls: the initial call generates comprehensive differential diagnoses from medical images without output constraints, while the second call distills these differentials into a single disease name. The second approach implements a CLIP-based solution that trains a joint embedding space for images and disease labels, enabling image classification through nearest neighbor retrieval. Both methods incorporate post-processing to format responses as "It is [Disease name]." and include heuristic word matching to improve performance by aligning disease names with query text.

## Key Results
- Claude 3 Opus two-stage approach achieved the highest scores on the competition leaderboard
- CLIP-based image classification demonstrated competitive performance through nearest neighbor retrieval
- Both solutions identified multi-stage LLM frameworks and CLIP-based classification as promising research directions
- The deltaBLEU evaluation metric's bias toward short, disease-focused responses was identified as a key factor influencing solution design

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage LLM framework improves performance by separating differential diagnosis generation from final label extraction.
- Mechanism: The first API call generates a comprehensive differential diagnosis list using both images and text, while the second API call distills this into a single disease name. This separation reduces the cognitive load on the model during generation.
- Core assumption: Claude 3 Opus can generate better differential diagnoses when not constrained by output format, and can better extract the most likely diagnosis when given a complete differential list.
- Evidence anchors:
  - [section]: "we observe that the disease diagnosis given by Claude 3 Opus was poorer quality when the prompt constrains the output format upon manual review"
  - [section]: "Therefore, we let the API generate differential responses only with the provided images alone without any constraints on format, and use a second API call to reformat the response into the desired form"
- Break condition: If the model cannot generate coherent differential diagnoses in the first stage, or if the second stage fails to extract the correct diagnosis from a valid differential list.

### Mechanism 2
- Claim: CLIP-based joint embedding enables effective image-disease classification through nearest neighbor retrieval in the learned embedding space.
- Mechanism: The model learns a joint embedding space where images and disease labels are mapped to the same semantic space. During inference, test images are embedded and their nearest neighbors in the label space are retrieved to determine the most likely disease.
- Core assumption: The joint embedding space successfully captures semantic similarity between images and their corresponding disease labels, and that nearest neighbor retrieval in this space will yield accurate classifications.
- Evidence anchors:
  - [section]: "The resulting joint embedding can be used to perform image classification via nearest neighbour retrieval"
  - [section]: "we embed each image associated with a given case in the competition test set and find 5 nearest neighbours for each embedded image"
- Break condition: If the embedding space does not properly align images and labels semantically, or if the nearest neighbors retrieved are not relevant to the query image.

### Mechanism 3
- Claim: The deltaBLEU evaluation metric's bias toward short, disease-name-focused responses incentivizes this specific solution approach.
- Mechanism: The competition's evaluation metric heavily penalizes incorrect k-mers but has a lower penalty for incomplete answers. This makes concise responses containing just the correct disease name score higher than longer, more comprehensive but partially incorrect responses.
- Core assumption: The deltaBLEU metric's design creates this scoring bias, and that participants optimized their solutions to exploit this characteristic of the evaluation.
- Evidence anchors:
  - [section]: "we have determined that a short response focusing on disease diagnosis alone is the most advantageous. This is due to two reasons... the evaluation metric's penalty for short responses is significantly smaller than a longer, partially correct response"
  - [section]: "Upon our initial exploration, the deltaBLEU metric defined by the competition organizers favors short responses given the relatively heavy penalty incurred on incorrect k-mers present and relatively low penalty on a incomplete answer in comparison"
- Break condition: If the evaluation metric is modified to better reward comprehensive answers or penalize incomplete responses more heavily.

## Foundational Learning

- Concept: Multimodal learning fundamentals
  - Why needed here: The task requires integrating visual (images) and textual (queries, context) information to generate medical diagnoses
  - Quick check question: What are the key challenges in training models that can process both images and text simultaneously?

- Concept: Large language model prompting strategies
  - Why needed here: The solution relies on carefully crafted prompts to guide Claude 3 Opus through a two-stage reasoning process
  - Quick check question: How does separating complex reasoning tasks into multiple prompts affect model performance compared to a single prompt approach?

- Concept: Image classification with contrastive learning
  - Why needed here: The CLIP-based solution uses contrastive learning to create a joint embedding space for images and disease labels
  - Quick check question: What is the key insight behind using contrastive learning for creating joint embeddings between different data modalities?

## Architecture Onboarding

- Component map: Input images and text -> Claude 3 Opus API (two-stage) or CLIP model -> Joint embedding space -> Nearest neighbor retrieval -> Post-processing -> Formatted diagnosis response

- Critical path:
  1. Load test case (images + text)
  2. For Claude solution: Call API twice (generate differentials → extract diagnosis)
  3. For CLIP solution: Embed images → find nearest neighbors → retrieve label
  4. Apply post-processing formatting
  5. Output final response

- Design tradeoffs:
  - API-based vs. fine-tuned models: API solutions are faster to implement but less customizable and potentially more expensive at scale
  - Two-stage vs. single-stage reasoning: Two-stage allows better performance but adds latency and complexity
  - Nearest neighbor vs. end-to-end classification: Nearest neighbor is simpler but may be less robust to distribution shifts

- Failure signatures:
  - Claude solution: Inconsistent outputs across similar inputs, failure to extract correct diagnosis from valid differentials
  - CLIP solution: Retrieved neighbors are semantically unrelated to query image, high variance in predictions across model initializations
  - Both: Post-processing word matching introduces incorrect diagnoses

- First 3 experiments:
  1. Compare single-stage vs. two-stage Claude prompts with identical temperature and prompt structure
  2. Test different batch sizes (128, 256, 512) for CLIP training and measure validation loss curves
  3. Evaluate retrieval quality by computing precision@k for nearest neighbor search in both image-image and image-text spaces

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the Claude 3 Opus solution vary with different prompt engineering strategies, particularly in terms of including or excluding the query text?
- Basis in paper: [explicit] The paper notes that including query text either in the first or second API call did not outperform using images alone, and suggests this may be due to inconsistent information in the query text or limitations in Claude's ability to reason with text and images simultaneously.
- Why unresolved: The paper only tested a limited set of prompt variations, and did not systematically explore different ways of incorporating the query text. The exact impact of query text on performance remains unclear.
- What evidence would resolve it: Conducting a systematic study of different prompt engineering strategies, including various ways of incorporating query text (e.g., as context, as additional features, etc.) and measuring their impact on performance metrics like deltaBLEU score.

### Open Question 2
- Question: How does the choice of batch size affect the performance of the CLIP-based image classification solution, and what is the optimal batch size for this task given the limited training data?
- Basis in paper: [explicit] The paper tested three different batch sizes (128, 256, 512) and observed that batch size 256 was most suitable for the task and the amount of training data available.
- Why unresolved: The paper only tested a limited range of batch sizes, and the optimal batch size may depend on various factors such as the size of the training data, the complexity of the task, and the specific architecture of the CLIP model. Further experimentation with a wider range of batch sizes is needed to determine the optimal setting.
- What evidence would resolve it: Conducting experiments with a wider range of batch sizes and measuring their impact on performance metrics like deltaBLEU score. Additionally, analyzing the trade-off between batch size and overfitting to determine the optimal batch size for this specific task and dataset.

### Open Question 3
- Question: How can the current evaluation metric (deltaBLEU) be improved to better assess the quality of generated responses, particularly in terms of semantic correctness and clinical relevance?
- Basis in paper: [inferred] The paper notes that the deltaBLEU metric favors short responses and may not adequately capture semantic correctness or clinical relevance. It suggests adding a semantic component to the metric, such as GPTScore, to provide more robust assessment of response quality.
- Why unresolved: The paper does not provide specific recommendations for improving the evaluation metric, and the impact of different evaluation metrics on model performance and clinical utility remains unclear. Developing and validating improved evaluation metrics is an important area for future research.
- What evidence would resolve it: Conducting experiments to compare the performance of different evaluation metrics (e.g., deltaBLEU, GPTScore, human evaluation) on the same set of generated responses. Analyzing the correlation between different metrics and clinical relevance to determine the most appropriate evaluation approach for this task.

## Limitations

- The solutions heavily rely on API access to proprietary models (Claude 3 Opus, GPT-4 Turbo), making them expensive and potentially inaccessible for widespread adoption.
- The training data size (842 cases) is relatively small for medical image classification tasks, raising concerns about overfitting and generalization to unseen conditions.
- The evaluation metric (deltaBLEU) focuses on word matching rather than clinical correctness, potentially incentivizing incomplete responses over comprehensive answers.

## Confidence

- **High Confidence**: The observation that two-stage prompting with Claude 3 Opus improves performance compared to single-stage constrained generation is well-supported by the methodology and results. The CLIP-based approach for creating joint embeddings follows established principles in multimodal learning.
- **Medium Confidence**: The claim that deltaBLEU metric biases toward short disease-name responses is plausible but not rigorously proven. While the authors provide reasoning based on the metric's design, empirical validation of this claim would strengthen it.
- **Low Confidence**: The generalizability of these solutions to broader medical domains beyond dermatology remains uncertain. The paper does not address how well the approaches would perform on other medical specialties or more diverse patient populations.

## Next Checks

1. **Metric Validation**: Conduct ablation studies varying response length and completeness to empirically verify the deltaBLEU metric's bias toward short disease-name responses versus comprehensive answers.

2. **Cross-Domain Generalization**: Test both solutions on medical datasets from different specialties (e.g., radiology, pathology) to assess domain transfer capability and identify limitations in multimodal reasoning across medical fields.

3. **Error Analysis**: Perform detailed error analysis categorizing failure modes (e.g., rare diseases, atypical presentations, image quality issues) to understand where the solutions break down and what additional capabilities would be needed for clinical deployment.