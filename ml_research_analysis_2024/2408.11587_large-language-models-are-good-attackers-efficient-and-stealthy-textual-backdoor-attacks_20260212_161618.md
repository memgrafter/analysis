---
ver: rpa2
title: 'Large Language Models are Good Attackers: Efficient and Stealthy Textual Backdoor
  Attacks'
arxiv_id: '2408.11587'
source_url: https://arxiv.org/abs/2408.11587
tags:
- backdoor
- attack
- attacks
- trigger
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of creating efficient and stealthy
  textual backdoor attacks in NLP systems. The authors propose EST-Bad, a method that
  leverages Large Language Models (LLMs) to optimize trigger words and inject them
  stealthily into benign text.
---

# Large Language Models are Good Attackers: Efficient and Stealthy Textual Backdoor Attacks

## Quick Facts
- arXiv ID: 2408.11587
- Source URL: https://arxiv.org/abs/2408.11587
- Reference count: 40
- Authors: Ziqiang Li; Yueqi Zeng; Pengfei Xia; Lei Liu; Zhangjie Fu; Bin Li
- One-line primary result: EST-Bad achieves high attack success rates (up to 93.5%) with minimal poisoning ratios (0.3-0.5%) across multiple datasets

## Executive Summary
This paper introduces EST-Bad, an efficient and stealthy textual backdoor attack method that leverages Large Language Models (LLMs) to optimize trigger words and inject them into benign text. The approach consists of three core strategies: optimizing inherent model flaws as triggers, stealthily injecting triggers with LLMs, and selecting the most impactful samples for backdoor injection. EST-Bad demonstrates superior attack performance while maintaining stealthiness compared to prior methods, achieving high attack success rates with minimal poisoning ratios across multiple datasets.

## Method Summary
EST-Bad employs a three-step approach to textual backdoor attacks. First, it optimizes trigger words by exploiting inherent model vulnerabilities using first-order Taylor approximation to find sensitive words that maximize the loss function. Second, it uses an LLM (ChatGPT) to stealthily inject these optimized triggers into benign text while preserving semantic meaning through carefully crafted prompts. Third, it selects the most impactful samples for poisoning using a similarity-based strategy that computes cosine similarity between clean and poisoned samples in feature space. The method is designed to be efficient, requiring minimal poisoned samples while maintaining high attack effectiveness and stealthiness.

## Key Results
- Achieves attack success rates up to 93.5% with minimal poisoning ratios (0.3-0.5%)
- Maintains strong stealthiness with low perplexity (PPL) and grammar error (GE) scores
- Demonstrates transferability to different model architectures (ALBERT, DistilBERT)
- Outperforms baseline methods in both attack success rate and stealthiness metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The optimized trigger word exploits inherent model flaws to achieve high attack effectiveness.
- Mechanism: Uses first-order Taylor approximation to find the most sensitive words to adversarial perturbation, identifying universal adversarial words (UAW) that maximize the loss function when inserted into benign text.
- Core assumption: DNNs exhibit consistent vulnerabilities to adversarial perturbations that can be leveraged as effective triggers.
- Evidence anchors:
  - [abstract]: "leveraging these inherent vulnerabilities as triggers for backdoor attacks appears more practical than crafting new ones from scratch"
  - [section III-C1]: "We formulate our trigger word optimization problem as: argmin_t Σ(x,y)∈Dnt L(fθ(T(x,t), k))"
  - [corpus]: Weak - corpus papers focus on backdoor attacks generally but don't directly validate this specific optimization approach
- Break condition: If the model's vulnerability patterns change significantly or if the Taylor approximation fails to capture the true loss landscape.

### Mechanism 2
- Claim: LLM-based stealthy trigger injection preserves semantic meaning while maintaining attack effectiveness.
- Mechanism: Uses ChatGPT to rephrase benign samples such that they contain the optimized trigger word while preserving the original meaning through carefully crafted prompts.
- Core assumption: LLMs have strong interpretive capabilities for human instructions and can generate grammatically correct text that naturally incorporates specific words.
- Evidence anchors:
  - [abstract]: "we employ a Large Language Model (LLM) to merge the optimized word with benign text by crafting guiding prompts"
  - [section III-C2]: "Utilizing ChatGPT, we construct an instructional prompt structured as: 'Rewrite the following text such that it contains trigger word '[Optimized Trigger]': '[SeedText]'.'"
  - [corpus]: Weak - while LLMs are mentioned in related work, specific validation of this stealthy injection approach is limited
- Break condition: If the LLM fails to properly incorporate the trigger word or significantly alters the semantic meaning of the original text.

### Mechanism 3
- Claim: Similarity-based sample selection improves poisoning efficiency by identifying the most impactful samples.
- Mechanism: Computes cosine similarity between clean and corresponding poisoned samples in feature space, then selects either the most similar (dirty-label) or most dissimilar (clean-label) samples based on the attack setting.
- Core assumption: The efficiency of backdoor injection depends on the similarity between clean and poisoned samples, with different patterns for dirty-label vs clean-label settings.
- Evidence anchors:
  - [abstract]: "meticulously selecting the most impactful samples for backdoor injection"
  - [section III-C3]: "We utilize a pre-trained feature extractor E to compute the cosine similarity (cos(·)) in the feature space between clean and corresponding poisoned samples"
  - [section IV-E]: "Our findings indicate that FUS-p-searched efficient samples exhibit higher similarity in dirty-label settings compared to randomly selected samples"
- Break condition: If the feature extractor fails to capture meaningful representations or if similarity doesn't correlate with attack effectiveness.

## Foundational Learning

- Concept: Universal Adversarial Perturbations (UAPs)
  - Why needed here: The trigger word optimization approach draws inspiration from UAP techniques in computer vision, adapting them to the discrete nature of text.
  - Quick check question: How does the first-order Taylor approximation help identify effective trigger words in the discrete text space?

- Concept: Cosine similarity in feature space
  - Why needed here: The similarity-based selection strategy relies on computing similarity between clean and poisoned samples in a learned feature space to identify impactful samples.
  - Quick check question: Why does the optimal similarity pattern differ between dirty-label and clean-label backdoor attacks?

- Concept: Fine-tuning vs pre-training in NLP
- Concept: Threat model assumptions in backdoor attacks
  - Why needed here: Understanding the threat model (attacker controls only training data, not model architecture) is crucial for evaluating the practical applicability of the attack.
  - Quick check question: What are the implications of the threat model assumption for the transferability of the attack across different model architectures?

## Architecture Onboarding

- Component map: Trigger Word Optimization -> Stealthy Trigger Injection -> Important Sample Selection -> Evaluation pipeline
- Critical path:
  1. Optimize trigger word using surrogate model
  2. Select impactful samples based on similarity
  3. Generate poisoned samples using LLM
  4. Evaluate attack effectiveness and stealthiness
- Design tradeoffs:
  - Effectiveness vs stealthiness: More aggressive trigger insertion increases ASR but reduces stealthiness
  - Computational cost vs accuracy: More optimization iterations improve trigger quality but increase runtime
  - Transferability vs specificity: Universal triggers work across models but may be less effective than model-specific ones
- Failure signatures:
  - Low ASR despite high poisoning ratio: Trigger optimization failed or LLM injection didn't preserve trigger effectiveness
  - High PPL/GE scores: LLM injection altered semantic meaning or introduced grammatical errors
  - Poor transferability: Surrogate model too different from victim model or sample selection strategy not generalizable
- First 3 experiments:
  1. Validate trigger word optimization on SST-2 with BERT surrogate model, measuring ASR vs random triggers
  2. Test LLM injection stealthiness by comparing PPL/GE of generated samples vs benign samples
  3. Evaluate sample selection strategy by comparing ASR of high-similarity vs random samples in dirty-label setting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the transferability of EST-Bad's trigger word optimization and stealthy trigger injection methods vary across different model architectures and training details?
- Basis in paper: [explicit] The paper discusses the transferability of EST-Bad to ALBERT and DistilBERT models, but does not provide a comprehensive analysis of its performance across a wider range of architectures.
- Why unresolved: The paper only explores transferability to two specific models, leaving open the question of how well EST-Bad would perform on other architectures or with different training configurations.
- What evidence would resolve it: Conducting experiments on a broader range of model architectures (e.g., RoBERTa, XLNet) and varying training details (e.g., different batch sizes, learning rates) to assess the consistency of EST-Bad's performance.

### Open Question 2
- Question: What is the impact of different prompt formulations on the effectiveness and stealthiness of the stealthy trigger injection using LLMs?
- Basis in paper: [explicit] The paper discusses the use of different prompts for the LLM-based trigger injection and mentions that some prompts are more effective than others, but does not provide a systematic analysis of the impact of prompt variations.
- Why unresolved: The paper only explores a limited set of prompts, leaving open the question of how different prompt formulations might affect the overall performance of the attack.
- What evidence would resolve it: Conducting a comprehensive study on the impact of various prompt formulations on the attack's success rate, stealthiness, and generalizability.

### Open Question 3
- Question: How does the proposed Similarity-based Selection Strategy (S3) compare to other sample selection strategies in terms of computational efficiency and effectiveness?
- Basis in paper: [explicit] The paper introduces S3 as a simple and effective sample selection strategy, but does not provide a direct comparison with other existing strategies in terms of computational cost and attack performance.
- Why unresolved: While the paper demonstrates the effectiveness of S3, it does not quantify its computational efficiency or compare it to other sample selection methods in detail.
- What evidence would resolve it: Conducting experiments to measure the computational cost of S3 compared to other sample selection strategies and analyzing the trade-off between efficiency and attack performance.

## Limitations
- The attack's effectiveness depends on the quality of the surrogate model and may not generalize well to significantly different architectures.
- The stealthiness claims rely heavily on perplexity and grammar error metrics, which may not fully capture human perception of naturalness.
- The computational efficiency claims are based on relative comparisons rather than absolute runtime measurements.

## Confidence

**High confidence**: The attack success rates and comparative advantages over baseline methods are well-supported by the experimental results. The three-component framework (trigger optimization, stealthy injection, and sample selection) is logically coherent and experimentally validated.

**Medium confidence**: The stealthiness claims rely heavily on perplexity and grammar error metrics, which may not fully capture human perception of naturalness. The transferability results show promise but are limited to a small set of model architectures.

**Low confidence**: The computational efficiency claims are based on relative comparisons rather than absolute runtime measurements. The sample selection strategy's effectiveness may vary significantly with different feature extractors or similarity metrics.

## Next Checks

1. **Robustness to different feature extractors**: Validate that the similarity-based sample selection strategy maintains effectiveness when using different pre-trained models as feature extractors, particularly those with architectures different from the surrogate model.

2. **Human evaluation of stealthiness**: Conduct human evaluation studies where participants rate the naturalness of poisoned samples to validate that perplexity and grammar error metrics accurately capture human perception of stealthiness.

3. **Cross-architecture transferability**: Test the attack's effectiveness on a broader range of model architectures, including encoder-decoder models and models with different pre-training objectives, to better understand the limits of transferability.