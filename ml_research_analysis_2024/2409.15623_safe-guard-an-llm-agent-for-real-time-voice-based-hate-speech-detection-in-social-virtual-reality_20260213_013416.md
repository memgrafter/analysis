---
ver: rpa2
title: 'Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in
  Social Virtual Reality'
arxiv_id: '2409.15623'
source_url: https://arxiv.org/abs/2409.15623
tags:
- speech
- hate
- detection
- audio
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Safe Guard, an LLM-based agent for real-time
  detection of voice-based hate speech in social VR platforms like VRChat. The system
  integrates OpenAI GPT-3.5 with audio feature analysis (using MFCCs and RMS) and
  a CNN classifier to reduce false positives.
---

# Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in Social Virtual Reality

## Quick Facts
- arXiv ID: 2409.15623
- Source URL: https://arxiv.org/abs/2409.15623
- Reference count: 40
- One-line primary result: The paper introduces Safe Guard, an LLM-based agent for real-time detection of voice-based hate speech in social VR platforms like VRChat.

## Executive Summary
Safe Guard is an innovative system that combines Large Language Models (LLMs) with audio feature analysis to detect hate speech in real-time voice interactions within social virtual reality environments. The system integrates OpenAI GPT-3.5 with audio feature analysis (using MFCCs and RMS) and a CNN classifier to reduce false positives. Safe Guard operates in two modes: conversational mode for single-user interactions and observational mode for group monitoring. The system converts audio to text, analyzes it with GPT, and cross-references with audio features to classify speech as hate or non-hate.

## Method Summary
The Safe Guard system processes real-time audio from social VR interactions by first converting speech to text using OpenAI Whisper. The transcribed text is then analyzed by GPT-3.5 using few-shot prompting to classify potential hate speech. Simultaneously, audio features (MFCC and RMS) are extracted using librosa and analyzed by a CNN classifier. The final classification decision is made through an AND gate logic where both models must agree that content is hate speech for it to be classified as such. The system was evaluated using a custom dataset combining the HATEMM dataset and validation clips from YouTube videos.

## Key Results
- The combined approach achieved 75% accuracy in hate speech detection
- False positives were reduced to 1% compared to GPT-3.5 alone
- Detection latency maintained at approximately 1.5 seconds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The combined LLM-audio model achieves higher precision in hate speech detection than either method alone.
- Mechanism: GPT-3.5 processes the transcribed text for contextual understanding, while a CNN classifier analyzes MFCC and RMS audio features. The final classification is only "hate" if both models agree.
- Core assumption: Textual and acoustic features are complementary for hate speech detection.
- Evidence anchors:
  - [abstract] "Evaluation using a custom dataset showed the combined approach achieved 75% accuracy, reduced false positives to 1%..."
  - [section] "The combined Method exhibited a more balanced performance... For hate speech, it achieved a high Precision of 0.96..."
- Break condition: If the audio features are highly corrupted (e.g., background noise, music), the CNN classifier may become unreliable, undermining the benefit of the combined approach.

### Mechanism 2
- Claim: Few-shot prompting significantly improves GPT-3.5's hate speech detection performance compared to direct prompting.
- Mechanism: Providing a few labeled examples in the prompt helps GPT-3.5 better understand the context and nuances of hate speech, leading to improved accuracy.
- Core assumption: LLMs can generalize from a small number of examples when properly contextualized.
- Evidence anchors:
  - [section] "Few-shot prompting emerged as the most effective method for hate speech detection, outperforming the other two methods... It achieved higher accuracy and balanced precision and recall..."
  - [section] "Table. 1 below compares solely the GPT’s performance across three prompting methods..."
- Break condition: If the examples provided are not representative of the target domain or contain ambiguous cases, few-shot prompting may not yield the expected improvement.

### Mechanism 3
- Claim: The integration of audio features reduces false positives in LLM-based hate speech detection.
- Mechanism: Audio features (MFCC, RMS) capture emotional tone and vocal intonations that are not present in text transcripts, helping to distinguish benign content from hate speech.
- Core assumption: Emotional tone and vocal cues are critical for accurate hate speech detection.
- Evidence anchors:
  - [abstract] "...reduced false positives to 1%..."
  - [section] "Kumar et al. [29] showed that incorporating context in LLM rule-based moderation corrected 35% of errors..."
  - [section] "By ensuring the detected hate speech was truly hate, the combined method can reduce the risk of incorrectly identifying non-hate speech as hate speech..."
- Break condition: If the audio features are not properly extracted or the CNN model is not well-trained, the integration may not effectively reduce false positives.

## Foundational Learning

- Concept: Speech-to-text conversion
  - Why needed here: Converts real-time voice interactions into text for LLM analysis.
  - Quick check question: What API was used for speech-to-text conversion in the system?

- Concept: Audio feature extraction (MFCC, RMS)
  - Why needed here: Captures emotional tone and vocal intonations to complement text-based analysis.
  - Quick check question: What are the two main audio features used in the CNN classifier?

- Concept: Prompt engineering for LLMs
  - Why needed here: Guides GPT-3.5 to accurately classify hate speech by providing context and examples.
  - Quick check question: Which prompting method achieved the highest accuracy in the evaluation?

## Architecture Onboarding

- Component map: Audio capture module → Speech-to-text (Whisper) → GPT-3.5 classifier → Audio feature extraction (librosa) → CNN classifier → Decision logic (AND gate) → Safe Guard agent output
- Critical path: Audio capture → Speech-to-text → GPT-3.5 analysis → Audio feature extraction → CNN prediction → Final classification
- Design tradeoffs:
  - LLM accuracy vs. computational cost: GPT-3.5 is less resource-intensive than GPT-4 but may have lower accuracy.
  - Real-time latency vs. detection accuracy: Adding audio feature analysis increases latency but improves precision.
  - False positives vs. false negatives: The combined model prioritizes reducing false positives, which may slightly increase false negatives.
- Failure signatures:
  - High latency: Issues with speech-to-text conversion or LLM processing.
  - Low precision: Problems with audio feature extraction or CNN classifier.
  - High false positive rate: Insufficient context in prompts or poor quality audio.
- First 3 experiments:
  1. Compare GPT-3.5 performance with direct, definition, and few-shot prompting using a small dataset.
  2. Evaluate CNN classifier accuracy using MFCC and RMS features on a labeled audio dataset.
  3. Test the combined model's performance and latency on a real-time voice dataset.

## Open Questions the Paper Calls Out

None

## Limitations

- The system's generalizability beyond the specific dataset used for evaluation remains uncertain, as the HATEMM dataset may not fully represent real-world VR interactions.
- The paper lacks detailed information about CNN architecture parameters and training process, making it difficult to assess the robustness of the audio analysis component.
- The evaluation does not address potential biases in the dataset or how the system might perform across different languages, accents, or cultural contexts.

## Confidence

- **High Confidence**: The core claim that combining LLM text analysis with audio feature extraction reduces false positives is well-supported by the presented evidence and aligns with established findings in multimodal hate speech detection.
- **Medium Confidence**: The claim of 75% accuracy and 1% false positive rate is based on evaluation results but requires independent validation across diverse datasets and real-world conditions.
- **Low Confidence**: The assertion that the system can operate effectively in real-time VR environments with minimal latency needs verification through deployment testing in actual social VR platforms.

## Next Checks

1. **Cross-dataset validation**: Test the combined model on multiple hate speech detection datasets (e.g., HateXplain, Davidson) to assess generalizability and identify potential dataset-specific biases.

2. **Real-world deployment trial**: Implement Safe Guard in a live VRChat environment with diverse user groups to measure actual detection accuracy, latency, and false positive/negative rates under realistic conditions.

3. **Ablation study on audio features**: Systematically evaluate the contribution of each audio feature (MFCC, RMS) and the CNN classifier to overall performance by testing variations: text-only, text+MFCC, text+RMS, and full combined model.