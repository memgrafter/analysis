---
ver: rpa2
title: Universal Prompt Optimizer for Safe Text-to-Image Generation
arxiv_id: '2402.10882'
source_url: https://arxiv.org/abs/2402.10882
tags:
- prompt
- prompts
- images
- inappropriate
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of generating unsafe content using
  text-to-image (T2I) models when given toxic prompts. The authors propose POSI, a
  universal prompt optimizer that modifies toxic prompts to generate safe and semantically
  preserving images without requiring access to the internal structure of T2I models.
---

# Universal Prompt Optimizer for Safe Text-to-Image Generation

## Quick Facts
- arXiv ID: 2402.10882
- Source URL: https://arxiv.org/abs/2402.10882
- Reference count: 28
- Key outcome: Reduces inappropriate image generation by up to 65% while maintaining semantic alignment

## Executive Summary
This paper introduces POSI, a universal prompt optimizer that modifies toxic text prompts to generate safe and semantically preserving images using black-box text-to-image models. The approach constructs a dataset of toxic-clean prompt pairs using GPT-3.5 Turbo, fine-tunes a language model via supervised learning, and optimizes it further using Proximal Policy Optimization with a reward function measuring toxicity and text alignment. Experiments demonstrate that POSI significantly reduces inappropriate image generation while maintaining semantic fidelity, and it can be combined with existing safety methods for enhanced performance. The method is model-agnostic and transferable across different T2I models including DALL-E 3 and Midjourney.

## Method Summary
The POSI framework operates in three stages: first, GPT-3.5 Turbo constructs a dataset of toxic-clean prompt pairs from the I2P dataset; second, a LLaMA 7B model is fine-tuned on this dataset using supervised learning with LoRA for efficiency; third, Proximal Policy Optimization refines the model using a reward function combining toxicity scores (from Q16 and NudeNet classifiers) and text alignment scores (from CLIP). The approach requires no access to the internal structure of T2I models, making it universally applicable. The reward function balances safety and semantic preservation by capping text alignment at 0.31 to prevent over-optimization.

## Key Results
- Reduces inappropriate image generation probability by up to 65% compared to original toxic prompts
- Maintains strong text-image alignment with BLIP similarity scores comparable to baseline methods
- Demonstrates transferability across different T2I models (DALL-E 3, Midjourney) without model-specific tuning
- Can be combined with existing safety methods to achieve enhanced performance

## Why This Works (Mechanism)

### Mechanism 1
GPT-3.5 Turbo rewrites toxic prompts into safe versions while preserving semantic meaning. The LLM learns to identify harmful words/phrases and substitute them with neutral equivalents. This assumes GPT-3.5 Turbo can distinguish between harmful and harmless semantic content. Break condition: If GPT-3.5 Turbo over-sanitizes or under-sanitizes, prompts will lose meaning or still generate unsafe images.

### Mechanism 2
The reward function balances toxicity reduction and text alignment through weighted scoring. It combines toxicity scores with CLIP similarity, capped at 0.31 to prevent over-optimization. This assumes the metrics accurately capture the safety-semantic trade-off. Break condition: If the cap is set incorrectly, images may not match prompts or toxicity may not reduce sufficiently.

### Mechanism 3
PPO optimization without internal T2I model access effectively improves prompt safety. The policy gradient approach uses only reward signals from generated images rather than gradients through the T2I model. This assumes the reward signal provides sufficient information for learning. Break condition: If the reward signal is too noisy or sparse, PPO training may not converge to effective prompt modifications.

## Foundational Learning

- Concept: Reinforcement Learning with Proximal Policy Optimization
  - Why needed here: To optimize the prompt generator without requiring access to the internal structure of T2I models
  - Quick check question: What is the key difference between PPO and standard policy gradient methods in terms of training stability?

- Concept: Text-to-Image Diffusion Models
  - Why needed here: Understanding how T2I models work is essential to grasp why prompt engineering can influence safety
  - Quick check question: How does a text prompt influence the generation process in diffusion models?

- Concept: CLIP (Contrastive Language-Image Pre-training)
  - Why needed here: Used to measure text-image alignment for the reward function
  - Quick check question: What does CLIP compute when comparing text and image embeddings?

## Architecture Onboarding

- Component map: GPT-3.5 Turbo → Supervised Fine-tuning (SFT) → Proximal Policy Optimization (PPO) → Text-to-Image Model
- Critical path: Toxic prompt → GPT-3.5 Turbo preprocessing → LLaMA fine-tuning → PPO optimization → Modified prompt → T2I generation → Safety evaluation
- Design tradeoffs: The system trades off semantic preservation against safety; using a lightweight LM (LLaMA) instead of a full LLM for the final optimizer to enable efficient training
- Failure signatures: High inappropriate probability scores despite prompt modification; low CLIP similarity scores indicating poor semantic preservation; unstable training during PPO phase
- First 3 experiments:
  1. Test GPT-3.5 Turbo's ability to rewrite toxic prompts by manually verifying a sample of generated clean prompts
  2. Evaluate the SFT model's performance by measuring toxicity and alignment scores on the validation set
  3. Run a small PPO training run with reduced iterations to verify reward signal stability before full training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of POSI vary when applied to prompts containing subtle or context-dependent toxicity versus overtly harmful content?
- Basis in paper: The paper tests POSI on datasets with explicit categories like sexual, harassment, self-harm, illegal activity, shocking, and violence, but does not address nuanced or context-dependent toxicity
- Why unresolved: The current evaluation focuses on clear-cut cases of inappropriate content, leaving uncertainty about performance on ambiguous or context-sensitive prompts
- What evidence would resolve it: Testing POSI on prompts with subtle or context-dependent toxicity and comparing its ability to maintain semantic alignment while reducing inappropriate image generation

### Open Question 2
- Question: To what extent does the reward function in POSI need to be adjusted for different T2I models or cultural contexts to maintain optimal safety and semantic preservation?
- Basis in paper: The reward function uses fixed thresholds (e.g., CLIP similarity capped at 0.31) and a single toxicity classifier (Q16), which may not generalize across diverse T2I models or cultural norms
- Why unresolved: The paper demonstrates transferability across models but does not explore the adaptability of the reward function to varying cultural sensitivities or model-specific characteristics
- What evidence would resolve it: Experiments tuning the reward function parameters for different T2I models or cultural contexts and evaluating the impact on safety and semantic alignment

### Open Question 3
- Question: How does the computational cost of POSI scale with the size and complexity of the T2I model or the length of the input prompts?
- Basis in paper: The paper uses a 7B-parameter LLaMA model and mentions that fine-tuning large models is time-consuming, but does not provide detailed analysis of computational scalability
- Why unresolved: While the paper highlights the use of lightweight models and LoRA for efficiency, it does not quantify how computational resources scale with model size or prompt complexity
- What evidence would resolve it: Profiling the computational cost of POSI across different model sizes and prompt lengths, including training and inference times

### Open Question 4
- Question: Can POSI be extended to handle multimodal prompts (e.g., text and image inputs) while maintaining safety and semantic alignment?
- Basis in paper: The paper focuses on text-to-image models with text-only prompts, but modern T2I models often support multimodal inputs
- Why unresolved: The current framework is designed for text-based prompts and does not address the challenges of integrating image inputs or ensuring safety in multimodal scenarios
- What evidence would resolve it: Extending POSI to multimodal prompts and evaluating its ability to safely and semantically align generated images with both text and image inputs

## Limitations

- The evaluation relies heavily on proxy metrics (toxicity classifiers, CLIP similarity) rather than human judgment of image safety and semantic fidelity
- The dataset construction process depends entirely on GPT-3.5 Turbo's ability to rewrite toxic prompts while preserving semantics, with no empirical validation
- Transfer learning results across different T2I models are presented without detailed analysis of model-specific performance variations

## Confidence

**High Confidence**: The fundamental approach of using RL to optimize prompts for safety while preserving semantic meaning is technically sound and well-grounded in existing literature. The architectural design is coherent and follows established practices.

**Medium Confidence**: The experimental results showing significant reduction in inappropriate image generation (up to 65%) are plausible given the methodology, but the reliance on classifier-based metrics rather than human evaluation introduces uncertainty about real-world effectiveness.

**Low Confidence**: The claim that POSI can be "combined with existing safety methods to achieve even better performance" is stated but not empirically demonstrated. The paper mentions this possibility but provides no experiments showing actual integration with other safety approaches.

## Next Checks

1. **Human Evaluation Study**: Conduct a human study where participants rate both the safety and semantic fidelity of images generated from original toxic prompts versus POSI-modified prompts across multiple T2I models. This would validate whether the classifier-based metrics accurately reflect actual safety improvements and semantic preservation.

2. **Safety Mechanism Bypass Test**: Systematically test whether POSI-modified prompts can bypass existing safety filters in T2I models (e.g., DALL-E 3's content policy). This would verify whether the approach genuinely improves safety or merely circumvents existing safeguards.

3. **Dataset Quality Validation**: Manually audit a statistically significant sample of the GPT-3.5 Turbo-generated clean prompts to verify that semantic meaning is preserved during the toxic-to-clean conversion process. This would validate a critical assumption underlying the entire approach.