---
ver: rpa2
title: 'AgentKit: Structured LLM Reasoning with Dynamic Graphs'
arxiv_id: '2404.11483'
source_url: https://arxiv.org/abs/2404.11483
tags:
- action
- agent
- agentkit
- arxiv
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AgentKit introduces a natural language-based framework for assembling
  complex AI agents from simple subtasks, using nodes and dependencies to form structured
  reasoning processes. It enables users to design agents without programming by chaining
  prompts for subtasks like planning, reflection, and learning, and supports dynamic
  graph modifications for advanced capabilities.
---

# AgentKit: Structured LLM Reasoning with Dynamic Graphs

## Quick Facts
- arXiv ID: 2404.11483
- Source URL: https://arxiv.org/abs/2404.11483
- Reference count: 40
- Primary result: State-of-the-art performance in Crafter (20.64% score, 12.8 reward) and WebShop (69.4% success rate)

## Executive Summary
AgentKit introduces a natural language-based framework for assembling complex AI agents from simple subtasks, using nodes and dependencies to form structured reasoning processes. It enables users to design agents without programming by chaining prompts for subtasks like planning, reflection, and learning, and supports dynamic graph modifications for advanced capabilities. In experiments, AgentKit agents achieved state-of-the-art performance in both Crafter (20.64% score, 12.8 reward) and WebShop (69.4% success rate), outperforming existing methods while demonstrating hierarchical planning, reflection, and knowledge base learning. The framework's modular, intuitive design makes it accessible to non-programmers while enabling sophisticated multi-functional agents.

## Method Summary
AgentKit assembles nodes (natural language prompts for subtasks) into a directed acyclic graph (DAG) with dependencies, using LLM reasoning to traverse the graph and execute tasks. Each node completes a specific subtask with a natural language prompt, and the user can chain nodes together like LEGO pieces. The framework supports dynamic graph modifications at inference time, enabling conditional branching and learning capabilities. A central database stores task specifications, instructions, and information from nodes. The evaluation process follows a Compose → Query → After-query → Database update → Next node critical path.

## Key Results
- Achieved state-of-the-art performance in WebShop with 69.4% success rate
- Reached 20.64% score and 12.8 reward in Crafter game environment
- Demonstrated hierarchical planning, reflection, and knowledge base learning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured reasoning via dynamic graphs enables LLMs to perform complex tasks by decomposing them into smaller, manageable subtasks.
- Mechanism: AgentKit uses nodes (natural language prompts for subtasks) connected in a DAG to enforce a "thought process". Each node's output becomes input for dependent nodes, creating a chain of reasoning that mimics human problem-solving.
- Core assumption: Decomposing complex tasks into subtasks and connecting them through dependencies improves LLM performance by reducing cognitive load and enabling focused reasoning on each subtask.
- Evidence anchors:
  - [abstract] "AgentKit offers a unified framework for explicitly constructing a complex 'thought process' from simple natural language prompts."
  - [section] "The nodes in AgentKit can be designed and combined in different ways to implement multiple advanced capabilities including on-the-fly hierarchical planning, reflection, and learning from interactions."
  - [corpus] Weak evidence - no direct citations to graph-based reasoning frameworks in the corpus.
- Break condition: If the decomposition of tasks into subtasks is not optimal or if dependencies between nodes are not well-defined, the reasoning chain may break down, leading to errors in the final solution.

### Mechanism 2
- Claim: Dynamic graph modifications allow AgentKit to adapt to changing circumstances and implement advanced capabilities like conditional branching and learning.
- Mechanism: Nodes and dependencies can be added or removed at inference time through coding, enabling IF...ELSE branching or FOR...LOOPS. This allows the agent to react to environmental signals and learn from interactions.
- Core assumption: The ability to dynamically modify the reasoning graph enables the agent to handle unexpected situations and improve its performance over time through learning.
- Evidence anchors:
  - [section] "To expand the capabilities, nodes and dependencies can be dynamically added and removed at inference time through coding, enabling complicated routing like IF...ELSE branching or FOR...LOOPS."
  - [section] "The set of dynamic nodes and dependencies naturally form a dynamic Directed A-cyclic Graph (DAG), with prompts as nodes and dependencies as edges."
  - [corpus] Weak evidence - no direct citations to dynamic graph modification in LLM agents in the corpus.
- Break condition: If the dynamic modifications are not implemented correctly or if the conditions for branching are not well-defined, the agent may get stuck in infinite loops or make incorrect decisions.

### Mechanism 3
- Claim: The modular, intuitive design of AgentKit makes it accessible to non-programmers while enabling sophisticated multi-functional agents.
- Mechanism: Each node is designed to complete a specific subtask with a natural language prompt, and the user can put together chains of nodes like LEGO pieces. The modular nature and intuitive design to simulate explicit human thought process allow a basic agent to be implemented as simple as a list of prompts for subtasks.
- Core assumption: The simplicity and modularity of the framework lower the barrier to entry for creating AI agents, making it accessible to a wider range of users.
- Evidence anchors:
  - [abstract] "In addition, due to the modular nature and the intuitive design to simulate explicit human thought process, a basic agent could be implemented as simple as a list of prompts for subtasks and therefore could be designed and tuned by someone without any programming experience."
  - [section] "This design intuitively instructs an LLM to follow the predefined 'thought process' by accomplishing all the subtasks before arriving at the final solution."
  - [corpus] Weak evidence - no direct citations to accessible AI agent frameworks in the corpus.
- Break condition: If the modular design leads to a lack of coherence between subtasks or if the intuitive design is not well-suited for the specific task, the agent's performance may suffer.

## Foundational Learning

- Concept: Directed Acyclic Graphs (DAGs)
  - Why needed here: DAGs are used to represent the dependencies between nodes in AgentKit, ensuring that each node is evaluated in the correct order.
  - Quick check question: What is the key property of a DAG that ensures there are no cycles in the graph?

- Concept: Chain-of-Thought Prompting
  - Why needed here: Chain-of-Thought prompting is used within each node to guide the LLM through a series of reasoning steps, leading to a final answer.
  - Quick check question: How does Chain-of-Thought prompting differ from standard prompting in terms of the LLM's reasoning process?

- Concept: Knowledge Bases
  - Why needed here: Knowledge bases are used to store and retrieve information that can be used by nodes in the graph to make informed decisions.
  - Quick check question: What are the key components of a knowledge base, and how are they used in the context of AgentKit?

## Architecture Onboarding

- Component map: Nodes (prompts for subtasks) -> Dependencies (connections defining evaluation order) -> Database (central storage) -> Dynamic Graph (modifiable DAG)
- Critical path: Compose (gather inputs) → Query (LLM reasoning) → After-query (process output) → Database update → Next node
- Design tradeoffs:
  - Modularity vs. coherence: More modular design may lead to less coherent reasoning between subtasks
  - Simplicity vs. expressiveness: Simpler prompts may be more accessible but less expressive for complex tasks
  - Dynamic vs. static: Dynamic graphs allow for adaptation but may be more complex to implement and debug
- Failure signatures:
  - Incomplete reasoning chain: If a node fails to provide the expected output, downstream nodes may not have necessary information
  - Incorrect dependencies: If dependencies between nodes are not well-defined, evaluation order may be incorrect
  - Runtime errors: Dynamic modifications to the graph may introduce bugs or infinite loops
- First 3 experiments:
  1. Create a simple agent with 2-3 nodes that performs a basic task (e.g., text summarization) to verify core functionality
  2. Implement a conditional branching node to handle different scenarios based on input (e.g., positive vs. negative sentiment analysis)
  3. Add a learning node that updates the knowledge base based on agent's interactions with environment (e.g., updating FAQ database)

## Open Questions the Paper Calls Out

- Question: How can the token costs of AgentKit be reduced while maintaining performance?
  - Basis in paper: [explicit] The paper mentions that increased LLM querying cost is a main limitation of AgentKit.
  - Why unresolved: While the paper demonstrates strong performance, it does not explore methods to reduce the number of LLM queries or optimize the graph structure to minimize costs.
  - What evidence would resolve it: Experiments comparing different graph pruning techniques, node sharing strategies, or alternative LLM models with lower per-query costs while maintaining performance levels.

- Question: How well does AgentKit generalize to domains outside of gaming and web interaction?
  - Basis in paper: [inferred] The paper demonstrates AgentKit on Crafter and WebShop, but does not test it on other types of tasks or domains.
  - Why unresolved: The paper shows strong results in two specific domains but does not explore whether the same approach works for scientific reasoning, creative tasks, or other real-world applications.
  - What evidence would resolve it: Testing AgentKit on diverse benchmark suites like BIG-Bench or specialized domains like medical diagnosis or legal reasoning.

- Question: What is the optimal balance between pre-defined node structures and dynamic adaptation?
  - Basis in paper: [explicit] The paper introduces dynamic nodes and dependencies but notes that modifications to nodes already evaluated are forbidden.
  - Why unresolved: The paper implements dynamic capabilities but does not systematically explore when to add/remove nodes or how to optimize this process for different task types.
  - What evidence would resolve it: Controlled experiments varying the frequency and criteria for dynamic modifications across different task complexities and measuring impact on performance and efficiency.

## Limitations
- Manual process design and prompt creation remains a significant barrier to entry despite the framework's accessibility claims
- Increased token costs due to multiple LLM queries for each agent execution
- Dynamic modifications cannot alter nodes that have already been evaluated, limiting certain types of adaptation

## Confidence
- High confidence: The core mechanism of using directed acyclic graphs to structure LLM reasoning through subtasks is well-established and theoretically sound.
- Medium confidence: The experimental results showing competitive performance in WebShop and Crafter, though exact replication details are limited.
- Low confidence: The claims about accessibility for non-programmers and the framework's ability to implement sophisticated multi-functional agents without detailed implementation examples.

## Next Checks
1. Implement a controlled experiment comparing AgentKit performance against baseline methods on a standardized reasoning task (e.g., logical inference or multi-step planning) while varying the complexity of task decomposition.
2. Conduct a user study with participants of varying programming experience levels to evaluate the claimed accessibility of the framework and identify usability bottlenecks.
3. Perform stress testing on the dynamic graph modification capabilities by introducing edge cases and unexpected environmental signals to assess the robustness of the IF...ELSE and FOR...LOOP implementations.