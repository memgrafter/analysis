---
ver: rpa2
title: 'Internal Consistency and Self-Feedback in Large Language Models: A Survey'
arxiv_id: '2407.14507'
source_url: https://arxiv.org/abs/2407.14507
tags:
- consistency
- language
- arxiv
- reasoning
- internal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the issues of reasoning deficiencies and hallucinations
  in large language models (LLMs) by proposing a unified perspective called "Internal
  Consistency." Internal consistency refers to the coherence of expressions among
  LLMs' latent, decoding, or response layers based on sampling methodologies. The
  authors introduce a theoretical framework called "Self-Feedback" to mine internal
  consistency.
---

# Internal Consistency and Self-Feedback in Large Language Models: A Survey

## Quick Facts
- arXiv ID: 2407.14507
- Source URL: https://arxiv.org/abs/2407.14507
- Reference count: 40
- Primary result: Proposes a unified framework called "Internal Consistency" and introduces "Self-Feedback" as a theoretical approach to address LLM reasoning deficiencies and hallucinations through self-evaluation and self-update mechanisms.

## Executive Summary
This survey addresses fundamental challenges in large language models, specifically reasoning deficiencies and hallucinations, by introducing a unified perspective called "Internal Consistency." The authors propose a theoretical framework named "Self-Feedback" consisting of two core modules: Self-Evaluation to capture consistency signals and Self-Update to leverage these signals for improving model responses or the model itself. The paper provides a systematic classification of existing studies, summarizes evaluation methods and benchmarks, and proposes several critical viewpoints including the "Hourglass Evolution of Internal Consistency" and the "Consistency Is (Almost) Correctness" hypothesis.

## Method Summary
The survey takes a comprehensive approach by synthesizing existing research on internal consistency and self-feedback mechanisms in large language models. Rather than conducting original experiments, the authors systematically review and classify studies across different tasks and methodological approaches. They introduce the Self-Feedback framework as a theoretical construct that can be applied to existing models through two key components: Self-Evaluation for detecting consistency signals within model layers, and Self-Update for using these signals to refine outputs or update the model itself. The work focuses on providing a conceptual framework and taxonomy rather than implementing new models.

## Key Results
- Introduces "Internal Consistency" as a unified perspective for addressing LLM reasoning and hallucination issues
- Proposes "Self-Feedback" framework with Self-Evaluation and Self-Update modules
- Systematically classifies existing studies by tasks and methodological lines of work
- Proposes the "Consistency Is (Almost) Correctness" hypothesis linking internal consistency to correctness
- Identifies the "Paradox of Latent and Explicit Reasoning" in model behavior

## Why This Works (Mechanism)
The proposed approach works by leveraging the inherent structure of language models to detect and correct inconsistencies during inference. The Self-Evaluation module acts as an internal consistency checker that examines the coherence between different layers of the model (latent, decoding, and response layers). When inconsistencies are detected, the Self-Update module uses these signals to either refine the current response or update the model's parameters. This creates a feedback loop where the model can self-correct based on its own internal coherence metrics, potentially reducing hallucinations and improving reasoning quality without requiring external supervision.

## Foundational Learning

**Internal Consistency**: The coherence of expressions across different layers of a language model (latent, decoding, response). Why needed: Provides a measurable signal for model quality. Quick check: Verify consistency scores correlate with human judgment of response quality.

**Self-Evaluation**: Module that captures internal consistency signals within the model. Why needed: Enables autonomous detection of reasoning errors. Quick check: Test whether consistency signals predict hallucination occurrences.

**Self-Update**: Module that leverages consistency signals to improve responses or update the model. Why needed: Transforms detected inconsistencies into actionable improvements. Quick check: Measure performance gains after applying self-updates to degraded model outputs.

**Latent Reasoning**: Reasoning processes occurring within hidden model layers. Why needed: Many LLM reasoning failures originate at this level. Quick check: Compare consistency between latent representations and final outputs.

**Hourglass Evolution**: Concept describing how internal consistency changes across model depth. Why needed: Understanding this pattern helps optimize where to apply consistency checks. Quick check: Plot consistency metrics across different model depths.

## Architecture Onboarding

**Component Map**: Input -> Self-Evaluation (Consistency Checker) -> Self-Update (Corrector) -> Refined Output

**Critical Path**: The core workflow follows: Input → Model Forward Pass → Self-Evaluation Module → Consistency Score → Decision Logic → Self-Update Application → Final Output

**Design Tradeoffs**: The main tradeoffs involve computational overhead (self-evaluation adds inference time), potential feedback loops (over-correction risks), and the challenge of defining meaningful consistency metrics across different task types.

**Failure Signatures**: Common failure modes include: false positive consistency signals leading to unnecessary corrections, over-reliance on internal metrics missing external validity issues, and computational bottlenecks when applying self-updates to large models.

**First Experiments**:
1. Baseline consistency measurement across different model architectures without self-update
2. Ablation study removing self-evaluation to quantify its impact on hallucination reduction
3. Controlled testing of self-update mechanisms on synthetically corrupted inputs

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in a dedicated section, though several implicit questions emerge from the work: How can self-feedback mechanisms be scaled to larger models without prohibitive computational costs? What are the optimal consistency metrics for different task types? How can the framework handle the tradeoff between internal consistency and external validity?

## Limitations
- Theoretical framework lacks empirical validation across diverse model architectures and tasks
- Heavy reliance on classification and synthesis rather than original experimental results
- "Consistency Is (Almost) Correctness" hypothesis requires rigorous statistical validation across multiple domains
- Potential computational overhead concerns when applying self-feedback during inference

## Confidence

**Medium**: The taxonomy and classification of existing methods appear thorough, though some categorization choices may be debatable given the rapid evolution of the field.

**Medium**: The proposed theoretical framework shows promise but requires empirical validation across different model architectures and tasks.

**Low**: Future research directions, while insightful, are speculative and lack grounding in systematic analysis of feasibility or potential impact.

## Next Checks

1. Implement and test the self-feedback framework across at least three different model architectures (encoder-only, decoder-only, and encoder-decoder) to verify generalizability claims.

2. Design a controlled experiment to statistically validate the "Consistency Is (Almost) Correctness" hypothesis across multiple domains and reasoning tasks.

3. Conduct ablation studies on the Self-Evaluation and Self-Update modules to quantify their individual contributions to model performance and identify potential redundancy or conflict between components.