---
ver: rpa2
title: 'LiHi-GS: LiDAR-Supervised Gaussian Splatting for Highway Driving Scene Reconstruction'
arxiv_id: '2412.15447'
source_url: https://arxiv.org/abs/2412.15447
tags:
- lidar
- image
- gaussian
- scene
- rendering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LiHi-GS, a Gaussian Splatting method with explicit
  LiDAR modeling for photorealistic 3D scene reconstruction in highway driving scenarios.
  The key innovation is a differentiable LiDAR rendering framework that enables both
  LiDAR supervision during training and LiDAR synthesis capability, addressing the
  limitations of prior methods that either use LiDAR only for initialization or ignore
  it entirely.
---

# LiHi-GS: LiDAR-Supervised Gaussian Splatting for Highway Driving Scene Reconstruction

## Quick Facts
- arXiv ID: 2412.15447
- Source URL: https://arxiv.org/abs/2412.15447
- Reference count: 40
- Key outcome: LiHi-GS introduces a differentiable LiDAR rendering framework that enables both LiDAR supervision during training and LiDAR synthesis capability for photorealistic 3D scene reconstruction in highway driving scenarios.

## Executive Summary
This paper proposes LiHi-GS, a Gaussian Splatting method with explicit LiDAR modeling for photorealistic 3D scene reconstruction in highway driving scenarios. The key innovation is a differentiable LiDAR rendering framework that enables both LiDAR supervision during training and LiDAR synthesis capability, addressing the limitations of prior methods that either use LiDAR only for initialization or ignore it entirely. The method introduces LiDAR visibility rates to handle sensor differences, decoupled camera-LiDAR pose optimization for temporal alignment, and 2D Gaussian scale compensation to preserve geometry at long distances. Evaluated on challenging highway datasets with sparse viewpoints and monotonous backgrounds, LiHi-GS outperforms state-of-the-art methods (NeuRAD, StreetGS, 3DGS, Instant-NGP) in both image and LiDAR novel view synthesis, particularly excelling at longer ranges (>183m) and scene editing tasks like ego/actor shifts.

## Method Summary
LiHi-GS extends Gaussian Splatting with a differentiable LiDAR rendering module that explicitly models LiDAR sensor characteristics. The framework introduces LiDAR visibility rates to account for differences between camera and LiDAR sensing, implements decoupled optimization of camera and LiDAR poses for temporal alignment, and employs 2D Gaussian scale compensation to maintain geometry fidelity at long distances. During training, LiDAR data provides direct supervision through a custom loss function that measures point-to-plane distances. The method also enables LiDAR novel view synthesis through its learned 3D representation, allowing for multi-modal scene understanding.

## Key Results
- Outperforms state-of-the-art methods (NeuRAD, StreetGS, 3DGS, Instant-NGP) in image and LiDAR novel view synthesis
- Achieves higher PSNR/SSIM scores and lower FID scores compared to baselines
- Excels particularly at longer ranges (>183m) and scene editing tasks like ego/actor shifts
- Demonstrates improved geometry learning through LiDAR supervision while maintaining real-time rendering capabilities

## Why This Works (Mechanism)
LiHi-GS addresses the fundamental challenge of highway driving scene reconstruction where traditional methods struggle with sparse viewpoints and monotonous backgrounds. By incorporating explicit LiDAR supervision, the method leverages the complementary sensing characteristics of LiDAR and cameras - LiDAR provides precise depth information while cameras capture appearance details. The differentiable LiDAR rendering framework allows the network to learn from both modalities simultaneously, with LiDAR visibility rates handling sensor-specific occlusions and the decoupled pose optimization ensuring temporal consistency between modalities.

## Foundational Learning

**Gaussian Splatting**: A rendering technique that projects 3D Gaussian primitives onto 2D image planes - needed for efficient neural scene representation and real-time rendering, quick check: can render at >30 FPS

**Differentiable LiDAR Rendering**: Framework that makes LiDAR simulation gradients computable - needed to enable end-to-end training with LiDAR supervision, quick check: gradients flow through LiDAR rendering module

**LiDAR Visibility Rates**: Probabilistic model of LiDAR ray occlusion - needed to handle fundamental differences between camera and LiDAR sensing, quick check: accounts for LiDAR's line-of-sight limitations

**Decoupled Pose Optimization**: Separate optimization of camera and LiDAR extrinsics - needed to handle temporal misalignment between sensors, quick check: reduces reprojection error for both modalities

**2D Gaussian Scale Compensation**: Adaptive scaling of Gaussian primitives based on distance - needed to preserve geometry at long ranges where traditional methods fail, quick check: maintains consistent splat size across depth ranges

## Architecture Onboarding

**Component Map**: Input Data -> Preprocessing -> Gaussian Initialization -> LiHi-GS Network -> Differentiable LiDAR Renderer -> Combined Loss -> Output 3D Scene

**Critical Path**: Camera images and LiDAR point clouds enter the network, undergo feature extraction, are rendered through both camera and LiDAR renderers, then combined losses guide parameter updates through backpropagation

**Design Tradeoffs**: LiDAR supervision improves geometry at cost of training complexity; decoupled pose optimization increases robustness but requires additional computation; 2D scale compensation improves long-range fidelity but adds parameter tuning complexity

**Failure Signatures**: Poor performance on urban scenes with complex occlusions; degraded results when LiDAR data is noisy or sparse; potential overfitting to highway monotony

**First Experiments**: 1) Ablation study removing LiDAR supervision to quantify geometry improvement, 2) Cross-dataset evaluation on urban environments, 3) User study comparing photorealism quality with baseline methods

## Open Questions the Paper Calls Out
The authors demonstrate strong technical innovation in addressing highway driving scene reconstruction challenges, but several limitations and uncertainties warrant consideration. The evaluation focuses primarily on quantitative metrics (PSNR, SSIM, FID) that may not fully capture photorealism quality in dynamic driving scenarios. The method's performance on urban environments with complex occlusions and multiple actors remains unexplored. Additionally, the computational overhead of LiDAR supervision during training could impact real-time deployment feasibility in resource-constrained autonomous systems.

## Limitations
- Evaluation relies heavily on quantitative metrics that may not capture true photorealism quality
- Performance on urban environments with complex occlusions and multiple actors remains unexplored
- Computational overhead of LiDAR supervision may impact real-time deployment feasibility

## Confidence
- LiDAR supervision improves geometry learning at long ranges: High
- Real-time rendering capability maintained: Medium
- Superior performance on monotonous backgrounds: High
- Effective ego/actor shift editing: Medium

## Next Checks
1. Evaluate on urban driving datasets with diverse traffic scenarios to test generalization beyond highway monotony
2. Conduct user studies comparing photorealism quality between LiHi-GS and baselines, supplementing quantitative metrics
3. Measure training and inference time overhead of LiDAR supervision on embedded hardware representative of autonomous vehicles