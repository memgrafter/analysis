---
ver: rpa2
title: Repeat-Aware Neighbor Sampling for Dynamic Graph Learning
arxiv_id: '2405.17473'
source_url: https://arxiv.org/abs/2405.17473
tags:
- temporal
- nodes
- neighbor
- sampling
- neighbors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses dynamic graph learning by introducing a repeat-aware
  neighbor sampling strategy to capture temporal evolution patterns in interactions.
  The key innovation is defining repeat-aware nodes as historically interacted destinations
  and extending this to high-order neighbors, then sampling recent neighbors before
  repeat interactions.
---

# Repeat-Aware Neighbor Sampling for Dynamic Graph Learning

## Quick Facts
- arXiv ID: 2405.17473
- Source URL: https://arxiv.org/abs/2405.17473
- Authors: Tao Zou; Yuhao Mao; Junchen Ye; Bowen Du
- Reference count: 40
- Key outcome: Achieves state-of-the-art link prediction performance with average precision up to 99.16% on Wikipedia dataset

## Executive Summary
This paper introduces a repeat-aware neighbor sampling strategy for dynamic graph learning that captures temporal evolution patterns by focusing on historically interacted nodes before repeat interactions. The approach extends to high-order neighbors and uses a time-aware aggregation mechanism to adaptively combine representations based on the significance of time interaction sequences. Experiments demonstrate state-of-the-art performance on link prediction tasks across six real-world datasets, outperforming existing methods like DyGFormer and GraphMixer.

## Method Summary
The paper addresses dynamic graph learning by defining repeat-aware nodes as historically interacted destinations and extending this concept to high-order neighbors. The RepeatMixer model employs an MLP-based encoder to learn temporal patterns from both first and high-order neighbor sequences, with a time-aware aggregation mechanism that combines representations using Pearson correlation coefficients. The model uses a sliding window approach to sample recent neighbors before repeat interactions, capturing pair-wise temporal patterns rather than individual node behavior. The method is evaluated on six datasets (Wikipedia, Reddit, MOOC, LastFM, Enron, UCI) for link prediction tasks.

## Key Results
- Achieves average precision scores up to 99.16% on Wikipedia dataset
- Consistently outperforms state-of-the-art baselines including DyGFormer (1st) and GraphMixer (2nd)
- Demonstrates generalizability by improving baseline models like TGN and TCL when integrated with repeat-aware sampling
- Shows effectiveness across both bipartite and non-bipartite graph structures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Repeat-aware neighbor sampling improves temporal pattern capture by including historically interacted nodes before repeat interactions
- Mechanism: Samples neighbors that appeared before repeat-aware nodes using a sliding window, capturing pair-wise temporal patterns between nodes
- Core assumption: Temporal evolution of interactions is better captured when considering the same interaction that occurred in the past
- Evidence anchors: [abstract] "whether two nodes will have interaction with each other in the future is highly correlated with the same interaction that happened in the past"; [section 3.1] "first-order repeat-aware nodes of the source node as the destination nodes that have interacted historically"

### Mechanism 2
- Claim: High-order repeat-aware sampling captures correlated temporal relationships in complex graph structures
- Mechanism: Extends first-order repeat-aware nodes to higher orders by treating neighbors of repeat-aware nodes as new repeat-aware nodes
- Core assumption: Temporal patterns between nodes are preserved and meaningful at higher-order neighborhoods
- Evidence anchors: [abstract] "we extend this concept to high orders as nodes in the destination node's high-order neighbors"; [section 3.1] "extend the concept into high orders to capture high-order temporal information"

### Mechanism 3
- Claim: Time-aware aggregation adaptively combines first and high-order temporal representations based on sequence similarity
- Mechanism: Computes Pearson correlation coefficients between time interval sequences of different orders, normalizes these as weights, and uses them to combine representations
- Core assumption: The significance of temporal patterns varies across orders and can be measured through time interval sequence similarity
- Evidence anchors: [abstract] "considering the varying temporal patterns on different orders, we introduce a time-aware aggregation mechanism that adaptively aggregates the temporal representations from different orders based on the significance of their interaction time sequences"; [section 3.3] "obtain the time-aware representations of interaction e = (u, v, t) by averaging the first-order representations and higher-order representations adaptively based on the significance in their time interaction sequences"

## Foundational Learning

- Concept: Dynamic graph representation learning
  - Why needed here: The paper operates in the domain of continuous-time dynamic graphs where interactions evolve over time, requiring temporal-aware node representations
  - Quick check question: What distinguishes continuous-time dynamic graph learning from snapshot-based approaches?

- Concept: Temporal pattern capture in sequences
  - Why needed here: The core innovation relies on capturing temporal patterns between nodes through their interaction sequences, not just static or recent behavior
  - Quick check question: How does the sliding window strategy in repeat-aware sampling differ from uniform or random sampling approaches?

- Concept: Graph sampling strategies and their tradeoffs
  - Why needed here: Understanding different neighbor sampling approaches (recent, uniform, time-aware) is crucial to appreciate why repeat-aware sampling provides advantages
  - Quick check question: What are the computational and representational tradeoffs between sampling recent neighbors versus repeat-aware neighbors?

## Architecture Onboarding

- Component map:
  Input: Dynamic graph as sequence of timestamped interactions with node/edge features -> Repeat-aware Neighbor Sampling: First-order and high-order neighbor extraction -> RepeatMixer Encoder: MLP-based architecture with segment embeddings -> Time-aware Aggregation: Pearson correlation-based weighting -> Output: Temporal representations for link prediction

- Critical path:
  1. Receive interaction (u, v, t) to predict
  2. Sample repeat-aware neighbors for both u and v at first and second order
  3. Encode temporal patterns using MLP with segment embeddings
  4. Aggregate representations using time-aware weighting
  5. Generate final representation for prediction

- Design tradeoffs:
  - Sampling strategy: Repeat-aware vs. recent neighbors (better pattern capture vs. computational complexity)
  - Order selection: First vs. second order (effectiveness vs. noise introduction)
  - Aggregation method: Adaptive weighting vs. simple concatenation (performance vs. model complexity)

- Failure signatures:
  - Poor performance on datasets with low repeat behavior ratios
  - Degradation when high-order sampling introduces irrelevant temporal information
  - Sensitivity to sliding window size parameter W

- First 3 experiments:
  1. Compare repeat-aware NSS against recent NSS on a dataset with high repeat behavior ratio to validate core mechanism
  2. Test RepeatMixer with first-order only vs. first + second order to measure high-order contribution
  3. Evaluate different aggregation methods (summation, concatenation, adaptive) to validate time-aware mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RepeatMixer scale with increasing temporal resolution (e.g., moving from daily to hourly interactions) in dynamic graphs?
- Basis in paper: [inferred] The paper uses Unix timestamps and demonstrates effectiveness across datasets with varying temporal granularity, but does not explicitly test performance across different temporal resolutions
- Why unresolved: The paper does not provide experiments varying the temporal granularity of interactions
- What evidence would resolve it: Experiments showing performance comparisons of RepeatMixer on the same dataset with different temporal resolutions would clarify how temporal granularity affects model performance

### Open Question 2
- Question: What is the impact of the repeat-aware neighbor sampling strategy on graph learning models in non-bipartite dynamic graphs compared to bipartite ones?
- Basis in paper: [explicit] The paper demonstrates effectiveness on both bipartite (Reddit, MOOC) and non-bipartite (Wikipedia, Enron, UCI) datasets, but does not provide a detailed comparative analysis
- Why unresolved: While the paper shows consistent performance across different graph types, it does not explicitly analyze whether the repeat-aware sampling strategy has differential effects on bipartite versus non-bipartite graphs
- What evidence would resolve it: A comparative study isolating the performance gains attributable to the repeat-aware sampling strategy specifically in bipartite versus non-bipartite graphs would clarify its differential impact

### Open Question 3
- Question: How does the RepeatMixer model perform in dynamic graph learning scenarios with extremely sparse interactions (e.g., less than 1% interaction density)?
- Basis in paper: [inferred] The paper evaluates on datasets with varying interaction densities but does not specifically test on extremely sparse graphs
- Why unresolved: The datasets used in the paper have reasonable interaction densities, and there is no explicit testing on graphs with extremely sparse interactions
- What evidence would resolve it: Experiments applying RepeatMixer to synthetic or real-world datasets with deliberately controlled, extremely low interaction densities would demonstrate the model's performance boundaries and robustness in sparse graph scenarios

## Limitations

- Performance claims rely heavily on datasets with high repeat interaction ratios, raising questions about generalizability to datasets with low temporal regularity
- The adaptive time-aware aggregation mechanism assumes Pearson correlation between time interval sequences effectively captures temporal significance, but this correlation measure's sensitivity to window size W=5 is not extensively validated
- High-order sampling (second-order) may introduce noise in sparse graphs, yet the paper doesn't provide ablation studies on different order depths or graph density thresholds

## Confidence

- High confidence: The repeat-aware neighbor sampling mechanism is well-defined and the empirical results show consistent improvements across multiple datasets
- Medium confidence: The time-aware aggregation approach is theoretically sound but depends on assumptions about temporal correlation that need further validation
- Low confidence: Generalizability claims to datasets with different temporal characteristics are not thoroughly supported by experiments

## Next Checks

1. **Dataset generalization test**: Evaluate RepeatMixer on a dataset with low repeat behavior ratio (<10%) to verify performance doesn't degrade significantly compared to recent-neighbor approaches
2. **Window size sensitivity analysis**: Systematically test different sliding window sizes (W=3, W=10, W=20) to measure impact on Pearson correlation-based aggregation and overall performance
3. **Ablation on high-order sampling**: Compare first-order only vs. first + second order performance across datasets with varying graph densities to identify when high-order sampling helps vs. hurts