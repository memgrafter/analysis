---
ver: rpa2
title: Advancing Speech Language Models by Scaling Supervised Fine-Tuning with Over
  60,000 Hours of Synthetic Speech Dialogue Data
arxiv_id: '2412.01078'
source_url: https://arxiv.org/abs/2412.01078
tags:
- speech
- instruction
- response
- large
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the scarcity of large-scale speech interaction
  datasets, particularly for Chinese, by constructing KE-SpeechChat, a high-quality
  synthetic dataset of over 60,000 hours of speech dialogues featuring 42,002 virtual
  speakers. The dataset is created by rewriting open-source instruction data into
  conversational formats and synthesizing speech using advanced TTS models with privacy-preserving
  voice profiles.
---

# Advancing Speech Language Models by Scaling Supervised Fine-Tuning with Over 60,000 Hours of Synthetic Speech Dialogue Data

## Quick Facts
- arXiv ID: 2412.01078
- Source URL: https://arxiv.org/abs/2412.01078
- Reference count: 30
- Primary result: Constructed KE-SpeechChat dataset with 60,000+ hours of synthetic speech dialogues and developed KE-Omni model for real-time Chinese and English speech interaction

## Executive Summary
This work addresses the scarcity of large-scale speech interaction datasets, particularly for Chinese, by constructing KE-SpeechChat, a high-quality synthetic dataset of over 60,000 hours of speech dialogues featuring 42,002 virtual speakers. The dataset is created by rewriting open-source instruction data into conversational formats and synthesizing speech using advanced TTS models with privacy-preserving voice profiles. The authors also introduce KE-Omni, a large speech language model capable of real-time Chinese and English speech interaction. Evaluations show KE-Omni outperforms baselines in speech-to-text instruction-following, modality alignment, and speech quality, validated by benchmarks like VoiceBench.

## Method Summary
The authors constructed KE-SpeechChat by rewriting open-source instruction data into conversational formats and synthesizing speech using advanced TTS models with privacy-preserving virtual voice libraries. The KE-Omni model architecture combines a frozen Whisper-large-v3 speech encoder with a LLaMA-3.1-8B-Instruct LLM and a streaming speech decoder. Training occurs in two stages: first fine-tuning the LLM on speech-text pairs, then training the speech decoder using chunk-based autoregressive generation with a compression ratio of 5 in the speech adapter. The model processes audio at 50 frames per second through the encoder, compressed to 10 frames per second for LLM processing.

## Key Results
- KE-Omni achieves superior performance on speech-to-text instruction-following tasks compared to baseline models
- The model demonstrates effective modality alignment between speech and text representations
- KE-Omni shows improved speech quality metrics validated on VoiceBench benchmark
- Real-time interaction capability demonstrated for both Chinese and English speech

## Why This Works (Mechanism)

### Mechanism 1
- Large-scale synthetic speech dialogue data enables training of speech language models that perform real-time interaction in both Chinese and English
- The dataset construction leverages LLMs for rewriting instruction data into conversational formats, TTS models for high-quality speech synthesis, and privacy-preserving virtual voice libraries
- Core assumption: Synthetic data quality can match or exceed human-recorded data for speech language model training when properly filtered and validated
- Evidence: [abstract] "over 60,000 hours of speech dialogues featuring 42,002 virtual speakers"; [section 3.2] "quality assurance and filtering on the synthetic speech"

### Mechanism 2
- Streaming speech synthesis with chunk-based autoregressive generation enables low-latency real-time interaction
- The speech decoder uses chunk size of 5 for autoregressive speech unit generation with delay of N steps
- Core assumption: Chunk-based autoregressive generation with appropriate delay parameters can achieve real-time performance without sacrificing speech quality
- Evidence: [section 4.3] "utilized a chunk-based autoregressive approach"; [section 5.1] "chunk size C is set to 5"

### Mechanism 3
- Freezing speech encoder parameters while fine-tuning only the speech adapter preserves robust speech representation while enabling effective speech-text alignment
- Whisper-large-v3 encoder processes audio at 50 fps, speech adapter compresses to 10 fps with compression ratio of 5
- Core assumption: The pretrained speech encoder's representations are sufficiently general to be useful across diverse speech interaction tasks without fine-tuning
- Evidence: [section 4.1] "parameters of the speech encoder are frozen, except for the speech adapter"; "utilize a compression ratio of 5 in our speech adapter"

## Foundational Learning

- **Text-to-speech synthesis quality and characteristics**: Why needed here - The entire dataset construction and model output depends on high-quality TTS synthesis that sounds natural and conversational. Quick check: What are the key metrics (DNSMOS, UTMOS) used to evaluate synthetic speech quality, and what thresholds indicate acceptable performance?

- **Speech-text alignment and modality bridging**: Why needed here - The model must align continuous speech signals with discrete text representations for effective processing and generation. Quick check: How does the compression ratio of 5 in the speech adapter affect the alignment between speech features and LLM token processing?

- **Autoregressive generation and streaming synthesis**: Why needed here - Real-time interaction requires generating speech incrementally rather than waiting for complete text responses. Quick check: What is the relationship between chunk size, delay steps, and achievable generation latency in streaming speech synthesis?

## Architecture Onboarding

- **Component map**: Speech encoder (Whisper-large-v3, frozen) → Speech adapter (compression ratio 5) → LLM (LLaMA-3.1-8B-Instruct) → Duration predictor (pretrained, frozen) → Speech unit generator (chunk-based autoregressive) → Unit-based vocoder (HiFi-GAN, frozen)
- **Critical path**: Speech input → Encoder → Adapter → LLM → Duration prediction → Unit generation → Speech output
- **Design tradeoffs**: Freezing encoder/vocoder vs. fine-tuning for task adaptation; chunk size vs. latency vs. quality; synthetic vs. real data for training
- **Failure signatures**: High CER/WER indicates poor speech-text alignment; low UTMOS indicates poor speech quality; high latency indicates suboptimal streaming parameters
- **First 3 experiments**:
  1. Validate synthetic speech quality meets CER/WER thresholds before training
  2. Test different chunk sizes and delay parameters for streaming latency/quality tradeoff
  3. Evaluate impact of freezing vs. fine-tuning speech encoder on alignment performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the impact of using different synthetic speech quality levels (e.g., varying DNSMOS thresholds) on the final KE-Omni model's performance?
- **Basis in paper**: [explicit] The paper discusses the use of DNSMOS ≥ 4.0 for constructing the voice library and filtering synthetic speech, but does not explore the effects of different quality thresholds
- **Why unresolved**: The paper uses a fixed DNSMOS threshold for quality assurance but does not investigate how varying this threshold might affect the model's performance or the dataset's overall quality
- **What evidence would resolve it**: Experimental results comparing KE-Omni models trained on datasets with different DNSMOS thresholds (e.g., ≥ 3.5, ≥ 4.0, ≥ 4.5) and their corresponding performance metrics

### Open Question 2
- **Question**: How does the inclusion of multi-turn conversations in the Ke-SpeechChat dataset affect the performance of KE-Omni in real-world dialogue scenarios?
- **Basis in paper**: [explicit] The paper mentions that the current Ke-SpeechChat dataset contains only single-turn conversations and that work on constructing multi-turn speech dialogues is underway
- **Why unresolved**: The paper does not provide insights into how multi-turn dialogues might improve the model's ability to handle more complex, context-dependent interactions
- **What evidence would resolve it**: Comparative studies of KE-Omni's performance on single-turn versus multi-turn dialogue tasks, including metrics such as response coherence, context retention, and user satisfaction

### Open Question 3
- **Question**: What are the potential risks and limitations of using synthetic data for training large speech language models, particularly in terms of bias and generalization to real-world scenarios?
- **Basis in paper**: [inferred] The paper highlights the use of synthetic data to address privacy concerns and data scarcity but does not extensively discuss potential biases or limitations in real-world applicability
- **Why unresolved**: The paper focuses on the construction and quality of synthetic data but does not delve into how synthetic data might introduce biases or affect the model's ability to generalize to diverse, real-world speech interactions
- **What evidence would resolve it**: Analysis of KE-Omni's performance on real-world speech datasets and identification of any biases or limitations in handling diverse accents, dialects, or spontaneous speech patterns

## Limitations

- Heavy reliance on synthetic data without direct comparison to human-recorded speech quality beyond filtering thresholds
- Streaming synthesis real-time performance claims lack empirical latency measurements and validation
- Frozen architecture components may limit adaptability to diverse speech characteristics without ablation studies

## Confidence

- **High Confidence**: Dataset construction methodology and model architecture specification are clearly described and technically sound
- **Medium Confidence**: Claims of outperforming baselines on benchmarks like VoiceBench are supported by methodology but lack detailed performance gap analysis
- **Low Confidence**: Assertions about synthetic data quality matching or exceeding human-recorded data are based on filtering thresholds rather than comprehensive quality comparisons

## Next Checks

1. **Synthetic vs. Real Data Quality Comparison**: Conduct controlled experiment comparing model performance trained on synthetic data versus equivalent amounts of human-recorded speech data, measuring subjective quality metrics like DNSMOS and UTMOS, as well as human preference studies for conversational naturalness.

2. **Streaming Latency Measurement**: Implement comprehensive latency benchmarking of the streaming synthesis approach, measuring end-to-end response time from speech input to generated output across different chunk sizes and delay parameters, and compare against real-time thresholds for conversational applications.

3. **Component Freezing Ablation Study**: Perform ablation study systematically unfreezing each frozen component (speech encoder, duration predictor, unit-based vocoder) while keeping others frozen, measuring impact on speech quality, alignment performance, and overall task metrics to quantify cost of frozen architecture approach.