---
ver: rpa2
title: Large Language Models Must Be Taught to Know What They Don't Know
arxiv_id: '2406.08391'
source_url: https://arxiv.org/abs/2406.08391
tags:
- confidence
- answer
- language
- uncertainty
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors study uncertainty estimation in large language models\
  \ (LLMs), addressing the problem that current out-of-the-box methods for eliciting\
  \ uncertainty\u2014such as perplexity or verbal prompts\u2014are unreliable and\
  \ poorly calibrated for open-ended generation tasks. They propose a fine-tuning\
  \ approach that trains the model to predict its own correctness using a small, labeled\
  \ dataset of model generations marked as correct or incorrect."
---

# Large Language Models Must Be Taught to Know What They Don't Know

## Quick Facts
- arXiv ID: 2406.08391
- Source URL: https://arxiv.org/abs/2406.08391
- Reference count: 40
- Primary result: Fine-tuning LLMs on correctness labels achieves strong uncertainty calibration with only ~1000 examples

## Executive Summary
Large language models struggle with reliable uncertainty estimation, often providing overconfident predictions on incorrect answers. Current black-box methods like perplexity and verbal prompts fail to produce well-calibrated uncertainty estimates for open-ended generation tasks. This paper proposes a fine-tuning approach that trains models to predict their own correctness using a small labeled dataset, achieving state-of-the-art calibration and selective prediction performance.

The method uses low-rank adapters (LoRA) to efficiently fine-tune base models on correct/incorrect answer pairs, with temperature scaling for post-hoc calibration. Only around 1000 labeled examples are needed to significantly outperform simpler baselines. The approach generalizes across subjects and formats, and can even estimate uncertainty for other models' generations. A user study demonstrates that calibrated confidence scores improve human-AI collaboration.

## Method Summary
The authors propose fine-tuning large language models to predict their own correctness using a small labeled dataset of correct and incorrect answers. They use low-rank adapters (LoRA) to efficiently update the model parameters, along with a KL regularization term to maintain the base model's behavior. The fine-tuning dataset consists of (question, answer, correctness) tuples, where correctness is determined by a separate grading model. During inference, the model outputs both an answer and a confidence score indicating its predicted correctness.

## Key Results
- Fine-tuning on ~1000 examples achieves ECE of 0.05-0.08 and AUROC of 0.85-0.90, outperforming black-box baselines
- LoRA fine-tuning through model features generalizes better than frozen-feature probes across subjects and formats
- Fine-tuned models can estimate uncertainty for other models' generations, with some models performing better on different models than themselves
- Calibrated confidence scores improve human-AI collaboration in a user study

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fine-tuning on a small labeled dataset of correct/incorrect answers can significantly improve LLM uncertainty calibration compared to black-box methods.
- **Mechanism**: Supervised learning trains the model to predict its own correctness by updating either the model's internal features (via LoRA) or adding a probe classifier on top of frozen features. This learns a direct mapping from model generations to correctness labels.
- **Core assumption**: There exists a learnable function from model internal representations or generation patterns to the correctness of the answer.
- **Evidence anchors**:
  - [abstract] "we show that fine-tuning on a small dataset of correct and incorrect answers can create an uncertainty estimate with good generalization and small computational overhead"
  - [section 5] "We consider three base models... and their instruction-tuned variants. For fine-tuning, we use 8-bit quantization and Low-Rank Adapters (LoRA)" and "we show that having around 1000 labeled examples is enough to improve performance over simpler baselines"
  - [corpus] Weak - no direct citations found, but the approach aligns with general supervised learning principles
- **Break condition**: If the mapping from internal representations to correctness is too complex or non-stationary, the small dataset may not capture sufficient patterns.

### Mechanism 2
- **Claim**: LoRA fine-tuning through the model's features outperforms frozen-feature probes for open-ended generation tasks.
- **Mechanism**: LoRA updates low-rank approximations of weight matrices, allowing the model to adjust intermediate features specifically for the task of predicting correctness, rather than relying on general language modeling features.
- **Core assumption**: The features learned during pretraining are not optimal for the task of uncertainty estimation, and task-specific feature adaptation improves performance.
- **Evidence anchors**:
  - [section 5] "We study three that vary in their number of trainable parameters and their use of prompting... LoRA: This parameterization is the same as Probe but with low-rank adapters (LoRA) added to the base model"
  - [section 6] "Probe is insufficient to generalize effectively from MC to OE, but training through the features of the model (LoRA + Prompt) does generalize effectively"
  - [corpus] Weak - general evidence for LoRA effectiveness but not specifically for uncertainty calibration
- **Break condition**: If the feature space is already well-suited for uncertainty estimation, additional adaptation may provide minimal benefit or could overfit.

### Mechanism 3
- **Claim**: Fine-tuned uncertainty models generalize across subjects, formats, and even to other models' generations without needing model-specific knowledge.
- **Mechanism**: The fine-tuning process learns general patterns of when models are likely to be correct or incorrect, rather than memorizing specific model behaviors, allowing transfer to new domains and other models.
- **Core assumption**: Correctness patterns are more related to the content and structure of questions/answers than to the specific model generating them.
- **Evidence anchors**:
  - [abstract] "We also investigate the mechanisms that enable reliable LLM uncertainty estimation, finding that many models can be used as general-purpose uncertainty estimators, applicable not just to their own uncertainties but also the uncertainty of other models"
  - [section 6] "we show that Mistral 7B actual has better AUROC values when applied to LLaMA-2 7B than LLaMA-2 7B applied to itself" and "LoRA + Prompt estimates generalize surprisingly well but is much worse than a model trained on the original answers"
  - [corpus] Weak - no direct citations found, but the concept aligns with transfer learning principles
- **Break condition**: If correctness patterns are highly model-specific or domain-dependent, generalization may fail.

## Foundational Learning

- **Concept**: Expected Calibration Error (ECE)
  - **Why needed here**: ECE is the primary metric for measuring how well predicted probabilities match empirical frequencies of correctness.
  - **Quick check question**: If a model assigns 70% confidence to 100 predictions and 70 of them are correct, what is its ECE (assuming perfect binning)?

- **Concept**: Area Under the Receiver Operating Characteristic Curve (AUROC)
  - **Why needed here**: AUROC measures the model's ability to discriminate between correct and incorrect answers, which is crucial for selective prediction.
  - **Quick check question**: If a perfect uncertainty estimator gets AUROC of 1.0, what AUROC would a random estimator achieve?

- **Concept**: Low-Rank Adaptation (LoRA)
  - **Why needed here**: LoRA enables efficient fine-tuning by updating low-rank approximations of weight matrices rather than full weights, making uncertainty calibration tractable for large models.
  - **Quick check question**: What is the key advantage of using LoRA over full fine-tuning for large language models?

## Architecture Onboarding

- **Component map**: Base LLM -> LoRA adapters -> Probe classifier -> Temperature scaling -> Evaluation metrics
- **Critical path**: Generate predictions and correctness labels for fine-tuning data -> Fine-tune base model with LoRA or probe classifier -> Apply temperature scaling on held-out calibration set -> Evaluate on test sets across different domains and formats
- **Design tradeoffs**:
  - LoRA vs full fine-tuning: parameter efficiency vs potential performance
  - Probe vs LoRA: computational simplicity vs feature adaptation
  - Dataset size: generalization vs labeling cost
  - Temperature scaling: post-hoc calibration vs end-to-end learning
- **Failure signatures**:
  - Poor ECE despite good accuracy: miscalibration persists
  - Low AUROC: inability to discriminate correct/incorrect
  - Generalization failure: overfitting to training distribution
  - Computational inefficiency: LoRA rank too high or training not optimized
- **First 3 experiments**:
  1. Compare ECE/AUROC of base model vs probe vs LoRA on MC MMLU
  2. Vary fine-tuning dataset size (100, 1000, 5000 examples) and measure performance
  3. Test generalization by applying model trained on MC MMLU to OE MMLU and vice versa

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can large language models be trained to produce calibrated uncertainty estimates without requiring separate models for question answering and uncertainty estimation?
- Basis in paper: [explicit]
- Why unresolved: The paper currently relies on two separate models for question answering and uncertainty estimation, with a goal of creating a single model that can handle both tasks.
- What evidence would resolve it: Development and validation of a unified model architecture that generates both answers and uncertainty estimates, showing comparable or improved performance to the current dual-model approach.

### Open Question 2
- Question: How do different pre-training or alignment procedures affect the ability of large language models to produce calibrated uncertainty estimates?
- Basis in paper: [explicit]
- Why unresolved: The paper suggests that an uncertainty-aware pre-training or alignment phase might become essential but implementing such a procedure while maintaining base language modeling abilities presents a challenging online learning problem.
- What evidence would resolve it: Comparative studies of various pre-training or alignment methods on their impact on uncertainty calibration, including both performance metrics and computational efficiency.

### Open Question 3
- Question: How do uncertainty estimates from fine-tuned models generalize to tasks with significantly different formats or subject matter than those used in fine-tuning?
- Basis in paper: [explicit]
- Why unresolved: While the paper shows some generalization across distribution shifts, it notes that fine-tuning relies on a diverse set of samples and the extent of generalization to significantly different tasks is not fully explored.
- What evidence would resolve it: Systematic evaluation of uncertainty estimates on a wide range of tasks with varying formats and subject matter, including tasks with no overlap to the fine-tuning dataset.

## Limitations

- The approach requires a labeled dataset of correct/incorrect answers, which may be expensive to obtain for new domains
- Generalization to highly specialized domains or languages not represented in the training data remains untested
- The paper does not address potential biases in the grading process or how subjective questions might affect calibration quality
- Cross-model generalization shows mixed results, with performance varying significantly across model pairs

## Confidence

- **High Confidence**: The core finding that fine-tuned uncertainty models outperform black-box methods (perplexity, verbal prompts) in both ECE and AUROC metrics
- **Medium Confidence**: The claim that ~1000 labeled examples are sufficient for good calibration
- **Medium Confidence**: The assertion that LoRA fine-tuning through model features outperforms frozen-feature probes
- **Low Confidence**: The broad claim about general-purpose uncertainty estimation across all models

## Next Checks

1. **Domain Transfer Robustness**: Test the fine-tuned uncertainty models on specialized domains (medical, legal, technical) with different question-answering patterns to validate claims about broad generalization.

2. **Subjectivity Impact**: Evaluate the calibration performance on subjective or opinion-based questions where correctness is not binary, testing whether the method degrades when objective ground truth is unavailable.

3. **Cost-Benefit Analysis**: Compare the computational and labeling costs of fine-tuning vs. the performance gains across different dataset sizes to determine the optimal trade-off for practical deployment.