---
ver: rpa2
title: 'Ensuring Safety and Trust: Analyzing the Risks of Large Language Models in
  Medicine'
arxiv_id: '2411.14487'
source_url: https://arxiv.org/abs/2411.14487
tags:
- medical
- safety
- llms
- performance
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces MedGuard, a comprehensive benchmark for\
  \ evaluating the safety and trustworthiness of large language models (LLMs) in medical\
  \ applications. MedGuard covers five key principles\u2014Truthfulness, Resilience,\
  \ Fairness, Robustness, and Privacy\u2014across ten specific aspects, using 1,000\
  \ expert-verified questions."
---

# Ensuring Safety and Trust: Analyzing the Risks of Large Language Models in Medicine

## Quick Facts
- arXiv ID: 2411.14487
- Source URL: https://arxiv.org/abs/2411.14487
- Reference count: 40
- Primary result: Current LLMs show significant safety gaps in medical applications, with domain-specific models underperforming generalists and prompt engineering providing limited improvements.

## Executive Summary
This study introduces MedGuard, a comprehensive benchmark for evaluating the safety and trustworthiness of large language models (LLMs) in medical applications. Testing 11 widely used LLMs reveals that current models generally perform poorly on most safety aspects, especially compared to human physicians. Medical-domain-specific models like Meditron-70B show significant safety shortcomings, and larger models tend to be safer but not consistently so. Prompt engineering provides limited safety improvements. The findings underscore the need for enhanced safety mechanisms and human oversight before deploying LLMs in high-stakes medical settings.

## Method Summary
The study systematically evaluates 11 LLMs using MedGuard, a benchmark consisting of 1,000 expert-verified multiple-choice questions covering ten safety aspects across five principles: Truthfulness, Resilience, Fairness, Robustness, and Privacy. Models are tested using standardized prompts with temperature set to zero for deterministic outputs. Safety performance is measured via Safety Index and compared to human physician performance using statistical tests like McNemar's test. The evaluation includes proprietary, open-source, and domain-specific models.

## Key Results
- Current LLMs generally perform poorly on medical safety aspects compared to human physicians
- Domain-specific models like Meditron-70B underperform generalist models on safety despite medical specialization
- Larger models show better safety performance but improvements are not uniform across all safety aspects
- Prompt engineering techniques provide only limited and inconsistent safety improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger LLMs show better safety performance in medical applications due to their capacity to encode more nuanced patterns and longer context windows.
- Mechanism: The increased model size allows for better generalization and more comprehensive handling of complex inputs, which reduces biased outputs and improves reasoning in clinical scenarios.
- Core assumption: The improvements in safety are driven by model size rather than just fine-tuning strategy or domain adaptation.
- Evidence anchors:
  - [abstract] "We also observed an imbalance between improvements in accuracy and safety, with models showing greater progress in accuracy than in safety-related outcomes."
  - [section] "While larger models are generally more capable, our results also show that simply scaling up models is not a reliable approach for achieving safer outcomes, for example Meditron-70B is not better than Llama3-8B-Instruct with respect to safety performance."
  - [corpus] Weak; no corpus neighbors directly address the mechanism.
- Break condition: If safety gains plateau or reverse as models scale further, or if fine-tuning for domain specificity harms safety despite size increases.

### Mechanism 2
- Claim: Fine-tuning LLMs on medical data does not necessarily improve safety and may degrade it by overwriting general safety alignments.
- Mechanism: Domain-specific models lose general safety alignments during fine-tuning, prioritizing domain knowledge over safety mechanisms, leading to higher hallucination and fairness issues.
- Core assumption: Safety alignments are not domain-agnostic and can be eroded during specialized fine-tuning.
- Evidence anchors:
  - [abstract] "Despite recent reports indicate that advanced LLMs like ChatGPT can match or even exceed human performance in various medical tasks, this study underscores a significant safety gap..."
  - [section] "In our evaluation of domain-specific LLMs, such as Meditron-70B, we observed that these models do not perform as safely as generalist models... This discrepancy likely stems from the prioritization of domain-specific knowledge during fine-tuning."
  - [corpus] Weak; corpus lacks direct evidence of safety degradation from fine-tuning.
- Break condition: If safety alignments are explicitly preserved during fine-tuning, or if new fine-tuning methods specifically target safety alongside domain knowledge.

### Mechanism 3
- Claim: Prompt engineering provides only limited and inconsistent improvements in LLM safety for medical tasks.
- Mechanism: The unsafe behaviors are learned during training and ingrained in the model, so external instructions (e.g., Chain-of-Thought, safety prompts) cannot reliably override them.
- Core assumption: Unsafe model behaviors are not easily mitigated by surface-level input modifications.
- Evidence anchors:
  - [abstract] "While incorporating certain safety-oriented prompting techniques can slightly improve performance, these gains are limited and inconsistent."
  - [section] "Prompt engineering is often used as an easily accessible option for enhancing LLM performance... our findings demonstrate that they do not necessarily translate into better safety outcomes."
  - [corpus] Weak; corpus does not provide specific evidence on prompting limits.
- Break condition: If robust prompt engineering techniques are developed that consistently improve safety, or if model architectures are changed to make safety more responsive to prompts.

## Foundational Learning

- Concept: Five principles of medical AI safety (Truthfulness, Resilience, Fairness, Robustness, Privacy)
  - Why needed here: These principles provide a structured framework to evaluate and compare LLM safety in medical contexts, covering both technical and ethical dimensions.
  - Quick check question: What are the five principles used in MedGuard to assess LLM safety in medicine?

- Concept: Benchmarking via multiple-choice questions with expert verification
  - Why needed here: MCQs enable scalable, automated, and fair evaluation while expert verification ensures real-world clinical relevance and accuracy.
  - Quick check question: Why does MedGuard use multiple-choice questions instead of free text for safety evaluation?

- Concept: Inter-annotator agreement for human performance baseline
  - Why needed here: High agreement between human experts validates the benchmark's reliability and establishes a strong baseline for LLM comparison.
  - Quick check question: How is the reliability of human performance measurements ensured in the MedGuard study?

## Architecture Onboarding

- Component map: Question generation (GPT-4) -> Manual verification (domain experts) -> Evaluation pipeline -> Safety Index scoring -> Leaderboard comparison
- Critical path: Generate questions (using GPT-4 and seed examples) → Manual verification by domain experts → Deploy in evaluation pipeline → Compare LLM outputs to ground truth or human experts → Analyze results by principle and aspect
- Design tradeoffs: Multiple-choice format simplifies automated evaluation but may not capture the full range of unsafe generative behaviors; expert verification ensures quality but limits scalability
- Failure signatures: Low scores in Equity or Resilience aspects, inconsistent prompt engineering results, domain-specific models underperforming generalists, or large gaps between accuracy and safety indices
- First 3 experiments:
  1. Evaluate a new LLM on MedGuard using the basic prompt and compare its safety index to accuracy index
  2. Test prompt engineering variations (CoT, safe prompt, combined) on the same model and measure safety improvements
  3. Run human expert evaluation on a subset of questions to validate LLM performance gaps and refine benchmark difficulty

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific safety mechanisms, beyond prompt engineering, are most effective at improving LLM safety in medical applications?
- Basis in paper: [explicit] The paper states that "incorporating certain safety-oriented prompting techniques can slightly improve performance, but the gains are limited and inconsistent," and suggests "more robust methods, such as fine-tuning with safety-focused datasets or redesigning the training process to emphasize safety, may be necessary."
- Why unresolved: The paper demonstrates that current prompt engineering techniques have limited and inconsistent effects on improving safety, highlighting the need for more effective approaches but not specifying which methods are most promising.
- What evidence would resolve it: Comparative studies evaluating different safety enhancement techniques (e.g., fine-tuning approaches, architectural modifications, training data curation) on comprehensive benchmarks like MedGuard would identify the most effective methods.

### Open Question 2
- Question: How can domain-specific medical LLMs be trained or fine-tuned to achieve safety performance comparable to generalist LLMs?
- Basis in paper: [explicit] The paper finds that "domain-specific models like PMC-LLaMA-13B and Meditron-70B underperform across several safety principles despite their specialized training or fine-tuning," suggesting that "more research is needed to investigate why these domain-specific models exhibit reduced safety compared to generalist LLMs."
- Why unresolved: The paper identifies a significant performance gap between domain-specific and generalist models in safety aspects but does not explain the underlying causes or propose solutions to bridge this gap.
- What evidence would resolve it: Research examining the training processes, data curation, and fine-tuning strategies of both domain-specific and generalist models could reveal the factors contributing to safety performance differences and guide improvements in domain-specific model development.

### Open Question 3
- Question: What are the long-term effects of continual fine-tuning on LLM safety, and how can catastrophic forgetting of safety alignments be prevented?
- Basis in paper: [explicit] The paper notes that "further fine-tuning can cause the model to forget previous safety alignments," citing related research on catastrophic forgetting in LLMs during continual fine-tuning.
- Why unresolved: While the paper acknowledges the risk of catastrophic forgetting of safety alignments during fine-tuning, it does not explore the extent of this issue or propose methods to prevent it in the context of medical LLMs.
- What evidence would resolve it: Longitudinal studies tracking the safety performance of LLMs through multiple rounds of fine-tuning, along with evaluations of techniques designed to mitigate catastrophic forgetting (e.g., elastic weight consolidation, rehearsal methods), would clarify the long-term impact and potential solutions.

## Limitations

- The benchmark relies on multiple-choice format, which may not fully capture the range of unsafe behaviors in open-ended medical interactions.
- Safety improvements from prompt engineering are limited and inconsistent, suggesting that surface-level interventions may be insufficient for ensuring reliable safety in high-stakes medical contexts.
- Domain-specific models like Meditron-70B underperform on safety despite their medical focus, raising concerns about current fine-tuning approaches for specialized LLMs.

## Confidence

- **High Confidence**: The observation that larger models generally perform better on safety metrics, though not uniformly (supported by direct comparisons across 11 models and statistical tests).
- **Medium Confidence**: The claim that domain-specific fine-tuning degrades safety due to loss of general safety alignments (based on comparative analysis but limited by lack of direct evidence on fine-tuning methods).
- **Low Confidence**: The assertion that prompt engineering cannot reliably improve safety (supported by experimental results but acknowledges that future prompt techniques may be more effective).

## Next Checks

1. Replicate the evaluation with open-source medical LLMs using publicly available fine-tuning protocols to verify if safety degradation is consistent across different training approaches.
2. Test additional prompt engineering techniques beyond Chain-of-Thought and safety prompts, including dynamic prompt adaptation based on model responses.
3. Conduct a longitudinal study tracking safety performance as newer, larger models are released to determine if scaling laws hold for medical safety specifically.