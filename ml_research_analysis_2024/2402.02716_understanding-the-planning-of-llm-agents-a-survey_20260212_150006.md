---
ver: rpa2
title: 'Understanding the planning of LLM agents: A survey'
arxiv_id: '2402.02716'
source_url: https://arxiv.org/abs/2402.02716
tags:
- planning
- arxiv
- preprint
- language
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically categorizes recent works on improving
  the planning ability of LLM-based agents into five directions: Task Decomposition,
  Multi-plan Selection, External Module, Reflection and Memory. It provides detailed
  analysis of each category''s ideas and limitations.'
---

# Understanding the planning of LLM agents: A survey

## Quick Facts
- arXiv ID: 2402.02716
- Source URL: https://arxiv.org/abs/2402.02716
- Authors: Xu Huang; Weiwen Liu; Xiaolong Chen; Xingmei Wang; Hao Wang; Defu Lian; Yasheng Wang; Ruiming Tang; Enhong Chen
- Reference count: 10
- Key outcome: Survey categorizes LLM agent planning works into five directions: Task Decomposition, Multi-plan Selection, External Module, Reflection, and Memory, showing performance increases with expenses and that reflection improves success rates for complex tasks.

## Executive Summary
This survey provides a comprehensive taxonomy of recent works on improving the planning ability of LLM-based agents. It systematically categorizes approaches into five main directions: Task Decomposition, Multi-plan Selection, External Module, Reflection, and Memory. The survey analyzes each category's ideas and limitations while presenting experimental results across four benchmarks that demonstrate how different planning strategies affect performance and costs.

## Method Summary
The survey implements six prompt-based methods (ZeroShot-CoT, Fewshot-CoT, CoT-SC, SayCan, ReAct, Reflexion) using OpenAI's text-davinci-003 on four benchmarks (ALFWorld, ScienceWorld, HotPotQA, FEVER). Experiments measure success rates, average rewards, and expenses (token costs in USD). The minimum viable reproduction requires setting up OpenAI API access, implementing the six methods, and running experiments on all four benchmarks while recording the specified metrics.

## Key Results
- Performance increases with expenses across all evaluated methods and benchmarks
- Reflection mechanisms improve success rates, especially for complex tasks
- LLM agents suffer from hallucinations leading to irrational plans and actions
- Current evaluation environments lack fine-grained step-wise assessments and realistic multi-modal feedback

## Why This Works (Mechanism)

### Mechanism 1
Task decomposition methods improve planning by breaking complex tasks into simpler sub-goals, allowing incremental handling of multi-step reasoning. The LLM decomposes tasks using prompts, then plans for each sub-goal sequentially, interleaving decomposition and planning based on feedback. Core assumption: Decomposition reduces cognitive load and prevents hallucinations in long reasoning chains.

### Mechanism 2
Multi-plan selection improves success by generating diverse candidate plans and using heuristic search to select optimal ones. The LLM generates multiple distinct plans via sampling strategies, then search algorithms evaluate and select candidates. Core assumption: Exploring multiple options increases likelihood of finding feasible optimal plans despite LLM variability.

### Mechanism 3
Reflection and refinement enable error correction through self-assessment and feedback integration. After plan generation, the LLM produces feedback, identifies errors, and iteratively refines the plan. Core assumption: LLMs can recognize their own reasoning flaws and generate actionable feedback for correction.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: Structures reasoning into explicit intermediate steps, making complex problem-solving more transparent and tractable for LLMs
  - Quick check question: How does CoT prompting differ from zero-shot prompting in terms of reasoning depth and task decomposition?

- Concept: Prompt engineering for task decomposition
  - Why needed here: Decomposing tasks requires carefully crafted prompts that instruct LLMs to break down goals into manageable sub-tasks, often using few-shot examples
  - Quick check question: What are the risks of task forgetting or hallucination when decomposing complex tasks into many sub-goals?

- Concept: External symbolic planners (PDDL)
  - Why needed here: Provide deterministic, constraint-aware planning that complements LLM's flexibility but lack natural language understanding
  - Quick check question: Why might integrating PDDL-based planners with LLMs improve plan feasibility compared to LLM-only planning?

## Architecture Onboarding

- Component map:
  Input -> Task Decomposition Module (optional) -> Plan Generation Module -> Multi-plan Selection Module (optional) -> External Planner Module (optional) -> Reflection/Refinement Module (optional) -> Memory Module (optional) -> Execution Environment

- Critical path:
  1. Task decomposition (if used)
  2. Plan generation
  3. Plan selection or refinement
  4. Execution and feedback
  5. Optional: Reflection and memory update

- Design tradeoffs:
  - Decomposition-first vs interleaved: Decomposition-first provides clearer task alignment but less adaptability; interleaved allows dynamic adjustment but risks longer reasoning chains
  - Single plan vs multi-plan: Single plans are cheaper but risk failure; multi-plan increases success likelihood but costs more tokens and computation
  - LLM-only vs hybrid with external planners: LLM-only is flexible but may hallucinate; hybrid improves feasibility but adds complexity

- Failure signatures:
  - Hallucinations in generated plans
  - Plan infeasibility due to unmet preconditions
  - Looping or redundant sub-tasks
  - Token exhaustion before plan completion
  - Reflection feedback reinforcing errors

- First 3 experiments:
  1. Compare success rates of zero-shot CoT vs decomposition-first prompting on a multi-step reasoning task
  2. Measure token usage and success rates when varying the number of candidate plans in multi-plan selection
  3. Evaluate the impact of a single reflection step on plan success rate for a complex task with known pitfalls

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we quantify and mitigate hallucinations in LLM-based planning agents to ensure the feasibility of generated plans?
- Basis in paper: The paper explicitly mentions that LLM agents often suffer from hallucinations, leading to irrational plans and actions that interact with non-existent items in the environment
- Why unresolved: Hallucinations are fundamental challenges due to LLMs' statistical learning nature and lack of symbolic reasoning capabilities
- What evidence would resolve it: Quantifiable metrics for hallucination detection in planning contexts and demonstrated techniques to reduce hallucination rates while maintaining performance

### Open Question 2
- Question: What are the trade-offs between real-time memory updates using RAG-based methods versus parameter updates using fine-tuning for LLM agents, and how can we optimize this balance?
- Basis in paper: The paper discusses both RAG-based and fine-tuning-based memory approaches, highlighting their distinct advantages and limitations in terms of update costs and memory capacity
- Why unresolved: The paper acknowledges trade-offs but does not provide comprehensive analysis or guidelines for choosing between approaches in different scenarios
- What evidence would resolve it: Comparative studies across various task domains and memory update frequencies, along with frameworks for optimal memory strategy selection

### Open Question 3
- Question: How can we develop more realistic and fine-grained evaluation environments for LLM-based planning agents that better reflect real-world scenarios?
- Basis in paper: The paper criticizes existing benchmarks for lacking fine-grained step-wise evaluations and using rule-based, simplistic feedback distant from real-world scenarios
- Why unresolved: The paper identifies limitations but does not propose concrete solutions for creating more realistic benchmarks
- What evidence would resolve it: Development of new evaluation environments with multi-modal feedback, complex constraints, and multiple valid solution paths

## Limitations
- Experimental results show correlation between expenses and performance but don't establish causation or prove fundamental superiority
- Corpus evidence supporting individual mechanisms is notably weak, with most claims relying on abstract statements
- Taxonomy boundaries between categories (especially Reflection and Memory) are not clearly delineated

## Confidence
- High Confidence: Taxonomy categorization into five directions is well-supported and provides useful organizational framework; experimental methodology is methodologically sound
- Medium Confidence: Claims about performance increasing with expenses and reflection improving success rates are plausible but require careful interpretation
- Low Confidence: Specific claims about superiority of particular mechanisms lack direct empirical support; effectiveness of external symbolic planners is theorized but not empirically validated

## Next Checks
1. Direct Mechanism Comparison: Conduct controlled experiments comparing decomposition-first prompting against interleaved decomposition on identical benchmarks, measuring success rates, hallucination rates, token efficiency, and plan feasibility scores
2. Multi-Plan Selection Calibration: Systematically vary the number of candidate plans generated (1, 3, 5, 10) while measuring success rates, token costs, and computational time to identify optimal trade-off point
3. Reflection Feedback Quality Analysis: Implement automated evaluation of reflection feedback quality by measuring how often self-generated feedback correctly identifies plan errors versus reinforcing existing mistakes, comparing against external expert feedback