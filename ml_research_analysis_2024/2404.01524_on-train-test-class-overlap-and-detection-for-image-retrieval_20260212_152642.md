---
ver: rpa2
title: On Train-Test Class Overlap and Detection for Image Retrieval
arxiv_id: '2404.01524'
source_url: https://arxiv.org/abs/2404.01524
tags:
- training
- image
- retrieval
- evaluation
- sets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper analyzes the impact of class overlap between training
  and evaluation sets in image retrieval, focusing on the popular GLDv2-clean dataset.
  It identifies and removes landmark categories that overlap with Revisited Oxford
  and Paris evaluation sets, creating a new dataset called RGLDv2-clean.
---

# On Train-Test Class Overlap and Detection for Image Retrieval

## Quick Facts
- arXiv ID: 2404.01524
- Source URL: https://arxiv.org/abs/2404.01524
- Reference count: 40
- Key outcome: Removing overlapping landmark categories from training data dramatically reduces retrieval performance, revealing class overlap as a critical evaluation bias.

## Executive Summary
This paper analyzes class overlap between training and evaluation sets in image retrieval, focusing on the GLDv2-clean dataset and its overlap with Revisited Oxford and Paris evaluation sets. The authors identify and remove overlapping landmark categories to create RGLDv2-clean, demonstrating that the original dataset leads to artificially inflated performance. They also introduce CiDeR, a single-stage end-to-end method for detecting objects of interest and extracting global image representations using spatial attention without location supervision. CiDeR outperforms state-of-the-art approaches on both existing clean datasets and the new RGLDv2-clean dataset.

## Method Summary
The paper presents two main contributions: analyzing class overlap in image retrieval datasets and proposing CiDeR, a single-stage end-to-end method for detecting objects of interest and extracting global image representations. CiDeR uses a ResNet101 backbone with SENet enhancement, selective context modules, and an attentional localization component that generates spatial attention maps without location supervision. The method applies multiple thresholding operations to create binary masks, which are then applied to feature tensors and fused with learnable weights. The model is trained using ArcFace loss and multi-resolution representations with whitening.

## Key Results
- Using GLDv2-clean leads to artificially inflated performance due to class overlap, with dramatic performance drops when using RGLDv2-clean
- CiDeR achieves state-of-the-art performance on both existing clean datasets (NC-clean, SfM-120k) and the new RGLDv2-clean dataset
- The method significantly improves mean average precision (mAP) across various evaluation protocols compared to existing approaches
- CiDeR outperforms detection-then-retrieval baselines without requiring location-annotated training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing overlapping landmark categories from the training set significantly degrades retrieval performance because the model overfits to the exact evaluation instances.
- Mechanism: When training and evaluation sets share the same landmark categories, the model learns to recognize specific landmark features instead of learning generalizable landmark representations. This allows the model to memorize evaluation landmarks during training, leading to artificially inflated performance.
- Core assumption: The performance drop is not due to the small number of removed images (only 1,565 out of 1.58 million) but due to the loss of landmark-specific memorization.
- Evidence anchors:
  - [abstract] "Not only is there a dramatic drop in performance, but it is inconsistent across methods, changing the ranking."
  - [section 3] "By removing a number of landmark categories from GLDv2-clean... we derive a revisited version... As shown in Table 2, RPar and ROxf have landmark overlap with GLDv2-clean respectively for 36 and 38 out of 70 queries, which corresponds to a percentage of 51% and 54%."
  - [corpus] Weak - no direct corpus evidence about landmark-specific memorization effects.
- Break condition: If performance drops similarly when removing non-overlapping landmarks of similar frequency, or if adding synthetic but visually similar landmarks does not cause performance inflation.

### Mechanism 2
- Claim: The CiDeR method achieves state-of-the-art performance by using spatial attention without location supervision, eliminating the need for specialized detection training data.
- Mechanism: The attentional localization (AL) component generates spatial attention maps that highlight objects of interest, and multiple thresholding operations create binary masks. These masks are applied to feature tensors and fused with learnable weights, allowing the model to focus on relevant regions without explicit object detection supervision.
- Core assumption: The spatial attention mechanism can effectively localize objects of interest without location supervision, and the thresholding and fusion approach can capture objects at different scales and contexts.
- Evidence anchors:
  - [abstract] "CiDeR uses spatial attention without location supervision, eliminating the need for specialized training sets and complex two-stage training and indexing pipelines."
  - [section 4] "It employs a spatial attention mechanism... which does not need location supervision."
  - [corpus] Weak - no direct corpus evidence about spatial attention without supervision effectiveness.
- Break condition: If the model fails to localize objects when location supervision is completely removed, or if the attention maps consistently miss objects of interest across diverse datasets.

### Mechanism 3
- Claim: The end-to-end, single-stage training pipeline of CiDeR is more efficient than two-stage detection-then-retrieval approaches while achieving better performance.
- Mechanism: By integrating detection and retrieval in a single network with a unified loss function, CiDeR avoids the computational overhead and potential error propagation of separate detection and retrieval stages. The model learns to extract global representations that focus on objects of interest through joint optimization.
- Core assumption: Joint optimization of detection and retrieval objectives leads to better feature representations than sequential optimization, and the computational savings from single-stage processing outweigh any potential performance benefits of specialized detection stages.
- Evidence anchors:
  - [abstract] "Importantly, as shown in Figure 1(b), this is a streamlined end-to-end approach that only needs single-stage training, single-stage indexing and is free of any location supervision."
  - [section 2] "CiDeR provides a single-stage training pipeline without the need for location supervision."
  - [corpus] Weak - no direct corpus evidence about computational efficiency comparisons.
- Break condition: If two-stage approaches with specialized detection networks consistently outperform single-stage methods on the same datasets, or if the computational savings are negligible compared to the performance benefits of specialized detection.

## Foundational Learning

- Concept: Class overlap between training and evaluation sets
  - Why needed here: Understanding why GLDv2-clean leads to artificially inflated performance requires knowing how class overlap affects model evaluation
  - Quick check question: If a model is trained on images of the Eiffel Tower and tested on the Eiffel Tower, what aspect of performance measurement is compromised?

- Concept: Spatial attention mechanisms without supervision
  - Why needed here: CiDeR's core innovation relies on using spatial attention to localize objects without requiring location-annotated training data
  - Quick check question: How does a spatial attention map differ from an object detection bounding box in terms of supervision requirements?

- Concept: Multi-stage vs. single-stage training pipelines
  - Why needed here: The paper contrasts CiDeR's single-stage approach with traditional two-stage detection-then-retrieval methods
  - Quick check question: What are the computational and architectural differences between training a detection network separately from a retrieval network versus training them jointly?

## Architecture Onboarding

- Component map: Backbone (ResNet101) → Backbone Enhancement (SENet) → Selective Context (SKNet†) → Attentional Localization (AL) → Spatial Pooling (GeM) → ArcFace Loss

- Critical path: Image → ResNet101 → SENet → SKNet† → AL (spatial attention + thresholding + fusion) → GeM pooling → ArcFace embedding → Training with ArcFace loss → Backpropagation through all components

- Design tradeoffs:
  - Single-stage vs. two-stage: CiDeR trades specialized detection accuracy for end-to-end efficiency and no supervision requirements
  - Fixed vs. dynamic mask background (β): Random sampling from normal distribution provides better robustness than fixed values
  - Number of masks (T): More masks provide finer object localization but increase computational cost and risk of overfitting

- Failure signatures:
  - Poor localization: Attention maps that don't focus on objects of interest, or binary masks that miss object boundaries
  - Overfitting to training set: High performance on GLDv2-clean but significant drop on RGLDv2-clean
  - Training instability: Loss not decreasing or oscillating, especially during the fine-tuning stage
  - Poor generalization: Good performance on base protocols but significant degradation on medium/hard protocols

- First 3 experiments:
  1. Verify class overlap: Implement the ranking and verification pipeline to confirm overlapping landmarks between GLDv2-clean and evaluation sets
  2. Ablation study: Train CiDeR with different component combinations (baseline → BE → SC → AL → all) to identify performance contributions
  3. Fine-tuning validation: Compare training with vs. without the two-stage fine-tuning process to confirm its effectiveness

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions in the text provided.

## Limitations
- The performance drop on RGLDv2-clean may be due to the small number of removed images (1,565 out of 1.58 million) rather than the loss of landmark-specific memorization
- The effectiveness of CiDeR's spatial attention mechanism without supervision is not fully validated against location-supervised alternatives
- Computational efficiency claims compared to two-stage methods lack quantitative comparisons

## Confidence
- Class overlap mechanism: Medium - supported by significant performance drop on RGLDv2-clean but lacking direct evidence of memorization effects
- CiDeR performance improvements: Medium - strong results on proposed dataset but limited ablation studies on individual components
- Single-stage training efficiency: Low - no comparative computational analysis provided

## Next Checks
1. Perform ablation studies to quantify the individual contributions of each CiDeR component (BE, SC, AL) to overall performance.
2. Compare CiDeR's performance against location-supervised detection-and-retrieval baselines on the same datasets.
3. Conduct a computational analysis comparing training and inference times between CiDeR and two-stage detection-and-retrieval approaches.