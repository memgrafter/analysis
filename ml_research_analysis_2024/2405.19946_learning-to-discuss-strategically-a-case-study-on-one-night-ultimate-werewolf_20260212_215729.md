---
ver: rpa2
title: 'Learning to Discuss Strategically: A Case Study on One Night Ultimate Werewolf'
arxiv_id: '2405.19946'
source_url: https://arxiv.org/abs/2405.19946
tags:
- player
- game
- role
- discussion
- werewolf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of strategic discussion in the
  One Night Ultimate Werewolf (ONUW) game, where players must deduce hidden roles
  and navigate deception. The authors formulate ONUW as a Multi-Phase Extensive-Form
  Bayesian Game and analyze Perfect Bayesian Equilibria (PBEs) in a simplified three-player
  setting, showing that discussion significantly impacts players' utilities by shaping
  beliefs.
---

# Learning to Discuss Strategically: A Case Study on One Night Ultimate Werewolf

## Quick Facts
- arXiv ID: 2405.19946
- Source URL: https://arxiv.org/abs/2405.19946
- Reference count: 40
- Primary result: RL-instructed LLM agents improve strategic discussion and performance in ONUW by learning to select optimal communication tactics

## Executive Summary
This work addresses strategic discussion in One Night Ultimate Werewolf (ONUW), where players must deduce hidden roles while navigating deception. The authors formulate ONUW as a Multi-Phase Extensive-Form Bayesian Game and analyze Perfect Bayesian Equilibria in simplified settings, demonstrating that discussion significantly impacts utilities by shaping beliefs. They propose an RL-instructed LLM-based agent framework that trains a discussion policy to select appropriate tactics (e.g., "Honest Evidence," "Deceptive Accusation") based on observations and beliefs. Experiments show this approach enhances agent performance and generalizability compared to direct LLM prompting or random tactics.

## Method Summary
The authors propose an RL-instructed LLM-based agent framework for strategic discussion in ONUW. They first formulate ONUW as a Multi-Phase Extensive-Form Bayesian Game and analyze Perfect Bayesian Equilibria in simplified three-player settings. For the main approach, they discretize discussion tactics into six categories (Honest/Deceptive Evidence/Accusation/Defense) and train a discussion policy using Conservative Q-Learning (CQL) on game logs generated by GPT-4. The agent framework integrates belief modeling, discussion tactic selection, and decision making components, with LLMs (GPT-4 and Gemini) serving as the backend for reasoning and text generation.

## Key Results
- RL-trained discussion policy significantly improves agent performance in both three-player and five-player ONUW games
- Strategic discussion tactics alter players' utilities by influencing beliefs about hidden roles
- The framework outperforms agents using direct LLM prompting or random tactic selection
- Discussion policy demonstrates good generalizability across different ONUW game settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discussion tactics significantly alter players' utilities by influencing their beliefs about hidden roles.
- Mechanism: Players have incomplete information about others' roles. Strategic discussion allows players to shape others' beliefs about their own and others' roles, which directly impacts voting decisions and ultimately determines utilities.
- Core assumption: Players' strategies and utilities are primarily determined by their beliefs about others' roles.
- Evidence anchors: Abstract shows discussion changes utilities by affecting beliefs; Theorem 4.1 demonstrates strategic role switching in PBE.

### Mechanism 2
- Claim: An RL-instructed LLM-based agent framework improves strategic discussion and performance.
- Mechanism: The framework integrates an RL-trained discussion policy into an LLM-based agent, enabling strategic selection of discussion tactics based on observations and beliefs to influence other players' beliefs and improve overall performance.
- Core assumption: LLMs can effectively model beliefs and generate strategic discussions, and RL can optimize discussion tactic selection.
- Evidence anchors: Abstract mentions framework improves performance; section 5.2 shows experimental effectiveness.

### Mechanism 3
- Claim: Discretization of discussion tactics enables explicit analysis and control over strategic communication.
- Mechanism: Categorizing discussion tactics into six discrete types provides a structured way to analyze and control communication strategy, facilitating RL policy training and understanding of chosen tactics.
- Core assumption: The six discretized tactics adequately capture essential strategies and can be effectively mapped to communication goals.
- Evidence anchors: Section 5.1 explains discretization challenges and approach; six categories defined for tactical control.

## Foundational Learning

- Concept: Multi-Phase Extensive-Form Bayesian Game (MP-EFBG)
  - Why needed here: ONUW involves multiple phases with incomplete information about roles, requiring a formal framework that captures sequential decision-making and belief updating.
  - Quick check question: Can you explain how MP-EFBG differs from a standard extensive-form game or a Bayesian game?

- Concept: Perfect Bayesian Equilibrium (PBE)
  - Why needed here: PBE provides a solution concept for games with incomplete information, ensuring belief consistency and sequential rationality in ONUW's strategic interactions.
  - Quick check question: How does PBE ensure that players' strategies are optimal given their beliefs and the strategies of other players?

- Concept: Reinforcement Learning (RL)
  - Why needed here: RL trains the discussion policy to learn optimal strategies for selecting discussion tactics based on observations and beliefs to maximize expected utility.
  - Quick check question: Can you explain how RL algorithms can be used to train a policy that selects discussion tactics in the ONUW game?

## Architecture Onboarding

- Component map: Observation → Belief Modeling → Discussion Tactic Selection → Decision Making → Action
- Critical path: Observation flows through belief modeling, tactic selection, and decision making to generate actions
- Design tradeoffs:
  - LLM-based vs. rule-based approaches: LLMs offer flexibility but may be less reliable
  - Discretized vs. continuous discussion tactics: Discretization simplifies analysis but may limit expressiveness
  - RL vs. direct prompting: RL optimizes for specific tasks but requires more data and resources
- Failure signatures:
  - LLM hallucinations or inconsistencies in belief modeling or decision making
  - RL policy converging to suboptimal tactics or failing to generalize
  - Discretized tactics not capturing strategic communication nuances
- First 3 experiments:
  1. Evaluate LLM's ability to model beliefs and generate strategic discussions without RL
  2. Train RL policy to select discussion tactics and compare performance to random/direct prompting
  3. Test framework scalability and generalizability across different ONUW game settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed framework generalize to other social deduction games beyond ONUW?
- Basis in paper: The authors discuss potential applicability to other communication games and tasks.
- Why unresolved: The paper focuses on ONUW as a case study without empirical evidence in other settings.
- What evidence would resolve it: Experimental results demonstrating effectiveness in other social deduction games like Werewolf