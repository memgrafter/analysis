---
ver: rpa2
title: Neural Fingerprints for Adversarial Attack Detection
arxiv_id: '2411.04533'
source_url: https://arxiv.org/abs/2411.04533
tags:
- adversarial
- detection
- fingerprints
- attack
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses adversarial attack detection in deep learning
  models by proposing a randomized defense method called Neural Fingerprints. The
  method generates a large bank of detectors by sampling random subsets of neurons
  from network layers, filtering those with high effect size between clean and attacked
  images.
---

# Neural Fingerprints for Adversarial Attack Detection

## Quick Facts
- arXiv ID: 2411.04533
- Source URL: https://arxiv.org/abs/2411.04533
- Reference count: 9
- Primary result: Randomized neural fingerprint detection achieves near-perfect detection rates against adversarial attacks

## Executive Summary
This paper introduces Neural Fingerprints, a randomized defense mechanism for detecting adversarial attacks in deep learning models. The method generates a large bank of detectors by sampling random subsets of neurons from network layers, filtering those with high effect size between clean and attacked images. At test time, random fingerprints are sampled and used with a likelihood ratio test for detection. The approach is evaluated on ImageNet using Inception V3 and ViT models against IFGSM and PGD attacks, achieving up to 99.9% detection rates with low false positive rates.

## Method Summary
The Neural Fingerprints method creates a defense by sampling random subsets of neurons from each network layer to form detection fingerprints. These fingerprints are evaluated for their ability to distinguish between clean and attacked images using effect size metrics. A large bank of 5,000 effective detectors is generated and stored. During inference, random fingerprints are sampled from this bank and combined using a likelihood ratio test to determine if an input is adversarial. This randomized selection provides protection against white-box attacks where attackers have full knowledge of the system architecture.

## Key Results
- Achieves up to 99.9% detection rates on ImageNet with Inception V3 and ViT models
- Maintains low false positive rates of 1-5% against IFGSM and PGD attacks
- Randomized fingerprint selection provides white-box attack resistance
- Effectiveness attributed to class information distribution across many neurons

## Why This Works (Mechanism)
The method exploits the fact that adversarial attacks typically modify only a small subset of features that directly impact the model's decision, while most neurons remain unaffected. By randomly sampling from a large bank of neuron subsets, the defense ensures that attackers cannot predict which features will be used for detection. The effect size filtering ensures that only the most discriminative neuron subsets are included in the detector bank, improving detection accuracy while maintaining computational efficiency through random sampling at test time.

## Foundational Learning
- Adversarial attacks and defense mechanisms: Essential for understanding the threat model and evaluation context
- Statistical hypothesis testing (likelihood ratio test): Critical for the detection framework's decision process
- Effect size measurement in machine learning: Used to filter and select the most effective detection fingerprints
- Randomized algorithms in security: Provides the theoretical foundation for white-box attack resistance
- Feature distribution in deep networks: Explains why random sampling of neuron subsets is effective

## Architecture Onboarding
Component map: Input image -> Random fingerprint sampling -> Likelihood ratio test -> Detection output
Critical path: Image features → Effect size filtering → Detector bank storage → Random sampling → Classification decision
Design tradeoffs: Large detector bank improves coverage but increases storage; random sampling provides security but may miss optimal detectors
Failure signatures: False negatives occur when attacks affect the same neuron subsets used for detection; false positives arise from natural variations mimicking attack patterns
First experiments: 1) Test detection accuracy with different fingerprint sizes, 2) Evaluate performance under adaptive white-box attacks, 3) Measure computational overhead of random sampling

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Scalability concerns for larger datasets and more complex architectures beyond tested models
- Uncertain performance under adaptive white-box attacks where adversaries can probe the system
- Computational overhead of maintaining 5,000 detectors and random sampling at inference
- Reliance on effect size separation may not hold for all attack types or model architectures

## Confidence
- Detection performance claims (99.9% detection, 1-5% false positives): High
- Randomized defense mechanism effectiveness: Medium
- Computational efficiency for real-world deployment: Low

## Next Checks
1. Evaluate the approach on larger, more diverse datasets (e.g., JFT-300M) and modern architectures (e.g., ConvNeXt, Swin Transformer) to assess scalability.
2. Conduct adaptive white-box attack experiments where adversaries can query the system and optimize against the random fingerprint sampling mechanism.
3. Measure the computational overhead of maintaining the detector bank and sampling fingerprints at inference time, comparing against baseline detection methods.