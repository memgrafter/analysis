---
ver: rpa2
title: 'Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment
  for Language Models'
arxiv_id: '2406.04274'
source_url: https://arxiv.org/abs/2406.04274
tags:
- learning
- policy
- arxiv
- which
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning large language models
  with human preferences in an offline setting, where only pre-collected preference
  data is available. The authors propose Self-Play with Adversarial Critic (SPAC),
  a novel offline preference optimization method that combines theoretical guarantees
  with practical scalability.
---

# Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models

## Quick Facts
- arXiv ID: 2406.04274
- Source URL: https://arxiv.org/abs/2406.04274
- Reference count: 40
- This paper proposes SPAC, a new offline preference optimization method with self-play that is the first provable and scalable approach to LLM alignment.

## Executive Summary
This paper addresses the challenge of aligning large language models with human preferences in an offline setting, where only pre-collected preference data is available. The authors propose Self-Play with Adversarial Critic (SPAC), a novel offline preference optimization method that combines theoretical guarantees with practical scalability. SPAC formulates the alignment problem as a Stackelberg game between a learner policy and an adversarial critic, using on-average pessimism to ensure robustness to sparse data coverage. Theoretically, SPAC is proven to converge to the optimal policy under single-policy concentrability with general function approximation. Empirically, the method is evaluated on a 7B Mistral model using Open LLM Leaderboard benchmarks, showing competitive performance against state-of-the-art alignment methods like DPO and SPIN, with an average improvement of 4.37% over the base model across multiple tasks.

## Method Summary
SPAC addresses offline preference optimization by formulating it as a Stackelberg game between a learner policy and an adversarial critic. The key innovation is using on-average pessimism instead of pointwise pessimism, which allows for scalable single-timescale optimization while maintaining theoretical guarantees. The method uses mirror descent for policy optimization and incorporates KL regularization to control deviation from a reference policy. A change-of-variable trick similar to DPO is employed to transform the double-timescale algorithm into a more practical single-timescale version. The approach is evaluated through iterative finetuning on subsets of UltraFeedback Binarized data, with performance measured on Open LLM Leaderboard benchmarks.

## Key Results
- SPAC achieves competitive performance against state-of-the-art alignment methods like DPO and SPIN
- The method shows an average improvement of 4.37% over the base model across multiple tasks
- Theoretical guarantees are provided for convergence to optimal policy under single-policy concentrability with general function approximation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SPAC converges to optimal policy under single-policy concentrability with general function approximation.
- Mechanism: Uses on-average pessimism instead of pointwise pessimism to construct lower bound on expected reward under learner's policy, enabling scalable Stackelberg game formulation solvable by single-timescale iterative self-play.
- Core assumption: Data covers where optimal policy can visit (single-policy concentrability), and function class F contains the true reward function.
- Evidence anchors:
  - [abstract] "SPAC, a new offline preference optimization method with self-play, inspired by the on-average pessimism technique from the offline RL literature, to be the first provable and scalable approach to LLM alignment."
  - [section] "Our approach is inspired by Xie et al. (2021a); Cheng et al. (2022) from the offline RL literature: we formulate offline preference optimization as a Stackelberg game... we propose an iterative algorithm (Algorithm 1) and replace the policy optimization step in Eq. (6) with mirror descent update."
- Break condition: If data does not satisfy single-policy concentrability, or if F does not contain the true reward function, convergence guarantees fail.

### Mechanism 2
- Claim: SPAC avoids reward hacking by preventing overoptimization of ill-estimated reward regions.
- Mechanism: Maintains on-average pessimistic reward estimate that is a lower bound on expected reward under learner's policy, discouraging exploitation of sparse data regions.
- Core assumption: Data is sparse but covers optimal policy's visitation, and KL regularization controls deviation from reference policy.
- Evidence anchors:
  - [section] "As classical RL theory suggests (Rashidinejad et al., 2021; Zhu et al., 2023; Wang et al., 2020), one can always find a problem instance in which such policy obtained from optimizing the empirically best value function suffers constant suboptimality with constant probability... In contrast, inspired by a different concept called on-average pessimism... SPAC only needs a reward estimate ft whose expectation under the learner's policy π is a lower bound."
- Break condition: If KL regularization is too weak, or if data coverage assumption fails, reward hacking may still occur.

### Mechanism 3
- Claim: SPAC is computationally scalable to large models like LLMs while maintaining theoretical guarantees.
- Mechanism: Transforms double-timescale algorithm into single-timescale direct preference optimization using change-of-variable trick similar to DPO, eliminating need for separate reward model per iteration.
- Core assumption: Policy class Π can implicitly represent reward functions needed for optimization, and exponential weighting can be approximated.
- Evidence anchors:
  - [section] "Since Line 4 can be written as ft(x, y) = η log πt+1(y|x)/πt(y|x) + log Zt(x), we can plug this into Line 3 similar to the derivation of DPO (Rafailov et al., 2024). This gives us a new algorithm Self-Play with Adversarial Critic (SPAC, Algorithm 2), which is an equivalent but more practical version of Algorithm 1."
- Break condition: If policy class cannot adequately represent reward functions, or if exponential weighting cannot be approximated, scalability fails.

## Foundational Learning

- Concept: Stackelberg games and bilevel optimization
  - Why needed here: SPAC formulates alignment as leader-follower game between learner policy and adversarial critic
  - Quick check question: What distinguishes a Stackelberg game from a general minimax game in this context?

- Concept: On-average pessimism vs pointwise pessimism
  - Why needed here: SPAC uses on-average pessimism for computational scalability while maintaining theoretical guarantees
  - Quick check question: How does maintaining a lower bound on expected reward differ from maintaining a pointwise lower confidence bound?

- Concept: Mirror descent and natural policy gradient updates
  - Why needed here: SPAC uses mirror descent for policy optimization, which naturally satisfies no-regret properties
  - Quick check question: Why does the natural policy gradient update automatically satisfy the requirements for the implicit reward function?

## Architecture Onboarding

- Component map:
  - Base LLM (7B Mistral in experiments)
  - Offline preference dataset (UltraFeedback Binarized)
  - Policy model (implicit reward via density ratio)
  - Reference policy (SFT model)
  - KL regularization mechanism
  - Self-play iterative loop

- Critical path:
  1. Initialize policy π1 from reference model
  2. For each iteration t: generate responses yj ~ πt
  3. Compute log density ratios using yj, y+ j, y- j
  4. Update policy πt+1 using mirror descent on modified DPO objective
  5. Return average of policies π[1:T]

- Design tradeoffs:
  - Single vs double timescale: SPAC uses single-timescale for scalability vs theoretical double-timescale
  - λ scaling: Large theoretical λ needed for convergence vs smaller practical λ with smoothing
  - Data usage: SPAC uses 1/3 of data per iteration vs full dataset approaches

- Failure signatures:
  - Policy performance degrades over iterations (overfitting/optimization instability)
  - Loss plateaus but performance doesn't improve (convergence issues)
  - Numerical instability with large λ values (regularization tuning problems)

- First 3 experiments:
  1. Single iteration SPAC with λ=1 vs DPO baseline on small dataset
  2. SPAC with λ=1000 (theoretical value) on small dataset to test convergence properties
  3. SPAC vs SPIN on same dataset to compare iterative self-play approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SPAC compare to other alignment methods when the preference data distribution significantly differs from the actual deployment distribution?
- Basis in paper: [inferred] The paper mentions that training prompts and testing prompts may follow different distributions, and evaluates on Open LLM Leaderboard, but doesn't explicitly test distribution shift scenarios.
- Why unresolved: The experiments use a single preference dataset (UltraFeedback Binarized) without testing on out-of-distribution prompts or creating synthetic distribution shifts.
- What evidence would resolve it: Experiments comparing SPAC's performance on in-distribution vs out-of-distribution prompts, or intentionally created distribution shift scenarios.

### Open Question 2
- Question: What is the exact relationship between the regularization hyperparameter λ and the number of iterations T in practice, and how does this affect convergence speed?
- Basis in paper: [explicit] The paper states λ = Θ(C√n/κ² log(|Π|/δ)) for theoretical guarantees, but uses λ = 1.0 in practice, noting that "huge numerics can often lead to numerical instability in optimization."
- Why unresolved: The theoretical λ is much larger than practical λ, and the paper doesn't provide a systematic study of how different λ values affect performance across different iteration counts.
- What evidence would resolve it: A systematic ablation study varying λ and T to show their relationship and impact on convergence speed and final performance.

### Open Question 3
- Question: What is the impact of the choice of y′ⱼ in Algorithm 2 on SPAC's performance, and how does this compare to the theoretical treatment?
- Basis in paper: [explicit] The paper notes that "the choice of y′ⱼ can be flexible" and mentions using chosen or rejected responses, but doesn't systematically compare different choices.
- Why unresolved: The paper uses a specific averaging approach but doesn't evaluate whether this is optimal or compare it to alternatives mentioned in the theoretical discussion.
- What evidence would resolve it: Ablation studies comparing SPAC's performance using different choices of y′ⱼ (chosen responses only, rejected responses only, averaging as implemented, etc.).

## Limitations
- Theoretical guarantees rely on specific assumptions (single-policy concentrability, function class containing true reward) that may not hold in practice
- Empirical evaluation limited to a single 7B model and one preference dataset, raising generalizability concerns
- GSM8k task performance degradation suggests limitations in handling mathematical reasoning tasks

## Confidence
- **High confidence**: Theoretical convergence guarantees under stated assumptions, basic algorithm implementation using DPO-style updates
- **Medium confidence**: Competitive performance claims against baselines (4.37% average improvement), numerical stability through log-sigmoid smoothing
- **Low confidence**: Scalability to much larger models (>7B parameters), effectiveness on diverse task distributions beyond tested benchmarks, generalization to different preference data distributions

## Next Checks
1. **Coverage verification**: Systematically evaluate the single-policy concentrability assumption by analyzing data coverage of optimal policy visitation across different model sizes and preference distributions

2. **Generalization testing**: Test SPAC on larger models (70B+ parameters) and alternative preference datasets (e.g., Anthropic HH-RLHF, Direct Preference Optimization datasets) to assess scalability and robustness

3. **Mathematical reasoning evaluation**: Design controlled experiments isolating mathematical reasoning tasks to determine if the GSM8k performance degradation is due to data limitations or algorithmic constraints in the preference optimization framework