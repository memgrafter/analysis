---
ver: rpa2
title: Probabilistic Abduction for Visual Abstract Reasoning via Learning Rules in
  Vector-symbolic Architectures
arxiv_id: '2401.16024'
source_url: https://arxiv.org/abs/2401.16024
tags:
- rule
- learn-vrf
- attribute
- reasoning
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study focuses on efficiently solving Raven's progressive matrices
  (RPM), a visual test for assessing abstract reasoning abilities, by using distributed
  computation and operators provided by vector-symbolic architectures (VSA). Instead
  of hard-coding the rule formulations associated with RPMs, our approach can learn
  the VSA rule formulations (hence the name Learn-VRF) with just one pass through
  the training data.
---

# Probabilistic Abduction for Visual Abstract Reasoning via Learning Rules in Vector-symbolic Architectures

## Quick Facts
- arXiv ID: 2401.16024
- Source URL: https://arxiv.org/abs/2401.16024
- Authors: Michael Hersche; Francesco di Stefano; Thomas Hofmann; Abu Sebastian; Abbas Rahimi
- Reference count: 40
- Key outcome: Learn-VRF achieves 81.3% average accuracy on I-RAVEN with 5k parameters, demonstrating strong OOD generalization through learned VSA rule formulations

## Executive Summary
This study introduces Learn-VRF, a novel approach to solving Raven's Progressive Matrices (RPM) using vector-symbolic architectures (VSA) with learned rule formulations. Rather than hard-coding rules, the method learns VSA representations through a single pass of training data while maintaining transparency and interpretability. The approach achieves strong performance on both in-distribution and out-of-distribution data, significantly outperforming pure connectionist baselines including large language models.

Learn-VRF leverages the expressive power of VSA to represent probabilistic rule formulations uniformly, enabling effective rule sharing across different attribute-rule pairs. With only 5k trainable parameters, the method demonstrates that compact, interpretable models can achieve competitive performance on abstract visual reasoning tasks while maintaining strong generalization capabilities to unseen rule-attribute combinations.

## Method Summary
The Learn-VRF approach uses vector-symbolic architectures to represent and learn rule formulations for solving Raven's Progressive Matrices. Instead of hard-coding rules, the method learns VSA representations through a single pass of training data. The approach employs fractional power encoding to create uniform, expressive probability mass function (PMF) representations in VSA space. Rule sharing is enabled through the VSA framework, allowing the model to generalize to unseen attribute-rule pairs. The method combines sampling-based rule selection and weighted rule combination strategies to make predictions, achieving strong performance with minimal parameters while maintaining interpretability.

## Key Results
- Achieves 81.3% average accuracy on I-RAVEN dataset using sampling-based rule selection
- Reaches 84.1% accuracy with weighted rule combination strategy
- Demonstrates strong out-of-distribution generalization, achieving almost perfect accuracy on most rule-attribute pairs
- Uses only 5k trainable parameters while outperforming pure connectionist baselines including large language models

## Why This Works (Mechanism)
Learn-VRF works by leveraging the distributed computation and expressive operators provided by vector-symbolic architectures. The key mechanism is the uniform PMF representation in VSA space using fractional power encoding, which enables effective rule sharing across different attribute-rule pairs. This representation allows the model to generalize to unseen combinations by reusing learned rules in novel contexts. The probabilistic abduction framework enables the system to reason about multiple possible rules simultaneously and combine them appropriately, rather than relying on a single hard-coded solution path.

## Foundational Learning
- **Vector-symbolic architectures**: Needed to represent complex relational structures in a distributed, compositional way; quick check: verify the VSA operations preserve similarity relationships appropriately
- **Fractional power encoding**: Required for creating uniform, expressive PMF representations in VSA space; quick check: ensure encoding maintains probabilistic semantics across different attribute dimensions
- **Probabilistic abduction**: Essential for reasoning about multiple possible rule interpretations simultaneously; quick check: validate that the probabilistic framework correctly handles uncertainty in rule selection
- **Rule sharing mechanisms**: Critical for achieving strong OOD generalization; quick check: confirm that learned rules can be effectively recombined in novel contexts
- **Visual attribute extraction**: Foundation for the approach, though pre-extracted in this study; quick check: assess the quality and completeness of extracted attributes
- **RPM structure understanding**: Necessary to formulate appropriate VSA representations; quick check: verify the model captures the compositional nature of RPM problems

## Architecture Onboarding

**Component Map**: Visual Attribute Extractor -> VSA Rule Learner -> Probabilistic Abduction Engine -> Prediction Layer

**Critical Path**: The most critical computational path is Visual Attribute Extractor -> VSA Rule Learner, as the quality of extracted attributes directly determines the effectiveness of learned VSA rule formulations.

**Design Tradeoffs**: The approach trades end-to-end visual processing capability for interpretability and strong OOD generalization. By using pre-extracted attributes and compact VSA representations, the model sacrifices some real-world applicability for transparency and rule-sharing benefits.

**Failure Signatures**: Poor performance on completely novel RPM structures that combine rules in unprecedented ways, degradation when attribute extraction is noisy or incomplete, and potential overfitting to specific attribute-rule patterns seen during training.

**Three First Experiments**:
1. Test Learn-VRF performance on RPMs with progressively more complex rule combinations to identify the limit of rule sharing capabilities
2. Evaluate the impact of different attribute extraction qualities on final performance to understand sensitivity to preprocessing
3. Compare Learn-VRF against a version using hard-coded VSA rules to quantify the benefit of learned formulations

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Relies on pre-extracted visual attribute labels rather than end-to-end visual processing, limiting real-world applicability
- May not capture complex reasoning patterns that could emerge with larger datasets or more intricate RPM configurations
- Limited evaluation of performance on completely novel RPM structures and unprecedented rule combinations
- Comparison with large language models may not be entirely fair due to different optimization objectives

## Confidence
- **High confidence**: In-distribution performance on I-RAVEN dataset
- **Medium confidence**: OOD generalization claims, particularly regarding rule sharing mechanisms
- **Medium confidence**: Superiority over pure connectionist baselines

## Next Checks
1. Evaluate the model's performance on end-to-end visual processing without pre-extracted attributes to assess real-world applicability
2. Test the model on RPM variants with novel structural patterns and combinations of rules that were never seen during training
3. Conduct ablation studies to quantify the specific contribution of the VSA framework versus other architectural components to the observed performance gains