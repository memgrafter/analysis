---
ver: rpa2
title: 'Transducer-Llama: Integrating LLMs into Streamable Transducer-based Speech
  Recognition'
arxiv_id: '2412.16464'
source_url: https://arxiv.org/abs/2412.16464
tags:
- training
- predictor
- speech
- vocabulary
- transducer-llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Transducer-Llama, a streaming speech recognition
  model that integrates large language models (LLMs) into a Factorized Transducer
  (FT) architecture. To address the challenge of applying LLMs with large vocabularies
  to speech systems, the authors propose a vocabulary adaptation technique that aligns
  LLMs with ASR-specific vocabularies, reducing training costs and data sparsity.
---

# Transducer-Llama: Integrating LLMs into Streamable Transducer-based Speech Recognition

## Quick Facts
- **arXiv ID**: 2412.16464
- **Source URL**: https://arxiv.org/abs/2412.16464
- **Reference count**: 36
- **Primary result**: 17% relative WER reduction over strong Factorized Transducer baseline

## Executive Summary
This paper introduces Transducer-Llama, a streaming speech recognition model that integrates large language models (LLMs) into a Factorized Transducer (FT) architecture. To address the challenge of applying LLMs with large vocabularies to speech systems, the authors propose a vocabulary adaptation technique that aligns LLMs with ASR-specific vocabularies, reducing training costs and data sparsity. They also introduce a weak-to-strong LM swap strategy, where a weaker LM is used during RNN-T loss training and then replaced with a strong LLM during decoding, followed by MWER loss fine-tuning. Experiments on LibriSpeech and multilingual LibriSpeech datasets show that Transducer-Llama achieves a 17% relative WER reduction over a strong FT baseline and a 32% WERR over an RNN-T baseline. The approach enables efficient integration of LLMs into streaming ASR systems while maintaining superior performance.

## Method Summary
Transducer-Llama integrates LLMs into streaming ASR through two key innovations: vocabulary adaptation and weak-to-strong LM swapping. The vocabulary adaptation technique aligns LLM vocabularies with ASR-specific vocabularies by modifying the embedding and output layers, enabling efficient use of LLMs with large vocabularies in ASR systems. The weak-to-strong LM swap strategy involves training with a weaker LM (stateless embedding layer) using RNN-T loss, then swapping to a strong LLM during decoding, followed by MWER loss fine-tuning. The architecture uses a 20-layer Emformer encoder for LibriSpeech or 30-layer streaming Conformer for multilingual LibriSpeech, with beam search decoding using size 10.

## Key Results
- Achieved 17% relative WER reduction over strong Factorized Transducer baseline on LibriSpeech
- Achieved 32% relative WER reduction over RNN-T baseline on LibriSpeech
- Demonstrated effectiveness on multilingual LibriSpeech dataset with similar performance gains

## Why This Works (Mechanism)
The integration works by addressing two fundamental challenges: vocabulary mismatch between LLMs and ASR systems, and computational efficiency during training. The vocabulary adaptation technique solves the first challenge by mapping the LLM's large vocabulary to the ASR-specific vocabulary, reducing training costs and data sparsity. The weak-to-strong LM swap strategy addresses the second challenge by using a simpler LM during the computationally expensive RNN-T training phase, then leveraging the powerful LLM only during inference and fine-tuning. This approach allows the model to benefit from strong language modeling without the computational burden of using LLMs throughout training.

## Foundational Learning

**Vocabulary adaptation** - Technique to align LLM vocabularies with ASR vocabularies
- Why needed: LLMs typically have large vocabularies that are incompatible with ASR systems, leading to inefficiency and data sparsity
- Quick check: Verify vocabulary overlap and token coverage between adapted LLM and ASR system

**Weak-to-strong LM swap** - Training strategy using weaker LM initially, stronger LM later
- Why needed: Prevents overfitting to strong LM during training while maintaining its benefits during inference
- Quick check: Monitor Pnb vs Pb balance during training and validate performance after LM swap

**MWER fine-tuning** - Minimum Word Error Rate training with N-best hypotheses
- Why needed: Optimizes the model directly for WER reduction rather than likelihood maximization
- Quick check: Validate WER improvement after MWER fine-tuning compared to baseline

## Architecture Onboarding

**Component map**: Audio input -> Emformer/Conformer encoder -> Joint network -> Predictor (LM) -> Output

**Critical path**: Encoder -> Joint network -> Predictor -> Output, where predictor (LM) is the key innovation

**Design tradeoffs**: Uses vocabulary adaptation to reduce computational cost of LLMs, but requires careful parameter tuning for alignment; weak-to-strong swap reduces training time but adds complexity to training pipeline

**Failure signatures**: Vocabulary adaptation mismatch causes performance degradation; weak LM overfitting prevents strong LM from improving results; MWER fine-tuning without proper N-best generation fails to improve WER

**First experiments**: 1) Test vocabulary adaptation with different averaging strategies for out-of-vocabulary tokens; 2) Validate weak-to-strong LM swap with different weak LM types; 3) Compare MWER fine-tuning with different N-best sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the vocabulary adaptation technique scale when applied to extremely large ASR vocabularies or when adapting to highly specialized domains?
- Basis in paper: The paper mentions the vocabulary adaptation technique aligns LLMs with ASR-specific vocabularies but does not explore its performance with very large or specialized vocabularies.
- Why unresolved: The paper only tests the technique with a standard 5000-token ASR vocabulary, leaving its scalability and adaptability to more complex vocabularies untested.
- What evidence would resolve it: Experiments comparing the performance and efficiency of the vocabulary adaptation technique across varying vocabulary sizes and specialized domains.

### Open Question 2
- Question: What is the impact of using different types of weak LMs (e.g., n-gram models, smaller neural LMs) during the weak-to-strong LM swap strategy on final ASR performance?
- Basis in paper: The paper uses a stateless embedding layer as the weak LM but does not compare its effectiveness against other types of weak LMs.
- Why unresolved: The choice of weak LM could significantly influence the training dynamics and final performance, but this aspect is not explored.
- What evidence would resolve it: Comparative studies evaluating the ASR performance when using various weak LM types during training.

### Open Question 3
- Question: How does the proposed Transducer-Llama framework perform in real-time streaming scenarios with varying latency constraints?
- Basis in paper: The paper focuses on streaming capabilities but does not provide empirical data on latency performance under different constraints.
- Why unresolved: While the framework is designed for streaming, its latency characteristics in practical, real-time applications are not quantified.
- What evidence would resolve it: Benchmarking the model's latency and performance across different streaming scenarios with varying latency requirements.

### Open Question 4
- Question: Can the weak-to-strong LM swap strategy be generalized to other E2E ASR architectures beyond the Factorized Transducer?
- Basis in paper: The strategy is demonstrated within the Transducer-Llama framework but its applicability to other architectures is not discussed.
- Why unresolved: The effectiveness of the strategy may depend on the specific architecture, and its generalizability remains untested.
- What evidence would resolve it: Applying the weak-to-strong LM swap strategy to other E2E ASR models (e.g., RNN-T, LAS) and comparing the results.

## Limitations
- Vocabulary adaptation strategy details (averaging mechanism, random initialization) are underspecified, potentially affecting reproducibility
- Weak-to-strong LM swap training dynamics are not fully detailed, with risks of encoder overfitting to weak LM
- MWER fine-tuning schedule parameters (N-best size, α, β) lack explicit optimization details and may require dataset-specific tuning

## Confidence

**High confidence**: The core claim that Transducer-Llama achieves significant WER reductions over strong baselines (17% relative WERR over FT, 32% over RNN-T) is supported by the experimental results on LibriSpeech and MLS.

**Medium confidence**: The weak-to-strong LM swap strategy is effective in practice, but the exact training dynamics and hyperparameters are not fully specified, leaving some uncertainty about reproducibility.

**Low confidence**: The long-term generalization and robustness of the approach to out-of-domain data or larger-scale ASR tasks remain untested.

## Next Checks

1. **Vocabulary adaptation ablation**: Test the impact of different vocabulary alignment strategies (e.g., mean vs. weighted averaging for OOV tokens, fixed vs. random initialization for ASR-specific tokens) on WER to determine optimal configuration.

2. **MWER fine-tuning sensitivity**: Evaluate how changes in N-best size, α, and β during MWER training affect final WER, and whether these parameters need dataset-specific tuning.

3. **Streaming efficiency analysis**: Measure decoding latency and memory usage with Llama2-0.5b and Llama3-8b during streaming ASR to assess practical deployment feasibility.