---
ver: rpa2
title: 'Large Language Models as Neurolinguistic Subjects: Discrepancy between Performance
  and Competence'
arxiv_id: '2411.07533'
source_url: https://arxiv.org/abs/2411.07533
tags:
- llms
- language
- form
- performance
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a neurolinguistic approach to evaluate the
  linguistic competence of large language models (LLMs) by distinguishing between
  performance and competence. Traditional psycholinguistic evaluations measure output
  probabilities, which may reflect statistical patterns rather than true linguistic
  understanding.
---

# Large Language Models as Neurolinguistic Subjects: Discrepancy between Performance and Competence

## Quick Facts
- **arXiv ID**: 2411.07533
- **Source URL**: https://arxiv.org/abs/2411.07533
- **Reference count**: 30
- **Primary result**: Language performance and competence are distinct constructs in LLMs, with higher competence in form (syntax/morphology) than meaning (conceptual understanding)

## Executive Summary
This paper introduces a neurolinguistic approach to evaluate the linguistic competence of large language models by distinguishing between performance and competence. Traditional psycholinguistic evaluations measure output probabilities, which may reflect statistical patterns rather than true linguistic understanding. The authors propose minimal pair probing combined with diagnostic probing to analyze internal activation patterns across model layers, allowing for a detailed examination of how LLMs represent form and meaning. They create new minimal pair datasets for Chinese and German to complement existing English datasets.

The study finds that psycholinguistic and neurolinguistic methods reveal distinct patterns, indicating that language performance and competence are separate constructs. LLMs exhibit higher competence and performance in form compared to meaning across all evaluation methods. Neurolinguistic probing shows that LLMs encode grammatical features earlier and more effectively than conceptual understanding, with meaning competence showing a linear correlation to form competence. This suggests that LLMs' understanding of meaning relies heavily on formal structures rather than independent conceptual comprehension.

## Method Summary
The authors employ minimal pair probing combined with diagnostic probing to evaluate LLMs' linguistic competence. Minimal pair probing creates controlled contrasts in form or meaning while holding other variables constant, allowing researchers to isolate specific linguistic phenomena. Diagnostic probing analyzes internal activation patterns across model layers to understand how linguistic information is represented and processed. The approach distinguishes between performance (measured by output probabilities) and competence (measured by activation patterns). The study creates new minimal pair datasets for Chinese (COMPS-ZH) and German (COMPS-DE) to complement existing English datasets, enabling cross-linguistic analysis. Models are evaluated at each layer to track how form and meaning competence develop through the network architecture.

## Key Results
- Psycholinguistic and neurolinguistic methods reveal distinct patterns, indicating that language performance and competence are separate constructs in LLMs
- LLMs exhibit systematically higher competence in form (syntax/morphology) than meaning (conceptual understanding) across all evaluation methods
- Meaning competence shows a linear correlation to form competence, suggesting that conceptual understanding relies heavily on formal structures rather than independent comprehension

## Why This Works (Mechanism)
The neurolinguistic approach works because it examines internal activation patterns rather than just output probabilities, revealing how linguistic information is actually processed and represented within the model. By using minimal pair probing, the method isolates specific linguistic phenomena while controlling for confounding variables. The layer-by-layer analysis tracks how different types of linguistic information emerge and develop throughout the model's architecture. The combination of performance metrics (output probabilities) with competence measures (activation patterns) provides a more complete picture of linguistic understanding than either approach alone.

## Foundational Learning

**Minimal pair probing**: Creating controlled contrasts where only one linguistic feature differs between examples. Why needed: Isolates specific linguistic phenomena while controlling for confounding variables. Quick check: Can the model distinguish between minimally different sentences that vary only in grammatical number or semantic meaning?

**Diagnostic probing**: Analyzing internal activation patterns to understand how information is represented. Why needed: Reveals processing mechanisms beyond surface-level outputs. Quick check: Do grammatical features activate earlier in the network than conceptual features?

**Form vs. meaning competence**: Distinguishing between syntactic/morphological knowledge and conceptual understanding. Why needed: Traditional evaluations conflate performance with true linguistic competence. Quick check: Does the model perform well on grammatical tasks but struggle with semantic reasoning?

## Architecture Onboarding

**Component map**: Input text -> Tokenizer -> Transformer layers (N) -> Output layer -> Minimal pair evaluation / Diagnostic probing analysis

**Critical path**: Input encoding through successive transformer layers where linguistic features are progressively processed and represented, with grammatical features emerging earlier than conceptual features

**Design tradeoffs**: The study balances between creating controlled minimal pairs that isolate specific phenomena versus maintaining ecological validity, and between layer-by-layer analysis depth versus computational feasibility

**Failure signatures**: LLMs may show high performance on form-based tasks but poor performance on meaning-based tasks, indicating a competence-performance dissociation; activation patterns may reveal that meaning is processed differently than form, with grammatical features encoded earlier and more robustly

**3 first experiments**: 1) Compare minimal pair performance across model families (GPT, BERT, LLaMA) to identify architectural differences in competence development; 2) Test the same models on additional languages with different morphological typologies; 3) Conduct ablation studies where specific linguistic features are selectively removed from training data

## Open Questions the Paper Calls Out
None

## Limitations
- Cross-linguistic scope limited to three languages (English, Chinese, German), potentially limiting generalizability to other language families
- Minimal pair datasets may not capture full complexity of linguistic competence, particularly for long-distance dependencies and pragmatic inference
- Layer-by-layer analysis provides correlational rather than causal evidence about how LLMs process linguistic information

## Confidence

**High confidence**: The general finding that performance metrics differ from competence measures in LLMs is well-supported by the evidence

**Medium confidence**: The specific claim that LLMs show systematically higher competence in form versus meaning requires careful interpretation of neurolinguistic probing results and may vary across model architectures

**Medium confidence**: The layer-wise analysis showing earlier encoding of grammatical versus conceptual features is compelling but could be influenced by architectural differences between model families

## Next Checks

1. Replicate the neurolinguistic probing analysis across a more diverse set of languages including non-Indo-European languages and languages with different morphological typologies

2. Design targeted minimal pair tests for specific linguistic phenomena not covered in current datasets, such as discourse-level meaning composition and pragmatic inference, to test whether the form-meaning competence gap persists

3. Conduct ablation studies where grammatical versus semantic features are selectively perturbed in training data to determine whether the observed form-meaning differences reflect architectural biases or training data distribution effects