---
ver: rpa2
title: Super Consistency of Neural Network Landscapes and Learning Rate Transfer
arxiv_id: '2402.17457'
source_url: https://arxiv.org/abs/2402.17457
tags:
- learning
- rate
- width
- sharpness
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the phenomenon of learning rate transfer
  across model sizes in deep neural networks. The authors propose the concept of "Super
  Consistency," where certain properties of the loss landscape, particularly the sharpness
  (largest Hessian eigenvalue), remain width-independent during training under specific
  scaling regimes.
---

# Super Consistency of Neural Network Landscapes and Learning Rate Transfer

## Quick Facts
- arXiv ID: 2402.17457
- Source URL: https://arxiv.org/abs/2402.17457
- Authors: Lorenzo Noci; Alexandru Meterez; Thomas Hofmann; Antonio Orvieto
- Reference count: 40
- Primary result: Sharpness (largest Hessian eigenvalue) remains width-independent during training under µP parameterization, correlating with successful learning rate transfer across model sizes

## Executive Summary
This paper introduces "Super Consistency," a phenomenon where certain properties of neural network loss landscapes, particularly sharpness (largest Hessian eigenvalue), remain width-independent during training across different model sizes. The authors demonstrate that this property correlates strongly with successful learning rate transfer, especially under the mean-field parameterization (µP) and Depth-µP scaling regimes. Through extensive experiments on various architectures (ConvNets, ResNets, Vision Transformers, GPT-2) and datasets (CIFAR-10, Tiny-ImageNet, ImageNet, WikiText), they show that progressive sharpening driven by feature learning is the key mechanism behind Super Consistency. The theoretical analysis of a two-layer linear network provides additional insights into why width-independent dynamics emerge under µP scaling.

## Method Summary
The research investigates Super Consistency through a systematic study of sharpness dynamics across model widths and depths. The authors train ConvNets with residual connections on CIFAR-10 using SGD and Adam under different parameterizations (µP, NTK, etc.), tracking the largest Hessian eigenvalue throughout training. They extend this analysis to deeper networks (ResNets) and larger datasets, comparing sharpness evolution under various scaling regimes. The study includes learning rate sweep experiments to evaluate transfer across widths, and analyzes NTK eigenvalue evolution to understand the relationship between feature learning and sharpening. A theoretical analysis of a two-layer linear network provides additional insights into the width-independent dynamics observed empirically.

## Key Results
- Sharpness remains width-independent during training under µP parameterization across multiple architectures and datasets
- Progressive sharpening correlates with successful learning rate transfer across model sizes
- Feature learning drives the progressive sharpening observed under µP, with NTK eigenvalues evolving during training
- Theoretical analysis of two-layer linear network confirms width-independent dynamics under µP scaling
- Super Consistency holds across different batch sizes and data augmentation settings

## Why This Works (Mechanism)
The phenomenon works because under µP parameterization, the neural network exhibits feature learning where the NTK evolves during training. This evolution is characterized by progressive sharpening of the loss landscape, where the largest Hessian eigenvalue increases over time. The key insight is that this sharpening process is width-independent, meaning that models of different widths but trained under the same parameterization follow similar sharpness trajectories. This width-independence allows for successful learning rate transfer because the relative scale of the loss landscape remains consistent across model sizes. The mechanism is distinct from NTK parameterization where the NTK remains fixed and no progressive sharpening occurs.

## Foundational Learning
- **Mean-field parameterization (µP)**: A scaling regime where parameters are scaled with network width to maintain consistent signal propagation. Why needed: Essential for understanding the specific conditions under which Super Consistency emerges. Quick check: Verify that parameter scaling follows µP rules (output multipliers scale as √width).
- **Neural Tangent Kernel (NTK)**: The kernel that describes the training dynamics of infinitely wide neural networks in function space. Why needed: Central to understanding the difference between feature learning and lazy training regimes. Quick check: Confirm NTK eigenvalues remain constant under NTK parameterization but evolve under µP.
- **Hessian eigenvalue analysis**: Computing the largest eigenvalue of the Hessian matrix to measure sharpness. Why needed: Sharpness is the primary metric used to quantify Super Consistency. Quick check: Validate Hessian computation using power iteration or Hutchinson's method.
- **Learning rate transfer**: The ability to use optimal learning rates from smaller models when scaling up. Why needed: The practical application and motivation for studying Super Consistency. Quick check: Demonstrate that learning rates optimal for small models work well for larger models under µP.
- **Feature learning vs lazy training**: The distinction between networks that learn features (NTK evolves) versus those that barely change during training. Why needed: Critical for understanding why µP exhibits Super Consistency while NTK does not. Quick check: Compare NTK eigenvalue evolution under different parameterizations.
- **Scaling regimes**: Different ways of scaling network parameters with width (µP, NTK, etc.). Why needed: Different scaling regimes lead to different training dynamics and Super Consistency properties. Quick check: Verify parameter scaling follows the correct regime for each experiment.

## Architecture Onboarding

**Component map:**
ConvNet/ResNet/ViT -> Parameter scaling (µP/NTK) -> Training (SGD/Adam) -> Sharpness tracking (Hessian) -> NTK analysis -> Learning rate transfer evaluation

**Critical path:**
Parameter scaling → Training dynamics → Sharpness evolution → Learning rate transfer success

**Design tradeoffs:**
- Computational cost of Hessian estimation vs accuracy of sharpness measurement
- Width range explored vs generalization to extremely large models
- Theoretical analysis complexity vs empirical coverage
- Choice of architectures vs breadth of applicability

**Failure signatures:**
- Sharpness not converging to width-independent values under µP
- Learning rate transfer failing despite Super Consistent sharpness
- NTK eigenvalues not evolving under µP as expected
- Computational instability in Hessian or NTK eigenvalue estimation

**First experiments to run:**
1. Train a small ConvNet (width 32) under µP with SGD on CIFAR-10, track sharpness throughout training
2. Repeat with width 64, 128, 256 to observe width-independence of sharpness dynamics
3. Compare sharpness evolution under µP vs NTK parameterization for the same architectures

## Open Questions the Paper Calls Out
- Does Super Consistency of the sharpness hold at scales beyond those tested (e.g., trillion-parameter models)?
- What is the precise mathematical relationship between feature learning and progressive sharpening in the sharpness dynamics?
- Can step-size tuners be designed that explicitly leverage Super Consistency to achieve optimal learning rate transfer across model sizes?

## Limitations
- Theoretical analysis limited to two-layer linear network, not fully capturing deep nonlinear networks
- Computational constraints limit experiments to moderate model sizes, leaving scalability question open
- Focus on specific architectures and datasets may limit generalizability to other domains
- The relationship between sharpness and generalization remains debated in the literature

## Confidence
- **High confidence**: Empirical observations of width-independent sharpness dynamics under µP parameterization across multiple architectures and datasets
- **Medium confidence**: The correlation between Super Consistency and successful learning rate transfer, given the limited theoretical grounding beyond the two-layer linear case
- **Low confidence**: The claim that Super Consistency is a "key factor" in learning rate transfer, as this implies causation rather than correlation, and alternative explanations are not fully explored

## Next Checks
1. Develop theoretical analysis for deeper nonlinear networks (e.g., ResNet) to complement the two-layer linear case and strengthen the causal link between Super Consistency and learning rate transfer
2. Evaluate Super Consistency and learning rate transfer on additional architectures (e.g., EfficientNet, Vision Transformers with different attention mechanisms) and datasets (e.g., ImageNet-21k, multilingual text corpora) to assess robustness
3. Investigate whether other properties of the loss landscape (e.g., gradient norms, Fisher information matrix eigenvalues) exhibit similar width-independent dynamics under µP, and whether these properties also correlate with learning rate transfer