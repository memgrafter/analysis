---
ver: rpa2
title: A Comparison of LLM Finetuning Methods & Evaluation Metrics with Travel Chatbot
  Use Case
arxiv_id: '2408.03562'
source_url: https://arxiv.org/abs/2408.03562
tags:
- data
- evaluation
- raft
- metrics
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research compares large language model (LLM) fine-tuning methods,
  including Quantized Low Rank Adapter (QLoRA), Retrieval Augmented fine-tuning (RAFT),
  and Reinforcement Learning from Human Feedback (RLHF), and additionally compared
  LLM evaluation methods including End to End (E2E) benchmark method of "Golden Answers",
  traditional natural language processing (NLP) metrics, RAG Assessment (Ragas), OpenAI
  GPT-4 evaluation metrics, and human evaluation, using the travel chatbot use case.
  The travel dataset was sourced from the Reddit API by requesting posts from travel-related
  subreddits to get travel-related conversation prompts and personalized travel experiences,
  and augmented for each fine-tuning method.
---

# A Comparison of LLM Finetuning Methods & Evaluation Metrics with Travel Chatbot Use Case

## Quick Facts
- arXiv ID: 2408.03562
- Source URL: https://arxiv.org/abs/2408.03562
- Authors: Sonia Meyer; Shreya Singh; Bertha Tam; Christopher Ton; Angel Ren
- Reference count: 40
- Primary result: Compared LLM fine-tuning methods (QLoRA, RAFT, RLHF) and evaluation metrics using travel chatbot use case, finding human evaluation most aligned with OpenAI GPT-4 metrics

## Executive Summary
This research evaluates different large language model fine-tuning approaches (QLoRA, RAFT, RLHF) and evaluation methods using a travel chatbot use case. The study sources travel-related conversation data from Reddit, fine-tunes LLaMa 2 7B and Mistral 7B models, and compares results across multiple evaluation metrics including traditional NLP metrics, RAG Assessment (Ragas), OpenAI GPT-4 evaluation, and human evaluation. The findings reveal significant discrepancies between automated metrics and human preferences, with Mistral RAFT emerging as the top-performing model before RLHF refinement. The study emphasizes the importance of keeping humans in the evaluation loop and demonstrates that traditional NLP metrics alone are insufficient for assessing LLM performance.

## Method Summary
The researchers collected travel-related conversation data from Reddit's travel subreddits to create a dataset of 2,000 examples. They fine-tuned two pretrained models (LLaMa 2 7B and Mistral 7B) using QLoRA and RAFT methods. Model outputs were evaluated using multiple approaches: traditional NLP metrics (ROUGE, BLEU), RAG Assessment (Ragas), OpenAI GPT-4 evaluation, and human evaluation with 10 participants. The best-performing model (Mistral RAFT) underwent additional RLHF training. The evaluation framework compared quantitative metrics against human judgment to assess alignment and identify the most effective fine-tuning approach for travel chatbot applications.

## Key Results
- Quantitative metrics (ROUGE, BLEU) and Ragas show poor correlation with human evaluation preferences
- OpenAI GPT-4 evaluation metrics align most closely with human judgment
- Mistral models generally outperformed LLaMa models across all evaluation methods
- RLHF significantly improved model performance after RAFT fine-tuning
- RAFT outperformed QLoRA but still required postprocessing for optimal results

## Why This Works (Mechanism)
The effectiveness of different fine-tuning methods depends on their ability to adapt pretrained models to domain-specific conversational patterns while maintaining general language understanding. QLoRA reduces memory requirements through quantization while preserving adapter-based fine-tuning benefits. RAFT incorporates retrieval augmentation during fine-tuning, allowing models to learn context-aware responses. RLHF refines model behavior through human preference learning, directly optimizing for human-aligned outputs. The evaluation methods work differently: traditional metrics measure surface-level similarity to reference text, while human evaluation captures nuanced aspects like helpfulness, coherence, and conversational quality that automated metrics miss.

## Foundational Learning
- Fine-tuning vs. Retrieval Augmentation: Why needed - determines whether to adapt model weights or retrieve external information during inference. Quick check - measure memory usage and response quality for both approaches.
- Human Evaluation Protocols: Why needed - automated metrics often disagree with human preferences. Quick check - compare metric scores against blind human rankings.
- Reinforcement Learning from Human Feedback: Why needed - optimizes for human preferences rather than likelihood of reference text. Quick check - measure preference rate in pairwise comparisons.
- Adapter-based Fine-tuning: Why needed - enables efficient parameter-efficient adaptation of large models. Quick check - compare performance with full fine-tuning using limited GPU resources.
- RAG Assessment Metrics: Why needed - evaluates retrieval-augmented generation quality beyond traditional metrics. Quick check - measure relevance and coherence scores for RAG outputs.

## Architecture Onboarding
- Component Map: Data Collection (Reddit API) -> Preprocessing -> Fine-tuning (QLoRA/RAFT) -> Inference Generation -> Multi-metric Evaluation (NLP Metrics, Ragas, GPT-4, Human)
- Critical Path: Fine-tuning → Inference → Human Evaluation (determines final model selection)
- Design Tradeoffs: Computational efficiency (QLoRA) vs. performance (full fine-tuning), automated evaluation speed vs. human evaluation accuracy, retrieval augmentation complexity vs. response quality
- Failure Signatures: Low human evaluation scores despite high traditional metrics indicate metric misalignment; high ROUGE scores with poor conversational quality suggest overfitting to reference text
- 3 First Experiments: 1) Compare QLoRA vs. RAFT on same dataset with identical evaluation protocol, 2) Run pairwise human evaluation between top automated metric performers, 3) Test GPT-4 evaluation consistency across multiple runs

## Open Questions the Paper Calls Out
None

## Limitations
- Small dataset size (2,000 examples) may limit generalizability of findings
- Reddit-sourced data may contain inconsistencies and biases affecting model performance
- Limited human evaluation panel (n=10) restricts statistical significance and demographic diversity
- Single domain focus (travel) may not translate to other use cases
- Potential evaluator bias without blind evaluation protocols

## Confidence
- Mistral RAFT superiority: Medium confidence (supported by evaluation data but limited by sample size)
- Human evaluation necessity: High confidence (strong evidence of metric misalignment)
- Traditional NLP metrics insufficiency: High confidence (clear divergence from human judgment)
- RLHF performance improvement: Medium confidence (based on single model post-RLHF results)

## Next Checks
1. Expand human evaluation panel to at least 50 participants with diverse demographic representation
2. Conduct cross-domain validation by testing same fine-tuning methods on non-travel datasets
3. Implement blind evaluation protocols where human evaluators are unaware of model origins