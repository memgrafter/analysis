---
ver: rpa2
title: 'Squat: Quant Small Language Models on the Edge'
arxiv_id: '2402.10787'
source_url: https://arxiv.org/abs/2402.10787
tags:
- quantization
- token
- arxiv
- quantized
- mobile
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Squat introduces an effective QAT framework with deployable quantization
  for small language models on mobile devices. It addresses the incompatibility of
  fine-grained quantization methods with mobile hardware by employing entropy-guided
  and distribution-aligned distillation to mitigate quantization distortion in attention
  modules, along with sub-8-bit token adaptive quantization that assigns varying bit
  widths to tokens based on importance.
---

# Squat: Quant Small Language Models on the Edge

## Quick Facts
- arXiv ID: 2402.10787
- Source URL: https://arxiv.org/abs/2402.10787
- Reference count: 40
- Primary result: Effective QAT framework with deployable quantization for small language models on mobile devices

## Executive Summary
Squat introduces an effective quantization-aware training (QAT) framework specifically designed for small language models (SLMs) on mobile devices. The framework addresses the fundamental incompatibility between fine-grained quantization methods and mobile hardware by employing entropy-guided and distribution-aligned distillation to mitigate quantization distortion in attention modules, along with sub-8-bit token adaptive quantization that assigns varying bit widths to tokens based on importance. Extensive experiments show substantial improvements over other QAT methods across various datasets, achieving up to 2.37× on-device speedup compared to FP16 counterparts.

## Method Summary
Squat employs a comprehensive QAT framework that combines entropy-guided and distribution-aligned distillation with token adaptive quantization. The method statistically maximizes the entropy of query and key distributions in attention modules while aligning attention map distributions between quantized and FP16 models. Token importance is assessed based on average attention to the initial token, allowing mixed-precision quantization with 4-bit and 8-bit assignments. A custom SIMD-based Multi-Kernel Mixed-Precision (MKMP) multiplier enables efficient execution of sub-8-bit mixed-precision computations on mobile hardware. The framework uses distillation loss, entropy loss (LE), and distribution loss (LD) with ratios rE=0.5 and rD=1 during training.

## Key Results
- Achieves up to 2.37× on-device speedup compared to FP16 counterparts
- Substantial improvements over other QAT methods across various datasets
- Effective mitigation of quantization distortion in attention modules through entropy-guided and distribution-aligned distillation

## Why This Works (Mechanism)

### Mechanism 1: Entropy-guided distillation
- **Claim**: Entropy-guided distillation mitigates quantization-induced information loss in attention mechanisms by maximizing the entropy of query and key distributions.
- **Core assumption**: The distributions of queries and keys in the self-attention module follow Gaussian distributions, and maximizing entropy for Gaussian distributions is equivalent to minimizing quantization error.
- **Evidence anchors**: Entropy loss is proposed to statistically maximize the output entropy of quantized query and key tensors, corresponding to minimizing quantization error under Gaussian assumptions.
- **Break condition**: If the query/key distributions deviate significantly from Gaussian (e.g., highly skewed or multimodal), the entropy maximization may not effectively minimize quantization error.

### Mechanism 2: Token adaptive quantization
- **Claim**: Token adaptive quantization assigns varying bit widths to tokens based on their importance, reducing redundancy while maintaining performance.
- **Core assumption**: Token importance can be effectively measured by average attention to the initial token, and this importance correlates with the token's contribution to model performance.
- **Evidence anchors**: Tokens are evaluated based on their average attention to the initial token, with more important tokens assigned 8-bit quantization and less important ones 4-bit quantization.
- **Break condition**: If the attention-based importance metric poorly correlates with actual token contribution, the mixed-precision allocation may sacrifice accuracy without sufficient efficiency gains.

### Mechanism 3: MKMP multiplier
- **Claim**: SIMD-based Multi-Kernel Mixed-Precision (MKMP) multiplier enables efficient execution of sub-8-bit mixed-precision computations on mobile hardware.
- **Core assumption**: Mobile processors can efficiently execute the concatenation and bit-shift operations required for the MKMP multiplier design without significant overhead.
- **Evidence anchors**: The MKMP multiplier extends existing INT8 multipliers by concatenating adjacent 4-bit weight values into 16-bit registers for parallel multiplication.
- **Break condition**: If the hardware overhead of concatenation and bit manipulation exceeds the computational savings, or if the mobile processor lacks efficient support for the required operations.

## Foundational Learning

- **Concept**: Quantization-Aware Training (QAT)
  - Why needed here: QAT allows joint optimization of weights and activations during training, reducing quantization error compared to post-training quantization methods, which is crucial for achieving high accuracy with aggressive quantization on resource-constrained mobile devices.
  - Quick check question: What is the key difference between QAT and PTQ, and why does this difference matter for small language models on mobile devices?

- **Concept**: Attention mechanism in transformers
  - Why needed here: Understanding how self-attention works is essential because the paper identifies quantization of queries and keys as the primary source of performance degradation, and the entropy-guided method specifically targets this component.
  - Quick check question: How does the self-attention mechanism compute attention scores, and why would quantizing the query and key matrices be particularly problematic?

- **Concept**: SIMD (Single Instruction Multiple Data) architecture
  - Why needed here: The MKMP multiplier design leverages SIMD capabilities, and understanding how SIMD instructions work is crucial for appreciating the computational efficiency gains and the challenges of sub-8-bit quantization on mobile devices.
  - Quick check question: What are the fundamental constraints of SIMD instructions that make sub-8-bit quantization challenging on mobile devices, and how does the MKMP multiplier address these constraints?

## Architecture Onboarding

- **Component map**: Entropy-guided and distribution-aligned distillation module -> Token Control Logic Module (TCLM) for importance-based quantization -> MKMP multiplier for mixed-precision MAC operations -> Standard QAT framework with forward and backward propagation

- **Critical path**: During training, tokens flow through TCLM for importance-based quantization, then through the MKMP multiplier for computation, while entropy and distribution losses are calculated and applied to the attention module to mitigate quantization distortion.

- **Design tradeoffs**: The framework trades implementation complexity (custom multiplier, importance-based quantization) for improved accuracy at aggressive quantization levels, while accepting some hardware-specific optimization that may not generalize to all edge platforms.

- **Failure signatures**: 
  - Accuracy degradation despite QAT (likely issues with entropy/distribution loss implementation)
  - No speedup on target hardware (likely MKMP multiplier not properly optimized for specific SIMD architecture)
  - Memory overflow (token adaptive quantization creating too many bit-width variations)

- **First 3 experiments**:
  1. Implement basic QAT with uniform quantization and verify it reproduces baseline results before adding Squat-specific components
  2. Add entropy loss component and measure its impact on attention module accuracy while keeping other components constant
  3. Integrate token adaptive quantization with importance scoring and validate that more important tokens indeed receive higher bit-width allocation

## Open Questions the Paper Calls Out
None

## Limitations
- The MKMP multiplier implementation lacks independent validation on actual mobile hardware
- The Gaussian distribution assumption for query/key tensors may not hold for all attention patterns
- The token importance metric based on attention to initial token lacks broader validation across different tasks

## Confidence
### High Confidence Claims
- The fundamental problem of quantization incompatibility with mobile hardware
- The use of QAT rather than PTQ for small language models
- The general architecture of using mixed-precision quantization to balance accuracy and efficiency

### Medium Confidence Claims
- The entropy-guided and distribution-aligned distillation method effectively mitigates quantization distortion
- The token adaptive quantization approach provides meaningful importance ranking
- The claimed speedup improvements are achievable on representative mobile hardware

### Low Confidence Claims
- The specific implementation details of the MKMP multiplier and its actual efficiency gains
- The generalization of the Gaussian distribution assumption across all attention patterns
- The robustness of the importance metric across diverse language tasks and model sizes

## Next Checks
1. **Hardware Validation Test**: Implement the MKMP multiplier on a representative mobile device and measure actual computational throughput and power consumption compared to baseline FP16 and INT8 implementations.

2. **Distribution Assumption Validation**: Analyze the actual distributions of query and key tensors across multiple attention heads and layers in the trained model. Compare these distributions against Gaussian assumptions and measure how well entropy maximization correlates with actual quantization error reduction in non-Gaussian cases.

3. **Importance Metric Ablation Study**: Conduct controlled experiments varying the token importance metric (e.g., using different attention patterns, gradient-based importance, or random assignment) to quantify the contribution of the specific attention-to-initial-token metric to overall model accuracy and efficiency.