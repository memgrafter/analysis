---
ver: rpa2
title: Improving Composed Image Retrieval via Contrastive Learning with Scaling Positives
  and Negatives
arxiv_id: '2404.11317'
source_url: https://arxiv.org/abs/2404.11317
tags:
- image
- examples
- negative
- uni00000011
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of insufficient positive and
  negative examples in Composed Image Retrieval (CIR) tasks, which limits the effectiveness
  of contrastive learning approaches. The authors propose two key methods to scale
  up training data: (1) a data generation pipeline using a multi-modal large language
  model to automatically create high-quality positive examples from existing image
  datasets, and (2) a two-stage fine-tuning framework that introduces static negative
  representations in the second stage to increase the number of negative examples.'
---

# Improving Composed Image Retrieval via Contrastive Learning with Scaling Positives and Negatives

## Quick Facts
- arXiv ID: 2404.11317
- Source URL: https://arxiv.org/abs/2404.11317
- Authors: Zhangchi Feng; Richong Zhang; Zhijie Nie
- Reference count: 40
- Primary result: State-of-the-art CIR performance on FashionIQ and CIRR datasets using contrastive learning with scaled positives/negatives

## Executive Summary
This paper addresses the fundamental challenge in Composed Image Retrieval (CIR) of insufficient positive and negative training examples for effective contrastive learning. The authors propose a novel approach that scales up training data through automated positive example generation using multi-modal large language models, combined with a two-stage fine-tuning framework that introduces static negative representations. Their method achieves state-of-the-art results on benchmark CIR datasets without requiring architectural modifications to existing models, demonstrating effectiveness in both standard and zero-shot settings.

## Method Summary
The paper introduces a two-pronged approach to scale up contrastive learning for CIR tasks. First, they employ a data generation pipeline using multi-modal LLMs to automatically create high-quality positive examples from existing image datasets, addressing the scarcity of positive pairs. Second, they implement a two-stage fine-tuning framework where stage 1 trains the model normally, and stage 2 introduces static negative representations to increase the diversity of negative examples. The method is designed as a plug-and-play solution that can be applied to existing CIR models without architectural changes, making it broadly applicable to the field.

## Key Results
- Achieves state-of-the-art performance on FashionIQ and CIRR datasets
- Demonstrates 1-6% improvements in R@10 and R@50 metrics compared to existing methods
- Shows strong performance in zero-shot settings, validating effectiveness in low-resource scenarios
- Outperforms baselines while maintaining computational efficiency through the two-stage framework

## Why This Works (Mechanism)
The approach works by addressing the data scarcity problem inherent in CIR tasks through systematic scaling of both positive and negative examples. By using multi-modal LLMs to generate diverse positive examples, the model learns richer representations of the target concepts. The two-stage fine-tuning with static negative representations ensures the model encounters sufficient negative examples during training, which is crucial for effective contrastive learning. The plug-and-play nature allows the method to enhance existing CIR models without requiring architectural modifications, making it practical for real-world deployment.

## Foundational Learning
- **Composed Image Retrieval (CIR)**: A task where models retrieve images based on textual modifications of reference images, requiring understanding of both visual and textual modalities. Needed to frame the problem context and understand why data scaling is critical.
- **Contrastive Learning**: A training paradigm that learns representations by comparing similar (positive) and dissimilar (negative) pairs, fundamental to understanding the paper's core methodology.
- **Multi-modal Large Language Models (MLLMs)**: AI models capable of processing and generating both text and images, essential for the automated positive example generation pipeline.
- **Zero-shot Generalization**: Model's ability to perform well on unseen tasks or domains without task-specific training, key metric for evaluating practical applicability.
- **R@K Metrics**: Recall at K metrics measuring retrieval performance by checking if the correct item appears in the top K results, standard evaluation metric for CIR tasks.

## Architecture Onboarding
**Component Map:** Multi-modal LLM -> Positive Example Generator -> Two-stage Fine-tuning Pipeline -> CIR Model
**Critical Path:** Data generation (positive examples) → Stage 1 fine-tuning → Stage 2 fine-tuning (with static negatives) → Final model evaluation
**Design Tradeoffs:** The approach trades computational overhead during training (two-stage fine-tuning) for improved downstream performance and zero-shot generalization, while maintaining model architecture unchanged for practical applicability.
**Failure Signatures:** Poor performance likely indicates issues with LLM-generated positive example quality, insufficient negative example diversity in stage 2, or improper hyperparameter tuning during fine-tuning stages.
**First Experiments:** 1) Evaluate LLM-generated positive example quality on a small validation set, 2) Test single-stage vs two-stage fine-tuning performance difference, 3) Measure impact of different negative sample sizes on final model performance

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Reliance on multi-modal LLMs for positive example generation may limit generalizability across different domains or LLM variants
- The static negative representation approach lacks theoretical justification compared to dynamic negative mining strategies
- Zero-shot generalization claims are based on limited experiments with only Fashion-IQ and CIRR datasets

## Confidence
- **High Confidence**: Empirical results showing SOTA performance on FashionIQ and CIRR datasets are well-documented and reproducible
- **Medium Confidence**: Two-stage fine-tuning framework effectiveness demonstrated, but freezing negatives in stage 2 needs more theoretical justification
- **Low Confidence**: Zero-shot generalization claims based on limited experiments without testing on truly out-of-domain data

## Next Checks
1. Ablation studies on LLM variants: Test how different multi-modal LLMs (GPT-4V, Claude 3, etc.) and prompt engineering strategies affect the quality of generated positive examples and downstream performance

2. Dynamic negative mining evaluation: Compare the static negative representation approach against adaptive negative mining strategies that update negative examples based on model learning progress

3. Cross-dataset zero-shot evaluation: Evaluate the model on CIR datasets from completely different domains (e.g., furniture, cars) to validate true zero-shot generalization capabilities beyond Fashion-IQ and CIRR