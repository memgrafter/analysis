---
ver: rpa2
title: 'SynTQA: Synergistic Table-based Question Answering via Mixture of Text-to-SQL
  and E2E TQA'
arxiv_id: '2409.16682'
source_url: https://arxiv.org/abs/2409.16682
tags:
- table
- text-to-sql
- answer
- question
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the synergy between Text-to-SQL and end-to-end
  question answering (E2E TQA) for table-based question answering (TQA). Through comprehensive
  evaluation, the authors identify complementary strengths: Text-to-SQL excels at
  arithmetic operations and long tables, while E2E TQA performs better on ambiguous
  questions, complex schemas, and table contents.'
---

# SynTQA: Synergistic Table-based Question Answering via Mixture of Text-to-SQL and E2E TQA

## Quick Facts
- **arXiv ID**: 2409.16682
- **Source URL**: https://arxiv.org/abs/2409.16682
- **Reference count**: 27
- **Primary result**: Achieves state-of-the-art results on benchmark datasets, with accuracy reaching 93.2% on WIKI SQL and 71.6% on WTQ

## Executive Summary
This paper investigates the synergy between Text-to-SQL and end-to-end question answering (E2E TQA) for table-based question answering (TQA). Through comprehensive evaluation, the authors identify complementary strengths: Text-to-SQL excels at arithmetic operations and long tables, while E2E TQA performs better on ambiguous questions, complex schemas, and table contents. Based on these findings, they propose SynTQA, an approach that integrates both methods via answer selection. Experiments demonstrate that ensembling models using either feature-based or LLM-based selectors significantly improves performance over individual models. The method achieves state-of-the-art results on benchmark datasets, with accuracy reaching 93.2% on WIKI SQL and 71.6% on WTQ, validating the effectiveness of combining both approaches.

## Method Summary
SynTQA integrates Text-to-SQL and E2E TQA approaches through answer selection to leverage their complementary strengths. The method involves fine-tuning base models (T5 for Text-to-SQL, OmniTab for E2E TQA) on table-question-answer datasets, then using a selector (either feature-based random forest or LLM-based) to choose between their predictions. The feature-based selector uses engineered features like question characteristics, table characteristics, and model confidence scores, while the LLM-based selector uses direct prompting without training data. The approach achieves state-of-the-art performance by exploiting Text-to-SQL's strength in arithmetic operations and long tables, and E2E TQA's advantage with ambiguous questions and complex schemas.

## Key Results
- Ensembling models using feature-based or LLM-based selectors significantly improves performance over individual models
- Achieves state-of-the-art results with 93.2% accuracy on WIKI SQL and 71.6% on WTQ
- Feature-based selectors show model confidence as the most impactful feature for answer selection
- LLM-based selectors demonstrate remarkable few-shot capabilities without requiring training data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Answer selectors can outperform individual Text-to-SQL or E2E TQA models by leveraging complementary strengths.
- **Mechanism**: By comparing predictions from both models and selecting the more reliable answer, the ensemble approach mitigates individual model weaknesses.
- **Core assumption**: The two model types make errors on different subsets of questions.
- **Evidence anchors**:
  - Experiments demonstrate that ensembling models using either feature-based or LLM-based selectors significantly improves performance over individual models.
  - Prompting methods (GPT and DIN-SQL) underperform fine-tuned models in table understanding on WTQ and WIKI SQL.
- **Break condition**: If both models fail on the same question types, the selector cannot recover accuracy.

### Mechanism 2
- **Claim**: Feature-based selectors improve performance by encoding model confidence and table characteristics.
- **Mechanism**: A random forest classifier is trained on features like question word, table size, and model confidence to predict the correct answer.
- **Core assumption**: These engineered features correlate with answer correctness.
- **Evidence anchors**:
  - Features designed include question characteristics (e.g., question word and length), table characteristics (e.g., table size, header and question overlapping, and truncation).
  - Confidence of Text-to-SQL and E2E TQA models is the most impactful feature for SYNTQA (RF).
- **Break condition**: If feature correlations change significantly with new datasets or model versions, the classifier may degrade.

### Mechanism 3
- **Claim**: LLM-based selectors can select correct answers without training data by leveraging in-context reasoning.
- **Mechanism**: Direct prompting with few-shot examples allows the LLM to choose between candidate answers based on contextual understanding.
- **Core assumption**: LLMs can effectively reason about table contents and answer plausibility in few-shot settings.
- **Evidence anchors**:
  - SYNTQA (GPT) does not require training data thanks to LLMs' remarkable few-shot capabilities.
  - A heuristic-enhanced prompting strategy elevates SOTA performance to 74.4% and 93.6% on WTQ and WIKI SQL.
- **Break condition**: If the LLM's few-shot reasoning ability degrades or the prompt engineering fails to capture critical distinctions.

## Foundational Learning

- **Concept**: Semantic parsing (Text-to-SQL)
  - **Why needed here**: Understanding how Text-to-SQL models convert natural language questions into executable SQL queries is essential for evaluating their strengths and limitations.
  - **Quick check question**: What is the primary intermediate representation used by Text-to-SQL models to answer table-based questions?

- **Concept**: End-to-end table question answering (E2E TQA)
  - **Why needed here**: Understanding how E2E TQA models directly generate answers from tables without intermediate SQL queries is crucial for assessing their complementary capabilities.
  - **Quick check question**: How do E2E TQA models differ from Text-to-SQL models in their approach to answering table-based questions?

- **Concept**: Model ensembling and answer selection
  - **Why needed here**: The core innovation relies on combining multiple models through answer selection, requiring understanding of how different models' outputs can be compared and chosen.
  - **Quick check question**: What are the two main approaches for answer selection described in the paper?

## Architecture Onboarding

- **Component map**: Question + Table → Text-to-SQL Model → SQL Query → Answer; Question + Table → E2E TQA Model → Direct Answer; Both answers → Answer Selector (Feature-based or LLM-based) → Final Selected Answer

- **Critical path**: 
  1. Both models process the same input
  2. Each model generates an answer with confidence score
  3. Selector receives both answers and their confidence scores
  4. Selector chooses the more reliable answer
  5. Selected answer is returned as final output

- **Design tradeoffs**:
  - Using fine-tuned models vs. prompting methods (fine-tuned models show better performance)
  - Feature-based vs. LLM-based selectors (feature-based requires training, LLM-based requires prompt engineering)
  - Computational cost vs. accuracy (LLM-based approaches are more expensive)

- **Failure signatures**:
  - Both models consistently fail on the same question types
  - Selector confidence scores are too similar to distinguish answers
  - Feature correlations change significantly with new data distributions

- **First 3 experiments**:
  1. Run both Text-to-SQL and E2E TQA models independently on a small dataset to identify their individual error patterns
  2. Implement a simple majority-vote selector and compare accuracy against individual models
  3. Train a feature-based selector using confidence scores and table characteristics, then evaluate its performance

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided content.

## Limitations
- The approach requires having both Text-to-SQL and E2E TQA models available, which may not be practical for all applications
- Feature-based selectors require training data and careful feature engineering, while LLM-based selectors are computationally expensive
- The paper doesn't extensively address cases where both models fail simultaneously
- Performance claims are limited to specific benchmark datasets (WTQ and WIKI SQL)

## Confidence

- **High confidence**: The complementary strengths of Text-to-SQL (arithmetic operations, long tables) and E2E TQA (ambiguous questions, complex schemas) are well-supported by experimental results
- **Medium confidence**: The effectiveness of both feature-based and LLM-based selectors is demonstrated, but the specific design choices for features and prompts could impact real-world performance
- **Medium confidence**: The claim of state-of-the-art results is supported, but the comparison is limited to specific benchmark datasets

## Next Checks
1. **Error Analysis**: Conduct a detailed error analysis to identify specific question types where both Text-to-SQL and E2E TQA models consistently fail, and evaluate whether the selector can recover accuracy in these cases
2. **Generalization Testing**: Test the approach on additional table-based QA datasets beyond WTQ and WIKI SQL to assess whether the observed complementary strengths hold across different data distributions and table schemas
3. **Computational Efficiency**: Measure the inference time and resource requirements of both selector approaches (feature-based vs. LLM-based) to provide a complete picture of the practical tradeoffs between accuracy and efficiency