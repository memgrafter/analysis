---
ver: rpa2
title: Offline Regularised Reinforcement Learning for Large Language Models Alignment
arxiv_id: '2405.19107'
source_url: https://arxiv.org/abs/2405.19107
tags:
- arxiv
- learning
- policy
- value
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DRO (Direct Reward Optimisation) as a framework
  for aligning large language models using single-trajectory data, where each example
  consists of a prompt, a model response, and a scalar reward (e.g., thumbs-up/down).
  Unlike traditional RLHF methods that require pairwise comparisons, DRO uses a simple
  mean-squared objective to learn both a policy and a value function from such data.
---

# Offline Regularised Reinforcement Learning for Large Language Models Alignment

## Quick Facts
- arXiv ID: 2405.19107
- Source URL: https://arxiv.org/abs/2405.19107
- Reference count: 36
- One-line primary result: DRO-V achieves 78.9% vs 67.5% win rate over SFT for T5-L on UltraFeedback dataset

## Executive Summary
This paper proposes DRO (Direct Reward Optimisation) as a framework for aligning large language models using single-trajectory data with scalar rewards. Unlike traditional RLHF methods that require pairwise comparisons, DRO uses a mean-squared objective to learn both policy and value function from prompt-completion-reward triplets. The authors introduce DRO-V, a practical algorithm that combines offline policy learning with value function learning, and demonstrate its superiority over KTO on the UltraFeedback dataset using T5 models.

## Method Summary
DRO-V learns from datasets of (prompt, completion, reward) triplets without requiring pairwise comparisons. The method jointly optimizes a policy network and value network using a mean-squared objective that combines reward, value function, and policy log-ratio. Training involves sampling batches from the dataset, computing the DRO loss, and updating both networks' parameters simultaneously. The algorithm is evaluated against SFT and KTO baselines using side-by-side comparisons judged by PaLM2.

## Key Results
- DRO-V achieves 78.9% vs 67.5% win rate over SFT for T5-L on UltraFeedback dataset
- DRO-V achieves 81.5% vs 78.2% win rate over SFT for T5-XL on UltraFeedback dataset
- DRO-V achieves 63.4% vs 57.5% win rate over KTO for T5-L and T5-XL models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DRO directly optimizes the policy without requiring a separate reward model, which reduces model complexity and potential for reward hacking.
- **Mechanism:** The DRO loss function combines the reward, value function, and policy log-ratio into a single mean-squared objective, allowing joint optimization without an intermediate reward model.
- **Core assumption:** The optimal policy can be expressed in terms of the reward and a value function that normalizes the policy logits.
- **Evidence anchors:** [abstract] "DRO uses a simple mean-squared objective that can be implemented in various ways"; [section 3.1] "The DRO objective. Now, we rearrange the optimality condition from above that holds jointly with π = π∗, V = V∗"
- **Break condition:** If the behavior policy µ does not cover the support of πref, the uniqueness of the optimal solution may fail.

### Mechanism 2
- **Claim:** The value function V in DRO provides a stable learning signal for the policy, even when the policy is off-policy relative to the data.
- **Mechanism:** The value function Vϕ is learned alongside the policy and subtracted from the reward in the policy gradient. This is crucial because the samples are off-policy, and without this term, the policy gradient would be biased.
- **Core assumption:** The value function can be approximated well enough from the single-trajectory data to provide a useful baseline.
- **Evidence anchors:** [section 3.3] "Second, the value function Vϕ(xi) which is subtracted from the reward r(xi, yi) is not simply a baseline used for the purpose of variance reduction as is usually the case in RL"; [section 3.3] "Thus it is important to maintain this value function estimate in the policy gradient."
- **Break condition:** If the value function Vϕ is poorly approximated, the policy gradient will be biased and learning will fail.

### Mechanism 3
- **Claim:** DRO-V's empirical superiority over KTO is due to its theoretically principled joint optimization of policy and value function, avoiding simplifying assumptions.
- **Mechanism:** KTO assumes a constant partition function Z for each batch, while DRO-V does not make this assumption and learns the value function V explicitly. This allows DRO-V to better capture the true reward structure in the data.
- **Core assumption:** The true value function V varies across prompts and cannot be approximated by a constant or batch-level statistic.
- **Evidence anchors:** [section 2] "KTO also makes strong simplifying assumptions, which as we will show, biases the method to produce suboptimal policies"; [section 3.1] "Unlike KTO, which assumes a constant partition function Z for each prompt of the batch, we do not make any assumptions on the form of Z or V"; [experiments] "DRO-V significantly outperforms KTO, when using T5 encoders with up to 3 billion parameters, on the UltraFeedback dataset."
- **Break condition:** If the data is such that a constant or batch-level value function is actually a good approximation, KTO might perform as well as or better than DRO-V.

## Foundational Learning

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed here: DRO is positioned as an alternative to RLHF methods, so understanding the RLHF framework is crucial for contextualizing DRO's contributions.
  - Quick check question: What are the main components of a typical RLHF pipeline, and how does DRO differ in its approach to aligning LLMs with human preferences?

- **Concept: Policy Optimization and Value Functions**
  - Why needed here: DRO jointly optimizes a policy and a value function, so a solid understanding of these concepts is essential for grasping how DRO works.
  - Quick check question: How does the value function in DRO differ from a standard baseline in policy gradient methods, and why is it important for off-policy learning?

- **Concept: KL Regularization and Entropy in RL**
  - Why needed here: The DRO objective is derived from a KL-regularized policy optimization setting, so familiarity with this concept is helpful for understanding the theoretical foundations of DRO.
  - Quick check question: What role does the KL regularization term play in the DRO objective, and how does it influence the learned policy?

## Architecture Onboarding

- **Component map:** Dataset (prompt, completion, reward) triplets -> Policy Network (πθ) -> Value Network (Vϕ) -> DRO Loss -> Optimizer -> Updated θ and ϕ

- **Critical path:**
  1. Sample a batch of (prompt, completion, reward) triplets from the dataset
  2. Compute the DRO loss for the batch, which involves the policy network, value network, and the reward data
  3. Compute gradients of the loss with respect to θ and ϕ
  4. Update θ and ϕ using the optimizer
  5. Repeat until convergence

- **Design tradeoffs:**
  - Separate vs. Shared Networks: Using separate networks for π and V was found to be empirically better than sharing parameters, despite the increased computational cost
  - Single vs. Multiple Values per Batch: Using a single value per batch was found to hurt performance compared to using a value for each example in the batch
  - Value Function Approximation: The choice of how to approximate the value function (e.g., using a neural network vs. a simpler function) can significantly impact performance

- **Failure signatures:**
  - Poor Value Function Approximation: If the value function is not learned well, the policy gradient will be biased, leading to suboptimal policies
  - Insufficient Data Coverage: If the behavior policy does not cover the support of the reference policy, the theoretical guarantees of DRO may not hold
  - Hyperparameter Sensitivity: The performance of DRO-V can be sensitive to the choice of the regularization parameter τ and the learning rates for the policy and value networks

- **First 3 experiments:**
  1. Ablation Study on Value Function: Train DRO-V with and without the value function to empirically demonstrate its importance for off-policy learning
  2. Comparison with KTO: Implement KTO and compare its performance to DRO-V on the UltraFeedback dataset using T5 models
  3. Hyperparameter Sensitivity Analysis: Systematically vary the regularization parameter τ and the learning rates to understand their impact on DRO-V's performance

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset scope and generalization: Evaluation based solely on UltraFeedback dataset limits confidence in generalizability across different preference distributions
- Model scale constraints: Results demonstrated only on T5 models up to 3 billion parameters; scalability to frontier LLMs remains untested
- Evaluation methodology limitations: Win-rate evaluation using PaLM2 as a judge introduces potential confounding factors and biases

## Confidence

**High confidence:** The theoretical framework of DRO and its derivation from KL-regularized policy optimization appears sound. The mean-squared objective formulation is mathematically rigorous.

**Medium confidence:** The empirical superiority of DRO-V over KTO on UltraFeedback dataset is supported by the reported win rates. However, this is based on a single dataset and model family.

**Low confidence:** Claims about computational efficiency gains and scalability to larger models are not empirically validated. The paper's assertions about DRO being "simpler" than RLHF methods need more rigorous benchmarking against established RLHF pipelines.

## Next Checks

1. **Cross-dataset validation:** Test DRO-V on at least two additional preference datasets (e.g., Anthropic's HH-RLHF, OpenWebText feedback data) to assess generalization across different preference distributions and domains.

2. **Computational benchmarking:** Conduct head-to-head runtime comparisons between DRO-V, KTO, and a standard RLHF pipeline (including reward model training) on identical hardware, measuring wall-clock time, memory usage, and convergence speed across different model scales.

3. **Human evaluation validation:** Conduct a human preference study where at least 100 participants evaluate outputs from DRO-V, KTO, and SFT models on a diverse set of prompts, comparing human preferences with the PaLM2 judge's decisions to assess alignment with human values.