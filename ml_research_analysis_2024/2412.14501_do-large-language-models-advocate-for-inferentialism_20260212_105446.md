---
ver: rpa2
title: Do Large Language Models Advocate for Inferentialism?
arxiv_id: '2412.14501'
source_url: https://arxiv.org/abs/2412.14501
tags:
- llms
- language
- semantics
- inferentialism
- meaning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper explores whether large language models (LLMs) like ChatGPT\
  \ and Claude can be understood through Robert Brandom\u2019s inferential semantics,\
  \ an anti-representationalist theory of meaning. Unlike traditional distributional\
  \ semantics, which views meaning through word co-occurrence, inferential semantics\
  \ grounds meaning in inferential roles and normative linguistic practices."
---

# Do Large Language Models Advocate for Inferentialism?

## Quick Facts
- **arXiv ID**: 2412.14501
- **Source URL**: https://arxiv.org/abs/2412.14501
- **Reference count**: 26
- **Primary result**: LLMs exhibit anti-representationalist properties and support inferential semantics through their attention mechanisms and RLHF-based normative practices

## Executive Summary
This paper explores whether large language models (LLMs) like ChatGPT and Claude can be understood through Robert Brandom's inferential semantics, an anti-representationalist theory of meaning. Unlike traditional distributional semantics that views meaning through word co-occurrence, inferential semantics grounds meaning in inferential roles and normative linguistic practices. The authors analyze how LLM mechanisms—such as attention heads, substitution, and anaphora—align with Brandom's ISA (Inference, Substitution, Anaphora) approach. They find that LLMs exhibit anti-representationalist properties and support a consensus theory of truth based on interactive feedback. While tensions remain due to LLMs' sub-symbolic nature, the study argues that inferential semantics offers valuable insights into how LLMs generate meaning without reference to external world representations.

## Method Summary
The paper provides a theoretical analysis comparing LLM architecture (Transformer-based) with Brandom's inferential semantics framework. The authors examine how attention mechanisms, substitution capabilities, and anaphora resolution in LLMs relate to the ISA (Inference, Substitution, Anaphora) components of inferential semantics. They also analyze how RLHF and other human-in-the-loop mechanisms create normative practices that ground truth and correctness in LLMs, developing a consensus theory of truth appropriate for these models.

## Key Results
- LLMs exhibit fundamentally anti-representationalist properties in their processing of language
- Transformer attention mechanisms capture inferential patterns consistent with inferential semantics
- RLHF mechanisms establish consensus-based normative practices that ground truth in LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs learn meaning through inferential patterns in language use rather than direct representation of world facts
- Mechanism: The Transformer architecture captures statistical relationships between tokens through attention mechanisms, creating distributed representations that encode inferential possibilities
- Core assumption: Meaning is determined by how words are used in context rather than by reference to external entities
- Evidence anchors:
  - [abstract] "We demonstrate that LLMs exhibit fundamentally anti-representationalist properties in their processing of language"
  - [section] "LLMs acquire inference capabilities from patterns of language use contained in the training data, without being explicitly given logical inference rules"
- Break condition: If empirical studies show LLMs cannot capture certain inferential patterns that humans easily process

### Mechanism 2
- Claim: LLMs exhibit quasi-compositional semantics consistent with inferentialism's rejection of strict compositionality
- Mechanism: Attention-based processing allows context-dependent meaning construction that doesn't follow traditional compositional rules
- Core assumption: Natural language is inherently quasi-compositional, with meaning emerging from accumulated inferential patterns rather than formal rules
- Evidence anchors:
  - [abstract] "Our analysis suggests that LLMs may challenge traditional assumptions in philosophy of language, including strict compositionality"
  - [section] "The quasi-compositional stance of inferentialism maintains that '[i]t is important not to treat languages as more compositional than they are'"
- Break condition: If models are developed that perfectly implement strict compositionality while maintaining performance

### Mechanism 3
- Claim: RLHF and other human-in-the-loop mechanisms create normative practices that ground truth and correctness in LLMs
- Mechanism: Human feedback through RLHF establishes consensus-based standards for what constitutes appropriate output
- Core assumption: Truth and correctness in LLMs are determined by interactive normative practices rather than correspondence to external facts
- Evidence anchors:
  - [abstract] "We further develop a consensus theory of truth appropriate for LLMs, grounded in their interactive and normative dimensions through mechanisms like RLHF"
  - [section] "Truth in inferentialism is based on the notion of normativity among linguistic subjects"
- Break condition: If alternative truth theories better explain LLM behavior or if RLHF proves insufficient for establishing norms

## Foundational Learning

- Concept: Anti-representationalism vs Representationalism
  - Why needed here: The paper's core argument is that LLMs align with anti-representationalist theories rather than traditional representationalist approaches
  - Quick check question: What is the key difference between how representationalist and anti-representationalist theories explain how language connects to the world?

- Concept: Inferential semantics and the ISA approach
  - Why needed here: The paper uses Brandom's inferential semantics framework, particularly the ISA (Inference, Substitution, Anaphora) components
  - Quick check question: How does Brandom's ISA approach translate traditional semantic concepts like reference and truth into anti-representationalist terms?

- Concept: Quasi-compositionality
  - Why needed here: The paper argues that LLMs exhibit quasi-compositional behavior that aligns with inferentialism's rejection of strict compositionality
  - Quick check question: Why does the paper argue that LLMs demonstrate quasi-compositionality rather than strict compositionality?

## Architecture Onboarding

- Component map: Token embedding → multi-head attention processing → layer normalization → feed-forward networks → output generation → RLHF fine-tuning
- Critical path: Token embedding → multi-head attention processing → layer normalization → feed-forward networks → output generation → RLHF fine-tuning
- Design tradeoffs: Static embeddings (Word2Vec) vs dynamic embeddings (Transformers); strict compositionality vs quasi-compositionality; representational vs anti-representational approaches
- Failure signatures: Hallucinations when training data lacks grounding; inability to capture certain inferential patterns; RLHF misalignment with human values
- First 3 experiments:
  1. Analyze attention patterns in induction heads to verify anaphora resolution capabilities
  2. Test compositional behavior with controlled linguistic stimuli to measure quasi-compositional properties
  3. Evaluate RLHF effectiveness by comparing pre/post-tuning outputs on normative tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs be considered rational agents capable of sapient behavior within Brandom's framework?
- Basis in paper: [explicit] The paper notes that Brandom distinguishes between sapient and sentient beings, with sapient creatures employing concepts within inferential reasoning. The paper questions whether LLMs can be considered rational agents capable of sapient behavior.
- Why unresolved: The paper acknowledges this as a fundamental question requiring systematic investigation beyond its scope. The sub-symbolic processing of LLMs and their lack of traditional rational agency create tension with Brandom's rationalist commitments.
- What evidence would resolve it: Empirical studies demonstrating LLMs' ability to engage in genuine inferential reasoning and conceptual employment, rather than merely pattern matching, would help resolve this question.

### Open Question 2
- Question: How can the tension between inferentialism's propositionalism and LLMs' sub-symbolic processing be resolved?
- Basis in paper: [explicit] The paper identifies a fundamental mismatch between inferentialism's commitment to propositional discreteness and LLMs' continuous vector representations. It presents both eliminativist and interpretability approaches as potential resolutions.
- Why unresolved: The paper presents this as an open theoretical challenge, noting that whether this tension can be resolved without abandoning either propositionalism or the sub-symbolic nature of LLMs remains uncertain.
- What evidence would resolve it: Development of a theoretical framework that successfully bridges propositional concepts with sub-symbolic neural representations, or conclusive evidence that such a bridge is impossible, would resolve this question.

### Open Question 3
- Question: How does the privileging of assertion in inferentialism apply to LLMs, which can handle various speech act types?
- Basis in paper: [explicit] The paper notes that Kukla and Lance criticize inferentialism for privileging assertion as the fundamental speech act type, and questions whether LLMs' architectural organization exhibits systematic differentiation between assertion and other speech act types.
- Why unresolved: The paper suggests that LLMs might not have difficulty handling various speech act types, but doesn't explore whether this challenges the inferentialist framework's assumptions about linguistic practice.
- What evidence would resolve it: Analysis of how LLMs process and generate different speech act types, and whether this reveals systematic patterns that either support or challenge the privileging of assertion in inferentialism.

## Limitations
- Theoretical analysis lacks direct empirical validation
- Consensus theory of truth through RLHF remains speculative
- Tension between propositionalism and sub-symbolic processing unresolved

## Confidence
- **High Confidence**: The alignment between Transformer attention mechanisms and inferential patterns in language use
- **Medium Confidence**: The quasi-compositional nature of LLM outputs and its relationship to inferential semantics
- **Low Confidence**: The consensus theory of truth grounded in RLHF mechanisms

## Next Checks
1. **Empirical Analysis of Attention Patterns**: Conduct controlled experiments analyzing attention head activations during anaphora resolution tasks to verify whether these mechanisms align with inferential semantics predictions about substitution and reference.

2. **Quasi-Compositionality Testing**: Design systematic linguistic stimuli testing both compositional and non-compositional behaviors across multiple LLM architectures to quantify the degree of quasi-compositionality and compare against inferential semantics predictions.

3. **RLHF Normativity Evaluation**: Implement comparative studies measuring changes in LLM outputs before and after RLHF fine-tuning on normative tasks, analyzing whether human feedback establishes consensus-based standards consistent with the proposed theory of truth.