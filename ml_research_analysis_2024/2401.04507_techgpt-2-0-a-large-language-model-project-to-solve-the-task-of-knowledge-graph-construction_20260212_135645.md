---
ver: rpa2
title: 'TechGPT-2.0: A large language model project to solve the task of knowledge
  graph construction'
arxiv_id: '2401.04507'
source_url: https://arxiv.org/abs/2401.04507
tags:
- data
- language
- fine-tuning
- large
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The TechGPT-2.0 project aimed to improve large language models
  (LLMs) for knowledge graph construction tasks like named entity recognition and
  relationship triple extraction. The authors trained two 7B parameter models and
  a QLoRA variant for long text processing on Huawei's Ascend servers.
---

# TechGPT-2.0: A large language model project to solve the task of knowledge graph construction

## Quick Facts
- arXiv ID: 2401.04507
- Source URL: https://arxiv.org/abs/2401.04507
- Reference count: 32
- Key outcome: TechGPT-2.0 trained two 7B parameter models and a QLoRA variant showing promising initial performance on NER and RTE tasks for knowledge graph construction

## Executive Summary
TechGPT-2.0 is a project that aims to improve large language models for knowledge graph construction tasks, specifically named entity recognition (NER) and relationship triple extraction (RTE). The team developed two 7B parameter models and a QLoRA variant for long text processing, trained on approximately 4 million instances of instruction fine-tuning data across various domains. The models were trained on Huawei's Ascend servers using techniques like LoRA and position interpolation to handle long text and prevent catastrophic forgetting. While detailed quantitative results are not yet available, the models demonstrated promising initial performance on NER and RTE tasks, showing that smaller LLMs can be effectively adapted for knowledge graph construction.

## Method Summary
The project collected approximately 4 million instances of instruction fine-tuning data across multiple domains, including general RTE and NER tasks, domain-specific tasks, dialogue data, and value alignment data. Two 7B parameter models and a QLoRA variant were trained using LoRA for parameter-efficient adaptation and position interpolation for handling long text inputs. The training was conducted on Huawei's Ascend servers using Mindspore and Mindformer libraries. The approach focused on converting raw data into instruction format and mixing it with general instruction data to maintain general capabilities while learning domain-specific knowledge graph construction tasks.

## Key Results
- Trained two 7B parameter models and a QLoRA variant for knowledge graph construction
- Models showed promising initial performance on NER and RTE tasks
- Successfully demonstrated smaller LLMs can be adapted for knowledge graph construction tasks
- Implemented position interpolation for handling long text processing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction fine-tuning data can unlock latent knowledge graph construction capabilities in pre-trained LLMs without requiring full retraining.
- Mechanism: LLMs already possess underlying semantic representations needed for NER and RTE, but lack task-specific prompting formats. Converting raw data into instruction-fine-tuning format and mixing with general instruction data teaches the model to recognize task patterns without catastrophic forgetting.
- Core assumption: LLM's pre-training corpus contains sufficient examples of entity relationships and domain knowledge that can be activated through appropriate fine-tuning.
- Evidence anchors:
  - [abstract] "The TechGPT-2.0 project aimed to improve large language models (LLMs) for knowledge graph construction tasks like named entity recognition (NER) and relationship triple extraction"
  - [section] "To ensure the sustained general ability of the large language model during the complete fine-tuning process, we adhere to conclusions drawn from prior experiments. A substantial volume of general instruction fine-tuning data is compiled, and this is amalgamated with knowledge graph domain data in appropriate proportions"
- Break condition: If pre-training data lacks sufficient domain-specific examples or instruction format mismatch is too large.

### Mechanism 2
- Claim: Position interpolation can extend the effective context window of LLMs without degrading performance on shorter texts.
- Mechanism: Scales longer position indices to original window range, maintaining relative distances while preventing attention score calculation degradation. This allows processing of longer documents while preserving original capabilities.
- Core assumption: Attention mechanism's relative distance computations remain effective when position indices are scaled rather than extrapolated.
- Evidence anchors:
  - [section] "The position interpolation method, enhancing the model's capacity to handle long texts. The key concept involves scaling the longer position index to the original window range, ensuring the maximum relative distance between any two tokens does not exceed the original window size"
- Break condition: If scaling causes too much information loss or relative distance assumptions break down for very long sequences.

### Mechanism 3
- Claim: Using QLoRA with 4-bit quantization significantly reduces training costs while maintaining performance comparable to full fine-tuning.
- Mechanism: Reduces parameter precision from 16 bits to 4 bits, lowering memory requirements and computational costs while low-rank adaptation matrices capture essential parameter changes needed for task adaptation.
- Core assumption: 4-bit quantization introduces minimal performance degradation for specific tasks being trained.
- Evidence anchors:
  - [abstract] "We offer two 7B large language model weights and a QLoRA weight specialized for processing lengthy texts"
  - [section] "QLoRA is a quantized version of LoRA, further reducing the bit representation from 16 bits to 4 bits, aiming to significantly reduce costs while preserving model effectiveness"
- Break condition: If quantization noise becomes significant for specific domain data or low-rank approximation is insufficient for task complexity.

## Foundational Learning

- Concept: Knowledge graph construction fundamentals (entities, relations, triples)
  - Why needed here: Project specifically targets NER and RTE tasks which are core components of knowledge graph construction
  - Quick check question: Can you explain the difference between named entity recognition and relationship triple extraction?

- Concept: Instruction fine-tuning methodology
  - Why needed here: Approach relies on converting raw data into instruction format for effective model adaptation
  - Quick check question: What are the key differences between full fine-tuning and parameter-efficient fine-tuning approaches?

- Concept: Attention mechanism and positional encoding
  - Why needed here: Position interpolation method for handling long texts depends on understanding how attention and positional encoding work together
  - Quick check question: How does the attention mechanism's computational complexity relate to input sequence length?

## Architecture Onboarding

- Component map: Base LLM (LLaMA2 architecture) -> Instruction fine-tuning pipeline -> QLoRA adapter modules -> Position interpolation module for long texts -> Data processing pipeline (Mindrecord format)

- Critical path:
  1. Data collection and preprocessing
  2. Model selection and configuration
  3. Instruction fine-tuning on Ascend server
  4. QLoRA adaptation for long text processing
  5. Evaluation and deployment

- Design tradeoffs:
  - Performance vs. computational cost (QLoRA vs full fine-tuning)
  - General capabilities vs. domain specialization
  - Model size vs. deployment constraints
  - Data quality vs. quantity

- Failure signatures:
  - Loss divergence during training
  - Hallucinations in generated outputs
  - Catastrophic forgetting of general capabilities
  - Poor performance on long text inputs

- First 3 experiments:
  1. Run a small-scale fine-tuning on a subset of the data to verify data format compatibility
  2. Test the position interpolation implementation on a simple long text task
  3. Compare QLoRA performance against a small full fine-tuning run on a validation task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TechGPT-2.0 compare to other LLMs specifically on knowledge graph construction tasks like NER and RTE?
- Basis in paper: [explicit] The paper states that TechGPT-2.0 was developed to improve LLMs for knowledge graph construction tasks, but doesn't provide detailed quantitative results or comparisons to other models.
- Why unresolved: The authors mention that due to server resource limitations, they couldn't pursue detailed experiments on their models, and only provide brief summaries of initial experimental outcomes.
- What evidence would resolve it: Comprehensive experimental results comparing TechGPT-2.0's performance on NER and RTE tasks against other established LLMs, with metrics like F1 scores, precision, and recall.

### Open Question 2
- Question: What is the optimal ratio of knowledge graph domain data to general instruction fine-tuning data for maintaining both domain-specific and general capabilities in LLMs?
- Basis in paper: [explicit] The authors mention using approximately 4 million instances of instruction fine-tuning data, combining general instruction data with knowledge graph domain data in appropriate proportions, but don't specify the exact ratio or test different ratios.
- Why unresolved: The paper states they adhere to conclusions from prior experiments but doesn't provide details on how the data proportions were determined or if alternative ratios were tested.
- What evidence would resolve it: Comparative experiments testing different ratios of knowledge graph domain data to general instruction data, measuring the impact on both domain-specific knowledge graph construction tasks and general language understanding abilities.

### Open Question 3
- Question: How does the QLoRA variant with position interpolation for long text processing perform compared to other long-context approaches for knowledge graph construction?
- Basis in paper: [explicit] The authors mention using QLoRA with position interpolation to handle long text processing, but don't provide detailed performance comparisons with other long-context methods.
- Why unresolved: While the authors discuss their approach to handling long text, they don't provide experimental results comparing this method to alternatives like direct extrapolation or other position embedding techniques.
- What evidence would resolve it: Detailed performance comparisons of QLoRA with position interpolation against other long-context methods (e.g., direct extrapolation, linear attention) on tasks requiring processing of long texts in knowledge graph construction scenarios.

## Limitations

- Models show only "promising initial performance" without detailed quantitative results, making it difficult to assess actual capability improvements
- Position interpolation method for handling long texts lacks validation on whether it effectively handles very long sequences
- 4-bit QLoRA quantization's impact on task-specific performance remains unverified with concrete metrics
- Dataset composition details are limited - while 4 million instances are mentioned, exact distribution across domains and quality metrics are not provided
- Training infrastructure (Huawei Ascend servers) may limit reproducibility for researchers without access to similar hardware

## Confidence

- High confidence: The general approach of using instruction fine-tuning for knowledge graph construction tasks is well-established and theoretically sound
- Medium confidence: The specific implementation details (position interpolation, QLoRA adaptation) are described but lack empirical validation
- Low confidence: Claims about handling long texts and maintaining general capabilities during fine-tuning are asserted but not demonstrated with quantitative results

## Next Checks

1. **Quantitative performance validation**: Run standardized NER and RTE benchmarks on the released models to verify the claimed "promising initial performance" with concrete F1 scores and accuracy metrics.

2. **Long text processing evaluation**: Test the position interpolation implementation on documents exceeding 8K tokens to validate whether the scaling approach maintains semantic coherence and task performance.

3. **General capability preservation**: Conduct ablation studies comparing the fine-tuned models against the base LLaMA2 model on general instruction following tasks to quantify any catastrophic forgetting effects.