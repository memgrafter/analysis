---
ver: rpa2
title: Clinical Reading Comprehension with Encoder-Decoder Models Enhanced by Direct
  Preference Optimization
arxiv_id: '2407.14000'
source_url: https://arxiv.org/abs/2407.14000
tags:
- preference
- training
- data
- examples
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles machine reading comprehension (MRC) over clinical
  text, specifically radiology reports, where the goal is to answer questions by extracting
  text spans from medical documents. The authors use encoder-decoder models (T5) and
  apply direct preference optimization (DPO), a method originally designed for aligning
  language models with human preferences, to enhance the MRC performance.
---

# Clinical Reading Comprehension with Encoder-Decoder Models Enhanced by Direct Preference Optimization

## Quick Facts
- arXiv ID: 2407.14000
- Source URL: https://arxiv.org/abs/2407.14000
- Reference count: 40
- Key outcome: State-of-the-art performance on RadQA dataset with 12-15 point F1 improvement over BERT-based models using T5 with DPO fine-tuning

## Executive Summary
This paper addresses machine reading comprehension (MRC) over clinical text, specifically radiology reports, by using encoder-decoder models (T5) enhanced with direct preference optimization (DPO). The authors tackle the challenge of clinical MRC where models must extract answer spans from medical documents to answer questions. Unlike previous approaches that used BERT-based models, this work leverages the generative capabilities of T5 and applies DPO to align the model with correct answer spans. The key innovation is the automatic generation of preference datasets without human annotation, using both model-based and rule-based approaches to create training pairs of correct versus incorrect answers. This approach achieves state-of-the-art results on the RadQA dataset, demonstrating significant improvements over prior work.

## Method Summary
The approach uses encoder-decoder models (specifically T5) for clinical MRC, treating the task as a sequence-to-sequence problem where the model generates answer spans. Direct Preference Optimization (DPO) is applied to fine-tune the model by optimizing for correct answer spans over incorrect ones. Since clinical preference data is scarce and expensive to annotate, the authors automatically generate preference datasets in two ways: a model-based approach that uses the model's own incorrect predictions as negative examples, and a rule-based approach that generates plausible negative answers through predefined rules. The model is first fine-tuned on the RadQA dataset using standard supervised learning, then further optimized using DPO with the automatically generated preference pairs. This two-stage training process allows the model to learn both from explicit question-answer pairs and from relative preferences between correct and incorrect answers.

## Key Results
- Achieved state-of-the-art performance on RadQA dataset with 12-15 point F1 score improvement over prior BERT-based models
- Encoder-decoder models (T5) significantly outperform previous BERT-based approaches on clinical MRC tasks
- DPO fine-tuning provides additional 1-3% performance gains beyond the baseline encoder-decoder improvements
- Automatic preference dataset generation proves effective for clinical MRC without requiring human annotation

## Why This Works (Mechanism)
The approach works by leveraging the generative strengths of encoder-decoder models for MRC tasks, allowing flexible answer generation rather than classification. DPO fine-tuning aligns the model with human-like preferences for correct answers by training it to distinguish between good and bad answer spans. The automatic preference generation methods create large-scale training data without human annotation costs, using either the model's own mistakes or rule-based negative examples to create preference pairs. This combination of powerful base architecture with preference-based fine-tuning addresses both the complexity of clinical language and the need for precise answer extraction.

## Foundational Learning
- **Encoder-decoder architecture**: Needed for sequence-to-sequence generation of answer spans; quick check: verify model can generate variable-length outputs
- **Direct Preference Optimization (DPO)**: Alignment technique that optimizes model to prefer certain outputs over others; quick check: ensure preference pairs are properly formatted for DPO
- **Clinical MRC task format**: Question-answering over medical documents requiring span extraction; quick check: confirm dataset follows extractive QA format
- **Automatic preference generation**: Methods to create training pairs without human annotation; quick check: validate generated negative examples are truly incorrect
- **T5 model variants**: Different sizes and configurations of T5 for experimentation; quick check: compare performance across T5 sizes
- **RadQA dataset characteristics**: Specific clinical MRC dataset used for evaluation; quick check: understand question types and answer formats in RadQA

## Architecture Onboarding
**Component Map**: Input text -> T5 encoder -> T5 decoder -> Answer span output -> DPO loss calculation -> Model update
**Critical Path**: The model processes the input clinical text and question through the encoder-decoder architecture to generate an answer span, which is then evaluated against the ground truth to calculate standard cross-entropy loss. For DPO fine-tuning, the generated answer is compared against preferred (correct) and dispreferred (incorrect) spans to calculate preference-based loss.
**Design Tradeoffs**: Encoder-decoder models offer more flexibility than extractive approaches but may generate answers not present in the source text; automatic preference generation saves annotation costs but may introduce noise; model-based preference generation is efficient but could reinforce model biases.
**Failure Signatures**: Incorrect answer generation outside source text boundaries; preference pairs that don't represent true preferences; degradation in performance when applying DPO to already well-performing models.
**First Experiments**: 1) Fine-tune T5 on RadQA using standard supervised learning to establish baseline; 2) Generate model-based preference pairs and apply DPO fine-tuning; 3) Generate rule-based preference pairs and compare DPO fine-tuning results.

## Open Questions the Paper Calls Out
None

## Limitations
- Automatic preference dataset generation without human validation may introduce noise or biases
- Results are limited to a single dataset (RadQA) and specific clinical reading comprehension format
- Performance gains from DPO (1-3%) are incremental compared to baseline improvements (12-15%) from architecture choice
- Use of proprietary GPT-4 for model-based preference generation limits reproducibility
- Lack of explicit error analysis makes it difficult to understand failure modes

## Confidence
- **High confidence**: Encoder-decoder models (T5) outperform prior BERT-based models on clinical MRC tasks
- **Medium confidence**: DPO fine-tuning provides additional performance gains beyond encoder-decoder baseline improvements
- **Medium confidence**: Automatic preference dataset generation is effective for clinical MRC tasks

## Next Checks
1. Conduct human evaluation of automatically generated preference pairs to assess quality and potential biases in the preference dataset
2. Replicate experiments on additional clinical reading comprehension datasets with varied question types to test generalizability
3. Perform ablation studies to isolate the contribution of encoder-decoder architecture versus DPO fine-tuning to the observed performance gains