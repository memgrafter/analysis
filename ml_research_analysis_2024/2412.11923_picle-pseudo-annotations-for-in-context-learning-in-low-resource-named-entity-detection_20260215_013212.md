---
ver: rpa2
title: 'PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named Entity
  Detection'
arxiv_id: '2412.11923'
source_url: https://arxiv.org/abs/2412.11923
tags:
- demonstrations
- entity
- in-context
- gold
- demonstration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the importance of demonstration quality
  for in-context learning (ICL) in named entity detection (NED). Through a systematic
  perturbation study, the authors find that ICL is surprisingly resilient to partially
  correct annotations, as long as the demonstrations maintain some semantic correspondence
  between text and labels.
---

# PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named Entity Detection

## Quick Facts
- arXiv ID: 2412.11923
- Source URL: https://arxiv.org/abs/2412.11923
- Authors: Sepideh Mamooler; Syrielle Montariol; Alexander Mathis; Antoine Bosselut
- Reference count: 40
- Key outcome: PICLe achieves 57.1% F1 score on average across five biomedical NED datasets, outperforming zero-shot prediction (46.4%) and matching or exceeding ICL with gold demonstrations when only 10-100 labeled examples are available.

## Executive Summary
This paper investigates how demonstration quality affects in-context learning (ICL) for named entity detection (NED). Through systematic perturbation studies, the authors find that ICL is surprisingly resilient to partially correct annotations as long as semantic correspondence between text and labels is maintained. Based on this finding, they propose PICLe (Pseudo-annotated In-Context Learning), a framework that generates noisy pseudo-annotations via zero-shot prediction and self-verification, clusters these demonstrations, and uses cluster-specific sets for inference with self-verification. Evaluated on five biomedical NED datasets with three language models, PICLe achieves strong performance, particularly in low-resource settings where only 10-100 labeled examples are available.

## Method Summary
PICLe addresses low-resource NED by using pseudo-annotated demonstrations for in-context learning. The method generates initial pseudo-annotations through zero-shot prompting on unlabeled samples, then applies self-verification to filter incorrect entity types. These samples are clustered using k-means based on text and annotation embeddings, with specialized k-means (Sp-k-means) maximizing inter-run diversity by sampling demonstrations from the same cluster for each inference round. During inference, multiple ICL passes are performed using cluster-specific demonstration sets, followed by final self-verification to consolidate predictions. The framework is evaluated on five biomedical NED datasets using micro-averaged precision, recall, and F1 scores.

## Key Results
- PICLe achieves 57.1% F1 score on average across five biomedical NED datasets
- Outperforms zero-shot prediction (46.4% F1) and matches or exceeds ICL with gold demonstrations when only 10-100 labeled examples are available
- Self-verification improves precision by +20% during inference while maintaining reasonable recall
- Specialized k-means (Sp-k-means) clustering provides better inter-run diversity than random sampling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICL demonstrations with partially correct annotations can be as effective as fully correct demonstrations for named entity detection
- Mechanism: The model learns semantic correspondence between text and labels rather than exact matching. As long as demonstrations maintain some semantic mapping between input text and entity mentions, even partially correct labels provide sufficient signal for task transfer. The model appears robust to noise as long as a sufficient number of correct label mappings remain in the demonstration set.
- Core assumption: The model's in-context learning capability relies on capturing semantic relationships rather than memorizing exact label spans, and this semantic learning is robust to partial noise in demonstrations.
- Evidence anchors:
  - [abstract] "Our surprising finding is that in-context demonstrations with partially correct annotated entity mentions can be as effective for task transfer as fully correct demonstrations"
  - [section 4.2] "Our results show that for a given perturbation factor, changes that preserve or increase the total number of entities in the demonstration (such as Substitution or Addition and Substitution) cause a less pronounced performance drop"
- Break condition: If semantic correspondence between text and labels is completely broken (as in Random OOD Label where entities are replaced with random words), performance drops below zero-shot level.

### Mechanism 2
- Claim: Clustering pseudo-annotated demonstrations by semantic similarity improves downstream prediction diversity and coverage
- Mechanism: By clustering pseudo-annotated samples based on text and annotation embeddings, PICLe creates demonstration sets that capture different aspects of the entity distribution. Each cluster provides demonstrations relevant to a specific semantic region of the input space. This specialized clustering (Sp-k-means) maximizes inter-run diversity by ensuring each inference run uses demonstrations from the same cluster, leading to more varied predictions that cover different entity types and contexts.
- Core assumption: Entity mentions and their contexts have meaningful semantic structure that can be captured through clustering, and this structure correlates with the types of entities that will appear in test queries.
- Evidence anchors:
  - [section 5] "We use k-means clustering to group the pseudo-annotated samples into K clusters based off the embedding of their text and pseudo-annotations"
  - [table 3] "For k-means, we randomly sample one demonstration per cluster, increasing the intra-run diversity. Conversely, in Sp-k-means, demonstrations in each round are all sampled from the same cluster, maximizing inter-run diversity"
- Break condition: If the clustering fails to capture meaningful semantic structure, the specialized demonstration sets won't provide complementary coverage.

### Mechanism 3
- Claim: Self-verification of both pseudo-annotations and predictions improves precision by filtering out incorrectly typed entities
- Mechanism: After generating pseudo-annotations or predictions, the model is prompted to verify whether each extracted entity is of the correct type. This two-stage verification acts as a quality filter that significantly improves precision by catching entities that don't match the expected type, even if they were correctly extracted from the text.
- Core assumption: The model has sufficient capability to distinguish between correct and incorrect entity types when explicitly asked, and this verification step doesn't overly reduce recall.
- Evidence anchors:
  - [section 5] "we improve the quality of these pseudo-annotations by prompting the model to verify each predicted entity (i.e., self-verification; Weng et al., 2023), and filter entities that are not of the correct entity type"
  - [table 3] "Self-verification improves performance during inference (rows #7 vs #8), especially precision (+20%)"
- Break condition: If the model's verification capability is poor, self-verification could either hurt performance by filtering correct entities or provide false confidence in incorrect ones.

## Foundational Learning

- Concept: Named Entity Detection (NED) task formulation
  - Why needed here: Understanding the task structure is crucial for implementing PICLe correctly. NED requires identifying all entity mentions of specific types in text, producing structured output rather than scalar predictions.
  - Quick check question: How does NED differ from named entity recognition (NER) in terms of output format and evaluation criteria?

- Concept: In-context learning (ICL) principles
  - Why needed here: PICLe builds on ICL by using pseudo-annotated demonstrations. Understanding how ICL works - that models learn from demonstration-label pairs without gradient updates - is essential for grasping why demonstration quality matters.
  - Quick check question: What is the key difference between ICL and traditional few-shot learning in terms of how the model adapts to new tasks?

- Concept: Zero-shot prompting and self-verification
  - Why needed here: PICLe uses zero-shot prompting to generate initial pseudo-annotations, then applies self-verification to improve quality. Understanding both techniques is necessary for implementing the pipeline correctly.
  - Quick check question: How does self-verification differ from standard prompting, and why might it improve the quality of generated annotations?

## Architecture Onboarding

- Component map: Unlabeled samples → Zero-shot NED → Self-verification → Clustering → Sp-k-means demonstration selection → ICL inference → Final self-verification → Output predictions

- Critical path: Unlabeled samples → Zero-shot NED → Self-verification → Clustering → Sp-k-means demonstration selection → ICL inference → Final self-verification → Output predictions

- Design tradeoffs:
  - Clustering vs random sampling: Clustering provides specialized demonstrations but requires embedding computation; random sampling is simpler but may provide less relevant examples
  - Single vs multiple verification passes: Multiple verifications improve precision but increase computational cost
  - Demonstration pool size: Larger pools provide more diversity but increase computation; smaller pools are faster but may miss entity types

- Failure signatures:
  - Low precision: Self-verification may be too aggressive, filtering correct entities
  - Low recall: Clustering may not capture all relevant entity types, or demonstration pools may be too small
  - Performance below zero-shot: Semantic correspondence in demonstrations may be broken, or clustering may be creating poor demonstration sets

- First 3 experiments:
  1. Test zero-shot NED with self-verification on a small sample to verify the basic annotation quality (expect F1 > 40% on biomedical data)
  2. Verify clustering by checking if samples with similar entity types group together (visualize embeddings or check cluster purity)
  3. Test single ICL inference with one cluster's demonstrations to verify the demonstration selection mechanism works (expect improvement over zero-shot)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which semantic correspondence between input text and entity labels enables effective in-context learning for NED?
- Basis in paper: [inferred] The paper shows that maintaining semantic correspondence is crucial for ICL effectiveness in NED, but doesn't explain the underlying mechanism.
- Why unresolved: The paper demonstrates empirically that semantic correspondence matters but doesn't investigate why this relationship exists or how it affects the model's internal representations during in-context learning.
- What evidence would resolve it: Detailed analysis of attention patterns and activation values in LLMs during ICL with varying degrees of semantic correspondence would help explain the mechanism.

### Open Question 2
- Question: How does the optimal number of entities in demonstration labels scale with model size and task complexity?
- Basis in paper: [inferred] The paper shows that demonstrations with more entities perform better but doesn't establish a clear relationship between entity count, model size, and task complexity.
- Why unresolved: While the paper identifies that entity count matters, it doesn't provide guidance on how to determine the optimal number of entities for different model scales or task complexities.
- What evidence would resolve it: Systematic experiments varying entity counts across different model sizes and task complexities, measuring performance curves to identify optimal ranges.

### Open Question 3
- Question: Can the PICLe framework be effectively extended to multi-task scenarios where multiple entity types need to be detected simultaneously?
- Basis in paper: [explicit] The paper focuses on single entity type detection but mentions potential generalization to other tasks.
- Why unresolved: The current PICLe framework is designed for single entity type detection and doesn't address the challenges of handling multiple entity types simultaneously, including potential interference between different entity type clusters.
- What evidence would resolve it: Experiments applying PICLe to datasets with multiple entity types, measuring performance compared to single-type detection and exploring modifications needed for multi-task scenarios.

## Limitations

- The perturbation study only examines four specific types of annotation errors, leaving open questions about other forms of noise and their impact on ICL performance
- The specialized k-means clustering approach (Sp-k-means) is presented as superior but lacks ablation studies comparing it against simpler alternatives
- The self-verification mechanism's effectiveness depends heavily on prompt quality, which isn't fully specified in the paper

## Confidence

- Mechanism 1 (semantic correspondence): **High** - Strongly supported by perturbation results showing performance degradation only when semantic mapping is broken
- Mechanism 2 (clustering improvement): **Medium** - Supported by results but lacks detailed ablation studies on clustering parameters
- Mechanism 3 (self-verification): **Medium** - Results show improvement but depend on unspecified prompt details

## Next Checks

1. Test PICLe on non-biomedical datasets (e.g., CoNLL or OntoNotes) to verify the perturbation study findings generalize beyond biomedical text
2. Perform ablation studies comparing Sp-k-means against random sampling and standard k-means with different k values to quantify the clustering contribution
3. Evaluate the impact of different prompt templates for self-verification by testing multiple formulations and measuring their effect on precision-recall tradeoff