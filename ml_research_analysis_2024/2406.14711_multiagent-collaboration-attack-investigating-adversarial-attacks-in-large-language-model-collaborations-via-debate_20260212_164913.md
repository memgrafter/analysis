---
ver: rpa2
title: 'MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large
  Language Model Collaborations via Debate'
arxiv_id: '2406.14711'
source_url: https://arxiv.org/abs/2406.14711
tags:
- agents
- adversary
- answer
- other
- debate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the vulnerabilities of language model collaborations
  via debate, focusing on adversarial attacks. The core method involves simulating
  an adversarial agent that attempts to persuade other models to select incorrect
  answers during a debate.
---

# MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate

## Quick Facts
- **arXiv ID**: 2406.14711
- **Source URL**: https://arxiv.org/abs/2406.14711
- **Reference count**: 40
- **Primary result**: Adversarial agents can reduce multi-agent debate system accuracy by 10-40% through persuasive arguments

## Executive Summary
This paper investigates adversarial attacks in multi-agent LLM collaborations via debate, focusing on how a single adversarial agent can systematically undermine system accuracy. The study introduces a framework where an adversary injects incorrect answers and uses persuasive arguments to convince other models to adopt them. Through experiments with four different LLM models and four datasets, the research demonstrates that adversaries can effectively reduce system accuracy by 10-40% and increase their agreement with other models by 17-43%. The work identifies model persuasiveness as a key attack factor and shows that common defenses like increasing debate rounds or agents provide limited protection.

## Method Summary
The study simulates multi-agent debate collaborations where one agent is adversarial, injecting incorrect answers and attempting to persuade others through argumentation. The framework uses three debate rounds with three agents (one adversary) and evaluates system performance across four datasets (MMLU, TruthfulQA, MedMCQA, Scalr) with 100 random samples each, tested five times for standard deviation. The research employs GPT-3.5, GPT-4o, Llama 3, Qwen 1.5, and Yi 1.5 models. Attack effectiveness is measured through system accuracy decrease (ΔAcc) and adversary agreement change (ΔAgr), with experiments testing both basic and optimized attacks using Best-of-N argument selection with preference model ranking.

## Key Results
- Adversarial agents can reduce system accuracy by 10-40% in multi-agent debate settings
- Adversary agreement with other models increases by 17-43% through persuasive arguments
- Model persuasiveness is identified as a key factor in attack success
- Increasing debate rounds or agents provides limited defense against adversarial influence
- Inference-time argument optimization and prompt-based mitigation show some effectiveness but are insufficient

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial agents can systematically reduce system accuracy by 10-40% in multi-agent debate settings.
- Mechanism: The adversary introduces a single incorrect answer and uses persuasive arguments to convince other models to adopt it. As models iterate their answers based on peer input, they increasingly converge on the incorrect answer, lowering majority vote accuracy.
- Core assumption: Models in debate are susceptible to persuasion, and debate rounds allow the adversary's incorrect answer to spread through argumentation.
- Evidence anchors:
  - [abstract] "adversaries can effectively undermine system accuracy by 10-40% and increase their agreement with other models by 17-43%."
  - [section] "the adversary is able to undermine the common objective with system accuracy decreases ranging from 10% to almost 40%"
- Break condition: If models are immune to persuasion or if debate rounds do not allow for opinion changes, the adversary's influence would be limited.

### Mechanism 2
- Claim: Model persuasiveness is a key determinant of attack success in collaborative debate.
- Mechanism: More persuasive models can generate arguments that other models find convincing, leading to higher agreement with the adversary and greater accuracy degradation. Persuasion strength varies across models.
- Core assumption: Different models have varying abilities to generate persuasive arguments, and this directly affects their success as adversaries.
- Evidence anchors:
  - [abstract] "Model persuasiveness is identified as a key factor in attack success"
  - [section] "We show how to evaluate it based on accuracy and agreement. And we highlight its relevance in Language Models due to its effect on collaboration."
- Break condition: If all models have equal persuasion ability or if persuasion is irrelevant to the debate outcome, model differences would not affect attack success.

### Mechanism 3
- Claim: Increasing the number of debate rounds or agents provides limited defense against adversarial attacks.
- Mechanism: Even with more rounds or agents, the adversary can still effectively persuade others to adopt the incorrect answer. More rounds don't allow honest models to "recover" from the adversary's influence.
- Core assumption: Additional debate rounds or agents do not inherently provide more opportunities for correct answers to prevail over persuasive incorrect ones.
- Evidence anchors:
  - [abstract] "increasing debate rounds or agents provides limited defense"
  - [section] "As the number of rounds increases, we do not observe the group models recover from the adversary's influence"
- Break condition: If additional rounds or agents provide more correct information or if models become more resistant to persuasion over time, the defense might improve.

## Foundational Learning

- Concept: Debate protocol mechanics
  - Why needed here: Understanding how models interact through debate rounds is crucial for grasping how adversarial influence spreads.
  - Quick check question: In the debate protocol, what happens in each round and how do models update their answers?

- Concept: Persuasion in language models
  - Why needed here: Persuasion ability is central to understanding why some models are more effective adversaries than others.
  - Quick check question: How does the paper measure and evaluate a model's persuasiveness?

- Concept: Majority vote ensemble methods
  - Why needed here: The system accuracy is measured via majority vote, so understanding this voting mechanism is essential for interpreting results.
  - Quick check question: How does the majority vote system determine the final answer in a multi-agent debate?

## Architecture Onboarding

- Component map:
  Question input → Initial model responses → Iterative debate rounds → Final majority vote
  Adversary component: Injects incorrect answer and generates persuasive arguments
  Argument optimization: Best-of-N selection using preference model
  Metrics collection: System accuracy and adversary agreement tracking

- Critical path:
  Generate initial answers → Debate rounds (adversary persuasion) → Majority vote → Accuracy calculation

- Design tradeoffs:
  More debate rounds vs. computational cost
  Model size and capability vs. persuasion effectiveness
  Argument optimization complexity vs. attack success rate

- Failure signatures:
  System accuracy remains stable despite adversary presence
  Adversary agreement decreases over debate rounds
  Models recover correct answers in later rounds

- First 3 experiments:
  1. Baseline: Run debate with no adversary to establish control accuracy
  2. Simple attack: Introduce adversary with basic arguments, measure accuracy drop
  3. Optimized attack: Apply Best-of-N argument optimization, compare success rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the persuasive power of LLMs vary across different domains (e.g., legal, medical, general knowledge) and what specific factors contribute to these differences?
- Basis in paper: Inferred from the observation that model persuasiveness is crucial in multi-agent collaborations and the study's use of diverse datasets (MMLU, TruthfulQA, MedMCQA, Scalr) representing different domains.
- Why unresolved: The paper does not provide a detailed analysis of how LLM persuasiveness varies across these domains or identify the specific factors that contribute to these differences. Understanding these variations could inform the development of more robust collaboration mechanisms tailored to specific domains.
- What evidence would resolve it: Conducting a comparative analysis of LLM persuasiveness across different domains using a standardized metric, and identifying the key factors (e.g., task complexity, domain expertise, argument structure) that influence persuasiveness in each domain.

### Open Question 2
- Question: What are the long-term effects of adversarial attacks on LLM collaboration, and how do these effects evolve over time with repeated interactions?
- Basis in paper: Inferred from the study's focus on short-term adversarial impacts and the lack of analysis on the persistence of these effects in ongoing collaborations.
- Why unresolved: The paper primarily examines the immediate impact of adversarial attacks on collaboration accuracy and agreement. It does not explore how these effects might accumulate or change over multiple interactions, which is crucial for understanding the sustainability of LLM collaborations in adversarial environments.
- What evidence would resolve it: Implementing a longitudinal study that tracks the performance of LLM collaborations over multiple rounds of interaction, both with and without adversarial influence, to observe how accuracy, agreement, and persuasive power evolve over time.

### Open Question 3
- Question: How effective are advanced mitigation strategies, such as dynamic argument filtering or adaptive debate protocols, in defending against adversarial attacks in LLM collaborations?
- Basis in paper: Inferred from the paper's discussion on the limitations of simple prompt-based mitigation and the need for more sophisticated defensive strategies.
- Why unresolved: The paper introduces basic prompt-based mitigation but acknowledges its insufficiency. It does not explore more advanced strategies that could dynamically adapt to adversarial tactics, such as filtering persuasive arguments or modifying debate protocols in real-time.
- What evidence would resolve it: Developing and testing advanced mitigation techniques, such as machine learning models that predict and filter persuasive arguments or adaptive debate protocols that adjust based on the detected presence of adversarial agents, and evaluating their effectiveness in maintaining collaboration accuracy and agreement.

## Limitations
- Experiments use only four datasets with 100 random samples each, potentially limiting generalizability
- Focus on English-language datasets and limited model combinations constrains broader applicability
- Absolute baseline accuracy across different models and datasets is not consistently reported
- Effectiveness of proposed mitigation strategies is not validated against adaptive adversaries
- Computational overhead and scalability of defense mechanisms are not explored

## Confidence

**High Confidence**: The core finding that adversarial agents can systematically reduce system accuracy by 10-40% through persuasion in debate settings is well-supported by experimental results across multiple models and datasets.

**Medium Confidence**: The claim that increasing debate rounds or agents provides limited defense is supported by the data but may depend on specific parameter choices. The effectiveness of Best-of-N argument optimization is demonstrated but relies on the quality of the preference model.

**Low Confidence**: The practical significance of the observed accuracy degradation depends on the baseline performance, which varies across datasets and is not consistently reported. The generalizability of results to real-world deployment scenarios remains uncertain.

## Next Checks

1. **Baseline Performance Analysis**: Calculate and report the absolute accuracy of each model-dataset combination in non-adversarial settings to better contextualize the significance of the observed accuracy drops.

2. **Adaptive Adversary Testing**: Evaluate the proposed mitigation strategies against adversaries who are aware of and can adapt to