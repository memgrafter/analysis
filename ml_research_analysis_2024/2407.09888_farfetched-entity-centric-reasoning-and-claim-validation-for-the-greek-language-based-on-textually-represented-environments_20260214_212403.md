---
ver: rpa2
title: 'FarFetched: Entity-centric Reasoning and Claim Validation for the Greek Language
  based on Textually Represented Environments'
arxiv_id: '2407.09888'
source_url: https://arxiv.org/abs/2407.09888
tags:
- evidence
- claim
- language
- greek
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FarFetched introduces an entity-centric reasoning framework for
  automated claim validation using aggregated evidence from news sources. It employs
  entity linking and semantic similarity to collect and combine information from diverse
  sources, then leverages textual entailment recognition to quantitatively assess
  claim credibility.
---

# FarFetched: Entity-centric Reasoning and Claim Validation for the Greek Language based on Textually Represented Environments

## Quick Facts
- arXiv ID: 2407.09888
- Source URL: https://arxiv.org/abs/2407.09888
- Authors: Dimitris Papadopoulos; Katerina Metropoulou; Nikolaos Matsatsinis; Nikolaos Papadakis
- Reference count: 21
- Primary result: Greek FEVER subset evaluation achieves 73% accuracy, significantly outperforming baseline

## Executive Summary
FarFetched introduces an entity-centric reasoning framework for automated claim validation using aggregated evidence from news sources. The system identifies entities in user claims, retrieves related article sections from a graph database, and leverages semantic similarity and textual entailment recognition to assess claim credibility. Demonstrated for the Greek language, FarFetched achieves strong performance on claim validation tasks while introducing Greek-specific STS and NLI models trained on translated benchmarks.

## Method Summary
FarFetched implements a multi-stage pipeline for claim validation: (1) Crawls Greek news articles and stores them in a Neo4j graph database with Article and Section nodes connected by HAS_SECTION relationships; (2) Uses JSI Wikifier API to identify and disambiguate entities, creating Entity nodes linked to sections via HAS_ENTITY and building shortest paths between entities; (3) Trains Greek sentence embeddings models using XLM-RoBERTa-base on parallel English-Greek data for semantic textual similarity; (4) Fine-tunes XLM-RoBERTa-base on Greek-English NLI data for textual entailment recognition; (5) For each claim, retrieves evidence via entity paths, ranks by semantic similarity, and validates using NLI classification into entailment, contradiction, or neutral categories.

## Key Results
- Greek FEVER subset evaluation achieves 73% accuracy, significantly outperforming baseline
- STS model attains 83.30 Pearson correlation on STS2017 benchmark
- NLI model achieves 78.3 F1-score on XNLI dataset
- System successfully demonstrates entity-centric reasoning for Greek claim validation

## Why This Works (Mechanism)

### Mechanism 1
- Claims can be validated by aggregating evidence from multiple news sources using entity-centric reasoning
- System identifies entities in user claims, retrieves related article sections via shortest paths between entities, and uses semantic similarity and NLI to validate claims
- Core assumption: Entity mentions act as connectors between disparate articles, revealing latent connections
- Break condition: Entity linking failures produce irrelevant entities, leading to incorrect evidence retrieval

### Mechanism 2
- Semantic similarity models can accurately rank candidate evidence sequences by relevance to user claims
- System trains bilingual sentence embeddings model on parallel corpora and compares claim vectors to evidence sequence vectors using cosine similarity
- Core assumption: Sentences with similar semantic content have similar vector representations
- Break condition: Poorly calibrated STS model incorrectly ranks evidence sequences

### Mechanism 3
- NLI models can determine whether user claims are entailed by, contradicted, or neutral to relevant evidence
- System fine-tunes cross-encoder XLM-RoBERTa-base model on Greek-English NLI data and predicts one of three labels with probabilities
- Core assumption: Semantic relationship between premise and hypothesis can be accurately classified
- Break condition: Poorly calibrated NLI model misclassifies semantic relationships

## Foundational Learning

- **Entity Linking (EL)**: Why needed - To identify and disambiguate named entities in claims and article sections, enabling connection of related information across sources. Quick check - How does the system handle highly ambiguous entities or name variations during entity linking?

- **Semantic Textual Similarity (STS)**: Why needed - To accurately rank candidate evidence sequences by semantic relevance to user claims. Quick check - What factors influence STS model performance and how can they be optimized for the target language?

- **Natural Language Inference (NLI)**: Why needed - To determine semantic relationship between user claims and most relevant evidence. Quick check - How does the NLI model handle claims with high presence of semantic properties like tense and aspect?

## Architecture Onboarding

- **Component map**: News Collection -> Graph Database Population -> Entity Linking -> Evidence Constructor -> Semantic Textual Similarity -> Natural Language Inference
- **Critical path**: 1) User provides claim; 2) Entity linking identifies relevant entities; 3) Evidence constructor retrieves related article sections; 4) STS model ranks evidence sequences; 5) NLI model determines semantic relationship; 6) System outputs validation result
- **Design tradeoffs**: Precision vs. recall in entity linking; shortest path vs. top-n evidence sequences; monolingual vs. multilingual models
- **Failure signatures**: Incorrect entity linking produces unrelated evidence; poorly ranked evidence; wrong NLI classification
- **First 3 experiments**: 1) Test entity linking on small claim set and verify identified entities; 2) Evaluate STS model on validation set with known similarity scores; 3) Assess NLI model accuracy on test set with annotated semantic relationships

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding real-time updates to news articles and their impact on claim validation accuracy, the precision-recall tradeoff in entity linking and its effect on downstream performance, and the system's ability to distinguish between factual evidence and opinionated statements when they appear in the same article.

## Limitations
- Methodology relies heavily on news data from only two Greek news sources, potentially introducing bias
- Entity linking component uses external API without clear handling of ambiguous entities
- Training data specifics for STS and NLI models remain vague, making reproducibility difficult
- Evaluation on Greek FEVER limited to subset with only 13 claims, raising statistical significance concerns
- Approach does not address multimodal claims or evidence, nor claims requiring temporal reasoning

## Confidence

- **High confidence**: Overall architecture and methodology description is clear and technically sound; use of entity linking, STS, and NLI in sequence is well-justified
- **Medium confidence**: Reported performance numbers are reasonable but may not be fully reproducible due to unspecified data sources and training details
- **Low confidence**: Scalability to broader claim types, handling of entity ambiguity, and model performance on complex reasoning claims are unclear

## Next Checks
1. **Entity Linking Robustness Test**: Evaluate JSI Wikifier performance on 100 ambiguous entity mentions in Greek news text, measuring precision, recall, and false positive rates across different pagerank thresholds
2. **Cross-Domain Generalization**: Test trained STS and NLI models on Greek claims from domains not represented in news corpus (scientific claims, social media) to assess transferability
3. **Statistical Significance Analysis**: Perform bootstrap resampling on Greek FEVER evaluation results to determine confidence intervals for 73% accuracy claim and assess statistical significance given small test set size