---
ver: rpa2
title: 'The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human
  Benchmarks'
arxiv_id: '2410.07391'
source_url: https://arxiv.org/abs/2410.07391
tags:
- capabilities
- performance
- gemini
- human
- cognitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks large language models and vision language
  models against human performance on the Wechsler Adult Intelligence Scale (WAIS-IV),
  a comprehensive assessment of human cognition. The study finds that most models
  demonstrate exceptional capabilities in working memory and verbal comprehension,
  performing at or above the 98th percentile compared to human norms.
---

# The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks

## Quick Facts
- arXiv ID: 2410.07391
- Source URL: https://arxiv.org/abs/2410.07391
- Reference count: 12
- Models demonstrate exceptional working memory and verbal comprehension but poor perceptual reasoning performance

## Executive Summary
This study benchmarks large language models and vision language models against human performance on the Wechsler Adult Intelligence Scale (WAIS-IV), a comprehensive assessment of human cognition. The research finds that most models demonstrate exceptional capabilities in working memory and verbal comprehension, performing at or above the 98th percentile compared to human norms. However, models consistently perform poorly on perceptual reasoning tasks, with multimodal models scoring in the extremely low range (0.1-10th percentile). Smaller and older model versions consistently perform worse, indicating that training data, parameter count, and advances in tuning are resulting in significant advances in cognitive ability.

## Method Summary
The study converted standardized cognitive test stimuli from the WAIS-IV into text prompts for LLMs and VLMs. Test items were translated into textual prompts that replicate the original instructions and examples, then model outputs were scored using human-established criteria. Individual test scores were collated into raw index scores and then converted to age-normed scores with accompanying performance percentiles using the same standardization approach as human test-takers. A representative set of state-of-the-art large language models was selected, encompassing a range of model sizes, architectures, and training datasets.

## Key Results
- Models achieved 98th+ percentile performance on working memory and verbal comprehension tasks
- All models performed extremely poorly on perceptual reasoning tasks (0.1-10th percentile)
- Newer and larger model versions consistently outperformed older and smaller versions across all cognitive domains

## Why This Works (Mechanism)

### Mechanism 1
- Converting standardized cognitive test stimuli into text prompts allows LLMs and VLMs to complete human intelligence assessments
- Test items are translated into textual prompts that replicate the original instructions and examples, then model outputs are scored using human-established criteria
- Core assumption: The conversion preserves the essential cognitive demands of the original test
- Break condition: If prompt conversion omits critical contextual cues required for valid reasoning, scores will no longer map to human norms

### Mechanism 2
- Performance percentiles from AI models can be meaningfully compared to human normative data
- Raw AI scores are converted to standard scores and percentiles using the same age-based norms as human test-takers
- Core assumption: AI responses can be reliably scored with the same rubrics used for humans
- Break condition: If AI responses fall outside the range of human response patterns, percentile mapping becomes invalid

### Mechanism 3
- Model size, version, and architecture influence cognitive performance on standardized tests
- Comparative testing of different models shows systematic performance differences aligned with known architectural changes
- Core assumption: Larger parameter counts and newer training data yield measurable cognitive gains
- Break condition: If newer models do not show improvement, the assumed relationship between architecture and cognition breaks down

## Foundational Learning

- Population norms and standardization: Understanding how WAIS-IV scores are normalized allows correct interpretation of AI performance percentiles. Quick check: What does a WAIS-IV scaled score of 15 represent in terms of percentile rank?

- Cognitive domain differentiation: The study distinguishes verbal comprehension, working memory, and perceptual reasoning; knowing these helps interpret which AI strengths and weaknesses are observed. Quick check: Which WAIS-IV index primarily measures visual-spatial reasoning?

- Prompt engineering and test fidelity: Translating test items into prompts must preserve the cognitive demand; otherwise, scores won't reflect true capability. Quick check: What key element must be retained when converting a visual puzzle into a text prompt?

## Architecture Onboarding

- Component map: Model inference engine → Prompt formatter (test item → text) → Scoring engine (rubric application) → Percentile mapper (raw → normed score)
- Critical path: Prompt generation → Model response → Scoring → Norm conversion → Result aggregation
- Design tradeoffs: Full prompt fidelity improves validity but may require longer context windows; truncation risks losing test intent
- Failure signatures: Scores falling outside plausible human ranges, systematic errors on specific subtest types, inconsistent performance across similar prompts
- First 3 experiments:
  1. Run a single-digit-span prompt through multiple model versions to confirm scaling effects
  2. Compare human and AI scoring on a small set of hand-graded responses for rubric alignment
  3. Test a perceptual reasoning item on both text-only and multimodal models to confirm modality dependency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications would enable generative models to achieve human-level performance on visual reasoning tasks?
- Basis in paper: The paper demonstrates consistent poor performance on perceptual reasoning tasks (PRI) across all multimodal models, with scores in the extremely low range (0.1-10th percentile), while noting that Claude 3.5 Sonnet showed significant improvements over Claude 3 Opus.
- Why unresolved: The paper identifies the performance gap but does not investigate specific architectural changes that could address it. The authors speculate that visual processing may require specialized architecture with enhanced interaction capabilities, similar to vertebrates.
- What evidence would resolve it: Comparative studies testing different architectural approaches (separate visual and language streams with enhanced interaction mechanisms) against baseline models on PRI subtests, demonstrating measurable improvements in visual reasoning capabilities.

### Open Question 2
- Question: How do the training data characteristics and parameter counts specifically influence the observed discrepancies between verbal comprehension and perceptual reasoning performance?
- Basis in paper: The paper notes that smaller and older model versions consistently performed worse, indicating training data, parameter count, and tuning advances affect cognitive ability, but these factors are proprietary and not publicly available for analysis.
- Why unresolved: The paper acknowledges these factors influence performance but cannot access the specific training data composition, parameter counts, or tuning approaches to establish causal relationships between these variables and the cognitive performance gaps.
- What evidence would resolve it: Detailed analysis of models with varying training data compositions, parameter counts, and tuning approaches on all WAIS-IV subtests, with statistical modeling to identify which factors most strongly predict performance differences between VCI, WMI, and PRI domains.

### Open Question 3
- Question: Can generative models be trained to overcome the observed relative weakness in mathematical reasoning compared to their exceptional capabilities in information storage and retrieval?
- Basis in paper: The paper consistently found a relative weakness in Arithmetic subtests compared to Digit Span across all models, indicating models struggle with mathematical reasoning despite excelling at storing and manipulating arbitrary information.
- Why unresolved: While the paper identifies this consistent pattern, it does not explore whether targeted training approaches could address this specific weakness or whether it represents a fundamental limitation of current generative architectures.
- What evidence would resolve it: Experimental studies training models with specialized mathematical reasoning curricula, then comparing performance on Arithmetic subtests before and after training, while controlling for general improvement from continued pretraining on diverse data.

## Limitations
- The assumption that textual prompt conversion preserves the cognitive demands of visual and spatial test items remains untested
- Scoring methodology relies on human consensus between two clinical psychologists, but exact criteria for handling ambiguous AI responses are unspecified
- The study lacks within-human variance estimates to determine if model differences exceed normal human variation

## Confidence
- High confidence: The observed performance differences between model versions (older/smaller vs newer/larger) are internally consistent with known architectural improvements in LLMs
- Medium confidence: The specific percentile rankings relative to human norms, given the untested conversion of visual reasoning tasks to text prompts
- Low confidence: The interpretation that model performance represents genuine "cognitive capability" rather than test-specific prompt-following ability

## Next Checks
1. Administer the same WAIS-IV items to human participants under both standard and text-prompt conditions to measure the impact of format conversion on scores
2. Have three independent raters score the same set of model responses using the WAIS-IV rubric, calculating inter-rater reliability to establish scoring consistency
3. Test the exact prompt set on a small sample of humans (n=20) to establish whether the percentile mapping methodology produces plausible results