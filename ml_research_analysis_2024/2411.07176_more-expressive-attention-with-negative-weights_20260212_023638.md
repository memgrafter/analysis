---
ver: rpa2
title: More Expressive Attention with Negative Weights
arxiv_id: '2411.07176'
source_url: https://arxiv.org/abs/2411.07176
tags:
- attention
- weights
- negative
- softmax
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cog Attention introduces negative attention weights to enhance
  model expressiveness and robustness against representational collapse. By enabling
  dynamic query-key inner products to handle operations like deletion or copying,
  it allows more efficient processing within a single head while reducing redundant
  information pathways.
---

# More Expressive Attention with Negative Weights

## Quick Facts
- arXiv ID: 2411.07176
- Source URL: https://arxiv.org/abs/2411.07176
- Reference count: 38
- Primary result: Cog Attention enables negative attention weights, improving expressiveness and robustness without additional parameters

## Executive Summary
Cog Attention introduces negative attention weights to enhance transformer expressiveness and robustness against representational collapse. By allowing dynamic query-key inner products to encode operations like deletion and copying within a single attention head, it achieves more efficient processing while reducing redundant information pathways. The method demonstrates consistent performance improvements across language modeling tasks (140M-1B parameters) and image generation, outperforming vanilla Transformers and variants without adding parameters or hyperparameters.

## Method Summary
Cog Attention replaces traditional softmax attention with a sign-based normalization mechanism that preserves negative weights. The method uses SignExp normalization where attention weights are computed from the sign of query-key inner products, followed by an exponential function with absolute maximum normalization. The approach maintains softmax in first and last layers for stable convergence while applying Cog Attention to middle layers. The method achieves enhanced expressiveness by enabling concurrent deletion and copying operations within single heads, reducing representational collapse by limiting information flow from earlier to later tokens, and producing more diverse attention patterns with reduced "attention sink" effects.

## Key Results
- Language modeling: Cogformer models achieve 44.35% average accuracy across NLP tasks vs 42.98% for baseline Transformers
- Image generation: U-ViC models with Cog Attention show improved FID scores compared to U-ViT baseline
- Efficiency: Cog Attention delivers performance gains without additional parameters or hyperparameters

## Why This Works (Mechanism)

### Mechanism 1
Cog Attention enables concurrent deletion and copying operations within a single attention head by leveraging the sign of query-key inner products. Traditional softmax requires separate heads or post-processing for different operations, while Cog Attention allows one head to assign negative weights to tokens that should be deleted while preserving others. The core assumption is that sign encoding can reliably represent semantic operations, though this requires the model to learn meaningful sign assignments.

### Mechanism 2
Negative attention weights reduce representational collapse by decreasing effective information pathways from earlier tokens to later positions. By assigning negative weights, Cog Attention limits the "over-squashing" where earlier tokens dominate later representations, maintaining diversity across positions. The theoretical justification assumes representational collapse primarily stems from excessive early-to-late information flow, with formal analysis showing negative weights reduce upper bounds on collapse.

### Mechanism 3
Cog Attention produces more diverse attention patterns with reduced "attention sink" compared to softmax. Softmax tends to concentrate weights on few tokens, creating inactive heads, while Cog Attention's flexibility allows more heads to participate meaningfully. This leads to sparser, more diverse patterns that could enhance extrapolation capabilities and KV cache compression, though the practical impact requires further validation.

## Foundational Learning

- Concept: Query-key inner product dynamics and their semantic interpretation
  - Why needed here: Understanding how sign of QK inner products encodes operations like deletion/copying is crucial for Cog Attention's expressiveness advantage
  - Quick check question: Why does the sign of query-key inner products matter for Cog Attention's ability to perform multiple operations in a single head?

- Concept: Representational collapse and information pathway analysis
  - Why needed here: The theoretical justification for Cog Attention's robustness relies on understanding information flows through transformer layers
  - Quick check question: How does reducing information pathways from earlier to later tokens help prevent representational collapse?

- Concept: Attention pattern analysis and "attention sink" phenomena
  - Why needed here: Evaluating Cog Attention's benefits requires understanding how attention patterns differ from softmax and their impact
  - Quick check question: What distinguishes an "attention sink" pattern from a diverse attention pattern, and why does this matter?

## Architecture Onboarding

- Component map:
  Query/Key/Value projection layers -> Cog Attention computation module -> Sign extraction and exponential normalization -> Attention weights with absolute value normalization -> Weighted sum with value vectors -> Feed-forward networks

- Critical path:
  1. Query and key vectors computed from input
  2. Inner product calculated and sign extracted
  3. Exponential function applied with absolute max normalization
  4. Attention weights computed with absolute value normalization
  5. Weighted sum with value vectors

- Design tradeoffs:
  - Positive: Enhanced expressiveness, reduced representational collapse, potential efficiency gains
  - Negative: Requires careful initialization, may need softmax preservation in first/last layers, increased complexity in attention computation
  - Neutral: Same parameter count, similar computational complexity

- Failure signatures:
  - Training instability or NaN errors (denominator becomes zero)
  - Slow convergence or getting stuck in local minima
  - Unexpected performance degradation on certain tasks
  - Attention patterns that don't reflect meaningful semantic operations

- First 3 experiments:
  1. Implement Cog Attention and verify it produces negative weights while maintaining training stability
  2. Compare attention patterns between Cog Attention and softmax on a simple task to observe diversity differences
  3. Test on a small language modeling task to confirm performance benefits and identify any failure modes

## Open Questions the Paper Calls Out

- How does Cog Attention affect robustness of language models in tasks involving complex, long-range dependencies?
- Can introduction of negative attention weights lead to unintended biases or artifacts in learned representations?
- How does choice of exponential function in Cog Attention affect model's performance and interpretability?

## Limitations

- Theoretical justification for reduced representational collapse is supported by formal analysis but requires broader validation across diverse architectures
- Empirical results show consistent improvements on tested tasks but generalization to other domains remains uncertain
- Edge case handling and behavior with extremely long sequences needs comprehensive analysis

## Confidence

- High Confidence: Empirical results showing Cog Attention outperforms softmax on language modeling (44.35% vs 42.98% accuracy) and image generation are well-documented
- Medium Confidence: Theoretical justification for reduced representational collapse has formal support but practical impact across architectures needs validation
- Low Confidence: Claims about enhanced extrapolation capabilities and KV cache compression benefits are mentioned but not empirically validated

## Next Checks

1. Conduct ablation study on Cog Attention layer placement to determine optimal positioning and quantify impact on performance and stability
2. Implement Cog Attention in architectures beyond decoder-only transformers and diffusion models to validate cross-domain generalization
3. Systematically evaluate edge cases and long sequence behavior to assess normalization stability and identify unintended attention patterns