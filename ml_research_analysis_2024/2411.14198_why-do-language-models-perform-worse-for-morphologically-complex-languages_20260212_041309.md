---
ver: rpa2
title: Why do language models perform worse for morphologically complex languages?
arxiv_id: '2411.14198'
source_url: https://arxiv.org/abs/2411.14198
tags:
- languages
- language
- performance
- association
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The performance of language models varies across languages, with
  morphologically complex languages often underperforming. We replicate prior analyses
  showing that fusional languages, like English, consistently outperform agglutinative
  languages, such as Turkish, in language modeling tasks.
---

# Why do language models perform worse for morphologically complex languages?

## Quick Facts
- arXiv ID: 2411.14198
- Source URL: https://arxiv.org/abs/2411.14198
- Authors: Catherine Arnett; Benjamin K. Bergen
- Reference count: 40
- Primary result: Performance gaps between fusional and agglutinative languages can be explained by dataset size disparities when scaled by byte premiums

## Executive Summary
This paper investigates why language models perform worse on morphologically complex languages, specifically comparing fusional languages (like English) with agglutinative languages (like Turkish). The authors replicate prior analyses showing consistent performance gaps and propose three potential causes: morphological alignment of tokenizers, tokenization quality, and dataset size disparities. They introduce a new metric called MorphScore to evaluate morphological alignment and test their hypotheses across 22 languages. The key finding is that when dataset sizes are scaled according to byte premiums—the number of bytes required to represent text in different languages—the performance gap between language types is significantly reduced.

## Method Summary
The study employs a multi-faceted approach to test three hypotheses about performance gaps between fusional and agglutinative languages. First, they develop MorphScore, a metric that measures how well tokenizer boundaries align with morpheme boundaries by assigning scores of 1 (aligned) or 0 (misaligned) for each word. Second, they analyze tokenization quality using R´enyi entropy and compression metrics on the FLORES-200 dataset. Third, they train language models using byte-premium-scaled datasets to test whether dataset size disparities explain the performance gap. The analysis spans 22 languages with morphological datasets from UD and UniMorph, using monolingual tokenizers from Chang et al. (2023) and evaluating performance through perplexity and benchmark scores.

## Key Results
- Fusional languages consistently outperform agglutinative languages in language modeling tasks
- Morphological alignment scores (MorphScore) show no correlation with performance gaps
- When datasets are scaled according to byte premiums, the performance gap between language types is significantly reduced
- Tokenization quality (measured by R´enyi entropy) has a minor effect on the performance gap

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Morphological alignment of tokenizers does not explain performance gaps between agglutinative and fusional languages.
- Mechanism: Tokenizers with higher morphological alignment scores (MorphScore) should correlate with better language model performance, especially for morphologically rich languages.
- Core assumption: Morphological alignment of token boundaries with morpheme boundaries improves language model learning efficiency.
- Evidence anchors:
  - [abstract] "We find some evidence that tokenization quality explains the performance gap, but none for the role of morphological alignment."
  - [section] "Morphological alignment of the tokenizer – or lack thereof – could impact language modeling performance, especially for morphologically rich languages."
  - [corpus] Weak evidence - only 5 of 25 related papers address morphological alignment specifically, and none provide strong contradicting evidence.
- Break condition: If morphological alignment scores negatively correlate with performance metrics, or if languages with higher alignment scores perform worse than those with lower scores.

### Mechanism 2
- Claim: Dataset size disparities, when scaled according to byte premiums, explain the performance gap between agglutinative and fusional languages.
- Mechanism: Languages with higher byte premiums require more bytes to represent the same content, effectively reducing the amount of training data available when datasets are measured in tokens rather than bytes.
- Core assumption: Language models require consistent amounts of training data measured in bytes rather than tokens to achieve comparable performance across languages.
- Evidence anchors:
  - [abstract] "Instead we find that the performance gap is most reduced when training datasets are of equivalent size across language types, but only when scaled according to the so-called 'byte-premium' – the different encoding efficiencies of different languages and orthographies."
  - [section] "These results suggest that no language is harder or easier for a language model to learn on the basis of its morphological typology. Differences in performance can be attributed to disparities in dataset size."
  - [corpus] Moderate evidence - several related papers discuss tokenization and morphology, but only one directly addresses byte premium scaling effects.
- Break condition: If byte-premium-scaled datasets still show performance gaps between language types, or if the correlation between byte premium and performance is weak or inconsistent.

### Mechanism 3
- Claim: Tokenization quality, as measured by R´enyi entropy, has a minor but measurable effect on the performance gap between language types.
- Mechanism: Agglutinative languages tend to have higher R´enyi entropy values due to their longer words and more unique word forms, leading to less efficient token frequency distributions.
- Core assumption: More even token frequency distributions (lower R´enyi entropy) lead to better language model performance.
- Evidence anchors:
  - [abstract] "We find some evidence that tokenization quality explains the performance gap, but none for the role of morphological alignment."
  - [section] "R´enyi entropy has been shown to correlate with downstream performance... Because of their larger number of low-frequency word forms, it is possible that agglutinative languages have higher numbers of low-frequency tokens."
  - [corpus] Moderate evidence - multiple related papers discuss tokenization quality metrics, though direct evidence linking R´enyi entropy to performance gaps is limited.
- Break condition: If R´enyi entropy shows no correlation with performance after controlling for other factors, or if the effect size is negligible compared to other variables.

## Foundational Learning

- Concept: Morphological typology (fusional vs agglutinative languages)
  - Why needed here: Understanding the fundamental difference between language types is crucial for interpreting the performance gap and designing appropriate experiments.
  - Quick check question: Can you explain the key difference between fusional and agglutinative languages using an example from English and Turkish?

- Concept: Tokenization algorithms (BPE, SentencePiece, etc.)
  - Why needed here: The study evaluates different tokenization approaches and their impact on language model performance across languages.
  - Quick check question: What is the primary goal of Byte Pair Encoding (BPE) in tokenization, and how might it behave differently for fusional vs agglutinative languages?

- Concept: Dataset scaling and measurement units (tokens vs bytes)
  - Why needed here: The critical finding relates to how dataset sizes are measured and scaled, particularly the concept of byte premiums.
  - Quick check question: If Language A has a byte premium of 2 relative to Language B, how much more training data (in bytes) would Language A need to receive the same effective training as Language B when measured in tokens?

## Architecture Onboarding

- Component map: MorphScore calculation -> Tokenization quality analysis (CTC, R´enyi entropy) -> Byte-premium scaling -> Model training -> Performance evaluation
- Critical path: The most critical path is the byte-premium scaling experiment, as it directly tests the primary hypothesis. This requires: dataset collection → byte premium calculation → model training with scaled datasets → performance evaluation → statistical analysis.
- Design tradeoffs: The study chose to use existing monolingual tokenizers rather than training new ones specifically for morphological alignment evaluation. This tradeoff allowed for broader language coverage but may have introduced noise from suboptimal tokenization for some languages.
- Failure signatures: Key failure modes include: (1) significant performance gaps persisting after byte-premium scaling, (2) strong negative correlation between MorphScore and performance, or (3) R´enyi entropy showing strong predictive power for performance gaps. Any of these would indicate a need to revisit the primary hypothesis.
- First 3 experiments:
  1. Recreate MorphScore datasets for the 22 languages using UD and UniMorph annotations
  2. Calculate CTC and R´enyi entropy for each tokenizer using FLORES-200 dataset
  3. Train a small-scale language model on byte-premium-scaled datasets for 3-4 languages spanning different morphological types to test the core hypothesis before full-scale experiments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MorphScore metric compare to other existing metrics for evaluating tokenizer morphological alignment?
- Basis in paper: [explicit] The paper introduces MorphScore as a new metric and uses it to evaluate morphological alignment of tokenizers across 22 languages.
- Why unresolved: The paper does not compare MorphScore to other existing metrics for evaluating morphological alignment of tokenizers.
- What evidence would resolve it: A comparative study of MorphScore with other existing metrics would help determine its effectiveness and potential advantages.

### Open Question 2
- Question: What are the specific linguistic features that contribute to the observed performance gap between agglutinative and fusional languages?
- Basis in paper: [explicit] The paper discusses the performance gap between agglutinative and fusional languages but does not delve into the specific linguistic features that contribute to this gap.
- Why unresolved: The paper focuses on testing hypotheses about the causes of the performance gap but does not identify the specific linguistic features involved.
- What evidence would resolve it: A detailed linguistic analysis of the specific features of agglutinative and fusional languages that affect language modeling performance would help identify the underlying causes.

### Open Question 3
- Question: How do byte premiums affect the training of language models for low-resource languages?
- Basis in paper: [explicit] The paper discusses the impact of byte premiums on language model performance and suggests that they may exacerbate data scarcity issues for low-resource languages.
- Why unresolved: The paper does not provide a detailed analysis of how byte premiums specifically affect the training process for low-resource languages.
- What evidence would resolve it: A study on the impact of byte premiums on the training of language models for low-resource languages would help understand the specific challenges and potential solutions.

### Open Question 4
- Question: What are the implications of the findings for the development of language models for under-resourced languages?
- Basis in paper: [explicit] The paper discusses the implications of the findings for language models for under-resourced languages, suggesting that byte premiums should be taken into account when scaling training data.
- Why unresolved: The paper does not provide a comprehensive discussion of the broader implications for the development of language models for under-resourced languages.
- What evidence would resolve it: A discussion of the broader implications for the development of language models for under-resourced languages, including potential strategies for addressing the challenges identified in the paper, would help guide future research and development efforts.

## Limitations

- The study focuses primarily on perplexity and benchmark scores, which may not fully represent real-world language model utility across different morphological types
- The MorphScore metric, while novel, may not capture all relevant aspects of morphological alignment between tokenizers and language structures
- Dataset size differences, while controlled for byte premiums, may still contain other confounding variables not measured in this study

## Confidence

**High Confidence Claims:**
- The observation that fusional languages consistently outperform agglutinative languages in language modeling tasks is well-established and replicated in this study
- The finding that morphological alignment scores (MorphScore) do not correlate with performance gaps is supported by the experimental data
- The methodology for calculating byte premiums and scaling datasets is technically sound

**Medium Confidence Claims:**
- The conclusion that tokenization quality (measured by R´enyi entropy) has a minor effect on performance gaps is supported but the effect size is small and may be influenced by other factors
- The assertion that dataset size disparities, when properly scaled by byte premiums, eliminate the performance gap is the central claim but relies on the assumption that byte premiums are the only relevant scaling factor

**Low Confidence Claims:**
- The broader implication that "no language is harder or easier for a language model to learn on the basis of its morphological typology" may be overstated, as the study does not explore all possible aspects of morphological complexity or language model architectures

## Next Checks

1. **Cross-architecture validation**: Test whether the byte-premium scaling effect holds across different language model architectures (RNNs, CNNs, and other transformer variants) to ensure the finding is not architecture-specific.

2. **Downstream task expansion**: Evaluate the performance gap and byte-premium scaling effects on a broader range of downstream tasks beyond the benchmarks used in this study, particularly focusing on tasks that may be more sensitive to morphological structure.

3. **Morpheme-aware tokenization comparison**: Implement and test morpheme-aware tokenization algorithms specifically designed for agglutinative languages and compare their performance to standard tokenizers when scaled by byte premiums, to isolate the effect of tokenization approach from dataset scaling.