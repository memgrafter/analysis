---
ver: rpa2
title: Improving GFlowNets for Text-to-Image Diffusion Alignment
arxiv_id: '2406.00633'
source_url: https://arxiv.org/abs/2406.00633
tags:
- diffusion
- reward
- learning
- bengio
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning text-to-image diffusion
  models with black-box reward functions, such as aesthetic scores or text-image alignment
  metrics. The authors propose Diffusion Alignment with GFlowNets (DAG), a method
  that leverages the GFlowNet framework to train diffusion models so that their output
  distribution is proportional to the reward function, rather than directly maximizing
  it.
---

# Improving GFlowNets for Text-to-Image Diffusion Alignment

## Quick Facts
- **arXiv ID:** 2406.00633
- **Source URL:** https://arxiv.org/abs/2406.00633
- **Reference count:** 24
- **Primary result:** Proposes Diffusion Alignment with GFlowNets (DAG) to align text-to-image diffusion models with black-box reward functions, achieving faster convergence and better reward-diversity trade-offs than RL baseline DDPO

## Executive Summary
This paper addresses the challenge of aligning text-to-image diffusion models with black-box reward functions such as aesthetic scores or text-image alignment metrics. The authors propose Diffusion Alignment with GFlowNets (DAG), a method that leverages the GFlowNet framework to train diffusion models so that their output distribution is proportional to the reward function, rather than directly maximizing it. This approach avoids mode collapse and improves sample efficiency. Two variants are introduced: DAG-DB, based on detailed balance, and DAG-KL, which uses a KL-based objective with REINFORCE-style gradients. Experiments on Stable Diffusion with rewards like aesthetic scores, image-text alignment, and compressibility show that DAG methods achieve faster convergence and better reward-diversity trade-offs compared to the RL baseline DDPO. Generated samples demonstrate improved alignment with prompts and reward characteristics.

## Method Summary
The paper introduces DAG (Diffusion Alignment with GFlowNets) to align text-to-image diffusion models with black-box reward functions. Unlike direct reward maximization which can lead to mode collapse, DAG trains the model so its output distribution is proportional to the reward function. Two variants are proposed: DAG-DB based on detailed balance and DAG-KL using a KL-based objective with REINFORCE-style gradients. The method uses LoRA parameter-efficient fine-tuning on Stable Diffusion v1.5 with a 50-step DDIM schedule, classifier-free guidance (CFG=5), and trains on 8 GPUs with batch size 8 per GPU. The approach shows improved sample efficiency and better reward-diversity trade-offs compared to the DDPO RL baseline.

## Key Results
- DAG methods achieve faster convergence and better reward-diversity trade-offs than DDPO RL baseline
- DAG-DB and DAG-KL variants demonstrate improved alignment with prompts and reward characteristics
- Experiments show avoidance of mode collapse compared to direct reward maximization approaches
- Sample efficiency improvements observed across aesthetic scores, image-text alignment, and compressibility reward functions

## Why This Works (Mechanism)
The method works by framing reward maximization as a distributional matching problem rather than direct optimization. Instead of maximizing the reward function directly (which can lead to mode collapse), DAG trains the diffusion model to produce outputs whose distribution is proportional to the reward function. This is achieved through the GFlowNet framework, which provides a principled way to learn such distributions without requiring explicit reward maximization. The forward-looking technique and flow function network help maintain diversity while optimizing for reward, addressing the key limitation of RL-based approaches that tend to collapse to high-reward modes.

## Foundational Learning
**Generative Flow Networks (GFlowNets)**: Learn to sample from distributions by constructing trajectories through a state space. Needed to provide a framework for learning distributions proportional to reward functions rather than maximizing rewards directly. Quick check: Verify understanding of the trajectory construction and reward matching properties.

**Diffusion Models**: Generate samples by gradually denoising random noise through a learned reverse process. Needed as the base architecture being aligned with reward functions. Quick check: Confirm understanding of the forward (noising) and reverse (denoising) processes.

**Detailed Balance**: A principle ensuring reversibility in stochastic processes. Needed for the DAG-DB variant to maintain proper distributional properties. Quick check: Understand how detailed balance relates to the flow function and reward matching.

**REINFORCE Algorithm**: A policy gradient method for reinforcement learning. Needed for the DAG-KL variant's gradient estimation. Quick check: Verify understanding of the score function gradient estimator and its variance properties.

**LoRA (Low-Rank Adaptation)**: A parameter-efficient fine-tuning method. Needed to adapt large diffusion models without full fine-tuning. Quick check: Confirm understanding of how low-rank decomposition reduces parameter count while maintaining performance.

## Architecture Onboarding

**Component Map**: Stable Diffusion v1.5 -> LoRA Adapter -> Flow Function Network -> Reward Function -> GFlowNet Objective -> Training Loop

**Critical Path**: The training loop receives prompts, generates images through the adapted diffusion model, computes rewards, updates the flow function network parameters through the GFlowNet objective, and backpropagates to update LoRA adapters. The forward-looking technique integrates with the data prediction network to maintain diversity.

**Design Tradeoffs**: Direct reward maximization vs. distributional matching (DAG avoids mode collapse but may be slower to converge initially). REINFORCE-based vs. detailed balance approaches (DAG-KL is more flexible but has higher variance, DAG-DB is more stable but requires specific reward function properties).

**Failure Signatures**: Mode collapse (low FID score between independent samples indicates loss of diversity), poor reward convergence (reward plateaus below target), training instability (exploding gradients or NaN values in flow function network updates).

**3 First Experiments**:
1. Train DAG-DB with aesthetic score reward on 45 animal prompts, monitoring reward progression and sample diversity every 10 epochs
2. Compare DAG-KL vs DAG-DB convergence rates and final reward achievement on image-text alignment task
3. Measure FID score between two independent batches of generated samples to quantify diversity preservation

## Open Questions the Paper Calls Out
**Open Question 1**: How does the proposed DAG-KL algorithm's performance scale when applied to larger and more complex diffusion models, such as SDXL or higher resolution models? The experiments were limited to Stable Diffusion v1.5, and scaling may introduce computational and optimization challenges not addressed in the current study.

**Open Question 2**: What is the impact of different noise schedule configurations on the performance of DAG-DB and DAG-KL algorithms in text-to-image diffusion alignment tasks? The paper uses a fixed noise schedule without investigating how changes affect the algorithms' ability to align diffusion models with reward functions.

**Open Question 3**: How does the introduction of multimodal conditioning (e.g., both text and image conditions) affect the performance of DAG algorithms compared to unimodal conditioning? The study is limited to text conditioning, leaving the effects of incorporating additional modalities unexplored.

## Limitations
- Flow function network architecture details and forward-looking technique integration are not fully specified
- Evaluation focuses primarily on reward scores without comprehensive user studies or long-term stability assessments
- Limited comparison with only DDPO baseline; additional baselines could strengthen claims
- Experiments restricted to Stable Diffusion v1.5, limiting generalizability to larger models

## Confidence
- **High**: Empirical observations for tested reward functions and datasets show consistent improvements in sample efficiency and reward achievement
- **Medium**: Core methodology appears sound but lacks complete implementation details for reproducibility
- **Low**: Generalization to other reward types, model architectures, or larger-scale applications remains unproven

## Next Checks
1. Implement the forward-looking technique and verify its effect on mode collapse through controlled experiments comparing DAG with and without this component
2. Conduct ablation studies on the flow function network architecture to determine optimal configurations for different reward types
3. Test the method with additional reward functions (e.g., CLIP-based alignment scores) to validate robustness across different alignment objectives