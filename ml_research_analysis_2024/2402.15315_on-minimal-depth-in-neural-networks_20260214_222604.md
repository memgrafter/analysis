---
ver: rpa2
title: On Minimal Depth in Neural Networks
arxiv_id: '2402.15315'
source_url: https://arxiv.org/abs/2402.15315
tags:
- depth
- minimal
- proposition
- networks
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates minimal depth requirements for representing\
  \ sum, max, and a\uFB03ne max functions in ReLU neural networks, and extends these\
  \ insights to polytope neural networks. It proves that determining the minimal depth\
  \ of the sum operation requires both operands to have equal minimal depth, while\
  \ for the max operation, the minimal depth cannot be deduced solely from operand\
  \ depths."
---

# On Minimal Depth in Neural Networks

## Quick Facts
- arXiv ID: 2402.15315
- Source URL: https://arxiv.org/abs/2402.15315
- Reference count: 26
- Key result: Establishes minimal depth requirements for sum, max, and affine max functions in ReLU networks; computes simplex depth as ⌈log2(n+1)⌉

## Executive Summary
This paper investigates the minimal depth requirements for representing fundamental operations (sum, max, affine max) in ReLU neural networks and extends these insights to polytope neural networks. The study establishes that sum operations require equal minimal depth when both operands have the same minimal depth, while max operations cannot have their minimal depth deduced solely from operand depths. The paper also establishes basic depth properties for polytope networks analogous to ReLU networks and computes the minimal depth of simplices as ⌈log2(n+1)⌉, providing a tight bound related to the open conjecture on minimal depth for continuous piecewise linear functions.

## Method Summary
The paper uses theoretical analysis of ReLU neural networks and polytope neural networks, employing tropical geometry and convex polytope theory to establish depth properties. The approach involves proving necessary and sufficient conditions for minimal depth representation of sum and max operations, deriving depth properties for polytope networks through their geometric representation, and computing the minimal depth of simplices by analyzing their geometric structure. The study leverages the isomorphism between ReLU networks and polytope networks via Newton polytope and support function mappings to translate results between geometric and algebraic representations.

## Key Results
- Sum operations require minimal depth equal to max{m1, m2} when m1≠m2, and cannot be improved when m1=m2
- Max operations have no sufficient conditions based solely on operand depths for determining minimal depth
- n-simplices have minimal depth ⌈log2(n+1)⌉, tightly related to the minimal depth conjecture for CPWL functions
- Established depth inclusions and computation properties for polytope networks analogous to ReLU networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimal depth of ReLU networks can be bounded by analyzing specific operations (sum, max) and their depth behavior.
- Mechanism: The paper establishes necessary and sufficient conditions for minimal depth in sum and max operations. For sum, if operands have equal minimal depth, the sum requires the same depth; otherwise, it requires max of the two depths. For max, minimal depth depends on both operands' depths plus structural properties.
- Core assumption: Minimal depth characterization is possible by decomposing CPWL functions into simpler components.
- Evidence anchors:
  - [abstract] "For the sum operation, we establish a sufficient condition on the minimal depth of the operands to find the minimal depth of the operation. In contrast, regarding the max operation, a comprehensive set of examples is presented, demonstrating that no sufficient conditions, depending solely on the depth of the operands, would imply a minimal depth for the operation."
  - [section] "Corollary 1 presents both a necessary condition and a sufficient condition for determining the minimal depth of the sum operation. Notably, when m1=m2, Corollary 1(b) cannot be improved."
- Break condition: If the CPWL function cannot be decomposed into sum or max operations with known minimal depth properties, the method fails.

### Mechanism 2
- Claim: Polytope neural networks provide geometric insight into ReLU network expressivity.
- Mechanism: The paper establishes depth properties for polytope networks (∆(m)) analogous to ReLU networks, including depth inclusions and computation from Minkowski sums and convex hulls. This allows translation of results between geometric and algebraic representations.
- Core assumption: The isomorphism between ReLU networks and polytope networks (via Newton polytope and support function mappings) preserves depth properties.
- Evidence anchors:
  - [abstract] "On polytope neural networks, we investigate several fundamental properties, deriving results equivalent to those of ReLU networks, such as depth inclusions and depth computation from vertices."
  - [section] "Theorem 3 (Hertrich et al. [14]). A positively homogeneous CPWL function f ∈ Υ(m) if and only if there exist fi ∈ Υ∆, i = 1, 2, such that f = f1 − f2 and N fi ∈ ∆(m)."
- Break condition: If the geometric representation becomes too complex or the isomorphism breaks down for certain CPWL functions, the method fails.

### Mechanism 3
- Claim: Minimal depth of simplices (⌈log2(n+1)⌉) provides a tight bound for the general conjecture.
- Mechanism: By proving that n-simplices require exactly ⌈log2(n+1)⌉ hidden layers, the paper establishes a lower bound that matches the conjectured upper bound, suggesting the conjecture is true.
- Core assumption: Simplices are representative of the most complex CPWL functions in terms of depth requirements.
- Evidence anchors:
  - [abstract] "Most notably, the minimal depth of simplices, which is strictly related to the minimal depth conjecture in ReLU networks."
  - [section] "Theorem 13. Any n-simplex has minimal depth ⌈log2(n + 1)⌉."
- Break condition: If there exists a CPWL function requiring more than ⌈log2(n+1)⌉ layers, the method fails.

## Foundational Learning

- Concept: Continuous Piecewise Linear (CPWL) functions
  - Why needed here: The entire paper revolves around characterizing which CPWL functions can be represented by ReLU networks of minimal depth
  - Quick check question: What mathematical property makes ReLU networks inherently CPWL functions?

- Concept: Tropical geometry and polytope theory
  - Why needed here: The paper uses the connection between ReLU networks and convex polytopes to establish depth properties
  - Quick check question: How does the Newton polytope of a ReLU network relate to its geometric representation?

- Concept: Minimal depth representation
  - Why needed here: The paper's central focus is determining the smallest number of hidden layers needed to represent specific functions
  - Quick check question: What is the difference between a function having minimal depth m versus just being representable in Υ(m)?

## Architecture Onboarding

- Component map: Sum operation analysis -> Max operation analysis -> Polytope network foundations -> Simplex depth computation -> Conjecture connection

- Critical path: Understanding the sum operation properties → Understanding the max operation properties → Establishing polytope network foundations → Computing simplex depth → Connecting to the general conjecture

- Design tradeoffs: The geometric approach (polytope networks) provides elegant depth characterizations but requires more abstract mathematical machinery. The algebraic approach (direct ReLU analysis) is more concrete but may miss deeper structural insights.

- Failure signatures: If depth bounds cannot be established for fundamental operations, the entire minimal depth characterization framework fails. If the polytope-ReLU isomorphism breaks down for certain functions, geometric insights become unreliable.

- First 3 experiments:
  1. Verify the sum operation depth property: Take two ReLU networks with depths m1 and m2, construct their sum, and verify whether it requires max{m1, m2} layers (when m1≠m2) or equal depth (when m1=m2).
  2. Test the max operation counterexample: Construct two ReLU networks with known minimal depths and verify that their max can have minimal depth less than max{m1, m2}+1.
  3. Validate the simplex depth calculation: For small dimensions (n=2,3,4), explicitly construct ReLU networks representing simplices and verify they require exactly ⌈log2(n+1)⌉ layers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the minimal depth of the max operation be determined solely from the minimal depths of its operands?
- Basis in paper: [explicit] The paper demonstrates that for the max operation, no sufficient conditions depending solely on the depth of the operands would imply a minimal depth for the operation.
- Why unresolved: The paper provides examples showing that different combinations of operand depths can lead to different minimal depths for the max operation, making it impossible to deduce the minimal depth based only on operand depths.
- What evidence would resolve it: A general formula or set of conditions that can predict the minimal depth of the max operation based on the properties of the operands beyond their depths.

### Open Question 2
- Question: Is the minimal depth of an n-simplex strictly related to the minimal depth conjecture in ReLU networks?
- Basis in paper: [explicit] The paper proves that the minimal depth of an n-simplex is ⌈log2(n + 1)⌉, which is strictly related to the minimal depth conjecture in ReLU networks.
- Why unresolved: While the paper establishes this relationship, it does not provide a direct proof that this result resolves the conjecture.
- What evidence would resolve it: A proof that demonstrates how the minimal depth of an n-simplex being ⌈log2(n + 1)⌉ directly implies or contradicts the minimal depth conjecture in ReLU networks.

### Open Question 3
- Question: Can the minimal depth of a polytope be computed efficiently using its decomposition into indecomposable polytopes?
- Basis in paper: [inferred] The paper discusses the minimal depth of indecomposable polytopes and suggests that understanding the minimal depth of polytopes is relevant to determining the minimal depth of positively homogeneous CPWL functions.
- Why unresolved: The paper provides a theoretical framework for computing the minimal depth of indecomposable polytopes but does not offer a practical algorithm or method for efficient computation.
- What evidence would resolve it: An efficient algorithm or method that can compute the minimal depth of a polytope by decomposing it into indecomposable polytopes and applying the results from the paper.

## Limitations

- The geometric framework relies heavily on abstract polytope theory, making practical verification challenging
- No sufficient conditions exist for determining minimal depth of max operations based solely on operand depths
- The connection between simplex depth and the general minimal depth conjecture remains unproven

## Confidence

- Sum operation characterization: Medium
- Max operation characterization: Low
- Simplex depth calculation: High

## Next Checks

1. Construct a comprehensive test suite of CPWL functions to empirically validate the sum operation depth bounds across various operand depth combinations.

2. Develop a systematic approach to generate counterexamples for the max operation conjecture, potentially revealing patterns that could lead to better sufficient conditions.

3. Implement a verification framework that maps ReLU networks to their polytope representations and vice versa, testing the isomorphism preservation for increasingly complex CPWL functions.