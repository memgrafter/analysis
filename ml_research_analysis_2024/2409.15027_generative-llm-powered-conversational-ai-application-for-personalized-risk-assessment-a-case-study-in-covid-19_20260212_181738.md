---
ver: rpa2
title: 'Generative LLM Powered Conversational AI Application for Personalized Risk
  Assessment: A Case Study in COVID-19'
arxiv_id: '2409.15027'
source_url: https://arxiv.org/abs/2409.15027
tags:
- risk
- llms
- data
- assessment
- template
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that generative large language models (LLMs)
  can perform disease risk assessment using conversational AI, achieving high AUC
  scores in low-data regimes. By fine-tuning pre-trained models like Llama2-7b and
  Flan-t5-xl with few-shot examples, the approach eliminates the need for traditional
  de novo training on tabular data.
---

# Generative LLM Powered Conversational AI Application for Personalized Risk Assessment: A Case Study in COVID-19

## Quick Facts
- arXiv ID: 2409.15027
- Source URL: https://arxiv.org/abs/2409.15027
- Reference count: 26
- Primary result: Generative LLMs achieve high AUC scores in low-data regimes for COVID-19 risk assessment using few-shot fine-tuning

## Executive Summary
This study demonstrates that generative large language models (LLMs) can perform disease risk assessment using conversational AI, achieving high AUC scores in low-data regimes. By fine-tuning pre-trained models like Llama2-7b and Flan-t5-xl with few-shot examples, the approach eliminates the need for traditional de novo training on tabular data. The method integrates streaming question-answer pairs into a mobile application, enabling real-time, no-code risk assessment and personalized feature importance analysis. Results show LLMs outperform traditional classifiers such as Logistic Regression and XGBoost, with models like T0-3b achieving an AUC of 0.75 in zero-shot settings.

## Method Summary
The method involves converting tabular patient data into natural language prompts using List or Text Templates, then fine-tuning pre-trained LLMs (Llama2-7b, Flan-T5-xl, T0-3b) with few-shot examples using LoRA. The system processes streaming Q&A pairs from patients through a mobile application, generating severity predictions and extracting feature importance from attention layers. The approach was validated on COVID-19 severity data from Children's Hospital of Michigan and UPMC Children's Hospital of Pittsburgh, comparing performance against traditional classifiers across zero-shot and few-shot settings.

## Key Results
- LLMs achieve high AUC scores (up to 0.75) in zero-shot settings for COVID-19 risk assessment
- Few-shot fine-tuning with 2-32 examples enables effective risk classification without traditional training
- T0-3b outperforms traditional classifiers (Logistic Regression, XGBoost, Random Forest) in low-data regimes
- Attention-based feature importance extraction provides interpretability for clinical decision-making

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning generative LLMs with few-shot examples enables accurate risk assessment in low-data regimes.
- Mechanism: Pre-trained generative LLMs already encode general language patterns and domain knowledge. Fine-tuning with a small number of task-specific examples (2-32 shots) adapts the model's weights using parameter-efficient methods (LoRA) to the risk classification task without retraining the full model.
- Core assumption: The general knowledge in pre-trained LLMs is sufficient to generalize to specific healthcare tasks with minimal task-specific data.
- Evidence anchors:
  - [abstract] "By fine-tuning pre-trained models like Llama2-7b and Flan-t5-xl with few-shot examples, the approach eliminates the need for traditional de novo training on tabular data."
  - [section II-E] "LoRA (Low-Rank Adaptation) [19]... involves training a small proportion of parameters by integrating trainable low-rank matrices into each layer of the pre-trained model."
- Break Condition: If the pre-training data lacks sufficient coverage of the healthcare domain or the task requires highly specialized knowledge not present in general language patterns.

### Mechanism 2
- Claim: LLMs outperform traditional classifiers in low-data regimes by leveraging pre-training.
- Mechanism: Traditional classifiers (Logistic Regression, XGBoost, Random Forest) learn patterns from scratch using only the task-specific data, while LLMs apply learned world knowledge to the task, allowing them to perform well even with minimal examples.
- Core assumption: The extensive pre-training on diverse datasets provides LLMs with generalizable knowledge that transfers to new tasks.
- Evidence anchors:
  - [abstract] "By achieving high Area Under the Curve (AUC) scores with a limited number of fine-tuning samples, our results demonstrate the potential of generative LLMs to outperform discriminative classification methods in low-data regimes."
  - [section IV-C] "LLMs like T0-3b achieved an AUC of 0.75 in the zero-shot setting, outperforming traditional methods even without task-specific fine-tuning."
- Break Condition: If the task domain is highly specialized and not well-represented in the pre-training data, or if the labeled data, though small, contains very specific patterns that require de novo learning.

### Mechanism 3
- Claim: Streaming QA pairs can be used as input for risk assessment by serializing tabular data into natural language prompts.
- Mechanism: The structured tabular data (feature vectors) is transformed into natural language using List Template or Text Template serialization methods, creating prompts that the LLM can process. This allows the model to interpret both structured and unstructured data formats.
- Core assumption: LLMs can effectively process serialized natural language representations of structured data and extract relevant features for prediction.
- Evidence anchors:
  - [section II-D] "To achieve serialization, the features in our dataset are denoted as f1, f2, ..., fd... We used two main serialization methods, the List Template and the Text Template..."
  - [section II-E] "The input to the LLMs is a serialized string generated from the tabular data using the previously explained serialization strategies."
- Break Condition: If the serialization process loses critical information about feature relationships or if the LLM struggles to interpret the natural language representation of numerical or categorical data.

## Foundational Learning

- Concept: Pre-trained language models and transfer learning
  - Why needed here: Understanding how pre-trained LLMs can be adapted to new tasks with minimal data is crucial for grasping the core innovation of this approach.
  - Quick check question: What is the key difference between fine-tuning a pre-trained LLM and training a model from scratch on the same task?

- Concept: Parameter-efficient fine-tuning (LoRA)
  - Why needed here: The paper uses LoRA to adapt LLMs with minimal computational cost, which is essential for practical deployment.
  - Quick check question: How does LoRA reduce the number of parameters that need to be trained compared to full fine-tuning?

- Concept: Attention mechanisms and feature importance
  - Why needed here: The paper derives feature importance from LLM attention layers, which is a novel interpretability approach for risk assessment.
  - Quick check question: How can attention scores from a language model's output layer be interpreted as feature importance for a classification task?

## Architecture Onboarding

- Component map:
  - Data Collection -> Serialization Layer -> LLM Core -> Feature Importance Module -> Mobile Application -> Database

- Critical path:
  1. Patient answers questions via mobile app
  2. Answers are serialized into natural language prompt
  3. Prompt is sent to fine-tuned LLM for risk assessment
  4. LLM returns severity prediction and attention-based feature importance
  5. Results are displayed to patient and clinician in real-time

- Design tradeoffs:
  - Serialization method choice (List vs Text Template) affects performance differently at various shot counts
  - Zero-shot vs few-shot fine-tuning balances generalization with task-specific accuracy
  - Attention-based feature importance provides interpretability but may be less precise than traditional methods

- Failure signatures:
  - Poor serialization leading to loss of feature information
  - Overfitting during few-shot fine-tuning (monitor validation loss)
  - Attention scores not correlating with actual feature importance
  - Mobile app latency affecting real-time interaction

- First 3 experiments:
  1. Compare List Template vs Text Template serialization performance across different shot counts (0, 2, 4, 8, 16, 32)
  2. Evaluate zero-shot performance of different LLMs to establish baseline capability
  3. Test attention-based feature importance extraction on known feature relationships to validate interpretability claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform in real-time conversational risk assessment compared to traditional methods when handling streaming QA pairs?
- Basis in paper: [explicit] The paper highlights the use of streaming QA pairs in conversational interfaces and compares LLM performance to traditional classifiers in low-data regimes.
- Why unresolved: While the paper demonstrates LLMs' effectiveness in zero-shot and few-shot settings, it does not provide a direct comparison of real-time conversational performance against traditional methods.
- What evidence would resolve it: A head-to-head comparison of real-time conversational risk assessment using LLMs versus traditional methods, measuring accuracy, latency, and user experience.

### Open Question 2
- Question: What are the long-term implications of using LLMs for disease risk assessment in terms of data privacy and model interpretability?
- Basis in paper: [explicit] The paper mentions the advantages of white-box models for data privacy and interpretability, but does not explore long-term implications.
- Why unresolved: The paper does not address potential challenges or benefits that may arise from prolonged use of LLMs in clinical settings.
- What evidence would resolve it: Longitudinal studies on data privacy breaches, model interpretability, and user trust in LLM-based risk assessments over time.

### Open Question 3
- Question: How do different serialization methods (List vs. Text Templates) impact the performance of LLMs in diverse healthcare applications beyond COVID-19?
- Basis in paper: [explicit] The paper compares List and Text Templates for COVID-19 severity risk assessment, showing varying performance.
- Why unresolved: The study focuses on COVID-19, and it is unclear if these findings generalize to other diseases or healthcare tasks.
- What evidence would resolve it: Experiments applying both serialization methods to various healthcare applications, comparing performance across different diseases and conditions.

## Limitations
- The dataset is relatively small (n=393) and comes from a single healthcare system, limiting external validity
- The feature importance method based on attention scores lacks validation against established techniques
- Serialization of tabular data to natural language prompts may introduce information loss

## Confidence

- High Confidence: The core finding that LLMs can perform risk classification tasks with few-shot learning is well-supported by the experimental results
- Medium Confidence: The claim that LLMs outperform traditional classifiers in low-data regimes is supported but requires more rigorous comparison
- Low Confidence: The feature importance extraction method using attention scores needs additional validation for clinical interpretability

## Next Checks
1. External Validation: Test the fine-tuned models on an independent dataset from a different healthcare system to assess generalization beyond the original training data
2. Clinical Performance Metrics: Evaluate model performance using additional metrics including precision, recall, F1-score, and calibration curves, particularly focusing on the model's ability to correctly identify high-risk cases
3. Feature Importance Validation: Compare the attention-based feature importance scores against established methods like SHAP or LIME values to verify the clinical interpretability of the results