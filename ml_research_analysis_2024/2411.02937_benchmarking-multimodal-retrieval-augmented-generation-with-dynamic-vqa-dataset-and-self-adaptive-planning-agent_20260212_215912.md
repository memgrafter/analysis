---
ver: rpa2
title: Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset
  and Self-adaptive Planning Agent
arxiv_id: '2411.02937'
source_url: https://arxiv.org/abs/2411.02937
tags:
- retrieval
- omnisearch
- questions
- knowledge
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dyn-VQA, a new dataset designed to benchmark
  multimodal retrieval-augmented generation (mRAG) models on dynamic, complex visual
  questions. Dyn-VQA contains 1,452 questions requiring adaptive retrieval strategies
  across query, tool, and time, including rapidly changing answers, multimodal knowledge,
  and multi-hop reasoning.
---

# Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent

## Quick Facts
- arXiv ID: 2411.02937
- Source URL: https://arxiv.org/abs/2411.02937
- Reference count: 30
- One-line primary result: OmniSearch achieves up to 50.03% F1-Recall on Dyn-VQA, outperforming existing mRAG methods and commercial search engines.

## Executive Summary
This paper introduces Dyn-VQA, a new dataset designed to benchmark multimodal retrieval-augmented generation (mRAG) models on dynamic, complex visual questions. Dyn-VQA contains 1,452 questions requiring adaptive retrieval strategies across query, tool, and time, including rapidly changing answers, multimodal knowledge, and multi-hop reasoning. Experiments show that existing heuristic mRAG approaches struggle on this dataset due to rigid retrieval processes. To address this, the authors propose OmniSearch, a self-adaptive planning agent that dynamically decomposes questions into sub-question chains and plans retrieval actions based on real-time feedback. OmniSearch significantly outperforms existing mRAG methods and commercial search engines, achieving up to 50.03% F1-Recall compared to 30.25% for GPT-4V with standard mRAG, demonstrating its effectiveness in handling complex, dynamic retrieval tasks.

## Method Summary
The authors introduce OmniSearch, a self-adaptive planning agent for multimodal retrieval-augmented generation. OmniSearch dynamically decomposes complex visual questions into sub-question chains and plans retrieval actions based on real-time feedback. It consists of a planning agent that generates sub-questions and retrieval plans, a retriever module that executes retrieval across multiple modalities, and a sub-question solver that evaluates retrieved content. Two versions are implemented: one based on GPT-4V with prompt engineering, and another based on Qwen-VL-Chat with instruction fine-tuning using synthetic data. The method is evaluated on Dyn-VQA, a new dataset containing 1,452 dynamic visual questions requiring adaptive retrieval strategies.

## Key Results
- OmniSearch achieves up to 50.03% F1-Recall on Dyn-VQA, significantly outperforming existing mRAG methods (30.25% for GPT-4V with standard mRAG).
- OmniSearch consistently outperforms commercial search engines (Bing Chat, Perplexity AI, Gemini) on complex multimodal retrieval tasks.
- The self-adaptive planning agent demonstrates superior ability to handle questions requiring multi-hop reasoning and rapidly changing answers compared to heuristic approaches.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OmniSearch's self-adaptive planning improves retrieval relevance by dynamically adjusting queries based on intermediate feedback.
- Mechanism: The planning agent decomposes complex questions into sub-question chains, evaluates retrieved content in real-time, and refines subsequent retrieval actions to target missing or ambiguous knowledge.
- Core assumption: Real-time feedback from retrieved content can reliably guide the next retrieval step toward the correct answer.
- Evidence anchors:
  - [abstract]: "OmniSearch significantly outperforms existing mRAG methods... achieving up to 50.03% F1-Recall compared to 30.25% for GPT-4V with standard mRAG"
  - [section]: "OmniSearch flexibly adjusts the next action according to question-solving state and retrieved content, with diverse purposes such as deepening comprehension of retrieved content"
  - [corpus]: "SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs" (weak, no direct evidence of dynamic adaptation)
- Break condition: If intermediate retrieval feedback is noisy or misleading, the agent may follow incorrect retrieval paths.

### Mechanism 2
- Claim: OmniSearch's modular retriever design allows flexible integration of diverse retrieval tools (web, image, multimodal) tailored to sub-questions.
- Mechanism: The retriever module executes retrieval actions across multiple modalities, enabling the agent to fetch knowledge beyond single-modality constraints.
- Core assumption: Questions requiring multimodal knowledge can be better served by accessing multiple retrieval tools dynamically.
- Evidence anchors:
  - [abstract]: "OmniSearch... dynamically decomposes complex multimodal questions into sub-question chains with retrieval action"
  - [section]: "retrieval scope extends to the entire Internet, offering intricate yet more comprehensive knowledge"
  - [corpus]: "Multimodal Iterative RAG for Knowledge-Intensive Visual Question Answering" (weak, no explicit multi-tool retrieval evidence)
- Break condition: If the retrieval tools are limited or if multimodal knowledge is unavailable, the benefit diminishes.

### Mechanism 3
- Claim: OmniSearch mitigates error propagation by re-evaluating and refining sub-answers at each step rather than propagating errors forward.
- Mechanism: At each step, the sub-question solver generates feedback on retrieved content, allowing the planning agent to correct misunderstandings before proceeding.
- Core assumption: Early detection and correction of retrieval or reasoning errors prevents compounding mistakes.
- Evidence anchors:
  - [abstract]: "OmniSearch... dynamically decomposes complex multimodal questions into sub-question chains with retrieval action"
  - [section]: "it rethinks the retrieved content and sub-questions to ensure the accuracy of the sub-answers, mitigating the risk of error propagation"
  - [corpus]: "Fine-Grained Knowledge Structuring and Retrieval for Visual Question Answering" (weak, no direct error correction evidence)
- Break condition: If the sub-question solver fails to detect errors, propagation cannot be mitigated.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: OmniSearch extends RAG by adding adaptive planning; understanding basic RAG is essential.
  - Quick check question: What are the two main steps in a standard RAG pipeline?
- Concept: Multimodal reasoning
  - Why needed here: OmniSearch handles questions requiring cross-modal knowledge; understanding multimodal integration is key.
  - Quick check question: Why might a question need both image and text retrieval?
- Concept: Planning agents and sub-question decomposition
  - Why needed here: OmniSearch's core innovation is planning-based retrieval; understanding decomposition is crucial.
  - Quick check question: How does breaking a question into sub-questions help retrieval?

## Architecture Onboarding

- Component map: Planning Agent → Retriever → Sub-question Solver → Feedback Loop
- Critical path: Input → Planner generates sub-question & retrieval plan → Retriever fetches data → Solver evaluates → Feedback to Planner → Next step or final answer
- Design tradeoffs: More retrieval steps improve accuracy but increase latency and cost; planner complexity vs. solver simplicity
- Failure signatures: Retrieval timeouts, ambiguous sub-questions, feedback loops without progress, incorrect final answers despite valid retrieval
- First 3 experiments:
  1. Run OmniSearch on a single known-good Dyn-VQA question and trace the sub-question chain.
  2. Replace the planning agent with a fixed heuristic planner and compare results.
  3. Disable feedback from the solver and observe error propagation.

## Open Questions the Paper Calls Out

- Open Question 1: How can OmniSearch be improved to handle questions requiring extended reasoning chains and long context knowledge? The paper mentions that OmniSearch struggles with questions requiring extended reasoning chains and tends to lose track of the original question and preceding information, as seen in the case study.
- Open Question 2: What are the most effective strategies for denoising, compressing, and summarizing context in OmniSearch to improve performance on long context questions? The paper suggests that improving the maximum length of the context window of MLLMs and enhancing the sub-problem solver's ability to denoise, compress, and summarize context are potential directions for improvement.
- Open Question 3: How can OmniSearch incorporate more precise retrieval techniques and a broader range of retrieval tools to enhance its performance? The paper mentions that the current OmniSearch does not support complex and fine-grained retrieval, as seen in the case study where it failed to identify the correct visual evidence and got caught in a "thinking trap."

## Limitations

- The Dyn-VQA dataset is newly introduced with no external validation of its difficulty or representativeness.
- The self-adaptive planning mechanism relies heavily on the planning agent's ability to generate useful sub-questions, which may not generalize to domains outside visual question answering.
- Limited testing conditions and no ablation on planner quality affect claims about scalability and robustness to real-world complexity.

## Confidence

- High: Core claim that OmniSearch outperforms existing mRAG methods on Dyn-VQA (supported by direct comparisons and clear metrics)
- Medium: Mechanism claims about adaptive planning and error mitigation (well-explained but not fully validated across diverse scenarios)
- Low: Claims about scalability and robustness to real-world complexity (limited testing conditions and no ablation on planner quality)

## Next Checks

1. Test OmniSearch on established multimodal QA datasets (e.g., OK-VQA, A-OKVQA) to assess generalizability.
2. Perform ablation studies removing the planning agent or feedback loop to isolate their contributions.
3. Conduct human evaluation of sub-question quality and retrieval relevance to validate the adaptive planning mechanism.