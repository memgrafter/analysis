---
ver: rpa2
title: Expert Proximity as Surrogate Rewards for Single Demonstration Imitation Learning
arxiv_id: '2402.01057'
source_url: https://arxiv.org/abs/2402.01057
tags:
- expert
- reward
- learning
- agent
- tdil
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of single-demonstration imitation
  learning (IL), where an agent must learn from only one expert trajectory without
  access to the ground truth reward function. The authors propose TDIL (Transition
  Discriminator-based IL), which addresses the issue of sparse rewards by introducing
  a denser surrogate reward function that considers environmental dynamics.
---

# Expert Proximity as Surrogate Rewards for Single Demonstration Imitation Learning

## Quick Facts
- arXiv ID: 2402.01057
- Source URL: https://arxiv.org/abs/2402.01057
- Reference count: 40
- One-line primary result: TDIL achieves expert-level performance in single-demonstration IL using transition discriminator-based surrogate rewards

## Executive Summary
This paper addresses the challenge of single-demonstration imitation learning by proposing TDIL, which introduces denser surrogate rewards through a transition discriminator. The method learns to distinguish valid from invalid state transitions in the environment, providing rewards for states proximal to expert states. Experiments on five MuJoCo benchmarks and the Adroit Door environment demonstrate that TDIL outperforms existing IL approaches and achieves expert-level performance. The proposed relative returns also enable effective blind model selection without requiring ground truth rewards.

## Method Summary
TDIL trains a transition discriminator to differentiate between valid and non-valid state transitions, computing surrogate rewards that encourage agents to navigate toward states proximal to expert states. The method combines basic IRL rewards (GAIL) with these denser TDIL rewards through an aggregated reward function. A key innovation is the use of relative returns (normalized by expert returns) as a blind model selection metric, which proves robust to potential inaccuracies in the transition discriminator. The approach is tested on standard MuJoCo benchmarks and the challenging Adroit Door environment, demonstrating superior performance compared to existing single-demonstration IL methods.

## Key Results
- TDIL achieves expert-level performance on HalfCheetah-v3, Hopper-v3, Walker2d-v3, and Ant-v3 environments
- The method outperforms baseline IL approaches including GAIL, DDPGfD, and CFIL on all tested benchmarks
- Relative returns provide effective blind model selection, showing strong correlation with ground truth performance without requiring reward access

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse rewards from expert trajectories can be densified by leveraging environmental transition dynamics
- Mechanism: TDIL uses a transition discriminator trained on agent interactions to estimate reachability between states, assigning positive rewards to states that can reach expert states within one transition
- Core assumption: States that can reach expert states within one action are meaningfully proximal for learning expert behavior
- Evidence anchors: [abstract] transition discriminator to differentiate valid and non-valid transitions; [section 4.3] maximum likelihood training with binary cross-entropy loss
- Break condition: Sparse connectivity (isolated islands of states) may prevent discriminator from generalizing reachability

### Mechanism 2
- Claim: Aggregating basic IRL rewards with TDIL surrogate rewards improves stability and performance
- Mechanism: The algorithm computes aggregated reward Ragg = βRIRL + (1-β)RTDIL, combining GAIL's optimality constraint with denser TDIL guidance
- Core assumption: Weighted combination of dense and sparse reward signals preserves optimality while accelerating learning
- Evidence anchors: [section 4.4] aggregated reward definition; [section 5.2] effective reward mechanism for SAC agents
- Break condition: β too low causes drift from expert states; β too high loses densification benefit

### Mechanism 3
- Claim: Relative returns can serve as blind model selection metric without ground truth rewards
- Mechanism: Uses rrelative = rraw agent / rraw expert normalization, which cancels out transition discriminator inaccuracies
- Core assumption: Errors affect agent and expert rewards proportionally, making their ratio robust
- Evidence anchors: [section 4.6] relative return definition; [section 5.3] clear positive correlation with ground truth return
- Break condition: Significant trajectory length differences between agent and expert may mislead normalization

## Foundational Learning

- Concept: Inverse Reinforcement Learning (IRL)
  - Why needed here: TDIL learns surrogate reward function without access to true reward
  - Quick check question: In IRL, what is the relationship between the optimal indicator Ot and the expert policy πe?

- Concept: Markov Decision Process (MDP) state transition dynamics
  - Why needed here: Transition discriminator relies on understanding valid state transitions to assign reachability rewards
  - Quick check question: How does the transition function P(st+1 | st, at) affect the definition of expert proximity in TDIL?

- Concept: Generative Adversarial Imitation Learning (GAIL) and f-divergence minimization
  - Why needed here: TDIL uses GAIL as basic IRL component and builds on f-divergence minimization framework
  - Quick check question: Why might adversarial training be unstable in single-demonstration IL settings?

## Architecture Onboarding

- Component map: SAC agent (policy + critic) -> Transition discriminator Dϕ -> Target discriminator ˆD -> Replay buffer B -> Expert trajectory τe

- Critical path:
  1. Agent interacts with environment, stores (st, at, st+1) in B
  2. Update Dϕ using B and τe with binary cross-entropy loss
  3. Soft-update ˆD from Dϕ for stability
  4. Sample batch from B and τe, compute rewards using ˆD
  5. Update SAC agent with aggregated rewards
  6. Optionally update SAC with BC loss for expert state optimality

- Design tradeoffs:
  - Dense rewards vs. preserving optimality: β balances RIRL and RTDIL
  - Transition discriminator accuracy vs. training cost: Requires large interaction data
  - Blind selection metric: Relative returns vs. raw returns

- Failure signatures:
  - Low transition discriminator accuracy → sparse rewards persist
  - NaN in CFIL baseline → adversarial instability in some environments
  - Poor performance on HumanCheetah-v3 without hard negatives → need for hard negative samples in complex tasks

- First 3 experiments:
  1. Train TDIL on HalfCheetah-v3 with β=0.9, monitor reward aggregation and convergence
  2. Evaluate transition discriminator accuracy on positive vs. hard negative samples across environments
  3. Compare blind model selection using relative returns vs. raw returns on Walker2d-v3

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TDIL performance scale with number of expert demonstrations beyond single-demonstration setting?
- Basis in paper: [explicit] Experiments with multiple demonstrations show expert-level performance, but impact of more demonstrations is not thoroughly analyzed
- Why unresolved: No detailed analysis of how increasing expert demonstrations affects TDIL performance
- What evidence would resolve it: Experiments with 2, 5, 10 demonstrations comparing TDIL against baselines

### Open Question 2
- Question: What is the impact of different state representations on TDIL performance, particularly in high-dimensional environments?
- Basis in paper: [inferred] Paper discusses challenges of geometric distances in high-dimensional spaces and suggests TDIL considers environmental dynamics, but doesn't explore state representation effects
- Why unresolved: No investigation of how different state representations might enhance or hinder TDIL effectiveness
- What evidence would resolve it: Experiments with raw states, learned embeddings, hierarchical representations comparing TDIL performance

### Open Question 3
- Question: How does TDIL perform in environments with complex or non-Markovian dynamics?
- Basis in paper: [explicit] TDIL considers environmental dynamics through transition discriminator, but performance in non-Markovian settings is not explored
- Why unresolved: No empirical evidence of TDIL's effectiveness in partial observability or long-term dependency scenarios
- What evidence would resolve it: Testing in POMDPs or environments with delayed rewards comparing TDIL against baselines

## Limitations
- Reliance on large interaction data for transition discriminator training may limit scalability to real-world robotics
- Assumes stationary, fully observable environment dynamics that may not hold in dynamic or partially observable settings
- Hyperparameter β requires tuning for each environment without clear selection guidance

## Confidence

High confidence in core algorithmic framework and experimental results on standard benchmarks
Medium confidence in blind model selection claim, needs further validation across diverse tasks
Low confidence in method's robustness to distribution shifts and partial observability

## Next Checks

1. Test TDIL's performance when expert demonstrations contain noise or distribution shifts from training conditions
2. Evaluate transition discriminator accuracy and reward quality on environments with sparse connectivity or isolated state islands
3. Compare blind model selection effectiveness using relative returns versus alternative metrics like behavioral cloning loss across broader task set