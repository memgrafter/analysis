---
ver: rpa2
title: 'ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models'
arxiv_id: '2406.16635'
source_url: https://arxiv.org/abs/2406.16635
tags:
- pruning
- sparsity
- criteria
- shadowllm
- predictor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ShadowLLM introduces a predictor-based approach to contextual sparsity
  in large language models (LLMs), where sparsity patterns are dynamically determined
  based on input context rather than fixed. The key innovation lies in using a single
  predictor at the first layer to model the entire model's sparsity pattern, improving
  latency by 20.6% over per-layer predictors.
---

# ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models

## Quick Facts
- **arXiv ID:** 2406.16635
- **Source URL:** https://arxiv.org/abs/2406.16635
- **Authors:** Yash Akhauri; Ahmed F AbouElhamayed; Jordan Dotzel; Zhiru Zhang; Alexander M Rush; Safeen Huda; Mohamed S Abdelfattah
- **Reference count:** 17
- **Primary result:** Achieves up to 20% speed-up over DejaVu with 20.6% performance improvement using single first-layer predictor, while maintaining accuracy across 30B parameter models.

## Executive Summary
ShadowLLM introduces a predictor-based approach to contextual sparsity in large language models, where sparsity patterns are dynamically determined based on input context rather than fixed. The key innovation lies in using a single predictor at the first layer to model the entire model's sparsity pattern, improving latency by 20.6% over per-layer predictors. Additionally, it employs gradient-informed pruning criteria (plainact) that outperform magnitude-based methods, achieving over 15% improvement in end-to-end accuracy across tasks like WikiText2 perplexity and zero-shot evaluations on OPT-13B. The method is validated on models up to 30 billion parameters, demonstrating significant gains in both accuracy and performance without compromising model quality.

## Method Summary
ShadowLLM employs a single predictor placed at the first layer of the transformer to predict sparsity patterns for the entire model. The predictor takes the first layer's attention output and generates binary masks for attention heads and neurons across all subsequent layers. During training, the predictor is trained using MSE loss on ~2,720 input-output examples from downstream tasks. The plainact pruning criterion uses gradient information to assess neuron importance by computing the expected sensitivity of the model on the loss if a head or neuron is removed. Global pruning is used across all layers rather than per-layer pruning. The method is evaluated on models ranging from 1.3B to 30B parameters, measuring perplexity on WikiText2 and zero-shot accuracy on seven downstream tasks at 50% sparsity.

## Key Results
- Achieves up to 20% speed-up over state-of-the-art DejaVu framework
- Single first-layer predictor improves performance by 20.6% without affecting accuracy
- Gradient-informed plainact pruning criteria achieve over 15% improvement in end-to-end accuracy
- Validated on models up to 30 billion parameters across multiple tasks
- Maintains model quality while significantly reducing latency

## Why This Works (Mechanism)

### Mechanism 1
Using a single predictor at the first layer improves model performance by 20.6% without affecting accuracy. The first layer's attention output contains sufficient information to predict the entire model's sparsity pattern, eliminating the overhead of per-layer predictors and reducing total FLOPs by 20%. This simplifies the inference pipeline and improves execution latency.

### Mechanism 2
Gradient-informed pruning criteria (plainact) outperform magnitude-based methods, achieving over 15% improvement in end-to-end accuracy. plainact measures the expected sensitivity of the model on the loss if a head or neuron is removed, using both activation and gradient information. This provides a more accurate assessment of neuron importance compared to simple magnitude-based criteria.

### Mechanism 3
Global pruning outperforms local (per-layer) pruning strategies. Global pruning allows for unbalanced pruning across layers, accommodating the varying importance of different layers. This prevents over-parameterization in some layers and the pruning of more important heads from under-parameterized layers.

## Foundational Learning

- **Concept:** Pruning criteria for neural networks
  - Why needed here: ShadowLLM evaluates several pruning criteria to find the most effective method for dynamically pruning LLMs
  - Quick check question: What is the difference between activation-based and gradient-based pruning criteria?

- **Concept:** Contextual sparsity in LLMs
  - Why needed here: ShadowLLM introduces a predictor-based approach to contextual sparsity, where sparsity patterns are dynamically determined based on input context
  - Quick check question: Why is contextual sparsity important in LLMs compared to static sparsity?

- **Concept:** Predictor design for sparsity patterns
  - Why needed here: ShadowLLM uses a single predictor at the first layer to model the entire model's sparsity pattern, improving performance and simplifying the inference pipeline
  - Quick check question: How does a unified predictor at the first layer differ from per-layer predictors in terms of performance and complexity?

## Architecture Onboarding

- **Component map:** Input layer -> First layer attention output -> Single predictor (ShadowLLM) -> Sparsity pattern prediction -> Dynamic pruning of attention heads and neurons

- **Critical path:** Input → First layer attention output → Single predictor → Sparsity pattern prediction → Dynamic pruning

- **Design tradeoffs:**
  - Single predictor vs. per-layer predictors: Simplified inference pipeline and improved performance vs. potentially more accurate local predictions
  - Gradient-informed pruning criteria vs. magnitude-based criteria: More accurate neuron importance assessment vs. potentially more computationally expensive

- **Failure signatures:**
  - Poor accuracy: The predictor is not accurately modeling the sparsity pattern, or the pruning criteria are not effective
  - Increased latency: The predictor is introducing significant overhead, or the dynamic pruning is not efficient
  - Model degradation: The sparsity patterns are too aggressive, leading to significant loss of model quality

- **First 3 experiments:**
  1. Implement the single predictor at the first layer and compare its performance to per-layer predictors
  2. Evaluate different pruning criteria (plainact vs. magnitude-based) and their impact on model accuracy
  3. Test global pruning vs. local pruning strategies and their effect on perplexity and downstream task performance

## Open Questions the Paper Calls Out

### Open Question 1
How would the predictor performance change if the predictor were placed at a later layer instead of the first layer? The paper shows that a single predictor at the first layer achieves good performance, but doesn't explore other placements.

### Open Question 2
Would using attention scores instead of activation magnitudes improve the pruning criteria? The paper explores various pruning criteria but doesn't investigate using attention scores as a criterion.

### Open Question 3
How does the performance scale when applying ShadowLLM to models larger than 30 billion parameters? The paper validates up to 30 billion parameters but doesn't explore larger models.

### Open Question 4
Would incorporating second-order gradient information improve the pruning criteria? The paper explores first-order methods and mentions Hessian-based methods but doesn't fully investigate second-order gradient information.

## Limitations

- Evaluation framework conflates predictor accuracy with pruning criterion performance, making it difficult to isolate which component drives claimed improvements
- Validation limited to models up to 30B parameters, leaving uncertainty about scalability to frontier models
- Computational overhead of plainact criterion during training is not quantified, critical for practical adoption

## Confidence

- **High confidence:** The 20.6% latency improvement from using a single first-layer predictor is well-supported by the architectural simplification and FLOPs reduction claim
- **Medium confidence:** The 15% accuracy improvement from plainact pruning criteria is supported by theoretical justification and ablation studies, but aggregated metric presentation makes precise attribution difficult
- **Medium confidence:** The superiority of global pruning over local pruning is demonstrated through perplexity comparisons, though explanation of varying layer importance is somewhat speculative

## Next Checks

1. **Ablation of predictor vs pruning criteria:** Run experiments with plainact criterion using per-layer predictors versus the single first-layer predictor with magnitude pruning to isolate which component drives accuracy improvements

2. **Scalability validation:** Test ShadowLLM on 70B+ parameter models to verify the claimed benefits scale to frontier model sizes, particularly examining whether the first-layer attention output remains sufficient for whole-model sparsity prediction

3. **Computational overhead measurement:** Quantify the additional training FLOPs required for plainact's gradient computations during the calibration phase and compare this to the inference-time savings to assess net efficiency gains