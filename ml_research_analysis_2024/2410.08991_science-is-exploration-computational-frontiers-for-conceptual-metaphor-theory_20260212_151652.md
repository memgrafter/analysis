---
ver: rpa2
title: 'Science is Exploration: Computational Frontiers for Conceptual Metaphor Theory'
arxiv_id: '2410.08991'
source_url: https://arxiv.org/abs/2410.08991
tags:
- basic
- metaphors
- metaphor
- meaning
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study explores whether large language models (LLMs) can identify\
  \ and explain conceptual metaphors in natural language, a challenging task requiring\
  \ analogical reasoning and encyclopedic knowledge. Using the MIP (Metaphor Identification\
  \ Procedure) annotation guidelines, the researchers prompt three models\u2014gpt-3.5-turbo,\
  \ gpt-4-turbo, and gpt-4o\u2014to detect metaphorical language in datasets from\
  \ the Wall Street Journal and Lakoff and Johnson's foundational work."
---

# Science is Exploration: Computational Frontiers for Conceptual Metaphor Theory

## Quick Facts
- arXiv ID: 2410.08991
- Source URL: https://arxiv.org/abs/2410.08991
- Reference count: 37
- Key outcome: LLMs can identify and explain conceptual metaphors with accuracy above chance, particularly excelling at simple metaphors while struggling with subtle and multi-word metaphors

## Executive Summary
This study investigates whether large language models can identify and explain conceptual metaphors in natural language, a task requiring analogical reasoning and encyclopedic knowledge. Using the Metaphor Identification Procedure (MIP) annotation guidelines, the researchers prompt three models—gpt-3.5-turbo, gpt-4-turbo, and gpt-4o—to detect metaphorical language in datasets from the Wall Street Journal and Lakoff and Johnson's foundational work. Results show that LLMs perform better than chance in identifying metaphors, with gpt-4o excelling in accuracy and depth. However, they struggle with subtle metaphors, multi-word units, and function words like prepositions. The findings suggest LLMs can apply linguistic procedures designed for humans and are promising tools for large-scale metaphor research, though challenges remain in nuanced understanding and cultural context.

## Method Summary
The researchers collected two datasets: the TroFi dataset containing 3,736 sentences from Wall Street Journal with metaphorical and literal annotations, and the MWLB dataset with 544 sentences from Lakoff and Johnson's work. They designed a prompt based on MIP guidelines and applied it to three LLMs (gpt-3.5-turbo, gpt-4-turbo, gpt-4o) via OpenAI API. The models processed text word-by-word, identifying metaphorical usage and providing basic meaning explanations. Outputs were parsed and evaluated against ground truth annotations using accuracy, precision, and recall metrics. Nuclear sampling with top_p=0.1 ensured high-probability outputs.

## Key Results
- LLMs achieve accuracy above chance in metaphor identification, with gpt-4o showing the highest performance
- Models struggle with subtle metaphors and multi-word expressions, particularly container metaphors
- LLMs can provide basic meaning explanations for metaphorical words, though quality varies by model and metaphor complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can apply the Metaphor Identification Procedure (MIP) despite lacking explicit "basic meaning" representations.
- Mechanism: LLMs infer basic meanings through contextualized embeddings and analogical reasoning over training data, effectively approximating the semantic contrast required by MIP Step 3b.
- Core assumption: Basic meanings are implicitly encoded in the distributional statistics of the training corpus, even if not explicitly represented.
- Evidence anchors: [abstract] "LLMs are able to apply procedural guidelines designed for human annotators"; [section 3] "we transform it into a prompt for instruction-tuned models" and show models can execute Step 3b despite the lack of direct representation.

### Mechanism 2
- Claim: Instruction-tuned models can perform Chain-of-Thought reasoning for metaphor annotation tasks.
- Mechanism: The structured prompt with examples and parenthetical explanations forces the model to explicitly articulate reasoning steps, mimicking human annotation procedures.
- Core assumption: The model's internal reasoning process can be made explicit through carefully designed prompts without changing the underlying capability.
- Evidence anchors: [section 3] "These expressions were introduced by the gpt-4o model itself during initial exploration and were then included in the prompt because they force the model to provide explanations"; [section 5] "The models are frequently able to recognize when a word is being used metaphorically and often provide a correct basic meaning".

### Mechanism 3
- Claim: LLMs encode cultural and contextual knowledge sufficient for metaphor understanding.
- Mechanism: The models leverage their training on diverse text corpora to recognize culturally specific metaphorical mappings without explicit cultural programming.
- Core assumption: Metaphoricity is learnable from distributional patterns in natural language without explicit cultural knowledge representation.
- Evidence anchors: [section 5] "they are ever able to do so" in identifying culturally nuanced metaphors like "Vietnam" as a metaphor; [section 5] "identifying and explaining both of these metaphors requires a nuanced understanding of both the semantics of the sentence and the cultural context".

## Foundational Learning

- Concept: Metaphor Identification Procedure (MIP)
  - Why needed here: The entire study operationalizes MIP for LLM evaluation; understanding its steps is crucial for interpreting results
  - Quick check question: What are the four steps of MIP and which step poses the greatest challenge for LLMs?

- Concept: Transformer attention mechanisms
  - Why needed here: The paper explicitly maps MIP steps to transformer operations (Step 1 to attention, Step 2 to tokenization)
  - Quick check question: How does the attention mechanism in transformers parallel Step 1 of MIP?

- Concept: Instruction tuning and in-context learning
  - Why needed here: The methodology relies on prompting LLMs with examples and structured tasks rather than fine-tuning
  - Quick check question: What role do the example annotations play in the model's ability to execute MIP?

## Architecture Onboarding

- Component map: Text preprocessing → LLM API call (gpt-3.5-turbo/gpt-4-turbo/gpt-4o) → Response parsing → Annotation extraction → Evaluation against ground truth

- Critical path: Prompt → LLM API call → Response parsing → Annotation extraction → Evaluation against ground truth

- Design tradeoffs:
  - Using instruction-tuned models vs. fine-tuning for specific metaphor detection
  - Word-by-word annotation vs. phrase-level analysis (affects container metaphor detection)
  - High-probability token sampling (top_p=0.1) for reliability vs. diversity

- Failure signatures:
  - Missing annotations for function words (prepositions, pronouns)
  - Inconsistent formatting in outputs
  - Truncation of responses mid-analysis
  - Over-labeling literal words as metaphorical

- First 3 experiments:
  1. Test basic MIP execution on simple metaphors (e.g., "demolish an argument") to verify prompt effectiveness
  2. Evaluate performance on the full TroFi dataset to measure precision/recall on literal vs. metaphorical usage
  3. Analyze MWLB subset with detailed qualitative evaluation of metaphor identification and basic meaning provision

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs encode and represent the "basic meanings" of words that MIP relies on for metaphor identification?
- Basis in paper: [explicit] The paper notes that basic meanings are not necessarily the most frequent meanings of words, yet LLMs still manage to identify metaphors, suggesting they encode this information somehow.
- Why unresolved: The paper observes that LLMs perform well at metaphor identification but doesn't investigate where or how they store this knowledge about basic meanings.
- What evidence would resolve it: Probing experiments or model analysis that reveals how basic meanings are represented in LLM internal representations.

### Open Question 2
- Question: Can open-source LLMs perform metaphor annotation with comparable accuracy to proprietary models like GPT-4?
- Basis in paper: [inferred] The authors suggest exploring open-source models' ability to annotate for conceptual metaphor as a future research direction, implying this hasn't been tested yet.
- Why unresolved: The study only tested OpenAI's proprietary models, leaving open the question of whether open-source alternatives can achieve similar performance.
- What evidence would resolve it: Direct comparison of metaphor annotation performance between open-source and proprietary models on the same datasets.

### Open Question 3
- Question: What is the relationship between metaphor identification performance and a model's training data or size?
- Basis in paper: [inferred] The authors tested different model sizes (3.5-turbo, 4-turbo, 4o) and observed performance differences, suggesting a potential relationship worth systematic investigation.
- Why unresolved: While the paper shows performance differences across model sizes, it doesn't analyze how training data composition or model size specifically affects metaphor understanding.
- What evidence would resolve it: Systematic studies varying model size and training data characteristics while measuring metaphor identification performance.

## Limitations

- Study focuses on specific annotation framework (MIP) and English-language datasets, limiting generalizability to other languages and cultural contexts
- Word-by-word annotation approach may underestimate true capabilities for detecting container metaphors and multi-word expressions
- Performance varies significantly across models, with even the best model struggling with subtle metaphors and function words

## Confidence

- High confidence: LLMs can execute procedural guidelines like MIP for metaphor identification
- Medium confidence: LLMs can provide basic meaning explanations for metaphorical words
- Medium confidence: Performance differences between model versions reflect genuine capability differences

## Next Checks

1. Test model performance on additional metaphor datasets with different linguistic structures and cultural contexts to assess generalizability
2. Implement phrase-level annotation protocols to evaluate detection of container metaphors and multi-word expressions
3. Compare instruction-tuned prompting against fine-tuning approaches for metaphor detection tasks to determine optimal methodology