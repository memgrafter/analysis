---
ver: rpa2
title: Hierarchical Blockmodelling for Knowledge Graphs
arxiv_id: '2408.15649'
source_url: https://arxiv.org/abs/2408.15649
tags:
- cpqr
- knowledge
- community
- graph
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first application of stochastic blockmodels
  to knowledge graphs for hierarchical entity clustering. The proposed method uses
  a nonparametric Bayesian approach that combines the Nested Chinese Restaurant Process
  and Stick Breaking Process to induce a hierarchy of entity communities.
---

# Hierarchical Blockmodelling for Knowledge Graphs

## Quick Facts
- arXiv ID: 2408.15649
- Source URL: https://arxiv.org/abs/2408.15649
- Authors: Marcin Pietrasik; Marek Reformat; Anna Wilbik
- Reference count: 40
- One-line primary result: First application of stochastic blockmodels to knowledge graphs for hierarchical entity clustering with ARI 0.34-0.83 and NMI 0.53-0.93 across hierarchy levels

## Executive Summary
This paper introduces the first application of stochastic blockmodels to knowledge graphs for hierarchical entity clustering. The proposed method uses a nonparametric Bayesian approach that combines the Nested Chinese Restaurant Process and Stick Breaking Process to induce a hierarchy of entity communities. The model learns both the hierarchical structure and entity assignments simultaneously through a collapsed Gibbs sampling scheme. Quantitative evaluation on synthetic and real-world datasets shows the method produces coherent hierarchies with strong clustering quality metrics.

## Method Summary
The approach integrates the Nested Chinese Restaurant Process (nCRP) with the Stick Breaking Process (SBP) to enable simultaneous learning of hierarchical structure and entity assignments in knowledge graphs. The nCRP generates tree structure by sampling entity paths through communities, while the SBP determines the level within each path where an entity belongs. Collapsed Gibbs sampling with marginalization of community relations and level memberships provides tractable inference through Bernoulli-Beta conjugacy for community relations and multinomial-stick conjugacy for level memberships. The method restricts community relations to sibling groups to preserve hierarchical structure while maintaining computational tractability.

## Key Results
- Adjusted Rand Index scores ranging from 0.34 to 0.83 across different hierarchy levels
- Normalized Mutual Information scores from 0.53 to 0.93 across hierarchy levels
- Coherent hierarchies produced on synthetic binary tree and real-world datasets (FB15k-237, WikiData)
- Learned community relations align with expected knowledge graph structures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The integration of the Nested Chinese Restaurant Process (nCRP) with the Stick Breaking Process (SBP) enables simultaneous learning of both the hierarchical structure and entity assignments in knowledge graphs.
- Mechanism: The nCRP generates the tree structure by sampling entity paths through communities, while the SBP determines the level within each path where an entity belongs. This dual sampling allows the model to discover both the hierarchy and community memberships without prior constraints.
- Core assumption: The hierarchical structure can be represented as a tree where entities belong to communities at specific levels along their sampled paths.
- Evidence anchors:
  - [abstract] "Specifically, this is achieved by the integration of the Nested Chinese Restaurant Process and the Stick Breaking Process into the generative model."
  - [section 4.1.1] "Paths define the tree structure over the community hierarchy by sampling from the nCRP... Level memberships are drawn from the stick breaking process, ai ~ Stick(µ, σ)"
  - [corpus] Weak evidence - no corpus neighbors directly discuss this specific integration of nCRP and SBP

### Mechanism 2
- Claim: Collapsed Gibbs sampling with marginalization of community relations and level memberships provides tractable inference for the model.
- Mechanism: By leveraging the Bernoulli-Beta conjugacy for community relations and the multinomial-stick conjugacy for level memberships, these variables are integrated out analytically, reducing the dimensionality of the sampling space and improving convergence speed.
- Core assumption: The conjugate priors (Beta for Bernoulli likelihood, stick breaking for multinomial) have closed-form posterior distributions that can be computed exactly.
- Evidence anchors:
  - [abstract] "The model learns both the hierarchical structure and entity assignments simultaneously through a collapsed Gibbs sampling scheme."
  - [section 4.2] "Collapsed Gibbs sampling refers to an extension of Gibbs sampling in which a subset of model variables are marginalized over and therefore do not need to be sampled directly."
  - [corpus] Weak evidence - no corpus neighbors discuss collapsed Gibbs sampling in this context

### Mechanism 3
- Claim: Restricting community relations to sibling groups preserves the hierarchical structure while maintaining computational tractability.
- Mechanism: Community relations are only modeled between communities sharing the same parent (sibling groups), with coarsening procedures applied when entities belong to different sibling groups. This prevents arbitrary interactions between distant communities in the hierarchy.
- Core assumption: The meaningful interactions in a knowledge graph occur primarily between entities in related communities rather than between arbitrary communities at different hierarchy levels.
- Evidence anchors:
  - [section 4.1.2] "Only the community relations between communities in the same sibling group are modelled in our approach."
  - [section 4.1.2] "This approach differs from the Multiscale Community Blockmodel in that while there are restricted entries in C, these values are never accessed."
  - [corpus] Weak evidence - no corpus neighbors discuss this specific sibling-group restriction approach

## Foundational Learning

- Concept: Chinese Restaurant Process (CRP)
  - Why needed here: The CRP is the foundational building block for the nCRP and provides the preferential attachment mechanism for community assignment.
  - Quick check question: What is the probability of seating the nth patron at an occupied table versus a new table in the CRP?
  - Answer: P(occupied) = (# at table)/(n + γ), P(new) = γ/(n + γ)

- Concept: Dirichlet Process and its variants
  - Why needed here: The CRP and SBP are both instances of Dirichlet processes, which provide the theoretical foundation for non-parametric Bayesian modeling of infinite community structures.
  - Quick check question: What is the key property of Dirichlet processes that makes them useful for non-parametric Bayesian modeling?
  - Answer: They allow for an infinite number of mixture components (communities) that can grow as needed with the data.

- Concept: Conjugate priors and marginalization
  - Why needed here: The collapsed Gibbs sampling approach relies on conjugate priors to enable analytic marginalization of certain variables, which is crucial for tractable inference.
  - Quick check question: What makes a prior "conjugate" to a likelihood, and why is this useful for marginalization?
  - Answer: A prior is conjugate when the posterior distribution is in the same family as the prior, allowing for closed-form integration.

## Architecture Onboarding

- Component map: Entity path sampling (nCRP) -> Level indicator sampling (multinomial from stick breaking) -> Community relation modeling (Beta-Bernoulli conjugacy) -> Likelihood computation
- Critical path: The inference process flows through path sampling → level indicator sampling → community relation marginalization → likelihood computation. Path sampling is the most computationally expensive component.
- Design tradeoffs: The hierarchical structure assumption trades flexibility for interpretability and computational tractability. The sibling-group restriction on community relations limits the model's ability to capture long-range dependencies.
- Failure signatures: Poor clustering quality at lower hierarchy levels, failure to converge in the Gibbs sampler, or communities that don't align with semantic expectations in the knowledge graph.
- First 3 experiments:
  1. Run the model on the synthetic binary tree dataset and verify that it correctly recovers the known hierarchy structure.
  2. Test the collapsed Gibbs sampling implementation by monitoring log likelihood convergence across iterations.
  3. Evaluate the effect of different hyperparameter settings (γ, µ, σ, λ, η) on the induced hierarchy structure and clustering quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of hyperparameters (γ, µ, σ, λ, η) affect the scalability of the model and the quality of the induced hierarchies?
- Basis in paper: [explicit] The paper discusses the role of hyperparameters in controlling the branching factor of the tree and the density of the generated knowledge graph. It also mentions that the inference scheme has a time complexity of O(|E|²|R|L + |E|³|R|L) and suggests future work on more scalable inference schemes.
- Why unresolved: The paper does not provide a systematic study of the impact of hyperparameters on scalability and clustering quality. It only mentions that hyperparameter tuning is "fickle" and that the choice of hyperparameters can influence the prior distributions of the model.
- What evidence would resolve it: Empirical studies evaluating the model's performance and scalability under different hyperparameter settings on various datasets, including larger knowledge graphs.

### Open Question 2
- Question: Can the model handle knowledge graphs with a large number of predicates (|R|)?
- Basis in paper: [inferred] The paper mentions that the time complexity of the inference scheme is O(|E|²|R|L + |E|³|R|L), which suggests that the model's scalability is affected by the number of predicates. The evaluation is conducted on datasets with a limited number of predicates (2, 79, and 6).
- Why unresolved: The paper does not provide evidence of the model's performance on knowledge graphs with a large number of predicates. It only mentions the potential challenge of scaling to large knowledge bases.
- What evidence would resolve it: Empirical studies evaluating the model's performance on knowledge graphs with a large number of predicates, comparing its scalability and clustering quality to other methods.

### Open Question 3
- Question: How does the model compare to other hierarchy induction methods that use knowledge graph embeddings as an intermediate representation?
- Basis in paper: [explicit] The paper mentions that knowledge graph embedding methods can be used in conjunction with hierarchical clustering methods to induce hierarchies. It also compares the model's performance to embedding and clustering methods (RDF2VEC and TransE) used in conjunction.
- Why unresolved: The paper does not provide a comprehensive comparison of the model to other hierarchy induction methods that use knowledge graph embeddings. It only compares the model to a limited set of baselines.
- What evidence would resolve it: A thorough comparison of the model to other hierarchy induction methods that use knowledge graph embeddings, evaluating their performance on various datasets and tasks.

## Limitations

- Computational complexity of O(|E|²|R|L + |E|³|R|L) becomes prohibitive for large-scale knowledge graphs
- Assumes hierarchical tree structures, potentially missing non-tree relationships
- Sibling-group restriction on community relations may limit ability to capture long-range dependencies

## Confidence

- Quantitative results (ARI 0.34-0.83, NMI 0.53-0.93) demonstrate coherent hierarchies on both synthetic and real datasets
- Confidence: Medium - limited to datasets with clear hierarchical structures
- Interpretability of learned community relations supported qualitatively but requires more systematic validation
- Computational complexity presents scalability concerns for large knowledge graphs

## Next Checks

1. Evaluate the model's performance on datasets with non-tree hierarchical structures to assess its limitations in handling complex graph topologies.
2. Conduct ablation studies to quantify the impact of the sibling-group restriction on community relations by comparing against models that allow arbitrary community interactions.
3. Test the scalability by applying the model to larger knowledge graphs and measuring how the O(|E|²|R|L + |E|³|R|L) complexity affects runtime and memory usage.