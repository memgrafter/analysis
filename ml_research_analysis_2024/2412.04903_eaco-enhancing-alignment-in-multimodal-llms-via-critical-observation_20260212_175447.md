---
ver: rpa2
title: 'EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation'
arxiv_id: '2412.04903'
source_url: https://arxiv.org/abs/2412.04903
tags:
- response
- score
- image
- critic
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a critic-based preference alignment method to
  enhance reasoning and reduce hallucinations in multimodal large language models.
  The approach involves self-generating multiple responses, using a fine-tuned critic
  model to evaluate and score them across multiple dimensions, and selecting preferred/non-preferred
  pairs for Direct Preference Optimization (DPO).
---

# EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation

## Quick Facts
- arXiv ID: 2412.04903
- Source URL: https://arxiv.org/abs/2412.04903
- Authors: Yongxin Wang; Meng Cao; Haokun Lin; Mingfei Han; Liang Ma; Jin Jiang; Yuhao Cheng; Xiaodan Liang
- Reference count: 40
- One-line primary result: Achieves 8.5% average improvement over baseline and reduces hallucinations by 65.6% on HallusionBench

## Executive Summary
EACO introduces a critic-based preference alignment method for multimodal large language models that addresses reasoning and hallucination challenges. The approach uses a fine-tuned critic model to evaluate multiple self-generated responses across five dimensions, selecting preferred and non-preferred pairs for Direct Preference Optimization (DPO). An additional supervised fine-tuning stage with model-generated captions further refines the model. Experiments demonstrate significant improvements in reasoning ability and hallucination reduction while requiring only 5,000 images for preference data generation.

## Method Summary
EACO employs a multi-stage approach beginning with training a critic model on a scoring evaluation instruction-tuning dataset. The current MLLM generates multiple responses for unlabeled images, which the critic evaluates across five dimensions: Relevance, Substantial Coverage, Basic Elements, Clarity, and Quality. Responses with the highest and lowest scores are selected as preferred and non-preferred pairs for DPO training. The process concludes with enhanced supervised fine-tuning using model-generated captions. The entire pipeline requires only 5,000 unlabeled images, making it more economical than methods requiring human-annotated preference data.

## Key Results
- Achieves 8.5% average improvement over LLaVA-v1.6-Mistral-7B across multiple benchmarks
- Reduces hallucinations by 65.6% on HallusionBench
- Improves reasoning ability by 21.8% on MME-Cognition benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EACO improves MLLM performance by leveraging self-generated preference data evaluated through a fine-tuned critic model, avoiding costly human or proprietary model feedback.
- Mechanism: The critic model scores generated responses across multiple dimensions (Relevance, Substantial Coverage, Basic Elements, Clarity, and Quality). Responses with the highest and lowest scores are selected as preferred and non-preferred pairs, which are then used in Direct Preference Optimization (DPO) to align the model toward higher-quality outputs.
- Core assumption: A fine-tuned critic model can effectively evaluate and rank MLLM responses without access to human-annotated data or proprietary models like GPT-4V.
- Break condition: If the critic model fails to accurately evaluate responses, the preference pairs selected for DPO would not represent true quality differences, leading to ineffective or even harmful model updates.

### Mechanism 2
- Claim: EACO reduces hallucinations by improving the model's ability to ground responses in visual evidence through multi-dimensional evaluation.
- Mechanism: The critic model assesses responses for visual faithfulness and relevance to the image content. By selecting and optimizing toward responses that score higher on these dimensions, the model learns to generate outputs that are more faithful to the visual input.
- Core assumption: Evaluating responses across multiple dimensions (not just helpfulness) captures different aspects of hallucination, allowing the model to learn more nuanced distinctions between accurate and hallucinated content.
- Break condition: If the critic's evaluation criteria do not align with true hallucination detection, or if the model learns to game the evaluation metrics without actually improving visual grounding, hallucination reduction may not occur.

### Mechanism 3
- Claim: EACO enhances reasoning ability by using self-generated preference data that captures complex reasoning chains and correct logical steps.
- Mechanism: The critic model evaluates responses for logical coherence and completeness of reasoning. By selecting responses that demonstrate better reasoning and using them in DPO, the model learns to generate more logically sound outputs.
- Core assumption: Self-generated responses can contain sufficient variation in reasoning quality that the critic can distinguish between good and poor reasoning chains.
- Break condition: If the self-generated responses do not exhibit meaningful differences in reasoning quality, or if the critic cannot accurately assess reasoning complexity, the DPO process may not effectively improve reasoning capabilities.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is the core algorithm used to align the MLLM with human-like preferences based on the preferred/non-preferred response pairs selected by the critic.
  - Quick check question: What is the key difference between DPO and traditional Reinforcement Learning from Human Feedback (RLHF)?

- Concept: Fine-tuning and Low-Rank Adaptation (LoRA)
  - Why needed here: The critic model and the MLLM are both fine-tuned using LoRA to efficiently adapt large models without full parameter updates, making the approach more scalable.
  - Quick check question: Why is LoRA preferred over full fine-tuning when working with large language models?

- Concept: Multimodal instruction tuning
  - Why needed here: The MLLM needs to understand and respond to both visual and textual inputs, which requires instruction tuning on multimodal datasets before applying preference alignment.
  - Quick check question: What is the primary goal of multimodal instruction tuning in the context of MLLMs?

## Architecture Onboarding

- Component map: Image → MLLM → Multiple Responses → Critic Evaluation → Preference Selection → DPO → Enhanced SFT → Improved MLLM
- Critical path: Image → MLLM → Multiple Responses → Critic Evaluation → Preference Selection → DPO → Enhanced SFT → Improved MLLM
- Design tradeoffs:
  - Using self-generated data reduces cost but may introduce bias or limit diversity compared to human-annotated data
  - Multi-dimensional evaluation provides nuanced feedback but increases computational complexity
  - The critic model adds an additional training stage but enables preference learning without proprietary models
- Failure signatures:
  - Critic model consistently gives similar scores to all responses (evaluation breakdown)
  - DPO training causes performance degradation (preference pairs not representative)
  - Enhanced SFT introduces hallucinations (generated captions not faithful to images)
- First 3 experiments:
  1. Test critic model evaluation consistency on a small set of responses with known quality differences
  2. Verify preference selection produces meaningful score gaps between preferred and non-preferred pairs
  3. Run DPO on a minimal dataset (100 samples) and measure performance change on a held-out validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal size of the preference dataset for achieving maximum performance gains in EACO?
- Basis in paper: [explicit] The paper discusses scaling up the preference dataset from 5k to 15k samples and observes diminishing returns in performance improvements.
- Why unresolved: While the paper provides evidence of diminishing returns beyond 5k samples, it does not pinpoint the exact optimal size where additional data ceases to provide meaningful benefits.
- What evidence would resolve it: A comprehensive study testing multiple dataset sizes (e.g., 2k, 3k, 4k, 5k, 7k, 10k) and analyzing performance gains at each stage would identify the optimal size.

### Open Question 2
- Question: How does EACO's critic model perform on complex tasks requiring chain-of-thought (CoT) reasoning compared to proprietary models like GPT-4V?
- Basis in paper: [explicit] The conclusion mentions that EACO's critic capabilities are largely confined to straightforward tasks like captioning and basic visual question answering, and do not yet achieve performance on par with state-of-the-art language models like GPT-4V for complex tasks requiring CoT reasoning.
- Why unresolved: The paper acknowledges this limitation but does not provide quantitative comparisons or specific benchmarks for CoT reasoning tasks.
- What evidence would resolve it: Testing EACO's critic model on established CoT reasoning benchmarks (like GSM8K or MATH) and comparing its performance against GPT-4V and other proprietary models would quantify the gap.

### Open Question 3
- Question: How does the choice of critic prompt style (Rating, Additive, or Subtractive) affect the long-term generalization and robustness of EACO across different types of multimodal tasks?
- Basis in paper: [explicit] The ablation study compares three different critic prompt styles and finds that the Rating Prompt performs best, but the study focuses on short-term performance metrics rather than long-term generalization.
- Why unresolved: The paper only examines immediate performance impacts of different prompt styles without investigating how these choices affect the model's ability to handle diverse, unseen tasks over time.
- What evidence would resolve it: Conducting longitudinal studies where EACO is tested on a wide variety of multimodal tasks over multiple training iterations, comparing the performance stability and generalization capabilities of each prompt style.

## Limitations

- The paper's reliance on self-generated data may introduce bias and limit the diversity of preference pairs compared to human-annotated data
- The hallucination reduction claims, while impressive, lack detailed analysis of the specific mechanisms by which the critic identifies hallucinations
- The reasoning improvement results may be influenced by the specific evaluation benchmarks used, and generalizability to other reasoning tasks remains untested

## Confidence

- High confidence: The methodology for using a critic model to generate preference data and applying DPO is technically sound and well-established in the literature
- Medium confidence: The quantitative results showing 8.5% average improvement and 65.6% hallucination reduction are impressive but depend on the specific benchmarks and evaluation criteria used
- Low confidence: The qualitative claim that EACO enhances "complex reasoning abilities" without detailed analysis of the types of reasoning improvements achieved

## Next Checks

1. Cross-validation with human evaluation: Have human annotators evaluate a subset of responses to verify that critic-selected preferences align with human judgments of quality, hallucination, and reasoning

2. Ablation study on evaluation dimensions: Test EACO performance using only single evaluation dimensions (e.g., Relevance only) versus the full multi-dimensional approach to quantify the contribution of each dimension

3. Generalization test on unseen reasoning tasks: Evaluate EACO on reasoning benchmarks not used in training to assess whether improvements transfer beyond the specific tasks in MME-Cognition