---
ver: rpa2
title: An Unconditional Representation of the Conditional Score in Infinite-Dimensional
  Linear Inverse Problems
arxiv_id: '2405.15643'
source_url: https://arxiv.org/abs/2405.15643
tags:
- score
- conditional
- posterior
- ucos
- inverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to shift computational complexity
  of sampling from the posterior distribution of a linear inverse problem to the training
  phase. The core idea is to learn a task-dependent unconditional score function and
  derive the conditional score via affine transformations involving the forward operator
  and measurement data.
---

# An Unconditional Representation of the Conditional Score in Infinite-Dimensional Linear Inverse Problems

## Quick Facts
- **arXiv ID**: 2405.15643
- **Source URL**: https://arxiv.org/abs/2405.15643
- **Reference count**: 40
- **Primary result**: Method shifts computational complexity from sampling to training by learning task-dependent unconditional score functions for linear inverse problems

## Executive Summary
This paper introduces a novel approach for solving linear inverse problems using score-based diffusion models in infinite-dimensional Hilbert spaces. The key innovation is an unconditional representation of the conditional score function that eliminates the need for forward model evaluations during sampling. By shifting computational complexity to an offline training phase, the method achieves significant efficiency gains while maintaining discretization invariance. The authors provide rigorous convergence analysis and demonstrate the approach on CT imaging and deblurring tasks, showing comparable or superior performance to existing methods.

## Method Summary
The method learns a task-dependent unconditional score function that incorporates the structure of the forward operator and measurement data. During training, the network learns to represent the score function of the prior conditioned on the specific measurement configuration. The conditional score for posterior sampling is then derived exactly via affine transformations using the trained unconditional score and precomputed measurement-dependent shifts. This approach avoids forward model evaluations during the sampling phase, significantly reducing computational cost while maintaining the benefits of infinite-dimensional formulation.

## Key Results
- Achieves comparable or better performance than unconditional approaches on CT imaging and deblurring tasks
- Eliminates forward model evaluations during sampling by shifting computational effort to offline training
- Demonstrates discretization invariance through infinite-dimensional Hilbert space formulation
- Provides rigorous convergence analysis bounding error between generated samples and true posterior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The conditional score can be derived exactly from a trained unconditional score using affine transformations involving the forward operator and measurement data.
- Mechanism: The method exploits the linear structure of the inverse problem and Gaussian likelihood to complete the square, expressing the posterior density as a transformed prior. This allows the conditional score to be written as an affine transformation of a task-dependent unconditional score.
- Core assumption: The forward model is linear and the likelihood is Gaussian.
- Evidence anchors:
  - [abstract]: "we show that the conditional score can be derived exactly from a trained (unconditional) score using affine transformations"
  - [section 3.2]: Theorem 3.7 provides the exact identity for the conditional score as an affine transformation
  - [corpus]: Weak - no direct citations, but related work on infinite-dimensional diffusion models supports the theoretical framework
- Break condition: If the forward model is nonlinear or the likelihood is non-Gaussian, the completion of the square identity no longer holds, breaking the exact affine relationship.

### Mechanism 2
- Claim: Shifting computational complexity from sampling to training eliminates forward model evaluations during posterior sampling.
- Mechanism: The task-dependent unconditional score incorporates the forward operator structure during training. During sampling, only the trained network and a precomputed measurement-dependent shift are needed, avoiding expensive forward operator evaluations.
- Core assumption: The task-dependent score can be trained offline and stored for use during sampling.
- Evidence anchors:
  - [abstract]: "avoids forward model evaluations during sampling by shifting computational effort to an offline training phase"
  - [section 3.3.1]: Details the offline training procedure that learns the task-dependent score
  - [corpus]: Moderate - related works on matrix-free implementations support this computational strategy
- Break condition: If the forward operator is extremely high-dimensional or changes frequently, the offline training cost may become prohibitive or the precomputed shift may become invalid.

### Mechanism 3
- Claim: The infinite-dimensional formulation ensures discretization invariance and robust convergence.
- Mechanism: By formulating the problem in a separable Hilbert space, the method avoids discretization-dependent errors. The convergence analysis bounds the error between generated samples and the true posterior in the function space.
- Core assumption: The prior and posterior satisfy the boundedness conditions required for the infinite-dimensional diffusion framework.
- Evidence anchors:
  - [abstract]: "Our approach is formulated in infinite-dimensional function spaces, making it inherently discretization-invariant"
  - [section 4]: Provides rigorous convergence analysis in infinite dimensions
  - [corpus]: Moderate - recent theoretical work on infinite-dimensional diffusion models supports these claims
- Break condition: If the problem dimensionality is too low or the forward operator has pathological properties, the infinite-dimensional framework may introduce unnecessary complexity without benefits.

## Foundational Learning

- Concept: Gaussian measures on Hilbert spaces and Cameron-Martin space
  - Why needed here: The method relies on Gaussian priors and the Cameron-Martin theorem for the equivalence of Gaussian measures
  - Quick check question: What is the relationship between a Gaussian measure and its Cameron-Martin space?

- Concept: Score-based diffusion models in infinite dimensions
  - Why needed here: The method extends score-based diffusion models from finite to infinite-dimensional spaces
  - Quick check question: How does the infinite-dimensional score function differ from the finite-dimensional one?

- Concept: Linear inverse problems and Bayesian formulation
  - Why needed here: The method specifically targets linear inverse problems with Bayesian posterior distributions
  - Quick check question: How does the Bayesian posterior for a linear inverse problem relate to the prior and likelihood?

## Architecture Onboarding

- Component map: Measurement data → Precompute shift → Sampling loop
- Critical path: Training phase → Precompute shift → Sampling phase
- Design tradeoffs:
  - Training complexity vs. sampling efficiency
  - Network architecture complexity vs. generalization
  - Discretization level vs. computational cost
- Failure signatures:
  - Poor sample quality → Check training loss convergence
  - High variance in samples → Check network capacity
  - Slow sampling → Check shift precomputation
- First 3 experiments:
  1. Train on simple Gaussian prior with diagonal forward operator (inpainting toy problem)
  2. Validate discretization invariance by comparing results across grid resolutions
  3. Compare sampling speed vs. conditional method baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed UCoS method scale to extremely high-dimensional inverse problems beyond the CT and deblurring examples tested (e.g., 3D medical imaging or seismic imaging with millions of unknowns)?
- Basis in paper: [inferred] The paper demonstrates UCoS on 2D CT imaging (256²) and deblurring (175²), but does not test on truly massive-scale problems where forward operator evaluations become prohibitively expensive even once.
- Why unresolved: The authors focus on demonstrating feasibility and comparing against baselines rather than pushing the method to its computational limits. They don't report scaling laws or performance on problems with more than ~65k dimensions.
- What evidence would resolve it: Experiments showing UCoS performance (accuracy, sampling time, memory usage) as a function of problem dimension, ideally up to 10⁶+ unknowns, and comparison to alternative methods in that regime.

### Open Question 2
- Question: Can the task-dependent unconditional score approach be extended to nonlinear inverse problems where the forward operator depends on the unknown parameters?
- Basis in paper: [explicit] Section 6 explicitly states that extending to nonlinear problems is non-trivial because the key identity relies on completing the square in the Gaussian likelihood, which requires linearity of the forward model.
- Why unresolved: The paper is deliberately focused on linear inverse problems, and the authors acknowledge this limitation without proposing concrete workarounds for nonlinear cases.
- What evidence would resolve it: Either a theoretical extension showing how to handle certain classes of nonlinear problems (e.g., via linearization or alternative score transformations), or numerical experiments demonstrating the method's performance on nonlinear test cases.

### Open Question 3
- Question: How sensitive is UCoS to the choice of the underlying diffusion process (beyond the Ornstein-Uhlenbeck process used here)? Could alternative processes improve the trade-off between offline training cost and online sampling efficiency?
- Basis in paper: [explicit] Remark 3.1 suggests that the general principle of transforming a task-dependent unconditional score into a conditional score applies to other linear diffusion models beyond OU, but the choice of diffusion process is not explored.
- Why unresolved: The paper uses a standard OU process without investigating whether other processes (e.g., with different time weighting or spectral properties) might yield better performance.
- What evidence would resolve it: Systematic comparison of UCoS performance using different diffusion processes (e.g., variable-speed diffusions, fractional Brownian motion) on the same inverse problems, showing trade-offs in training time, sampling efficiency, and accuracy.

## Limitations

- Primary uncertainty around practical scalability to extremely high-dimensional inverse problems where forward operator evaluations become prohibitively expensive
- Method assumes Gaussian likelihood and linear forward operators, limiting applicability to nonlinear problems
- Infinite-dimensional framework may introduce unnecessary complexity for problems with pathological forward operators

## Confidence

**High confidence** in the theoretical framework and affine transformation identity for conditional scores - supported by rigorous mathematical proofs in Sections 3.2 and 4.

**Medium confidence** in the computational benefits during sampling - while the elimination of forward model evaluations is theoretically sound, practical performance depends on the complexity of the trained network and the dimensionality of the shift vector.

**Medium confidence** in the discretization invariance claims - the infinite-dimensional formulation provides theoretical guarantees, but practical discretization choices may still impact performance.

## Next Checks

1. **Scalability test**: Evaluate the method on linear inverse problems with increasingly large forward operators (e.g., 2D to 3D tomography) to quantify how training time scales with problem dimension and whether the computational benefits during sampling are maintained.

2. **Non-Gaussian likelihood extension**: Test the method's performance when the likelihood deviates from Gaussian assumptions (e.g., Poisson noise in photon-limited imaging) to understand the limits of the affine transformation approach and identify potential modifications.

3. **Discretization sensitivity analysis**: Systematically vary the discretization level across multiple orders of magnitude and measure both sample quality (via metrics like PSNR/SSIM) and computational cost to empirically validate the claimed discretization invariance.