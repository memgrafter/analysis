---
ver: rpa2
title: 'GraphSPNs: Sum-Product Networks Benefit From Canonical Orderings'
arxiv_id: '2408.09451'
source_url: https://arxiv.org/abs/2408.09451
tags:
- graph
- graphs
- permutation
- learning
- pspn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes GraphSPNs, a tractable deep generative model
  for graphs that enables exact and efficient probabilistic inference over arbitrary
  parts of graphs. The key challenge addressed is ensuring permutation invariance
  in sum-product networks (SPNs) to handle the factorial number of possible node orderings
  in graphs.
---

# GraphSPNs: Sum-Product Networks Benefit From Canonical Orderings

## Quick Facts
- arXiv ID: 2408.09451
- Source URL: https://arxiv.org/abs/2408.09451
- Authors: Milan Papež; Martin Rektoris; Václav Šmídl; Tomáš Pevný
- Reference count: 36
- Key outcome: GraphSPNs achieve highest validity without check by using canonical ordering, outperforming other variants on QM9 molecular generation

## Executive Summary
GraphSPNs introduce tractable deep generative models for graphs by addressing the permutation invariance challenge in sum-product networks. The paper demonstrates that imposing canonical orderings on graphs significantly simplifies the target distribution, enabling SPNs to learn more effectively. Experiments on the QM9 molecular dataset show that GraphSPNs are competitive with intractable models in generating valid, unique, and novel molecules. The sorting-based variant achieves the best performance without requiring post-generation validity checks.

## Method Summary
GraphSPNs use sum-product networks with virtual node-padding to handle graphs of varying sizes by embedding them into fixed-size spaces. The key innovation is addressing permutation invariance through four approaches: exact averaging over all permutations (intractable for large graphs), sorting to impose canonical ordering, k-ary subgraph sampling, and random sampling. The SPN architecture requires smoothness, decomposability, and tractable input layers to guarantee exact inference. Sorting-based GraphSPNs benefit from simplified target distributions under canonical ordering, achieving highest validity without correction.

## Key Results
- Sorting-based GraphSPN variant achieves highest validity without correction on QM9 dataset
- GraphSPNs are competitive with intractable models in generating valid, unique, and novel molecules
- Canonical ordering reduces the effective number of modes in the target distribution, simplifying learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sorting graphs into a canonical ordering reduces the effective number of modes in the target distribution, making it easier for SPNs to learn.
- Mechanism: The sort operation imposes a unique, deterministic ordering on each graph. Since SPNs are permutation-sensitive, this allows them to learn a simpler, canonicalized distribution rather than a factorial number of equivalent permutations.
- Core assumption: The canonical ordering preserves the essential structure of the graph while eliminating redundancy.
- Evidence anchors:
  - [abstract] "The sorting-based GraphSPN variant, which benefits from the simplified target distribution under canonical ordering, achieves the highest validity without validity without check and outperforms other GraphSPN variants."
  - [section] "if we ensure that pspn m,n always sees the same, user-defined, canonical representation of G, then the permutation invariance of(1) is guaranteed."
- Break condition: If the chosen sorting scheme is not invariant or loses important structural information, the learned model may not generalize well.

### Mechanism 2
- Claim: Virtual node-padding enables SPNs to handle graphs of varying sizes by embedding them into a fixed-size space.
- Mechanism: Each n-node graph is padded with virtual nodes to match a maximum expected size m. This allows a single SPN with fixed root scope to model all graphs.
- Core assumption: The maximum graph size m is known or can be estimated from the data.
- Evidence anchors:
  - [section] "We design the SPN in (1) as pspn m,n(X, A) := pspn m (paddingm,n(X, A))."
  - [section] "paddingm,n extends X by an extra category to express that there is no node at a specific position in the graph."
- Break condition: If m is too small, valid graphs are truncated; if too large, the SPN becomes unnecessarily complex.

### Mechanism 3
- Claim: Tractable inference is achieved by ensuring smoothness, decomposability, and tractable input layers in the SPN.
- Mechanism: Smoothness ensures consistent scopes for sum units, decomposability ensures disjoint scopes for product units, and tractable input layers allow closed-form computation of marginals and other queries.
- Core assumption: The input distributions and functions are chosen from tractable families.
- Evidence anchors:
  - [section] "To this end, the sum units have to satisfy smoothness, the product units have to satisfy decomposability, and the input units have to be tractable."
  - [section] "The expectation (6) admits a closed-form solution only if both f and p satisfy certain structural constraints."
- Break condition: If any of the structural constraints are violated, the SPN loses its tractability guarantees.

## Foundational Learning

- Concept: Permutation invariance in probability distributions
  - Why needed here: Graphs are inherently permutation invariant; the probability of a graph must not depend on node ordering.
  - Quick check question: What does it mean for a probability distribution over graphs to be permutation invariant, and why is it important?

- Concept: Sum-Product Networks (SPNs) and their tractability
  - Why needed here: GraphSPNs are built on SPNs, which provide exact and efficient probabilistic inference under certain conditions.
  - Quick check question: What are the three key properties (smoothness, decomposability, tractable inputs) that an SPN must satisfy to guarantee tractable inference?

- Concept: Canonical ordering of graphs
  - Why needed here: Imposing a canonical ordering simplifies the target distribution and makes it easier for SPNs to learn.
  - Quick check question: How does imposing a canonical ordering on graphs reduce the complexity of the target distribution for an SPN?

## Architecture Onboarding

- Component map: Graph (X, A) -> Virtual node-padding -> Sorting (for sort variant) -> SPN -> Probability distribution

- Critical path:
  1. Preprocess graphs (kekulize, remove H, permute atoms)
  2. Apply virtual node-padding to fix graph size
  3. Sort graphs (for sort variant) to impose canonical ordering
  4. Pass through SPN to compute probability
  5. Use tractable inference for queries (marginalization, conditioning, etc.)

- Design tradeoffs:
  - Exact vs. approximate permutation invariance: Exact (Janossy) is intractable for large graphs; approximate methods (k-ary, random sampling) are more scalable but may lose expressiveness.
  - Fixed vs. variable SPN size: Virtual node-padding fixes SPN size but may introduce unnecessary complexity if m is large.
  - Canonical ordering: Simplifies learning but requires a domain-specific sorting scheme.

- Failure signatures:
  - Low validity without correction: Indicates the model is not learning the chemical valency rules well.
  - Poor uniqueness: Suggests the model is generating duplicate molecules.
  - High computational cost: May indicate inefficient implementation or overly complex SPN architecture.

- First 3 experiments:
  1. Train a GraphSPN with the none variant (no invariance) and evaluate validity and uniqueness on QM9.
  2. Train a GraphSPN with the sort variant and compare its performance to the none variant.
  3. Experiment with different values of k in the k-ary variant to find the best tradeoff between tractability and expressiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the computational cost of exact permutation invariance scale with graph size, and are there efficient approximations that preserve exact invariance?
- Basis in paper: [explicit] The paper states that exact permutation invariance is computationally infeasible for large graphs, requiring n! passes through the SPN.
- Why unresolved: The paper only discusses exact invariance theoretically but doesn't explore efficient computational methods to approximate it while maintaining exactness.
- What evidence would resolve it: Empirical results comparing computational costs and performance of exact vs. approximate methods across varying graph sizes.

### Open Question 2
- Question: How sensitive is GraphSPN performance to the choice of canonical ordering, and can this ordering be learned rather than predetermined?
- Basis in paper: [explicit] The paper shows that sorting-based GraphSPNs benefit from canonical ordering but notes that the "right ordering often depends on domain knowledge."
- Why unresolved: The paper uses a fixed canonical ordering (RDKit's) but doesn't investigate whether this is optimal or if the ordering can be learned from data.
- What evidence would resolve it: Experiments comparing different canonical orderings or learned ordering strategies.

### Open Question 3
- Question: How does GraphSPN's performance on conditional generation compare to intractable models when conditioning on larger portions of the graph?
- Basis in paper: [explicit] The paper shows conditional generation examples but only for small conditioning sets.
- Why unresolved: The paper demonstrates conditional generation capability but doesn't systematically compare performance against intractable models as conditioning set size increases.
- What evidence would resolve it: Systematic evaluation of conditional generation quality across varying conditioning set sizes compared to baseline models.

## Limitations

- The paper focuses on small molecular graphs (QM9) and doesn't address scalability to larger, more complex graphs.
- Sorting-based canonical ordering requires domain-specific knowledge and may not generalize across different graph types.
- The computational cost of exact permutation invariance remains prohibitive for large graphs, limiting the approach's applicability.

## Confidence

- High: Sorting-based GraphSPN improves validity without correction
- Medium: Generalizability to larger/more complex graphs
- Medium: Effectiveness of approximate invariance methods

## Next Checks

1. Test GraphSPNs on larger molecular datasets (ZINC, ChEMBL) to evaluate scaling behavior
2. Compare sorting-based invariance against learned permutation-invariant architectures like E-GNNs
3. Conduct ablation studies on sorting criteria (e.g., by degree, betweenness, random permutations) to isolate the key factors in the canonical ordering approach