---
ver: rpa2
title: Properties that allow or prohibit transferability of adversarial attacks among
  quantized networks
arxiv_id: '2405.09598'
source_url: https://arxiv.org/abs/2405.09598
tags:
- adversarial
- transferability
- attack
- networks
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how quantization impacts the transferability
  of adversarial attacks among compressed neural networks. Experiments with five attack
  algorithms (FGSM, JSMA, UAP, CW, and BA) on MNIST and CIFAR10 models reveal that
  quantization reduces transferability, primarily due to quantization shift and misalignment
  of loss gradients.
---

# Properties that allow or prohibit transferability of adversarial attacks among quantized networks

## Quick Facts
- arXiv ID: 2405.09598
- Source URL: https://arxiv.org/abs/2405.09598
- Reference count: 40
- Primary result: Quantization reduces transferability of adversarial attacks due to quantization shift and gradient misalignment

## Executive Summary
This study investigates how quantization impacts the transferability of adversarial attacks among compressed neural networks. Experiments with five attack algorithms (FGSM, JSMA, UAP, CW, and BA) on MNIST and CIFAR10 models reveal that quantization reduces transferability, primarily due to quantization shift and misalignment of loss gradients. Notably, UAP exhibits higher transferability than its internal FGSM component at higher distortion levels, suggesting that attack performance can be enhanced by increasing perturbation magnitude. The study also demonstrates that the average transferability among different bitwidth versions of a model can serve as an indicator of transferability to models with varying architectures and capacities. These findings highlight the nuanced effects of quantization on adversarial robustness and transferability, providing insights for securing embedded AI systems.

## Method Summary
The study trains quantized and full-precision models on MNIST and CIFAR10 using DoReFa-Net quantization with 1, 2, 4, 8, 12, and 16 bitwidths. Five attack algorithms (FGSM, JSMA, UAP, BA, CW) are implemented using the ART library with specific hyperparameters. Adversarial examples are generated and transferred among models of different bitwidths and architectures. Transferability is measured as adversarial accuracy, comparing the effectiveness of attacks when transferred between quantized models versus full-precision models.

## Key Results
- Quantization significantly reduces transferability of all tested attack algorithms
- UAP shows higher transferability than its internal FGSM component at higher distortion levels
- Average transferability among different bitwidth versions of a model can predict transferability to models with different architectures
- Loss gradient misalignment and quantization shift are primary mechanisms reducing transferability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Loss gradient misalignment between source and target networks reduces transferability of gradient-based attacks.
- Mechanism: When networks have different quantization levels, the weight and activation values are grouped into different discrete buckets. This causes the loss gradients computed for adversarial examples to point in slightly different directions for each network. During transfer, the perturbation vector aligned with the source gradient may not be effective on the target network if the gradient directions are misaligned.
- Core assumption: The loss gradients of the source and target networks should be aligned for successful transfer.
- Evidence anchors:
  - [abstract]: "quantization reduces transferability, primarily due to quantization shift and misalignment of loss gradients"
  - [section]: "misalignment of gradient between source and target means that the perturbation direction transferred from source may not be able to cause the same effect at the target"
  - [corpus]: Weak. Related work focuses on transferability tricks but not on gradient misalignment due to quantization.

### Mechanism 2
- Claim: Quantization shift causes feature sensitivity changes, hindering transferability of attacks like JSMA.
- Mechanism: JSMA relies on individual feature modifications based on feature sensitivity. When quantization levels change, the discrete levels of activation and weight values change, altering the sensitivity of features. Features that were effective in causing misclassification in the source network may no longer be sensitive in the target network with a different bitwidth.
- Core assumption: Feature sensitivity remains consistent across networks with different quantization levels.
- Evidence anchors:
  - [abstract]: "quantization reduces transferability, primarily due to quantization shift and misalignment of loss gradients"
  - [section]: "JSMA crafts adversarial examples by manipulating individual features...Due to quantization shift, the same features may no longer be sensitive on a different network with different bitwidth"
  - [corpus]: Weak. The corpus focuses on boosting transferability but doesn't address quantization shift.

### Mechanism 3
- Claim: UAP allows higher magnitudes of distortion than its internal FGSM, enhancing transferability at higher distortions.
- Mechanism: UAP internally uses FGSM to compute perturbations for individual images. However, UAP allows setting the maximum allowed distortion to higher values than FGSM while still keeping the adversarial image recognizable. This higher distortion level can make UAPs more transferable than FGSM at higher distortions.
- Core assumption: Higher distortion levels always improve transferability.
- Evidence anchors:
  - [abstract]: "UAP exhibits higher transferability than its internal FGSM component at higher distortion levels"
  - [section]: "UAP allows to increase the maximum allowed distortion to higher values than the internal algorithm while still keeping the adversarial image recognizable"
  - [corpus]: Weak. The corpus doesn't discuss UAP or distortion levels.

## Foundational Learning

- Concept: Adversarial examples and their transferability
  - Why needed here: Understanding how adversarial examples work and why they transfer (or don't transfer) between models is fundamental to this study.
  - Quick check question: What is the key property that allows adversarial examples to transfer from a source model to a target model?

- Concept: Quantization and its effects on neural networks
  - Why needed here: Quantization is the central technique being studied, and understanding how it affects network behavior (e.g., loss gradients, feature sensitivity) is crucial.
  - Quick check question: How does quantization change the weight and activation values in a neural network?

- Concept: Attack algorithms (FGSM, JSMA, UAP, BA, CW)
  - Why needed here: Different attack algorithms have different mechanisms for creating adversarial examples, and understanding these mechanisms is key to understanding their transferability properties.
  - Quick check question: How does the Jacobian Saliency Map based Attack (JSMA) select features to modify in an image?

## Architecture Onboarding

- Component map: MNIST and CIFAR10 datasets -> Mnist A, Resnet20, Resnet32, Resnet44, Cifar A models -> Quantized versions (1, 2, 4, 8, 12, 16 bits) -> Five attack algorithms (FGSM, JSMA, UAP, BA, CW) -> ART library

- Critical path:
  1. Train full-precision and quantized models on MNIST and CIFAR10
  2. Craft adversarial examples using each attack algorithm on each model
  3. Transfer adversarial examples between models with different bitwidths
  4. Measure transferability (adversarial accuracy) for each transfer
  5. Analyze results to understand transferability properties

- Design tradeoffs:
  - Model complexity vs. transferability: More complex models may have better transferability but are harder to train and deploy on embedded devices.
  - Attack strength vs. transferability: Stronger attacks (e.g., CW) may be less transferable than weaker attacks (e.g., FGSM).
  - Quantization level vs. accuracy: Lower bitwidth models are more efficient but may have lower accuracy.

- Failure signatures:
  - Low adversarial accuracy: The attack failed to transfer to the target model.
  - Unrecognizable images: The adversarial examples are too distorted to be useful.
  - High variance in results: The transferability is inconsistent across different runs.

- First 3 experiments:
  1. Train Mnist A and Resnet20 models on MNIST and CIFAR10, respectively, and evaluate their accuracy on the test sets.
  2. Craft adversarial examples using FGSM on the full-precision versions of Mnist A and Resnet20, and measure their transferability to the quantized versions.
  3. Craft adversarial examples using JSMA on the full-precision versions of Mnist A and Resnet20, and measure their transferability to the quantized versions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the direction of perturbation in search-based attacks like Boundary Attack fundamentally differ from gradient-based attacks, and how does this directional difference quantitatively impact transferability rates across different bitwidths?
- Basis in paper: [explicit] The paper states that "adversarial examples exist in a broad contiguous regions in input space; thus, rather than an exact example, the direction of perturbation is important for transferability" and notes that "in case of the Boundary Attack, perturbations are added to a random image, moving it towards the original image" which is "opposite to the adversarial direction."
- Why unresolved: The paper observes poor transferability of Boundary Attack but does not quantify the directional difference or its specific impact on transferability rates across various bitwidths through controlled experiments.
- What evidence would resolve it: Experiments comparing perturbation directions (cosine similarity to adversarial directions) of Boundary Attack versus gradient-based attacks, and correlating these with transferability rates across different bitwidth networks.

### Open Question 2
- Question: Can the average transferability of adversarial examples among different bitwidth versions of a network be used as a reliable predictor for transferability to networks with different architectures and capacities, and what are the limits of this predictive relationship?
- Basis in paper: [explicit] The paper states that "the overall success-rate of a transfer-based attack can be estimated by observing the performance of the attack when it is transferred among different bitwidth versions of the source model" and "the average transferability of an attack among the different bitwidth versions of the same model can be an indication of transferability when the attack is transferred to another model which may be different in not only bitwidth but also in architecture and model capacity."
- Why unresolved: While the paper observes a pattern suggesting this relationship, it does not validate this prediction with extensive cross-architecture testing or establish confidence intervals for the prediction accuracy.
- What evidence would resolve it: Large-scale experiments testing transferability from multiple source architectures to diverse target architectures, with statistical analysis of prediction accuracy using average bitwidth transferability as a predictor.

### Open Question 3
- Question: What is the precise mechanism by which quantization shift phenomenon affects the transferability of attacks like JSMA that modify individual features, and can this be mitigated through adaptive feature selection methods?
- Basis in paper: [explicit] The paper states that "Due to quantization shift, the same features may no longer be sensitive on a different network with different bitwidth" and that "when the attack is transferred to another network with different bitwidth, the same perturbations are no longer able to produce similar change in activations."
- Why unresolved: The paper identifies quantization shift as a cause of poor transferability but does not explore the underlying mechanism or propose methods to adapt feature selection for different bitwidths.
- What evidence would resolve it: Analysis of feature activation distributions across bitwidths, identification of which feature modifications are most affected by quantization shift, and experiments testing adaptive feature selection methods that account for bitwidth differences.

## Limitations
- Findings primarily based on MNIST and CIFAR10 datasets, limiting generalizability
- Limited set of model architectures tested, may not represent all network types
- Lack of quantitative measures for quantization shift and gradient misalignment effects
- No exploration of potential mitigation strategies for reduced transferability

## Confidence
- High confidence: Quantization reduces transferability and UAP exhibits higher transferability than FGSM at higher distortion levels
- Medium confidence: Quantization shift and gradient misalignment are primary mechanisms for reduced transferability
- Low confidence: Average transferability among bitwidth versions predicts transferability to different architectures

## Next Checks
1. Quantify quantization shift and gradient misalignment by measuring alignment of loss gradients between source and target models with different quantization levels
2. Test findings on diverse datasets (e.g., ImageNet) and more complex model architectures (e.g., EfficientNet, Vision Transformers)
3. Investigate techniques to mitigate quantization shift and gradient misalignment effects, such as gradient regularization or adaptive quantization methods