---
ver: rpa2
title: Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks
arxiv_id: '2403.13112'
source_url: https://arxiv.org/abs/2403.13112
tags:
- memory
- https
- input
- output
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces prompt-in-decoder (PID), a method for improving
  the efficiency of encoder-decoder transformer models on decomposable tasks where
  multiple outputs are needed for a single shared input. PID avoids duplicate encoding
  by encoding the input once and decoding outputs in parallel, while sharing the input
  key-value cache to increase operational intensity and reduce memory access.
---

# Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks

## Quick Facts
- arXiv ID: 2403.13112
- Source URL: https://arxiv.org/abs/2403.13112
- Authors: Bo-Ru Lu; Nikita Haduong; Chien-Yu Lin; Hao Cheng; Noah A. Smith; Mari Ostendorf
- Reference count: 40
- Key outcome: Prompt-in-decoder (PID) reduces computation by 2-10x and speeds up inference by up to 4.6x on decomposable tasks

## Executive Summary
This paper introduces prompt-in-decoder (PID), a method for improving the efficiency of encoder-decoder transformer models on decomposable tasks where multiple outputs are needed for a single shared input. PID avoids duplicate encoding by encoding the input once and decoding outputs in parallel, while sharing the input key-value cache to increase operational intensity and reduce memory access. Experiments on dialogue state tracking, summarization, and question-answering tasks show that PID achieves comparable or better performance than state-of-the-art models while providing significant computational savings.

## Method Summary
PID encodes the input once to produce shared embeddings M, then decodes all prompts in parallel using this shared encoding. Unlike previous approaches that encode the input separately for each prompt, PID shares the input key-value cache across all prompts during cross-attention computation. This reduces memory access and increases operational intensity, leading to computational savings that scale with the number of subtasks. The method is particularly effective for tasks with many subtasks or long outputs, achieving 2-10x computation reduction and up to 4.6x speedup in inference.

## Key Results
- PID achieves computation reduction of 2-10x across three different tasks (dialogue state tracking, summarization, question answering)
- Inference speedups of up to 4.6x compared to standard T5 models
- Performance comparable or better than state-of-the-art models (PIE, T5) while being significantly more efficient
- Greater efficiency gains with larger number of subtasks (e.g., 30 slots in MultiWoZ 2.4)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PID reduces redundant input encoding by sharing a single encoded input across multiple subtasks
- Mechanism: Instead of encoding the input once per prompt (as in PIE), PID encodes the input once and shares the encoded embeddings across all prompts during decoding
- Core assumption: The prompts are independent of the input encoding and can be handled in the decoder
- Evidence anchors:
  - [abstract] "PID avoids duplicate encoding by encoding the input once and decoding outputs in parallel"
  - [section] "In contrast, PID uses a single shared M for each prompt"
  - [corpus] Weak evidence - no corpus papers directly address this specific mechanism

### Mechanism 2
- Claim: PID increases operational intensity by reducing memory access during cross-attention
- Mechanism: By sharing the input key-value cache across all prompts, PID reduces the number of memory reads during cross-attention computation
- Core assumption: Cross-attention is the dominant cost in transformer decoding and benefits most from reduced memory access
- Evidence anchors:
  - [abstract] "increasing the operational intensity (ratio of numbers of arithmetic operation to memory access) of decoding process by sharing the input key-value cache"
  - [section] "the dot product operation in the cross-attention shares and broadcasts K and computes the dot product of K and Q, resulting in lower memory access"
  - [corpus] No direct corpus evidence for this specific operational intensity mechanism

### Mechanism 3
- Claim: PID achieves speedups that scale with the number of subtasks
- Mechanism: The computational savings from shared encoding and reduced memory access multiply as more subtasks are added
- Core assumption: Each subtask can be processed independently once the input is encoded
- Evidence anchors:
  - [abstract] "We achieve computation reduction that roughly scales with the number of subtasks"
  - [section] "PID offers greater reductions in computational costs (2-10x) and further accelerates efficiency when dealing with a larger number of subtasks"
  - [corpus] No corpus evidence directly addressing scaling with subtask count

## Foundational Learning

- Concept: Transformer attention mechanisms
  - Why needed here: Understanding how self-attention and cross-attention work is crucial to grasping why PID is efficient
  - Quick check question: What is the difference between self-attention and cross-attention in an encoder-decoder transformer?

- Concept: Operational intensity and memory bandwidth
  - Why needed here: PID's efficiency gains come from improving the ratio of computations to memory accesses
  - Quick check question: Why does reducing memory access improve computational efficiency in modern hardware?

- Concept: Encoder-decoder architecture
  - Why needed here: PID modifies how prompts are handled in the encoder-decoder framework
  - Quick check question: In standard encoder-decoder models, where are prompts typically placed and why?

## Architecture Onboarding

- Component map:
  - Encoder: Processes input once, outputs shared embeddings M
  - Decoder: Processes all prompts in parallel, using shared M for cross-attention
  - Key-value cache: Shared across all prompts to reduce memory access
  - Cross-attention: Modified to broadcast shared K and compute QÂ·K dot products

- Critical path:
  1. Encode input X once to produce M
  2. For each prompt Zu, decode output Yu using shared M
  3. Combine all Yu outputs

- Design tradeoffs:
  - Pros: Reduced computation, faster inference, better scaling with subtasks
  - Cons: Limited to decomposable tasks, prompts must be independent of input encoding
  - Memory vs. speed: Sharing K-V cache saves memory bandwidth but may increase memory usage for storing multiple outputs

- Failure signatures:
  - If prompts are input-dependent: Performance degrades as encoding is no longer shared
  - If memory bandwidth is not the bottleneck: Speedups may not materialize
  - If subtasks are not truly independent: Parallel decoding may produce incorrect results

- First 3 experiments:
  1. Compare PIE vs PID on a simple decomposable task (e.g., multiple QA pairs on same document)
  2. Measure memory access and compute operations using profiling tools
  3. Test scaling behavior with increasing number of subtasks

## Open Questions the Paper Calls Out
The paper explicitly states that while subtasking experiments use only encoder-decoder models, the strategy of sharing an embedding and decomposing a task should work with decoder-only models, but experimental analysis is left to future work.

## Limitations
- The analysis of operational intensity improvements lacks quantitative measurement of actual memory access patterns
- Limited comparison with other efficiency methods beyond the PIE baseline
- Experimental evaluation focuses on three specific task types without exploring boundaries of where PID fails
- Computational complexity analysis uses FLOPs as proxy but doesn't account for memory hierarchy effects

## Confidence
- High confidence: The core claim that PID reduces redundant encoding and achieves computational savings is well-supported by experimental results across multiple datasets and tasks
- Medium confidence: The claim about operational intensity improvements through shared key-value caching is theoretically sound but lacks empirical validation through memory access measurements
- Low confidence: The paper's discussion of failure modes is limited, and conditions under which PID would not provide benefits are not thoroughly explored

## Next Checks
1. Profile actual memory access patterns during PID inference compared to baseline methods, measuring cache hit rates, memory bandwidth utilization, and operational intensity ratios to empirically verify the claimed efficiency mechanisms.

2. Systematically test PID on tasks that violate its assumptions - including input-dependent prompts, interdependent subtasks, and non-decomposable tasks - to identify the precise boundaries of where the method breaks down.

3. Compare PID against other efficiency approaches for multi-output generation, such as parallel decoding with separate encoders, input caching strategies, or task-specific architectures, to establish whether the claimed 2-10x improvements are competitive with state-of-the-art alternatives.