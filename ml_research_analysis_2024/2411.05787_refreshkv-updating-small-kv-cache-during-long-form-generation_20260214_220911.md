---
ver: rpa2
title: 'RefreshKV: Updating Small KV Cache During Long-form Generation'
arxiv_id: '2411.05787'
source_url: https://arxiv.org/abs/2411.05787
tags:
- attention
- tokens
- cache
- recycled
- full
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently generating long
  sequences with large language models by proposing Recycled Attention, an inference-time
  method that dynamically maintains a smaller key-value (KV) cache based on attention
  patterns of neighboring tokens. The method alternates between full attention steps
  and recycled attention steps, where the latter attends only to the top K tokens
  from the previous full attention step, reducing both computation and memory movement.
---

# RefreshKV: Updating Small KV Cache During Long-form Generation

## Quick Facts
- **arXiv ID**: 2411.05787
- **Source URL**: https://arxiv.org/abs/2411.05787
- **Reference count**: 18
- **Primary result**: Recycled Attention achieves comparable speedups to eviction-based baselines while improving long-context task accuracy by 2x (63% vs 22%)

## Executive Summary
This paper introduces Recycled Attention, an inference-time method for efficient long-sequence generation in large language models. The approach dynamically maintains a smaller key-value cache based on attention patterns of neighboring tokens, alternating between full attention steps and recycled attention steps. Applied to Llama-3.1-8B and Qwen2-7B models, Recycled Attention achieves comparable speedups to baselines like StreamingLLM and H2O while significantly improving performance on long-context tasks like RULER, with accuracy increasing by 2x. The method also shows better perplexity on language modeling tasks and recovers more attention mass than baselines.

## Method Summary
Recycled Attention addresses long-context generation by alternating between full attention steps (every S steps) and recycled attention steps (S-1 steps in between). During recycled attention, the model attends only to the top K tokens from the previous full attention step, reducing computation while maintaining performance. The method leverages the observation that neighboring tokens tend to attend to similar subsets of past tokens. A dynamic scheduling variant further improves the performance-efficiency trade-off by adapting the frequency of full attention steps based on query similarity. Continued pre-training with Recycled Attention enhances adaptation across different stride settings.

## Key Results
- Recycled Attention achieves 63% accuracy on RULER benchmark vs 22% for StreamingLLM baseline (2x improvement)
- Comparable perplexity to baselines on language modeling tasks while maintaining similar decoding speed
- Top-K attention recovery rate exceeds 90% for neighboring tokens across multiple models and datasets
- Dynamic scheduling improves performance-efficiency trade-off compared to fixed stride approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recycled Attention recovers a large fraction of the total attention mass from full attention steps by leveraging the overlap in attention patterns between neighboring tokens.
- Mechanism: After each full attention step, the method identifies the top K tokens most attended to in that step. For the next S-1 steps, attention is restricted to these top K tokens plus local tokens. Since neighboring tokens tend to attend to similar subsets of past tokens, this approach approximates full attention while reducing computation.
- Core assumption: Consecutive tokens during generation place high attention mass over a similar subset of past tokens.
- Evidence anchors:
  - [abstract] "Our key intuition is that neighboring tokens during generation place high attention mass over a similar subset of past tokens"
  - [section] "Preliminary study: Attention mass overlap between neighboring tokens... Figure 2 shows this attention recovery rate... demonstrates that the topK past tokens at tth attention step cover, on average, more than 90% of the attention mass at subsequent times t + i."
  - [corpus] Weak - no direct mention of attention mass overlap in related papers.

### Mechanism 2
- Claim: Alternating between full and recycled attention steps provides a favorable trade-off between inference speed and model performance compared to permanent KV cache eviction.
- Mechanism: By maintaining the full KV cache but only performing full attention every S steps, the method reduces both attention computation (quadratic in sequence length) and KV cache movement overhead. The recycled attention steps use a smaller, dynamically updated cache that approximates full attention based on recent attention patterns.
- Core assumption: The cost of recomputing attention scores every S steps is outweighed by the savings from reduced attention computation and cache movement in the intervening steps.
- Evidence anchors:
  - [abstract] "Compared to previously proposed inference-time acceleration method which attends only to local context or tokens with high accumulative attention scores, our approach flexibly chooses tokens that are relevant to the current decoding step."
  - [section] "Table 1 compares the memory and attention compute requirements of Recycled Attention with baselines... Recycled Attention requires larger KV cache memory compared to eviction-based baselines, but similar to vanilla attention... However, our decoding latency is on par with both StreamingLLM and H2O."
  - [corpus] Weak - related papers focus on cache compression/eviction rather than this alternating pattern approach.

### Mechanism 3
- Claim: Dynamic scheduling based on query similarity further improves the performance-efficiency trade-off by adapting the frequency of full attention steps to the model's needs.
- Mechanism: Instead of a fixed stride, the method computes the cosine similarity between the current query vector and the query vector from the last full attention step. If similarity is high, recycled attention suffices; if low, full attention is performed. This allows more frequent full attention when the model's attention patterns change significantly.
- Core assumption: Query vectors with high cosine similarity will have similar attention patterns, making recycled attention sufficient.
- Evidence anchors:
  - [section] "At every Sth decode step, we first determine whether we need to perform full attention instead of always performing full attention by default. We calculate the cosine similarity between query vectors..."
  - [section] "Table 5 reports our results. We observe that using dynamic strides improves the performance-efficiency trade-off across all settings. Compared to fixed stride of 10... dynamic stride with query comparison stride of 5... achieves lower perplexity with a slightly faster decoding time on both domains."
  - [corpus] Weak - no direct mention of query similarity-based scheduling in related papers.

## Foundational Learning

- Concept: Transformer attention mechanism and KV cache
  - Why needed here: The entire method relies on understanding how self-attention works and how the KV cache stores intermediate computations for efficient generation.
  - Quick check question: In a transformer decoder, what are the three matrices computed from the input, and which one is stored in the KV cache?

- Concept: Flash Attention and its limitations
  - Why needed here: Recycled Attention needs to recompute attention scores every S steps to identify top K tokens, which is not stored by Flash Attention. Understanding this limitation is crucial for implementation.
  - Quick check question: What key optimization does Flash Attention perform that makes it incompatible with Recycled Attention's need to access attention scores?

- Concept: Grouped Query Attention (GQA)
  - Why needed here: The paper uses Llama-3.1-8B and Qwen2-7B, both of which employ GQA. The method must aggregate attention scores across query heads in the same group to select top K tokens.
  - Quick check question: In GQA, how many KV heads does each query head attend to, and how does this differ from standard multi-head attention?

## Architecture Onboarding

- Component map:
  - Full KV cache (Cf) -> Recycled KV cache (Cr) -> Scheduler -> Attention scorer -> Layer manager

- Critical path:
  1. Prefill input with full attention to initialize Cf and Cr
  2. For each generation step:
     - Check scheduler to determine attention mode
     - If recycled: generate using Cr, update Cr (evict lowest attention score token, add new token)
     - If full: update Cf with new tokens, generate using Cf, recompute attention scores, reinitialize Cr with top K tokens

- Design tradeoffs:
  - KV cache size vs. speed: Larger K improves performance but reduces speed gains
  - Stride S vs. efficiency: Larger S increases speed but may miss important tokens
  - Fixed vs. dynamic scheduling: Fixed is simpler but dynamic can adapt to changing attention patterns
  - Memory vs. recomputation: Maintaining full cache increases memory usage but avoids permanent token loss

- Failure signatures:
  - Degraded performance on tasks requiring non-local context (e.g., multi-NIAH, aggregation tasks)
  - Inconsistent perplexity improvements across different model architectures
  - Suboptimal dynamic scheduling decisions leading to unnecessary full attention steps

- First 3 experiments:
  1. Ablation study varying K and S on language modeling task to identify optimal hyperparameters
  2. Comparison of fixed stride vs. dynamic scheduling on RULER benchmark to measure performance gains
  3. Continued pre-training with Recycled Attention to evaluate adaptation benefits across different stride settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Recycled Attention vary across different types of long-context tasks, particularly those requiring multi-hop reasoning or complex information synthesis?
- Basis in paper: [explicit] The paper notes that Recycled Attention shows significant gains on aggregation and retrieval tasks but observes minimal performance degradation on multi-NIAH settings that require returning multiple values.
- Why unresolved: While the paper provides performance metrics on various RULER tasks, it does not deeply analyze how Recycled Attention handles tasks requiring multi-hop reasoning or complex synthesis beyond retrieval and aggregation.
- What evidence would resolve it: Experiments comparing Recycled Attention's performance on multi-hop reasoning tasks (e.g., variable tracking with multiple hops) versus simpler retrieval tasks, with detailed analysis of attention patterns.

### Open Question 2
- Question: What is the impact of using dynamic stride scheduling on the performance-efficiency trade-off for different types of language models and tasks?
- Basis in paper: [explicit] The paper explores dynamic scheduling based on query similarity and finds it improves the performance-efficiency trade-off, but the experiments are limited to specific models and tasks.
- Why unresolved: The paper does not provide a comprehensive analysis of how dynamic stride scheduling affects different models (e.g., models with varying attention mechanisms) or task types.
- What evidence would resolve it: A study varying dynamic stride scheduling across a diverse set of language models and task types, with detailed performance and efficiency metrics.

### Open Question 3
- Question: How does continued pre-training with Recycled Attention affect the model's ability to generalize to unseen long-context tasks and domains?
- Basis in paper: [explicit] The paper shows that continued pre-training with Recycled Attention improves performance-efficiency trade-off for the Llama-3.1 model, but does not explore generalization to new tasks or domains.
- Why unresolved: The experiments focus on specific tasks and domains, and do not assess whether the improvements from continued pre-training generalize to other scenarios.
- What evidence would resolve it: Testing the model's performance on a variety of unseen long-context tasks and domains after continued pre-training with Recycled Attention, compared to models without such pre-training.

## Limitations

- The method maintains full KV cache memory overhead, which could be prohibitive for extremely long sequences beyond 128K context length
- Evaluation focuses on greedy decoding, limiting applicability to sampling-based generation strategies common in practical deployments
- The approach assumes relatively stable attention patterns between neighboring tokens, which may break down in tasks requiring rapid context switching

## Confidence

**High Confidence Claims:**
- The alternating attention pattern (full + recycled) consistently reduces computation and KV cache movement overhead compared to vanilla attention
- The method achieves comparable speedups to eviction-based baselines (StreamingLLM, H2O) on language modeling tasks
- Top-K attention recovery rates exceeding 90% for neighboring tokens is empirically validated across multiple models and datasets

**Medium Confidence Claims:**
- The 2x accuracy improvement on RULER benchmark (63% vs 22%) represents a consistent trend, though the absolute numbers may vary with task configuration
- Dynamic scheduling based on query similarity provides meaningful improvements over fixed stride, though the magnitude varies by dataset and model
- Continued pre-training with Recycled Attention improves adaptation across different stride settings

**Low Confidence Claims:**
- The method's performance on short-context tasks or instruction-tuned models remains unverified
- The scalability limits for sequence lengths beyond 128K context are not empirically established
- The generalization of attention recovery patterns to highly diverse attention distributions (e.g., multi-document QA, code generation) is assumed but not tested

## Next Checks

1. **Scalability Test**: Evaluate Recycled Attention on sequence lengths exceeding 128K context (e.g., 256K or 512K) to identify memory bottlenecks and performance degradation points. Measure both accuracy and decoding speed as context length increases.

2. **Domain Transfer Evaluation**: Test the method on short-context tasks (â‰¤4K tokens) and instruction-tuned models to verify whether the attention recovery assumptions hold in these scenarios. Include both accuracy metrics and qualitative analysis of attention pattern stability.

3. **Sampling Strategy Validation**: Implement beam search and sampling-based decoding with Recycled Attention to assess whether the attention recovery mechanism remains effective when the model explores multiple candidate tokens per step. Compare perplexity and task performance against greedy decoding results.