---
ver: rpa2
title: Exploiting LLMs' Reasoning Capability to Infer Implicit Concepts in Legal Information
  Retrieval
arxiv_id: '2410.12154'
source_url: https://arxiv.org/abs/2410.12154
tags:
- query
- legal
- retrieval
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of legal information retrieval
  where queries often contain real-life scenarios with vocabulary not specific to
  the legal domain. The proposed method leverages large language models (LLMs) to
  infer implicit legal concepts and facts from queries through two approaches: legal
  term extraction and query reformulation.'
---

# Exploiting LLMs' Reasoning Capability to Infer Implicit Concepts in Legal Information Retrieval

## Quick Facts
- arXiv ID: 2410.12154
- Source URL: https://arxiv.org/abs/2410.12154
- Authors: Hai-Long Nguyen; Tan-Minh Nguyen; Duc-Minh Nguyen; Thi-Hai-Yen Vuong; Ha-Thanh Nguyen; Xuan-Hieu Phan
- Reference count: 9
- Primary result: Achieved F2-scores of 0.8449 and 0.7601 on COLIEE 2022 and 2023 datasets, outperforming all participating teams

## Executive Summary
This paper addresses the challenge of legal information retrieval where queries often contain real-life scenarios with vocabulary not specific to the legal domain. The proposed method leverages large language models (LLMs) to infer implicit legal concepts and facts from queries through two approaches: legal term extraction and query reformulation. These expanded queries are then integrated into both lexical-based (BM25) and semantic-based (BERT) ranking models. The final ensemble system combines results from multiple models using weighted averaging. Experiments on COLIEE 2022 and 2023 datasets demonstrate that LLM-extracted information significantly improves retrieval accuracy.

## Method Summary
The proposed retrieval system integrates LLM-based query expansion with traditional ranking models. Legal terms are extracted from queries using zero-shot prompting, then concatenated with the original query for BM25 ranking. Query reformulation is performed using LLMs to rewrite queries in legal style, maintaining semantic integrity while incorporating legal reasoning. These expanded queries are used with both BM25 and BERT-based semantic ranking models. The final ensemble combines outputs from all three models (BM25 with expanded terms, BERT for original queries, BERT for reformulated queries) using weighted averaging optimized through grid search.

## Key Results
- Achieved F2-scores of 0.8449 on COLIEE 2022 dataset and 0.7601 on COLIEE 2023 dataset
- Outperformed highest results among all participating teams in both competitions
- Significant improvement in retrieval accuracy through LLM-extracted legal terms and reformulated queries
- Ensemble approach combining lexical and semantic models proved superior to single-model methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based query expansion with legal terms improves lexical matching performance.
- Mechanism: The proposed method extracts legal terms from queries using zero-shot prompting, then concatenates these terms with the original query. This expanded query contains legal vocabulary that directly overlaps with relevant statutes, improving BM25's ability to retrieve matching documents.
- Core assumption: Legal terms extracted by LLMs accurately represent the implicit legal concepts in the query.
- Evidence anchors:
  - [abstract] "The proposed retrieval system integrates additional information from the term–based expansion and query reformulation to improve the retrieval accuracy."
  - [section 3.1] "Legal terms related to the query are extracted using zero–shot prompting techniques on LLMs."
  - [corpus] Weak evidence - only general papers on legal retrieval without specific LLM-term expansion validation.

### Mechanism 2
- Claim: LLM-based query reformulation preserves semantic coherence while adding legal context.
- Mechanism: The method uses LLMs to rewrite queries in legal style, maintaining semantic integrity while incorporating legal reasoning. This reformulated query is used with BERT-based semantic ranking models.
- Core assumption: Legal-style reformulated queries maintain semantic relevance while improving legal specificity.
- Evidence anchors:
  - [abstract] "The proposed retrieval system integrates additional information from the term–based expansion and query reformulation to improve the retrieval accuracy."
  - [section 3.2] "To preserve the semantic integrity, the legal–style oriented query reformulation is performed by zero–shot prompting technique on LMMs."
  - [corpus] Weak evidence - general papers on query reformulation without legal-specific LLM validation.

### Mechanism 3
- Claim: Ensemble of lexical and semantic models with learned weights outperforms single-model approaches.
- Mechanism: The system combines BM25 (with expanded queries) and two BERT models (original and reformulated queries) using weighted averaging, optimizing weights through grid search on validation data.
- Core assumption: Different models capture complementary aspects of relevance that can be optimally combined.
- Evidence anchors:
  - [abstract] "The final ensemble retrieval system outperformed the highest results among all participating teams in the COLIEE 2022 and 2023 competitions."
  - [section 3.3] "The final relevance scores from all three ranking models will be weighted ensembled using the equation 1."
  - [corpus] Moderate evidence - general ensemble methods in IR, but specific legal LLM-ensemble validation is limited.

## Foundational Learning

- Concept: BM25 ranking algorithm
  - Why needed here: Understanding the baseline lexical matching method that the LLM-expanded queries improve upon.
  - Quick check question: What parameter in BM25 controls the document length normalization?

- Concept: BERT fine-tuning for sequence classification
  - Why needed here: The semantic ranking models are BERT-based classifiers trained to distinguish relevant from non-relevant query-document pairs.
  - Quick check question: How does pairwise training work for BERT in information retrieval tasks?

- Concept: Zero-shot prompting techniques
  - Why needed here: Both legal term extraction and query reformulation rely on zero-shot prompting without task-specific fine-tuning.
  - Quick check question: What makes zero-shot prompting different from few-shot prompting in LLM applications?

## Architecture Onboarding

- Component map: Query → LLM Legal Term Extraction → BM25 Model; Query → LLM Query Reformulation → BERT Model (Original); Query → LLM Query Reformulation → BERT Model (Reformulated); All three model outputs → Weighted Ensemble → Post-processing (thresholding)
- Critical path: LLM extraction/reformulation → model inference → ensemble → thresholding
- Design tradeoffs: Using separate BERT models for original and reformulated queries versus single multi-task BERT; concatenation vs. separate processing for expanded queries
- Failure signatures: Low recall despite high precision suggests threshold too high; poor performance on lexical queries indicates term extraction failing; semantic-only models underperforming suggests reformulation losing meaning
- First 3 experiments:
  1. Run BM25 with original queries vs. LLM-expanded queries to verify lexical improvement
  2. Test BERT performance on original vs. reformulated queries separately
  3. Validate ensemble weights by ablation testing (removing each model component)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the LLM-extracted legal terms and reformulated queries perform when applied to non-Japanese legal corpora or multilingual legal retrieval tasks?
- Basis in paper: [inferred] The paper focuses on Japanese legal corpus and COLIEE datasets, but does not test the approach on other languages or legal systems.
- Why unresolved: The study is limited to a single language and legal system, leaving the generalizability of the LLM-based expansion techniques untested.
- What evidence would resolve it: Experiments applying the same query expansion methods to English or multilingual legal datasets, comparing performance across different legal systems.

### Open Question 2
- Question: What is the optimal balance between LLM-based query expansion and traditional lexical/semantic matching methods for different query types (legal statements vs. real-life scenarios)?
- Basis in paper: [explicit] The paper mentions that legal statements are typically concise while specific scenarios involve more complex underlying logical reasoning, suggesting different approaches might be needed.
- Why unresolved: The paper uses a fixed ensemble approach but doesn't analyze whether different query types benefit differently from the various expansion methods.
- What evidence would resolve it: Ablation studies or adaptive weighting schemes that test the contribution of each expansion method for different query types.

### Open Question 3
- Question: How does the performance of the proposed retrieval system scale with increasing corpus size and complexity of legal provisions?
- Basis in paper: [inferred] The paper uses a corpus of 782 legal provisions, but does not test the system on larger or more complex legal corpora.
- Why unresolved: The study is limited to a relatively small legal corpus, leaving questions about scalability and performance on more extensive legal systems unanswered.
- What evidence would resolve it: Experiments on progressively larger legal corpora (e.g., national legal systems with thousands of provisions) measuring precision, recall, and computational efficiency.

## Limitations

- The system relies heavily on zero-shot LLM performance without task-specific fine-tuning, which may limit effectiveness for specialized legal domains
- Only tested on Japanese legal corpus and COLIEE datasets, limiting generalizability to other languages and legal systems
- Lacks ablation studies showing individual contribution of each component to overall performance

## Confidence

- **High confidence**: The overall system architecture and methodology are clearly described and logically sound
- **Medium confidence**: The reported performance metrics are impressive but lack detailed ablation analysis
- **Low confidence**: The reproducibility of Japanese-specific LLM prompts and zero-shot prompting effectiveness for legal tasks

## Next Checks

1. **Component Ablation Testing**: Systematically remove each LLM-based component and measure performance drop to quantify individual contributions
2. **Cross-Domain Generalization**: Test the same methodology on legal datasets from different jurisdictions to evaluate generalizability
3. **Prompt Engineering Analysis**: Conduct controlled experiments varying prompt templates to identify optimal structures for different legal domains