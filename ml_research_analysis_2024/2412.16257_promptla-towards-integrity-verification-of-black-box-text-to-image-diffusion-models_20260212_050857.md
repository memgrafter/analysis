---
ver: rpa2
title: 'PromptLA: Towards Integrity Verification of Black-box Text-to-Image Diffusion
  Models'
arxiv_id: '2412.16257'
source_url: https://arxiv.org/abs/2412.16257
tags:
- integrity
- promptla
- diffusion
- images
- verification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first integrity verification framework
  for black-box text-to-image diffusion models. The authors address the challenge
  of detecting model tampering where attackers fine-tune models to generate illegal
  content while circumventing safeguards.
---

# PromptLA: Towards Integrity Verification of Black-box Text-to-Image Diffusion Models

## Quick Facts
- arXiv ID: 2412.16257
- Source URL: https://arxiv.org/abs/2412.16257
- Reference count: 40
- First integrity verification framework for black-box text-to-image diffusion models achieving >0.96 mean AUC

## Executive Summary
This paper introduces PromptLA, the first integrity verification framework for black-box text-to-image diffusion models. The authors address the challenge of detecting model tampering where attackers fine-tune models to generate illegal content while circumventing safeguards. The proposed method measures integrity violations by computing KL divergence between feature distributions of images generated from different models using the same prompt. To achieve efficient verification, they develop PromptLA, a novel prompt selection algorithm based on learning automaton that identifies discriminating prompts with minimal queries.

## Method Summary
The method measures integrity violations by computing KL divergence between feature distributions of images generated from original and potentially tampered T2I models using the same prompts. PromptLA uses learning automaton to iteratively explore and eliminate less effective prompts while converging to the most discriminating ones. The framework extracts features using Inception-v3, approximates distributions as multivariate normal, and applies statistical hypothesis testing to determine integrity violations. It's evaluated on four advanced T2I models (SDXL, FLUX.1, etc.) with various integrity violations including full fine-tuning, LoRA, parameter modification, and version rollback.

## Key Results
- PromptLA achieves mean AUC exceeding 0.96 in integrity detection
- Outperforms baseline methods by more than 0.2 in AUC
- Shows strong effectiveness, generalization, and robustness against image-level post-processing
- Requires fewer queries than random selection strategies
- Works across different types of integrity violations and unknown attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model integrity violations cause detectable shifts in the feature distribution of generated images
- Mechanism: When a T2I model is fine-tuned, the underlying image generation process changes, altering the statistical properties of generated images. These changes manifest as shifts in feature distributions extracted by Inception-v3
- Core assumption: Feature distributions from a fixed pre-trained model can capture statistical differences between original and modified T2I models
- Break condition: If the feature extractor is not robust to the types of modifications made, or if modifications don't substantially alter statistical properties

### Mechanism 2
- Claim: KL divergence can quantify the distance between feature distributions of images from different models
- Mechanism: By approximating feature distributions as multivariate normal distributions and computing their KL divergence, we can measure how different two models' outputs are under the same prompt
- Core assumption: Feature distributions of generated images can be well-approximated by multivariate normal distributions
- Break condition: If feature distributions deviate significantly from multivariate normal, or if approximation introduces too much bias

### Mechanism 3
- Claim: Learning automaton can efficiently identify discriminating prompts that maximize KL divergence between models
- Mechanism: The PromptLA algorithm treats prompt selection as a stochastic optimization problem, using learning automaton to iteratively explore and eliminate less effective prompts while converging to the most discriminating ones
- Core assumption: There exists a subset of prompts that can effectively distinguish between original and modified models, and these can be found through iterative exploration
- Break condition: If optimal prompt doesn't exist in the prompt library, or if learning automaton fails to converge due to high stochasticity

## Foundational Learning

- Concept: KL divergence between probability distributions
  - Why needed here: KL divergence is the core metric for measuring differences between feature distributions of images from original vs. modified models
  - Quick check question: What does KL divergence measure between two probability distributions, and why is it suitable for comparing feature distributions?

- Concept: Learning automaton and reinforcement learning
  - Why needed here: Learning automaton is used in PromptLA to efficiently search for discriminating prompts without exhaustive testing
  - Quick check question: How does a learning automaton balance exploration and exploitation when searching for optimal actions in a stochastic environment?

- Concept: Feature extraction using pre-trained models
  - Why needed here: Inception-v3 is used to extract features from generated images that capture their statistical properties for comparison
  - Quick check question: Why is Inception-v3 suitable for feature extraction in this context, and what properties make a good feature extractor for integrity verification?

## Architecture Onboarding

- Component map: Prompt library generation (GPT-4) -> Feature extractor (Inception-v3) -> KL divergence computation module -> Learning automaton (PromptLA) -> Statistical hypothesis testing module -> Integrity decision module (threshold comparison)

- Critical path: Prompt → Image generation → Feature extraction → KL divergence computation → Learning automaton optimization → Integrity decision

- Design tradeoffs:
  - Accuracy vs. query cost: More prompts and images increase accuracy but raise computational cost
  - Robustness vs. sensitivity: Feature extraction must be robust to post-processing while sensitive to model changes
  - Generalization vs. specificity: Method must work across different types of integrity violations without being overfitted

- Failure signatures:
  - High false positive rate: Original model being flagged as modified (threshold too low or feature extractor too sensitive)
  - Low detection rate: Modified model not being detected (threshold too high or feature extractor not sensitive enough)
  - Slow convergence: Learning automaton taking too many iterations to find discriminating prompts

- First 3 experiments:
  1. Verify KL divergence works: Compare feature distributions from original vs. clearly modified models using simple prompts, confirm significant KL divergence
  2. Test feature extractor robustness: Apply common post-processing to generated images and verify feature distributions remain consistent
  3. Validate learning automaton: Test PromptLA on a simple scenario with known discriminating prompts, verify it converges to them efficiently

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a theoretically unified and convincing metric to measure the magnitude of integrity violations in deep neural networks, including text-to-image models?
- Basis in paper: The authors note that "different types of integrity violations can hardly be measured with respect to a unified metric regarding magnitude" and that "a theoretically unified and convincing metric on the magnitude of integrity violations to DNN, including T2I models, remains a challenge to be addressed."
- Why unresolved: Current approaches rely on ad-hoc measurements like Euclidean distance between parameters or decision boundaries, which don't capture the full complexity of model modifications and their impact on functionality.

### Open Question 2
- Question: How can integrity verification methods be extended to handle continuous prompt optimization rather than discrete prompt selection?
- Basis in paper: The authors state "In our future work, we will explore optimizing prompts in continuous spaces" and their current PromptLA algorithm operates on discrete prompt libraries.
- Why unresolved: Continuous prompt optimization would allow for more nuanced and potentially more effective discrimination between modified and original models, but raises challenges in gradient computation and optimization stability for black-box models.

### Open Question 3
- Question: How can integrity verification frameworks be generalized to other complex generative tasks beyond text-to-image models?
- Basis in paper: The authors mention "aim to extend the integrity verification framework to various complex generative tasks beyond text-to-image" as future work, suggesting current methods are T2I-specific.
- Why unresolved: Different generative modalities (text, audio, video) have different characteristics (randomness patterns, output complexity, access costs) that may require adapted verification approaches.

## Limitations

- The framework relies heavily on feature distribution shifts as indicators of model tampering, which may not capture all types of integrity violations
- The method assumes that generated images follow approximately normal distributions in feature space, an assumption that may not hold for all models or modifications
- Effectiveness depends on having sufficient diversity in the prompt library, and the quality of GPT-4-generated prompts could impact performance

## Confidence

- **High confidence**: KL divergence as a distance metric between feature distributions, and the general framework of using statistical hypothesis testing for integrity verification
- **Medium confidence**: The specific choice of Inception-v3 as feature extractor and the multivariate normal approximation for feature distributions
- **Medium confidence**: The effectiveness of PromptLA in real-world scenarios with unknown attacks, as evaluations were primarily on known attack types

## Next Checks

1. **Cross-model generalization test**: Apply the framework to additional T2I models not included in the original evaluation (e.g., Stable Video Diffusion, Midjourney API) to verify robustness across model architectures and training methodologies.

2. **Adversarial prompt analysis**: Systematically evaluate how the framework performs when attackers specifically design prompts to minimize distributional differences between original and modified models, testing the limits of prompt-based detection.

3. **Real-world deployment simulation**: Test the framework in a scenario where the verifier has limited access to the model (e.g., API rate limits, partial model access) and must make integrity decisions with incomplete information, measuring false positive/negative rates under these constraints.