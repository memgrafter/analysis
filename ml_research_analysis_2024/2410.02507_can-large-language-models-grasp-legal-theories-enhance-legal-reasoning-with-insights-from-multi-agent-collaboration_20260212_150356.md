---
ver: rpa2
title: Can Large Language Models Grasp Legal Theories? Enhance Legal Reasoning with
  Insights from Multi-Agent Collaboration
arxiv_id: '2410.02507'
source_url: https://arxiv.org/abs/2410.02507
tags:
- legal
- charge
- rule
- llms
- fact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the problem of improving large language models\u2019\
  \ (LLMs) understanding of legal theories and their reasoning capabilities. It introduces\
  \ a challenging task called \"confusing charge prediction\" to better evaluate LLMs\
  \ in this domain."
---

# Can Large Language Models Grasp Legal Theories? Enhance Legal Reasoning with Insights from Multi-Agent Collaboration

## Quick Facts
- arXiv ID: 2410.02507
- Source URL: https://arxiv.org/abs/2410.02507
- Reference count: 17
- Large language models achieve up to 67.7% accuracy improvement on complex legal reasoning tasks using the proposed multi-agent framework.

## Executive Summary
This paper introduces MALR, a multi-agent framework designed to enhance large language models' understanding of legal theories and reasoning capabilities. The framework tackles "confusing charge prediction" tasks, where models must distinguish between similar legal charges. By automatically decomposing complex legal tasks and extracting insights from legal rules through a non-parametric learning approach, MALR significantly outperforms existing methods on multiple real-world legal datasets.

## Method Summary
MALR employs a multi-agent system with four core components: an auto-planner that decomposes complex legal tasks into sub-tasks, role assignment for specialized agents to handle each sub-task, adaptive rule-insights training that learns from trial-and-error experiences, and reasoning with rule-insights that supplements legal rules with extracted insights. The framework uses non-parametric learning where LLMs gain experience through reasoning attempts, draw insights from both successes and errors, and store these insights to supplement legal rules. This mimics human learning processes and encourages LLMs to focus on key details within legal rules.

## Key Results
- MALR achieves up to 67.7% accuracy improvement for smaller LLMs and up to 14.3% for larger ones compared to strongest baselines
- The framework outperforms existing methods across multiple real-world datasets including CAIL2018, CJO, and CAIL-I
- Human evaluation shows MALR achieves 65% accuracy versus 62.5% for human experts on a small sample of 20 cases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Auto-planner decomposition reduces inconsistent reasoning by breaking complex legal tasks into sub-tasks.
- **Mechanism**: The framework uses an LLM to automatically decompose a legal reasoning task into a sequence of sub-tasks based on the legal rule and fact description. Each sub-task is then handled by a specialized agent.
- **Core assumption**: Complex reasoning tasks benefit from decomposition into smaller, more manageable sub-tasks that can be handled by specialized agents.
- **Evidence anchors**:
  - [abstract]: "encouraging LLMs to automatically decompose complex legal tasks"
  - [section 4.1]: "we designed an automatic planning module to decompose the task"
  - [corpus]: Weak - no direct corpus evidence found for auto-planning decomposition effectiveness
- **Break condition**: If the auto-planner fails to identify the correct sub-tasks or if the sub-tasks are not independent enough to be handled separately.

### Mechanism 2
- **Claim**: Adaptive rule-insights training improves understanding of legal rules by learning from trial-and-error experiences.
- **Mechanism**: The framework uses a non-parametric learning approach where LLMs gain experience through reasoning attempts, draw insights from both successes and errors, and store these insights to supplement legal rules.
- **Core assumption**: LLMs can learn to extract key information from legal rules through self-reflection on their reasoning successes and failures.
- **Evidence anchors**:
  - [abstract]: "employing non-parametric learning, encouraging LLMs to automatically decompose complex legal tasks and mimic human learning process to extract insights from legal rules"
  - [section 4.3]: "we design the insights training module, as shown in Figure 3 (B), which consists of three core processes: experience gaining, insights drawing from errors and successes, and insights filtering"
  - [corpus]: Weak - no direct corpus evidence found for non-parametric learning effectiveness in legal reasoning
- **Break condition**: If the LLM fails to identify meaningful patterns from its experiences or if the insights drawn are incorrect or misleading.

### Mechanism 3
- **Claim**: Reasoning through insights improves critical thinking by supplementing rules with key information and enabling fact-checking.
- **Mechanism**: The framework retrieves relevant insights for each question to improve reasoning, and guides LLMs to ask factuality questions when facing knowledge gaps.
- **Core assumption**: Supplementing legal rules with extracted insights and enabling external knowledge feedback improves LLM's ability to handle complex reasoning.
- **Evidence anchors**:
  - [abstract]: "These insights supplement the rules, encouraging LLMs to focus on key factors from legal knowledge"
  - [section 4.4]: "The generated insights serve two purposes: (1) they supplement the rules as additional notes, and (2) they guide LLMs to inquire about uncertainties when facing knowledge gaps"
  - [corpus]: Weak - no direct corpus evidence found for insight-based reasoning improvement
- **Break condition**: If the insights are not relevant to the specific case or if the external knowledge feedback is incorrect or unavailable.

## Foundational Learning

- **Concept**: Multi-step reasoning and decomposition
  - Why needed here: Legal reasoning often involves complex, multi-step processes that require breaking down into smaller, manageable tasks
  - Quick check question: Can you identify the key steps in a legal reasoning process and explain how they might be decomposed?

- **Concept**: Non-parametric learning and self-reflection
  - Why needed here: The framework relies on the LLM's ability to learn from its own experiences without explicit training data
  - Quick check question: How might an LLM use self-reflection to improve its performance on a specific task?

- **Concept**: Rule-based reasoning and element extraction
  - Why needed here: Understanding legal rules requires identifying key elements and their relationships
  - Quick check question: Can you identify the key elements in a legal rule and explain how they relate to each other?

## Architecture Onboarding

- **Component map**: Auto-planner -> Role Assignment -> Adaptive Rule-Insights Training -> Reasoning with Rule-Insights
- **Critical path**: Auto-planner → Role Assignment → Adaptive Rule-Insights Training → Reasoning with Rule-Insights
- **Design tradeoffs**:
  - Complexity vs. Performance: More complex decomposition and insights may improve performance but increase computational cost
  - Generalization vs. Specialization: Specialized agents may perform better on specific tasks but may not generalize well to new tasks
- **Failure signatures**:
  - Inconsistent reasoning: Auto-planner fails to identify correct sub-tasks
  - Overlooking key details: Insights training fails to extract important information
  - Hallucinations: Reasoning through insights leads to incorrect conclusions
- **First 3 experiments**:
  1. Test auto-planner decomposition on a simple legal task with known sub-tasks
  2. Evaluate insights training on a small set of cases with known correct and incorrect answers
  3. Assess reasoning through insights on a set of cases with varying levels of complexity and external knowledge availability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MALR compare to human experts in real-world legal settings beyond the curated datasets?
- Basis in paper: [inferred] The paper mentions human evaluation showing humans achieved 62.5% accuracy compared to MALR's 65% on a small sample of 20 cases.
- Why unresolved: The paper only tested on a small sample of cases. Real-world legal reasoning involves more complex scenarios, incomplete information, and requires consideration of legal precedents and context.
- What evidence would resolve it: A large-scale study comparing MALR's performance to experienced legal professionals across diverse real-world cases with varying complexity levels.

### Open Question 2
- Question: How does MALR handle cases involving multiple confusing charges where the distinctions are not based on a single element but rather a combination of elements?
- Basis in paper: [inferred] The paper focuses on confusing charge pairs with single-element differences. The legal reasoning process described involves breaking down rules into four elements (Subject, Mental, Object, Conduct).
- Why unresolved: Many real legal cases involve charges with overlapping elements and nuanced distinctions. The current framework may struggle with cases requiring multi-faceted analysis.
- What evidence would resolve it: Testing MALR on datasets containing cases with multiple confusing charges and evaluating its ability to distinguish between them based on combined element analysis.

### Open Question 3
- Question: What is the long-term effectiveness of the rule-insights learning mechanism? Does MALR's performance degrade over time as it encounters new legal scenarios not covered in its training data?
- Basis in paper: [inferred] The paper describes a trial-and-error learning process where MALR generates insights from successful and failed reasoning attempts. The framework is tested on static datasets.
- Why unresolved: The paper doesn't address how MALR adapts to evolving legal landscapes, new charges, or changing interpretations of existing laws. It also doesn't explore the potential for catastrophic forgetting.
- What evidence would resolve it: Longitudinal studies tracking MALR's performance over extended periods and across evolving legal datasets, including measures of adaptability and robustness to concept drift.

## Limitations
- Limited generalizability beyond tested Chinese legal domains
- Benchmark-dependent improvements may not transfer to other legal reasoning tasks
- Framework's reliance on LLMs for task decomposition introduces potential brittleness

## Confidence
- **High**: Claims about MALR's superior performance on tested datasets
- **Medium**: Claims about decomposition and insights improving accuracy
- **Low**: Claims about generalisability and robustness to new legal domains

## Next Checks
1. Conduct ablation tests to measure the individual contributions of auto-planning, role assignment, and insights training.
2. Evaluate MALR on a held-out, cross-jurisdictional dataset to test robustness to different legal systems.
3. Analyse cases where MALR underperforms to identify systematic failure modes in decomposition or insights generation.