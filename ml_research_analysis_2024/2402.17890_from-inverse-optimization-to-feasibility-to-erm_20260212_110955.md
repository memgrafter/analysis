---
ver: rpa2
title: From Inverse Optimization to Feasibility to ERM
arxiv_id: '2402.17890'
source_url: https://arxiv.org/abs/2402.17890
tags:
- convex
- optimization
- cited
- problem
- inverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of contextual inverse optimization,
  where the goal is to infer unknown problem parameters (such as cost vectors) of
  an optimization problem from known solutions and contextual information. The authors
  focus on contextual inverse linear programming (CILP) and propose a reduction of
  CILP to a convex feasibility problem, enabling the use of standard algorithms like
  alternating projections.
---

# From Inverse Optimization to Feasibility to ERM

## Quick Facts
- arXiv ID: 2402.17890
- Source URL: https://arxiv.org/abs/2402.17890
- Reference count: 40
- One-line primary result: Proposes reduction of contextual inverse linear programming to convex feasibility and ERM, enabling scalable first-order optimization with convergence guarantees.

## Executive Summary
This paper addresses contextual inverse linear programming (CILP) by proposing a novel reduction to convex feasibility and empirical risk minimization (ERM). The authors exploit KKT conditions to construct a convex set of feasible cost vectors, enabling the use of alternating projections. They further reduce the problem to ERM on a smooth, convex loss satisfying the Polyak-Lojasiewicz condition, allowing scalable first-order optimization methods. The proposed method achieves improved performance compared to existing approaches while providing theoretical convergence guarantees without requiring additional assumptions like degeneracy or interpolation.

## Method Summary
The method reduces CILP to convex feasibility by constructing a convex set C of cost vectors via KKT conditions that yield known optimal decisions. This feasibility problem is solved using alternating projections. The authors further reduce CILP to ERM by defining a loss function as the mean squared distance between predicted cost vectors and the feasible set C. For linear models, this loss is convex and satisfies the Polyak-Lojasiewicz condition, enabling gradient descent methods with linear convergence guarantees. The approach uses preconditioned gradient descent or adaptive optimizers and includes a margin parameter for robustness.

## Key Results
- CILP can be reduced to convex feasibility and ERM problems with standard convergence guarantees
- Proposed method outperforms existing approaches on decision loss and is comparable on estimate loss
- Method scales well with problem dimension and dataset size, demonstrating computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** KKT conditions enable reduction to convex feasibility
- **Mechanism:** KKT optimality conditions are used to construct convex set C of cost vectors yielding known optimal decisions, which is intersected with feasible set F to create a convex feasibility problem
- **Core assumption:** KKT conditions are necessary and sufficient for LP optimality, and sets C and F are convex in cost vector c
- **Evidence anchors:** [abstract] "reduce CILP to a convex feasibility problem allowing the use of standard algorithms such as alternating projections"; [section] "KKT conditions give necessary and sufficient conditions for the optimality of the LP"
- **Break condition:** If optimization problem is not linear or KKT conditions are not both necessary and sufficient, convexity of C cannot be guaranteed

### Mechanism 2
- **Claim:** ERM reduction enables scalable first-order optimization
- **Mechanism:** Loss function h(θ) is defined as mean squared distance between predicted cost vectors and feasible set C, shown to be convex and PL for linear models
- **Core assumption:** Loss function h(θ) is convex and satisfies PL condition for linear models with accessible gradient
- **Evidence anchors:** [abstract] "reduce CILP to empirical risk minimization (ERM) on a smooth, convex loss that satisfies the Polyak-Lojasiewicz condition"; [section] "h(θ) is not necessarily strongly-convex but satisfies the PL inequality"
- **Break condition:** If model is non-linear, h(θ) may not be convex or PL, breaking linear convergence guarantees

### Mechanism 3
- **Claim:** Proposed sub-optimality metric Γ(θ, (z, x*)) provides meaningful decision quality measure
- **Mechanism:** Metric uses projection of predicted cost vector onto feasible set C as proxy for ground-truth cost, measuring optimality gap in decision with scale invariance
- **Core assumption:** Projection PC(cθ) is good proxy for ground-truth cost vector c*, and decisions are bounded
- **Evidence anchors:** [abstract] "propose a new sub-optimality metric"; [section] "We use the projection of cθ onto C as a proxy for the ground-truth c* and define the suboptimality gap as: Γ(z, x*) := ⟨PC(cθ)/∥PC(cθ)∥², ˆx(cθ) − x*⟩"
- **Break condition:** If projection PC(cθ) is poor proxy for c*, or if decisions are unbounded, metric may not accurately reflect decision quality

## Foundational Learning

- **Concept: Linear Programming and KKT Conditions**
  - Why needed here: KKT conditions construct feasible set C of cost vectors yielding known optimal decisions
  - Quick check question: Can you write the KKT conditions for a standard form LP and explain why they are both necessary and sufficient for optimality?

- **Concept: Convex Optimization and Alternating Projections**
  - Why needed here: Reduction to convex feasibility enables use of alternating projections algorithm
  - Quick check question: What are the convergence guarantees for alternating projections, and under what conditions do they apply?

- **Concept: Empirical Risk Minimization and the Polyak-Lojasiewicz Condition**
  - Why needed here: ERM reduction on PL loss enables scalable first-order methods with linear convergence
  - Quick check question: What is the Polyak-Lojasiewicz condition, and how does it relate to strong convexity? Can you give an example of a function that satisfies PL but not strong convexity?

## Architecture Onboarding

- **Component map:** Input (zi, x* i) -> Model (fθ(zi) = ziθ) -> Feasibility reduction (construct sets C and F) -> ERM reduction (define loss h(θ)) -> Optimization (alternating projections or gradient descent) -> Output (learned θ)

- **Critical path:** 1) Construct feasible set C for each data point using KKT conditions; 2) Define loss function h(θ) as mean squared distance between predicted and feasible cost vectors; 3) Minimize h(θ) using alternating projections or gradient descent; 4) Use learned model to predict cost vectors for new contextual information

- **Design tradeoffs:**
  - Linear vs non-linear models: Linear models allow convex loss and strong convergence guarantees but may be less expressive
  - Exact vs approximate projections: Exact projections are computationally expensive but approximate projections may lead to slower convergence
  - Margin parameter χ: Adding margin improves robustness but requires tuning

- **Failure signatures:**
  - Alternating projections fail to converge: Intersection C ∩ F may be empty or projections not computed accurately
  - Gradient descent converges slowly: Loss h(θ) may not be well-conditioned or learning rate poorly tuned
  - Sub-optimality metric does not correlate with decision quality: Projection PC(cθ) may not be good proxy for ground-truth cost

- **First 3 experiments:**
  1. Verify KKT conditions are both necessary and sufficient for LP optimality on small synthetic dataset
  2. Implement alternating projections to solve convex feasibility problem and verify convergence on small dataset
  3. Implement gradient descent to minimize ERM loss and compare convergence speed to alternating projections on larger dataset

## Open Questions the Paper Calls Out
- How to extend framework to handle unknown constraints in optimization problem
- Whether the framework can be generalized to non-linear models beyond linear prediction

## Limitations
- Convexity properties critical to reductions may not extend to non-linear models
- Effectiveness of sub-optimality metric depends on quality of projection as proxy for ground-truth cost
- Computational complexity of exact projections scales poorly with problem size

## Confidence

**High confidence:** The reduction from CILP to convex feasibility via KKT conditions is well-established theoretically with strong guarantees for linear programs.

**Medium confidence:** The ERM reduction and associated convergence guarantees hold for linear models under stated assumptions but may not generalize to non-linear cases.

**Medium confidence:** The proposed sub-optimality metric Γ(θ, (z, x*)) provides meaningful measure of decision quality, though effectiveness depends on problem-specific factors.

## Next Checks

1. Test algorithm's behavior on non-linear optimization problems to assess breakdown of convexity assumptions and convergence guarantees.

2. Experiment with approximate projections onto C (using tolerance ε) to evaluate tradeoff between computational efficiency and solution quality.

3. Validate sub-optimality metric Γ(θ, (z, x*)) on problems with known ground-truth cost vectors to assess correlation with actual decision quality.