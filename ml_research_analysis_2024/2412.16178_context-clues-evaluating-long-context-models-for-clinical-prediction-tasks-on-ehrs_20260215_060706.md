---
ver: rpa2
title: 'Context Clues: Evaluating Long Context Models for Clinical Prediction Tasks
  on EHRs'
arxiv_id: '2412.16178'
source_url: https://arxiv.org/abs/2412.16178
tags:
- context
- mamba
- llama
- hyena
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic evaluation of how context
  length impacts electronic health record (EHR) modeling using subquadratic architectures
  like Mamba. While prior EHR foundation models used short 512-token contexts due
  to transformer scaling limitations, this work evaluates models across 8 context
  lengths up to 16k tokens.
---

# Context Clues: Evaluating Long Context Models for Clinical Prediction Tasks on EHRs

## Quick Facts
- arXiv ID: 2412.16178
- Source URL: https://arxiv.org/abs/2412.16178
- Authors: Michael Wornow; Suhana Bedi; Miguel Angel Fuentes Hernandez; Ethan Steinberg; Jason Alan Fries; Christopher Re; Sanmi Koyejo; Nigam H. Shah
- Reference count: 40
- Key outcome: Mamba-based models achieve state-of-the-art AUROC scores on 9/14 clinical prediction tasks from EHRSHOT benchmark, demonstrating the potential of long-context subquadratic architectures for EHR modeling

## Executive Summary
This paper presents the first systematic evaluation of how context length impacts electronic health record (EHR) modeling using subquadratic architectures like Mamba. While prior EHR foundation models used short 512-token contexts due to transformer scaling limitations, this work evaluates models across 8 context lengths up to 16k tokens. The authors find that Mamba-based models achieve state-of-the-art AUROC scores on 9/14 clinical prediction tasks from the EHRSHOT benchmark, outperforming the previous best model by 0.03 points. Beyond performance, they analyze three EHR-specific properties - copy-forwarding creating token repetition, irregular time intervals between events, and disease progression increasing token complexity - and find that while all negatively impact model performance, longer context models are more robust to these challenges.

## Method Summary
The authors evaluate 16 models across four architectures (GPT, Llama, Mamba, Hyena) and six context lengths (512-16k tokens) using next-token prediction on deidentified longitudinal EHR data from an academic medical center (2.5M patients for training, 0.5M for validation). They use the EHRSHOT benchmark (14 binary classification tasks on 7k patients' longitudinal EHRs) for evaluation, extracting embeddings from the last token of each patient's sequence and training logistic regression heads. The study systematically analyzes how EHR-specific properties (token repetition, irregular time intervals, disease progression) correlate with model performance across different context lengths and architectures.

## Key Results
- Mamba-based models achieve state-of-the-art AUROC scores on 9/14 tasks from the EHRSHOT clinical prediction benchmark
- Longer context versions of Mamba and Llama consistently achieve lower perplexities across all token positions compared to shorter contexts
- Longer context models are more robust to EHR-specific challenges like copy-forwarding and irregular time intervals, achieving significantly lower Brier scores across all quartiles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subquadratic architectures like Mamba achieve state-of-the-art AUROC scores on clinical prediction tasks by leveraging longer context windows to process more comprehensive patient histories.
- Mechanism: Mamba's state-space model (SSM) design allows linear scaling with input length, avoiding the quadratic complexity of transformer attention. This enables processing full-length EHRs (potentially 10k+ events) without computational explosion, allowing the model to capture longer-term disease progression patterns and more complete clinical trajectories.
- Core assumption: The relationship between context length and predictive performance in EHR modeling follows similar scaling laws as observed in other domains, despite the unique noise characteristics of clinical data.
- Evidence anchors: abstract showing Mamba surpasses prior SOTA on 9/14 tasks; section confirming SOTA AUROC scores on 9/14 tasks.

### Mechanism 2
- Claim: Longer context models are more robust to EHR-specific challenges like copy-forwarding and irregular time intervals.
- Mechanism: Extended context windows allow models to better distinguish between meaningful clinical events and artificial repetitions caused by billing practices. With more complete temporal information, models can better handle irregular time intervals by maintaining broader context for temporal reasoning rather than being forced to make predictions based on sparse or fragmented sequences.
- Core assumption: The additional context provided by longer windows enables better discrimination between signal and noise in EHR data, rather than simply amplifying all patterns equally.
- Evidence anchors: abstract showing longer context models are more robust to extreme levels of EHR-specific properties; section showing lower Brier scores across all quartiles.

### Mechanism 3
- Claim: Disease progression in EHRs creates increasing token complexity over time, which longer context models handle more effectively.
- Mechanism: As patients age, their medical histories become more complex with multiple comorbidities and varied disease trajectories. Longer context models can maintain more complete representations of earlier health states, allowing them to better contextualize later, more complex clinical events. This contrasts with shorter contexts that may lose critical early information needed for understanding disease progression.
- Core assumption: The relationship between disease complexity and token perplexity in EHRs follows a predictable pattern that can be modeled effectively with sufficient context.
- Evidence anchors: abstract showing later tokens are harder to predict and longer context models are more robust; section showing later tokens have higher perplexity and longer contexts achieve lower perplexities.

## Foundational Learning

- Concept: State-space models (SSMs) as an alternative to transformers for sequence modeling
  - Why needed here: SSMs provide linear scaling with sequence length, making them computationally feasible for processing full-length EHRs that can exceed 10k tokens, whereas transformers scale quadratically
  - Quick check question: How does the computational complexity of SSMs compare to transformers when processing sequences of length n?

- Concept: Copy-forwarding and its impact on sequence modeling
  - Why needed here: Understanding how billing practices create artificial repetition in EHR data is crucial for designing models that can distinguish meaningful clinical signals from administrative noise
  - Quick check question: What metric would you use to quantify the level of artificial repetition in an EHR sequence caused by copy-forwarding?

- Concept: Temporal irregularity in clinical event sequences
  - Why needed here: EHR data exhibits highly irregular time intervals between events, unlike the uniform spacing in natural language, requiring models to handle diverse temporal scales within single sequences
  - Quick check question: How would you measure the irregularity of time intervals between clinical events in a patient's timeline?

## Architecture Onboarding

- Component map: Input sequence → Tokenizer (OMOP CDM formatting) → Model (Mamba/Transformer variant) → Context window selection → Logistic regression head → Binary classification output
- Critical path: Tokenization → Context window selection → Model inference → Representation aggregation → Classification
- Design tradeoffs: Longer context lengths improve performance but increase computational cost; simpler tokenization strategies reduce complexity but may lose temporal information; different architectures offer varying balances of performance and efficiency
- Failure signatures: Performance degradation with increased context length (as seen with Hyena), sensitivity to copy-forwarding artifacts, inability to handle irregular time intervals effectively
- First 3 experiments:
  1. Compare Mamba performance across context lengths (1k, 4k, 8k, 16k) on a subset of EHRSHOT tasks to verify the context length scaling relationship
  2. Measure n-gram repetition rates in synthetic EHR data with varying levels of artificial repetition to understand model sensitivity to copy-forwarding
  3. Evaluate model performance on patients stratified by time interval irregularity to quantify the impact of temporal irregularity on different architectures

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions in the text provided, but based on the limitations section, key areas for future work include: how EHR-specific properties like copy-forwarding, irregular time intervals, and disease progression affect the pretraining process itself, not just fine-tuning performance; whether the benefits of longer context generalize across different healthcare systems and documentation practices; and how to optimize temporal modeling specifically for EHR data beyond standard positional embeddings.

## Limitations
- Primary limitation is reliance on proprietary deidentified EHR data from an academic medical center that is not publicly available, limiting exact reproduction of results
- Evaluation based on a limited comparison set of only four architectures, potentially missing performance differences with other emerging subquadratic architectures
- Findings may not generalize across different healthcare systems with varying documentation practices, patient populations, or billing procedures

## Confidence

**High Confidence**: The finding that longer context models achieve higher AUROC scores on clinical prediction tasks (Mamba achieving SOTA on 9/14 tasks) is well-supported by the experimental results with systematic evaluation across 8 context lengths and 4 architectures.

**Medium Confidence**: The claim that longer context models are more robust to EHR-specific challenges (copy-forwarding, irregular time intervals, disease progression) is supported by the analysis but relies on indirect evidence showing correlation rather than establishing causation.

**Medium Confidence**: The superiority of Mamba over transformer-based architectures for long-context EHR modeling is demonstrated on this specific task set, but the evaluation doesn't include newer transformer variants optimized for long sequences that might perform differently.

## Next Checks

1. **Cross-Dataset Generalization**: Train and evaluate the best-performing models (Mamba at longer context lengths) on a completely independent EHR dataset from a different healthcare system to verify that the context length benefits generalize beyond the original training distribution.

2. **Alternative Temporal Modeling**: Compare the performance of standard positional embeddings against time-aware transformer variants or explicit temporal encoding schemes to determine if the observed benefits of longer context are primarily due to sequence length or could be further improved with better temporal modeling.

3. **Real-World Deployment Metrics**: Evaluate the models using clinical utility metrics beyond AUROC, such as calibration curves, decision curve analysis, or clinician-in-the-loop studies to assess whether the statistical improvements translate to practical clinical value and whether the longer context models show different behavior in high-stakes prediction scenarios.