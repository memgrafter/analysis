---
ver: rpa2
title: 'Self-Supervised Learning for Time Series: A Review & Critique of FITS'
arxiv_id: '2410.18318'
source_url: https://arxiv.org/abs/2410.18318
tags:
- time
- fits
- series
- data
- frequency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive evaluation of the Frequency
  Interpolation for Time Series (FITS) model, demonstrating that a simple one-layer
  neural network trained in the complex frequency domain can achieve competitive performance
  with significantly fewer parameters compared to state-of-the-art models. The authors
  successfully reproduced FITS results on benchmark datasets and found that the model
  excels at capturing periodic and seasonal patterns but struggles with trending,
  non-periodic, or random-resembling behavior.
---

# Self-Supervised Learning for Time Series: A Review & Critique of FITS

## Quick Facts
- arXiv ID: 2410.18318
- Source URL: https://arxiv.org/abs/2410.18318
- Reference count: 40
- This paper presents a comprehensive evaluation of the Frequency Interpolation for Time Series (FITS) model, demonstrating that a simple one-layer neural network trained in the complex frequency domain can achieve competitive performance with significantly fewer parameters compared to state-of-the-art models.

## Executive Summary
This paper presents a comprehensive evaluation of the Frequency Interpolation for Time Series (FITS) model, demonstrating that a simple one-layer neural network trained in the complex frequency domain can achieve competitive performance with significantly fewer parameters compared to state-of-the-art models. The authors successfully reproduced FITS results on benchmark datasets and found that the model excels at capturing periodic and seasonal patterns but struggles with trending, non-periodic, or random-resembling behavior. They introduced two novel hybrid approaches combining FITS with DLinear, achieving the best results of any known open-source model on multivariate regression tasks and showing promising improvements on price datasets. The study also provides valuable insights into when FITS performs well (data with clear periodicity) versus when simpler baselines like Naïve forecasting perform better (random or non-periodic data), contributing both methodological improvements and practical guidance for time series forecasting applications.

## Method Summary
The paper reproduces the FITS model, which uses a single complex-valued linear layer in the frequency domain to perform time series forecasting. The method transforms time series data using FFT, applies low-pass filtering, and learns frequency interpolation. The authors also introduce hybrid approaches combining FITS with DLinear, which decomposes time series into trend and seasonal components. The evaluation uses 9 benchmark datasets and 2 price datasets, comparing performance against models like DLinear, NLinear, PatchTST, and ARIMA using MSE and MAE metrics for multivariate long-term forecasting.

## Key Results
- FITS achieves competitive performance with significantly fewer parameters by leveraging frequency domain interpolation via a single complex-valued linear layer
- The model excels at periodic and seasonal patterns but struggles with trending, non-periodic, or random-resembling behavior
- Two novel hybrid approaches combining FITS with DLinear achieved the best results of any known open-source model on multivariate regression tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FITS achieves competitive performance with significantly fewer parameters by leveraging frequency domain interpolation via a single complex-valued linear layer
- Mechanism: FITS transforms time series data into the frequency domain using FFT, applies a low-pass filter to remove high-frequency noise, and then learns to interpolate between input and output frequencies using a single complex linear layer. This exploits the fact that many time series have dominant periodic patterns that can be captured in the frequency domain with minimal parameters
- Core assumption: The time series data contains sufficient periodic/seasonal patterns that can be effectively captured in the frequency domain
- Evidence anchors:
  - [abstract] "training a one-layer neural network in the complex frequency domain can achieve competitive performance with significantly fewer parameters"
  - [section 3.2] "By training in the complex frequency domain, the neural network layer efficiently learns amplitude scaling and phase shifting to interpolate time series"
  - [corpus] Weak evidence - no direct citations about frequency domain effectiveness
- Break condition: When time series data lacks clear periodicity or contains dominant non-periodic trends that cannot be captured in the frequency domain

### Mechanism 2
- Claim: The hybrid FITS + DLinear approach improves performance by combining frequency-domain periodic pattern capture with time-domain trend/seasonal decomposition
- Mechanism: DLinear first decomposes the time series into trend and seasonal components using moving average filtering, then FITS processes the residual signal in the frequency domain. The final prediction combines all three components, allowing the model to capture both periodic patterns (FITS) and non-periodic trends/seasonality (DLinear)
- Core assumption: Time series data contains both periodic patterns that benefit from frequency domain processing AND non-periodic trends/seasonality that benefit from time-domain decomposition
- Evidence anchors:
  - [abstract] "two novel hybrid approaches, where we attempt to remedy the weaknesses of FITS by combining it with DLinear"
  - [section 3.5] "FITS takes a different approach by first transforming the time series data into the frequency domain before modeling with a complex-valued linear layer"
  - [corpus] Moderate evidence - TS2Vec-Ensemble paper shows self-supervised frameworks can enhance forecasting, supporting hybrid approaches
- Break condition: When either component (periodic or non-periodic patterns) is absent or when the decomposition introduces information loss

### Mechanism 3
- Claim: FITS performs poorly on random-resembling data because Fourier transformation cannot effectively capture non-periodic patterns
- Mechanism: When time series data exhibits random walk behavior or lacks periodicity, the Fourier components become meaningless for forecasting. The model attempts to fit noise in the frequency domain, leading to poor generalization and performance worse than simple baselines like Naïve forecasting
- Core assumption: Random walk data has no exploitable frequency-domain structure that can improve forecasting beyond simple baselines
- Evidence anchors:
  - [abstract] "FITS especially excels at capturing periodic and seasonal patterns, but struggles with trending, non-periodic, or random-resembling behavior"
  - [section 3.2] "high frequencies in the model usually only represent the noisy terms in the time series"
  - [corpus] Weak evidence - no direct citations about random walk limitations
- Break condition: When time series exhibits strong autocorrelation or when the signal-to-noise ratio is sufficiently high

## Foundational Learning

- Concept: Fourier Transform and frequency domain representation
  - Why needed here: FITS fundamentally operates in the frequency domain, so understanding how signals are decomposed into sinusoidal components is essential
  - Quick check question: What is the Nyquist-Shannon sampling theorem and why is it important for time series analysis?

- Concept: Time series stationarity and differencing
  - Why needed here: ARIMA and DLinear models require understanding of stationarity concepts for proper model selection and interpretation
  - Quick check question: How does first-order differencing transform a non-stationary time series into a stationary one?

- Concept: Autocorrelation and memory in time series
  - Why needed here: The Hurst exponent and ACF analysis are used to understand predictability patterns in the data, which directly informs whether FITS or other models are appropriate
  - Quick check question: What does an ACF plot that decays slowly indicate about a time series' memory properties?

## Architecture Onboarding

- Component map: Normalization → FFT → low-pass filtering → complex linear layer → inverse FFT → denormalization. Hybrid models add DLinear decomposition (trend/seasonal) before or after FITS processing.
- Critical path: For FITS: FFT computation → frequency interpolation learning → inverse FFT. For hybrids: decomposition → FITS processing → combination of components.
- Design tradeoffs: Single-layer simplicity vs. potential for deeper architectures; frequency domain focus vs. time domain flexibility; parameter efficiency vs. modeling capacity.
- Failure signatures: Poor performance on exchange rate and stock data indicates FITS struggles with random-resembling data; degraded performance on longer forecast horizons suggests limitations in capturing long-term dependencies.
- First 3 experiments:
  1. Run FITS on Electricity dataset (clear periodicity) vs. Exchange Rate dataset (random-resembling) to observe performance differential
  2. Test hybrid FITS + DLinear on multivariate price dataset to verify improved performance over standalone models
  3. Vary harmonic order and lookback window on ETTh1 to understand parameter sensitivity and optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Hurst exponent be reliably calculated for time series with multiple linear trends or non-stationary noise patterns?
- Basis in paper: [explicit] The paper notes that the Hurst exponent becomes unreliable for noisy signals with linear trends and suggests it measures noise levels rather than predictability
- Why unresolved: The authors observe that H ≈ 0.5 can appear in non-random data when non-stationary and noisy, but don't provide an alternative method or correction for this limitation
- What evidence would resolve it: Comparative analysis of H values before and after detrending, or development of a modified H calculation that accounts for multiple trend components

### Open Question 2
- Question: What is the theoretical limit of improvement when combining FITS with DLinear, and at what point does additional complexity become counterproductive?
- Basis in paper: [explicit] The authors note that hybrid models show improvement but question whether added complexity is necessary, as default FITS may already capture most relevant frequency components
- Why unresolved: The paper shows marginal improvements from hybrid approaches but doesn't explore the theoretical performance ceiling or optimal parameter configurations for different data types
- What evidence would resolve it: Systematic ablation studies varying the number of layers and neurons in hybrid models across diverse datasets, identifying the point where MSE improvements plateau

### Open Question 3
- Question: Can the FITS model be adapted to effectively handle signals with periods longer than the output sequence length?
- Basis in paper: [explicit] The authors identify that FITS struggles with signals where the major period exceeds the output length due to spectral leakage artifacts
- Why unresolved: While the paper demonstrates this failure mode and shows spectral leakage effects, it doesn't propose solutions or investigate whether windowing techniques or adaptive sampling could mitigate this limitation
- What evidence would resolve it: Experiments testing modified FITS architectures with variable window sizes or alternative spectral decomposition methods on datasets with known long-period signals

### Open Question 4
- Question: How does the choice of activation function (ModReLU vs CReLU) affect the stability and convergence of deep FITS models in practice?
- Basis in paper: [explicit] The authors implement both activation functions for complex-valued networks but note that CReLU may alter phase information while ModReLU introduces a learnable dead-zone
- Why unresolved: The paper shows CReLU performs worse than ModReLU in image classification but doesn't provide empirical comparison for time series forecasting tasks with the specific architecture used
- What evidence would resolve it: Controlled experiments comparing training stability, convergence speed, and final performance of deep FITS variants using different activation functions on benchmark datasets with varying periodicity characteristics

## Limitations

- The paper lacks rigorous parameter counting across all compared models to support the claim of achieving competitive performance with significantly fewer parameters
- Missing quantitative analysis of computational cost comparison between FITS and baselines across different hardware configurations
- The exact implementation details of low-pass filter cutoff frequency selection and normalization procedures are not fully documented

## Confidence

**High Confidence**: The core finding that FITS excels at periodic/seasonal patterns while struggling with random-resembling data is well-supported by the empirical results across multiple datasets.

**Medium Confidence**: The hybrid approaches combining FITS with DLinear show promising results, but the relatively small number of datasets tested limits generalizability.

**Low Confidence**: The assertion that FITS can achieve "competitive performance with significantly fewer parameters" lacks rigorous parameter counting across all compared models.

## Next Checks

1. Create synthetic time series with controlled periodicity, trends, and noise levels to systematically validate when FITS outperforms or underperforms baselines, particularly focusing on the transition point between periodic and random-resembling behavior.

2. Test the hybrid FITS + DLinear approach on additional multivariate time series datasets beyond the 9 benchmarks and 2 price datasets to verify the claimed superiority in multivariate regression tasks.

3. Conduct systematic ablation studies varying the lookback window, harmonic order, and low-pass filter parameters to understand the robustness of FITS performance across different configurations and identify optimal settings for different data characteristics.