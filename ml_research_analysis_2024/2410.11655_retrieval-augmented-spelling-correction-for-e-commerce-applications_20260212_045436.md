---
ver: rpa2
title: Retrieval Augmented Spelling Correction for E-Commerce Applications
arxiv_id: '2410.11655'
source_url: https://arxiv.org/abs/2410.11655
tags:
- retrieval
- spelling
- context
- fine-tuning
- mistral-7b
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of spelling correction for e-commerce
  queries, especially distinguishing genuine misspellings from unconventional brand
  names. It proposes a Retrieval Augmented Generation (RAG) approach, where product
  names are retrieved from a catalog and used as context for a fine-tuned large language
  model (LLM) to predict correct spellings.
---

# Retrieval Augmented Spelling Correction for E-Commerce Applications

## Quick Facts
- **arXiv ID**: 2410.11655
- **Source URL**: https://arxiv.org/abs/2410.11655
- **Reference count**: 7
- **Primary result**: RAG approach with ColBERT retrieval and contextual fine-tuning achieves 70.1 F1 score, 34.2 points above RAG baseline

## Executive Summary
This paper addresses the challenge of spelling correction for e-commerce queries, particularly distinguishing genuine misspellings from unconventional brand names. The authors propose a Retrieval Augmented Generation (RAG) approach where product names are retrieved from a catalog and used as context for a fine-tuned large language model to predict correct spellings. Experiments with Mistral-7B and Claude-3-sonnet LLMs, and BM25, Fuzzy BM25, and ColBERT retrieval models, demonstrate that RAG consistently improves spelling correction performance, with fine-tuning the LLM with retrieved context further enhancing results.

## Method Summary
The proposed approach combines retrieval and generation to improve spelling correction accuracy. First, relevant product names are retrieved from the catalog using different retrieval models (BM25, Fuzzy BM25, ColBERT). These retrieved product names serve as context for a large language model (Mistral-7B or Claude-3-sonnet) to determine whether a query contains a spelling error and what the correct spelling should be. The system is trained on a dataset of misspelled queries paired with their corrections, with a special focus on queries containing brand names. The LLM is fine-tuned to leverage the retrieved context effectively, learning to distinguish between genuine misspellings and unconventional but correct brand names.

## Key Results
- RAG consistently improves spelling correction performance across all tested retrieval and LLM combinations
- ColBERT retrieval model outperforms BM25 and Fuzzy BM25 variants in all scenarios
- Fine-tuning the LLM with retrieved context further improves results, especially for queries containing brand names
- Best configuration (ColBERT + contextual fine-tuning) achieves 70.1 F1 score, a 34.2 point increase over RAG alone
- System demonstrates ability to handle 99% of queries without failure

## Why This Works (Mechanism)
The RAG approach works by providing the LLM with relevant product catalog information as context, enabling it to distinguish between genuine spelling errors and unconventional brand names. When a user enters a query, the retrieval step fetches similar product names from the catalog. This context helps the LLM make informed decisions about whether the query is misspelled or intentionally using a non-standard spelling (as is common with brand names). Fine-tuning the LLM with this retrieved context teaches it to leverage catalog information effectively, resulting in improved accuracy for both generic misspellings and brand-specific queries.

## Foundational Learning
**E-commerce query processing**: Understanding how users search for products using potentially misspelled or unconventional terms is crucial for this application. *Why needed*: E-commerce platforms must handle diverse user input patterns. *Quick check*: Are query patterns consistent with typical e-commerce behavior?

**Retrieval Augmented Generation**: RAG combines information retrieval with text generation to improve model performance on tasks requiring external knowledge. *Why needed*: Standard spelling correction models lack access to product catalog information. *Quick check*: Does the retrieval step return relevant context for the query?

**Catalog-based context**: Using product catalogs as a knowledge source for query understanding and correction. *Why needed*: Product catalogs contain the ground truth for valid product names and brand spellings. *Quick check*: Is the catalog comprehensive and up-to-date?

**Fine-tuning with contextual information**: Training models to effectively utilize external context for improved decision-making. *Why needed*: LLMs need to learn how to interpret and use retrieved catalog information. *Quick check*: Does fine-tuning improve performance on brand name queries?

**Retrieval model comparison**: Evaluating different retrieval approaches (BM25, Fuzzy BM25, ColBERT) for effectiveness in the spelling correction task. *Why needed*: The quality of retrieved context directly impacts correction accuracy. *Quick check*: Which retrieval model provides the most relevant context?

## Architecture Onboarding

**Component map**: User query -> Retrieval model -> Product catalog -> Retrieved context -> LLM (fine-tuned) -> Corrected query

**Critical path**: The most critical path is: Query input -> Retrieval -> Context assembly -> LLM inference -> Output correction. Any failure in retrieval or context assembly directly impacts the quality of corrections.

**Design tradeoffs**: The system trades computational overhead (retrieval + LLM inference) for improved accuracy, particularly for brand names. The choice between different retrieval models involves balancing speed (BM25) against semantic matching capability (ColBERT). Fine-tuning adds training complexity but significantly improves performance.

**Failure signatures**: Failures occur when (1) retrieval returns irrelevant products, leading to incorrect corrections; (2) the LLM misinterprets the context and changes valid brand names; (3) queries reference products not in the catalog; (4) the system fails to handle rare or new brand names not present in training data.

**3 first experiments**:
1. Evaluate retrieval quality by measuring recall of correct product names for a sample of misspelled queries
2. Test LLM performance on queries with and without retrieved context to quantify RAG benefit
3. Assess fine-tuning impact by comparing performance on brand name queries before and after contextual fine-tuning

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to English e-commerce queries from a single catalog, limiting generalizability
- Method assumes catalog completeness - cannot correct for products not in the catalog
- Computational overhead of retrieval and LLM inference presents scalability challenges for real-time applications
- 1% query failure rate may mask systematic issues with specific product categories or name patterns

## Confidence
- **High Confidence**: The core RAG methodology improves spelling correction performance relative to baseline models; the retrieval step provides useful context for distinguishing misspellings from brand names
- **Medium Confidence**: The relative performance ranking of retrieval models (ColBERT > BM25 variants) and the benefit of contextual fine-tuning; the 34.2 point F1 improvement over RAG baseline is robust but may not generalize to different query distributions
- **Low Confidence**: The absolute performance numbers (70.1 F1) and the specific 1% failure rate, as these are specific to the evaluated dataset and may not reflect real-world deployment scenarios

## Next Checks
1. Evaluate the approach on multi-language e-commerce catalogs to assess cross-lingual generalization and identify language-specific challenges
2. Conduct A/B testing in production to measure end-to-end impact on search conversion rates and user satisfaction, beyond spelling correction metrics
3. Test robustness with incomplete catalogs and out-of-vocabulary brand names to understand failure modes when products are not indexed