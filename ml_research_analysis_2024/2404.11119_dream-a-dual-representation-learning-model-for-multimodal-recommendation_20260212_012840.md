---
ver: rpa2
title: 'DREAM: A Dual Representation Learning Model for Multimodal Recommendation'
arxiv_id: '2404.11119'
source_url: https://arxiv.org/abs/2404.11119
tags:
- modal
- information
- representation
- recommendation
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces DREAM, a dual representation learning model
  for multimodal recommendation that addresses three key issues in existing approaches:
  insufficient modal information utilization, modal information forgetting, and representation
  misalignment between behavioral and multimodal data. DREAM employs separate dual
  lines (Behavior Line and Modal Line) with a Modal-specific Encoder to extract fine-grained
  representations, introduces Similarity Supervised Signal to constrain modal representations
  and prevent information forgetting, and uses Behavior-Modal Alignment with Intra-Alignment
  and Inter-Alignment to fuse dual representations.'
---

# DREAM: A Dual Representation Learning Model for Multimodal Recommendation

## Quick Facts
- arXiv ID: 2404.11119
- Source URL: https://arxiv.org/abs/2404.11119
- Reference count: 16
- Primary result: Achieves state-of-the-art performance on multimodal recommendation, improving baselines by 6.68%, 6.45%, and 6.55% in Recall@10 on Baby, Sports, and Clothing datasets respectively.

## Executive Summary
This paper introduces DREAM, a dual representation learning model that addresses three key challenges in multimodal recommendation: insufficient modal information utilization, modal information forgetting, and representation misalignment between behavioral and multimodal data. DREAM employs separate dual lines (Behavior Line and Modal Line) with a Modal-specific Encoder to extract fine-grained representations from both user-item interactions and multimodal features. The model introduces Similarity Supervised Signal to constrain modal representations and prevent information forgetting, and uses Behavior-Modal Alignment with Intra-Alignment and Inter-Alignment to fuse dual representations. Extensive experiments on three public Amazon datasets demonstrate that DREAM achieves state-of-the-art performance, significantly outperforming existing methods.

## Method Summary
DREAM processes user-item interactions through a Behavior Line using LightGCN to capture behavioral patterns, while multimodal features (text and vision) are processed through a Modal Line with a Modal-specific Encoder that includes filter gates and relation graphs to denoise and extract semantic relationships. The model employs Similarity Supervised Signal (S3) to constrain modal embeddings to retain original modal similarity information, preventing the modal information forgetting problem. Behavior-Modal Alignment (BMA) module with Intra-Alignment and Inter-Alignment aligns representations within and across domains using contrastive loss before fusing them for final prediction via BPR loss.

## Key Results
- Achieves state-of-the-art performance on three public datasets (Baby, Sports, Clothing)
- Improves best baselines by 6.68%, 6.45%, and 6.55% in Recall@10 respectively
- Improves best baselines by 6.19%, 7.05%, and 5.76% in NDCG@10 respectively
- Effectively mitigates modal information forgetting through S3 mechanism

## Why This Works (Mechanism)

### Mechanism 1: Similarity Supervised Signal (S3)
- Claim: DREAM mitigates Modal Information Forgetting by using S3 to constrain modal embeddings to retain original modal similarity information
- Mechanism: S3 uses Mean Square Error between similarity matrices computed from learned modal representations and original multimodal features, stopping gradients on original features
- Core assumption: Modal embeddings tend to deviate from original features during training in existing methods; constraining similarity information prevents this deviation
- Evidence anchors: Abstract mentions modal information forgetting; section details S3 implementation
- Break condition: If the similarity constraint is too strong, it may prevent modal representations from learning task-relevant transformations, hurting recommendation performance

### Mechanism 2: Dual Lines with Modal-specific Encoder
- Claim: DREAM achieves better information utilization by using separate dual lines with Modal-specific Encoder for fine-grained modal representation extraction
- Mechanism: Behavior Line uses LightGCN on interaction graph; Modal Line uses Modal-specific Encoder with filter gates and relation graphs to denoise and capture semantic relationships
- Core assumption: Simple concatenation or linear mapping of multimodal features is insufficient for extracting fine-grained modal information
- Evidence anchors: Abstract mentions insufficient modal information utilization; section describes dual lines and Modal-specific Encoder
- Break condition: If the dual lines become too isolated, the model may fail to effectively fuse behavior and modal information for recommendation

### Mechanism 3: Behavior-Modal Alignment (BMA)
- Claim: DREAM addresses representation misalignment through BMA module with Intra-Alignment and Inter-Alignment
- Mechanism: Intra-Alignment aligns representations within each domain (behavior-user/item, modal-user/item) using InfoNCE loss; Inter-Alignment aligns behavior and modal representations of same users/items using InfoNCE loss
- Core assumption: Behavior and modal representations have significantly different distributions that need explicit alignment before fusion
- Evidence anchors: Abstract mentions representation misalignment; section details BMA module implementation
- Break condition: If alignment is too aggressive, it may erase meaningful domain-specific information, reducing recommendation quality

## Foundational Learning

- Concept: Graph Convolutional Networks (GCNs) for recommendation
  - Why needed here: DREAM uses LightGCN on behavior graph for behavior representation learning
  - Quick check question: How does LightGCN simplify GCN for recommendation compared to standard GCN?

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: DREAM uses InfoNCE for both Intra-Alignment and Inter-Alignment in BMA module
  - Quick check question: What is the difference between Intra-Alignment and Inter-Alignment in terms of what representations they align?

- Concept: Multimodal representation learning and fusion
  - Why needed here: DREAM processes vision and text modalities separately then fuses them with modality-specific weights
  - Quick check question: Why might text modality be more important than vision modality for recommendation according to DREAM's experiments?

## Architecture Onboarding

- Component map: User-Item interaction → Behavior Line (LightGCN) → Modal Line (Filter Gates + Relation Graphs) → S3 Constraint → BMA (Intra + Inter Alignment) → Sum → Prediction

- Critical path: User-Item interaction → Behavior Line (LightGCN) → Modal Line (Filter Gates + Relation Graphs) → S3 Constraint → BMA (Intra + Inter Alignment) → Sum → Prediction

- Design tradeoffs:
  - Separate dual lines vs. joint learning: Dual lines allow specialized learning but require careful alignment
  - Filter gates vs. direct feature use: Filter gates denoise but add complexity and parameters
  - S3 strength: Too weak loses forgetting mitigation; too strong prevents useful transformation

- Failure signatures:
  - Poor performance despite convergence: May indicate filter gates are over-denoising or alignment is too aggressive
  - Slow convergence: May indicate insufficient modality information utilization or ineffective S3
  - Modal embeddings drifting from originals despite S3: May indicate S3 weight is too low

- First 3 experiments:
  1. Compare behavior-only and modal-only representations to quantify their individual contributions
  2. Test different S3 weights to find optimal balance between preserving original features and learning task-relevant transformations
  3. Evaluate impact of removing either filter gates or relation graphs from Modal-specific Encoder to understand their individual importance

## Open Questions the Paper Calls Out

- What is the optimal balance between behavior and modal representations in multimodal recommendation systems?
- How can the Modal Information Forgetting problem be generalized and addressed in other multimodal learning contexts beyond recommendation?
- What are the limitations of the current BMA module when scaling to larger numbers of modalities or more complex modality relationships?
- How does the proposed method perform on sequential or temporal multimodal recommendation tasks?

## Limitations

- Limited details on exact implementation of filter gates and relation graphs in Modal-specific Encoder
- S3 impact evaluated through performance metrics rather than direct similarity preservation analysis
- BMA assumes linear separability between behavior and modal representations which may not hold universally

## Confidence

- High confidence: DREAM's overall architecture and its improvement over baselines (6.68%, 6.45%, 6.55% Recall@10 gains)
- Medium confidence: The effectiveness of the S3 module in preventing modal information forgetting, as direct evidence is limited
- Low confidence: The specific impact of each component (filter gates, relation graphs, alignment mechanisms) due to lack of detailed ablation studies

## Next Checks

1. Conduct ablation studies to isolate the impact of S3 module on modal embeddings by measuring similarity preservation metrics alongside recommendation performance
2. Test DREAM on additional multimodal datasets beyond Amazon reviews to verify generalizability across different domains
3. Implement a variant without the BMA module to quantify the exact contribution of behavior-modal alignment to overall performance