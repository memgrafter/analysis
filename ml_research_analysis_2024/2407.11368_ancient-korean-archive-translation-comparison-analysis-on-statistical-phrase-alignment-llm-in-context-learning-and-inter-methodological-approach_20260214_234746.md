---
ver: rpa2
title: 'Ancient Korean Archive Translation: Comparison Analysis on Statistical phrase
  alignment, LLM in-context learning, and inter-methodological approach'
arxiv_id: '2407.11368'
source_url: https://arxiv.org/abs/2407.11368
tags:
- translation
- statistical
- alignment
- bleu
- korean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compares three methods for translating ancient texts
  with sparse corpora: (1) statistical phrase alignment using the Moses toolkit, (2)
  in-context learning with large language models (SOLAR-10.7B and XGLM), and (3) an
  inter-methodological approach combining statistical alignment with Sentence Piece
  (SP) tokenization. The proposed SP-based method achieved a BLEU score of 36.71,
  surpassing the 32.57 score of the best existing Seq2Seq model and outperforming
  LLM-based approaches (SOLAR-12.7, XGLM-1.7) on the Joseon Dynasty annals corpus
  (252,773 records).'
---

# Ancient Korean Archive Translation: Comparison Analysis on Statistical phrase alignment, LLM in-context learning, and inter-methodological approach

## Quick Facts
- arXiv ID: 2407.11368
- Source URL: https://arxiv.org/abs/2407.11368
- Reference count: 7
- Primary result: Proposed Sentence Piece-based statistical method achieved BLEU score of 36.71, surpassing Seq2Seq models and LLM-based approaches on Joseon Dynasty annals corpus

## Executive Summary
This study compares three methods for translating ancient Korean texts with sparse corpora: statistical phrase alignment using Moses toolkit, in-context learning with large language models (SOLAR-10.7B and XGLM), and an inter-methodological approach combining statistical alignment with Sentence Piece tokenization. The proposed unified tokenization approach achieved a BLEU score of 36.71, outperforming both traditional Seq2Seq models (32.57) and LLM-based approaches on the Joseon Dynasty annals corpus. The key innovation leverages token-to-token alignment and unified source-target tokenization to improve translation accuracy for low-resource ancient languages.

## Method Summary
The study evaluates three approaches for translating Classical Chinese to Modern Korean using the Joseon Dynasty annals corpus (252,773 records). The first method uses Moses toolkit with statistical phrase alignment and character-level tokenization. The second method tests in-context learning with SOLAR-10.7B and XGLM-1.7 LLMs. The third approach combines Moses with Sentence Piece (SP) tokenization using 10,000 BPE tokens applied to a unified source-target corpus. All methods use 202,218 training records and 50,555 test records, with character limits of 128 for Chinese and 1024 for Korean. Translation quality is evaluated using sacreBLEU scores.

## Key Results
- Proposed Sentence Piece-based statistical method achieved BLEU score of 36.71
- Outperformed best Seq2Seq model (BLEU 32.57) and LLM approaches (SOLAR-12.7, XGLM-1.7)
- Unified tokenization approach proved superior to character-level tokenization baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-to-token alignment with unified source-target tokenization improves translation accuracy for low-resource ancient languages
- Mechanism: By tokenizing both source and target languages using a shared Sentence Piece vocabulary, the model leverages direct token correspondences that exist between Classical Chinese and Modern Korean, particularly for expressions that remain unchanged
- Core assumption: Combined vocabulary captures sufficient shared linguistic features to enable meaningful token alignments
- Evidence anchors: [abstract] and [section 4] show unified tokenization approach

### Mechanism 2
- Claim: Statistical phrase alignment based on token-to-token matching rather than self-attention performs better for aligned corpora with sparse resources
- Mechanism: Uses GIZA++ for word-level alignment followed by phrase table construction, explicitly mapping tokens between languages based on co-occurrence patterns
- Core assumption: Annals corpus provides sufficient aligned sentence pairs to train meaningful phrase tables despite resource scarcity
- Evidence anchors: [abstract] and [section 4] demonstrate statistical phrase alignment approach

### Mechanism 3
- Claim: Byte Pair Encoding tokenization preserves linguistic subword information while reducing vocabulary size for ancient languages
- Mechanism: BPE iteratively merges frequent character pairs into subword units, creating compact vocabulary that handles both archaic characters and complex morphology
- Core assumption: BPE can effectively handle Classical Chinese character-level features while maintaining compatibility with Modern Korean word structures
- Evidence anchors: [section 5.3] and [section 4] describe BPE tokenization implementation

## Foundational Learning

- Concept: Statistical Machine Translation principles (phrase alignment, language models, decoding)
  - Why needed here: The entire approach builds on traditional SMT components like Moses toolkit, GIZA++ alignment, and phrase-based decoding
  - Quick check question: What are the three main components required for a phrase-based SMT system?

- Concept: Tokenization methods for low-resource languages (Sentence Piece, Byte Pair Encoding)
  - Why needed here: The paper demonstrates that unified tokenization using Sentence Piece with BPE significantly improves translation performance
  - Quick check question: How does Sentence Piece differ from traditional word-based tokenization in handling rare or unknown words?

- Concept: BLEU score calculation and interpretation
  - Why needed here: The paper's primary evaluation metric is BLEU score, with specific values used to compare different approaches
  - Quick check question: What does a BLEU score of 0 indicate about a translation system's performance?

## Architecture Onboarding

- Component map: Web scraping → Character filtering → Sentence splitting → Tokenization → Unified BPE tokenization → Word alignment (GIZA++) → Phrase table construction → Language model (SRILM 3-gram) → Phrase-based decoding → sacreBLEU evaluation

- Critical path: Corpus preparation → Unified tokenization → Word alignment → Phrase table construction → Language model training → Decoding → BLEU evaluation

- Design tradeoffs:
  - Tokenization granularity: Character-level vs. word-level vs. subword (BPE)
  - Corpus size vs. quality: Larger corpora may include noise, smaller corpora may lack coverage
  - Traditional SMT vs. neural approaches: SMT provides interpretability but may lack flexibility of neural models

- Failure signatures:
  - BLEU score near 0: Indicates fundamental alignment or tokenization issues
  - Inconsistent performance across sentence lengths: Suggests tokenization or alignment problems with longer sequences
  - High GPU memory usage during testing: May indicate inefficient tokenization or model architecture issues

- First 3 experiments:
  1. Baseline SMT with character-level tokenization to establish performance floor
  2. Unified BPE tokenization without alignment integration to isolate tokenization effect
  3. Full proposed method with unified BPE tokenization and integrated alignment to verify combined benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed inter-methodological approach scale to larger ancient text corpora beyond 252,773 records?
- Basis in paper: [explicit] The study uses specific corpus size and acknowledges limitations in LLM comparison
- Why unresolved: Only tested on one corpus size, LLM testing limited by GPU constraints
- What evidence would resolve it: Performance comparisons on progressively larger ancient text corpora with resource utilization metrics

### Open Question 2
- Question: What specific architectural modifications to LLMs would improve their performance on ancient text translation?
- Basis in paper: [explicit] LLM testing was limited to 20 contexts and couldn't perform comprehensive testing
- Why unresolved: Limited LLM testing contexts and computational constraints
- What evidence would resolve it: Systematic testing with ancient text-specific fine-tuning, larger context windows, and specialized prompt engineering

### Open Question 3
- Question: How do different tokenization strategies affect translation quality for ancient texts with unique linguistic features?
- Basis in paper: [explicit] Study only tested BPE vs character-level tokenization
- Why unresolved: Only one alternative tokenization method tested, didn't explore other modern approaches
- What evidence would resolve it: Comparative analysis of multiple tokenization strategies on same corpus with comprehensive performance metrics

## Limitations
- Corpus representativeness may be limited to formal historical language rather than full diversity of Classical Chinese expressions
- Results may not generalize to other ancient-modern language pairs beyond Classical Chinese to Korean
- LLM implementation details (prompt engineering, temperature settings) were not fully specified

## Confidence
- **High Confidence**: Moses-SP approach achieves BLEU 36.71; unified BPE tokenization outperforms character-level; statistical phrase alignment provides meaningful translations
- **Medium Confidence**: Token-to-token alignment superior to self-attention for aligned corpora; inter-methodological approach more efficient than pure neural approaches; SentencePiece integration effective
- **Low Confidence**: Generalizability to other ancient language pairs; efficiency claims without computational resource measurements; specific mechanism of unified tokenization improvement

## Next Checks
1. Recreate experiments using a second classical-modern Korean corpus to verify BLEU score of 36.71 is not corpus-specific
2. Perform paired bootstrap resampling to calculate 95% confidence intervals for BLEU scores across all three methods
3. Apply unified SentencePiece tokenization and statistical alignment approach to a different ancient-modern language pair to test generalizability