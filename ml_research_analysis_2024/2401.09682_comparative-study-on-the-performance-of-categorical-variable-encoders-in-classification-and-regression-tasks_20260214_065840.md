---
ver: rpa2
title: Comparative Study on the Performance of Categorical Variable Encoders in Classification
  and Regression Tasks
arxiv_id: '2401.09682'
source_url: https://arxiv.org/abs/2401.09682
tags:
- encoder
- performance
- encoders
- datasets
- categorical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides theoretical and empirical analysis on the performance
  of categorical variable encoders in classification and regression tasks. It proves
  that the one-hot encoder is the best choice for ATI models (e.g.
---

# Comparative Study on the Performance of Categorical Variable Encoders in Classification and Regression Tasks

## Quick Facts
- arXiv ID: 2401.09682
- Source URL: https://arxiv.org/abs/2401.09682
- Reference count: 40
- Key outcome: One-hot encoder is best for ATI models with sufficient data, target encoders excel for tree-based models under same condition; GLMM and MinHash are alternatives when data is insufficient

## Executive Summary
This study provides both theoretical and empirical analysis of categorical variable encoders across 28 datasets and 8 machine learning models. The research proves that one-hot encoding can mimic any other encoder in affine transformation-based models (ATI) by learning suitable weights when data is sufficient. For tree-based models, target encoders and their variants are shown to be most suitable. The comprehensive experiments validate these theoretical findings and identify GLMM and MinHash encoders as robust alternatives when data is limited.

## Method Summary
The study conducts comprehensive experiments using 28 datasets (15 binary classification and 13 regression tasks) with 8 machine learning models (4 ATI models and 4 tree-based models) and 14 different encoders. Data preprocessing includes filling missing values and normalization. Models are trained with default hyperparameters from scikit-learn, XGBoost, and LightGBM libraries, with experiments repeated 10 times using different random seeds. Performance is evaluated using F1 score for classification tasks and RMSE for regression tasks, with time costs recorded for encoding and model training.

## Key Results
- One-hot encoder performs best for ATI models when average samples per level (ASPL) is sufficient
- Target encoders excel for tree-based models under sufficient data conditions
- GLMM encoder achieves more robust performance for ATI models when data is insufficient
- MinHash encoder is a viable alternative for tree-based models with limited data
- Performance differences between encoders converge to zero as ASPL increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: One-hot encoding can mimic any other encoder for ATI models when data is sufficient.
- Mechanism: The affine transformation in ATI models allows learned weights to replicate the effect of any encoder mapping, since the one-hot representation spans the input space and the model can adjust weights accordingly.
- Core assumption: Sufficient data per categorical level exists so that the model can accurately learn the required weights.
- Evidence anchors:
  - [abstract]: "the one-hot encoder is the best choice for ATI models in the sense that it can mimic any other encoders by learning suitable weights from the data"
  - [section II.A]: "For a categorical variable V = {v1, v2, ..., vc} and any encoder ϕ : V → Rl... there exists an equivalent linear transformation h(x) = W OHϕOH(x)"
  - [corpus]: Weak - no direct citations found; corpus focuses on embedding and distance metrics, not affine reconstruction.
- Break condition: When the average number of samples per level (ASPL) is too low, the learned weights cannot accurately approximate the target mapping, leading to performance degradation.

### Mechanism 2
- Claim: Target encoders perform well for tree-based models when data is sufficient.
- Mechanism: Target encoders preserve optimal splits in decision trees by encoding levels with values close to the conditional mean, maintaining contiguous partitions that minimize impurity measures.
- Core assumption: The conditional distribution of the target variable is similar across nodes for splits not involving the encoded variable.
- Evidence anchors:
  - [abstract]: "we explain why the target encoder and its variants are the most suitable encoders for tree-based models"
  - [section III.A]: "Mean Encoder will preserve the optimal split at root node when impurity is measured by mean squared error (MSE)" and "Mean Encoder will preserve the optimal split at root node when impurity is measured by entropy"
  - [corpus]: Weak - corpus neighbors discuss embedding and kernel methods but do not directly address tree split optimality.
- Break condition: When ASPL is low, the estimated conditional means are unreliable, leading to suboptimal splits and degraded performance.

### Mechanism 3
- Claim: Performance differences between encoders converge to zero as ASPL increases.
- Mechanism: With more samples per level, the empirical estimates of conditional distributions and model parameters become more accurate, reducing the gap between the best encoder and alternatives like one-hot or target encoders.
- Core assumption: The true conditional distributions are stable and can be estimated with sufficient data.
- Evidence anchors:
  - [abstract]: "one-hot encoder performs best for ATI models when data is sufficient, while target encoders excel for tree-based models under the same condition"
  - [section II.B]: "As ASPL increases, the average performance of the one-hot encoder converges towards the best encoder"
  - [section III.B]: "the performance of the Mean Encoder drops as ASPL decreases... When ASPL is sufficiently large... the Mean encoder performs very well"
  - [corpus]: Weak - corpus neighbors do not provide evidence on convergence behavior with increasing ASPL.
- Break condition: When data is insufficient, the variance in estimates remains high, preventing convergence to the best encoder's performance.

## Foundational Learning

- Concept: Affine transformations in machine learning models.
  - Why needed here: Understanding how ATI models process inputs is key to explaining why one-hot encoding can replicate other encoders.
  - Quick check question: Can you write the mathematical form of an affine transformation applied to an input vector in a neural network layer?

- Concept: Decision tree splitting criteria and impurity measures.
  - Why needed here: Necessary to understand why target encoders preserve optimal splits in tree-based models.
  - Quick check question: What are the common impurity measures used in decision tree algorithms, and how do they relate to the target variable?

- Concept: Conditional expectation and empirical estimation.
  - Why needed here: Core to understanding how target encoders work and their reliance on data sufficiency.
  - Quick check question: How does the law of large numbers justify using sample means to estimate conditional expectations when ASPL is large?

## Architecture Onboarding

- Component map: Data preprocessing (handling missing values, encoding categorical variables) -> Model training (various ATI and tree-based models) -> Evaluation (performance metrics across datasets)
- Critical path: For a new dataset, first determine ASPL to assess data sufficiency. Then select model type (ATI or tree-based) to guide encoder choice. Apply the selected encoder, train the model, and evaluate performance.
- Design tradeoffs: One-hot encoding introduces high dimensionality, which can increase training time and memory usage, especially for high-cardinality features. Target encoders are more compact but require sufficient data per level to avoid overfitting.
- Failure signatures: Poor performance with one-hot encoding despite sufficient data may indicate model complexity issues or improper hyperparameter tuning. Target encoder failure with low ASPL manifests as unstable or inaccurate splits in tree models.
- First 3 experiments:
  1. Test one-hot encoding on an ATI model (e.g., logistic regression) with a dataset having high ASPL to verify convergence to best encoder performance.
  2. Apply a target encoder (e.g., Mean Encoder) to a tree-based model (e.g., Random Forest) with high ASPL and observe split quality.
  3. Repeat experiment 1 with a low ASPL dataset to observe the breakdown in one-hot encoding performance and compare with alternative encoders like GLMM.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of categorical encoders vary when applied to ensemble models that combine ATI and tree-based components, such as gradient boosting with neural network base learners?
- Basis in paper: [inferred] The paper systematically compares encoders across pure ATI and pure tree-based models, but does not address hybrid architectures that combine both paradigms.
- Why unresolved: The paper's theoretical analysis focuses on the affine transformation properties of ATI models and the split optimization properties of tree-based models, without considering how these properties interact in ensemble methods that combine both.
- What evidence would resolve it: Empirical results comparing the performance of various encoders on ensemble models that incorporate both ATI and tree-based components, across datasets with different characteristics.

### Open Question 2
- Question: What is the impact of categorical variable cardinality distribution across levels (e.g., power-law vs uniform) on the effectiveness of different encoders, beyond just the average samples per level metric?
- Basis in paper: [inferred] The paper uses average samples per level (ASPL) as the primary metric for data sufficiency, but doesn't explore how the distribution of samples across levels affects encoder performance.
- Why unresolved: The theoretical analysis assumes uniform distribution of samples across levels, and empirical results are aggregated without examining the impact of different cardinality distributions on encoder effectiveness.
- What evidence would resolve it: Comparative experiments showing encoder performance across datasets with varying cardinality distributions (uniform, power-law, bimodal) while controlling for total sample count and ASPL.

### Open Question 3
- Question: How do different categorical encoders affect the interpretability of machine learning models, particularly in terms of feature importance and decision boundaries?
- Basis in paper: [explicit] The paper mentions that contrast encoders are "usually used in statistics" and "less frequently used in machine learning," suggesting an awareness of interpretability considerations.
- Why unresolved: The experimental evaluation focuses solely on predictive performance metrics (F1 score, RMSE) without examining how different encoders impact model interpretability or the ability to understand model decisions.
- What evidence would resolve it: Analysis of feature importance rankings, decision boundary visualizations, and interpretability metrics across different encoder-model combinations to quantify the trade-off between performance and interpretability.

## Limitations

- The study uses default hyperparameters across all models and encoders, which may mask performance differences that could emerge with careful optimization.
- The research does not thoroughly examine how categorical variable cardinality interacts with encoder performance, particularly for high-cardinality features where one-hot encoding becomes problematic.
- The theoretical analysis assumes general conditions that may not hold in all domains, particularly when categorical variables have structured relationships or hierarchical properties.

## Confidence

- **High confidence**: The claim that one-hot encoding can mimic other encoders in ATI models when data is sufficient, supported by theoretical proofs and consistent empirical results across multiple datasets.
- **Medium confidence**: The recommendation that target encoders are optimal for tree-based models, as the theoretical justification is sound but the empirical evidence is less robust across diverse dataset characteristics.
- **Low confidence**: The specific performance rankings of alternative encoders (GLMM, MinHash) under data scarcity conditions, as these results appear more dataset-dependent and less theoretically grounded.

## Next Checks

1. **Cross-validation stability test**: Repeat the experiments using k-fold cross-validation rather than a single train-test split to assess whether the reported performance differences are consistent across different data partitions.

2. **Cardinality stress test**: Systematically vary the cardinality of categorical features in synthetic datasets while holding ASPL constant to isolate the impact of cardinality on encoder performance, particularly for one-hot encoding.

3. **Hyperparameter sensitivity analysis**: Conduct a grid search over key hyperparameters (learning rate, tree depth, regularization) for both ATI and tree-based models to determine whether the observed encoder performance differences persist under optimal model configurations.