---
ver: rpa2
title: 'FoldGPT: Simple and Effective Large Language Model Compression Scheme'
arxiv_id: '2407.00928'
source_url: https://arxiv.org/abs/2407.00928
tags:
- arxiv
- block
- foldgpt
- parameter
- blocks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FoldGPT addresses the challenge of deploying large language models
  on resource-constrained mobile devices by exploiting depth redundancy in LLMs. The
  method combines block removal based on learnable gating parameters with group parameter
  sharing, where blocks within a group share identical weights.
---

# FoldGPT: Simple and Effective Large Language Model Compression Scheme

## Quick Facts
- arXiv ID: 2407.00928
- Source URL: https://arxiv.org/abs/2407.00928
- Authors: Songwei Liu; Chao Zeng; Lianqiang Li; Chenqian Yan; Lean Fu; Xing Mei; Fangmin Chen
- Reference count: 10
- Key outcome: Achieves 96.25% of original model performance after removing 36% of parameters

## Executive Summary
FoldGPT addresses the challenge of deploying large language models on resource-constrained mobile devices by exploiting depth redundancy in LLMs. The method combines block removal based on learnable gating parameters with group parameter sharing, where blocks within a group share identical weights. This approach reduces model parameters while maintaining performance through tail-layer distillation and fine-tuning. Experiments on models ranging from 1B to 7B parameters show that FoldGPT achieves state-of-the-art performance, maintaining 96.25% of the original model's performance even after removing 36% of parameters.

## Method Summary
FoldGPT is a two-step compression strategy that first removes redundant blocks using learnable gating parameters to rank block importance, then applies group parameter sharing to the remaining blocks. The gating mechanism models inter-block coupling effects to identify truly redundant layers based on cosine similarity analysis. After block removal, remaining blocks are organized into groups where all but the first block share weights, supplemented with learnable scaling coefficients. The compressed model is then fine-tuned using LoRA with tail-layer distillation to restore performance, focusing on self-attention and self-attention value relations in the last layer.

## Key Results
- Maintains 96.25% of original model performance after removing 36% of parameters
- Outperforms existing methods with up to 5.24% performance improvement while increasing sparsity by 4.39%
- Successfully compresses models ranging from 1B to 7B parameters across multiple benchmark tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Block redundancy exists due to high cosine similarity between layer outputs in LLMs
- Mechanism: By measuring cosine similarity between activations of consecutive blocks, the paper identifies blocks that contribute minimal new information to the information flow path
- Core assumption: High cosine similarity (>0.9) between consecutive block outputs indicates functional redundancy that can be removed without significant performance loss
- Evidence anchors:
  - [abstract] "outputs of most layers exhibit significant similarity... this similarity becomes more pronounced as the model size increases"
  - [section 3.1] "The cosine similarity of most intermediate blocks in the LLMs is greater than 0.9, indicating their minimal role in the information transmission path"
  - [corpus] Weak - no direct citations about cosine similarity in related works
- Break condition: If cosine similarity drops below threshold (0.8) across consecutive blocks, or if removing high-similarity blocks causes disproportionate performance degradation

### Mechanism 2
- Claim: Learnable gating parameters enable more accurate block importance ranking than static metrics like BI
- Mechanism: Gating parameters are introduced per component within each block, allowing the model to learn which blocks are critical for information flow through joint optimization with a polarization objective
- Core assumption: Modeling inter-block coupling through learnable gating parameters yields better importance rankings than metrics that don't account for these relationships
- Evidence anchors:
  - [abstract] "Based on the learnable gating parameters, we determine the block importance ranking while modeling the coupling effect between blocks"
  - [section 3.2] "our approach models the interactions between blocks, resulting in superior performance" compared to BI metrics
  - [corpus] Weak - related works focus on different pruning approaches without mentioning gating-based coupling modeling
- Break condition: If gating parameters fail to polarize properly (don't converge to values near 0 or 1), or if the learned ranking doesn't correlate with actual block importance when tested

### Mechanism 3
- Claim: Group parameter sharing combined with child block learnable scaling parameters maintains performance while further compressing model size
- Mechanism: After block removal, remaining blocks are organized into groups where all but the first (parent) block share the parent's weights, supplemented with learnable scaling coefficients to adapt the parent weights to each child block's specific needs
- Core assumption: The small number of learnable scaling parameters per child block is sufficient to compensate for weight sharing, especially when combined with fine-tuning and distillation
- Evidence anchors:
  - [abstract] "For the retained blocks, we apply a specially designed group parameter sharing strategy, where blocks within the same group share identical weights"
  - [section 3.3] "we introduce additional learnable scaling coefficients for the weights in each child block" and "experiments show that this strategy effectively enhances the performance"
  - [corpus] Weak - related works mention parameter sharing but not specifically for post-training compression with child block adaptation
- Break condition: If the scaling parameters cannot adequately compensate for weight sharing, leading to performance degradation beyond what fine-tuning can recover

## Foundational Learning

- Concept: Cosine similarity as a measure of vector alignment
  - Why needed here: The paper uses cosine similarity to quantify redundancy between layer outputs, so understanding this metric is crucial for evaluating the redundancy analysis
  - Quick check question: If two vectors have cosine similarity of 0.95, what does this indicate about their directional relationship?

- Concept: Learnable gating mechanisms in neural networks
  - Why needed here: The paper introduces learnable gating parameters to determine block importance, which is a specialized application of gating mechanisms
  - Quick check question: How do learnable gating parameters differ from static attention weights in transformer architectures?

- Concept: Knowledge distillation and its application to model compression
  - Why needed here: The paper uses tail-layer distillation to recover performance after compression, which is a key part of the recovery strategy
  - Quick check question: What is the difference between standard knowledge distillation and the tail-layer distillation approach used in this paper?

## Architecture Onboarding

- Component map:
  Input preprocessing → Transformer blocks (with gating) → Block removal stage → Group parameter sharing → Fine-tuning with LoRA → Tail-layer distillation
  Key components: Learnable gating parameters, group sharing mechanism, LoRA adapters, distillation loss

- Critical path:
  1. Learnable gating parameters are optimized to identify redundant blocks
  2. Identified blocks are removed based on sparsity target
  3. Remaining blocks are organized into groups with parameter sharing
  4. Scaling parameters are introduced for child blocks
  5. LoRA fine-tuning with tail-layer distillation recovers performance

- Design tradeoffs:
  - Block removal vs. parameter sharing: Block removal reduces computation but may harm performance; parameter sharing reduces parameters without affecting compute
  - Group size selection: Larger groups increase compression but may hurt performance; smaller groups have opposite effect
  - Fine-tuning budget: More fine-tuning improves recovery but increases cost; less fine-tuning is cheaper but may leave performance gaps

- Failure signatures:
  - Poor gating polarization: Gating parameters remain near 0.5 instead of converging to 0 or 1
  - Excessive performance drop: Model performance degrades beyond acceptable thresholds after compression
  - Overfitting during fine-tuning: Model starts memorizing training data rather than generalizing

- First 3 experiments:
  1. Verify cosine similarity analysis: Compute cosine similarity between consecutive blocks in a small transformer to confirm high redundancy exists
  2. Test gating mechanism: Apply the gating approach to a small model and verify it can identify truly redundant blocks
  3. Validate parameter sharing: Test the group sharing approach with varying group sizes on a small model to find the optimal balance between compression and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal group size for parameter sharing in FoldGPT, and how does it vary across different model architectures and scales?
- Basis in paper: [explicit] The paper mentions that group parameter sharing is applied but doesn't extensively explore the impact of varying group sizes across different models.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the method rather than optimizing group size parameters, leaving questions about scalability and adaptability to different model architectures.
- What evidence would resolve it: Systematic experiments varying group sizes across multiple model architectures (different attention mechanisms, FFN configurations) and scales would reveal optimal group sizes and their impact on performance versus compression trade-offs.

### Open Question 2
- Question: How does FoldGPT's performance compare to quantization methods like LLM.int8() when applied to the same compressed model?
- Basis in paper: [inferred] The paper extensively compares FoldGPT to other pruning methods but doesn't explore combining it with quantization techniques, which could potentially yield even greater compression.
- Why unresolved: The study focuses on FoldGPT's standalone performance and doesn't investigate synergistic effects with quantization, missing potential complementary benefits.
- What evidence would resolve it: Experiments applying quantization (e.g., LLM.int8() or GPTQ) to FoldGPT-compressed models, measuring performance, compression ratio, and inference speed to determine optimal combinations.

### Open Question 3
- Question: What is the impact of FoldGPT on the robustness and generalization of LLMs across diverse downstream tasks and domains?
- Basis in paper: [inferred] While the paper evaluates FoldGPT on several benchmark tasks, it doesn't comprehensively assess robustness to distribution shifts, adversarial examples, or specialized domains.
- Why unresolved: The evaluation focuses on standard benchmarks without exploring how compression affects the model's ability to handle out-of-distribution data or maintain performance across diverse applications.
- What evidence would resolve it: Extensive testing on adversarial examples, domain adaptation tasks, and robustness benchmarks (like WILDS) would reveal whether FoldGPT-compressed models maintain generalization capabilities across varied conditions.

## Limitations
- Weak theoretical grounding: The paper relies heavily on empirical observations without rigorous mathematical proof of why the cosine similarity threshold ensures functional redundancy
- Limited dataset scope: Experiments focus primarily on WikiText-2 and PTB for language modeling, with limited diversity in downstream tasks
- Unexplored hyper-parameters: Critical settings like group size, gating parameter initialization, and fine-tuning configurations are not fully specified

## Confidence
- High confidence: The basic premise that layer redundancy exists in LLMs (supported by cosine similarity observations across multiple model sizes)
- Medium confidence: The effectiveness of the learnable gating mechanism for block importance ranking (demonstrated empirically but not theoretically justified)
- Medium confidence: The group parameter sharing strategy's ability to maintain performance (supported by experimental results but with limited ablation studies)

## Next Checks
1. **Ablation study on gating mechanism**: Remove the learnable gating component and compare performance using only static metrics like BI to quantify the contribution of the coupling modeling approach
2. **Generalization testing**: Apply FoldGPT to models outside the LLaMA/Gemma family (e.g., OPT or Falcon) and evaluate performance on tasks beyond common sense reasoning to test method robustness
3. **Efficiency analysis**: Measure actual inference latency and memory usage on mobile devices to validate the claimed deployment benefits beyond parameter reduction metrics