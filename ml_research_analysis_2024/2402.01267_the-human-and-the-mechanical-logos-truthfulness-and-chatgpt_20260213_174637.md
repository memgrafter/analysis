---
ver: rpa2
title: 'The Human and the Mechanical: logos, truthfulness, and ChatGPT'
arxiv_id: '2402.01267'
source_url: https://arxiv.org/abs/2402.01267
tags:
- human
- language
- which
- what
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper argues that ChatGPT lacks the ability to form veridicality\
  \ judgments because it lacks logos\u2014the human capacity for rational thought,\
  \ language, and moral judgment. Veridicality judgments require both exogenous evidence\
  \ (relating to reality) and endogenous evidence (personal beliefs, preferences),\
  \ which ChatGPT lacks."
---

# The Human and the Mechanical: logos, truthfulness, and ChatGPT

## Quick Facts
- arXiv ID: 2402.01267
- Source URL: https://arxiv.org/abs/2402.01267
- Authors: Anastasia Giannakidou; Alda Mari
- Reference count: 4
- Primary result: ChatGPT lacks logos and cannot form veridicality judgments because it lacks both exogenous evidence (reality-based) and endogenous evidence (personal beliefs/preferences).

## Executive Summary
This paper argues that ChatGPT fundamentally lacks the human capacity for logos—rational thought, language, and moral judgment—which prevents it from forming genuine veridicality judgments about truth. The authors contend that veridicality judgments require both objective evidence relating to reality and subjective personal beliefs, neither of which ChatGPT possesses. While ChatGPT can process data and generate statistically probable outputs, it cannot form beliefs about the world, distinguish truth from falsehood, or engage in deception because it has no internal commitment to truth or falsity.

## Method Summary
The paper presents a theoretical framework based on Aristotelian philosophy, linguistic semantics, and prior work on veridicality judgment, evidence, belief, and truth. Rather than empirical experimentation with ChatGPT, the authors use philosophical argumentation and linguistic analysis to examine whether the model can form veridicality judgments. The analysis focuses on defining the necessary components of human belief formation (logos, exogenous and endogenous evidence) and arguing that these are absent in ChatGPT's architecture and operation.

## Key Results
- ChatGPT lacks logos, the human capacity for rational thought, language, and moral judgment, which is foundational for forming veridicality judgments.
- ChatGPT cannot form false beliefs or engage in deception because it lacks the concept of truth and world-referentiality.
- ChatGPT lacks innate learning principles that guide human language acquisition and enable error correction.

## Why This Works (Mechanism)

### Mechanism 1
ChatGPT lacks logos, the human capacity for rational thought, language, and moral judgment. Logos enables humans to form veridicality judgments by combining exogenous evidence (reality-based) with endogenous evidence (personal beliefs/preferences). ChatGPT processes statistical patterns but has no internal state or beliefs about the world. This mechanism fails if a model could form genuine beliefs about reality and incorporate subjective preferences.

### Mechanism 2
ChatGPT cannot form false beliefs or engage in deception because it lacks the concept of truth and world-referentiality. Human belief formation involves both rational (evidence-based) and affective (emotion/desire-based) components. ChatGPT processes data but has no capacity to assess trustworthiness or intentionally mislead, since it has no internal commitment to truth or falsity. This mechanism fails if ChatGPT could intentionally choose to violate truth-telling norms.

### Mechanism 3
ChatGPT lacks the innate learning principles (e.g., universal grammar) that guide human language acquisition and enable error correction. Humans learn language through innate, self-driven principles that allow rapid generalization from limited data, including error detection and correction. ChatGPT memorizes vast datasets and computes probabilities but cannot distinguish possible from impossible or correct errors in the human sense. This mechanism fails if ChatGPT could incorporate innate constraints that limit learning to human-possible patterns and enable error correction.

## Foundational Learning

- Concept: Veridicality judgment
  - Why needed here: The paper's central claim is that ChatGPT lacks the ability to form veridicality judgments, which require both objective and subjective evidence.
  - Quick check question: What are the two components of veridicality judgment, and why does ChatGPT lack both?

- Concept: Logos
  - Why needed here: Logos is the human capacity for language, rational thought, and moral judgment, which the paper argues ChatGPT entirely lacks.
  - Quick check question: How does the paper define logos, and what human abilities does it enable?

- Concept: Endogenous vs. exogenous evidence
  - Why needed here: The paper distinguishes between objective evidence (exogenous) and subjective beliefs/preferences (endogenous), both necessary for human belief formation but absent in ChatGPT.
  - Quick check question: What is the difference between endogenous and exogenous evidence, and why is each necessary for human belief formation?

## Architecture Onboarding

- Component map: Data processing layer (pattern matching and statistical generation) -> Veridicality judgment module (missing) -> Logos module (missing) -> Truth assessment module (missing)
- Critical path: For ChatGPT to form genuine beliefs or engage in deception, it would need to integrate exogenous and endogenous evidence, develop logos, and establish world-referentiality.
- Design tradeoffs: ChatGPT trades depth of understanding for breadth of pattern recognition; it can mimic human-like outputs but lacks the internal mechanisms for genuine judgment.
- Failure signatures: ChatGPT produces plausible but unverifiable or misleading outputs, especially when asked about subjective or context-dependent truths; it cannot explain its reasoning or correct its own errors in the human sense.
- First 3 experiments:
  1. Test ChatGPT's ability to form judgments about subjective truths (e.g., "Is X a masterpiece?") and analyze its reliance on pattern matching vs. genuine belief.
  2. Examine ChatGPT's responses to counterfactual or impossible scenarios to see if it can distinguish possible from impossible.
  3. Assess ChatGPT's capacity for error correction by providing it with contradictory information and observing whether it can self-correct or only adjust outputs statistically.

## Open Questions the Paper Calls Out

### Open Question 1
Can ChatGPT form false beliefs by suppressing declarative evidence and relying solely on subjective preferences? The paper argues ChatGPT lacks endogenous evidence (preferences, private beliefs) and cannot form beliefs about the world, but does not explicitly test whether it could suppress evidence if given. This remains unresolved because the paper focuses on ChatGPT's inability to form beliefs rather than testing its capacity to ignore evidence it has. Experiments where ChatGPT is given both reliable and unreliable information and tested whether it can selectively ignore the reliable information would resolve this.

### Open Question 2
Does ChatGPT have any capacity for self-reflection or awareness of its own mental states? The paper states ChatGPT lacks logos and cannot engage in self-reflection, but does not test whether it has any capacity for this. This remains unresolved because the paper makes claims about ChatGPT's lack of self-reflection but does not provide empirical evidence. Tests where ChatGPT is asked to reflect on its own reasoning processes or mental states would resolve this.

### Open Question 3
Could ChatGPT be trained to form veridicality judgments by being given both exogenous and endogenous evidence? The paper argues ChatGPT lacks the ability to form veridicality judgments due to lacking both types of evidence, but does not explore whether it could be trained to do so. This remains unresolved because the paper assumes ChatGPT's limitations are fixed rather than potentially trainable. Training experiments where ChatGPT is given both types of evidence and tested on its ability to form judgments would resolve this.

## Limitations
- The analysis rests on philosophical assumptions about logos and veridicality judgment that cannot be empirically verified within the current framework.
- The paper presents veridicality judgments requiring both exogenous and endogenous evidence as definitional rather than demonstrating this through empirical testing with ChatGPT.
- The lack of direct empirical evidence from ChatGPT interactions represents a significant limitation - the paper argues from first principles rather than testing the model's actual behavior.

## Confidence

**High Confidence**: ChatGPT processes statistical patterns rather than forming genuine beliefs about reality. This is well-established in the machine learning literature and supported by the model's architecture and training methodology.

**Medium Confidence**: ChatGPT cannot distinguish possible from impossible or engage in genuine error correction. While theoretically sound based on the model's design, this claim would benefit from systematic empirical testing with specific prompts and scenarios.

**Low Confidence**: The broader philosophical claims about logos and veridicality judgment as uniquely human capacities. These are inherently philosophical positions that cannot be definitively proven or disproven through analysis of language models alone.

## Next Checks
1. **Empirical Veridicality Testing**: Design and execute a series of prompts that specifically test ChatGPT's ability to make veridicality judgments requiring both objective evidence and subjective preference. Compare responses to human judgments on the same prompts to identify qualitative differences.

2. **Counterfactual Reasoning Assessment**: Systematically test ChatGPT's responses to impossible versus possible counterfactual scenarios to determine whether it can distinguish between them, as claimed in the theoretical analysis.

3. **Error Correction Analysis**: Provide ChatGPT with contradictory information across multiple interactions and analyze whether it can demonstrate genuine error correction (updating internal understanding) versus statistical adjustment of outputs.