---
ver: rpa2
title: Parameter-Efficient Instruction Tuning of Large Language Models For Extreme
  Financial Numeral Labelling
arxiv_id: '2405.06671'
source_url: https://arxiv.org/abs/2405.06671
tags:
- tags
- task
- xbrl
- financial
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a generative approach to solve the financial
  numeric labelling task. We propose a novel FLAN-FinXC framework, that makes use
  of parameter-efficient instruction tuning of LLMs for this extreme labelling task.
---

# Parameter-Efficient Instruction Tuning of Large Language Models For Extreme Financial Numeral Labelling

## Quick Facts
- arXiv ID: 2405.06671
- Source URL: https://arxiv.org/abs/2405.06671
- Authors: Subhendu Khatuya; Rajdeep Mukherjee; Akash Ghosh; Manjunath Hegde; Koustuv Dasgupta; Niloy Ganguly; Saptarshi Ghosh; Pawan Goyal
- Reference count: 7
- Primary result: Achieves Macro-F1 of 66.23% on FNXL dataset, significantly outperforming previous best of 47.54%

## Executive Summary
This paper introduces FLAN-FinXC, a parameter-efficient instruction tuning approach for extreme financial numeral labeling (XFNL) using large language models. The framework generates XBRL tag documentations instead of raw tags, then uses a tag matcher to predict final labels. By leveraging FLAN-T5-Large with LoRA, the model achieves state-of-the-art performance with only 0.08% of parameters being trained. The approach demonstrates significant improvements over traditional fine-tuning methods and specialized financial models.

## Method Summary
The FLAN-FinXC framework uses instruction-tuned FLAN-T5 models to generate tag documentations for financial numerals, followed by an unsupervised tag matcher that compares these documentations to XBRL tag documentations using Sentence-T5-XXL embeddings and cosine similarity. The model is trained using LoRA parameter-efficient fine-tuning, which updates only 0.08% of parameters while maintaining performance. The approach addresses the XFNL task by generating rich semantic descriptions rather than discrete labels, enabling better handling of extreme label spaces.

## Key Results
- Achieves Macro-F1 of 66.23% on FNXL dataset (vs previous best of 47.54%)
- Outperforms T5-Large and FLAN-T5-Large by 9.83% and 6.76% Macro-F1 respectively
- Demonstrates significant improvements in Hits@1 metric (7% improvement over baseline)
- Shows effectiveness of LoRA with only 0.08% parameters trained

## Why This Works (Mechanism)

### Mechanism 1: Instruction Tuning Advantage
- **Claim:** Instruction tuning aligns FLAN-T5 models faster to XFNL task than fine-tuning from scratch
- **Mechanism:** FLAN-T5 models pre-trained on 1.8K+ diverse tasks learn to interpret financial statement context and questions jointly, producing tag documentation instead of raw labels
- **Core assumption:** The model can generalize to unseen tags by learning semantic embeddings of tag documentations
- **Evidence anchors:** [abstract] highlights pre-training on 1.8K tasks; [section 4] shows output corresponds to tag description

### Mechanism 2: LoRA Parameter Efficiency
- **Claim:** LoRA reduces trainable parameters to 0.08% while maintaining performance
- **Mechanism:** LoRA approximates weight updates with low-rank matrices (rank=2), drastically cutting memory and compute cost
- **Core assumption:** Low-rank update captures essential task-specific changes without degrading generalization
- **Evidence anchors:** [abstract] shows 2.4% Macro-F1 gains over Prefix-Tuned; [section 7] confirms rank=2 setting

### Mechanism 3: Documentation-Based Classification
- **Claim:** Generating tag documentation improves classification by providing richer semantic context
- **Mechanism:** LLM outputs descriptive text (avg. 28 words) embedded by Sentence-T5-XXL and compared via cosine similarity to all tag documentations
- **Core assumption:** XBRL tag documentations are distinct enough semantically for reliable nearest-neighbor matching
- **Evidence anchors:** [abstract] describes unsupervised tag matcher; [section 4] details cosine similarity matching

## Foundational Learning

- **Concept: Instruction Tuning vs Fine-Tuning**
  - Why needed here: Demonstrates instruction tuning FLAN-T5-Large outperforms fine-tuned T5-Large, showing benefit of pre-alignment to task formats
  - Quick check question: What advantage does instruction tuning have over fine-tuning for a task with unseen tags?
  - **Answer:** It enables zero-shot or few-shot capability by learning to generate semantically meaningful outputs rather than memorizing label IDs

- **Concept: Parameter-Efficient Fine-Tuning (PEFT)**
  - Why needed here: Training large models is expensive; LoRA reduces trainable parameters to 0.08% while retaining performance
  - Quick check question: How does LoRA achieve parameter efficiency?
  - **Answer:** By decomposing weight updates into low-rank matrices (rank=2), only these small matrices are trained instead of full weights

- **Concept: Embedding-Based Nearest-Neighbor Matching**
  - Why needed here: Tag matcher compares generated tag documentation embeddings to all tag documentation embeddings using cosine similarity
  - Quick check question: Why is cosine similarity used for tag matching?
  - **Answer:** Because it measures semantic similarity between embeddings, allowing approximate matches even when generated text is not exact

## Architecture Onboarding

- **Component map:** Input Prompt Builder -> FLAN-T5-Large with LoRA -> Tag documentation -> Sentence-T5-XXL Encoder -> Cosine Similarity Matcher -> XBRL Tag Mapper
- **Critical path:** Input → FLAN-T5-Large → Tag documentation → Sentence-T5-XXL → Cosine similarity → XBRL tag
- **Design tradeoffs:** Generating documentation vs raw tags (richer semantics vs longer outputs); LoRA rank=2 vs higher (efficiency vs capacity); Sentence-T5-XXL vs BERT (stronger embeddings vs smaller model size)
- **Failure signatures:** Low Hits@1 (Tag Matcher unable to disambiguate similar documentations); poor zero-shot performance (LLM fails to generalize to unseen tags); high training time (LoRA rank too low)
- **First 3 experiments:**
  1. **Instruction Tuning vs Fine-Tuning:** Train FLAN-T5-Large with LoRA vs T5-Large and compare Macro-F1 on FNXL
  2. **Rank Sensitivity:** Vary LoRA rank (1, 2, 4) and measure Macro-F1 and training time
  3. **Documentation vs Tag Generation:** Train same model to generate raw tags and compare performance with documentation-based approach

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the proposed model perform when incorporating external financial knowledge to address subtle tag differences identified in error analysis?
- **Basis in paper:** [inferred] Authors mention integrating external financial knowledge is a potential future direction to improve performance on challenging cases
- **Why unresolved:** Current model relies solely on sentence-level text without external financial knowledge
- **What evidence would resolve it:** Experiments with extended model incorporating external financial knowledge compared to current model on same test set

### Open Question 2
- **Question:** How does model's performance change when considering context beyond sentence level, such as surrounding paragraphs and associated tables?
- **Basis in paper:** [inferred] Authors acknowledge labeling numerals solely based on sentence-level text can be challenging due to context dependence on surrounding elements
- **Why unresolved:** Current model only considers sentence-level text, potentially limiting ability to capture full context
- **What evidence would resolve it:** Evaluating model performance with additional context from surrounding paragraphs and tables vs current sentence-level approach

### Open Question 3
- **Question:** How does proposed model compare to other large language models, such as BloombergGPT, specifically pre-trained on financial data?
- **Basis in paper:** [explicit] Authors mention BloombergGPT is proprietary with limited accessibility and lack of transparency in data collection and training protocols
- **Why unresolved:** Limited availability and transparency of BloombergGPT prevented direct comparison
- **What evidence would resolve it:** Experiments comparing proposed model's performance with BloombergGPT on same test set (assuming access)

## Limitations
- Reliance on tag documentation matching may fail if generated documentation doesn't accurately capture semantic essence of correct tag
- Only validated on financial datasets (FNXL and FiNER), limiting generalizability to other domains with extreme label spaces
- Tag matcher uses unsupervised nearest-neighbor approach without considering tag hierarchy or relationships

## Confidence

- **High Confidence:** FLAN-T5-Large with LoRA architecture and training procedure (macro-F1 of 66.23% on FNXL dataset)
- **Medium Confidence:** Effectiveness of instruction tuning over fine-tuning for this task (based on limited ablation)
- **Low Confidence:** Robustness of tag matcher to semantically similar documentations and unseen tags

## Next Checks

1. **Zero-Shot Generalization Test:** Evaluate model on completely unseen financial statements (not in FNXL training set) to verify if instruction tuning enables true zero-shot capabilities for rare tags

2. **Tag Documentation Quality Analysis:** Manually examine 50 random predictions where model's generated documentation was semantically distant from correct tag documentation to identify systematic failure patterns

3. **Embedding Space Visualization:** Project Sentence-T5-XXL embeddings of generated vs ground truth tag documentations into 2D space (t-SNE) for cases where model failed to predict correct tag, to assess whether issue is embedding quality or documentation generation