---
ver: rpa2
title: "Risk-sensitive control as inference with R\xE9nyi divergence"
arxiv_id: '2411.01827'
source_url: https://arxiv.org/abs/2411.01827
tags:
- control
- optimal
- risk-sensitive
- policy
- divergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper extends control as inference (CaI) to risk-sensitive\
  \ settings using R\xE9nyi divergence variational inference (RCaI). The key idea\
  \ is to replace the standard Kullback-Leibler (KL) divergence in CaI with R\xE9\
  nyi divergence, where the order parameter $\\alpha$ controls the balance between\
  \ risk-aversion and risk-seeking behavior."
---

# Risk-sensitive control as inference with Rényi divergence

## Quick Facts
- arXiv ID: 2411.01827
- Source URL: https://arxiv.org/abs/2411.01827
- Reference count: 40
- This paper extends control as inference to risk-sensitive settings using Rényi divergence variational inference

## Executive Summary
This paper introduces risk-sensitive control as inference (RCaI) by replacing the standard Kullback-Leibler divergence in control-as-inference with Rényi divergence. The order parameter α controls the balance between risk-aversion and risk-seeking behavior, with α > 1 yielding risk-averse policies and α < 1 yielding risk-seeking policies. The work shows that RCaI is equivalent to log-probability regularized risk-sensitive control with exponential utility, revealing structural connections to maximum entropy control, optimal posterior for CaI, and linearly-solvable control. Based on this framework, the paper derives risk-sensitive reinforcement learning methods including policy gradient and soft actor-critic algorithms that recover their risk-neutral counterparts as the risk-sensitivity parameter vanishes.

## Method Summary
The paper implements risk-sensitive soft actor-critic (RSAC) by modifying the standard SAC algorithm with Rényi divergence variational inference. The implementation follows stable-baselines3 SAC structure with modifications to incorporate the Rényi divergence order parameter η in the policy and value network updates. The method is tested on Pendulum-v1 environment with shared hyperparameters between SAC and RSAC (learning rate 10⁻³, discount factor 0.99), using risk sensitivity parameters η ∈ {-0.02, -0.01, 0.01, 0.02}. Robustness is evaluated by testing policies on perturbed environments with pendulum length changes (l = 1.25, 1.5) without retraining. The experiment runs 20 trials per policy with 100 sampling paths per trial to collect average episode costs and empirical cost distributions under perturbations.

## Key Results
- RCaI using Rényi divergence is equivalent to log-probability regularized risk-sensitive control with exponential utility
- The optimal policy in RCaI has the same Gibbs distribution form as in MaxEnt control and linearly-solvable control
- Risk-sensitive SAC policies are more robust to system perturbations compared to standard SAC, with larger risk-sensitivity parameters yielding better performance under disturbances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rényi divergence variational inference with parameter α directly controls risk sensitivity of the optimal policy
- Mechanism: The order parameter α = 1+η in Rényi divergence replaces the KL divergence in standard control-as-inference, transforming the inference objective into a log-probability regularized risk-sensitive control problem with exponential utility
- Core assumption: The equivalence between minimizing Rényi divergence and maximizing the variational Rényi bound holds under assumptions in Theorem 2
- Evidence anchors: [abstract] "RCaI is shown to be equivalent to log-probability regularized risk-sensitive control", [section 3.1] "RCaI using Rényi divergence is equivalent to maximizing the above variational Rényi bound"

### Mechanism 2
- Claim: The optimal policy in RCaI has the same Gibbs distribution form as in MaxEnt control and linearly-solvable control
- Mechanism: Both LP regularization (via Rényi divergence) and Rényi entropy regularization lead to optimal policies of the form π_t*(u|x) ∝ exp(-Q_t(x,u)), where Q_t satisfies a soft Bellman equation
- Core assumption: The Bellman equation derivation in Theorem 3 is valid under stated assumptions
- Evidence anchors: [abstract] "the risk-sensitive optimal policy can be obtained by solving a soft Bellman equation", [section 3.2] "the Bellman equation for Problem (9) is Vt(xt) = -log ∫U exp(-Qt(xt,u'))du' + infπt D1+η(πt∥π*t)"

### Mechanism 3
- Claim: RCaI provides a unifying framework that recovers risk-neutral methods as the risk-sensitivity parameter vanishes
- Mechanism: As η → 0 (equivalently α → 1), the variational Rényi bound converges to the standard variational bound, and risk-sensitive methods converge to their risk-neutral counterparts
- Core assumption: The convergence of the variational Rényi bound to the standard bound as η → 0 holds
- Evidence anchors: [abstract] "As the risk-sensitivity parameter vanishes, we recover the risk-neutral CaI and RL", [section 4.1] "In the risk-neutral limit η → 0, this estimator converges to the MaxEnt policy gradient estimator"

## Foundational Learning

- Concept: Rényi divergence and its relationship to KL divergence
  - Why needed here: The paper extends CaI by replacing KL divergence with Rényi divergence, where the order parameter α controls risk sensitivity
  - Quick check question: What happens to Rényi divergence as α → 1? (It converges to KL divergence)

- Concept: Control as inference framework and variational inference
  - Why needed here: RCaI builds on the CaI framework by using Rényi divergence variational inference instead of KL divergence
  - Quick check question: In standard CaI, what is the relationship between the optimal posterior and the MaxEnt control policy for deterministic systems? (They are equivalent)

- Concept: Risk-sensitive control and exponential utility
  - Why needed here: The paper shows that RCaI is equivalent to log-probability regularized risk-sensitive control with exponential utility
  - Quick check question: How does the sign of the risk-sensitivity parameter η affect the resulting policy? (η > 0 yields risk-averse policies, η < 0 yields risk-seeking policies)

## Architecture Onboarding

- Component map: Variational Rényi Bound -> Soft Bellman Equation -> Optimal Policy -> RL Algorithm (Policy Gradient or Soft Actor-Critic)
- Critical path: Variational Rényi Bound → Soft Bellman Equation → Optimal Policy → RL Algorithm (Policy Gradient or Soft Actor-Critic)
- Design tradeoffs:
  - Using Rényi divergence vs KL divergence: Rényi provides risk-sensitivity control but may have numerical stability issues for large |η|
  - LP regularization vs Rényi entropy regularization: Both yield same optimal policy structure but have different derivations and interpretations
  - Risk-sensitivity parameter selection: Larger |η| provides more risk-sensitivity but may cause numerical instability
- Failure signatures:
  - Numerical instability: Occurs for large |η| values due to exponential terms in gradients
  - Vanishing gradients: May occur when the variance of the exponentiated returns is too large
  - Policy collapse: May occur if the temperature parameter is not properly tuned
- First 3 experiments:
  1. Verify the equivalence between Rényi divergence variational inference and log-probability regularized risk-sensitive control on a simple linear quadratic system
  2. Test the convergence of the risk-sensitive policy gradient to the risk-neutral version as η → 0
  3. Compare the robustness of risk-sensitive soft actor-critic to standard soft actor-critic under system perturbations on Pendulum-v1 environment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the numerical stability of RSAC scale with the magnitude of the risk-sensitivity parameter η, and can this be theoretically bounded?
- Basis in paper: [explicit] The paper mentions that numerical instability occurs for large |η| due to the gradient terms involving exp(ηQ(ϕ)(xt, ut)), with |η| > 0.03 causing learning failure in Pendulum-v1
- Why unresolved: While the paper identifies the issue, it doesn't provide a theoretical framework for predicting or bounding the maximum stable η for different environments or cost scales
- What evidence would resolve it: A theoretical analysis deriving stability bounds on η as a function of cost function magnitude, or empirical studies systematically mapping η ranges to stability across multiple environments

### Open Question 2
- Question: What is the optimal choice of the Rényi divergence order parameter (1+η) for specific control tasks, and how does it relate to environmental uncertainty?
- Basis in paper: [explicit] The paper shows that η determines risk sensitivity but doesn't provide guidance on selecting η for specific tasks, noting this as an important future work
- Why unresolved: The relationship between environmental properties (noise levels, disturbance magnitude) and optimal η values is not established, making practical application challenging
- What evidence would resolve it: Empirical studies correlating environmental uncertainty metrics with optimal η values, or theoretical bounds linking system properties to ideal risk-sensitivity parameters

### Open Question 3
- Question: How do function approximation methods (e.g., neural networks) affect the performance and stability of RSAC compared to risk-neutral soft actor-critic?
- Basis in paper: [inferred] The paper mentions this as future work, noting that "the compatibility of a function approximator for RSAC" needs exploration
- Why unresolved: The paper doesn't investigate how different function approximators (critic architectures, policy networks) interact with the risk-sensitive objective or affect convergence properties
- What evidence would resolve it: Comparative studies of RSAC performance with various network architectures (depth, width, activation functions) against standard SAC, including convergence rates and policy quality metrics

## Limitations

- The experimental validation is limited to a single environment (Pendulum-v1) and small perturbations, providing limited generalizability of robustness claims
- The paper lacks comparative analysis with existing risk-sensitive RL methods, making it difficult to assess practical advantages
- Numerical stability issues for large |η| values are identified but not theoretically characterized or bounded

## Confidence

- **High confidence**: Core equivalences established in Theorem 2 and Theorem 3, structural similarities between control frameworks
- **Medium confidence**: Practical implementation details and numerical stability of derived algorithms, particularly for large risk-sensitivity parameters
- **Low confidence**: Generalizability of robustness claims beyond the single Pendulum-v1 environment, performance relative to existing risk-sensitive methods

## Next Checks

1. **Numerical stability validation**: Systematically test the RSAC algorithm across a wider range of η values (|η| > 0.02) on Pendulum-v1 and other continuous control environments (e.g., Hopper, HalfCheetah) to identify numerical stability thresholds and potential failure modes

2. **Cross-environment robustness**: Evaluate the risk-sensitive policies on environments with different types of perturbations (e.g., friction changes, external forces, system delays) and compare against multiple risk-sensitive baselines (CPO, RARL, EPOpt) to validate the generality of the robustness claims

3. **Convergence and sample efficiency analysis**: Conduct ablation studies on the convergence properties and sample efficiency of the risk-sensitive policy gradient and SAC variants compared to their risk-neutral counterparts, tracking learning curves, final performance, and sensitivity to hyperparameters across multiple random seeds