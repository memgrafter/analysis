---
ver: rpa2
title: 'MiCEval: Unveiling Multimodal Chain of Thought''s Quality via Image Description
  and Reasoning Steps'
arxiv_id: '2410.14668'
source_url: https://arxiv.org/abs/2410.14668
tags:
- step
- mcot
- miceval
- reasoning
- correctness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MiCEval, a novel automated evaluation framework
  for Multimodal Chain-of-Thought (MCoT) reasoning. MiCEval evaluates both the image
  description and reasoning steps in MCoT answers using a fine-grained, human-annotated
  dataset with 903 MCoT answers and 2,889 annotated steps.
---

# MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image Description and Reasoning Steps

## Quick Facts
- arXiv ID: 2410.14668
- Source URL: https://arxiv.org/abs/2410.14668
- Reference count: 40
- Primary result: MiCEval achieves higher correlation with human judgments than existing methods for MCoT evaluation

## Executive Summary
This paper introduces MiCEval, a novel automated evaluation framework for Multimodal Chain-of-Thought (MCoT) reasoning that addresses the challenge of evaluating both image description and reasoning steps in MCoT answers. The framework leverages a fine-grained, human-annotated dataset with 903 MCoT answers and 2,889 annotated steps to assess correctness, relevance, and informativeness at both step-level and MCoT-level. Experiments demonstrate that MiCEval-based evaluation metrics achieve higher correlation with human judgments compared to existing methods like CLIP and LLM-Score, with GPT-4o achieving a Somer's-D score of 0.284. The framework also highlights current MLLM weaknesses in complex reasoning tasks and provides actionable insights for improving MCoT evaluation.

## Method Summary
MiCEval introduces a comprehensive evaluation framework that breaks down MCoT answers into description and reasoning steps, each annotated for correctness, relevance, and informativeness. The framework computes overall scores using a geometric mean of step correctness scores, ensuring balanced performance across both modalities. It employs few-shot learning with MLLMs as verifiers to evaluate MCoT quality, using metrics like Correctnesstype and Correctnessall that combine description and reasoning scores. The approach is validated on four state-of-the-art MLLMs across eight multimodal datasets, demonstrating superior alignment with human judgments compared to existing evaluation methods.

## Key Results
- MiCEval-based metrics achieve higher correlation with human judgments than CLIP and LLM-Score baselines
- GPT-4o achieves a Somer's-D score of 0.284, outperforming other baseline methods
- The framework identifies specific weaknesses in current MLLMs, particularly in complex reasoning tasks
- Few-shot learning shows mixed results, with some models improving while others decline in performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MiCEval's fine-grained step-level annotation improves alignment with human judgments compared to holistic evaluation.
- Mechanism: By breaking down MCoT into description and reasoning steps, and annotating each step for correctness, relevance, and informativeness, MiCEval captures errors that holistic methods miss. This granularity allows MLLMs to focus on specific weaknesses in each modality.
- Core assumption: Step-level evaluation provides more actionable feedback than overall correctness scores.
- Evidence anchors:
  - [abstract]: "MiCEval-based evaluation metrics achieve higher correlation with human judgments compared to existing methods like CLIP and LLM-Score."
  - [section]: "MiCEval introduces a dataset comprising 903 MLLM-generated MCoT answers and 2,889 human-annotated MCoT steps, which serve as the foundation for our comprehensive step-wise evaluations."
- Break condition: If the overhead of step-level annotation outweighs the benefits in evaluation accuracy, or if MLLMs cannot effectively process the granular feedback.

### Mechanism 2
- Claim: The Correctness(i) D ⊙ Correctness(i) R metric (geometric mean of description and reasoning correctness) better captures overall MCoT quality than single-modality metrics.
- Mechanism: This metric ensures both visual and textual components are accurate, preventing cases where strong performance in one modality masks errors in the other. The geometric mean penalizes imbalanced performance.
- Core assumption: Both modalities contribute equally to overall MCoT quality.
- Evidence anchors:
  - [section]: "Correctness(i) = Correctness(i) D ⊙ Correctness(i) R" and "The overall MCoT score is computed as the geometric mean of all step correctness scores."
  - [abstract]: "MiCEval introduces metrics that compute overall scores based on step type (description or reasoning)."
- Break condition: If one modality is inherently more important for certain tasks, or if the geometric mean is too harsh on minor imbalances.

### Mechanism 3
- Claim: Few-shot evaluation improves MLLM verifier performance compared to zero-shot, especially for complex reasoning tasks.
- Mechanism: Few-shot prompts provide examples that help MLLMs understand the specific evaluation criteria and reduce instruction-following errors, particularly for nuanced tasks like error type classification.
- Core assumption: MLLMs can effectively learn from few examples when given clear evaluation criteria.
- Evidence anchors:
  - [section]: "On MiCEval-NORMAL, only LLaV A-1.6-Mistral-7B demonstrated improvement, with a few-shot average accuracy of 0.657 compared to 0.575 in the zero-shot setting."
  - [section]: "The distribution of invalid output proportions for each model across different settings and datasets is shown in Table 5."
- Break condition: If the few-shot examples don't generalize well to new tasks, or if MLLMs overfit to the demonstration style rather than learning the underlying evaluation criteria.

## Foundational Learning

- Concept: Multimodal Chain-of-Thought (MCoT) reasoning
  - Why needed here: Understanding MCoT is essential for comprehending why step-level evaluation is necessary and how MiCEval addresses current evaluation limitations.
  - Quick check question: What are the two main components of MCoT that need to be evaluated separately?

- Concept: Geometric mean as an evaluation metric
  - Why needed here: The geometric mean is used in MiCEval to combine description and reasoning correctness scores, ensuring balanced performance across modalities.
  - Quick check question: Why might a geometric mean be preferred over an arithmetic mean when combining scores from different modalities?

- Concept: Spearman's rank correlation and Somer's D
  - Why needed here: These statistical measures are used to evaluate how well MiCEval metrics align with human judgments, which is the primary validation of the framework.
  - Quick check question: What does a higher Somer's D score indicate about the relationship between two sets of rankings?

## Architecture Onboarding

- Component map: Human-annotated dataset (903 MCoTs, 2,889 steps) -> Step type classification -> Step-level evaluation using correctness metrics -> Geometric mean combination -> Overall MCoT score -> Comparison with human judgments
- Critical path: 1) Collect and annotate MCoT data, 2) Train or prompt MLLMs for step type classification, 3) Evaluate each step using the appropriate correctness metric, 4) Combine step scores into overall MCoT score, 5) Compare against human judgments
- Design tradeoffs: Step-level annotation provides more detailed feedback but requires more human effort; geometric mean ensures balanced performance but may be too harsh on minor imbalances; few-shot learning reduces prompt engineering effort but may not generalize as well as zero-shot
- Failure signatures: Poor correlation with human judgments despite high internal consistency; MLLMs consistently misclassify step types; geometric mean produces overly low scores for otherwise reasonable MCoTs
- First 3 experiments:
  1. Replicate the Pairwise Comparison results on a small subset of the dataset to verify the step type classification accuracy.
  2. Test the Correctnesstype and Correctnessall metrics on a held-out validation set to compare their correlation with human judgments.
  3. Evaluate the impact of different few-shot prompt formats on MLLM verifier performance across the five fine-grained tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the instruction-following limitations of current MLLMs in complex reasoning tasks be systematically addressed?
- Basis in paper: [explicit] The paper explicitly states that current MLLMs show notable weaknesses in both visual and language modalities, especially in handling complex reasoning tasks, and that their instruction-following abilities are not yet able to handle complex instruction tasks effectively.
- Why unresolved: The paper identifies these weaknesses but does not propose specific methods or frameworks to overcome them, leaving a gap in understanding how to improve MLLMs' ability to follow complex instructions in MCoT evaluation.
- What evidence would resolve it: Developing and testing enhanced instruction-tuning methods, architectural improvements, or hybrid approaches that combine MLLMs with external reasoning modules to improve performance on complex MCoT tasks.

### Open Question 2
- Question: What is the optimal number of shots for few-shot evaluation of MLLMs as MCoT verifiers across different task complexities?
- Basis in paper: [explicit] The paper shows that the number of shots does not always correlate with better model performance, with some models declining in performance as shot numbers increased, and that the selection of 1-shot was based on preliminary experiments.
- Why unresolved: The paper does not provide a systematic analysis of how optimal shot numbers vary across different task complexities or MLLM architectures, leaving uncertainty about the generalizability of the 1-shot selection.
- What evidence would resolve it: Conducting comprehensive experiments varying shot numbers across a broader range of task complexities and MLLM architectures to identify optimal shot counts for different evaluation scenarios.

### Open Question 3
- Question: How does the step type classification accuracy of MLLMs impact the overall performance of MiCEval metrics?
- Basis in paper: [explicit] The paper acknowledges that the MLLM's classification of step type affects the MiCEval performance based on the Correctnesstype, and that current research on MCoT step type classification has limitations.
- Why unresolved: The paper does not provide a detailed analysis of how misclassifications in step type affect downstream evaluation metrics or whether alternative classification methods could improve overall performance.
- What evidence would resolve it: Performing ablation studies where step type classifications are artificially manipulated to measure their impact on MiCEval metric performance, and comparing MiCEval results using different step type classification approaches.

## Limitations
- The framework relies heavily on the quality and comprehensiveness of the human-annotated dataset, which may have inherent biases or limited representation of edge cases
- The geometric mean approach assumes equal importance of both modalities, which may not hold for all types of multimodal reasoning tasks
- The few-shot learning improvements may be specific to the MLLM models tested and might not generalize across different model architectures

## Confidence
- High Confidence: The step-level annotation approach and its correlation with human judgments are well-supported by experimental results
- Medium Confidence: The geometric mean combination of description and reasoning scores is theoretically sound but may need adjustment for specific use cases
- Medium Confidence: The few-shot learning improvements are observed but may be model-dependent and limited in generalizability

## Next Validation Checks
1. **Dataset Diversity Validation**: Test the MiCEval framework on additional multimodal datasets not included in the original annotation process to verify its generalizability across different domains and reasoning complexities.

2. **Modality Weighting Analysis**: Conduct ablation studies to determine optimal weighting schemes for description and reasoning scores based on task type, moving beyond the geometric mean assumption to identify when modality-specific adjustments are needed.

3. **Cross-Model Verification**: Evaluate the framework's performance using multiple different MLLM verifiers (not just GPT-4o) to assess the robustness of MiCEval metrics across various model architectures and capabilities.