---
ver: rpa2
title: Are Large Language Models Consistent over Value-laden Questions?
arxiv_id: '2407.02996'
source_url: https://arxiv.org/abs/2407.02996
tags:
- uni00000044
- uni00000048
- uni0000004c
- uni0000004f
- uni00000051
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether large language models (LLMs) exhibit
  consistent values across different question formulations. The authors define value
  consistency as similarity of answers across paraphrases, related questions within
  a topic, multiple-choice and open-ended use-cases, and multilingual translations.
---

# Are Large Language Models Consistent over Value-laden Questions?

## Quick Facts
- arXiv ID: 2407.02996
- Source URL: https://arxiv.org/abs/2407.02996
- Authors: Jared Moore; Tanvi Deshpande; Diyi Yang
- Reference count: 40
- Primary result: Large models (34b+ parameters) are relatively consistent across value-laden questions, performing on par with humans

## Executive Summary
This study investigates whether large language models exhibit consistent values across different question formulations. The authors define value consistency as similarity of answers across paraphrases, related questions within a topic, multiple-choice and open-ended use-cases, and multilingual translations. They introduce a novel dataset, VALUE CONSISTENCY, containing 8,000 questions across 300+ topics in four languages, generated using GPT-4. The study finds that large models (34b+ parameters) are relatively consistent across these measures, performing on par with human participants. Base models are more consistent than fine-tuned models, and models are more consistent on uncontroversial topics than controversial ones. The study also finds that models cannot be steered to particular values using low-dimensional representations like Schwartz's values.

## Method Summary
The authors generated a novel dataset of 8,000 questions across 300+ topics in four languages using GPT-4. They then prompted various base and fine-tuned LLM models with these questions across multiple measures: paraphrase consistency (comparing responses to paraphrased questions), topic consistency (comparing responses to related questions within a topic), use-case consistency (comparing multiple-choice vs open-ended responses), and multilingual consistency (comparing responses across translations). Consistency was measured using d-dimensional Jensen-Shannon divergence. For open-ended responses, a separate stance classifier (LLaMA-3) was used to classify responses before computing consistency.

## Key Results
- Large models (≥34B parameters) show significant consistency across paraphrases, topics, use-cases, and languages, performing on par with human participants
- Base models are more consistently consistent than fine-tuned models across all topics
- Models are more consistent on uncontroversial topics than controversial ones
- Models cannot be steered to particular values using low-dimensional representations like Schwartz's values

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Large models (≥34B parameters) are relatively consistent across paraphrases, use-cases, multilingual translations, and within topics.
- **Mechanism**: Models with sufficient parameter count develop more stable latent representations for value-laden concepts, reducing variance in responses across semantically similar prompts.
- **Core assumption**: Larger models have better generalization and internal consistency for abstract value judgments.
- **Evidence anchors**:
  - [abstract] "we find that large models (34b+ parameters) are relatively consistent across these measures, performing on par with human participants"
  - [section] "we find that large models (≥34 b) are relatively consistent across our measures, performing on par with human participants on topic and paraphrase consistency"
- **Break condition**: If model size is reduced below the threshold or if training data lacks sufficient diversity in value contexts, consistency degrades significantly.

### Mechanism 2
- **Claim**: Base models are more consistently consistent than fine-tuned models across all topics.
- **Mechanism**: Fine-tuning for alignment introduces topic-specific biases that create uneven consistency profiles, while base models maintain more uniform value representations.
- **Core assumption**: Alignment fine-tuning prioritizes task-specific behavior over consistent value representation.
- **Evidence anchors**:
  - [abstract] "Base models are both more consistent compared to fine-tuned models and are uniform in their consistency across topics"
  - [section] "Fig. 6 shows that base models are more consistent compared to alignment fine-tuned models, especially on topic consistency"
- **Break condition**: If fine-tuning data is perfectly balanced across all topics, the consistency gap between base and fine-tuned models would narrow.

### Mechanism 3
- **Claim**: Models cannot be steered to particular values using low-dimensional representations like Schwartz's values.
- **Mechanism**: Low-dimensional value representations lack the specificity and context needed to meaningfully influence model behavior compared to unrelated values.
- **Core assumption**: Value steerability requires rich, contextual representations rather than simple categorical labels.
- **Evidence anchors**:
  - [abstract] "the study also finds that models cannot be steered to particular values using low-dimensional representations like Schwartz's values"
  - [section] "we find that unrelated values are more influential than relevant ones... models were not steerable to these values"
- **Break condition**: If more sophisticated steering techniques or richer value representations are developed, steerability might improve.

## Foundational Learning

- **Concept**: Jensen-Shannon divergence as a measure of distributional similarity
  - Why needed here: Used to quantify consistency between different model responses across paraphrases, topics, and translations
  - Quick check question: If two distributions are identical, what is their Jensen-Shannon divergence?

- **Concept**: d-dimensional Jensen-Shannon divergence (D-D div.)
  - Why needed here: Extends pairwise comparison to multiple distributions, enabling consistency measurement across multiple paraphrases or questions
  - Quick check question: How does D-D divergence differ from average pairwise Jensen-Shannon divergence?

- **Concept**: Semantic equivalence vs. topic consistency
  - Why needed here: Critical distinction for understanding why models perform differently on paraphrases versus related questions within a topic
  - Quick check question: Would you expect higher consistency for paraphrases or for related questions about the same topic? Why?

## Architecture Onboarding

- **Component map**: Data generation (GPT-4) -> Model inference (multiple-choice/open-ended) -> Stance classification (LLaMA-3) -> Consistency calculation (Jensen-Shannon divergence) -> Results aggregation

- **Critical path**: Data generation → Model querying → Stance classification → Consistency calculation → Results aggregation

- **Design tradeoffs**:
  - Using GPT-4 for data generation ensures quality but may introduce bias
  - Log probability vs. generation for multiple-choice balances consistency measurement with computational efficiency
  - Open-ended classification requires additional LLM inference but captures richer model behavior

- **Failure signatures**:
  - Low consistency scores across all measures suggest model instability or poor value representation
  - High variance in topic consistency indicates topic-specific biases or inconsistencies
  - Inconsistent classification of open-ended responses suggests issues with the stance detection model

- **First 3 experiments**:
  1. Compare consistency scores when using different classification models for open-ended responses
  2. Test consistency on a subset of topics with human-verified answers as ground truth
  3. Evaluate consistency when varying temperature and sampling parameters during model inference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes fine-tuned models to be less consistently consistent than base models?
- Basis in paper: [inferred] from finding that base models are more consistent than fine-tuned models
- Why unresolved: The paper notes that fine-tuning data is not available for analysis, preventing direct investigation of the cause
- What evidence would resolve it: Access to and analysis of fine-tuning data for the models studied, allowing researchers to identify specific training factors that impact consistency

### Open Question 2
- Question: How can we make models more consistent over values in some domains than others?
- Basis in paper: [explicit] from discussion section noting the need for future work on domain-specific consistency
- Why unresolved: The paper identifies this as an open question but does not propose specific methods
- What evidence would resolve it: Development and validation of techniques that can selectively improve consistency in targeted domains while maintaining flexibility in others

### Open Question 3
- Question: How do multi-turn conversations over long context windows affect model consistency?
- Basis in paper: [explicit] from discussion noting that such conversations "may dramatically shift model behavior in ways we cannot anticipate"
- Why unresolved: The study uses single-turn prompts and cannot assess conversational effects
- What evidence would resolve it: Empirical studies comparing model consistency across multiple conversational turns versus single interactions

### Open Question 4
- Question: Can we direct models to particular behaviors using low-dimensional representations beyond Schwartz values?
- Basis in paper: [explicit] from finding that models cannot be steered to Schwartz values and call for future research
- Why unresolved: The study only tested one specific value framework (Schwartz) and found it ineffective
- What evidence would resolve it: Successful steering experiments using alternative low-dimensional representations (personality traits, cultural dimensions, etc.)

### Open Question 5
- Question: Does the consistency of LLM values generalize across different languages and cultures?
- Basis in paper: [inferred] from finding that models are most consistent in English on U.S.-based topics, suggesting potential cultural bias
- Why unresolved: The study primarily reports on U.S.-based topics in English, limiting cross-cultural conclusions
- What evidence would resolve it: Comparative studies of model consistency across diverse cultural contexts and non-English languages with equivalent topic coverage

## Limitations
- The study relies on automated metrics (Jensen-Shannon divergence) rather than human judgment to measure consistency
- Questions were generated by GPT-4, which may introduce subtle biases into the dataset
- The study only tests consistency across single-turn prompts and doesn't address multi-turn conversation effects

## Confidence
- High: Base models are more consistently consistent than fine-tuned models
- Medium: Large models (≥34B parameters) show significant consistency across value-laden questions
- Low: Models cannot be steered to particular values using Schwartz's values framework

## Next Checks
1. Conduct human evaluation studies to verify that the consistency measured by Jensen-Shannon divergence correlates with human judgments of value consistency
2. Test consistency across longer temporal intervals to assess whether models maintain stable values over time
3. Evaluate whether more sophisticated value representation schemes (beyond Schwartz's framework) could successfully steer model responses on value-laden topics