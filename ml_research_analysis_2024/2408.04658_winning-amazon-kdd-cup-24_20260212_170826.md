---
ver: rpa2
title: Winning Amazon KDD Cup'24
arxiv_id: '2408.04658'
source_url: https://arxiv.org/abs/2408.04658
tags:
- dataset
- track
- product
- task
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the winning solution for the Amazon KDD Cup
  2024 Multi-Task Online Shopping Challenge for LLMs. The team developed a single
  model per track by fine-tuning Qwen2-72B-Instruct on a custom training dataset created
  by processing multiple public datasets and using LLMs for data augmentation and
  synthetic data generation.
---

# Winning Amazon KDD Cup'24

## Quick Facts
- arXiv ID: 2408.04658
- Source URL: https://arxiv.org/abs/2408.04658
- Authors: Chris Deotte; Ivan Sorokin; Ahmet Erdem; Benedikt Schifferer; Gilberto Titericz; Simon Jegou
- Reference count: 40
- Primary result: First place in each individual track and overall of Amazon's KDD Cup 2024 Multi-Task Online Shopping Challenge for LLMs

## Executive Summary
This paper presents the winning solution for the Amazon KDD Cup 2024 Multi-Task Online Shopping Challenge, where the team developed a single model per track by fine-tuning Qwen2-72B-Instruct on a custom training dataset. The solution involved processing multiple public datasets and using LLMs for data augmentation and synthetic data generation, followed by LoRA adapters, wise-ft interpolation, and Logits Processors for output constraints. The final model was deployed using AWQ 4-bit quantization and vLLM inference to meet strict time constraints while achieving first place in each individual track and overall.

## Method Summary
The team fine-tuned Qwen2-72B-Instruct on a custom dataset of 502k synthetic and processed samples using 8x NVIDIA A100 GPUs, which took approximately 24 hours. They created multiple LoRA adapters (v7, v8, v7b, v9b) trained on different data subsets and merged them into an ensemble, then applied wise-ft to optimize the interpolation between base and ensemble weights. Logits Processors were employed to constrain model output to valid formats, and AWQ 4-bit quantization with vLLM inference enabled deployment within 20-140 minute time constraints per track.

## Key Results
- Achieved first place in each individual track: Shopping Concept Understanding, Shopping Knowledge Reasoning, User Behavior Alignment, Multi-lingual Abilities, and Overall
- Successfully predicted test datasets within strict time constraints of 20 to 140 minutes depending on the track
- Developed a single model per track approach using ensembled LoRA adapters and wise-ft interpolation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning Qwen2-72B-Instruct on a custom dataset improves task-specific performance across diverse online shopping tasks.
- Mechanism: Supervised fine-tuning on 502k synthetic and processed samples teaches the model the patterns and formats required for the 57 tasks in ShopBench.
- Core assumption: The synthetic and processed dataset adequately represents the distribution of real evaluation tasks, including unseen tasks.
- Evidence anchors:
  - [abstract] "We fine-tune Qwen2-72B-Instruct on our own training dataset... Our solution achieved the first place in each individual track"
  - [section 4.2] "We fine-tuned Qwen2-72B-Instruct [18] on our developed training dataset using 8x NVIDIA A100... Training on 500k examples takes around 24 hours"
  - [corpus] Weak - no direct evidence of dataset coverage, only mention of 502k samples total
- Break condition: If the training dataset poorly represents evaluation tasks, fine-tuning will overfit to irrelevant patterns or fail on unseen tasks.

### Mechanism 2
- Claim: Ensembling multiple LoRA adapters and applying wise-ft improves generalization by balancing base and fine-tuned model weights.
- Mechanism: Multiple adapters trained on different data subsets are merged with the base model, then wise-ft interpolates between base and ensemble weights to optimize for distribution shift.
- Core assumption: Different adapters capture complementary task patterns, and wise-ft can find an optimal interpolation factor.
- Evidence anchors:
  - [section 4.3] "Our five track solutions are created from 4 fine-tuned LoRA adapters... Version 7 adapter was trained with 417k samples whereas v8 was trained with 462k"
  - [section 4.4] "We implemented wise-ft by rescaling the ensembled adapter weights... For each track, we optimized α based on leaderboard results"
  - [corpus] Weak - no explicit leaderboard results for wise-ft effectiveness shown
- Break condition: If adapters are too similar or wise-ft interpolation is poorly optimized, ensemble performance may not improve over single adapter.

### Mechanism 3
- Claim: Logits Processors constrain model output to valid formats, improving evaluation metric scores.
- Mechanism: Processors modify logits to favor digits, commas, or prompt tokens depending on task type, ensuring generated text is parseable by evaluation scripts.
- Core assumption: Constraining outputs does not harm task performance but ensures compliance with evaluation format requirements.
- Evidence anchors:
  - [section 4.5] "We employed a variety of logits processors to generate outputs in specific formats... These logits processors were particularly useful in Phase 1"
  - [abstract] "We employed Logits Processors to constrain the model output on relevant tokens for the tasks"
  - [corpus] Weak - no quantitative evidence of impact on scores
- Break condition: If constraints are too strict, they may prevent correct answers that don't match the expected format.

## Foundational Learning

- Concept: Supervised fine-tuning of LLMs
  - Why needed here: The competition provided only 96 examples, requiring synthetic data generation and fine-tuning to teach task-specific behaviors
  - Quick check question: What loss function is used during supervised fine-tuning when only answer tokens are considered?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: Enables efficient fine-tuning of 72B parameter models on limited GPU memory while allowing adapter ensembling
  - Quick check question: How does LoRA modification of model weights differ from full fine-tuning?

- Concept: Quantization for inference optimization
  - Why needed here: 4-bit quantization reduces model size from 150GB to 40GB, enabling deployment on 16GB T4 GPUs within time constraints
  - Quick check question: What is the relationship between quantization bit depth and inference speed/memory usage?

## Architecture Onboarding

- Component map: Qwen2-72B-Instruct → LoRA adapters (v7, v8, v7b, v9b) → wise-ft interpolation → Logits Processors → vLLM inference with 4-bit AWQ quantization
- Critical path: Data preparation → Fine-tuning (multiple adapters) → Ensemble + wise-ft → Logits processing → Quantized inference
- Design tradeoffs: Larger models provide better performance but require more aggressive quantization; more adapters improve diversity but increase complexity
- Failure signatures: Poor generalization indicates dataset distribution mismatch; inference timeouts suggest quantization or batching issues
- First 3 experiments:
  1. Fine-tune base model on subset of training data and evaluate on development set
  2. Test wise-ft interpolation factors (α) on single adapter to find optimal value
  3. Validate Logits Processors by checking parseable output format on sample tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the distribution shift between the evaluation data from the ShopBench dataset and the collected training data specifically impact model performance, and what additional techniques beyond wise-ft could be explored to address this?
- Basis in paper: [explicit] The paper mentions using wise-ft to account for distribution shifts between the ShopBench dataset and the collected training data, and provides some results showing accuracy gains.
- Why unresolved: The paper only briefly mentions wise-ft and doesn't explore other potential methods for handling distribution shifts. The impact of distribution shift on model performance is not fully characterized.
- What evidence would resolve it: A more detailed analysis of the distribution shift between the datasets, and experiments comparing wise-ft to other techniques like domain adaptation, data augmentation, or ensembling base and fine-tuned models.

### Open Question 2
- Question: What is the optimal balance between supervised fine-tuning and reinforcement learning from human feedback (RLHF) for this type of multi-task online shopping challenge, and how does this balance vary across different task types?
- Basis in paper: [explicit] The paper mentions that supervised fine-tuning was sufficient for the competition, but hypothesizes that RLHF might not be necessary because many answers have exact solutions. However, this hypothesis is not tested.
- Why unresolved: The paper only uses supervised fine-tuning and does not compare it to RLHF. The optimal balance between these two approaches is unknown, and it may vary depending on the task type.
- What evidence would resolve it: Experiments comparing supervised fine-tuning to RLHF, and to hybrid approaches, across different task types in the online shopping domain.

### Open Question 3
- Question: How does the choice of base model (e.g., Qwen2-72B, LLaMa3-70B, Smaug-72B) impact performance on the different task types in the online shopping challenge, and what characteristics of the base model are most important for success?
- Basis in paper: [explicit] The paper compares different base models (Qwen2-72B, LLaMa3-70B, Smaug-72B) in terms of their zero-shot performance on the different tracks, but does not explore how the choice of base model impacts performance after fine-tuning.
- Why unresolved: The paper only compares base model performance in a zero-shot setting. It does not explore how the choice of base model impacts performance after fine-tuning, or what characteristics of the base model (e.g., size, architecture, pre-training data) are most important for success in this domain.
- What evidence would resolve it: Experiments comparing the performance of different base models after fine-tuning on the online shopping tasks, and an analysis of the characteristics of the base models that correlate with performance.

## Limitations
- The exact composition and quality of the synthetic training dataset is not fully specified, making it difficult to assess whether the reported success generalizes to other competition scenarios
- No quantitative evidence is provided showing the individual contribution of each technical component (LoRA ensembling, wise-ft, Logits Processors) to the final performance
- The specific optimization of wise-ft interpolation weights (α values) is based on leaderboard feedback rather than systematic ablation studies

## Confidence

**High confidence**: The general approach of fine-tuning large language models on domain-specific data for multi-task learning is well-established and the technical implementation details (LoRA, quantization, inference optimization) are sound

**Medium confidence**: The effectiveness of the ensembled LoRA adapters and wise-ft interpolation, as the paper claims improvement but lacks ablation evidence

**Low confidence**: The claim that Logits Processors significantly contributed to the win, as only qualitative statements are provided without performance metrics

## Next Checks

1. **Dataset Coverage Validation**: Perform a detailed analysis of the 502k training examples to verify they adequately represent the distribution of the 57 ShopBench tasks, particularly the 39 unseen tasks, through task-type frequency analysis and similarity scoring against the 96 development examples

2. **Component Ablation Study**: Systematically remove each key component (wise-ft interpolation, LoRA ensembling, Logits Processors) and measure the performance drop on a held-out validation set to quantify their individual contributions to the winning score

3. **Generalization Test**: Apply the trained model to a different but related multi-task online shopping benchmark (such as the Shopping MMLU dataset mentioned in related work) to assess whether the solution generalizes beyond the specific Amazon KDD Cup evaluation conditions