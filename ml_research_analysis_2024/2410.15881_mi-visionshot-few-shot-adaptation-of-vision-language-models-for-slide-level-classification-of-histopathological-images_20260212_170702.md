---
ver: rpa2
title: 'MI-VisionShot: Few-shot adaptation of vision-language models for slide-level
  classification of histopathological images'
arxiv_id: '2410.15881'
source_url: https://arxiv.org/abs/2410.15881
tags:
- learning
- image
- shot
- language
- patches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MI-VisionShot addresses the challenge of prompt-dependent variability
  in zero-shot transfer for slide-level classification of histopathological images.
  The method proposes a training-free adaptation approach based on prototypical learning,
  which constructs class prototypes by retrieving the most discriminative patches
  within each slide using a vision-language model.
---

# MI-VisionShot: Few-shot adaptation of vision-language models for slide-level classification of histopathological images

## Quick Facts
- arXiv ID: 2410.15881
- Source URL: https://arxiv.org/abs/2410.15881
- Authors: Pablo Meseguer; Rocío del Amor; Valery Naranjo
- Reference count: 20
- Primary result: Achieves up to 8.2% and 7.4% improvements in low-shot learning scenarios (k=2, k=4) compared to zero-shot transfer approaches

## Executive Summary
MI-VisionShot addresses the challenge of prompt-dependent variability in zero-shot transfer for slide-level classification of histopathological images. The method proposes a training-free adaptation approach based on prototypical learning, which constructs class prototypes by retrieving the most discriminative patches within each slide using a vision-language model. Experimental results on a renal cell carcinoma dataset demonstrate that MI-VisionShot surpasses zero-shot transfer with lower variability, achieving significant improvements in low-shot learning scenarios.

## Method Summary
MI-VisionShot is a training-free adaptation framework that leverages vision-language models for slide-level classification of histopathological images. The approach constructs class prototypes by retrieving the top-K most discriminative patches within each slide based on similarity to class text embeddings. It processes gigapixel whole slide images under a multiple-instance learning paradigm, selecting only the most informative patches to create global slide embeddings through batch global average pooling. Class prototypes are built by averaging training slide embeddings per class, and predictions are made by computing similarity between test samples and class prototypes. The method shows particular effectiveness in few-shot scenarios (k=2, 4, 8, 16 samples per class) where traditional zero-shot approaches struggle with variability.

## Key Results
- MI-VisionShot achieves 8.2% improvement in balanced accuracy for k=2 samples per class compared to zero-shot transfer
- MI-VisionShot achieves 7.4% improvement in balanced accuracy for k=4 samples per class compared to zero-shot transfer
- The method demonstrates lower variability across different text prompts compared to baseline approaches
- Performance improvements are consistent across 5-fold cross-validation experiments on the TCGA renal cell carcinoma dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method reduces prompt-dependent variability by retrieving only the top-K patches most similar to the class text embedding, creating more discriminative prototypes.
- Mechanism: Instead of using all patches from a slide, the approach computes similarity scores between each patch embedding and the class text embedding, then selects the K most similar patches to construct the prototype. This weighted selection focuses the prototype on the most class-representative regions.
- Core assumption: The most discriminative patches for a class will have embeddings most similar to the text embedding of that class label.
- Evidence anchors:
  - [abstract]: "Our framework takes advantage of the excellent representation learning of VLM to create prototype-based classifiers under a multiple-instance setting by retrieving the most discriminative patches within each slide."
  - [section]: "MI-VisionShot retrieves the top-K patches with a greater representation of the class according to the text information."
  - [corpus]: No direct evidence in neighbors, but related works mention prototype-based approaches and few-shot learning strategies.
- Break condition: If the vision-language model's embeddings are not well-aligned between vision and text spaces, the similarity scores would not reliably identify class-representative patches.

### Mechanism 2
- Claim: Prototype learning with few training samples creates more effective classifiers than zero-shot transfer with multiple prompts.
- Mechanism: By averaging the global embeddings of only K training samples per class (rather than relying on text prompts), the method creates class prototypes that are less variable and more tailored to the specific dataset distribution.
- Core assumption: Even with few training samples, the global embeddings constructed from the most informative patches provide sufficient signal to create effective prototypes.
- Evidence anchors:
  - [abstract]: "MI-VisionShot to surpass zero-shot transfer with lower variability, even in low-shot scenarios."
  - [section]: "These metrics show that selecting the most representative instances within a bag is even more important in scenarios with fewer data where the prototypes calculated as the average of all samples are not as well defined."
  - [corpus]: Weak evidence - neighbors discuss few-shot learning but don't specifically address prototype construction from limited samples.
- Break condition: If the dataset has high intra-class variability or the few training samples are not representative, the prototypes may not capture the class distribution well.

### Mechanism 3
- Claim: The multiple-instance learning framework effectively handles gigapixel whole slide images by focusing on patch-level information without requiring full image processing.
- Mechanism: The approach processes individual patches independently through the vision encoder, then aggregates only the most relevant patches into a global embedding, avoiding the computational burden of processing entire WSIs.
- Core assumption: Patch-level features can be aggregated meaningfully to represent the whole slide, and selecting only top-K patches preserves diagnostic information.
- Evidence anchors:
  - [abstract]: "Our framework takes advantage of the excellent representation learning of VLM to create prototype-based classifiers under a multiple-instance setting by retrieving the most discriminative patches within each slide."
  - [section]: "WSI can span up to more than 100.000 squared pixels, thus translating a very large number of instances after patching them to follow the MIL paradigm."
  - [corpus]: Weak evidence - neighbors mention MIL approaches but don't specifically discuss patch selection for gigapixel images.
- Break condition: If diagnostically important features are distributed across many patches or if background tissue contains relevant context, selecting only top-K patches could lose critical information.

## Foundational Learning

- Concept: Vision-language model alignment through contrastive pretraining
  - Why needed here: The method relies on the vision encoder producing embeddings that are meaningfully comparable to text embeddings in the same space
  - Quick check question: Can you explain how CLIP-style models create aligned vision and text embedding spaces?

- Concept: Multiple instance learning (MIL) framework
  - Why needed here: Whole slide images contain thousands of patches, and the method needs to handle this as a bag-of-instances problem rather than processing the entire image
  - Quick check question: What is the key difference between standard supervised learning and multiple instance learning in terms of label assignment?

- Concept: Prototype-based few-shot learning
  - Why needed here: The method constructs class prototypes from few training samples rather than using traditional fine-tuning or zero-shot transfer
  - Quick check question: How does prototypical learning differ from nearest centroid classification in the few-shot setting?

## Architecture Onboarding

- Component map: Vision encoder (PLIP) -> Similarity calculator -> Top-K selector -> BGAP -> Prototype constructor -> Inference engine
- Critical path: Vision encoder → Similarity calculator → Top-K selector → BGAP → Prototype constructor → Inference engine
- Design tradeoffs:
  - K value selection: Too small loses information, too large includes irrelevant patches
  - Patch size and resolution: Affects feature quality and computational cost
  - Text prompt formulation: Impacts text embedding quality and alignment
  - Number of training samples: Balances between prototype quality and few-shot constraint
- Failure signatures:
  - High variance across seeds: May indicate prototype instability or poor sample selection
  - Performance degradation with larger K: Suggests inclusion of non-discriminative patches
  - Poor zero-shot baseline: Indicates vision-language model may not be well-suited for the domain
  - Large gap between k=2 and k=16 performance: Suggests prototype quality highly dependent on sample count
- First 3 experiments:
  1. Implement patch-level similarity scoring with a fixed K=200 and verify that selected patches visually correspond to class-relevant regions
  2. Test prototype construction with varying numbers of training samples (k=2, k=4, k=8) and measure variance across seeds
  3. Compare BGAP embeddings vs. top-K weighted embeddings using silhouette score to validate improved class separation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of patches to select for MI-VisionShot to achieve the best performance in slide-level classification tasks?
- Basis in paper: [explicit] The paper states that MI-VisionShot retrieves the top-K patches with a greater representation of the class according to the text information, and experiments are conducted with different sets of top-K instances.
- Why unresolved: The paper shows that K=200 yields the best performance in their experiments, but this might not be the optimal value for all datasets or tasks.
- What evidence would resolve it: Conducting extensive experiments with various values of K on different datasets and tasks to determine the optimal number of patches for MI-VisionShot.

### Open Question 2
- Question: How does MI-VisionShot compare to other vision-language models in terms of performance and efficiency?
- Basis in paper: [explicit] The paper compares MI-VisionShot to other training-free adaptation methods based on prototypical learning, such as TIP-Adapter and MI-SimpleShot, but does not compare it to other vision-language models.
- Why unresolved: The paper does not provide a comprehensive comparison of MI-VisionShot to other vision-language models, making it difficult to assess its relative performance and efficiency.
- What evidence would resolve it: Conducting a thorough comparison of MI-VisionShot to other vision-language models on various datasets and tasks, considering both performance and computational efficiency.

### Open Question 3
- Question: How does MI-VisionShot perform on other tasks in digital pathology beyond slide-level classification?
- Basis in paper: [explicit] The paper focuses on slide-level classification tasks, but mentions that further research should focus on exploiting the alignment between images and natural language in other crucial tasks in digital pathology.
- Why unresolved: The paper does not explore the performance of MI-VisionShot on other tasks in digital pathology, such as image captioning or region of interest detection.
- What evidence would resolve it: Evaluating MI-VisionShot on various tasks in digital pathology, such as image captioning, region of interest detection, or survival outcome prediction, to assess its versatility and potential applications.

## Limitations

- Performance highly dependent on text prompt formulation, with different prompts yielding varying results in zero-shot transfer scenarios
- Optimal K value selection is critical but not thoroughly explored across different datasets or class distributions
- Assumes that most similar patches to class text embeddings are the most discriminative, which may not hold for complex histopathological patterns

## Confidence

- **High Confidence**: The experimental results showing improved performance over zero-shot transfer with lower variability (8.2% and 7.4% improvements for k=2, k=4) are well-supported by the 5-fold cross-validation results on the RCC dataset.
- **Medium Confidence**: The claim that prototype learning with few training samples creates more effective classifiers than zero-shot transfer is supported by the results but may be dataset-dependent.
- **Low Confidence**: The generalizability of the approach to other cancer types and histopathological domains is suggested but not demonstrated beyond the RCC dataset.

## Next Checks

1. **Prompt sensitivity analysis**: Systematically vary text prompts across different formulations and measure performance variance to quantify the robustness to prompt engineering.

2. **K-value optimization across classes**: Test different K values (e.g., K=50, 100, 200, 500) for each RCC subtype individually to determine if optimal patch selection varies by class complexity.

3. **Cross-domain validation**: Apply MI-VisionShot to a different histopathological dataset (e.g., breast cancer or lung cancer) using the same framework and prompts to evaluate whether the performance gains transfer beyond renal cell carcinoma.