---
ver: rpa2
title: 'Laser: Parameter-Efficient LLM Bi-Tuning for Sequential Recommendation with
  Collaborative Information'
arxiv_id: '2409.01605'
source_url: https://arxiv.org/abs/2409.01605
tags:
- recommendation
- item
- laser
- sequential
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of sequential recommendation by
  proposing a parameter-efficient framework called Laser, which leverages Large Language
  Models (LLMs) to encode item semantics while incorporating user-specific collaborative
  information. The core method involves a Bi-Tuning approach that uses trainable virtual
  tokens (prefix and suffix) to adapt LLMs for recommendation tasks without updating
  their parameters.
---

# Laser: Parameter-Efficient LLM Bi-Tuning for Sequential Recommendation with Collaborative Information

## Quick Facts
- arXiv ID: 2409.01605
- Source URL: https://arxiv.org/abs/2409.01605
- Reference count: 40
- Primary result: Parameter-efficient LLM-based sequential recommendation achieving up to 13.27% improvement in NDCG@10

## Executive Summary
This paper addresses the challenge of leveraging Large Language Models (LLMs) for sequential recommendation tasks while maintaining parameter efficiency. The proposed Laser framework introduces a Bi-Tuning approach that adapts LLMs using only trainable virtual tokens (prefix and suffix) while freezing the LLM parameters. Additionally, an MoE-based querying transformer (M-Former) is introduced to effectively integrate user-specific collaborative information encoded by frozen ID-based sequential recommender systems. The framework demonstrates significant improvements over state-of-the-art baselines while maintaining high parameter efficiency with only 0.135M trainable parameters.

## Method Summary
Laser employs a two-stage training strategy to adapt pre-trained LLMs for sequential recommendation tasks. The framework first formulates user interaction sequences into coherent text using item attributes (title, category, brand), then applies Bi-Tuning by inserting trainable virtual tokens at both prefix and suffix positions while freezing LLM parameters. The M-Former component uses a mixture-of-experts approach to integrate collaborative information through a router that selects appropriate query experts based on user characteristics. The two-stage training process first optimizes item embeddings and then fine-tunes the recommendation model based on these fixed embeddings, achieving parameter-efficient adaptation without compromising recommendation quality.

## Key Results
- Achieves up to 13.27% improvement in NDCG@10 over state-of-the-art baselines
- Achieves up to 11.37% improvement in Recall@10 on real-world datasets
- Maintains high parameter efficiency with only 0.135M trainable parameters
- Demonstrates effectiveness across multiple Amazon review datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Bi-Tuning method adapts LLMs to sequential recommendation by inserting trainable virtual tokens at the prefix and suffix while freezing LLM parameters.
- Mechanism: By freezing LLM parameters and optimizing only the trainable virtual tokens, the method reduces the number of trainable parameters while allowing the LLM to learn task-specific information through the prefix (collaborative information) and suffix (space conversion).
- Core assumption: Virtual tokens can effectively serve as placeholders to capture task-specific information without updating the LLM's core parameters.
- Evidence anchors:
  - [abstract]: "Bi-Tuning works by inserting trainable virtual tokens at both the prefix and suffix of the input sequence and freezing the LLM parameters, thus optimizing the LLM for the sequential recommendation."
  - [section]: "Specifically, Bi-Tuning works by inserting trainable virtual tokens at both the prefix and suffix of the input sequence and freezing the LLM parameters, thus optimizing the LLM for the sequential recommendation."
  - [corpus]: No direct evidence found in corpus; this is an inference from the paper's claims.
- Break condition: If virtual tokens cannot capture task-specific information effectively, the method would fail to adapt the LLM to the recommendation task.

### Mechanism 2
- Claim: The M-Former captures diverse user characteristics by employing a set of query experts to integrate user-specific collaborative information.
- Mechanism: The M-Former uses a router to select the most appropriate query expert based on the collaborative characteristics of a specific user, allowing the expert to interact with collaborative information encoded by frozen ID-based sequential recommender systems.
- Core assumption: Different types of users have diverse collaborative characteristics that can be effectively captured by different query experts.
- Evidence anchors:
  - [abstract]: "Furthermore, to capture the characteristics of different types of users when integrating the collaborative information via the prefix, we introduce M-Former, a lightweight MoE-based querying transformer that uses a set of query experts to integrate diverse user-specific collaborative information encoded by frozen ID-based sequential recommender systems."
  - [section]: "In order to select the most appropriate expert to deal with user-specific collaborative information, we set up a router to calculate the scores of different experts given the specific user."
  - [corpus]: No direct evidence found in corpus; this is an inference from the paper's claims.
- Break condition: If the router cannot accurately select the appropriate query expert, the method would fail to capture diverse user characteristics.

### Mechanism 3
- Claim: The two-stage training strategy improves recommendation results by first finding appropriate parameter weights to obtain high-quality item embeddings and then further training based on these fixed embeddings.
- Mechanism: In the first training stage, item embeddings are updated using current parameter weights, and the best-performing epoch is selected. In the second stage, Laser is trained for multiple epochs based on these fixed item embeddings to achieve optimal recommendation results.
- Core assumption: High-quality item embeddings are necessary for effective user-item similarity comparison and recommendation.
- Evidence anchors:
  - [abstract]: "Experimental results on real-world datasets show that Laser significantly outperforms state-of-the-art baselines, achieving improvements of up to 13.27% in NDCG@10 and 11.37% in Recall@10, while maintaining high parameter efficiency with only 0.135M trainable parameters."
  - [section]: "We employ a two-stage training strategy to first find the most appropriate parameter weights to obtain high-quality item embeddings used for user-item similarity comparison, and then further train Laser based on these fixed item embeddings to achieve the best recommendation results."
  - [corpus]: No direct evidence found in corpus; this is an inference from the paper's claims.
- Break condition: If the two-stage training does not lead to improved item embeddings, the method would fail to enhance recommendation results.

## Foundational Learning

- Concept: Large Language Models (LLMs)
  - Why needed here: LLMs are used to encode item semantics and provide item/user embeddings for similarity comparison and next item prediction.
  - Quick check question: What is the role of LLMs in the Laser framework?

- Concept: Parameter-efficient Fine-tuning
  - Why needed here: The Bi-Tuning method reduces the number of trainable parameters by freezing LLM parameters and optimizing only the trainable virtual tokens.
  - Quick check question: How does the Bi-Tuning method achieve parameter efficiency?

- Concept: Mixture of Experts (MoE)
  - Why needed here: The M-Former employs a set of query experts to capture diverse user characteristics and integrate collaborative information.
  - Quick check question: What is the purpose of using MoE in the M-Former?

## Architecture Onboarding

- Component map: Input text formulation -> Bi-Tuning (prefix and suffix) -> M-Former (collaborative information) -> Two-stage training -> Recommendation
- Critical path: Input text → Bi-Tuning (prefix and suffix) → M-Former (collaborative information) → Two-stage training → Recommendation
- Design tradeoffs:
  - Parameter efficiency vs. model performance: Bi-Tuning reduces trainable parameters but may limit the model's ability to learn complex patterns.
  - Complexity of M-Former vs. effectiveness: More query experts may improve performance but increase computational cost.
- Failure signatures:
  - Poor recommendation results: May indicate issues with Bi-Tuning, M-Former, or two-stage training.
  - High computational cost: May indicate inefficiencies in the M-Former or two-stage training.
- First 3 experiments:
  1. Test the effectiveness of Bi-Tuning by comparing recommendation results with and without trainable virtual tokens.
  2. Evaluate the impact of the number of query experts in M-Former on recommendation performance.
  3. Assess the contribution of two-stage training to recommendation results by comparing with single-stage training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Laser scale with the size of the LLM backbone, and what is the optimal trade-off between model size and recommendation quality?
- Basis in paper: [inferred] The paper uses ChatGLM2-6B and demonstrates significant improvements over baselines, but does not explore larger or smaller LLMs.
- Why unresolved: The paper focuses on a single LLM backbone and does not conduct experiments with different model sizes to determine the optimal balance between computational efficiency and recommendation accuracy.
- What evidence would resolve it: Systematic experiments comparing Laser's performance using LLMs of varying sizes (e.g., 1B, 3B, 13B parameters) on the same datasets, measuring both recommendation metrics and computational costs.

### Open Question 2
- Question: How does Laser's performance compare to traditional sequential recommendation methods in terms of interpretability and explainability of recommendations?
- Basis in paper: [inferred] The paper focuses on recommendation accuracy and parameter efficiency but does not discuss the interpretability of the generated recommendations.
- Why unresolved: The paper does not provide any analysis of the interpretability or explainability of the recommendations produced by Laser, which is an important consideration for practical deployment in recommendation systems.
- What evidence would resolve it: User studies or qualitative analysis comparing the interpretability of recommendations from Laser versus traditional methods, such as SASRec or BERT4Rec, potentially using techniques like attention visualization or case studies.

### Open Question 3
- Question: How does Laser's performance vary across different types of user behavior patterns (e.g., long-term vs. short-term preferences, diverse vs. focused interests)?
- Basis in paper: [inferred] The paper mentions that M-Former captures diverse user characteristics but does not provide a detailed analysis of how Laser performs for different user behavior patterns.
- Why unresolved: The paper does not segment the user base or analyze Laser's performance for different types of user behavior patterns, which would provide insights into the method's strengths and weaknesses in handling diverse user preferences.
- What evidence would resolve it: Experiments that segment users based on their behavior patterns (e.g., using clustering or other methods) and evaluate Laser's performance for each segment, comparing it to traditional methods to identify any differences in effectiveness.

## Limitations

- The theoretical justification for why freezing LLM parameters while only optimizing virtual tokens leads to effective recommendation adaptation remains unclear
- Evaluation is limited to Amazon review datasets with specific item attributes, limiting generalizability to different recommendation domains
- The two-stage training strategy introduces additional complexity in hyperparameter tuning and may not generalize well to different LLM backbones

## Confidence

**High Confidence Claims**:
- The parameter efficiency claim (0.135M trainable parameters) is well-supported by the experimental results and architectural design
- The baseline comparison methodology is sound, with appropriate metrics (Recall@10, NDCG@10, MRR) and multiple dataset evaluations

**Medium Confidence Claims**:
- The effectiveness of Bi-Tuning for sequential recommendation adaptation is supported by empirical results but lacks theoretical justification
- The M-Former's ability to capture diverse user characteristics through MoE is demonstrated but could benefit from more extensive ablation studies

**Low Confidence Claims**:
- The universal applicability of the proposed framework across different recommendation domains and LLM architectures
- The scalability of the approach to much larger item catalogs and more complex recommendation scenarios

## Next Checks

1. **Ablation Study on Bi-Tuning Components**: Conduct a comprehensive ablation study removing either the prefix tokens, suffix tokens, or both to quantify their individual contributions to recommendation performance. This would validate the claim that both components are necessary for optimal performance.

2. **Cross-Dataset Generalization Test**: Evaluate Laser on datasets with different characteristics (e.g., different item attribute types, varying levels of sparsity, different user interaction patterns) to assess the robustness of the claimed improvements across diverse recommendation scenarios.

3. **Alternative LLM Backbone Validation**: Test the framework with different LLM architectures (e.g., smaller or larger models, different pre-training objectives) to verify that the parameter-efficient adaptation strategy generalizes beyond the specific ChatGLM2-6B backbone used in the experiments.