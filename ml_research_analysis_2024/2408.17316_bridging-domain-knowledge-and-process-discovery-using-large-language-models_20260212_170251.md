---
ver: rpa2
title: Bridging Domain Knowledge and Process Discovery Using Large Language Models
arxiv_id: '2408.17316'
source_url: https://arxiv.org/abs/2408.17316
tags:
- process
- claim
- block
- knowledge
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to incorporate domain knowledge
  into process discovery by leveraging Large Language Models (LLMs). The key idea
  is to use LLMs to translate natural language descriptions of business processes
  into declarative rules, which are then used to guide the Inductive Mining (IMr)
  framework during process model construction.
---

# Bridging Domain Knowledge and Process Discovery Using Large Language Models

## Quick Facts
- arXiv ID: 2408.17316
- Source URL: https://arxiv.org/abs/2408.17316
- Reference count: 18
- Primary result: LLM-extracted rules significantly improve process model alignment with domain knowledge

## Executive Summary
This paper presents a novel approach to integrate domain knowledge into process discovery by leveraging Large Language Models (LLMs) to translate natural language process descriptions into declarative rules. These rules are then used to guide the Inductive Mining (IMr) framework during process model construction. The framework addresses the challenge of incorporating valuable domain knowledge, often expressed in natural language by experts or in textual process documentation, into automated process discovery methods. A case study with the UWV employee insurance agency demonstrates that discovered process models, guided by rules extracted from ChatGPT, show significant improvements in alignment with domain knowledge compared to models discovered without such guidance.

## Method Summary
The proposed framework translates natural language process descriptions into declarative rules using LLMs, which are then validated and integrated into the IMr framework to guide process model discovery. The approach involves defining specific tasks for the LLM, extracting and validating rules, and iteratively refining them through interaction with domain experts. The method is demonstrated through a case study where LLM-extracted rules are used to improve the alignment of discovered process models with domain knowledge.

## Key Results
- LLM-extracted rules significantly improve process model alignment with domain knowledge
- The framework successfully translates natural language descriptions into syntactically correct declarative rules
- Interactive refinement with LLMs helps resolve ambiguities in process descriptions
- Validated rules effectively guide the IMr framework to discover more accurate process models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs translate natural language process descriptions into declarative rules that can be directly consumed by IMr framework.
- Mechanism: The LLM is prompted with a specific role definition, knowledge of supported declarative templates, and examples of both positive and negative cases. This allows it to generate syntactically correct and semantically meaningful constraints.
- Core assumption: LLMs can reliably map natural language to formal rule syntax when given clear task definitions and examples.
- Evidence anchors:
  - [abstract] "We use rules derived from LLMs to guide model construction, ensuring alignment with both domain knowledge and actual process executions."
  - [section] "We explain in our prompt the set of constraints we support, detailing both the syntax and the semantics of these constraints."
- Break condition: If the LLM generates syntactically incorrect rules or misinterprets the semantic meaning of the natural language input, the rules will fail validation or lead to incorrect process models.

### Mechanism 2
- Claim: Interactive refinement with LLMs improves the quality and precision of extracted rules through clarifying questions.
- Mechanism: The LLM is encouraged to ask questions when it encounters ambiguities in the process descriptions. Domain experts provide answers, which the LLM uses to refine and adjust the declarative constraints.
- Core assumption: LLMs can identify knowledge gaps and ambiguities in process descriptions and formulate meaningful questions to resolve them.
- Evidence anchors:
  - [section] "We facilitate a more detailed understanding of the provided textual descriptions by encouraging the LLM to express uncertainty and address it by asking questions."
- Break condition: If the LLM fails to identify relevant ambiguities or asks irrelevant questions, the refinement process will not improve the quality of the rules.

### Mechanism 3
- Claim: Rule validation ensures syntactic correctness and alignment with event log activities before integration into IMr.
- Mechanism: After rule extraction, the system checks that each constraint conforms to the predefined language syntax and that activity labels match those in the event log. If errors are found, an error-handling loop prompts the LLM to correct its output.
- Core assumption: Automated validation can catch errors in LLM-generated rules before they are used in process discovery.
- Evidence anchors:
  - [section] "The set of declarative constraints extracted after answering the questions is: not-co-existence(Block Claim 2, Block Claim 1), not-co-existence(Block Claim 2, Block Claim 3)..."
- Break condition: If the validation process is too strict and rejects valid rules, or too lenient and allows invalid rules, the framework's effectiveness will be compromised.

## Foundational Learning

- Concept: Declarative rules in process mining
  - Why needed here: The framework uses declarative rules (like Declare templates) to encode domain knowledge that guides the IMr process discovery algorithm.
  - Quick check question: What is the difference between a response(a,b) and precedence(a,b) declarative template?

- Concept: Inductive Mining (IMr) framework
  - Why needed here: The proposed approach builds upon the IMr framework, using LLM-extracted rules as input to guide model construction.
  - Quick check question: How does the IMr framework use rules to prune the search space during process discovery?

- Concept: Prompt engineering for LLMs
  - Why needed here: Effective task definition and examples are crucial for getting LLMs to generate correct declarative rules from natural language.
  - Quick check question: What are the key components of a good prompt for translating natural language to formal rules?

## Architecture Onboarding

- Component map: LLM Interface -> Rule Validator -> IMr Framework -> Interactive Loop
- Critical path:
  1. Domain expert provides process description
  2. LLM generates initial rules
  3. Rules are validated
  4. If validation fails, error handling loop with LLM
  5. Validated rules are used in IMr framework
  6. Process model is discovered and reviewed by domain expert
  7. Feedback is provided to LLM for refinement
- Design tradeoffs:
  - Accuracy vs. complexity: More detailed prompts and examples improve accuracy but increase complexity
  - Automation vs. human oversight: Fully automated rule extraction is faster but may miss domain-specific nuances
  - Strict vs. lenient validation: Strict validation ensures quality but may reject valid rules
- Failure signatures:
  - Incorrect process models: Likely due to invalid or misinterpreted rules
  - LLM generating non-answers or incorrect syntax: Indicates prompt engineering issues
  - Validation loop never terminating: Suggests fundamental mismatch between rules and event log
- First 3 experiments:
  1. Test LLM rule extraction with a simple, well-defined process description and verify syntactic correctness
  2. Validate rule extraction with an ambiguous process description and observe LLM's clarifying questions
  3. Run end-to-end test with a small event log and simple process description, comparing discovered model with and without LLM-extracted rules

## Open Questions the Paper Calls Out

- Question: How does the integration of domain knowledge through LLMs affect the accuracy and completeness of discovered process models compared to traditional process discovery methods?
  - Basis in paper: [explicit] The paper discusses the use of LLMs to integrate domain knowledge into process discovery and demonstrates its effectiveness through a case study.
  - Why unresolved: While the paper shows improvements, it does not provide a comprehensive comparison with other methods or quantify the impact on accuracy and completeness.
  - What evidence would resolve it: A detailed comparative analysis with other process discovery methods, including quantitative metrics on accuracy and completeness, would help resolve this question.

- Question: What are the limitations of using LLMs for translating natural language process descriptions into declarative rules, and how can these limitations be addressed?
  - Basis in paper: [inferred] The paper mentions that LLMs tend to provide confident answers without indicating uncertainty, and it discusses the challenges of adhering to strict expectations in rule extraction.
  - Why unresolved: The paper does not explore the full extent of these limitations or propose specific strategies to overcome them.
  - What evidence would resolve it: Research into the specific limitations of LLMs in this context, along with proposed solutions or improvements, would provide clarity.

- Question: How can the framework be extended to support a wider range of declarative templates and rule specification patterns beyond those currently used?
  - Basis in paper: [explicit] The paper mentions the flexibility of the framework to support other rule specification languages and discusses future work on expanding declarative templates.
  - Why unresolved: The paper does not detail the specific steps or challenges involved in extending the framework to support additional templates and patterns.
  - What evidence would resolve it: A roadmap or prototype demonstrating the integration of new declarative templates and patterns would help address this question.

## Limitations
- The approach's effectiveness may vary across different domains and process complexities
- Rule extraction quality heavily depends on prompt engineering and LLM model capabilities
- The validation process assumes syntactic correctness guarantees semantic validity

## Confidence
- Rule extraction quality: Medium confidence
- Framework integration: High confidence
- Generalizability across domains: Low confidence

## Next Checks
1. Test rule extraction accuracy across 5+ diverse process domains with varying complexity levels
2. Measure precision-recall tradeoff between fully automated rule extraction vs. expert-augmented approaches
3. Evaluate discovered models' performance on unseen event logs to test generalization capability