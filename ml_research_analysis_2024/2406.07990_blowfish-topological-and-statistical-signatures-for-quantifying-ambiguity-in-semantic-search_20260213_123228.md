---
ver: rpa2
title: 'Blowfish: Topological and statistical signatures for quantifying ambiguity
  in semantic search'
arxiv_id: '2406.07990'
source_url: https://arxiv.org/abs/2406.07990
tags:
- queries
- ambiguity
- embedding
- semantic
- topological
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a topological approach to detecting and quantifying
  ambiguity in sentence embeddings, with potential applications in vector search and
  Retrieval Augmented Generation (RAG) systems. The authors define ambiguity in sentences
  based on the number of semantic domains they relate to, and use topological data
  analysis (TDA) to extract features that distinguish ambiguous from unambiguous queries.
---

# Blowfish: Topological and statistical signatures for quantifying ambiguity in semantic search

## Quick Facts
- arXiv ID: 2406.07990
- Source URL: https://arxiv.org/abs/2406.07990
- Authors: Thomas Roland Barillot; Alex De Castro
- Reference count: 34
- One-line primary result: Introduces topological data analysis to detect ambiguity in sentence embeddings using persistent homology features (W1(H0) and LTmax(H1)).

## Executive Summary
This paper presents a novel approach to quantifying ambiguity in sentence embeddings using topological data analysis. The method leverages persistent homology to extract topological signatures from query neighborhoods in embedding space, distinguishing ambiguous queries (spanning multiple semantic domains) from unambiguous ones. The core insight is that ambiguous queries exhibit distinct topological patterns - higher Wasserstein distances between connected components and longer-lived loops in the Vietoris-Rips complex.

The authors demonstrate their approach on a proprietary dataset using three different embedding models, showing consistent topological signatures across models. The proposed "BLOWFISH toolbox" could be applied to improve vector search and Retrieval Augmented Generation systems by identifying and handling ambiguous queries appropriately.

## Method Summary
The method breaks a proprietary dataset into chunks of varying sizes (3, 5, and 10 lines), using these chunks as both queries and answers in a retrieval experiment. For each query, the system retrieves top-k nearest neighbors from the corpus, constructs a neighborhood, and computes topological features based on persistent homology. Specifically, it calculates the Wasserstein distance between connected components (W1(H0)) and the maximum loop lifetime in H1 (LTmax(H1)). These features are then compared between ambiguous queries (e.g., 10-line queries against 3-line documents) and unambiguous queries (e.g., 5-line queries against 10-line documents) to identify statistical signatures of ambiguity.

## Key Results
- Ambiguous queries exhibit significantly higher W1(H0) values compared to unambiguous queries
- Long-lived loops in H1 (LTmax(H1)) are more prevalent in ambiguous query neighborhoods
- Topological signatures remain consistent across different embedding model architectures (SBERT-multiqa, SBERT-msmarco, GTR-T5-XL)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ambiguous queries exhibit higher Wasserstein distance (W1(H0)) between connected components than unambiguous queries.
- Mechanism: The query neighborhood in embedding space contains multiple semantic clusters (topics) for ambiguous queries, leading to greater separation between connected components and thus higher W1(H0).
- Core assumption: The embedding space is sufficiently structured that multiple semantic domains create detectable topological features.
- Evidence anchors:
  - [abstract] "ambiguous queries (e.g., 10-line queries against 3-line documents) exhibit different distributions of W1(H0)"
  - [section] "ambiguous queries have higher W1(H0) values"
  - [corpus] Weak evidence - the proprietary dataset structure isn't fully described

### Mechanism 2
- Claim: Long-lived loops in H1 (LTmax(H1)) are more prevalent in ambiguous query neighborhoods.
- Mechanism: The presence of multiple semantic domains creates approximate discontinuities in the embedding manifold, forming persistent 1-dimensional loops in the Vietoris-Rips complex.
- Core assumption: Discontinuities between semantic domains manifest as topological loops that persist across scales.
- Evidence anchors:
  - [abstract] "longer-lived loops in H1" for ambiguous queries
  - [section] "presence of persistent loops could corresponds to punctures in the manifold"
  - [corpus] Weak evidence - no direct visualization of loops in the corpus data

### Mechanism 3
- Claim: Topological signatures are invariant across different embedding model architectures.
- Mechanism: The underlying structure of ambiguity in text is captured consistently regardless of the specific embedding model used.
- Core assumption: Different embedding models capture semantic relationships in a way that preserves ambiguity structure.
- Evidence anchors:
  - [abstract] "These topological signatures are consistent across different embedding models"
  - [section] "H0 and H1 features distributions across embedding models reinforces their alignment with the formal definition of ambiguity"
  - [corpus] No corpus evidence - claim is based on experimental observation across models

## Foundational Learning

- Concept: Persistent homology and Vietoris-Rips complexes
  - Why needed here: To compute topological features (W1(H0) and LTmax(H1)) from query neighborhood embeddings
  - Quick check question: What does H0 homology capture in persistent homology, and why is it relevant for detecting semantic clusters?

- Concept: Cosine similarity and nearest neighbor retrieval
  - Why needed here: To construct the query neighborhood by retrieving top-k similar documents
  - Quick check question: How does changing the value of k affect the topological features computed from the neighborhood?

- Concept: UMAP dimensionality reduction and HDBSCAN clustering
  - Why needed here: To approximate semantic domains (topics) from embeddings when ground truth labels are unavailable
  - Quick check question: Why might silhouette score be used to evaluate the quality of semantic domain clustering?

## Architecture Onboarding

- Component map: Embedding models → FAISS index → Query processing → Nearest neighbor retrieval → Topological feature extraction (W1(H0), LTmax(H1)) → Ambiguity classification
- Critical path: Query embedding → FAISS retrieval (top-k) → Neighborhood construction → Topological feature computation → Ambiguity scoring
- Design tradeoffs: Using smaller embedding models for feature computation vs. accuracy vs. computational cost; trade-off between k value and noise in topological features
- Failure signatures: Ambiguous queries classified as unambiguous (false negatives) when W1(H0) distributions overlap; incorrect topic clustering leading to wrong ambiguity labels
- First 3 experiments:
  1. Test W1(H0) distribution differences on a synthetic dataset with known semantic clusters
  2. Vary the scaling factor ε to observe how topological features change with neighborhood size
  3. Apply the method to a public dataset (e.g., Wikipedia) to validate on non-proprietary data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the topological signatures of ambiguity scale with increasing dataset size and complexity?
- Basis in paper: [inferred] The paper uses a proprietary dataset of 26 documents and explores ambiguity signatures with varying chunk sizes, but does not investigate how these signatures behave with larger or more diverse datasets.
- Why unresolved: The authors focus on a specific, controlled experiment with a limited dataset, which may not generalize to real-world scenarios where datasets are significantly larger and more varied.
- What evidence would resolve it: Experiments using larger, publicly available datasets (e.g., Wikipedia, news articles) with varying levels of semantic complexity and domain diversity.

### Open Question 2
- Question: Can the BLOWFISH toolbox be effectively applied to non-textual data modalities (e.g., images, audio) for ambiguity detection?
- Basis in paper: [inferred] The paper focuses exclusively on sentence embeddings and text-based ambiguity, but the underlying topological methods could theoretically be extended to other data types with appropriate embeddings.
- Why unresolved: The authors do not explore or discuss the applicability of their methods beyond text, leaving open the question of generalizability to other modalities.
- What evidence would resolve it: Applying the homology-based features and KDE methods to embeddings from image or audio models, and comparing ambiguity detection performance across modalities.

### Open Question 3
- Question: What is the computational complexity and scalability of the homology-based features for real-time ambiguity detection in large-scale systems?
- Basis in paper: [explicit] The authors propose a "BLOWFISH toolbox" for ambiguity detection but do not discuss computational efficiency, runtime, or memory requirements for large-scale deployments.
- Why unresolved: The paper presents theoretical and experimental results but lacks practical implementation details for production systems where latency and resource usage are critical.
- What evidence would resolve it: Benchmarking the homology computation and KDE steps on datasets of varying sizes, measuring execution time, memory usage, and scalability with increasing query volume.

## Limitations
- Use of proprietary dataset from BlackRock limits reproducibility and generalizability of results
- Exact hyperparameters for UMAP and HDBSCAN not fully specified, introducing variability
- Model invariance claim lacks theoretical justification and extensive validation across diverse embedding architectures

## Confidence
- **High confidence**: The fundamental premise that ambiguous queries have distinct topological signatures in their embedding neighborhoods
- **Medium confidence**: The invariance of topological signatures across different embedding models
- **Low confidence**: The robustness of the method on non-proprietary datasets

## Next Checks
1. **Synthetic Data Experiment**: Create a synthetic dataset with known semantic clusters and test whether the topological features (W1(H0) and LTmax(H1)) can reliably distinguish between ambiguous (queries spanning multiple clusters) and unambiguous (queries within a single cluster) cases.
2. **Public Dataset Validation**: Apply the method to a public dataset, such as Wikipedia or a subset of the Semantic Scholar corpus, to validate the approach on non-proprietary data and assess its generalizability.
3. **Hyperparameter Sensitivity Analysis**: Systematically vary the UMAP and HDBSCAN hyperparameters (e.g., n_neighbors, min_dist, min_cluster_size) to determine their impact on the topological features and ambiguity classification accuracy.