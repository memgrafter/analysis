---
ver: rpa2
title: 'HawkEye: Training Video-Text LLMs for Grounding Text in Videos'
arxiv_id: '2403.10228'
source_url: https://arxiv.org/abs/2403.10228
tags:
- video
- grounding
- temporal
- hawkeye
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HawkEye, a video-text LLM designed to address
  the challenge of grounding text queries in long and complicated videos. The core
  method involves constructing a large-scale video-text corpus (InternVid-G) with
  segment-level captions and negative spans, and introducing two new time-aware training
  objectives.
---

# HawkEye: Training Video-Text LLMs for Grounding Text in Videos

## Quick Facts
- arXiv ID: 2403.10228
- Source URL: https://arxiv.org/abs/2403.10228
- Reference count: 40
- Primary result: HawkEye achieves state-of-the-art performance on zero-shot temporal video grounding, significantly outperforming existing video-text LLMs

## Executive Summary
This paper addresses the challenge of grounding text queries in long and complicated videos by proposing HawkEye, a video-text LLM with novel training objectives and representation methods. The key innovation is a coarse-grained representation of video segments using categorical labels (beginning, middle, end, throughout) combined with recursive grounding for precise localization. The model is trained on a large-scale video-text corpus (InternVid-G) with segment-level captions and negative spans, enabling it to outperform existing approaches on temporal video grounding tasks while maintaining performance on other video-text tasks.

## Method Summary
HawkEye uses VideoChat2 stage 2 checkpoint as initialization and fine-tunes on a combination of instruction tuning data (VideoChat2-IT) and the InternVid-G dataset containing 715k segment-level captions from 83,614 YouTube videos. The method employs coarse-grained video segment representation (beginning/middle/end/throughout) instead of precise timestamps, which is more robust and easier for LLMs to learn. Recursive grounding enables precise localization through multiple rounds of interval refinement. The model uses 12 frames as input and implements two time-aware training objectives: temporal video grounding (multiple choice) and video segment captioning. Fine-tuning is performed on the Q-Former, query tokens, and LoRA parameters while freezing the visual encoder.

## Key Results
- Achieves state-of-the-art performance on zero-shot temporal video grounding
- Significantly outperforms baselines on Charades-STA and ActivityNet-Captions benchmarks
- Maintains comparable performance on other video-text tasks while excelling at temporal grounding
- Demonstrates effectiveness of coarse-grained representation and recursive grounding techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coarse-grained representation reduces cognitive load by replacing precise temporal coordinates with categorical labels
- Mechanism: Categorizing segments into four categories (beginning, middle, end, throughout) allows the LLM to determine approximate locations rather than precise timestamps, reducing complexity
- Core assumption: LLMs struggle with precise numerical reasoning and timestamp interpretation but can effectively reason about categorical spatial-temporal relationships
- Evidence anchors: Abstract mentions coarse-grained method is "more robust and easier for LLMs to learn"; section explains categorical relationships enable segment location inference

### Mechanism 2
- Claim: Recursive grounding enables precise localization through iterative refinement
- Mechanism: Model first identifies coarse category, then recursively narrows search interval by focusing on identified segment and resampling frames at higher rate
- Core assumption: Model can effectively narrow search interval with each iteration, and increased frame rate provides sufficient resolution
- Evidence anchors: Abstract mentions recursive grounding enables representation of shorter video segments through multiple rounds; section explains binary search-inspired refinement process

### Mechanism 3
- Claim: Random cropping prevents overfitting to specific query-answer pairs
- Mechanism: Randomly cropping video input within negative span forces model to rely on video content rather than memorizing query-answer relationships
- Core assumption: Model can identify relevant segment regardless of position within randomly cropped input
- Evidence anchors: Section explains cropping enables different answers for same query across epochs, preventing overfitting to shortcut relations

## Foundational Learning

- Concept: Temporal video grounding
  - Why needed here: Paper aims to improve LLM ability to localize specific segments in long-form videos based on textual queries
  - Quick check question: What is the primary difference between temporal video grounding and video question answering?

- Concept: Coarse-grained representation
  - Why needed here: Proposes using categorical labels (beginning, middle, end, throughout) instead of precise timestamps as LLMs struggle with numerical reasoning
  - Quick check question: How does coarse-grained representation reduce cognitive load compared to frame-level or second-level representation?

- Concept: Recursive grounding
  - Why needed here: Introduces recursive grounding to iteratively refine localization through multiple rounds of coarse-grained identification and resampling
  - Quick check question: How does recursive grounding leverage binary search idea to achieve precise localization?

## Architecture Onboarding

- Component map: Video frames → Video encoder (frozen) → Q-Former → LLM → Coarse-grained representation → Recursive grounding → Final output

- Critical path: Video frames processed through frozen visual encoder, transformed by Q-Former, interpreted by LLM with coarse-grained representation, refined through recursive grounding to produce final output

- Design tradeoffs: Precision vs. computational cost (more frames/rounds increase precision but also computation); coarse-grained vs. fine-grained representation (easier to learn but may sacrifice precision)

- Failure signatures: Model consistently misidentifies coarse category in early recursive grounding iterations; overfitting to specific query-answer pairs; struggles with numerical reasoning using frame-level representation

- First 3 experiments: 1) Evaluate performance of different video segment representation methods on temporal grounding benchmark; 2) Investigate impact of random cropping on generalization; 3) Analyze effectiveness of recursive grounding by varying rounds and measuring localization precision

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does HawkEye compare to other video-text LLMs on tasks beyond temporal grounding and video QA, such as video captioning or summarization?
- Basis in paper: [explicit] Paper mentions maintaining comparable performance on other video-text tasks but lacks specific results for captioning or summarization
- Why unresolved: Paper focuses on temporal grounding and video QA evaluation without detailed results for other video-text tasks
- What evidence would resolve it: Conduct experiments evaluating HawkEye on video captioning, summarization, and other video-text tasks compared to existing LLMs

### Open Question 2
- Question: How does coarse-grained representation perform when dealing with videos containing multiple overlapping or closely related events?
- Basis in paper: [inferred] Paper proposes coarse-grained representation but doesn't discuss handling videos with complex or overlapping events
- Why unresolved: Paper demonstrates coarse-grained effectiveness but lacks evidence for handling videos with multiple overlapping events
- What evidence would resolve it: Test HawkEye on videos with complex or overlapping events and evaluate segment identification accuracy

### Open Question 3
- Question: How does performance change with different numbers of input frames, and what is optimal number for balancing performance and efficiency?
- Basis in paper: [explicit] Paper mentions using 12 frames but doesn't explore impact of varying frame numbers on performance or efficiency
- Why unresolved: Paper provides frame count but doesn't investigate how varying frames affects performance or efficiency
- What evidence would resolve it: Conduct experiments evaluating performance and inference speed using different frame counts to determine optimal balance

## Limitations
- Corpus construction relies on CLIP-based similarity scores and heuristic merging rules with unclear sensitivity to construction choices
- Recursive grounding convergence properties not thoroughly analyzed, with potential for oscillation or failure to converge
- Limited evaluation on novel video domains with different characteristics from training distribution
- Weak corpus evidence for proposed mechanisms, with primary validation coming from experimental results

## Confidence

**High Confidence**: Coarse-grained representation improves LLM performance on temporal grounding tasks, well-supported by consistent experimental improvements over baselines

**Medium Confidence**: Recursive grounding effectiveness supported by experimental results, but robustness to initialization and convergence guarantees require further investigation

**Low Confidence**: Random cropping prevents overfitting claim lacks strong empirical validation, mentioned as benefit but without controlled isolation experiments

## Next Checks

1. **Convergence Analysis**: Vary recursive grounding rounds and analyze localization precision vs. computation trade-offs; measure consistency across multiple inference runs with different random seeds

2