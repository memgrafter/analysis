---
ver: rpa2
title: Rethinking Non-Negative Matrix Factorization with Implicit Neural Representations
arxiv_id: '2404.04439'
source_url: https://arxiv.org/abs/2404.04439
tags:
- representations
- neural
- in-nmf
- frequency
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an implicit neural representation (INR)-based
  framework for Non-negative Matrix Factorization (NMF) to enable factorization of
  irregularly-sampled time-frequency representations that cannot be directly stored
  in matrix form, such as Constant-Q transforms or sinusoidal models. The core idea
  is to replace matrix factors with learnable functions (implicit neural representations)
  that can be sampled at arbitrary points in time-frequency space.
---

# Rethinking Non-Negative Matrix Factorization with Implicit Neural Representations

## Quick Facts
- arXiv ID: 2404.04439
- Source URL: https://arxiv.org/abs/2404.04439
- Authors: Krishna Subramani; Paris Smaragdis; Takuya Higuchi; Mehrez Souden
- Reference count: 29
- Key outcome: Proposed iN-NMF method performs equivalently to standard NMF on regularly-sampled data (KL-divergence ~5×10⁻⁵) while generalizing to different window sizes without re-training

## Executive Summary
This paper proposes an implicit neural representation (INR)-based framework for Non-negative Matrix Factorization (NMF) to enable factorization of irregularly-sampled time-frequency representations that cannot be directly stored in matrix form. The core innovation replaces traditional matrix factors with learnable continuous functions that can be sampled at arbitrary points in time-frequency space. Experiments on the TIMIT dataset demonstrate that iN-NMF performs equivalently to standard NMF on regularly-sampled data while also generalizing to different window sizes without re-training.

## Method Summary
The method replaces the standard NMF matrix factors W and H with learnable continuous functions W(f; θ) and H(t; θ) that can be queried at arbitrary frequency and time coordinates. These functions are parameterized by neural networks with Fourier encodings, allowing the model to directly process irregularly-sampled time-frequency data without rasterization. The optimization minimizes KL-divergence between observed magnitudes and the sum of products of learned basis and activation functions using gradient descent.

## Key Results
- iN-NMF achieves KL-divergence values around 5×10⁻⁵ on TIMIT dataset, performing equivalently to standard NMF
- The method generalizes to different DFT window sizes (1000-2500 samples) without re-training, while standard NMF requires separate models for each size
- iN-NMF achieves comparable source separation performance (SDR, SIR, SAR, STOI metrics) to NMF across varying window sizes
- Successfully applied to hybrid time-frequency representations combining STFT, sinusoidal models, and CQT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing matrix factors with learnable implicit neural representations enables factorization of irregularly-sampled time-frequency data without rasterization.
- Mechanism: Standard NMF requires a matrix V with regularly sampled time-frequency bins. This work replaces the fixed matrix W and H with learnable functions W(f; θ) and H(t; θ) that can be queried at any arbitrary (f,t) coordinates, allowing direct processing of data that cannot be arranged in a regular grid.
- Core assumption: The underlying signal can be approximated by a sum of K continuous basis functions, each defined over the continuous frequency axis, and K activation functions over the continuous time axis.

### Mechanism 2
- Claim: Implicit neural representations with Fourier encodings allow generalization across different time-frequency resolutions without retraining.
- Mechanism: By learning continuous basis and activation functions instead of discrete matrices, the model can be queried at any resolution. Once trained on one resolution, the learned functions can be sampled at different resolutions without retraining.
- Core assumption: The learned continuous functions W(f; θ) and H(t; θ) generalize across different sampling rates and maintain their representational quality when resampled.

### Mechanism 3
- Claim: The KL-divergence based optimization with gradient descent can learn implicit neural representations that approximate the standard NMF solution for regularly-sampled data.
- Mechanism: The optimization problem minimizes KL-divergence between the observed magnitudes and the sum of products of the learned basis and activation functions. Gradient descent updates the neural network parameters to find functions that approximate the NMF decomposition.
- Core assumption: The optimization landscape for learning continuous basis and activation functions is sufficiently smooth and has good local minima that correspond to meaningful factorizations.

## Foundational Learning

- Concept: Non-negative Matrix Factorization (NMF)
  - Why needed here: The paper builds on NMF as the foundational technique and extends it from discrete matrices to continuous functions.
  - Quick check question: What is the objective function minimized in standard NMF and how does it relate to the divergence used in this paper?

- Concept: Implicit Neural Representations (INRs)
  - Why needed here: INRs are the key innovation that allows replacing discrete matrix factors with continuous learnable functions that can be sampled at arbitrary points.
  - Quick check question: How does the Fourier encoding in INRs help with learning high-frequency details compared to direct scalar inputs?

- Concept: Time-Frequency Representations
  - Why needed here: Understanding different T-F representations (STFT, CQT, sinusoidal models) is crucial for appreciating why irregularly-sampled data cannot be directly factorized with standard NMF.
  - Quick check question: What makes Constant-Q Transform different from Short-Time Fourier Transform in terms of sampling structure?

## Architecture Onboarding

- Component map:
  Input tuples (ti, fi, mi) -> Fourier positional encoding -> Basis networks Wk(fi) and activation networks Hk(ti) -> Sum of products -> KL-divergence loss

- Critical path:
  1. Parse input data into (ti, fi, mi) tuples
  2. Apply Fourier encoding to frequency and time values
  3. Pass encoded values through basis and activation networks
  4. Compute KL-divergence loss
  5. Backpropagate gradients and update network parameters
  6. Repeat until convergence

- Design tradeoffs:
  - Number of basis functions K vs. model complexity and overfitting
  - Network architecture (depth, width, activation functions) vs. representational power
  - Fourier encoding dimension J vs. frequency resolution
  - Learning rate and optimization strategy vs. convergence speed

- Failure signatures:
  - KL-divergence not decreasing: learning rate too high/low, poor initialization
  - Learned functions not capturing signal structure: insufficient network capacity
  - Poor generalization across resolutions: overfit to training resolution
  - Slow convergence: too many basis functions or complex architecture

- First 3 experiments:
  1. Implement the basic iN-NMF framework on regularly-sampled STFT data and verify KL-divergence matches standard NMF
  2. Test generalization by training on one DFT size and evaluating on different sizes without retraining
  3. Apply to irregularly-sampled CQT data and compare reconstruction quality to interpolated NMF baseline

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several emerge from the work:
- How well do learned iN-NMF bases generalize to completely unseen time-frequency representations beyond those used in training?
- What is the computational complexity trade-off between iN-NMF and traditional NMF in terms of training time and inference efficiency?
- How does the choice of implicit neural representation architecture affect the quality of factorization for different types of audio signals?

## Limitations
- The generalization claim across window sizes is demonstrated only on a single dataset (TIMIT) with magnitude spectrograms
- Computational complexity of replacing matrices with neural networks is not analyzed
- The choice of sinusoidal activation functions and Fourier encodings lacks systematic comparison with alternative INR architectures

## Confidence
- **High**: The core mechanism of replacing matrix factors with learnable continuous functions is technically sound and the mathematical formulation is rigorous
- **Medium**: The empirical demonstration on TIMIT shows competitive performance with standard NMF, but the sample size and diversity of test conditions could be expanded
- **Low**: Claims about applicability to arbitrary irregularly-sampled representations are supported by preliminary experiments but lack comprehensive validation across different data types

## Next Checks
1. Test generalization across diverse datasets beyond speech, including music and environmental audio, to verify the method's robustness to different signal characteristics
2. Conduct ablation studies comparing different INR architectures (SIREN vs. sinusoidal activations, varying Fourier encoding dimensions) to quantify the impact on reconstruction quality and generalization
3. Analyze computational complexity and runtime performance relative to standard NMF, particularly for real-time applications where query latency matters