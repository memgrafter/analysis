---
ver: rpa2
title: 'Vision-Language and Large Language Model Performance in Gastroenterology:
  GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models'
arxiv_id: '2409.00084'
source_url: https://arxiv.org/abs/2409.00084
tags:
- performance
- prompt
- question
- gastroenterology
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the medical reasoning performance of large
  language models (LLMs) and vision-language models (VLMs) on gastroenterology board
  exam questions. Proprietary models like GPT-4o (73.7%) and Claude3.5-Sonnet (74.0%)
  achieved the highest accuracy, outperforming open-source models like Llama3.1-405b
  (64%) and quantized models like Phi3-14b (48.7%).
---

# Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models

## Quick Facts
- arXiv ID: 2409.00084
- Source URL: https://arxiv.org/abs/2409.00084
- Reference count: 40
- Proprietary models (GPT-4o 73.7%, Claude3.5 74.0%) outperformed open-source models (Llama3.1-405b 64%) on gastroenterology board exam questions

## Executive Summary
This study evaluates the medical reasoning performance of large language models and vision-language models on gastroenterology board exam questions. Proprietary models like GPT-4o and Claude3.5-Sonnet achieved the highest accuracy (73.7-74.0%), while open-source models like Llama3.1-405b reached 64% accuracy. Quantized models showed moderate performance (Phi3-14b at 48.7%), demonstrating that quantization enables local deployment with acceptable accuracy loss. Vision-language models performed best when provided with human-crafted image descriptions rather than raw images or LLM-generated captions.

## Method Summary
The study evaluated 300 gastroenterology board exam-style multiple-choice questions from ACG self-assessments using various LLMs and VLMs across web interfaces, APIs, and local quantized models. Models were tested in zero-shot settings with optimized prompts and structured outputs where available. For image-based questions (138 total), models were evaluated with no image, LLM-generated captions, direct images, and human-crafted image descriptions. Performance was measured as accuracy on correct answers, with human validation used to resolve ambiguous responses.

## Key Results
- Proprietary models (GPT-4o 73.7%, Claude3.5-Sonnet 74.0%) achieved highest accuracy
- Open-source models (Llama3.1-405b 64%) performed moderately well
- Quantized models (Phi3-14b 48.7%) enabled local deployment with acceptable accuracy loss
- Human-crafted image descriptions improved VLM accuracy by 10% compared to raw images or LLM captions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured output formats significantly improve LLM performance on medical MCQs.
- Mechanism: Function calling or LangChain structured output injects explicit schema into prompts, forcing the model to produce deterministic, parseable results rather than free-form prose.
- Core assumption: The structured format reduces hallucination and ambiguity, improving answer precision.
- Evidence anchors:
  - [section]: "We found that structured output generation, when available, enhanced performance by 5 –10% (Figure 3.a and Supplementary Table S2)."
  - [section]: "The function call is the OpenAI and Claude3 tool, which can generate structure outputs via a schema consisting of a schema description, parameters for output, and parameter definitions."
- Break condition: If the schema is too complex or mismatched to the model's output capabilities, it may cause failure or degraded accuracy.

### Mechanism 2
- Claim: Quantization preserves most performance while enabling local deployment of large models.
- Mechanism: 8-bit quantization reduces memory usage from 4× parameters to ~1× parameters, making models like Llama2-7b runnable on consumer GPUs with minimal accuracy loss.
- Core assumption: Medical reasoning tasks are not extremely sensitive to the small precision loss introduced by quantization.
- Evidence anchors:
  - [section]: "The performance change was greatest for Llama3 -8b, which increased from 43.3% to 31% accuracy after 8-bit quantization."
  - [section]: "A comparison of the full -precision open-source models with their quantized counterparts revealed comparable results for 3 out of the 4 models."
- Break condition: If quantization level is too aggressive (e.g., 4-bit or extreme pruning), accuracy may degrade beyond usability thresholds.

### Mechanism 3
- Claim: Human-crafted image descriptions boost VLM performance far more than raw images or LLM-generated captions.
- Mechanism: Short, focused human hints capture essential visual information without overwhelming the model, whereas direct images or auto-generated captions introduce noise or irrelevant detail.
- Core assumption: VLMs struggle with extracting key medical findings from complex endoscopy/radiology images without guided context.
- Evidence anchors:
  - [abstract]: "VLMs did not improve performance when provided with images directly or with LLM-generated captions, but showed a 10% increase in accuracy with human-crafted image descriptions."
  - [section]: "In contrast, providing a one -sentence human hint that captured the core information from the image resulted in performance improvements for all the models."
- Break condition: If human hints become too detailed or reveal answers outright, they may introduce bias or reduce the generalizability of the evaluation.

## Foundational Learning

- Concept: Token-based pricing and memory estimation
  - Why needed here: To correctly budget for API costs and local deployment feasibility.
  - Quick check question: If a 300-token question plus options is processed with max_token=512, how many tokens are billed for input and output?

- Concept: Structured vs. unstructured LLM outputs
  - Why needed here: To decide whether to use function calling or rely on parsing free text.
  - Quick check question: What happens to the accuracy if you remove the structured schema and use raw prompt responses?

- Concept: Quantization levels and GGUF format
  - Why needed here: To select the right local model size and quantization for target hardware.
  - Quick check question: How much RAM does an 8-bit quantized 7B-parameter model require versus full-precision?

## Architecture Onboarding

- Component map:
  - Data pipeline: ACG SA MCQs → tokenization → prompt formatting → LLM call → response parsing → evaluation
  - Model layer: Web interfaces (GPT-4o, Claude3.5), APIs (OpenAI, Anthropic), local quantized models (GGUF)
  - Evaluation layer: Structured output parsing or GPT-3.5 extraction → human validation fallback

- Critical path:
  1. Prompt engineering (experiment 0) → optimal function call + prompt
  2. Batch evaluation of text questions via API or local model
  3. Image questions: compare no-image, LLM-caption, direct-image, human-hint scenarios
  4. Quantized local deployment for cost/control

- Design tradeoffs:
  - Accuracy vs. cost: GPT-4o highest accuracy but expensive; quantized Llama3-8b cheaper, slightly lower
  - Flexibility vs. privacy: Cloud APIs convenient but expose data; local quantized models private but require more setup
  - Structured output vs. parsing: Structured is cleaner but not universally supported; unstructured needs extra extraction step

- Failure signatures:
  - Inconsistent outputs across runs → increase temperature to 0 or fix random seed
  - Function call schema mismatch → revert to unstructured output
  - Quantized model crashes → reduce number of layers offloaded to GPU or use lighter quantization

- First 3 experiments:
  1. Run a small batch of text questions through GPT-4o API with structured output to verify end-to-end pipeline
  2. Compare one open-source quantized model (e.g., Llama3-8bQ8) against GPT-4o on a subset to measure performance drop
  3. Test image scenario with one VLM: no image vs. human hint vs. LLM caption to observe accuracy delta

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does model fine-tuning consistently mitigate performance degradation during quantization across different medical domains?
- Basis in paper: [explicit] The paper notes that fine-tuned versions of Llama2-7b (medicineLLM) and Llama3-8b (OpenBioLLM) did not show improved or comparable performance when run in their 8-bit quantized forms compared to their source models.
- Why unresolved: The study only tested two fine-tuned models and did not explore other fine-tuning approaches or medical domains. The generalizability of these findings to other models and tasks is unknown.
- What evidence would resolve it: Systematic evaluation of multiple fine-tuned models across various medical specialties and quantization levels, comparing performance to both full-precision and non-fine-tuned quantized models.

### Open Question 2
- Question: What is the optimal balance between model size and quantization level for achieving the best performance-to-resource ratio in medical applications?
- Basis in paper: [inferred] The paper compares performance of different quantized models (Phi3-14b-Q6 at 48.7% vs Llama3-8b-Q8 at 31%) and discusses the trade-offs between model size and quantization, but does not provide a systematic analysis of optimal combinations.
- Why unresolved: The study only examined a limited set of model sizes and quantization levels. The relationship between these factors and performance across different medical tasks remains unclear.
- What evidence would resolve it: Comprehensive benchmarking of multiple model sizes and quantization levels across diverse medical tasks, measuring both performance and resource requirements to identify optimal configurations.

### Open Question 3
- Question: How can consistency in LLM outputs be improved without sacrificing accuracy in medical applications?
- Basis in paper: [explicit] The paper found that while setting a fixed random seed and lower temperatures led to more consistent outputs, this did not necessarily improve accuracy. The relationship between consistency and accuracy remains unclear.
- Why unresolved: The study only explored basic consistency measures (seed, temperature) and did not investigate more sophisticated approaches to balancing consistency and accuracy.
- What evidence would resolve it: Development and evaluation of advanced consistency techniques (e.g., ensemble methods, calibration approaches) specifically for medical applications, measuring both consistency and accuracy across diverse medical tasks.

## Limitations
- Restricted access to the 300 ACG gastroenterology board exam questions prevents independent verification of core results
- Performance gaps between proprietary and open-source models may reflect prompt optimization differences rather than true capability differences
- VLM findings based on single image-enhanced question set haven't been validated across different medical imaging domains

## Confidence

**High Confidence**: The comparative ranking of models (GPT-4o > Claude3.5 > Llama3.1-405b > quantized models) and the observation that quantization causes modest accuracy degradation (3-12% drops) are well-supported by multiple evaluation runs and consistent with established LLM scaling laws.

**Medium Confidence**: The claim that structured output improves accuracy by 5-10% is supported but limited to models where function calling was available. The mechanism is plausible but could benefit from ablation studies comparing structured vs unstructured outputs within the same model family.

**Low Confidence**: The VLM-specific findings (no improvement with raw images or LLM-generated captions, but 10% gain with human hints) are based on a single image-enhanced question set and haven't been validated across different medical imaging domains or hint styles.

## Next Checks

1. **Cross-domain validation**: Test whether human-crafted image descriptions improve VLM performance on radiology or pathology images from other medical specialties, or whether the 10% gain is specific to gastroenterology endoscopy images.

2. **Prompt parity analysis**: Implement identical prompt structures across all model families (including proprietary ones) and measure whether the performance gap between GPT-4o and Llama3.1 narrows when using the same prompt optimization techniques.

3. **Quantization sensitivity mapping**: Systematically vary quantization levels (Q4_K_M, Q5_K_M, Q6_K, Q8_0) for each model and map accuracy degradation curves to identify optimal trade-offs between memory usage and medical reasoning performance.