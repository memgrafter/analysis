---
ver: rpa2
title: 'Large Language Models for Biomedical Text Simplification: Promising But Not
  There Yet'
arxiv_id: '2408.03871'
source_url: https://arxiv.org/abs/2408.03871
tags:
- language
- biomedical
- evaluation
- https
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates large language models for biomedical text
  simplification using the PLABA2023 dataset. The authors fine-tune various LLMs including
  T5, SciFive, BART, BioGPT, and GPT-4, along with control token mechanisms and prompt-based
  learning.
---

# Large Language Models for Biomedical Text Simplification: Promising But Not There Yet

## Quick Facts
- arXiv ID: 2408.03871
- Source URL: https://arxiv.org/abs/2408.03871
- Reference count: 40
- Key outcome: LLMs show promise for biomedical text simplification but still struggle with balancing simplification and meaning preservation

## Executive Summary
This paper investigates large language models for biomedical text simplification using the PLABA2023 dataset. The authors fine-tune various LLMs including T5, SciFive, BART, BioGPT, and GPT-4, along with control token mechanisms and prompt-based learning. While BART-large with control tokens achieves the highest SARI score of 46.54 and T5-base achieves the highest BERTScore of 72.62, human evaluations reveal that the models still struggle with balancing simplification quality with meaning preservation. The work demonstrates that while LLMs show promise for biomedical text simplification, they still have limitations in this challenging domain.

## Method Summary
The authors fine-tune various LLMs including T5, SciFive, BART, BioGPT, and GPT-4 on the PLABA dataset, along with control token mechanisms and prompt-based learning. They apply control tokens to optimize simplification by adjusting sentence complexity, word rank, and length ratio. The models are evaluated using automatic metrics (SARI, BERTScore, BLEU, ROUGE) and human evaluation on simplicity, accuracy, fluency, completeness, and faithfulness.

## Key Results
- BART-large with control tokens achieves the highest SARI score of 46.54
- T5-base achieves the highest BERTScore of 72.62
- In human evaluations, BART-w-CTs ranks 2nd on sentence-simplicity (92.84) and 3rd on term-simplicity (82.33)
- ChatGPT-prompting in round 2 achieves 2nd place in several categories including simplified term accuracy (92.26) and completeness (96.58)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning large encoder-decoder models on biomedical text simplifies domain-specific terminology while preserving meaning.
- Mechanism: Domain-specific pre-training (e.g., SciFive) and fine-tuning align model vocabulary with biomedical jargon, enabling better substitution with lay language equivalents.
- Core assumption: The model's pre-trained vocabulary and attention mechanisms can effectively map complex biomedical terms to simpler synonyms or paraphrases during decoding.
- Evidence anchors:
  - [abstract] "fine-tune various LLMs including T5, SciFive, BART, BioGPT, and GPT-4, along with control token mechanisms and prompt-based learning"
  - [section] "Using the publicly available PLABA (Plain Language Adaptation of Biomedical Abstracts) data set from [9], we demonstrate the capabilities of such models"
- Break condition: If biomedical terminology is too domain-specific or context-dependent for the model's vocabulary to reliably map to lay terms.

### Mechanism 2
- Claim: Control tokens optimize simplification by adjusting sentence complexity, word rank, and length ratio.
- Mechanism: Control tokens like <DEPENDENCYTREEDEPTH_x>, <WORDRANK_x>, and <LENGTHRATIO_x> constrain the decoding process to balance simplification depth with information preservation.
- Core assumption: The model's decoder can interpret and apply these control signals effectively to modify output complexity without losing key information.
- Evidence anchors:
  - [section] "We applied the modified control token strategy in [8] for both BART-base and BART-large models. The training includes 2 stages, leveraging both Wikilarge training set [47] and our split of training set from PLABA [48]."
  - [section] "BART-Large with Control Token (BART-L-w-CT) mechanisms reported the highest SARI score of 46.54"
- Break condition: If control token optimization leads to over-simplification, causing loss of critical biomedical details.

### Mechanism 3
- Claim: Prompt-based learning with GPT models enables zero-shot simplification without extensive fine-tuning.
- Mechanism: Carefully crafted prompts guide GPT-3.5 and GPT-4 to rephrase biomedical abstracts into simpler language while maintaining core content.
- Core assumption: GPT models' instruction-following capabilities and large pre-training corpora include sufficient biomedical context to perform meaningful simplification.
- Evidence anchors:
  - [abstract] "ChatGPT-prompting in round 2 achieves 2nd place in several categories including simplified term accuracy (92.26) and completeness (96.58)"
  - [section] "We applied simplifications using 'GPT-3.5-turbo' and GPT-4 via its API 6. Example prompts we used can be found in Figure 9"
- Break condition: If GPT models lack sufficient biomedical domain knowledge, leading to inaccurate term replacements or missing critical details.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how encoder-decoder models like T5 and BART process input text and generate simplified output is crucial for interpreting model behavior and fine-tuning strategies.
  - Quick check question: How do self-attention and cross-attention layers in a transformer-based model contribute to text simplification?

- Concept: Biomedical terminology and health literacy
  - Why needed here: Recognizing the complexity of biomedical language and the importance of health literacy helps in evaluating simplification quality and identifying potential information loss.
  - Quick check question: Why is preserving critical biomedical information crucial when simplifying abstracts for public health literacy?

- Concept: Automatic evaluation metrics (SARI, BERTScore, BLEU, ROUGE)
  - Why needed here: Interpreting quantitative results requires understanding how these metrics assess simplification quality, meaning preservation, and n-gram overlap.
  - Quick check question: How does SARI differ from BLEU in evaluating text simplification, and why might it be more appropriate for this task?

## Architecture Onboarding

- Component map: Biomedical abstracts -> Pre-processing (tokenization, control token calculation) -> Models (T5, SciFive, BART-w-CTs, GPT-3.5/4) -> Evaluation (automatic metrics, human evaluation) -> Simplified biomedical abstracts
- Critical path: Pre-process data -> Fine-tune/optimize models -> Generate simplified outputs -> Evaluate using automatic and human metrics
- Design tradeoffs:
  - Model size vs. computational efficiency (using LoRA for large models)
  - Control token optimization vs. information preservation
  - Zero-shot prompting vs. fine-tuning for GPT models
- Failure signatures:
  - Low SARI scores indicating poor simplification quality
  - High BERTScore but low SARI suggesting meaning preservation at the cost of simplification
  - Human evaluation scores showing low simplicity despite high automatic metrics
- First 3 experiments:
  1. Fine-tune T5-base on PLABA dataset and evaluate using SARI and BERTScore
  2. Apply control tokens to BART-large and compare performance to baseline BART
  3. Test GPT-3.5 with different prompts on a subset of PLABA data and evaluate simplification quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop an automatic evaluation metric that effectively balances both text simplicity and meaning preservation for biomedical text simplification?
- Basis in paper: [explicit] The paper explicitly states that developing a new automatic metric that can effectively reflect both text simplicity and meaning preservation would be very useful.
- Why unresolved: Current metrics like SARI, BLEU, ROUGE, and BERTScore each have limitations - SARI may assign lower scores when outputs closely resemble inputs, while BERTScore focuses on meaning preservation but may still yield high scores for poor simplification quality.
- What evidence would resolve it: Development and validation of a new evaluation metric that correlates well with human judgments of both simplicity and meaning preservation across diverse biomedical text simplification tasks.

### Open Question 2
- Question: What are the optimal control token values for biomedical text simplification across different sentence complexities and terminology?
- Basis in paper: [explicit] The paper uses Nevergrad to find optimal control token values but notes this is done on validation sets and may not generalize to all types of biomedical text.
- Why unresolved: The optimal control token values likely vary depending on the specific biomedical domain, sentence structure complexity, and terminology used. The current approach finds static optimal values that may not adapt well to diverse inputs.
- What evidence would resolve it: Systematic experiments testing different control token value ranges across various biomedical subdomains and sentence types, potentially leading to adaptive control token mechanisms.

### Open Question 3
- Question: How can we standardize human evaluation procedures for biomedical text simplification to improve inter-annotator agreement and reliability?
- Basis in paper: [explicit] The paper identifies significant inter-annotator agreement issues, with Cohen's Kappa scores ranging from 0.008 to 0.583 and Krippendorff's alpha scores from -0.285 to 0.449, indicating inconsistent human judgments.
- Why unresolved: The paper highlights challenges including annotator background differences, lack of standardized training materials, and difficulty in defining Likert scale options for evaluating both meaning preservation and simplicity simultaneously.
- What evidence would resolve it: Development of standardized annotation guidelines, training protocols, and potentially new evaluation frameworks that account for both semantic preservation and simplification quality while minimizing subjective interpretation differences.

## Limitations

- Dataset representativeness and scale: The PLABA2023 dataset contains only 750 abstracts, which may not capture the full diversity of biomedical terminology and writing styles.
- Evaluation metric limitations: Current automatic metrics have known limitations in capturing semantic equivalence and simplification quality.
- Control token optimization complexity: The control token mechanism introduces multiple hyperparameters that require careful tuning, with insufficient sensitivity analysis provided.

## Confidence

**High confidence**: The observation that LLMs show promise for biomedical text simplification is well-supported by the results.

**Medium confidence**: The conclusion that "large language models for biomedical text simplification are promising but not there yet" is reasonable given the results, though the specific threshold for "there yet" is subjective.

**Low confidence**: Claims about specific mechanisms (e.g., exactly how control tokens improve simplification, why certain models outperform others) are based on observed correlations rather than mechanistic understanding.

## Next Checks

1. **Ablation study on control tokens**: Systematically remove each control token type (sentence depth, word rank, length ratio) from BART-w-CTs to quantify their individual contributions to performance improvements.

2. **Cross-dataset generalization test**: Evaluate the best-performing models on an independent biomedical text simplification dataset to assess whether the PLABA2023 training provides generalizable capabilities or overfits to this specific dataset's characteristics.

3. **Human evaluation correlation analysis**: Conduct detailed correlation analysis between automatic metrics and human evaluation scores across all model variants to identify which automatic metrics best predict human-perceived simplification quality.