---
ver: rpa2
title: LLMs learn governing principles of dynamical systems, revealing an in-context
  neural scaling law
arxiv_id: '2402.00795'
source_url: https://arxiv.org/abs/2402.00795
tags:
- time
- figure
- loss
- in-context
- transition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the ability of large language models (LLMs)
  to learn the underlying dynamics of various stochastic and deterministic dynamical
  systems without fine-tuning or prompt engineering. The authors propose a method
  called Hierarchy-PDF to extract learned transition rules from LLM outputs and quantify
  the discrepancy between these learned rules and ground truth using Bhattacharyya
  distance for stochastic systems and squared deviations from the mean for deterministic
  systems.
---

# LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law

## Quick Facts
- arXiv ID: 2402.00795
- Source URL: https://arxiv.org/abs/2402.00795
- Reference count: 34
- LLMs can learn governing principles of dynamical systems without fine-tuning or prompt engineering

## Executive Summary
This paper investigates the ability of large language models (LLMs) to learn the underlying dynamics of various stochastic and deterministic dynamical systems without fine-tuning or prompt engineering. The authors propose a method called Hierarchy-PDF to extract learned transition rules from LLM outputs and quantify the discrepancy between these learned rules and ground truth using Bhattacharyya distance for stochastic systems and squared deviations from the mean for deterministic systems. Experiments on multiple dynamical systems, including Markov chains, noisy logistic maps, and continuous-time processes, demonstrate that LLMs can accurately learn transition rules and that the accuracy improves with increasing input context length, revealing an in-context neural scaling law.

## Method Summary
The authors use LLaMA-2 models to predict time series data from dynamical systems, then apply a Hierarchy-PDF algorithm to extract probability density functions of multi-digit numbers directly from LLM outputs. They compare the learned transition rules to ground truth using Bhattacharyya distance for stochastic systems and squared deviations from the mean (SDM) for deterministic systems. The method involves tokenizing and serializing dynamical system time series as strings, prompting the LLM, extracting learned probability densities, and measuring discrepancies between ground truth and learned transition rules.

## Key Results
- LLMs can accurately recover probabilistic transition rules underlying deterministic, chaotic, and stochastic time series
- The accuracy of learned physical rules increases with the length of the input context window
- The method demonstrates effectiveness across Markov chains, noisy logistic maps, and continuous-time processes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs learn transition rules governing dynamical systems by progressively refining probability distributions through hierarchical softmax.
- Mechanism: The LLM outputs hierarchical softmax distributions where each digit prediction refines the previous one, allowing extraction of multi-digit number PDFs through recursive refinement.
- Core assumption: The LLM's internal representations capture sufficient statistical structure of the dynamical system to enable accurate digit-by-digit prediction.
- Evidence anchors:
  - [abstract]: "We present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs."
  - [section]: "the LLM's softmax prediction for the ith digit, ui, provides a histogram of ten bins of width 0.1i"
- Break condition: If the LLM fails to capture higher-order statistical dependencies in the time series, the hierarchical refinement would not converge to accurate transition rules.

### Mechanism 2
- Claim: LLMs exhibit an in-context neural scaling law where accuracy improves with context length without weight updates.
- Mechanism: As more time steps are observed in context, the LLM's predictions of transition probabilities converge toward ground truth, following a power-law relationship.
- Core assumption: The LLM's architecture enables effective in-context learning of statistical patterns without explicit training on the dynamical system.
- Evidence anchors:
  - [abstract]: "the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of a neural scaling law"
  - [section]: "LLMs can accurately recover the probablistic transition rules underlying determinisitic, chaotic, and stochastic time series"
- Break condition: If the context window is too short or the dynamical system is too complex, the scaling law would break down.

### Mechanism 3
- Claim: The Bhattacharyya distance and SDM provide effective metrics for comparing learned vs ground truth transition rules.
- Mechanism: These distance metrics quantify the discrepancy between LLM-predicted probability distributions and true transition functions.
- Core assumption: The extracted hierarchical PDFs from LLMs can be meaningfully compared to ground truth distributions using standard statistical distances.
- Evidence anchors:
  - [section]: "Measure the discrepancy, between the ground truth Pij and learned ˜Pij, using Bhattacharyya distance"
  - [section]: "the squared deviations from the mean (SDM) for deterministic systems"
- Break condition: If the LLM's predictions become too peaked or the ground truth distributions are too complex, these metrics may become unstable or insensitive.

## Foundational Learning

- Concept: Markov processes and transition matrices
  - Why needed here: Understanding that the next state depends only on the current state is fundamental to modeling dynamical systems
  - Quick check question: What property of Markov chains allows us to model time series as sequential state transitions?

- Concept: Probability density functions and hierarchical softmax
  - Why needed here: Extracting multi-digit number predictions requires understanding how LLMs output hierarchical probability distributions
  - Quick check question: How does hierarchical softmax enable resolution of multi-digit numbers through sequential digit predictions?

- Concept: Scaling laws in neural networks
  - Why needed here: The in-context scaling law is analogous to traditional training scaling laws, understanding this framework is crucial
  - Quick check question: What parameters typically govern neural scaling laws, and how does in-context learning differ?

## Architecture Onboarding

- Component map: LLM (LLaMA-2) → Tokenization → Hierarchy-PDF extraction → Distance metric computation
- Critical path: Time series → LLM prediction → Hierarchical PDF refinement → Transition rule extraction → Accuracy evaluation
- Design tradeoffs: Tokenization precision vs context window size; Refinement depth vs computational cost; Temperature vs prediction variance
- Failure signatures: Early plateauing of loss curves (suggesting out-of-distribution data); Large fluctuations in deterministic system losses; Inability to handle continuous state spaces
- First 3 experiments:
  1. Test discrete Markov chain with 4 states to verify basic extraction pipeline
  2. Test noisy logistic map with varying context lengths to observe scaling behavior
  3. Compare Bhattacharyya distance vs SDM for deterministic vs stochastic systems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training modifications could enhance LLMs' ability to learn and predict complex dynamical systems with longer context windows?
- Basis in paper: [explicit] The paper demonstrates that LLMs can learn governing principles of dynamical systems, but also notes that larger models like LLaMA-70b do not show qualitative differences in learning compared to smaller models like LLaMA-13b.
- Why unresolved: The study uses existing LLaMA models without modifications, so it doesn't explore how architectural changes might improve learning.
- What evidence would resolve it: Experiments comparing LLMs with modified architectures or training regimes against standard models on the same dynamical systems tasks.

### Open Question 2
- Question: How do LLMs encode and represent the learned transition rules of dynamical systems internally, and can this representation be decoded to reveal the underlying mechanisms?
- Basis in paper: [inferred] The paper introduces the Hierarchy-PDF algorithm to extract learned transition rules, suggesting that the internal representation is complex and hierarchical.
- Why unresolved: The paper extracts transition rules but doesn't investigate how they are encoded within the model's internal states.
- What evidence would resolve it: Analysis of attention patterns or intermediate representations in LLMs during dynamical system prediction tasks.

### Open Question 3
- Question: Can the in-context learning ability of LLMs for dynamical systems be extended to non-Markovian processes or systems with long-range dependencies?
- Basis in paper: [explicit] The paper focuses on Markovian processes and notes that Takens' embedding theorem allows prediction of deterministic systems by expanding the state vector.
- Why unresolved: The study only tests Markovian systems, leaving open the question of performance on non-Markovian processes.
- What evidence would resolve it: Experiments testing LLMs on non-Markovian dynamical systems or systems requiring memory of multiple past states beyond Markovian assumptions.

## Limitations

- The depth of "understanding" demonstrated by LLMs remains unclear - whether this represents true comprehension of governing principles versus pattern memorization
- The method's effectiveness for continuous-state systems versus discrete-state systems shows variability that isn't fully characterized
- The in-context neural scaling law may be contingent on specific model architecture (LLaMA-2) and may not generalize across different LLM architectures

## Confidence

**High Confidence:**
- LLMs can extract transition rules from dynamical systems through in-context learning
- The hierarchical softmax approach enables multi-digit number prediction with increasing precision
- Bhattacharyya distance and SDM are valid metrics for comparing learned vs ground truth transition rules

**Medium Confidence:**
- The existence of an in-context neural scaling law relating context length to prediction accuracy
- The generalizability of the method across different types of dynamical systems (stochastic vs deterministic)
- The robustness of the extraction method for continuous-state systems

**Low Confidence:**
- The depth of "understanding" demonstrated by LLMs - whether this represents true comprehension of governing principles
- The method's performance on highly complex, high-dimensional dynamical systems
- The stability of metrics when dealing with highly peaked distributions or delta functions

## Next Checks

1. **Cross-Architecture Validation**: Test the Hierarchy-PDF extraction method across multiple LLM architectures (GPT, BERT variants, etc.) to verify whether the observed scaling law and learning capabilities are architecture-dependent or generalizable properties of LLMs.

2. **Extrapolation Capacity Test**: Design experiments where LLMs are exposed to partial trajectories of dynamical systems and evaluate their ability to predict long-term behavior or system evolution beyond the observed context, distinguishing between pattern matching and genuine principle learning.

3. **Complexity Boundary Analysis**: Systematically vary the complexity of dynamical systems (increasing state space dimensionality, nonlinearity, chaoticity) to identify the boundaries where the extraction method fails or the scaling law breaks down, providing insight into the method's practical limitations.