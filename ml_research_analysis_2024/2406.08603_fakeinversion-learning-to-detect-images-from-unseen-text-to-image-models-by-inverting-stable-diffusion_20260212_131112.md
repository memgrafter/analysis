---
ver: rpa2
title: 'FakeInversion: Learning to Detect Images from Unseen Text-to-Image Models
  by Inverting Stable Diffusion'
arxiv_id: '2406.08603'
source_url: https://arxiv.org/abs/2406.08603
tags:
- images
- image
- fake
- evaluation
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FakeInversion, a novel method for detecting
  images generated by unseen text-to-image models. The core idea is to leverage features
  extracted from a pre-trained Stable Diffusion model, specifically inverted latent
  noise maps and reconstructed images, to train a detector that generalizes well to
  higher-fidelity models like DALL-E 3 and Imagen.
---

# FakeInversion: Learning to Detect Images from Unseen Text-to-Image Models by Inverting Stable Diffusion

## Quick Facts
- **arXiv ID**: 2406.08603
- **Source URL**: https://arxiv.org/abs/2406.08603
- **Reference count**: 40
- **Primary result**: Introduces FakeInversion, a method that uses features from inverting Stable Diffusion to detect images from unseen high-fidelity text-to-image models like DALL-E 3 and Imagen.

## Executive Summary
This paper introduces FakeInversion, a novel approach for detecting images generated by unseen text-to-image models. The method leverages features extracted from a pre-trained Stable Diffusion model, specifically inverted latent noise maps and reconstructed images, to train a detector that generalizes well to higher-fidelity models. The authors also propose a new evaluation protocol, SynRIS, which uses reverse image search to mitigate stylistic and thematic biases in the real image sets. This ensures a more reliable assessment of the detector's ability to identify fake images from various models. FakeInversion achieves state-of-the-art performance across multiple training and evaluation setups, demonstrating significant improvements over existing methods.

## Method Summary
The core of FakeInversion is to use features obtained by inverting a pre-trained Stable Diffusion model to detect images from unseen text-to-image models. The method takes as input the original image, an approximate noise map recovered via text-conditioned DDIM inversion with Stable Diffusion, and a reconstruction obtained by denoising the approximate noise map. These three components are concatenated and fed into a ResNet-50 classifier. The inversion process is guided by text captions generated by BLIP-2 and embedded using CLIP. The method is trained on fake images from Stable Diffusion v1.5 and real images from LAION, and evaluated on fake images from various models (DALL-E 3, Imagen, Midjourney, etc.) and real images found via reverse image search (RIS).

## Key Results
- FakeInversion achieves state-of-the-art performance in detecting images from unseen high-fidelity text-to-image models like DALL-E 3 and Imagen.
- The proposed evaluation protocol, SynRIS, provides a more reliable assessment by mitigating stylistic and thematic biases in the real image sets.
- The method generalizes well across multiple training and evaluation setups, demonstrating significant improvements over existing methods.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Text-conditioned DDIM inversion features improve generalization because they encode a "likelihood estimate" under the SD model that differs systematically between real and fake images.
- **Mechanism**: The inversion process maps an image to a noise latent that, when decoded, reconstructs the image. The reconstruction error and the intermediate noise map together approximate the likelihood of the image under the SD model. This likelihood signal is preserved across different generators, making it useful for detection.
- **Core assumption**: Different text-to-image models overgeneralize in similar ways, producing samples with lower likelihood under any diffusion model.
- **Evidence anchors**:
  - [abstract]: "we propose a new synthetic image detector that uses features obtained by inverting an open-source pre-trained Stable Diffusion model."
  - [section]: "If we view the forward DDIM mapping Fθ as an approximation of that true bijective mapping... it can be shown (see App. B.1) that, in the first-order approximation, the log-likelihood of the data given that underlying model can be estimated from the input image z0, its imperfect reconstruction ˆz0, and the noise map zT alone."
  - [corpus]: Weak/no evidence. No cited papers discuss likelihood estimation via DDIM inversion for detection.
- **Break condition**: If the generative models are trained on fundamentally different data distributions or architectures that produce high likelihood under SD, the likelihood signal may not generalize.

### Mechanism 2
- **Claim**: The proposed evaluation protocol (SynRIS) is harder and more reliable because it ensures real and fake images are thematically and stylistically aligned, preventing detectors from relying on superficial style differences.
- **Mechanism**: By using reverse image search to find real images that match the content and style of fake images, the evaluation set removes biases towards particular themes or styles, forcing the detector to focus on intrinsic generative artifacts.
- **Core assumption**: Existing evaluation protocols (e.g., LAION-based) create easy detection tasks by pairing fake images with real images from very different distributions.
- **Evidence anchors**:
  - [abstract]: "we introduce a new challenging evaluation protocol that uses reverse image search to mitigate stylistic and thematic biases in the detector evaluation."
  - [section]: "To ensure that detectors are not biased toward any particular theme or style, we need sets of real and fake images that are themselves stylistically and thematically aligned."
  - [corpus]: Weak/no evidence. No cited papers discuss using reverse image search for evaluation protocol design.
- **Break condition**: If reverse image search fails to find sufficiently similar real images, or if the matching process introduces its own biases.

### Mechanism 3
- **Claim**: The combination of the original image, its DDIM reconstruction, and the inverted noise map provides a richer feature set than using the original image alone, leading to better generalization.
- **Mechanism**: Each component captures different aspects of the image: the original image has low-level artifacts, the reconstruction reveals model-specific reconstruction errors, and the noise map encodes the model's internal representation of the image.
- **Core assumption**: The three signals together capture more discriminative information than any single signal.
- **Evidence anchors**:
  - [abstract]: "our model takes as input 1) the original image, 2) the approximate noise map recovered via text-conditioned DDIM [52] inversion with Stable Diffusion (SD), and 3) the reconstruction obtained by 'denoising' the approximate noise map."
  - [section]: "Table 4 shows that both RGB and reconstruction residual-based models perform significantly worse than the proposed method that uses both the input image, its reconstruction, and the inversion map."
  - [corpus]: Weak/no evidence. No cited papers discuss combining these three signals for detection.
- **Break condition**: If the additional signals do not provide discriminative information beyond the original image, or if the model overfits to the training set.

## Foundational Learning

- **Concept**: Diffusion Models and DDIM Inversion
  - **Why needed here**: The core of the method relies on extracting features from a pre-trained diffusion model via DDIM inversion. Understanding how diffusion models work and how DDIM inversion operates is crucial for implementing and debugging the method.
  - **Quick check question**: What is the difference between DDIM sampling and standard diffusion sampling, and how does DDIM enable inversion?

- **Concept**: Text Conditioning and CLIP Embeddings
  - **Why needed here**: The method uses text captions and CLIP embeddings to condition the inversion process. Understanding how CLIP works and how text conditioning affects diffusion models is essential for correctly implementing the pipeline.
  - **Quick check question**: How does CLIP embed text, and how is this embedding used to condition the diffusion model during inversion?

- **Concept**: Feature Extraction and Classification
  - **Why needed here**: The method extracts features from the inversion process and uses them as input to a classifier. Understanding how to extract and combine these features, and how to train a classifier on them, is necessary for implementing the detection model.
  - **Quick check question**: How are the original image, reconstruction, and noise map combined as input to the classifier, and what type of classifier is used?

## Architecture Onboarding

- **Component map**: Image → BLIP Captioner → CLIP Embedder → SD Encoder → DDIM Inversion → DDIM Reconstruction → SD Decoder → Feature Concatenation → Classifier (ResNet50)
- **Critical path**: Image → BLIP → CLIP → SD Encoder → DDIM Inversion → DDIM Reconstruction → SD Decoder → Feature Concatenation → Classifier
- **Design tradeoffs**:
  - Using a pre-trained SD model vs. training a custom inversion model: Pre-trained model is faster to implement but may not be optimal for inversion.
  - Text conditioning vs. unconditional inversion: Text conditioning provides better reconstructions but requires a captioner and embedder.
  - ResNet50 vs. other architectures: ResNet50 is a good starting point but other architectures may be more suitable.
- **Failure signatures**:
  - Poor reconstructions: May indicate issues with the inversion process or text conditioning.
  - Classifier overfitting: May indicate insufficient data or overly complex model.
  - Low detection accuracy: May indicate issues with the feature extraction or classification.
- **First 3 experiments**:
  1. Implement and test the DDIM inversion and reconstruction pipeline on a small set of images.
  2. Train and evaluate a classifier using only the original images as input.
  3. Train and evaluate a classifier using the original images, reconstructions, and noise maps as input.

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- The effectiveness of the proposed mechanism depends on the assumption that different text-to-image models produce samples with systematically lower likelihood under any diffusion model, which lacks strong empirical validation.
- The SynRIS evaluation protocol's reliability depends on the quality and coverage of reverse image search results, which could introduce new biases or fail to find sufficiently similar real images.
- The paper does not adequately address how potential failures in the evaluation protocol would impact the reliability of the results.

## Confidence
- **High Confidence**: The technical implementation details of the FakeInversion pipeline (DDIM inversion, feature extraction, ResNet50 classification) are clearly specified and reproducible.
- **Medium Confidence**: The claim that combining original images, reconstructions, and noise maps provides superior discriminative features is supported by ablation studies, but the underlying reasons for this improvement are not fully explained.
- **Low Confidence**: The core mechanism that likelihood estimates from SD inversion generalize across different text-to-image models, and the assertion that SynRIS provides a more reliable evaluation, are both weakly supported by evidence.

## Next Checks
1. **Likelihood Distribution Analysis**: Systematically compare the likelihood distributions of real and fake images under multiple diffusion models (not just SD) to verify if the proposed likelihood signal is indeed systematic and generalizable.
2. **Reverse Image Search Quality Audit**: Conduct a detailed analysis of the reverse image search results to quantify the similarity between matched real and fake image pairs, and test how sensitive the detector's performance is to variations in this similarity.
3. **Ablation on Conditioning**: Evaluate the performance of the detector using unconditional DDIM inversion (without text conditioning) to determine whether the text conditioning is essential for the observed generalization benefits.