---
ver: rpa2
title: 'WebQuest: A Benchmark for Multimodal QA on Web Page Sequences'
arxiv_id: '2409.13711'
source_url: https://arxiv.org/abs/2409.13711
tags:
- question
- screen
- information
- screens
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces WebQuest, a benchmark dataset for multimodal
  question answering across multiple web pages. The dataset addresses the challenge
  of reasoning across information from different web pages and screens, which is a
  common real-world scenario but not well-represented in existing benchmarks.
---

# WebQuest: A Benchmark for Multimodal QA on Web Page Sequences

## Quick Facts
- arXiv ID: 2409.13711
- Source URL: https://arxiv.org/abs/2409.13711
- Reference count: 40
- Primary result: WebQuest reveals significant performance gaps between single-screen and multi-screen reasoning tasks for multimodal models

## Executive Summary
WebQuest introduces a novel benchmark dataset for evaluating multimodal question answering across multiple web pages and screens. The dataset addresses a critical gap in existing benchmarks by requiring models to reason across information from different web pages - a common real-world scenario. The benchmark includes three categories of increasing difficulty: single-screen QA, multi-screen QA, and trace QA, where models must answer questions based on navigation traces through multiple screens. The authors evaluate several state-of-the-art multimodal models and find significant performance gaps between single-screen and multi-screen reasoning tasks.

## Method Summary
The WebQuest benchmark evaluates multimodal models on question answering tasks using web page sequences. The dataset contains 217 QA pairs across three categories: single-screen (questions about one web UI), multi-screen (questions requiring reasoning across multiple related pages), and trace QA (questions about entire navigation sequences). Models are evaluated using zero-shot and Chain-of-Thought prompting techniques, with performance measured using a relaxed accuracy metric allowing ±0.05 margin for floating-point answers. The evaluation includes GPT-4V, Gemini Flash, Claude 3, InstructBLIP, and PaliGemma models.

## Key Results
- Significant performance gap exists between single-screen and multi-screen reasoning tasks across all evaluated models
- Chain-of-Thought prompting leads to substantial improvements across all models, especially on multi-screen tasks
- Multi-screen reasoning remains challenging even for the largest multimodal models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WebQuest's multi-screen design reveals reasoning gaps that single-screen benchmarks miss
- Mechanism: By requiring information extraction and synthesis across multiple related web pages, WebQuest exposes model limitations in cross-page reasoning that are not evident when evaluating on single-page datasets
- Core assumption: The gap between single-screen and multi-screen performance indicates fundamental reasoning limitations rather than just computational overhead
- Evidence anchors:
  - [abstract]: "revealing a significant gap between single-screen and multi-screen reasoning"
  - [section 4.3]: "we notice that all the MLLMs perform worse than the single screen split"
  - [corpus]: Weak - neighboring papers focus on single-page or sequential reasoning but not cross-page synthesis
- Break condition: If performance on multi-screen tasks improves to match single-screen performance without architectural changes, suggesting the gap was due to model size or context window limitations rather than reasoning capability

### Mechanism 2
- Chain-of-thought prompting significantly improves multi-screen reasoning by decomposing complex questions into manageable sub-tasks
- Mechanism: CoT prompts force models to explicitly analyze each screen, extract relevant information, and then synthesize answers, which compensates for the model's difficulty in handling multi-screen information holistically
- Core assumption: The improvement from CoT indicates that models have the latent capability for multi-screen reasoning but struggle with information organization and synthesis
- Evidence anchors:
  - [abstract]: "we investigate inference time techniques like Chain-of-Thought prompting to improve model capabilities on multi-screen reasoning"
  - [section 4.4.1]: "Chain-of-thought [31] prompting results in significant improvements across all models, even with the largest models"
  - [corpus]: Weak - neighboring papers focus on CoT for single-page tasks, not cross-page synthesis
- Break condition: If CoT prompts stop improving performance on increasingly complex multi-screen tasks, suggesting a fundamental architectural limitation

### Mechanism 3
- WebQuest's three-tier difficulty structure (single-screen, multi-screen, trace QA) enables fine-grained capability assessment
- Mechanism: The progressive difficulty allows identification of specific failure modes at each level, revealing whether models struggle with basic extraction, cross-page synthesis, or long-sequence reasoning
- Core assumption: The consistent performance drop across all models at each difficulty tier indicates genuine capability gaps rather than model-specific weaknesses
- Evidence anchors:
  - [abstract]: "WebQuest includes three question categories: single-screen QA, multi-screen QA, and QA based on navigation traces"
  - [section 3]: "We split the dataset into three categories of increasing difficulty"
  - [section 4.3]: "We analyze the results in the sections below" showing performance degradation across categories
- Break condition: If models show inconsistent performance patterns across categories, suggesting the difficulty levels are not well-calibrated

## Foundational Learning

- Concept: Information extraction across multiple UI layouts
  - Why needed here: WebQuest requires models to extract relevant information from web pages with varying layouts and structures, a critical skill for real-world web agents
  - Quick check question: Can you identify which elements of a web page contain the information needed to answer a given question when the layout changes?

- Concept: Cross-document reasoning and synthesis
  - Why needed here: The multi-screen and trace QA categories require combining information from multiple sources to answer questions, testing the model's ability to synthesize information
  - Quick check question: Given two web pages about the same product, can you determine which offers the better deal by comparing prices and features?

- Concept: Sequential reasoning through navigation traces
  - Why needed here: Trace QA evaluates whether models can reason about information encountered during a browsing session, including irrelevant pages
  - Quick check question: If a user navigates through 10 web pages to find a restaurant, can you identify which pages contain relevant information for answering questions about restaurant options?

## Architecture Onboarding

- Component map: Input processing -> Information extraction -> Reasoning engine -> Answer generation
- Critical path: Question → Screen analysis → Information extraction → Cross-screen synthesis → Answer generation
- Design tradeoffs:
  - Resolution vs. context window: Higher resolution screens provide more detail but reduce the number of screens that can be processed
  - Single-pass vs. multi-pass processing: Processing all screens at once vs. iterative refinement through CoT
  - Open vs. closed domain: Allowing external knowledge vs. constraining to screen content only
- Failure signatures:
  - Performance drop on multi-screen tasks indicates reasoning limitations
  - Inconsistent answers across similar questions suggest information extraction issues
  - Out-of-memory errors on long traces indicate context window constraints
- First 3 experiments:
  1. Compare single-screen vs. multi-screen performance to establish baseline capability gap
  2. Test CoT prompting effectiveness across different question types and screen counts
  3. Evaluate model performance on progressively longer navigation traces to identify context window limits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively evaluate multimodal models on long sequences of web pages with varying layouts and content types?
- Basis in paper: [explicit] The paper introduces WebQuest, a benchmark dataset for multimodal QA on web page sequences, and evaluates several state-of-the-art models on this dataset.
- Why unresolved: The paper acknowledges the difficulty of evaluating models on long sequences of web pages with varying layouts and content types, and suggests that future research could explore using richer screen information, such as OCR, DOM, and screen annotations, to enhance model performance.
- What evidence would resolve it: Comparative studies evaluating different methods for handling long sequences of web pages, including the use of richer screen information, and their impact on model performance.

### Open Question 2
- Question: How can we improve the performance of multimodal models on tasks requiring reasoning across multiple web pages with different layouts and content types?
- Basis in paper: [explicit] The paper shows that there is a significant performance gap between single-screen and multi-screen reasoning tasks, and suggests that Chain-of-Thought prompting can improve model performance on multi-screen reasoning tasks.
- Why unresolved: The paper does not explore other methods for improving model performance on multi-screen reasoning tasks, such as fine-tuning on specific types of web pages or using more advanced reasoning techniques.
- What evidence would resolve it: Comparative studies evaluating different methods for improving model performance on multi-screen reasoning tasks, including fine-tuning, Chain-of-Thought prompting, and other reasoning techniques.

### Open Question 3
- Question: How can we extend WebQuest to support personalized, multi-turn dialogue scenarios?
- Basis in paper: [inferred] The paper mentions that extending WebQuest to support personalized, multi-turn dialogue scenarios is a compelling avenue for future research.
- Why unresolved: The paper does not provide any details on how to extend WebQuest to support personalized, multi-turn dialogue scenarios, or what the challenges and opportunities of such an extension would be.
- What evidence would resolve it: Research exploring the design and implementation of WebQuest extensions for personalized, multi-turn dialogue scenarios, including the collection of new data and the evaluation of model performance.

## Limitations

- Dataset scale: The current dataset contains only 217 QA pairs, which may limit statistical significance and generalizability
- Model coverage: The evaluation focuses on commercial models, missing open-source alternatives that might perform differently
- Task diversity: The web pages and tasks may not fully represent the diversity of real-world web browsing scenarios

## Confidence

**High Confidence Claims**:
- WebQuest successfully reveals performance gaps between single-screen and multi-screen reasoning tasks
- Chain-of-Thought prompting provides consistent improvements across all evaluated models
- The benchmark effectively captures the complexity of real-world web-based reasoning scenarios

**Medium Confidence Claims**:
- The three-tier difficulty structure accurately represents progressive reasoning complexity
- The current performance gaps indicate fundamental reasoning limitations rather than model size or implementation issues

**Low Confidence Claims**:
- Generalizability of findings to other multimodal models not evaluated in the study
- Long-term significance of the performance gaps without additional data and model evaluations

## Next Checks

1. **Dataset Expansion Validation**: Evaluate whether performance patterns hold as the dataset scales from 217 to the targeted 2000+ QA pairs, particularly examining if the single-screen vs. multi-screen gap persists or narrows with more examples.

2. **Cross-Model Comparison**: Test additional open-source multimodal models (e.g., LLaVA, Qwen-VL) on WebQuest to determine if the observed performance patterns are consistent across different model architectures and training approaches.

3. **Real-World Transfer**: Conduct a user study comparing WebQuest task performance with actual human web browsing and information retrieval tasks to validate whether the benchmark accurately reflects real-world web reasoning capabilities.