---
ver: rpa2
title: 'Towards Foundation Models on Graphs: An Analysis on Cross-Dataset Transfer
  of Pretrained GNNs'
arxiv_id: '2412.17609'
source_url: https://arxiv.org/abs/2412.17609
tags:
- pretraining
- pretrained
- graph
- downstream
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work analyzes cross-dataset transfer of pretrained graph neural
  networks (GNNs) for foundation models on graphs. The authors extend GPSE, a structural
  pretraining approach, by introducing Feature-Structuralization to encode feature
  information into additional graph structure while maintaining feature-agnosticism.
---

# Towards Foundation Models on Graphs: An Analysis on Cross-Dataset Transfer of Pretrained GNNs

## Quick Facts
- arXiv ID: 2412.17609
- Source URL: https://arxiv.org/abs/2412.17609
- Authors: Fabrizio Frasca, Fabian Jogl, Moshe Eliasof, Matan Ostrovsky, Carola-Bibiane Schönlieb, Thomas Gärtner, Haggai Maron
- Reference count: 22
- Key outcome: Cross-dataset transfer of pretrained GNNs improves generalization only with sufficient downstream data and depends on pretraining data quantity and properties; Feature-Structuralization encodes feature information into graph structure but requires feature space similarity

## Executive Summary
This work analyzes cross-dataset transfer of pretrained graph neural networks (GNNs) for foundation models on graphs. The authors extend GPSE, a structural pretraining approach, by introducing Feature-Structuralization to encode feature information into additional graph structure while maintaining feature-agnosticism. They evaluate pretrained models on three molecular datasets (zinc, peptides, molpcba) with varying training sample sizes and pretraining dataset combinations. Results show pretrained embeddings improve generalization only with sufficient downstream data and the degree of improvement depends on pretraining data quantity and properties. Feature information can help but requires similarity between pretraining and downstream feature spaces.

## Method Summary
The authors use GPSE (Graph Positional and Structural Encoder) as the pretraining backbone, extending it with Feature-Structuralization to convert categorical features into additional graph structure. They pretrain on three molecular datasets (zinc-12k, ogbg-molpcba, peptides-func) individually and in combinations, then evaluate on downstream tasks with varying training sample sizes (0.01 to 0.5 of available data). The evaluation uses a 3-layer GIN architecture with linear transformation and feature encoder, measuring performance via R2 for pretraining targets and Average Precision/Mean Absolute Error for downstream tasks.

## Key Results
- Pretrained embeddings improve downstream generalization only when sufficient downstream data is available, being detrimental in data-scarce settings
- The degree of improvement depends on the quantity and properties of pretraining data, with dataset similarity affecting transfer performance
- Feature-Structuralization can encode feature information into graph structure while maintaining feature-agnosticism, but requires similarity between pretraining and downstream feature spaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretrained embeddings improve downstream generalization only when sufficient downstream data is available.
- Mechanism: The pretrained model learns general structural patterns during pretraining that can be leveraged in downstream tasks, but this transfer benefit requires enough downstream samples to learn how to adapt these general patterns to task-specific requirements.
- Core assumption: The structural patterns learned during pretraining are sufficiently general to be useful across different molecular datasets, but not so specific that they dominate downstream learning.
- Evidence anchors:
  - [abstract] "Our preliminary results indicate that embeddings from pretrained models improve generalization only with enough downstream data points"
  - [section] "Pretrained embeddings can be — but are not always — beneficial (Q1). Pre-trained embeddings improve generalization over the baseline with sufficient data, but can be detrimental in data-scarce settings"
  - [corpus] Weak - The corpus doesn't directly address this specific mechanism but related papers on cross-dataset transferability provide indirect support
- Break condition: If downstream data is too scarce, the pretrained embeddings may introduce noise or bias that prevents the model from learning task-specific patterns effectively.

### Mechanism 2
- Claim: The degree of improvement from pretraining depends on the quantity and properties of pretraining data.
- Mechanism: More diverse and larger pretraining datasets provide richer structural patterns that can transfer better to downstream tasks, while datasets with similar structural and feature properties to the target dataset provide more relevant transfer benefits.
- Core assumption: The pretraining datasets contain structural patterns that are both general enough to transfer and specific enough to be useful for the downstream task.
- Evidence anchors:
  - [abstract] "in a degree which depends on the quantity and properties of pretraining data"
  - [section] "When pretraining becomes beneficial, all pretraining mixes seem to provide better generalization than the baseline... However, the composition of the corpus may have a strong impact, which tend to reflect the similarity between source and target datasets"
  - [corpus] Moderate - The GSTBench paper in the corpus directly studies transferability of graph SSL methods across datasets
- Break condition: If pretraining datasets are too dissimilar from the target dataset or too small, the transfer benefits diminish significantly.

### Mechanism 3
- Claim: Feature-Structuralization can encode feature information into additional structure while maintaining feature-agnosticism, but requires similarity between pretraining and downstream feature spaces.
- Mechanism: By converting categorical features into additional graph structure (feature-nodes connected to original nodes), the pretraining process can learn structural patterns that incorporate feature information without being tied to specific feature encodings. However, this only works well when the feature spaces are similar across datasets.
- Core assumption: The categorical feature space can be meaningfully converted into graph structure without losing essential information about feature interactions.
- Evidence anchors:
  - [abstract] "Feature-Structuralization to encode feature information into additional graph structure while maintaining feature-agnosticism"
  - [section] "Feature information can lead to improvements, but currently requires some similarities between pretraining and downstream feature spaces"
  - [corpus] Moderate - The "Using pretrained GNNs with token mixers as geometric featurizers" paper suggests feature-agnostic approaches can work across domains
- Break condition: When pretraining and downstream datasets have very different feature spaces (e.g., zinc vs. molpcba), the structuralization approach struggles because the converted feature structure doesn't capture meaningful relationships.

## Foundational Learning

- Concept: Graph Neural Networks and their message-passing mechanism
  - Why needed here: Understanding how GNNs process graph structure is essential for grasping why pretrained embeddings can help or hinder downstream tasks
  - Quick check question: How does a standard GNN layer update node representations using neighbor information?

- Concept: Transfer learning and domain adaptation
  - Why needed here: The paper fundamentally investigates when and how knowledge from pretrained models transfers to new datasets, which requires understanding transfer learning principles
  - Quick check question: What factors typically influence the success of transfer learning between domains?

- Concept: Feature encoding and categorical feature spaces
  - Why needed here: The Feature-Structuralization approach relies on converting categorical features into graph structure, requiring understanding of how features are encoded and represented
  - Quick check question: How might categorical features with different value ranges affect the structuralization process?

## Architecture Onboarding

- Component map: Pretraining phase (GPSE backbone + prediction heads) -> Feature-Structuralization module (converts categorical features to graph structure) -> Downstream pipeline (pretrained model → linear transformation → feature encoder → GNN → prediction) -> Evaluation (varying downstream training sizes)
- Critical path: Pretraining → Feature encoding → Downstream adaptation → Evaluation
- Design tradeoffs:
  - Using structuralization vs. raw features: Structuralization maintains feature-agnosticism but may lose some feature information; raw features preserve information but tie the model to specific encodings
  - Single vs. multi-dataset pretraining: Multi-dataset pretraining provides more diverse patterns but requires more computational resources
  - Pretraining target selection: Different P/SE targets may capture different aspects of graph structure
- Failure signatures:
  - Pretrained embeddings hurt performance in low-data regimes: Indicates the model is overfitting to pretraining patterns
  - Structuralization underperforms with dissimilar feature spaces: Shows the structuralization isn't capturing meaningful feature relationships
  - No improvement from pretraining: Suggests the pretraining data lacks relevant patterns for the downstream task
- First 3 experiments:
  1. Reproduce the zinc downstream results with varying training ratios to verify the basic pretraining benefit mechanism
  2. Test Feature-Structuralization on zinc (in-domain) vs. molpcba (out-of-domain) to observe the feature space similarity effect
  3. Compare single-dataset pretraining (zinc only) vs. multi-dataset pretraining (zinc+molpcba) to verify the diversity benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural modifications to Feature-Structuralization would enable it to work effectively when pretraining and downstream datasets have mismatched feature spaces?
- Basis in paper: [explicit] The paper states that Feature-Structuralization "struggles when features differ across datasets" and that "Feature information can lead to improvements, but currently requires some similarities between pretraining and downstream feature spaces."
- Why unresolved: The paper demonstrates that structuralization fails in cross-domain feature scenarios but doesn't propose or test specific architectural solutions to address this limitation.
- What evidence would resolve it: Experimental results showing improved cross-dataset transfer when using alternative structuralization designs that handle feature space mismatches, such as adaptive feature encoding schemes or domain-invariant feature representations.

### Open Question 2
- Question: How does the quantity and diversity of pretraining data affect the generalization gap between in-domain and out-of-domain transfer?
- Basis in paper: [explicit] The paper shows "the gap between in- and off-domain pretraining narrows with more data, it never fully closes" and observes that "Adding data from other sources can still be beneficial."
- Why unresolved: While the paper tests different pretraining dataset combinations, it doesn't systematically explore the relationship between pretraining corpus size/diversity and the persistent performance gap between in-domain and out-of-domain transfer.
- What evidence would resolve it: Comprehensive experiments varying both the absolute size and the diversity of pretraining datasets, quantifying how these factors affect the in-domain/out-of-domain performance gap across multiple graph domains.

### Open Question 3
- Question: Why does pretraining on structurally similar but feature-dissimilar datasets (like peptides+molpcba) sometimes outperform pretraining on the target dataset alone?
- Basis in paper: [explicit] The paper observes that "Pretraining mixes that do not include the target dataset can perform as well as pretraining on the target" and speculates this "may be mostly due to an increase in pretraining data."
- Why unresolved: The authors acknowledge the performance paradox but don't investigate the underlying mechanisms, such as whether structural similarity compensates for feature dissimilarity or if there are other factors at play.
- What evidence would resolve it: Controlled experiments isolating the effects of structural versus feature similarity, analysis of learned representations to identify which aspects of the pretraining data are most beneficial for transfer, and ablation studies on pretraining data composition.

## Limitations
- Limited empirical scope: The analysis is restricted to molecular datasets with specific structural and feature characteristics, which may not generalize to other graph domains
- Unclear pretraining dataset selection criteria: The paper doesn't fully justify why these three particular molecular datasets were chosen or how they represent the broader landscape of potential pretraining data
- Feature-Structuralization performance gaps: The approach struggles significantly when pretraining and downstream feature spaces differ, but the paper doesn't provide clear guidance on when this approach is appropriate

## Confidence

- **High confidence**: The finding that pretrained embeddings improve generalization only with sufficient downstream data, supported by direct experimental results showing performance degradation in low-data regimes
- **Medium confidence**: The claim about Feature-Structuralization maintaining feature-agnosticism while encoding feature information, though this appears to work well only within similar feature spaces
- **Medium confidence**: The observation that pretraining dataset properties and quantity affect transfer performance, with some uncertainty about which specific properties matter most

## Next Checks

1. **Dataset diversity validation**: Test the pretraining approach on non-molecular graph datasets (e.g., social networks, citation graphs) to assess whether the observed transfer patterns hold across different graph domains
2. **Feature space similarity analysis**: Systematically measure feature space similarity between pretraining and downstream datasets to establish quantitative thresholds for when Feature-Structuralization becomes effective
3. **Alternative pretraining targets**: Evaluate whether different pretraining targets beyond P/SE (e.g., node/edge prediction, graph-level properties) could provide more robust transfer across dissimilar feature spaces