---
ver: rpa2
title: 'CadVLM: Bridging Language and Vision in the Generation of Parametric CAD Sketches'
arxiv_id: '2409.17457'
source_url: https://arxiv.org/abs/2409.17457
tags:
- sketch
- image
- cadvlm
- language
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CadVLM, the first multimodal vision-language
  model for parametric CAD sketch generation, leveraging pre-trained foundation models
  for language and vision to jointly model sketch primitives and images. The method
  combines a text encoder-decoder and an image encoder-decoder with a contrastive
  loss to align visual and textual representations.
---

# CadVLM: Bridging Language and Vision in the Generation of Parametric CAD Sketches

## Quick Facts
- **arXiv ID**: 2409.17457
- **Source URL**: https://arxiv.org/abs/2409.17457
- **Reference count**: 35
- **Primary result**: First multimodal vision-language model for parametric CAD sketch generation, outperforming baselines with up to 45.2% CAD F1 and 23.8% Sketch Accuracy.

## Executive Summary
CadVLM introduces a novel multimodal vision-language model for parametric CAD sketch generation, leveraging pretrained foundation models for language (CodeT5+) and vision (ViT-MAE) to jointly model sketch primitives and images. The model combines a text encoder-decoder with an image encoder-decoder, using a contrastive loss to align visual and textual representations. Extensive experiments on the SketchGraphs dataset demonstrate CadVLM's superior performance in CAD autocompletion and autoconstraint tasks, achieving state-of-the-art results and enabling image-conditioned generation that approaches expert-level performance.

## Method Summary
CadVLM is a multimodal transformer-based encoder-decoder architecture that integrates text and image modalities for CAD sketch generation. The model uses pretrained CodeT5+ for text encoding/decoding and ViT-MAE for image encoding/decoding, trained jointly on the SketchGraphs dataset. It employs an autoregressive approach for generating sketch primitives, conditioned on partial sketches and multimodal embeddings. The training combines image-text contrastive loss, image reconstruction loss, and language modeling loss, with the contrastive component ensuring alignment between visual and textual representations of sketches.

## Key Results
- Achieves up to 45.2% CAD F1 and 23.8% Sketch Accuracy on SketchGraphs dataset
- Outperforms state-of-the-art baselines in CAD autocompletion and autoconstraint tasks
- Enables image-conditioned generation approaching expert-level performance
- Ablation studies demonstrate value of image modality and multimodal alignment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Contrastive loss aligns image and text embeddings, enabling integration of geometric reasoning from both modalities
- **Mechanism**: Image-text contrastive loss ensures positive pairs have similar representations while pushing negative pairs apart in shared embedding space
- **Core assumption**: Visual and textual representations of same sketch can be meaningfully compared and aligned
- **Evidence**: Outperforms baselines on CAD tasks; explicit description of contrastive loss function
- **Break condition**: If image encoder fails to extract meaningful features, alignment becomes ineffective

### Mechanism 2
- **Claim**: Pretrained CodeT5+ and ViT-MAE provide strong inductive biases accelerating learning in CAD domain
- **Mechanism**: Pretrained models' representations of syntax and visual patterns serve as strong starting point, reducing need for extensive CAD-specific data
- **Core assumption**: Knowledge from natural language, code, and general images transfers to CAD domain
- **Evidence**: Superior performance on SketchGraphs; significant cross-domain knowledge transfer capabilities demonstrated
- **Break condition**: If CAD domain is too dissimilar from pretraining data, transfer may fail

### Mechanism 3
- **Claim**: Autoregressive generation of sketch primitives enables coherent and valid CAD sketches
- **Mechanism**: Text decoder generates primitives step-by-step, conditioned on partial sketch and multimodal embeddings
- **Core assumption**: Sequence of sketch primitives can be modeled as Markov process
- **Evidence**: Enables image-conditioned generation approaching expert-level performance
- **Break condition**: If autoregressive order doesn't align with sketch structure, generation may become inconsistent

## Foundational Learning

- **Autoregressive modeling**
  - Why needed: CAD sketches naturally represented as sequences of primitives and constraints, generated step-by-step
  - Quick check: What is mathematical formulation of autoregressive loss for CAD autocompletion?

- **Contrastive learning**
  - Why needed: Aligning image and text embeddings crucial for multimodal reasoning in CAD sketches
  - Quick check: How does image-text contrastive loss ensure alignment of positive and negative pairs?

- **Transformer-based architectures**
  - Why needed: Transformers excel at sequence modeling and attention-based reasoning essential for handling CAD primitives and constraints
  - Quick check: What are key components of transformer encoder-decoder architecture?

## Architecture Onboarding

- **Component map**: Image encoder (ViT-MAE) → Contrastive loss → Text decoder (CodeT5+) → CAD output (primitives)
- **Critical path**: Image encoder → Contrastive loss → Text decoder → CAD output (primitives)
- **Design tradeoffs**:
  - Multimodal vs. unimodal: Multimodal improves performance but increases complexity and training cost
  - Fine-tuning vs. training from scratch: Fine-tuning leverages pretraining but may limit adaptation to CAD-specific patterns
  - Autoregressive vs. non-autoregressive: Autoregressive ensures coherence but is slower
- **Failure signatures**:
  - Low Sketch Accuracy: Image encoder may not extract useful features
  - Low Entity Accuracy: Text decoder may not generate correct primitives
  - Poor convergence: Contrastive loss may not align embeddings effectively
- **First 3 experiments**:
  1. Test unimodal CadVLM (text only) vs. multimodal CadVLM to measure impact of image modality
  2. Test CadVLM with and without contrastive loss to measure impact of alignment
  3. Test CadVLM with different input entity ratios (20%, 40%, 60%, 80%) to measure robustness

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does alignment of image and text representations in CadVLM affect performance on CAD autoconstraint tasks given discrepancy between ViT-MAE pretraining task and CAD autoconstraint tasks?
- **Basis**: Paper acknowledges pretraining task discrepancy but doesn't provide detailed experimental results comparing performance variants with and without image-text alignment on CAD autoconstraint task
- **What would resolve**: Detailed ablation study showing performance difference of CadVLM-w/o-ITC and CadVLM-w/o-IDL&ITC on CAD autoconstraint task, including Sketch Accuracy, Entity Accuracy, and CAD F1 metrics

### Open Question 2
- **Question**: What is impact of entity-level modeling inductive bias on computational efficiency of CadVLM, and is marginal performance gain worth increased computational and memory demands?
- **Basis**: Paper states incorporating this inductive bias increases computation and memory demands while only marginally enhancing performance
- **What would resolve**: Comprehensive analysis of computational cost (training time, memory usage) of entity-level modeling variant compared to standard CadVLM, with discussion on practical implications

### Open Question 3
- **Question**: How does performance of CadVLM vary with different input entity ratios in CAD autocompletion task, and what is minimum input entity ratio required for acceptable performance?
- **Basis**: Paper conducts experiment with different input entity ratios (20%, 40%, 60%, 80%) and reports performance metrics
- **What would resolve**: Detailed analysis of performance scaling with input entity ratio, including minimum ratio required for acceptable performance and visualizations of performance trends

## Limitations
- **Dataset Generalization**: Performance on real-world, proprietary CAD datasets or different design domains remains unverified
- **Pretraining Transfer Assumptions**: Effectiveness depends on overlap between pretraining data and CAD semantics
- **Contrastive Loss Alignment Quality**: Degree of alignment and its impact on downstream tasks not rigorously quantified
- **Autoregressive Generation Coherence**: May not handle non-sequential or context-dependent generation well

## Confidence
- **High Confidence**: Claims about superior performance on SketchGraphs tasks (Entity Accuracy, Sketch Accuracy, CAD F1)
- **Medium Confidence**: Claims about effectiveness of multimodal alignment and pretraining transfer
- **Low Confidence**: Claims about real-world applicability and robustness to diverse CAD datasets

## Next Checks
1. **Cross-Dataset Generalization Test**: Evaluate CadVLM on real-world, proprietary CAD dataset or different domain (e.g., architectural sketches) to assess robustness beyond SketchGraphs
2. **Pretraining Transfer Analysis**: Conduct ablation studies measuring impact of pretraining data overlap with CAD semantics, testing on CAD sketches with highly specialized constructs not present in pretraining
3. **Contrastive Loss Alignment Quantification**: Analyze quality of image-text alignment by measuring cosine similarity of positive and negative pairs during training; test model performance with and without contrastive loss to quantify impact on multimodal reasoning