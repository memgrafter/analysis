---
ver: rpa2
title: 'LLMs as mediators: Can they diagnose conflicts accurately?'
arxiv_id: '2412.14675'
source_url: https://arxiv.org/abs/2412.14675
tags:
- moral
- causal
- scale
- misalignment
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tested whether OpenAI's GPT-3.5 and GPT-4 can distinguish
  between disagreements stemming from causal code misalignments (differences in beliefs
  about facts) versus moral code misalignments (differences in values), a skill essential
  for effective mediation. Using a vignette-based replication of prior human subject
  research, the authors found that both LLMs understand the semantic distinction between
  causal and moral codes similarly to humans.
---

# LLMs as mediators: Can they diagnose conflicts accurately?

## Quick Facts
- arXiv ID: 2412.14675
- Source URL: https://arxiv.org/abs/2412.14675
- Reference count: 0
- LLMs can semantically distinguish between causal and moral reasoning but struggle with moral code diagnosis

## Executive Summary
This study tested whether OpenAI's GPT-3.5 and GPT-4 can distinguish between disagreements stemming from causal code misalignments (differences in beliefs about facts) versus moral code misalignments (differences in values), a skill essential for effective mediation. Using a vignette-based replication of prior human subject research, the authors found that both LLMs understand the semantic distinction between causal and moral codes similarly to humans. However, GPT-4 tended to overestimate causal disagreements and underestimate moral disagreements in moral misalignment scenarios—especially when using a proximate scale with concrete language. GPT-3.5 performed worse overall, showing weaker diagnostic accuracy on both scales. While GPT-4 demonstrated human-like diagnostic competence with a distal scale, both models struggled with moral code recognition. The study concludes that while GPT-4 shows promise for mediating conflicts when using abstract language, further refinement is needed for reliable moral code diagnosis.

## Method Summary
The study replicated Study 1 from Koçak et al. (2023) by using OpenAI's GPT-3.5 and GPT-4 APIs to respond to vignette-based conflict scenarios. The authors generated 75 responses per model per condition, presenting identical vignettes and scale items in randomized order, then excluded responses with out-of-bounds values or missing items. They calculated scale scores for each subscale (4 items each) and conducted multi-group confirmatory factor analysis to test configural, metric, and scalar invariance across human and LLM samples. The study compared mean attributions across conditions using t-tests and performed hierarchical clustering on combined human and LLM data to identify mis-diagnosis patterns.

## Key Results
- Both LLMs understand the semantic distinction between causal and moral codes similarly to humans
- GPT-4 tends to overestimate causal disagreements and underestimate moral disagreements in moral misalignment scenarios
- GPT-3.5 shows weaker diagnostic accuracy overall compared to GPT-4
- GPT-4 demonstrates human-like diagnostic competence with abstract (distal) scales but not with concrete (proximate) scales

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs trained on large text corpora can semantically distinguish between causal and moral reasoning because such distinctions are abundant in natural language.
- Mechanism: The model learns to map conversational contexts onto latent dimensions representing causal vs. moral code differences through exposure to human discourse patterns.
- Core assumption: Human-authored text contains sufficient variance and explicit markers distinguishing factual from value-based reasoning.
- Evidence anchors:
  - [abstract] "Given that arguments about causality and values are abundant in text that constitutes the corpus on which LLMs are trained, we have reason to believe that they can diagnose disagreements stemming from differences in causal or moral arguments."
  - [section 1] "Prior research indicates that to be able to mediate conflict, observers of disagreements between parties must be able to reliably distinguish the sources of their disagreement as stemming from differences in beliefs about what is true (causality) vs. differences in what they value (morality)."
  - [corpus] Weak - corpus neighbors focus on moral reasoning in LLMs but not specifically on causal/moral code distinction diagnosis.
- Break condition: If training data lacks balanced representation of causal vs. moral reasoning contexts, or if moral reasoning is expressed primarily through emotional valence rather than explicit value statements.

### Mechanism 2
- Claim: GPT-4 demonstrates more reliable diagnostic accuracy than GPT-3.5 when using abstract (distal) scales due to better generalization from training data.
- Mechanism: Abstract language allows the model to apply learned patterns without being anchored to specific situational details, reducing context-specific bias.
- Core assumption: Abstract representations are more generalizable across diverse contexts than concrete language.
- Evidence anchors:
  - [abstract] "While GPT-4 showed promise for mediating conflicts when using abstract language, further refinement is needed for reliable moral code diagnosis."
  - [section 3.4] "When using the proximate scale, GPT 4 tends to attribute any disagreement to causal code misalignments."
  - [section 3.7] "when using the proximate scale, LLMs are more likely than humans to mis-diagnose sources of conflict reflected in conversations."
- Break condition: If abstract language itself becomes too removed from the conversational context, leading to loss of diagnostic specificity.

### Mechanism 3
- Claim: Both LLMs struggle with recognizing moral code disagreements in moral misalignment scenarios, tending to overestimate causal disagreements.
- Mechanism: The models may default to causal explanations when moral reasoning involves emotional or value-based language that doesn't map cleanly onto their learned representations.
- Core assumption: Moral disagreements expressed through value statements are harder to parse than causal disagreements stated as factual disagreements.
- Evidence anchors:
  - [abstract] "GPT-4 tended to overestimate causal disagreements and underestimate moral disagreements in moral misalignment scenarios."
  - [section 3.5] "LLMs exhibit a tendency to attribute disagreements to causal code differences even when moral disagreements are indicated and no reference is made to causal code differences."
  - [section 3.7] "LLMs are more likely than humans to mis-diagnose sources of conflict reflected in conversations" in moral misalignment conditions.
- Break condition: If the model's training data disproportionately emphasizes causal over moral reasoning, or if moral reasoning is primarily expressed through non-verbal cues absent from text.

## Foundational Learning

- Concept: Semantic distinction between causal and moral codes
  - Why needed here: The entire study tests whether LLMs can distinguish these two types of disagreements
  - Quick check question: Can you explain in one sentence how a disagreement about "what will happen if we do X" differs from "whether doing X is right or wrong"?

- Concept: Measurement invariance in factor analysis
  - Why needed here: The study uses multi-group confirmatory factor analysis to compare how humans and LLMs interpret the same scale
  - Quick check question: What does it mean if scalar invariance is not achieved between human and LLM responses?

- Concept: Exploratory vs. confirmatory factor analysis
  - Why needed here: The study uses both approaches to validate that LLMs understand the scale structure similarly to humans
  - Quick check question: When would you choose EFA over CFA, and vice versa, in validating a measurement instrument?

## Architecture Onboarding

- Component map: Python script for API interaction -> Vignette generation and randomization -> Scale administration -> Data validation and filtering -> Statistical analysis module (MGCFA, EFA, t-tests, cluster analysis)

- Critical path: 1. Generate vignette conversation -> 2. Randomize and present scale items -> 3. Collect and validate responses -> 4. Perform measurement invariance testing -> 5. Conduct diagnostic accuracy analysis

- Design tradeoffs:
  - Using fixed temperature (1.0) for consistency vs. allowing variability in responses
  - Excluding responses with out-of-bounds values vs. imputing missing data
  - Using four items per subscale vs. expanding to capture more nuance

- Failure signatures:
  - High rate of excluded responses (>10%) indicates potential issues with question comprehension
  - Lack of configural invariance suggests fundamental differences in how models understand the construct
  - Systematic overestimation of causal disagreements indicates bias in moral reasoning recognition

- First 3 experiments:
  1. Test model performance on a simplified binary classification task (causal vs. moral) to establish baseline capability
  2. Compare model performance across different temperature settings to assess consistency
  3. Test model performance on manually constructed examples where the correct answer is unambiguous

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does GPT-4's tendency to overestimate causal disagreements and underestimate moral disagreements in moral misalignment scenarios persist when tested with different conflict scenarios beyond the daycare center vignette?
- Basis in paper: [explicit] The authors note that GPT-4 showed this pattern specifically when using the proximate scale with concrete language in the daycare center scenario, but suggest this might not be entirely bad since Kocak et al. (2023) found human participants see people who disagree due to differences in their causal codes as being more likely to resolve their differences.
- Why unresolved: The study only tested one specific scenario (daycare center disagreement). Different conflict contexts might elicit different diagnostic patterns from LLMs.
- What evidence would resolve it: Testing GPT-4 (and GPT-3.5) with multiple diverse conflict scenarios across various domains (business, personal, political) using both proximate and distal scales to see if the diagnostic bias pattern generalizes or varies by context.

### Open Question 2
- Question: How do emotional valence and sentiment analysis capabilities of LLMs affect their ability to diagnose moral code misalignments compared to causal code misalignments?
- Basis in paper: [inferred] The authors note that "judgments about values, when made in the first person, tap into emotional valence" and express uncertainty about whether LLM training data allows them to assess emotional resonance in the same way humans do.
- Why unresolved: The study used a purely vignette-based approach without explicitly testing the role of emotional content or sentiment in the LLMs' diagnostic capabilities.
- What evidence would resolve it: Conducting experiments that manipulate the emotional tone and sentiment of conflict scenarios while keeping the underlying causal/moral structure constant, then analyzing how this affects LLM diagnostic accuracy.

### Open Question 3
- Question: Can fine-tuning or prompt engineering techniques improve GPT-4's ability to correctly identify moral code misalignments while maintaining its ability to identify causal code misalignments?
- Basis in paper: [explicit] The authors note that GPT-4's bias toward seeing causal code misalignments "might not be entirely bad" but also acknowledge it's a limitation that needs refinement for reliable moral code diagnosis.
- Why unresolved: The study used only standard prompting without exploring whether specific fine-tuning or prompt engineering approaches could address the diagnostic bias.
- What evidence would resolve it: Testing various fine-tuning approaches (including domain-specific training on conflict resolution data) and prompt engineering techniques (including few-shot examples, chain-of-thought prompting) to see if they can improve moral code diagnosis while preserving causal code identification.

## Limitations

- Scale Specification Uncertainty: The exact wording of the distal and proximate scales used in the original human study is not provided in the manuscript, requiring recreation based on examples.
- Zero-Shot Performance Limitations: Both GPT-3.5 and GPT-4 were tested without fine-tuning or task-specific training, which may not reflect real-world mediation applications.
- Sample Size Constraints: While adequate for statistical testing, the relatively small sample sizes per group and high exclusion rates limit generalizability.

## Confidence

- High Confidence: Both LLMs understand the semantic distinction between causal and moral codes similarly to humans
- Medium Confidence: GPT-4 demonstrates human-like diagnostic competence with abstract (distal) scales but overestimates causal disagreements with concrete (proximate) scales
- Low Confidence: GPT-4 shows promise for mediating conflicts when using abstract language

## Next Checks

1. Cross-Lingual Validation: Test the same diagnostic framework across multiple languages to determine whether the causal/moral code distinction is culturally universal or language-dependent.

2. Human-Machine Comparison in Dynamic Scenarios: Conduct interactive mediation simulations where human participants and LLMs must respond to evolving conflict scenarios, testing not just diagnostic accuracy but also the ability to provide appropriate mediation responses.

3. Fine-Tuning Impact Assessment: Fine-tune GPT-4 specifically on causal/moral code distinction tasks using a balanced dataset, then compare diagnostic accuracy improvements against the zero-shot baseline to quantify the gap between current capabilities and optimal performance.