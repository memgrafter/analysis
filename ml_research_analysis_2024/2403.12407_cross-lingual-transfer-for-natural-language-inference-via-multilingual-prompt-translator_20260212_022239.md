---
ver: rpa2
title: Cross-Lingual Transfer for Natural Language Inference via Multilingual Prompt
  Translator
arxiv_id: '2403.12407'
source_url: https://arxiv.org/abs/2403.12407
tags:
- prompt
- language
- cross-lingual
- knowledge
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MPT improves cross-lingual transfer by translating soft prompts
  from source to target language via a multilingual prompt translator, trained jointly
  with a cross-lingual alignment task on an external corpus. In few-shot XNLI experiments,
  MPT achieves 7.5% absolute gains at 4 shots and 3.1% at 64 shots over state-of-the-art
  baselines, with up to 18.4% relative improvement when transferring to distant languages.
---

# Cross-Lingual Transfer for Natural Language Inference via Multilingual Prompt Translator

## Quick Facts
- arXiv ID: 2403.12407
- Source URL: https://arxiv.org/abs/2403.12407
- Authors: Xiaoyu Qiu; Yuechen Wang; Jiaxin Shi; Wengang Zhou; Houqiang Li
- Reference count: 21
- Key outcome: MPT achieves 7.5% absolute gains at 4 shots and 3.1% at 64 shots over state-of-the-art baselines, with up to 18.4% relative improvement when transferring to distant languages.

## Executive Summary
This paper introduces Multilingual Prompt Translator (MPT), a framework that improves cross-lingual transfer for natural language inference by translating soft prompts from source to target language. The key innovation is a multilingual prompt translator that maps semantic and task-relevant information from source prompts to target prompts while preserving task knowledge. MPT is trained jointly with a cross-lingual alignment task on an external parallel corpus, allowing it to transfer knowledge across languages with minimal target language supervision.

## Method Summary
MPT builds on multilingual PLMs (XLM-RoBERTa-base) and extends them with prompt-based transfer learning. The framework trains soft prompts in the source language, then uses a multilingual prompt translator to convert these prompts to the target language. An auxiliary cross-lingual alignment task on external parallel data minimizes KL divergence between predicted word probabilities from source and target prompt inputs, enforcing language-consistent predictions. The model is optimized using combined cross-entropy and KL divergence losses, enabling effective cross-lingual transfer with minimal target language supervision.

## Key Results
- MPT achieves 7.5% absolute gains at 4 shots and 3.1% at 64 shots over state-of-the-art baselines on XNLI
- Up to 18.4% relative improvement when transferring to distant languages
- Outperforms Fine-Tuning, Soft Prompting, and Prompt-learning from Cross-lingual Templates baselines
- Performance improves with more external data but diminishes after a certain point

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translating source soft prompts to target language via a multilingual prompt translator improves cross-lingual transferability.
- Mechanism: The translator acts as a parameter-efficient adapter that maps semantic and task-relevant information from the source prompt space into the target prompt space while preserving task knowledge.
- Core assumption: The semantic structure of the task (e.g., NLI labels) is preserved across languages, so a transformation of prompt embeddings can transfer task knowledge while adapting to language-specific features.
- Evidence anchors: [abstract]: "a multilingual prompt translator is introduced to properly process crucial knowledge embedded in prompt by changing language knowledge while retaining task knowledge." [section]: "we exploit a multilingual prompt translator designed to translate pS into pT, reasonably transferring S knowledge to T knowledge and preserving task knowledge."

### Mechanism 2
- Claim: Adding an auxiliary cross-lingual alignment task on external parallel corpus improves multilingual knowledge transfer in the target prompt.
- Mechanism: The alignment task minimizes Kullback-Leibler divergence between predicted word probabilities from source and target prompt inputs on parallel sentences, enforcing that the target prompt produces language-consistent predictions.
- Core assumption: Parallel sentences contain equivalent semantic content, so aligning prediction distributions on them will lead to shared cross-lingual knowledge in the prompt space.
- Evidence anchors: [abstract]: "we extend an external corpus as auxiliary data, on which an alignment task for predicted answer probability is designed to convert language knowledge, thereby equipping target prompt with multilingual knowledge."

### Mechanism 3
- Claim: Soft prompts with learnable parameters are more effective than fixed or hard prompts for cross-lingual transfer.
- Mechanism: Soft prompts act as continuous task adapters that can be optimized end-to-end during training, allowing them to capture task-specific cues and generalize better than manually designed discrete prompts.
- Core assumption: Continuous embeddings can represent task-relevant features more flexibly than discrete token sequences, and these embeddings can be transferred across languages via the translator.
- Evidence anchors: [abstract]: "we propose a novel framework, Multilingual Prompt Translator (MPT), where a multilingual prompt translator is introduced to properly process crucial knowledge embedded in prompt..."

## Foundational Learning

- Concept: Cross-lingual transfer learning with multilingual PLMs
  - Why needed here: MPT builds on multilingual PLMs (XLM-RoBERTa) and extends them with prompt-based transfer; understanding how PLMs handle multiple languages is essential.
  - Quick check question: How does a multilingual PLM like XLM-RoBERTa handle input in different languages during fine-tuning?

- Concept: Prompt-based learning and soft prompts
  - Why needed here: MPT uses soft prompts as continuous task adapters; knowing how they differ from hard prompts and how they are optimized is key to understanding the approach.
  - Quick check question: What is the main difference between soft prompts and discrete prompt tokens in terms of representation and training?

- Concept: Kullback-Leibler divergence and alignment loss
  - Why needed here: The alignment task uses KLD to match prediction distributions across languages; understanding KLD is necessary to grasp how the translator is trained.
  - Quick check question: In the context of aligning two probability distributions, what does minimizing KL divergence achieve?

## Architecture Onboarding

- Component map: Multilingual PLM backbone (XLM-RoBERTa-base) -> Soft prompt encoder (LSTM + MLP) -> Prompt translator (two-layer perceptron) -> MLM head for token prediction -> Verbalizer mapping label words to NLI classes -> Loss modules: CE loss for classification, KLD loss for alignment

- Critical path: 1. Sample few-shot NLI data in source language 2. Train soft prompt on source language with CE loss 3. Translate source prompt to target prompt using prompt translator 4. Jointly train translator and prompts on both classification and alignment tasks 5. Apply translated target prompt to target language test data

- Design tradeoffs:
  - Prompt length vs overfitting: longer prompts give more capacity but risk overfitting on few shots
  - Alignment task weight (α) vs task performance: higher alignment can hurt source task if misaligned
  - External corpus size vs training efficiency: more data improves alignment but increases compute

- Failure signatures:
  - Performance drops sharply on distant languages: translator may not handle large linguistic gaps
  - No improvement over vanilla prompting: alignment task may be ineffective or corpus poor
  - Overfitting on small datasets: prompt length or model capacity too large

- First 3 experiments:
  1. Vary prompt length (1, 2, 4, 8) and measure average accuracy on XNLI to find optimal length
  2. Sweep α from 0.0 to 1.0 to balance classification vs alignment loss impact
  3. Test with and without external corpus to measure alignment task contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MPT scale with larger external parallel corpora? Is there an optimal size beyond which gains plateau or diminish?
- Basis in paper: [explicit] The paper notes that MPT performance improves with more external data but starts to diminish after a certain point.
- Why unresolved: The paper only tested up to 8000 additional data points and did not explore larger scales.
- What evidence would resolve it: Systematic experiments varying the size of the external parallel corpus well beyond 8000 examples to identify the scaling curve and potential saturation point.

### Open Question 2
- Question: Can the multilingual prompt translator architecture be improved beyond the linear two-layer design? Are more complex architectures (e.g., Transformer-based) beneficial or detrimental?
- Basis in paper: [explicit] The paper experimented with Linear-1, Linear-2, LSTM-1, LSTM-2, Transf.-1, and Transf.-2 architectures and found Linear-2 to work best, but did not extensively explore other possibilities.
- Why unresolved: The study only tested a limited set of architectures and did not explore more advanced or hybrid designs.
- What evidence would resolve it: Comprehensive ablation studies testing various architectures (e.g., deeper networks, attention mechanisms, residual connections) and their impact on cross-lingual transfer performance.

### Open Question 3
- Question: How well does MPT generalize to other cross-lingual tasks beyond natural language inference, such as named entity recognition, question answering, or machine translation?
- Basis in paper: [inferred] The paper focuses exclusively on XNLI and does not evaluate MPT on other cross-lingual tasks.
- Why unresolved: The study's scope is limited to one specific task, leaving the generalizability of the approach to other tasks unexplored.
- What evidence would resolve it: Extensive experiments applying MPT to a diverse set of cross-lingual tasks and comparing its performance against task-specific baselines and existing cross-lingual transfer methods.

## Limitations

- The effectiveness of the multilingual prompt translator heavily depends on the quality and semantic equivalence of the auxiliary parallel corpus (PAWS-15)
- The mechanism by which the prompt translator preserves task knowledge while changing language knowledge remains underspecified
- The framework's effectiveness on more complex cross-lingual reasoning tasks or in truly low-resource scenarios beyond the few-shot settings tested remains unknown

## Confidence

- **High Confidence**: The empirical results showing MPT outperforming baselines on XNLI across multiple shot settings and languages are well-documented and reproducible
- **Medium Confidence**: The mechanism by which prompt translation preserves task knowledge while adapting to target languages is plausible but not fully validated
- **Low Confidence**: The generalizability of MPT to other cross-lingual tasks beyond NLI, and its effectiveness with truly minimal data or distant language pairs not well-represented in the training corpus, remains uncertain

## Next Checks

1. **Cross-Task Generalization Test**: Apply MPT to other cross-lingual tasks (e.g., cross-lingual QA, sentiment analysis) to verify whether the prompt translation mechanism generalizes beyond NLI

2. **Corpus Quality Analysis**: Systematically evaluate the impact of using different auxiliary corpora with varying quality and linguistic diversity

3. **Linguistic Distance Sensitivity**: Conduct controlled experiments varying the linguistic distance between source and target languages (e.g., same family vs. distant families) while holding other factors constant