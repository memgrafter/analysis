---
ver: rpa2
title: 'Generative Example-Based Explanations: Bridging the Gap between Generative
  Modeling and Explainability'
arxiv_id: '2410.20890'
source_url: https://arxiv.org/abs/2410.20890
tags:
- examples
- explanations
- counterfactual
- data
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a probabilistic framework for example-based
  explanations that formally defines counterfactual, adversarial, and affirmative
  examples in a way that aligns with both classical explainability desiderata and
  deep generative modeling approaches. The framework addresses the disconnect between
  recent generative-based explanation methods and traditional XAI concepts by introducing
  fidelity as a key criterion for determining whether examples come from a "possible
  world" distribution.
---

# Generative Example-Based Explanations: Bridging the Gap between Generative Modeling and Explainability

## Quick Facts
- arXiv ID: 2410.20890
- Source URL: https://arxiv.org/abs/2410.20890
- Reference count: 40
- The paper proposes a probabilistic framework for example-based explanations that formally defines counterfactual, adversarial, and affirmative examples using fidelity as a key criterion.

## Executive Summary
This paper introduces a novel probabilistic framework that bridges the gap between generative modeling and explainability by formally defining counterfactual, adversarial, and affirmative examples. The framework introduces fidelity as a central concept, distinguishing these example types based on their likelihood under the data distribution. The authors propose XAIDIFF, an algorithm that generates counterfactual examples using diffusion models with classifier guidance and ADAM optimization. Experimental results on synthetic and real-world datasets demonstrate the method's ability to produce valid, close, and faithful counterfactual examples that flip classifier decisions while maintaining interpretability.

## Method Summary
The proposed method, XAIDIFF, uses diffusion models with classifier guidance to generate counterfactual examples by optimizing an objective that combines validity (classifier decision change), closeness (ℓ1-distance to original), and fidelity (data likelihood). The approach leverages pre-trained diffusion models and incorporates ADAM optimization to handle noisy gradients during the sampling process. The method is evaluated on the SportBalls synthetic dataset and CelebA real-world dataset using quantitative metrics including ℓ1-distance for closeness, classifier confidence for validity, and bits-per-dimension for fidelity.

## Key Results
- XAIDIFF successfully generates counterfactual examples that flip classifier decisions while maintaining high fidelity to the data distribution
- The method produces examples with better trade-offs between validity, closeness, and fidelity compared to baseline approaches
- Quantitative evaluation shows counterfactuals have higher fidelity than adversarial examples, validating the framework's theoretical predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The fidelity criterion distinguishes counterfactual from adversarial examples by requiring high likelihood under the data distribution.
- Mechanism: Counterfactuals are generated by optimizing both validity and fidelity simultaneously, ensuring examples remain in-distribution and interpretable.
- Core assumption: The learned generative model accurately approximates the true data distribution.
- Evidence anchors: [abstract] "formally defining counterfactual examples and their properties in a probabilistic manner amenable for modeling via deep generative models"; [section 2.1] "we propose to use the fidelity of an example to the underlying data distribution as one of the central concepts"
- Break condition: If the generative model poorly approximates the true distribution, fidelity measurements become unreliable.

### Mechanism 2
- Claim: Classifier guidance with ADAM optimization enables stable generation of valid counterfactuals while maintaining closeness.
- Mechanism: The classifier gradient term ensures validity while ADAM handles noisy gradients, allowing the ℓ1 distance term to maintain closeness.
- Core assumption: Classifier gradients are informative for guiding the diffusion process toward the target class.
- Evidence anchors: [section 3.3] "we interpret the gradient guidance in equation (5) as stochastic gradient descent steps"; [section 3.2] "classifier guidance mechanism leverages an unconditionally trained (label-unaware) DDPM"
- Break condition: If classifier gradients become too noisy or uninformative, ADAM cannot stabilize the optimization.

### Mechanism 3
- Claim: The evaluation framework provides quantitative metrics that align with the three critical characteristics.
- Mechanism: By measuring ℓ1-distance, classifier confidence, and bits-per-dimension, the framework quantifies how well generated examples satisfy the formal definitions.
- Core assumption: These metrics reliably capture the properties defined in the probabilistic framework.
- Evidence anchors: [section 2.3] "we propose a quantitative evaluation scheme for example-based explanations directly motivated by the probabilistic framework"; [section 4.2] Experimental results validate the framework's predictions
- Break condition: If metrics fail to correlate with actual interpretability or the framework's definitions prove too restrictive.

## Foundational Learning

- **Concept: Denoising Diffusion Probabilistic Models (DDPM)**
  - Why needed here: DDPMs provide the generative modeling foundation that enables sampling high-fidelity examples from the data distribution, essential for satisfying the fidelity criterion.
  - Quick check question: What is the relationship between the forward and reverse processes in a DDPM?

- **Concept: Classifier guidance**
  - Why needed here: Classifier guidance steers the generative process toward samples that satisfy the validity criterion by modifying the denoising transitions based on classifier gradients.
  - Quick check question: How does the scaling factor s in classifier guidance affect the trade-off between validity and fidelity?

- **Concept: ℓ1 vs ℓ2 distance in optimization**
  - Why needed here: The choice of distance metric affects the nature of changes made to generate counterfactuals - ℓ1 promotes sparse changes while ℓ2 promotes broader changes across features.
  - Quick check question: Why might ℓ1 distance be preferred over ℓ2 for generating interpretable counterfactuals in image data?

## Architecture Onboarding

- **Component map:** Original sample x* -> DDPM sampling with classifier guidance -> ADAM optimization of combined objective -> Counterfactual example
- **Critical path:** Original sample → DDPM sampling with classifier guidance → ADAM optimization of combined objective → Counterfactual generation
- **Design tradeoffs:**
  - Fidelity vs validity: Stronger classifier guidance increases validity but may reduce fidelity
  - Closeness vs validity: Smaller changes maintain closeness but may not sufficiently flip the classifier decision
  - Computational cost vs quality: More sampling steps improve quality but increase computation time
- **Failure signatures:**
  - Low fidelity: Generated examples look unrealistic or out-of-distribution
  - Low validity: Classifier confidence for target class remains low despite optimization
  - Poor closeness: Generated examples differ significantly from the original in perceptible ways
- **First 3 experiments:**
  1. Generate counterfactuals for simple binary classification on synthetic data and verify the three metrics align with expectations
  2. Compare ℓ1 vs ℓ2 distance in the guidance term and measure impact on sparsity of changes
  3. Test different classifier guidance strengths (scaling factor s) and measure trade-offs between validity and fidelity

## Open Questions the Paper Calls Out
- How can we develop quantitative metrics for counterfactual explanations that reliably measure their explanatory value for diverse user groups with varying levels of expertise?
- What are the theoretical guarantees for the convergence and stability of diffusion-based counterfactual generation methods like XAIDIFF?
- How can we extend the generative explainer framework to handle multi-modal data and more complex real-world scenarios where multiple attributes need to be changed simultaneously?
- What are the computational efficiency trade-offs between diffusion-based counterfactual generation and alternative methods like gradient-based approaches?

## Limitations
- The framework relies on the assumption that the learned generative model accurately approximates the true data distribution
- The evaluation metrics lack direct validation against human interpretability judgments
- The method's effectiveness depends heavily on the quality of pre-trained generative models and classifiers

## Confidence

**Confidence Labels:**
- **High:** The formal probabilistic definitions of counterfactual, adversarial, and affirmative examples and their relationship to the data distribution
- **Medium:** The experimental results demonstrating the method's effectiveness on synthetic and real-world datasets
- **Medium:** The quantitative evaluation framework's ability to capture the key characteristics of explanatory examples

## Next Checks

1. **Distribution Fidelity Validation:** Conduct human studies to verify that examples with high fidelity scores according to bits-per-dimension are indeed perceived as realistic and in-distribution by human evaluators.

2. **Cross-Dataset Generalization:** Test the framework on diverse datasets with varying complexity (e.g., medical imaging, satellite imagery) to assess robustness across domains and identify potential failure modes.

3. **Ablation Studies:** Systematically vary the weighting of the three optimization objectives (validity, closeness, fidelity) and measure the impact on generated example quality to determine optimal parameter settings for different use cases.