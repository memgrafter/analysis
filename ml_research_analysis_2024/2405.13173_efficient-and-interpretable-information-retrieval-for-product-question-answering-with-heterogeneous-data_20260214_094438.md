---
ver: rpa2
title: Efficient and Interpretable Information Retrieval for Product Question Answering
  with Heterogeneous Data
arxiv_id: '2405.13173'
source_url: https://arxiv.org/abs/2405.13173
tags:
- information
- lexical
- query
- dense
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hybrid information retrieval model that jointly
  learns dense semantic and sparse lexical representations for product question answering.
  The model combines lexical matching with semantic matching to improve ranking performance
  while maintaining interpretability.
---

# Efficient and Interpretable Information Retrieval for Product Question Answering with Heterogeneous Data

## Quick Facts
- **arXiv ID**: 2405.13173
- **Source URL**: https://arxiv.org/abs/2405.13173
- **Reference count**: 0
- **Primary result**: Hybrid model outperforms independently trained sparse and dense retrievers by 10.95% and 2.7% respectively in MRR@5 score

## Executive Summary
This paper presents a hybrid information retrieval model that jointly learns dense semantic and sparse lexical representations for product question answering with heterogeneous data. The model combines lexical matching with semantic matching to improve ranking performance while maintaining interpretability. By using a dual-encoder architecture with BERT-base-uncased, the approach achieves 92.7% hit rate at top-5 and reduces latency by 30% compared to cross encoders while maintaining comparable performance.

## Method Summary
The method uses a dual-encoder architecture with shared BERT-base-uncased parameters to jointly learn dense semantic representations (from the [CLS] token) and sparse lexical representations (from the MLM head with top-k token selection). The model is trained using contrastive loss with sparsity regularization, combining semantic and lexical scores via linear interpolation. The approach is evaluated on the hetPQA dataset containing product attributes, bullet points, community answers, descriptions, on-site publications, and reviews.

## Key Results
- Hybrid approach outperforms independently trained sparse retrievers by 10.95% and dense retrievers by 2.7% in MRR@5 score
- Achieves 92.7% hit rate at top-5 on heterogeneous product QA dataset
- Reduces latency by 30% and computational load by 38% compared to state-of-the-art cross encoders
- Provides interpretability through expanded tokens that reveal semantic connections between queries and evidence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint learning of dense semantic and sparse lexical representations improves retrieval performance over using either representation independently
- Mechanism: The model learns two complementary representations in parallel - dense semantic representations capture summarized meaning while sparse lexical representations capture explicit term importance with expansion. These are combined via linear interpolation to leverage both semantic and lexical matching strengths
- Core assumption: Semantic and lexical matching capture complementary information that is additive when combined
- Evidence anchors:
  - [abstract] "Our evaluation demonstrates that our hybrid approach outperforms independently trained retrievers by 10.95% (sparse) and 2.7% (dense) in MRR@5 score"
  - [section] "Our model is trained to minimize the following contrastive loss for each kind of representation" and "Collectively, the training procedure minimizes the following loss function"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism

### Mechanism 2
- Claim: Sparse lexical representation with term expansion reduces vocabulary mismatch problems while maintaining interpretability
- Mechanism: The model uses BERT's MLM head to compute importance weights for vocabulary terms, applies log operation to reduce dominance of fewer terms, and uses top-k selection to enforce sparsity. This creates interpretable sparse vectors where non-zero entries indicate important terms
- Core assumption: The MLM head's logit values are meaningful indicators of term importance for retrieval
- Evidence anchors:
  - [abstract] "Expansion-enhanced sparse lexical representation improves information retrieval (IR) by minimizing vocabulary mismatch problems during lexical matching"
  - [section] "We collect the aggregated importance over the lexical terms, W = w1···|V | through the max pooling layer. To reduce computational complexity during score calculation, we enforce sparsity in W by retaining only the top-k weights"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism

### Mechanism 3
- Claim: Single-stage ranking with joint representations is more efficient than two-stage pipelines while maintaining performance
- Mechanism: By jointly learning dense and sparse representations and combining them in a single scoring function, the model eliminates the need for separate retrieval and re-ranking stages. This reduces latency by 30% and computational load by 38% compared to cross encoders
- Core assumption: The joint representations can achieve comparable performance to two-stage pipelines without the additional computational overhead
- Evidence anchors:
  - [abstract] "Our model offers better interpretability and performs comparably to state-of-the-art cross encoders while reducing response time by 30% (latency) and cutting computational load by approximately 38% (FLOPs)"
  - [section] "Our hybrid model outperforms all other two-tower rankers... with a moderate 21% increase in encoding GFLOPs and has a linear interaction cost"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism

## Foundational Learning

- Concept: Dense vs sparse representations in information retrieval
  - Why needed here: Understanding the trade-offs between semantic matching (dense) and exact term matching (sparse) is crucial for appreciating the hybrid approach
  - Quick check question: What is the main advantage of sparse representations and what is their primary limitation compared to dense representations?

- Concept: Contrastive learning for retrieval
  - Why needed here: The model uses contrastive loss functions to train both dense and sparse representations, requiring understanding of how positive and negative examples are used
  - Quick check question: In contrastive learning for retrieval, what is the relationship between the query, positive example, and negative examples?

- Concept: BERT's [CLS] token and MLM head
  - Why needed here: The model leverages both the [CLS] token encoding for dense representations and the MLM head logits for sparse representations
  - Quick check question: What is the difference between the [CLS] token representation and the token sequence representations in BERT?

## Architecture Onboarding

- Component map:
  Query → Dense Encoder → [CLS] representation
  Query → Sparse Encoder → MLM head → Top-k selection
  Information → Dense Encoder → [CLS] representation
  Information → Sparse Encoder → MLM head → Top-k selection
  Score = α × (dense query · dense info) + (1-α) × (sparse query · sparse info)

- Critical path:
  Query → Dense Encoder → [CLS] representation
  Query → Sparse Encoder → MLM head → Top-k selection
  Information → Dense Encoder → [CLS] representation
  Information → Sparse Encoder → MLM head → Top-k selection
  Score = α × (dense query · dense info) + (1-α) × (sparse query · sparse info)

- Design tradeoffs:
  - k value vs sparsity: Higher k improves recall but increases computation and storage
  - α value vs matching strategy: Higher α emphasizes semantic matching, lower α emphasizes lexical matching
  - Joint training vs separate training: Joint training may lead to better coordination but could also cause interference

- Failure signatures:
  - Poor MRR@5 but good Hit@5: Model finds relevant documents but ranks them poorly
  - Good performance on one source type but poor on others: Model overfits to specific evidence formats
  - High latency despite claims: Inefficient implementation of sparse representation operations

- First 3 experiments:
  1. Ablation study: Remove either dense or sparse component and measure performance drop
  2. Hyperparameter sweep: Vary k (top-k tokens) and α (interpolation weight) to find optimal combination
  3. Source-specific evaluation: Measure performance on each evidence source separately to identify weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the hybrid model change when using different sparsity levels (k) for the lexical representation?
- Basis in paper: [explicit] The paper discusses the effect of varying the number of top-k tokens considered in the sparse representation on MRR@5 score.
- Why unresolved: The paper only provides results for k values of 128, 256, and 512, leaving the performance at other sparsity levels unexplored.
- What evidence would resolve it: Conducting experiments with a wider range of k values and analyzing the resulting MRR@5 scores would provide insights into the optimal sparsity level for the lexical representation.

### Open Question 2
- Question: How does the hybrid model's performance compare to other state-of-the-art hybrid models in the literature?
- Basis in paper: [inferred] The paper mentions that the hybrid approach outperforms independently trained sparse and dense retrievers, but it does not compare its performance to other hybrid models.
- Why unresolved: The paper focuses on comparing the hybrid model to sparse and dense retrievers separately, without considering other hybrid approaches.
- What evidence would resolve it: Conducting experiments comparing the hybrid model to other state-of-the-art hybrid models, such as those using query and document expansion or cross-encoder architectures, would provide a more comprehensive evaluation of its performance.

### Open Question 3
- Question: How does the hybrid model's performance change when using different types of encoders, such as RoBERTa or DeBERTa, instead of BERT?
- Basis in paper: [inferred] The paper uses BERT as the base encoder for the hybrid model, but it does not explore the impact of using other transformer-based encoders.
- Why unresolved: The paper does not investigate the potential benefits or drawbacks of using different encoder architectures.
- What evidence would resolve it: Conducting experiments with different encoder architectures and comparing their performance in terms of MRR@5 and other evaluation metrics would provide insights into the optimal encoder choice for the hybrid model.

## Limitations
- The sparse lexical representation mechanism relies on MLM head logits as importance weights without validation that these logits correlate with retrieval relevance
- Evaluation is conducted on a single product QA dataset (hetPQA), limiting generalizability to other domains or languages
- The trade-off between sparsity level (k) and retrieval performance is not thoroughly explored

## Confidence
- **High confidence**: The hybrid model outperforms individual sparse and dense retrievers by 10.95% and 2.7% respectively in MRR@5 score. This is directly stated in the abstract and supported by the ablation study.
- **Medium confidence**: The 30% latency reduction and 38% computational load reduction claims. While stated in the abstract, the comparison is against cross encoders and lacks detailed timing measurements.
- **Low confidence**: The effectiveness of the MLM-based sparse representation. The paper assumes MLM logits are meaningful importance weights but doesn't provide empirical evidence that these logits correlate with actual term importance for retrieval tasks.

## Next Checks
1. **MLM logit validation study**: Conduct correlation analysis between MLM head logits and human-judged term importance across diverse queries to verify that the sparse representation mechanism actually captures meaningful lexical importance rather than BERT pretraining artifacts.

2. **Cross-domain generalization test**: Evaluate the hybrid model on at least two non-product domains (e.g., academic QA, legal document retrieval) to assess whether the 10.95% improvement over sparse retrievers and 2.7% improvement over dense retrievers generalizes beyond the hetPQA dataset.

3. **Ablation of source-aware scaling**: Remove the source-aware scaling component and measure performance degradation to determine whether this mechanism provides genuine improvement or merely compensates for imbalanced training data across heterogeneous evidence sources.