---
ver: rpa2
title: Rethinking Time Series Forecasting with LLMs via Nearest Neighbor Contrastive
  Learning
arxiv_id: '2412.04806'
source_url: https://arxiv.org/abs/2412.04806
tags:
- time
- series
- forecasting
- learning
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NNCL-TLLM, a novel approach that leverages
  large language models (LLMs) for time series forecasting through nearest neighbor
  contrastive learning. The method addresses challenges in adapting LLMs to time series
  data by learning neighborhood-aware text prototypes that align with both word token
  embeddings and time series characteristics.
---

# Rethinking Time Series Forecasting with LLMs via Nearest Neighbor Contrastive Learning

## Quick Facts
- arXiv ID: 2412.04806
- Source URL: https://arxiv.org/abs/2412.04806
- Authors: Jayanie Bogahawatte; Sachith Seneviratne; Maneesha Perera; Saman Halgamuge
- Reference count: 40
- Key outcome: NNCL-TLLM achieves superior performance in few-shot forecasting scenarios and competitive results in long-term and short-term forecasting across multiple benchmark datasets.

## Executive Summary
This paper introduces NNCL-TLLM, a novel approach that leverages large language models (LLMs) for time series forecasting through nearest neighbor contrastive learning. The method addresses challenges in adapting LLMs to time series data by learning neighborhood-aware text prototypes that align with both word token embeddings and time series characteristics. NNCL-TLLM formulates prompts using contrastive learning to bridge the gap between text and time series modalities, while only fine-tuning layer normalization and positional embeddings of the LLM to reduce computational cost. Experiments show that NNCL-TLLM achieves superior performance in few-shot forecasting scenarios and competitive results in long-term and short-term forecasting across multiple benchmark datasets, demonstrating its effectiveness in data-scarce settings and its ability to harness the strong prior knowledge of pre-trained LLMs.

## Method Summary
NNCL-TLLM converts univariate time series into embeddings through patching and patch embedding layers, then learns time series compatible text prototypes (TCTPs) by clustering word token embeddings to represent time series characteristics. The method uses nearest neighbor contrastive learning to formulate prompts by finding top-k nearest neighbor TCTPs for each time series embedding, creating positive pairs that are pulled together in embedding space. Only layer normalization and positional embeddings of the LLM are fine-tuned, preserving pre-trained knowledge while adapting to time series forecasting. The approach demonstrates competitive or superior performance across multiple benchmark datasets with reduced computational cost compared to full fine-tuning.

## Key Results
- Superior performance in few-shot forecasting scenarios compared to baseline methods
- Competitive results in long-term forecasting with MSE and MAE metrics
- Strong performance in short-term forecasting with SMAPE, MASE, and OW A metrics
- Effective performance in data-scarce settings while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Time series compatible text prototypes (TCTPs) enable alignment between LLM word embeddings and time series data without explicit mapping.
- Mechanism: The method learns TCTPs by minimizing Euclidean distance between word token embeddings and their nearest TCTP embeddings, effectively clustering word embeddings into prototypes that represent time series characteristics.
- Core assumption: Word embeddings can capture sufficient semantic similarity to represent time series patterns when appropriately clustered into TCTPs.
- Evidence anchors:
  - [abstract]: "generate time series compatible text prototypes such that each text prototype represents both word token embeddings in its neighborhood and time series characteristics"
  - [section]: "we propose a new concept called: time series compatible text prototypes (TCTPs) which are text prototypes that represent time series characteristics and may not make sense from a natural language perspective"
- Break condition: If time series patterns cannot be meaningfully represented through clustering of word embeddings, or if the learned TCTPs fail to capture domain-specific time series characteristics.

### Mechanism 2
- Claim: Nearest neighbor contrastive learning (NNCL) improves time series forecasting by aligning TCTPs with time series embeddings through positive pair attraction.
- Mechanism: NNCL formulates prompts by finding top-k nearest neighbor TCTPs for each time series embedding, creating positive pairs that are pulled together in embedding space while negative samples are pushed apart.
- Core assumption: Contrastive learning can effectively align representations from different modalities (text and time series) when positive pairs are well-defined.
- Evidence anchors:
  - [abstract]: "draw inspiration from Nearest Neighbor Contrastive Learning to formulate the prompt while obtaining the top-$k$ nearest neighbor time series compatible text prototypes"
  - [section]: "Inspired by the idea of NNCL in the domain of computer vision (Dwibedi et al. 2021), we propose to adopt NNCL to formulate the input prompt utilizing TCTPs"
- Break condition: If the nearest neighbor search fails to find meaningful correspondences between time series embeddings and TCTPs, or if the contrastive loss doesn't improve alignment.

### Mechanism 3
- Claim: Fine-tuning only layer normalization and positional embeddings preserves LLM prior knowledge while adapting to time series forecasting.
- Mechanism: By freezing multi-head attention and feed-forward layers that contain most pre-trained knowledge, the method adapts the LLM through minimal parameter updates to the normalization and positional components.
- Core assumption: Layer normalization and positional embeddings are sufficient to adapt the LLM's output for time series forecasting without losing pre-trained capabilities.
- Evidence anchors:
  - [abstract]: "fine-tune the layer normalization and positional embeddings of the LLM, keeping the other layers intact, reducing the trainable parameters and decreasing the computational cost"
  - [section]: "Finetuning of the LLM can enable the capability to forecast the time series with the help of the formulated input prompt while preserving the prior it has learned during extensive training"
- Break condition: If layer normalization and positional embeddings alone cannot capture the necessary adaptations for time series forecasting, or if the frozen layers cannot accommodate the new task requirements.

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: To align representations from different modalities (text and time series) by pulling positive pairs together and pushing negative pairs apart
  - Quick check question: What is the difference between contrastive loss and standard cross-entropy loss in representation learning?

- Concept: Time series decomposition and patching
  - Why needed here: To create manageable representations of time series data that can be processed by the LLM architecture
  - Quick check question: Why might time series decomposition methods like STL struggle with non-stationary patterns?

- Concept: Prompt engineering for LLMs
  - Why needed here: To formulate inputs that effectively leverage the LLM's pre-trained capabilities for a new modality (time series)
  - Quick check question: How does the quality of prompt formulation impact the effectiveness of LLM adaptation to new tasks?

## Architecture Onboarding

- Component map: Input → Time series patching → TCTP generation → NNCL support set → Prompt formulation → Frozen LLM layers → Output projection
- Critical path: Time series → Patch embeddings → Linear layer → Embedding Z → NN search in support set → Top-k TCTPs → Concatenated prompt → LLM output → Linear layer → Forecast
- Design tradeoffs: Full fine-tuning vs. selective fine-tuning (computational cost vs. adaptation capability); explicit decomposition vs. learned representations (domain specificity vs. generalization)
- Failure signatures: Poor nearest neighbor matching (TCTPs don't align with time series); contrastive loss doesn't converge; fine-tuned layers overfit to training data
- First 3 experiments:
  1. Test TCTP generation with synthetic word embeddings and known time series patterns to verify clustering works as expected
  2. Validate NNCL nearest neighbor search by checking if similar time series embeddings retrieve semantically similar TCTPs
  3. Compare full fine-tuning vs. selective fine-tuning on a small dataset to quantify parameter efficiency vs. performance trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of NNCL-TLLM compare when using different LLM backbones beyond GPT-2 and Llama-2?
- Basis in paper: [explicit] The paper mentions GPT-2 as the primary backbone and briefly compares with Llama-2, but notes that GPT-2 achieved better performance with fewer parameters
- Why unresolved: The comparison is limited to only two LLM architectures, leaving open questions about the generalizability of NNCL-TLLM to other transformer-based LLMs or newer architectures
- What evidence would resolve it: Systematic experiments using various LLM backbones (e.g., BERT, T5, Claude) with consistent hyperparameters and evaluation metrics would clarify which architectures work best with NNCL-TLLM

### Open Question 2
- Question: How does NNCL-TLLM perform on multivariate time series with strong inter-channel dependencies?
- Basis in paper: [explicit] The paper explicitly states "A limitation that needs to be addressed is incorporating the channel dependencies of multivariate time series" and uses a channel independence strategy
- Why unresolved: The current implementation processes univariate time series independently, which may miss important cross-channel temporal patterns present in many real-world applications
- What evidence would resolve it: Performance comparisons on multivariate datasets with varying degrees of channel correlation, comparing the channel independence approach with joint modeling approaches

### Open Question 3
- Question: What is the impact of different TCTP embedding sizes and support set configurations on model performance?
- Basis in paper: [explicit] The paper mentions conducting experiments with varying values for the number of TCTPs ([1000, 5000, 8000]) and support set sizes (x10, x20, x50, x70, x100 times the number of TCTPs)
- Why unresolved: While the paper explores some parameter variations, the full sensitivity analysis across all combinations and their impact on different forecasting horizons and dataset types remains unexplored
- What evidence would resolve it: Comprehensive ablation studies systematically varying TCTP embedding dimensions, support set sizes, and k-nearest neighbor values across multiple datasets and forecasting tasks would reveal optimal configurations

## Limitations
- Heavy reliance on hyperparameter sensitivity, particularly for patch size and stride, which could significantly impact the learned TCTPs' ability to capture time series patterns
- Limited evaluation on highly non-stationary or multivariate time series data, as the current formulation focuses on univariate series
- Selective fine-tuning approach may constrain performance on highly specialized forecasting tasks where full adaptation is needed

## Confidence

**High Confidence:** The core hypothesis that contrastive learning can align text and time series modalities is well-supported by the general success of contrastive methods in representation learning. The selective fine-tuning approach has strong theoretical grounding in transfer learning literature.

**Medium Confidence:** The specific implementation details of TCTP generation and the effectiveness of neighborhood-aware prototypes in capturing time series characteristics are supported by the paper's experiments but lack extensive ablation studies. The computational efficiency claims are plausible but not thoroughly validated.

**Low Confidence:** The generalizability of the method to highly non-stationary or multivariate time series data is questionable given the current formulation's focus on univariate series and the limited evaluation across diverse time series domains.

## Next Checks

1. **Ablation Study on TCTP Generation:** Conduct controlled experiments varying the number of TCTPs (U) and patch parameters (C, S) to quantify their impact on forecasting accuracy. This would validate whether the learned prototypes genuinely capture time series patterns rather than overfitting to specific configurations.

2. **Cross-Domain Robustness Testing:** Evaluate NNCL-TLLM on time series with significantly different characteristics (e.g., highly non-stationary financial data, seasonal patterns with varying periods, or multivariate series) to assess the method's generalization beyond the current benchmark datasets.

3. **Comparative Analysis with Explicit Decomposition:** Implement a variant using traditional time series decomposition (e.g., STL or wavelet-based methods) instead of learned TCTPs, then compare forecasting performance and computational efficiency to determine whether the learned approach provides meaningful advantages over established methods.