---
ver: rpa2
title: A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning
  Task
arxiv_id: '2402.11917'
source_url: https://arxiv.org/abs/2402.11917
tags:
- token
- node
- position
- attention
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper performs a mechanistic analysis of a transformer trained
  on a symbolic multi-step reasoning task - path finding in randomly generated binary
  trees. The authors reverse-engineer the model's internal mechanisms by analyzing
  its attention patterns and activations.
---

# A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task

## Quick Facts
- arXiv ID: 2402.11917
- Source URL: https://arxiv.org/abs/2402.11917
- Reference count: 40
- Primary result: 99.7% accuracy on pathfinding in randomly generated binary trees

## Executive Summary
This paper performs a mechanistic analysis of a transformer trained on a symbolic multi-step reasoning task - path finding in randomly generated binary trees. The authors reverse-engineer the model's internal mechanisms by analyzing its attention patterns and activations. They identify several key components: edge token concatenation, backward chaining, path merging, and one-step lookahead. The model achieves 99.7% accuracy on a test set of 15,000 unseen trees, demonstrating the ability of transformers to perform deductive reasoning up to a certain depth before resorting to heuristics.

## Method Summary
The authors trained a 6-layer decoder-only transformer (embedding dim 128, 1 head, feedforward dim 512) on pathfinding tasks in randomly generated binary trees (16 nodes each). The input format consists of edge lists followed by goal and root nodes, with output being the path from root to goal. The model was trained with AdamW (lr=1e-3, batch size 64, weight decay 0.01) for less than 24 hours on a single RTX A6000 GPU, achieving 99.7% accuracy on a held-out test set of 15,000 trees.

## Key Results
- The model performs backward chaining, climbing the tree one level per layer using "deduction heads"
- Path merging occurs through parallel backward chaining from multiple register tokens
- One-step lookahead mechanism identifies child nodes and evaluates leaf status for informed guessing
- Linear probes and causal scrubbing validate the identified mechanisms

## Why This Works (Mechanism)

### Mechanism 1: Backward Chaining
- Claim: The model performs backward chaining to climb the tree one level per layer using "deduction heads".
- Mechanism: Attention heads in consecutive layers copy the source node of the current edge into the final token position, effectively moving one level up the tree each layer.
- Core assumption: The edge token concatenation in the first layer creates edge embeddings that can be used by subsequent layers.
- Evidence anchors: Abstract mentions backward chaining; section describes deduction heads copying source tokens; related papers lack direct evidence of backward chaining mechanisms.

### Mechanism 2: Path Merging
- Claim: The model uses path merging by performing backward chaining in parallel from multiple register tokens.
- Mechanism: The model stores subpaths at different token positions through parallel backward chaining, then merges them on the final token position.
- Core assumption: Register tokens can store intermediate subpaths that can be merged later.
- Evidence anchors: Abstract mentions storing intermediate results; section describes parallel backward chaining and merging; related work mentions buffer mechanism but lacks specific evidence.

### Mechanism 3: One-Step Lookahead
- Claim: The model uses one-step lookahead to identify child nodes of the current position and evaluate whether they are leaf nodes.
- Mechanism: Attention heads in the final two layers attend to target nodes of edges where the source is not the current position, increasing logits for non-leaf children.
- Core assumption: The model can represent children and leaf status in its activations.
- Evidence anchors: Abstract mentions one-step lookahead; section describes attention heads in final layers; related papers lack direct evidence of one-step lookahead mechanisms.

## Foundational Learning

- Concept: Edge embeddings and token concatenation
  - Why needed here: The model needs to combine source and target nodes of edges to enable backward chaining
  - Quick check question: If the model sees edge [A][B], what information should be available at position [B] after the first layer?

- Concept: Attention masking and causality
  - Why needed here: The model must only attend to previous tokens to maintain causal consistency in generation
  - Quick check question: Why can't a token attend to tokens that come after it in the sequence?

- Concept: Linear probing for activation analysis
  - Why needed here: To verify that specific information (like edges or children) is encoded in model activations
  - Quick check question: If a linear probe achieves high accuracy on predicting a task from activations, what does this tell us about information encoding?

## Architecture Onboarding

- Component map: Edge token concatenation (L1) -> Backward chaining (L2-L6) -> Path merging (parallel register tokens) -> One-step lookahead (L5-L6) -> Output prediction
- Critical path: Edge token concatenation (L1) → Backward chaining (L2-L6) → Path merging (parallel register tokens) → One-step lookahead (L5-L6) → Output prediction
- Design tradeoffs: Single attention head limits parallelization but simplifies analysis; fixed tree depth (16 nodes) constrains problem complexity; register tokens trade memory for computational depth.
- Failure signatures: Incorrect path predictions when depth > L-1, confusion between similar subtrees, failure to merge subpaths correctly.
- First 3 experiments:
  1. Run causal scrubbing on each attention head to verify backward chaining hypothesis
  2. Perform activation patching on register tokens to test their causal role
  3. Train linear probes on intermediate activations to verify information encoding hypotheses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do transformers' mechanisms for symbolic reasoning in synthetic tasks translate to their performance on natural language reasoning tasks?
- Basis in paper: The paper acknowledges that findings from synthetic settings do not support broader claims about transformers' general reasoning capabilities in natural language.
- Why unresolved: The study's task was specifically designed with a symbolic, structured format, distinct from the complexities of natural language, such as multi-token embeddings and ambiguous representations.
- What evidence would resolve it: Direct mechanistic analysis of transformers on natural language reasoning tasks, comparing attention patterns and activation structures to those found in the synthetic setting.

### Open Question 2
- Question: What role does the order of edge lists in input prompts play in the model's reasoning strategy, and how does the model adapt to different edge list arrangements?
- Basis in paper: The paper mentions that preliminary observations suggest the model uses similar mechanisms with minor variations for different edge list arrangements, but a detailed examination was not conducted.
- Why unresolved: The study focused on a specific edge list order (backward order) and did not explore how the model's reasoning strategy changes with other arrangements.
- What evidence would resolve it: Comparative analysis of the model's attention patterns and activation structures when trained and tested on edge lists in different orders (e.g., forward order, random order).

### Open Question 3
- Question: How does the model's one-step lookahead mechanism interact with the backward chaining process, and what determines the model's choice between these strategies?
- Basis in paper: The paper describes the one-step lookahead as a fallback mechanism used when backward chaining is insufficient, particularly effective on long paths with lower branching factors.
- Why unresolved: The study identifies the existence and basic function of the one-step lookahead but does not detail the decision-making process for switching between strategies or the interaction between them.
- What evidence would resolve it: Detailed analysis of the model's activation patterns and attention scores during reasoning, correlating the use of backward chaining versus one-step lookahead with path characteristics and model confidence levels.

## Limitations
- The mechanistic analysis relies heavily on correlational evidence from linear probes and indirect causal evidence from causal scrubbing and activation patching
- The single attention head per layer architecture may not generalize to multi-head transformers where mechanisms could be distributed across heads
- The edge concatenation mechanism depends critically on the specific tokenization scheme, which is not fully specified in the paper

## Confidence
- **High confidence**: The backward chaining mechanism is well-supported by both correlational and causal evidence, with clear attention patterns showing source token copying across layers.
- **Medium confidence**: Path merging is supported by correlational evidence but the causal evidence is less direct, relying on the assumption that register tokens store intermediate subpaths.
- **Low confidence**: The one-step lookahead mechanism has the weakest evidence, with linear probes suggesting head involvement but limited causal validation of its role in the final decision-making.

## Next Checks
1. Perform layer-wise causal ablation by systematically activation patching on each layer's attention heads to quantify their individual contributions to path prediction accuracy, particularly testing the backward chaining hypothesis at each depth level.

2. Evaluate model performance on trees with depth exceeding the model's capacity (L > 6) and with non-binary branching factors to identify the precise limits of the backward chaining mechanism.

3. Replicate the analysis with a multi-head transformer (4-8 heads per layer) to determine whether the identified mechanisms are distributed across heads or concentrated in specific heads, and how path merging operates in the presence of parallel attention patterns.