---
ver: rpa2
title: Fast, High-Quality and Parameter-Efficient Articulatory Synthesis using Differentiable
  DSP
arxiv_id: '2409.02451'
source_url: https://arxiv.org/abs/2409.02451
tags:
- speech
- synthesis
- ddsp
- articulatory
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a fast, high-quality, and parameter-efficient
  articulatory vocoder based on Differentiable Digital Signal Processing (DDSP). The
  model synthesizes speech from electromagnetic articulography (EMA), F0, and loudness
  using a Harmonic-plus-Noise model with several improvements including cosine harmonics,
  loudness-conditioned FiLM layers, and a post-convolution balancing layer.
---

# Fast, High-Quality and Parameter-Efficient Articulatory Synthesis using Differentiable DSP

## Quick Facts
- arXiv ID: 2409.02451
- Source URL: https://arxiv.org/abs/2409.02451
- Authors: Yisi Liu; Bohan Yu; Drake Lin; Peter Wu; Cheol Jun Cho; Gopala Krishna Anumanchipalli
- Reference count: 0
- The model achieves state-of-the-art performance with 4.9x faster CPU inference and only 0.4M parameters while maintaining high audio quality

## Executive Summary
This paper introduces a parameter-efficient articulatory vocoder based on Differentiable Digital Signal Processing (DDSP) that synthesizes speech from electromagnetic articulography (EMA), F0, and loudness signals. The model addresses the significant parameter efficiency challenge in articulatory synthesis by employing several key innovations: cosine harmonics for reduced complexity, loudness-conditioned FiLM layers for better conditioning, and a post-convolution balancing layer for improved stability. The approach achieves state-of-the-art performance with a word error rate of 6.67% and mean opinion score of 3.74, representing improvements of 1.63% and 0.16 over the previous best method.

## Method Summary
The proposed articulatory vocoder uses a Harmonic-plus-Noise model architecture built on DDSP principles. The system takes EMA signals representing tongue and lip positions, F0 values, and loudness as inputs. The model employs cosine harmonics to reduce the number of parameters needed for harmonic synthesis, loudness-conditioned FiLM (Feature-wise Linear Modulation) layers to effectively incorporate loudness conditioning, and a post-convolution balancing layer to stabilize training. The model is trained end-to-end using a combination of spectral convergence loss, L1 loss, and feature matching loss. Evaluation was conducted on both real EMA data (MNGU0 dataset) and pseudo EMA data (LJ Speech dataset generated via acoustic-to-articulatory inversion), demonstrating robust performance across different input modalities.

## Key Results
- Achieves state-of-the-art WER of 6.67% and MOS of 3.74, improving over previous best by 1.63% and 0.16 respectively
- 4.9x faster CPU inference compared to previous articulatory synthesis methods
- Requires only 0.4M parameters compared to 9M parameters in previous state-of-the-art
- Maintains high-quality audio synthesis across both real EMA and pseudo EMA inputs

## Why This Works (Mechanism)
The model's effectiveness stems from three key design choices that address fundamental challenges in articulatory synthesis. First, cosine harmonics reduce parameter count by exploiting the smooth, periodic nature of speech harmonics while maintaining audio quality. Second, loudness-conditioned FiLM layers provide effective conditioning of the harmonic and noise components by modulating feature maps based on loudness, enabling better control over speech dynamics. Third, the post-convolution balancing layer stabilizes the training process by addressing the instability that arises when combining multiple DDSP components, ensuring consistent performance across different input conditions.

## Foundational Learning
- Electromagnetic Articulography (EMA): Measures tongue and lip positions during speech using magnetic sensors. Why needed: Provides direct articulatory measurements as input to the model rather than acoustic features. Quick check: Verify that EMA data contains synchronized time-aligned articulatory movements with corresponding speech.

- Differentiable Digital Signal Processing (DDSP): Combines deep learning with interpretable DSP components that are fully differentiable. Why needed: Enables end-to-end training while maintaining interpretability and efficiency of traditional signal processing. Quick check: Confirm that all components (harmonic synthesis, noise filtering) are differentiable and can be trained with gradient descent.

- Acoustic-to-Articulatory Inversion (AAI): Converts acoustic speech signals to pseudo EMA measurements. Why needed: Allows training on large speech datasets without requiring actual articulography recordings. Quick check: Validate that pseudo EMA generated by AAI maintains reasonable articulatory patterns and correlates with actual speech content.

- Feature-wise Linear Modulation (FiLM): Conditioning technique that modulates feature maps using learned scale and shift parameters. Why needed: Enables effective incorporation of loudness information into the harmonic and noise synthesis processes. Quick check: Verify that FiLM layers successfully modulate features based on loudness variations in the input.

## Architecture Onboarding

Component map: EMA + F0 + Loudness -> FiLM Layers -> Harmonic Synthesis (cosine) + Noise Synthesis -> Post-convolution balancing -> Audio Output

Critical path: The forward pass flows from input conditioning (EMA, F0, loudness) through the FiLM layers that modulate both harmonic and noise synthesis components, followed by the post-convolution balancing layer that combines these elements into the final audio output. The harmonic synthesis uses cosine-based representation for efficiency, while the noise component is filtered to match the desired spectral characteristics.

Design tradeoffs: The use of cosine harmonics instead of traditional sinusoidal representations trades some modeling flexibility for significant parameter reduction (approximately 4x fewer parameters). The FiLM conditioning approach requires additional parameters for the conditioning networks but provides more effective control over speech dynamics compared to direct concatenation. The post-convolution balancing layer adds complexity but is essential for training stability when combining multiple DDSP components.

Failure signatures: Training instability manifests as exploding gradients or NaN values in the harmonic synthesis when the post-convolution balancing layer is not properly initialized. Poor loudness conditioning results in flat or monotonic speech output regardless of input loudness variations. Excessive harmonic parameter reduction leads to audible artifacts in high-frequency regions of the synthesized speech.

First experiments:
1. Train the model with only harmonic synthesis (no noise component) to isolate the effectiveness of cosine harmonics
2. Remove FiLM layers and use direct concatenation for loudness conditioning to measure the impact of the conditioning approach
3. Test with varying numbers of harmonics (K=25, K=50, K=100) to find the optimal tradeoff between quality and parameter efficiency

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How do the model's performance metrics change when using real EMA data versus pseudo EMA data from acoustic-to-articulatory inversion (AAI)?
- Basis in paper: The paper notes that the LJ Speech dataset uses pseudo EMA labels generated by AAI, and the model achieves comparable performance to the MNGU0 dataset with real EMA data.
- Why unresolved: The paper does not provide a direct comparison between real EMA and pseudo EMA data, nor does it discuss the potential degradation in performance due to the use of pseudo EMA.
- What evidence would resolve it: A direct comparison of the model's performance metrics (WER, MOS, UTMOS) using real EMA data versus pseudo EMA data on the same dataset would clarify the impact of using pseudo EMA.

### Open Question 2
- Question: What is the effect of increasing the number of harmonics (K) beyond the tested value on the model's performance and parameter efficiency?
- Basis in paper: The paper mentions that the model uses K = 50 harmonics and achieves high performance, but does not explore the impact of using more harmonics on performance and parameter efficiency.
- Why unresolved: The paper does not investigate the trade-off between the number of harmonics and the model's performance or parameter efficiency, leaving the optimal number of harmonics unclear.
- What evidence would resolve it: Experiments varying the number of harmonics (K) and measuring the resulting changes in performance metrics (WER, MOS, UTMOS) and parameter efficiency would provide insights into the optimal number of harmonics.

### Open Question 3
- Question: How does the model's performance vary across different speakers and speaking styles when trained on a diverse multi-speaker dataset?
- Basis in paper: The paper discusses the model's performance on single-speaker datasets (MNGU0 and LJ Speech) but does not address multi-speaker capabilities or performance across different speaking styles.
- Why unresolved: The paper does not explore the model's generalization to different speakers or speaking styles, which is crucial for real-world applications where variability in speech is common.
- What evidence would resolve it: Training and evaluating the model on a multi-speaker dataset with diverse speaking styles and measuring performance metrics (WER, MOS, UTMOS) across different speakers would demonstrate the model's robustness and generalization capabilities.

## Limitations
- Evaluated primarily on single-speaker datasets, limiting assessment of cross-speaker generalization capabilities
- Uses pseudo EMA data for one of the main evaluation datasets, raising questions about performance with real articulatory measurements
- Limited comparison with other articulatory synthesis approaches beyond one baseline method
- Performance evaluation conducted on relatively small test sets without comprehensive speaker variability testing

## Confidence
- Parameter efficiency improvements: High - Direct measurements show 22x reduction in parameters
- Speed improvements: High - CPU inference measurements demonstrate 4.9x speedup
- Quality improvements: Medium - MOS improvements shown but limited evaluation scope
- Cross-speaker generalization: Low - Not evaluated on multi-speaker datasets
- Real vs pseudo EMA performance: Low - No direct comparison provided

## Next Checks
1. Test speaker generalization by evaluating the model on multiple speakers from different articulatory datasets to assess cross-speaker performance
2. Conduct ablation studies to isolate the contribution of each proposed component (cosine harmonics, FiLM layers, post-convolution balancing) to the overall performance
3. Compare against additional articulatory synthesis baselines beyond VOCONART, including non-DDSP approaches, to establish broader state-of-the-art positioning