---
ver: rpa2
title: 'Tailor: Size Recommendations for High-End Fashion Marketplaces'
arxiv_id: '2401.01978'
source_url: https://arxiv.org/abs/2401.01978
tags:
- size
- event
- history
- fashion
- product
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of providing accurate size recommendations
  in high-end fashion marketplaces, where incorrect sizing leads to increased returns
  and customer dissatisfaction. The authors propose "Tailor," a sequence classification
  approach that leverages both implicit user signals (Add2Bag interactions) and explicit
  signals (ReturnReason) to improve size recommendation accuracy.
---

# Tailor: Size Recommendations for High-End Fashion Marketplaces

## Quick Facts
- arXiv ID: 2401.01978
- Source URL: https://arxiv.org/abs/2401.01978
- Reference count: 21
- Key result: SSP-Attention achieves 45.7% improvement in size recommendation accuracy over baseline

## Executive Summary
This paper addresses the critical problem of size recommendation accuracy in high-end fashion marketplaces, where incorrect sizing leads to high return rates and customer dissatisfaction. The authors propose Tailor, a sequence classification approach that leverages both implicit user signals (Add2Bag interactions) and explicit signals (ReturnReason) to improve size recommendation accuracy. Their approach uses two models: SSP-LSTM, which employs Bi-directional LSTMs, and SSP-Attention, which leverages an attention mechanism. The SSP-Attention model achieves a 45.7% improvement in accuracy over the baseline SFNet model, and the use of Add2Bag interactions increases user coverage by 24.5% compared to using only Orders.

## Method Summary
The Tailor approach frames size recommendation as a sequence classification problem, processing historical user interactions as event sequences to predict the correct size for new purchases. The method uses two models: SSP-LSTM with Bi-directional LSTMs for capturing sequential patterns, and SSP-Attention with Transformer-based attention mechanisms for focusing on relevant historical events. Both models process user event histories (Order and Add2Bag interactions) along with product metadata (product_id, brand_id, category_id, scale_id) to predict size position labels. The models are trained on a large dataset of approximately 3.2M purchases from 400k products and 750k users, using Adam optimizer with early stopping for 25 epochs, achieving real-time latency under 15ms suitable for production deployment.

## Key Results
- SSP-Attention model achieves 45.7% improvement in accuracy over SFNet baseline
- Add2Bag interactions increase user coverage by 24.5% compared to Orders-only approach
- Models demonstrate low latency (under 15ms) suitable for real-time recommendations

## Why This Works (Mechanism)
The approach works by leveraging sequential patterns in user behavior and product characteristics to predict optimal sizing. The attention mechanism in SSP-Attention allows the model to focus on the most relevant historical interactions when making size predictions, while the inclusion of Add2Bag data captures user intent before purchase, providing richer context than purchase-only signals. The combination of implicit signals (user browsing and adding to cart behavior) with explicit signals (return reasons) creates a more comprehensive understanding of sizing preferences and fit issues.

## Foundational Learning
- **Sequence classification**: Needed to process user interaction histories as ordered sequences; check by verifying model can handle variable-length input sequences
- **Attention mechanisms**: Required to focus on relevant historical events; check by visualizing attention weights on test sequences
- **Bi-directional LSTMs**: Used to capture both past and future context in sequences; check by comparing uni-directional vs bi-directional performance
- **Embedding layers**: Transform categorical features into continuous representations; check by verifying embedding dimensions match input cardinality
- **Transformer blocks**: Process sequential data with self-attention; check by validating positional encoding implementation
- **Multi-task learning**: Implicitly handled through combined signal types; check by measuring contribution of each signal type through ablation

## Architecture Onboarding

Component map: User event sequences -> Embedding layers -> Transformer/ Bi-LSTM blocks -> Attention mechanism -> Softmax output

Critical path: Event sequence input → Embedding transformation → Sequential processing (Transformer/Bi-LSTM) → Attention-weighted aggregation → Size prediction

Design tradeoffs: Attention mechanism vs pure sequential processing balances interpretability with computational efficiency; Add2Bag vs Orders-only data increases coverage but may introduce noisier signals

Failure signatures: Poor performance indicates insufficient event history length, incorrect event types used, or suboptimal hyperparameter tuning

First experiments:
1. Train SSP-LSTM baseline with 10-event history to establish baseline performance
2. Implement attention mechanism in SSP-Attention and compare with LSTM-only approach
3. Test different history lengths (5, 10, 20 events) to find optimal sequence length

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- Limited to a single marketplace dataset, raising questions about generalizability across different fashion retail contexts
- No discussion of computational resource requirements or scalability considerations for larger user bases
- Validation period limited to one year of data without testing on temporally distinct periods

## Confidence
High confidence for core methodological claims (accuracy improvement, latency metrics)
Medium confidence for implementation-specific details (exact preprocessing steps, hyperparameter values)

## Next Checks
1. Verify event sequence construction by testing different history lengths (5, 10, 20 events) to confirm the 20-event threshold is optimal
2. Conduct ablation studies removing Add2Bag interactions to quantify their specific contribution beyond Orders-only baseline
3. Evaluate model performance on a temporally distinct test period to assess temporal generalization and potential data leakage concerns