---
ver: rpa2
title: 'MiMoTable: A Multi-scale Spreadsheet Benchmark with Meta Operations for Table
  Reasoning'
arxiv_id: '2412.11711'
source_url: https://arxiv.org/abs/2412.11711
tags:
- table
- meta
- operations
- question
- difficulty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MiMoTable, a multi-scale spreadsheet benchmark
  designed to evaluate large language models' table reasoning capabilities in real-world
  scenarios. Unlike existing benchmarks that focus on simple tables with single-row/column
  headers, MiMoTable includes 428 spreadsheets covering seven domains and features
  complex headers, multiple sheets, and multiple tables within sheets.
---

# MiMoTable: A Multi-scale Spreadsheet Benchmark with Meta Operations for Table Reasoning

## Quick Facts
- arXiv ID: 2412.11711
- Source URL: https://arxiv.org/abs/2412.11711
- Authors: Zheng Li; Yang Du; Mao Zheng; Mingyang Song
- Reference count: 17
- Claude-3.5-Sonnet achieves 77.4% accuracy on MiMoTable, significantly lower than its performance on simpler benchmarks

## Executive Summary
MiMoTable is a novel multi-scale spreadsheet benchmark designed to evaluate large language models' table reasoning capabilities in real-world scenarios. Unlike existing benchmarks that focus on simple tables with single-row/column headers, MiMoTable includes 428 spreadsheets covering seven domains with complex headers, multiple sheets, and multiple tables within sheets. The benchmark introduces a new criterion using six meta operations (Lookup, Edit, Compare, Calculate, Visualize, and Reasoning) to assess question difficulty and provides a systematic way to grade existing benchmarks based on required capabilities.

## Method Summary
The authors collected 428 multi-scale spreadsheets from publicly available sources, covering seven domains with diverse table structures including simple and complex headers, single and multiple sheets, and multiple tables within sheets. They generated 1,719 question-answer pairs using GPT-4o and evaluated various LLMs (open-source, closed-source, and tabular) on the benchmark. The evaluation used accuracy as the primary metric and introduced a difficulty scoring system based on six meta operations that break down table reasoning tasks into Lookup, Edit, Compare, Calculate, Visualize, and Reasoning components. The benchmark also leverages GPT-4o with a code interpreter plugin for precise calculations and comparisons.

## Key Results
- Claude-3.5-Sonnet achieves 77.4% accuracy on MiMoTable, significantly lower than its performance on simpler benchmarks
- LLM performance decreases as table difficulty increases, validating the effectiveness of the meta operations difficulty grading
- Code-based spreadsheet processing (GPT-4o-CI) outperforms text-based approaches for Calculate and Compare operations on simple and medium difficulty tables

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MiMoTable exposes LLMs' inability to handle hierarchical table headers and multiple similar tables
- Mechanism: Real-world spreadsheets contain complex hierarchical headers and multiple sheets with similar table structures, which break the flat, single-header assumptions of existing benchmarks
- Core assumption: Hierarchical headers lose their relational meaning when converted to markdown text format
- Evidence anchors:
  - [abstract] "Unlike existing benchmarks that focus on simple tables with single-row/column headers, MiMoTable includes 428 spreadsheets covering seven domains and features complex headers, multiple sheets, and multiple tables within sheets"
  - [section 2.1] "A simple header refers to a single-row or single-column header, while all others are considered complex headers"
  - [corpus] Found 25 related papers; average neighbor FMR=0.478 indicates moderate relevance to spreadsheet reasoning literature
- Break Condition: If models can parse nested header structures or maintain sheet context across similar tables, this mechanism weakens

### Mechanism 2
- Claim: Meta operations provide granular difficulty grading that correlates with actual model performance
- Mechanism: By decomposing table reasoning into six meta operations with difficulty scores, benchmarks can be objectively compared based on required capabilities rather than task labels
- Core assumption: Difficulty scores calculated from meta operations (1-4 range) accurately predict model performance degradation
- Evidence anchors:
  - [abstract] "Experiments have shown that the performance of LLMs decreases as the difficulty of benchmarks increases, thereby proving the effectiveness of our proposed new criterion"
  - [section 2.2] "We define the difficulty score of a question qi as follows: qsi = msqi + (KiP1 si âˆ’ msqi)/Mmsqi"
  - [section 3.2] "The x-axis represents the difficulty of the benchmarks graded by meta operations, and the y-axis shows the accuracy of the LLMs on these benchmarks"
- Break Condition: If models show non-monotonic performance patterns across difficulty scores, the grading mechanism breaks

### Mechanism 3
- Claim: Code-based spreadsheet processing (GPT-4o-CI) outperforms text-based approaches for certain operations
- Mechanism: Uploading actual spreadsheet files enables models to execute Python code in a sandbox, preserving structural relationships and enabling precise calculations
- Core assumption: Code interpreter plugins can accurately parse spreadsheet formats and execute operations without errors
- Evidence anchors:
  - [section 3.2] "GPT-4o-CI performs better than GPT-4o-TXT in Calculate and Compare when the table difficulty is simple and medium"
  - [section 2.3] "we leverage GPT-4o with the code interpreter plugin to get initial answers"
  - [corpus] Moderate FMR scores suggest this approach is novel compared to existing spreadsheet benchmarks
- Break Condition: If code generation fails on complex spreadsheets or introduces more errors than text-based methods

## Foundational Learning

- Concept: Hierarchical data structures
  - Why needed here: Understanding complex table headers requires recognizing parent-child relationships between header cells
  - Quick check question: Can you explain how a multi-level header like "Year | Q1 | Q2 | Q3 | Q4" with nested subcategories would be represented in markdown?

- Concept: Meta operations decomposition
  - Why needed here: Breaking down table reasoning into Lookup, Edit, Calculate, Compare, Visualize, and Reasoning operations enables systematic evaluation
  - Quick check question: Given a question like "Which product had the highest sales in Q3?", what meta operations would be required and in what order?

- Concept: Difficulty scaling in multi-step problems
  - Why needed here: Calculating difficulty scores requires understanding how to weight multiple operations and their combinations
  - Quick check question: If a question requires Lookup(1) + Calculate(2) + Reasoning(3), what would be its difficulty score using the provided formula?

## Architecture Onboarding

- Component map: Table collection -> Markdown extraction -> Meta operation classification -> Question generation -> Answer generation (code interpreter) -> Human validation -> Performance evaluation
- Critical path: Table collection -> Markdown extraction -> Question generation -> Answer generation -> Human validation
- Design tradeoffs: Using markdown loses structural information vs. maintaining original spreadsheet format; using code interpreter adds complexity but enables precise operations
- Failure signatures: Poor performance on hierarchical headers indicates markdown conversion issues; inconsistent code execution suggests plugin limitations
- First 3 experiments:
  1. Test markdown conversion accuracy on spreadsheets with 2-3 level hierarchical headers
  2. Validate meta operation classification accuracy on sample questions across all six operation types
  3. Compare GPT-4o-TXT vs GPT-4o-CI performance on Calculate and Compare operations using simple spreadsheets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of the proposed meta operations change when used for supervised fine-tuning (SFT) of models?
- Basis in paper: [inferred] The paper mentions that the effectiveness of meta operations was not validated through SFT, and suggests this as future work.
- Why unresolved: The paper does not provide experimental results or analysis on the impact of SFT using meta operations, leaving uncertainty about their practical utility in model training.
- What evidence would resolve it: Conducting SFT experiments with and without meta operations and comparing model performance on table reasoning tasks would clarify their effectiveness.

### Open Question 2
- Question: How do different hyperparameters affect the performance of various models on the MiMoTable benchmark?
- Basis in paper: [inferred] The paper states that default hyperparameters were used for all models without optimization or adaptation for different models.
- Why unresolved: The lack of hyperparameter tuning means the reported results may not reflect the optimal performance of the models, limiting the benchmark's utility for comparing model capabilities.
- What evidence would resolve it: Systematically testing and optimizing hyperparameters for each model on the MiMoTable benchmark would provide insights into their true performance potential.

### Open Question 3
- Question: How does the performance of models on long-context table reasoning tasks compare to their performance on the MiMoTable benchmark?
- Basis in paper: [inferred] The paper suggests developing long-context table reasoning benchmarks and studying in-context learning for table reasoning as future work, implying a gap in current evaluations.
- Why unresolved: The MiMoTable benchmark does not specifically address long-context scenarios, leaving uncertainty about model performance in more complex, real-world table reasoning tasks.
- What evidence would resolve it: Creating and evaluating models on a long-context table reasoning benchmark would reveal their ability to handle extended and intricate table data.

## Limitations

- The benchmark's reliance on markdown conversion may not fully capture the structural complexity of real-world spreadsheets, particularly for hierarchical headers and multi-sheet relationships
- The human validation process may introduce subjective biases in difficulty classification and meta operation assignment
- Claims about code-based processing superiority are based on limited comparisons and may not generalize across all spreadsheet types and operations

## Confidence

- **High Confidence:** The observation that Claude-3.5-Sonnet achieves 77.4% accuracy on MiMoTable (significantly lower than simpler benchmarks) is empirically demonstrated and robust across different LLM types
- **Medium Confidence:** The effectiveness of meta operations for difficulty grading is supported by the correlation between difficulty scores and performance degradation, though the scoring formula's sensitivity to different operation combinations needs further validation
- **Low Confidence:** Claims about code-based processing superiority (GPT-4o-CI vs GPT-4o-TXT) are based on limited comparisons and may not generalize across all spreadsheet types and operations

## Next Checks

1. Test the markdown conversion accuracy on spreadsheets with complex hierarchical headers (3+ levels) to quantify information loss compared to original spreadsheet formats
2. Validate the consistency of meta operation classification by having multiple annotators classify the same set of questions and measuring inter-annotator agreement
3. Compare performance on spreadsheets with multiple similar tables across different sheets to quantify the impact of sheet context on LLM reasoning accuracy