---
ver: rpa2
title: 'Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive
  Decoding and Distillation'
arxiv_id: '2402.14874'
source_url: https://arxiv.org/abs/2402.14874
tags:
- reasoning
- contrastive
- answer
- amateur
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Distillation Contrastive Decoding (DCD),
  a method that improves the reasoning capabilities of large language models (LLMs)
  during inference. Unlike previous approaches that require a separate smaller amateur
  model, DCD leverages distillation techniques such as Dropout and Quantization to
  simulate an amateur model internally, thereby eliminating the need for additional
  models and reducing memory usage.
---

# Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation

## Quick Facts
- **arXiv ID**: 2402.14874
- **Source URL**: https://arxiv.org/abs/2402.14874
- **Reference count**: 39
- **Primary result**: DCD improves LLM reasoning by 3.79% on GSM8K and 5.9% on StrategyQA by simulating amateur models internally

## Executive Summary
This paper introduces Distillation Contrastive Decoding (DCD), a method that improves the reasoning capabilities of large language models during inference. Unlike previous contrastive decoding approaches that require separate amateur models, DCD leverages distillation techniques like Dropout and Quantization to simulate an amateur model internally. By integrating contrastive chain-of-thought prompts with these distillation methods, DCD addresses key limitations of Contrastive Decoding while eliminating the need for additional models and reducing memory usage. Experiments on GSM8K and StrategyQA datasets demonstrate significant performance improvements over existing methods.

## Method Summary
DCD uses Contrastive Chain-of-thought Prompting combined with distillation techniques to simulate an amateur model internally. The method applies Dropout to attention weights or uses Quantization to degrade the model's reasoning capabilities in a controlled manner, creating the necessary contrast with the expert model. This internal amateur model generation eliminates the need for external smaller models while maintaining the contrastive decoding framework. The approach uses both valid and invalid CoT demonstrations to guide the model through step-by-step reasoning, minimizing logical errors.

## Key Results
- DCD achieves up to 3.79% improvement on GSM8K compared to existing methods
- DCD shows 5.9% improvement on StrategyQA dataset
- The method works with various model architectures including Llama2-7B, Mistral-7B, and DeepSeek-7B

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Dropout applied to attention weights introduces controlled variability that simulates amateur reasoning errors
- **Core assumption**: Disrupting attention mechanism produces systematic reasoning errors similar to smaller models
- **Evidence anchors**: Section description of dropout implementation; weak corpus evidence
- **Break condition**: Dropout rate >0.4 or <0.2 weakens contrast

### Mechanism 2
- **Claim**: Quantization reduces precision in deterministic way, degrading reasoning capabilities
- **Core assumption**: Lower precision arithmetic produces consistently weaker reasoning outputs
- **Evidence anchors**: Section on quantization implementation; weak corpus evidence
- **Break condition**: Too aggressive quantization degrades outputs beyond usefulness

### Mechanism 3
- **Claim**: Contrastive Chain-of-thought prompting with valid/invalid demonstrations enables learning from both success and failure
- **Core assumption**: Models can effectively learn from contrasting examples
- **Evidence anchors**: Section on CP formulation; human learning analogy
- **Break condition**: Too many invalid demonstrations cause confusion

## Foundational Learning

- **Concept: Contrastive Decoding**
  - **Why needed**: Essential to understand how CD works with expert/amateur models
  - **Quick check**: What is the mathematical formula for selecting next token in contrastive decoding?

- **Concept: Knowledge Distillation**
  - **Why needed**: DCD uses dropout and quantization as distillation techniques
  - **Quick check**: What is the difference between dropout and quantization as distillation techniques?

- **Concept: Chain-of-Thought Reasoning**
  - **Why needed**: DCD builds on CoT prompting and intermediate reasoning steps
  - **Quick check**: What are "bridging objects" in chain-of-thought reasoning?

## Architecture Onboarding

- **Component map**: Expert model (full precision) -> Amateur model (distilled via dropout/quantization) -> Contrastive decoder (combines logits) -> Token selection -> Output
- **Critical path**: Query → Expert model forward pass → Distilled amateur model forward pass → Logit combination → Token selection → Output
- **Design tradeoffs**: Dropout provides variability but is nondeterministic; quantization is deterministic but may be too aggressive; synthetic prompts are flexible but require external LLM
- **Failure signatures**: Poor performance when dropout rate mis-tuned; model confusion with too many invalid prompts; quantization causing excessive degradation
- **First 3 experiments**:
  1. Test different dropout rates (0.1 to 0.5) on small GSM8K subset
  2. Compare quantization methods (4-bit GPTQ vs 4-bit AWQ vs 8-bit) on same subset
  3. Validate DCD works with different base models (Llama2-7B, Mistral-7B, DeepSeek-7B) on GSM8K

## Open Questions the Paper Calls Out

- **Open Question 1**: How does DCD perform on more complex reasoning tasks beyond arithmetic and commonsense reasoning?
  - **Basis**: Study focuses on GSM8K and StrategyQA, not complex multi-step reasoning
  - **Why unresolved**: Limited to arithmetic and commonsense reasoning tasks
  - **What evidence resolves**: Experiments on datasets involving logical inference or scientific problem-solving

- **Open Question 2**: Can DCD be effectively scaled and adapted to larger, tuned models?
  - **Basis**: Paper mentions potential for larger models but hasn't explored
  - **Why unresolved**: Study focuses on base models only
  - **What evidence resolves**: Testing on instruction-tuned models like GPT-4 or Llama 2-Chat

- **Open Question 3**: What is the optimal dropout rate for DCD across different architectures and tasks?
  - **Basis**: Study finds moderate dropout optimal but provides no universal solution
  - **Why unresolved**: Impact may differ across model architectures and reasoning tasks
  - **What evidence resolves**: Comprehensive study across different model sizes, architectures, and reasoning tasks

## Limitations

- Dropout and quantization effectiveness is based on minimal empirical validation with limited parameter space exploration
- Claims of scalability to larger models are weak since only 7B parameter models were tested
- The optimal dropout rate is not established as universal, varying across datasets and potentially architectures

## Confidence

- **High confidence**: Contrastive decoding improvement over baseline CoT prompting is well-established and reproducible
- **Medium confidence**: Implementation choices for dropout and quantization are theoretically justified but lack comprehensive ablation studies
- **Low confidence**: Scalability claims to stronger base models are particularly weak due to limited testing on only 7B models

## Next Checks

1. Conduct systematic grid search across dropout rates (0.1, 0.2, 0.3, 0.4, 0.5) on both GSM8K and StrategyQA to identify optimal range
2. Compare different quantization approaches (GPTQ vs AWQ vs 8-bit vs 4-bit) and bit-widths on held-out validation set
3. Test DCD on larger models (13B, 33B, 70B) and different architectures to verify claimed scalability and architecture independence