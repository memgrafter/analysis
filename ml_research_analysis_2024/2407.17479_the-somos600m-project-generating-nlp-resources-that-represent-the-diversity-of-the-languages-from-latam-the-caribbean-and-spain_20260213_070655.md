---
ver: rpa2
title: 'The #Somos600M Project: Generating NLP resources that represent the diversity
  of the languages from LATAM, the Caribbean, and Spain'
arxiv_id: '2407.17479'
source_url: https://arxiv.org/abs/2407.17479
tags:
- spanish
- language
- corpus
- espa
- para
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Somos600M Project addressed the lack of Spanish and co-official
  language instruction datasets and evaluation leaderboards for instruction-tuning
  large language models (LLMs). The team organized a hackathon, dataset collection
  campaign, and translation validation effort to create open-source resources.
---

# The #Somos600M Project: Generating NLP resources that represent the diversity of the languages from LATAM, the Caribbean, and Spain

## Quick Facts
- arXiv ID: 2407.17479
- Source URL: https://arxiv.org/abs/2407.17479
- Authors: María Grandury
- Reference count: 40
- Key outcome: Created open-source instruction datasets and evaluation leaderboards for Spanish, Catalan, and Euskera through hackathon, dataset collection, and translation validation

## Executive Summary
The #Somos600M Project addresses the critical shortage of Spanish and co-official language instruction datasets and evaluation leaderboards for instruction-tuning large language models. Through a community-driven effort involving a hackathon, dataset collection campaign, and translation validation, the project generated 2.33 million instruction examples and collected 22 evaluation datasets. The resulting resources provide the first comprehensive open-source instruction dataset and generative LLM leaderboard for Spanish, Catalan, and Euskera, advancing NLP development for these underrepresented languages.

## Method Summary
The project employed a multi-pronged approach: organizing a hackathon to generate synthetic instruction datasets across diverse topics, launching a dataset collection campaign to transform existing NLP resources into question-answer pairs, and conducting community-driven translation and validation of English evaluation benchmarks. The effort involved 18 hackathon teams, native speaker validation of translations, and collaboration with Hugging Face and Argilla to create an open leaderboard system for evaluating LLMs in the target languages.

## Key Results
- Generated 2.33 million instruction examples covering diverse topics and Spanish dialects
- Collected 22 evaluation datasets including validated translations of popular English benchmarks
- Established the first open generative LLM leaderboard for Spanish, Catalan, and Euskera

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Open instruction datasets enable zero-shot capabilities for LLMs in underrepresented languages.
- Mechanism: Fine-tuning LLMs with natural language instructions improves their ability to follow prompts without task-specific examples, expanding use cases like RAG and chat interactions.
- Core assumption: Synthetic instruction generation can produce diverse, high-quality examples covering multiple Spanish dialects and co-official languages.
- Evidence anchors:
  - [abstract] "This fine-tuning improves their zero-shot capabilities (Wei et al., 2021) and adaptability, relevant for AI alignment, chat-like interactions and retrieval augmented generation (RAG) applications."
  - [section 2] "Since 2020, we have seen a trend to fine-tune language models using English natural language instructions... there are more than 2,000 'instruct' datasets in the Hugging Face Hub. To the best of our knowledge, in total in our languages there are 227k instructions in Catalan... and only 14k originally created in Spanish."
- Break condition: If synthetic data fails to cover low-resource languages or contains biases that harm downstream model performance.

### Mechanism 2
- Claim: Community-driven validation improves translation quality for evaluation datasets.
- Mechanism: Native speakers review machine-translated benchmarks to correct errors and ensure cultural/linguistic appropriateness, creating reliable evaluation resources.
- Core assumption: Native speaker validation can scale effectively with volunteer participation and clear annotation guidelines.
- Evidence anchors:
  - [abstract] "In collaboration with Hugging Face and Argilla, we launched a community effort for native Spanish speakers to validate these translations."
  - [section 5] "In the validation of the Okapi translations, a total of 61 persons participated... Moreover, with the support of 37 persons, 100% of DIBT's prompts were validated."
- Break condition: If volunteer participation drops significantly or annotation guidelines are unclear, leading to inconsistent validation quality.

### Mechanism 3
- Claim: Interdisciplinary hackathons generate diverse, high-quality instruction datasets.
- Mechanism: Bringing together technical and linguistic experts fosters creativity in dataset topics and ensures linguistic diversity across Spanish dialects and co-official languages.
- Core assumption: Diverse participant backgrounds lead to broader topic coverage and higher data quality.
- Evidence anchors:
  - [section 3.1] "The hackathon was open to everyone... and targeted individuals with both technical and linguistic backgrounds, encouraging interdisciplinary teams."
  - [section 4.1] "We highlight the high number of countries represented in the chosen topics (e.g., Colombian Aeronautical Regulation, Refugee Legal Assistance, Peruvian Constitution, international traditional recipes), as well as the project on Guarani culture."
- Break condition: If teams lack either technical skills or linguistic knowledge, resulting in low-quality or linguistically inappropriate datasets.

## Foundational Learning

- Concept: Zero-shot learning in NLP
  - Why needed here: Understanding how instruction-tuning enables models to perform tasks without task-specific examples is crucial for grasping the project's impact.
  - Quick check question: How does instruction-tuning differ from traditional supervised fine-tuning, and why is it particularly valuable for low-resource languages?

- Concept: Synthetic data generation
  - Why needed here: The project relies heavily on generating instruction datasets synthetically due to the scarcity of existing resources.
  - Quick check question: What are the key challenges in generating high-quality synthetic instruction data, and how can they be addressed?

- Concept: Community annotation and validation
  - Why needed here: The project uses community efforts to validate translations and improve dataset quality, making understanding this process essential.
  - Quick check question: What are the best practices for organizing community annotation efforts, and how can you ensure consistent quality across volunteers?

## Architecture Onboarding

- Component map: Hackathon platform -> Synthetic data generation tools (transformers, distilabel) -> Argilla annotation platform -> Leaderboard system -> Community volunteer coordination
- Critical path: Hackathon → Dataset generation → Validation → Leaderboard integration → Model fine-tuning
- Design tradeoffs: Balancing dataset diversity with quality, managing volunteer participation vs. professional annotation, open access vs. computational resource requirements
- Failure signatures: Low-quality synthetic data, inconsistent validation results, incomplete language coverage, leaderboard evaluation bias
- First 3 experiments:
  1. Generate a small synthetic instruction dataset for a specific Spanish dialect and validate with native speakers
  2. Translate and validate a subset of English benchmarks, measuring annotation consistency
  3. Create a prototype leaderboard using existing validated datasets to test evaluation pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of instruction-tuned LLMs trained on the #Somos600M dataset compare to models trained on machine-translated English instruction datasets when evaluated on Spanish and co-official language tasks?
- Basis in paper: [explicit] The paper states that Spanish-speaking communities are forced to use error-prone machine-translated English datasets or manually validate them, highlighting the need for native instruction datasets.
- Why unresolved: The paper presents the creation of the first native Spanish instruction dataset but does not provide any performance comparisons between models trained on this dataset versus machine-translated English datasets.
- What evidence would resolve it: Empirical results comparing zero-shot and few-shot performance of LLMs fine-tuned on the #Somos600M dataset versus machine-translated English datasets across various Spanish and co-official language tasks.

### Open Question 2
- Question: What is the impact of dialectal and co-official language representation in the #Somos600M instruction dataset on the performance of LLMs across different Spanish varieties and languages?
- Basis in paper: [explicit] The paper emphasizes the importance of representing the diversity of Spanish dialects and co-official languages in the instruction dataset to improve LLM performance.
- Why unresolved: While the dataset includes diverse topics and languages, the paper does not analyze how this representation affects model performance across different dialects and languages.
- What evidence would resolve it: Detailed analysis of LLM performance on tasks specific to different Spanish dialects and co-official languages, comparing models trained on the diverse #Somos600M dataset versus those trained on more homogeneous datasets.

### Open Question 3
- Question: How effective are the current evaluation datasets in the #Somos600M leaderboard at measuring the linguistic proficiency and ethical aspects of generative LLMs in Spanish and co-official languages?
- Basis in paper: [explicit] The paper mentions plans to expand the leaderboard to include evaluations of ethical aspects (e.g., biases, hate speech) and linguistic aspects (e.g., language variety adequacy).
- Why unresolved: The first version of the leaderboard includes various tasks but does not yet include comprehensive evaluations of ethical and linguistic aspects as mentioned in the paper's future plans.
- What evidence would resolve it: Results from the expanded leaderboard including specific metrics and analyses of LLM performance on ethical and linguistic evaluation tasks in Spanish and co-official languages.

## Limitations

- Synthetic data generation quality and potential biases are not thoroughly documented
- Community validation consistency may vary due to volunteer participation
- Evaluation leaderboard coverage may not represent all Spanish dialects across LATAM regions

## Confidence

**High Confidence**: The successful creation of 2.33 million instruction examples through the hackathon, and the establishment of a community-driven validation process for evaluation datasets. These claims are directly supported by quantitative metrics and participation data provided in the paper.

**Medium Confidence**: The claim that these resources will significantly advance LLM development for underrepresented languages. While the resource creation is documented, the actual impact on model performance and downstream applications requires further empirical validation.

**Low Confidence**: The assertion that the synthetic data generation and community validation processes are scalable and sustainable long-term. The paper provides initial success metrics but lacks longitudinal data on participation retention and quality maintenance.

## Next Checks

1. **Quality Assessment of Synthetic Data**: Conduct a comprehensive evaluation of a random sample of the 2.33 million instruction examples to assess linguistic diversity, task coverage, and potential biases. This should include both automated quality metrics and human expert review to validate the claims about dataset quality and diversity.

2. **Validation Consistency Analysis**: Perform a detailed analysis of the community validation process by examining inter-annotator agreement scores, identifying systematic error patterns, and assessing the impact of volunteer training materials on annotation quality. This would validate the reliability of the translated evaluation datasets.

3. **Leaderboard Benchmark Coverage**: Conduct a gap analysis of the 22 evaluation datasets against the full spectrum of Spanish dialects and co-official languages to identify underrepresented regions or language varieties. This would validate the comprehensiveness claims and guide future resource collection efforts.