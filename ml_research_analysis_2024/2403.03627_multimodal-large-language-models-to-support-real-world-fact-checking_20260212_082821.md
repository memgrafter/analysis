---
ver: rpa2
title: Multimodal Large Language Models to Support Real-World Fact-Checking
arxiv_id: '2403.03627'
source_url: https://arxiv.org/abs/2403.03627
tags:
- image
- confidence
- explanation
- llav
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework for evaluating multimodal large
  language models (MLLMs) as fact-checkers for real-world claims. The authors collect
  predictions, explanations, and confidence levels from models like GPT-4V, MiniGPT-v2,
  and LLaVA-1.5 across datasets of multimodal claims.
---

# Multimodal Large Language Models to Support Real-World Fact-Checking

## Quick Facts
- arXiv ID: 2403.03627
- Source URL: https://arxiv.org/abs/2403.03627
- Reference count: 30
- This paper evaluates MLLMs as fact-checkers using intrinsic knowledge without retrieval, finding GPT-4V achieves >80% accuracy while open models can be improved with prompt ensembles.

## Executive Summary
This paper presents a framework for evaluating multimodal large language models (MLLMs) as fact-checkers for real-world claims. The authors collect predictions, explanations, and confidence levels from models like GPT-4V, MiniGPT-v2, and LLaVA-1.5 across datasets of multimodal claims. GPT-4V shows high accuracy (>80%) and strong reasoning abilities, while open-source models like LLaVA lag behind but can be improved with prompt ensembles and in-context learning. The authors also analyze failure reasons and the impact of images on fact-checking. Overall, the paper provides insights into leveraging MLLMs to combat multimodal misinformation.

## Method Summary
The paper evaluates MLLMs as fact-checkers using an evidence-free approach that leverages only the models' intrinsic knowledge and reasoning capabilities. Four datasets containing multimodal claims (Fauxtography, COSMOS, MOCHEG, and Post-4V) are used with ground-truth veracity labels. Models including GPT-4V, MiniGPT-v2, and LLaVA-1.5 are prompted to output predictions, explanations, and confidence levels. The authors employ prompt ensembles (6 variations per claim) and in-context learning (1-shot and 2-shot demonstrations) to enhance open-source model performance without fine-tuning.

## Key Results
- GPT-4V achieves over 80% accuracy in fact-checking multimodal claims
- Prompt ensembles and in-context learning improve LLaVA-1.5 performance by 7.6-9.0 percentage points
- Reasoning-first prompting significantly increases instances of uncertainty expression in GPT-4V
- Images contribute meaningfully to fact-checking accuracy beyond text alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLLMs can fact-check real-world claims using only their parametric knowledge without external retrieval.
- Mechanism: The model leverages its pre-trained multimodal representations and reasoning pathways to compare visual and textual inputs against stored knowledge, producing a verdict.
- Core assumption: The knowledge required for fact-checking is already encoded in the model weights from pretraining.
- Evidence anchors:
  - [abstract] "Our methodology is evidence-free, leveraging only these models' intrinsic knowledge and reasoning capabilities."
  - [section] "We hypothesize that multimodal large language models (MLLMs) trained on large amounts of data can serve as a sufficient substitute for the retrieval of evidence and perform fact-checking in an evidence-free fashion"
  - [corpus] Weak – related work discusses retrieval-augmented generation but does not confirm pure parametric fact-checking performance.
- Break condition: The required factual knowledge is not present in pretraining data or the model lacks reasoning capability to connect multimodal cues to known facts.

### Mechanism 2
- Claim: In-context learning (ICL) improves MLLM fact-checking accuracy by demonstrating the task format.
- Mechanism: Providing examples within the prompt conditions the model to follow the desired output structure and apply similar reasoning patterns to new inputs.
- Core assumption: The model can generalize from a small number of demonstrations without explicit fine-tuning.
- Evidence anchors:
  - [section] "We further explore two approaches that do not require fine-tuning to enhancing the model's performance: ... In-Context Learning (ICL)"
  - [section] "On the COSMOS dataset, LLaV A+ICL-1 brings increases of 7.6 and 9.0 in both metrics."
  - [corpus] Moderate – similar ICL usage in other MLLM benchmarks shows improved few-shot performance.
- Break condition: Model fails to generalize from examples or demonstrations introduce bias that harms performance on unseen cases.

### Mechanism 3
- Claim: Confidence calibration improves when models are prompted to reason before predicting.
- Mechanism: Explicit reasoning step forces the model to surface uncertainty signals before committing to a prediction, improving calibration.
- Core assumption: The model's reasoning step accesses latent uncertainty that is suppressed when predicting directly.
- Evidence anchors:
  - [section] "Reasoning first significantly increases the instances of GPT-4V expressing uncertainty."
  - [section] "For example, the number of uncertain cases rises from 29 to 72 on Fauxtography and 10 to 38 on Post-4V."
  - [corpus] Weak – no direct corpus evidence of reasoning-before-prediction improving calibration; this is an empirical finding in the paper.
- Break condition: Reasoning step introduces noise or the model's uncertainty signals are not meaningful.

## Foundational Learning

- Concept: Multimodal representation alignment
  - Why needed here: Fact-checking requires integrating visual and textual information into a coherent understanding.
  - Quick check question: Can you explain how an MLLM maps an image and text into a shared embedding space?

- Concept: Prompt engineering for structured outputs
  - Why needed here: Extracting predictions, explanations, and confidence levels requires precise prompt formatting.
  - Quick check question: What format should you use to get a model to output a prediction, explanation, and confidence score in one response?

- Concept: In-context learning mechanics
  - Why needed here: ICL is used to boost performance without fine-tuning, so understanding its constraints is critical.
  - Quick check question: How many examples are typically needed in ICL before diminishing returns set in?

## Architecture Onboarding

- Component map: Input layer (multimodal claim) -> Encoder (vision + text encoders with cross-attention) -> Knowledge store (parametric weights) -> Reasoning module (explanation generator) -> Output layer (prediction, explanation, confidence) -> Post-processing (calibration and uncertainty handling)

- Critical path:
  1. Receive multimodal claim
  2. Encode image and text
  3. Retrieve relevant knowledge from parameters
  4. Perform reasoning to generate explanation
  5. Output prediction with confidence
  6. Calibrate confidence score

- Design tradeoffs:
  - Accuracy vs. speed: Larger models are more accurate but slower and costlier.
  - Open-source vs. proprietary: Open models are more customizable but lag in performance.
  - Instruction tuning vs. fine-tuning: Instruction tuning enables zero-shot use but may be less precise than fine-tuning.

- Failure signatures:
  - Low confidence + high error: Model is unsure but wrong → knowledge gap or ambiguous input.
  - High confidence + high error: Model is overconfident but wrong → hallucination or bias.
  - No response: Model refuses due to safety filters or prompt format mismatch.

- First 3 experiments:
  1. Run the baseline prompt on a small sample to verify output structure.
  2. Test ICL with 1 and 2 examples to measure accuracy gains.
  3. Compare reasoning-first vs. prediction-first prompts to observe calibration changes.

## Open Questions the Paper Calls Out
None

## Limitations
- The knowledge-based approach without retrieval may fail on claims requiring up-to-date information beyond the model's pretraining cutoff
- Open-source models show substantial performance gaps that may not be fully bridgeable through prompt engineering alone
- The claim that MLLMs can serve as a complete substitute for evidence retrieval in fact-checking remains speculative

## Confidence

**High Confidence**: GPT-4V's superior performance (>80% accuracy) and strong reasoning capabilities are well-supported by the empirical results across all four datasets. The consistent improvement from reasoning-first prompting is also reliably demonstrated.

**Medium Confidence**: The effectiveness of prompt ensembles and in-context learning for improving LLaVA-1.5's performance, while shown, may not generalize equally well to other open-source MLLMs or different fact-checking domains.

**Low Confidence**: The claim that MLLMs can serve as a complete substitute for evidence retrieval in fact-checking remains speculative. The paper demonstrates capability on specific datasets but doesn't establish this as a general principle.

## Next Checks
1. **Temporal Generalization Test**: Evaluate model performance on claims about events occurring after the model's pretraining cutoff to assess knowledge currency limitations.

2. **Cross-Domain Robustness**: Apply the same evaluation framework to fact-checking claims in domains not represented in the current datasets (scientific claims, legal statements, etc.) to test generalizability.

3. **Human Expert Comparison**: Conduct a controlled study comparing MLLM fact-checking outputs against human expert judgments on the same claims to establish ground truth reliability and identify systematic biases.