---
ver: rpa2
title: 'CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language Models'
arxiv_id: '2412.19331'
source_url: https://arxiv.org/abs/2412.19331
tags:
- parts
- object
- objects
- images
- body
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the novel task of part-focused semantic co-segmentation,
  which involves identifying and segmenting common objects, as well as common and
  unique object parts across images. To address this task, the authors propose CALICO,
  the first LVLM designed for multi-image part-level reasoning segmentation.
---

# CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language Models

## Quick Facts
- arXiv ID: 2412.19331
- Source URL: https://arxiv.org/abs/2412.19331
- Authors: Kiet A. Nguyen; Adheesh Juvekar; Tianjiao Yu; Muntasir Wahed; Ismini Lourentzou
- Reference count: 40
- Key outcome: CALICO achieves 6.3% relative improvement in mean IoU on MIXED PARTS while using 18× fewer image tokens, reducing TFLOPS by 32.6%, and accelerating inference by 51.3%

## Executive Summary
This paper introduces CALICO, a novel large vision-language model (LVLM) designed for part-focused semantic co-segmentation. The task involves identifying and segmenting common objects and their parts across multiple images, including both shared and unique components. CALICO features a Correspondence Extraction Module that identifies semantic part-level correspondences using DINOv2 features fused with CLIP embeddings, and Correspondence Adaptation Modules that efficiently inject this information into the LLM. The authors also curate MIXED PARTS, a large-scale multi-image segmentation dataset with ~2.4M samples across ~44K images, to support training and evaluation.

## Method Summary
CALICO takes as input a set of images and processes them through a Q-Former + EVA-CLIP vision module to extract visual embeddings, which are then projected into a Vicuna-based LLM space. A Correspondence Extraction Module (CEM) fuses DINOv2's self-supervised features with CLIP features via cross-attention to capture semantic part-level correspondences. Correspondence Adaptation Modules (CAMs) inject this semantic correspondence information into the LLM at layers 11 and 22 using parameter-efficient LoRA layers (0.3% of total parameters). The model generates segmentation masks using a SAM decoder and text labels through next-token prediction. CALICO is trained on the MIXED PARTS dataset using a combined loss of text prediction and segmentation loss, with the model initialized from GLaMM's pretrained checkpoint and trained for 10 epochs.

## Key Results
- CALICO achieves 6.3% relative improvement in mean IoU on MIXED PARTS compared to baselines
- Uses 18× fewer image tokens (32 vs 576) than GLaMM, reducing TFLOPS by 32.6%
- Accelerates inference by 51.3% while maintaining strong performance
- Demonstrates effectiveness of parameter-efficient fine-tuning (0.3% parameters trained)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CEM enables semantic part-level correspondences across images by leveraging self-supervised features from DINOv2.
- **Mechanism**: CEM fuses semantic embeddings from DINOv2 with CLIP features via cross-attention, creating a rich representation that captures part-level relationships across similar objects in different images.
- **Core assumption**: DINOv2's self-supervised training produces features with strong semantic correspondence capabilities that can be effectively fused with CLIP features.
- **Evidence anchors**: [section] "Observing the effectiveness and efficiency of BLIP-2's Q-Former cross-attention mechanism [30], especially in multi-image settings [31], we propose using Q-Former in tandem with a strong CLIP vision encoder [48, 61] to extract visual embeddings from the input images." [corpus] Weak - no direct evidence in corpus about DINOv2's correspondence capabilities specifically.

### Mechanism 2
- **Claim**: CAMs inject semantic correspondence information into the LLM in a parameter-efficient manner, enabling multi-image understanding.
- **Mechanism**: CAMs linearly project the LLM's last hidden state into the vision embedding space, use this as guidance to enrich query tokens, and then extract semantic-rich visual information from the fused CLIP+DINOv2 embeddings. This information is projected back into the language space and added to the LLM's visual tokens.
- **Core assumption**: Linear projections and token enrichment can effectively inject semantic correspondence information without full fine-tuning.
- **Evidence anchors**: [section] "Due to the high cost of training LLMs with billions of parameters, many works have taken advantage of adaptive modules...by freezing the LLMs and only training the modules for downstream tasks [20, 21, 31, 74]." [corpus] Weak - no direct evidence about CAM effectiveness in corpus.

### Mechanism 3
- **Claim**: Q-Former's efficient visual token extraction enables CALICO to process multiple images with significantly fewer tokens than baselines, improving efficiency without sacrificing performance.
- **Mechanism**: Q-Former uses a small set of learnable query tokens to extract visual information from CLIP embeddings, reducing sequence length from 256-576 tokens to just 32 tokens per image.
- **Core assumption**: A small set of query tokens can effectively capture the necessary visual information for the task.
- **Evidence anchors**: [section] "Whereas projecting CLIP embeddings directly into the language model space preserves their long sequence lengths (e.g., 256 or 576 tokens [27, 50]) and thus increasing compute, Q-Former uses a much shorter set of learnable query tokens to extract visual information (e.g., 32 tokens [30, 31])." [corpus] Weak - no direct evidence about Q-Former efficiency in corpus.

## Foundational Learning

- **Concept**: Cross-attention mechanisms in vision-language models
  - Why needed here: CALICO uses Q-Former's cross-attention to extract visual features from CLIP embeddings, and CEM uses cross-attention to fuse DINOv2 and CLIP features.
  - Quick check question: What is the difference between self-attention and cross-attention in transformer architectures?

- **Concept**: Parameter-efficient fine-tuning techniques (LoRA)
  - Why needed here: CALICO uses LoRA layers to adapt the LLM with minimal trainable parameters (0.3% of total), enabling efficient multi-image understanding.
  - Quick check question: How does LoRA differ from full fine-tuning in terms of parameter count and computational cost?

- **Concept**: Semantic correspondence in computer vision
  - Why needed here: The core innovation relies on establishing semantic correspondences between parts across different images, which is fundamental to the task.
  - Quick check question: What are the key challenges in establishing semantic correspondences between object parts across different images?

## Architecture Onboarding

- **Component map**: Input images → Q-Former → EVA-CLIP → CEM (DINOv2 fusion) → CAMs (at layers 11 and 22) → LLM reasoning → [SEG] token extraction → SAM decoder → segmentation masks

- **Critical path**: Images are processed through Q-Former and EVA-CLIP to extract visual embeddings, which are fused with DINOv2 features in the CEM. This semantic correspondence information is injected into the LLM via CAMs at layers 11 and 22, enabling multi-image reasoning. The LLM generates text labels and [SEG] tokens, which are converted to segmentation masks by the SAM decoder.

- **Design tradeoffs**: Using Q-Former reduces computational cost but may limit visual information; using frozen DINOv2 preserves pre-trained correspondence knowledge but may not adapt to task-specific nuances.

- **Failure signatures**: Poor segmentation quality could indicate issues with CEM feature fusion, CAM injection, or Q-Former visual extraction; poor text labels could indicate issues with LLM reasoning or grounding.

- **First 3 experiments**:
  1. Verify Q-Former extracts meaningful visual features by comparing outputs with and without Q-Former
  2. Test CAM effectiveness by comparing performance with CAMs at different layers or with different numbers of CAMs
  3. Validate CEM fusion by comparing semantic correspondence quality with and without DINOv2 features

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How would CALICO perform on real-world, uncontrolled datasets with occlusions and viewpoint variations compared to its performance on MIXED PARTS?
- **Basis in paper**: [inferred] The paper mentions CALICO's strong performance on MIXED PARTS but notes "its generalizability to complex, real-world environments remains to be fully validated"
- **Why unresolved**: The MIXED PARTS dataset was curated and controlled. Real-world applications would introduce challenges like occlusions, lighting variations, and complex backgrounds not present in the dataset
- **What evidence would resolve it**: Testing CALICO on real-world multi-image datasets like COCO or LVIS with occlusions, or in applications like quality control or medical imaging with complex, uncontrolled conditions

### Open Question 2
- **Question**: What is the impact of increasing the number of images beyond two in the part-focused semantic co-segmentation task?
- **Basis in paper**: [explicit] The paper focuses on image pairs (NI = 2) and states "we pair up individual images corresponding to our common object, common part, and unique part localization subtasks"
- **Why unresolved**: The paper only evaluates on pairs of images. Multi-image reasoning with 3+ images could reveal whether the correspondence extraction modules scale effectively
- **What evidence would resolve it**: Experiments testing CALICO on 3, 4, or 5-image sets, measuring performance degradation and computational efficiency compared to pairwise processing

### Open Question 3
- **Question**: How would incorporating non-visual attributes like material properties or weight improve CALICO's object-part reasoning compared to visual features alone?
- **Basis in paper**: [explicit] The paper discusses limitations in the Broader Impact section, noting that "reliance on visual segmentation alone may introduce risks in tasks where non-visual attributes are crucial"
- **Why unresolved**: CALICO is purely vision-based. Real-world objects with similar appearance but different material properties (plastic vs. metal) may be misinterpreted
- **What evidence would resolve it**: Comparative experiments testing CALICO against variants augmented with material/texture embeddings or other sensory data inputs on tasks requiring material discrimination

## Limitations
- Dataset bias and generalization concerns due to curated MIXED PARTS dataset not fully representing real-world complexity
- Scalability constraints for real-time processing of many images despite efficiency gains
- Ambiguity in part definitions with potential for inconsistent human annotation

## Confidence

**High Confidence Claims:**
- CALICO achieves strong performance on the MIXED PARTS dataset with 6.3% relative improvement in mIoU
- The parameter-efficient design (0.3% of parameters fine-tuned) is effective for the task
- Q-Former significantly reduces computational cost while maintaining performance

**Medium Confidence Claims:**
- The Correspondence Extraction Module effectively captures semantic part-level correspondences across images
- DINOv2 features provide meaningful semantic correspondence information when fused with CLIP features
- The cascaded pipeline baseline is appropriately designed for comparison

**Low Confidence Claims:**
- CALICO's performance will generalize to completely unseen object categories and complex real-world scenarios
- The semantic correspondence mechanisms will scale effectively to very large numbers of images
- The efficiency gains will translate directly to all deployment scenarios

## Next Checks

1. **Cross-Dataset Generalization Test** - Evaluate CALICO on a held-out test set containing object categories and part types not present in MIXED PARTS, or on established datasets like Pascal VOC Part or COCO-Part, to assess true generalization capability beyond the curated dataset.

2. **Ablation of Correspondence Components** - Systematically remove or replace the Correspondence Extraction Module (CEM) with simpler baselines (e.g., direct CLIP feature comparison) and test CAM effectiveness with varying numbers of modules to isolate which components contribute most to performance gains.

3. **Scalability and Efficiency Analysis** - Measure CALICO's performance and computational cost as the number of input images increases from 2 to 8+ images, and test on hardware with different memory constraints to understand practical deployment limitations.