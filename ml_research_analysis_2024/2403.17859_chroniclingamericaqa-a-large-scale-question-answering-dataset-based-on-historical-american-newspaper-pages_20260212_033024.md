---
ver: rpa2
title: 'ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical
  American Newspaper Pages'
arxiv_id: '2403.17859'
source_url: https://arxiv.org/abs/2403.17859
tags:
- dataset
- question
- historical
- text
- newspaper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ChroniclingAmericaQA, a large-scale question-answering
  dataset created from historical American newspaper pages spanning 120 years (1800-1920).
  The dataset contains 487K question-answer pairs and addresses the challenge of utilizing
  historical documents, which often have noisy OCR text and archaic language.
---

# ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical American Newspaper Pages

## Quick Facts
- arXiv ID: 2403.17859
- Source URL: https://arxiv.org/abs/2403.17859
- Reference count: 40
- Dataset: 487K question-answer pairs from historical American newspapers (1800-1920)

## Executive Summary
This paper introduces ChroniclingAmericaQA, a large-scale question-answering dataset created from historical American newspaper pages spanning 120 years (1800-1920). The dataset contains 487K question-answer pairs and addresses the challenge of utilizing historical documents, which often have noisy OCR text and archaic language. The authors employed a framework that involved correcting OCR text using GPT-3.5 Turbo, generating questions with a T5-base model, and applying multiple filtering steps to ensure quality. Evaluation results show that models fine-tuned on ChroniclingAmericaQA significantly outperform those trained on other datasets, with BERT-base-ChroniclingAmericaQA achieving 63.29% EM and 69.43% F1 scores on corrected OCR text.

## Method Summary
The dataset creation framework involves three main stages: OCR correction, question generation, and filtering. GPT-3.5 Turbo is used to correct noisy OCR text from historical newspapers, addressing issues like spelling errors and archaic language. A T5-base model then generates questions based on the corrected text. Multiple filtering steps are applied, including duplicate removal, answer length validation, and quality checks using GPT-3.5 Turbo to ensure grammatical correctness and relevance. The final dataset contains 487K question-answer pairs derived from 120 years of American newspaper archives.

## Key Results
- BERT-base fine-tuned on ChroniclingAmericaQA achieves 63.29% EM and 69.43% F1 on corrected OCR text
- Fine-tuning on relevant historical data significantly outperforms general-domain models (BERT-base-uncased F1: 45.63%)
- OCR correction using GPT-3.5 Turbo improves model performance on historical documents

## Why This Works (Mechanism)
The dataset creation framework works by addressing the specific challenges of historical documents: noisy OCR text, archaic language, and domain-specific terminology. GPT-3.5 Turbo corrects OCR errors while preserving historical context, T5-base generates questions tailored to the historical domain, and multi-stage filtering ensures high-quality QA pairs. This domain-specific approach allows models to learn patterns unique to historical American newspapers rather than generic text.

## Foundational Learning
1. **OCR correction with LLMs**: Needed to handle noisy historical text with spelling variations and OCR errors. Quick check: Compare corrected vs. original text for preservation of historical terminology.
2. **Domain-specific question generation**: Required to create relevant questions that reflect historical context rather than modern language patterns. Quick check: Analyze question distribution for temporal consistency.
3. **Multi-stage filtering pipeline**: Essential to maintain dataset quality when using automated generation methods. Quick check: Validate filtering criteria against human-annotated samples.

## Architecture Onboarding
**Component map**: OCR Text -> GPT-3.5 Turbo Correction -> T5-base Question Generation -> Multi-stage Filtering -> Final Dataset

**Critical path**: The OCR correction stage is most critical as errors here propagate through question generation and filtering, potentially degrading the entire dataset quality.

**Design tradeoffs**: The choice between automated correction (faster, scalable) versus manual verification (more accurate but expensive) impacts dataset size and quality. The authors prioritized scalability using GPT-3.5 Turbo with quality controls.

**Failure signatures**: Poor OCR correction manifests as garbled historical terms in questions; inadequate filtering shows as duplicate or nonsensical QA pairs; domain mismatch appears as anachronistic questions.

**First experiments**: 1) Compare model performance on corrected vs. uncorrected OCR text; 2) Evaluate question generation quality across different historical periods; 3) Test filtering effectiveness on manually annotated validation sets.

## Open Questions the Paper Calls Out
The study does not explicitly call out open questions beyond those mentioned in the limitations section regarding the generalizability of the approach to other historical contexts and languages.

## Limitations
- GPT-3.5 Turbo corrections may introduce model hallucinations or smooth out historically accurate spellings
- Dataset focuses exclusively on American newspapers from 1800-1920, limiting generalizability
- Performance improvements may result from overfitting to dataset-specific characteristics rather than genuine historical language understanding

## Confidence
- **High confidence**: The dataset creation methodology is sound and the large scale (487K pairs) is verifiable
- **Medium confidence**: Performance improvements from fine-tuning are demonstrated but may not generalize beyond the specific domain
- **Medium confidence**: The importance of OCR correction is validated but the long-term impact on historical accuracy requires further study

## Next Checks
1. Evaluate model performance on a held-out test set with manual verification to assess real-world generalization
2. Compare performance across different historical document collections with varying OCR quality to test robustness
3. Analyze the impact of GPT-3.5 Turbo corrections on historically significant terminology and spellings through qualitative review