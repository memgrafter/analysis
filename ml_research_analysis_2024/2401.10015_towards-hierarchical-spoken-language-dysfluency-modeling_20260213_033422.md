---
ver: rpa2
title: Towards Hierarchical Spoken Language Dysfluency Modeling
arxiv_id: '2401.10015'
source_url: https://arxiv.org/abs/2401.10015
tags:
- speech
- disfluency
- lian
- alignment
- transcription
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hierarchical Unconstrained Disfluency Modeling
  (H-UDM), an extension of the prior UDM framework for modeling speech disfluencies.
  H-UDM integrates transcription and detection modules using a recursive alignment
  approach to address both word-level and phoneme-level disfluencies without requiring
  extensive manual annotations.
---

# Towards Hierarchical Spoken Language Dysfluency Modeling

## Quick Facts
- arXiv ID: 2401.10015
- Source URL: https://arxiv.org/abs/2401.10015
- Reference count: 18
- Primary result: H-UDM improves disfluency detection F1 scores from 62.4% to 73.1% at word level and 64.5% to 67.2% at phonetic level

## Executive Summary
This paper introduces Hierarchical Unconstrained Disfluency Modeling (H-UDM), an extension of the prior UDM framework for modeling speech disfluencies. H-UDM integrates transcription and detection modules using a recursive alignment approach to address both word-level and phoneme-level disfluencies without requiring extensive manual annotations. The method employs an Unconstrained Recursive Forced Aligner (URFA) with monotonicity constraints and a Text Refresher to generate disfluency-aware transcriptions. Experimental results on aphasia and dyslexia speech datasets show significant improvements in disfluency detection performance.

## Method Summary
H-UDM uses a WavLM encoder feeding into a Unconstrained Forced Aligner (UFA) module that combines conformer architecture with CTC loss for monotonicity. The method generates 2D alignments between speech and text, then applies recursive segmentation at word boundaries with smoothed re-segmentation. A Text Refresher analyzes alignment discrepancies to detect word-level disfluencies. Template matching algorithms identify disfluency types from alignment patterns. The approach operates hierarchically, detecting disfluencies at both phoneme and word levels through multiple recursive iterations.

## Key Results
- F1 scores increased from 62.4% to 73.1% at word level across multiple recursive iterations
- Phonetic-level detection improved from 64.5% to 67.2% F1 scores
- CTC constraint significantly enhanced performance across all metrics in ablation studies

## Why This Works (Mechanism)

### Mechanism 1
The recursive alignment approach improves phoneme boundary detection by refining non-monotonic alignments through multiple iterations. Each iteration segments disfluent speech at word boundaries and performs new alignment on each segment, reducing variance from global non-monotonicity and improving boundary accuracy.

### Mechanism 2
The CTC monotonicity constraint improves alignment robustness by providing a supervised monotonic baseline during training. CTC loss is added to the UFA training objective, encouraging monotonic alignment tendencies while dynamic alignment search still allows non-monotonicity for disfluency modeling.

### Mechanism 3
The text refresher generates disfluency-aware word transcriptions by comparing ASR output with 2D alignment to identify insertions and deletions. The text refresher analyzes discrepancies between Whisper's perfect transcription and the phoneme-level 2D alignment to detect word-level disfluencies that ASR systems typically miss.

## Foundational Learning

- **Non-monotonic alignment**: Required because disfluent speech contains repetitions, insertions, and deletions that need alignments to move backward or skip forward in time, unlike fluent speech which follows monotonic progression. Quick check: Why can't we use standard forced aligners that assume monotonic alignment for disfluent speech?

- **Dynamic programming alignment search**: Needed because alignment between speech and text for disfluent speech requires searching through possible alignment paths to find the best non-monotonic match. Quick check: What is the computational complexity of the boundary-aware Viterbi algorithm used for alignment search?

- **Template matching for disfluency detection**: Used because without human-labeled data for all disfluency types, template matching provides a rule-based approach to identify patterns in the alignment that correspond to specific disfluency categories. Quick check: How does the template matching algorithm distinguish between a phoneme deletion and a replacement?

## Architecture Onboarding

- **Component map**: WavLM encoder → UFA module (conformer + CTC + phoneme classifier) → Dynamic alignment search → 2D alignment generation → Smoothed re-segmentation → Recursive URFA iterations → Text refresher → Template matching detector

- **Critical path**: Audio → UFA alignment → 2D alignment → template matching detection. The UFA module and 2D alignment generation are bottlenecks as they directly affect detection accuracy.

- **Design tradeoffs**: Recursive vs. single-pass alignment (recursive provides better accuracy but increases computation time); CTC constraint (improves training stability but may reduce non-monotonic alignment flexibility); Template matching vs. learned detection (template matching requires no labeled data but may miss complex disfluency patterns).

- **Failure signatures**: Poor word boundary detection → recursive iterations don't improve results; High phoneme error rate → 2D alignment errors propagate to disfluency detection; CTC dominates training → alignment becomes too monotonic, missing disfluencies.

- **First 3 experiments**: 1) Compare UFA with and without CTC constraint on VCTK dataset to verify monotonicity benefit; 2) Test single iteration vs. multiple recursive iterations on Buckeye dataset to measure boundary improvement; 3) Evaluate template matching detection accuracy with perfect vs. UFA-generated 2D alignments to isolate alignment quality impact.

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal number of recursive iterations for URFA in different speech disorders (e.g., aphasia vs dyslexia)? The paper shows recursive modeling improves performance but reaches saturation, mentioning experiments up to 3rd-order URFA. A systematic ablation study varying recursive iteration depth on multiple speech disorder datasets would resolve this.

### Open Question 2
How would incorporating articulatory features improve disfluency detection compared to phoneme-based approaches? The limitations section states "phoneme units may not be optimal" and suggests articulatory units could enhance alignment modeling. Direct comparison of H-UDM using phoneme vs articulatory unit representations would provide evidence.

### Open Question 3
What scaling laws apply to H-UDM for disfluent speech, and how do they differ from scaling laws for fluent speech ASR? Section 2.2 discusses ASR scalability and Fig. 4 showing different scaling behaviors for perfect vs imperfect speech. Scaling experiments varying dataset size and model parameters for H-UDM would resolve this.

## Limitations

- Template matching algorithms lack sufficient specification for exact thresholds and handling of complex disfluency patterns
- Evaluation relies heavily on synthetic data augmentation that may not capture full complexity of real disordered speech
- Computational complexity of recursive approach for real-time applications is not addressed

## Confidence

**High Confidence**: Improvement in phonetic transcription accuracy (dPER reduction from 20.3% to 18.1%) is well-supported by controlled experiments on multiple datasets.

**Medium Confidence**: Recursive alignment approach benefits are demonstrated but rely heavily on synthetic data. Real-world validation on spontaneous disordered speech is needed.

**Low Confidence**: Template matching algorithm's ability to accurately detect all disfluency types without false positives is claimed but not rigorously validated.

## Next Checks

1. **Real-world validation**: Test H-UDM on spontaneous disordered speech from multiple clinical populations, comparing against human expert annotations to verify performance on naturalistic data beyond controlled datasets.

2. **Ablation study**: Systematically remove the CTC constraint, recursive iterations, and text refresher components to quantify their individual contributions to performance gains and identify the critical path for improvement.

3. **Generalization assessment**: Evaluate H-UDM on unseen disfluency types and speaking styles not represented in training data to test the method's robustness and ability to generalize beyond learned patterns.