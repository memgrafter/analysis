---
ver: rpa2
title: Optimistic Model Rollouts for Pessimistic Offline Policy Optimization
arxiv_id: '2401.05899'
source_url: https://arxiv.org/abs/2401.05899
tags:
- policy
- offline
- orpo
- p-mdp
- rollout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of underutilizing generalization
  ability of dynamics models in model-based offline reinforcement learning due to
  pessimism. The authors propose constructing an Optimistic MDP (O-MDP) alongside
  the traditional Pessimistic MDP (P-MDP).
---

# Optimistic Model Rollouts for Pessimistic Offline Policy Optimization

## Quick Facts
- arXiv ID: 2401.05899
- Source URL: https://arxiv.org/abs/2401.05899
- Reference count: 40
- Primary result: ORPO achieves 30% improvement over P-MDP baselines on D4RL benchmarks and state-of-the-art performance on 8/12 datasets

## Executive Summary
This paper addresses the challenge of underutilizing generalization ability of dynamics models in model-based offline reinforcement learning due to pessimism. The authors propose constructing an Optimistic MDP (O-MDP) alongside the traditional Pessimistic MDP (P-MDP). Their framework, ORPO (Optimistic Rollout for Pessimistic Optimization), trains an optimistic rollout policy in the O-MDP to generate more out-of-distribution (OOD) model rollouts, then relabels these with penalized rewards and optimizes the output policy in the P-MDP. Theoretically, they demonstrate that ORPO policies can be lower-bounded in linear MDPs. Empirically, ORPO significantly outperforms P-MDP baselines by 30% on D4RL benchmarks and achieves state-of-the-art performance on 8 out of 12 datasets. The method shows particular advantages in generalization tasks requiring policy adaptation to unseen environments.

## Method Summary
ORPO constructs both an Optimistic MDP and a Pessimistic MDP using an ensemble of dynamics models. The optimistic rollout policy (trained via SAC in the O-MDP) generates diverse, high-value out-of-distribution samples. These samples are then relabeled with pessimistic rewards based on uncertainty quantification and used to train the output policy (optimized via TD3+BC in the P-MDP). This decoupling allows the rollout policy to explore broadly while maintaining the conservative guarantees of pessimistic optimization. The framework uses ensemble variance as the primary uncertainty heuristic and demonstrates improved performance across D4RL benchmark tasks and generalization challenges.

## Key Results
- ORPO achieves 30% improvement over Pessimistic MDP baselines on D4RL benchmark tasks
- State-of-the-art performance on 8 out of 12 D4RL datasets (random, medium, medium-replay, medium-expert)
- Significant generalization performance on custom "Halfcheetah-jump" and "Halfcheetah-jump-hard" tasks
- Theoretical lower bound guarantee for ORPO policies in linear MDPs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimistic model rollouts encourage the policy to explore out-of-distribution (OOD) regions that have high value estimates, leveraging the generalization ability of the learned dynamics model.
- Mechanism: By constructing an Optimistic MDP (O-MDP) with positive uncertainty penalties, the rollout policy is incentivized to sample actions with high estimated values and high uncertainty. This leads to more OOD model rollouts compared to a Pessimistic MDP (P-MDP) which penalizes such actions.
- Core assumption: The learned dynamics model has sufficient generalization capacity to provide reasonable characterizations of OOD state-action pairs, making them potentially valuable for policy improvement.
- Evidence anchors:
  - [abstract] "The P-MDP discourages the policies from learning in out-of-distribution (OOD) regions beyond the support of offline datasets, which can under-utilize the generalization ability of dynamics models. In contrast, we propose constructing an Optimistic MDP (O-MDP)."
  - [section] "We initially observed the potential benefits of optimism brought by encouraging more OOD rollouts. Motivated by this observation, we present ORPO..."
  - [corpus] Weak. Related papers focus on pessimism and conservative estimation but do not explicitly discuss optimism in OOD sampling.
- Break condition: If the dynamics model's generalization capacity is poor or if OOD actions are strictly harmful (e.g., cause immediate failure in the environment), then optimistic rollouts may lead to dangerous or useless samples.

### Mechanism 2
- Claim: Pessimistic policy optimization in the P-MDP provides a lower bound on performance in the real MDP, ensuring that the final output policy avoids risky state-action regions with large model errors.
- Mechanism: The P-MDP uses penalized rewards based on uncertainty quantification (e.g., ensemble variance) to construct a conservative estimate of the true MDP. Optimizing the output policy in this P-MDP ensures that the policy's performance in the real MDP is lower-bounded by its performance in the P-MDP.
- Core assumption: The uncertainty quantification used in the P-MDP (e.g., ensemble variance) forms a valid upper bound on the model error between the learned and true dynamics.
- Evidence anchors:
  - [section] "To avoid over-estimation on out-of-distribution (OOD) data, prior methods construct a Pessimistic Markov Decision Process (P-MDP) based on uncertainty quantification of dynamics models..."
  - [section] "The learned dynamics model define a model MDPcM = (S, A,bT , r, γ). Then the goal switches to find a policy π(a|s) that maximizes the expected discounted return with respect to ρπbT..."
  - [corpus] Weak. Related papers discuss uncertainty quantification for pessimism but do not explicitly prove the lower bound property in the context of ORPO.
- Break condition: If the uncertainty quantification is inaccurate or if the P-MDP is too conservative, the output policy may under-explore and miss high-value regions, limiting performance.

### Mechanism 3
- Claim: Decoupling the training of the optimistic rollout policy from the pessimistic output policy allows for effective utilization of both generalization and safety.
- Mechanism: The rollout policy is trained in the O-MDP to generate diverse, high-value OOD samples. These optimistic rollouts are then relabeled with penalized rewards and used to train the output policy in the P-MDP. This decoupling allows the rollout policy to explore broadly while the output policy remains conservative.
- Core assumption: The optimistic rollouts, when relabeled with penalized rewards, still contain valuable information for improving the output policy in the P-MDP.
- Evidence anchors:
  - [abstract] "We present ORPO, a simple yet effective model-based offline RL framework. ORPO generates Optimistic model Rollouts for Pessimistic offline policy Optimization."
  - [section] "To this end, we present a novel model-based offline RL algorithmic framework called ORPO... which decouples the training of rollout policies from the pessimistic policy optimization."
  - [corpus] Weak. Related papers do not discuss this specific decoupling strategy.
- Break condition: If the optimistic rollouts are too far from the data distribution or if the relabeling process destroys the value information, the output policy may not benefit from the additional samples.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The entire framework is built on the MDP formulation, with the goal of finding a policy that maximizes the expected discounted return. Both the P-MDP and O-MDP are derived from the underlying MDP.
  - Quick check question: What are the components of an MDP and how is the value function defined?

- Concept: Uncertainty Quantification in Dynamics Models
  - Why needed here: Uncertainty quantification is crucial for constructing both the P-MDP and O-MDP. It is used to penalize or reward actions based on the model's confidence in its predictions.
  - Quick check question: How is uncertainty typically quantified in ensemble dynamics models, and how does this relate to the epistemic and aleatoric components?

- Concept: Offline Reinforcement Learning
  - Why needed here: ORPO is an offline RL method, meaning it learns from a fixed dataset without interacting with the environment. This requires careful handling of out-of-distribution actions to avoid overestimation.
  - Quick check question: What are the key challenges in offline RL, and how do pessimism and optimism address these challenges?

## Architecture Onboarding

- Component map:
  Dynamics Model Ensemble -> Optimistic Rollout Policy (SAC) -> Pessimistic Output Policy (TD3+BC) -> Uncertainty Quantifier -> Replay Buffers

- Critical path:
  1. Train the dynamics model ensemble on the offline dataset.
  2. Construct the P-MDP and O-MDP using the uncertainty quantifier.
  3. Train the optimistic rollout policy in the O-MDP to generate diverse samples.
  4. Relabel the optimistic rollouts with penalized rewards according to the P-MDP.
  5. Train the pessimistic output policy using the offline dataset, relabeled optimistic rollouts, and pessimistic rollouts.
  6. Evaluate the output policy in the real environment.

- Design tradeoffs:
  - Optimism vs. Pessimism: Too much optimism can lead to dangerous OOD samples, while too much pessimism can result in overly conservative policies.
  - Rollout Length: Longer rollouts can provide more diverse samples but may also accumulate more model error.
  - Uncertainty Heuristic: Different heuristics (e.g., ensemble variance, max aleatoric) may perform better in different environments.

- Failure signatures:
  - Poor Performance: The output policy may underperform if the dynamics model's generalization is poor or if the P-MDP is too conservative.
  - Instability: The training process may be unstable if the optimistic rollouts are too far from the data distribution or if the relabeling process is not handled correctly.
  - Overfitting: The output policy may overfit to the offline dataset if the optimistic rollouts are not diverse enough.

- First 3 experiments:
  1. Implement the dynamics model ensemble and uncertainty quantifier on a simple environment (e.g., CartPole) to verify that they are learning correctly.
  2. Train the optimistic rollout policy in the O-MDP and visualize the generated samples to ensure they are exploring OOD regions.
  3. Train the pessimistic output policy using the offline dataset and relabeled optimistic rollouts, and evaluate its performance in the real environment to verify the overall framework.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ORPO vary when using different uncertainty quantification methods for the dynamics model?
- Basis in paper: [explicit] The paper mentions using Ensemble Std, Ensemble Var, and Max Aleatoric as uncertainty heuristics, but does not provide a systematic comparison of their impact on ORPO performance.
- Why unresolved: The paper only briefly mentions these heuristics without conducting a thorough ablation study on their effects.
- What evidence would resolve it: A controlled experiment comparing ORPO performance using each uncertainty quantification method across multiple datasets.

### Open Question 2
- Question: What is the impact of the rollout policy's exploration-exploitation trade-off on ORPO's final performance?
- Basis in paper: [inferred] The paper discusses using SAC to train the optimistic rollout policy but does not explore how different exploration strategies affect the quality of the generated rollouts and subsequent policy optimization.
- Why unresolved: The paper focuses on the overall framework but does not investigate the sensitivity of ORPO to the rollout policy's exploration behavior.
- What evidence would resolve it: Experiments varying the exploration parameters of SAC during rollout policy training and measuring the resulting ORPO performance.

### Open Question 3
- Question: How does ORPO perform in environments with sparse rewards or long horizons?
- Basis in paper: [inferred] The paper evaluates ORPO on standard D4RL benchmarks and two generalization tasks, but does not explore its performance in more challenging settings with sparse rewards or longer episode lengths.
- Why unresolved: The current evaluation focuses on dense reward environments with relatively short horizons, leaving the question of ORPO's scalability unanswered.
- What evidence would resolve it: Testing ORPO on environments specifically designed to be challenging due to sparse rewards or long horizons, and comparing its performance to other offline RL methods.

## Limitations

- Theoretical guarantees are limited to linear MDPs and may not extend to complex, high-dimensional environments
- Performance depends heavily on the quality of uncertainty quantification and dynamics model generalization
- Relabeling process for optimistic rollouts could introduce bias if pessimistic penalties are not properly calibrated

## Confidence

- Mechanism 1 (Optimistic rollouts leveraging model generalization): Medium confidence - Strong empirical evidence from D4RL experiments, but limited theoretical grounding beyond linear MDP analysis
- Mechanism 2 (Pessimistic optimization provides lower bound): Medium confidence - Theoretical guarantees exist for linear MDPs, but real-world applicability depends on accurate uncertainty quantification
- Mechanism 3 (Decoupling rollout and output policy training): High confidence - The algorithmic framework is clearly specified and the decoupling strategy is intuitive and well-supported by experimental results

## Next Checks

1. **Uncertainty quantification sensitivity analysis**: Systematically vary the uncertainty quantification method (ensemble variance, dropout, ensemble disagreement) and measure impact on performance across different dataset types.
2. **Generalization robustness test**: Evaluate ORPO performance on additional out-of-distribution datasets with varying degrees of distributional shift to quantify the generalization bounds more comprehensively.
3. **Failure mode characterization**: Intentionally train dynamics models with known generalization failures and measure how ORPO's performance degrades compared to pure pessimistic baselines.