---
ver: rpa2
title: Is Table Retrieval a Solved Problem? Exploring Join-Aware Multi-Table Retrieval
arxiv_id: '2404.09889'
source_url: https://arxiv.org/abs/2404.09889
tags:
- tables
- table
- retrieval
- relevance
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of retrieving multiple tables
  for complex open-domain question answering, where tables often require joins that
  cannot be inferred from the query alone. The authors propose a re-ranking method
  using a mixed-integer program that jointly optimizes table-query relevance (both
  coarse and fine-grained) and table-table compatibility based on inferred join relationships.
---

# Is Table Retrieval a Solved Problem? Exploring Join-Aware Multi-Table Retrieval

## Quick Facts
- arXiv ID: 2404.09889
- Source URL: https://arxiv.org/abs/2404.09889
- Authors: Peter Baile Chen; Yi Zhang; Dan Roth
- Reference count: 6
- Key outcome: Proposed re-ranking method using MIP formulation significantly improves table retrieval F1 scores by up to 9.3% and end-to-end QA accuracy by up to 5.4% on Spider and Bird datasets.

## Executive Summary
This paper addresses the challenge of retrieving multiple tables for complex open-domain question answering where tables often require joins that cannot be inferred from the query alone. The authors propose a re-ranking method using a mixed-integer program that jointly optimizes table-query relevance (both coarse and fine-grained) and table-table compatibility based on inferred join relationships. Their method significantly improves table retrieval F1 scores by up to 9.3% and end-to-end QA accuracy by up to 5.4% compared to strong baselines on Spider and Bird datasets.

## Method Summary
The method formulates table retrieval as a mixed-integer program that considers both table-query relevance and table-table compatibility. Queries are decomposed into sub-queries using an LLM, and semantic similarity between sub-queries and table columns is computed with a bi-encoder model. A flow network is constructed to enforce connectedness among selected tables, ensuring joinability. The MIP solver jointly optimizes relevance scores and compatibility while maintaining constraints for connectivity and coverage.

## Key Results
- Achieved up to 9.3% improvement in table retrieval F1 scores compared to strong baselines
- Demonstrated up to 5.4% improvement in end-to-end QA accuracy
- Showed consistent performance gains across both Spider and Bird datasets
- Validated effectiveness of joint optimization over sequential approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Re-ranking using a mixed-integer program jointly optimizes table-query relevance and table-table compatibility based on inferred join relationships.
- Mechanism: The MIP formulation includes decision variables for table selection, sub-query coverage, and column joins. It maximizes coarse-grained relevance (ribi), fine-grained relevance (rqikdqik), and compatibility scores (ωkl_ij ckl_ij) under constraints ensuring connectivity and coverage.
- Core assumption: Joint optimization of relevance and compatibility leads to better table sets than sequential or relevance-only approaches.
- Evidence anchors:
  - [abstract]: "We use a novel re-ranking method formulated as a mixed-integer program that considers not only table-query relevance but also table-table relevance that requires inferring join relationships."
  - [section]: "We propose to solve the problem by adjusting the ranking produced by table-query relevance based on relationships inferred from table-table relevance to find the best retrieval result."
  - [corpus]: Weak - no direct evidence in related papers about MIP-based re-ranking.
- Break condition: If join relationships are already provided as gold constraints, the benefit of inferring them diminishes.

### Mechanism 2
- Claim: Fine-grained query-table relevance via sub-query decomposition improves retrieval by mapping parts of the query to specific columns.
- Mechanism: Queries are decomposed into (concept, attribute) pairs using an LLM, then semantic similarity between each sub-query and table columns is computed with a bi-encoder model. This ensures coverage of all required information.
- Core assumption: Query decomposition into sub-queries better aligns with how information is stored in normalized tables.
- Evidence anchors:
  - [section]: "we compute the score by first decomposing the query and then computing the semantic similarity between each sub-query and columns from candidate tables."
  - [section]: "The main purpose of measuring fine-grained table-query relevance is to find different columns... that can map to different parts of the query."
  - [corpus]: Weak - related papers focus on retrieval but not on this decomposition strategy.
- Break condition: If queries are too simple or tables are denormalized, decomposition may not add value.

### Mechanism 3
- Claim: Enforcing connectedness among selected tables ensures retrieved tables can be joined to answer the query.
- Mechanism: A flow network is constructed where tables are nodes and edges represent compatibility. The MIP enforces that the selected K tables form a connected subgraph, ensuring joinability.
- Core assumption: Connectedness in the compatibility graph implies the tables can be joined to produce the answer.
- Evidence anchors:
  - [section]: "we formulate the following connectedness problem on this graph: the K selected nodes are connected... This problem can be solved by augmenting the original graph and reducing it to a maximum flow problem."
  - [section]: "To ensure connectedness among the selected tables... we create an augmented graph G′... Then, we define the capacity h(m, n) for each edge between nodes m and n in N′."
  - [corpus]: Weak - related papers mention table retrieval but not connectedness enforcement via flow networks.
- Break condition: If the true answer requires tables that are not directly connected (e.g., via union operations), connectedness constraint may exclude them.

## Foundational Learning

- Concept: Mixed-Integer Programming (MIP)
  - Why needed here: MIP allows joint optimization of discrete decisions (which tables to select) and continuous objectives (relevance scores) under complex constraints.
  - Quick check question: What is the role of binary decision variables in the MIP formulation, and how do they enforce the constraint that selected tables must be joinable?

- Concept: Semantic similarity and embedding models
  - Why needed here: Semantic similarity between queries and table contents (columns) is computed using pre-trained embedding models to assess relevance beyond exact keyword matches.
  - Quick check question: How does the system compute fine-grained relevance between a sub-query and a table column, and why is this better than coarse-grained relevance alone?

- Concept: Graph connectivity and flow networks
  - Why needed here: Ensuring retrieved tables can be joined requires modeling them as a graph and enforcing that the selected subset is connected, solved via maximum flow.
  - Quick check question: Explain how the augmented graph and flow capacities ensure that the K selected tables form a connected subgraph.

## Architecture Onboarding

- Component map: Query encoder (bi-encoder) -> Sub-query decomposition module (LLM) -> Fine-grained relevance calculator (bi-encoder) -> Compatibility scorer -> MIP solver (Gurobi) -> LLM (GPT-3.5 Turbo)

- Critical path:
  1. Compute coarse-grained relevance (query vs. tables)
  2. Decompose query into sub-queries
  3. Compute fine-grained relevance (sub-queries vs. columns)
  4. Infer compatibility scores (table-table)
  5. Solve MIP to re-rank tables
  6. Feed top-K tables to LLM for SQL generation

- Design tradeoffs:
  - Coarse vs. fine-grained relevance: Coarse is fast but may miss distributed information; fine-grained ensures coverage but increases computation.
  - Joint vs. sequential optimization: Joint ensures global coherence but is more complex; sequential is simpler but may miss optimal combinations.
  - Inferred vs. gold join constraints: Inferred is more general but less accurate; gold is accurate but not always available.

- Failure signatures:
  - Poor retrieval despite good fine-grained relevance: May indicate sub-query decomposition is not aligning with table structure.
  - Connectedness constraint excludes correct tables: May occur if true answer requires union or non-connected tables.
  - MIP solver fails to find feasible solution: May indicate conflicting constraints or too restrictive parameters.

- First 3 experiments:
  1. Compare coarse-grained only vs. coarse + fine-grained relevance (JAR-D vs. baseline) to validate sub-query decomposition.
  2. Compare fine-grained only vs. full re-ranking (JAR-D vs. JAR-F) to validate table-table relevance importance.
  3. Evaluate with and without gold key-foreign key constraints (JAR-F vs. JAR-G) to measure impact of inferred join relationships.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can table retrieval systems effectively handle union operations between horizontally partitioned tables, in addition to join operations?
- Basis in paper: [inferred] The paper mentions that union operations are another common relationship among tables that was not explored in their work, and detecting unionable tables during retrieval differs from detecting joinable tables.
- Why unresolved: The paper focused solely on join-aware table retrieval and did not address how to handle union operations or how to combine joinable and unionable tables to return complete answers.
- What evidence would resolve it: A study demonstrating a method for detecting unionable tables during retrieval and evaluating its effectiveness on a dataset with horizontally partitioned tables.

### Open Question 2
- Question: How does the proposed re-ranking method perform when key foreign-key constraints are not provided and need to be inferred during test time?
- Basis in paper: [explicit] The paper states that key foreign-key constraints are not always specified and may need to be inferred during test time, and they performed evaluations on two setups: with and without the gold key foreign-key constraints provided by the original datasets.
- Why unresolved: While the paper showed that their method can effectively leverage provided constraints, it did not fully explore the performance when constraints need to be inferred.
- What evidence would resolve it: An evaluation of the re-ranking method's performance on a dataset without provided key foreign-key constraints, and a comparison with a baseline method that uses inferred constraints.

### Open Question 3
- Question: How does the proposed re-ranking method scale to larger table corpora and more complex queries?
- Basis in paper: [inferred] The paper evaluated their method on two datasets (Spider and Bird) with a limited number of tables and queries, but did not discuss how the method would perform on larger datasets or more complex queries.
- Why unresolved: The scalability of the method to real-world scenarios with larger table corpora and more complex queries is unclear.
- What evidence would resolve it: An evaluation of the method's performance on a larger dataset with more tables and complex queries, and an analysis of the method's runtime and memory usage as the dataset size increases.

## Limitations

- Reliance on LLM-based sub-query decomposition introduces variability and computational overhead
- Connectedness constraint may exclude valid table combinations requiring union operations
- Method assumes query information is distributed across multiple columns, which may not hold for all question types

## Confidence

- High confidence: Core finding that joint optimization improves retrieval performance (consistent F1 improvements across datasets)
- Medium confidence: Specific mechanisms (sub-query decomposition and connectedness enforcement) due to limited ablation studies
- Medium confidence: LLM-based components due to variability across model versions and prompts

## Next Checks

1. **Ablation on sub-query decomposition quality**: Systematically evaluate retrieval performance with varying sub-query decomposition quality (e.g., using gold vs. LLM-generated decompositions) to isolate the impact of decomposition accuracy on overall performance.

2. **Stress test on disconnected table scenarios**: Create test cases where the correct answer requires union operations or disconnected table sets to evaluate whether the connectedness constraint systematically excludes valid solutions.

3. **Scalability evaluation**: Measure inference time and memory usage for the MIP solver and LLM components on larger datasets (more tables, more complex queries) to assess practical deployment feasibility.