---
ver: rpa2
title: Automatically Generating Numerous Context-Driven SFT Data for LLMs across Diverse
  Granularity
arxiv_id: '2405.16579'
source_url: https://arxiv.org/abs/2405.16579
tags:
- context
- data
- arxiv
- queries
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AugCon, a novel method for automatically
  generating high-quality, diverse SFT data from custom corpus. The method uses Context-Split-Tree
  to recursively derive queries and split context, a scorer trained via contrastive
  learning to rank and filter queries, and a synergistic integration of self-alignment
  and self-improving to obtain high-fidelity responses.
---

# Automatically Generating Numerous Context-Driven SFT Data for LLMs across Diverse Granularity

## Quick Facts
- arXiv ID: 2405.16579
- Source URL: https://arxiv.org/abs/2405.16579
- Reference count: 40
- Automatically generates high-quality, diverse SFT data from custom corpus using Context-Split-Tree and contrastive learning

## Executive Summary
This paper introduces AugCon, a novel method for automatically generating high-quality, diverse SFT data from custom corpus. The method uses Context-Split-Tree to recursively derive queries and split context, a scorer trained via contrastive learning to rank and filter queries, and a synergistic integration of self-alignment and self-improving to obtain high-fidelity responses. Experiments on both human and automatic evaluations, including four widely-used benchmarks in English and Chinese, demonstrate the significant advantages of AugCon in producing high diversity, quality, and fidelity SFT data compared to several state-of-the-art methods.

## Method Summary
AugCon automatically generates SFT data through a three-stage pipeline. First, Context-Split-Tree (CST) recursively derives queries from context and splits it into semantically independent sub-contexts until reaching minimum granularity. Second, a scorer trained via contrastive learning ranks and filters the generated queries for quality and diversity. Finally, high-fidelity responses are generated through a synergistic integration of principle-driven self-alignment and self-improving, which uses alignment principles and optimal few-shot examples to guide the LLM.

## Key Results
- Significantly outperforms state-of-the-art methods in human evaluations on realism, diversity, relevance, accuracy, and satisfaction
- Achieves strong performance on automatic benchmarks (SQuAD1.1, TriviaQA, DROP, WebGLM-QA) in both English and Chinese
- Demonstrates high diversity and fidelity in generated SFT data across different granularity levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context-Split-Tree (CST) enables multi-granularity query generation by recursively deriving queries and splitting context.
- Mechanism: CST starts with an entire context, uses an LLM to derive a query from it, then splits the context into two semantically independent sub-contexts. Each sub-context recursively repeats the process until reaching minimum granularity.
- Core assumption: An LLM can accurately derive queries matching the granularity of its input context and semantically split that context into two independent parts.
- Evidence anchors:
  - [abstract] "Context-Split-Tree (CST), an innovative approach for recursively deriving queries and splitting context to cover full granularity"
  - [section] "we ask the LLM to split this context into two contexts that are as independent as possible"
  - [corpus] Weak - no direct corpus evidence showing the split quality or query granularity matching
- Break condition: The split fails when one child context's length is not less than its parent or when ROUGE-L similarity indicates hallucinations.

### Mechanism 2
- Claim: Contrastive learning trains a scorer to rank and filter queries for quality and diversity.
- Mechanism: The scorer is trained using positive samples (queries generated from well-designed prompts) and negative samples (queries generated from suboptimal prompts or fewer examples). It learns to assign higher scores to high-quality, diverse queries.
- Core assumption: Contrastive learning can effectively distinguish between high-quality and low-quality queries without requiring stronger LLMs like ChatGPT.
- Evidence anchors:
  - [abstract] "we train a scorer through contrastive learning to collaborate with CST to rank and refine queries"
  - [section] "we apply contrastive learning to train a scorer to judge the degree of adherence to instruct prompt and few-shot examples"
  - [corpus] Weak - no direct corpus evidence showing the scorer's effectiveness in ranking diverse queries
- Break condition: The scorer fails when it cannot differentiate between high-quality and low-quality queries or when it introduces bias toward certain query types.

### Mechanism 3
- Claim: Synergistic integration of self-alignment and self-improving produces high-fidelity responses.
- Mechanism: Alignment principles guide the LLM to produce responses matching human preferences, while self-improving uses random search to find optimal few-shot examples from human-annotated pairs.
- Core assumption: Alignment principles and optimal few-shot examples can guide LLMs to generate responses that are both high-quality and aligned with human values.
- Evidence anchors:
  - [abstract] "a synergistic integration of self-alignment and self-improving is introduced to obtain high-fidelity responses"
  - [section] "we employ a principle-driven self-alignment approach to guide the LLM in producing high-fidelity responses"
  - [corpus] Weak - no direct corpus evidence showing the effectiveness of this synergistic integration
- Break condition: The approach fails when the principles are not specific enough or when the self-improving process cannot find suitable examples.

## Foundational Learning

- Concept: Context granularity and semantic independence
  - Why needed here: CST relies on splitting contexts into semantically independent parts while maintaining appropriate granularity
  - Quick check question: If a context contains 100 sentences, what is the maximum number of questions CST can generate?
- Concept: Contrastive learning for ranking
  - Why needed here: The scorer uses contrastive learning to distinguish high-quality from low-quality queries
  - Quick check question: How does contrastive learning differ from traditional supervised learning in this context?
- Concept: Self-alignment and principle-driven generation
  - Why needed here: High-fidelity responses require alignment with human values and preferences
  - Quick check question: What role do alignment principles play in guiding LLM responses?

## Architecture Onboarding

- Component map: Corpus -> CST Engine -> Scorer -> Response Generator -> SFT Data
- Critical path: Context extraction → CST generation → Scorer ranking → Response generation
- Design tradeoffs:
  - Query quantity vs. quality: More queries increase coverage but may reduce quality
  - Granularity vs. independence: Finer granularity may reduce semantic independence
  - Prompt complexity vs. LLM capability: More complex prompts require more capable LLMs
- Failure signatures:
  - Low diversity: Scorer fails to differentiate queries or CST produces redundant splits
  - Low fidelity: Alignment principles are ineffective or self-improving cannot find good examples
  - Scalability issues: Context splitting becomes inefficient with very large contexts
- First 3 experiments:
  1. Test CST with simple contexts to verify query granularity matching
  2. Validate scorer ranking by comparing with human judgments on query quality
  3. Evaluate response fidelity with different alignment principle sets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the minimum length threshold λ impact the granularity distribution and diversity of generated queries?
- Basis in paper: [explicit] The paper mentions that λ is "like the lower bound and upper bound to control the granularity distribution of generated questions" and that "One can easily adjust the overall average granularity of generated queries by adjusting the length threshold."
- Why unresolved: While the paper discusses the theoretical impact of λ, it does not provide empirical results showing how different values of λ affect the distribution of query granularity or the diversity of the generated queries.
- What evidence would resolve it: Experiments showing the impact of different λ values on query granularity distribution, diversity metrics (e.g., ROUGE-L scores), and the performance of fine-tuned models on downstream tasks.

### Open Question 2
- Question: How does the Context-Split-Tree (CST) method compare to other recursive text splitting methods in terms of query quality and diversity?
- Basis in paper: [inferred] The paper claims that CST is "innovative" and produces queries with "high diversity, quality, and fidelity," but it does not directly compare CST to other recursive splitting methods.
- Why unresolved: The paper does not provide a direct comparison of CST to other recursive splitting methods, such as those used in retrieval-augmented generation (RAG) or other context-driven data generation approaches.
- What evidence would resolve it: A comparative study evaluating CST against other recursive splitting methods on metrics like query diversity, relevance, and the performance of fine-tuned models on downstream tasks.

### Open Question 3
- Question: How does the scorer trained via contrastive learning compare to other query ranking methods in terms of effectiveness and efficiency?
- Basis in paper: [explicit] The paper claims that the scorer is "data-efficient and can achieve effective performance without the need for stronger LLMs," but it does not compare it to other ranking methods.
- Why unresolved: The paper does not provide a direct comparison of the contrastive learning-based scorer to other query ranking methods, such as those based on heuristic algorithms or direct scoring.
- What evidence would resolve it: A comparative study evaluating the contrastive learning-based scorer against other ranking methods on metrics like query quality, diversity, and the computational efficiency of the ranking process.

## Limitations
- Heavy reliance on LLM capabilities for query generation and context splitting without analysis of performance across different model sizes
- Scorer training lacks transparency in implementation details and potential bias introduction
- Evaluation framework may not fully capture semantic independence of context splits or granularity coverage

## Confidence

**High Confidence**: The overall methodology framework is well-articulated and follows logical progression from query generation to response refinement. The integration of multiple components (CST, scorer, alignment principles) is coherent and technically sound.

**Medium Confidence**: The claims about achieving "high diversity, quality, and fidelity" are supported by evaluation results, but the evaluation methodology has limitations that prevent full confidence. The specific implementation details needed for exact reproduction are not fully specified.

**Low Confidence**: The scalability claims and performance guarantees across different corpus types and sizes are not well-supported. The paper doesn't provide analysis of how the method performs with extremely large contexts or highly specialized domains.

## Next Checks

1. **Query Diversity Validation**: Conduct a systematic analysis comparing the distribution of generated queries against the original corpus content. Measure the semantic coverage using topic modeling or clustering to verify that the generated queries span the full range of concepts present in the corpus.

2. **Context Split Independence Test**: Implement a quantitative evaluation of the semantic independence of context splits produced by CST. Use semantic similarity metrics (e.g., BERTScore, Sentence-BERT) to measure the overlap between parent and child contexts, and verify that the splits maintain meaningful independence while preserving relevant information.

3. **Scorer Generalization Assessment**: Test the scorer's ability to rank queries from unseen domains or corpus types. Generate queries from multiple diverse sources and evaluate whether the scorer maintains consistent quality rankings or shows bias toward specific query patterns learned during training.