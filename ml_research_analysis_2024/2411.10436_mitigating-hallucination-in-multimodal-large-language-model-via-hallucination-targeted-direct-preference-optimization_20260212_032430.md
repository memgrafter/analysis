---
ver: rpa2
title: Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted
  Direct Preference Optimization
arxiv_id: '2411.10436'
source_url: https://arxiv.org/abs/2411.10436
tags:
- hallucinations
- hallucination
- data
- preference
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucination in multimodal
  large language models (MLLMs), which limits their practical applications. The core
  method idea is Hallucination-targeted Direct Preference Optimization (HDPO), which
  constructs three types of preference pair data to address hallucinations from insufficient
  visual capabilities, long context generation, and multimodal conflicts.
---

# Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization

## Quick Facts
- arXiv ID: 2411.10436
- Source URL: https://arxiv.org/abs/2411.10436
- Reference count: 17
- Primary result: HDPO reduces hallucination rate from 35.1% to 15.8%, representing a 55% improvement

## Executive Summary
This paper introduces Hallucination-targeted Direct Preference Optimization (HDPO), a novel method for addressing hallucination problems in multimodal large language models (MLLMs). HDPO constructs three types of preference pair data to target hallucinations arising from insufficient visual capabilities, long context generation, and multimodal conflicts. The approach achieves state-of-the-art performance across multiple hallucination evaluation datasets while maintaining object coverage, demonstrating a substantial reduction in hallucination rates without compromising the model's ability to describe visual content.

## Method Summary
HDPO employs Direct Preference Optimization (DPO) framework to fine-tune MLLMs specifically for hallucination mitigation. The method constructs three distinct types of preference pair data: (1) pairs addressing insufficient visual capabilities by comparing outputs with and without proper visual grounding, (2) pairs targeting long context generation issues by comparing concise versus verbose responses, and (3) pairs resolving multimodal conflicts by comparing responses that correctly align visual and textual information. Through this targeted preference optimization, HDPO effectively reduces hallucinations while preserving the model's descriptive capabilities across various multimodal tasks.

## Key Results
- HDPO reduces hallucination rate from 35.1% to 15.8%, representing a 55% improvement over baseline
- The method achieves superior performance across multiple hallucination evaluation datasets
- HDPO maintains similar levels of object coverage compared to original models while significantly reducing hallucinations

## Why This Works (Mechanism)
HDPO works by leveraging preference-based learning to explicitly guide MLLMs away from hallucinatory outputs through carefully constructed training pairs. By targeting specific hallucination types with dedicated preference pairs, the method provides clear optimization signals that help the model distinguish between accurate and hallucinated responses. The direct preference optimization approach allows for more fine-grained control over output quality compared to traditional supervised fine-tuning, enabling the model to learn nuanced patterns that differentiate factual responses from hallucinations while preserving essential visual grounding and contextual understanding.

## Foundational Learning
1. **Direct Preference Optimization (DPO)**: A reinforcement learning technique that uses preference data to optimize model outputs; needed for enabling fine-grained control over response quality beyond supervised learning.
2. **Multimodal Grounding**: The ability of models to correctly align visual and textual information; critical for preventing cross-modal hallucinations where text contradicts visual evidence.
3. **Context Window Management**: Techniques for handling long-form generation without introducing hallucinations; essential for maintaining coherence in extended responses.
4. **Preference Pair Construction**: The methodology of creating contrastive training examples; fundamental to the HDPO approach for targeted hallucination mitigation.
5. **Hallucination Types in MLLMs**: Understanding different hallucination categories (visual, contextual, cross-modal); necessary for designing effective mitigation strategies.
6. **Automated Hallucination Metrics**: Evaluation frameworks for quantifying hallucination rates; crucial for measuring the effectiveness of mitigation approaches.

## Architecture Onboarding

**Component Map**: Visual Encoder -> Multimodal Fusion -> Text Decoder -> Preference Comparator -> Optimized Output

**Critical Path**: Input image and text → Visual feature extraction → Multimodal fusion → Text generation → Preference-based optimization → Refined output

**Design Tradeoffs**: HDPO trades increased training complexity and computational overhead for significantly improved hallucination mitigation. The preference pair construction requires additional data curation effort but provides more targeted optimization signals compared to traditional supervised fine-tuning approaches.

**Failure Signatures**: Potential failures include overfitting to specific preference patterns, reduced creativity in responses due to excessive optimization for factual accuracy, and possible degradation in handling novel or ambiguous visual scenarios that weren't well-represented in the preference training data.

**First Experiments**:
1. Test HDPO on a small subset of the MultiModalFact dataset to validate the preference optimization process
2. Compare hallucination rates before and after HDPO training on RecountBench to measure immediate effectiveness
3. Evaluate object coverage retention on MM-Vet to ensure descriptive capabilities are preserved

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation is constrained by a relatively small number of evaluation datasets (three primary datasets)
- Performance improvements are evaluated primarily through automated metrics that may not fully capture semantic accuracy or real-world usability
- The paper lacks extensive analysis of computational overhead and inference latency introduced by the HDPO training process

## Confidence
- **High confidence**: The core methodology of HDPO and its three preference pair data construction strategies is clearly described and technically sound
- **Medium confidence**: The reported performance improvements (55% reduction in hallucination rate) are likely accurate but may be somewhat inflated by evaluation dataset selection
- **Medium confidence**: The claim that HDPO maintains similar object coverage while reducing hallucinations is supported but could benefit from more diverse validation

## Next Checks
1. Evaluate HDPO on additional multimodal datasets beyond the three primary ones, particularly those with different visual domains and question types to assess generalization
2. Conduct human evaluation studies to validate automated metric results and assess real-world semantic accuracy improvements
3. Measure and report the computational overhead, training time, and inference latency impacts of HDPO compared to baseline models