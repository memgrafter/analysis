---
ver: rpa2
title: 'MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning
  Dataset for LVLMs'
arxiv_id: '2406.11833'
source_url: https://arxiv.org/abs/2406.11833
tags:
- image
- images
- answer
- data
- mmdu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMDU, a comprehensive benchmark and MMDU-45k,
  a large-scale instruction tuning dataset, designed to evaluate and improve Large
  Vision-Language Models (LVLMs) for multi-turn and multi-image conversations. The
  authors address the gap in existing benchmarks that focus on single-turn, single-image
  inputs and fail to adequately assess real-world human-AI interaction capabilities.
---

# MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs

## Quick Facts
- arXiv ID: 2406.11833
- Source URL: https://arxiv.org/abs/2406.11833
- Authors: Ziyu Liu; Tao Chu; Yuhang Zang; Xilin Wei; Xiaoyi Dong; Pan Zhang; Zijian Liang; Yuanjun Xiong; Yu Qiao; Dahua Lin; Jiaqi Wang
- Reference count: 40
- Primary result: MMDU benchmark and MMDU-45k instruction-tuning dataset developed to evaluate and improve LVLMs for multi-turn, multi-image conversations

## Executive Summary
This paper introduces MMDU, a comprehensive benchmark and MMDU-45k, a large-scale instruction tuning dataset, designed to evaluate and improve Large Vision-Language Models (LVLMs) for multi-turn and multi-image conversations. The authors address the gap in existing benchmarks that focus on single-turn, single-image inputs and fail to adequately assess real-world human-AI interaction capabilities. They construct the MMDU benchmark by clustering relevant images and text descriptions from Wikipedia and generating multi-turn question-answer pairs with the assistance of GPT-4o. The evaluation reveals a significant performance disparity between closed-source and open-source LVLMs, highlighting the need for conversational instruction tuning data. Fine-tuning open-source LVLMs on MMDU-45k significantly improves their performance on MMDU and existing benchmarks, generating longer and more accurate conversations.

## Method Summary
The MMDU benchmark is constructed by clustering related images and text descriptions from Wikipedia articles, then generating multi-turn question-answer pairs using GPT-4o with human supervision. The MMDU-45k dataset is created through a three-step process: (1) collecting relevant images and text descriptions from Wikipedia, (2) clustering them into coherent image-text groups, and (3) generating question-answer pairs using GPT-4o with human verification. The evaluation framework uses GPT-4o as a judge to assess model responses based on accuracy, relevance, and consistency across conversation turns. Fine-tuning experiments are conducted on open-source LVLMs using the MMDU-45k dataset to demonstrate performance improvements on both the MMDU benchmark and existing evaluation datasets.

## Key Results
- Significant performance gap identified between closed-source and open-source LVLMs on multi-turn, multi-image dialogue tasks
- Open-source LVLMs show marked improvement on MMDU benchmark after fine-tuning on MMDU-45k instruction-tuning dataset
- Fine-tuned models generate longer, more accurate conversations compared to pre-fine-tuning baselines
- Clustering accuracy for MMDU-45k ranges from 89-94% across sampled entries

## Why This Works (Mechanism)
The benchmark works by creating realistic multi-turn dialogue scenarios that require models to reason across multiple images and maintain context over extended conversations. The clustering approach groups semantically related visual content, while the question-answer generation process creates natural conversational flows. GPT-4o judging provides consistent evaluation criteria across different model responses, enabling fair comparison of performance improvements.

## Foundational Learning
- **Multi-image reasoning**: Ability to process and correlate information across multiple visual inputs simultaneously. Why needed: Real-world conversations often involve multiple visual references that must be integrated. Quick check: Model can accurately answer questions requiring comparison between different images in the same conversation.
- **Context maintenance**: Tracking conversational history across multiple turns while incorporating new visual information. Why needed: Human dialogues build upon previous exchanges, requiring models to maintain coherent context. Quick check: Model responses reference earlier conversation points appropriately in later turns.
- **Visual-textual alignment**: Mapping between visual elements and textual descriptions within the same context. Why needed: Effective multi-image dialogue requires understanding relationships between what is seen and what is described. Quick check: Model can correctly identify which textual description corresponds to which visual element.

## Architecture Onboarding
- **Component map**: Wikipedia content extraction -> Image-text clustering -> GPT-4o Q&A generation -> Human verification -> MMDU-45k dataset creation -> LVLM fine-tuning -> MMDU benchmark evaluation
- **Critical path**: Image-text clustering and GPT-4o Q&A generation are the most time-intensive components, requiring substantial computational resources and human oversight
- **Design tradeoffs**: Using GPT-4o for generation ensures high-quality dialogues but introduces potential bias from the language model's knowledge base; human verification adds quality control but limits scalability
- **Failure signatures**: Poor clustering leads to incoherent image-text groups; GPT-4o generation errors propagate through the dataset; human verification bottlenecks slow dataset construction
- **3 first experiments**: 1) Test clustering algorithm on sample Wikipedia content to verify grouping quality, 2) Generate sample Q&A pairs with GPT-4o to assess coherence and relevance, 3) Run small-scale fine-tuning on open-source LVLM to verify training pipeline functionality

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the clustering accuracy of MMDU-45k impact the overall quality and reliability of the benchmark?
- Basis in paper: [explicit] The paper mentions an experiment where five sets of 100 entries each were sampled from MMDU-45k and evaluated by GPT-4o for clustering accuracy, yielding rates of 94%, 90%, 92%, 89%, and 91%.
- Why unresolved: The paper does not provide a detailed analysis of how these clustering accuracy rates correlate with the performance of LVLMs on the benchmark.
- What evidence would resolve it: Further analysis correlating clustering accuracy with LVLM performance on MMDU would provide insights into the impact of clustering accuracy on benchmark quality.

### Open Question 2
- Question: What are the specific limitations of MMDU in assessing specialized domain expertise, such as mathematical problem-solving?
- Basis in paper: [explicit] The paper acknowledges that MMDU is designed to assess LVLMs' proficiency in daily scenarios rather than specialized domain expertise.
- Why unresolved: The paper does not provide detailed examples or analysis of how MMDU might fall short in evaluating specialized domain expertise.
- What evidence would resolve it: Additional case studies or experiments comparing MMDU's performance on daily scenarios versus specialized domains would highlight its limitations.

### Open Question 3
- Question: How do the evaluation criteria used by GPT-4o align with human judgment, and what are the potential biases introduced by using an LLM as a judge?
- Basis in paper: [explicit] The paper mentions a comparison between GPT-4o's scoring and human judgment, showing high similarity in Pearson, Spearman, and Kendall metrics.
- Why unresolved: The paper does not delve into potential biases or limitations of using an LLM as a judge, nor does it provide a detailed analysis of how the evaluation criteria might differ from human judgment.
- What evidence would resolve it: A more in-depth analysis of potential biases and a comparison of evaluation criteria between GPT-4o and human judges would provide insights into the reliability of using an LLM as a judge.

## Limitations
- Benchmark construction relies heavily on GPT-4o for question-answer pair generation, potentially introducing model-specific biases
- Clustering approach may miss important contextual relationships between visual elements or introduce noise
- Performance comparisons may be influenced by factors beyond benchmark design, such as architectural differences between models
- Instruction-tuning dataset construction lacks detailed quality control measures and validation procedures

## Confidence
- High confidence in the identification of the gap in existing benchmarks for multi-turn, multi-image conversations
- Medium confidence in the effectiveness of MMDU-45k for improving LVLM performance, pending replication studies
- Medium confidence in the benchmark's representativeness of real-world multi-image dialogue scenarios
- Low confidence in the generalizability of results across different LVLM architectures and fine-tuning approaches

## Next Checks
1. Conduct human evaluation of a subset of MMDU dialogues to assess quality, diversity, and real-world relevance compared to human-human multi-image conversations
2. Replicate the instruction-tuning experiments using different seed models and compare performance gains against fine-tuning on alternative multi-turn dialogue datasets
3. Test model performance on out-of-distribution multi-turn, multi-image tasks not included in MMDU to evaluate generalization capabilities