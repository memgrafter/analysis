---
ver: rpa2
title: Random Masking Finds Winning Tickets for Parameter Efficient Fine-tuning
arxiv_id: '2405.02596'
source_url: https://arxiv.org/abs/2405.02596
tags:
- masking
- random
- arxiv
- learning
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores parameter-efficient fine-tuning (PEFT) by introducing
  Random Masking, a method that randomly masks out model parameters during fine-tuning.
  Surprisingly, Random Masking with a carefully tuned learning rate can match the
  performance of standard PEFT methods like LoRA while using significantly fewer trainable
  parameters.
---

# Random Masking Finds Winning Tickets for Parameter Efficient Fine-tuning

## Quick Facts
- **arXiv ID**: 2405.02596
- **Source URL**: https://arxiv.org/abs/2405.02596
- **Reference count**: 40
- **Primary result**: Random masking with tuned learning rates matches LoRA performance using far fewer trainable parameters.

## Executive Summary
This paper introduces Random Masking, a parameter-efficient fine-tuning method that randomly masks out model parameters during fine-tuning. Surprisingly, with carefully tuned learning rates, Random Masking can match the performance of standard PEFT methods like LoRA while using significantly fewer trainable parameters. The key insight is that sparse masking requires larger learning rates, which work well due to a flatter loss landscape and more distant solutions induced by the masking. Theoretical analysis of a linear regression model supports these empirical findings, showing that sparse masking leads to smaller eigenvalues, larger stable learning rates, and more distant solutions.

## Method Summary
The method applies random binary masks to pretrained model parameters, training only the unmasked parameters using sparse matrix operations. The authors conduct a grid search over learning rates for each trainable parameter ratio, using AdamW optimizer for 5 epochs. They evaluate on OPT models (125m, 1.3b, 13b) across SuperGLUE benchmark datasets and additional tasks like SST-2, SQuAD, and DROP. The approach requires implementing sparse matrix storage via spops library and carefully tuning learning rates per masking ratio.

## Key Results
- Random Masking with as little as 0.001% trainable parameters achieves non-trivial accuracy
- Performance matches or exceeds LoRA while using 10-1000Ã— fewer trainable parameters
- Larger learning rates are necessary and beneficial for sparse masking due to flatter loss landscapes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random masking induces a flatter loss landscape, allowing larger learning rates without divergence
- Mechanism: Masking reduces trainable parameters, reducing Hessian's largest eigenvalue (smoothness coefficient). Smaller smoothness coefficient means flatter loss landscape, permitting larger learning rates
- Core assumption: Loss landscape flatness is directly tied to Hessian's largest eigenvalue
- Evidence anchors: Abstract states masking induces flatter loss landscape; optimization theory links smoothness coefficient to maximum stable learning rate
- Break condition: If Hessian's largest eigenvalue doesn't decrease with sparser masking, larger learning rates could still cause divergence

### Mechanism 2
- Claim: Sparse masking leads to more distant solutions, necessitating larger learning rates
- Mechanism: As trainable parameter ratio decreases, optimization travels longer path to reach minimizer because many dimensions are deactivated, requiring larger learning rates
- Core assumption: Distance to minimizer increases as more parameters are masked
- Evidence anchors: Abstract states flatter loss landscape gives rise to more distant solutions; iterates have to travel further to reach minimizer
- Break condition: If optimization can find nearby minimizers even with sparse masking, necessity for large learning rates would be reduced

### Mechanism 3
- Claim: Pretrained models' expressiveness compensates for parameter loss due to masking
- Mechanism: Pretrained models have large parameter redundancy, so even with significant masking, remaining trainable parameters are sufficient to fit downstream tasks
- Core assumption: Pretrained models have inherent expressiveness and generalization ability that allows adaptation with fewer trainable parameters
- Evidence anchors: Experiments show non-trivial accuracy with 0.001% trainable parameters; success attributed to expressive power of pretrained LLMs
- Break condition: If downstream task is too complex or requires specific parameter configurations that are masked out, expressiveness may not be sufficient

## Foundational Learning

- **Concept**: Loss landscape and Hessian spectrum
  - Why needed here: Understanding how masking affects loss landscape's flatness is crucial to explaining why larger learning rates work
  - Quick check question: What is the relationship between Hessian's largest eigenvalue and maximum stable learning rate in gradient descent?

- **Concept**: Parameter redundancy in neural networks
  - Why needed here: Recognizing that pretrained models have redundant parameters helps explain why masking out large fraction can still allow effective fine-tuning
  - Quick check question: How does parameter redundancy in pretrained models contribute to success of parameter-efficient fine-tuning methods?

- **Concept**: Lottery ticket hypothesis and neural network pruning
  - Why needed here: These concepts provide context for why masking certain parameters can still lead to effective models
  - Quick check question: How does lottery ticket hypothesis relate to idea that randomly masking parameters can still yield effective fine-tuning?

## Architecture Onboarding

- **Component map**: Pretrained model (OPT, Llama2) -> Random binary mask -> Sparse matrix implementation -> Optimizer (AdamW) -> Downstream task data and loss function

- **Critical path**: 
  1. Load pretrained model and generate random mask
  2. Implement sparse matrix to store trainable parameters
  3. Fine-tune model using optimizer with carefully tuned learning rate
  4. Evaluate performance on downstream task

- **Design tradeoffs**:
  - Sparser masking reduces computational cost but requires larger learning rates and may affect performance on complex tasks
  - Larger learning rates can speed up convergence but risk instability if loss landscape is not flat enough

- **Failure signatures**:
  - Divergence during training (learning rate too large for given masking ratio)
  - Poor performance on downstream tasks (masking ratio too sparse or learning rate not properly tuned)
  - Increased training time without improvement (learning rate too small for given masking ratio)

- **First 3 experiments**:
  1. Implement Random Masking on small pretrained model (OPT-125m) with moderate masking ratio (0.1%) and grid search for optimal learning rate
  2. Compare performance of Random Masking with full fine-tuning and LoRA on standard benchmark (SST-2) to validate effectiveness
  3. Analyze Hessian spectrum before and after masking to confirm flattening of loss landscape

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is theoretical explanation for why randomly masked parameters in pretrained model lead to flatter loss landscape?
- Basis in paper: Paper empirically observes masking induces flatter loss landscape and proposes theoretical analysis using overparameterized linear regression model, but doesn't fully explain why this occurs in complex models like LLMs
- Why unresolved: Paper provides empirical evidence and theoretical analysis for linear model but doesn't extend explanation to complex neural network architectures
- What evidence would resolve it: Theoretical framework explaining relationship between masking, loss landscape curvature, and model architecture in complex models; empirical studies comparing Hessian spectrum of masked and unmasked models across architectures

### Open Question 2
- Question: How does choice of mask structure (random vs. structured) affect performance and learning dynamics of Random Masking?
- Basis in paper: Paper briefly explores structured masking (selecting trainable parameters along columns of weight matrix) and finds degraded performance compared to random masking
- Why unresolved: Paper provides initial evidence that mask structure matters but doesn't provide comprehensive analysis of how different mask structures influence effectiveness
- What evidence would resolve it: Systematic study comparing performance and learning dynamics of Random Masking with various mask structures across different tasks and model architectures; theoretical analysis of how mask structure affects loss landscape and optimization process

### Open Question 3
- Question: What is optimal strategy for selecting trainable parameter ratio and learning rate in Random Masking?
- Basis in paper: Paper shows optimal learning rate increases as trainable parameter ratio decreases but doesn't provide general strategy for selecting these hyperparameters
- Why unresolved: Paper provides empirical evidence on relationship between learning rate and trainable parameter ratio but doesn't offer principled approach for selecting hyperparameters in practice
- What evidence would resolve it: Comprehensive study investigating impact of trainable parameter ratio and learning rate on performance across various tasks, model architectures, and dataset sizes; theoretical analysis of optimization dynamics under different hyperparameter settings

## Limitations
- Theoretical analysis relies heavily on linear regression model, which may not fully capture complexity of deep neural networks
- Does not explore impact of masking strategies beyond random selection (structured or adaptive masking)
- Claims success is primarily due to expressiveness of pretrained models rather than method itself are speculative

## Confidence
- **High Confidence**: Empirical finding that Random Masking with tuned learning rates can match LoRA performance while using fewer trainable parameters is well-supported by experimental results
- **Medium Confidence**: Theoretical explanation linking sparse masking to flatter loss landscapes and larger stable learning rates is plausible but requires further validation on more complex models and tasks
- **Low Confidence**: Claim that success is primarily due to expressiveness of pretrained models rather than method itself is speculative and needs more rigorous testing

## Next Checks
1. Conduct empirical measurements of Hessian spectrum before and after masking on deep neural networks (e.g., transformers) to validate theoretical prediction of flatter loss landscape with sparser masking
2. Extend Random Masking approach to include structured or adaptive masking strategies (e.g., masking based on parameter importance or gradient magnitude) and compare their performance against random masking
3. Evaluate Random Masking on increasingly complex downstream tasks (e.g., long-form question answering, code generation) to determine if method's effectiveness scales with task difficulty and if larger learning rates remain beneficial