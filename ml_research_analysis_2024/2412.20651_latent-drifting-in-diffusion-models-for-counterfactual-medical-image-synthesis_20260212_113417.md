---
ver: rpa2
title: Latent Drifting in Diffusion Models for Counterfactual Medical Image Synthesis
arxiv_id: '2412.20651'
source_url: https://arxiv.org/abs/2412.20651
tags:
- image
- diffusion
- medical
- generation
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating realistic medical
  images using diffusion models trained on natural images, which face a distribution
  shift when applied to medical domains. The authors propose Latent Drifting (LD),
  a method that adapts the latent space distribution of pre-trained diffusion models
  to better match medical image data.
---

# Latent Drifting in Diffusion Models for Counterfactual Medical Image Synthesis

## Quick Facts
- arXiv ID: 2412.20651
- Source URL: https://arxiv.org/abs/2412.20651
- Authors: Yousef Yeganeh; Azade Farshad; Ioannis Charisiadis; Marta Hasny; Martin Hartenberger; Björn Ommer; Nassir Navab; Ehsan Adeli
- Reference count: 40
- Primary result: Latent Drifting improves medical image generation quality by adapting pre-trained diffusion models to medical domains through latent space distribution modification

## Executive Summary
This paper addresses the challenge of generating realistic medical images using diffusion models trained on natural images, which face a distribution shift when applied to medical domains. The authors propose Latent Drifting (LD), a method that adapts the latent space distribution of pre-trained diffusion models to better match medical image data. LD is implemented as a hyperparameter that modifies the mean of the latent distribution during both forward and reverse diffusion processes. The method is evaluated on three longitudinal medical imaging datasets (brain MRI and chest X-ray) for counterfactual image generation tasks including disease progression and aging.

## Method Summary
Latent Drifting introduces a signed scalar hyperparameter δ that modifies the mean of the latent space distribution during fine-tuning and inference of pre-trained diffusion models. The method frames fine-tuning as a min-max optimization problem, where the goal is to minimize the distance between synthetic and target distributions while maximizing the probability that generated samples belong to the target domain. LD is combined with various fine-tuning approaches (Textual Inversion, DreamBooth, Custom Diffusion, and basic fine-tuning) and evaluated on brain MRI and chest X-ray datasets for counterfactual image generation tasks.

## Key Results
- LD significantly improves image quality metrics (FID, KID) compared to fine-tuning without LD
- Classification performance (AUC) is enhanced when using LD-generated counterfactual images
- The method enables realistic generation of counterfactual medical images conditioned on text and image inputs while preserving image fidelity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Latent Drifting modifies the mean of the latent distribution during both forward and reverse diffusion processes to adapt pre-trained models to medical image distributions
- Mechanism: The method introduces a signed scalar hyperparameter δ that is added to the mean of the latent space distribution during fine-tuning. This allows the model to adjust its learned representations to better match the target medical image distribution without requiring access to the original training data
- Core assumption: The distribution shift between natural and medical images can be effectively addressed by modifying the latent space distribution rather than the entire model architecture
- Evidence anchors:
  - [abstract]: "LD is implemented as a hyperparameter that modifies the mean of the latent distribution during both forward and reverse diffusion processes"
  - [section]: "For the fine-tuning through Latent Drifting, LD is added to the target zT of the forward process, as well as the reverse processes: pθ(xt-1|xt) = N(xt-1; µθ(xt, t) + δ, Σθ(xt, t))"
  - [corpus]: Weak evidence - corpus papers discuss medical image generation with diffusion models but do not specifically address latent distribution modification

### Mechanism 2
- Claim: Latent Drifting enables counterfactual image generation by framing fine-tuning as a min-max optimization problem
- Mechanism: The method poses fine-tuning as a counterfactual explanation optimization problem where the goal is to minimize the distance between synthetic and target distributions while maximizing the probability that generated samples belong to the target domain. The δ parameter is tuned via grid search to minimize this distance
- Core assumption: The distribution shift can be modeled as a counterfactual generation problem where the pre-trained model parameters need to be adjusted to generate samples from a different but related distribution
- Evidence anchors:
  - [abstract]: "Latent Drifting enables diffusion models to be conditioned for medical images fitted for the complex task of counterfactual image generation"
  - [section]: "We define LD as a hyperparameter in diffusion models to adapt the learned distribution of a pre-trained model Dθ to a new data distribution DGT"
  - [corpus]: Weak evidence - corpus papers discuss counterfactual generation but not specifically the min-max optimization framing used here

### Mechanism 3
- Claim: Latent Drifting improves sample diversity and quality by making the latent space distribution more stable during fine-tuning
- Mechanism: Without LD, the latent space distribution has high variance during fine-tuning, leading to unstable results. LD creates a more resilient and robust distribution that better handles the domain shift. This is demonstrated through visualizations showing channel-wise data distribution and latent space distribution changes
- Core assumption: A more stable latent space distribution leads to better adaptation of the pre-trained model to the target domain
- Evidence anchors:
  - [section]: "As it can be seen, the data and latent distribution have a high variance when fine-tuned withoutLD, while the model with LD reaches a stable point that is resilient and robust to distribution shift"
  - [section]: "The discrepancy between the generated data distribution Dθ and the target data distribution DGT can be quantified by a distance function d(DGT, Dθ)"
  - [corpus]: Weak evidence - corpus papers discuss diffusion model stability but not specifically the role of latent space distribution stability in domain adaptation

## Foundational Learning

- Concept: Diffusion models and latent diffusion models
  - Why needed here: The entire method builds on understanding how diffusion models work, particularly the latent space representation and the forward/reverse processes
  - Quick check question: Can you explain the difference between the forward diffusion process (adding noise) and the reverse process (denoising) in a diffusion model?

- Concept: Distribution shift and domain adaptation
  - Why needed here: The core problem being solved is the distribution shift between natural images (where pre-trained models are trained) and medical images (the target domain)
  - Quick check question: What are the key differences between natural images and medical images that would cause a distribution shift?

- Concept: Counterfactual explanations and generation
  - Why needed here: The method frames the fine-tuning problem as a counterfactual generation task, requiring understanding of how to generate samples that are similar to but different from existing data
  - Quick check question: How does a counterfactual explanation differ from an adversarial example in machine learning?

## Architecture Onboarding

- Component map: Pre-trained diffusion model (Stable Diffusion v1.4) -> Latent Drifting hyperparameter δ -> Fine-tuning methods (Textual Inversion, DreamBooth, Custom Diffusion, Basic Fine-tuning) -> Counterfactual generation -> Evaluation metrics (FID, KID, AUC)
- Critical path: Pre-trained model → Fine-tuning with LD → Counterfactual generation → Evaluation
- Design tradeoffs: Using LD adds a hyperparameter that needs to be tuned but enables better domain adaptation without requiring large amounts of medical data. The tradeoff is between model complexity and adaptation quality
- Failure signatures: High FID/KID scores indicating poor generation quality, low AUC scores on classification tasks, unstable latent space distributions during fine-tuning, or failure to converge during optimization
- First 3 experiments:
  1. Implement basic Latent Drifting on a simple pre-trained diffusion model using a small medical dataset to verify the δ parameter has the expected effect on latent space distribution
  2. Compare image generation quality with and without LD using standard metrics (FID, KID) on a validation set
  3. Test counterfactual generation capabilities by generating samples with different disease states or age conditions and evaluating with classification models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal drift parameter (δ) value for different medical imaging domains, and how does it vary across different anatomical structures?
- Basis in paper: [explicit] The paper mentions that δ is a hyperparameter that requires grid search optimization and discusses its effect on distribution matching, but does not provide systematic analysis of optimal values across different medical domains
- Why unresolved: The paper only provides limited ablation studies on δ in the supplement and does not systematically explore how optimal δ values might differ between brain MRI, chest X-rays, and other potential medical imaging applications
- What evidence would resolve it: Systematic evaluation of model performance across multiple δ values for different anatomical regions and imaging modalities, potentially revealing domain-specific optimal parameters

### Open Question 2
- Question: How does Latent Drifting perform on 3D medical image generation compared to 2D slices, and what architectural modifications would be needed?
- Basis in paper: [inferred] The paper focuses exclusively on 2D medical image generation and manipulation, while noting related work on 3D medical image generation with diffusion models, suggesting this is an unexplored area
- Why unresolved: The paper does not investigate 3D applications, and the extension of LD to volumetric data would require different architectural considerations and evaluation metrics
- What evidence would resolve it: Implementation and evaluation of LD on 3D medical imaging datasets (e.g., full volumetric brain MRI) with appropriate 3D metrics and comparison to existing 3D diffusion models

### Open Question 3
- Question: Can Latent Drifting be effectively combined with other distribution alignment techniques (e.g., domain adaptation, feature normalization) to further improve medical image generation?
- Basis in paper: [explicit] The paper presents LD as a standalone method for distribution matching but acknowledges it could be combined with other fine-tuning approaches, suggesting potential for integration with additional techniques
- Why unresolved: The paper only evaluates LD in combination with existing fine-tuning methods (Textual Inversion, DreamBooth, Custom Diffusion, basic fine-tuning) but does not explore combinations with domain adaptation or feature normalization techniques
- What evidence would resolve it: Comparative studies evaluating LD alone versus LD combined with domain adaptation methods, normalization techniques, or other distribution alignment approaches across multiple medical imaging tasks

## Limitations
- The method relies on modifying only the mean of the latent space distribution, which may not capture complex distribution shifts that involve higher-order statistics
- The grid search optimization for the δ parameter could become computationally expensive for high-dimensional latent spaces or when multiple fine-tuning methods are combined
- The effectiveness depends heavily on the quality of the pre-trained diffusion model's latent space representations, which were trained on natural images and may not capture all relevant medical image features

## Confidence
- High confidence in the mechanism of modifying latent space distribution to address domain shift, supported by clear mathematical formulation and implementation details
- Medium confidence in the generalizability of the method across different medical imaging modalities and tasks, as the evaluation is limited to brain MRI and chest X-ray datasets
- Low confidence in the scalability of the grid search optimization for δ when applied to larger, more complex medical datasets or when combining with multiple fine-tuning approaches simultaneously

## Next Checks
1. **Latent Space Distribution Analysis**: Conduct detailed analysis of the latent space distributions before and after LD application, including visualization of feature distributions and statistical tests to quantify the improvement in stability and alignment with target medical image distributions

2. **Cross-Modality Generalization Test**: Apply LD to a different medical imaging modality (e.g., CT scans or histopathology images) to evaluate whether the method generalizes beyond the brain MRI and chest X-ray datasets used in the original evaluation

3. **Ablation Study on δ Optimization**: Perform systematic ablation experiments to determine the sensitivity of LD performance to different δ optimization strategies, including comparing grid search with gradient-based optimization methods and evaluating the impact of δ initialization on convergence and final performance