---
ver: rpa2
title: 'Symmetry From Scratch: Group Equivariance as a Supervised Learning Task'
arxiv_id: '2410.03989'
source_url: https://arxiv.org/abs/2410.03989
tags:
- group
- learning
- equivariant
- equivariance
- architectures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces symmetry-cloning, a supervised learning method
  that enables general MLPs to learn group-equivariant behavior from group-equivariant
  architectures. The core idea is to train a group-agnostic model to mimic the input-output
  mapping of a group-equivariant model, thereby learning its symmetry properties without
  architectural constraints.
---

# Symmetry From Scratch: Group Equivariance as a Supervised Learning Task

## Quick Facts
- arXiv ID: 2410.03989
- Source URL: https://arxiv.org/abs/2410.03989
- Reference count: 5
- Authors: Haozhe Huang; Leo Kaixuan Cheng; Kaiwen Chen; Alán Aspuru-Guzik
- One-line primary result: Symmetry-cloning enables MLPs to learn group-equivariant behavior from group-equivariant architectures through supervised learning

## Executive Summary
This paper introduces symmetry-cloning, a novel supervised learning approach that enables group-agnostic MLPs to learn group-equivariant behavior by mimicking the input-output mappings of group-equivariant architectures. The method addresses a key limitation in equivariant neural networks by allowing flexible models to capture inductive biases without being constrained to specific symmetry groups. The authors demonstrate this on MNIST classification tasks with T(2) and C4 symmetries, showing that cloned MLPs can match or approach the performance of group-equivariant models on both symmetric and symmetry-breaking tasks.

## Method Summary
Symmetry-cloning trains a group-agnostic model to mimic the behavior of a group-equivariant architecture through supervised learning. The process involves first training an equivariant model on the target task, then generating input-output pairs by applying group transformations to inputs and recording the equivariant outputs. A group-agnostic MLP is then trained to replicate this mapping, effectively learning the symmetry properties. KL regularization is used to prevent weights from drifting too far from the learned initialization during downstream task training. The method is demonstrated on T(2) and C4 symmetries using MNIST, with models including mlp2cnn and mlp2gcnn variants.

## Key Results
- T(2)-cloned models achieve 77.83% accuracy on symmetry-breaking tasks compared to 97.53% for standard MLPs
- Symmetry-cloned models maintain competitive performance on symmetric tasks while adapting to symmetry-breaking scenarios
- MLP-Mixer variants can be symmetry-cloned but require significantly more computation time than CNN-based approaches
- KL regularization effectively prevents weight drift while allowing task-specific adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Group-agnostic models can learn approximate equivariance by mimicking input-output mappings of group-equivariant architectures
- Mechanism: Supervised learning maps between input transformations and equivariant outputs creates implicit symmetry learning without architectural constraints
- Core assumption: Group-equivariant models produce consistent outputs under group transformations that can be captured by universal function approximators
- Evidence anchors: [abstract] "we show that general machine learning architectures (i.e., MLPs) can learn symmetries directly as a supervised learning task from group equivariant architectures"
- Break condition: When the universal approximator lacks sufficient capacity to represent the equivariant mapping, or when symmetry-breaking is too severe for the model to learn meaningful patterns

### Mechanism 2
- Claim: KL regularization prevents learned weights from drifting too far from symmetry-initialized parameters during downstream task training
- Mechanism: Constraining weight distribution evolution maintains learned equivariant structure while allowing task-specific adaptation
- Core assumption: Initial symmetry learning creates useful parameter distributions that can be preserved with regularization
- Evidence anchors: [section] "we use a KL constraint to prevent the weight distribution from drifting away from the equivariant initialization too far too quickly"
- Break condition: When the KL regularization strength β is too high, preventing necessary adaptation to task-specific patterns; when symmetry-initialized parameters are poor starting points for the specific downstream task

### Mechanism 3
- Claim: Symmetry-cloning enables universal function approximators to capture both symmetric and symmetry-breaking features without hard architectural constraints
- Mechanism: Learning equivariant behavior through supervised mimicry provides inductive bias while maintaining flexibility to adapt to real-world asymmetries
- Core assumption: Group-equivariant architectures encode symmetries that can be extracted through input-output observation
- Evidence anchors: [abstract] "This simple formulation enables machine learning models with group-agnostic architectures to capture the inductive bias of group-equivariant architectures"
- Break condition: When the equivariant architecture being mimicked is itself approximate due to discretization, or when the supervised learning process fails to converge to meaningful equivariant parameters

## Foundational Learning

- Concept: Group theory fundamentals (groups, subgroups, representations, equivariance)
  - Why needed here: Understanding how symmetry groups like T(2) and C4 operate is essential for designing and evaluating symmetry-cloning
  - Quick check question: What is the difference between a group and a representation of a group in the context of equivariant neural networks?

- Concept: Convolutional neural network architecture and group convolutions
  - Why needed here: The method builds on group-equivariant convolutions as the source of symmetry knowledge to be cloned
  - Quick check question: How does a standard 2D convolution enforce translational equivariance, and how is this extended in group convolutions?

- Concept: Universal approximation theorem and model capacity
  - Why needed here: The core claim relies on universal function approximators being able to learn the equivariant mapping
  - Quick check question: Under what conditions can a universal approximator learn a specific function mapping, and what factors affect convergence?

## Architecture Onboarding

- Component map: Input → MLP layers (symmetry-cloned) → Pooling → Classification head
- Critical path: 1) Train equivariant architecture → 2) Generate training data (inputs + transformations → outputs) → 3) Train group-agnostic model to mimic → 4) Apply KL regularization → 5) Evaluate on downstream tasks
- Design tradeoffs:
  - More MLP layers increase capacity but also training complexity and computation time
  - Higher KL regularization preserves symmetry but may limit task adaptation
  - Direct cloning of entire architectures vs. layer-by-layer cloning affects scalability
- Failure signatures:
  - Feature maps not maintaining equivariance under transformations
  - Poor performance on both symmetric and symmetry-breaking tasks
  - Extremely long training times without convergence
  - Weight distributions drifting significantly from initialized values
- First 3 experiments:
  1. Train 9-block mlp2cnn on T(2) cloning task and visualize feature maps for translated inputs vs. CNN baseline
  2. Train mlp2gcnn on C4 cloning task and evaluate performance on both rotational symmetry and symmetry-breaking tasks
  3. Apply symmetry-cloned MLP layers as initialization for MNIST classification with KL regularization, comparing freeze vs. unfreeze scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical bounds on the approximate clonability of any symmetry with any unconstrained model?
- Basis in paper: [inferred] The paper states "there are no theoretical bounds on the approximate clonability of any symmetry with any unconstrained model" and acknowledges that "Most tunable hyperparameters were chosen arbitrarily."
- Why unresolved: The paper is primarily empirical and does not provide theoretical guarantees on convergence or data requirements for symmetry-cloning.
- What evidence would resolve it: Mathematical proofs establishing bounds on how well unconstrained models can learn group-equivariant behavior from group-equivariant architectures, or empirical studies systematically varying model architectures and hyperparameters to establish performance limits.

### Open Question 2
- Question: How can the symmetry-cloning process be optimized to speed up convergence and reduce computational costs?
- Basis in paper: [explicit] The paper states "we aim to optimize the symmetry-cloning process, explore ways to speed up the convergence process" and notes that "We notice a significant increase in computation time required for symmetry-cloning a mlpmixer model."
- Why unresolved: The authors acknowledge computational challenges with MLP-Mixer models and do not provide optimization strategies beyond basic supervised learning.
- What evidence would resolve it: Comparative studies of different optimization techniques, architectural modifications, or training strategies that reduce the number of iterations or computational resources needed for successful symmetry-cloning.

### Open Question 3
- Question: Can symmetry-cloning be extended to more complex groups beyond planar symmetries, such as SO(3) for molecular applications?
- Basis in paper: [explicit] The paper states "Extending symmetry-cloning to SO(3) could allow for more optimized learning of molecules in voxel representations using existing 3D-Unet architectures" as future work.
- Why unresolved: The current work focuses on 2D planar signals (T(2) and C4 groups), and while the authors suggest extending to 3D, they have not demonstrated this.
- What evidence would resolve it: Successful application of symmetry-cloning methods to SO(3)-equivariant architectures for 3D molecular data, with performance comparisons to existing 3D-equivariant models.

## Limitations

- The method requires first training a group-equivariant architecture solely to generate training data for the symmetry-cloning process, creating computational overhead
- Evaluation is limited to controlled synthetic MNIST tasks with well-defined symmetry transformations, raising questions about generalization to real-world datasets
- The paper provides limited theoretical justification for when symmetry-cloning will succeed versus fail, particularly regarding universal function approximator capacity requirements

## Confidence

- **High Confidence**: The basic feasibility of symmetry-cloning on controlled synthetic tasks (MNIST with T(2) and C4 symmetries) is well-demonstrated through quantitative results
- **Medium Confidence**: The claim that symmetry-cloning enables flexible adaptation to symmetry-breaking tasks while maintaining performance on symmetric tasks is supported but requires more diverse validation
- **Low Confidence**: The assertion that this approach generalizes to arbitrary symmetry groups and real-world datasets lacks sufficient empirical support

## Next Checks

1. **Capacity Sensitivity Analysis**: Systematically vary MLP layer sizes and numbers to identify the minimum architecture required for successful symmetry cloning, establishing practical limits of the approach

2. **Real-World Dataset Testing**: Apply symmetry-cloning to a non-synthetic dataset (e.g., CIFAR-10 with natural transformations, or molecular property prediction) to assess generalization beyond controlled MNIST experiments

3. **Symmetry Group Generalization**: Test whether models trained via symmetry-cloning on one symmetry group (e.g., T(2)) can effectively transfer to related but different groups (e.g., T(3) or other discrete rotation groups), evaluating the scope of learned equivariance