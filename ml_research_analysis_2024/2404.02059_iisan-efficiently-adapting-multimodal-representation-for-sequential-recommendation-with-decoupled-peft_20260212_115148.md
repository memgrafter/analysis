---
ver: rpa2
title: 'IISAN: Efficiently Adapting Multimodal Representation for Sequential Recommendation
  with Decoupled PEFT'
arxiv_id: '2404.02059'
source_url: https://arxiv.org/abs/2404.02059
tags:
- efficiency
- iisan
- arxiv
- peft
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IISAN addresses the inefficiency of multimodal sequential recommendation
  models by decoupling parameter-efficient fine-tuning (PEFT) from pre-trained foundation
  models. The core idea involves creating separate intra-modal and inter-modal Side
  Adapted Networks (SANs) that operate independently from frozen multimodal backbones,
  enabling caching strategies to reduce computation.
---

# IISAN: Efficiently Adapting Multimodal Representation for Sequential Recommendation with Decoupled PEFT

## Quick Facts
- **arXiv ID:** 2404.02059
- **Source URL:** https://arxiv.org/abs/2404.02059
- **Reference count:** 40
- **Primary result:** Reduces GPU memory usage from 47GB to 3GB and training time per epoch from 443s to 22s while maintaining performance comparable to full fine-tuning

## Executive Summary
IISAN introduces a novel approach to multimodal sequential recommendation by decoupling parameter-efficient fine-tuning (PEFT) modules from frozen foundation models. The method employs separate intra-modal and inter-modal Side Adapted Networks (SANs) that operate independently from pre-trained multimodal backbones, enabling caching strategies to dramatically reduce computation. IISAN achieves performance comparable to full fine-tuning while reducing GPU memory usage by over 15x and training time by more than 20x. The work also introduces TPME (Training-time, Parameter, and GPU Memory Efficiency), a composite metric that provides a more comprehensive assessment of model efficiency than parameter count alone.

## Method Summary
IISAN addresses the inefficiency of multimodal sequential recommendation models by creating a decoupled architecture where trainable SANs operate separately from frozen multimodal backbones (BERT, ViT, CLIP). The method consists of intra-modal SANs for visual and textual modality adaptation, an inter-modal SAN for cross-modal interactions, and optional LayerDrop and caching strategies. During training, only the SANs are updated through backpropagation while backbone parameters remain frozen, enabling substantial efficiency gains. The caching strategy stores pre-extracted backbone features to eliminate redundant forward passes, further improving practical efficiency.

## Key Results
- Reduces GPU memory usage from 47GB to 3GB during training
- Decreases training time per epoch from 443s to 22s
- Achieves HR@10 and NDCG@10 scores comparable to full fine-tuning (FFT)
- Introduces TPME metric that corrects the misconception that parameter efficiency alone represents overall model efficiency

## Why This Works (Mechanism)

### Mechanism 1
Decoupling PEFT modules from frozen backbone reduces computation graph size. By separating trainable SANs from frozen multimodal encoders, backward pass only traverses SAN layers instead of full backbone. This works because foundation model parameters remain static during training, eliminating need for gradient computation through them. The evidence shows IISAN's unique approach to omitting backward propagation through large foundation models.

### Mechanism 2
Caching pre-extracted backbone features eliminates redundant forward passes. Since decoupled SANs operate on frozen backbone outputs, same item representations can be cached and reused across training iterations. This works because item representations remain constant when backbone is frozen. The caching strategy significantly enhances practical efficiency by alleviating memory constraints for storing foundation model weights.

### Mechanism 3
Intra-modal and inter-modal SANs provide complementary adaptation capabilities. Separate SANs handle modality-specific adaptation (intra) and cross-modal interactions (inter), capturing richer multimodal representations than single-path approaches. This works because different adaptation needs exist within and between modalities. Ablation studies show removing either component reduces performance.

## Foundational Learning

- **Concept: Parameter-efficient fine-tuning (PEFT) fundamentals**
  - Why needed here: IISAN builds on PEFT concepts but innovates with decoupled architecture
  - Quick check question: What distinguishes embedded PEFT (Adapter, LoRA) from decoupled approaches?

- **Concept: Multimodal representation learning**
  - Why needed here: IISAN specifically handles text-image interactions in sequential recommendation
  - Quick check question: How do intra-modal and inter-modal representations differ in multimodal models?

- **Concept: Transformer computational complexity**
  - Why needed here: Understanding why backbone computations dominate memory/time costs
  - Quick check question: What makes self-attention O(n²) and why does this matter for long sequences?

## Architecture Onboarding

- **Component map:** Frozen multimodal backbone (BERT/ViT/CLIP) → produces static item embeddings → Intra-modal SANs (text, image) → modality-specific adaptation → Inter-modal SAN → cross-modal interaction modeling → LayerDrop → optional layer reduction for efficiency → Caching layer → optional storage of backbone outputs → Sequential encoder → final recommendation predictions

- **Critical path:**
  1. Preprocess items through frozen backbone (once if caching)
  2. Forward pass through SANs for adaptation
  3. Fusion and prediction
  4. Only backward through SANs (no backbone gradients)

- **Design tradeoffs:**
  - Caching vs. flexibility: Caching saves time but prevents dynamic backbone adaptation
  - LayerDrop vs. accuracy: Dropping layers improves efficiency but may lose information
  - SAN capacity vs. parameter efficiency: Larger SANs may capture more but reduce efficiency gains

- **Failure signatures:**
  - High GPU memory usage → backbone not properly frozen or caching not implemented
  - Slow training → missing caching or SANs too large
  - Performance degradation → LayerDrop too aggressive or SANs under-parameterized

- **First 3 experiments:**
  1. Baseline: FFT with full fine-tuning to establish performance ceiling
  2. Caching test: Compare uncached vs cached training time/memory
  3. SAN ablation: Remove intra-modal or inter-modal SANs to verify their contributions

## Open Questions the Paper Calls Out
- **Open Question 1:** How does the LayerDrop rate affect the balance between efficiency gains and performance degradation in IISAN?
- **Open Question 2:** How would IISAN perform when applied to multimodal retrieval tasks beyond sequential recommendation?
- **Open Question 3:** What is the optimal weighting strategy for the TPME metric components in different deployment scenarios?

## Limitations
- Assumes frozen backbone parameters remain optimal throughout training, potentially missing task-specific fine-tuning opportunities
- LayerDrop effectiveness not thoroughly validated across different backbone architectures
- TPME metric uses fixed weights that may not generalize across different computational constraints and task priorities
- Empirical evaluation limited to three Amazon datasets, restricting generalizability to other recommendation domains

## Confidence
- **High Confidence:** The core decoupling mechanism and caching strategy are well-established principles that directly address known inefficiencies in multimodal fine-tuning
- **Medium Confidence:** The TPME metric provides valuable perspective, though its universal applicability requires further validation across diverse recommendation scenarios
- **Medium Confidence:** Performance claims (HR@10, NDCG@10) are well-supported within the tested datasets, but cross-dataset generalization remains unproven

## Next Checks
1. **Cross-Dataset Generalization Test:** Evaluate IISAN on non-Amazon multimodal recommendation datasets (e.g., fashion, travel, or multimedia platforms) to verify robustness across different domain characteristics and item modalities

2. **Dynamic Backbone Adaptation Experiment:** Implement a hybrid approach where backbone parameters are partially updated during training to assess whether the efficiency gains justify the performance trade-off compared to full fine-tuning

3. **TPME Parameter Sensitivity Analysis:** Systematically vary the TPME weighting factors (currently 0.45, 0.1, 0.45) across different computational resource constraints to determine optimal weight configurations for various deployment scenarios (mobile vs. data center vs. edge computing)