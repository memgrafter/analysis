---
ver: rpa2
title: Deep progressive reinforcement learning-based flexible resource scheduling
  framework for IRS and UAV-assisted MEC system
arxiv_id: '2408.01248'
source_url: https://arxiv.org/abs/2408.01248
tags:
- agent
- task
- resource
- number
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a deep progressive reinforcement learning-based
  flexible resource scheduling framework for intelligent reflecting surface (IRS)
  and unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) systems.
  The goal is to minimize the system's energy consumption by jointly optimizing UAV
  locations, IRS phase shift, task offloading, and resource allocation with a variable
  number of UAVs.
---

# Deep progressive reinforcement learning-based flexible resource scheduling framework for IRS and UAV-assisted MEC system

## Quick Facts
- **arXiv ID:** 2408.01248
- **Source URL:** https://arxiv.org/abs/2408.01248
- **Reference count:** 36
- **Key outcome:** Deep progressive RL framework minimizes energy consumption in IRS/UAV-assisted MEC by jointly optimizing UAV locations, IRS phase shifts, task offloading, and resource allocation with variable UAV numbers.

## Executive Summary
This paper proposes a deep progressive reinforcement learning-based flexible resource scheduling (FRES) framework for intelligent reflecting surface (IRS) and unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) systems. The framework addresses the challenge of minimizing system energy consumption by jointly optimizing UAV locations, IRS phase shifts, task offloading decisions, and resource allocation while handling a variable number of UAVs. The proposed approach includes a multi-task agent to handle mixed integer nonlinear programming, a progressive scheduler to adapt to changing UAV numbers, and a light taboo search to enhance global optimization. Simulation results demonstrate the framework's superiority in achieving real-time and optimal resource scheduling in dynamic MEC environments.

## Method Summary
The FRES framework employs deep progressive reinforcement learning to minimize energy consumption in IRS and UAV-assisted MEC systems. It uses a multi-task agent with shared layers and two output heads - one for classification (task offloading) and one for regression (resource allocation) - trained simultaneously. The progressive scheduler dynamically adjusts the neural network architecture when UAV numbers change, adding or removing neurons to prevent catastrophic forgetting. A light taboo search (LTS) refines DRL actions by exploring neighborhoods of solutions based on channel gain guidance. The system operates in a dynamic environment where the number of UAVs can vary, requiring the agent to adapt without retraining from scratch.

## Key Results
- The FRES framework achieves superior energy consumption performance compared to conventional DRL algorithms and heuristic methods in IRS/UAV-assisted MEC systems.
- Progressive scheduling successfully adapts to varying numbers of UAVs without catastrophic forgetting, maintaining performance across different system configurations.
- Light taboo search enhances global optimization by refining DRL actions and escaping local minima through channel-guided exploration.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Progressive scheduler adjusts neural network architecture to prevent catastrophic forgetting when UAV numbers change.
- **Mechanism:** When UAVs increase, the scheduler adds neurons to shared and standalone layers using specific equations; when UAVs decrease, it removes neurons by setting weights and biases to zero.
- **Core assumption:** Learned knowledge from old tasks is preserved in retained neurons, allowing immediate reuse without retraining.
- **Evidence anchors:** Abstract mentions "immune to catastrophic forgetting"; section states "agent do not need to retrain again."
- **Break condition:** Performance degrades if removed neurons contained critical features for old tasks.

### Mechanism 2
- **Claim:** Multi-task agent improves solution quality by jointly optimizing classification and regression tasks.
- **Mechanism:** Agent uses two output heads with different loss functions - cross-entropy for offloading decisions and MSE for resource allocation - optimized simultaneously.
- **Core assumption:** Shared representation captures features useful for both tasks, making joint training more effective than separate models.
- **Evidence anchors:** Abstract describes "novel multi-task agent"; section explains "two different losses to optimize different task heads."
- **Break condition:** Multi-task benefit disappears if shared representation becomes too task-specific.

### Mechanism 3
- **Claim:** Light Taboo Search enhances global search by escaping local minima using channel-guided moves.
- **Mechanism:** LTS generates solution neighborhoods by randomly changing UE offloading decisions, selecting the best neighbor using channel gain as the "light move operator," with taboo list preventing cycling.
- **Core assumption:** Channel gain strongly correlates with energy efficiency, making greedy moves guided by it lead to better global solutions.
- **Evidence anchors:** Abstract mentions "light taboo search to enhance global search"; section states "LTS employs the light move operator with highest channel gain."
- **Break condition:** Performance worsens if channel gain poorly predicts energy consumption.

## Foundational Learning

- **Concept: Mixed Integer Nonlinear Programming (MINLP)**
  - Why needed here: Problem jointly optimizes discrete offloading decisions and continuous resource allocations under non-convex constraints.
  - Quick check question: What distinguishes an MINLP from a pure integer or continuous problem?

- **Concept: Deep Reinforcement Learning (DRL) State-Action-Reward**
  - Why needed here: Agent learns to map environment states (channel gains, task demands) to resource scheduling actions to minimize energy consumption.
  - Quick check question: In this system, what constitutes the state, action, and reward?

- **Concept: Catastrophic Forgetting in Continual Learning**
  - Why needed here: Number of UAVs changes dynamically; without progressive learning, agent loses ability to handle previous UAV counts.
  - Quick check question: How does progressive learning prevent catastrophic forgetting when network structure changes?

## Architecture Onboarding

- **Component map:** Environment -> Multi-task agent -> Action -> LTS refinement -> Apply to MEC -> Observe reward -> Store transition -> Train agent
- **Critical path:** State → Multi-task agent → Action → LTS refinement → Apply to MEC → Observe reward → Store transition → Train agent
- **Design tradeoffs:**
  - Progressive learning vs. fixed architecture: Saves retraining time but adds complexity
  - LTS exploration vs. pure DRL: Improves global search at cost of extra computation
  - Multi-task vs. separate agents: Shares representation but risks interference
- **Failure signatures:**
  - Catastrophic forgetting: Performance drops when UAV count changes
  - LTS stalls: Energy consumption plateaus early due to poor neighborhood exploration
  - Multi-task interference: One task's loss dominates, degrading the other's performance
- **First 3 experiments:**
  1. Validate progressive scheduler: Train with 3 UAVs, then add one; measure loss/reward stability
  2. Compare LTS vs. no refinement: Run identical scenarios with and without LTS; record energy consumption convergence
  3. Multi-task vs. single-task ablation: Replace multi-head agent with two independent agents; compare final energy efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does FRES performance change when UAV numbers vary beyond tested range (2-10 UAVs)?
- **Basis in paper:** Explicit mention of progressive scheduler adapting to varying UAV numbers, but scalability limits unexplored
- **Why unresolved:** Experiments only tested limited UAV range; framework scalability to larger UAV fleets not addressed
- **What evidence would resolve it:** Additional simulations with broader UAV range comparing energy consumption, training time, and solution quality

### Open Question 2
- **Question:** How does FRES perform in dynamic channel environments with varying interference levels?
- **Basis in paper:** Explicit mention of dynamic environment operation, but interference level impact not investigated
- **Why unresolved:** Experiments use fixed channel model; framework robustness to real-world interference variations unknown
- **What evidence would resolve it:** Simulations with different channel models and interference scenarios comparing energy consumption and task completion rates

### Open Question 3
- **Question:** How does FRES performance compare to other advanced optimization algorithms for MEC resource scheduling?
- **Basis in paper:** Explicit comparison to DRL algorithms and heuristic methods, but other advanced optimization techniques not explored
- **Why unresolved:** Comparison limited to specific algorithm set; relative performance to other state-of-the-art methods unclear
- **What evidence would resolve it:** Implementing and comparing FRES with other advanced optimization algorithms like metaheuristics or distributed optimization methods

## Limitations

- Progressive learning mechanism claims lack empirical validation through ablation studies
- Light Taboo Search effectiveness relies on unproven correlation between channel gain and energy efficiency
- Network architecture specifics remain underspecified beyond basic layer counts
- Simulation results may favor proposed method through baseline choice rather than fundamental superiority

## Confidence

- Progressive learning mechanism: Medium - architectural description clear but validation limited
- Multi-task agent benefits: Low - no comparison with single-task alternatives provided
- LTS effectiveness: Low - correlation between channel gain and energy efficiency unproven
- Overall framework performance: Medium - simulation results show improvement but baseline choices may favor method

## Next Checks

1. Conduct ablation study removing progressive scheduler to quantify catastrophic forgetting impact across UAV count changes
2. Implement single-task variants of the agent to empirically verify multi-task learning benefits
3. Analyze LTS contribution by comparing energy consumption trajectories with/without taboo search refinement across multiple problem instances