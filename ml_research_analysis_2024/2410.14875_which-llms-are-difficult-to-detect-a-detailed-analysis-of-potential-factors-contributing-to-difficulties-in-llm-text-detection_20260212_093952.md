---
ver: rpa2
title: Which LLMs are Difficult to Detect? A Detailed Analysis of Potential Factors
  Contributing to Difficulties in LLM Text Detection
arxiv_id: '2410.14875'
source_url: https://arxiv.org/abs/2410.14875
tags:
- texts
- llms
- openai
- dataset
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluated AI-generated text detection
  across multiple large language models (LLMs) and writing domains. Using two datasets
  spanning scientific writing, opinion statements, story generation, and student essays,
  researchers trained 92 classifiers to distinguish AI-generated from human-written
  text.
---

# Which LLMs are Difficult to Detect? A Detailed Analysis of Potential Factors Contributing to Difficulties in LLM Text Detection

## Quick Facts
- **arXiv ID**: 2410.14875
- **Source URL**: https://arxiv.org/abs/2410.14875
- **Reference count**: 11
- **Primary result**: Detection difficulty varies significantly by domain and LLM family, with scientific writing proving most challenging and OpenAI models consistently the most difficult to detect

## Executive Summary
This study systematically evaluated AI-generated text detection across multiple large language models and writing domains using two datasets spanning scientific writing, opinion statements, story generation, and student essays. Researchers trained 92 classifiers to distinguish AI-generated from human-written text, revealing that detection difficulty varies significantly by both domain and LLM family. Scientific writing proved most challenging overall, with the lowest mean AUC score of 0.852 for detecting OpenAI-generated texts. The study found that classifiers trained on one LLM family often generalized reasonably well to detect texts from other families, though performance dropped significantly for Llama and GLM families when using non-specialized classifiers.

## Method Summary
The research employed a comprehensive experimental design involving 92 classifiers trained to distinguish AI-generated from human-written text across four domains: scientific writing, opinion statements, story generation, and student essays. The study used multiple datasets and evaluated five LLM families (OpenAI, Llama, PaLM, Claude, GLM) to assess detection performance. Text complexity was measured using entropy and out-of-vocabulary ratios to compare AI-generated and human-written texts, particularly in student essays. The analysis systematically examined how detection accuracy varied across different domains and LLM families, with special attention to generalization between different model types.

## Key Results
- Scientific writing proved most difficult to detect overall, with lowest mean AUC score of 0.852 for OpenAI-generated texts
- OpenAI's GPT-3.5 and GPT-4o models were consistently the most difficult to detect across all domains except when using OpenAI-specific classifiers
- In student essay detection, OpenAI models achieved near-human text complexity as measured by entropy and OOV ratios

## Why This Works (Mechanism)
The study's detection approach works by leveraging statistical patterns in text that distinguish AI-generated from human-written content. Classifiers learn to identify subtle differences in word choice, sentence structure, and linguistic patterns that vary between human authors and different LLM families. The mechanism relies on training data diversity across domains and LLM families to capture these patterns effectively.

## Foundational Learning
- **Text complexity metrics**: Entropy and out-of-vocabulary ratios measure linguistic diversity and novelty; needed to quantify how closely AI-generated text mimics human writing patterns
- **Classifier generalization**: Understanding how models trained on one LLM family perform on others; needed to assess detection system robustness across different AI generators
- **Domain-specific detection**: Recognition that detection difficulty varies by writing type; needed because different domains have different linguistic conventions and expectations
- **AUC score interpretation**: Area under the ROC curve measures classifier performance; needed to quantify detection accuracy across different experimental conditions
- **LLM family characteristics**: Different models have distinct generation patterns; needed to understand why certain models are harder to detect
- **Training data diversity**: Multiple datasets across domains improve detection; needed to ensure classifiers capture broad patterns rather than overfitting to specific text types

## Architecture Onboarding
- **Component map**: Training data -> Feature extraction -> Classifier training -> Evaluation -> Generalization testing
- **Critical path**: Data preparation and feature engineering are most critical, as detection accuracy depends heavily on capturing the right linguistic patterns
- **Design tradeoffs**: Specialized classifiers achieve higher accuracy but require more training data, while general classifiers offer broader coverage but lower performance
- **Failure signatures**: Poor generalization to new LLM families, domain-specific overfitting, and false positives with human-written text
- **3 first experiments**: 1) Test classifier performance on a held-out domain, 2) Evaluate cross-family generalization, 3) Compare detection accuracy across different feature extraction methods

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on only four domains (scientific writing, opinion statements, stories, and student essays) may not capture full spectrum of LLM applications
- Analysis covers only five LLM families, potentially missing detection patterns in newer or less common models
- Entropy and OOV metrics provide partial rather than complete assessment of human-likeness

## Confidence
- **High**: Scientific writing is most difficult to detect (AUC 0.852 for OpenAI texts) and OpenAI models are generally hardest to detect across domains
- **Medium**: Classifiers trained on one LLM family generalize to others
- **Medium**: OpenAI models achieve near-human text complexity in student essays based on entropy and OOV ratios

## Next Checks
1. Test detection performance on specialized domains beyond the four studied, particularly in professional domains like legal briefs, medical reports, or technical documentation
2. Evaluate detection accuracy against newer LLM releases and emerging models not included in the original five families
3. Conduct human evaluation studies comparing AI-generated and human-written texts across all domains to validate statistical detection metrics align with human judgment