---
ver: rpa2
title: Overfitting In Contrastive Learning?
arxiv_id: '2407.15863'
source_url: https://arxiv.org/abs/2407.15863
tags:
- overfitting
- learning
- positive
- training
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines overfitting in unsupervised contrastive learning,
  a phenomenon well-documented in supervised learning but not thoroughly explored
  in unsupervised settings. The authors use SimCLR, an unsupervised contrastive learning
  approach, to investigate whether overfitting can occur and identify its mechanism.
---

# Overfitting In Contrastive Learning?

## Quick Facts
- arXiv ID: 2407.15863
- Source URL: https://arxiv.org/abs/2407.15863
- Authors: Zachary Rabin; Jim Davis; Benjamin Lewis; Matthew Scherreik
- Reference count: 4
- Primary result: Overfitting occurs in unsupervised contrastive learning, driven primarily by positive similarity term

## Executive Summary
This paper investigates overfitting in unsupervised contrastive learning using SimCLR, a phenomenon previously documented in supervised learning but not thoroughly explored in unsupervised settings. The authors train a ResNet18 on a CIFAR10 subset for 5K epochs and track training and validation losses along with positive and negative similarity metrics. They discover that overfitting does occur in contrastive learning, with validation loss diverging from training loss after approximately 500 epochs. Crucially, they identify that the positive similarity term in the loss function is the primary driver of overfitting, as the model learns to decrease positive similarity for training examples but fails to generalize this behavior to validation data. This insight suggests that monitoring positive similarity during training could lead to earlier detection of overfitting and reduced training times.

## Method Summary
The authors use SimCLR, an unsupervised contrastive learning approach, to investigate overfitting. They train a ResNet18 on a subset of CIFAR10 (Airplane, Automobile, Ship, Truck classes) for 5K epochs using the Adam optimizer with learning rate 0.001. The SimCLR architecture includes a projection head and uses various augmentations including random cropping, flipping, color jittering, grayscaling, and Gaussian blurring. They track overall loss L, positive similarity, negative similarity, and training vs validation loss divergence per epoch to observe overfitting patterns.

## Key Results
- Overfitting occurs in contrastive learning with validation loss diverging from training loss after ~500 epochs
- Positive similarity term is the primary driver of overfitting in contrastive learning
- Negative similarity continues to decrease for both training and validation data, indicating asymmetric overfitting behavior
- Early detection of overfitting is possible by monitoring positive similarity rather than overall loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning can overfit to training data, causing validation loss to diverge from training loss.
- Mechanism: The model learns to minimize positive similarity for training examples but fails to generalize this behavior to validation examples, pushing validation positive pairs apart.
- Core assumption: The positive similarity term in the loss function is the primary driver of overfitting in contrastive learning.
- Evidence anchors:
  - [abstract] "the authors discover that the positive similarity term in the loss function is the primary driver of overfitting"
  - [section] "we observe that the positive similarity is the term that drives the overfitting" (Results and Discussion)
  - [corpus] Weak - corpus papers focus on benign overfitting in linear models and attention, not contrastive learning mechanisms
- Break condition: If validation positive similarity continues to decrease alongside training positive similarity, the mechanism doesn't apply.

### Mechanism 2
- Claim: Early detection of overfitting is possible by monitoring positive similarity rather than overall loss.
- Mechanism: Positive similarity diverges from validation data before overall loss shows overfitting patterns, allowing earlier intervention.
- Core assumption: The positive similarity metric is more sensitive to generalization gaps than the overall loss.
- Evidence anchors:
  - [abstract] "stopping training when overfitting is seen in the positive similarity can reduce training times"
  - [section] "overfitting in the positive similarity can be detected earlier in the training process as opposed to overfitting in the overall loss" (Results and Discussion)
  - [corpus] Missing - no direct corpus support for early detection claims
- Break condition: If positive similarity and overall loss diverge at the same epoch consistently, early detection advantage disappears.

### Mechanism 3
- Claim: The negative similarity term does not contribute to overfitting in contrastive learning.
- Mechanism: Negative similarity continues to decrease for both training and validation data, indicating the model maintains the ability to push negative pairs apart.
- Core assumption: Overfitting in contrastive learning is asymmetric, affecting only the positive similarity component.
- Evidence anchors:
  - [section] "the negative similarity continues to decrease" (Results and Discussion, Figure 2)
  - [abstract] "the positive similarity term in the loss function is the primary driver of overfitting"
  - [corpus] Weak - corpus papers don't discuss negative vs positive similarity dynamics in contrastive learning
- Break condition: If negative similarity also diverges between training and validation, the mechanism is incomplete.

## Foundational Learning

- Concept: Supervised vs unsupervised learning distinction
  - Why needed here: The paper explicitly contrasts overfitting behavior between supervised and unsupervised settings
  - Quick check question: What is the key difference in how supervised and unsupervised learning approaches handle labels during training?

- Concept: Contrastive learning objective function decomposition
  - Why needed here: Understanding how the loss breaks into positive and negative similarity terms is crucial for the overfitting mechanism
  - Quick check question: How does rearranging the SimCLR loss equation reveal its two-part structure?

- Concept: Generalization gap measurement
  - Why needed here: The paper's core contribution relies on detecting when validation performance diverges from training performance
  - Quick check question: What metric would you monitor to detect if a model is overfitting during training?

## Architecture Onboarding

- Component map: Data augmentation -> Backbone network (ResNet18) -> Projection head -> Loss computation (positive/negative similarity) -> Parameter updates via Adam optimizer
- Critical path: Data augmentation → Backbone network → Projection head → Loss computation (positive/negative similarity) → Parameter updates via Adam optimizer
- Design tradeoffs: Simpler backbone (ResNet18) enables faster experimentation but may limit representation quality; extensive augmentation improves robustness but increases computation
- Failure signatures: Validation loss diverging from training loss after plateau, positive similarity increasing for validation data while decreasing for training data
- First 3 experiments:
  1. Replicate the CIFAR10 subset experiment with different backbone architectures (e.g., ResNet50) to test architecture sensitivity
  2. Vary the augmentation strength to determine if stronger augmentation delays overfitting
  3. Test different temperature hyperparameter values (τ) to see how temperature affects the positive similarity dynamics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the overfitting phenomenon in contrastive learning be mitigated or prevented through architectural modifications to the SimCLR framework?
- Basis in paper: [inferred] The paper identifies overfitting in contrastive learning and notes that the positive similarity term drives this phenomenon, but does not explore potential architectural solutions.
- Why unresolved: The authors focus on characterizing overfitting rather than developing mitigation strategies. They do not investigate whether changes to the network architecture, loss function, or training procedure could prevent or reduce overfitting.
- What evidence would resolve it: Experiments comparing different SimCLR variants with modified architectures (e.g., different projection heads, backbone networks, or loss formulations) and their susceptibility to overfitting.

### Open Question 2
- Question: How does the overfitting behavior observed in SimCLR on CIFAR10 generalize to other datasets, particularly those with different characteristics (e.g., larger datasets, different image complexities, or varying class distributions)?
- Basis in paper: [explicit] The authors only test on a subset of CIFAR10 and do not explore other datasets or compare overfitting across different data characteristics.
- Why unresolved: The study is limited to a single dataset, leaving open questions about the generalizability of the overfitting phenomenon to other domains or data types.
- What evidence would resolve it: Comparative experiments on diverse datasets (e.g., ImageNet, STL-10, or domain-specific image collections) showing whether overfitting occurs similarly or differently across various data characteristics.

### Open Question 3
- Question: What is the relationship between the observed overfitting in contrastive learning and the downstream task performance when using the learned representations?
- Basis in paper: [inferred] While the paper demonstrates overfitting in the contrastive learning process, it does not investigate how this affects the quality of representations for downstream tasks.
- Why unresolved: The study focuses on the training dynamics of SimCLR but does not evaluate whether the overfitting observed in the unsupervised learning phase impacts the utility of learned features for tasks like classification or object detection.
- What evidence would resolve it: Experiments comparing downstream task performance using representations learned with varying numbers of epochs, correlating the degree of overfitting with task-specific metrics.

## Limitations
- Limited to single architecture (ResNet18) and dataset subset, reducing generalizability
- Exact temperature hyperparameter value unspecified, which could affect observed dynamics
- Claims about early detection through positive similarity monitoring lack direct corpus support

## Confidence

**High confidence**: Overfitting occurs in contrastive learning (well-documented through loss divergence patterns)

**Medium confidence**: Positive similarity is the primary driver of overfitting (supported by empirical observations but limited theoretical backing)

**Low confidence**: Early detection of overfitting via positive similarity monitoring (minimal corpus support and requires further validation)

## Next Checks

1. **Architecture sensitivity test**: Replicate experiments with different backbone architectures (e.g., ResNet50, Vision Transformer) to verify if the positive similarity overfitting mechanism is architecture-independent

2. **Temperature hyperparameter sweep**: Systematically vary the temperature parameter τ across multiple orders of magnitude to determine its effect on positive similarity dynamics and overfitting timing

3. **Dataset generalization study**: Apply the same experimental protocol to other unsupervised learning datasets (e.g., STL-10, CIFAR100) to test whether positive similarity-driven overfitting is a general phenomenon beyond the CIFAR10 subset