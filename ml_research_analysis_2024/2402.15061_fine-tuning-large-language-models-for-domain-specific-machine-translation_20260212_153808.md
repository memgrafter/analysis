---
ver: rpa2
title: Fine-tuning Large Language Models for Domain-specific Machine Translation
arxiv_id: '2402.15061'
source_url: https://arxiv.org/abs/2402.15061
tags:
- translation
- dragft
- domain-specific
- fine-tuning
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of improving domain-specific
  machine translation (MT) performance for large language models (LLMs), which struggle
  with terminology translation due to limited domain-specific knowledge. The authors
  propose DragFT, a fine-tuning framework that enhances LLMs through three techniques:
  dictionary-enhanced prompting (specifically Dict-rephrasing, which replaces domain-specific
  terms in source sentences with target-language equivalents), RAG-based few-shot
  example selection (which retrieves high-quality, contextually relevant examples),
  and fine-tuning with few-shot examples.'
---

# Fine-tuning Large Language Models for Domain-specific Machine Translation

## Quick Facts
- **arXiv ID**: 2402.15061
- **Source URL**: https://arxiv.org/abs/2402.15061
- **Reference count**: 20
- **Primary result**: DragFT framework improves domain-specific MT performance by 7.46-13.96 BLEU points on 13B-parameter LLMs

## Executive Summary
This paper addresses the challenge of domain-specific machine translation for large language models, which struggle with terminology translation due to limited domain-specific knowledge. The authors propose DragFT, a fine-tuning framework that enhances LLMs through dictionary-enhanced prompting (Dict-rephrasing), RAG-based few-shot example selection, and fine-tuning with few-shot examples. They construct high-quality IT domain translation datasets and evaluate DragFT on three 13B-parameter LLM backbones across three domain-specific datasets. Results show significant performance improvements in BLEU and COMET metrics compared to strong baselines like GPT-3.5 and GPT-4o.

## Method Summary
DragFT is a fine-tuning framework that improves domain-specific machine translation for large language models through three key techniques: dictionary-enhanced prompting (Dict-rephrasing) that replaces domain-specific terms in source sentences with target-language equivalents, RAG-based few-shot example selection that retrieves high-quality, contextually relevant examples from an external vector database, and fine-tuning with few-shot examples using LoRA (Low-Rank Adaptation) for computational efficiency. The framework uses domain-specific dictionaries extracted from training sets and constructs high-quality parallel datasets in IT, Law, and Medical domains for multiple language pairs.

## Key Results
- DragFT improves BLEU scores by 7.46 to 13.96 points compared to strong baselines
- Terminology translation success rate improves from 73.05% to 80.07% on the IT domain
- Consistent performance gains across three 13B-parameter LLM backbones (Tigerbot-13B, Baichuan2-13B, Llama2-13B)
- Significant reduction in unaligned translation words while maintaining general translation capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dictionary-enhanced prompting (Dict-rephrasing) improves domain-specific terminology translation by replacing source terms with target equivalents before model input.
- Mechanism: Direct substitution of domain-specific terms in the source sentence with their target language counterparts allows the model to receive cleaner context without needing to infer specialized terms.
- Core assumption: The model's performance bottleneck is term ambiguity rather than overall contextual understanding.
- Evidence anchors:
  - [abstract] "Dict-rephrasing, which replaces domain-specific terms in source sentences with target-language equivalents"
  - [section 3.2] "Using the Dict-rephrasing, the terminology of '挂耳板' and '连接器' in the source sentence of '左挂耳板到主板的左挂耳连接器(J6081)的低速信号线缆' are directly rephrased to 'mounting ear plate' and 'connector'"
  - [corpus] Weak evidence - no direct citations to similar dictionary replacement approaches in the corpus
- Break condition: If the dictionary contains incorrect mappings or if context around the term is critical for meaning, substitution could degrade translation quality.

### Mechanism 2
- Claim: RAG-based few-shot example selection improves fine-tuning by retrieving semantically similar and contextually relevant examples from an external vector database.
- Mechanism: Vectorizing extra corpora and calculating similarity scores between source sentences and database entries ensures fine-tuning examples closely match the target domain and style.
- Core assumption: High-quality examples must be both semantically similar and stylistically consistent with the source sentences.
- Evidence anchors:
  - [abstract] "RAG-based few-shot example selection (which retrieves high-quality, contextually relevant examples)"
  - [section 3.3] "To ensure the quality of few-shot examples, we apply the idea of RAG and design a few-shot example selection mechanism based on it"
  - [corpus] Weak evidence - only one corpus neighbor discusses in-context example selection, but not RAG-based methods
- Break condition: If the vector database is too small or lacks domain coverage, retrieval quality degrades and may introduce noise rather than relevant examples.

### Mechanism 3
- Claim: Fine-tuning with few-shot examples preserves both ICL capabilities and fine-tuning benefits, unlike pure instruction tuning.
- Mechanism: Combining instruction tuning with demonstration examples allows the model to learn both the task format and domain-specific patterns simultaneously.
- Core assumption: Few-shot examples provide necessary context that instruction tuning alone cannot capture.
- Evidence anchors:
  - [abstract] "Fine-tuning with few-shot examples further enhances performance when using in-domain examples"
  - [section 3.4] "It is reported that fine-tuning with few-shot examples helps maintain the few-shot learning capabilities of LLMs while preserving the benefits of fine-tuning"
  - [corpus] Moderate evidence - one corpus neighbor explicitly discusses "The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities"
- Break condition: If the few-shot examples are poorly selected or too dissimilar from the target domain, fine-tuning could actually harm performance by introducing conflicting patterns.

## Foundational Learning

- Concept: In-context learning (ICL) - providing demonstration examples within prompts to guide model behavior without parameter updates
  - Why needed here: DragFT leverages ICL benefits but enhances them through fine-tuning, so understanding the baseline is critical
  - Quick check question: What is the main limitation of ICL that DragFT addresses through fine-tuning?

- Concept: Retrieval-Augmented Generation (RAG) - integrating external knowledge sources during generation through similarity-based retrieval
  - Why needed here: The RAG-based example selection mechanism is central to DragFT's approach to quality example selection
  - Quick check question: How does RAG differ from simple random sampling of examples for fine-tuning?

- Concept: LoRA (Low-Rank Adaptation) - efficient fine-tuning method using low-rank matrices to approximate parameter updates
  - Why needed here: DragFT uses LoRA for computational efficiency when fine-tuning large 13B-parameter models
  - Quick check question: What is the primary advantage of LoRA over full fine-tuning for large language models?

## Architecture Onboarding

- Component map: Source sentence → Dictionary replacement → RAG example retrieval → Prompt construction → LoRA fine-tuning → Evaluation

- Critical path: Source sentence → Dictionary replacement → RAG example retrieval → Prompt construction → LoRA fine-tuning → Evaluation

- Design tradeoffs: Dictionary replacement reduces training data size but risks losing context; RAG retrieval ensures quality but adds computational overhead; LoRA saves memory but may limit adaptation capacity

- Failure signatures: 
  - Poor terminology translation → Check dictionary accuracy and coverage
  - Degraded performance on general domain → Verify RAG threshold settings and example quality
  - Overfitting to training domain → Monitor validation performance and adjust LoRA rank

- First 3 experiments:
  1. Test Dict-rephrasing impact: Compare translation quality with and without dictionary replacement on a small validation set
  2. Validate RAG retrieval quality: Check similarity score distributions and manually inspect top-k retrieved examples
  3. Optimize LoRA configuration: Test different rank values and learning rates to balance adaptation quality with training efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DragFT's performance compare to domain-specific fine-tuning methods when applied to low-resource languages?
- Basis in paper: [inferred] The authors note that they focused on Zh⇔En and De⇔En translation directions and have not validated the effectiveness of their methods on low-resource languages. They also rely on machine translation metrics rather than human evaluation to assess translation quality.
- Why unresolved: The paper explicitly states that they have not validated the effectiveness of DragFT on low-resource languages and rely on machine translation metrics rather than human evaluation.
- What evidence would resolve it: Conducting experiments on low-resource language pairs and comparing DragFT's performance to domain-specific fine-tuning methods using both machine translation metrics and human evaluation.

### Open Question 2
- Question: What is the impact of different similarity score thresholds (k) and maximum number of examples (n) in the RAG-based few-shot example selection mechanism on DragFT's performance?
- Basis in paper: [explicit] The authors mention that they set the similarity score threshold k to 0.7 and the maximum number of examples n to 2 in their experiments.
- Why unresolved: The paper does not explore the impact of different values for k and n on DragFT's performance.
- What evidence would resolve it: Conducting experiments with different values for k and n and analyzing the impact on DragFT's performance across various domain-specific datasets.

### Open Question 3
- Question: How does DragFT's performance change when applied to other LLM backbones with different parameter scales?
- Basis in paper: [explicit] The authors deploy DragFT on three 13B-parameter LLM backbones (Tigerbot-13B, Baichuan2-13B, and Llama2-13B) and show significant performance improvements.
- Why unresolved: The paper does not explore the performance of DragFT when applied to other LLM backbones with different parameter scales.
- What evidence would resolve it: Conducting experiments with DragFT on other LLM backbones with varying parameter scales and comparing the performance across different domains.

## Limitations
- Evaluation limited to three 13B-parameter LLM backbones and three domain-specific datasets
- Dictionary replacement assumes high-quality dictionary mappings without validation
- RAG-based example selection relies on vector similarity scores without detailed analysis of retrieval quality
- Does not address potential catastrophic forgetting of general translation capabilities

## Confidence
- **High Confidence**: The overall effectiveness of the DragFT framework in improving domain-specific translation quality
- **Medium Confidence**: The specific mechanisms of Dict-rephrasing and RAG-based example selection
- **Low Confidence**: The scalability of the approach to smaller or larger models, different language pairs, and other domain types

## Next Checks
1. Implement a systematic evaluation of the domain-specific dictionaries by measuring translation quality with and without Dict-rephrasing on a validation set, and conduct manual inspection of dictionary term mappings to identify potential errors or context-dependent issues.

2. Analyze the distribution of similarity scores from the RAG-based example selection, examine the top-k retrieved examples for relevance and quality, and test the sensitivity of translation performance to different similarity score thresholds.

3. Evaluate the fine-tuned models on out-of-domain translation tasks to measure potential degradation in general translation capabilities, and compare performance with non-domain-specific fine-tuned baselines to assess the tradeoff between specialization and generalization.