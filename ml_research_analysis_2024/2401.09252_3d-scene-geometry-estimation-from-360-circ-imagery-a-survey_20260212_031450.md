---
ver: rpa2
title: '3D Scene Geometry Estimation from 360$^\circ$ Imagery: A Survey'
arxiv_id: '2401.09252'
source_url: https://arxiv.org/abs/2401.09252
tags:
- depth
- spherical
- estimation
- images
- layout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This survey comprehensively reviews methodologies for 3D scene\
  \ geometry estimation from 360\xB0 imagery, covering single-view, stereo, and multi-view\
  \ approaches. It addresses the challenges posed by spherical camera models and non-uniform\
  \ sampling in equirectangular projections."
---

# 3D Scene Geometry Estimation from 360$^\circ$ Imagery: A Survey

## Quick Facts
- arXiv ID: 2401.09252
- Source URL: https://arxiv.org/abs/2401.09252
- Reference count: 40
- Primary result: Comprehensive review of 3D scene geometry estimation from 360° imagery, covering single-view, stereo, and multi-view approaches with reported mean relative errors around 0.05-0.15 and layout estimation corner errors below 1% with 3D IoU above 80%.

## Executive Summary
This survey comprehensively reviews methodologies for 3D scene geometry estimation from 360° imagery, addressing the unique challenges posed by spherical camera models and non-uniform sampling in equirectangular projections. The paper systematically categorizes approaches into single-view, stereo, and multi-view methods, surveying both traditional and deep learning techniques for layout and depth estimation. It provides a thorough compilation of relevant datasets, evaluation metrics, and state-of-the-art results, highlighting the growing trend of deep learning solutions in this field. The survey also identifies key challenges such as dataset limitations, evaluation standardization, and the need for robust multi-view approaches to address occlusions for immersive applications.

## Method Summary
The survey covers three main approaches to 3D scene geometry estimation from 360° imagery: single-view, stereo, and multi-view methods. Single-view approaches include layout estimation techniques (e.g., HorizonNet, Pano2CAD) and depth estimation methods (e.g., spherical UNets, CoordConv adaptations). Stereo methods employ epipolar geometry and deep learning-based matching for 360° images, while multi-view approaches utilize Structure from Motion and bundle adjustment techniques adapted for panoramic data. The survey emphasizes the importance of handling spherical distortions through various adaptations such as circular padding, deformable convolutions, and spherical convolutions. Evaluation metrics include corner error, pixel error, 3D IoU for layout estimation, and MRE, MAE, RMSE, RMSLE, and accuracy for depth estimation.

## Key Results
- Single-image depth estimation methods achieve mean relative errors around 0.05-0.15 on standard datasets
- Layout estimation techniques report corner errors below 1% and 3D IoU above 80%
- Deep learning solutions are increasingly dominant, with various adaptations for spherical data
- Challenges remain in dataset standardization, evaluation metrics, and robust multi-view approaches for outdoor scenes

## Why This Works (Mechanism)
The survey works by systematically categorizing and analyzing 3D scene geometry estimation methods for 360° imagery, providing a comprehensive overview of the field's current state. It identifies key challenges posed by spherical camera models and non-uniform sampling, and surveys both traditional and deep learning approaches to address these issues. By compiling relevant datasets, evaluation metrics, and state-of-the-art results, the survey offers a valuable resource for researchers and practitioners in the field, highlighting trends and open questions for future research.

## Foundational Learning
1. Equirectangular Projection: Why needed - To understand the standard representation of 360° images; Quick check - Verify that the width is twice the height (2:1 aspect ratio)
2. Spherical Geometry: Why needed - To comprehend the unique challenges of 360° imagery; Quick check - Confirm understanding of great circles and spherical triangles
3. Layout Estimation: Why needed - To recognize the importance of room structure in indoor scenes; Quick check - Identify layout types (e.g., L-shaped, T-shaped) in sample 360° images
4. Depth Estimation: Why needed - To grasp the concept of inferring 3D structure from 2D images; Quick check - Verify understanding of depth map representations and scale ambiguity
5. Deep Learning Adaptations: Why needed - To appreciate the modifications required for spherical data; Quick check - Explain the purpose of CoordConv, deformable convolutions, and circular padding
6. Evaluation Metrics: Why needed - To understand how to assess the performance of geometry estimation methods; Quick check - Calculate corner error and 3D IoU for a given layout estimation result

## Architecture Onboarding

Component map: Dataset -> Preprocessing -> Model Architecture -> Training -> Evaluation

Critical path: Dataset preparation and preprocessing -> Model architecture selection and implementation -> Training with spherical adaptations -> Evaluation using appropriate metrics

Design tradeoffs: Balancing model complexity with computational efficiency, choosing between monocular and multi-view approaches based on application requirements, and selecting appropriate evaluation metrics for the specific task

Failure signatures: Poor performance due to inadequate handling of spherical distortions, overfitting or underfitting due to inappropriate hyperparameter settings, and scale ambiguity in monocular depth estimation

First experiments:
1. Implement and evaluate HorizonNet on the Stanford 2D-3D dataset for layout estimation
2. Train a spherical UNet model on the 3D60 dataset for depth estimation and compare with baseline methods
3. Conduct an ablation study to quantify the impact of CoordConv and circular padding on model performance

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can deep learning-based monocular depth estimation methods be improved for outdoor scenes with a wider range of depth values?
- Basis in paper: [explicit] The paper mentions that depth maps produced by single-panorama approaches present scale ambiguity and that most methods consider indoor panoramas with common decoration objects and furniture, leading to poor performance in outdoor scenarios.
- Why unresolved: Current monocular depth estimation methods are primarily trained on indoor datasets, which limits their ability to handle the diverse depth ranges and scene complexities found in outdoor environments.
- What evidence would resolve it: Development and evaluation of monocular depth estimation models trained on diverse outdoor datasets, demonstrating improved performance on a wide range of outdoor scenes with varying depth values.

### Open Question 2
- Question: What are the most effective strategies for developing standardized datasets and evaluation metrics for 360° stereo and multi-view-based depth estimation?
- Basis in paper: [explicit] The paper highlights the lack of standardized datasets and benchmarks for stereo and multi-view-based depth estimation in 360° imagery, noting that existing datasets like Middlebury and KITTI for perspective images could serve as inspiration.
- Why unresolved: Current datasets and evaluation metrics for 360° stereo and multi-view-based depth estimation are limited and lack standardization, hindering fair comparison and progress in the field.
- What evidence would resolve it: Creation and release of comprehensive 360° stereo and multi-view-based depth estimation datasets with consistent ground truth annotations and development of standardized evaluation metrics tailored to these specific scenarios.

### Open Question 3
- Question: How can geometrically correct learning-based tools be developed for sparse and dense matching, plane-aware image oversegmentation, and pose estimation in 360° imagery?
- Basis in paper: [explicit] The paper suggests that there is a need for developing learning-based tools that are geometrically correct for tasks such as sparse and dense matching, plane-aware image oversegmentation, and pose estimation in 360° imagery.
- Why unresolved: Current learning-based methods for these tasks often rely on planar CNNs or hand-crafted features that may not fully capture the unique properties of 360° imagery, leading to suboptimal performance.
- What evidence would resolve it: Development and evaluation of learning-based methods that incorporate geometric constraints and are specifically designed for 360° imagery, demonstrating improved performance on tasks such as sparse and dense matching, plane-aware image oversegmentation, and pose estimation.

## Limitations
- The survey focuses primarily on deep learning approaches, potentially underrepresenting classical computer vision techniques
- Rapid evolution of the field may lead to new methods emerging between the survey's completion and publication
- The focus on RGB imagery excludes potentially valuable complementary modalities such as depth sensors or multi-spectral data

## Confidence
- High: Widely benchmarked methods on established datasets (Stanford 2D-3D, 3D60)
- Medium: Emerging techniques and specialized datasets due to limited independent validation

## Next Checks
1. Implement and evaluate a representative method (e.g., HorizonNet or UniFuse) on a standard dataset (Stanford 2D-3D or 3D60) to verify the reported performance metrics and assess the practical challenges of spherical data handling.
2. Conduct an ablation study to quantify the impact of specific spherical adaptations (e.g., CoordConv, deformable convolutions, circular padding) on model performance compared to baseline implementations without these modifications.
3. Extend the evaluation to include a comparison with classical computer vision approaches on a subset of the dataset to assess the practical advantages and limitations of deep learning methods in 360° geometry estimation tasks.