---
ver: rpa2
title: 'PF-GNN: Differentiable particle filtering based approximation of universal
  graph representations'
arxiv_id: '2401.17752'
source_url: https://arxiv.org/abs/2401.17752
tags:
- graph
- graphs
- pf-gnn
- each
- tree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the limitations of graph neural networks (GNNs)
  in expressive power, which is bounded by the 1-dimensional Weisfeiler-Lehman (WL)
  test for graph isomorphism. The authors propose Particle Filter Graph Neural Networks
  (PF-GNN), a method that learns more discriminative graph representations by approximating
  the search tree of colorings generated by exact graph isomorphism solvers.
---

# PF-GNN: Differentiable particle filtering based approximation of universal graph representations

## Quick Facts
- arXiv ID: 2401.17752
- Source URL: https://arxiv.org/abs/2401.17752
- Authors: Mohammed Haroon Dupty; Yanfei Dong; Wee Sun Lee
- Reference count: 40
- Primary result: PF-GNN achieves 100% accuracy on certain graph isomorphism tests and outperforms leading GNNs on real-world benchmarks by approximating the search tree of colorings generated by exact graph isomorphism solvers.

## Executive Summary
The paper addresses the fundamental expressiveness limitation of graph neural networks (GNNs), which are bounded by the 1-dimensional Weisfeiler-Lehman (WL) test for graph isomorphism. Particle Filter Graph Neural Networks (PF-GNN) are proposed to learn more discriminative graph representations by approximating the search tree of colorings generated by exact graph isomorphism solvers. The method uses particle filtering to maintain a belief distribution over graph colorings, updating it with stochastic transitions and re-weighting based on learned observation functions, allowing PF-GNN to capture structural features beyond the 1-WL test.

## Method Summary
PF-GNN combines particle filtering with graph neural networks to approximate the search tree of colorings used in exact graph isomorphism solvers. The method maintains a belief distribution over graph colorings using K particles, each representing a possible coloring state. At each step, a policy function selects a node to individualize (assign a unique color), the coloring is refined using a GNN, and particles are re-weighted based on a learned observation function that scores the discriminative power of the coloring. The belief distribution is updated through re-sampling to focus computational resources on promising colorings. The entire process is end-to-end differentiable, allowing PF-GNN to be trained with any GNN backbone. Experiments show that PF-GNN achieves 100% accuracy on certain graph isomorphism tests and competitive performance on real-world graph classification and regression tasks.

## Key Results
- PF-GNN achieves 100% accuracy on the TRIANGLES-large and CSL isomorphism detection benchmarks, outperforming all baseline GNNs.
- On real-world datasets (ZINC, QM9, OGBG-MOLHIV), PF-GNN achieves competitive or superior performance compared to state-of-the-art GNNs.
- The method demonstrates linear scaling in runtime with the number of individualization-refinement steps, while significantly improving discriminative power over 1-WL equivalent GNNs.

## Why This Works (Mechanism)
PF-GNN works by approximating the exhaustive search tree of colorings used in exact graph isomorphism solvers through particle filtering. By maintaining a belief distribution over colorings and iteratively refining them through stochastic node individualization and GNN-based refinement, PF-GNN can explore the space of colorings more efficiently than exact solvers while remaining differentiable. The learned policy function guides the search towards promising colorings, and the observation function provides feedback on the discriminative power of each coloring, allowing the model to focus on high-value states. This approach effectively bypasses the 1-WL expressiveness limit by approximating the full search tree that exact solvers explore.

## Foundational Learning
- **1-dimensional Weisfeiler-Lehman (WL) test**: A heuristic for graph isomorphism that colors nodes based on their neighborhoods; limited to distinguishing graphs that differ in their 1-hop neighborhood structure. (Why needed: Establishes the expressiveness limit that PF-GNN aims to overcome. Quick check: Can WL distinguish regular graphs with identical degree sequences?)
- **Graph Neural Networks (GNNs)**: Neural networks that operate on graph-structured data by aggregating information from neighboring nodes. (Why needed: PF-GNN builds upon GNNs as the base architecture for refining colorings. Quick check: Does the GNN backbone maintain permutation invariance?)
- **Particle filtering**: A sequential Monte Carlo method for approximating posterior distributions by maintaining a set of weighted samples (particles). (Why needed: Provides the framework for approximating the search tree of colorings. Quick check: Does the particle set converge to high-probability colorings?)
- **Individualization-Refinement (IR) algorithms**: Exact graph isomorphism solvers that iteratively assign unique colors to nodes and refine colorings based on neighborhood structure. (Why needed: PF-GNN approximates the search tree generated by IR algorithms. Quick check: Does PF-GNN explore colorings that IR would consider?)
- **Differentiable programming**: Programming paradigms where all operations, including sampling and optimization, are differentiable and can be trained with gradient-based methods. (Why needed: Enables end-to-end training of PF-GNN with standard deep learning frameworks. Quick check: Are all components of PF-GNN differentiable and trainable with backpropagation?)

## Architecture Onboarding

**Component map**: Input graph -> GNN backbone -> 1-WL coloring -> Particle initialization -> (Transition -> Observation update -> Resampling) x T -> Final representation

**Critical path**: The core computational path involves the iterative transition (node selection and individualization) and observation update (coloring scoring and re-weighting) steps, which are repeated T times to refine the belief distribution over colorings.

**Design tradeoffs**: 
- K (number of particles) vs. expressiveness: More particles allow better approximation of the search tree but increase computational cost.
- T (number of IR steps) vs. runtime: More steps improve discriminative power but linearly increase runtime.
- Policy function complexity vs. generalization: More complex policies may better guide the search but risk overfitting to training data.

**Failure signatures**:
- Sub-100% accuracy on isomorphism tests: Likely due to insufficient K or T, or poorly learned policy/observation functions.
- Runtime significantly higher than baseline GNN: May indicate inefficient implementation or excessive K/T values.
- Overfitting on real-world datasets: Could result from overly complex policy or observation functions.

**3 first experiments**:
1. Implement PF-GNN with K=10 particles and T=2 IR steps on a small synthetic graph dataset, verify that the particle set evolves and that the belief distribution changes over iterations.
2. Test PF-GNN on the TRIANGLES-small dataset with varying K (1, 5, 10, 20) and T (1, 2, 3) to identify the minimum configuration achieving 100% accuracy.
3. Compare the runtime of PF-GNN with the base GNN on a medium-sized real-world dataset, ensuring the runtime scales linearly with T.

## Open Questions the Paper Calls Out
None

## Limitations
- Exact architecture details for the policy and observation functions are not fully specified, requiring implementation decisions that may affect results.
- Hyperparameters for training (learning rate, batch size, weight for policy loss) are not provided for all datasets, necessitating additional tuning.
- The method's scalability to very large graphs (thousands of nodes) is not thoroughly evaluated, and runtime may become prohibitive with large K and T.

## Confidence
- **High confidence** in the conceptual framework and theoretical motivation (the WL expressiveness limitation is well-established, and particle filtering is a mature technique).
- **Medium confidence** in the empirical results due to the high performance on synthetic benchmarks, which may not generalize to real-world noise and complexity.
- **Low confidence** in the reproducibility of exact results without full specification of policy and observation function architectures.

## Next Checks
1. **Architecture verification**: Implement the policy and observation functions with specified MLPs, and verify their output distributions and gradients.
2. **Hyperparameter sensitivity**: Systematically test the effect of K (number of particles) and T (IR steps) on both accuracy and runtime, especially on the TRIANGLES-large and CSL datasets.
3. **Robustness testing**: Evaluate PF-GNN on noisy graph datasets or with adversarial perturbations to assess real-world applicability and robustness.