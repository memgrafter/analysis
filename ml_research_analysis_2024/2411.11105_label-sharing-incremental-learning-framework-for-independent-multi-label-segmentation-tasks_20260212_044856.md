---
ver: rpa2
title: Label Sharing Incremental Learning Framework for Independent Multi-Label Segmentation
  Tasks
arxiv_id: '2411.11105'
source_url: https://arxiv.org/abs/2411.11105
tags:
- label
- segmentation
- tasks
- labels
- sharing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a novel label sharing framework for multi-label
  segmentation tasks across multiple datasets with disparate label sets. The framework
  transforms multiple datasets into a single large dataset with shared labels by systematically
  mapping individual label sets to a common label space, eliminating the need for
  task-specific architectural adaptations.
---

# Label Sharing Incremental Learning Framework for Independent Multi-Label Segmentation Tasks

## Quick Facts
- arXiv ID: 2411.11105
- Source URL: https://arxiv.org/abs/2411.11105
- Authors: Deepa Anand; Bipul Das; Vyshnav Dangeti; Antony Jerald; Rakesh Mullick; Uday Patil; Pakhi Sharma; Prasad Sudhakar
- Reference count: 12
- Primary result: Novel label sharing framework transforms multiple datasets with disparate label sets into a single large dataset with shared labels, enabling seamless incremental learning of new tasks without compromising overall accuracy

## Executive Summary
This paper proposes a label sharing framework for multi-label segmentation tasks across multiple datasets with disparate label sets. The framework systematically maps individual label sets to a common label space by grouping labels based on average relative sizes, eliminating the need for task-specific architectural adaptations. Experimental results on medical image segmentation datasets demonstrate that the proposed method achieves performance on par with multi-task schemes while enabling seamless incremental learning of new tasks. The approach is parameter and data efficient, requiring only a single model to address all segmentation tasks.

## Method Summary
The label sharing framework transforms multiple datasets with disparate label sets into a single large dataset with shared labels. Labels are grouped based on average relative sizes within each task, with ties broken arbitrarily. A single neural network is trained on the combined dataset using shared labels as outputs. For incremental learning, new tasks are incorporated by mapping their labels to existing shared labels based on relative sizes, requiring only fine-tuning of the pretrained model. The framework is tested on medical imaging datasets including TotalSegmentator (pelvic, abdomen, chest), Head and Neck, and an in-house extremity localization dataset.

## Key Results
- Achieves Dice Similarity Coefficient performance on par with multi-task schemes while using a single shared model
- Enables seamless incremental learning of new tasks without compromising performance on previously learned tasks
- Reduces parameter count compared to training individual models for each task, with output channels growing only in O(nmax) order

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The label sharing framework transforms multiple datasets with disparate label sets into a single large dataset with shared labels, eliminating the need for task-specific architectural adaptations.
- Mechanism: By systematically mapping individual label sets to a common label space, the framework creates shared abstract labels that represent groups of semantically similar structures across tasks. This mapping is performed based on average relative sizes of segmentation masks within each task.
- Core assumption: Labels can be meaningfully grouped based on relative size ordering, even when the underlying images are visually domain-disjoint.
- Evidence anchors:
  - [abstract] "transforms multiple datasets with disparate label sets into a single large dataset with shared labels"
  - [section 2.1] "We sort the labels within tasks according to their relative sizes and match them across tasks, with ties broken arbitrarily"
  - [corpus] Weak evidence - no direct support for relative size-based grouping effectiveness
- Break condition: If the label sets across tasks cannot be meaningfully partitioned into groups where each group contains at most one label from each task, or if the relative size ordering does not capture semantic similarity.

### Mechanism 2
- Claim: The framework enables seamless incremental learning of new tasks without compromising overall accuracy.
- Mechanism: New tasks can be incorporated by computing average relative sizes of structures to be segmented and mapping individual labels to existing shared labels. This requires only storing a table of average relative sizes from previously trained tasks.
- Core assumption: The shared label space created for initial tasks is sufficiently general to accommodate new tasks without requiring architectural modifications.
- Evidence anchors:
  - [abstract] "label sharing framework is naturally amenable for incremental learning where segmentations for new datasets can be easily learnt"
  - [section 2.2] "The label sharing framework provides a natural way to incorporate new tasks to the segmentation network in a plug-and-play fashion"
  - [corpus] Weak evidence - no direct support for effectiveness of incremental learning on new tasks
- Break condition: If the number of labels in the new task exceeds the maximum number of shared labels created initially, or if the new task's labels cannot be meaningfully mapped to existing shared labels.

### Mechanism 3
- Claim: The label sharing framework results in parameter and data efficient models.
- Mechanism: By using a single model with shared parameters across all tasks, the framework reduces the total number of parameters compared to training individual models for each task. The number of output channels grows only in the order of O(nmax), where nmax is the largest number of labels in any individual task.
- Core assumption: A single model with shared parameters can effectively learn to segment all tasks simultaneously when labels are appropriately grouped.
- Evidence anchors:
  - [abstract] "results in parameter and data efficient models"
  - [section 2.1] "This makes the number of parameters in the last layer of the neural network grow in the order of only O(nmax)"
  - [corpus] Weak evidence - no direct support for parameter efficiency claims
- Break condition: If the performance degradation from using a single shared model exceeds acceptable thresholds compared to task-specific models.

## Foundational Learning

- Concept: Semantic segmentation
  - Why needed here: The framework is designed for multi-label segmentation tasks across multiple datasets, requiring understanding of how to delineate distinct anatomical structures or pathologies within medical images.
  - Quick check question: What is the difference between semantic segmentation and instance segmentation in the context of medical imaging?

- Concept: Multi-task learning
  - Why needed here: The framework addresses the setting where segmentation models need to be built for multiple datasets, each with its own label set, requiring understanding of how to learn multiple related tasks simultaneously.
  - Quick check question: How does multi-task learning differ from transfer learning in the context of medical image segmentation?

- Concept: Incremental learning
  - Why needed here: The framework is designed to naturally accommodate new segmentation tasks without compromising performance on previously learned tasks, requiring understanding of how to learn continuously from new data.
  - Quick check question: What is catastrophic forgetting in the context of incremental learning, and how might the label sharing framework help mitigate it?

## Architecture Onboarding

- Component map: Encoder (shared across all tasks) -> Decoder (single shared decoder with multiple output channels) -> Label mapping (mapping from individual task labels to shared abstract labels) -> Incremental learning module (optional fine-tuning capability)

- Critical path:
  1. Data preparation: Combine all datasets, compute average relative sizes for each label
  2. Label mapping: Create shared label space by grouping labels based on relative size ordering
  3. Model training: Train single nnUNet model on combined dataset with shared labels
  4. Evaluation: Assess performance on each individual task using original label sets

- Design tradeoffs:
  - Single shared model vs. multiple task-specific models: Shared model offers parameter efficiency but may sacrifice some task-specific performance
  - Relative size-based grouping vs. semantic similarity-based grouping: Simpler to implement but may not capture all semantic relationships
  - Fixed shared label space vs. dynamic label space expansion: Fixed space enables incremental learning but may limit expressiveness for very diverse tasks

- Failure signatures:
  - Performance degradation on tasks with labels that don't map well to shared labels
  - Confusion between labels in the same shared group that have different semantic meanings
  - Inability to accommodate new tasks with more labels than the initial shared label space allows

- First 3 experiments:
  1. Train label sharing model on two tasks with similar label sets to verify basic functionality
  2. Train label sharing model on two tasks with dissimilar label sets to test relative size-based grouping
  3. Add a third task incrementally to test the incremental learning capability and verify no catastrophic forgetting occurs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can shared labels be automatically generated for new tasks without manual intervention, and what criteria should define inter-task label similarity?
- Basis in paper: [explicit] The authors state that an important line of work that remains to be done is automatic generation of shared labels, which may require new definitions of inter-task label similarity.
- Why unresolved: The current framework requires manual grouping of labels based on average relative sizes, which may not scale well or capture semantic similarities.
- What evidence would resolve it: Development and validation of an automatic algorithm that can generate shared labels for new tasks based on learned or defined similarity metrics, and demonstration that this approach maintains or improves segmentation performance compared to manual grouping.

### Open Question 2
- Question: How does the label sharing framework perform across different imaging modalities (e.g., MRI, ultrasound) and in multi-modal settings?
- Basis in paper: [explicit] The authors mention that it remains to be studied how this framework extends to other imaging modalities, or even to multi-modal settings.
- Why unresolved: The experiments were conducted only on CT datasets, limiting the generalizability of the approach to other imaging types.
- What evidence would resolve it: Experimental validation of the label sharing framework on diverse imaging modalities (MRI, ultrasound, etc.) and multi-modal datasets, demonstrating consistent performance improvements over existing methods.

### Open Question 3
- Question: What is the impact of label sharing on catastrophic forgetting when incrementally learning new tasks, and how can this be mitigated?
- Basis in paper: [inferred] While the paper demonstrates that incremental learning does not compromise performance for new tasks, it does not explicitly address catastrophic forgetting of previously learned tasks.
- Why unresolved: The experiments focus on adding new tasks without evaluating the retention of performance on previously learned tasks after multiple incremental updates.
- What evidence would resolve it: Long-term incremental learning experiments showing consistent performance across all tasks after multiple additions, and comparison with methods specifically designed to mitigate catastrophic forgetting (e.g., elastic weight consolidation, rehearsal methods).

## Limitations
- Reliance on relative size-based label grouping may not always capture true semantic relationships between anatomical structures
- Incremental learning capability is limited by the constraint that new tasks must have label counts not exceeding the maximum number of shared labels established initially
- Framework's effectiveness has only been demonstrated on CT datasets, limiting generalizability to other imaging modalities

## Confidence

**High confidence**: The framework's ability to transform multiple datasets into a single shared label space using relative size ordering

**Medium confidence**: The parameter and data efficiency claims, as these depend on the specific implementation and dataset characteristics

**Medium confidence**: The incremental learning capability, as it requires additional validation on truly diverse and challenging new tasks

## Next Checks

1. Conduct ablation studies comparing relative size-based grouping with semantic similarity-based grouping to quantify the impact of the grouping strategy on segmentation performance

2. Test the framework on datasets with highly dissimilar label sets to evaluate its robustness when semantic relationships are weak or ambiguous

3. Implement and validate a dynamic label space expansion mechanism to overcome the limitation of fixed maximum label counts in incremental learning scenarios