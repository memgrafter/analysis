---
ver: rpa2
title: 'Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of Free-Form
  QA'
arxiv_id: '2408.09235'
source_url: https://arxiv.org/abs/2408.09235
tags:
- answer
- human
- evaluation
- llms
- judges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a reference-guided verdict method that uses
  multiple LLMs as judges to evaluate free-form QA outputs, addressing the limitations
  of traditional metrics like EM and F1 that fail to capture semantic depth and lexical
  variation. The approach provides judges with the input question, candidate answer,
  and reference answer, and aggregates verdicts via majority voting across multiple
  LLMs to improve reliability and reduce individual model biases.
---

# Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of Free-Form QA
## Quick Facts
- arXiv ID: 2408.09235
- Source URL: https://arxiv.org/abs/2408.09235
- Reference count: 13
- Primary result: Reference-guided verdict with multiple LLMs improves QA evaluation reliability and alignment with human judgments

## Executive Summary
This paper addresses the limitations of traditional metrics like Exact Match (EM) and F1 score for evaluating free-form QA outputs, which fail to capture semantic depth and lexical variation. The proposed method introduces a reference-guided verdict system that uses multiple LLMs as judges, providing each with the input question, candidate answer, and reference answer. By aggregating verdicts through majority voting across multiple judges, the approach reduces individual model biases and significantly improves inter-rater agreement with human evaluations (Cohen's kappa up to 0.96).

## Method Summary
The reference-guided verdict method employs multiple LLM judges to evaluate free-form QA outputs by providing them with the question, candidate answer, and reference answer as context. Each judge independently produces a verdict, and the final evaluation is determined through majority voting across all judges. This approach addresses the semantic limitations of traditional metrics like EM and F1 by leveraging the reasoning capabilities of LLMs to assess answer quality beyond surface-level lexical matching. The method demonstrates improved alignment with human judgments while maintaining robustness across different prompt designs and task complexities.

## Key Results
- Significant improvement in inter-rater agreement with human judgments (Cohen's kappa up to 0.96)
- Robust performance across three QA datasets with varying prompt designs
- Majority voting across multiple LLM judges reduces individual model biases and improves evaluation reliability

## Why This Works (Mechanism)
The approach works by leveraging multiple LLM judges to provide diverse perspectives on answer quality, with majority voting serving as an aggregation mechanism that reduces individual model biases. By providing judges with both the reference answer and candidate answer, the system enables semantic comparison beyond simple lexical matching. The reference answer acts as ground truth context that guides the LLM's evaluation reasoning, while the multi-judge setup ensures that idiosyncratic biases of individual models are averaged out through consensus.

## Foundational Learning
- **Cohen's kappa**: A statistical measure of inter-rater agreement that accounts for chance agreement, essential for quantifying evaluation reliability between human and LLM judges
- **Exact Match (EM)**: A strict metric requiring candidate answers to match reference answers exactly, including punctuation and word order, which fails to capture semantic equivalence
- **F1 score**: A metric measuring overlap between candidate and reference answers, limited by its focus on lexical similarity rather than semantic understanding
- **Majority voting**: An ensemble technique where the most common verdict among judges determines the final outcome, reducing individual model biases
- **Reference-guided evaluation**: A paradigm where LLM judges are provided with reference answers as ground truth context for evaluation
- **Free-form QA evaluation**: The task of assessing open-ended question answering outputs that may vary significantly in phrasing while maintaining semantic correctness

## Architecture Onboarding
**Component map**: Question -> Multiple LLM Judges -> Individual Verdicts -> Majority Voting -> Final Evaluation

**Critical path**: Input question and answers flow to multiple LLM judges, each produces verdict independently, majority voting aggregates results to produce final evaluation score

**Design tradeoffs**: Simple majority voting vs. weighted voting schemes (simplicity and robustness vs. potential for nuanced aggregation), multiple judges vs. single judge (reduced bias vs. computational cost), reference answer inclusion vs. reference-free evaluation (semantic guidance vs. flexibility for alternative valid answers)

**Failure signatures**: Poor performance on tasks requiring creative or alternative valid answers, domain-specific biases if reference answers contain domain-specific terminology, computational overhead with large numbers of judges, potential for consensus on incorrect evaluations if all judges share similar biases

**First experiments**:
1. Test on a single QA dataset with varying numbers of LLM judges to identify optimal ensemble size
2. Compare majority voting against simple average of confidence scores from individual judges
3. Evaluate performance on questions with multiple valid reference answers to test reference-guided limitations

## Open Questions the Paper Calls Out
None

## Limitations
- Results may be domain-specific, as experiments were conducted on only three QA datasets without exploring diverse domains or multilingual contexts
- The approach doesn't address potential biases introduced by using reference answers as ground truth, which may constrain evaluation of creative or alternative valid responses
- The study focuses on majority voting without exploring more sophisticated ensemble methods or confidence-weighted aggregation schemes

## Confidence
- **High confidence**: The improvement in inter-rater agreement (Cohen's kappa up to 0.96) is well-supported by experimental results
- **Medium confidence**: The claim of improved alignment with human judgments requires further validation across broader task types and domains
- **Medium confidence**: The robustness across varying prompt designs is demonstrated but may not generalize to significantly different evaluation contexts

## Next Checks
1. Test the reference-guided verdict method on non-QA tasks (e.g., summarization, dialogue, or creative writing) to evaluate cross-task generalizability
2. Conduct experiments with LLMs of varying sizes and capabilities to determine if performance gains persist across different model families
3. Implement alternative aggregation methods beyond majority voting (such as confidence-weighted voting or hierarchical judging) to assess whether current approach is optimal