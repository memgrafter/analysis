---
ver: rpa2
title: Aligning LLMs with Individual Preferences via Interaction
arxiv_id: '2410.03642'
source_url: https://arxiv.org/abs/2410.03642
tags:
- user
- arxiv
- llms
- preferences
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for aligning large language models
  (LLMs) with individual user preferences through interactive conversations. The approach
  constructs a diverse pool of 3,310 distinct user personas, then generates a tree-structured
  multi-turn preference dataset via multi-LLM collaboration, where one LLM role-plays
  users while others generate personalized responses.
---

# Aligning LLMs with Individual Preferences via Interaction

## Quick Facts
- arXiv ID: 2410.03642
- Source URL: https://arxiv.org/abs/2410.03642
- Authors: Shujin Wu; May Fung; Cheng Qian; Jeonghwan Kim; Dilek Hakkani-Tur; Heng Ji
- Reference count: 24
- Primary result: Improves average alignment scores by 32.0% across conversation turns

## Executive Summary
This paper introduces a method for aligning large language models with individual user preferences through interactive conversations. The approach constructs a diverse pool of 3,310 distinct user personas, then generates a tree-structured multi-turn preference dataset via multi-LLM collaboration, where one LLM role-plays users while others generate personalized responses. The dataset is used to fine-tune LLMs using supervised fine-tuning and reinforcement learning. The authors establish ALOE, a benchmark with 100 test cases and metrics including Alignment Level and Improvement Rate, to evaluate how well models dynamically adapt to individual preferences.

## Method Summary
The method involves three main stages: persona pool creation, preference dataset construction, and model fine-tuning. Starting with 20 seed profiles and 20 seed personalities, the system iteratively generates new personas using GPT-4o while filtering semantically similar ones via cosine similarity on sentence embeddings. Four specialized LLMs then collaborate to generate multi-turn conversations: one role-plays users based on personas, another infers revealed traits, one generates preferred responses using inferred traits, and one generates rejected responses without traits. The resulting dataset is used to fine-tune LLMs in two stages - first with supervised fine-tuning on preferred responses only, then with direct preference optimization on pairwise comparisons.

## Key Results
- Mainstream LLMs struggle with personalized alignment without fine-tuning
- Proposed method improves average alignment scores by 32.0% across conversation turns
- Enhancement rates remain stable at 10.8%-13.3% across all conversation turns
- Models show consistent improvement in aligning with user preferences throughout interactions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative self-generation and semantic similarity filtering creates a diverse persona pool that enables personalized alignment.
- Mechanism: The system starts with seed personas, then iteratively generates new ones using GPT-4o, filtering out semantically similar ones via cosine similarity on sentence embeddings. This ensures distinct personas covering diverse user profiles and personalities.
- Core assumption: Semantic similarity on persona descriptions correlates with conversational similarity in real interactions.
- Evidence anchors:
  - [abstract] "establish a diverse pool of 3,310 distinct user personas by initially creating seed examples, which are then expanded through iterative self-generation and filtering"
  - [section 2.1] "We adopt the Sentence Transformers (Reimers and Gurevych, 2019) to compute the embedding of profile descriptions and measure the cosine similarity between new and existing profiles"
  - [corpus] Weak - no direct evidence corpus has similar approaches, though mentions related personalization work.

### Mechanism 2
- Claim: Multi-LLM collaboration with distinct roles generates high-quality pairwise preference data.
- Mechanism: Four LLMs play different roles - role-playing LLM simulates user, induction LLM identifies revealed persona traits, preferred LLM generates personalized responses, rejected LLM generates generic responses. This creates tree-structured multi-turn conversations.
- Core assumption: Role specialization improves data quality compared to single LLM generation.
- Evidence anchors:
  - [abstract] "Guided by distinct user personas, we leverage multi-LLM collaboration to develop a multi-turn preference dataset containing 3K+ multi-turn conversations in tree structures"
  - [section 2.1] "For each conversation turn i, the role-playing LLM generates a simulated user's message mi. To generate pairwise responses for each round, we consider two different lines..."
  - [corpus] Weak - corpus mentions related preference learning work but not this specific multi-LLM approach.

### Mechanism 3
- Claim: Two-stage training (SFT then DPO) effectively aligns LLMs with individual preferences.
- Mechanism: First stage uses SFT on preferred responses only to establish baseline alignment. Second stage uses DPO with pairwise data (preferred vs rejected responses) to fine-tune preferences.
- Core assumption: Combining SFT and DPO is more effective than either alone for preference alignment.
- Evidence anchors:
  - [abstract] "Finally, we apply supervised fine-tuning and reinforcement learning to enhance LLMs using this dataset"
  - [section 2.2] "Employing the constructed preference dataset, we fine-tune multiple LLMs following the training recipe described below. Supervised Fine-tuning We first implement supervised fine-tuning (SFT)..."
  - [section 5.1] "The results in Table 1 show that for all evaluated LLMs, both the average AL and the IR can be improved when incorporating pairwise responses via RL (Ours vs. SFT-Preferred)."

## Foundational Learning

- Concept: Semantic similarity and embedding spaces
  - Why needed here: Used to filter persona pool and ensure diversity by measuring cosine similarity between persona embeddings
  - Quick check question: What embedding model is used to compute persona similarity, and what threshold determines if a new persona is too similar to existing ones?

- Concept: Multi-task learning and role specialization
  - Why needed here: Different LLMs play specialized roles (role-playing, induction, preferred response, rejected response) to generate high-quality preference data
  - Quick check question: How many distinct LLM roles are used in the preference data construction, and what is the purpose of each role?

- Concept: Preference learning and ranking
  - Why needed here: DPO uses pairwise comparisons between preferred and rejected responses to align model with human preferences
  - Quick check question: What training objective is used in the reinforcement learning stage, and how does it incorporate pairwise preferences?

## Architecture Onboarding

- Component map: Persona generation → Preference data construction → SFT training → DPO training → ALOE benchmark evaluation
- Critical path: Persona pool creation → Multi-turn conversation generation → Pairwise response creation → Model fine-tuning → Evaluation
- Design tradeoffs: Larger persona pool increases diversity but requires more computation; more conversation turns provide better alignment signals but increase dataset size
- Failure signatures: Poor alignment scores in ALOE indicate issues with persona quality, data construction, or training process
- First 3 experiments:
  1. Test persona generation with different semantic similarity thresholds to find optimal diversity level
  2. Validate that preferred responses are consistently rated higher than rejected responses by human evaluators
  3. Compare alignment performance with different ratios of preferred vs rejected responses in DPO training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the alignment method scale with the number of conversation turns beyond 10, and what are the limitations in terms of context length and model capacity?
- Basis in paper: [inferred] The paper mentions that interactive turns are limited to 10 due to resource constraints, which may mask the model's shortcomings in aligning with individual preferences during deeper interactions.
- Why unresolved: The paper does not explore the impact of increasing the number of conversation turns on the model's alignment performance, leaving uncertainty about the scalability and limitations of the method.
- What evidence would resolve it: Conducting experiments with varying numbers of conversation turns (e.g., 10, 20, 50) and analyzing the alignment performance and resource requirements would provide insights into the scalability and limitations of the method.

### Open Question 2
- Question: How does the diversity and distinctiveness of the persona pool affect the model's ability to align with individual preferences, and what are the optimal strategies for persona creation and filtering?
- Basis in paper: [explicit] The paper describes an iterative self-generation and filtering process for creating a diverse persona pool, but does not explore the impact of persona diversity on alignment performance or the optimal strategies for persona creation.
- Why unresolved: The paper does not investigate how the diversity and distinctiveness of the persona pool influence the model's alignment capabilities, nor does it explore alternative strategies for persona creation and filtering.
- What evidence would resolve it: Comparing the alignment performance of models trained on persona pools with varying levels of diversity and distinctiveness, and experimenting with different persona creation and filtering strategies, would provide insights into the optimal approaches for persona pool construction.

### Open Question 3
- Question: How does the model's performance on alignment tasks generalize to different domains and user populations, and what are the potential biases and limitations in the current approach?
- Basis in paper: [inferred] The paper focuses on multi-turn daily conversations and does not explicitly address the generalization of the alignment method to other domains or user populations, nor does it discuss potential biases and limitations.
- Why unresolved: The paper does not investigate the generalizability of the alignment method to different domains and user populations, nor does it explore potential biases and limitations in the current approach.
- What evidence would resolve it: Evaluating the model's performance on alignment tasks in different domains and with diverse user populations, and analyzing potential biases and limitations, would provide insights into the generalizability and robustness of the alignment method.

## Limitations

- The approach relies heavily on synthetic data generated by LLMs rather than actual human preferences, which may not capture true user behavior
- Persona diversity filtering assumes semantic similarity correlates with conversational similarity, which may not hold for all user types
- The method is evaluated primarily on daily conversations and may not generalize to specialized domains or complex user interactions

## Confidence

**High Confidence:** The dataset construction methodology (persona pool creation, multi-LLM collaboration, and preference data generation) is well-specified and reproducible based on the paper's description and Appendix B.

**Medium Confidence:** The effectiveness of the two-stage training approach (SFT + DPO) for preference alignment is supported by experimental results, though the specific hyperparameters and training details could affect reproducibility.

**Low Confidence:** The generalizability of the approach to real-world user interactions is uncertain, as the method is evaluated primarily on synthetic data generated by LLMs rather than actual human preferences.

## Next Checks

1. **Persona Diversity Validation:** Conduct human evaluations to verify that the generated persona pool covers a representative range of user preferences and that semantically similar personas produce distinguishable conversation patterns.

2. **Real User Testing:** Test the fine-tuned models with actual human users to validate that improvements in the ALOE benchmark translate to better real-world personalized interactions.

3. **Ablation Study:** Perform systematic ablation studies removing components (e.g., multi-LLM collaboration, SFT stage, DPO stage) to quantify their individual contributions to alignment performance.