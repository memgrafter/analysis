---
ver: rpa2
title: 'MOSSBench: Is Your Multimodal Language Model Oversensitive to Safe Queries?'
arxiv_id: '2406.17806'
source_url: https://arxiv.org/abs/2406.17806
tags:
- request
- image
- safety
- oversensitivity
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MOSSBench, the first benchmark for evaluating
  oversensitivity in multimodal large language models (MLLMs). MOSSBench identifies
  three visual stimuli types that trigger unwarranted refusals of benign queries:
  Exaggerated Risk, Negated Harm, and Counterintuitive Interpretation.'
---

# MOSSBench: Is Your Multimodal Language Model Oversensitive to Safe Queries?

## Quick Facts
- arXiv ID: 2406.17806
- Source URL: https://arxiv.org/abs/2406.17806
- Authors: Xirui Li; Hengguang Zhou; Ruochen Wang; Tianyi Zhou; Minhao Cheng; Cho-Jui Hsieh
- Reference count: 40
- Primary result: First benchmark identifying oversensitivity in MLLMs, with 300 curated image-text pairs showing up to 76% refusal rates on benign queries

## Executive Summary
This paper introduces MOSSBench, the first benchmark for evaluating oversensitivity in multimodal large language models (MLLMs). MOSSBench identifies three visual stimuli types that trigger unwarranted refusals of benign queries: Exaggerated Risk, Negated Harm, and Counterintuitive Interpretation. The benchmark comprises 300 manually curated image-text pairs, validated by human annotators. Empirical studies on 20 MLLMs reveal that oversensitivity is prevalent, with refusal rates reaching up to 76% for harmless queries. The findings show that safer models are more oversensitive, and different stimuli types cause errors at specific reasoning stages. These insights underscore the need for refined safety mechanisms to balance caution with contextually appropriate responses.

## Method Summary
MOSSBench consists of 300 manually curated image-text pairs covering three types of visual stimuli that trigger oversensitivity: Exaggerated Risk, Negated Harm, and Counterintuitive Interpretation. The benchmark evaluates 20 MLLMs using both human evaluation (AMT workers) and automated evaluation (GPT-4V as judge) to measure Average Refusal Rate (ARR) - the percentage of benign queries that models refuse to answer. The study reveals that oversensitivity is prevalent across models, with refusal rates up to 76% for harmless queries.

## Key Results
- MLLMs exhibit significant oversensitivity, refusing benign queries at rates up to 76% when exposed to specific visual stimuli
- Safer models show higher oversensitivity rates, suggesting a trade-off between safety and helpfulness
- Different visual stimuli types cause errors at distinct reasoning stages: perception, intent, and safety assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models reject benign queries due to perceived danger from visual stimuli (Exaggerated Risk)
- Mechanism: Visual elements in images are misinterpreted as high-risk, triggering safety filters despite contextual safety
- Core assumption: Safety mechanisms prioritize avoiding risk over contextual understanding
- Evidence anchors: "Exaggerated Risk, involving scenarios where elements in a context may appear dangerous at first but actually pose little to no real threat"; "Concretely, when MLLMs inappropriately reject queries that human judges consider benign"
- Break condition: If models can better integrate contextual cues to assess actual risk

### Mechanism 2
- Claim: Models focus on harmful elements and ignore contextual negation (Negated Harm)
- Mechanism: Safety filters detect harmful objects and override context indicating the image opposes such behaviors
- Core assumption: Safety mechanisms detect harmful content but lack contextual reasoning to recognize negation
- Evidence anchors: "Negated Harm, where the content, although including harmful elements, actually discourages such behaviors"; "the model overlooks the prohibition sign's context and concentrates only on the scene of domestic violence"
- Break condition: If models can recognize and interpret negation cues in images

### Mechanism 3
- Claim: Models misinterpret benign requests due to unlikely harmful assumptions (Counterintuitive Interpretation)
- Mechanism: Models assume worst-case scenarios for ambiguous queries, ignoring more plausible interpretations
- Core assumption: Safety mechanisms err on the side of caution by assuming potential harm in ambiguous situations
- Evidence anchors: "Counterintuitive Interpretation, where benign intentions are misinterpreted by the models in a way that is out of context"; "MLLMs often misinterpret such queries and assume the question concerns the safety of placing the girl in the cage instead"
- Break condition: If models can better assess likelihood of interpretations based on context

## Foundational Learning

- Concept: Cognitive distortions in humans
  - Why needed here: Understanding how human biases parallel model behaviors helps in diagnosing oversensitivity
  - Quick check question: What are examples of cognitive distortions in humans, and how do they relate to model oversensitivity?

- Concept: Multimodal reasoning stages
  - Why needed here: Identifying which reasoning stage (perception, intent, safety) causes errors is crucial for targeted improvements
  - Quick check question: What are the three stages of multimodal reasoning, and how might errors at each stage lead to oversensitivity?

- Concept: Safety alignment in AI
  - Why needed here: Understanding how safety mechanisms work helps in identifying why they might cause oversensitivity
  - Quick check question: How do safety mechanisms in AI models typically work, and what are potential downsides of overly cautious safety filters?

## Architecture Onboarding

- Component map: Image processing -> Caption generation -> Query interpretation -> Safety assessment -> Response
- Critical path: Multimodal input → Perception module → Intent reasoning → Safety decision → Output
- Design tradeoffs: Balancing safety (avoiding harm) vs. helpfulness (providing useful responses) is key
- Failure signatures: Frequent refusals of benign queries, inconsistent responses to similar stimuli, over-reliance on perceived risk
- First 3 experiments:
  1. Test model responses to images with benign elements that could be misinterpreted as risky
  2. Evaluate model ability to recognize negation cues in images with harmful elements but opposing context
  3. Assess model interpretations of ambiguous queries to see if it assumes worst-case scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the degree of oversensitivity in MLLMs correlate with the specific safety alignment techniques used during training (e.g., RLHF vs. red teaming vs. content filtering)?
- Basis in paper: [inferred] The paper notes that oversensitivity is prevalent across MLLMs and suggests a potential trade-off between safety and oversensitivity, but does not directly analyze the impact of different alignment techniques
- Why unresolved: The paper does not investigate the specific training methodologies of the models studied, making it difficult to isolate the effect of individual techniques on oversensitivity
- What evidence would resolve it: A systematic study comparing the oversensitivity levels of MLLMs trained with different safety alignment techniques on the MOSSBench dataset

### Open Question 2
- Question: What are the long-term effects of oversensitivity on user trust and engagement with MLLMs?
- Basis in paper: [explicit] The paper highlights the need for refined safety mechanisms that balance caution with contextually appropriate responses to improve reliability in real-world applications
- Why unresolved: The paper focuses on the prevalence and causes of oversensitivity but does not explore its impact on user behavior and perceptions over time
- What evidence would resolve it: Longitudinal studies tracking user interactions with MLLMs exhibiting varying degrees of oversensitivity, measuring trust, satisfaction, and continued usage

### Open Question 3
- Question: Can we develop more nuanced safety mechanisms for MLLMs that distinguish between genuine threats and benign queries containing potentially alarming visual stimuli?
- Basis in paper: [explicit] The paper identifies three types of visual stimuli that trigger oversensitivity and suggests the need for refined safety mechanisms that balance caution with contextually appropriate responses
- Why unresolved: The paper does not propose or evaluate potential solutions for mitigating oversensitivity while maintaining safety
- What evidence would resolve it: The development and evaluation of new safety mechanisms that can accurately assess the context and intent of queries, reducing false refusals while effectively blocking harmful content

## Limitations

- Human evaluation relies on AMT workers whose cultural perceptions may affect benchmark validity across different populations
- Automated evaluation using GPT-4V as judge introduces circularity concerns by using an MLLM to evaluate MLLM behavior
- Benchmark focuses exclusively on English-language queries and Western cultural contexts, limiting cross-cultural validation

## Confidence

- High Confidence: The identification of three distinct visual stimuli types causing oversensitivity is well-supported by empirical evidence across 20 MLLMs
- Medium Confidence: The correlation between model safety ratings and oversensitivity levels requires further validation with a larger sample of models and safety metrics
- Medium Confidence: The claim that different stimuli types affect specific reasoning stages is supported but would benefit from more granular error analysis

## Next Checks

1. Conduct cross-cultural validation by translating MOSSBench into multiple languages and evaluating with diverse human annotator pools to test cultural bias assumptions
2. Implement a meta-evaluation framework using multiple judge models (not just GPT-4V) to reduce evaluation bias and validate the automated assessment methodology
3. Design ablation studies that systematically remove safety filters from models to quantify the exact contribution of each safety mechanism to oversensitivity rates