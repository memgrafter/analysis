---
ver: rpa2
title: 'JaPOC: Japanese Post-OCR Correction Benchmark using Vouchers'
arxiv_id: '2409.19948'
source_url: https://arxiv.org/abs/2409.19948
tags:
- correction
- japanese
- error
- text
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents JaPOC, a post-OCR correction benchmark for
  Japanese vouchers. The study addresses the challenge of correcting OCR errors in
  Japanese corporate documents, particularly company names affected by seal impressions.
---

# JaPOC: Japanese Post-OCR Correction Benchmark using Vouchers

## Quick Facts
- arXiv ID: 2409.19948
- Source URL: https://arxiv.org/abs/2409.19948
- Reference count: 15
- T5Retrieva model achieved 94.8% accuracy, improving OCR accuracy by 9.4 percentage points

## Executive Summary
This paper introduces JaPOC, a post-OCR correction benchmark specifically designed for Japanese vouchers. The benchmark addresses the challenge of correcting OCR errors in Japanese corporate documents, particularly company names affected by seal impressions. Using real-world voucher images and two OCR services (Vision API and Robota API), the authors created a dataset with human-annotated ground truth. They then proposed T5-based language models as baselines for error correction, demonstrating that appropriately chosen pre-trained language models can effectively correct OCR errors in Japanese vouchers, with T5Retrieva outperforming both rule-based methods and other T5 variants.

## Method Summary
The study constructed an OCR error correction dataset using 11,000 voucher images with cropped company name regions, human-annotated ground truth, and OCR results from three services (Vision API, Robota API, Japanese OCR). The evaluation metric was word-level accuracy where all characters must match ground truth. For the neural approach, T5-based models were fine-tuned with 32 batch size, 64 max length, 5e-5 learning rate, 15,000 iterations, and gradient accumulation of 2 steps. A rule-based baseline used edit distance search against the National Tax Agency's corporate database with thresholds of edit distance <2 and ratio <0.30.

## Key Results
- T5Retrieva achieved 94.8% accuracy on average, improving OCR accuracy by 9.4 percentage points
- T5Retrieva outperformed rule-based approach (87.2%) and other T5 variants (9.3%)
- Rule-based approach achieved 87.2% accuracy with edit distance thresholds
- Benchmark and models are publicly available for future research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JaPOC's T5Retrieva model achieves high accuracy because it leverages pre-trained Japanese language understanding from T5X architecture fine-tuned on voucher-specific error patterns.
- Mechanism: The T5-based encoder-decoder architecture allows sequence-to-sequence transformation of OCR outputs to corrected text, capturing contextual dependencies missed by rule-based approaches.
- Core assumption: Pre-trained Japanese language models contain sufficient linguistic patterns to correct OCR errors specific to company names in vouchers.
- Evidence anchors:
  - [abstract]: "T5Retrieva model achieved 94.8% accuracy on average, significantly improving OCR accuracy by 9.4 percentage points"
  - [section]: "T5Retrieva achieves 94.8%, which is an improvement of 9.4 points from the accuracy of 85.4% before the correction"
  - [corpus]: Weak evidence - corpus contains general post-OCR correction papers but no specific comparison to Japanese voucher benchmarks
- Break condition: The mechanism fails when OCR errors create novel token sequences outside the pre-training distribution, or when company names contain specialized terminology not covered in pre-training data.

### Mechanism 2
- Claim: The benchmark construction method ensures meaningful evaluation by using real-world voucher images with actual seal-induced noise patterns.
- Mechanism: By sampling from real invoice images and annotating ground truth with human experts, the benchmark captures authentic error distributions rather than synthetic noise.
- Core assumption: Real voucher images contain representative error patterns that generalize to production OCR systems.
- Evidence anchors:
  - [section]: "We have constructed an OCR error correction dataset that includes the effects of seals on Japanese vouchers"
  - [section]: "We prepared a set of images by cropping the textual regions of company name items using Japanese invoices with permission"
  - [corpus]: Weak evidence - corpus shows general post-OCR correction approaches but doesn't address voucher-specific challenges
- Break condition: The mechanism breaks when real-world error patterns shift significantly (e.g., new seal designs) or when the sampled dataset doesn't represent the full diversity of voucher formats.

### Mechanism 3
- Claim: Rule-based approach provides a practical baseline by leveraging edit distance search against a comprehensive corporate database.
- Mechanism: The Levenshtein distance metric combined with database search finds close matches to OCR outputs, correcting errors when distance thresholds are met.
- Core assumption: Corporate names follow predictable patterns that can be captured by edit distance metrics and existing business registries.
- Evidence anchors:
  - [section]: "This approach uses the National Tax Agency's corporate database to perform a search by edit distance"
  - [section]: "If the edit distance is less than two and the ratio is less than 0.30, the candidate text is output as the corrected text"
  - [corpus]: No direct evidence - corpus focuses on neural approaches rather than database-driven correction
- Break condition: The mechanism fails when OCR errors exceed the edit distance threshold, when corporate names have high variation, or when the database becomes outdated.

## Foundational Learning

- Concept: OCR error types in Japanese text recognition
  - Why needed here: Understanding the specific error patterns (seals, font variations, character recognition) is crucial for designing effective correction models
  - Quick check question: What are the three main types of OCR errors observed in Japanese voucher processing according to the paper?

- Concept: Language model fine-tuning for sequence-to-sequence tasks
  - Why needed here: The T5-based approach requires understanding how to adapt pre-trained models for OCR correction specifically
  - Quick check question: What are the key hyperparameters used for fine-tuning T5 models in this study?

- Concept: Edit distance metrics and database search algorithms
  - Why needed here: The rule-based baseline relies on understanding how edit distance calculations work and how to efficiently search large corporate databases
  - Quick check question: What are the two conditions that must be met for the rule-based approach to output a corrected text?

## Architecture Onboarding

- Component map: Data pipeline → OCR services (Vision API, Robota API) → Error detection → T5-based correction or Rule-based correction → Evaluation metrics
- Critical path: Image preprocessing → OCR recognition → Post-OCR correction → Accuracy evaluation
- Design tradeoffs: Neural approach (T5) vs rule-based approach - T5 provides higher accuracy but requires more computational resources and pre-training data; rule-based is faster but less accurate
- Failure signatures: Low correction accuracy indicates poor model generalization, high false positives suggest overcorrection, zero improvement from baseline suggests model isn't learning error patterns
- First 3 experiments:
  1. Test baseline OCR accuracy on sample voucher images to establish error patterns
  2. Implement simple rule-based correction with edit distance to establish performance floor
  3. Fine-tune T5Retrieva on the training set and evaluate on validation set to tune hyperparameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do pre-trained weights affect OCR error correction performance for different types of Japanese company names (e.g., those with foreign words, long names, or unusual kanji combinations)?
- Basis in paper: [explicit] The paper notes that "error correction accuracy depends on the pre-trained weights even when the same T5 and hyperparameters are used" and observes different correction patterns between T5Retrieva and T5Megagon models.
- Why unresolved: The experiments only tested two pre-trained models without analyzing performance across different company name characteristics or systematically comparing weight selection strategies.
- What evidence would resolve it: Controlled experiments comparing multiple pre-trained weights across diverse company name types, with error analysis showing which model characteristics correlate with correction success for specific name patterns.

### Open Question 2
- Question: What is the optimal threshold for applying post-OCR correction when OCR accuracy is already high, to avoid degrading results?
- Basis in paper: [explicit] The authors note that "when OCR accuracy is high, error correction may worsen it more than it improves it" and observe this in the Robota set where correction reduced accuracy.
- Why unresolved: The paper demonstrates the problem but doesn't propose or test any decision mechanism for when to apply correction versus leaving results unchanged.
- What evidence would resolve it: Empirical studies determining confidence thresholds or confidence-scoring methods that predict when correction will be beneficial versus harmful, validated across multiple OCR service accuracy levels.

### Open Question 3
- Question: How does the proposed post-OCR correction method generalize to other Japanese text types beyond company names on vouchers?
- Basis in paper: [inferred] The benchmark focuses specifically on company names, and the authors mention that "information such as amounts and dates do not have linguistic meanings as a single piece of information, and effective error correction is considered to be difficult," suggesting limited scope.
- Why unresolved: The evaluation only tested company name correction, leaving unclear whether the approach works for other text types like addresses, dates, or amounts that appear on vouchers.
- What evidence would resolve it: Experiments extending the correction approach to multiple voucher text fields (addresses, dates, amounts) with benchmark datasets and performance comparisons across these text types.

## Limitations
- Benchmark focuses specifically on Japanese voucher company names affected by seal impressions, limiting generalizability
- Dataset size (11,000 samples) may be insufficient for full generalization across all Japanese voucher formats
- Study doesn't address computational efficiency or latency requirements for production deployment

## Confidence
- **High confidence**: The benchmark construction methodology and evaluation metrics are clearly defined and reproducible
- **Medium confidence**: The T5Retrieva model's superiority over other approaches is well-supported within the specific voucher domain
- **Medium confidence**: The rule-based baseline provides a reasonable performance floor, though its limitations are acknowledged

## Next Checks
1. **Cross-domain validation**: Test T5Retrieva on other Japanese document types (invoices, receipts, forms) to assess generalization beyond vouchers

2. **Ablation study on seal impact**: Systematically vary seal position, size, and density in test images to quantify their specific impact on OCR error correction performance

3. **Real-world deployment monitoring**: Implement the model in a production OCR pipeline for 3+ months, tracking correction accuracy, false positive rates, and user feedback to identify edge cases not captured in the benchmark