---
ver: rpa2
title: Analyzing Context Contributions in LLM-based Machine Translation
arxiv_id: '2410.16246'
source_url: https://arxiv.org/abs/2410.16246
tags:
- trgt
- contributions
- context
- example
- contribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper conducts a comprehensive analysis of how large language\
  \ models (LLMs) use different parts of context during machine translation. The authors\
  \ employ an attribution-based approach to measure the contribution of various context\
  \ parts\u2014including few-shot examples, source text, and target prefix\u2014to\
  \ generated translations."
---

# Analyzing Context Contributions in LLM-based Machine Translation

## Quick Facts
- arXiv ID: 2410.16246
- Source URL: https://arxiv.org/abs/2410.16246
- Reference count: 40
- One-line primary result: Attribution-based analysis reveals how different parts of context contribute to LLM-based translation, showing source dominance, positional bias, and potential for detecting hallucinations.

## Executive Summary
This paper conducts a comprehensive analysis of how large language models utilize different parts of context during machine translation. Using an attribution-based approach (ALTI), the authors measure the contribution of various context parts including few-shot examples, source text, and target prefix to generated translations. The study reveals systematic patterns in how information flows from different context components and how these patterns change based on model pretraining, translation direction, and generation stage.

The research demonstrates that source parts of few-shot examples consistently contribute more than target parts, that pretraining on parallel data reduces reliance on few-shot examples, and that anomalous context contributions can potentially indicate pathological translations. The findings provide new insights into the mechanisms of in-context learning for translation and suggest novel approaches for detecting hallucinations in LLM outputs.

## Method Summary
The study employs an attribution-based approach (ALTI) to measure token-to-token contributions across transformer layers, then aggregates these to compute part-level contributions from different context components. The analysis uses Llama-2 7B and Tower 7B base models (plus Tower-Instruct 7B) on WMT22 test sets for German↔English and Russian↔English translation pairs with 5-shot examples. Due to computational constraints, samples exceeding 400 tokens or with generated sequences shorter than 10 tokens were filtered out. Translation quality is evaluated using BLEU, COMET-22, and COMETKiwi scores, while contribution patterns are analyzed across different generation stages and translation directions.

## Key Results
- Source parts of few-shot examples contribute more than target parts across all translation directions, regardless of whether translating into or out of English
- Continued pretraining on parallel data reduces the influence of few-shot examples and diminishes positional bias in contribution patterns
- Earlier few-shot examples exhibit higher contributions than later ones, demonstrating strong positional bias that can be partially overcome by relevant content
- Anomalous context contributions, particularly low source contributions, can potentially indicate pathological translations like hallucinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Source parts of few-shot examples contribute more than target parts regardless of translation direction.
- Mechanism: ALTI attribution reveals that information flow from source tokens to generated translations consistently exceeds flow from target tokens in few-shot examples.
- Core assumption: ALTI accurately captures the relative influence of different context parts on generated outputs.
- Evidence anchors:
  - [abstract] "the source part of few-shot examples appears to contribute more than its corresponding targets, irrespective of translation direction"
  - [section 4.2] "the source of each few-shot example consistently contributes more than its corresponding target"
  - [corpus] Weak - no direct corpus evidence supporting this specific claim about source-target asymmetry
- Break condition: If ALTI method incorrectly measures information flow, or if translation direction fundamentally changes information flow patterns.

### Mechanism 2
- Claim: Continued pretraining on parallel data reduces few-shot example influence and positional bias.
- Mechanism: Models trained on parallel data develop stronger task-specific knowledge, reducing dependence on in-context examples for task completion.
- Core assumption: Parallel data pretraining provides sufficient task-specific knowledge to reduce reliance on few-shot examples.
- Evidence anchors:
  - [abstract] "finetuning LLMs with parallel data alters the contribution patterns of different context parts"
  - [section 4.2] "Training on parallel data reduces the impact of the provided examples on the translated sequence"
  - [corpus] Weak - corpus contains studies on LLM pretraining but not specifically this positional bias reduction
- Break condition: If pretraining data quality is insufficient, or if positional bias is inherent to model architecture rather than task knowledge.

### Mechanism 3
- Claim: Low source contributions can indicate pathological translations like hallucinations.
- Mechanism: When models generate translations without properly attending to source text, anomalous context contribution patterns emerge, revealing copying or fabrication behaviors.
- Core assumption: Source text contribution is necessary for correct translation generation.
- Evidence anchors:
  - [abstract] "inspecting anomalous context contributions can potentially uncover pathological translations, such as hallucinations"
  - [section 4.2] "Close inspection of context contributions can uncover anomalous translations"
  - [section 6] "Low source contributions are, in some cases, predictive of hallucinations"
  - [corpus] Moderate - corpus contains related work on hallucination detection but not this specific contribution-based approach
- Break condition: If models can generate reasonable translations without source text contribution, or if contribution measurement is unreliable.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: The study examines how LLMs use few-shot examples during translation, requiring understanding of ICL paradigm
  - Quick check question: What distinguishes in-context learning from traditional fine-tuning approaches?

- Concept: Attribution methods for neural networks
  - Why needed here: ALTI attribution method is used to measure context part contributions, requiring understanding of input attribution techniques
  - Quick check question: How does ALTI differ from traditional attention-based interpretation methods?

- Concept: Transformer information flow
  - Why needed here: Understanding how token representations mix across layers is crucial for interpreting ALTI results
  - Quick check question: What does the token-to-token contribution matrix represent in terms of information flow?

## Architecture Onboarding

- Component map: ALTI computation -> Part-level contribution aggregation -> Analysis and interpretation
- Critical path: ALTI computation → Part-level contribution aggregation → Analysis and interpretation
- Design tradeoffs: Computational intensity vs. granularity of analysis (filtering needed for large contexts)
- Failure signatures: Anomalous contribution patterns (very low source contribution, unusual positional bias)
- First 3 experiments:
  1. Verify ALTI attribution accuracy by comparing with simpler attention-based methods
  2. Test contribution patterns on a controlled dataset with known translation quality
  3. Examine contribution evolution on pathological translations identified through other methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do context contributions evolve in larger LLM models (e.g., 70B+ parameters) compared to the 7B models studied?
- Basis in paper: [inferred] The paper explicitly states that due to computational resource constraints, they only analyzed 7B parameter models, raising the question of whether findings hold for larger models.
- Why unresolved: The study was limited to 7B parameter models due to high computational requirements of the ALTI attribution method, preventing analysis of larger models.
- What evidence would resolve it: Conducting the same attribution-based analysis on 70B+ parameter models would reveal if positional biases, source contributions, and anomaly detection patterns scale or change with model size.

### Open Question 2
- Question: How do different families of LLMs (beyond LLaMA-based models) utilize context in machine translation tasks?
- Basis in paper: [explicit] The authors state they focused exclusively on LLaMA-based models to study the TOWER family specifically, but acknowledge it's unclear whether findings generalize to other LLM families.
- Why unresolved: The study's focus on LLaMA-based models means we don't know if positional biases, source contribution patterns, and other findings apply to models like GPT, Claude, or other architectures.
- What evidence would resolve it: Applying the same analysis framework to other major LLM families would show whether these context utilization patterns are universal or specific to certain model architectures.

### Open Question 3
- Question: Under what conditions can low source contributions reliably predict hallucinations in LLM-based translation, and when might they be misleading?
- Basis in paper: [explicit] The authors found that low source contributions can potentially indicate hallucinations, but their quantitative analysis showed this relationship is inconsistent across models and translation directions.
- Why unresolved: While the paper identified some correlation between low source contributions and hallucinations in certain models (TOWER), this relationship doesn't hold universally, and the paper doesn't identify the conditions that make this prediction reliable.
- What evidence would resolve it: Systematic analysis across more models, languages, and translation tasks would identify specific conditions (model architecture, training data, task type) under which low source contributions are predictive versus when they're misleading indicators.

## Limitations

- Computational constraints necessitated aggressive filtering of test samples, potentially introducing selection bias
- The ALTI attribution method lacks direct comparison with alternative attribution approaches
- Limited analysis to LLaMA-based models prevents generalization to other LLM families

## Confidence

- **High Confidence**: The observation that source parts of few-shot examples contribute more than target parts is well-supported by multiple analyses across different language pairs and models.
- **Medium Confidence**: The claim that pretraining on parallel data reduces few-shot example influence and positional bias is supported but could be affected by model-specific factors beyond pretraining data.
- **Low Confidence**: The hypothesis that anomalous context contributions can reliably indicate hallucinations requires further validation, as the relationship between contribution patterns and translation pathologies needs more systematic study.

## Next Checks

1. **Cross-Validation of Attribution Method**: Apply multiple attribution methods (e.g., Integrated Gradients, attention-based methods) to the same translation tasks and compare the resulting contribution patterns. This would help establish whether the observed source-target asymmetry is a robust finding or specific to the ALTI method.

2. **Controlled Hallucination Experiment**: Generate a test set with known hallucinated translations through systematic manipulation (e.g., removing source text, adding contradictory examples) and measure whether context contribution patterns reliably predict hallucination presence across different types of pathological generation.

3. **Model Size Scalability Test**: Replicate the core contribution analyses on a smaller, computationally tractable model (e.g., 1.3B parameters) without aggressive filtering to assess whether the observed patterns hold when analyzing a larger fraction of the test set, and examine how contribution patterns scale with model size.