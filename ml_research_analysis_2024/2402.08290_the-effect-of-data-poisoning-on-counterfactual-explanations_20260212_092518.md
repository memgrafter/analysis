---
ver: rpa2
title: The Effect of Data Poisoning on Counterfactual Explanations
arxiv_id: '2402.08290'
source_url: https://arxiv.org/abs/2402.08290
tags:
- data
- poisoning
- recourse
- cost
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the vulnerability of counterfactual explanations
  to data poisoning attacks. The authors formalize data poisoning mechanisms for increasing
  the cost of recourse on local, sub-group, and global levels.
---

# The Effect of Data Poisoning on Counterfactual Explanations

## Quick Facts
- arXiv ID: 2402.08290
- Source URL: https://arxiv.org/abs/2402.08290
- Authors: André Artelt; Shubham Sharma; Freddy Lecué; Barbara Hammer
- Reference count: 40
- Primary result: Data poisoning can significantly increase the cost of counterfactual explanations with as little as 5% of training data, and existing defenses fail to detect these attacks.

## Executive Summary
This paper studies the vulnerability of counterfactual explanations to data poisoning attacks. The authors propose a model-agnostic algorithm that strategically adds poisonous training instances to increase the cost of recourse (i.e., the difficulty of achieving favorable outcomes through actionable changes). The attack can be targeted at local, sub-group, or global levels by controlling the density function that defines affected instances. Experiments on benchmark datasets show that even small poisoning budgets can significantly increase recourse costs while evading detection by existing defense methods. A case study on water distribution network event detection demonstrates real-world implications for system safety and trustworthiness.

## Method Summary
The method introduces a data poisoning algorithm that adds realistic poisonous instances to training data by leveraging closest counterfactual explanations or adversarial samples. Algorithm 1 constructs poisonous instances that are similar to existing training samples but strategically placed near decision boundaries to manipulate counterfactual explanations. The poisoning can target different levels: locally for specific instances, sub-groups of instances, or globally for all instances. The algorithm evaluates effectiveness through increased cost of recourse (ℓ1 norm), while maintaining plausible data points that evade traditional outlier detection methods.

## Key Results
- Even 5% poisoning budget significantly increases recourse costs across multiple datasets and classifiers
- Data poisoning increases sparsity of counterfactual explanations while maintaining plausibility
- Classical outlier detection and data sanitization defenses fail to detect poisonous instances
- Real-world case study shows potential safety implications for water distribution network monitoring

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data poisoning can increase the cost of recourse by manipulating the decision boundary through strategically placed poisonous instances.
- Mechanism: The algorithm adds realistic training instances that are close to the decision boundary and aligned with the counterfactual direction of targeted samples, shifting the boundary to make recourse more expensive.
- Core assumption: Counterfactual explanations that are close to the decision boundary are particularly vulnerable to boundary shifts, and classifiers can be manipulated without severely degrading predictive performance.
- Evidence anchors:
  - [abstract] "This work studies the vulnerability of counterfactual explanations to data poisoning attacks... showing that even small poisoning budgets (5% of training data) can significantly increase recourse costs."
  - [section 4.1.1] "Theorem 1 (Local Recourse Poisoning Data Sets for 1-Nearest Neighbor Classifiers)... samples on the decision boundary are data poisoning instances."

### Mechanism 2
- Claim: The algorithm can target different levels of poisoning (local, sub-group, global) by controlling the density function ϕ(·) that defines which instances are affected.
- Mechanism: By constructing a probability density that concentrates on specific regions or groups, the algorithm can focus the poisoning effect on targeted areas while minimizing impact on others.
- Core assumption: The attacker can define and sample from a density function that accurately represents the targeted instances or groups.
- Evidence anchors:
  - [abstract] "...for increasing the cost of recourse on three different levels: locally for a single instance, a sub-group of instances, or globally for all instances."
  - [section 4.1] "Given the fact that counterfactual explanations constitute a local explanation, potential data poisonings can have effects on different levels or areas in data space."

### Mechanism 3
- Claim: Classic outlier detection and data sanitization methods fail to detect the poisonous instances because they are designed to look realistic and be on the data manifold.
- Mechanism: The algorithm ensures poisonous instances are similar to existing training samples from the same class, making them difficult to distinguish from normal data using traditional outlier detection.
- Core assumption: Outlier detection methods rely on distance metrics or density estimates that cannot easily distinguish between realistic poisonous instances and normal training data.
- Evidence anchors:
  - [abstract] "Furthermore, we find that existing defense methods fail to detect those poisonous samples."
  - [section 6.3.5] "we observe (see Figure B.10) that all evaluated defense methods for outlier detection struggle to identify the poisonous samples."

## Foundational Learning

- Concept: Counterfactual explanations and their properties
  - Why needed here: The paper builds its poisoning attack specifically around manipulating the cost of counterfactual explanations, so understanding what they are and how they're computed is fundamental.
  - Quick check question: What are the two key properties of counterfactual explanations mentioned in the paper?

- Concept: Data poisoning attacks and their mechanisms
  - Why needed here: The paper proposes a new type of data poisoning attack, so understanding traditional data poisoning concepts is essential for grasping the novelty.
  - Quick check question: How does data poisoning differ from adversarial attacks in terms of when they occur in the ML pipeline?

- Concept: Outlier detection and data sanitization methods
  - Why needed here: The paper evaluates how existing defense methods fail against the new poisoning attack, so understanding these methods is crucial for interpreting the results.
  - Quick check question: What is the basic principle behind most data sanitization defenses against poisoning?

## Architecture Onboarding

- Component map: Target samples selection (Dtarget) -> Counterfactual generation mechanism (CF) -> Poisonous instance construction (Algorithm 1) -> Classifier training and evaluation pipeline -> Defense method evaluation module

- Critical path: Dtarget → CF → Poisonous instance construction → Training with poisoned data → Evaluation of cost of recourse → Defense detection evaluation

- Design tradeoffs:
  - Number of poisonous instances vs. stealthiness (more instances = more impact but easier to detect)
  - Closeness to decision boundary vs. plausibility (closer = more effective but potentially less realistic)
  - Global vs. local targeting (broader impact vs. more focused attack)

- Failure signatures:
  - Predictive performance drops significantly (model is overfitting to poison)
  - Defense methods successfully detect poisonous instances (sampling strategy is too noisy)
  - No statistically significant increase in cost of recourse (poisonous instances are ineffective)

- First 3 experiments:
  1. Run Algorithm 1 with a small poisoning budget (5%) on a simple classifier (1-NN) to verify basic functionality using Theorem 1 as ground truth.
  2. Evaluate the increase in cost of recourse on a global level using the Diabetes dataset and a DNN classifier with DiCE counterfactuals.
  3. Test the effectiveness of the Isolation Forest defense method on the poisonous instances generated in experiment 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the cost of recourse vary across different data poisoning budgets and counterfactual generation methods?
- Basis in paper: [explicit] The paper empirically evaluates the impact of data poisoning on the cost of recourse using different poisoning budgets (5% to 70%) and counterfactual generation methods (NUN, DiCE, Proto).
- Why unresolved: While the paper shows a general trend of increasing cost with poisoning budget, it does not provide a detailed analysis of the specific relationship between budget size and cost across different methods.
- What evidence would resolve it: A comprehensive analysis of the cost-recourse relationship for each method and budget level, including statistical significance tests and visualizations.

### Open Question 2
- Question: How effective are data sanitization methods in detecting poisonous instances in counterfactual explanations?
- Basis in paper: [explicit] The paper evaluates the performance of several data sanitization methods (Isolation Forest, LOF, k-NN defense, ℓ2-defense, slab-defense) in detecting poisonous instances.
- Why unresolved: The paper finds that existing methods struggle to detect poisonous instances, but it does not explore the reasons behind this failure or propose alternative detection strategies.
- What evidence would resolve it: A deeper investigation into the characteristics of poisonous instances that make them difficult to detect, along with the development and evaluation of new detection methods.

### Open Question 3
- Question: How does data poisoning affect the fairness of counterfactual explanations?
- Basis in paper: [explicit] The paper considers sub-group data poisoning to increase the difference in the cost of recourse between protected groups, but it does not provide a comprehensive analysis of the fairness implications.
- Why unresolved: The paper only focuses on group fairness in terms of cost of recourse, but it does not explore other fairness aspects such as individual fairness or disparate impact.
- What evidence would resolve it: A thorough analysis of the fairness implications of data poisoning on counterfactual explanations, including metrics for individual fairness and disparate impact, along with mitigation strategies.

## Limitations

- The poisoning effectiveness depends heavily on the quality and diversity of counterfactual explanations, which may be limited in practice
- The attack assumes attacker knowledge of target classifier architecture or query access to obtain counterfactuals
- Defense evaluation is limited to classical outlier detection methods; more advanced defenses like adversarial training are not explored

## Confidence

- **High confidence**: The mechanism that data poisoning can increase recourse cost by manipulating the decision boundary is well-supported by theoretical results (Theorem 1) and empirical evaluations showing significant increases in ℓ1 cost even with small poisoning budgets (5%).
- **Medium confidence**: The claim that the algorithm can effectively target different poisoning levels (local, sub-group, global) through the density function ϕ(·) is supported by experimental design but relies on assumptions about density function accuracy.
- **Low confidence**: The assumption that poisonous instances remain undetected because they are designed to be realistic is based on evaluation against classical outlier detection methods only; more sophisticated defenses could potentially detect these attacks.

## Next Checks

1. Evaluate the poisoning attack against advanced defenses like adversarial training, certified robustness methods, or anomaly detection using deep generative models.

2. Systematically vary the quality and diversity of counterfactual explanations used to generate poisonous instances and measure the impact on poisoning effectiveness.

3. Apply the poisoning attack to a real-world dataset with known fairness concerns (e.g., COMPAS) and evaluate both the increase in recourse cost and potential disparate impact on protected subgroups.