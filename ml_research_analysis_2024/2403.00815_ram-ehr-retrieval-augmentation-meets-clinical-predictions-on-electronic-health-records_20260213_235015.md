---
ver: rpa2
title: 'RAM-EHR: Retrieval Augmentation Meets Clinical Predictions on Electronic Health
  Records'
arxiv_id: '2403.00815'
source_url: https://arxiv.org/abs/2403.00815
tags:
- knowledge
- medical
- information
- clinical
- ram-ehr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents RAM-EHR, a retrieval-augmented framework to
  enhance clinical predictions using Electronic Health Records (EHRs). The key idea
  is to collect multiple knowledge sources, convert them to text format, and use dense
  retrieval to obtain information related to medical concepts.
---

# RAM-EHR: Retrieval Augmentation Meets Clinical Predictions on Electronic Health Records

## Quick Facts
- **arXiv ID**: 2403.00815
- **Source URL**: https://arxiv.org/abs/2403.00815
- **Reference count**: 17
- **Primary result**: Outperforms knowledge-enhanced baselines with 3.4% gain in AUROC and 7.2% gain in AUPR

## Executive Summary
RAM-EHR is a retrieval-augmented framework designed to enhance clinical predictions using Electronic Health Records (EHRs). The framework addresses the challenge of complex medical code names by collecting multiple knowledge sources, converting them to text format, and using dense retrieval to obtain semantically related information. It then augments a local EHR predictive model with consistency regularization to capture complementary information from patient visits and summarized knowledge. Experiments on MIMIC-III and CRADLE datasets demonstrate that RAM-EHR outperforms knowledge-enhanced baselines with significant improvements in AUROC and AUPR metrics.

## Method Summary
RAM-EHR integrates external knowledge into clinical prediction tasks through a multi-stage process. First, it converts multiple heterogeneous knowledge sources (PubMed, DrugBank, MeSH, Wikipedia, and biomedical knowledge graphs) into a unified text corpus. Dense retrieval is then used to encode medical codes and passages into dense vectors, capturing semantic similarity rather than exact string matches. An LLM generates concise summaries from retrieved passages to reduce irrelevant information and input length. The framework employs co-training with consistency regularization, where one model uses only patient visit information and another uses summarized knowledge, with KL divergence encouraging consistent predictions while maintaining complementary information.

## Key Results
- Achieves 3.4% gain in AUROC and 7.2% gain in AUPR compared to knowledge-enhanced baselines
- Outperforms all knowledge-enhanced baselines across both MIMIC-III and CRADLE datasets
- Incorporating all knowledge sources yields highest performance, with DrugBank alone contributing minimally due to limited scope

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dense retrieval with multiple knowledge sources improves semantic coverage of medical concepts in EHRs
- Mechanism: The framework converts multiple heterogeneous knowledge sources into a unified text corpus, then uses dense retrieval to encode medical codes and passages into dense vectors, capturing semantic similarity rather than exact string matches
- Core assumption: Dense vector representations can effectively bridge the gap between medical codes with non-uniform surface names and their semantic meaning in external knowledge
- Evidence anchors:
  - [abstract] "uses dense retrieval to obtain information related to medical concepts. This strategy addresses the difficulties associated with complex names for the concepts."
  - [section 3.2] "we leverage dense retrieval (DR) to encode corpus and medical codes as dense vectors, intuitively capturing the semantics of medical codes and addressing the alignment issue between EHR and external knowledge"
- Break condition: If dense retrieval fails to capture semantic relationships due to poor encoding quality or if the corpus lacks relevant knowledge for specific medical concepts

### Mechanism 2
- Claim: LLM summarization of retrieved passages produces task-specific knowledge that is more useful than raw concatenated text
- Mechanism: After retrieving top-k passages for each medical code, an LLM generates concise summaries tailored to the downstream prediction task, filtering out irrelevant information and reducing input length
- Core assumption: LLMs can effectively distill relevant information from multiple passages into a single coherent summary that captures task-specific knowledge
- Evidence anchors:
  - [abstract] "to reduce irrelevant information and reduce the length of the input text, we utilize an LLM to summarize the top-retrieved passages into concise and informative knowledge summaries relevant to downstream tasks"
  - [section 3.2] "we utilize an LLM to generate the summarized knowledge ei for medical code ci" with prompt design ensuring task-specific summaries
- Break condition: If the LLM generates summaries that are too generic, miss critical information, or introduce hallucinated content that negatively impacts predictions

### Mechanism 3
- Claim: Co-training with consistency regularization effectively combines complementary information from visit-based and knowledge-augmented models
- Mechanism: Two models are trained simultaneously - one using only patient visit information and another using summarized knowledge, with KL divergence regularization encouraging their predictions to be consistent while maintaining complementary information
- Core assumption: Patient visits capture co-occurrence relationships while summarized knowledge provides semantic information, and these complementary views can be effectively combined through co-training
- Evidence anchors:
  - [abstract] "augments the local EHR predictive model co-trained with consistency regularization to capture complementary information from patient visits and summarized knowledge"
  - [section 3.3] "Two losses in Eq. 5 are designed to encourage fθ and gϕ regularize each other, which can stabilize the learning for two models"
- Break condition: If the regularization strength is not properly tuned, the models may collapse to making identical predictions, eliminating the benefit of complementary information

## Foundational Learning

- Concept: Dense retrieval and semantic matching
  - Why needed here: EHR medical codes often have non-uniform surface names (abbreviations, colloquial terms) that make exact matching ineffective; dense retrieval captures semantic relationships
  - Quick check question: How does dense retrieval differ from traditional keyword-based retrieval in handling medical terminology variations?

- Concept: Knowledge graph embeddings and semantic representation
  - Why needed here: Medical knowledge exists in multiple structured and unstructured forms; understanding how to represent this knowledge semantically is crucial for effective retrieval and summarization
  - Quick check question: What are the advantages of using knowledge graph embeddings versus traditional symbolic representations for medical concepts?

- Concept: Co-training and consistency regularization
  - Why needed here: The framework relies on combining predictions from two different models trained on complementary data views; understanding co-training helps in implementing and tuning this approach
  - Quick check question: How does consistency regularization between two models help prevent overfitting and improve generalization?

## Architecture Onboarding

- Component map: Patient visit data → Hypergraph Transformer (fθ) for local predictions; Medical codes + surface names → Dense retrieval → LLM summarization → Knowledge-augmented Transformer (gϕ); Co-training with consistency loss → Final prediction as weighted combination
- Critical path: Medical code → Dense retrieval → LLM summarization → Knowledge-augmented model → Co-training → Prediction
- Design tradeoffs: Using summarized knowledge reduces input length but may lose some detail; co-training adds complexity but improves performance; multiple knowledge sources increase coverage but also retrieval complexity
- Failure signatures: Poor performance if retrieval returns irrelevant passages; summaries lack task-specific information; co-training causes model collapse; one knowledge source dominates others
- First 3 experiments:
  1. Test dense retrieval quality by measuring similarity between retrieved passages and ground truth relevant documents for sample medical codes
  2. Evaluate LLM summarization quality by comparing generated summaries against human-written summaries for the same medical codes
  3. Run ablation study with and without co-training to measure the contribution of consistency regularization to overall performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of external knowledge sources impact the performance of RAM-EHR on different clinical prediction tasks?
- Basis in paper: [explicit] The paper mentions that incorporating all corpus yields the highest performance and that using DrugBank alone contributes minimally, likely due to its limited scope of medication information.
- Why unresolved: While the paper provides some insights into the impact of different knowledge sources, a comprehensive analysis of how various combinations of knowledge sources affect performance across different clinical tasks is not presented.
- What evidence would resolve it: Conducting extensive experiments using different combinations of knowledge sources and evaluating their performance on various clinical prediction tasks would provide evidence to answer this question.

### Open Question 2
- Question: How can the efficiency of RAM-EHR be improved, especially when dealing with large-scale EHR datasets?
- Basis in paper: [inferred] The paper mentions that integrating the augmented model gφ can result in additional time complexity, and using a lightweight model like Clin-MobileBERT can improve efficiency. However, it does not explore other potential methods to optimize the framework's efficiency.
- Why unresolved: The paper acknowledges the efficiency concern but does not delve into potential solutions or optimizations to address this issue, especially for large-scale datasets.
- What evidence would resolve it: Investigating and implementing various techniques to optimize the retrieval, summarization, and co-training processes, and evaluating their impact on efficiency and performance would provide evidence to answer this question.

### Open Question 3
- Question: How does RAM-EHR perform on other types of clinical tasks beyond phenotyping prediction and CVD outcome prediction?
- Basis in paper: [inferred] The paper focuses on two specific clinical prediction tasks (phenotyping prediction on MIMIC-III and CVD outcome prediction on CRADLE) and mentions that there are numerous other clinical tasks and prediction models that exist.
- Why unresolved: The paper does not explore the performance of RAM-EHR on a broader range of clinical tasks, leaving uncertainty about its generalizability and effectiveness across different domains.
- What evidence would resolve it: Evaluating RAM-EHR on a diverse set of clinical tasks, such as disease progression prediction, treatment response prediction, or patient risk stratification, and comparing its performance with other state-of-the-art methods would provide evidence to answer this question.

## Limitations

- The effectiveness of RAM-EHR depends heavily on the quality of dense retrieval and LLM summarization, neither of which are directly evaluated in the paper
- The framework assumes that dense retrieval can effectively bridge the semantic gap between medical codes and external knowledge, but this is not empirically validated
- The quality of LLM-generated summaries is assumed to be task-relevant, but no evaluation of summary quality or hallucination risk is provided
- The co-training mechanism requires careful hyperparameter tuning that is not fully specified in the paper

## Confidence

- **High Confidence**: The core claim that combining patient visit data with summarized external knowledge improves clinical predictions is supported by strong experimental results (3.4% AUROC and 7.2% AUPR gains over baselines)
- **Medium Confidence**: The effectiveness of dense retrieval and LLM summarization is theoretically sound, but lacks direct empirical validation
- **Low Confidence**: The co-training mechanism's contribution is difficult to isolate, as the paper does not provide clear ablation studies separating the effects of consistency regularization from other components

## Next Checks

1. **Dense Retrieval Quality Evaluation**: Measure the semantic similarity between retrieved passages and ground truth relevant documents for a sample of medical codes using metrics like Recall@K and MRR

2. **LLM Summary Quality Assessment**: Compare LLM-generated summaries against human-written summaries for the same medical codes using metrics like ROUGE and BERTScore, and perform manual inspection for hallucination and task relevance

3. **Co-training Contribution Isolation**: Run an ablation study that compares RAM-EHR performance with and without consistency regularization, and with different regularization strengths to quantify the contribution of the co-training mechanism