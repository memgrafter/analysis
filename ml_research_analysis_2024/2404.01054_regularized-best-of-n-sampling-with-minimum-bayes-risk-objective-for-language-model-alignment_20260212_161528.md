---
ver: rpa2
title: Regularized Best-of-N Sampling with Minimum Bayes Risk Objective for Language
  Model Alignment
arxiv_id: '2404.01054'
source_url: https://arxiv.org/abs/2404.01054
tags:
- reward
- mbr-bon
- proxy
- reference
- gold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses reward hacking in Best-of-N (BoN) sampling
  for language model alignment. BoN sampling can overfit to an imperfect proxy reward
  model, leading to degraded performance.
---

# Regularized Best-of-N Sampling with Minimum Bayes Risk Objective for Language Model Alignment

## Quick Facts
- arXiv ID: 2404.01054
- Source URL: https://arxiv.org/abs/2404.01054
- Reference count: 40
- This paper proposes MBR-BoN, which incorporates the Minimum Bayes Risk (MBR) objective as a proximity regularizer to mitigate reward hacking in Best-of-N sampling for language model alignment.

## Executive Summary
This paper addresses the problem of reward hacking in Best-of-N sampling, where language models overfit to imperfect proxy reward models leading to degraded performance on the true objective. The authors propose MBR-BoN, which combines proxy reward scores with a Minimum Bayes Risk (MBR) objective that measures proximity to a reference policy. Experiments on AlpacaFarm and Anthropic's hh-rlhf datasets demonstrate that MBR-BoN outperforms vanilla Best-of-N in most settings, particularly when proxy reward models have weak correlation with gold reference rewards. The method also shows promise for generating high-quality preference datasets for Direct Preference Optimization (DPO).

## Method Summary
MBR-BoN is a regularized Best-of-N sampling method that incorporates the Minimum Bayes Risk (MBR) objective as a proximity regularizer. For each input prompt, the method samples N responses using nucleus sampling, computes both a proxy reward score and an MBR objective for each response, and selects the response with the highest combined score (reward score + β × MBR objective). The MBR objective quantifies the similarity between responses and the reference policy using cosine similarity of MPNet embeddings. The hyperparameter β controls the tradeoff between reward optimization and proximity to the reference policy, and is tuned on a development set.

## Key Results
- MBR-BoN outperforms vanilla BoN in most settings, especially when proxy reward models have weak correlation with gold reference rewards
- The method is effective across different datasets (AlpacaFarm, hh-rlhf) and reward model combinations
- MBR-BoN-generated datasets lead to better-performing models when used for Direct Preference Optimization (DPO) training
- The optimal regularization strength β depends on the specific proxy and gold reward models, requiring development set tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MBR-BoN mitigates reward hacking by using the Minimum Bayes Risk (MBR) objective as a proximity regularizer.
- Mechanism: The MBR objective quantifies the similarity between generated responses and the reference policy, encouraging outputs that are more representative of the reference model's behavior. This reduces over-optimization to an imperfect proxy reward model.
- Core assumption: The MBR objective is a reliable measure of proximity to the reference policy, even when the number of samples is small.
- Evidence anchors:
  - [abstract] "MBR-BoN, which incorporates the Minimum Bayes Risk (MBR) objective as a proximity regularizer to mitigate this issue."
  - [section 3] "The MBR objective serves as a proximity regularizer by its nature which we show in Sec-tion 3."
  - [corpus] "Found 25 related papers... Top related titles: Optimal Policy Minimum Bayesian Risk, Soft Best-of-n Sampling for Model Alignment, Variational Best-of-N Alignment."
- Break condition: If the utility function used in MBR does not accurately reflect the true similarity between responses and the reference policy, the regularization may not effectively mitigate reward hacking.

### Mechanism 2
- Claim: MBR-BoN outperforms vanilla BoN in most settings, especially when the proxy reward model is weakly correlated with the gold reference reward model.
- Mechanism: By incorporating the MBR objective, MBR-BoN balances the reward score with the proximity to the reference policy. This prevents the model from overfitting to the proxy reward, leading to better performance on the true objective.
- Core assumption: The proxy reward model is an imperfect proxy for the true objective, and over-optimizing it leads to degraded performance.
- Evidence anchors:
  - [abstract] "Experiments on AlpacaFarm and Anthropic's hh-rlhf datasets show that MBR-BoN outperforms vanilla BoN in most settings."
  - [section 5.1] "Overall, MBR-BoN outperforms BoN and MBR in most of the settings, showing that the method is effective in a wide range of tasks."
  - [corpus] "Found 25 related papers... Top related titles: Bayesian Reward Models for LLM Alignment, Sampling-Efficient Test-Time Scaling..."
- Break condition: If the proxy reward model is highly correlated with the gold reference reward model, the benefit of MBR-BoN over vanilla BoN may be minimal.

### Mechanism 3
- Claim: MBR-BoN is an effective strategy for generating a pairwise preference learning dataset for Direct Preference Optimization (DPO).
- Mechanism: MBR-BoN generates high-quality and on-policy responses that are representative of the reference policy. Models trained on a dataset generated with MBR-BoN outperform those trained on a dataset generated with vanilla BoN.
- Core assumption: High-quality and on-policy responses are important characteristics of efficient preference datasets.
- Evidence anchors:
  - [abstract] "We also evaluate MBR-BoN to generate a pairwise preference learning dataset for Direct Preference Optimization (DPO). Empirical results show that models trained on a dataset generated with MBR-BoN outperform those with vanilla BoN."
  - [section 5.2] "The result shows the potential of MBR-BoN as a tool for generating pairwise preference datasets for preference learning."
  - [corpus] "Found 25 related papers... Top related titles: Best of mini-N in-loop Sampling: A Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling, Majority of the Bests: Improving Best-of-N via Bootstrapping."
- Break condition: If the DPO training process does not benefit from on-policy data, the advantage of MBR-BoN for dataset generation may be reduced.

## Foundational Learning

- Concept: Reward hacking
  - Why needed here: Understanding reward hacking is crucial to grasp the problem that MBR-BoN aims to solve. It explains why over-optimizing an imperfect proxy reward model can lead to degraded performance on the true objective.
  - Quick check question: What is reward hacking, and how does it occur in the context of Best-of-N sampling?

- Concept: Minimum Bayes Risk (MBR) decoding
  - Why needed here: MBR decoding is the foundation of MBR-BoN. It provides the mechanism for quantifying the proximity of generated responses to the reference policy.
  - Quick check question: How does MBR decoding differ from Maximum a Posteriori (MAP) decoding, and why is it relevant for mitigating reward hacking?

- Concept: Proximity regularization
  - Why needed here: Proximity regularization is a common technique in preference learning to prevent the model from deviating too far from the reference model. MBR-BoN uses the MBR objective as a form of proximity regularization.
  - Quick check question: What is the purpose of proximity regularization in preference learning, and how does MBR-BoN implement it?

## Architecture Onboarding

- Component map:
  - Language Model (πref) -> Proxy Reward Model (R) -> MBR Objective -> MBR-BoN Algorithm (selects best response)

- Critical path:
  1. Sample N responses from the language model.
  2. Compute the proxy reward score for each response.
  3. Compute the MBR objective for each response.
  4. Select the response with the highest combined score.

- Design tradeoffs:
  - The choice of the utility function U in the MBR objective affects the quality of the proximity regularization.
  - The hyperparameter β controls the tradeoff between the reward score and the proximity to the reference policy. Tuning β requires a development set.
  - MBR-BoN is computationally more expensive than vanilla BoN due to the computation of the MBR objective.

- Failure signatures:
  - If the utility function U does not accurately reflect the true similarity between responses, MBR-BoN may not effectively mitigate reward hacking.
  - If the proxy reward model is highly correlated with the gold reference reward model, the benefit of MBR-BoN over vanilla BoN may be minimal.
  - If the development set for tuning β is too small or unrepresentative, the hyperparameter may not be optimally set.

- First 3 experiments:
  1. Evaluate MBR-BoN on a small dataset with a known proxy reward model to verify that it mitigates reward hacking compared to vanilla BoN.
  2. Tune the hyperparameter β on a development set and evaluate the performance of MBR-BoN with different values of β.
  3. Generate a pairwise preference dataset using MBR-BoN and train a model using DPO to assess the quality of the generated dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal regularization strength β for MBR-BoN across different proxy reward models and datasets?
- Basis in paper: [explicit] The authors state that "the optimal value of β depends on the choice of the language model, dataset, and proxy reward model" and they use a development set to tune β for each setting.
- Why unresolved: The paper only provides specific β values for the experiments conducted (e.g., AlpacaFarm with SHP-Large: β=0.5, OASST: β=20.0) but does not establish a general principle or method for determining optimal β across different scenarios.
- What evidence would resolve it: A systematic study varying β across multiple datasets, proxy reward models, and language models to identify patterns or develop a heuristic for selecting β without requiring extensive development set tuning.

### Open Question 2
- Question: How does MBR-BoN perform with pairwise reward models as proxy rewards?
- Basis in paper: [explicit] The authors note that "PairRM is a pairwise reward model that estimates the preference for a pair of responses rather than computing an absolute preference for a response" and state that "The use of a pairwise reward model as a proxy reward model for RBoN is future work."
- Why unresolved: The paper evaluates MBR-BoN with absolute reward models (SHP-Large, SHP-XL, OASST) but explicitly excludes the pairwise reward model PairRM from evaluation as a proxy.
- What evidence would resolve it: Implementing MBR-BoN with PairRM as the proxy reward model and evaluating its performance compared to vanilla BoN and other configurations.

### Open Question 3
- Question: Can MBR-BoN be extended to preference optimization algorithms beyond DPO?
- Basis in paper: [explicit] The authors state that "Our experiments on preference learning are limited to the evaluation of DPO" and note this as a limitation, suggesting evaluation of MBR-BoN for other preference optimization algorithms is future work.
- Why unresolved: While the paper demonstrates MBR-BoN's effectiveness for generating preference datasets for DPO, it does not explore its application to other algorithms like PPO, KTO, or ORPO.
- What evidence would resolve it: Implementing MBR-BoN with other preference optimization algorithms and comparing their performance to standard implementations.

## Limitations

- The optimal regularization strength β requires development set tuning for each specific proxy and gold reward model combination, making the method sensitive to hyperparameter selection.
- MBR-BoN introduces computational overhead compared to vanilla BoN due to the additional MBR objective computation for each sampled response.
- The paper's evaluation is limited to instruction following tasks, and it remains unclear whether the performance gains generalize to other language tasks or reward model architectures.

## Confidence

**High Confidence**: The core mechanism of using MBR as a proximity regularizer to mitigate reward hacking is well-supported by the empirical results.

**Medium Confidence**: The claim that MBR-BoN is particularly effective when proxy reward models are weakly correlated with gold rewards is supported by the data, but the relationship between correlation strength and performance improvement could be more thoroughly characterized.

**Low Confidence**: The assertion that MBR-BoN is an effective strategy for generating pairwise preference datasets for DPO training, while supported by initial results, lacks extensive validation.

## Next Checks

1. **Correlation Threshold Analysis**: Systematically vary the correlation strength between proxy and gold reward models across a wider range of values to identify the precise threshold at which MBR-BoN begins to outperform vanilla BoN.

2. **Computational Efficiency Evaluation**: Conduct a detailed analysis of the computational overhead introduced by MBR-BoN, including both wall-clock time and memory usage, and compare this against the performance gains across different sample sizes N.

3. **Cross-Domain Generalization Study**: Apply MBR-BoN to language tasks beyond instruction following (such as summarization, dialogue, or creative writing) with different types of reward models to assess whether the performance gains generalize across task domains and reward model architectures.