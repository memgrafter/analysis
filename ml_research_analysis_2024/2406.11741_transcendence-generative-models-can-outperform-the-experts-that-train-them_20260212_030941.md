---
ver: rpa2
title: 'Transcendence: Generative Models Can Outperform The Experts That Train Them'
arxiv_id: '2406.11741'
source_url: https://arxiv.org/abs/2406.11741
tags:
- transcendence
- temperature
- distribution
- reward
- chess
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of "transcendence," where generative
  models trained on human-generated data can outperform their expert sources. The
  authors theoretically prove that low-temperature sampling in autoregressive transformers
  enables this phenomenon by implicitly performing majority voting over diverse experts,
  thereby denoising individual errors.
---

# Transcendence: Generative Models Can Outperform The Experts That Train Them

## Quick Facts
- arXiv ID: 2406.11741
- Source URL: https://arxiv.org/abs/2406.11741
- Authors: Edwin Zhang; Vincent Zhu; Naomi Saphra; Anat Kleiman; Benjamin L. Edelman; Milind Tambe; Sham M. Kakade; Eran Malach
- Reference count: 40
- One-line primary result: Generative models trained on human-generated data can outperform their expert sources through low-temperature sampling that performs majority voting over diverse experts

## Executive Summary
This paper introduces the concept of "transcendence," where generative models trained on human-generated data can achieve performance exceeding any single expert in their training dataset. The authors theoretically prove that low-temperature sampling in autoregressive transformers enables this phenomenon by implicitly performing majority voting over diverse experts, thereby denoising individual errors. They validate their findings through chess-playing transformer models trained on human game transcripts, showing that these models achieve higher ratings than any single expert in the training dataset at low temperatures.

## Method Summary
The study trains autoregressive transformer decoders (50M parameters) on next-token prediction using cross-entropy loss across chess game transcripts filtered by maximum player ratings (1000, 1300, 1500) and natural language data (SQuAD 2.0). Models are evaluated using temperature-controlled sampling (τ=0.001, 0.75, 1.0, 1.5) and compared against Stockfish chess engines for chess tasks or standard QA metrics for language tasks. The theoretical framework connects low-temperature sampling to majority voting over experts, requiring dataset diversity for transcendence to occur.

## Key Results
- Chess models achieve ratings above 1450 at temperature τ=0.001, compared to maximum training rating of 1300
- Transcendence requires dataset diversity - ChessFormer 1500 fails to transcend due to lack of diversity in high-level play
- Low-temperature sampling (τ approaching 0) concentrates probability mass on consensus actions across experts
- Similar transcendence effects observed in natural language question-answering tasks using SQuADv2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-temperature sampling performs majority voting over diverse experts, denoising individual errors.
- Mechanism: The softmax with low temperature concentrates probability mass on the most probable outputs across all experts. When multiple experts have non-zero probability on the correct action, low temperature amplifies the consensus while suppressing noise from individual errors.
- Core assumption: The input distribution p has full support, and the test distribution ptest overlaps with multiple expert domains.
- Evidence anchors:
  - [abstract] "low-temperature sampling in autoregressive transformers enables this phenomenon by implicitly performing majority voting over diverse experts, thereby denoising individual errors"
  - [section 3.2] "we make the subtle connection here that low-temperature sampling can be thought of as performing majority vote between the experts"
  - [corpus] Weak - corpus neighbors discuss ensemble methods but not specifically temperature-based majority voting

### Mechanism 2
- Claim: Transcendence requires dataset diversity across expert domains.
- Mechanism: When experts specialize in different regions of input space, their combined knowledge covers more scenarios. The model can then leverage this broader coverage to outperform any single expert by averaging their strengths.
- Core assumption: The training data contains sufficient diversity such that no single expert dominates all input regions.
- Evidence anchors:
  - [section 4.2] "Chessformer 1500 likely is not transcendent due to a lack of diversity in its dataset"
  - [section 3.4] "dataset diversity as a necessary condition for practically effective majority voting"
  - [corpus] Weak - corpus neighbors discuss diversity in ensembles but not specifically for transcendence

### Mechanism 3
- Claim: The arg-max predictor must outperform the best expert for transcendence to be possible.
- Mechanism: Temperature sampling creates a continuous path from the learned distribution to the arg-max distribution. If the arg-max version of the model already outperforms experts, lowering temperature will eventually achieve transcendence.
- Core assumption: The model's learned distribution has sufficient mass on high-reward actions for the arg-max to be superior.
- Evidence anchors:
  - [section 3.2] "Proposition 2. Rptest( ˆfmax) > maxi∈[k] Rptest(fi) if and only if there exists some temperature τ ∈(0,1) s.t. for all 0 ≤ τ ′ ≤ τ, it holds that Rptest( ˆfτ ′) >maxi∈[k]Rptest(fi)"
  - [section 4.1] "Chessformer 1000 and Chessformer 1300 models are able to transcend to around 1500 rating at temperature τ equal to 0.001"
  - [corpus] Weak - corpus neighbors discuss distillation and transfer learning but not arg-max performance

## Foundational Learning

- Concept: Autoregressive modeling and cross-entropy loss
  - Why needed here: The paper's theoretical framework relies on understanding how generative models learn conditional probability distributions through minimizing cross-entropy loss
  - Quick check question: Why does minimizing cross-entropy loss cause the model to learn the average of expert distributions?

- Concept: Temperature sampling and softmax
  - Why needed here: Low-temperature sampling is the key mechanism enabling transcendence, requiring understanding of how temperature affects probability distributions
  - Quick check question: What happens to the softmax output as temperature approaches zero?

- Concept: Ensemble methods and majority voting
  - Why needed here: The paper connects transcendence to established ensemble learning theory, requiring understanding of how combining diverse experts can outperform individuals
  - Quick check question: Under what conditions does majority voting improve upon individual expert performance?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training (cross-entropy loss) -> Temperature-controlled inference -> Stockfish evaluation -> Rating calculation
- Critical path: Chess game transcripts → 50M parameter transformer training → Temperature sampling (τ=0.001) → Stockfish evaluation → Glicko-2 rating computation
- Design tradeoffs: Character-level vs move-level tokenization, blind chess play vs board state input, transformer size vs training time
- Failure signatures: Lack of transcendence indicates insufficient dataset diversity or arg-max model not outperforming individual experts; training instability may indicate learning rate or batch size issues
- First 3 experiments:
  1. Train ChessFormer 1000 at temperature 1.0 to verify baseline performance below expert maximum
  2. Train ChessFormer 1000 at temperature 0.001 to confirm transcendence occurs as predicted
  3. Train ChessFormer 1500 to verify that lack of diversity prevents transcendence, confirming the theoretical requirement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does transcendence occur in domains beyond chess and natural language processing, and what are the underlying mechanisms?
- Basis in paper: [inferred] The paper mentions the possibility of transcendence in other domains but does not explore it in depth.
- Why unresolved: The paper focuses primarily on chess and SQuADv2, leaving other domains unexplored.
- What evidence would resolve it: Experiments in domains like computer vision, robotics, or healthcare could demonstrate transcendence in those areas.

### Open Question 2
- Question: How does dataset diversity specifically contribute to transcendence, and can it be quantified or optimized?
- Basis in paper: [explicit] The paper highlights the necessity of dataset diversity for transcendence but does not provide a method to quantify or optimize it.
- Why unresolved: The paper discusses the role of diversity but does not offer a framework for measuring or enhancing it.
- What evidence would resolve it: A method to measure diversity (e.g., entropy-based metrics) and experiments showing how optimizing diversity affects transcendence would provide clarity.

### Open Question 3
- Question: Can transcendence be achieved without low-temperature sampling, and what alternative mechanisms might enable it?
- Basis in paper: [explicit] The paper focuses on low-temperature sampling as a key mechanism but acknowledges other potential sources of transcendence.
- Why unresolved: The paper does not explore alternative mechanisms in depth, such as Bayesian weighting or reinforcement learning objectives.
- What evidence would resolve it: Experiments comparing transcendence across different mechanisms (e.g., Bayesian methods, RL objectives) would clarify their effectiveness.

### Open Question 4
- Question: How does transcendence scale with model size, and are there diminishing returns or plateaus in performance?
- Basis in paper: [inferred] The paper uses models of varying sizes but does not analyze the relationship between size and transcendence.
- Why unresolved: The paper does not provide a systematic analysis of how model size impacts transcendence.
- What evidence would resolve it: Experiments with models of different sizes and architectures, measuring transcendence across a range of scales, would address this question.

## Limitations

- The chess transcendence results depend on the assumption that human players in the training data represent distinct "experts" with complementary strengths, which isn't rigorously validated.
- The theoretical framework assumes conditions about expert diversity and independence of errors that may not hold in more complex real-world scenarios.
- Low-temperature sampling relies on the model's learned distribution having appropriate concentration properties, creating uncertainty about whether transcendence persists across different architectures or sampling strategies.

## Confidence

- High Confidence: The theoretical connection between low-temperature sampling and majority voting over experts is well-established through mathematical proofs. The empirical demonstration of chess rating improvements at τ=0.001 is statistically significant with proper confidence intervals.
- Medium Confidence: The requirement for dataset diversity as a necessary condition for transcendence is supported by the ChessFormer 1500 negative result, but this could also reflect other factors like model capacity limitations.
- Low Confidence: The generalizability of transcendence to domains beyond structured games and question answering is speculative, with limited evidence for how it might manifest in domains with continuous action spaces.

## Next Checks

1. **Cross-Domain Validation**: Test transcendence on a completely different domain such as medical diagnosis or code generation, where experts have distinct but complementary knowledge bases, to verify the general applicability of the phenomenon.

2. **Error Correlation Analysis**: Systematically analyze the correlation structure of errors across different expert groups in the training data to quantify how independent versus correlated mistakes affect transcendence potential.

3. **Architecture Scaling Study**: Evaluate whether transcendence scales with model size by training larger transformer architectures (500M+ parameters) on the same chess datasets to determine if the effect is limited to smaller models or persists at scale.