---
ver: rpa2
title: 'LlamBERT: Large-scale low-cost data annotation in NLP'
arxiv_id: '2403.15938'
source_url: https://arxiv.org/abs/2403.15938
tags:
- data
- arxiv
- language
- llambert
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LlamBERT is a hybrid method that uses a large language model to
  annotate a small subset of unlabeled text, then fine-tunes a smaller transformer
  encoder (BERT/RoBERTa) on those labels to classify the rest. On IMDb sentiment,
  Llama-2-70b-chat labels trained RoBERTa-large to 96.68% accuracy, close to the 96.54%
  gold-standard baseline, and 1000x faster inference than the LLM.
---

# LlamBERT: Large-scale low-cost data annotation in NLP

## Quick Facts
- arXiv ID: 2403.15938
- Source URL: https://arxiv.org/abs/2403.15938
- Reference count: 32
- Primary result: LlamBERT achieves near-state-of-the-art accuracy at 1000x lower inference cost by using LLM annotations to fine-tune smaller transformers

## Executive Summary
LlamBERT is a hybrid approach that leverages large language models to annotate a small subset of unlabeled text, then fine-tunes a smaller transformer encoder (BERT/RoBERTa) on those labels to classify the rest. The method achieves near-state-of-the-art performance on IMDb sentiment classification (96.68% accuracy) and UMLS neuroanatomy classification (96.92% accuracy) while reducing inference costs by orders of magnitude compared to using LLMs directly.

## Method Summary
LlamBERT uses Llama-2-70b-chat to annotate a small subset of unlabeled data (10,000 examples for both IMDb and UMLS datasets), then fine-tunes a pre-trained BERT or RoBERTa model on these annotations. The fine-tuned model is then used to classify the remaining unlabeled data. The approach also explores combining LLM-annotated data with gold-standard training data for further accuracy improvements.

## Key Results
- On IMDb sentiment, Llama-2-70b-chat labels trained RoBERTa-large to 96.68% accuracy, close to the 96.54% gold-standard baseline
- On UMLS neuroanatomy, the approach reaches 96.92% accuracy with BiomedBERT-large, near the 97.08% baseline
- Inference on IMDb's 7.816 million reviews takes 48h 28m with fine-tuned RoBERTa-large vs. approximately 367 days using Llama-2-70b-chat directly

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated labels can effectively train a smaller transformer encoder to near-state-of-the-art performance.
- Mechanism: The LLM serves as a high-quality annotator for a small subset of unlabeled data. The labeled subset is then used to fine-tune a smaller transformer encoder, which can generalize to the full dataset with significantly lower inference costs.
- Core assumption: The LLM's annotation quality is sufficient to train a smaller model that approaches the LLM's performance.
- Evidence anchors:
  - [abstract] "Our results indicate that the LlamBERT approach slightly compromises on accuracy while offering much greater cost-effectiveness."
  - [section] "Llama-2-70b-chat labels trained RoBERTa-large to 96.68% accuracy, close to the 96.54% gold-standard baseline."

### Mechanism 2
- Claim: Fine-tuning a transformer encoder on LLM-annotated data is more efficient than using the LLM directly for inference on large datasets.
- Mechanism: The LLM's high inference cost is avoided by first annotating a small subset and then using a fine-tuned smaller model for the rest.
- Core assumption: The smaller model can achieve comparable accuracy to the LLM after fine-tuning on LLM-annotated data.
- Evidence anchors:
  - [section] "Inference on the test data with roberta-large took 9m 18s, after fine-tuning for 2h 33m. Thus, we can estimate that labeling the entirety of IMDb's 7.816 million movie reviews would take about 48h 28m with roberta-large. In contrast, the same task would require approximately 367 days on our setup using Llama-2-70b-chat."

### Mechanism 3
- Claim: Combining LLM-annotated data with gold-standard data further improves accuracy.
- Mechanism: The combined approach uses both the LLM-annotated subset and the gold-standard training data for fine-tuning, leveraging the strengths of both.
- Core assumption: Gold-standard data provides additional valuable information that complements the LLM-annotated data.
- Evidence anchors:
  - [abstract] "Combining the LlamBERT technique with fine-tuning on gold-standard data yielded the best results in both cases, achieving state-of-the-art accuracy on the IMDb benchmark."

## Foundational Learning

- Concept: Binary text classification
  - Why needed here: The IMDb and UMLS tasks are both binary classification problems, requiring understanding of sentiment (positive/negative) and relevance to the nervous system (yes/no).
  - Quick check question: Can you explain the difference between binary and multi-class classification in NLP?

- Concept: Fine-tuning transformer encoders
  - Why needed here: The core of LlamBERT is fine-tuning BERT/RoBERTa on LLM-annotated data to create a cost-effective classifier.
  - Quick check question: What are the key steps in fine-tuning a pre-trained transformer model for a new task?

- Concept: LLM prompting and few-shot learning
  - Why needed here: The LLM's performance depends on the quality of the prompt and the use of few-shot examples to guide its annotations.
  - Quick check question: How does the number of few-shot examples affect the performance of a large language model in a new task?

## Architecture Onboarding

- Component map: LLM (Llama-2-70b-chat) -> Subset annotation -> Transformer encoder (BERT/RoBERTa) fine-tuning -> Classification of remaining data

- Critical path:
  1. Select a small subset of unlabeled data
  2. Use LLM to annotate the subset with a natural language prompt
  3. Parse LLM responses into desired categories
  4. Fine-tune a transformer encoder on the labeled subset
  5. Apply the fine-tuned encoder to classify the rest of the unlabeled data

- Design tradeoffs:
  - LLM size vs. annotation quality: Larger LLMs provide higher annotation quality but at a higher cost
  - Subset size vs. fine-tuning accuracy: Larger subsets lead to better fine-tuned model accuracy but increase annotation cost
  - Fine-tuning data source: Using only LLM-annotated data is cheaper but may sacrifice some accuracy compared to combining with gold-standard data

- Failure signatures:
  - Low accuracy on the test set: Could indicate issues with LLM annotation quality, fine-tuning process, or model choice
  - High inference cost: May suggest the need for a smaller LLM or more efficient fine-tuning strategy
  - Slow training: Could be due to large subset size or inefficient fine-tuning implementation

- First 3 experiments:
  1. Fine-tune a small BERT model (e.g., distilbert-base) on a 1,000-example subset annotated by Llama-2-7b-chat
  2. Compare the performance of Llama-2-7b-chat and Llama-2-70b-chat on annotating a 100-example subset of the IMDb dataset
  3. Fine-tune roberta-large on a 10,000-example subset annotated by Llama-2-70b-chat, then evaluate on the full IMDb test set

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several remain unresolved:
- How does LlamBERT's performance scale when applied to multi-class classification tasks beyond binary sentiment and neuroanatomy?
- What is the optimal subset size for LLM annotation to balance accuracy and cost-efficiency?
- How robust is LlamBERT to noisy or biased LLM annotations?
- Can LlamBERT's cost-effectiveness be improved by combining it with active learning or semi-supervised methods?
- How does LlamBERT perform on low-resource languages or specialized domains with limited pre-trained models?

## Limitations

- Performance is highly dependent on LLM annotation quality, which may vary across domains
- The approach focuses on binary classification tasks, limiting generalizability to multi-class problems
- The trade-off between subset size and annotation cost is not fully explored, leaving questions about optimal subset selection strategies

## Confidence

- High Confidence: The core claim that LlamBERT achieves near-state-of-the-art accuracy at much lower cost is well-supported by empirical results on both IMDb and UMLS datasets.
- Medium Confidence: The claim that combining LLM-annotated data with gold-standard data further improves accuracy is supported by results, but the optimal ratio is not explored.
- Low Confidence: The paper does not address potential biases in LLM annotations or how they might propagate to the fine-tuned model.

## Next Checks

1. Test LlamBERT with different prompts for the same task to assess the impact of prompt quality on annotation accuracy and downstream model performance.
2. Apply LlamBERT to a multi-class classification task (e.g., topic classification) to evaluate its effectiveness beyond binary problems.
3. Analyze the LLM's annotations for potential biases (e.g., sentiment bias in IMDb) and assess whether these biases are amplified in the fine-tuned model.