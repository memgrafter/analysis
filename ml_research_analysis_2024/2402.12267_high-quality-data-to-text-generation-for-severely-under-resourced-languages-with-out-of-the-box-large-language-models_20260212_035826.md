---
ver: rpa2
title: High-quality Data-to-Text Generation for Severely Under-Resourced Languages
  with Out-of-the-box Large Language Models
arxiv_id: '2402.12267'
source_url: https://arxiv.org/abs/2402.12267
tags:
- english
- human
- languages
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that large language models (LLMs) can achieve
  state-of-the-art data-to-text generation performance for severely under-resourced
  languages (Irish, Welsh, Breton, Maltese) without any task-specific fine-tuning.
  By combining LLMs with machine translation, the authors produce outputs that match
  or exceed human performance according to human evaluations, though BLEU scores are
  significantly lower than for English.
---

# High-quality Data-to-Text Generation for Severely Under-Resourced Languages with Out-of-the-box Large Language Models

## Quick Facts
- arXiv ID: 2402.12267
- Source URL: https://arxiv.org/abs/2402.12267
- Authors: Michela Lorandi; Anya Belz
- Reference count: 23
- Key outcome: LLMs achieve state-of-the-art data-to-text generation for severely under-resourced languages without fine-tuning, matching or exceeding human performance in human evaluations

## Executive Summary
This paper demonstrates that large language models can produce state-of-the-art data-to-text generation results for severely under-resourced languages (Irish, Welsh, Breton, Maltese) without any task-specific fine-tuning. By leveraging machine translation as an intermediate step, the authors show that LLMs can generate outputs matching or exceeding human quality according to human evaluations, despite significantly lower BLEU scores compared to English. This work highlights both the potential of LLMs to bridge the performance gap for under-resourced languages and the limitations of automatic evaluation metrics like BLEU for non-task-specific systems.

## Method Summary
The authors employ a zero-shot approach using large language models for data-to-text generation in severely under-resourced languages. Their method involves translating source data into English, generating text using an English LLM, then translating the output back to the target language. This pipeline leverages existing high-quality translation systems and powerful LLMs without requiring any language-specific fine-tuning or additional training data for the target languages.

## Key Results
- LLMs without task-specific fine-tuning achieve state-of-the-art performance for severely under-resourced languages
- Human evaluations show generated text matching or exceeding human performance quality
- BLEU scores are significantly lower than English despite superior human evaluation results, demonstrating BLEU's limitations for this task

## Why This Works (Mechanism)
The approach works by exploiting the strong generative capabilities of large language models trained on English data, combined with the availability of high-quality translation systems. By translating to and from English, the method leverages the LLM's ability to understand structured data and generate coherent text, while machine translation handles the language-specific aspects. This circumvents the need for extensive training data in under-resourced languages while still producing high-quality outputs.

## Foundational Learning

1. **Data-to-text generation** - The task of automatically generating natural language descriptions from structured data or meaning representations
   - Why needed: Core task being evaluated
   - Quick check: Can you explain the difference between data-to-text and text-to-text tasks?

2. **Zero-shot learning** - Performing tasks without any task-specific training or fine-tuning
   - Why needed: The paper's main contribution is achieving results without fine-tuning
   - Quick check: What distinguishes zero-shot from few-shot learning approaches?

3. **BLEU score limitations** - Automatic metrics may not align with human quality judgments for generative tasks
   - Why needed: Critical insight about evaluation methodology
   - Quick check: Can you name other evaluation metrics that might be more appropriate?

## Architecture Onboarding

Component map: Structured data -> Machine Translation (X2EN) -> LLM (English) -> Machine Translation (EN2X) -> Generated text

Critical path: The translation-generation-translation pipeline is the essential sequence, where each step must succeed for quality output.

Design tradeoffs: The approach trades potential translation errors for the benefit of leveraging powerful English LLMs without fine-tuning requirements.

Failure signatures: Poor outputs typically stem from translation errors propagating through the pipeline or LLM misinterpretation of translated structured data.

First experiments:
1. Verify translation quality in both directions for each target language
2. Test LLM generation with simple English data structures before full pipeline
3. Compare human evaluation results with multiple automatic metrics

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on machine translation introduces potential error propagation
- Evaluation methodology's comprehensiveness and potential biases need scrutiny
- Focus on a small set of under-resourced languages limits generalizability

## Confidence

High confidence in the finding that human evaluations yield superior results compared to automatic metrics for this task

Medium confidence in the generalizability of results across different under-resourced languages

Medium confidence in the claim that LLMs without fine-tuning can match or exceed human performance

Low confidence in the scalability of the approach to more complex data-to-text generation tasks

## Next Checks

1. Replicate the study with additional under-resourced languages and more diverse data-to-text tasks to assess generalizability

2. Conduct ablation studies removing the machine translation step to quantify error propagation effects

3. Compare results against other automatic evaluation metrics (METEOR, ROUGE, BERTScore) to better understand the gap between human and automatic evaluations