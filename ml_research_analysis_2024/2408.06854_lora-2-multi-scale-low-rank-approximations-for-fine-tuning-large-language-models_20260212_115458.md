---
ver: rpa2
title: 'LoRA$^2$ : Multi-Scale Low-Rank Approximations for Fine-Tuning Large Language
  Models'
arxiv_id: '2408.06854'
source_url: https://arxiv.org/abs/2408.06854
tags:
- lora
- lora2
- parameters
- matrix
- orthogonal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LoRA2, a multi-scale low-rank approximation
  method for fine-tuning large language models. The key idea is to train two sets
  of low-rank matrices (LoRAs) on mutually orthogonal planes using orthogonal projection
  theory, with dual regularization to minimize overlap and enhance learning space.
---

# LoRA$^2$ : Multi-Scale Low-Rank Approximations for Fine-Tuning Large Language Models

## Quick Facts
- **arXiv ID**: 2408.06854
- **Source URL**: https://arxiv.org/abs/2408.06854
- **Reference count**: 20
- **Primary result**: Achieves 0.72% parameter efficiency on GLUE benchmark with performance comparable to full fine-tuning

## Executive Summary
LoRA$^2$ introduces a multi-scale low-rank approximation method for fine-tuning large language models that trains two sets of low-rank matrices on mutually orthogonal planes. The method combines orthogonal projection theory with dual regularization to minimize overlap and enhance learning space, while improving the importance score algorithm to reduce parameter sensitivity calculations by approximately 98.5%. Experimental results on GLUE benchmark show significant parameter reduction (0.72%) while maintaining high performance, outperforming existing parameter-efficient fine-tuning methods.

## Method Summary
LoRA$^2$ is a parameter-efficient fine-tuning method that decomposes weight updates into two sets of low-rank matrices trained on orthogonal planes. The approach uses Singular Value Decomposition (SVD) to project parameter increment matrices onto mutually orthogonal planes, with dual regularization enforcing orthogonality between the two LoRA sets. An improved importance scoring mechanism prunes singular values based on sensitivity and uncertainty metrics, reducing computational overhead by approximately 98.5% while maintaining pruning effectiveness.

## Key Results
- Achieves 0.72% parameter efficiency compared to full fine-tuning on GLUE benchmark
- With further parameter reduction to 0.17M, achieves comparable results to baseline with 8× more parameters
- Consistently outperforms parameter-efficient baselines on downstream tasks with average performance lead of over 1%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Orthogonal projection of LoRA matrices onto two mutually orthogonal planes increases the effective learning space compared to single-scale LoRA.
- **Mechanism**: Decomposing the weight update matrix into two sets of low-rank matrices (uv and UV) trained on orthogonal planes avoids redundancy and allows each set to capture complementary information. Dual regularization enforces orthogonality between uv and UV, minimizing overlap and maximizing representational capacity.
- **Core assumption**: The incremental update matrix can be effectively decomposed into two orthogonal components without loss of expressiveness, and orthogonal components capture more diverse information than overlapping ones.
- **Evidence anchors**: [abstract] "We first combine orthogonal projection theory to train a set of LoRAs in two mutually orthogonal planes." [section] "We utilize Singular Value Decomposition (SVD) to project the parameter increment matrix ∆ onto a mutually orthogonal plane." [corpus] Weak - the cited papers focus on multi-scale LoRA but don't explicitly discuss orthogonal planes.
- **Break condition**: If the orthogonal constraint is too strict, it may limit the expressiveness of the model and prevent it from fitting complex patterns in the data.

### Mechanism 2
- **Claim**: Pruning singular values based on an importance score that accounts for both parameter sensitivity and uncertainty leads to better parameter efficiency without sacrificing performance.
- **Mechanism**: The importance score combines sensitivity (gradient magnitude) and uncertainty (local variation) to rank singular values. By pruning less important singular values, the method reduces the number of trainable parameters while preserving those that contribute most to performance. The modification that excludes column matrix calculations reduces computation by 98.5% without affecting pruning decisions.
- **Core assumption**: Sensitivity and uncertainty are good proxies for the importance of singular values, and the exclusion of column matrix calculations is valid because their sensitivity scores are already incorporated through the row matrices.
- **Evidence anchors**: [abstract] "Then, we improve the importance score algorithm, which reduce parameter sensitivity score calculations by approximately 98.5%." [section] "Since the importance score of each singular value Λ includes the average of the sensitivity scores of all parameters in matrices u and U, the ranking is unaffected. Thus, in practical terms, we can disregard the sensitivities of matrices u and U." [corpus] Weak - AdaLoRA uses a similar importance scoring mechanism but doesn't discuss the 98.5% reduction or the exclusion of column matrices.
- **Break condition**: If the importance scoring metric fails to capture true parameter importance, pruning may remove critical parameters and degrade performance.

### Mechanism 3
- **Claim**: Training LoRA matrices with Gaussian initialization ensures that the initial parameter increment is zero, preventing disruption to the pre-trained model's behavior.
- **Mechanism**: Initializing the low-rank matrices (A and B in standard LoRA, or uv, UV, and Λ in LoRA2) with Gaussian noise such that their product is zero ensures the fine-tuned model starts with the same behavior as the pre-trained model. This allows for gradual adaptation without catastrophic forgetting.
- **Core assumption**: A zero-initialized parameter increment allows the model to start from the pre-trained state and adapt gradually without disrupting existing knowledge.
- **Evidence anchors**: [section] "The matrix Λ is initialized with zero while u, v, U and V adopt a random Gaussian initialization to ensure ∆ = 0 at the beginning of training." [section] "LoRA avoids forward propagation latency caused by inserting additional neural modules while demonstrating stable performance." [corpus] Weak - this is a standard initialization technique in LoRA literature but not explicitly discussed in the cited papers.
- **Break condition**: If the initialization is not truly zero or if the Gaussian noise is too large, it may introduce unwanted perturbations to the pre-trained model.

## Foundational Learning

- **Concept**: Low-Rank Matrix Approximation
  - **Why needed here**: LoRA$^2$ is built on the principle that weight updates can be approximated by low-rank matrices, which significantly reduces the number of trainable parameters.
  - **Quick check question**: Why does approximating weight updates with low-rank matrices reduce the number of trainable parameters?

- **Concept**: Orthogonal Projection
  - **Why needed here**: The method uses orthogonal projection theory to decompose the weight update matrix into two mutually orthogonal components, which increases the effective learning space.
  - **Quick check question**: How does projecting onto mutually orthogonal planes help increase the model's learning capacity?

- **Concept**: Singular Value Decomposition (SVD)
  - **Why needed here**: SVD is used to decompose the weight update matrix and identify important singular values for pruning.
  - **Quick check question**: What role does SVD play in identifying which parameters to prune in LoRA$^2$?

## Architecture Onboarding

- **Component map**: Pre-trained model weights (frozen) -> Two sets of low-rank matrices (uv and UV) trained on orthogonal planes -> Diagonal matrix Λ containing singular values -> Dual regularization terms enforcing orthogonality -> Importance scoring mechanism for pruning

- **Critical path**:
  1. Initialize matrices with Gaussian noise ensuring ∆ = 0
  2. Compute gradients and importance scores
  3. Apply dual regularization to enforce orthogonality
  4. Prune singular values based on importance scores
  5. Update the low-rank matrices

- **Design tradeoffs**:
  - Orthogonality vs. expressiveness: Stricter orthogonality constraints may limit the model's ability to fit complex patterns
  - Parameter efficiency vs. performance: Aggressive pruning may reduce performance but improve efficiency
  - Computation cost vs. accuracy: The modified importance scoring reduces computation but relies on assumptions about parameter importance

- **Failure signatures**:
  - Performance degradation after pruning indicates important parameters were removed
  - Slow convergence or unstable training may indicate overly strict orthogonality constraints
  - Memory issues may occur if the rank is set too high or if pruning is insufficient

- **First 3 experiments**:
  1. Verify that with k=1 and r=1, the model initializes with zero parameter increment and produces identical outputs to the pre-trained model
  2. Test the orthogonality constraint by measuring the dot product between uv and UV matrices during training
  3. Validate the importance scoring by comparing pruned vs. unpruned models on a small dataset to confirm that pruning doesn't degrade performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal number of low-rank matrices to train in mutually orthogonal planes for different types of downstream tasks?
- **Basis in paper**: [inferred] The paper discusses training two sets of LoRAs on mutually orthogonal planes, but does not explore the impact of varying the number of LoRAs.
- **Why unresolved**: The paper focuses on the case of two LoRAs, and does not investigate how the number of LoRAs might affect performance across different task types or complexities.
- **What evidence would resolve it**: Experiments comparing performance across a range of tasks with varying numbers of LoRAs (e.g., 2, 3, 4, 5) to determine the optimal configuration for different task characteristics.

### Open Question 2
- **Question**: How does the choice of hyperparameter k (the dimension to which data is projected) affect the performance and efficiency of LoRA$^2$ across different model sizes and tasks?
- **Basis in paper**: [explicit] The paper mentions k as a hyperparameter but does not provide a detailed analysis of its impact on performance or how it should be tuned for different scenarios.
- **Why unresolved**: While the paper uses k in the formulation, it does not explore the sensitivity of the model to this parameter or provide guidelines for its selection.
- **What evidence would resolve it**: A comprehensive study varying k across a range of values for different model sizes and task types, measuring performance and computational efficiency to establish best practices for k selection.

### Open Question 3
- **Question**: Can the importance score algorithm be further optimized to reduce computational overhead while maintaining or improving performance?
- **Basis in paper**: [explicit] The paper mentions reducing parameter sensitivity score calculations by approximately 98.5%, but does not explore further optimization possibilities.
- **Why unresolved**: The current importance score algorithm is presented as an improvement, but there may be additional optimizations that could be explored to further reduce computational cost or improve accuracy.
- **What evidence would resolve it**: Development and testing of alternative importance score algorithms, comparing their computational efficiency and performance against the current method to identify potential improvements.

## Limitations
- The orthogonal projection mechanism lacks direct empirical validation showing that orthogonality specifically improves performance over standard multi-scale approaches
- The 98.5% reduction in parameter sensitivity calculations is asserted but validation focuses on pruning decisions rather than confirming actual computational savings
- Claims about "mutually orthogonal planes" and "dual regularization" are supported by theoretical formulations but lack ablation studies isolating these components' contributions

## Confidence
- **High confidence**: The general framework of using low-rank approximations for parameter-efficient fine-tuning, the performance improvements on GLUE benchmark, and the parameter reduction claims are well-supported by experimental results.
- **Medium confidence**: The orthogonal projection mechanism's contribution to performance gains and the 98.5% computational reduction are supported by theoretical reasoning but would benefit from more direct empirical validation.
- **Low confidence**: The specific claims about "mutually orthogonal planes" and the exact mechanisms by which orthogonality enhances learning space are novel contributions that lack direct comparative evidence against non-orthogonal multi-scale approaches.

## Next Checks
1. **Orthogonality validation**: Measure the dot products between trained uv and UV matrices across different runs to empirically verify that they remain orthogonal throughout training, and compare performance against a version where orthogonality is not enforced.
2. **Computational savings verification**: Implement both the full importance scoring algorithm and the optimized version, then measure actual computation time and parameter sensitivity calculations to verify the 98.5% reduction claim.
3. **Component ablation study**: Create controlled experiments isolating the three main contributions (orthogonal projection, dual regularization, importance scoring optimization) to determine which components drive the performance improvements versus parameter efficiency gains.