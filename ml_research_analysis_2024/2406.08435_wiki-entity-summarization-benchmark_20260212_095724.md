---
ver: rpa2
title: Wiki Entity Summarization Benchmark
arxiv_id: '2406.08435'
source_url: https://arxiv.org/abs/2406.08435
tags:
- entity
- dataset
- entities
- datasets
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce WIKES, a large-scale benchmark for entity
  summarization in knowledge graphs that addresses limitations of existing small,
  biased datasets. WIKES uses Wikipedia abstracts and Wikidata to automatically generate
  summaries without manual annotation, preserving graph structure through a random
  walk sampling method that captures second-hop neighborhoods.
---

# Wiki Entity Summarization Benchmark

## Quick Facts
- arXiv ID: 2406.08435
- Source URL: https://arxiv.org/abs/2406.08435
- Reference count: 40
- Authors introduce WIKES, a large-scale benchmark for entity summarization that addresses size and bias limitations of existing datasets

## Executive Summary
This paper introduces WIKES, a large-scale benchmark for entity summarization in knowledge graphs that addresses critical limitations of existing small, biased datasets. The benchmark leverages Wikipedia abstracts and Wikidata to automatically generate summaries without manual annotation, preserving graph structure through a random walk sampling method. WIKES includes a scalable dataset generator producing diverse datasets across domains, with empirical evaluations showing minimal frequency bias compared to existing benchmarks and superior performance using LinkSum over traditional methods.

## Method Summary
WIKES generates entity summarization benchmarks by extracting subgraphs from Wikidata and annotating them with summaries from Wikipedia abstracts. The method uses random walk sampling with log-transformed, degree-proportional walk counts to preserve KG topology while capturing second-hop neighborhoods. DistilBERT embeddings compute cosine similarity between abstracts and relation names to identify the most relevant summary relations. The approach automatically scales to produce multiple dataset sizes across different domains, with built-in evaluation metrics including F1-score and Mean Average Precision.

## Key Results
- WIKES datasets show minimal frequency bias compared to existing benchmarks like ESBM
- LinkSum outperforms PageRank and RELIN in entity summarization tasks on WIKES
- The benchmark successfully preserves KG structure through second-hop neighborhood expansion
- Automatic summary generation via Wikipedia abstracts eliminates manual annotation bias while maintaining scalability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automatic summary generation via Wikipedia abstracts eliminates manual annotation bias.
- Mechanism: Abstracts are created and reviewed by multiple contributors, reducing individual bias and providing contextually rich summaries without costly human labeling.
- Core assumption: Wikipedia abstracts are sufficiently representative and unbiased for entity summaries.
- Evidence anchors:
  - [abstract] "We use the Wikipedia abstract as a summary instead of manual annotators. Each abstract is a collaboration of many users; as such, it should not introduce obvious biases."
  - [section] "Leveraging Wikipedia, we avoid time-consuming manual annotation and enable the automatic generation of large-scale datasets."
- Break condition: If abstracts contain significant bias or are too generic, summaries may not capture salient entity features.

### Mechanism 2
- Claim: Random walk sampling with degree-proportional walk counts preserves KG topology.
- Mechanism: Assigning random walks to nodes based on log-transformed, normalized degree ensures proportional sampling of high- and low-degree nodes, maintaining graph structure.
- Core assumption: Degree-based walk allocation reflects true graph connectivity and avoids over-sampling hubs.
- Evidence anchors:
  - [section] "We redefine the number of random walks assigned to nodes based on their degrees, ensuring the distribution remains proportional to real data."
  - [section] "The logarithmic transformation is applied to reduce the impact of high-degree nodes and also low-degree nodes, making it more manageable for the random walk."
- Break condition: If log-normalization still over-represents hubs, sampled graph may skew toward popular entities.

### Mechanism 3
- Claim: Two-hop neighborhood expansion retains meaningful entity connections for summarization.
- Mechanism: Sampling up to second-hop neighbors ensures relationships between first-hop neighbors are captured, preserving local graph structure needed for summary relevance.
- Core assumption: Second-hop context is necessary to distinguish salient vs. incidental triples.
- Evidence anchors:
  - [section] "Our approach can be used to extract further subgraphs at the scale needed for benchmarking in a given scenario."
  - [section] "We observe a similar behavior with MAP... adding the two-hop neighborhood makes the sample follow the graph distribution."
- Break condition: If second-hop expansion introduces too many irrelevant entities, summaries become noisy.

## Foundational Learning

- Concept: Random walk Markov property
  - Why needed here: Underpins the sampling strategy that preserves graph topology without exhaustive enumeration.
  - Quick check question: What ensures a random walk doesn't revisit the same nodes excessively?

- Concept: Cosine similarity in embedding space
  - Why needed here: Used to select the most semantically relevant Wikidata property to annotate summaries from Wikipedia links.
  - Quick check question: How does cosine similarity between abstract and relation embeddings determine the summary relation?

- Concept: Graph density and connectivity metrics
  - Why needed here: Essential to compare benchmark quality and avoid sampling bias in evaluation.
  - Quick check question: Why is preserving connected components critical for downstream KG tasks?

## Architecture Onboarding

- Component map: Wikipedia parser -> Wikidata graph builder -> DistilBERT embedder -> Random walk sampler -> Graph connector -> Summary annotator
- Critical path: Extract -> Embed -> Sample -> Connect -> Annotate
- Design tradeoffs:
  - Manual annotation vs. Wikipedia abstracts: scale vs. control
  - Log-degree normalization vs. raw degree: balanced sampling vs. simplicity
  - Two-hop expansion vs. single-hop: richer context vs. computational cost
- Failure signatures:
  - Disconnected components after sampling
  - High-frequency bias in resulting summaries
  - Degraded F1/MAP due to irrelevant second-hop entities
- First 3 experiments:
  1. Generate small dataset with default parameters; verify connectivity and degree distribution.
  2. Replace Wikipedia abstracts with manually curated ones; compare F1/MAP to assess bias.
  3. Reduce random walk depth to 1-hop; measure impact on summary quality metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of entity summarization models scale with increasing dataset size in WIKES?
- Basis in paper: [explicit] The authors evaluate models on small, medium, and large versions of WIKES datasets, but do not explicitly analyze scaling trends.
- Why unresolved: The paper presents performance metrics for different dataset sizes but does not investigate how model performance changes as the dataset grows, which is crucial for understanding scalability.
- What evidence would resolve it: A systematic analysis of model performance (F1-score, MAP) across all three dataset sizes (small, medium, large) to identify trends in scalability and potential bottlenecks.

### Open Question 2
- Question: Can the WIKES benchmark be extended to other knowledge graphs beyond Wikidata and Wikipedia?
- Basis in paper: [inferred] The authors mention that WIKES is "generalizable to multiple domains" but do not explore its applicability to other KGs.
- Why unresolved: The paper focuses on Wikidata and Wikipedia as data sources but does not investigate whether the WIKES approach can be adapted to other KGs with different structures and properties.
- What evidence would resolve it: An empirical study applying the WIKES methodology to other KGs (e.g., DBpedia, YAGO) and comparing the resulting benchmarks in terms of quality and bias.

### Open Question 3
- Question: How does the random walk sampling method in WIKES compare to other sampling techniques in terms of preserving KG structure and reducing bias?
- Basis in paper: [explicit] The authors describe their random walk sampling method but do not compare it to alternative sampling approaches.
- Why unresolved: The paper introduces the random walk method as a way to capture KG structure but does not provide a comparative analysis with other sampling techniques (e.g., random node sampling, snowball sampling) in terms of their ability to preserve structure and minimize bias.
- What evidence would resolve it: A controlled experiment comparing the random walk method to other sampling techniques using metrics like graph density, degree distribution, and frequency bias on the same source KG.

## Limitations
- Automatic summary generation relies on Wikipedia abstracts, which may not capture domain-specific nuances or entity relationships beyond surface text
- Benchmark effectiveness is tied to Wikidata quality and completeness, which may have coverage gaps for certain entity types
- Random walk sampling may still over-represent popular entities despite log-degree normalization

## Confidence
- High confidence: Benchmark generation methodology and dataset creation pipeline are clearly specified and reproducible
- Medium confidence: Wikipedia abstracts provide unbiased summaries is reasonable but untested against manually curated alternatives
- Low confidence: Second-hop expansion consistently improves summary quality across all domains lacks systematic validation

## Next Checks
1. Generate a small dataset using WIKES toolkit and manually verify that the degree distribution and connected components match expected values before running any summarization algorithms
2. Compare summary quality metrics (F1, MAP) between WIKES datasets using Wikipedia abstracts versus manually curated summaries for a subset of entities to quantify annotation bias
3. Run LinkSum with varying random walk depths (1-hop, 2-hop, 3-hop) on the same dataset and measure the impact on summary quality to validate the necessity of second-hop expansion