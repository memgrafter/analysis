---
ver: rpa2
title: Leveraging Large Vision-Language Model as User Intent-aware Encoder for Composed
  Image Retrieval
arxiv_id: '2412.11087'
source_url: https://arxiv.org/abs/2412.11087
tags:
- image
- prompt
- task
- caption
- intent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of Composed Image Retrieval (CIR),
  where a hybrid-modality query consisting of a reference image and a relative caption
  is used to retrieve target images. The core method idea is to leverage a large vision-language
  model (LVLM) as a user intent-aware encoder, combined with a novel hybrid intent
  instruction module that provides explicit intent guidance at both task and instance
  levels.
---

# Leveraging Large Vision-Language Model as User Intent-aware Encoder for Composed Image Retrieval

## Quick Facts
- arXiv ID: 2412.11087
- Source URL: https://arxiv.org/abs/2412.11087
- Reference count: 14
- One-line primary result: State-of-the-art performance on three CIR benchmarks using LVLM as user intent-aware encoder with hybrid intent instruction module

## Executive Summary
This paper addresses Composed Image Retrieval (CIR) by proposing CIR-LVLM, which leverages a Large Vision-Language Model (LVLM) as a user intent-aware encoder. The key innovation is a hybrid intent instruction module that provides explicit guidance at both task and instance levels through carefully designed prompts. The method achieves state-of-the-art performance across three prominent benchmarks, demonstrating superior accuracy in capturing user intent while maintaining acceptable inference efficiency through a single-pass encoding approach.

## Method Summary
The CIR-LVLM framework processes hybrid-modality queries consisting of reference images and relative captions by first extracting visual features through a frozen vision encoder, then transforming these features into sentence-level prompts via a Connector module with learnable query embeddings. These prompts, combined with task-level instructions and instance-specific soft prompts from a learnable pool, are fed into an LVLM fine-tuned for retrieval. The model uses weighted mean pooling to aggregate LLM outputs and contrastive loss for training, enabling it to understand complex user modifications while preserving relevant visual details.

## Key Results
- Achieves state-of-the-art performance on Fashion-IQ, Shoes, and CIRR datasets
- LVLM-based encoder outperforms traditional VLPM approaches in capturing user intent
- Hybrid intent instruction module significantly improves retrieval accuracy
- Single-pass encoding approach maintains acceptable inference efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LVLM as encoder provides superior user intent understanding compared to traditional VLPMs
- Mechanism: Advanced reasoning and instruction-following capabilities allow comprehension of complex user modifications while preserving relevant visual details
- Core assumption: LVLM's pre-training on diverse multimodal data and instruction tuning enables better understanding of complex instructions
- Evidence anchors:
  - [abstract]: "explore the advanced reasoning and instruction-following capabilities of LVLM"
  - [section]: "LLM is more advantageous in discerning user intent and extracting the desired information"
  - [corpus]: Weak - No direct corpus evidence comparing LVLM vs VLPM performance in CIR tasks

### Mechanism 2
- Claim: Hybrid intent instruction module with task and instance-specific prompts significantly improves understanding
- Mechanism: Task prompt provides explicit task-level guidance while instance-specific soft prompt allows focus on subtle intent differences
- Core assumption: Explicit instructions help understand complex task requirements, instance-specific guidance is more effective than universal prompts
- Evidence anchors:
  - [abstract]: "design a novel hybrid intent instruction module to provide explicit intent guidance at two levels"
  - [section]: "instance-specific soft prompt refines the LLM's focus toward task-specific nuances"
  - [corpus]: Weak - Limited evidence on instance-specific soft prompt effectiveness

### Mechanism 3
- Claim: Connector module with learnable query embeddings effectively captures comprehensive visual information
- Mechanism: Transforms reference image features into sentence-level prompts enabling LLM to perceive comprehensive visual information
- Core assumption: Mapping visual features to sentence-level prompts allows more effective processing than direct cross-attention
- Evidence anchors:
  - [abstract]: "deploy the Connector containing a set of learnable query embeddings to adaptively capture the desired visual content"
  - [section]: "generate a sentence-level prompt that enables the model to perceive comprehensive information"
  - [corpus]: Weak - No direct corpus evidence comparing Connector-based approaches

## Foundational Learning

- Concept: Cross-modal feature fusion strategies in multimodal retrieval
  - Why needed here: Understanding different fusion approaches is crucial for appreciating the innovation of using LVLM as encoder and Connector module
  - Quick check question: What are the key limitations of early-fusion approaches in composed image retrieval that the LVLM-based approach aims to address?

- Concept: Prompt engineering and instruction tuning for large language models
  - Why needed here: The effectiveness of hybrid intent instruction module depends on understanding how to craft effective prompts for LLMs
  - Quick check question: How do task-level and instance-level prompts differ in their purpose and implementation in the CIR-LVLM framework?

- Concept: Contrastive learning and representation learning for retrieval
  - Why needed here: The training objective uses contrastive loss to align query and target image representations
  - Quick check question: How does the contrastive loss function ensure that the model learns to distinguish between relevant and irrelevant image pairs?

## Architecture Onboarding

- Component map:
  Reference image -> Vision Encoder (frozen) -> Connector -> Sentence-level prompt
  Reference image + Relative caption + Task prompt + Instance-specific soft prompt -> LVLM
  Target image -> Vision Encoder (frozen) -> Connector -> Sentence-level prompt
  Target image + Task prompt + Instance-specific soft prompt -> LVLM
  Pooling layer aggregates LLM outputs
  Contrastive loss aligns query and target representations

- Critical path:
  1. Reference image → Vision Encoder → Connector → Sentence-level prompt
  2. Reference image + Relative caption + Task prompt + Instance-specific soft prompt → LVLM
  3. Target image → Vision Encoder → Connector → Sentence-level prompt
  4. Target image + Task prompt + Instance-specific soft prompt → LVLM
  5. Pooling layer aggregates LLM outputs
  6. Contrastive loss aligns query and target representations

- Design tradeoffs:
  - LVLM vs VLPM: Better reasoning capabilities but potentially higher computational cost
  - Instance-specific vs universal prompts: More accurate guidance but increased complexity
  - Learnable Connector vs direct feature fusion: Better visual information preservation but additional parameters
  - Single-pass encoding vs generative retrieval: Better efficiency but requires careful prompt design

- Failure signatures:
  - Poor performance on short relative captions: Model over-relies on reference image information
  - Inconsistent performance across benchmarks: Model fails to generalize beyond training distribution
  - High computational cost: Inefficient implementation of LVLM inference
  - Hallucinations in retrieved images: LVLM generates incorrect associations between query and target

- First 3 experiments:
  1. Ablation study comparing LVLM vs text encoder from CLIP/BLIP-2 to verify the importance of advanced reasoning capabilities
  2. Test different prompt lengths and prompt pool sizes to find optimal configuration
  3. Evaluate performance with different pooling strategies (last token, regular mean, weighted mean) to verify the effectiveness of weighted mean pooling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal prompt pool size (M) and soft prompt length (Lp × K) for maximizing retrieval performance across different CIR datasets?
- Basis in paper: [explicit] The paper experimentally varies these parameters and finds optimal values of M=45 and Lp × K=40 for Fashion-IQ, but acknowledges this may vary by dataset
- Why unresolved: The optimal values were determined empirically for specific datasets; different datasets with varying characteristics may require different configurations
- What evidence would resolve it: Systematic experiments varying M and Lp × K across multiple diverse CIR datasets, establishing general principles for selecting these hyperparameters

### Open Question 2
- Question: How do different LVLM architectures (e.g., LLaVA vs Qwen-VL-Chat) compare in terms of user intent-aware encoding capabilities for CIR tasks?
- Basis in paper: [explicit] The paper compares LLaVA-1.6 and Qwen-VL-Chat, finding performance differences, but acknowledges this is preliminary
- Why unresolved: The comparison was limited to two models with different parameter sizes and training approaches; a comprehensive evaluation across multiple LVLM architectures is needed
- What evidence would resolve it: Controlled experiments training multiple LVLMs (with similar parameters) on the same CIR tasks, measuring retrieval accuracy, inference efficiency, and robustness

### Open Question 3
- Question: Can LVLM-based CIR models effectively handle relative captions that describe abstract concepts or complex relationships between objects?
- Basis in paper: [inferred] The paper demonstrates success with concrete modifications but does not explicitly test abstract concepts
- Why unresolved: The evaluation datasets primarily contain concrete, visual modifications; the model's ability to reason about abstract relationships or concepts is untested
- What evidence would resolve it: Creating and evaluating on CIR datasets containing abstract concepts and measuring retrieval success

## Limitations
- Limited direct empirical evidence comparing LVLM vs traditional VLPM performance
- Effectiveness of instance-specific soft prompt mechanism not fully validated
- Computational efficiency claims lack detailed analysis of inference costs

## Confidence

**High Confidence**: The core architecture design (Connector module + LVLM encoder + hybrid prompts) is technically sound and well-motivated. The paper demonstrates clear improvements over existing methods on multiple benchmarks.

**Medium Confidence**: The claim that LVLM's instruction-following capabilities are the primary driver of performance improvements. Empirical evidence relies on comparisons to other methods rather than controlled ablation studies.

**Low Confidence**: The scalability and generalization claims. The paper tests on three benchmarks but doesn't explore performance degradation with longer relative captions or more complex modification instructions.

## Next Checks

1. **Ablation Study with VLPM Baseline**: Replace the LVLM encoder with a standard text encoder from CLIP/BLIP-2 while keeping all other components identical to isolate the LVLM's contribution to performance gains.

2. **Soft Prompt Analysis**: Conduct systematic study varying the soft prompt pool size from 1 to 50+ prompts, measuring both performance and computational overhead. Visualize learned soft prompt representations to verify they capture meaningful task-specific patterns.

3. **Computational Efficiency Benchmark**: Measure end-to-end inference time and memory usage for CIR-LVLM compared to baseline methods across different hardware configurations. Include detailed breakdowns of encoding time and analyze performance scaling with batch size.