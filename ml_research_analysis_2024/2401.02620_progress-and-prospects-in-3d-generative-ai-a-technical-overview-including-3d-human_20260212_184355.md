---
ver: rpa2
title: 'Progress and Prospects in 3D Generative AI: A Technical Overview including
  3D human'
arxiv_id: '2401.02620'
source_url: https://arxiv.org/abs/2401.02620
tags:
- generation
- human
- diffusion
- methods
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive technical overview of 3D generative
  AI progress in 2023, covering 3D object generation, 3D human models, and 3D human
  motion synthesis. The field has rapidly advanced due to improvements in 2D diffusion
  models, multi-view consistency techniques, realistic human body representations
  like SMPL-X, and efficient neural rendering methods like NeRF and 3D Gaussian Splatting.
---

# Progress and Prospects in 3D Generative AI: A Technical Overview including 3D human

## Quick Facts
- arXiv ID: 2401.02620
- Source URL: https://arxiv.org/abs/2401.02620
- Authors: Song Bai; Jie Li
- Reference count: 40
- Key outcome: Comprehensive technical overview of 3D generative AI progress in 2023, covering 3D object generation, 3D human models, and 3D human motion synthesis

## Executive Summary
This paper provides a comprehensive technical overview of 3D generative AI progress in 2023, covering 3D object generation, 3D human models, and 3D human motion synthesis. The field has rapidly advanced due to improvements in 2D diffusion models, multi-view consistency techniques, realistic human body representations like SMPL-X, and efficient neural rendering methods like NeRF and 3D Gaussian Splatting. For 3D object generation, methods can be categorized as iterative refinement approaches producing high-quality models or single-pass techniques generating models in seconds. Notable results include RichDreamer achieving the highest quality with 8K resolution outputs and Direct2.5 generating models in just 10 seconds.

## Method Summary
The paper categorizes 3D generation methods into two main approaches: iterative refinement using Score Distillation Sampling (SDS) for high-quality outputs, and single-pass techniques using tri-plane representations for speed. Iterative methods generate multi-view consistent images through fine-tuned 2D diffusion models, then optimize 3D representations (NeRF or 3DGS) through repeated back-propagation using SDS loss. Single-pass methods leverage pre-trained 2D diffusion models to generate coherent multi-view images in one forward pass, fitting them to 3D geometry without iterative optimization. Human-specific generation benefits from parametric priors like SMPL-X, reducing the complexity of learning human shape and pose from data.

## Key Results
- 3D object generation methods achieve high quality through iterative refinement (RichDreamer with 8K resolution) or speed through single-pass techniques (Direct2.5 in 10 seconds)
- Human 3D generation uses SMPL-X parametric priors for structured body representation, with iterative methods (DreamWaltz, HumanNorm) producing detailed avatars and non-iterative approaches (Chupa) generating models from single images in minutes
- 3D motion synthesis leverages transformer-based models and diffusion approaches to generate human motions from text (Story2Motion, DIMOS)
- Key challenges include maintaining multi-view consistency, improving fidelity and accuracy, and developing precise evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI-generated 3D content achieves high quality through iterative refinement using Score Distillation Sampling (SDS) on multi-view images generated by 2D diffusion models.
- Mechanism: The process starts with generating multiple consistent 2D images from different angles using fine-tuned diffusion models, then optimizes a 3D representation (NeRF or 3DGS) through repeated back-propagation against these images using SDS loss.
- Core assumption: Multi-view consistency from the 2D diffusion model is sufficient to guide accurate 3D reconstruction when iteratively refined.
- Evidence anchors:
  - [abstract] "methods can be categorized as iterative refinement approaches producing high-quality models"
  - [section] "The first involves iterative refinement to achieve detailed and coherent models in 3D space, usually requiring over an hour per model. This category often utilizes techniques like Score Distillation Sampling (SDS)"
  - [corpus] Weak evidence - no direct corpus papers discussing SDS mechanism
- Break condition: When the 2D diffusion model fails to maintain consistency across views, the 3D reconstruction will contain artifacts or collapse.

### Mechanism 2
- Claim: Single-pass 3D generation achieves speed by leveraging tri-plane representations and pre-trained 2D diffusion models to generate coherent multi-view images in one forward pass.
- Mechanism: Uses orthogonal planar representations (tri-planes) that are decoded through neural networks to generate multi-view images simultaneously, then fits these to 3D geometry without iterative optimization.
- Core assumption: Pre-trained 2D diffusion models can generate coherent multi-view images from a single latent representation without additional consistency training.
- Evidence anchors:
  - [abstract] "single-pass techniques generating models in seconds"
  - [section] "The second category involves neural networks generating multi-view images in a single step, subsequently fitting these into 3D models... They often employ tri-planes techniques"
  - [corpus] Weak evidence - no corpus papers specifically discussing tri-plane implementations
- Break condition: When the tri-plane representation cannot capture complex 3D geometry, resulting in low-fidelity or incomplete models.

### Mechanism 3
- Claim: Human-specific 3D generation benefits from using SMPL(-X) as a parametric prior, reducing the complexity of learning human shape and pose from data.
- Mechanism: SMPL(-X) provides a structured representation of human body shape, pose, and facial expressions that can be conditioned on image or text inputs, with fine details added through diffusion-based refinement.
- Core assumption: The parametric human model captures sufficient variation in human appearance and articulation to serve as a good prior for AI generation.
- Evidence anchors:
  - [abstract] "realistic human body representations like SMPL-X"
  - [section] "Techniques commonly employed include the SMPL [9] and its successor, SMPL-X [10], widely used for human modeling and training prior"
  - [corpus] Weak evidence - no corpus papers discussing SMPL-X parametric advantages
- Break condition: When clothing, hair, or extreme poses fall outside the SMPL(-X) representation capabilities, resulting in unrealistic outputs.

## Foundational Learning

- Concept: Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) as neural 3D representations
  - Why needed here: These are the primary 3D representations used for storing and rendering AI-generated 3D content
  - Quick check question: What is the fundamental difference between how NeRF and 3DGS represent 3D scenes?

- Concept: Diffusion models and score distillation sampling (SDS)
  - Why needed here: These are the core generative techniques used for both 2D image generation and 3D optimization
  - Quick check question: How does SDS differ from standard diffusion model training objectives?

- Concept: Multi-view consistency and camera parameter encoding
  - Why needed here: Ensuring generated 2D images from different viewpoints correspond to the same 3D object is critical for successful 3D reconstruction
  - Quick check question: What techniques are used to encode camera parameters so they can be incorporated into diffusion model conditioning?

## Architecture Onboarding

- Component map: Input (text/image) → 2D diffusion model → multi-view image generation → 3D representation optimization (NeRF/3DGS) → output 3D model/scene
- Critical path: For iterative methods: text/image → diffusion model → multi-view images → SDS optimization → 3D geometry → rendering
- Design tradeoffs: Quality vs speed (iterative methods produce higher quality but take hours vs single-pass methods that take seconds but produce lower quality)
- Failure signatures: Inconsistent multi-view images leading to broken geometry, over-smoothing from excessive SDS iterations, or unrealistic human poses from SMPL(-X) limitations
- First 3 experiments:
  1. Test 2D diffusion model consistency by generating multiple views of the same object and measuring similarity metrics
  2. Implement a basic SDS optimization loop with a simple NeRF to verify the refinement process works
  3. Test SMPL(-X) conditioning by generating human poses from text and evaluating the realism of the outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal architectures and training strategies for achieving high-quality 3D human model generation in a single pass (non-iterative methods) that can match the quality of iterative approaches?
- Basis in paper: [explicit] The paper discusses both iterative methods (e.g., DreamWaltz, HumanNorm) achieving high quality but requiring hours, and non-iterative methods (e.g., GTA, Chupa) being faster but producing lower quality models. It states "Non-iterative human generation methods... is also noteworthy" and "GTA is particularly distinguished for its rapid texture modeling capabilities."
- Why unresolved: While non-iterative methods offer significant speed advantages, they currently sacrifice quality. The paper does not identify a method that successfully combines the speed of non-iterative approaches with the quality of iterative ones.
- What evidence would resolve it: A paper demonstrating a non-iterative 3D human generation method that achieves quality metrics (e.g., user preference scores, detail richness) comparable to or exceeding current iterative methods, while maintaining the speed advantage of non-iterative approaches.

### Open Question 2
- Question: How can multi-view consistency be effectively maintained in 3D scene generation, particularly for scenes with complex backgrounds and objects?
- Basis in paper: [explicit] The paper states "Most existing 3D AIGC models struggle with objects and scenes that include backgrounds" and "maintaining sufficient consistency across multiple images remains the biggest challenge in 3D generation." It also mentions that "these issues are also being addressed by some papers" like ZeroNVS using DDIM for consistency.
- Why unresolved: While some progress has been made, the paper indicates this remains a significant challenge, especially for scenes with backgrounds. Current methods often require object extraction before 3D reconstruction, which is a limitation.
- What evidence would resolve it: A method that can generate 3D scenes with complex backgrounds and objects while maintaining high multi-view consistency without requiring prior object extraction, validated through comprehensive quantitative and qualitative evaluations.

### Open Question 3
- Question: What are the most effective metrics for evaluating the quality and realism of AI-generated 3D content, particularly for human models and motions?
- Basis in paper: [explicit] The paper states "This field still lacks a precise metrics, with many papers relying on blind user tests for comparison" and "I think From a user perspective, scoring metrics worth considering include: the model's generalization capability, 3D model precision, 3D texture detail, the proportion of factual errors noticeable to the human eye, and understanding of input text or images."
- Why unresolved: The paper acknowledges the lack of precise metrics and suggests potential considerations, but does not propose a comprehensive evaluation framework. Current evaluations rely heavily on user studies and adapted 2D image metrics.
- What evidence would resolve it: A standardized evaluation framework for AI-generated 3D content that includes objective metrics for 3D model quality, texture detail, multi-view consistency, and motion realism, validated across multiple methods and datasets, with clear guidelines for implementation and comparison.

## Limitations
- The paper focuses exclusively on 2023 developments, potentially missing important precursor work and longer-term trends
- Evaluation metrics for 3D generative quality are lacking, with many comparisons relying on subjective user preference studies
- Technical implementation details, hyperparameter settings, and code availability information are not provided

## Confidence
- High Confidence: The categorization of 3D generation methods into iterative refinement and single-pass approaches accurately represents the methodological landscape; identification of key challenges (multi-view consistency, fidelity, evaluation metrics) reflects current state of the field
- Medium Confidence: Specific performance comparisons between methods may vary depending on evaluation protocols and datasets used; effectiveness of SMPL(-X) as a parametric prior is well-established but optimal integration methods continue to evolve
- Low Confidence: Precise time-to-generate estimates may vary significantly based on hardware and implementation details not specified; claims about specific dataset performance without standardized benchmarks are difficult to verify

## Next Checks
1. Replicate Multi-View Consistency: Generate multi-view images of the same object using the diffusion model conditioning techniques described, then measure CLIP similarity scores and perform structured user studies to verify consistency claims across different methods

2. Benchmark Evaluation Metrics: Implement a standardized evaluation pipeline comparing Chamfer Distance, F-score, and perceptual metrics (LPIPS) across at least three representative 3D generation methods using a common dataset like CO3D to establish baseline performance ranges

3. Parametric Prior Validation: Test SMPL(-X) conditioning effectiveness by generating human poses from text prompts and evaluating both quantitative metrics (joint position error) and qualitative assessments of pose realism across clothing types and extreme poses