---
ver: rpa2
title: Learning production functions for supply chains with graph neural networks
arxiv_id: '2407.18772'
source_url: https://arxiv.org/abs/2407.18772
tags:
- supply
- inventory
- data
- product
- transactions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses temporal production graphs (TPGs), a new graph
  ML setting where nodes' edges are governed by hidden production functions. Existing
  GNNs cannot capture these relationships, so the authors introduce a new class of
  models that combine temporal GNNs with an inventory module to jointly learn production
  functions and predict future edges.
---

# Learning production functions for supply chains with graph neural networks

## Quick Facts
- arXiv ID: 2407.18772
- Source URL: https://arxiv.org/abs/2407.18772
- Reference count: 34
- Primary result: Introduces temporal production graphs (TPGs) and shows inventory module improves edge prediction by 11%-62% and production function learning by 6%-50%

## Executive Summary
This paper addresses temporal production graphs (TPGs), a new graph ML setting where nodes' edges are governed by hidden production functions. The authors introduce a new class of models that combine temporal GNNs with an inventory module to jointly learn production functions and predict future edges. The inventory module explicitly tracks each firm's inventory and updates it using attention weights learned from node embeddings, with a specialized loss function that encourages correct production mappings while penalizing impossible transactions.

The authors demonstrate their approach on real and synthetic supply chain data. On real data (Tesla, industrial equipment), their models outperform baselines by 11%-62% for edge prediction and learn production functions 6%-50% better than alternatives. Their open-source simulator, SupplySim, generates realistic data matching real supply chains on key characteristics like degree distribution, community structure, and time-varying transactions with possible shocks. A case study shows the model can generate future transactions under supply chain disruptions, maintaining high accuracy even 7 timesteps after training data ends.

## Method Summary
The method combines temporal GNNs (SC-TGN or SC-GraphMixer) with an inventory module to learn production functions and predict future supply chain transactions. The inventory module tracks each firm's inventory and updates it using attention weights learned from node embeddings. A specialized loss function penalizes inventory debt and rewards consumption, encouraging sparse but meaningful attention weights. The approach is trained end-to-end with a combination of edge prediction loss, inventory loss, and update penalty, using negative sampling with perturbation and historical negatives.

## Key Results
- Outperforms baselines by 11%-62% for edge prediction on real supply chain data
- Learns production functions 6%-50% better than alternatives
- Maintains high accuracy (MRR > 0.5) even 7 timesteps after training data ends
- Inventory module is robust to 20% missing firms in transaction data
- SupplySim generates realistic synthetic data matching real supply chains on degree distribution, community structure, and time-varying transactions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The inventory module can learn production functions by using attention weights to map products consumed to products produced.
- **Mechanism:** The inventory module explicitly tracks each firm's inventory and updates it using attention weights learned from node embeddings. These attention weights represent how much of each input product is needed to produce one unit of an output product. The specialized loss function penalizes inventory debt (consumption exceeding inventory) and rewards consumption, encouraging sparse but meaningful attention weights.
- **Core assumption:** The observed transactions contain enough information to infer the underlying production functions through temporal patterns and inventory constraints.
- **Evidence anchors:**
  - [abstract]: "learns production functions via attention weights and a special loss function"
  - [section]: "we need to learn the function mapping from products externally supplied to products internally consumed from the inventory"
- **Break condition:** If the dataset is too sparse or if production functions vary significantly across firms, the attention weights may not converge to meaningful values.

### Mechanism 2
- **Claim:** Temporal GNNs combined with the inventory module outperform static models for edge prediction in supply chains.
- **Mechanism:** Temporal GNNs capture the dynamic evolution of the supply chain network, while the inventory module provides additional constraints based on production functions. This combination allows the model to predict not just which transactions are likely, but also their amounts, by incorporating inventory constraints.
- **Core assumption:** The temporal dynamics of supply chains contain sufficient signal for GNNs to learn meaningful representations, and the inventory module can effectively regularize these representations.
- **Evidence anchors:**
  - [abstract]: "outperforming the strongest baseline by 11%-62% for edge prediction"
  - [section]: "Static: learns a static vector to represent each node" vs "SC-TGN: 0.522 (0.003)"
- **Break condition:** If the temporal patterns are too noisy or if the inventory constraints are not aligned with the actual production functions, the combination may not provide additional benefit.

### Mechanism 3
- **Claim:** The simulator can generate realistic supply chain data that matches real data on key characteristics.
- **Mechanism:** The simulator incorporates realistic features of supply chains, such as tier structure, preferential attachment for supplier selection, and the ARIO model for transaction generation. It also allows for shocks and missing data to test model robustness.
- **Core assumption:** The simplified model of supply chain dynamics (e.g., tier structure, preferential attachment) captures the essential features of real supply chains.
- **Evidence anchors:**
  - [section]: "our synthetic data matches real supply chain networks on key characteristics (Figure 2)"
  - [section]: "Our simulator enables us to share data, test models under controlled and varied settings"
- **Break condition:** If the real-world supply chains have dynamics that are not captured by the simulator (e.g., complex strategic behavior, geographical constraints), the generated data may not be fully representative.

## Foundational Learning

- **Concept: Temporal Graph Neural Networks**
  - Why needed here: Supply chains are inherently dynamic, with transactions occurring over time. Standard GNNs cannot capture these temporal dependencies.
  - Quick check question: What is the key difference between a static GNN and a temporal GNN in how they process node features?

- **Concept: Attention Mechanisms**
  - Why needed here: The inventory module uses attention weights to learn the mapping between input and output products in production functions.
  - Quick check question: How does the inventory module use attention weights differently from a standard attention layer in a transformer?

- **Concept: Graph Representation Learning**
  - Why needed here: The models need to learn meaningful representations of firms and products from the supply chain graph structure.
  - Quick check question: Why might a firm's embedding benefit from aggregating information from its neighbors in the supply chain graph?

## Architecture Onboarding

- **Component map:**
  - Inventory Module -> SC-TGN/SC-GraphMixer -> Decoder -> Edge prediction
  - Inventory Module: Tracks firm inventories, learns production functions via attention weights, provides penalties/caps for edge prediction
  - SC-TGN: Temporal Graph Network with extensions for hyperedges and edge weight prediction
  - SC-GraphMixer: Simpler temporal GNN based on MLP-mixer
  - Decoder: Predicts edge existence and weight from node embeddings
  - Loss functions: Separate losses for edge prediction, inventory updates, and regularization

- **Critical path:**
  1. Process transactions to update node memories in SC-TGN or link/node encodings in SC-GraphMixer
  2. Generate node embeddings (via GNN or concatenation)
  3. Use embeddings in inventory module to update attention weights and inventory
  4. Predict edges using decoder, with penalties/caps from inventory module
  5. Compute losses and backpropagate

- **Design tradeoffs:**
  - SC-TGN vs SC-GraphMixer: SC-TGN is more complex but generally performs better; SC-GraphMixer is simpler and faster
  - Direct attention weights vs embedding-based: Direct learning is simpler but embedding-based can share information across product pairs
  - Including inventory penalties/caps: Helps when data is complete, hurts when data is missing

- **Failure signatures:**
  - Low MRR but low train loss: Model is overfitting or struggling with temporal generalization
  - High inventory loss but low MAP: Inventory module is not learning meaningful production functions
  - Performance drop under shocks: Model is not robust to supply chain disruptions

- **First 3 experiments:**
  1. Run SC-TGN on SS-std without inventory module to establish baseline performance
  2. Add inventory module to SC-TGN and evaluate production function learning (MAP)
  3. Test model performance under supply shocks (SS-shocks) to assess robustness

## Open Questions the Paper Calls Out
- **Question:** How does the performance of the inventory module degrade when faced with significantly higher levels of missing data (e.g., 50% of firms missing vs. 20% in the experiments)?
- **Question:** Can the inventory module be adapted to handle situations where products have multiple valid production functions (i.e., substitute products or firm-specific production functions)?
- **Question:** What is the theoretical connection between the inventory module's loss function and causal inference in temporal production graphs?

## Limitations
- Performance degrades significantly with missing data (45-65% MAP decrease with 20% missing firms)
- The approach assumes one way to make each product, which may not hold in real-world scenarios
- Model's robustness to supply chain shocks is demonstrated but only for specific shock types

## Confidence
- **High confidence:** The temporal GNN + inventory module architecture demonstrably improves edge prediction over static baselines (11-62% MRR improvement). The simulator's ability to generate realistic supply chain data is well-validated against real data characteristics.
- **Medium confidence:** The inventory module's attention weights meaningfully capture production functions. While MAP scores show improvement (6-50% over baselines), the direct interpretability of attention weights as production functions needs further validation.
- **Low confidence:** The model's robustness to supply chain shocks is demonstrated but only for specific shock types. The generalizability to other disruption scenarios (geopolitical events, natural disasters) remains unclear.

## Next Checks
1. Test the model's performance on real-world supply chain datasets with known data gaps to quantify the practical impact of missing transactions on production function learning.
2. Conduct ablation studies removing the inventory module to precisely measure its contribution to edge prediction vs. production function learning.
3. Evaluate the model on supply chain data from different industries to assess whether the learned production functions transfer across sectors or remain domain-specific.