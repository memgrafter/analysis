---
ver: rpa2
title: A Closer Look at the Robustness of Contrastive Language-Image Pre-Training
  (CLIP)
arxiv_id: '2402.07410'
source_url: https://arxiv.org/abs/2402.07410
tags:
- clip
- training
- accuracy
- robustness
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work comprehensively investigates the safety objectives of
  CLIP models, focusing on three key properties: resilience to visual factor variations,
  calibrated uncertainty estimations, and the ability to detect anomalous inputs.
  The study analyzes 83 CLIP models and 127 ImageNet classifiers across 10 visual
  factors, 5 types of out-of-distribution data, and 8 natural and challenging test
  conditions.'
---

# A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)

## Quick Facts
- arXiv ID: 2402.07410
- Source URL: https://arxiv.org/abs/2402.07410
- Reference count: 40
- Primary result: Comprehensive analysis of CLIP model safety objectives across visual factors, OOD detection, and calibration

## Executive Summary
This study provides the first comprehensive investigation of CLIP model safety objectives, analyzing 83 CLIP models and 127 ImageNet classifiers across 10 visual factors, 5 OOD types, and 8 test conditions. The research reveals that CLIP models exhibit systematically different robustness patterns than ImageNet classifiers, with training data distribution playing a crucial role in determining robustness characteristics. The study challenges existing assumptions about CLIP calibration superiority and identifies shape bias as a key differentiator between CLIP and standard models, though this bias diminishes after ImageNet fine-tuning.

## Method Summary
The researchers conducted extensive experiments evaluating 83 diverse CLIP models and 127 ImageNet classifiers across multiple safety dimensions. They analyzed factor-level effective robustness for 10 visual factors (pose, lighting, texture, etc.), out-of-distribution detection performance using 5 OOD datasets, and calibration quality using both Expected Calibration Error and Negative Log Likelihood. The study compared models trained on different distributions (LAION vs WIT) and evaluated the impact of fine-tuning on ImageNet datasets. Statistical analysis was performed to identify correlations between model characteristics and performance across the three safety objectives.

## Key Results
- CLIP models show systematically different robustness patterns than ImageNet classifiers on 6 out of 10 visual factors
- Training data distribution (LAION vs WIT) significantly influences CLIP model robustness characteristics
- CLIP models exhibit shape bias in predictions, but this bias diminishes after ImageNet fine-tuning
- CLIP models are not consistently more calibrated than ImageNet models, contradicting prior findings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP models trained on different distributions (LAION vs WIT) show systematically different robustness patterns across visual factors
- Mechanism: The training data distribution fundamentally shapes the visual features that CLIP models prioritize during contrastive learning, leading to domain-specific robustness characteristics
- Core assumption: The visual statistics and content of LAION and WIT datasets differ significantly in ways that affect how models learn to represent pose, lighting, background, and other visual factors
- Evidence anchors:
  - [abstract] "training distribution plays an important role in CLIP robustness against visual factors"
  - [section 4.1] "we observe that CLIP models trained on LAION demonstrate higher robustness on Shape factor than those trained on WIT, while this reverses for Background and Pose factors"
  - [corpus] Weak evidence - no corpus papers directly compare LAION vs WIT training effects
- Break condition: If LAION and WIT datasets have overlapping visual statistics and content distributions, the systematic differences in robustness patterns would disappear

### Mechanism 2
- Claim: CLIP models exhibit shape bias in predictions, but this bias diminishes after ImageNet fine-tuning
- Mechanism: The contrastive learning objective with text supervision initially encourages shape-based reasoning, but standard ImageNet fine-tuning shifts the model toward texture-based features common in ImageNet training
- Core assumption: The text supervision in CLIP training emphasizes shape-relevant concepts, while ImageNet fine-tuning emphasizes texture features that improve classification accuracy on ImageNet
- Evidence anchors:
  - [abstract] "CLIP models are biased towards shape when making predictions. However, we have also found that this bias diminishes after fine-tuning on ImageNet"
  - [section 4.2] "CLIP models are more likely to make predictions based on shape compared to the other three groups" and "shape bias in CLIP weakens after fine-tuning on ImageNet"
  - [corpus] Weak evidence - no corpus papers directly analyze shape vs texture bias changes through fine-tuning
- Break condition: If fine-tuning procedures preserve or enhance shape bias rather than diminish it, or if shape bias is primarily determined by model architecture rather than training procedure

### Mechanism 3
- Claim: CLIP models' calibration performance depends heavily on training data distribution and quantity
- Mechanism: The statistical properties and scale of training data determine how well CLIP models can generalize their uncertainty estimates to out-of-distribution conditions
- Core assumption: Different training distributions provide varying levels of coverage for the feature space, affecting the model's ability to produce well-calibrated predictions under distribution shift
- Evidence anchors:
  - [abstract] "CLIP models are not consistently more calibrated than other ImageNet models, which contradicts existing findings"
  - [section 6.1] "CLIP models trained on different distributions or quantities are not consistently grouped together" and "CLIP models trained on LAION-80M dataset exhibit much lower calibration performance when compared to standard models"
  - [corpus] Weak evidence - no corpus papers directly compare calibration across different CLIP training distributions
- Break condition: If calibration is primarily determined by model architecture or fine-tuning procedures rather than training data characteristics

## Foundational Learning

- Concept: Visual factor robustness analysis
  - Why needed here: Understanding how models perform on specific visual transformations (pose, lighting, texture) is crucial for safety-critical applications where certain factors may be more important than others
  - Quick check question: What is the difference between measuring overall accuracy and factor-level effective robustness, and why is the latter more informative for safety analysis?

- Concept: Out-of-distribution detection metrics
  - Why needed here: Evaluating model uncertainty and anomaly detection requires understanding metrics like AUROC and FPR@95 to quantify performance across different OOD scenarios
  - Quick check question: How do AUROC and FPR@95 differ in what they measure, and why might one be preferred over the other for safety-critical applications?

- Concept: Calibration error metrics
  - Why needed here: Reliable uncertainty estimation requires understanding both ECE (Estimated Calibration Error) and NLL (Negative Log Likelihood) to evaluate predictive confidence quality
  - Quick check question: What is the key difference between ECE and NLL as calibration metrics, and how does temperature scaling affect their values?

## Architecture Onboarding

- Component map: CLIP models (83 total) with visual encoders (ResNet, ConvNeXt, ViT) -> Contrastive learning with text supervision -> Evaluation across 10 visual factors, 5 OOD types, 8 test conditions -> Analysis of robustness, OOD detection, calibration

- Critical path: (1) Select diverse model architectures and training configurations, (2) Run evaluations across all test conditions, (3) Aggregate results by model group and training characteristic, (4) Perform comparative analysis to identify influential factors

- Design tradeoffs: Using multiple training distributions (LAION vs WIT) provides broader insights but increases complexity; evaluating 83 CLIP models provides statistical power but requires significant computational resources; comparing against 127 ImageNet models establishes baselines but may obscure CLIP-specific behaviors

- Failure signatures: Unexpected calibration patterns may indicate dataset curation issues; inconsistent OOD detection performance across similar models may suggest architectural sensitivities; factor-level robustness differences that don't align with training data characteristics may indicate implementation bugs

- First 3 experiments:
  1. Run factor-level robustness analysis on a small subset of CLIP models (ViT/B-16 trained on LAION-80M vs WIT) across 3-4 visual factors to verify the methodology works
  2. Test calibration performance on a single test set (ImageNet-V2) for both CLIP and standard models to establish baseline calibration differences
  3. Evaluate OOD detection on a single OOD dataset (ImageNet-O) for CLIP models with different training sources to verify the correlation with ID accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different training data curation techniques impact the calibration performance of CLIP models?
- Basis in paper: [explicit] The paper states "When evaluating dataset curation, it is crucial to consider its impact on the calibration performance of CLIP models."
- Why unresolved: The paper only observes that training data distribution and quantity impact CLIP's calibration, but does not investigate specific curation techniques.
- What evidence would resolve it: Controlled experiments comparing different data curation methods (e.g., filtering, augmentation, reweighting) and their effects on CLIP calibration across various distributions.

### Open Question 2
- Question: What fine-tuning procedures can improve CLIP's out-of-distribution detection performance without sacrificing classification accuracy?
- Basis in paper: [explicit] The paper notes that "some CLIP-FT models even achieve worse OOD detection performance than Zero-shot CLIP models" and that "exploring impacts of fine-tuning datasets other than ImageNet-1K and ImageNet-12K would be another interesting direction."
- Why unresolved: While the paper identifies that additional fine-tuning on ImageNet-12K improves OOD detection, it does not explore alternative fine-tuning strategies or datasets.
- What evidence would resolve it: Comparative analysis of various fine-tuning approaches (e.g., different datasets, objectives, or architectures) and their effects on both classification accuracy and OOD detection performance.

### Open Question 3
- Question: How can test-time prompts be optimized to simultaneously improve CLIP's classification accuracy, calibration, and out-of-distribution detection performance?
- Basis in paper: [explicit] The paper concludes that "it would be interesting to study this when conducting prompt learning" in the context of how different prompt sets affect CLIP's three safety objectives.
- Why unresolved: The paper only examines a few fixed prompt sets and observes mixed effects on the safety objectives, but does not investigate systematic prompt optimization methods.
- What evidence would resolve it: Experiments using prompt learning techniques (e.g., prompt tuning, prompt engineering) to jointly optimize for classification accuracy, calibration, and OOD detection across various distributions and tasks.

## Limitations

- The study cannot establish causal relationships between training characteristics and model behavior, only correlational patterns
- The analysis assumes that the 10 visual factors and 5 OOD types comprehensively capture safety-relevant scenarios for all deployment contexts
- The research cannot disentangle whether observed differences stem from the contrastive learning objective itself, the text supervision, or other training hyperparameters

## Confidence

- **High confidence**: CLIP models demonstrate systematically different robustness patterns across visual factors compared to ImageNet classifiers; shape bias exists in CLIP models and diminishes after ImageNet fine-tuning
- **Medium confidence**: Training distribution significantly influences CLIP robustness across visual factors; CLIP models are not consistently better calibrated than ImageNet models
- **Low confidence**: The specific mechanisms by which training data characteristics influence calibration and OOD detection performance; the relative importance of different visual factors for safety-critical applications

## Next Checks

1. **Controlled ablation study**: Train CLIP models with identical architectures and hyperparameters except for training data source (LAION vs WIT), then measure factor-level robustness differences to isolate the effect of training distribution from other variables.

2. **Fine-tuning intervention experiment**: Take CLIP models with strong shape bias and systematically vary the fine-tuning procedure (data augmentation, learning rate schedules, loss functions) to determine which aspects of fine-tuning reduce shape bias versus other effects.

3. **Cross-dataset generalization test**: Evaluate the same CLIP models on multiple datasets with known visual factor characteristics (e.g., COCO for background complexity, Pascal VOC for object variety) to validate that observed robustness patterns generalize beyond the ImageNet-based test suite.