---
ver: rpa2
title: Improved Algorithm for Adversarial Linear Mixture MDPs with Bandit Feedback
  and Unknown Transition
arxiv_id: '2403.04568'
source_url: https://arxiv.org/abs/2403.04568
tags:
- lemma
- linear
- transition
- mdps
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies adversarial linear mixture MDPs with bandit
  feedback and unknown transition. The authors propose a new algorithm that leverages
  the visit information of all states and introduces a new self-normalized concentration
  tailored specifically to handle non-independent noises.
---

# Improved Algorithm for Adversarial Linear Mixture MDPs with Bandit Feedback and Unknown Transition

## Quick Facts
- arXiv ID: 2403.04568
- Source URL: https://arxiv.org/abs/2403.04568
- Authors: Long-Fei Li; Peng Zhao; Zhi-Hua Zhou
- Reference count: 40
- Achieves Õ(d√(HS³K) + √(HSAK)) regret with high probability

## Executive Summary
This paper studies adversarial linear mixture MDPs with bandit feedback and unknown transition dynamics. The authors propose a new algorithm that improves upon previous work by leveraging visit information from all states rather than just one state with maximum uncertainty. By introducing a new self-normalized concentration lemma tailored for non-independent noises and using a least squares estimator that utilizes information from all states, the algorithm achieves strictly better regret bounds when the horizon H is less than or equal to the number of states S.

## Method Summary
The paper proposes VLSUOB-REPS, an algorithm for adversarial linear mixture MDPs with bandit feedback. It uses a least squares estimator for the unknown transition parameter that leverages visit information from all states, as opposed to only a single state in prior work. The algorithm constructs confidence sets for transition parameters using a new self-normalized concentration lemma designed to handle non-independent noises. Online mirror descent is applied over the occupancy measure space with loss estimators and upper occupancy bounds to update policies. The algorithm achieves improved regret bounds by simultaneously exploring the orientations of every state and properly accounting for noise correlations.

## Key Results
- Achieves Õ(d√(HS³K) + √(HSAK)) regret with high probability
- Strictly improves previous best-known Õ(dS²√K + √(HSAK)) result
- Primary improvement comes from using all-state information in least squares estimation and new self-normalized concentration for non-independent noises

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm achieves improved regret by leveraging visit information of all states rather than just one state with maximum uncertainty.
- Mechanism: By using the transition information of all states to construct a least squares estimator for the unknown transition parameter, the algorithm can simultaneously explore the orientations of every state, leading to a tighter confidence set and improved regret bound.
- Core assumption: The self-normalized concentration lemma (Lemma 1) can handle non-independent noises across different states.
- Evidence anchors:
  - [abstract]: "Our advancements are primarily attributed to (i) a new least square estimator for the transition parameter that leverages the visit information of all states, as opposed to only one state in prior work, and (ii) a new self-normalized concentration tailored specifically to handle non-independent noises"
  - [section]: "To enhance the utility of visitation data, we introduce a new least square estimator for the unknown transition parameter that leverages the visit information of all states, as opposed to only a single state in Zhao et al. (2023a)."
- Break condition: If the non-independent noise structure cannot be properly handled, the self-normalized concentration would fail, and the algorithm would revert to the weaker performance of using only one state's information.

### Mechanism 2
- Claim: The new self-normalized concentration lemma specifically designed for non-independent noises enables tighter confidence bounds for the transition parameter.
- Mechanism: The lemma adapts techniques from the dynamic assortment problem to handle state correlations in reinforcement learning, allowing the algorithm to bound the estimation error across all states simultaneously rather than just one.
- Core assumption: The dynamic assortment concentration technique can be adapted to the reinforcement learning setting with appropriate modifications.
- Evidence anchors:
  - [abstract]: "a new self-normalized concentration tailored specifically to handle non-independent noises, originally proposed in the dynamic assortment area and firstly applied in reinforcement learning to handle correlations between different states"
  - [section]: "We address this key challenge by introducing a new self-normalized concentration lemma tailored specifically to accommodate non-independent random noises. This lemma was originally proposed by Périvier and Goyal (2022) for the dynamic assortment problem"
- Break condition: If the adaptation from dynamic assortment to reinforcement learning introduces significant errors or the correlation structure is too complex, the concentration bounds may become vacuous.

### Mechanism 3
- Claim: The combination of full-state exploration and proper noise handling enables the algorithm to strictly improve upon the previous best-known regret bound.
- Mechanism: By simultaneously exploring all state orientations and properly accounting for noise correlations, the algorithm achieves a regret bound of Õ(d√(HS³K) + √(HSAK)), which improves upon the previous Õ(dS²√K + √(HSAK)) bound when H ≤ S.
- Core assumption: The layered MDP structure ensures H ≤ S, making the new bound strictly better than the previous one.
- Evidence anchors:
  - [abstract]: "Our result strictly improves the previous best-known Õ(dS²√K + √(HSAK)) result in Zhao et al. (2023a) since H ≤ S holds by the layered MDP structure"
  - [section]: "Our result strictly improves the eO(dS2√K + √HSAK) regret of Zhao et al. (2023a) since H ≤ S by the layered MDP structure"
- Break condition: If the layered MDP assumption is violated (H > S), the improvement may not hold, and the algorithm's performance could degrade to be comparable with or worse than previous methods.

## Foundational Learning

- Concept: Self-normalized concentration inequalities for vector-valued martingales
  - Why needed here: To build confidence sets for the unknown transition parameter while accounting for the non-independent noise structure across states
  - Quick check question: How does the self-normalized concentration handle the fact that only one state is visited per step, making noises across states non-independent?

- Concept: Linear mixture MDPs and function approximation
  - Why needed here: The problem setting involves linear mixture MDPs where the transition kernel is a linear function of feature mappings, requiring understanding of how to estimate parameters in this context
  - Quick check question: What is the difference between linear mixture MDPs and linear MDPs in terms of how the transition is parameterized?

- Concept: Online Mirror Descent (OMD) in occupancy measure space
  - Why needed here: The algorithm applies OMD over the occupancy measure space to optimize policies while maintaining feasibility under the estimated transition
  - Quick check question: How does OMD in occupancy measure space differ from standard OMD in action space, and why is this distinction important for adversarial MDPs?

## Architecture Onboarding

- Component map:
  Transition estimator -> Confidence set construction -> Loss estimator -> Online Mirror Descent -> Policy extraction

- Critical path:
  1. At each step, observe state and take action according to current policy
  2. Update transition parameter estimates using all visited states' information
  3. Construct confidence sets for transition parameters
  4. Compute upper occupancy bounds and construct loss estimators
  5. Apply OMD to update occupancy measures
  6. Extract new policy from updated occupancy measures

- Design tradeoffs:
  - Using all state information vs. only maximum uncertainty state: Better regret but more complex concentration analysis
  - Biased loss estimation with exploration parameter γ: Encourages exploration but requires tuning γ for performance
  - Regularization parameter λk: Balances bias-variance tradeoff in transition estimation, requires careful scheduling

- Failure signatures:
  - Poor exploration leading to large confidence sets: Indicates γ is too small or loss estimator is not properly biased
  - Unstable parameter estimates: Suggests λk scheduling is incorrect or concentration bounds are too loose
  - Suboptimal policies: Could indicate OMD step size η is poorly chosen or confidence sets are not accurate enough

- First 3 experiments:
  1. Compare regret performance with and without using all state information (vs. only maximum uncertainty state)
  2. Test sensitivity to exploration parameter γ by varying it across orders of magnitude
  3. Evaluate performance under different λk schedules to find optimal regularization for various problem scales

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the dependence on S in the regret bound be eliminated without increasing the dependence on A?
- Basis in paper: [explicit] The paper explicitly states "how to close this gap is an important open question" and "the main hardness of simultaneously eliminating the dependence of the occupancy measure difference on both S and A is that though the transition kernel P of a linear mixture MDP admits a linear structure, the occupancy measure still has a complicated recursive form."
- Why unresolved: The occupancy measure has a recursive form that inherently involves both state and action spaces, making it challenging to eliminate the dependence on S without increasing the dependence on A.
- What evidence would resolve it: A theoretical proof showing a new algorithm that achieves a regret bound with no dependence on S or A, or a lower bound proving that such a dependence is unavoidable.

### Open Question 2
- Question: Can the techniques from dynamic assortment problems be further adapted to improve regret bounds for other RL problems with non-independent noises?
- Basis in paper: [explicit] The paper states "This is the first work that bridges the two distinct fields: dynamic assortment and RL theory" and discusses adapting a self-normalized concentration lemma from dynamic assortment to handle state correlations in RL.
- Why unresolved: The adaptation of techniques from dynamic assortment to RL is novel, but its full potential for improving regret bounds in other RL problems with non-independent noises remains unexplored.
- What evidence would resolve it: Empirical or theoretical results demonstrating improved regret bounds for other RL problems (e.g., linear MDPs, contextual bandits) by applying similar self-normalized concentration techniques to handle non-independent noises.

### Open Question 3
- Question: Is it possible to develop a more efficient algorithm for computing the upper occupancy bound uk(s, a) defined in (10)?
- Basis in paper: [inferred] The paper mentions that "The above step can be computed efficiently by the Comp-UOB procedure of Jin et al. (2020a)" but does not provide details on the computational complexity or potential improvements.
- Why unresolved: The efficiency of computing the upper occupancy bound is crucial for practical implementation, but the current method's computational complexity and potential optimizations are not thoroughly explored.
- What evidence would resolve it: An algorithm with improved time or space complexity for computing the upper occupancy bound, or a theoretical analysis proving the current method is optimal.

## Limitations
- The algorithm's performance critically depends on the layered MDP structure assumption (H ≤ S), which may not hold in all practical scenarios.
- The adaptation of self-normalized concentration from dynamic assortment to reinforcement learning introduces complexity that could lead to implementation challenges.
- The algorithm requires careful tuning of exploration parameters and regularization schedules, which may not generalize well across different problem instances.

## Confidence
- **High Confidence**: The regret bound improvement from Õ(dS²√K) to Õ(d√(HS³K)) when H ≤ S is mathematically rigorous and follows from the algorithm's design.
- **Medium Confidence**: The adaptation of self-normalized concentration for non-independent noises from dynamic assortment to reinforcement learning is theoretically justified but may face practical implementation challenges.
- **Low Confidence**: The assumption that H ≤ S always holds in practice is strong, and violations of this assumption could significantly degrade performance.

## Next Checks
1. **Layered MDP Assumption Test**: Evaluate algorithm performance on problems where H > S to quantify the degradation in regret bounds and identify when the improvement breaks down.
2. **Exploration Sensitivity Analysis**: Systematically vary the exploration parameter γ across multiple orders of magnitude to determine optimal ranges and identify sensitivity patterns.
3. **Feature Mapping Quality Impact**: Test the algorithm with different quality feature mappings (from well-designed to random features) to assess robustness and identify minimum requirements for effective performance.