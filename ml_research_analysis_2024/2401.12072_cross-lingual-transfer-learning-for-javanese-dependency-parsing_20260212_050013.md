---
ver: rpa2
title: Cross-lingual Transfer Learning for Javanese Dependency Parsing
arxiv_id: '2401.12072'
source_url: https://arxiv.org/abs/2401.12072
tags:
- learning
- transfer
- language
- javanese
- dependency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of dependency parsing for Javanese,
  a low-resource language with over 80 million speakers but limited NLP resources.
  The authors propose using cross-lingual transfer learning to improve parsing performance,
  employing both standard transfer learning (TL) and hierarchical transfer learning
  (HTL) methods.
---

# Cross-lingual Transfer Learning for Javanese Dependency Parsing

## Quick Facts
- **arXiv ID**: 2401.12072
- **Source URL**: https://arxiv.org/abs/2401.12072
- **Reference count**: 8
- **Primary result**: Cross-lingual transfer learning improves Javanese dependency parsing by 10% in both UAS and LAS metrics

## Executive Summary
This paper addresses the challenge of dependency parsing for Javanese, a low-resource language with over 80 million speakers but limited NLP resources. The authors propose using cross-lingual transfer learning to improve parsing performance, employing both standard transfer learning (TL) and hierarchical transfer learning (HTL) methods. The HTL approach uses an intermediate language (Indonesian) in addition to the source language during training. The proposed models significantly outperform the baseline model, achieving a 10% improvement in both Unlabeled Attachment Score (UAS) and Labeled Attachment Score (LAS) metrics. The best performance was obtained using the HTL method with Italian and English as source languages and Indonesian as the intermediate language. FastText embeddings were found to be slightly more effective than BERT-based embeddings for this task.

## Method Summary
The authors employed two cross-lingual transfer learning approaches: standard transfer learning (TL) and hierarchical transfer learning (HTL). In TL, models were trained on source languages and directly applied to Javanese. HTL introduced an intermediate language (Indonesian) between source and target languages. The pipeline used UDPipe for tokenization and pre-processing, followed by biaffine parser training. FastText and BERT-based embeddings were tested, with the best results achieved using FastText embeddings in the HTL setup with Italian and English as source languages.

## Key Results
- Cross-lingual transfer learning improved Javanese parsing by 10% in both UAS and LAS metrics
- HTL method with Indonesian as intermediate language outperformed standard TL
- FastText embeddings slightly outperformed BERT-based embeddings for this task
- Best performance achieved using HTL with Italian and English as source languages

## Why This Works (Mechanism)
The paper doesn't explicitly detail the underlying mechanism, but the effectiveness likely stems from shared linguistic features between source languages and Javanese, particularly through Indonesian as an intermediate language that shares some typological features with Javanese.

## Foundational Learning
- **Dependency parsing**: Understanding syntactic relationships between words; needed for evaluating parsing quality; quick check: can you explain subject-verb-object vs subject-object-verb structures?
- **Cross-lingual transfer learning**: Knowledge transfer across languages; needed to apply models from resource-rich to resource-poor languages; quick check: can you describe how word order differences affect transfer?
- **Hierarchical transfer learning**: Multi-step knowledge transfer through intermediate languages; needed to bridge linguistic gaps; quick check: can you explain why Indonesian serves as a good intermediate for Javanese?
- **Embedding representations**: Word vector representations capturing semantic and syntactic information; needed for model input; quick check: can you differentiate between subword and character-level embeddings?
- **biaffine parser architecture**: Neural network for dependency parsing; needed for the parsing task; quick check: can you describe the role of attention mechanisms in biaffine parsers?

## Architecture Onboarding

Component map: UDPipe preprocessing -> biaffine parser training -> evaluation with UAS/LAS metrics

Critical path: Raw text -> tokenization/segmentation (UDPipe) -> embedding generation -> biaffine parser training -> dependency parsing output -> UAS/LAS evaluation

Design tradeoffs: The paper chose between FastText and BERT embeddings, balancing computational efficiency against contextual representation quality. The HTL approach trades increased training complexity for better performance when an appropriate intermediate language is available.

Failure signatures: Poor performance would manifest as low UAS/LAS scores, particularly on long-range dependencies or complex syntactic constructions. The baseline model serves as a reference point for identifying such failures.

First experiments:
1. Run baseline model on Javanese test data to establish reference performance
2. Train TL model using Italian as source language and evaluate on Javanese
3. Train HTL model using Italian and Indonesian as intermediate language, then evaluate on Javanese

## Open Questions the Paper Calls Out
None

## Limitations
- HTL effectiveness depends on availability of linguistically related intermediate languages
- Source language selection appears arbitrary without systematic exploration
- Limited embedding comparison (only one BERT-based model tested)
- Evaluation metrics may not fully capture parsing quality for morphologically complex languages

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Cross-lingual transfer learning significantly improves Javanese parsing | High |
| HTL outperforms standard TL methods | Medium |
| FastText outperforms BERT embeddings for this task | Medium |

## Next Checks

1. Test the HTL method with languages from different families (e.g., using Japanese or Turkish as source languages) to evaluate its robustness when no linguistically related intermediate language is available.

2. Expand the embedding comparison to include multiple BERT-based models (multilingual BERT, language-specific BERT, XLM-R) and different FastText variants (cbow, skip-gram, subword information) to establish more definitive conclusions about embedding effectiveness.

3. Conduct a detailed error analysis comparing the baseline, TL, and HTL outputs to identify specific parsing challenges that transfer learning addresses versus those that remain problematic.