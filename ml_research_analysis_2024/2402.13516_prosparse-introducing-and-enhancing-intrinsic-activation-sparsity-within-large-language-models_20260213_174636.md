---
ver: rpa2
title: 'ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within
  Large Language Models'
arxiv_id: '2402.13516'
source_url: https://arxiv.org/abs/2402.13516
tags:
- sparsity
- activation
- arxiv
- prosparse
- regularization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ProSparse, a method to enhance activation
  sparsity in large language models (LLMs) by converting non-ReLU models into ReLU-activated
  ones without performance degradation. The approach combines three steps: activation
  function substitution, progressive sparsity regularization with increasing L1 factors,
  and activation threshold shifting.'
---

# ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models

## Quick Facts
- arXiv ID: 2402.13516
- Source URL: https://arxiv.org/abs/2402.13516
- Reference count: 40
- Primary result: Achieves 89.32% activation sparsity on LLaMA2-7B while maintaining performance and enabling up to 4.52× inference speedup

## Executive Summary
ProSparse introduces a three-step method to enhance intrinsic activation sparsity in large language models by converting non-ReLU models into ReLU-activated ones without performance degradation. The approach combines activation function substitution, progressive sparsity regularization with increasing L1 factors, and activation threshold shifting. Applied to LLaMA2-7B, LLaMA2-13B, and MiniCPM-1B, ProSparse achieves high sparsity levels (89.32%, 88.80%, and 87.89% respectively) while maintaining comparable performance to their original Swish-activated versions. The method demonstrates significant practical inference acceleration, achieving up to 4.52× speedup using PowerInfer and 2.44× with custom GPU operators.

## Method Summary
ProSparse converts non-ReLU LLMs to ReLU-activated ones through a three-step pipeline: (1) activation function substitution across all FFN layers, (2) progressive sparsity regularization with increasing L1 factors along sine curves through multiple training stages, and (3) FATReLU threshold shifting by moving ReLU activation threshold to a positive value. The method progressively increases regularization factors to avoid radical shifts in activation distribution, applies L1 regularization to force sparsity, and prunes weakly-contributed neurons through threshold shifting. This approach enables intrinsic activation sparsity that approximate acceleration algorithms can exploit for practical inference speedup.

## Key Results
- Achieves 89.32% activation sparsity on LLaMA2-7B, 88.80% on LLaMA2-13B, and 87.89% on MiniCPM-1B
- Maintains comparable performance to original Swish-activated versions across all tested models
- Demonstrates up to 4.52× inference speedup using PowerInfer and 2.44× speedup with custom GPU operators
- Shows progressive regularization achieves higher sparsity than fixed-factor L1 regularization without performance degradation

## Why This Works (Mechanism)

### Mechanism 1
Progressive sparsity regularization achieves higher activation sparsity than fixed-factor L1 regularization without performance degradation. The regularization factor λ increases gradually through multiple stages (warmup + incremental), allowing the model to adapt to shifting activation distributions without radical changes that would harm performance. Smooth, incremental increases in L1 regularization can force higher sparsity while giving the model time to adapt to the changing loss landscape.

### Mechanism 2
Activation threshold shifting with FATReLU increases sparsity by pruning weakly-contributed neurons without performance loss. Shifting ReLU activation threshold from 0 to a positive value t means only activations above t pass through, eliminating low-magnitude activations that contribute little to output while maintaining representational capacity. Many non-zero activation elements have minimal impact on final outputs and can be safely pruned.

### Mechanism 3
ReLU activation function substitution creates intrinsic sparsity by naturally producing zero activations, unlike GELU/Swish. Replacing non-sparse activation functions with ReLU converts non-zero outputs to exact zeros, creating intrinsic activation sparsity that approximate acceleration algorithms can exploit. ReLU's mathematical property of outputting exact zeros for negative inputs creates structural sparsity exploitable for inference acceleration.

## Foundational Learning

- **L1 regularization and sparsity regularization**: Why needed here: ProSparse uses L1 regularization on activation outputs to force sparsity, requiring understanding of how L1 norms create sparse solutions. Quick check question: What mathematical property of L1 regularization encourages sparsity compared to L2 regularization?

- **Progressive/incremental learning schedules**: Why needed here: The progressive increase in regularization factor follows principles similar to learning rate scheduling, requiring understanding of how gradual changes prevent catastrophic shifts. Quick check question: How does a cosine annealing schedule differ from linear scheduling in terms of gradient stability?

- **Activation function properties and neural network dynamics**: Why needed here: Understanding why ReLU creates sparsity versus GELU/Swish requires knowledge of activation function behavior and their impact on neural computation. Quick check question: What happens to gradient flow when ReLU activation is used versus Swish in deep networks?

## Architecture Onboarding

- **Component map**: ReLU activation substitution -> Progressive L1 regularization -> FATReLU threshold shifting -> Sparse GPU operators for acceleration
- **Critical path**: 1. Substitute activation functions across all FFN layers, 2. Apply progressive L1 regularization through multi-stage training, 3. Implement and tune FATReLU threshold shifting, 4. Validate sparsity-performance tradeoff, 5. Implement sparse GPU operators for acceleration
- **Design tradeoffs**: Sparsity vs. performance (higher sparsity may degrade task performance), Training cost vs. runtime efficiency (progressive regularization requires more training tokens), Hardware compatibility (sparse operators must work across different GPU architectures)
- **Failure signatures**: Performance collapse during training (regularization factor increasing too rapidly), Insufficient sparsity gains (incorrect threshold t or inadequate regularization strength), Training instability (learning rate too high when combined with L1 regularization)
- **First 3 experiments**: 1. Baseline comparison: Implement ReLU substitution only (no regularization) and measure baseline sparsity vs. original Swish model, 2. Progressive regularization tuning: Test different λ schedules (fixed vs. progressive) with same total regularization "budget", 3. Threshold sensitivity: Sweep FATReLU threshold t values to find optimal balance between sparsity gain and performance retention

## Open Questions the Paper Calls Out

- How does ProSparse perform on models significantly larger than 13B parameters (e.g., 70B or 175B)? The authors acknowledge this as a limitation and suggest that more comprehensive studies on huge-scale models should be included in the future.

- What is the optimal strategy for determining the threshold value t in FATReLU activation threshold shifting for different model scales and tasks? The paper mentions that t must be carefully chosen but doesn't provide a systematic method for determining optimal thresholds.

- Can the sparsity regularization technique be extended to attention layers in addition to FFN layers to achieve higher overall sparsity and acceleration? The authors acknowledge this limitation, noting that they only focus on sparsity-based acceleration of FFN layers.

## Limitations

- Progressive regularization schedule lacks precise specification of λ values and stage durations, affecting reproducibility
- FATReLU threshold optimization appears empirical rather than theoretically grounded
- Methodology primarily validated on specific model architectures (LLaMA2, MiniCPM) and may not generalize universally to all LLM architectures

## Confidence

- **High confidence**: The core mechanism of ReLU substitution creating intrinsic sparsity is well-established and mathematically sound
- **Medium confidence**: The progressive regularization approach shows empirical effectiveness but lacks theoretical guarantees for optimal scheduling
- **Medium confidence**: The FATReLU threshold optimization works empirically but the optimal threshold determination method needs further validation

## Next Checks

1. Test ProSparse on additional model architectures (e.g., OPT, BLOOM) to verify generalization across different LLM families
2. Conduct ablation studies isolating each ProSparse component (ReLU substitution, progressive regularization, FATReLU) to quantify individual contributions to sparsity gains
3. Evaluate the approach's effectiveness on smaller models (< 1B parameters) to determine scalability limits and practical utility across the full range of LLM sizes