---
ver: rpa2
title: 'No More Distractions: an Adaptive Up-Sampling Algorithm to Reduce Data Artifacts'
arxiv_id: '2401.13907'
source_url: https://arxiv.org/abs/2401.13907
tags:
- data
- label
- tokens
- token
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a simple adaptive up-sampling method to mitigate
  data artifacts (spurious correlations) in natural language inference datasets like
  SNLI. The authors analyze the SNLI training set, identifying tokens that strongly
  correlate with specific labels, and train an ELECTRA-small model that learns these
  spurious correlations rather than true semantic reasoning.
---

# No More Distractions: an Adaptive Up-Sampling Algorithm to Reduce Data Artifacts

## Quick Facts
- **arXiv ID**: 2401.13907
- **Source URL**: https://arxiv.org/abs/2401.13907
- **Reference count**: 3
- **Primary result**: Adaptive up-sampling of biased tokens reduces spurious correlations in NLI datasets, improving model accuracy by 0.52% overall and 0.29% on corrected tokens.

## Executive Summary
This paper proposes an adaptive up-sampling method to mitigate data artifacts (spurious correlations) in natural language inference datasets like SNLI. The authors identify tokens that strongly correlate with specific labels and apply round-robin oversampling of minority-label examples for these tokens until label distributions become approximately uniform. This approach increases training data size by 7.5% and significantly reduces data artifacts. The corrected training data yields improved model performance, with overall accuracy increasing from 89.15% to 89.67%, and accuracy on the subset containing corrected tokens rising from 92.94% to 93.23%.

## Method Summary
The authors analyze the SNLI training set to identify tokens that strongly correlate with specific labels, revealing spurious correlations that cause models to learn superficial patterns rather than true semantic reasoning. They propose the Adaptive Up-Sampling Data Artifacts Correction (AUDAC) algorithm, which performs round-robin oversampling of minority-label examples for the most biased tokens until label distributions become approximately uniform. By applying AUDAC to the top 10 biased tokens in SNLI, the training data size increases from 550,152 to 597,580 examples. The method demonstrates that simple data-level corrections can effectively reduce spurious correlations without requiring manual annotation or introducing additional bias.

## Key Results
- AUDAC algorithm increases SNLI training data by 7.5% (550,152 → 597,580 examples)
- Overall model accuracy improves from 89.15% to 89.67%
- Accuracy on subset containing corrected tokens improves from 92.94% to 93.23%

## Why This Works (Mechanism)
The AUDAC algorithm addresses spurious correlations by balancing the label distribution for biased tokens. When certain tokens appear predominantly with one label, models learn to associate those tokens with that label regardless of actual semantic meaning. By up-sampling minority-label examples for these tokens, AUDAC creates a more balanced training distribution that forces the model to learn genuine semantic relationships rather than token-label associations.

## Foundational Learning
- **Token-label correlation analysis**: Understanding how individual tokens correlate with specific labels is essential for identifying spurious patterns that models exploit.
  - *Why needed*: Reveals superficial patterns in training data that undermine model generalization
  - *Quick check*: Compute Pearson correlation between token presence and label distribution

- **Round-robin oversampling**: A technique for balancing class distributions by iteratively sampling from underrepresented classes.
  - *Why needed*: Prevents overfitting to majority class while maintaining data diversity
  - *Quick check*: Verify that after oversampling, class frequencies are approximately equal

- **Data artifact identification**: The process of detecting spurious correlations and biases in training datasets.
  - *Why needed*: Essential first step for any data-level intervention strategy
  - *Quick check*: Compare token frequencies across different label classes

## Architecture Onboarding
- **Component map**: SNLI dataset -> Token correlation analysis -> AUDAC algorithm -> Balanced dataset -> Model training
- **Critical path**: Token correlation analysis → AUDAC oversampling → Model training → Evaluation
- **Design tradeoffs**: Simple up-sampling vs. complex de-biasing methods; computational cost vs. performance gain
- **Failure signatures**: If AUDAC increases training size without improving accuracy, or if new biases are introduced during oversampling
- **First experiments**: 1) Run correlation analysis on SNLI to identify top-10 biased tokens, 2) Apply AUDAC to these tokens and measure size increase, 3) Train model on corrected data and compare performance metrics

## Open Questions the Paper Calls Out
None

## Limitations
- The method's scalability to larger token sets and other NLP tasks remains untested
- Reliance on correlation analysis may miss more subtle or complex spurious patterns
- The 7.5% data increase is computationally manageable but may become prohibitive at larger scales

## Confidence
- **High Confidence**: The core mechanism of adaptive up-sampling for reducing label imbalance for biased tokens is well-established and empirically validated on SNLI
- **Medium Confidence**: The generalizability of this approach to other NLP tasks and domains beyond NLI remains unproven
- **Medium Confidence**: The claim that this method reduces spurious correlations without introducing new biases needs broader validation across diverse datasets

## Next Checks
1. Apply AUDAC to other benchmark NLI datasets (MultiNLI, ANLI) to assess cross-dataset effectiveness and identify potential domain-specific limitations
2. Evaluate model robustness to adversarial examples before and after AUDAC application to determine if spurious correlation reduction translates to improved out-of-distribution generalization
3. Test the algorithm's performance on larger token sets (beyond top-10) to establish scalability limits and computational overhead thresholds