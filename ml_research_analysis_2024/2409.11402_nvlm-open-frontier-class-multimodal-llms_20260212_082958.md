---
ver: rpa2
title: 'NVLM: Open Frontier-Class Multimodal LLMs'
arxiv_id: '2409.11402'
source_url: https://arxiv.org/abs/2409.11402
tags:
- image
- multimodal
- arxiv
- training
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces NVLM 1.0, a family of multimodal large language\
  \ models (LLMs) that achieve state-of-the-art performance on vision-language tasks\
  \ while maintaining and even improving text-only performance compared to their LLM\
  \ backbones. The authors propose three distinct architectures\u2014NVLM-D (decoder-only),\
  \ NVLM-X (cross-attention), and NVLM-H (hybrid)\u2014and introduce a 1-D tile-tagging\
  \ design for dynamic high-resolution images, which significantly boosts performance\
  \ on multimodal reasoning and OCR-related tasks."
---

# NVLM: Open Frontier-Class Multimodal LLMs

## Quick Facts
- arXiv ID: 2409.11402
- Source URL: https://arxiv.org/abs/2409.11402
- Reference count: 40
- Key outcome: NVLM achieves frontier-class performance on vision-language tasks while improving text-only math and coding benchmarks by 4.3 points after multimodal training

## Executive Summary
NVLM 1.0 introduces a family of multimodal large language models that achieve state-of-the-art performance on vision-language tasks while maintaining and improving text-only capabilities. The authors propose three distinct architectures—NVLM-D (decoder-only), NVLM-X (cross-attention), and NVLM-H (hybrid)—and introduce a novel 1-D tile-tagging design for handling high-resolution images. Through meticulous dataset curation and architectural innovation, NVLM models achieve frontier-class results, with NVLM-D 72B reaching 853 on OCRBench and 59.7 on MMMU while also improving text-only math and coding performance by 4.3 points.

## Method Summary
NVLM employs a two-stage training approach: multimodal pretraining (freezing both LLM and vision encoder to train modality-alignment modules) followed by supervised fine-tuning (freezing vision encoder to train LLM and alignment modules). The models use InternViT-6B as a frozen vision encoder and Qwen2-72B-Instruct as the LLM backbone. A key innovation is the 1-D tile-tagging system for dynamic high-resolution images, which uses simple text tags to inform spatial structure without requiring 2-D positional information. The authors carefully curate both pretraining and SFT datasets, finding that quality and task diversity matter more than scale, even during pretraining.

## Key Results
- NVLM-D 72B achieves 853 on OCRBench and 59.7 on MMMU, rivaling leading proprietary and open-access models
- Text-only performance improves by 4.3 points on average after multimodal training, particularly in math and coding tasks
- Cross-attention architecture shows better computational efficiency for high-resolution images compared to decoder-only approach
- 1-D tile-tagging design significantly boosts OCR and reasoning performance for dynamic high-resolution images

## Why This Works (Mechanism)

### Mechanism 1
Decoder-only multimodal LLMs suffer text-only performance degradation after multimodal training due to catastrophic forgetting of text capabilities. When multimodal SFT data dominates training, the LLM updates parameters primarily for vision-language tasks, gradually overwriting text-specific representations and fine-tuning adaptations.

### Mechanism 2
Cross-attention-based models achieve better computational efficiency than decoder-only models when handling high-resolution images. Cross-attention processes image tokens through separate attention layers rather than unrolling all tokens through the full LLM decoder, reducing sequence length and memory consumption.

### Mechanism 3
Tile tagging with 1-D tags significantly improves OCR and reasoning performance for dynamic high-resolution images. Text-based tile tags inform the LLM about the spatial structure and position of image tiles without requiring 2-D positional information, enabling better localization and reasoning across tile boundaries.

## Foundational Learning

- **Vision encoder pretraining on large-scale image-text pairs**: Provides strong visual feature extraction that can generalize across diverse tasks without requiring additional vision encoder training. *Quick check*: Why does the paper keep the vision encoder frozen during all training stages?

- **Dynamic high-resolution tiling for handling variable image sizes**: Enables efficient processing of high-resolution images by dividing them into manageable tiles while maintaining global context through thumbnail tiles. *Quick check*: How does the dynamic tiling approach differ from using specialized high-resolution vision encoders?

- **Catastrophic forgetting in continual learning**: Explains why decoder-only models lose text capabilities during multimodal training and why text-only data must be blended in. *Quick check*: What mechanism prevents text performance degradation in cross-attention models?

## Architecture Onboarding

- **Component map**: Vision encoder (frozen) -> modality-alignment module (MLP projector, gated cross-attention, or hybrid) -> LLM decoder -> output generation
- **Critical path**: Vision encoder → modality-alignment module → LLM decoder → output generation
- **Design tradeoffs**: Decoder-only: unified processing but computational inefficiency with high-resolution images; Cross-attention: computational efficiency but additional parameters and complexity; Hybrid: balances both approaches but adds architectural complexity
- **Failure signatures**: Text performance degradation (insufficient text-only data in SFT), poor OCR results (missing or ineffective tile tagging), slow training (inappropriate architecture choice for image resolution)
- **First 3 experiments**: 1) Compare text-only performance of decoder-only model with and without text-only SFT data; 2) Measure computational efficiency differences between NVLM-X and NVLM-D on high-resolution images; 3) Test different tile tagging formats (1-D vs 2-D) on OCRBench performance

## Open Questions the Paper Calls Out

### Open Question 1
What is the exact relationship between dataset quality and model performance across different architectures (decoder-only, cross-attention, hybrid) during multimodal pretraining? The paper states dataset quality and task diversity are more important than scale, but lacks quantitative analysis comparing performance differences between high-quality vs. lower-quality datasets across the three architectures during pretraining.

### Open Question 2
How does the 1-D tile-tagging design specifically impact the model's ability to reason about spatial relationships in high-resolution images compared to other tile-tagging approaches? The paper introduces the 1-D approach and shows it performs better than 2-D grid or bounding box tags, but doesn't provide detailed analysis of why this approach is superior for reasoning tasks.

### Open Question 3
What are the specific mechanisms by which multimodal training improves text-only performance, particularly in math and coding tasks? The paper notes improved text-only math and coding benchmarks after multimodal training but doesn't provide detailed analysis of the transfer learning mechanisms or explain why multimodal math data specifically enhances text-only math reasoning capabilities.

## Limitations

- Dataset quality vs. scale comparison only tested at single scale (20K steps), making it difficult to isolate true impact
- Architecture comparisons conducted on different model sizes (72B vs 34B), introducing confounding variables
- Text performance improvements depend heavily on specific text-only to multimodal SFT data ratio (1:4), not thoroughly explored
- Proprietary model comparisons lack full transparency in evaluation methodology

## Confidence

**High Confidence**: Decoder-only models experience text-only performance degradation during multimodal training due to catastrophic forgetting
**Medium Confidence**: Cross-attention models are more computationally efficient for high-resolution images, but lacks direct efficiency benchmarks
**Low Confidence**: 1-D tile tagging significantly improves OCR performance, but lacks comparison to alternative approaches or ablation studies

## Next Checks

1. **Text Performance Stability Test**: Train NVLM-D 72B with varying ratios of text-only to multimodal SFT data (1:10, 1:4, 1:1, 4:1) and measure corresponding text-only benchmark performance

2. **Architecture Efficiency Benchmark**: Implement both NVLM-D and NVLM-X architectures at same model scale (72B) and measure wall-clock training time, memory consumption, and throughput for processing high-resolution images (1344px)

3. **Tile Tagging Ablation Study**: Create variants of NVLM-D without tile tags, with 2-D positional tags, and with alternative spatial encoding methods, then evaluate OCRBench and MMMU performance across all variants