---
ver: rpa2
title: Linear Recency Bias During Training Improves Transformers' Fit to Reading Times
arxiv_id: '2409.11250'
source_url: https://arxiv.org/abs/2409.11250
tags:
- bias
- attention
- surprisal
- recency
- alibi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether incorporating recency bias into Transformer
  language models can improve their alignment with human reading times. Standard Transformers
  retain complete context, unlike human memory which is lossy.
---

# Linear Recency Bias During Training Improves Transformers' Fit to Reading Times

## Quick Facts
- arXiv ID: 2409.11250
- Source URL: https://arxiv.org/abs/2409.11250
- Authors: Christian Clark; Byung-Doh Oh; William Schuler
- Reference count: 17
- Key outcome: ALiBi with mixed slopes during training and inference significantly improves Transformer fit to human reading times compared to baseline and other recency bias methods

## Executive Summary
This paper investigates whether incorporating recency bias into Transformer language models can improve their alignment with human reading times. Standard Transformers retain complete context, unlike human memory which is lossy. The study introduces ALiBi, a recency bias technique that upweights recent context, and compares it against a standard Transformer and a de Varda-Marelli bias. Results show that ALiBi with mixed slopes during both training and inference achieves the best fit to reading times, significantly outperforming the baseline. Analysis of attention heads suggests that varied slopes enable the model to better track different linguistic dependencies, offering insights into how memory decay mechanisms could improve both language models and cognitive models of sentence processing.

## Method Summary
The researchers trained small-scale Transformer language models (2 layers, 4 heads, 256 embedding size) on The Pile corpus, implementing three variants: a baseline Transformer without recency bias, a model with ALiBi bias (linear bias added to attention scores with mixed slopes), and a model with de Varda-Marelli bias (exponential decay). They tested these models with recency bias applied during both training and inference, or only during inference. The models were evaluated on six reading time corpora using linear mixed-effects regression to measure how well surprisal estimates predicted reading times, with performance measured by improvement in log-likelihood (ΔLogLik) over baseline predictors.

## Key Results
- ALiBi with mixed slopes during both training and inference achieved the best fit to reading times (ΔLogLik = 0.04)
- Training with recency bias (not just inference) dramatically improved ALiBi's performance
- ALiBi significantly outperformed both the baseline Transformer and de Varda-Marelli bias across all six reading time corpora
- Analysis suggests different attention heads with varying slopes specialize in tracking linguistic dependencies at different distances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ALiBi's mixture of slopes allows different attention heads to specialize in tracking linguistic dependencies at varying distances.
- Mechanism: By assigning different decay rates to each head, the model can preserve long-distance dependencies (like coreference) in heads with slower decay while focusing on nearby dependencies (like arguments) in heads with faster decay.
- Core assumption: Different linguistic dependencies have characteristic distances from their dependents, and memory decay should be tuned accordingly.
- Evidence anchors:
  - [abstract] "A follow-up experiment suggests that the varied slopes may enable different attention heads in a model with ALiBi to track different kinds of linguistic dependencies."
  - [section 7] "For coreference dependencies, however, the first Transformer layer shows similar attention scores from the heads with slopes 1/16 and 1/4, and the second layer has the highest mean attention score in the head with slope 1/16. This suggests that the model makes relatively greater use of an attention head with less decay for longer-distance coreference dependencies."

### Mechanism 2
- Claim: Training with recency bias (not just inference) allows the model to learn probability distributions that are compatible with the biased attention mechanism.
- Mechanism: During training, the model learns to predict tokens using the same recency-biased attention it will use at inference time, avoiding a mismatch between training and inference distributions.
- Core assumption: The probability estimates learned during training are sensitive to the attention mechanism used during training.
- Evidence anchors:
  - [section 5] "Experiment 2 therefore compares the LMs tested in the previous experiment with a similar set of LMs that include recency bias during both training and inference."
  - [section 5.2] "Including recency bias during training, instead of inference only, slightly decreases the ∆LogLik from the dVM bias, but dramatically increases performance from ALiBi."

### Mechanism 3
- Claim: Recency bias in attention scores implements a form of memory decay that brings Transformers closer to human-like processing, improving their fit to reading times.
- Mechanism: By upweighting recent context and downweighting distant context, the model's probability estimates become more aligned with human expectations that are subject to memory decay.
- Core assumption: Human reading times are influenced by memory decay effects, and models without such decay will have poorer fit to reading times.
- Evidence anchors:
  - [abstract] "However, standard Transformers work with a lossless representation of the entire previous linguistic context, unlike models of human language processing that include memory decay."
  - [section 3.2] "Such a bias brings Transformers more in line with cognitive models that include some notion of decay or lossy context."

## Foundational Learning

- Concept: Transformer attention mechanism
  - Why needed here: Understanding how ALiBi modifies attention scores requires knowledge of the base Transformer attention mechanism.
  - Quick check question: How are attention scores calculated in a standard Transformer layer before softmax is applied?

- Concept: Linear vs. exponential decay in attention
  - Why needed here: ALiBi uses linear bias added to attention scores, which becomes exponential decay after softmax; understanding this distinction is crucial for interpreting the results.
  - Quick check question: If a linear bias of the form m*(j-i) is added to attention scores, what is the functional form of the decay in the final attention weights after softmax?

- Concept: Mixed-effects regression modeling
  - Why needed here: The paper uses linear mixed-effects models to evaluate the fit of surprisal estimates to reading times, accounting for random effects of subjects and sentences.
  - Quick check question: What is the purpose of including by-subject random slopes for word position and surprisal in the regression models?

## Architecture Onboarding

- Component map:
  Base Transformer language model -> ALiBi bias implementation -> Different slope values per attention head -> Training loop with recency bias -> Evaluation pipeline with linear mixed-effects models

- Critical path:
  1. Initialize Transformer with specified architecture
  2. Implement ALiBi bias with mixed slopes
  3. Train model on Pile corpus with ALiBi bias applied throughout
  4. Generate surprisal estimates on reading time corpora
  5. Fit linear mixed-effects models to evaluate fit to reading times
  6. Analyze attention head behavior for different dependency types

- Design tradeoffs:
  - Mixed slopes vs. uniform slope: Mixed slopes allow specialization but increase complexity; uniform slope is simpler but may not capture dependency distance patterns as well.
  - Training with bias vs. inference only: Training with bias ensures compatibility but may limit flexibility; inference only allows testing of post-hoc modifications but may create mismatch.

- Failure signatures:
  - Poor fit to reading times despite ALiBi implementation
  - No difference in attention head behavior across slopes
  - Degradation in language modeling perplexity
  - Inability to converge during training

- First 3 experiments:
  1. Train a baseline Transformer without recency bias and evaluate fit to reading times
  2. Implement ALiBi with mixed slopes and train with bias during both training and inference, then evaluate
  3. Implement ALiBi with uniform slope and compare performance to mixed slopes version

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the improvement from ALiBi's mixed slopes depend on the specific slope values or their relative distribution?
- Basis in paper: [explicit] The authors note that mixed slopes outperform uniform slopes but don't test whether the specific decay rates or their relative proportions matter.
- Why unresolved: The study only tests a fixed set of slopes (2^-8h/H) and compares against uniform slopes, but doesn't explore the parameter space systematically.
- What evidence would resolve it: A systematic grid search over different slope combinations and distributions, measuring reading time fit across multiple corpora.

### Open Question 2
- Question: How does ALiBi's performance compare to other memory decay mechanisms like explicit forgetting buffers or adaptive context truncation?
- Basis in paper: [inferred] The authors contrast ALiBi with a simple exponential decay (dVM bias) but don't compare against other cognitive-inspired memory mechanisms.
- Why unresolved: The study focuses only on ALiBi and one alternative bias mechanism, leaving open whether more complex memory models might perform better.
- What evidence would resolve it: Direct comparison between ALiBi and models implementing other memory decay strategies (e.g., ACT-R-like activation decay, context window truncation based on attention entropy).

### Open Question 3
- Question: Do different attention heads with varying slopes learn to specialize in specific dependency types beyond the three tested (first/second arguments, coreference)?
- Basis in paper: [explicit] The authors find that heads with different slopes show sensitivity to different dependency distances, but only test three specific dependency types.
- Why unresolved: The study's dependency analysis is limited to three types, potentially missing other linguistic phenomena that heads might specialize in.
- What evidence would resolve it: A comprehensive analysis of attention head specialization across multiple dependency types (e.g., control, raising, coordination, long-distance dependencies) using dependency parsing or syntactic annotations.

## Limitations
- Small model size (2 layers, 4 heads) and limited training data (2M tokens) may not reflect larger model behavior
- Analysis of attention head behavior based on single model instance without statistical validation across multiple runs
- Does not explore whether observed effects scale with model size or training data

## Confidence
- **High confidence**: The core finding that ALiBi with mixed slopes during both training and inference significantly improves fit to reading times
- **Medium confidence**: The interpretation that mixed slopes allow attention heads to specialize for different dependency distances
- **Low confidence**: The claim that training with recency bias is essential for good performance

## Next Checks
1. **Replication with larger models**: Train ALiBi variants with 4-12 layers and 8-32 heads on full Pile corpus to verify scaling properties of the recency bias effects.
2. **Statistical validation of attention patterns**: Conduct multiple training runs and quantify the consistency of attention head specialization across models, including statistical tests for dependency distance effects.
3. **Alternative memory decay mechanisms**: Implement exponential decay variants of ALiBi and compare performance to the linear bias, testing whether the specific decay form is critical or if the general principle of recency weighting is sufficient.