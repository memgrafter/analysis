---
ver: rpa2
title: 'FORML: A Riemannian Hessian-free Method for Meta-learning on Stiefel Manifolds'
arxiv_id: '2402.18605'
source_url: https://arxiv.org/abs/2402.18605
tags:
- riemannian
- optimization
- learning
- forml
- meta-learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FORML, a first-order Hessian-free method
  for meta-learning on Stiefel manifolds. The method addresses the computational complexity
  of Riemannian meta-learning, which requires second-order derivatives and involves
  expensive Riemannian operations.
---

# FORML: A Riemannian Hessian-free Method for Meta-learning on Stiefel Manifolds

## Quick Facts
- **arXiv ID:** 2402.18605
- **Source URL:** https://arxiv.org/abs/2402.18605
- **Reference count:** 40
- **Primary result:** FORML achieves competitive few-shot classification accuracy with 3-20x computational speedup over second-order Riemannian methods

## Executive Summary
This paper introduces FORML, a first-order Hessian-free method for meta-learning on Stiefel manifolds that addresses the computational complexity of traditional Riemannian meta-learning approaches. By replacing expensive second-order derivatives with a first-order approximation using Kronecker sums, FORML significantly reduces computational load while maintaining competitive performance on few-shot classification tasks. The method enforces orthogonality constraints on the classification layer by formulating it on the Stiefel manifold, which improves representation reuse and acts as regularization to prevent overfitting.

## Method Summary
FORML is a first-order approximation method for meta-learning on Stiefel manifolds that avoids expensive second-order Hessian computations. The method places the last fully-connected classification layer on the Stiefel manifold, enforcing orthogonality constraints through Riemannian gradient descent with retraction and projection operations. Inner-loop updates use gradient descent on the Stiefel manifold while outer-loop updates employ a first-order approximation formula using Kronecker sums to avoid nested differentiation. The algorithm is evaluated on few-shot classification tasks using mini-ImageNet, CUB, and FC100 datasets with various backbone networks including Conv-4, Conv-6, ResNet-10, and ResNet-18.

## Key Results
- FORML achieves 3-20x speedup in runtime and memory usage compared to its non-approximation-based Riemannian counterpart (RMAML)
- FORML outperforms its non-Riemannian counterpart (MAML) in terms of classification accuracy on standard few-shot benchmarks
- The method demonstrates competitive performance against state-of-the-art methods while maintaining significant computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FORML replaces expensive second-order Riemannian backpropagation with a first-order approximation that avoids Hessian computations while preserving gradient direction accuracy.
- Mechanism: The method approximates the differential of the retraction operator on the Stiefel manifold using Kronecker sums of matrices, enabling first-order updates without nested differentiation through retraction and projection steps.
- Core assumption: The retraction operator on the Stiefel manifold can be locally approximated by a first-order Taylor expansion that preserves orthogonality constraints while maintaining convergence properties.
- Evidence anchors:
  - [abstract] "introduces a Hessian-free approach that uses a first-order approximation of derivatives on the Stiefel manifold"
  - [section] "By ignoring those terms and using vec(·) operator, the derivatives with respect to Θ which is an orthogonal matrix (a point on Stiefel manifold) can be written as: vec(d(Θ(t)))−0.5α vec(d(Θ(t))(∇Θ(t) Li)T Θ(t))−0.5α vec(Θ(t)(∇Θ(t) Li)T d(Θ(t)))"
  - [corpus] Weak corpus evidence - no direct citations found, indicating novelty
- Break condition: If the first-order approximation significantly deviates from the true Riemannian gradient direction, convergence to optimal solutions may fail, particularly on highly curved manifold regions.

### Mechanism 2
- Claim: Enforcing orthogonality constraints on the classification layer parameters improves representation reuse and acts as regularization to prevent overfitting.
- Mechanism: By placing the last fully-connected layer on the Stiefel manifold, the weight matrix remains orthonormal, effectively computing cosine similarities between inputs and class prototypes, which enhances transfer of learned representations across tasks.
- Core assumption: Orthonormal weight matrices provide better class separation and generalization than unconstrained Euclidean counterparts, particularly in few-shot scenarios where data is limited.
- Evidence anchors:
  - [abstract] "using a Stiefel fully-connected layer that enforces orthogonality constraint on the parameters of the last classification layer as the head of the backbone network, strengthens the representation reuse of the gradient-based meta-learning methods"
  - [section] "normalizing the input of the head leads to computing the cosine similarities between the input and the output classes, which results in increasing the representation reuse phenomena"
  - [corpus] Weak corpus evidence - limited direct citations for this specific combination
- Break condition: If the orthogonality constraint becomes too restrictive for the specific classification task, it may limit the model's ability to learn optimal decision boundaries, potentially degrading performance on complex datasets.

### Mechanism 3
- Claim: First-order Riemannian meta-learning achieves competitive performance with significantly reduced computational complexity and memory footprint compared to second-order approaches.
- Mechanism: By avoiding second-order Hessian computations and using efficient Kronecker sum approximations, FORML reduces both the computational load per iteration and the memory required to store intermediate gradient computations.
- Core assumption: The computational savings from avoiding second-order operations outweigh any potential accuracy loss from first-order approximation, particularly when combined with the regularization benefits of orthogonality constraints.
- Evidence anchors:
  - [abstract] "significantly reduces the computational load and memory footprint" and "Our experimental results across various few-shot learning datasets, demonstrate the superiority of our proposed method compared to the state-of-the-art methods"
  - [section] "FORML shows a significant improvement over its non-Riemannian counterpart (i.e MAML) in terms of classification accuracy" and "FORML demonstrates a significant improve over its non-approximation-based version, RMAML, by performing an epoch and an inner-level optimization loop more than 3 times and 20 times faster, respectively"
  - [corpus] Moderate corpus evidence with 5 related papers averaging 0.444 FMR score
- Break condition: If the computational savings are insufficient to offset the overhead of Riemannian operations, or if the first-order approximation becomes too coarse for complex optimization landscapes.

## Foundational Learning

- Concept: Riemannian manifolds and optimization on manifolds
  - Why needed here: Understanding the geometric structure where parameters reside and how standard gradient descent must be adapted to respect manifold constraints is fundamental to implementing FORML
  - Quick check question: What are the three key Riemannian operations (orthogonal projection, retraction, parallel transport) and how do they differ from Euclidean gradient descent?

- Concept: Stiefel manifolds and orthogonality constraints
  - Why needed here: The Stiefel manifold specifically enforces orthonormal matrices, which is crucial for understanding why placing the classification layer here computes cosine similarities and how this affects representation reuse
  - Quick check question: How does the constraint P^T P = I on the Stiefel manifold relate to computing cosine similarity between input vectors and class prototypes?

- Concept: First-order approximation of second-order derivatives
  - Why needed here: Understanding how FORML avoids Hessian computations through Kronecker sum approximations is essential for both implementation and debugging
  - Quick check question: How does the Kronecker sum (A ⊕ B) = A ⊗ I + I ⊗ B simplify the computation of the first-order approximation compared to explicit second-order differentiation?

## Architecture Onboarding

- Component map: Backbone network (Conv-4, Conv-6, ResNet variants) with parameters in Euclidean space → Stiefel manifold-constrained classification layer → Riemannian gradient descent with retraction → First-order approximation using Kronecker sums
- Critical path: Forward pass through backbone → Normalize input to Stiefel layer → Compute cosine similarity using orthogonal weights → Loss computation → Inner-loop Riemannian gradient descent with retraction → Outer-loop first-order approximation update using Kronecker sums → Repeat
- Design tradeoffs: Using Stiefel constraints provides regularization and representation reuse benefits but may limit flexibility in learning optimal decision boundaries. The first-order approximation reduces computational cost but may sacrifice some accuracy compared to second-order methods. The choice of which layer(s) to place on the manifold affects both performance and computational requirements.
- Failure signatures: If orthogonality constraints are not properly maintained, weights may drift off the manifold leading to invalid cosine similarity computations. If the first-order approximation is too coarse, meta-learning may converge to suboptimal solutions. If Riemannian operations are incorrectly implemented, gradients may point in wrong directions causing divergence.
- First 3 experiments:
  1. Verify orthogonality constraint: Implement a test that checks P^T P = I for the Stiefel layer weights after each update to ensure manifold constraints are maintained
  2. Compare gradient directions: Compute both exact Riemannian gradients and first-order approximations for a simple task to verify the approximation quality and identify when it breaks down
  3. Measure computational overhead: Profile the runtime of Riemannian operations (retraction, projection) versus their Euclidean counterparts to quantify the computational savings from the first-order approximation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FORML's performance compare to other state-of-the-art meta-learning methods on even more challenging few-shot learning benchmarks beyond FC100?
- Basis in paper: [inferred] The paper demonstrates FORML's superiority on FC100 but suggests potential for further testing on even more challenging datasets.
- Why unresolved: The paper does not provide results on benchmarks more challenging than FC100, such as those with larger domain gaps or more complex tasks.
- What evidence would resolve it: Conducting experiments on additional challenging few-shot learning datasets and comparing FORML's performance against other state-of-the-art methods would provide the necessary evidence.

### Open Question 2
- Question: Can FORML be effectively extended to multi-modal or multi-task Riemannian meta-learning scenarios?
- Basis in paper: [explicit] The authors mention the intention to develop a multi-modal or multi-task Riemannian meta-learning method in the future.
- Why unresolved: The paper does not explore or provide results on multi-modal or multi-task extensions of FORML.
- What evidence would resolve it: Developing and testing a multi-modal or multi-task version of FORML and evaluating its performance on relevant datasets would provide the necessary evidence.

### Open Question 3
- Question: How does FORML's performance scale with the size of the model (e.g., deeper networks or larger datasets)?
- Basis in paper: [inferred] The paper demonstrates FORML's effectiveness on various backbone networks but does not explore scaling to significantly larger models or datasets.
- Why unresolved: The paper does not provide results on scaling FORML to larger models or datasets, which could reveal potential limitations or advantages.
- What evidence would resolve it: Conducting experiments with deeper networks or larger datasets and comparing FORML's performance against other methods would provide the necessary evidence.

## Limitations

- The paper lacks comprehensive ablation studies examining the individual contributions of Stiefel manifold constraints versus first-order approximation
- Performance is only evaluated on classification tasks, leaving questions about applicability to other meta-learning domains like regression or reinforcement learning
- The exact performance degradation from avoiding second-order derivatives remains unclear despite demonstrated computational savings

## Confidence

- FORML's computational efficiency gains: **High confidence** - Supported by direct runtime comparisons showing 3-20x speedups over RMAML
- Performance improvements over MAML: **Medium confidence** - Demonstrated on standard benchmarks but lacks ablation studies isolating the orthogonality effect
- First-order approximation accuracy: **Medium confidence** - The Kronecker sum approach is theoretically sound but practical approximation quality varies with problem complexity

## Next Checks

1. Conduct ablation studies comparing FORML with: (a) standard MAML with Stiefel constraints, (b) FORML without Stiefel constraints, and (c) second-order Riemannian meta-learning to isolate each mechanism's contribution
2. Test FORML on non-classification meta-learning tasks (e.g., regression or few-shot reinforcement learning) to evaluate generalizability beyond the demonstrated scope
3. Perform sensitivity analysis on the first-order approximation parameters to determine when the approximation breaks down and how it affects convergence rates across different manifold curvatures