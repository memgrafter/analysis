---
ver: rpa2
title: 'Understanding Fairness-Accuracy Trade-offs in Machine Learning Models: Does
  Promoting Fairness Undermine Performance?'
arxiv_id: '2411.17374'
source_url: https://arxiv.org/abs/2411.17374
tags:
- fairness
- human
- consistency
- decisions
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates fairness-accuracy trade-offs in university
  admissions using a real-world dataset of 870 applicant profiles. Three ML models
  (XGBoost, Bi-LSTM, and KNN) were compared against human evaluators across three
  decision stages: shortlisting, admission recommendation, and final offers.'
---

# Understanding Fairness-Accuracy Trade-offs in Machine Learning Models: Does Promoting Fairness Undermine Performance?

## Quick Facts
- arXiv ID: 2411.17374
- Source URL: https://arxiv.org/abs/2411.17374
- Reference count: 18
- Key outcome: ML models achieve 14.08%-18.79% higher fairness consistency than human evaluators in university admissions while maintaining high accuracy

## Executive Summary
This study evaluates fairness-accuracy trade-offs in university admissions using a real-world dataset of 870 applicant profiles. Three ML models (XGBoost, Bi-LSTM, and KNN) were compared against human evaluators across three decision stages: shortlisting, admission recommendation, and final offers. The research introduces a consistency metric to measure individual fairness, quantifying agreement in decisions among similar applicants. Results show that ML models significantly outperform human evaluators in fairness consistency, with XGBoost achieving 75.21% and 74.77% consistency scores for admission recommendation and offer decisions respectively, while Bi-LSTM reached 80.73% and 77.97%. These scores represent improvements of 14.08% to 18.79% over human decision-making. While human evaluators excelled in precision (84.64% for shortlisting), Bi-LSTM achieved comparable accuracy (82.76%) and F1-score (81.76%) to human decisions. The findings suggest that ML models can enhance fairness consistency in admissions while maintaining high accuracy, advocating for a hybrid approach combining human expertise with ML-driven consistency checks.

## Method Summary
The study uses a real-world university admissions dataset with 870 applicant profiles, featuring academic scores, personal statements, leadership experiences, and offer status labels. Three ML models (XGBoost, Bi-LSTM, and KNN) were implemented with BERT embeddings for textual features. The data was split into 80% training, 10% validation, and 10% test sets. Models were trained with hyperparameter search on the validation set, and performance was evaluated using standard classification metrics plus a novel consistency score measuring individual fairness through agreement among similar applicants.

## Key Results
- ML models achieved 14.08%-18.79% higher fairness consistency than human evaluators across admission stages
- XGBoost: 75.21% consistency for admission recommendation, 74.77% for final offers
- Bi-LSTM: 80.73% consistency for admission recommendation, 77.97% for final offers
- Human evaluators showed highest precision (84.64%) in shortlisting but lower consistency scores (56.32%-60.23%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Consistency metric directly quantifies individual fairness by measuring agreement in decisions among similar applicants.
- Mechanism: The consistency score is computed as one minus the average absolute difference between a classifier's prediction for an applicant and the average prediction for their k-nearest neighbors. This operationalizes the principle that similar individuals should receive similar treatment.
- Core assumption: Nearest neighbors in the embedding space represent truly similar applicants with comparable profiles.
- Evidence anchors: [abstract] "To evaluate individual fairness, we introduce a consistency metric that quantifies agreement in decisions among ML models and human experts with diverse backgrounds."

### Mechanism 2
- Claim: ML models can achieve higher fairness consistency than human evaluators by reducing cognitive bias and subjective judgment.
- Mechanism: ML models apply standardized evaluation patterns across similar cases, whereas human evaluators introduce variability through subjective judgment and cognitive biases, especially in later decision stages.
- Core assumption: The models are trained on representative data and don't inherit or amplify existing biases in the dataset.
- Evidence anchors: [abstract] "Our analysis reveals that ML models surpass human evaluators in fairness consistency by margins ranging from 14.08% to 18.79%."

### Mechanism 3
- Claim: Hybrid approach combining human expertise with ML-driven consistency checks optimizes both performance and fairness in admission decisions.
- Mechanism: ML models handle initial screening with high consistency while humans apply nuanced judgment in complex cases, with ML serving as a consistency check to identify potential biases in human decisions.
- Core assumption: Human and ML strengths are complementary - humans excel at contextual understanding while ML excels at consistent pattern application.
- Evidence anchors: [abstract] "Our findings highlight the potential of using ML to enhance fairness in admissions while maintaining high accuracy, advocating a hybrid approach combining human judgement and ML models."

## Foundational Learning

- Concept: Individual fairness principle
  - Why needed here: The paper's core contribution is operationalizing individual fairness through consistency metrics in university admissions
  - Quick check question: What is the key difference between individual fairness and group fairness in the context of this paper?

- Concept: Nearest neighbor consistency computation
  - Why needed here: Understanding how the consistency metric works is crucial for interpreting the results and potentially implementing similar systems
  - Quick check question: How does changing the value of k in the k-nearest neighbors affect the stability and sensitivity of the consistency score?

- Concept: BERT embeddings for semi-structured text
  - Why needed here: The paper uses BERT to encode textual features (PIQ and Leadership Experience) into numerical representations for ML models
  - Quick check question: Why might the [CLS] token embedding be chosen as the representation for each feature rather than other pooling strategies?

## Architecture Onboarding

- Component map: Raw applicant profiles → BERT embedding generation → Feature concatenation → ML models (XGBoost, Bi-LSTM, KNN) → Classification metrics + Consistency metric → Human decision comparison

- Critical path: Data preprocessing → Model training/validation → Consistency evaluation → Performance comparison

- Design tradeoffs:
  - Model complexity vs. interpretability: XGBoost offers better interpretability than Bi-LSTM
  - Embedding granularity: Using individual feature embeddings vs. combined document embedding
  - k value selection: 5 neighbors balances local similarity with statistical stability

- Failure signatures:
  - Low consistency scores despite high accuracy: Indicates inconsistency in treating similar cases
  - High variance between human decision stages: Suggests cognitive bias increases with decision complexity
  - Model performance degradation on validation set: Potential overfitting to training data biases

- First 3 experiments:
  1. Compare consistency scores using different k values (3, 5, 7) to assess sensitivity
  2. Evaluate model performance when trained on subsets of features to identify which features most impact fairness consistency
  3. Test whether retraining models on debiased versions of the dataset improves both accuracy and consistency metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of cognitive biases (such as confirmation bias or halo effect) specifically manifest across different stages of the admission process?
- Basis in paper: [explicit] The paper identifies cognitive bias in human decisions but does not specify which types of cognitive biases are most prevalent or how they vary across decision stages.
- Why unresolved: The paper only notes the existence of cognitive bias through lower consistency scores without characterizing the specific nature of these biases.
- What evidence would resolve it: Detailed qualitative analysis of decision patterns, expert interviews, or controlled experiments identifying specific bias types at each decision stage.

### Open Question 2
- Question: What is the optimal hybrid approach that combines human expertise with ML-driven consistency checks, and how should decision authority be distributed between humans and models?
- Basis in paper: [explicit] The paper advocates for a hybrid approach but does not specify the optimal balance or decision authority distribution between human and ML components.
- Why unresolved: While the paper demonstrates that ML improves consistency, it does not provide a framework for integrating human and ML decision-making effectively.
- What evidence would resolve it: Comparative studies testing different hybrid models with varying degrees of human and ML authority, measuring both fairness and overall admission quality.

### Open Question 3
- Question: How do the fairness consistency improvements of ML models translate to actual equity outcomes for underrepresented or disadvantaged applicant groups?
- Basis in paper: [inferred] The paper measures consistency across similar profiles but does not examine whether ML models improve outcomes for specific demographic or socioeconomic groups.
- Why unresolved: The consistency metric measures treatment of similar profiles but does not address whether certain groups receive more equitable treatment overall.
- What evidence would resolve it: Demographic analysis of admission outcomes showing changes in acceptance rates and success metrics for different applicant groups when using ML-assisted versus purely human decisions.

### Open Question 4
- Question: How does the performance of ML models change when applied to admission datasets from different universities, countries, or educational systems?
- Basis in paper: [inferred] The study uses a single dataset from one university, suggesting generalizability concerns.
- Why unresolved: The paper demonstrates effectiveness on one dataset but does not test model performance across different contexts or admission criteria.
- What evidence would resolve it: Comparative studies using admission datasets from multiple institutions with different admission criteria, applicant pools, and educational systems.

## Limitations
- The study uses a single dataset from one university, limiting generalizability to different admission contexts
- The consistency metric assumes nearest neighbors in embedding space represent truly similar applicants, which may not capture all relevant fairness dimensions
- The paper does not specify how human evaluators' decisions were collected or whether evaluation criteria were aligned with ML models

## Confidence

**High Confidence:** The claim that ML models achieve higher consistency scores than human evaluators is well-supported by the reported metrics and methodology, though the practical significance depends on the baseline human decision quality.

**Medium Confidence:** The assertion that ML models maintain high accuracy while improving fairness consistency is supported by the results, but the comparison between human precision and ML accuracy may not be directly comparable due to different evaluation criteria.

**Low Confidence:** The recommendation for a hybrid approach combining human expertise with ML consistency checks is presented without empirical validation of this specific integration strategy or evidence of how such a system would perform in practice.

## Next Checks

1. **Dataset Validation:** Obtain the original dataset and verify that the preprocessing steps (particularly BERT embedding generation and feature concatenation) match the paper's description to ensure faithful reproduction.

2. **Consistency Metric Sensitivity:** Test the consistency scores across different k values (3, 5, 7, 10) to determine if the reported improvements over human evaluators are robust to parameter selection.

3. **Human-ML Alignment:** Conduct a validation study where human evaluators apply the same decision rubric used by ML models to assess whether the consistency improvements are due to standardized criteria rather than model superiority.