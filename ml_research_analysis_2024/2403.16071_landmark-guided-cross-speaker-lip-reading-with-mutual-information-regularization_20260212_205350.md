---
ver: rpa2
title: Landmark-Guided Cross-Speaker Lip Reading with Mutual Information Regularization
arxiv_id: '2403.16071'
source_url: https://arxiv.org/abs/2403.16071
tags:
- visual
- speaker
- reading
- features
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles the problem of speaker-independent lip reading
  by proposing a model that uses landmark-guided fine-grained visual features instead
  of full mouth-cropped images, reducing speaker-specific appearance variations. A
  max-min mutual information regularization approach is introduced to encourage learning
  of speaker-insensitive latent representations.
---

# Landmark-Guided Cross-Speaker Lip Reading with Mutual Information Regularization

## Quick Facts
- arXiv ID: 2403.16071
- Source URL: https://arxiv.org/abs/2403.16071
- Authors: Linzhi Wu; Xingyu Zhang; Yakun Zhang; Changyan Zheng; Tiejun Liu; Liang Xie; Ye Yan; Erwei Yin
- Reference count: 0
- Primary result: Achieves 1.83% WER for overlapped speakers and 10.21% WER for unseen speakers on GRID dataset

## Executive Summary
This paper addresses the challenge of speaker-independent lip reading by introducing a landmark-guided visual feature extraction approach combined with mutual information regularization. The method extracts fine-grained visual features using 3D patches centered on lip landmarks rather than full mouth-cropped images, reducing speaker-specific appearance variations. A max-min mutual information regularization approach is proposed to learn speaker-insensitive latent representations. Experiments on the GRID dataset demonstrate superior performance compared to competitive baselines, achieving 1.83% WER for overlapped speakers and 10.21% WER for unseen speakers.

## Method Summary
The approach uses landmark-guided fine-grained visual features instead of mouth-cropped images, extracting 3D patches centered on lip landmarks to reduce speaker-specific appearance variations. A max-min mutual information regularization approach is introduced to learn speaker-insensitive latent representations. The architecture employs a hybrid CTC/attention model with conformer encoder, speaker identification branch, and MI regularization. Training occurs in two stages: first jointly training VSR and speaker ID, then freezing speaker ID and training VSR with MI regularization. The model is evaluated on the GRID dataset with 34 speakers using 20 lip landmarks per frame and grayscale video frames resized to 360×288.

## Key Results
- Achieves 1.83% WER for overlapped speakers and 10.21% WER for unseen speakers on GRID dataset
- Outperforms competitive baselines on both overlapped and unseen speaker splits
- Demonstrates robustness to different patch sizes (24×24 shown effective)
- Shows effectiveness of landmark-guided visual features and MI regularization for cross-speaker generalization

## Why This Works (Mechanism)

### Mechanism 1
Landmark-guided fine-grained visual features reduce speaker-specific appearance variations, improving cross-speaker generalization. By extracting tubelets (3D patches) centered on lip landmarks instead of full mouth-cropped images, the model focuses on local lip movements while discarding global speaker-specific visual traits. Relative landmark positions and inter-frame motion vectors further enrich the representation with speaker-independent dynamics. This assumes lip landmarks are geometrically stable across speakers and capture essential motion patterns without encoding identity.

### Mechanism 2
Max-min mutual information regularization decouples speaker identity features from speech content features in latent space. The model simultaneously minimizes MI between speaker identification features and speech content features while maximizing MI between front-end and back-end encoder features. This encourages learning of speaker-insensitive representations by disentangling speaker identity and speech content in the learned feature space. The effectiveness depends on accurate MI estimation and optimization via neural bounds.

### Mechanism 3
Hybrid CTC/attention architecture with conformer encoder balances monotonic alignment and long-range dependencies for robust lip reading. CTC loss enforces monotonic alignment between input frames and output tokens, while attention-based decoder captures complex dependencies. Conformer combines self-attention and convolution to model both global and local temporal dynamics. This assumes the video-to-text mapping satisfies monotonic alignment assumptions and the hybrid architecture can jointly optimize both objectives.

## Foundational Learning

- Concept: Mutual Information Estimation
  - Why needed here: To quantify and regularize the dependence between speaker identity and speech content features
  - Quick check question: What is the difference between the vCLUB upper bound and the Jensen-Shannon lower bound used in the paper?

- Concept: Lip Landmark Detection and Tracking
  - Why needed here: To provide stable geometric cues for extracting fine-grained visual features
  - Quick check question: How many landmarks are used, and why are only 20 lip landmarks selected from the full 68?

- Concept: Hybrid CTC/Attention Decoding
  - Why needed here: To balance alignment accuracy and sequence modeling flexibility in lip reading
  - Quick check question: What role does the λ hyperparameter play in the joint training objective?

## Architecture Onboarding

- Component map: Video frames → Face alignment detector → 68 landmarks per frame → 3D patch extraction → Intra-frame relative position encoding → Attentive fusion → Lip motion extraction → Concatenation → Conformer encoder (3 blocks) → CTC loss (auxiliary) → Transformer decoder (3 blocks) → Cross-entropy loss; Speaker identification MLP → Cross-entropy loss; MI estimator networks → Min-max MI loss
- Critical path: Front-end visual features → Conformer encoder → Transformer decoder → Output sequence
- Design tradeoffs: Landmark-guided patches reduce speaker bias but may lose global context; MI regularization adds complexity and training instability risk; Hybrid CTC/attention balances alignment and flexibility but requires careful λ tuning
- Failure signatures: High WER on unseen speakers → front-end not speaker-invariant enough; Training instability → MI estimation bounds too loose or speaker ID module poorly initialized; Overfitting to training speakers → MI regularization not effective or landmark detection noisy
- First 3 experiments: 1) Compare landmark-guided 3D patches vs full mouth crops on overlapped speakers to verify visual clue benefit; 2) Test MI regularization ablation (with/without) on unseen speakers to confirm disentanglement effect; 3) Vary λ in hybrid CTC/attention to find optimal balance between alignment and sequence modeling

## Open Questions the Paper Calls Out
None

## Limitations
- Mutual information regularization lacks direct empirical validation through ablation studies
- No quantitative comparison of landmark-guided features versus traditional mouth-cropped approaches
- GRID dataset has limited vocabulary (51 words) and controlled conditions, limiting real-world generalization
- Evaluation focuses primarily on WER metrics without analyzing failure modes across different speakers

## Confidence

**High confidence**: The hybrid CTC/attention architecture and conformer encoder implementation are well-established in the literature, with clear technical specifications provided. The landmark detection and 3D patch extraction methodology follows standard computer vision practices.

**Medium confidence**: The speaker identification branch and its integration with the main VSR model appears technically sound, but the effectiveness of the max-min MI regularization approach is less certain without direct empirical validation.

**Low confidence**: The actual impact of landmark-guided fine-grained features versus full mouth crops on cross-speaker generalization is not directly demonstrated through controlled experiments.

## Next Checks

1. **Ablation Study on MI Regularization**: Conduct controlled experiments comparing the full model with versions without MI regularization on both overlapped and unseen speaker splits. Measure changes in WER and analyze feature space representations to confirm speaker-insensitive learning.

2. **Direct Visual Feature Comparison**: Implement and evaluate a baseline using traditional mouth-cropped images instead of landmark-guided patches on the same architecture. Compare performance metrics to quantify the specific benefit of the landmark-guided approach.

3. **Cross-Dataset Generalization Test**: Evaluate the trained model on a different lip reading dataset (e.g., LRW or LRS2) to assess whether the speaker-insensitive features learned generalize beyond the GRID dataset's controlled conditions.