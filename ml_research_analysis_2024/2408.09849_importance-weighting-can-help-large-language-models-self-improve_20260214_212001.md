---
ver: rpa2
title: Importance Weighting Can Help Large Language Models Self-Improve
arxiv_id: '2408.09849'
source_url: https://arxiv.org/abs/2408.09849
tags:
- answer
- weight
- data
- samples
- valid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of improving large language models
  (LLMs) through self-training, particularly when external supervision is costly.
  While existing self-improvement methods filter generated data based on answer correctness,
  this work highlights the overlooked issue of distribution shift, where even correct
  answers may harm model performance if they deviate significantly from the true data
  distribution.
---

# Importance Weighting Can Help Large Language Models Self-Improve

## Quick Facts
- arXiv ID: 2408.09849
- Source URL: https://arxiv.org/abs/2408.09849
- Reference count: 19
- Self-improvement of LLMs using distribution shift detection via importance weighting improves performance without external rewards

## Executive Summary
This paper addresses the challenge of improving large language models (LLMs) through self-training when external supervision is costly. While existing methods filter generated data based on answer correctness, this work highlights the overlooked issue of distribution shift, where even correct answers may harm model performance if they deviate significantly from the true data distribution. To address this, the authors introduce DS weight, a metric inspired by importance weighting techniques, which approximates the extent of distribution shift using a small validation set. By combining DS weight with self-consistency for answer correctness, the proposed IWSI framework filters out high-DSE samples and fine-tunes the model. Experiments on six datasets across arithmetic reasoning, natural language inference, and commonsense reasoning tasks show that IWSI consistently outperforms baseline self-improvement methods and matches the performance of approaches using external reward models.

## Method Summary
The IWSI framework generates candidate answers using self-consistency, filters them by both correctness (majority voting) and distribution shift extent (DS weight), then fine-tunes the model on the filtered subset. DS weight is computed as a symmetrized ratio of validation loss to training loss, approximating the density ratio between test and training distributions. This allows filtering out correct-but-high-shift samples that would otherwise degrade performance during fine-tuning.

## Key Results
- IWSI consistently outperforms baseline self-improvement methods across six datasets
- IWSI achieves comparable performance to reward-model-based approaches without requiring external supervision
- DS weight filtering is largely orthogonal to correctness filtering, providing complementary value
- Performance gains are robust to validation set composition and size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distribution shift in self-generated data harms self-improvement, and DS weight can detect this.
- Mechanism: DS weight approximates the ratio of test-to-training density by comparing the validation set loss distribution to the training sample loss. Samples with DS weight far from 1 indicate high distribution shift, and removing them improves fine-tuning stability.
- Core assumption: The loss on a tiny validation set is a good proxy for the underlying data distribution; higher relative loss implies greater shift.
- Evidence anchors:
  - [abstract] "we propose a new metric called DS weight to approximate DSE, inspired by the Importance Weighting methods"
  - [section 3.2] Eq. 3 defines the naive weight as the ratio of validation loss to training loss, and Eq. 4 symmetrizes it around 1
  - [corpus] No direct evidence; only related self-improvement methods are mentioned, not DS weight specifically
- Break condition: If validation set is too small or unrepresentative, DS weight will misestimate DSE, leading to incorrect filtering.

### Mechanism 2
- Claim: Filtering by DS weight and correctness together yields better self-improvement than either alone.
- Mechanism: DS weight filters out correct-but-high-DSE samples; majority voting filters out incorrect samples. Together they preserve only high-quality, low-shift data for fine-tuning.
- Core assumption: Correctness and DSE are largely independent; filtering on both removes different failure modes.
- Evidence anchors:
  - [abstract] "filtering out correct but with high distribution shift extent (DSE) samples could also benefit the results of self-improvement"
  - [section 4.5] Analysis shows DSE and uncertainty (a proxy for correctness) are nearly orthogonal for StrategyQA and ANLI-A1
  - [corpus] Weak; corpus contains related self-improvement methods but not explicit orthogonality studies
- Break condition: If DSE and correctness are strongly correlated, DS weight adds little beyond correctness filtering.

### Mechanism 3
- Claim: A small validation set suffices to compute DS weight reliably.
- Mechanism: DS weight uses the mean validation loss, which has variance inversely proportional to validation set size; with ~100 samples, variance is small enough to stabilize filtering.
- Core assumption: The sample mean of validation loss is a stable estimator; larger validation sets reduce variance further.
- Evidence anchors:
  - [section 4.4] "Eq. 8 implies that increasing ùëÅùë£ can scale down the variance of ¬ØLùë£, thus making the estimation more stable"
  - [section 4.4] Empirical table shows different validation set compositions yield nearly identical accuracy
  - [corpus] No corpus evidence for validation set size; assumption drawn from statistical theory
- Break condition: If validation set is unrepresentative of true data distribution, DS weight will be biased regardless of size.

## Foundational Learning

- Concept: Importance Weighting (IW)
  - Why needed here: DS weight is directly inspired by IW; understanding IW clarifies why loss ratio approximates density ratio.
  - Quick check question: In IW, why do we weight training samples by the ratio of test-to-training density?
- Concept: Distribution Shift
  - Why needed here: DS weight measures DSE; knowing what distribution shift is explains why high-DSE samples hurt fine-tuning.
  - Quick check question: What happens to model performance if training data distribution differs from test data distribution?
- Concept: Self-Consistency
  - Why needed here: Used as the correctness filter; understanding majority voting shows how DS weight complements it.
  - Quick check question: How does majority voting select the most likely correct answer among multiple candidates?

## Architecture Onboarding

- Component map: LLM generates multiple candidate answers with CoT -> majority voting picks most consistent answer -> DS weight computed per sample using validation loss ratio -> samples ranked by DS weight and filtered by percentile -> fine-tuning on filtered set with standard cross-entropy loss
- Critical path: Data generation -> majority voting -> DS weight computation -> filtering -> fine-tuning
- Design tradeoffs: DS weight is cheap (one forward pass on validation set) but naive; more sophisticated IW methods (e.g., DIW) could improve accuracy but at higher cost
- Failure signatures: (a) No performance gain -> validation set too small/unrepresentative; (b) Performance drops -> over-aggressive filtering; (c) Instability -> DS weight variance too high
- First 3 experiments:
  1. Verify DS weight correlates with loss distribution shift by computing it on a known shifted dataset
  2. Test filtering thresholds (k) on a small dev set to find sweet spot before full training
  3. Compare IWSI vs LMSI on a held-out test set to confirm DS weight adds value

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mathematical relationship between DS weight and other filtering criteria like answer uncertainty?
- Basis in paper: Explicit - The authors conduct orthogonality analysis comparing DS weight to answer uncertainty (entropy) in Section 4.5.
- Why unresolved: While the authors find that DS weight is nearly orthogonal to uncertainty for some tasks, they don't provide a precise mathematical formulation of this relationship or explain why it varies across different task types.
- What evidence would resolve it: A comprehensive mathematical model quantifying the correlation between DS weight and answer uncertainty across various task domains, with explanation of the underlying factors causing task-specific variations.

### Open Question 2
- Question: How does the optimal filtering threshold ùëò vary with dataset size and model scale?
- Basis in paper: Inferred - The authors explore the effect of filtering threshold ùëò in Section 4.3 but only test on fixed dataset sizes and a single model scale.
- Why unresolved: The authors only test filtering thresholds on six datasets with relatively similar sizes and one model scale (Llama3-8B), leaving open questions about how these parameters interact with dataset size and model scale.
- What evidence would resolve it: Extensive experiments varying dataset sizes and model scales while testing filtering thresholds, accompanied by theoretical analysis explaining the observed patterns.

### Open Question 3
- Question: What is the theoretical justification for why DS weight calculated using a simple mean-based approach works better than the standard importance weighting formula?
- Basis in paper: Explicit - The authors discuss in Section 4.2 that their DS weight variant (IWSI) outperforms the standard importance weighting approach (IWSI-w), but don't provide theoretical explanation for this observation.
- Why unresolved: The authors empirically show that using DS weight for ranking outperforms using it for weighted loss, but they don't explain the theoretical reasons behind this phenomenon or provide conditions under which each approach would be optimal.
- What evidence would resolve it: Mathematical proof or theoretical framework explaining the conditions under which ranking-based vs. weighting-based approaches to importance weighting are superior, with empirical validation across different model architectures and task types.

## Limitations

- Reliance on small validation set may lead to misestimation of distribution shift if the set is unrepresentative
- Assumption that correctness and distribution shift are largely independent may not hold across all datasets or tasks
- No exploration of more sophisticated importance weighting methods that could improve filtering accuracy
- Limited analysis of how DS weight behaves with systematically biased LLM generations

## Confidence

- High confidence: The mechanism that DS weight approximates distribution shift via loss ratio is mathematically sound, and the empirical results show consistent improvement over baselines across six datasets
- Medium confidence: The claim that DS weight and correctness are largely orthogonal is supported by analysis on two datasets (StrategyQA, ANLI-A1) but not comprehensively validated across all six
- Medium confidence: The assertion that IWSI matches reward-model-based methods is based on relative performance but lacks absolute metric comparisons to clarify the gap

## Next Checks

1. Test DS weight performance with validation sets of varying sizes (10, 50, 200, 500 samples) to confirm the stability assumption and identify the minimum viable size
2. Conduct an ablation study where DS weight is replaced with a learned importance weight estimator (e.g., using a small reward model) to assess whether more sophisticated methods improve filtering accuracy
3. Analyze the correlation between DS weight and correctness across all six datasets to verify the orthogonality assumption and identify any failure modes where the two metrics overlap significantly