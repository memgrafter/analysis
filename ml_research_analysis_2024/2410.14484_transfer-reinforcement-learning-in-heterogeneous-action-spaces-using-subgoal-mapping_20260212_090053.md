---
ver: rpa2
title: Transfer Reinforcement Learning in Heterogeneous Action Spaces using Subgoal
  Mapping
arxiv_id: '2410.14484'
source_url: https://arxiv.org/abs/2410.14484
tags:
- learner
- agent
- policy
- subgoal
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses transfer reinforcement learning for agents
  with different action spaces, proposing a method to learn a subgoal mapping between
  an expert and a learner agent. The core idea is to train a Long Short Term Memory
  (LSTM) network to predict the learner's subgoal sequence for unseen tasks based
  on the expert's subgoal sequence, then use this prediction to warm initialize the
  learner's high-level policy.
---

# Transfer Reinforcement Learning in Heterogeneous Action Spaces using Subgoal Mapping

## Quick Facts
- arXiv ID: 2410.14484
- Source URL: https://arxiv.org/abs/2410.14484
- Reference count: 37
- One-line primary result: Achieved average METEOR score of 0.5090 across K-fold cross-validation for predicting learner subgoal trajectories

## Executive Summary
This paper addresses transfer reinforcement learning for agents with different action spaces by proposing a method to learn subgoal mappings between expert and learner agents. The approach uses an LSTM network to predict the learner's subgoal sequence for unseen tasks based on the expert's subgoal sequence, then warm-initializes the learner's high-level policy with these predictions. The method significantly improves sample efficiency and training time in a chess environment with bishop and knight agents, outperforming learning from scratch and direct subgoal transfer approaches.

## Method Summary
The method trains an LSTM network to map expert subgoal sequences to learner subgoal sequences using supervised learning on training tasks. For unseen tasks, the expert's subgoal sequence is extracted from demonstrations, mapped to predicted learner subgoals via the trained LSTM, and used to warm-initialize the learner's high-level policy. The low-level policy is trained via supervised learning on expert demonstrations, and both policies are then fine-tuned using reinforcement learning. This hierarchical approach enables efficient transfer learning across heterogeneous action spaces.

## Key Results
- Achieved average METEOR score of 0.5090 across K-fold cross-validation for subgoal trajectory prediction
- Significantly outperformed learning from scratch and direct subgoal transfer in chess environment
- Demonstrated robust performance across three distinct cases of prediction accuracy, improving sample efficiency even with imperfect mappings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subgoal trajectory mapping enables learning across heterogeneous action spaces by compressing primitive actions into meaningful behavioral abstractions.
- Mechanism: The LSTM network learns to map sequences of expert subgoals to corresponding learner subgoal sequences, allowing the learner to imitate high-level behaviors without needing direct action-to-action translations.
- Core assumption: The mapping function is invertible and can be learned via supervised learning, and subgoal sequences contain sufficient information to reconstruct optimal behaviors.
- Evidence anchors:
  - [abstract] "We learn this subgoal mapping by training a Long Short Term Memory (LSTM) network for a distribution of tasks and then use this mapping to predict the learner subgoal sequence for unseen tasks"
  - [section] "By extracting sequences of subgoals from agent trajectories, we can learn the high-level behavior of agents whose goal is to achieve a terminal goal state"
- Break condition: The mapping fails when subgoal sequences are not representative of the underlying optimal policy or when the expert and learner have fundamentally incompatible subgoal spaces.

### Mechanism 2
- Claim: Warm initialization of the learner's high-level policy with predicted subgoal sequences accelerates convergence to optimal policies.
- Mechanism: Supervised learning is used to bias the high-level policy towards the predicted subgoal sequence, incorporating noise to encourage exploration while maintaining initial guidance.
- Core assumption: The predicted subgoal sequence, even when imperfect, provides useful directional guidance for policy learning.
- Evidence anchors:
  - [abstract] "we use this mapping to predict the learner subgoal sequence for unseen tasks, thereby improving the speed of learning by biasing the agent's policy towards the predicted learner subgoal sequence"
  - [section] "we propose to warm initialize the high level policy of the learner with the mapping's predicted subgoal trajectory using supervised learning"
- Break condition: Warm initialization provides no benefit or actively harms learning when the predicted subgoal sequence is systematically incorrect or when the task distribution differs significantly from training.

### Mechanism 3
- Claim: Hierarchical reinforcement learning with separate high-level (subgoal selection) and low-level (primitive action execution) policies enables efficient transfer learning.
- Mechanism: The high-level policy selects subgoals that the low-level policy then executes using primitive actions, allowing the learner to benefit from expert guidance at the subgoal level while learning action execution independently.
- Core assumption: The low-level policy can effectively learn to execute any given subgoal sequence, and the high-level policy can be effectively warm initialized.
- Evidence anchors:
  - [section] "we develop a hierarchical reinforcement learning framework where a high level policy selects the subgoal that the learner agent should pursue next and a low level policy selects primitive actions necessary to meet that selected subgoal"
  - [section] "Using this mapping, we then design a transfer learning algorithm...that can reduce the training time for the learner agent in unseen tasks"
- Break condition: The hierarchical decomposition fails when subgoal selection and action execution are too tightly coupled, or when the low-level policy cannot execute the subgoals predicted by the high-level policy.

## Foundational Learning

- Concept: Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks
  - Why needed here: To model temporal dependencies in subgoal sequences and learn the mapping function between expert and learner subgoals
  - Quick check question: What architectural feature of LSTMs allows them to capture long-term dependencies in sequences?

- Concept: Hierarchical Reinforcement Learning
  - Why needed here: To decompose the learning problem into subgoal selection (high-level) and action execution (low-level), enabling more efficient transfer learning
  - Quick check question: How does separating subgoal selection from action execution improve sample efficiency in transfer learning scenarios?

- Concept: Supervised Learning for Policy Initialization
  - Why needed here: To warm initialize the learner's high-level policy with the predicted subgoal sequence before reinforcement learning fine-tuning
  - Quick check question: Why might supervised learning be preferable to random initialization for the high-level policy in transfer learning scenarios?

## Architecture Onboarding

- Component map: Expert subgoal sequence -> LSTM mapping network -> Predicted learner subgoal sequence -> High-level policy (warm initialized) -> Low-level policy -> Primitive actions -> Environment state

- Critical path:
  1. Train LSTM mapping network on expert-learner subgoal pairs from training tasks
  2. For new task, generate expert demonstration and predict learner subgoals using trained mapping
  3. Warm initialize high-level policy with predicted subgoals via supervised learning
  4. Train low-level policy via supervised learning on expert demonstrations
  5. Fine-tune both policies using reinforcement learning

- Design tradeoffs:
  - LSTM vs. Transformer: LSTMs are simpler and sufficient for the sequence-to-sequence mapping task, while Transformers might offer better performance but at higher computational cost
  - Bidirectional vs. Unidirectional LSTMs: Bidirectional provides better context but doubles parameter count
  - Noise level in warm initialization: Higher noise encourages exploration but may slow convergence; lower noise provides stronger initial guidance but may limit exploration

- Failure signatures:
  - Poor mapping accuracy (low METEOR scores) indicates the LSTM is not learning meaningful subgoal relationships
  - High variance in learning curves suggests the warm initialization is not providing consistent guidance
  - Suboptimal final performance indicates the hierarchical decomposition or fine-tuning process is flawed

- First 3 experiments:
  1. Test LSTM mapping accuracy on held-out tasks from the training distribution (compute METEOR scores)
  2. Test transfer learning performance with perfect mapping predictions (ground truth subgoals) to establish upper bound
  3. Test transfer learning performance with varying levels of noise in warm initialization to find optimal exploration-exploitation balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the prediction accuracy of the LSTM network for subgoal mapping vary with different hyperparameters such as network depth, hidden layer size, and training data size?
- Basis in paper: [explicit] The paper mentions that the LSTM network is trained with a specific architecture (bidirectional LSTM with 150 and 100 dimensions for encoder and decoder, respectively) and uses K-fold cross-validation to assess prediction accuracy.
- Why unresolved: The paper does not explore the impact of different hyperparameters on the prediction accuracy of the LSTM network.
- What evidence would resolve it: Experiments varying the network architecture (e.g., different hidden layer sizes, depths) and training data size, along with corresponding prediction accuracy results.

### Open Question 2
- Question: How does the warm initialization of the learner's high-level policy with predicted subgoals impact the learning speed and final performance compared to other initialization strategies, such as random initialization or initialization with subgoals from a different agent?
- Basis in paper: [explicit] The paper proposes warm initialization of the high-level policy with predicted subgoals and compares its performance to learning from scratch and direct subgoal transfer from the expert agent.
- Why unresolved: The paper does not compare warm initialization with other initialization strategies.
- What evidence would resolve it: Experiments comparing the learning speed and final performance of policies initialized with predicted subgoals, random subgoals, and subgoals from a different agent.

### Open Question 3
- Question: How does the performance of the proposed transfer learning method scale with the complexity of the task distribution, such as the number of subgoals, the size of the state space, and the diversity of tasks within the distribution?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of the proposed method on a chess environment with a limited number of tasks and subgoals.
- Why unresolved: The paper does not investigate the scalability of the method to more complex task distributions.
- What evidence would resolve it: Experiments applying the proposed method to task distributions with varying levels of complexity, such as a larger number of subgoals, a larger state space, and a more diverse set of tasks.

## Limitations

- Generalization capability across truly unseen task distributions remains uncertain, as validation used held-out tasks from the same distribution
- The hierarchical decomposition lacks ablation studies to quantify the benefit of separating subgoal selection from action execution versus flat approaches
- Only three discrete cases of prediction accuracy are tested, without systematic analysis of the relationship between mapping error and transfer performance

## Confidence

- Confidence is High for the core claim that subgoal mapping enables transfer learning across heterogeneous action spaces, given the demonstrated METEOR scores of 0.5090 and significant performance improvements over baselines
- Confidence is Medium for the claim that warm initialization accelerates convergence, as the paper shows improvement but doesn't explore optimal noise levels systematically
- Confidence is Low for the robustness claim across different prediction accuracy levels, as only three discrete cases are tested without a continuous analysis of the prediction-error to performance relationship

## Next Checks

1. Test transfer performance on tasks from completely different distributions than training to assess true generalization capability
2. Conduct ablation studies removing the hierarchical structure to quantify the benefit of separating subgoal selection from action execution
3. Systematically vary prediction error rates to establish the relationship between mapping accuracy and transfer learning performance, identifying the threshold below which transfer fails