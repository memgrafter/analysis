---
ver: rpa2
title: Large Language Models Are Neurosymbolic Reasoners
arxiv_id: '2401.09334'
source_url: https://arxiv.org/abs/2401.09334
tags:
- symbolic
- agent
- action
- games
- text-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that large language models (LLMs) can function
  as neurosymbolic reasoners for text-based games. The proposed LLM agent leverages
  external symbolic modules (e.g., calculators, navigators) to perform symbolic reasoning
  tasks like arithmetic, map reading, sorting, and common sense reasoning.
---

# Large Language Models Are Neurosymbolic Reasoners

## Quick Facts
- arXiv ID: 2401.09334
- Source URL: https://arxiv.org/abs/2401.09334
- Authors: Meng Fang, Shilong Deng, Yudi Zhang, Zijing Shi, Ling Chen, Mykola Pechenizkiy, Jun Wang
- Reference count: 15
- Key outcome: LLM agent achieves 88% average performance across symbolic reasoning tasks in text-based games

## Executive Summary
This paper demonstrates that large language models can function as neurosymbolic reasoners by leveraging external symbolic modules for text-based games. The proposed approach uses carefully designed prompts to guide an LLM agent in interacting with both game environments and symbolic modules (calculator, navigator, sorter, knowledge base) to complete tasks involving arithmetic, map reading, sorting, and common sense reasoning. The zero-shot prompting approach achieves an average performance of 88% across all tasks, outperforming strong baselines like DRRN and Behavior Cloned Transformer with symbolic modules.

## Method Summary
The method employs a zero-shot prompting approach where an LLM agent (GPT-3.5-turbo or GPT-4) receives observations and valid action sets from both text-based game environments and external symbolic modules. Through carefully designed prompts with role initialization and action constraints, the agent selects appropriate actions that interact with these modules to complete symbolic reasoning tasks. The approach requires no additional training, relying instead on the LLM's pretraining knowledge and in-context learning through prompt engineering.

## Key Results
- LLM agent achieves 88% average performance across all symbolic reasoning tasks
- Outperforms DRRN and Behavior Cloned Transformer baselines with symbolic modules
- Successfully handles arithmetic, map reading, sorting, and common sense reasoning tasks
- Demonstrates effective zero-shot learning without additional training

## Why This Works (Mechanism)

### Mechanism 1
LLMs can perform zero-shot symbolic reasoning by leveraging external symbolic modules. The agent receives observations and valid action sets from both environments, then uses prompts to select actions that interact with these modules. Core assumption: LLMs have sufficient world knowledge to understand symbolic reasoning tasks when properly prompted. Evidence: 88% average performance across tasks. Break condition: Insufficient world knowledge or failed prompting strategy.

### Mechanism 2
Constrained prompting improves LLM agent performance in text-based games. Specific action constraints are added to prompts based on task requirements, preventing invalid action sequences. Core assumption: LLMs can follow explicit rules when clearly stated in prompts. Evidence: Performance improvements with constrained prompts. Break condition: LLM ignores or misinterprets constraints.

### Mechanism 3
The LLM agent can generalize across different symbolic tasks without additional training. In-context learning through carefully designed prompts enables handling arithmetic, map reading, sorting, and common sense reasoning. Core assumption: LLMs have learned generalizable patterns during pretraining. Evidence: 88% average performance across diverse tasks. Break condition: Tasks require reasoning patterns not present in pretraining data.

## Foundational Learning

- **Prompt engineering for task-specific constraints**
  - Why needed: LLM agent requires explicit guidance to follow correct action sequences and avoid invalid actions
  - Quick check: How would you modify the prompt to ensure the agent takes items before placing them in containers?

- **Symbolic reasoning with external modules**
  - Why needed: Text-based games often require mathematical calculations, navigation, and sorting difficult for LLMs to perform directly
  - Quick check: What types of symbolic modules would you add to handle a task requiring logical deduction?

- **Zero-shot learning through in-context examples**
  - Why needed: The approach aims to achieve good performance without additional training
  - Quick check: How would you structure a prompt to guide the LLM through a multi-step reasoning task?

## Architecture Onboarding

- **Component map**: LLM agent (GPT-3.5-turbo) ↔ Symbolic modules (calculator, navigator, sorter, knowledge base) ↔ Text-based game environment
- **Critical path**: Observation → Prompt construction → LLM action selection → Symbolic module or environment interaction → New observation
- **Design tradeoffs**: Zero-shot learning (no training required) vs. potentially lower performance than trained models; flexibility of LLMs vs. potential for inconsistent outputs
- **Failure signatures**: Invalid action sequences, getting stuck in loops, incorrect use of symbolic modules, failure to complete tasks
- **First 3 experiments**:
  1. Test LLM agent with only one symbolic module (calculator) on arithmetic tasks to verify basic functionality
  2. Test constrained vs. unconstrained prompting on a simple map reading task to measure performance improvement
  3. Test agent with all four symbolic modules on a complex task requiring multiple reasoning types to verify integration

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of LLM agents change with different sizes of training data or with limited expert demonstrations? Basis: The paper mentions Behavior Cloned Transformer requires extensive expert data while LLM agents work in a zero-shot manner. Unresolved because the paper doesn't explore how LLM performance scales with varying amounts of training data. Evidence needed: Experiments comparing LLM agent performance with different amounts of expert data or few-shot demonstrations.

### Open Question 2
How robust are LLM agents to variations in symbolic module interfaces or unexpected module outputs? Basis: The paper assumes well-defined symbolic modules with predictable outputs. Unresolved because the paper doesn't test agent performance when symbolic modules produce unexpected outputs. Evidence needed: Testing LLM agents with modified or noisy symbolic module outputs.

### Open Question 3
Can LLM agents maintain consistent reasoning performance across longer text-based games with more complex symbolic reasoning chains? Basis: The paper shows 88% average performance but doesn't explore performance decay over extended gameplay. Unresolved because experiments focus on individual tasks rather than long-term performance. Evidence needed: Testing LLM agents on extended games requiring multiple sequential symbolic reasoning steps.

## Limitations

- Reliance on specific prompt engineering techniques without systematic framework for prompt optimization
- Limited evaluation scope focusing exclusively on text-based games without testing real-world applications
- Unclear scalability to more complex tasks requiring sophisticated reasoning chains or numerous symbolic modules

## Confidence

**High Confidence**: LLMs can function as neurosymbolic reasoners when provided with appropriate prompts and external symbolic modules. Well-supported by experimental results showing consistent performance across multiple task types.

**Medium Confidence**: Constrained prompting significantly improves performance. While performance improvements are shown, evaluation could be more rigorous with additional ablation studies.

**Medium Confidence**: Zero-shot generalization across different symbolic tasks. 88% average performance is promising but limited task scope reduces confidence in broad generalization claims.

## Next Checks

1. **Prompt Robustness Analysis**: Conduct systematic ablation studies testing the LLM agent with progressively less constrained prompts to quantify impact of different prompting strategies.

2. **Scalability Testing**: Design and evaluate tasks requiring chaining multiple symbolic modules together in complex reasoning sequences to measure performance degradation.

3. **Cross-Domain Generalization**: Test the LLM agent on non-game applications requiring symbolic reasoning (mathematical problem solving, logical deduction) to assess transferability of prompting strategies.