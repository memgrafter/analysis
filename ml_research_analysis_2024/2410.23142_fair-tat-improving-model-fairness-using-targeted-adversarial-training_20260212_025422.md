---
ver: rpa2
title: 'FAIR-TAT: Improving Model Fairness Using Targeted Adversarial Training'
arxiv_id: '2410.23142'
source_url: https://arxiv.org/abs/2410.23142
tags:
- adversarial
- classes
- training
- robustness
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of fairness in adversarial training,
  where robustness against adversarial attacks comes at the cost of reduced accuracy
  on clean data and uneven performance across classes. The authors propose Fair Targeted
  Adversarial Training (FAIR-TAT), which uses targeted adversarial attacks instead
  of untargeted ones, prioritizing "hard" classes that are frequently misclassified.
---

# FAIR-TAT: Improving Model Fairness Using Targeted Adversarial Training

## Quick Facts
- arXiv ID: 2410.23142
- Source URL: https://arxiv.org/abs/2410.23142
- Reference count: 40
- Primary result: FAIR-TAT improves clean accuracy, maintains adversarial robustness, and enhances class-wise fairness through targeted adversarial training

## Executive Summary
FAIR-TAT addresses the fairness problem in adversarial training where robustness against attacks comes at the cost of reduced accuracy on clean data and uneven performance across classes. The method uses targeted adversarial attacks instead of untargeted ones, prioritizing "hard" classes that are frequently misclassified based on class-wise false positive scores (CFPS). By dynamically adjusting the training setup to defend against class-specific perturbations, FAIR-TAT achieves better trade-offs between robustness and accuracy while improving class-wise fairness. The approach outperforms state-of-the-art methods on both adversarial attacks and common corruptions.

## Method Summary
FAIR-TAT modifies standard adversarial training by incorporating targeted adversarial attacks that prioritize classes with high class-wise false positive scores (CFPS). During training, CFPS scores are calculated to identify vulnerable classes, and a multinomial distribution is built from these scores to guide target sampling. The method also implements class-specific perturbation margin scaling based on robust accuracy to balance defense across different classes. FAIR-TAT can be combined with existing adversarial training methods like TRADES to enhance fairness while maintaining overall robustness.

## Key Results
- Improves clean accuracy while maintaining adversarial robustness against PGD, AutoAttack, and Squares
- Enhances class-wise fairness by reducing performance disparities across different classes
- Demonstrates transferability of fairness improvements to unseen attacks and common corruptions
- Outperforms state-of-the-art methods including AT, TRADES, FAT, FRL, CFA, BAT, and WAT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prioritizing classes with high CFPS during adversarial training reduces inter-class confusion and improves fairness
- Mechanism: High CFPS indicates frequent misclassification, so targeted attacks toward these classes strengthen their decision boundaries
- Core assumption: Classes with high CFPS are more vulnerable to adversarial perturbations
- Evidence anchors: Abstract mentions targeted attacks enable favorable adversarial fairness trade-offs; section explains CFPS vulnerability and decision boundary refinement
- Break condition: If CFPS does not correlate with actual vulnerability to adversarial attacks

### Mechanism 2
- Claim: Dynamic perturbation margin scaling per class based on robust accuracy improves fairness
- Mechanism: Classes with lower robust accuracy receive smaller perturbation margins for more precise decision boundaries
- Core assumption: Different classes have distinct optimal perturbation margins
- Evidence anchors: Section references that various classes exhibit distinct preferences for perturbation margins
- Break condition: If uniform perturbation margins work equally well across classes

### Mechanism 3
- Claim: Targeted adversarial training transfers fairness improvements to unseen attacks and common corruptions
- Mechanism: Learning to defend against class-specific perturbations develops robust, balanced feature representations
- Core assumption: Features learned from targeted attacks generalize to other perturbation types
- Evidence anchors: Abstract states FAIR-TAT surpasses baselines against other adversaries and common corruptions
- Break condition: If the model overfits to targeted attacks without learning generalizable features

## Foundational Learning

- Concept: Fast Gradient Sign Method (FGSM)
  - Why needed here: FGSM is mentioned as a baseline attack method in related work
  - Quick check question: What is the key difference between FGSM and PGD in terms of how they generate adversarial examples?

- Concept: Projected Gradient Descent (PGD)
  - Why needed here: PGD is the primary attack method used in FAIR-TAT for generating both untargeted and targeted adversarial examples
  - Quick check question: How does PGD iteratively improve upon the single-step FGSM approach?

- Concept: Class-wise false positive scores (CFPS)
  - Why needed here: CFPS is the core metric used to identify vulnerable classes and guide the targeted sampling strategy
  - Quick check question: How is CFPS calculated and what does a high value indicate about class relationships?

## Architecture Onboarding

- Component map: Training loop -> Target sampler -> Perturbation manager -> Model -> Evaluation
- Critical path: Calculate CFPS scores on validation set -> Build multinomial distribution from CFPS -> During each training iteration: Sample target class -> Generate targeted adversarial example -> Update model parameters -> Update CFPS scores periodically
- Design tradeoffs:
  - Targeted vs untargeted attacks: Targeted provides more control but may overfit to specific target classes
  - Static vs dynamic CFPS: Dynamic adapts to training progress but adds computational overhead
  - Class-specific vs uniform epsilon: Class-specific improves fairness but complicates hyperparameter tuning
- Failure signatures:
  - Clean accuracy drops significantly without corresponding robust accuracy gains
  - Worst-class accuracy improves but overall accuracy drops substantially
  - Model becomes overly sensitive to specific perturbations
  - CFPS distribution becomes too skewed, causing sampling imbalance
- First 3 experiments:
  1. Implement basic targeted PGD training with uniform target sampling to establish baseline performance
  2. Add CFPS-based target sampling while keeping uniform epsilon to isolate the effect of target selection
  3. Implement class-specific epsilon scaling based on robust accuracy to evaluate combined effect on fairness

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored based on the experimental results and limitations discussed.

## Limitations
- Limited scalability evidence beyond CIFAR-10 and CIFAR-100 datasets
- Computational overhead of CFPS-based sampling not fully quantified
- Performance on domain-specific datasets beyond natural images not evaluated
- Theoretical relationship between CFPS and optimal perturbation margins not established

## Confidence
- Targeted attacks improve fairness by addressing class-specific vulnerabilities: Medium
- Dynamic perturbation margin scaling enhances balanced robustness: Low
- Transferability of fairness improvements to unseen attacks: Low
- Overall improvement in clean accuracy and robustness trade-offs: Medium

## Next Checks
1. Conduct ablation studies to isolate the individual contributions of targeted attacks versus CFPS-based sampling versus class-specific epsilon scaling on fairness metrics
2. Test transferability claims by evaluating FAIR-TAT-trained models on a broader suite of unseen attack types and corruption patterns not used during training
3. Perform statistical analysis to verify the correlation between CFPS values and actual vulnerability to targeted adversarial attacks across different model architectures and datasets