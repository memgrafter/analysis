---
ver: rpa2
title: 'Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech Evaluation'
arxiv_id: '2411.12719'
source_url: https://arxiv.org/abs/2411.12719
tags:
- mushra
- scores
- systems
- test
- reference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies two key shortcomings in the MUSHRA test
  for TTS evaluation: reference-matching bias, where raters penalize systems that
  exceed human reference quality, and judgment ambiguity, where vague rating criteria
  lead to inconsistent scores. To address these, the authors propose two refined variants:
  MUSHRA-NMR (omitting the explicitly mentioned reference) and MUSHRA-DG (providing
  detailed scoring guidelines).'
---

# Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech Evaluation

## Quick Facts
- arXiv ID: 2411.12719
- Source URL: https://arxiv.org/abs/2411.12719
- Reference count: 25
- Key outcome: Proposes refined MUSHRA variants (MUSHRA-NMR, MUSHRA-DG) that reduce reference-matching bias and judgment ambiguity, validated with 47,100 ratings from 471 listeners in Hindi and Tamil

## Executive Summary
This paper identifies two key shortcomings in the MUSHRA test for TTS evaluation: reference-matching bias, where raters penalize systems that exceed human reference quality, and judgment ambiguity, where vague rating criteria lead to inconsistent scores. To address these, the authors propose two refined variants: MUSHRA-NMR (omitting the explicitly mentioned reference) and MUSHRA-DG (providing detailed scoring guidelines). Evaluations on 47,100 ratings from 471 listeners in Hindi and Tamil show MUSHRA-NMR reduces bias, enabling fairer ratings for high-quality systems, while MUSHRA-DG reduces score variance by 41-58% through fine-grained assessment. Combining both variants yields the most reliable and granular results, with system scores aligning closer to CMOS benchmarks. The MANGO dataset is released for further research.

## Method Summary
The authors conducted MUSHRA evaluations on three TTS systems (FS2, ST2, VITS) in Hindi and Tamil using 471 native speakers who provided 47,100 ratings. They systematically compared standard MUSHRA against two variants: MUSHRA-NMR (omitting explicit reference mention) and MUSHRA-DG (providing detailed scoring guidelines with sub-dimensions for pronunciation, prosody, and artifacts). The evaluation measured mean scores, variance, rank correlations, and alignment with CMOS benchmarks. The combined MUSHRA-DG-NMR variant was also tested to assess synergistic effects.

## Key Results
- MUSHRA-NMR reduces reference-matching bias, enabling fairer ratings for high-quality systems
- MUSHRA-DG reduces score variance by 41-58% through fine-grained assessment
- Combining MUSHRA-NMR and MUSHRA-DG yields the most reliable and granular results, with system scores aligning closer to CMOS benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Omitting the explicitly mentioned reference (MUSHRA-NMR) reduces reference-matching bias.
- **Mechanism:** By removing the explicit mention of the human reference, raters are no longer primed to compare every system output against it, allowing them to assess each system on its own merits rather than how closely it matches the reference.
- **Core assumption:** Raters naturally try to match system outputs to the reference when it is explicitly mentioned, which unfairly penalizes systems that exceed the reference in naturalness or prosody.
- **Evidence anchors:**
  - [abstract] "MUSHRA-NMR reduces bias, enabling fairer ratings for high-quality systems"
  - [section] "Specifically, we construct Anchor-Y by degrading ST2 outputs... As expected, from the Tamil MUSHRA scores in Table 2, this anchor does indeed score poorly with a mean of 20.08. Interestingly, we see that despite a very low-quality anchor, other systems are not rated very highly..."
  - [corpus] Weak - the corpus evidence here is indirect; it shows TTS systems can exceed human quality but doesn't directly prove the bias mechanism.
- **Break condition:** If raters still subconsciously try to align with an unmentioned reference, or if other cues in the test implicitly signal the reference.

### Mechanism 2
- **Claim:** Providing detailed scoring guidelines (MUSHRA-DG) reduces judgment ambiguity.
- **Mechanism:** By breaking down naturalness into explicit sub-dimensions (pronunciation mistakes, prosody, artifacts, etc.) and providing a formula, raters have clear criteria for evaluation, reducing subjective interpretation and variance.
- **Core assumption:** Ambiguity in MUSHRA ratings arises because raters focus on different aspects of speech quality without clear guidance.
- **Evidence anchors:**
  - [abstract] "MUSHRA-DG (providing detailed scoring guidelines)"
  - [section] "Specifically, some raters may prioritize prosody, others voice quality, and yet others the presence of digital artifacts... Hence, clear guidelines which take into account a fine-grained evaluation across different aspects would help"
  - [corpus] Weak - corpus evidence doesn't directly support the mechanism; it shows the problem exists but not that guidelines solve it.
- **Break condition:** If the guidelines are too complex, leading to cognitive overload, or if raters ignore them and default to subjective judgment.

### Mechanism 3
- **Claim:** Combining MUSHRA-NMR and MUSHRA-DG yields the most reliable and granular results.
- **Mechanism:** MUSHRA-NMR addresses the bias from explicit reference mention, while MUSHRA-DG addresses ambiguity in rating criteria. Together, they ensure both fairness and clarity in evaluation.
- **Core assumption:** The two biases (reference-matching and judgment ambiguity) are independent and can be mitigated separately.
- **Evidence anchors:**
  - [abstract] "Combining both variants yields the most reliable and granular results, with system scores aligning closer to CMOS benchmarks"
  - [section] "It allows modern TTS systems to be evaluated without being unfairly penalized for surpassing the reference in naturalness or prosody. The detailed scoring for pronunciation, prosody, and other factors provides actionable insights..."
  - [corpus] Weak - corpus doesn't provide direct evidence that the combination is superior, only that both variants individually help.
- **Break condition:** If one bias dominates the other, making the combination less effective than addressing the dominant bias alone.

## Foundational Learning

- **Concept:** Understanding of MUSHRA test methodology and its conventional implementation.
  - **Why needed here:** The paper builds on MUSHRA but identifies its shortcomings; understanding the base methodology is crucial for grasping the proposed variants.
  - **Quick check question:** What are the key components of a standard MUSHRA test, and how do they contribute to its evaluation process?

- **Concept:** Knowledge of TTS system evaluation metrics and their limitations.
  - **Why needed here:** The paper compares MUSHRA to other evaluation methods (MOS, CMOS) and highlights why MUSHRA might be preferable despite its flaws.
  - **Quick check question:** What are the main limitations of MOS and CMOS tests for TTS evaluation, and how does MUSHRA attempt to address them?

- **Concept:** Understanding of statistical measures used in subjective evaluation (mean, variance, confidence intervals).
  - **Why needed here:** The paper relies heavily on statistical analysis of human ratings to demonstrate the effectiveness of proposed variants.
  - **Quick check question:** How do mean and variance of subjective ratings inform us about the reliability and consistency of an evaluation method?

## Architecture Onboarding

- **Component map:** WebMUSHRA platform (modified for session management and event tracking) -> TTS systems (FS2, ST2, VITS, XTTS) -> Human raters (native speakers of Hindi and Tamil) -> Proposed MUSHRA variants (MUSHRA-NMR, MUSHRA-DG, MUSHRA-DG-NMR)
- **Critical path:** Synthesize speech samples → Conduct MUSHRA tests with variants → Collect human ratings → Analyze results (mean scores, variance, correlations) → Validate against CMOS benchmarks
- **Design tradeoffs:** MUSHRA-DG provides more granular data but takes twice as long to complete; MUSHRA-NMR may be simpler but could still have residual bias; combining both gives best results but at highest cost
- **Failure signatures:** High variance in ratings across raters or utterances indicates judgment ambiguity; system scores consistently below reference indicate reference-matching bias; low correlations with CMOS suggest the variant isn't capturing true quality differences
- **First 3 experiments:**
  1. Run MUSHRA-NMR test and compare score distributions and variances to standard MUSHRA
  2. Run MUSHRA-DG test and analyze fault isolation data to see if it reveals system weaknesses not apparent in standard MUSHRA
  3. Combine MUSHRA-NMR and MUSHRA-DG (MUSHRA-DG-NMR) and validate that scores align more closely with CMOS benchmarks while maintaining low variance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum number of systems (n) that can be evaluated in MUSHRA before cognitive overload starts impacting the scores?
- Basis in paper: [explicit] The paper states "it remains to be seen how large n can be before cognitive overload starts impacting the scores" after studying cognitive load with n=7 systems.
- Why unresolved: The paper only tested with 7 systems and found no significant cognitive overload, but did not explore larger numbers.
- What evidence would resolve it: A study testing MUSHRA with varying numbers of systems (e.g., 8, 10, 15, 20) and measuring score consistency and correlation with smaller-scale tests.

### Open Question 2
- Question: How do the proposed MUSHRA variants (MUSHRA-NMR and MUSHRA-DG) perform in languages other than Hindi and Tamil, particularly in English?
- Basis in paper: [inferred] The paper acknowledges this as a limitation, stating "we did not extend our analysis to English" and that "Future studies should include evaluations in English to generalize our findings."
- Why unresolved: The study was limited to Hindi and Tamil due to scope and resource constraints, so the generalizability to other language families is unknown.
- What evidence would resolve it: Conducting MUSHRA-NMR and MUSHRA-DG evaluations in English and other languages, comparing results to the original MUSHRA and to CMOS tests.

### Open Question 3
- Question: What is the optimal anchor quality for MUSHRA evaluations that balances calibration needs with minimizing reference-matching bias?
- Basis in paper: [explicit] The paper found that high-quality anchors (Anchor-X) unfairly bias raters against other systems, while low-quality anchors (Anchor-Y) had no effect on ratings. The authors suggest there is "merit in conducting MUSHRA evaluations without anchors."
- Why unresolved: The paper did not identify an optimal anchor quality level; it only showed that both high and low-quality anchors have drawbacks.
- What evidence would resolve it: Testing MUSHRA with anchors of varying quality levels (e.g., poor, fair, good, excellent) and measuring their impact on system rankings and score variance compared to no-anchor conditions.

## Limitations

- The study focuses exclusively on Hindi and Tamil languages, limiting generalizability to other linguistic contexts
- Only three TTS systems were evaluated, which may not capture the full range of modern TTS quality variations
- The corpus evidence for bias mechanisms is largely indirect, relying on observed patterns rather than controlled experiments
- The detailed scoring guidelines in MUSHRA-DG may introduce cognitive load that wasn't fully measured or accounted for

## Confidence

- **High confidence**: The statistical analysis showing 41-58% variance reduction in MUSHRA-DG scores
- **Medium confidence**: The claim that MUSHRA-NMR reduces reference-matching bias, based primarily on score distribution changes
- **Medium confidence**: The assertion that combining variants yields optimal results, though this is supported by correlation with CMOS benchmarks

## Next Checks

1. Conduct cross-linguistic validation by applying MUSHRA-NMR and MUSHRA-DG to TTS systems in languages with different phonological and prosodic structures
2. Perform a controlled experiment isolating the reference-matching bias by systematically varying the quality gap between TTS outputs and human reference
3. Measure cognitive load and completion time differences between standard MUSHRA and MUSHRA-DG to quantify the cost of increased granularity