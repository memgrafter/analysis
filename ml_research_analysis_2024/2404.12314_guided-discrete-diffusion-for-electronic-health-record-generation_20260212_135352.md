---
ver: rpa2
title: Guided Discrete Diffusion for Electronic Health Record Generation
arxiv_id: '2404.12314'
source_url: https://arxiv.org/abs/2404.12314
tags:
- data
- real
- synthetic
- diffusion
- prevalence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EHR-D3PM, a discrete diffusion model for
  generating synthetic electronic health records. The method extends multinomial diffusion
  to handle tabular medical code sequences, incorporating an attention-based architecture
  that captures feature correlations.
---

# Guided Discrete Diffusion for Electronic Health Record Generation

## Quick Facts
- arXiv ID: 2404.12314
- Source URL: https://arxiv.org/abs/2404.12314
- Reference count: 5
- Primary result: EHR-D3PM achieves superior synthetic EHR generation with better fidelity, utility, and privacy than GAN and diffusion baselines

## Executive Summary
This paper introduces EHR-D3PM, a discrete diffusion model designed specifically for generating synthetic electronic health records containing tabular medical codes. The method extends multinomial diffusion to handle discrete categorical variables while incorporating an attention-based architecture to capture complex feature correlations. Through experiments on MIMIC-III and two large private datasets, the model demonstrates superior performance in terms of data fidelity, utility for downstream tasks, and privacy preservation compared to existing GAN and diffusion baselines. The guided generation capability enables the creation of balanced synthetic data for rare diseases while maintaining realistic EHR structure.

## Method Summary
EHR-D3PM extends multinomial diffusion models to handle discrete EHR code sequences through independent categorical noise transitions for each medical code. The model employs a transformer-based denoiser with linear self-attention to efficiently capture feature correlations across the entire code sequence. For conditional generation, the method uses energy-guided Langevin dynamics in the latent space, where samples are iteratively updated to satisfy target conditions while staying close to the learned unconditional distribution. The approach is trained on real EHR data and evaluated across three datasets (MIMIC-III, D1, D2) using fidelity metrics (CMD, MMD, MCAD), utility metrics (disease prediction AUROC/AUPRC), and privacy metrics (AIR, MIR).

## Key Results
- EHR-D3PM outperforms GAN and diffusion baselines on MIMIC-III, D1, and D2 across CMD, MMD, and MCAD metrics
- Guided generation enables balanced synthetic data creation for rare diseases, improving downstream prediction AUROC/AUPRC
- Privacy analysis shows EHR-D3PM achieves favorable AIR and MIR scores compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multinomial diffusion with uniform categorical noise enables stable generation of discrete EHR codes
- Mechanism: Each medical code is represented as a one-hot vector, with forward diffusion gradually replacing observed codes with uniform noise over all possible codes, creating a known denoising path. The reverse process learns to reconstruct original codes from noise.
- Core assumption: Discrete EHR codes can be treated as independent categorical variables that diffuse independently, yet the model architecture captures inter-code dependencies during denoising.
- Evidence anchors:
  - [abstract] "discrete diffusion model... tailored for generation of tabular medical codes in Electronic Health Records (EHRs)"
  - [section 4.2] "q(xt|xt−1, x0) = ∏Ni=1 Cat( x(i)t ; βtx(i)t−1 + (1 − βt)qnoise )"
- Break condition: If code dependencies are highly non-linear or temporal, treating codes as independent diffusion variables will fail to capture joint distributions.

### Mechanism 2
- Claim: Transformer-based architecture with linear self-attention enables efficient modeling of high-dimensional EHR feature correlations
- Mechanism: Each diffusion step's latent representation zL,t is fed through a transformer with position and time embeddings, producing logits for the next state. The linear attention block reduces computational complexity from O(N²) to O(N).
- Core assumption: EHR code correlations can be approximated effectively by self-attention over the entire code sequence without requiring autoregressive conditioning.
- Evidence anchors:
  - [section 4.2] "LinMSA refers to the efficient multi-head self-attention block proposed by Wang et al. (2020), which has linear complexity with respect to the input dimension."
  - [section 4.2] "One bottleneck of transformer models is that the computational complexity of the attention module is quadratic to the dimension of input data."
- Break condition: If code dependencies are primarily sequential or require long-range memory beyond transformer capacity, the linear attention may miss critical dependencies.

### Mechanism 3
- Claim: Energy-guided Langevin dynamics in latent space enables conditional generation without retraining
- Mechanism: For a target condition c (e.g., disease presence), the latent variable y(k) is iteratively updated using ∇y(k)[DKL(y(k)) − Vθ(y(k))] + √(2ητ)ε, where Vθ(y(k)) = log(p(c|y(k))). This drives the latent space toward samples satisfying the condition while staying close to the learned unconditional distribution.
- Core assumption: The classifier p(c|x) approximates the true conditional qdata(c|x) and can be applied in latent space without breaking the diffusion model's learned structure.
- Evidence anchors:
  - [section 4.3] "we can propose a training-free conditional generator as follows: pθ(x|c) ∝ pθ(x) · p(c|x)"
  - [section 4.3] "the last-layer latent variable zL,t in Equation(4) (before the softmax layer) lies in a continuous space."
- Break condition: If the classifier is poorly calibrated or the latent space is too constrained, guided sampling will either collapse or fail to produce valid EHR codes.

## Foundational Learning

- Concept: Multinomial diffusion and categorical noise models
  - Why needed here: EHR codes are discrete and categorical; continuous diffusion models cannot handle them directly
  - Quick check question: What is the key difference between Gaussian diffusion and multinomial diffusion in terms of noise distribution?

- Concept: Self-attention and transformer architectures
  - Why needed here: EHR code dependencies must be modeled across the entire feature set efficiently
  - Quick check question: Why does linear self-attention reduce complexity compared to standard self-attention?

- Concept: Langevin dynamics and energy-guided sampling
  - Why needed here: Conditional generation requires sampling from a posterior without retraining the base model
  - Quick check question: How does the energy function Vθ(y(k)) = log(p(c|y(k))) guide the latent space toward desired conditions?

## Architecture Onboarding

- Component map: Discrete EHR codes (one-hot) -> Multinomial diffusion forward process -> Transformer denoiser (LinMSA + position/time embeddings) -> MLP -> Softmax logits -> Categorical sampling -> Next diffusion step

- Critical path:
  1. Encode input into latent space with embeddings
  2. Apply L transformer layers with linear self-attention
  3. Produce logits and apply softmax for conditional probabilities
  4. Sample from categorical distribution to obtain next diffusion step
  5. For conditional sampling, apply Langevin updates in latent space

- Design tradeoffs:
  - Independent diffusion vs. joint modeling of code dependencies
  - Linear attention for scalability vs. full attention for richer dependencies
  - Unconditional training vs. conditional sampling without retraining

- Failure signatures:
  - Mode collapse: Generated prevalence distributions mismatch real data
  - Privacy leakage: High AIR/MIR scores indicate memorization
  - Conditional bias: Guided samples drift from realistic EHR structure

- First 3 experiments:
  1. Train unconditional model and compare prevalence correlation vs. baselines
  2. Evaluate MMD and CMD metrics on synthetic vs. real data
  3. Apply guided sampling for a rare disease and measure AUROC improvement in downstream prediction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does incorporating longitudinal features into EHR-D3PM affect synthetic data quality and downstream task performance?
- Basis in paper: [explicit] "Incorporating longitudinal feature into our model is an interesting research direction" mentioned in Limitations section
- Why unresolved: The current EHR-D3PM focuses on tabular medical code generation without considering temporal dependencies between records
- What evidence would resolve it: Comparative experiments showing fidelity, utility, and privacy metrics with and without longitudinal feature modeling

### Open Question 2
- Question: What specific vulnerabilities do diffusion-based generative models have to advanced privacy attacks targeting EHR data?
- Basis in paper: [explicit] "the vulnerability of diffusion-based generative models in EHR generation to more advanced privacy attacks is a largely unexplored area" mentioned in Limitations section
- Why unresolved: The paper demonstrates strong privacy preservation but acknowledges need for investigation against sophisticated adversarial attacks
- What evidence would resolve it: Empirical evaluation of EHR-D3PM against various privacy attacks (e.g., attribute inference, membership inference) with varying attack sophistication

### Open Question 3
- Question: How does incorporating differential privacy into discrete diffusion models affect synthetic EHR quality and privacy guarantees?
- Basis in paper: [explicit] "providing formal privacy guarantees, e.g., by incorporating differential privacy, which is largely unexplored in diffusion-based models for discrete data" mentioned in Conclusion
- Why unresolved: Current EHR-D3PM shows mild vulnerability risks but hasn't explored DP mechanisms for discrete diffusion
- What evidence would resolve it: Systematic comparison of fidelity, utility, and privacy metrics between standard EHR-D3PM and DP-enhanced variants across multiple privacy budgets

## Limitations

- The exact architecture details of the attention-based denoiser (layer counts, hidden dimensions) are not specified
- Hyperparameters for the energy-guided Langevin dynamics (η, λ values, update steps) are not clearly stated
- Evaluation is limited to three datasets, with performance on extremely rare conditions or highly imbalanced datasets not thoroughly explored

## Confidence

- **High confidence**: The core mechanism of multinomial diffusion for discrete categorical variables and the transformer-based architecture for capturing code correlations
- **Medium confidence**: The energy-guided Langevin dynamics for conditional generation, though specific implementation details require more validation
- **Low confidence**: The scalability and robustness of the model across different EHR datasets, given limited evaluation scope

## Next Checks

1. **Architecture Sensitivity Analysis**: Systematically vary the transformer layer count, hidden dimensions, and attention mechanism to assess their impact on fidelity and utility metrics

2. **Conditional Generation Robustness**: Evaluate the guided sampling approach on a broader range of conditions, including extremely rare diseases and combinations of multiple conditions

3. **Privacy Leakage Assessment**: Conduct a more rigorous privacy analysis by testing the model's resistance to membership inference attacks and attribute inference attacks