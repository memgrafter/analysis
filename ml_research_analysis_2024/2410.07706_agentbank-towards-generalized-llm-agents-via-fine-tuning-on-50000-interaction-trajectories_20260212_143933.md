---
ver: rpa2
title: 'AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction
  Trajectories'
arxiv_id: '2410.07706'
source_url: https://arxiv.org/abs/2410.07706
tags:
- agent
- tasks
- arxiv
- data
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces AgentBank, the largest open agent trajectory
  dataset with over 50k trajectories spanning 16 tasks across 5 skill dimensions.
  The dataset is built using a novel annotation pipeline that avoids difficulty bias
  by separating action and rationale generation.
---

# AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories

## Quick Facts
- arXiv ID: 2410.07706
- Source URL: https://arxiv.org/abs/2410.07706
- Reference count: 20
- Introduces AgentBank, the largest open agent trajectory dataset with over 50k trajectories spanning 16 tasks across 5 skill dimensions

## Executive Summary
This paper introduces AgentBank, a large-scale trajectory dataset for training generalized LLM agents, and presents Samoyed models fine-tuned on this data to achieve superior agent capabilities. The dataset construction pipeline uses a novel annotation method that separates action and rationale generation to avoid difficulty bias, enabling successful annotation of challenging trajectories. Samoyed models trained on AgentBank show significant improvements over open-source baselines and match GPT-3.5-Turbo on held-in tasks while demonstrating strong generalization to unseen held-out tasks across five skill dimensions.

## Method Summary
The method involves constructing AgentBank through a novel annotation pipeline that separates gold action generation (using answer forcing, heuristic search, or reformatting) from rationale generation to avoid difficulty bias. Samoyed models are fine-tuned from Llama-2 using supervised fine-tuning on the AgentBank trajectories combined with a mixture of generalist instruction data (ShareGPT) and code data (Evol-CodeAlpaca) at an 80:10:10 ratio. The training uses auto-regressive loss on ground-truth responses with AdamW optimizer, cosine scheduler, and batch size of 128.

## Key Results
- AgentBank contains over 50k interaction trajectories across 16 tasks spanning 5 skill dimensions
- Samoyed models outperform previous open-source baselines on held-in tasks and match GPT-3.5-Turbo performance
- Strong generalization demonstrated on held-out tasks (Bamboogle, TheoremQA, IC-Bash, MiniWoB++, ScienceWorld) across all five skill dimensions
- Scaling trends show performance improvements with more trajectories, and mixture training mitigates catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large-scale trajectory tuning enables generalization to unseen tasks
- Mechanism: Training on 50k+ diverse interaction trajectories across 16 tasks and 5 skill dimensions exposes the model to a wide variety of problem-solving patterns, reasoning steps, and tool usage scenarios. This breadth of experience allows the model to transfer learned capabilities to new, unseen tasks within the same skill dimensions.
- Core assumption: The model can extract and generalize underlying patterns from diverse trajectories rather than simply memorizing specific solutions.
- Evidence anchors:
  - [abstract]: "Models trained on AgentBank... show significant improvements in agent capabilities, outperforming previous open-source baselines and matching GPT-3.5-Turbo on held-in tasks while demonstrating strong generalization to unseen held-out tasks."
  - [section 7.1]: "Scaling trends" analysis showing that increasing the number of tasks and trajectories improves performance on held-out tasks.
  - [corpus]: Weak - corpus doesn't directly address this mechanism.

### Mechanism 2
- Claim: Separating action and rationale generation reduces difficulty bias
- Mechanism: By first generating gold actions (using methods like answer forcing or heuristic search) and then separately generating rationales, the annotation process avoids filtering out difficult trajectories that the teacher model cannot solve. This ensures the training data represents the full difficulty distribution of the task space.
- Core assumption: Generating rationales for gold actions is easier than generating both actions and rationales simultaneously, allowing successful annotation of more challenging trajectories.
- Evidence anchors:
  - [section 2.2]: "Unlike previous methods that generate action and CoT simultaneously, we separate the annotation of gold actions and their corresponding rationales"
  - [section 4.2]: Detailed description of the separated annotation process
  - [section B]: Experiment showing difficulty bias in traditional GPT-exploration pipeline versus reduced bias in AgentBank

### Mechanism 3
- Claim: Mixing generalist instruction data and code data with trajectory data improves generalization and mitigates catastrophic forgetting
- Mechanism: Generalist instruction data maintains the model's broad instruction-following capabilities, while code data provides structured, logical reasoning patterns. Combining these with trajectory data creates a more balanced training distribution that supports both specialized agent capabilities and general reasoning skills.
- Core assumption: The benefits of specialized trajectory training can be preserved while maintaining general capabilities through careful data mixing.
- Evidence anchors:
  - [section 5]: "we adopt a mixture of AGENT BANK Dagent, the general domain instruction dataset Dgeneral, and the code dataset Dcode for fine-tuning"
  - [section 7.2]: Ablation study showing that mixture training leads to better generalization and mitigates catastrophic forgetting
  - [section 7.2]: "Mixture training alleviates catastrophic forgetting" with performance metrics on general tasks

## Foundational Learning

- Concept: Supervised fine-tuning (SFT) on interaction trajectories
  - Why needed here: This work uses SFT to adapt base LLMs to agent capabilities by training on expert demonstration trajectories. Understanding SFT is crucial for implementing the core training procedure.
  - Quick check question: What is the difference between supervised fine-tuning and reinforcement learning in the context of training agents?

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: CoT rationales are generated for each action step in the trajectories and are crucial for the model to learn the underlying reasoning process, not just the actions themselves.
  - Quick check question: How does including CoT rationales in training data affect the model's ability to handle unseen tasks compared to training without rationales?

- Concept: Data contamination analysis
  - Why needed here: Since the evaluation involves tasks that may overlap with training data, understanding how to assess and report contamination is important for interpreting results correctly.
  - Quick check question: What methods can be used to detect potential data contamination between training and evaluation sets?

## Architecture Onboarding

- Component map: AgentBank dataset construction pipeline (task collection → action annotation → rationale generation) → Samoyed training pipeline (SFT on mixed dataset) → Evaluation framework (held-in and held-out tasks across five skill dimensions)
- Critical path: The most critical sequence is task collection → action annotation → rationale generation → dataset mixing → SFT training → evaluation. Any failure in the annotation pipeline will directly impact model quality.
- Design tradeoffs: The choice between using answer forcing vs. GPT exploration for action annotation involves a tradeoff between scalability (answer forcing can reprocess failed trajectories) and potential bias (answer forcing assumes access to gold answers). The decision to mix generalist data involves balancing specialized agent capability with general reasoning ability.
- Failure signatures: If the model shows good performance on held-in tasks but poor generalization to held-out tasks, this suggests insufficient diversity in the training trajectories. If the model performs poorly on both, the annotation quality or base model capacity may be insufficient.
- First 3 experiments:
  1. Train a baseline model on a subset of AgentBank (e.g., 1k trajectories) and evaluate on held-in tasks to establish minimum performance.
  2. Train with varying mixture ratios of generalist instruction data (0%, 10%, 20%, 50%) to find the optimal balance between specialized and general capabilities.
  3. Train with and without CoT rationales on a held-in task to quantify the impact of rationales on performance.

## Open Questions the Paper Calls Out

Open Question 1
- Question: How does the performance of SAMOYED scale with model size beyond 13B parameters?
- Basis in paper: [inferred] The paper mentions that experiments were conducted on 7B and 13B models, but does not explore larger model sizes.
- Why unresolved: The authors explicitly state that they leave the comparison at larger scales (e.g., Lemur-70B and xLAM-8×7B) for future work due to limited resources.
- What evidence would resolve it: Training and evaluating SAMOYED models at larger scales (e.g., 30B, 70B) and comparing their performance on held-in and held-out tasks.

Open Question 2
- Question: How would incorporating more sophisticated agent mechanisms (e.g., Reflexion, ReWOO) affect SAMOYED's performance?
- Basis in paper: [explicit] The authors mention that they have not fully explored the potential of equipping SAMOYED with more sophisticated agent mechanisms like Reflexion and ReWOO.
- Why unresolved: The paper focuses on improving agent performance through supervised fine-tuning on expert trajectories and does not investigate the impact of advanced agent mechanisms.
- What evidence would resolve it: Training SAMOYED with integrated Reflexion or ReWOO mechanisms and evaluating their performance on held-in and held-out tasks.

Open Question 3
- Question: How would exploration-based methods compare to trajectory tuning for optimizing LLM agents?
- Basis in paper: [explicit] The authors mention that they primarily focus on improving agent performance via supervised fine-tuning on expert trajectories and leave the investigation of exploration-based methods (e.g., Song et al., 2024; Xiong et al., 2024) for future work.
- Why unresolved: The paper does not explore or compare the effectiveness of exploration-based methods in optimizing LLM agents.
- What evidence would resolve it: Implementing and evaluating exploration-based methods for optimizing LLM agents and comparing their performance with trajectory tuning on held-in and held-out tasks.

Open Question 4
- Question: How would the development of strong generalized multi-agent systems based on open-source LLMs compare to single-agent models like SAMOYED?
- Basis in paper: [explicit] The authors mention that they are centered around building strong ReAct-style single-agent models and that the development of strong generalized multi-agent systems based on open-source LLMs is still an under-explored area.
- Why unresolved: The paper does not investigate or compare the performance of multi-agent systems with single-agent models like SAMOYED.
- What evidence would resolve it: Developing and evaluating multi-agent systems based on open-source LLMs and comparing their performance with SAMOYED on held-in and held-out tasks.

## Limitations

- The evaluation methodology lacks detailed contamination analysis and unclear test set boundaries, particularly for held-in tasks where GPT-3.5-Turbo comparison is made
- The effectiveness of the separated annotation pipeline is supported by internal comparisons but lacks direct validation through downstream task performance
- Scaling analysis shows correlation between trajectory quantity and performance but doesn't definitively prove causation versus other factors like annotation quality improvements

## Confidence

**High Confidence**: The dataset construction methodology and its technical implementation are well-documented and reproducible. The claim that AgentBank is the largest open agent trajectory dataset with over 50k trajectories is directly verifiable through the provided data. The mechanism of mixing generalist instruction and code data with trajectory data to mitigate catastrophic forgetting is supported by clear ablation studies.

**Medium Confidence**: The generalization claims to held-out tasks are reasonably supported by the experimental design but could be strengthened with more rigorous contamination analysis. The scaling trends showing improvement with more trajectories demonstrate correlation but don't definitively prove causation versus other factors.

**Low Confidence**: The claim about separating action and rationale generation reducing difficulty bias relies primarily on internal pipeline comparisons rather than downstream task performance improvements. The assertion that this separation is crucial for building the dataset lacks strong empirical validation.

## Next Checks

1. **Contamination Analysis**: Perform detailed n-gram overlap analysis between training trajectories and evaluation tasks across all 16 domains to quantify potential data contamination, particularly for the held-in tasks where GPT-3.5-Turbo comparison is made.

2. **Annotation Strategy Comparison**: Train models using different annotation strategies (simultaneous vs. separated action/rationale generation) on identical trajectory subsets and compare their performance on held-out tasks to directly validate the difficulty bias reduction claim.

3. **Trajectory Diversity vs. Quantity**: Design an experiment that controls for diversity while varying trajectory quantity (e.g., using trajectories from fewer tasks but with the same total count) to isolate whether scaling performance improvements come from diversity breadth or sheer volume.