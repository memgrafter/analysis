---
ver: rpa2
title: 'TWOLAR: a TWO-step LLM-Augmented distillation method for passage Reranking'
arxiv_id: '2403.17759'
source_url: https://arxiv.org/abs/2403.17759
tags:
- queries
- documents
- retrieval
- reranking
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TWOLAR is a two-step knowledge distillation method for passage
  reranking that leverages LLM reranking capabilities to create a diverse training
  dataset and introduce a novel scoring strategy. The approach uses 20K queries with
  documents retrieved via four distinct methods, reranked by ChatGPT to generate high-quality
  training data.
---

# TWOLAR: a TWO-step LLM-Augmented distillation method for passage Reranking

## Quick Facts
- arXiv ID: 2403.17759
- Source URL: https://arxiv.org/abs/2403.17759
- Authors: Davide Baldelli; Junfeng Jiang; Akiko Aizawa; Paolo Torroni
- Reference count: 40
- TWOLAR achieves state-of-the-art performance on TREC-DL and BEIR benchmarks using 3B parameter model, matching models with 11B-20B parameters

## Executive Summary
TWOLAR introduces a novel two-step knowledge distillation approach for passage reranking that leverages LLM capabilities to generate high-quality training data. The method uses 20K queries with documents retrieved through four distinct methods, then reranked by ChatGPT to create diverse training datasets. This approach addresses the challenge of limited high-quality labeled data in information retrieval while achieving superior performance compared to existing supervised and LLM distillation methods.

The paper demonstrates that TWOLAR can match or exceed state-of-the-art performance on multiple benchmarks while using models with significantly fewer parameters. On TREC-DL2019 and TREC-DL2020, TWOLAR-xl achieves nDCG@10 scores of 73.51 and 70.84 respectively, outperforming both supervised methods and other LLM distillation approaches.

## Method Summary
TWOLAR employs a two-step knowledge distillation process that begins with generating high-quality training data using ChatGPT-3.5-turbo. The method uses 20K queries with documents retrieved via four distinct methods, creating diverse input for the LLM reranker. ChatGPT then reorders these documents, producing scores and orderings that serve as pseudo-labels for training. A novel scoring strategy is introduced that transforms these LLM scores into usable training targets, addressing the challenge of ordinal ranking supervision. The distilled model is trained on this synthetic dataset to learn reranking capabilities without requiring extensive human-labeled data.

## Key Results
- TWOLAR-xl achieves nDCG@10 scores of 73.51 and 70.84 on TREC-DL2019 and TREC-DL2020 respectively
- Matches and sometimes outperforms state-of-the-art models with three orders of magnitude more parameters (3B vs 11B-20B)
- Demonstrates strong performance across BEIR benchmark datasets
- Significantly enhances document reranking ability compared to traditional supervised methods

## Why This Works (Mechanism)
The effectiveness of TWOLAR stems from leveraging LLM's superior reranking capabilities to create high-quality training data that captures complex ranking patterns. By using multiple retrieval methods and a diverse set of queries, the approach ensures the distilled model learns robust reranking patterns rather than overfitting to specific retrieval patterns. The novel scoring strategy transforms LLM outputs into appropriate training targets that preserve the ordinal nature of ranking while being amenable to supervised learning.

## Foundational Learning
**LLM-based Knowledge Distillation**: Required for understanding how large language models can transfer their capabilities to smaller models through synthetic data generation. Quick check: Verify understanding of teacher-student model relationships in distillation.

**Information Retrieval Metrics**: Essential for evaluating reranking performance using measures like nDCG@10. Quick check: Calculate nDCG for a sample ranked list to confirm metric comprehension.

**Ordinal Regression**: Needed for understanding how ranking scores can be treated as ordinal targets rather than absolute values. Quick check: Distinguish between regression and ordinal regression in ranking contexts.

**Cross-Modal Learning**: Important for understanding how text-based LLM outputs can effectively train models for information retrieval tasks. Quick check: Identify scenarios where cross-modal knowledge transfer is beneficial.

**Data Augmentation in IR**: Crucial for grasping how synthetic data can improve model performance when labeled data is scarce. Quick check: Compare different data augmentation strategies for ranking tasks.

## Architecture Onboarding

Component Map: Query Generator -> Document Retrieval (4 methods) -> LLM Reranker -> Score Transformer -> Distilled Model

Critical Path: The most important sequence is Query Generator → Document Retrieval → LLM Reranker → Score Transformer → Distilled Model, as this chain creates the synthetic training data that enables the entire approach.

Design Tradeoffs: The method trades computational cost of LLM inference during training data generation for superior model performance with fewer parameters. This approach requires significant upfront investment in generating synthetic data but results in more efficient models at inference time.

Failure Signatures: Poor performance may manifest if the LLM reranking introduces systematic biases, if the score transformation doesn't adequately preserve ranking quality, or if the diverse retrieval methods don't provide sufficient coverage of the query space.

First Experiments:
1. Validate score transformation preserves ordinal relationships by comparing LLM rankings with transformed scores
2. Test model performance on held-out queries from the same distribution as training data
3. Evaluate sensitivity to the number of queries and diversity of retrieval methods

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Heavy reliance on ChatGPT-3.5-turbo for training data generation introduces cost and reproducibility concerns
- Evaluation focuses primarily on nDCG@10 metrics, potentially missing other relevant quality measures
- Claims of parameter efficiency based on specific model choices without accounting for architectural differences

## Confidence
**Generalizability beyond tested datasets** (High confidence): Results may not transfer to other domains or languages
**Impact of LLM choice on performance** (Medium confidence): Different LLM models could significantly affect results
**Sufficiency of training data diversity** (Medium confidence): Current diversity may not prevent overfitting

## Next Checks
1. Test TWOLAR on additional benchmarks outside BEIR and TREC-DL to assess domain generalizability
2. Conduct ablation studies comparing different LLM models for distillation to quantify their impact on final performance
3. Evaluate model performance with different nDCG cutoffs (e.g., nDCG@5 vs nDCG@20) to understand sensitivity to different ranking depths