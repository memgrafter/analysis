---
ver: rpa2
title: Formal Theorem Proving by Rewarding LLMs to Decompose Proofs Hierarchically
arxiv_id: '2411.01829'
source_url: https://arxiv.org/abs/2411.01829
tags:
- proof
- lemmas
- theorem
- proofs
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to improve LLMs' theorem-proving abilities
  by hierarchically decomposing proofs into lemmas. The approach uses reinforcement
  learning with rewards for both complete proofs and correct novel lemmas, even when
  the full theorem cannot be proven.
---

# Formal Theorem Proving by Rewarding LLMs to Decompose Proofs Hierarchically

## Quick Facts
- arXiv ID: 2411.01829
- Source URL: https://arxiv.org/abs/2411.01829
- Authors: Kefan Dong; Arvind Mahankali; Tengyu Ma
- Reference count: 22
- Primary result: Improved theorem-proving performance using hierarchical decomposition with reinforcement learning

## Executive Summary
This paper presents a method to improve LLMs' theorem-proving abilities by hierarchically decomposing proofs into lemmas. The approach uses reinforcement learning with rewards for both complete proofs and correct novel lemmas, even when the full theorem cannot be proven. During training, the model generates and proves new lemmas not present in the training dataset (37.7% of proved lemmas are novel). The method, called Proof Decomposer with RL (ProD-RL), outperforms supervised fine-tuning, improving pass rates from 40.8% to 45.5% on the AFP test set and from 36.5% to 39.5% on an out-of-distribution test set.

## Method Summary
The method involves supervised fine-tuning of a pretrained LLM on formal theorem-proving data, followed by reinforcement learning that rewards correct partial proofs and novel lemmas. The model generates proof trees where child nodes represent proposed lemmas, and correctness is verified by the Isabelle proof assistant. The approach is particularly effective for theorems of low-to-medium difficulty but less robust to distribution shifts compared to baseline methods.

## Key Results
- Pass@16 rates improve from 40.8% to 45.5% on AFP test set
- Pass@16 rates improve from 36.5% to 39.5% on out-of-distribution test set
- 37.7% of training replay buffer consists of novel lemmas proposed and proved during training
- Most significant improvements observed for theorems with ground-truth proof depth ≤ 2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The RL reward for correct partial proofs (sub-trees) enables learning from incomplete but valid progress
- Mechanism: When a theorem is too hard to prove completely, the model still receives positive reward for any correct and novel lemmas it proposes and proves, creating a credit assignment signal even for failed top-level attempts
- Core assumption: Correct lemmas are useful stepping stones that will generalize to proving harder theorems in the future
- Evidence anchors:
  - [abstract] "Our reward mechanism is inspired by how mathematicians train themselves: even if a theorem is too challenging to be proved by the current model, a positive reward is still given to the model for any correct and novel lemmas that are proposed and proved in this process"
  - [section 3.2] "we reward correct partial proofs (i.e., proof sub-trees) even if the original theorem (i.e., the root node) is not proved entirely"
  - [corpus] Weak - no direct evidence about effectiveness of partial reward mechanism
- Break condition: If proposed lemmas are too trivial or too specific to individual problems, they won't generalize to new theorems

### Mechanism 2
- Claim: Hierarchical decomposition into lemmas reduces the effective complexity of theorem proving
- Mechanism: By proposing lemmas that break down a complex theorem into simpler sub-problems, the model can tackle each piece with higher probability of success rather than facing the full problem directly
- Core assumption: Many theorems can be decomposed into simpler lemmas that are within the model's current capability
- Evidence anchors:
  - [abstract] "This paper presents a method to improve LLMs' theorem-proving abilities by hierarchically decomposing proofs into lemmas"
  - [section 2] "the theorem prover needs to propose and prove lemmas to decompose the proof hierarchically itself"
  - [corpus] Weak - no direct evidence about decomposition effectiveness
- Break condition: If the theorem structure doesn't allow clean decomposition, or if the decomposition requires domain knowledge the model lacks

### Mechanism 3
- Claim: Novel lemma generation during training creates a curriculum of increasingly difficult problems
- Mechanism: The model proposes and proves lemmas not in the training dataset, which then become new training examples, creating a self-improving cycle where the model gradually learns to prove harder and harder statements
- Core assumption: The model can generate meaningful novel lemmas that extend beyond the training distribution
- Evidence anchors:
  - [abstract] "During training, our model proposes and proves lemmas that are not in the training dataset. In fact, these newly-proposed correct lemmas consist of 37.7% of the training replay buffer"
  - [section 3.2] "During training, our model proposes and proves lemmas that are not in the training dataset"
  - [corpus] Weak - no evidence about quality or utility of novel lemmas
- Break condition: If novel lemmas are mostly trivial restatements or don't meaningfully extend the model's capabilities

## Foundational Learning

- Concept: Proof verification in formal systems (Isabelle/Lean)
  - Why needed here: The entire approach relies on automatically checking whether generated proofs are correct, which requires understanding how formal proof assistants work
  - Quick check question: What is the difference between local correctness and global correctness of a proof tree node?

- Concept: Reinforcement learning with sparse rewards
  - Why needed here: The training method uses RL to learn proof generation, which requires understanding credit assignment and reward shaping techniques
  - Quick check question: How does hindsight experience replay apply to this theorem proving setup?

- Concept: Hierarchical task decomposition
  - Why needed here: The core technique involves breaking down complex proofs into simpler lemmas, which requires understanding when and how to decompose problems
  - Quick check question: What are the trade-offs between proposing many small lemmas versus fewer larger lemmas?

## Architecture Onboarding

- Component map: Pretrained LLM (7B parameters) -> Formal verifier (Isabelle) -> RL training loop -> Replay buffer -> Value function network
- Critical path: Theorem statement → LLM generates conditional proof with lemmas → Each lemma proved separately → Theorem proved using lemmas → Verifier checks all components → Reward assigned → Model updated
- Design tradeoffs:
  - Lemma proposal vs. direct proof: More lemmas can simplify individual steps but increase total proof length
  - Depth limit: Prevents infinite proof trees but may cut off valid long proofs
  - Temperature during RL: Higher exploration vs. more consistent outputs
  - Context truncation: Faster generation vs. potential loss of relevant information
- Failure signatures:
  - Model proposes trivial lemmas that don't simplify the problem
  - Proof trees grow too deep without reaching conclusion
  - Generated lemmas are not globally useful despite being locally correct
  - Performance degrades on out-of-distribution theorems
- First 3 experiments:
  1. Compare pass@16 rates on AFP test set between SFT baseline and ProD-RL with varying proof tree depths
  2. Measure percentage of novel lemmas in the replay buffer over RL training rounds
  3. Analyze pass rates by ground-truth proof depth to identify which difficulty levels benefit most from hierarchical decomposition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the ProD-RL method be extended to handle more complex theorems that require deeper proof hierarchies?
- Basis in paper: [explicit] The paper notes that the improvement of ProD-RL is mostly significant for theorems with low-to-medium difficulty (ground-truth proof depth ≤ 2), and that both models' pass rates are low for more complex theorems.
- Why unresolved: The current method may not be scaling effectively to handle deeper proof hierarchies, and the paper does not explore techniques to address this limitation.
- What evidence would resolve it: Testing ProD-RL on theorems with deeper ground-truth proof hierarchies and comparing its performance to baseline methods, along with exploring architectural changes or training techniques to improve performance on complex theorems.

### Open Question 2
- Question: What causes the ProD-RL method to be less robust to distribution shifts compared to baseline methods?
- Basis in paper: [explicit] The paper observes that ProD-RL is less robust to distribution shifts, performing worse than SFT w/o lemma proposal on the miniF2F dataset, which contains theorems very different from the training data.
- Why unresolved: The paper does not investigate the specific reasons for this reduced robustness, leaving the underlying causes unexplored.
- What evidence would resolve it: Analyzing the types of lemmas proposed and the structure of proofs generated by ProD-RL on out-of-distribution datasets, and comparing them to those generated by baseline methods, could reveal patterns contributing to the reduced robustness.

### Open Question 3
- Question: How can the ProD-RL method be adapted to generate more meaningful and useful lemmas for theorems that do not require hierarchical decomposition?
- Basis in paper: [explicit] The paper notes that when tested on miniF2F theorems, the model failed to propose meaningful lemmas, suggesting that proving these types of mathematics questions typically does not require hierarchical decomposition.
- Why unresolved: The current method focuses on hierarchical decomposition, which may not be the most effective approach for all types of theorems, particularly those that can be solved with direct proofs.
- What evidence would resolve it: Experimenting with modifications to the ProD-RL method to encourage the generation of more diverse types of lemmas, or developing alternative methods that can effectively handle both hierarchical and direct proof approaches, and evaluating their performance on a variety of theorem types.

## Limitations
- The method shows diminishing returns on harder theorems and struggles with distribution shifts
- Heavy reliance on Isabelle proof assistant for verification creates potential bottlenecks
- 37.7% novel lemma generation raises questions about whether these represent meaningful mathematical advances

## Confidence
- **High Confidence**: The core experimental results showing improved pass@16 rates on AFP test sets (40.8% to 45.5%) are well-documented and verifiable
- **Medium Confidence**: The claim about 37.7% novel lemmas being generated during training is supported by corpus evidence but lacks detailed analysis of lemma quality and usefulness
- **Low Confidence**: The assertion that hierarchical decomposition is the primary driver of improvement, as opposed to other factors like better RL optimization or improved generation strategies

## Next Checks
1. **Novel Lemma Quality Analysis**: Conduct a detailed examination of the 37.7% novel lemmas to determine if they represent genuinely useful mathematical abstractions or merely dataset-specific variations, including expert assessment of their mathematical significance
2. **Cross-System Generalization Test**: Evaluate ProD-RL on a different formal proof system (e.g., Lean or Coq) to determine if the hierarchical decomposition approach generalizes beyond Isabelle, or if it's tightly coupled to Isabelle's specific proof language
3. **Scalability to Harder Theorems**: Test the method on theorems requiring proof depths beyond the current depth limit to assess whether the approach can scale to more complex mathematical problems, or if performance degrades significantly for deeper proofs