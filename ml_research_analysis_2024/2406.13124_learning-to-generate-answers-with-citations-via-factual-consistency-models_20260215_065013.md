---
ver: rpa2
title: Learning to Generate Answers with Citations via Factual Consistency Models
arxiv_id: '2406.13124'
source_url: https://arxiv.org/abs/2406.13124
tags:
- citation
- calf
- citations
- factual
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CaLF, a weakly-supervised fine-tuning method
  that uses factual consistency models (FCMs) to improve citation generation in large
  language models (LLMs). The method alternates between generating diverse responses
  with citations and fine-tuning the LLM using FCM-filtered data, while emphasizing
  factual unit tokens via focused learning.
---

# Learning to Generate Answers with Citations via Factual Consistency Models

## Quick Facts
- **arXiv ID:** 2406.13124
- **Source URL:** https://arxiv.org/abs/2406.13124
- **Reference count:** 40
- **Primary result:** CaLF achieves 34.1, 15.5, and 10.5 citation F1 improvements over in-context learning, vanilla fine-tuning, and state-of-the-art methods, respectively, while maintaining response quality.

## Executive Summary
The paper introduces CaLF, a weakly-supervised fine-tuning method that uses factual consistency models (FCMs) to improve citation generation in large language models. The method alternates between generating diverse responses with citations and fine-tuning the LLM using FCM-filtered data, while emphasizing factual unit tokens via focused learning. On the ALCE few-shot citation benchmark, CaLF achieves significant citation F1 improvements while maintaining response quality and demonstrating robust generalization through domain transfer experiments.

## Method Summary
CaLF is a weakly-supervised fine-tuning method that alternates between generating diverse answer candidates with citations and fine-tuning the LLM on FCM-filtered data. The approach uses a factual consistency model to filter generated answers, retaining only those where cited passages support the generated statements. During fine-tuning, Shapley-based token relevance weights are computed to emphasize factual unit tokens through focused learning. The training process dynamically stops when the proportion of filtered examples no longer improves or after reaching a maximum number of iterations.

## Key Results
- Achieves 34.1 citation F1 improvement over in-context learning on ALCE benchmark
- Shows 15.5 citation F1 improvement over vanilla fine-tuning
- Demonstrates 10.5 citation F1 improvement over state-of-the-art methods
- Maintains response quality while improving factuality (lower factual error rate)
- Shows robust generalization in domain transfer experiments with Hagrid dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Alternating between generating diverse responses and fine-tuning on FCM-filtered data enriches the training set with high-quality cited examples.
- **Mechanism:** In each iteration, the LLM generates a large set of candidate answers. An FCM filters this set to retain only those answers where cited passages support the generated statements. Fine-tuning on this filtered subset focuses learning on factual consistency.
- **Core assumption:** The FCM can reliably detect factual consistency between a generated statement and its cited passages.
- **Evidence anchors:**
  - [abstract] "Our approach alternates between generating texts with citations and supervised fine-tuning with FCM-filtered citation data."
  - [section 4.1] "Each answer candidate ˆy ∈ ˆY is subsequently filtered via an answer quality assurance function Qϕ..."
- **Break condition:** If the FCM is biased or unreliable, the filtered training data will include incorrect citations, causing the model to learn to hallucinate or cite irrelevant passages.

### Mechanism 2
- **Claim:** Focused learning re-weights the loss contribution of tokens based on their relevance to factual consistency, encouraging the model to prioritize learning citation-relevant tokens.
- **Mechanism:** Shapley values from the FCM measure each token's importance for predicting factual consistency. These relevance weights are mapped to the LLM's tokenization and used to scale the loss during fine-tuning, so more important tokens have a larger impact on updates.
- **Core assumption:** Shapley-based token relevance correlates with the token's importance for producing accurate citations.
- **Evidence anchors:**
  - [section 4.2] "We measure the relevance for each token in an answer for ensuring the factual consistency... and subsequently modify the NLL loss computation... for the instruction-tuning of the LLM F by re-weighting the loss contribution of the t-th token according to relevance weights wt ∈ W."
  - [section 4.2] "The intuition is to have the LLM concentrate on tokens related to factual knowledge during fine-tuning..."
- **Break condition:** If the token alignment between the FCM and LLM is inaccurate, irrelevant tokens may be over-weighted or important ones under-weighted, harming citation quality.

### Mechanism 3
- **Claim:** Iterative training with a dynamic stopping criterion balances training progress with computational efficiency.
- **Mechanism:** After each iteration, CaLF checks whether the proportion of filtered examples has improved. Training stops when this proportion no longer improves or after a maximum number of iterations. This ensures learning stabilizes without excessive computation.
- **Core assumption:** The ratio of filtered examples correlates with citation performance improvements.
- **Evidence anchors:**
  - [section 4] "The number of iterations is determined dynamically by stopping when the proportion of filtered examples over the candidates does not improve between iterations or once the maximum number of iterations is reached."
  - [section 6, Iterative Training] "We see a majority of citation performance improvements within the first three iterations after which citation F1 stabilizes."
- **Break condition:** If the stopping criterion is too conservative, training may halt before full convergence; if too lenient, unnecessary iterations waste resources.

## Foundational Learning

- **Concept:** Factual consistency models (FCMs)
  - **Why needed here:** FCMs provide a differentiable, model-based signal for whether a generated statement is factually supported by cited passages, enabling weak supervision and token-level relevance weighting.
  - **Quick check question:** What is the output space of the FCM used in CaLF, and why is a continuous score preferred over a binary one?

- **Concept:** Shapley values for feature importance
  - **Why needed here:** Shapley values assign each token a contribution score for the FCM's consistency prediction, enabling fine-grained re-weighting of the training loss.
  - **Quick check question:** How does normalizing Shapley values per sentence prevent bias toward shorter or longer sentences?

- **Concept:** Token alignment between models with different tokenizers
  - **Why needed here:** The FCM and LLM may use different tokenizers; aligning their token spans is necessary to map FCM relevance scores to LLM token weights.
  - **Quick check question:** What strategy does CaLF use to handle cases where multiple LLM tokens align to a single FCM token?

## Architecture Onboarding

- **Component map:** Question input → Retriever → Passage set P → P + question → LLM → answer candidate set ˆY → ˆY + P → FCM → filter via Qϕ → weak training set ˆD → ˆD + few-shot D → CaLF training loop: Generate → Filter → Compute Shapley → Align → Fine-tune → Output: fine-tuned LLM Fk
- **Critical path:** Generate → Filter → Compute Shapley → Align → Fine-tune. Any failure in this chain (e.g., FCM not producing scores, Shapley computation error, alignment mismatch) will break training.
- **Design tradeoffs:**
  - FCM choice: AlignScore is cheaper than TRUE but slightly less accurate on factual consistency; more accurate FCMs improve citation quality but increase computation.
  - Token alignment: Exact span alignment is complex; CaLF uses a heuristic average-over-span approach, which is efficient but may misalign in rare cases.
  - Stopping criterion: Dynamic ratio check is efficient but may miss subtle performance plateaus; fixed iteration count is simpler but less adaptive.
- **Failure signatures:**
  - Citation F1 plateaus early → FCM filtering too strict or LLM not generating diverse enough candidates.
  - Training diverges → Shapley relevance weights too extreme; check normalization and alignment logic.
  - No improvement across iterations → Check FCM quality, retrieval coverage, or few-shot D diversity.
- **First 3 experiments:**
  1. **Sanity check:** Run CaLF with a trivial FCM (e.g., always outputs 1) to confirm training loop works and training does not diverge.
  2. **Ablation test:** Run CaLF without focused learning (LFL) to measure its contribution; compare citation F1 to full CaLF.
  3. **Token alignment validation:** Create a small hand-crafted example where FCM and LLM tokenizations are known; verify that the alignment algorithm maps relevance scores correctly.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the quality of the factual consistency model (FCM) impact the performance of CaLF?
- **Basis in paper:** [explicit] The paper investigates the extent to which the quality of an FCM translates to improved citation production for CaLF by comparing the quality of different FCMs on the TRUE benchmark and their corresponding CaLF performance on ASQA.
- **Why unresolved:** The paper only tests a limited number of FCMs and does not explore the full range of possible FCM qualities. It is unclear how much the performance of CaLF would degrade if a lower-quality FCM were used.
- **What evidence would resolve it:** A comprehensive study comparing the performance of CaLF using a wide range of FCMs with varying qualities would be needed.

### Open Question 2
- **Question:** Can CaLF be extended to handle questions that require multiple steps of reasoning or involve complex information-seeking tasks?
- **Basis in paper:** [inferred] The paper focuses on improving citation generation for long-form question answering tasks, but does not explicitly address the ability of CaLF to handle complex information-seeking tasks that require multiple steps of reasoning.
- **Why unresolved:** The paper does not provide any evidence or discussion on the ability of CaLF to handle such tasks. It is unclear whether the current approach would be sufficient or if modifications would be needed.
- **What evidence would resolve it:** Experiments evaluating the performance of CaLF on complex information-seeking tasks that require multiple steps of reasoning would be needed.

### Open Question 3
- **Question:** How does CaLF compare to other methods for improving the factuality of language model generations, such as direct preference optimization or retrieval-augmented generation with critique tokens?
- **Basis in paper:** [explicit] The paper compares CaLF to in-context learning, vanilla fine-tuning, and state-of-the-art methods like Self-RAG on citation generation benchmarks. However, it does not directly compare CaLF to other factuality improvement methods like direct preference optimization or retrieval-augmented generation with critique tokens.
- **Why unresolved:** The paper does not provide any evidence or discussion on the relative effectiveness of CaLF compared to these other methods. It is unclear whether CaLF would outperform or underperform these methods on factuality metrics.
- **What evidence would resolve it:** Experiments directly comparing the performance of CaLF to these other methods on factuality benchmarks would be needed.

## Limitations

- The quality and robustness of the factual consistency model (FCM) directly determines whether the training data is truly high-quality, and the paper only tests a limited number of FCMs.
- The token alignment between FCM and LLM tokenizers uses a heuristic approach that may misalign in cases of tokenization differences, potentially causing important tokens to be under-weighted or irrelevant ones over-weighted.
- The stopping criterion based on the ratio of filtered examples improving may not perfectly correlate with citation performance and could halt training prematurely or run unnecessary iterations.

## Confidence

- **High confidence:** The overall framework of alternating generation and fine-tuning with FCM-filtered data is sound and well-supported by results. The citation F1 improvements are substantial and statistically significant.
- **Medium confidence:** The Shapley-based focused learning mechanism is theoretically justified, but the practical implementation details (particularly token alignment) introduce uncertainty about whether the mechanism works as intended in all cases.
- **Medium confidence:** Domain transfer experiments show generalization, but the paper only tests one additional domain (Hagrid) and doesn't analyze which factors contribute to successful transfer versus failure.

## Next Checks

1. **FCM reliability validation:** Create a controlled test set with known factual consistency relationships and measure the FCM's precision and recall. Compare citation F1 improvements when using different FCMs (AlignScore vs. TRUE) to quantify the impact of FCM quality on the overall method.

2. **Token alignment error analysis:** Construct small examples where FCM and LLM tokenizations differ significantly, then trace through the alignment algorithm to identify cases where relevance weights are misassigned. Measure how often such misalignments occur in the actual training data and their impact on citation quality.

3. **Stopping criterion sensitivity:** Run CaLF with fixed iteration counts (1, 3, 5, 10) and compare citation F1 trajectories. Analyze whether the dynamic stopping criterion consistently identifies the optimal stopping point or whether it sometimes halts training prematurely or runs unnecessary iterations.