---
ver: rpa2
title: 'Mixed Preference Optimization: Reinforcement Learning with Data Selection
  and Better Reference Model'
arxiv_id: '2403.19443'
source_url: https://arxiv.org/abs/2403.19443
tags:
- reward
- training
- dataset
- preference
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes MPO (Mixed Preference Optimization), a two-stage
  training method that combines the strengths of RLHF and DPO for LLM alignment. The
  key innovation is a curriculum learning approach: first train DPO on an "easy" dataset
  (preference pairs with large reward score differences), then perform RLHF on a "difficult"
  dataset (pairs with small reward differences) using the trained DPO model as the
  reference.'
---

# Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model

## Quick Facts
- arXiv ID: 2403.19443
- Source URL: https://arxiv.org/abs/2403.19443
- Authors: Qi Gou; Cam-Tu Nguyen
- Reference count: 22
- Primary result: MPO outperforms vanilla DPO and PPO in LLM alignment tasks using a two-stage curriculum learning approach

## Executive Summary
This paper introduces Mixed Preference Optimization (MPO), a novel two-stage training method that combines the strengths of Direct Preference Optimization (DPO) and Reinforcement Learning from Human Feedback (RLHF) for aligning large language models. MPO addresses key limitations in both DPO (distribution shift) and PPO (sample efficiency) by implementing a curriculum learning approach. The method first trains DPO on an "easy" dataset with large reward score differences between preference pairs, then performs PPO on a "difficult" dataset with small reward differences using the trained DPO model as the reference. Experiments on HH-RLHF and TLDR datasets demonstrate that MPO outperforms both vanilla DPO and PPO across reward-based, GPT-4, and human evaluations while being more sample-efficient than PPO.

## Method Summary
MPO implements a two-stage training pipeline for LLM alignment. In Stage 1, a reward model is trained to distinguish preferred completions, and preference pairs are split into "easy" (large reward differences) and "difficult" (small reward differences) datasets based on a threshold. DPO is then trained on the easy dataset to quickly obtain a relatively optimal policy. In Stage 2, PPO is performed on the difficult dataset using the trained DPO model as the reference rather than the standard SFT model, with a KL-divergence constraint keeping the policy close to the DPO model. This approach leverages the computational efficiency of DPO for initial alignment while using PPO's online refinement capabilities to handle distribution shift in more ambiguous preference pairs.

## Key Results
- MPO outperforms vanilla DPO and PPO on HH-RLHF and TLDR datasets across reward-based, GPT-4, and human evaluations
- MPO achieves better sample efficiency than vanilla PPO, requiring less data for training
- Using DPO as the reference model in PPO training improves alignment effectiveness compared to using SFT
- The two-stage curriculum learning approach effectively mitigates distribution shift issues in DPO

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MPO mitigates distribution shift in DPO by using a two-stage training process that starts with easy-to-distinguish preference pairs.
- Mechanism: The first stage uses DPO on an "easy" dataset (large reward differences), quickly obtaining a relatively optimal policy. The second stage uses PPO with this DPO model as the reference on a "difficult" dataset (small reward differences), allowing online refinement to handle distribution shift.
- Core assumption: Reward differences between completion pairs reliably indicate their distinguishability for DPO training.
- Evidence anchors: [abstract] "The key innovation is a curriculum learning approach: first train DPO on an 'easy' dataset (preference pairs with large reward score differences), then perform RLHF on a 'difficult' dataset (pairs with small reward differences)"

### Mechanism 2
- Claim: Using DPO as the reference model in PPO training improves alignment effectiveness compared to using the SFT model.
- Mechanism: The PPO optimization problem includes a KL-divergence term that keeps the current policy close to the DPO model rather than the SFT model, allowing exploration in a better region of the policy space.
- Core assumption: The DPO model represents a better-aligned starting point than the SFT model for PPO optimization.
- Evidence anchors: [abstract] "we use DPO as the reference model rather than the SFT model as in vanilla PPO, allowing us to train PPO more effectively with less data."

### Mechanism 3
- Claim: Data quality significantly impacts DPO performance, with noisy preference pairs hindering optimization.
- Mechanism: By filtering out preference pairs with small reward differences (indicating similar quality completions), MPO improves DPO training effectiveness by reducing noise in the training signal.
- Core assumption: Small reward differences between completion pairs indicate noisy or ambiguous human preferences that hurt DPO training.
- Evidence anchors: [section] "DPO is more susceptible to noises caused by response pairs with similar qualities in the dataset."

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: MPO incorporates PPO, which is the core RL algorithm used in RLHF for fine-tuning LLMs based on human preferences.
  - Quick check question: What are the three main steps in the RLHF pipeline, and how does PPO fit into this process?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: MPO uses DPO in its first stage to quickly obtain a good initial policy before refining with PPO.
  - Quick check question: How does DPO differ from traditional RLHF approaches in terms of computational efficiency and convergence speed?

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: PPO is used in MPO's second stage for online refinement of the policy, with the KL-divergence constraint keeping it close to the DPO model.
  - Quick check question: What is the purpose of the KL-divergence constraint in PPO, and how does using DPO as the reference model differ from using SFT?

## Architecture Onboarding

- Component map: SFT model -> Reward model -> Data sampling module -> Data selection module -> DPO trainer -> PPO trainer -> Evaluation framework

- Critical path: 1. Train reward model on preference dataset 2. Generate completions using SFT model 3. Calculate reward differences and split into easy/difficult sets 4. Train DPO on easy set 5. Train PPO on difficult set using DPO as reference 6. Evaluate final model

- Design tradeoffs:
  - Computational cost: MPO requires training both DPO and PPO, but less PPO data than vanilla PPO
  - Data quality vs quantity: MPO uses only a subset of preference pairs for DPO training, trading data volume for quality
  - Model complexity: Adding reward modeling and data selection steps increases implementation complexity

- Failure signatures:
  - Poor reward model accuracy → ineffective easy/difficult split
  - Overfitting to easy set → DPO doesn't generalize well for PPO refinement
  - Insufficient difficult set size → PPO doesn't converge properly
  - Reward hacking → model exploits reward model rather than aligning with true preferences

- First 3 experiments:
  1. Train DPO on easy set only vs full dataset to verify improved performance
  2. Train PPO using SFT vs DPO as reference model to confirm reference model importance
  3. Vary the reward difference threshold for easy/difficult split to find optimal value

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MPO perform when trained on preference datasets with varying degrees of reward score differences between completion pairs?
- Basis in paper: [explicit] The paper demonstrates that MPO uses a threshold θ to partition datasets into easy and hard sets based on reward score differences, but does not explore how different threshold values affect performance.
- Why unresolved: The paper only tests γ = 1 and γ = 2 as threshold values, leaving the optimal threshold selection unexplored.
- What evidence would resolve it: Empirical results showing MPO performance across a range of threshold values (e.g., γ = 0.5, 1, 1.5, 2, 2.5) on multiple datasets would identify the optimal threshold for data selection.

### Open Question 2
- Question: Can MPO be effectively extended to multi-turn dialogue datasets where context history spans multiple responses?
- Basis in paper: [inferred] The paper focuses on single-turn instruction-response pairs from HH-RLHF and TLDR datasets, but does not address scenarios with extended conversational contexts.
- Why unresolved: The methodology section does not discuss how MPO would handle the increased complexity of maintaining coherence across multiple dialogue turns.
- What evidence would resolve it: Experiments applying MPO to multi-turn dialogue datasets (e.g., MultiWOZ, TopicalChat) and comparing performance against baseline methods would demonstrate its effectiveness in conversational settings.

### Open Question 3
- Question: How does MPO's computational efficiency compare to DPO and PPO when scaling to larger base models (e.g., LLaMA-13B, LLaMA-30B)?
- Basis in paper: [explicit] The paper states that MPO is "more sample (consequently, less computation) cost compared to the vanilla PPO" when using LLaMA-2-7B, but does not investigate scaling effects.
- Why unresolved: The experiments only use LLaMA-2-7B, leaving the relationship between model size and computational efficiency unexplored.
- What evidence would resolve it: Comparative runtime and memory usage analysis of MPO, DPO, and PPO across different model sizes (7B, 13B, 30B parameters) would quantify how computational advantages scale with model capacity.

## Limitations

- The effectiveness of the easy/difficult dataset split relies heavily on the quality of the reward model, which is not extensively validated
- The method's success depends on the assumption that reward differences correlate well with human distinguishability, which may not hold consistently across different domains
- The paper only tests MPO on single-turn instruction-response pairs, leaving its effectiveness in multi-turn dialogue settings unexplored

## Confidence

- **Medium confidence** in Mechanism 1 (curriculum learning via easy/difficult split): While conceptually sound, the paper doesn't provide ablation studies showing the impact of different threshold values for splitting datasets
- **Medium confidence** in Mechanism 2 (DPO as better reference model): The theoretical advantage is clear, but empirical evidence comparing PPO with DPO vs SFT references is limited to specific datasets
- **Medium confidence** in overall experimental results: The paper shows improved performance on HH-RLHF and TLDR datasets, but the sample size for human evaluations is not specified

## Next Checks

1. **Ablation study on threshold selection**: Systematically vary the reward difference threshold used to split easy and difficult datasets to identify optimal values and test robustness to this hyperparameter.

2. **Reference model comparison**: Conduct controlled experiments comparing PPO performance when using SFT, DPO (trained on easy set), and DPO (trained on full dataset) as reference models to isolate the effect of the reference model choice.

3. **Reward model validation**: Measure the correlation between reward model scores and human preference judgments on held-out data to verify that the reward model can reliably identify "easy" vs "difficult" preference pairs.