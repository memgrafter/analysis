---
ver: rpa2
title: Evaluating Structural Generalization in Neural Machine Translation
arxiv_id: '2406.13363'
source_url: https://arxiv.org/abs/2406.13363
tags:
- generalization
- sentences
- transformer
- translation
- patterns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SGET, a machine translation dataset designed
  to evaluate compositional generalization, particularly structural generalization,
  which previous benchmarks had overlooked. The dataset was constructed using a rule-based
  approach with PCFGs and RBMT, ensuring strict control over lexical items and syntactic
  structures.
---

# Evaluating Structural Generalization in Neural Machine Translation

## Quick Facts
- **arXiv ID:** 2406.13363
- **Source URL:** https://arxiv.org/abs/2406.13363
- **Reference count:** 23
- **Primary result:** Neural MT models struggle with structural generalization compared to lexical generalization

## Executive Summary
This paper introduces SGET, a machine translation dataset designed to evaluate compositional generalization, particularly structural generalization, which previous benchmarks had overlooked. The dataset was constructed using a rule-based approach with PCFGs and RBMT, ensuring strict control over lexical items and syntactic structures. The evaluation involved three neural models: LSTM, Transformer, and Llama 2. The primary finding was that all models struggled with structural generalization compared to lexical generalization, consistent with results from semantic parsing tasks. The paper also highlighted differences in model performance across tasks and found that the naturalness of sentences had minimal impact on model performance. Additionally, the inclusion of concatenated sentences in training improved model performance on longer sentences and recursion patterns.

## Method Summary
The authors created SGET using a rule-based approach with Probabilistic Context-Free Grammars (PCFGs) to generate English sentences with controlled vocabulary and syntactic structures, then used Rule-Based Machine Translation (RBMT) to produce Japanese translations. The dataset includes 26 generalization categories covering recursion, passivization, dative shift, and morphological variations. Three neural models were evaluated: LSTM and Transformer trained from scratch using OpenNMT-py, and Llama 2 fine-tuned with LoRA. Models were assessed using Exact Match, BLEU, and Partial Match metrics on both test and generalization sets.

## Key Results
- All models showed significantly worse structural generalization compared to lexical generalization
- Morphological generalization patterns (tense alternation, argument structure) were particularly challenging
- Including concatenated sentences in training improved performance on recursion patterns
- Sentence naturalness had minimal impact on model performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Controlled rule-based dataset construction isolates structural generalization from lexical and length confounds.
- **Mechanism:** PCFGs generate sentences with strictly defined vocabulary and syntax; RBMT ensures controlled parallel translations without distributional overlap between train and generalization sets.
- **Core assumption:** Generated sentences without selectional restrictions are "unnatural enough" to stress models but still evaluable.
- **Evidence anchors:** [abstract] "We adopt a rule-based method to construct SGET so that we can control lexical items and syntactic structures..."; [section] "This rule-based method generates sentences with only the defined vocabulary and production rules..."
- **Break condition:** If RBMT introduces hidden lexical overlap or models rely heavily on word-frequency cues, the control breaks.

### Mechanism 2
- **Claim:** Including concatenated sentences in training removes length generalization as a confound, allowing cleaner recursion-depth evaluation.
- **Mechanism:** Longer sentences created by concatenating shorter ones expose models to sentence lengths exceeding those in generalization set, ensuring poor recursion performance is not due to sentence length.
- **Core assumption:** Concatenation preserves grammatical dependencies without introducing novel syntactic patterns beyond recursion.
- **Evidence anchors:** [section] "We also concatenate sentences to make them longer than any sentence in the generalization set..."; [section] "Table 6 gives the results of Transformer and Llama 2 trained or fine-tuned with/without concatenated sentences..."
- **Break condition:** If concatenated sentences create unintended recursive structures or change distribution of syntactic depths, confounds re-enter.

### Mechanism 3
- **Claim:** Morphological generalization difficulty is separable from structural generalization by controlling tense alternation patterns.
- **Mechanism:** Tense alternation patterns require correct morphological inflection (e.g., pastâ†’present) while keeping syntactic structure constant, isolating morphological from structural generalization.
- **Core assumption:** Models must generate correct target-language verb suffixes without relying on lexical overlap in training.
- **Evidence anchors:** [section] "We propose a generalization category for tense... This requires morphological generalization..."; [section] "Unsurprisingly, in the patterns requiring morphological generalizations to an unseen tense form..."
- **Break condition:** If models rely on lexical similarity across tenses rather than morphological rules, separation fails.

## Foundational Learning

- **Concept:** Compositional generalization vs. lexical generalization
  - Why needed here: Distinguishing structural generalization (unseen syntactic patterns) from lexical generalization (unseen word combinations) is core to interpreting SGET results.
  - Quick check question: If a model translates "The cat saw the dog" to Japanese correctly, but fails on "The dog saw the cat," is this lexical or structural generalization?

- **Concept:** Probabilistic Context-Free Grammar (PCFG) sentence generation
  - Why needed here: PCFGs enable precise control over vocabulary and syntactic structures, critical for creating train/generalization splits without distributional bleed.
  - Quick check question: How does setting production rule probabilities affect sentence length distribution in generated datasets?

- **Concept:** Morphological inflection in Japanese (suffixes for tense, voice)
  - Why needed here: Japanese morphological markers (e.g., passive "-rare", past "-ta") must be correctly generated; errors reveal morphological generalization capacity.
  - Quick check question: What Japanese suffix changes when converting an active to passive sentence?

## Architecture Onboarding

- **Component map:** PCFG generator -> English sentence pool -> RBMT pipeline (parse -> transduce -> output) -> Japanese sentence pool -> Sentence filter (selectional restrictions) -> cleaned dataset -> OpenNMT-py (LSTM/Transformer) or LoRA fine-tune (Llama 2) -> trained model -> Evaluation (Exact Match, BLEU, Partial Match) -> performance metrics

- **Critical path:** Dataset construction -> model training -> generalization evaluation -> error analysis

- **Design tradeoffs:**
  - RBMT vs. learned NMT: RBMT ensures structural control but may produce overly literal translations; NMT introduces distributional overlap.
  - Concatenation vs. synthetic recursion: Concatenation is simpler but may not fully mimic recursive depth growth.
  - Selectional restrictions: Improves naturalness but removes some structural variation.

- **Failure signatures:**
  - Exact Match drops sharply on generalization but BLEU stays high -> lexical generalization intact, structural missing.
  - Partial Match low on a pattern -> failure in that syntactic construct specifically.
  - Both Exact and BLEU low on generalization -> fundamental training data or model capacity issue.

- **First 3 experiments:**
  1. Run PCFG generation with/without selectional restrictions; compare sentence naturalness and downstream Exact Match.
  2. Train Transformer with concatenated sentences removed; measure recursion depth performance drop.
  3. Replace RBMT with learned NMT for Japanese; evaluate if Exact Match on generalization set changes significantly.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the inclusion of concatenated sentences in the training set impact the model's ability to handle long-distance dependencies in complex syntactic structures beyond recursion?
- **Basis in paper:** [explicit] The paper found that adding concatenated sentences improved performance on recursion patterns, suggesting enhanced ability to handle longer sentences.
- **Why unresolved:** The study only examined the impact on recursion patterns. It's unclear if this improvement generalizes to other complex syntactic structures requiring long-distance dependencies.
- **What evidence would resolve it:** Testing models on a variety of syntactic structures with long-distance dependencies, comparing performance with and without concatenated sentences in the training set.

### Open Question 2
- **Question:** To what extent do the naturalness of sentences and selectional restrictions in the training data influence the model's ability to generalize to unseen syntactic structures?
- **Basis in paper:** [explicit] The paper investigated the impact of selectional restrictions and found minimal effect on Llama 2's performance, suggesting naturalness may not be crucial.
- **Why unresolved:** The study only examined one aspect of naturalness (selectional restrictions) and focused on a specific model. It's unclear if these findings generalize to other models or aspects of naturalness.
- **What evidence would resolve it:** Systematic investigation of the impact of various aspects of naturalness (e.g., semantic coherence, plausibility) on model performance across different architectures.

### Open Question 3
- **Question:** How do different types of morphological generalization (e.g., tense, voice, argument structure) impact model performance in machine translation, and are there systematic differences across languages?
- **Basis in paper:** [explicit] The paper found that morphological generalization was challenging for models, particularly in patterns requiring unseen tense forms or argument structure alternations.
- **Why unresolved:** The study only examined a limited set of morphological generalizations in English-Japanese translation. It's unclear how these findings generalize to other types of morphological changes or language pairs.
- **What evidence would resolve it:** Systematic investigation of model performance on various morphological generalizations across multiple language pairs, comparing the difficulty and impact on translation quality.

## Limitations

- Rule-based construction method may lack ecological validity and introduce artificiality that affects generalization assessment
- RBMT-based Japanese translations may produce overly literal outputs that don't reflect actual translation patterns
- Morphological generalization evaluation may be confounded by tokenization/segmentation issues with Japanese morphological markers

## Confidence

**High confidence**: The finding that all models struggle with structural generalization compared to lexical generalization is well-supported by consistent Exact Match drops across multiple syntactic patterns (recursion, passivization, dative shift) and model architectures.

**Medium confidence**: The claim that concatenated sentences in training improve recursion performance is supported by Table 6 results, but the mechanism isn't fully explored.

**Low confidence**: The assertion that sentence naturalness has minimal impact on model performance lacks strong support from the paper's analysis.

## Next Checks

1. **Ecological validity assessment**: Generate a small subset of SGET sentences using a learned NMT system instead of RBMT, then compare model performance on these vs. RBMT-translated sentences to determine if translation quality artifacts are confounding structural generalization results.

2. **Tokenization impact study**: Systematically vary BPE vocabulary size and tokenization granularity for Japanese morphological markers, then measure impact on morphological generalization performance across tense alternation patterns to isolate tokenization effects.

3. **Selectional restriction ablation**: Train models on SGET datasets with varying degrees of selectional restriction application (full restrictions, partial restrictions, none) and measure corresponding changes in Exact Match scores on generalization sets to quantify naturalness effects on structural generalization assessment.