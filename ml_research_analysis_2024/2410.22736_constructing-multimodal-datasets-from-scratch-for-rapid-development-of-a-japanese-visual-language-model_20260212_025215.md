---
ver: rpa2
title: Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese
  Visual Language Model
arxiv_id: '2410.22736'
source_url: https://arxiv.org/abs/2410.22736
tags:
- japanese
- dataset
- data
- images
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of high-quality Japanese multimodal
  datasets for training Visual Language Models (VLMs). To solve this, the authors
  propose constructing native Japanese datasets from scratch rather than relying on
  machine-translated data.
---

# Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model

## Quick Facts
- **arXiv ID:** 2410.22736
- **Source URL:** https://arxiv.org/abs/2410.22736
- **Reference count:** 37
- **Primary result:** Native Japanese datasets outperform machine-translated data for Japanese VLM training

## Executive Summary
This paper addresses the critical shortage of high-quality Japanese multimodal datasets needed for training Visual Language Models. The authors demonstrate that constructing native Japanese datasets from scratch yields superior performance compared to using machine-translated data. They develop three distinct dataset types: Japanese image-text pairs collected from web archives, interleaved data, and instruction data generated directly from images using an existing VLM via APIs. The resulting VILA-jp model achieves state-of-the-art performance on Japanese VLM benchmarks including Heron Bench, JA-VLM-Bench-In-the-Wild, and JA-VG-VQA-500, establishing a new standard for Japanese multimodal model development.

## Method Summary
The authors construct three types of native Japanese multimodal datasets from scratch. First, they collect Japanese image-text pairs by crawling web archives, ensuring authentic Japanese language usage rather than relying on translations. Second, they gather interleaved data that mimics the natural flow of Japanese text with visual elements. Third, they generate Japanese instruction data directly from images using an existing VLM through API calls, creating task-oriented pairs without intermediate translation steps. These datasets are then used to train the VILA-jp model, which is evaluated against existing Japanese VLM benchmarks to demonstrate performance improvements over models trained on machine-translated data.

## Key Results
- VILA-jp achieves state-of-the-art performance on Heron Bench, JA-VLM-Bench-In-the-Wild, and JA-VG-VQA-500
- Native Japanese datasets significantly outperform machine-translated alternatives in Japanese VLM training
- The approach demonstrates the feasibility of building competitive VLMs without relying on English-centric datasets

## Why This Works (Mechanism)
The superior performance stems from avoiding translation artifacts that introduce semantic drift and cultural misalignment. Native Japanese data preserves linguistic nuances, idiomatic expressions, and cultural context that are critical for accurate visual understanding. Machine translation often fails to capture the precise relationships between Japanese language and visual elements, leading to degraded model performance. Additionally, the diverse data sources (web archives, interleaved content, API-generated instructions) provide comprehensive coverage of Japanese multimodal scenarios that better represent real-world usage patterns.

## Foundational Learning
- **Multimodal dataset construction**: Native language data collection avoids translation artifacts and cultural misalignments
  - Why needed: Machine translation introduces errors and loses cultural context critical for visual understanding
  - Quick check: Compare model performance on translated vs native datasets for same language

- **Visual Language Model training**: Direct image-to-text instruction generation via APIs enables task-specific data creation
  - Why needed: Creates diverse, realistic instruction-following capabilities without manual annotation
  - Quick check: Evaluate instruction-following accuracy on held-out tasks

- **Web scraping for dataset collection**: Systematic harvesting of Japanese image-text pairs from archives
  - Why needed: Provides large-scale, authentic language data at lower cost than manual collection
  - Quick check: Verify language authenticity and diversity through linguistic analysis

## Architecture Onboarding
- **Component map:** Web scraper -> Native dataset collection -> API-based instruction generation -> VILA-jp training -> Benchmark evaluation
- **Critical path:** Dataset collection (image-text pairs, interleaved data, instruction data) → VILA-jp model training → benchmark evaluation
- **Design tradeoffs:** Native data collection vs translation quality vs development speed; API-based instruction generation vs manual annotation cost
- **Failure signatures:** Poor Japanese language quality in datasets → degraded VLM performance; biased instruction generation → task-specific failures
- **First experiments:**
  1. Benchmark VILA-jp against translated-data models on all three Japanese VLM datasets
  2. Perform ablation study removing each dataset type to identify contribution to overall performance
  3. Test cross-lingual transfer to English or other language multimodal tasks

## Open Questions the Paper Calls Out
- How does the performance gap between native and translated datasets scale with model size?
- What is the optimal balance between different dataset types for maximum performance?
- Can the native dataset construction approach be effectively transferred to other languages with different cultural contexts?

## Limitations
- Validation scope limited to specific Japanese VLM benchmarks without broader task generalization
- Potential overfitting to Japanese language benchmarks used in evaluation
- Lack of detailed bias analysis for instruction data generated via API from existing VLMs
- No comparison with human-annotated Japanese instruction data to validate API-generated quality
- Limited exploration of dataset size scaling effects on performance

## Confidence
- **High:** Native Japanese datasets outperform machine-translated data for Japanese VLM training
- **Medium:** Dataset construction methodology is sound but may have sampling biases
- **Low:** Generalizability to other languages or domains beyond Japanese is unproven

## Next Checks
1. Conduct zero-shot transfer experiments on non-Japanese multimodal datasets to assess cross-lingual generalization
2. Perform detailed bias analysis of instruction data generated by API-based approach, comparing with human-annotated Japanese instruction data
3. Implement comprehensive ablation study to quantify individual contributions of each dataset type to overall model performance
4. Scale experiments across different model sizes to understand performance gap evolution
5. Test the methodology on other non-English languages to evaluate transferability