---
ver: rpa2
title: A Novel Approach to Eliminating Hallucinations in Large Language Model-Assisted
  Causal Discovery
arxiv_id: '2411.12759'
source_url: https://arxiv.org/abs/2411.12759
tags:
- causal
- llms
- discovery
- debate
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses hallucinations in large language models (LLMs)
  when used for causal discovery. The authors conducted a hallucination survey across
  six popular LLMs, finding an average hallucination rate of 50%.
---

# A Novel Approach to Eliminating Hallucinations in Large Language Model-Assisted Causal Discovery

## Quick Facts
- arXiv ID: 2411.12759
- Source URL: https://arxiv.org/abs/2411.12759
- Authors: Grace Sng; Yanming Zhang; Klaus Mueller
- Reference count: 0
- Primary result: Proposes RAG and multi-LLM debate methods to reduce LLM hallucinations in causal discovery, achieving hallucination rates of 13.9% and 12.5% respectively.

## Executive Summary
This study addresses the critical issue of hallucinations in large language models (LLMs) when used for causal discovery tasks. Through a systematic evaluation across six popular LLMs, the authors found an average hallucination rate of 50% in assessing causal relationships. To combat this, they propose two innovative methods: Retrieval Augmented Generation (RAG) and a multi-LLM debate with an arbiter. Both approaches show significant promise in reducing hallucinations, with RAG achieving a 13.9% rate and the debate method reaching 12.5% with just one round. These findings offer practical solutions for improving the reliability of LLMs in causal discovery applications.

## Method Summary
The study evaluates six LLMs (GPT-4, Claude 3.5 Sonnet, Llama3 8b, Mixtral 8x7b, Gemini 1.5 Pro, GPT-3.5) on causal discovery tasks using a life expectancy dataset with 9 variables and 18 edges. For each edge, 10 prompts (2 general, 8 specific) assess causality on a 1-4 scale. Hallucinations are identified through causal debate charts that visualize contradictions between general and specific prompt responses. Two hallucination-reduction methods are tested: RAG augmentation with a simulated "true" causal graph corpus, and a multi-LLM debate where two LLMs debate positions while an arbiter synthesizes final ratings.

## Key Results
- Average LLM hallucination rate of 50% across six models in baseline evaluation
- RAG augmentation reduced hallucination rate to 13.9% by providing domain-specific knowledge
- Multi-LLM debate with arbiter achieved 12.5% hallucination rate with one round
- Both methods significantly outperformed baseline LLM performance in causal discovery tasks

## Why This Works (Mechanism)

### Mechanism 1: Retrieval Augmented Generation (RAG)
- Claim: RAG reduces LLM hallucinations by providing additional domain-specific knowledge.
- Mechanism: Augments LLM's pre-trained knowledge with relevant external documents, reducing reliance on potentially inaccurate internal representations.
- Core assumption: Quality of RAG corpus directly impacts hallucination reduction.
- Evidence anchors:
  - [abstract] "We propose using Retrieval Augmented Generation (RAG) to reduce hallucinations when quality data is available."
  - [section] "After conducting the RAG survey, the average LLM hallucination rate dropped from 50% in the hallucination survey to 13.9%."
  - [corpus] Found 25 related papers; average neighbor FMR=0.37. RAG-focused papers include Hyper-RAG and MedCoT-RAG.
- Break condition: Poor quality or irrelevant RAG corpus may not decrease hallucinations and could increase them.

### Mechanism 2: Multi-LLM Debate with Arbiter
- Claim: Multi-LLM debate reduces hallucinations by leveraging diverse knowledge and creating checks and balances.
- Mechanism: Multiple LLMs debate causal relationships, with arbiter synthesizing responses to arrive at consensus, reducing impact of individual model hallucinations.
- Core assumption: Combined knowledge of multiple LLMs is more reliable than any single LLM, and arbiter can effectively evaluate and synthesize responses.
- Evidence anchors:
  - [abstract] "Additionally, we introduce a novel method employing multiple LLMs with an arbiter in a debate to audit edges in causal graphs, achieving a comparable reduction in hallucinations to RAG."
  - [section] "Even though we conducted only one round of debate in each experiment, hallucination rates dropped to levels comparable to those in RAG."
  - [corpus] Multiagent debate approach mentioned in corpus neighbor "Improving constraint-based discovery with robust propagation and reliable LLM priors."
- Break condition: If debating LLMs have similar hallucinations or arbiter cannot effectively evaluate responses, debate method may not reduce hallucinations.

### Mechanism 3: Causal Debate Charts
- Claim: Causal debate charts help identify hallucinations by visualizing contradictions in LLM responses.
- Mechanism: Presents LLM ratings for general and specific prompts on bidirectional bar chart, making inconsistencies in causal reasoning visually apparent.
- Core assumption: Visual representation of contradictory ratings will make hallucinations more detectable than text-based analysis alone.
- Evidence anchors:
  - [section] "The ratings generated in response to the ten prompts are visually represented using the Causal Debate Chart... We do this by counting the number of bar-pairs where one variable dominates the other."
  - [section] "The model is hallucinating if the causal dominance for the general and colored bars contradict, which implies a logical inconsistency."
  - [corpus] Weak evidence; no direct corpus support found for causal debate charts.
- Break condition: If visualization is not clear or evaluation criteria are not consistently applied, hallucinations may go undetected.

## Foundational Learning

- Concept: Causal discovery and causal graphs
  - Why needed here: Understanding the task LLMs are being used for is crucial to evaluating their performance and hallucinations.
  - Quick check question: What is the difference between correlation and causation, and why is this distinction important in causal discovery?

- Concept: Retrieval Augmented Generation (RAG)
  - Why needed here: RAG is a key mechanism proposed to reduce hallucinations, so understanding how it works is essential.
  - Quick check question: How does RAG differ from traditional fine-tuning, and what are the advantages of using RAG?

- Concept: Multi-agent systems and ensemble methods
  - Why needed here: The debate method relies on multiple LLMs working together, similar to ensemble approaches in machine learning.
  - Quick check question: How do ensemble methods in machine learning improve performance, and how might this concept apply to the debate approach with LLMs?

## Architecture Onboarding

- Component map: Data source -> Causal graph generation -> Prompt engineering -> LLM evaluation -> RAG augmentation -> Multi-LLM debate -> Causal debate chart visualization -> Hallucination rate calculation and analysis

- Critical path: 1. Generate causal graph 2. Create prompts for each edge 3. Evaluate each edge with each LLM 4. Apply RAG or debate method as needed 5. Visualize results using causal debate charts 6. Calculate and analyze hallucination rates

- Design tradeoffs:
  - RAG vs. debate method: RAG requires quality corpus but is simpler to implement; debate method doesn't need new data but is more complex.
  - Number of LLMs in debate: More LLMs could provide better coverage but increase computational cost.
  - Prompt engineering: More specific prompts may better detect hallucinations but could also lead to overfitting.

- Failure signatures: High hallucination rates persisting across multiple LLMs or methods, inconsistent results between RAG and debate approaches, visual patterns in causal debate charts that don't align with expected causal relationships

- First 3 experiments:
  1. Implement causal graph generation and basic LLM evaluation without any hallucination reduction methods.
  2. Add RAG augmentation to one LLM and compare hallucination rates to the baseline.
  3. Implement a simple two-LLM debate without an arbiter and evaluate its effectiveness compared to RAG.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the debate method generalize across different causal discovery datasets beyond life expectancy data?
- Basis in paper: [explicit] The authors note "Repeating the experiments with more datasets is paramount to show that the debate method generalizes" and acknowledge this as future work.
- Why unresolved: Only one dataset (life expectancy) was used to validate the debate method, limiting generalizability claims.
- What evidence would resolve it: Testing the debate method across multiple diverse datasets (medical, economic, social sciences) and comparing hallucination rates and accuracy against RAG and baseline LLMs.

### Open Question 2
- Question: How does increasing the number of debate rounds affect hallucination rates and accuracy in causal graph auditing?
- Basis in paper: [explicit] The authors cite Du et al.'s work showing accuracy increases with more agents and rounds, but only tested one round in their own debate method.
- Why unresolved: The study only implemented one round of debate, leaving the relationship between debate rounds and performance unexplored.
- What evidence would resolve it: Systematic testing of the debate method with 2, 3, 4+ rounds of debate, measuring changes in hallucination rates and accuracy in causal edge detection.

### Open Question 3
- Question: Would multiple arbiter LLMs (a "jury" approach) outperform a single arbiter in the debate method?
- Basis in paper: [explicit] The authors propose creating "more than one arbiter role like a jury in a court of law" as future work to improve the debate method.
- Why unresolved: The current debate method uses only one arbiter LLM, and the potential benefits of multiple arbiters remain untested.
- What evidence would resolve it: Implementing a multi-arbiter system where several arbiter LLMs debate the debaters' responses, comparing hallucination rates and accuracy against the single arbiter approach.

## Limitations

- The study is based on a single dataset with 9 variables and 18 edges, limiting statistical power and representativeness.
- The "hallucination" definition relies on logical inconsistencies between general and specific prompts, which may not capture all types of LLM errors in causal reasoning.
- The simulated RAG corpus may not reflect real-world data quality variations and complexity.

## Confidence

- **High confidence**: The 50% baseline hallucination rate across six LLMs is well-supported by the systematic evaluation methodology.
- **Medium confidence**: The effectiveness of RAG (reducing hallucinations to 13.9%) and debate methods (12.5%) is demonstrated, but the simulated "true" causal graph corpus may not reflect real-world data quality variations.
- **Low confidence**: The causal debate chart visualization approach lacks extensive validation, and the arbiter's synthesis mechanism is not fully specified.

## Next Checks

1. Test the RAG and debate methods on multiple real-world causal discovery datasets with varying complexity and noise levels.
2. Conduct ablation studies to determine the optimal number of debating LLMs and debate rounds for the multi-LLM approach.
3. Evaluate the impact of hallucination-reduction methods on both false positive and false negative rates in causal relationship identification.