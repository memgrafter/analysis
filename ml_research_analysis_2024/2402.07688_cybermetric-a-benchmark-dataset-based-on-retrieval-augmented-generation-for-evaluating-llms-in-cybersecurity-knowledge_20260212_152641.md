---
ver: rpa2
title: 'CyberMetric: A Benchmark Dataset based on Retrieval-Augmented Generation for
  Evaluating LLMs in Cybersecurity Knowledge'
arxiv_id: '2402.07688'
source_url: https://arxiv.org/abs/2402.07688
tags:
- questions
- human
- llms
- cybersecurity
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CyberMetric, a large-scale cybersecurity
  knowledge benchmark dataset created using GPT-3.5 and Retrieval-Augmented Generation
  (RAG) to evaluate Large Language Models (LLMs) across nine domains of cybersecurity.
  The dataset includes 10,000 validated multiple-choice questions, supplemented by
  smaller subsets of 80, 500, and 2,000 questions for detailed analysis.
---

# CyberMetric: A Benchmark Dataset based on Retrieval-Augmented Generation for Evaluating LLMs in Cybersecurity Knowledge

## Quick Facts
- arXiv ID: 2402.07688
- Source URL: https://arxiv.org/abs/2402.07688
- Reference count: 26
- Primary result: 10,000 validated cybersecurity multiple-choice questions created using RAG with GPT-3.5; top models (GPT-4o, Mixtral-8x7B-Instruct) outperform human experts in accuracy

## Executive Summary
This paper introduces CyberMetric, a comprehensive cybersecurity knowledge benchmark dataset designed to evaluate Large Language Models (LLMs) across nine distinct cybersecurity domains. The dataset was created using Retrieval-Augmented Generation with GPT-3.5 to generate questions from authoritative sources including NIST standards, RFC documents, and research papers. Human experts invested over 200 hours validating the dataset to ensure accuracy and relevance. The study tested 25 state-of-the-art LLMs and found that top performers like GPT-4o and Mixtral-8x7B-Instruct surpassed human experts in accuracy, while smaller models lagged behind.

## Method Summary
The CyberMetric dataset was constructed through a multi-stage process combining automated generation with human validation. PDF documents from cybersecurity sources were extracted, chunked into 8k token segments, and stored in a document index. GPT-3.5-turbo used Retrieval-Augmented Generation to query this index and generate 10 multiple-choice questions per chunk. The generated questions underwent semantic filtering using Falcon-180B, grammar correction with T5, and human expert validation to remove incorrect, ambiguous, or outdated questions. The final dataset includes 10,000 questions plus smaller validated subsets of 80, 500, and 2,000 questions for detailed analysis. Twenty-five LLM models were evaluated on these datasets using consistent parameters (temperature 1.0, top p 0.9, top k 50) on AWS ml.p4d.24xlarge instances.

## Key Results
- GPT-4o, GPT-4-turbo, Mixtral-8x7B-Instruct, Falcon-180B-Chat, and GEMINI-pro 1.0 achieved the highest accuracy scores, surpassing human expert performance
- Smaller models including Llama-3-8B, Phi-2, and Gemma-7b consistently underperformed compared to human experts
- Accuracy comparisons between fully validated subsets (CyberMetric-80, -500) and the full 10k set showed no significant accuracy drop, indicating dataset quality
- Performance varied significantly across the nine cybersecurity domains, with some models showing domain-specific strengths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using RAG with GPT-3.5 ensures domain-relevant, diverse cybersecurity question generation from authoritative sources
- Mechanism: GPT-3.5 queries a document chunk index and generates 10 questions per chunk, synthesizing structured Q&A from curated text
- Core assumption: Document chunks are topically coherent and free from non-cybersecurity content
- Evidence anchors: [section III-B] GPT-3.5-turbo used RAG to generate questions from NIST standards and research papers; [section III-A] Data collection excludes non-relevant sections
- Break condition: If source documents contain outdated or incorrect cybersecurity information, RAG output may propagate these errors

### Mechanism 2
- Claim: Human expert validation over 200 hours removes incorrect, ambiguous, or outdated questions, improving dataset quality
- Mechanism: Field experts review flagged questions, correcting multiple-correct-answer cases, time-relevant errors, and source inaccuracies
- Core assumption: Human validators are consistently calibrated and can detect subtle semantic errors
- Evidence anchors: [section III-D] Describes four categories of errors removed by human experts; [section IV-C] Notes 2-3% of questions may still have issues
- Break condition: If validators disagree or overlook subtle inaccuracies, the dataset may still contain incorrect answers

### Mechanism 3
- Claim: Comparing LLM performance on fully validated subsets against the full 10k set identifies dataset accuracy drift
- Mechanism: Measure accuracy drop between validated and full datasets; a large drop (>30%) signals unvalidated subset inaccuracy
- Core assumption: Fully validated subsets are correct by construction and serve as ground truth
- Evidence anchors: [section IV-C] CyberMetric-80 and -500 are fully validated with no significant accuracy drop observed; [section III-E] Explains validation process for 80 and 500 question subsets
- Break condition: If validated subsets contain hidden errors or the larger set has systematic bias, accuracy comparisons may mislead model ranking

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Enables LLMs to answer domain-specific questions by fetching relevant context from authoritative documents, reducing hallucinations
  - Quick check question: In a RAG pipeline, what step ensures the LLM has access to the right document snippets before generating an answer?

- Concept: Multiple-choice question validation
  - Why needed here: Ensures each question has a single correct answer and is unambiguous, critical for fair benchmarking
  - Quick check question: What is the key difference between a "multiple correct answers" error and an "incorrect information in source" error during dataset validation?

- Concept: Domain-specific benchmark construction
  - Why needed here: Provides a fair comparison of LLM capabilities across cybersecurity subfields rather than general knowledge
  - Quick check question: Why is it important to balance question distribution across multiple cybersecurity domains in a benchmark dataset?

## Architecture Onboarding

- Component map: PDF extraction -> chunking (8k tokens) -> document store -> GPT-3.5 queries -> 10 Q&A per chunk -> Falcon-180B semantic check -> T5 grammar correction -> human review -> validation -> subset creation -> LLM testing

- Critical path: Data collection -> chunking -> GPT-3.5 generation -> expert validation -> subset creation -> LLM testing

- Design tradeoffs:
  - Open-source vs proprietary LLMs: Open-source models like Falcon-180B reduce costs but may lag proprietary GPT-4o in accuracy
  - Chunk size (8k tokens): Balances coverage vs context coherence; too large risks mixing unrelated topics
  - Human validation hours: 200 hours ensures quality but scales poorly; future work could automate more of this

- Failure signatures:
  - Low LLM accuracy variance across models -> possible dataset ambiguity
  - High accuracy on full set but low on validated subsets -> unvalidated errors
  - Grammar/temporal errors in final dataset -> insufficient post-generation filtering

- First 3 experiments:
  1. Rerun GPT-3.5 generation on a small validated document set, measure question accuracy vs ground truth
  2. Run LLM evaluation on CyberMetric-80 only, compare to human expert scores to establish baseline
  3. Conduct ablation: remove Falcon-180B semantic filtering, measure increase in non-cybersecurity questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum number of parameters required for an LLM to surpass human performance on the CyberMetric-80 dataset?
- Basis in paper: The paper notes that smaller models like Llama-3-8B, Phi-2, and Gemma-7b lagged behind human performance, while larger models like GPT-4o and Mixtral-8x7B-Instruct outperformed humans
- Why unresolved: The paper tested 25 models with varying parameter sizes but did not perform a systematic analysis to identify the exact parameter threshold where human performance is surpassed
- What evidence would resolve it: A systematic study varying model sizes around the observed performance threshold (between 7B and 70B parameters) with multiple models at each size would establish the minimum parameter count required to surpass human performance

### Open Question 2
- Question: How would LLM performance change if given access to real-time cybersecurity updates and databases during testing?
- Basis in paper: The paper discusses how LLMs struggle with "Time Relevant Questions" and "Complex Computations" where current information or external tools would be beneficial
- Why unresolved: The study used a closed-book testing methodology and did not test how performance would improve with access to current data or computational resources
- What evidence would resolve it: Testing the same models with internet access or real-time cybersecurity databases during the CyberMetric assessment would quantify the performance improvement

### Open Question 3
- Question: Would specialized cybersecurity training improve LLM performance beyond general knowledge-based approaches?
- Basis in paper: The study used general knowledge questions from 9 cybersecurity domains but did not explore whether models trained specifically on cybersecurity datasets would perform better
- Why unresolved: The paper tested state-of-the-art LLMs as they exist today but did not investigate whether domain-specific fine-tuning or training would yield better results
- What evidence would resolve it: Comparing the performance of general LLMs against models specifically trained on cybersecurity datasets using the same CyberMetric benchmark would determine if specialized training provides a significant advantage

## Limitations

- Reliance on human expert validation introduces potential subjectivity and scalability concerns despite 200-hour investment
- Domain coverage, while broad across nine areas, may miss emerging threats or niche cybersecurity specializations
- Use of GPT-3.5 for question generation could introduce biases from its training data, potentially favoring certain types of questions or perspectives
- Benchmark's effectiveness in predicting real-world cybersecurity performance is not established

## Confidence

**High Confidence**: The core methodology of using RAG for domain-specific question generation and human validation for quality control is well-established and theoretically sound

**Medium Confidence**: The claim that validated subsets accurately represent the full dataset's quality is reasonable but difficult to verify independently; assertion that some models surpass human experts may be influenced by specific question selection

**Low Confidence**: The generalizability of these results to other cybersecurity contexts or future threats remains uncertain

## Next Checks

1. **Temporal Stability Test**: Re-run the entire validation process (both automated and human) on a 10% sample of questions after a 6-month interval to measure consistency and identify potential drift in standards or interpretations

2. **Cross-Domain Transfer Analysis**: Select top-performing models from this benchmark and evaluate them on an unrelated cybersecurity task (e.g., incident response simulation) to assess whether high benchmark scores translate to practical capabilities

3. **Error Type Categorization**: Conduct a detailed error analysis on questions where models outperformed humans, classifying whether these represent genuine model superiority, question ambiguity, or validation inconsistencies