---
ver: rpa2
title: 'Control+Shift: Generating Controllable Distribution Shifts'
arxiv_id: '2409.07940'
source_url: https://arxiv.org/abs/2409.07940
tags:
- distribution
- shift
- shifts
- datasets
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for generating controllable distribution
  shifts in realistic datasets using decoder-based generative models. The authors
  create datasets with varying intensities of distribution shifts by manipulating
  the latent space of generative models, allowing for systematic analysis of model
  performance degradation under different types of shifts (extend, overlap, and truncation).
---

# Control+Shift: Generating Controllable Distribution Shifts

## Quick Facts
- **arXiv ID:** 2409.07940
- **Source URL:** https://arxiv.org/abs/2409.07940
- **Authors:** Roy Friedman; Rhea Chowers
- **Reference count:** 40
- **Primary result:** Method for generating controllable distribution shifts using decoder-based generative models

## Executive Summary
This paper introduces a novel framework for generating controllable distribution shifts in realistic datasets using decoder-based generative models. By manipulating the latent space of these models, the authors create datasets with varying intensities of distribution shifts, enabling systematic analysis of model robustness. The framework generates three types of shifts (extend, overlap, and truncation) and allows precise control over shift intensity through latent space manipulation.

## Method Summary
The method uses a decoder-based generative model (specifically an EDM with score-based architecture) to generate controllable distribution shifts. The framework manipulates subsets of the latent space to create three types of shifts: extend, overlap, and truncation. For each shift type, the authors generate training data from one subset of the latent space and test data from another, with controlled overlap between these subsets. This allows precise control over the intensity of the distribution shift. The method is demonstrated on CIFAR10 and a 10-class subset of ImageNet, with evaluation of various classifier architectures.

## Key Results
- Models consistently show performance degradation with increasing shift intensity, even when visual differences are imperceptible to humans
- Data augmentations improve robustness but don't completely solve the problem, with models still experiencing significant accuracy drops
- Increasing training dataset size beyond a certain point doesn't improve robustness, while stronger inductive biases (e.g., convolutional networks) do increase robustness
- The drop in accuracy is approximately linear with respect to the distance from the training distribution's support

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distribution shift can be precisely controlled by manipulating latent space subsets.
- Mechanism: By defining two distinct subsets of the latent space with controlled overlap, the generative model produces shifted datasets where the shift intensity is proportional to the overlap between these subsets.
- Core assumption: The generative model's mapping from latent space to image space is injective and continuously differentiable, preserving the relative overlap of subsets.
- Evidence anchors:
  - [abstract] "Our approach systematically creates datasets with varying intensities of distribution shifts"
  - [section] "This framework allows for very simple control over the intensity of the shift as per Definition 1"
  - [corpus] Weak evidence - neighboring papers discuss distribution shifts but don't specifically address latent space manipulation for controllable shifts
- Break condition: If the generative model mapping is not injective, the relative overlap in latent space won't translate to the same overlap in image space.

### Mechanism 2
- Claim: Performance degradation is approximately linear with respect to distance from the training distribution's support.
- Mechanism: As the test distribution moves further from the training distribution in latent space, model accuracy drops at a rate proportional to the 1-NN distance between training and test samples.
- Core assumption: The decision boundaries learned by models are smooth and continuous in the latent space.
- Evidence anchors:
  - [abstract] "we empirically observe that the degradation of performance in many models is linear in a sense of distance from the support of the training distribution"
  - [section] "we find that the drop in accuracy is mostly a linear function of the 1-NN distance defined in Sec. 3.1"
  - [corpus] No direct evidence - neighboring papers discuss robustness but not the specific linear relationship
- Break condition: If the learned decision boundaries are discontinuous or highly non-linear, the linear relationship would break down.

### Mechanism 3
- Claim: Data augmentations increase robustness by effectively expanding the training distribution's support.
- Mechanism: Augmentation techniques introduce variations during training that cause the model to learn features invariant to certain types of transformations, effectively creating a larger region of the latent space that the model can generalize from.
- Core assumption: The augmentations used are representative of the types of shifts that might occur in deployment.
- Evidence anchors:
  - [abstract] "We see this degradation even when using data augmentations"
  - [section] "training with augmentations increases the support of the observed distribution during training"
  - [corpus] Moderate evidence - neighboring papers discuss data augmentation for robustness but not in the context of controlled distribution shifts
- Break condition: If augmentations don't cover the types of shifts present in the test distribution, they won't improve robustness to those specific shifts.

## Foundational Learning

- Concept: Latent space manipulation in generative models
  - Why needed here: Understanding how changes in latent space translate to changes in generated images is crucial for creating controlled distribution shifts
  - Quick check question: If we linearly interpolate between two latent codes, what happens to the generated images?

- Concept: Distribution shift and covariate shift
  - Why needed here: The paper focuses on covariate shift where the input distribution changes but the labeling function remains constant
  - Quick check question: What's the difference between covariate shift and concept shift?

- Concept: 1-NN distance as a measure of distribution shift
  - Why needed here: The paper uses 1-NN distance as a proxy for measuring the intensity of distribution shift in the image space
  - Quick check question: How would you compute the 1-NN distance between two datasets?

## Architecture Onboarding

- Component map: EDM generative model -> Latent space manipulation module -> Dataset generation pipeline -> Classifier evaluation framework -> Analysis and visualization tools

- Critical path: 
  1. Sample latent codes from defined subsets
  2. Generate images using the generative model
  3. Train classifiers on the original distribution
  4. Test classifiers on shifted distributions
  5. Analyze performance degradation

- Design tradeoffs:
  - Using high-quality generative models enables realistic image generation but increases computational cost
  - Simpler shift types (truncation) are easier to implement but may not capture complex real-world shifts
  - Larger training sets provide better estimates of robustness but increase experiment runtime

- Failure signatures:
  - Non-linear degradation patterns suggest the generative model mapping isn't preserving the intended shift properties
  - Unexpected robustness patterns may indicate issues with the classifier architecture or training procedure
  - Visual artifacts in generated images suggest problems with the generative model

- First 3 experiments:
  1. Generate and visualize images from the three shift types at different intensities to verify the generation process
  2. Train a simple classifier (e.g., ResNet20) on the original distribution and test on each shift type to confirm performance degradation
  3. Compare performance with and without augmentations to verify their impact on robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do models trained with augmentations on the proposed controllable datasets perform on other real-world distribution shifts (e.g., ImageNet-C, WILDS datasets)?
- Basis in paper: [explicit] The paper states that while augmentations improve robustness, there is still a significant accuracy drop under distribution shift. It suggests that augmentations increase the support of the observed distribution during training, which might explain the increased robustness.
- Why unresolved: The paper focuses on evaluating models on the generated controllable datasets. It doesn't test how well models trained on these datasets generalize to other, real-world distribution shifts.
- What evidence would resolve it: Experiments showing the performance of models trained with augmentations on the proposed datasets when tested on other established distribution shift benchmarks like ImageNet-C or WILDS.

### Open Question 2
- Question: What is the theoretical explanation for the observed linear relationship between accuracy drop and 1-NN distance in the latent space?
- Basis in paper: [explicit] The paper empirically observes that the drop in accuracy is approximately linear with respect to the distance from the training distribution's support.
- Why unresolved: While the paper provides empirical evidence of the linear relationship, it does not offer a theoretical explanation for why this linearity exists.
- What evidence would resolve it: A theoretical framework or mathematical proof that explains the linear relationship between accuracy drop and distance from the training distribution's support, possibly connecting it to the properties of the learned decision boundaries in the latent space.

### Open Question 3
- Question: How do other types of generative models (e.g., GANs, VAEs) compare to score-based models in generating controllable distribution shifts, and what impact does this have on model robustness?
- Basis in paper: [inferred] The paper uses score-based models (SBMs) to generate the controllable datasets, but mentions that in principle any decoder-based generative model (DBGM) can be used.
- Why unresolved: The paper only explores the use of SBMs for generating the controllable datasets. It doesn't investigate how other types of generative models might perform in this task or how the choice of generative model affects the resulting distribution shifts and model robustness.
- What evidence would resolve it: Experiments comparing the performance of models trained and tested on datasets generated using different types of generative models (e.g., GANs, VAEs) and analyzing the impact on model robustness to distribution shifts.

## Limitations
- The study assumes the generative model's mapping from latent to image space is injective and smooth, which may not hold for all models
- Experiments are limited to specific datasets (CIFAR10 and ImageNet-10) and shift types
- Focus on covariate shifts with constant labeling functions may not capture all types of real-world distribution shifts

## Confidence

- **High Confidence**: Performance degradation occurs with increasing shift intensity
- **Medium Confidence**: The linear relationship between accuracy degradation and 1-NN distance
- **Medium Confidence**: Data augmentations increase robustness but don't fully solve the problem

## Next Checks

1. Verify Generative Model Mapping: Conduct ablation studies to test whether small perturbations in latent space translate to small, perceptually consistent changes in image space.

2. Test Additional Shift Types: Implement and evaluate additional types of distribution shifts (e.g., rotations, translations, color shifts) to determine if the observed linear degradation pattern holds across different shift mechanisms.

3. Cross-Dataset Generalization: Apply the framework to different datasets and generative models (e.g., VAEs, GANs) to assess the robustness of the findings beyond CIFAR10 and EDM models.