---
ver: rpa2
title: 'EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees'
arxiv_id: '2406.16858'
source_url: https://arxiv.org/abs/2406.16858
tags:
- draft
- eagle-2
- arxiv
- speculative
- eagle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes EAGLE-2, a lossless acceleration algorithm
  for large language model (LLM) inference. It addresses the inefficiency of autoregressive
  decoding by introducing context-aware dynamic draft trees into speculative sampling.
---

# EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees

## Quick Facts
- arXiv ID: 2406.16858
- Source URL: https://arxiv.org/abs/2406.16858
- Authors: Yuhui Li; Fangyun Wei; Chao Zhang; Hongyang Zhang
- Reference count: 14
- Primary result: Achieves 3.05x-4.26x speedup, 20%-40% faster than EAGLE-1

## Executive Summary
EAGLE-2 introduces a lossless acceleration algorithm for large language model inference by improving speculative sampling with context-aware dynamic draft trees. The method leverages well-calibrated confidence scores from draft models to dynamically adjust draft tree structure, increasing accepted draft tokens and achieving significant speedup improvements. Extensive experiments across six tasks and three LLM series demonstrate consistent performance gains while maintaining output distribution consistency with vanilla autoregressive decoding.

## Method Summary
EAGLE-2 improves speculative sampling by using confidence scores from the draft model to dynamically adjust draft tree structure. During expansion, it selects top-k nodes based on global acceptance probabilities calculated from confidence scores. The reranking phase selects top-m tokens from all draft tokens using value-based ranking. The method maintains consistency with vanilla autoregressive decoding by not modifying the draft model structure or verification stage, only optimizing the expansion and reranking phases.

## Key Results
- Achieves 3.05x-4.26x speedup across six tasks
- 20%-40% faster than EAGLE-1
- Maintains identical output distribution with vanilla autoregressive decoding
- Works across three LLM series: Vicuna, LLaMA2-Chat, LLaMA3-Instruct

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The draft model's confidence score approximates the acceptance rate of draft tokens.
- Mechanism: EAGLE-2 uses confidence scores from the draft model to estimate acceptance rates, enabling dynamic adjustment of the draft tree structure without additional overhead.
- Core assumption: The draft model is well-calibrated, meaning its confidence scores accurately reflect the acceptance rates of draft tokens.
- Evidence anchors: [abstract] "This improvement leverages the fact that the draft model of EAGLE is well-calibrated: the confidence scores from the draft model approximate acceptance rates with small errors."

### Mechanism 2
- Claim: Context-aware dynamic draft trees yield better performance than static draft trees.
- Mechanism: EAGLE-2 dynamically adjusts the draft tree structure based on context-dependent acceptance rates of draft tokens, increasing the number of accepted draft tokens and achieving higher speedup ratios.
- Core assumption: The acceptance rate of draft tokens is not only position-dependent but also context-dependent.
- Evidence anchors: [abstract] "Interestingly, we found that the acceptance rate of draft tokens is also context-dependent."

### Mechanism 3
- Claim: EAGLE-2 achieves lossless acceleration while ensuring the distribution of the generated text remains unchanged.
- Mechanism: EAGLE-2 does not alter the training and inference of the draft model, nor does it affect the verification stage. It only improves the expansion and reranking phases, ensuring consistency with vanilla autoregressive decoding.
- Core assumption: The adjustments made to the draft tree structure do not introduce bias or alter the distribution of the generated text.
- Evidence anchors: [abstract] "EAGLE-2 also ensures that the distribution of the generated text remains unchanged, making it a lossless acceleration algorithm."

## Foundational Learning

- Concept: Speculative sampling
  - Why needed here: Understanding speculative sampling is crucial for grasping the core idea behind EAGLE-2, which is an improvement over speculative sampling methods.
  - Quick check question: What is the main goal of speculative sampling, and how does it achieve this goal?

- Concept: Autoregressive decoding
  - Why needed here: Autoregressive decoding is the baseline method that EAGLE-2 aims to accelerate. Understanding its sequential nature and limitations is essential for appreciating the benefits of EAGLE-2.
  - Quick check question: What are the main drawbacks of autoregressive decoding, and how does it work?

- Concept: Tree attention
  - Why needed here: EAGLE-2 uses tree attention to expand the draft tree efficiently. Understanding how tree attention works and its advantages over standard attention is crucial for implementing and optimizing EAGLE-2.
  - Quick check question: How does tree attention differ from standard attention, and what are its benefits in the context of speculative decoding?

## Architecture Onboarding

- Component map: Original LLM -> Draft model -> Draft tree -> Expansion phase -> Reranking phase -> Verification stage -> Final output
- Critical path:
  1. Input prefix to draft model
  2. Generate draft tokens and their confidence scores
  3. Expand draft tree based on top-k nodes with highest values
  4. Rerank draft tokens and select top m nodes
  5. Flatten selected tokens and adjust attention mask
  6. Input flattened tokens to original LLM for verification
  7. Determine acceptance of draft tokens and generate final output

- Design tradeoffs:
  - Tree depth vs. overhead: Deeper trees can generate more tokens per cycle but increase the overhead of the draft model
  - Number of candidates vs. accuracy: More candidates per node can improve accuracy but increase the complexity of the draft tree
  - Value-based expansion vs. confidence-based expansion: Value-based expansion considers the global acceptance probability, while confidence-based expansion only considers the local acceptance probability

- Failure signatures:
  - Low speedup ratio: Indicates that the draft model is not generating enough accepted tokens or that the overhead of the draft model is too high
  - Inconsistent output distribution: Suggests that the adjustments to the draft tree structure are introducing bias or altering the distribution of the generated text
  - High rejection rate: Implies that the draft model's confidence scores are not accurately approximating the acceptance rates of draft tokens

- First 3 experiments:
  1. Compare the acceptance rates of draft tokens at different positions in the draft tree to verify the context-dependent nature of acceptance rates
  2. Analyze the correlation between the draft model's confidence scores and the acceptance rates of draft tokens to validate the well-calibrated nature of the draft model
  3. Implement and test the value-based expansion and reranking phases to assess their impact on the speedup ratio and average acceptance length

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EAGLE-2's performance scale with increasingly larger draft trees or deeper draft trees?
- Basis in paper: [inferred] The paper mentions that EAGLE-2 dynamically adjusts the draft tree structure based on context and confidence scores, but does not explore the impact of varying the maximum depth or number of nodes in the draft tree.
- Why unresolved: The experiments conducted in the paper used fixed draft tree parameters (total number of draft tokens, depth, and nodes selected during expansion) for each model size. The performance impact of adjusting these parameters was not investigated.
- What evidence would resolve it: Conducting experiments with varying draft tree parameters (depth, number of nodes, etc.) and comparing the speedup ratios and acceptance lengths would provide insights into the scalability of EAGLE-2's performance.

### Open Question 2
- Question: How does EAGLE-2's performance compare to other speculative sampling methods when using a smaller draft model?
- Basis in paper: [explicit] The paper states that EAGLE-2 does not modify the structure of the draft model and uses the same draft model weights as EAGLE across all tasks. It also mentions that the draft model's confidence score is a good approximation of the acceptance rate.
- Why unresolved: The experiments conducted in the paper used draft models of varying sizes for different original LLMs. The performance impact of using a smaller draft model, which could potentially reduce the overhead during the drafting phase, was not investigated.
- What evidence would resolve it: Conducting experiments with EAGLE-2 using smaller draft models and comparing the speedup ratios and acceptance lengths with other speculative sampling methods would provide insights into the impact of draft model size on EAGLE-2's performance.

### Open Question 3
- Question: How does EAGLE-2's performance generalize to other tasks and datasets beyond those evaluated in the paper?
- Basis in paper: [inferred] The paper evaluates EAGLE-2's performance on six tasks (multi-turn conversation, code generation, mathematical reasoning, instruction following, summarization, and question answering) using specific datasets for each task. However, it does not explore the performance on other tasks or datasets.
- Why unresolved: The paper focuses on a specific set of tasks and datasets that are commonly used in the LLM community. The performance of EAGLE-2 on other tasks or datasets, such as those involving more complex reasoning or specialized domains, was not investigated.
- What evidence would resolve it: Conducting experiments with EAGLE-2 on a wider range of tasks and datasets, including those involving more complex reasoning or specialized domains, and comparing the speedup ratios and acceptance lengths would provide insights into the generalizability of EAGLE-2's performance.

## Limitations

- The calibration of draft models across diverse contexts and different LLM architectures remains an open question
- The relationship between draft tree depth and overhead is treated as a simple tradeoff without exploring scenarios where overhead might dominate
- The claim of lossless acceleration relies on theoretical consistency proofs rather than extensive distributional analysis

## Confidence

**High Confidence**: The empirical results demonstrating speed improvements (3.05x-4.26x) across six tasks and three LLM series are well-supported by the experimental methodology.

**Medium Confidence**: The mechanism of using confidence scores to approximate acceptance rates is theoretically sound, but the robustness of this approximation across different model families and tasks needs more extensive validation.

**Low Confidence**: The claim of lossless acceleration while maintaining identical output distribution relies on theoretical consistency proofs rather than extensive distributional analysis.

## Next Checks

1. **Cross-Model Calibration Analysis**: Test the correlation between draft model confidence scores and actual acceptance rates across different LLM architectures (not just different sizes of the same model family) and domains to verify the robustness of the confidence score approximation mechanism.

2. **Context Sensitivity Benchmark**: Design experiments that systematically vary contextual factors (prompt complexity, domain specificity, instruction length) to measure how acceptance rates change with context, quantifying the relationship between context and acceptance probability.

3. **Distribution Consistency Verification**: Implement comprehensive statistical tests (KL divergence, maximum mean discrepancy) comparing the output distributions of EAGLE-2 versus vanilla autoregressive decoding across all six task domains to empirically verify the lossless acceleration claim.