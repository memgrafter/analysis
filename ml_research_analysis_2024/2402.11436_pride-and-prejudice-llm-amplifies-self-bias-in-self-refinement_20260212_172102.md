---
ver: rpa2
title: 'Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement'
arxiv_id: '2402.11436'
source_url: https://arxiv.org/abs/2402.11436
tags:
- bias
- uni00000013
- llms
- self-bias
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper formally defines and quantifies LLM self-bias using\
  \ two principled statistics\u2014bias and distance skewness\u2014measuring how LLMs\
  \ favor their own outputs during iterative self-refinement. Across six diverse LLMs,\
  \ four languages, and three tasks (translation, constrained text generation, and\
  \ math reasoning), self-bias is found to amplify through iterative refinement, leading\
  \ to false positive corrections rather than true performance gains."
---

# Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement

## Quick Facts
- arXiv ID: 2402.11436
- Source URL: https://arxiv.org/abs/2402.11436
- Reference count: 31
- One-line primary result: Self-bias in LLMs amplifies through iterative self-refinement, favoring their own outputs over true task improvement.

## Executive Summary
This paper formally defines and quantifies LLM self-bias using bias and distance skewness statistics, measuring how LLMs favor their own outputs during iterative self-refinement. Across six diverse LLMs, four languages, and three tasks, self-bias is found to amplify through iterative refinement, leading to false positive corrections rather than true performance gains. The study reveals that while self-refinement improves fluency and understandability, it does not improve task-specific quality. Two mitigation strategies are identified: increasing model size and incorporating external feedback with accurate assessment, both of which significantly reduce self-bias and improve actual task performance.

## Method Summary
The study implements an iterative self-refinement pipeline where an LLM generates initial output, then generates self-feedback, and refines based on that feedback. This process repeats for a fixed number of iterations (typically 10). Self-bias is quantified using two statistics: bias (measuring systematic favoritism toward own outputs) and distance skewness (measuring asymmetry in evaluation distributions). The pipeline is tested across six LLMs (GPT-4, GPT-3.5, Gemini, LLaMA2, Mixtral, DeepSeek) on three tasks: translation (using Flores-200), constrained text generation (using CommonGen Hard), and math reasoning (using MATH). Two mitigation strategies are evaluated: using larger model variants and replacing self-feedback with external feedback from InstructScore.

## Key Results
- Self-bias amplifies monotonically through iterative refinement steps, with distance skewness values growing from near zero to approximately 0.3-0.4
- Self-refinement improves fluency and understandability but not task-specific quality (BLEURT scores plateau or decline while LLM feedback scores improve)
- Larger models show significantly less self-bias, with LLaMA2-70B exhibiting approximately 40% less bias than LLaMA2-7B
- External feedback with accurate assessment reduces self-bias by providing oracle-like error annotations based on reference text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-bias amplifies because the model's self-evaluation conflates fluency improvements with task-specific gains
- Mechanism: At each refinement step, the LLM receives feedback based on its own quality assessment. If feedback is biased toward self-generated outputs, the model iteratively optimizes for properties it overvalues (e.g., fluency, style alignment) rather than true task metrics
- Core assumption: The LLM's self-evaluation metric is systematically skewed toward its own outputs, and this skew is preserved or magnified through feedback loops
- Evidence anchors:
  - [abstract]: "Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias"
  - [section]: "We find that self-bias is universal in self-refine and self-rewarding pipelines... This bias causes LLMs to optimize for false positive corrections rather than improving the actual output quality"
  - [corpus]: Weak; no direct citations found on amplification dynamics

### Mechanism 2
- Claim: Larger models are less susceptible to self-bias due to greater internal consistency and instruction-following precision
- Mechanism: With more parameters and better pretraining, larger models are more capable of objectively assessing outputs against ground truth or external criteria
- Core assumption: Model size correlates with the ability to resist narcissistic tendencies in evaluation and follow task instructions accurately
- Evidence anchors:
  - [abstract]: "To mitigate such biases, we discover that larger model size... can significantly reduce bias in the self-refine pipeline"
  - [section]: "We demonstrate that LLMs with larger parameter size can have less self-bias throughout self-refinement steps"
  - [corpus]: Weak; no direct citations linking model size to bias resistance

### Mechanism 3
- Claim: External feedback reduces self-bias by introducing independent, reference-grounded evaluation
- Mechanism: External feedback forces the LLM to correct specific, objectively defined errors rather than optimizing for perceived improvements in fluency or style
- Core assumption: The external feedback mechanism is accurate and unbiased, and the LLM can incorporate it effectively
- Evidence anchors:
  - [abstract]: "To mitigate such biases, we discover that... external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline"
  - [section]: "Interestingly, both Gemini and GPT-4's bias estimation is improved throughout the refinement process, as the external feedback model can over-penalize low-quality outputs"
  - [corpus]: Weak; no direct citations on external feedback effectiveness

## Foundational Learning

- Concept: Self-bias in LLM evaluation
  - Why needed here: The entire study hinges on quantifying and mitigating the tendency of LLMs to favor their own outputs
  - Quick check question: What are the two statistics used to measure self-bias, and what does each capture?

- Concept: Iterative refinement pipeline
  - Why needed here: The mechanism of self-bias amplification depends on the feedback loop structure
  - Quick check question: In each refinement step, what information does the LLM have access to from previous steps?

- Concept: Evaluation metrics and their calibration
  - Why needed here: The study uses BLEURT and human MQM scores as ground truth, requiring proper calibration
  - Quick check question: Why can't BLEURT scores be linearly mapped to MQM human scores, and what method is used instead?

## Architecture Onboarding

- Component map: Input prompt → LLM initial generation → LLM self-feedback → LLM refinement → (loop) → Output
- Critical path: Generation → Feedback → Refinement → Evaluation
- Design tradeoffs: Self-feedback is scalable but biased; external feedback is accurate but costly; larger models reduce bias but increase computational cost
- Failure signatures: Bias increases monotonically with iterations; distance skewness diverges from zero; BLEURT scores plateau or decline while LLM feedback scores improve
- First 3 experiments:
  1. Run self-refine on a low-resource translation pair (e.g., Yor-En) with GPT-4; record bias and BLEURT per iteration
  2. Replace self-feedback with InstructScore; compare bias trajectories
  3. Repeat experiment with LLaMA2-7B and LLaMA2-70B; measure difference in bias amplification

## Open Questions the Paper Calls Out

- Question: Does LLM have more bias towards LLMs that follow the same pretraining procedure, data, or learning objectives?
  - Basis in paper: [explicit] The paper states it would be worthwhile to explore measurement of bias between different LLMs and bias arising when comparing original models and their knowledge-distilled counterparts
  - Why unresolved: The paper focuses on quantifying self-bias and does not investigate biases between different LLMs or between original and knowledge-distilled counterparts
  - What evidence would resolve it: Conducting experiments comparing bias of LLMs towards each other, considering factors such as pretraining procedure, data, and learning objectives

- Question: Do knowledge-distilled LLMs have more biases over the original LLMs, such as Vicuna to GPT4 or Alpaca to ChatGPT?
  - Basis in paper: [explicit] The paper mentions it would be worthwhile to explore measurement of bias that exists between different LLMs, as well as bias arising when comparing original models and their knowledge-distilled counterparts
  - Why unresolved: The paper focuses on quantifying self-bias and does not investigate biases between original models and their knowledge-distilled counterparts
  - What evidence would resolve it: Conducting experiments comparing bias of knowledge-distilled LLMs towards their original counterparts

- Question: How does the size of the language model affect its susceptibility to self-bias?
  - Basis in paper: [inferred] The paper mentions that larger models are more resistant to self-bias, but doesn't provide detailed analysis of how size affects susceptibility
  - Why unresolved: The paper doesn't investigate the relationship between model size and susceptibility to self-bias in detail
  - What evidence would resolve it: Conducting experiments with language models of varying sizes and analyzing their susceptibility to self-bias

## Limitations
- Results may not generalize beyond the three tested task types and six LLM architectures due to lack of prior citations linking these specific mechanisms
- Effectiveness of external feedback assumes the feedback mechanism is consistently accurate and unbiased, which may not hold across all domains
- Relationship between model size and bias reduction lacks theoretical grounding regarding why larger models resist self-bias more effectively

## Confidence
- Self-bias amplification mechanism: Medium - Empirical demonstration but theoretical explanation relies on assumptions about feedback loop dynamics
- Model size as mitigation: Medium - Results show correlation but causation not definitively established; alternative explanations not fully controlled for
- External feedback effectiveness: Medium - While external feedback reduces bias in experiments, study doesn't explore scenarios where external feedback might introduce its own biases

## Next Checks
1. Test self-bias amplification across additional task types (e.g., code generation, summarization) and LLM architectures not included in the original study to assess generalizability
2. Conduct ablation studies comparing model size effects against instruction-tuning effects by testing both base and instruction-tuned versions of the same model families
3. Evaluate the robustness of external feedback mitigation by introducing controlled noise or bias into the feedback mechanism and measuring the impact on self-bias reduction