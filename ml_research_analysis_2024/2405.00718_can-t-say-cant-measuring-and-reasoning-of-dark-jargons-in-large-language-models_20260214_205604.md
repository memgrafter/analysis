---
ver: rpa2
title: Can't say cant? Measuring and Reasoning of Dark Jargons in Large Language Models
arxiv_id: '2405.00718'
source_url: https://arxiv.org/abs/2405.00718
tags:
- cant
- learning
- scene
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces CantCounter, a framework to evaluate how\
  \ Large Language Models (LLMs) understand and handle \"cant\" or \"dark jargon\"\
  \ in sensitive domains like politics, drugs, racism, weapons, and LGBT issues. It\
  \ constructs two datasets\u2014Cant (containing domain-specific hidden language)\
  \ and Scene (containing related contexts)\u2014and employs a four-stage process:\
  \ Fine-Tuning to generate domain-relevant scenes, Co-Tuning to match cant with scenes,\
  \ Data-Diffusion to create diverse Q&A queries, and Data-Analysis to assess LLM\
  \ performance."
---

# Can't say cant? Measuring and Reasoning of Dark Jargons in Large Language Models
## Quick Facts
- arXiv ID: 2405.00718
- Source URL: https://arxiv.org/abs/2405.00718
- Reference count: 40
- Large Language Models show vulnerabilities to bypassing safety filters using domain-specific hidden language ("cant")

## Executive Summary
This paper introduces CantCounter, a framework designed to evaluate how Large Language Models handle "cant" or "dark jargon" in sensitive domains including politics, drugs, racism, weapons, and LGBT issues. The framework constructs two datasets—Cant (domain-specific hidden language) and Scene (related contexts)—and employs a four-stage process: Fine-Tuning, Co-Tuning, Data-Diffusion, and Data-Analysis. Experiments reveal that LLMs are susceptible to bypassing safety filters using cant, with performance varying significantly by question type, learning method, and domain sensitivity. The study highlights critical vulnerabilities in current LLM safety mechanisms and the need for improved approaches to detect and handle hidden jargon.

## Method Summary
The CantCounter framework operates through a four-stage pipeline to evaluate LLM performance with cant. First, Fine-Tuning generates domain-relevant scenes using ChatGPT to create context-rich scenarios. Second, Co-Tuning matches cant terms with these scenes to establish associations. Third, Data-Diffusion creates diverse Q&A queries by varying question formats and contexts. Finally, Data-Analysis assesses model responses across multiple dimensions including accuracy, refusal rates, and sensitivity to different domains. The framework was tested on multiple LLMs including ChatGPT, GPT-4, and Bard, revealing varying susceptibility to cant-based bypass attempts across different sensitive domains and question types.

## Key Results
- LLMs show highest accuracy (45.38%) on Multiple-choice questions when tested with cant
- Zero-shot learning outperforms One-shot learning in sensitive domains
- GPT-4 never refuses responses while other models show higher refusal rates on racism compared to LGBT topics

## Why This Works (Mechanism)
The framework exploits the gap between model safety training and the ability to recognize domain-specific hidden language. By creating synthetic scenarios that embed cant within seemingly innocuous contexts, the evaluation reveals how models can be prompted to bypass safety filters through contextual manipulation rather than direct violation of explicit terms.

## Foundational Learning
- **Cant/Dark Jargon**: Domain-specific hidden language used to discuss sensitive topics covertly. Needed to understand how users might circumvent content filters. Quick check: Can the model distinguish between legitimate terminology and coded language in context?
- **Fine-Tuning vs Co-Tuning**: Sequential training approaches where Fine-Tuning establishes domain context and Co-Tuning links cant terms to those contexts. Needed to create realistic scenarios for testing. Quick check: Does the model maintain contextual understanding when cant terms are introduced?
- **Zero-shot vs One-shot Learning**: Different prompting strategies where Zero-shot provides no examples while One-shot provides one. Needed to test model robustness to minimal guidance. Quick check: How does model performance vary with different levels of instructional context?

## Architecture Onboarding
Component Map: Scene Generation -> Cant Matching -> Query Generation -> Response Analysis
Critical Path: Cant identification within context -> Model response generation -> Safety filter evaluation
Design Tradeoffs: Synthetic data generation provides control but may lack real-world authenticity; automated cant detection risks false positives/negatives
Failure Signatures: High refusal rates on legitimate cant usage indicate over-sensitivity; low refusal rates on clear violations indicate under-protection
First Experiments:
1. Test model responses on ground-truth annotated cant versus synthetic cant
2. Compare performance across different prompt engineering strategies
3. Evaluate model behavior when cant appears in mixed legitimate/illegitimate contexts

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Dataset construction may introduce bias in identifying what constitutes "dark jargon"
- Generated scenes may not fully represent real-world cant usage patterns
- Limited scope to only five sensitive domains constrains generalizability

## Confidence
- LLMs are susceptible to bypassing filters using cant: Medium
- Different models show varying refusal rates with specific patterns: Low
- Performance variations reflect inherent vulnerabilities: Medium

## Next Checks
1. Validate the Cant dataset's authenticity by comparing automatically identified cant terms against expert-annotated ground truth from domain specialists across all five sensitive domains.
2. Test model responses on naturally occurring cant usage from real-world sources (social media posts, underground forums) rather than synthetically generated scenes to assess ecological validity.
3. Conduct ablation studies removing specific components of the CantCounter framework (e.g., scene generation, co-training) to isolate which elements most strongly influence the measured vulnerabilities.