---
ver: rpa2
title: 'Adaptable Moral Stances of Large Language Models on Sexist Content: Implications
  for Society and Gender Discourse'
arxiv_id: '2410.00175'
source_url: https://arxiv.org/abs/2410.00175
tags:
- sexist
- moral
- language
- llms
- sexism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how large language models (LLMs) can apply
  moral reasoning to explain both criticism and defense of sexist language. We tested
  eight state-of-the-art LLMs using the Moral Foundations Theory (MFT) framework,
  asking them to generate arguments for why implicitly sexist social media posts are
  or are not sexist.
---

# Adaptable Moral Stances of Large Language Models on Sexist Content: Implications for Society and Gender Discourse

## Quick Facts
- **arXiv ID**: 2410.00175
- **Source URL**: https://arxiv.org/abs/2410.00175
- **Reference count**: 15
- **Key outcome**: LLMs can generate comprehensible arguments both for and against sexist content using moral foundations, with more accurate models showing nuanced reasoning across progressive and traditional values.

## Executive Summary
This study examines how large language models apply moral reasoning to explain both criticism and defense of sexist language. Using the Moral Foundations Theory framework, eight state-of-the-art LLMs were tested on their ability to generate arguments for why implicitly sexist social media posts are or are not sexist. The models produced contextually relevant explanations that human evaluators found helpful in understanding diverse perspectives on sexism. Notably, the study revealed that more accurate models distinguish between progressive and traditional moral values when critiquing versus defending sexist content, while less accurate models showed less nuanced reasoning. This research highlights both the potential misuse of LLMs to justify sexist views and their value as tools for understanding societal perspectives on gender issues.

## Method Summary
The study employed the Explainable Detection of Online Sexism (EDOS) dataset, specifically the EDOS-implicit subset containing 2,140 sentences with implicit sexism. Eight LLMs were selected including gpt-3.5-turbo, LLaMA-2, Vicuna, Mistral, WizardLM, Zephyr, Falcon, and GPT4ALL-j. Prompts were designed based on Moral Foundations Theory to generate arguments for and against implicit sexist sentences. Evaluations included human ratings assessing comprehensibility, relevance, and helpfulness, as well as automatic scoring by GPT-4. The analysis examined which moral foundations (Care, Equality, Proportionality, Loyalty, Authority, Purity) were cited in arguments and how frequently different models referenced these foundations when defending versus criticizing sexist content.

## Key Results
- All eight tested LLMs produced comprehensible and contextually relevant arguments about sexist content that human evaluators found helpful
- More accurate models distinguished between progressive and traditional moral values when critiquing versus defending sexist content
- Less accurate models showed less nuanced reasoning and failed to adjust their moral foundation citations appropriately based on whether they were asked to defend or criticize sexist language

## Why This Works (Mechanism)
The study leverages the inherent capability of large language models to understand context and generate coherent arguments by providing structured prompts based on established moral philosophy frameworks. The Moral Foundations Theory provides a systematic way to categorize and analyze the types of moral reasoning LLMs employ when addressing sensitive content.

## Foundational Learning
- **Moral Foundations Theory**: A framework for understanding moral reasoning across cultures; needed to categorize LLM arguments and ensure systematic analysis
- **Implicit bias detection**: Understanding subtle forms of discrimination; needed to select appropriate test cases from EDOS dataset
- **Prompt engineering**: Crafting effective instructions for LLMs; needed to elicit specific types of moral reasoning responses
- **Human evaluation methodology**: Systematic assessment of generated content; needed to validate LLM outputs beyond automated metrics
- **Model accuracy correlation**: Relationship between overall model performance and task-specific reasoning; needed to identify patterns in moral reasoning sophistication

## Architecture Onboarding
- **Component map**: EDOS dataset -> LLM prompt generation -> Argument generation -> Human evaluation/GPT-4 evaluation -> Moral foundation analysis
- **Critical path**: Dataset selection → Prompt design → Model generation → Evaluation → Analysis
- **Design tradeoffs**: Balancing model diversity (including both proprietary and open-source models) against consistency in evaluation methods; choosing between comprehensive human evaluation and scalable automated assessment
- **Failure signatures**: Models generating off-topic responses, refusing to engage with prompts, or producing arguments that don't align with MFT categories
- **First experiments**: 1) Test prompt templates on a small subset of the dataset to refine instructions, 2) Run a pilot evaluation with a subset of models to calibrate human rating scales, 3) Analyze moral foundation distribution in initial outputs to identify potential biases

## Open Questions the Paper Calls Out
None

## Limitations
- Human evaluation relied on Prolific raters, introducing potential demographic biases in how sexist arguments are perceived
- Temperature parameters varied across models (0.5, 0.1, and 1.0), potentially affecting consistency of argument generation
- Sample size for human evaluation (50 raters per model) may not capture the full spectrum of perspectives on gender issues

## Confidence
- **Medium** for claim that more accurate models demonstrate nuanced moral reasoning
- **Medium** for generalizability of findings to other sensitive topics beyond sexism
- **Low** for long-term implications regarding LLM deployment

## Next Checks
1. Replicate the analysis with a larger and more diverse pool of human evaluators, including different age groups, cultural backgrounds, and levels of familiarity with feminist discourse
2. Test the same methodology on a different sensitive topic (such as racism or ableism) using a comparable dataset to determine whether the observed relationship between model accuracy and moral foundation diversity holds across different types of bias
3. Conduct a longitudinal analysis by testing the same prompts on current versions of the models to assess whether the observed patterns in moral reasoning remain stable over time or change with model improvements