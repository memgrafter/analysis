---
ver: rpa2
title: 'InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document Understanding
  with Instructions'
arxiv_id: '2401.13313'
source_url: https://arxiv.org/abs/2401.13313
tags:
- document
- tasks
- dataset
- instructions
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InstructDoc, the first large-scale visual
  instruction-tuning dataset for visual document understanding (VDU) tasks. It contains
  30 diverse datasets covering 12 VDU tasks, with 30K held-out evaluation instances.
---

# InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document Understanding with Instructions

## Quick Facts
- arXiv ID: 2401.13313
- Source URL: https://arxiv.org/abs/2401.13313
- Reference count: 20
- Introduces InstructDoc, the first large-scale visual instruction-tuning dataset for VDU tasks, enabling zero-shot performance across diverse document types.

## Executive Summary
This paper addresses the challenge of zero-shot generalization in visual document understanding (VDU) by introducing InstructDoc, a large-scale instruction-tuning dataset covering 12 diverse VDU tasks across 30 datasets. The authors propose InstructDr, a novel instruction-based document reading and understanding model that connects document images to large language models through a trainable bridging module called Document-former. By training on diverse instruction templates, InstructDr achieves state-of-the-art zero-shot performance on various unseen VDU datasets, tasks, and domains without task-specific fine-tuning.

## Method Summary
The authors construct InstructDoc by collecting 30 publicly available VDU datasets and manually crafting diverse instructions in a unified format for each dataset, following the schema of user intent and answer style. They implement InstructDr by extending BLIP-2 with Document-former, a trainable bridging module that converts document images into representations suitable for large language models. The model is trained using multi-task instruction tuning on the collected datasets, with parameter-efficient optimization that updates only the Document-former while keeping other parameters frozen. Zero-shot performance is evaluated on held-out datasets across three settings: TestCross-Dataset, TestCross-Task, and TestCross-Domain.

## Key Results
- InstructDr achieves state-of-the-art zero-shot performance on unseen VDU datasets compared to existing multimodal LLMs and ChatGPT
- The model outperforms supervised SOTA models on certain datasets like FUNSD and CORD
- Parameter-efficient instruction tuning by updating only Document-former and the projection FFN layer preserves the LLM's original capabilities while adding VDU skills

## Why This Works (Mechanism)

### Mechanism 1
- Claim: InstructDr generalizes across unseen VDU datasets, tasks, and domains through instruction tuning with a unified sequence-to-sequence format.
- Mechanism: The model uses Document-former to convert document images, OCR, and layout features into a representation that LLMs can understand. By training on diverse instruction templates that include intent and answer style, the model learns to interpret various task formats without task-specific fine-tuning.
- Core assumption: A single bridging module can capture the complex relationships between visual, textual, and layout modalities of diverse document types.
- Evidence anchors:
  - [abstract] "Experiments show that InstructDr achieves the highest zero-shot performance among existing MLLMs and outperforms ChatGPT on a wide range of VDU datasets with instructions."
  - [section] "Our goal is to enable the model to perform a wide range of VDU tasks with instructions rather than improving the accuracy of text recognition."
- Break condition: If the Document-former cannot adequately represent the document's visual and layout features, zero-shot performance on unseen tasks will degrade significantly.

### Mechanism 2
- Claim: Parameter-efficient instruction tuning by updating only Document-former and the projection FFN layer preserves the LLM's original capabilities while adding VDU skills.
- Mechanism: By freezing most parameters and only training the bridging module, the model retains its general reasoning abilities while learning to process document-specific features.
- Core assumption: The LLM's base reasoning capabilities are sufficient for VDU tasks when provided with appropriate document representations.
- Evidence anchors:
  - [abstract] "We optimize the model by minimizing the negative log-likelihood between the ground-truth and predictions."
  - [section] "To train the LLM efficiently, we update only the parameters of the Document-former... while keeping other parameters frozen during training."
- Break condition: If the base LLM lacks sufficient reasoning capabilities for complex VDU tasks, parameter-efficient tuning will be insufficient.

### Mechanism 3
- Claim: Using multiple diverse instructions per dataset improves robustness to instruction variations.
- Mechanism: Training with varied instruction templates teaches the model to focus on task semantics rather than specific phrasing.
- Core assumption: The model can generalize from diverse instruction phrasings to novel instruction formulations.
- Evidence anchors:
  - [section] "Our model exhibited the smallest performance variance and outperformed BLIP-2... This indicates InstructDoc empowers the model with the ability to deal with a variety of instructions."
  - [section] "Moreover, our instruction annotations, including query rephrasing and answer styles, helped to improve the zero-shot performance."
- Break condition: If instruction diversity is insufficient or if instructions are too ambiguous, the model may not learn robust task representations.

## Foundational Learning

- Concept: Vision-language pretraining and multimodal representation learning
  - Why needed here: The model builds on BLIP-2, which requires understanding how vision encoders and LLMs can be connected for multimodal tasks.
  - Quick check question: What is the role of the Q-former in BLIP-2, and how does Document-former extend this concept?

- Concept: Instruction tuning and few-shot learning
  - Why needed here: The model's performance relies on learning from instructions rather than task-specific examples.
  - Quick check question: How does instruction tuning differ from traditional fine-tuning, and what are its advantages for zero-shot generalization?

- Concept: Document structure and layout analysis
  - Why needed here: Understanding documents requires processing both content and spatial relationships.
  - Quick check question: What document features (beyond text) are critical for tasks like information extraction and question answering?

## Architecture Onboarding

- Component map: Vision encoder (CLIP) -> Document-former -> Projection FFN -> LLM (FlanT5)
- Critical path: Document image -> Vision encoder -> Document-former -> LLM -> Answer generation
  - The Document-former is the critical bottleneck; failures here cascade to poor performance.
- Design tradeoffs:
  - Parameter efficiency vs. model capacity: Only training Document-former limits adaptation but preserves base capabilities.
  - Instruction diversity vs. annotation cost: More instructions improve robustness but require more manual effort.
  - Single-page vs. multi-page handling: Parallel encoding is simpler but may miss cross-page dependencies.
- Failure signatures:
  - Poor OCR quality -> Incorrect answers, especially for text-heavy tasks
  - Document-former underfitting -> Failure to adapt to new document types
  - Instruction ambiguity -> High variance in performance across different instructions
- First 3 experiments:
  1. Ablation: Remove Document-former and use raw vision encoder features -> Expect significant performance drop
  2. Multi-page handling: Compare mean pooling vs. attention-based aggregation -> Evaluate cross-page reasoning
  3. Instruction variation: Test with single vs. multiple instructions per dataset -> Measure robustness to phrasing changes

## Open Questions the Paper Calls Out

- How does the performance of InstructDr scale with the number of instruction templates per dataset?
- What is the impact of different OCR qualities on the zero-shot performance of InstructDr across various VDU tasks?
- How does InstructDr's performance compare to supervised models when fine-tuned on small amounts of task-specific data?

## Limitations

- The evaluation relies heavily on comparisons with relatively few baseline models, lacking direct comparisons with specialized document understanding models like LayoutLM or DocFormer.
- The Document-former architecture lacks detailed specifications, making it difficult to assess its contribution to performance gains versus the instruction-tuning methodology itself.
- The paper doesn't provide systematic analysis of how different OCR engines or error rates impact results, which is crucial for real-world deployment.

## Confidence

**High Confidence**: The dataset construction methodology and the basic premise that instruction tuning can enable zero-shot VDU generalization are well-supported. The multi-task instruction format is clearly defined and reproducible.

**Medium Confidence**: The claim that InstructDr achieves "state-of-the-art" performance is supported by experimental results but could be strengthened with more comprehensive baseline comparisons and error analysis. The parameter-efficient tuning approach (freezing most parameters) is sound but its optimality for VDU tasks isn't fully established.

**Low Confidence**: The paper's assertion that a single bridging module can adequately capture all document modality relationships across diverse tasks seems optimistic. Without detailed architectural analysis or ablation studies isolating the Document-former's contribution, this remains an open question.

## Next Checks

1. Architecture Ablation Study: Remove the Document-former and directly connect the vision encoder to the LLM to quantify its specific contribution to performance improvements.

2. Cross-LLM Generalization: Test whether the instruction-tuning approach transfers to different LLM architectures (e.g., LLaMA, GPT variants) to assess the robustness of the methodology beyond the specific FlanT5 implementation.

3. Failure Mode Analysis: Systematically evaluate performance degradation on documents with varying OCR quality, layout complexity, and domain-specific formatting to identify the practical limits of zero-shot generalization.