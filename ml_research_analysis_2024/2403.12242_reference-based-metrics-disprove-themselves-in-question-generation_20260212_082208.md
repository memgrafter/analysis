---
ver: rpa2
title: Reference-based Metrics Disprove Themselves in Question Generation
arxiv_id: '2403.12242'
source_url: https://arxiv.org/abs/2403.12242
tags:
- question
- metrics
- naco
- questions
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Reference-based metrics like BLEU and BERTScore are widely used
  to evaluate question generation (QG), but they often fail to accurately assess question
  quality when only a single reference is available. This study replicates the annotation
  process for SQuAD and HotpotQA benchmarks, collecting a second human-written reference
  for each example.
---

# Reference-based Metrics Disprove Themselves in Question Generation

## Quick Facts
- arXiv ID: 2403.12242
- Source URL: https://arxiv.org/abs/2403.12242
- Reference count: 37
- Reference-based metrics like BLEU and BERTScore fail to accurately assess question quality when only a single reference is available.

## Executive Summary
Reference-based metrics like BLEU and BERTScore are widely used to evaluate question generation (QG), but they often fail to accurately assess question quality when only a single reference is available. This study replicates the annotation process for SQuAD and HotpotQA benchmarks, collecting a second human-written reference for each example. Experiments show that reference-based metrics struggle to distinguish between high-quality questions and flawed ones, often scoring non-questions or random questions higher than valid human-written questions. To address this, the authors propose NACo, a reference-free metric that evaluates questions based on three criteria: naturalness, answerability, and complexity. NACo leverages large language models with Chain-of-Thought reasoning to score these dimensions, achieving state-of-the-art alignment with human judgment. The metric successfully differentiates high-quality questions from flawed ones, outperforming existing reference-based and reference-free metrics.

## Method Summary
The study replicates the annotation process on SQuAD and HotpotQA benchmarks to collect a second human-written reference for each example. Four groups of candidate questions are created: human-annotated, GPT-3.5 generated, non-questions using similar wording to references, and random training questions. Existing reference-based metrics (BLEU-4, BLEURT, ROUGE-L, BERTScore, Q-BLEU) and reference-free metrics (QAScore, RQUGE) are evaluated alongside the proposed NACo metric. NACo uses Chain-of-Thought reasoning with LLMs to assess naturalness, answerability, and complexity. Human evaluation serves as ground truth for validating metric performance.

## Key Results
- Reference-based metrics fail to distinguish high-quality questions from flawed ones, often scoring non-questions higher than valid human-written questions.
- NACo outperforms all existing metrics in aligning with human judgment across naturalness, answerability, and complexity criteria.
- The proposed metric successfully separates all four groups of candidate questions by significant margins, unlike reference-based metrics.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reference-based metrics like BLEU and BERTScore fail to accurately assess question quality when only a single reference is available because they heavily rely on word overlap and semantic similarity with the reference.
- **Mechanism:** These metrics compute scores based on n-gram overlap or embedding similarity between the generated question and the reference. When only one reference exists, the metrics can be misled by questions that share many words with the reference but lack essential components of a good question (e.g., being a non-question, lacking complexity, or being irrelevant to the answer).
- **Core assumption:** A single reference cannot capture the diversity of valid ways to phrase a question, and reference-based metrics are too sensitive to surface-level similarities rather than deeper question quality criteria.
- **Evidence anchors:**
  - [abstract]: "Reference-based metrics such as BLEU and BERTScore are widely used to evaluate question generation (QG). In this study, on QG benchmarks such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics."
  - [section]: "Fig. 1 also reveals issues with the reference-based metrics in accurately assessing Groups 1, 2, and 3. Notably, for all five reference-based metrics, Group 3, non-question sentences with wording similar to the references, receives the highest average score."

### Mechanism 2
- **Claim:** NACo, a reference-free metric, accurately distinguishes high-quality questions from flawed ones by evaluating three essential criteria: naturalness, answerability, and complexity, using large language models with Chain-of-Thought reasoning.
- **Mechanism:** NACo uses LLMs to directly assess whether a question is grammatically correct and clear (naturalness), whether it can be answered using the given context and answer (answerability), and whether it requires the expected level of reasoning (complexity). This multi-dimensional approach captures aspects of question quality that reference-based metrics miss.
- **Core assumption:** The quality of a question can be effectively evaluated by examining its linguistic properties, its grounding to the context and answer, and its reasoning requirements, without needing to compare it to a reference question.
- **Evidence anchors:**
  - [abstract]: "We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models."
  - [section]: "Fig. 1 shows the average normalized scores given by reference-based metrics to the four groups of candidate questions, all based on the same references. NACo uniquely succeeds in separating all four groups by significant margins, unlike the seven existing metrics."

### Mechanism 3
- **Claim:** The Chain-of-Thought reasoning process used by NACo enables the LLM to perform step-by-step reasoning to answer the question, which allows for a more accurate assessment of answerability and complexity compared to direct evaluation methods.
- **Mechanism:** By instructing the LLM to explicitly reason through the steps needed to answer the question, NACo can count the number of reasoning steps and compare the final answer to the ground truth. This process provides a more nuanced evaluation of whether the question is answerable and how complex it is.
- **Core assumption:** The LLM's ability to perform Chain-of-Thought reasoning is a reliable proxy for human-like reasoning and can be used to accurately assess the answerability and complexity of a question.
- **Evidence anchors:**
  - [abstract]: "We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models."
  - [section]: "We utilize large language models (LLMs), which have demonstrated potential utility in data annotation tasks (Liu et al., 2023; Wang et al., 2023; Lin and Chen, 2023; Chiang and Lee, 2023), and their Chain-of-Thought (CoT) (Wei et al., 2022) process. We design CoT prompts for the LLM to directly measure the three criteria."

## Foundational Learning

- **Concept:** Question Generation (QG) evaluation metrics
  - **Why needed here:** Understanding the different types of evaluation metrics (reference-based vs. reference-free) and their strengths and weaknesses is crucial for appreciating the novelty and significance of NACo.
  - **Quick check question:** What is the key difference between reference-based and reference-free evaluation metrics for QG?

- **Concept:** Large Language Models (LLMs) and Chain-of-Thought reasoning
  - **Why needed here:** NACo leverages LLMs with CoT reasoning to evaluate question quality. Understanding how LLMs work and how CoT reasoning can be used for evaluation is essential for grasping the technical details of NACo.
  - **Quick check question:** How does Chain-of-Thought reasoning help LLMs perform more accurate evaluations?

- **Concept:** Question quality criteria (naturalness, answerability, complexity)
  - **Why needed here:** NACo evaluates questions based on these three criteria. Understanding what each criterion means and why it is important for question quality is necessary for understanding NACo's multi-dimensional approach.
  - **Quick check question:** Why is it important for a question to be natural, answerable, and have the appropriate level of complexity?

## Architecture Onboarding

- **Component map:** Data Collection -> Metric Comparison -> NACo Implementation -> Human Evaluation -> Analysis
- **Critical path:** The critical path involves collecting the data, implementing NACo, running the experiments, and analyzing the results to demonstrate NACo's superiority over existing metrics.
- **Design tradeoffs:**
  - Using LLMs for evaluation vs. human evaluation: LLMs are more scalable and cost-effective but may have limitations in understanding context and performing complex reasoning.
  - Multi-dimensional evaluation vs. single-dimensional evaluation: NACo's multi-dimensional approach captures more aspects of question quality but may be more complex to implement and interpret.
  - Reference-free vs. reference-based evaluation: NACo does not require references, making it more flexible, but it relies on the LLM's ability to assess question quality independently.
- **Failure signatures:**
  - If NACo performs poorly on questions that require complex reasoning or domain-specific knowledge, it may indicate limitations in the underlying LLM's reasoning capabilities.
  - If NACo's scores do not correlate well with human judgments, it may suggest issues with the CoT prompts or the weighting of the different criteria.
  - If NACo is too sensitive to the specific LLM used, it may indicate a lack of robustness in the metric.
- **First 3 experiments:**
  1. Replicate the data collection process on a small subset of QG benchmarks to validate the methodology.
  2. Implement NACo with a simple LLM and evaluate it on a small set of questions to test the basic functionality.
  3. Compare NACo's performance against existing metrics on a larger set of questions to assess its effectiveness in distinguishing high-quality questions from flawed ones.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does NACo perform when evaluated against human judgment for questions that require multi-step mathematical reasoning?
- **Basis in paper:** [explicit] The paper mentions that GPT-3.5, the underlying QA model for NACo, struggles with mathematical reasoning, as seen in a case study where it failed to correctly answer a question requiring subtraction of birth and death years.
- **Why unresolved:** The paper does not provide comprehensive results or analysis of NACo's performance on questions requiring mathematical reasoning, which could be a significant limitation for certain domains or datasets.
- **What evidence would resolve it:** Conducting experiments with datasets containing questions that require multi-step mathematical reasoning and evaluating NACo's performance against human judgment would provide insights into its effectiveness in such scenarios.

### Open Question 2
- **Question:** How does the performance of NACo vary with different underlying LLM choices, particularly for questions requiring complex reasoning?
- **Basis in paper:** [explicit] The paper experiments with five different LLMs (Llama3-8B, Mixtral-8x7B, Claude3-Haiku, GPT3.5-turbo, and GPT4o) for NACo and reports correlation with human judgment, but does not extensively analyze the impact of LLM choice on NACo's performance for questions requiring complex reasoning.
- **Why unresolved:** The paper does not provide a detailed analysis of how the choice of underlying LLM affects NACo's ability to evaluate questions requiring complex reasoning, which could impact its effectiveness in certain domains or datasets.
- **What evidence would resolve it:** Conducting experiments with various underlying LLMs and datasets containing questions requiring complex reasoning, and analyzing NACo's performance for each combination, would provide insights into the impact of LLM choice on its effectiveness.

### Open Question 3
- **Question:** Can NACo be adapted to evaluate questions in domains where references are extremely limited or difficult to collect?
- **Basis in paper:** [explicit] The paper mentions that NACo relies on a subset of examples from the specific dataset to find the expected complexity of a question in that dataset, which requires access to a reasonable number of references.
- **Why unresolved:** The paper does not explore how NACo can be adapted or modified to handle scenarios where references are extremely limited or difficult to collect, which could limit its applicability in certain domains or datasets.
- **What evidence would resolve it:** Developing and testing modifications to NACo that can handle scenarios with limited references, such as incorporating unsupervised learning techniques or leveraging external knowledge bases, would provide insights into its adaptability in such situations.

## Limitations

- The study's findings are primarily based on SQuAD and HotpotQA datasets, which may limit generalizability to other QG benchmarks or domains.
- The effectiveness of NACo depends heavily on the performance of the underlying LLM, which could vary across different model versions or implementations.
- The human evaluation process, while carefully designed, may still contain subjective biases that could affect the validation of metric performance.

## Confidence

- **High Confidence:** The core finding that reference-based metrics struggle with single-reference evaluation is well-supported by experimental evidence showing systematic failures across multiple metrics.
- **Medium Confidence:** NACo's superior performance relative to existing metrics is demonstrated, but the specific contribution of each evaluation criterion (naturalness, answerability, complexity) to overall performance requires further investigation.
- **Medium Confidence:** The Chain-of-Thought methodology for answerability and complexity assessment is promising but may have limitations with more complex reasoning tasks or specialized domains.

## Next Checks

1. Test NACo on additional QG datasets beyond SQuAD and HotpotQA to assess generalizability across different domains and question types.
2. Conduct ablation studies to determine the relative importance of each evaluation criterion (naturalness, answerability, complexity) in NACo's performance.
3. Evaluate NACo's performance on questions requiring multi-hop reasoning or domain-specific knowledge to identify potential limitations in the LLM's reasoning capabilities.