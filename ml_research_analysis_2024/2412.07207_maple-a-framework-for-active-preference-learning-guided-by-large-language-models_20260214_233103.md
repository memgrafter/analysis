---
ver: rpa2
title: 'MAPLE: A Framework for Active Preference Learning Guided by Large Language
  Models'
arxiv_id: '2412.07207'
source_url: https://arxiv.org/abs/2412.07207
tags:
- learning
- maple
- preference
- language
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAPLE introduces a Bayesian active preference learning framework
  that leverages large language models to reduce uncertainty in preference function
  inference. The method integrates natural language feedback with conventional pairwise
  trajectory rankings, employing a language-conditioned active query selection mechanism
  to identify informative and easy-to-answer queries.
---

# MAPLE: A Framework for Active Preference Learning Guided by Large Language Models

## Quick Facts
- **arXiv ID**: 2412.07207
- **Source URL**: https://arxiv.org/abs/2412.07207
- **Reference count**: 8
- **Primary result**: MAPLE achieves improved sample efficiency and preference inference quality in route planning and grid navigation tasks, outperforming B-REX by up to 0.05 in accuracy and reducing policy cost differences by 0.32.

## Executive Summary
MAPLE is a Bayesian active preference learning framework that integrates large language models (LLMs) to infer user preferences from natural language feedback and pairwise trajectory comparisons. By conditioning query selection on language understanding, the framework reduces uncertainty in preference functions and improves sample efficiency. Evaluated on OpenStreetMap-based route planning and HomeGrid benchmarks, MAPLE demonstrates superior performance over traditional active preference learning methods, particularly when natural language input is available.

## Method Summary
MAPLE combines Bayesian active learning with LLM-driven query selection to optimize preference learning. It first elicits user preferences through pairwise trajectory comparisons and optional natural language feedback. An uncertainty-aware Bayesian model updates the preference function based on this data. To further reduce uncertainty, MAPLE employs a language-conditioned active query selection mechanism that leverages LLM reasoning to identify queries that are both informative and easy to answer. This approach contrasts with prior methods that rely solely on conventional query selection strategies, enabling more efficient preference function inference in complex environments.

## Key Results
- In OpenStreetMap routing tasks, MAPLE outperforms B-REX with test accuracy improvements of up to 0.05.
- MAPLE reduces policy cost differences by up to 0.32 compared to baseline methods.
- Language-conditioned query selection further enhances performance, particularly in complex environments like HomeGrid.

## Why This Works (Mechanism)
MAPLE leverages LLMs to interpret and act on natural language feedback, enabling more nuanced preference modeling than trajectory comparisons alone. The Bayesian framework continuously updates the preference function as new data arrives, while the LLM-guided query selection focuses on reducing the most uncertain aspects of the model. This synergy allows MAPLE to extract more informative feedback from fewer interactions, leading to faster convergence and higher-quality policy optimization.

## Foundational Learning
- **Bayesian Active Learning**: Used to update preference functions based on observed feedback; needed for uncertainty-aware decision making, quick check: does the model reduce uncertainty with each query?
- **Pairwise Preference Elicitation**: Standard method for collecting user preferences; needed for structured preference learning, quick check: are preferences consistently defined across annotators?
- **Natural Language Understanding**: Enables integration of free-form feedback; needed to capture richer preference information, quick check: does the LLM correctly interpret intent?
- **Query Selection Strategies**: Focus on maximizing information gain; needed to reduce sample complexity, quick check: are selected queries truly informative?

## Architecture Onboarding
**Component Map**: User Feedback -> Preference Model -> Query Selector -> LLM -> Updated Preference Model
**Critical Path**: User feedback (natural language + pairwise comparisons) → Preference model update → LLM-driven query selection → New query generation → Repeat
**Design Tradeoffs**: Uses LLMs for query selection, trading off inference cost for improved sample efficiency; requires access to reliable language models.
**Failure Signatures**: Performance degrades if LLM misinterprets user intent or if natural language feedback is ambiguous or inconsistent.
**First Experiments**:
1. Compare MAPLE’s accuracy against B-REX on OpenStreetMap routing with varying numbers of feedback queries.
2. Test MAPLE’s robustness to noisy natural language feedback in the HomeGrid environment.
3. Perform ablation studies isolating the contribution of language-conditioned query selection.

## Open Questions the Paper Calls Out
None

## Limitations
- MAPLE’s performance depends heavily on the quality and consistency of natural language feedback, which may vary across annotators.
- The framework assumes reliable LLM responses for query selection, introducing potential bias if the LLM’s understanding of preferences doesn’t align with human intent.
- Experiments are limited to structured domains (route planning, grid navigation), raising uncertainty about scalability to high-dimensional or long-horizon tasks.

## Confidence
- **High Confidence**: Claims about improved sample efficiency and preference inference quality relative to B-REX in tested benchmarks, supported by quantitative metrics.
- **Medium Confidence**: General applicability of language-conditioned query selection for reducing query complexity, as results are limited to specific domains.
- **Low Confidence**: Robustness to noisy or adversarial natural language feedback, as this scenario is not explicitly tested.

## Next Checks
1. Conduct stress tests with noisy or adversarial natural language feedback to evaluate MAPLE’s robustness in preference learning.
2. Extend experiments to high-dimensional continuous control tasks (e.g., MuJoCo or humanoid robotics) to assess scalability.
3. Perform ablation studies isolating the contribution of language-conditioned query selection versus Bayesian active learning alone.