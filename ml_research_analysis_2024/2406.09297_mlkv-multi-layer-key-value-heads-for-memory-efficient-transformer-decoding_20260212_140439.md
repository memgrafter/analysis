---
ver: rpa2
title: 'MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding'
arxiv_id: '2406.09297'
source_url: https://arxiv.org/abs/2406.09297
tags:
- heads
- size
- memory
- mlkv
- cache
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Multi-Layer Key-Value (MLKV) sharing, a novel
  approach to reduce memory usage in transformer decoding by extending KV sharing
  across transformer layers. The authors demonstrate that MLKV significantly reduces
  memory usage compared to Multi-Query Attention (MQA) and Grouped-Query Attention
  (GQA), with up to 6x reduction in KV cache size.
---

# MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding

## Quick Facts
- arXiv ID: 2406.09297
- Source URL: https://arxiv.org/abs/2406.09297
- Authors: Zayd Muhammad Kawakibi Zuhri; Muhammad Farid Adilazuarda; Ayu Purwarianti; Alham Fikri Aji
- Reference count: 3
- One-line primary result: MLKV reduces KV cache memory usage by up to 6x while maintaining near-parity with baseline performance

## Executive Summary
This paper introduces Multi-Layer Key-Value (MLKV) sharing, a novel approach to reduce memory usage in transformer decoding by extending KV sharing across transformer layers. The authors demonstrate that MLKV significantly reduces memory usage compared to Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), with up to 6x reduction in KV cache size. Evaluations on NLP benchmarks show minimal performance loss, with MLKV variants achieving near-parity with baseline models.

## Method Summary
The paper proposes MLKV, which extends KV sharing from within-layer (as in MQA/GQA) to across multiple transformer layers. The method converts existing transformer checkpoints to MLKV through an uptraining strategy, using 5% of the original training dataset. During conversion, KV heads are merged by averaging weights, and intermediate MLP layers are upsized to maintain parameter count. The approach is evaluated on Pythia-160M using deduplicated The Pile dataset, with experiments running for approximately 22 hours using AdamW optimizer.

## Key Results
- MLKV achieves up to 6x reduction in KV cache memory usage compared to standard multi-head attention
- MLKV variants show minimal performance degradation on NLP benchmarks (ARC-e, LAMBADA, PIQA, SciQ)
- Sharing KV heads to every second layer provides optimal memory-accuracy trade-off, though more extreme configurations are viable for memory-constrained use cases

## Why This Works (Mechanism)

### Mechanism 1
Sharing KV heads across multiple transformer layers reduces KV cache memory usage proportionally to the ratio of shared heads to total heads. By reusing the same KV projection weights across layers, the KV cache only needs to store activations for the reduced number of unique KV heads instead of one per layer. This directly reduces the KV cache tensor dimensions from [b, s, l, h, dk] to [b, s, m, g, dk] where m < l.

### Mechanism 2
The uptraining strategy allows conversion of existing transformer checkpoints to MLKV without catastrophic forgetting. By continuing pre-training on a subset of the original dataset (5%), the model can adapt its weights to accommodate the merged KV heads while maintaining learned representations. The averaging of KV head weights during conversion preserves essential information.

### Mechanism 3
MLKV achieves better memory-accuracy trade-offs than GQA/MQA at equivalent KV head counts by optimizing the distribution of shared heads across layers. MLKV can allocate shared KV heads to layers that compute similar semantic representations, while GQA/MQA are limited to sharing only within layers. This allows MLKV to maintain performance with fewer total KV heads.

## Foundational Learning

- Concept: Transformer attention mechanism and multi-head attention
  - Why needed here: Understanding how attention works and why multiple heads are used is fundamental to grasping why sharing KV heads reduces memory usage
  - Quick check question: In multi-head attention, what are the dimensions of the query, key, and value projections relative to the model dimension?

- Concept: KV caching in autoregressive decoding
  - Why needed here: The motivation for KV sharing comes from the memory bottleneck of KV caching during inference, so understanding how KV caching works is essential
  - Quick check question: How does KV caching reduce computational complexity during autoregressive decoding?

- Concept: Parameter sharing and model compression techniques
  - Why needed here: MLKV is fundamentally a parameter sharing technique, so understanding how parameter sharing affects model capacity and performance is important
  - Quick check question: What are the typical trade-offs when applying parameter sharing techniques to neural networks?

## Architecture Onboarding

- Component map:
  Attention mechanism with query, key, and value projections -> MLP layers that process attention outputs -> KV cache storage for key and value activations -> Weight sharing logic that determines which layers/heads share KV projections

- Critical path:
  1. Forward pass through attention mechanism
  2. KV cache storage and retrieval
  3. MLP processing
  4. Weight sharing configuration

- Design tradeoffs:
  - More aggressive KV sharing (lower mg) provides greater memory savings but potentially more performance degradation
  - Configuration of which layers share KV heads affects both memory usage and performance
  - Balancing parameter count across layers when compensating for merged KV heads

- Failure signatures:
  - Out-of-memory errors during inference with high batch sizes/sequence lengths
  - Degraded benchmark performance as KV head count decreases
  - Training instability during uptraining phase

- First 3 experiments:
  1. Verify KV cache size reduction by measuring memory usage with different mg configurations
  2. Test benchmark performance degradation across the mg spectrum (1 to l)
  3. Compare inference throughput with baseline MHA, MQA, and GQA configurations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do MLKV models perform when trained from scratch rather than through uptraining?
- Basis in paper: The authors acknowledge that they did not train models from scratch and thus do not know how MLKV performs on models that are natively imbued with this KV sharing scheme from the pre-training phase.
- Why unresolved: The experiments only used uptrained models from existing checkpoints, which may not capture the full potential of MLKV.
- What evidence would resolve it: Training MLKV models from scratch and comparing their performance with both baseline models and uptrained MLKV models would provide insights into the effectiveness of native MLKV training.

### Open Question 2
- Question: How does MLKV perform at larger model scales, such as billion-parameter models?
- Basis in paper: The authors mention that their experiments were conducted on relatively small models (160 million and 410 million parameters) and acknowledge that MLKV has yet to be tested at billion-parameter scales.
- Why unresolved: The scalability of MLKV to larger models is uncertain, and its performance in such scenarios is not yet known.
- What evidence would resolve it: Evaluating MLKV on models with billions of parameters would demonstrate its effectiveness and limitations at larger scales.

### Open Question 3
- Question: What is the impact of MLKV on tasks beyond the limited downstream tasks evaluated in the study?
- Basis in paper: The authors state that due to the relatively small size of the model tested, the number and variety of downstream tasks that can be reliably used to compare performance is limited.
- Why unresolved: The study's evaluation was restricted to a few benchmarks, and its impact on other tasks remains unexplored.
- What evidence would resolve it: Conducting experiments on a wider range of tasks and datasets would provide a comprehensive understanding of MLKV's applicability and performance across different domains.

## Limitations
- Limited evaluation to Pythia-160M model architecture and deduplicated The Pile dataset
- Uptraining strategy effectiveness for cross-layer KV sharing not thoroughly validated
- No empirical analysis of layer similarity or attention pattern compatibility across layers

## Confidence
**High Confidence**: The core claim that MLKV reduces KV cache memory usage is well-supported by the mathematical formulation and confirmed by memory usage measurements.

**Medium Confidence**: The claim that MLKV achieves near-parity with baseline performance on NLP benchmarks is supported by experimental results, but these are limited to a single model size and dataset.

**Low Confidence**: The claim about uptraining being an effective strategy for converting existing checkpoints to MLKV lacks sufficient evidence and optimal parameter tuning.

## Next Checks
1. Test MLKV on multiple model architectures (e.g., GPT-2 variants, OPT models) and different dataset domains to verify whether the memory-accuracy trade-offs observed with Pythia-160M generalize across the transformer family.

2. Conduct empirical analysis of attention pattern similarity across transformer layers using cosine similarity or other metrics to validate the core assumption that layers compute similar representations and can therefore share KV heads effectively.

3. Systematically vary the uptraining dataset size, learning rate schedules, and duration to determine the minimum viable training requirements for successful MLKV conversion, and compare against alternative adaptation strategies like full fine-tuning or adapter-based approaches.