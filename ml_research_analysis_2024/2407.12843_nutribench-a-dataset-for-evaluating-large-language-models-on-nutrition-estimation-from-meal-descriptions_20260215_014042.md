---
ver: rpa2
title: 'NutriBench: A Dataset for Evaluating Large Language Models on Nutrition Estimation
  from Meal Descriptions'
arxiv_id: '2407.12843'
source_url: https://arxiv.org/abs/2407.12843
tags:
- serving
- food
- meal
- carbohydrates
- total
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NUTRI BENCH , the first publicly available
  dataset for evaluating large language models (LLMs) on nutrition estimation from
  natural language meal descriptions. The dataset contains 5,000 human-verified meal
  descriptions with macro-nutrient annotations, organized into 15 subsets varying
  in complexity.
---

# NutriBench: A Dataset for Evaluating Large Language Models on Nutrition Estimation from Meal Descriptions

## Quick Facts
- arXiv ID: 2407.12843
- Source URL: https://arxiv.org/abs/2407.12843
- Reference count: 40
- Primary result: LLMs achieve 51.48% accuracy for carbohydrate estimation, outperforming professional nutritionists in speed

## Executive Summary
This paper introduces NUTRI BENCH, the first publicly available dataset for evaluating large language models on nutrition estimation from natural language meal descriptions. The dataset contains 5,000 human-verified meal descriptions with macro-nutrient annotations, organized into 15 subsets varying in complexity. The authors evaluate seven leading LLMs across four prompting strategies, including Chain-of-Thought and Retrieval-Augmented Generation. Results show that GPT-3.5 with Chain-of-Thought achieves the highest accuracy of 51.48% for carbohydrate estimation. Surprisingly, most LLMs outperformed professional nutritionists in accuracy while providing significantly faster estimates.

## Method Summary
The authors constructed NUTRI BENCH by curating 5,000 meal descriptions from various sources and annotating them with macro-nutrient information. They evaluated seven LLMs (GPT-3.5, Llama3 variants, Alpaca, MedAlpaca) using four prompting strategies: Base, Chain-of-Thought, Retrieval-Augmented Generation, and RAG+CoT. Performance was measured using Acc@7.5 (accuracy within ±7.5g of ground truth) and answer rate metrics across 15 subsets varying by item count and serving description type.

## Key Results
- GPT-3.5 with Chain-of-Thought achieves highest accuracy of 51.48% for carbohydrate estimation
- Most LLMs outperform professional nutritionists in accuracy while providing faster estimates
- LLMs struggle significantly with uncommon serving sizes and high-carbohydrate foods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-Thought prompting enables LLMs to handle complex meal descriptions by breaking down reasoning into discrete steps.
- Mechanism: CoT induces step-by-step decomposition of meal components and serving sizes, reducing errors from holistic reasoning failures.
- Core assumption: LLMs struggle with multi-step reasoning in nutrition estimation unless explicitly guided.
- Evidence anchors:
  - [abstract] "GPT-3.5 with Chain-of-Thought achieves the highest accuracy of 51.48% for carbohydrate estimation"
  - [section] "CoT consistently improves both answer rate and accuracy across 15 subsets and 7 LLMs"
  - [corpus] Weak - no direct citation of CoT benefits in nutrition tasks
- Break condition: If CoT steps are too complex or ambiguous, errors may compound rather than resolve.

### Mechanism 2
- Claim: Retrieval-Augmented Generation improves accuracy when serving descriptions are aligned between query and retrieved context.
- Mechanism: RAG provides external nutrition knowledge that LLMs use to refine predictions, but only when serving units match.
- Core assumption: LLMs can effectively integrate retrieved nutrition facts when serving descriptions are consistent.
- Evidence anchors:
  - [abstract] "RAG improves accuracy for metric serving queries"
  - [section] "RAG consistently improves both Acc@7.5 and AR for metric serving queries"
  - [corpus] Weak - general RAG effectiveness cited but not specific to nutrition serving alignment
- Break condition: If serving units differ (e.g., metric vs natural), RAG may introduce conversion errors or misdirection.

### Mechanism 3
- Claim: LLMs outperform nutritionists in speed and stress tolerance for nutrition estimation tasks.
- Mechanism: LLMs process all meal description complexities uniformly without cognitive load, unlike humans.
- Core assumption: Human stress and time constraints significantly impact nutrition estimation accuracy.
- Evidence anchors:
  - [abstract] "most LLMs outperformed even a professional nutritionist and non-experts"
  - [section] "it takes the nutritionist in total of 50 minutes to complete all 90 queries" vs "LLMs can answer all 90 queries within minutes"
  - [corpus] Weak - general LLM speed cited but not specific to nutrition task comparison
- Break condition: If queries become too complex or ambiguous, human expertise may still surpass LLM performance.

## Foundational Learning

- Concept: Large Language Models and prompting strategies (CoT, RAG)
  - Why needed here: Understanding how different prompting methods affect LLM performance on nutrition estimation tasks
  - Quick check question: What is the difference between Chain-of-Thought and Retrieval-Augmented Generation prompting?

- Concept: Nutrition estimation and carbohydrate counting for diabetes management
  - Why needed here: Domain knowledge required to evaluate LLM performance on realistic nutrition estimation scenarios
  - Quick check question: Why is accurate carbohydrate estimation critical for individuals with diabetes?

- Concept: Dataset construction and evaluation methodology
  - Why needed here: Understanding how NUTRI BENCH was created and how LLM performance is measured
  - Quick check question: What evaluation metric is used to determine if an LLM's carbohydrate prediction is "correct"?

## Architecture Onboarding

- Component map: Data curation -> Cleaning/filtering -> Serving description extraction -> RETRI-DB construction -> NUTRI BENCH generation -> Human verification -> LLM evaluation pipeline
- Critical path: Data curation -> Cleaning/filtering -> NUTRI BENCH generation -> Human verification -> LLM evaluation
- Design tradeoffs: Larger LLMs generally perform better but require more computation; CoT improves accuracy but may reduce answer rate; RAG helps with aligned servings but introduces retrieval complexity
- Failure signatures: Parsing errors (incorrect food identification), serving size errors (wrong quantity interpretation), incorrect predictions (hallucination or wrong prior knowledge)
- First 3 experiments:
  1. Evaluate GPT-3.5 with Base prompt on single-item natural serving subset
  2. Evaluate Llama3-8B with CoT prompt on double-item metric serving subset
  3. Evaluate GPT-3.5 with RAG+CoT prompt on triple-item indirect retrieval subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve LLM performance on high-carbohydrate foods to reduce prediction errors?
- Basis in paper: [explicit] The paper observes that high-carbohydrate foods lead to larger prediction errors, with a positive correlation between true carbohydrate content and MAE.
- Why unresolved: While the paper identifies this trend, it does not propose or test specific solutions to address the issue.
- What evidence would resolve it: Experiments testing different prompting strategies, fine-tuning approaches, or retrieval methods specifically focused on high-carbohydrate foods could provide insights into potential solutions.

### Open Question 2
- Question: What is the optimal way to handle serving size conversions in LLM-based nutrition estimation?
- Basis in paper: [explicit] The paper finds that LLMs struggle to convert between metric and natural serving descriptions, even when instructed to do so.
- Why unresolved: The paper identifies the problem but does not explore potential solutions or the underlying reasons for this difficulty.
- What evidence would resolve it: Experiments testing different approaches to serving size conversion, such as explicit conversion instructions, additional training data, or specialized models for unit conversion, could provide insights into effective solutions.

### Open Question 3
- Question: How can we improve the accuracy of LLM predictions for uncommon food items?
- Basis in paper: [explicit] The paper observes that the model struggles with uncommon food items, often making unsure predictions or hallucinations instead of refraining from answering.
- Why unresolved: While the paper identifies this issue, it does not propose or test specific methods to address the challenge of uncommon food items.
- What evidence would resolve it: Experiments testing different approaches, such as incorporating more diverse food data, using retrieval methods with broader coverage, or developing specialized models for uncommon items, could provide insights into potential solutions.

## Limitations
- Dataset Scope: NUTRI BENCH focuses exclusively on US-based meal descriptions with English language inputs, limiting generalizability to other cuisines and languages.
- Evaluation Metrics: The choice of ±7.5g tolerance for Acc@7.5 is somewhat arbitrary and may not reflect clinical significance for different health conditions.
- Human Expert Comparison: The comparison with a single professional nutritionist provides limited statistical power and may not represent broader expert populations.

## Confidence
- High Confidence: The general finding that Chain-of-Thought prompting improves LLM performance on structured reasoning tasks
- Medium Confidence: The specific performance numbers and relative ranking of different LLMs across prompting strategies
- Low Confidence: The claim that LLMs significantly outperform professional nutritionists in nutrition estimation accuracy

## Next Checks
1. **Cross-Cuisine Validation**: Test NUTRI BENCH-trained LLMs on meal descriptions from diverse culinary traditions to assess cultural generalizability of nutrition estimation capabilities.

2. **Extended Expert Benchmarking**: Conduct a larger-scale comparison with multiple nutritionists and dietitians across different specialties to establish more robust human-AI performance baselines.

3. **Clinical Outcome Simulation**: Evaluate whether LLM nutrition estimations translate to clinically meaningful outcomes by simulating their use in diabetes management scenarios with varying accuracy thresholds.