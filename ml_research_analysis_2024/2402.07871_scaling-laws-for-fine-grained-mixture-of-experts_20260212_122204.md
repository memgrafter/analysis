---
ver: rpa2
title: Scaling Laws for Fine-Grained Mixture of Experts
arxiv_id: '2402.07871'
source_url: https://arxiv.org/abs/2402.07871
tags:
- granularity
- training
- scaling
- number
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a new hyperparameter, granularity, which enables
  precise control over the size of experts in Mixture-of-Experts (MoE) models. By
  adjusting granularity, MoE models can achieve significantly better efficiency compared
  to dense Transformers, with the efficiency gap widening as model size and training
  budget increase.
---

# Scaling Laws for Fine-Grained Mixture of Experts

## Quick Facts
- arXiv ID: 2402.07871
- Source URL: https://arxiv.org/abs/2402.07871
- Reference count: 40
- Introduces granularity hyperparameter for precise control over expert sizes in MoE models, achieving better efficiency than dense Transformers

## Executive Summary
This work introduces a new hyperparameter called granularity that enables precise control over the size of experts in Mixture-of-Experts (MoE) models. By adjusting granularity, MoE models can achieve significantly better efficiency compared to dense Transformers, with the efficiency gap widening as model size and training budget increase. The study derives new scaling laws that incorporate variable training duration, model size, and granularity, allowing for the computation of optimal training configurations.

## Method Summary
The researchers developed a theoretical framework that models MoE efficiency through a new granularity parameter, which determines the number of experts relative to the total model capacity. They conducted extensive experiments training MoE models across various sizes and granularities, measuring training efficiency and performance. The scaling laws were derived by fitting power-law relationships to the empirical data, showing how optimal granularity changes with model size and training duration.

## Key Results
- MoE models with optimal granularity consistently outperform dense Transformers at any computational budget
- The efficiency gap between MoE and dense models widens as model size and training budget increase
- New scaling laws successfully predict optimal granularity configurations for maximum efficiency

## Why This Works (Mechanism)
The mechanism works by distributing computational load across multiple specialized experts rather than requiring all parameters to be active simultaneously. Granularity acts as a lever to balance the trade-off between having enough experts for specialization versus having sufficiently large experts for meaningful computation. Higher granularity allows more fine-grained specialization but requires more routing overhead, while lower granularity reduces routing complexity but limits specialization potential.

## Foundational Learning
- Mixture-of-Experts architecture: Why needed - enables conditional computation where only relevant experts are activated for each input. Quick check - verify that gating mechanism properly routes tokens to appropriate experts.
- Scaling laws in deep learning: Why needed - provides theoretical framework for predicting model performance and efficiency at different scales. Quick check - confirm power-law relationships hold across the tested range.
- Expert capacity and routing: Why needed - determines how many tokens each expert can process and how routing decisions are made. Quick check - ensure no expert is overloaded or underutilized during training.

## Architecture Onboarding
- Component map: Data -> Tokenization -> MoE Layer (Gating + Experts) -> Dense Layer -> Output
- Critical path: Input tokens flow through gating mechanism to select active experts, which process the tokens in parallel, then combine results through weighted summation
- Design tradeoffs: Higher granularity increases specialization but adds routing overhead; expert capacity limits determine computational efficiency; routing algorithms affect load balancing
- Failure signatures: Expert overload (some experts receive too many tokens), poor routing (gating mechanism fails to distinguish between expert capabilities), capacity underutilization (experts remain idle)
- First experiments: 1) Test different granularity values on a fixed model size to observe efficiency trends, 2) Vary expert capacity while holding granularity constant to measure impact on performance, 3) Compare routing algorithms (e.g., top-1 vs top-2) with optimal granularity settings

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Theoretical framework assumes granularity can be optimized independently of other hyperparameters, potentially oversimplifying complex interactions
- Scaling law derivations rely on power-law relationships that may not hold at extreme model sizes or training durations
- Analysis focuses on efficiency metrics rather than absolute performance on downstream tasks

## Confidence
The claim that MoE models can consistently outperform dense Transformers at any computational budget with optimal granularity settings receives a Medium confidence rating. While the experimental results are compelling within the tested ranges, the generality of this claim across all possible computational regimes and model architectures remains to be established.

## Next Checks
1. Test the scaling laws at model sizes beyond 70B parameters to verify if the power-law relationships hold at extreme scales
2. Conduct ablation studies to isolate the effects of granularity from other MoE-specific hyperparameters like expert capacity and routing algorithms
3. Evaluate optimal granularity configurations on a broader range of downstream tasks to confirm that efficiency gains translate to maintained or improved task performance across different domains