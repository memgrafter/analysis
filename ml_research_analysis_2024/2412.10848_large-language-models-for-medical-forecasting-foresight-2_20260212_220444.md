---
ver: rpa2
title: Large Language Models for Medical Forecasting -- Foresight 2
arxiv_id: '2412.10848'
source_url: https://arxiv.org/abs/2412.10848
tags:
- patient
- text
- concepts
- medical
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Foresight 2 (FS2) is a large language model fine-tuned on hospital
  data for modeling patient timelines. It can understand patients' clinical notes
  and predict SNOMED codes for diagnosis suggestions, risk forecasting, and medication
  recommendations.
---

# Large Language Models for Medical Forecasting -- Foresight 2

## Quick Facts
- arXiv ID: 2412.10848
- Source URL: https://arxiv.org/abs/2412.10848
- Reference count: 20
- Primary result: FS2 significantly outperforms GPT-4-turbo in risk forecasting (P@5 - 0.90 vs 0.65) when fine-tuned on hospital data

## Executive Summary
Foresight 2 (FS2) is a large language model fine-tuned on hospital data for modeling patient timelines and medical forecasting. By training on the free text portion of the MIMIC-III dataset with extracted biomedical concepts and contextual patient timelines, FS2 demonstrates significant improvements over previous state-of-the-art methods for next biomedical concept prediction and risk forecasting. The model shows that smaller, specialized models can outperform larger general models when fine-tuned on high-quality clinical data, highlighting the importance of incorporating hospital-specific information into medical LLMs.

## Method Summary
FS2 is built on top of a pretrained LLM (Mistralv0.1-7B or LLaMAv2-7B) that is fine-tuned on contextualized patient timelines created from MIMIC-III data. The timelines consist of SNOMED codes representing biomedical concepts and their surrounding context extracted from clinical notes. The model uses a modified language modeling objective that trains only on the SNOMED codes while ignoring the context during loss calculation. Data preparation involves extracting biomedical concepts using MedCAT, bucketing by 1-day intervals, and building patient timelines with context. The fine-tuning process uses specific hyperparameters (lr=1e-5, batch_size=1, gradient_accumulation_steps=2) for approximately one epoch.

## Key Results
- Next biomedical concept prediction: FS2 achieves precision/recall of 0.73/0.66 vs 0.52/0.32 for previous state-of-the-art
- Next disorder prediction: FS2 achieves precision/recall of 0.69/0.62 vs 0.46/0.25 for previous state-of-the-art
- Risk forecast task: FS2 outperforms GPT-4-turbo with P@5 of 0.90 vs 0.65

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning a general LLM on hospital data enables superior risk forecasting compared to larger, non-specialized models.
- Mechanism: The specialized training data captures domain-specific clinical patterns and temporal dependencies that general models lack, allowing the fine-tuned model to better predict disease progression and risk.
- Core assumption: Hospital data contains unique patterns and dependencies not present in general training data, and these patterns are learnable by the model.
- Evidence anchors: Abstract shows FS2 performs significantly better on risk forecast tasks (P@5 - 0.90 vs 0.65) than GPT-4-turbo; paper section states small models outperform larger ones when fine-tuned on specialized data.

### Mechanism 2
- Claim: Contextualized patient timelines with surrounding text improve prediction accuracy over concept-only timelines.
- Mechanism: By including the surrounding text where biomedical concepts are mentioned, the model can capture qualifiers, negations, and contextual information that modify the meaning of the concept, leading to more accurate predictions.
- Core assumption: The surrounding text provides meaningful information that modifies or qualifies the biomedical concept, and this information is learnable by the model.
- Evidence anchors: Abstract mentions patient timelines in FS2 are contextualized; methodology section explains retention of contextual information is crucial for capturing qualifying information and including negated or hypothetical concepts.

### Mechanism 3
- Claim: Using SNOMED codes as standardized output format enables effective ranking and prediction of biomedical concepts.
- Mechanism: SNOMED codes provide a standardized, hierarchical ontology that allows the model to rank predictions based on probability and ensures compatibility with existing healthcare informatics systems.
- Core assumption: SNOMED codes cover the relevant biomedical concepts in the dataset and provide a meaningful hierarchy for ranking predictions.
- Evidence anchors: Abstract states FS2 can predict SNOMED codes for various biomedical use cases; methodology section lists four reasons for using SNOMED codes including standardization, ranking capability, and privacy preservation.

## Foundational Learning

- Concept: Temporal modeling in patient timelines
  - Why needed here: The model needs to understand the sequence and timing of events in a patient's medical history to make accurate predictions about future conditions.
  - Quick check question: Can you explain how the model uses temporal separators (e.g., "<1 day later>") to understand the timing of events in a patient's timeline?

- Concept: Biomedical concept extraction and linking
  - Why needed here: The model relies on accurately extracting and linking biomedical concepts from clinical text to create the patient timelines used for training and inference.
  - Quick check question: How does the NER+L tool (MedCAT) contribute to the accuracy of the patient timelines, and what are its limitations?

- Concept: Evaluation metrics for biomedical concept prediction
  - Why needed here: The model's performance is evaluated using custom metrics that account for the temporal window, concept temporality (new vs recurring), and the number of candidates considered.
  - Quick check question: Can you explain the difference between precision/recall for new vs recurring concepts, and why this distinction is important for evaluating the model's performance?

## Architecture Onboarding

- Component map: Pretrained LLM (Mistralv0.1-7B or LLaMAv2-7B) -> Fine-tuning on contextualized patient timelines -> Prediction of SNOMED codes for biomedical concepts and risk forecasting
- Critical path: Data preparation (extract concepts, create timelines) → Model fine-tuning (on contextualized timelines) → Inference (predict next concepts or risk) → Evaluation (using custom metrics)
- Design tradeoffs: Using a smaller, fine-tuned model vs a larger, general model; including contextual information vs keeping timelines concise; using SNOMED codes vs free text for output; balancing precision and recall for different types of concepts
- Failure signatures: Poor performance on specific concept types; overfitting to the training data; inability to handle long sequences; sensitivity to noisy or inconsistent data
- First 3 experiments:
  1. Test the model's performance on a held-out validation set to check for overfitting and ensure generalization.
  2. Ablation study: Remove contextual information from the timelines and compare performance to the full model to quantify the importance of context.
  3. Test the model's performance on a different hospital dataset (if available) to assess its ability to generalize to new data sources.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would FS2's performance change if trained on larger, more diverse hospital datasets beyond MIMIC-III?
- Basis in paper: The paper mentions that "significantly larger hospital datasets and general medical literature are needed to better cover all possible biomedical concepts found in SNOMED and prevent biases or inaccuracies that can stem from using a single hospital as the training dataset."
- Why unresolved: The current model was trained on MIMIC-III data from a single hospital system, limiting its generalizability across different healthcare settings and patient populations.
- What evidence would resolve it: Training FS2 on multiple diverse hospital datasets and comparing performance metrics against the current model would demonstrate whether dataset diversity improves prediction accuracy and generalizability.

### Open Question 2
- Question: Can FS2 be extended to generate predictions in free text format rather than just SNOMED codes, and how would this impact clinical utility?
- Basis in paper: The paper states "We also note a limitation with respect to the ontological classification systems such as SNOMED or ICD-10 - these systems may not cover all details and nuances within the clinical text" and mentions this will be explored in future work.
- Why unresolved: While SNOMED codes provide standardization, they may not capture all clinical nuances, and the current model's output is limited to these codes despite being able to understand free text.
- What evidence would resolve it: Implementing a variant of FS2 that generates free text predictions and evaluating its performance against SNOMED-only output on clinical tasks would show whether free text generation improves clinical utility.

### Open Question 3
- Question: What is the impact of different context window sizes on FS2's prediction accuracy for various clinical tasks?
- Basis in paper: The paper shows FS2 uses contextual information from clinical notes, but doesn't explore how varying the amount of context affects performance.
- Why unresolved: The current implementation uses fixed context windows (50 tokens on each side of extracted entities), but optimal context size may vary by clinical task or concept type.
- What evidence would resolve it: Systematically varying context window sizes during training and testing FS2 on different clinical prediction tasks would reveal optimal context requirements for various use cases.

## Limitations
- The comparative advantage over GPT-4-turbo relies on a single risk forecasting benchmark without cross-validation across multiple clinical scenarios or datasets.
- The temporal modeling mechanism assumes that 1-day bucketing and "<X days later>" separators adequately capture clinical progression patterns, though this temporal granularity is not validated for different types of conditions.
- The generalizability claim that "small models outperform much larger ones when fine-tuned on high-quality, specialised data" extends beyond the evidence presented, as only one specialized dataset (MIMIC-III) was used.

## Confidence
- **High confidence**: The technical implementation details of the fine-tuning procedure and the methodology for creating contextualized patient timelines are well-specified and reproducible.
- **Medium confidence**: The performance improvements over baseline models (next concept prediction) are supported by clear metrics, though the comparison to GPT-4-turbo in risk forecasting may not fully account for differences in evaluation methodology.
- **Low confidence**: The generalizability claim that "small models outperform much larger ones when fine-tuned on high-quality, specialised data" extends beyond the evidence presented, as only one specialized dataset (MIMIC-III) was used.

## Next Checks
1. **Temporal Validation**: Re-split the dataset ensuring strict temporal separation between training and test sets (all test patient data occurs after all training data chronologically) and re-evaluate risk forecasting performance to confirm no data leakage.

2. **Clinical Expert Review**: Have clinical experts manually review a sample of the model's top-5 risk predictions for 50 patients to assess whether the predictions align with reasonable clinical expectations and identify any systematic biases or clinically implausible outputs.

3. **Cross-Dataset Generalization**: Fine-tune the same model architecture on a different hospital dataset (e.g., eICU or a different institution's records) and evaluate whether the performance gains over baseline models transfer, testing the generalizability of the hospital data fine-tuning approach.