---
ver: rpa2
title: 'ASR Benchmarking: Need for a More Representative Conversational Dataset'
arxiv_id: '2409.12042'
source_url: https://arxiv.org/abs/2409.12042
tags:
- speech
- talkbank
- dataset
- audio
- segments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study reveals that existing ASR models, which perform well
  on standard benchmarks like LibriSpeech and Fleurs, struggle significantly when
  tested on a more realistic conversational dataset derived from TalkBank. The dataset,
  consisting of unstructured phone conversations, introduces challenges like disfluencies,
  interruptions, and diverse accents that are not well represented in traditional
  benchmarks.
---

# ASR Benchmarking: Need for a More Representative Conversational Dataset

## Quick Facts
- arXiv ID: 2409.12042
- Source URL: https://arxiv.org/abs/2409.12042
- Reference count: 19
- Performance of state-of-the-art ASR models degrades substantially (up to 3x WER increase) when tested on conversational speech data

## Executive Summary
Current ASR models achieve impressive performance on standard benchmarks like LibriSpeech and Fleurs, but struggle significantly when evaluated on real-world conversational data. This study introduces a new benchmark based on the TalkBank dataset, which captures the complexities of unstructured phone conversations including disfluencies, interruptions, and diverse accents. Across multiple state-of-the-art ASR systems, including Whisper, wav2vec2, and Canary, performance dropped substantially when tested on this conversational dataset, with WER increasing by up to 3x compared to controlled settings. The research also found a positive correlation between the presence of conversational markers and higher WER, highlighting the limitations of current ASR systems in handling real-world conversational speech.

## Method Summary
The study preprocesses the TalkBank dataset (CallFriend and CallHome) to create a conversational speech benchmark, applying manual filtering, speaker-channel alignment, timestamp alignment, and automated filtering based on ASR outputs. Three ASR models (Whisper Large V3, wav2vec2-XLSR-Multilingual-56, Canary) are evaluated on this dataset and compared against performance on controlled benchmarks (LibriSpeech, Fleurs, CommonVoice). The primary evaluation metric is Word Error Rate (WER), with additional analysis of the correlation between conversational markers and WER. The TalkBank dataset is segmented in two ways: Segments (annotation-based) and Speaker Switch (speaker-based), allowing analysis of segmentation impact on performance.

## Key Results
- ASR models show up to 3x increase in WER when tested on conversational data versus controlled benchmarks
- All evaluated models (Whisper, wav2vec2, Canary) exhibit similar performance degradation patterns on conversational speech
- Positive correlation observed between conversational markers (laughter, pauses, interjections) and higher WER
- Speaker-based segmentation yields better performance than annotation-based segmentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASR models trained on controlled datasets underperform significantly on unstructured conversational speech.
- Mechanism: Controlled datasets (LibriSpeech, Fleurs) lack disfluencies like pauses, interruptions, and diverse accents, which are prevalent in real-world conversational data. Models optimized for clean, structured speech fail when exposed to noisy, unstructured conversational environments.
- Core assumption: The distribution shift between training and evaluation datasets directly impacts model performance.
- Evidence anchors:
  - [abstract] "ASR systems have achieved remarkable performance on widely used benchmarks such as LibriSpeech and Fleurs. However, these benchmarks do not adequately reflect the complexities of real-world conversational environments..."
  - [section] "Our analysis reveals a substantial performance drop in conversational settings. For instance, while CANARY achieves a WER of 0.19 on LibriSpeech, its WER on TalkBank rises to 0.54."
  - [corpus] Found 25 related papers, indicating limited prior work on conversational ASR benchmarks.
- Break condition: If ASR models are trained with sufficient conversational data containing disfluencies, the performance gap may reduce.

### Mechanism 2
- Claim: The presence of conversational-specific markers correlates with higher WER.
- Mechanism: Conversational markers (laughter, pauses, interjections) introduce variability that ASR models are not trained to handle, leading to increased errors.
- Core assumption: ASR models lack the ability to interpret and accurately transcribe conversational disfluencies.
- Evidence anchors:
  - [abstract] "Furthermore, we observe a correlation between Word Error Rate and the presence of speech disfluencies..."
  - [section] "In Figure 3, we plot the correlation score between the number of conversational-specific markers, normalized by the length of the transcripts, and the associated WER. The 'overall' category represents the combined effect of all conversational-specific markers. Our findings indicate that all ASR models exhibit a small positive correlation between the presence of these markers and the WER."
  - [corpus] Weak evidence; corpus mentions related work but lacks direct evidence of marker impact.
- Break condition: If models are fine-tuned with conversational data containing these markers, the correlation may weaken.

### Mechanism 3
- Claim: Speaker-based segmentation improves ASR performance compared to annotation-based segmentation.
- Mechanism: Longer, speaker-consistent segments reduce the impact of short-term acoustic variability and allow models to better capture speaker characteristics.
- Core assumption: ASR models benefit from longer, contextually coherent audio segments.
- Evidence anchors:
  - [section] "When comparing the two variants of TalkBank—Segments and Speaker Switch—our results indicate that all approaches perform better on Segments, with the difference being over three times."
  - [abstract] "Additionally, we analyze the impact of different speech disfluencies and find a correlation between WER and these disfluencies."
  - [corpus] Limited evidence; corpus does not provide direct support for segmentation impact.
- Break condition: If models are trained with mixed segment lengths, the performance difference may diminish.

## Foundational Learning

- Concept: Word Error Rate (WER)
  - Why needed here: WER is the primary metric for evaluating ASR performance in this study.
  - Quick check question: How is WER calculated, and what does a lower WER indicate about ASR performance?

- Concept: Speech disfluencies
  - Why needed here: Disfluencies are central to understanding why ASR models underperform on conversational data.
  - Quick check question: What are common examples of speech disfluencies, and how might they affect ASR transcription accuracy?

- Concept: Self-supervised learning in ASR
  - Why needed here: Many modern ASR models use self-supervised learning, which is relevant to understanding their training and performance.
  - Quick check question: How does self-supervised learning differ from supervised learning in the context of ASR, and what are its advantages?

## Architecture Onboarding

- Component map: ASR models (Whisper, wav2vec2, Canary) -> Preprocessing pipeline -> TalkBank dataset -> Evaluation metrics (WER)
- Critical path: Data preprocessing -> Model inference -> WER calculation -> Performance analysis
- Design tradeoffs: Controlled datasets offer clean training data but lack real-world complexity; conversational datasets are more realistic but introduce noise and variability.
- Failure signatures: High WER on conversational data, correlation between disfluencies and WER, poor performance on speaker switch variant.
- First 3 experiments:
  1. Evaluate baseline WER on LibriSpeech and Fleurs to establish performance benchmarks.
  2. Preprocess TalkBank data and compute WER on both Segments and Speaker Switch variants.
  3. Analyze correlation between conversational markers and WER to identify failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ASR models be effectively trained to handle conversational speech disfluencies, such as pauses, interruptions, and laughter, without compromising performance on controlled speech datasets?
- Basis in paper: [explicit] The paper highlights that current ASR models perform significantly worse on conversational datasets like TalkBank compared to controlled datasets like LibriSpeech, and suggests that models still underperform in conversational environments.
- Why unresolved: The study identifies the gap but does not propose or test specific training strategies to improve ASR performance on conversational speech.
- What evidence would resolve it: Experimental results comparing ASR models trained with and without conversational speech data, showing improvements in WER on conversational benchmarks without degrading performance on controlled datasets.

### Open Question 2
- Question: What are the specific acoustic and linguistic features of conversational speech that contribute most to ASR errors, and how can these be systematically addressed in model design?
- Basis in paper: [inferred] The paper observes a correlation between the presence of conversational markers (e.g., laughter, pauses, interruptions) and higher WER, but does not analyze which features are most detrimental or how to mitigate them.
- Why unresolved: The study identifies the correlation but lacks a detailed breakdown of the features or targeted solutions to improve ASR robustness.
- What evidence would resolve it: A detailed analysis of ASR errors linked to specific conversational features, along with experiments testing targeted model modifications (e.g., incorporating disfluency-aware architectures or training techniques).

### Open Question 3
- Question: How can ASR benchmarks be expanded to better represent diverse conversational settings, including different languages, accents, and demographic factors?
- Basis in paper: [explicit] The authors plan to expand the dataset to include gender and other demographic information and incorporate more diverse conversational settings in future work.
- Why unresolved: The current TalkBank dataset is limited in scope, and the paper does not explore how to systematically include broader conversational diversity.
- What evidence would resolve it: Development and evaluation of a multilingual, demographically diverse conversational ASR benchmark, demonstrating improved model performance across varied real-world scenarios.

## Limitations

- Study focuses on a single conversational dataset (TalkBank) and three ASR models, limiting generalizability to all conversational speech and modern ASR systems
- Performance degradation may be partially dataset-specific rather than universal limitation of current ASR approaches
- Correlation analysis shows only small positive correlations between conversational markers and WER, suggesting relationship may not be as strong as implied
- Lacks ablation studies to isolate specific impact of different conversational features on ASR performance

## Confidence

- **High Confidence**: The finding that existing ASR models show substantial performance degradation (up to 3x increase in WER) when tested on conversational data compared to controlled benchmarks.
- **Medium Confidence**: The correlation between conversational markers and higher WER.
- **Medium Confidence**: The conclusion that current benchmarks inadequately represent real-world conversational speech.

## Next Checks

1. **Dataset Diversity Test**: Evaluate the same ASR models on additional conversational datasets (e.g., Fisher, Switchboard) to determine if the performance degradation is consistent across different conversational data sources.

2. **Feature Ablation Study**: Conduct controlled experiments isolating specific conversational features (disfluencies, speaker changes, background noise) to identify which factors contribute most significantly to ASR performance degradation.

3. **Fine-tuning Impact Analysis**: Fine-tune the evaluated ASR models on the TalkBank dataset and measure performance improvement, which would help distinguish between fundamental architectural limitations and training data mismatch issues.