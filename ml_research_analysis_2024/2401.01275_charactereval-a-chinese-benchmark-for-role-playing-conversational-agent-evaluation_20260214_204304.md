---
ver: rpa2
title: 'CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation'
arxiv_id: '2401.01275'
source_url: https://arxiv.org/abs/2401.01275
tags:
- role-playing
- arxiv
- character
- gpt-4
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CharacterEval introduces a Chinese benchmark for evaluating Role-Playing
  Conversational Agents (RPCAs). It includes a high-quality dataset of 1,785 multi-turn
  dialogues from Chinese novels and scripts, covering 77 characters.
---

# CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation

## Quick Facts
- arXiv ID: 2401.01275
- Source URL: https://arxiv.org/abs/2401.01275
- Reference count: 13
- Chinese benchmark for evaluating Role-Playing Conversational Agents (RPCAs) with 1,785 multi-turn dialogues from Chinese novels and scripts

## Executive Summary
CharacterEval introduces a comprehensive Chinese benchmark for evaluating Role-Playing Conversational Agents (RPCAs). The benchmark includes a high-quality dataset of 1,785 multi-turn dialogues from Chinese novels and scripts, covering 77 characters. The evaluation framework employs thirteen metrics across four dimensions: conversational ability, character consistency, role-playing attractiveness, and personality back-testing. To facilitate evaluation, a role-playing reward model (CharacterRM) was developed, which achieves higher correlation with human judgments than GPT-4. Experiments demonstrate that Chinese LLMs, such as BC-Character-Turbo, outperform GPT-4 in Chinese role-playing conversations, particularly in character consistency and role-playing attractiveness.

## Method Summary
The CharacterEval benchmark was developed through a multi-stage process. First, researchers collected 1,785 multi-turn dialogues from Chinese novels and scripts, ensuring diversity across 77 different characters. These dialogues were then annotated using thirteen metrics spanning four evaluation dimensions. To enable automated evaluation, a role-playing reward model (CharacterRM) was trained on human judgments to predict the quality of RPCA responses. The evaluation framework compares Chinese LLMs against GPT-4 across the thirteen metrics, with particular focus on Chinese language performance. The benchmark also includes personality back-testing capabilities to verify character consistency over extended conversations.

## Key Results
- Chinese LLMs (e.g., BC-Character-Turbo) outperform GPT-4 in Chinese role-playing conversations
- CharacterRM achieves higher correlation with human judgments than GPT-4
- RPCAs show strongest performance in character consistency and role-playing attractiveness metrics

## Why This Works (Mechanism)
CharacterEval works by providing a structured framework for evaluating RPCAs that captures both conversational quality and character-specific attributes. The thirteen metrics across four dimensions create a comprehensive evaluation landscape that goes beyond simple language understanding. The role-playing reward model (CharacterRM) learns from human preferences to predict RPCA quality, enabling scalable evaluation without constant human annotation. The use of Chinese literary sources provides culturally and linguistically appropriate character examples that reflect authentic Chinese communication patterns and personality archetypes.

## Foundational Learning
- Multi-turn dialogue evaluation: Why needed - to assess conversational coherence and engagement over time; Quick check - ensure metrics capture turn-taking patterns and topic continuity
- Character consistency metrics: Why needed - to verify agents maintain persona throughout conversation; Quick check - measure personality trait adherence across multiple dialogue turns
- Role-playing reward modeling: Why needed - to automate evaluation while preserving human judgment quality; Quick check - compare model predictions against human annotator agreements
- Cross-LLM benchmarking: Why needed - to establish relative performance across different model architectures; Quick check - ensure fair comparison conditions and evaluation protocols
- Chinese linguistic features: Why needed - to capture language-specific nuances in role-playing; Quick check - verify metrics account for Chinese-specific conversational patterns
- Fictional source adaptation: Why needed - to bridge gap between fictional dialogue and real conversation; Quick check - assess how well fictional patterns generalize to natural dialogue

## Architecture Onboarding

Component map: Chinese novels/scripts -> Dialogue extraction -> Character profiling -> Metric annotation -> CharacterRM training -> RPCA evaluation

Critical path: The evaluation pipeline flows from source material through automated processing to final RPCA assessment. The most critical components are the dialogue extraction (ensuring quality input data) and the CharacterRM (enabling scalable evaluation).

Design tradeoffs: The choice of fictional sources provides rich character examples but may not reflect real conversational patterns. The thirteen metrics provide comprehensive coverage but increase evaluation complexity. CharacterRM enables automation but requires careful training to match human judgment quality.

Failure signatures: Poor character consistency scores may indicate inadequate persona modeling in RPCAs. Low role-playing attractiveness scores suggest the agent fails to engage users emotionally. High conversational ability but low character consistency indicates technical proficiency without persona adherence.

Three first experiments:
1. Evaluate a baseline RPCA on a subset of the benchmark to establish performance baselines
2. Test CharacterRM's correlation with human judgments on a validation set
3. Compare Chinese LLMs against GPT-4 on the same character dialogues to establish relative performance

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Dataset relies exclusively on fictional sources, which may not capture authentic conversational patterns
- Character consistency metric may be limited by subjective interpretation of character traits
- Comparison between Chinese LLMs and GPT-4 is based on Chinese-specific evaluation, limiting cross-linguistic generalization

## Confidence
- High confidence: Dataset construction methodology and collection of 1,785 multi-turn dialogues from Chinese novels and scripts is well-documented and verifiable
- Medium confidence: Assertion that Chinese LLMs outperform GPT-4 in Chinese role-playing conversations is supported by the evaluation framework
- Medium confidence: Claim that CharacterRM achieves higher correlation with human judgments than GPT-4 is based on reported experiments

## Next Checks
1. Replicate the benchmark evaluation using an independent Chinese conversational dataset from different sources (e.g., social media or customer service interactions) to test generalizability
2. Conduct a cross-linguistic evaluation where GPT-4's performance in Chinese is compared against its performance in other languages to isolate whether results are specifically about Chinese language capabilities or more general RPCA performance
3. Implement a blind human evaluation study where annotators are unaware of which model generated responses to reduce potential bias in the correlation measurements