---
ver: rpa2
title: Correlations Are Ruining Your Gradient Descent
arxiv_id: '2407.10780'
source_url: https://arxiv.org/abs/2407.10780
tags:
- gradient
- decorrelation
- neural
- descent
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reveals that correlations in input data at every layer
  of a deep neural network cause parameters to exist in a non-orthonormal basis, skewing
  gradient descent updates. By introducing a decorrelation mechanism at each layer,
  the study demonstrates that gradient descent can be better aligned with natural
  gradients, improving training efficiency.
---

# Correlations Are Ruining Your Gradient Descent

## Quick Facts
- **arXiv ID**: 2407.10780
- **Source URL**: https://arxiv.org/abs/2407.10780
- **Reference count**: 20
- **Primary result**: Decorrelation at each layer improves training efficiency for backpropagation and approximate gradient methods

## Executive Summary
This paper reveals that correlations in input data at every layer of a deep neural network cause parameters to exist in a non-orthonormal basis, skewing gradient descent updates. By introducing a decorrelation mechanism at each layer, the study demonstrates that gradient descent can be better aligned with natural gradients, improving training efficiency. A novel decorrelation method is proposed, formulated as either a linear transformation or a recurrent dynamical system, with benefits including stability across different scales and preservation of activation norms. Experiments on CIFAR10, CIFAR100, and Tiny ImageNet show that decorrelation significantly accelerates training convergence for backpropagation, feedback alignment, and node perturbation algorithms, with feedback alignment achieving accuracy surpassing standard backpropagation in dense networks. The approach enables distributed computing and neuromorphic hardware applications while offering insights into biological neural processing.

## Method Summary
The paper proposes a decorrelation mechanism applied at each layer of a neural network to improve gradient descent training. The method involves introducing a square decorrelation matrix at each layer, updated based on mini-batch statistics and normalized to preserve activation norms. The decorrelation can be implemented as either a linear transformation (adding O(n²) parameters per layer) or as recurrent dynamics requiring numerical integration. Training uses the Adam optimizer with tuned learning rates for both forward weights and decorrelation matrices. The approach is tested on CIFAR10, CIFAR100, and Tiny ImageNet datasets using both fully-connected and convolutional architectures.

## Key Results
- Decorrelation significantly accelerates training convergence for backpropagation, feedback alignment, and node perturbation algorithms
- Feedback alignment with decorrelation achieves accuracy surpassing standard backpropagation in dense networks
- The method provides stability across different scales and preserves activation norms while improving training efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data correlations at each layer cause parameters to enter a non-orthonormal basis, skewing gradient descent updates.
- Mechanism: When input data to a layer has correlations (covariance structure), the parameters effectively live in a skewed coordinate system. Gradient descent, which assumes Euclidean geometry, steps in the wrong direction because the parameter space is non-orthonormal.
- Core assumption: The correlation structure of inputs at every layer is maintained during training and affects parameter updates.
- Evidence anchors:
  - [abstract] "correlations in the data at any linear transformation, including node responses at every layer of a neural network, cause a non-orthonormal relationship between the model's parameters"
  - [section 2.3] "The presence of the inverse of the data correlation term is not only surprising but also an underappreciated aspect of natural gradients"
  - [corpus] Weak - no direct citations found supporting this specific claim
- Break condition: If decorrelation mechanisms fail to maintain orthogonality or if input distributions change too rapidly for the decorrelation matrices to adapt.

### Mechanism 2
- Claim: Decorrelating inputs at each layer aligns gradient descent with natural gradients, improving training efficiency.
- Mechanism: By removing correlations from layer inputs, the parameter space becomes orthonormal. This makes regular gradient descent updates more aligned with natural gradient updates, which account for the geometry of the loss landscape.
- Core assumption: Decorrelation can be achieved and maintained throughout training without destroying useful signal structure.
- Evidence anchors:
  - [abstract] "By introducing a decorrelation mechanism at each layer... gradient descent can be better aligned with natural gradients, improving training efficiency"
  - [section 3.3] "decorrelation of input states at every layer of a network alleviates one half of the difference between the natural gradient update and regular gradient descent"
  - [corpus] Weak - limited direct evidence for decorrelation improving gradient alignment
- Break condition: If decorrelation removes too much signal correlation that is actually beneficial for learning, or if the decorrelation process introduces numerical instability.

### Mechanism 3
- Claim: Decorrelation enables approximate gradient methods (like feedback alignment) to achieve performance competitive with or exceeding backpropagation.
- Mechanism: Approximate methods struggle because they rely on compromised gradient signals. Decorrelation removes data correlation issues, allowing these methods to work effectively despite their approximations in gradient computation.
- Core assumption: The data correlation component is a major factor limiting approximate gradient methods, not just the gradient approximation itself.
- Evidence anchors:
  - [abstract] "Experiments on CIFAR10, CIFAR100, and Tiny ImageNet show that decorrelation significantly accelerates training convergence for backpropagation, feedback alignment, and node perturbation algorithms, with feedback alignment achieving accuracy surpassing standard backpropagation in a dense network"
  - [section 4] "the accuracy achieved by feedback alignment (FA) is increased far above what was previously achievable, even surpassing the accuracy of backpropagation in a dense network"
  - [corpus] Weak - no corpus citations directly supporting this specific finding
- Break condition: If the improvement is primarily due to regularization effects rather than decorrelation itself, or if the benefits don't transfer to other architectures.

## Foundational Learning

- Concept: Natural gradient descent
  - Why needed here: The paper positions decorrelation as addressing half of what natural gradients fix (data correlations), while existing natural gradient methods address gradient correlations.
  - Quick check question: What is the key difference between natural gradient descent and standard gradient descent in terms of the geometry they assume?

- Concept: Matrix correlation/covariance
  - Why needed here: Understanding how correlation matrices affect parameter spaces and why their inversion is computationally expensive.
  - Quick check question: How does a correlation matrix transform a standard basis into a non-orthonormal basis?

- Concept: Backpropagation vs approximate gradient methods
  - Why needed here: The paper compares standard backpropagation with feedback alignment and node perturbation, showing how decorrelation helps all methods.
  - Quick check question: What is the fundamental difference between backpropagation and feedback alignment in terms of how they compute weight updates?

## Architecture Onboarding

- Component map: Forward weights (Wi) -> Decorrelation matrix (Mi) -> Nonlinearity -> Backward pass
- Critical path: Forward pass → Compute decorrelated activations → Apply nonlinearity → Backward pass → Update decorrelation matrices → Update forward weights
- Design tradeoffs: Linear transformation decorrelation adds O(n²) parameters per layer (computationally expensive for wide layers) vs recurrent implementation requiring numerical integration but potentially more biologically plausible.
- Failure signatures: Training instability, exploding/vanishing gradients, decorrelation matrices becoming singular, performance worse than baseline without decorrelation.
- First 3 experiments:
  1. Implement decorrelation on a simple 2-layer network on MNIST and compare convergence speed to baseline
  2. Test different decorrelation learning rates (ηM) to find optimal balance between decorrelation strength and stability
  3. Compare linear vs recurrent decorrelation implementations on a small network to understand computational tradeoffs

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided.

## Limitations

- The evidence anchors for core mechanisms are weak, with no direct corpus citations supporting the specific claims about how data correlations affect parameter spaces or how decorrelation improves gradient alignment.
- Experimental results are not independently verified and lack detailed implementation specifications, making reproduction challenging.
- It's unclear whether the benefits come from decorrelation itself versus regularization effects, and the generalizability to architectures beyond dense networks is uncertain.

## Confidence

- High confidence in the mathematical formulation of the decorrelation mechanism and its theoretical grounding in natural gradient principles.
- Medium confidence in the experimental results showing improved training efficiency, as the findings are specific to the tested datasets and architectures.
- Low confidence in the claims about biological plausibility and distributed computing applications, as these are not directly tested in the experiments.

## Next Checks

1. Independently reproduce the core results on CIFAR10 with both standard backpropagation and feedback alignment, comparing accuracy and convergence speed with and without decorrelation.
2. Test the decorrelation mechanism on additional architectures (e.g., ResNet, Vision Transformer) to assess generalizability beyond dense networks.
3. Conduct ablation studies to isolate the effects of decorrelation from potential regularization benefits, such as testing with and without normalization of the decorrelation matrices.