---
ver: rpa2
title: Optimistic Thompson Sampling for No-Regret Learning in Unknown Games
arxiv_id: '2402.09456'
source_url: https://arxiv.org/abs/2402.09456
tags:
- regret
- reward
- algorithms
- game
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Thompson Sampling (TS)-based algorithms for
  learning in unknown games with bandit feedback. The core method, Optimistic Thompson
  Sampling (OTS), leverages information on opponents' actions and reward structures
  to achieve over tenfold improvements in experimental budgets compared to conventional
  approaches.
---

# Optimistic Thompson Sampling for No-Regret Learning in Unknown Games

## Quick Facts
- arXiv ID: 2402.09456
- Source URL: https://arxiv.org/abs/2402.09456
- Reference count: 40
- Primary result: OTS-based algorithms achieve over tenfold improvements in experimental budgets and regret bounds that depend logarithmically on action space size

## Executive Summary
This paper introduces Optimistic Thompson Sampling (OTS) algorithms for learning in unknown games with bandit feedback. The key innovation is the Optimism-then-NoRegret (OTN) framework that combines optimistic imagined rewards with no-regret update rules. The proposed methods achieve significant improvements over conventional approaches, demonstrating over tenfold better sample efficiency and logarithmic dependence on action space size rather than polynomial, effectively addressing the curse of multi-player in bandit game settings.

## Method Summary
The paper proposes a framework where agents construct optimistic imagined reward sequences using estimation algorithms (GP regression or linear models), then apply no-regret update rules (Hedge or Regret Matching) to these sequences. The OTN framework separates the estimation problem from the no-regret learning problem, allowing the use of full-information adversarial bandit algorithms. The optimism mechanism ensures that the imagined rewards are sufficiently optimistic to prevent linear regret, while the no-regret update rules guarantee sublinear regret against the imagined rewards.

## Key Results
- OTS-based algorithms achieve over tenfold improvements in experimental budgets compared to conventional approaches
- Regret bounds depend logarithmically on action space size, significantly alleviating the curse of multi-player
- OTS-RM outperforms other algorithms in sample efficiency and convergence speed across various game settings
- OTS-based algorithms achieve sublinear regret in matrix games, linear games, and kernelized games

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimistic Thompson Sampling (OTS) with multiple posterior samples prevents linear regret in adversarial bandit games.
- Mechanism: When the agent consistently plays the wrong action, OTS generates multiple optimistic samples for each action. The maximum of these samples for the correct action grows faster than for the wrong action, eventually flipping the agent's strategy to the optimal choice.
- Core assumption: The noise distribution has sufficient anti-concentration properties, and the number of samples M grows appropriately with time t.
- Evidence anchors:
  - [abstract] "OTS can fix the divergence issue with the help of Gaussian anti-concentration behavior"
  - [section] "The efficacy of the OTS-type method hinges on the bounded nature of the imagined rewards" and detailed proof in Appendix E.3
  - [corpus] No direct evidence; requires theoretical analysis
- Break condition: If the noise distribution lacks anti-concentration (e.g., extremely heavy-tailed distributions), or if M doesn't grow fast enough with t, the optimism effect may fail to overcome the adversary's exploitation.

### Mechanism 2
- Claim: Exploiting opponent action information and reward structure eliminates the curse of multi-player.
- Mechanism: By incorporating opponent actions into the estimation of mean rewards and using kernelized models, the regret bound depends logarithmically on the action space size rather than polynomially.
- Core assumption: The reward function can be modeled as a sample from a Gaussian process with a known kernel structure.
- Evidence anchors:
  - [abstract] "our algorithms achieve regret bounds that depend logarithmically on the size of the action space, significantly alleviating the curse of multi-player"
  - [section] "when employing squared exponential kernels to model the reward dynamics, the regret for OTS-Hedge is refined to O(√log|A| + √log(|A|T) log(T)^(d+1) · √T)"
  - [corpus] Weak evidence; related works discuss structure exploitation but not specifically this logarithmic improvement
- Break condition: If the true reward function doesn't match the assumed kernel structure, or if the kernel parameters are poorly chosen, the logarithmic improvement may not materialize.

### Mechanism 3
- Claim: The Optimism-then-NoRegret (OTN) framework unifies various algorithmic approaches and achieves sublinear regret.
- Mechanism: The framework constructs an optimistic imagined reward sequence using estimation algorithms, then applies full-information adversarial bandit algorithms to this sequence. The optimism ensures that pessimism and estimation errors are bounded, leading to sublinear regret.
- Core assumption: The imagined reward sequence can be constructed to be sufficiently optimistic and bounded, and the full-information adversarial bandit algorithm has sublinear regret on bounded sequences.
- Evidence anchors:
  - [abstract] "the Optimism-then-NoRegret (OTN) framework, a pioneering methodology that seamlessly incorporates our advancements with established algorithms"
  - [section] Proposition 3.1 provides the general regret decomposition, and Theorem 4.2 gives the regret bound
  - [corpus] Weak evidence; related works discuss optimism in bandit algorithms but not this specific unified framework
- Break condition: If the optimism condition (Definition 4.1) cannot be satisfied, or if the full-information adversarial bandit algorithm doesn't have sublinear regret on the imagined sequence, the framework fails.

## Foundational Learning

- Concept: Gaussian Process Regression and Kernel Methods
  - Why needed here: The algorithms model the reward function as a sample from a Gaussian process with a known kernel, requiring understanding of GP regression, posterior updates, and kernel properties.
  - Quick check question: Can you derive the posterior mean and variance for a GP given observations at certain points?

- Concept: Multi-armed Bandit Algorithms and Regret Analysis
  - Why needed here: The algorithms are based on Thompson Sampling and UCB methods for bandits, requiring understanding of exploration-exploitation tradeoff, regret definitions, and analysis techniques.
  - Quick check question: What is the difference between frequentist and Bayesian regret analysis in bandit problems?

- Concept: Game Theory and Nash Equilibria
  - Why needed here: The setting is a repeated game where players aim to maximize their rewards, requiring understanding of Nash equilibria, regret minimization in games, and best response strategies.
  - Quick check question: How does the concept of regret in online learning relate to the concept of Nash equilibrium in game theory?

## Architecture Onboarding

- Component map:
  - Estimation Algorithm -> Optimism Mechanism -> No-Regret Algorithm -> Regret Analysis

- Critical path:
  1. Observe opponent's action and noisy reward
  2. Update posterior distribution of reward function using GP regression or linear model
  3. Construct optimistic imagined reward sequence using multiple samples (OTS) or confidence bounds (UCB)
  4. Apply no-regret update rule (Hedge or Regret Matching) to imagined rewards
  5. Sample next action from updated distribution

- Design tradeoffs:
  - OTS vs UCB: OTS uses stochastic bounds and multiple samples, potentially more sample-efficient but requires careful choice of M; UCB uses deterministic bounds, simpler but may be less adaptive
  - Hedge vs Regret Matching: Hedge updates multiplicatively and has O(√T log|A|) regret; Regret Matching updates additively and has O(√T|A|) regret but may be more suitable for game settings
  - Kernel choice: Different kernels (linear, squared exponential, Matern) have different properties and maximum information gain bounds, affecting regret bounds

- Failure signatures:
  - Linear regret: Indicates the optimism condition is not satisfied or the adversary is exploiting the agent's strategy
  - High variance in regret: Suggests poor choice of kernel parameters or insufficient exploration
  - Slow convergence: May indicate overly conservative optimism or poor choice of no-regret algorithm

- First 3 experiments:
  1. Matrix game with known payoff matrix: Compare OTS-Hedge and UCB-Hedge against IWE-Hedge to verify the advantage of exploiting structure
  2. Linear game with known feature mapping: Compare OTS-RM and UCB-RM against IWE-RM to verify the advantage of optimism in RM-based algorithms
  3. Kernelized game with known kernel: Vary the kernel parameters (lengthscale, smoothness) and observe the effect on regret to understand the importance of kernel choice

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Optimistic Thompson Sampling (OTS) algorithms compare in multi-player games with more than two players?
- Basis in paper: [explicit] The paper mentions that the results can be extended to multiplayer games by treating all other players as an abstract player, but does not provide experimental results for more than two players.
- Why unresolved: The paper focuses on two-player games and provides limited discussion on extending the results to multi-player scenarios.
- What evidence would resolve it: Experimental results comparing OTS algorithms in games with three or more players, showing convergence rates and regret bounds in such settings.

### Open Question 2
- Question: What is the impact of using different kernel functions on the performance of OTS algorithms in kernelized games?
- Basis in paper: [explicit] The paper mentions using squared exponential and Matern kernels in the context of information gain bounds, but does not explore the impact of different kernels on algorithm performance.
- Why unresolved: The paper provides theoretical bounds but does not empirically investigate the effects of various kernel choices on OTS performance.
- What evidence would resolve it: Comparative experiments using different kernel functions (e.g., linear, polynomial, Gaussian) in kernelized games, measuring regret and convergence rates.

### Open Question 3
- Question: How does the Optimistic Thompson Sampling algorithm handle non-stationary environments where the opponent's strategy changes abruptly?
- Basis in paper: [explicit] The paper discusses a non-stationary opponent in the context of matrix games but does not provide a detailed analysis of OTS's robustness to sudden strategy changes.
- Why unresolved: The paper provides limited discussion on the algorithm's adaptability to abrupt changes in opponent behavior.
- What evidence would resolve it: Experiments simulating scenarios with sudden changes in opponent strategy, measuring the algorithm's ability to adapt and maintain low regret.

## Limitations

- Theoretical analysis relies on idealized assumptions about Gaussian processes and anti-concentration properties that may not hold in practical scenarios
- Empirical evaluation is limited to specific game types and synthetic opponents, raising questions about generalization to more complex strategic environments
- Limited discussion on parameter sensitivity and hyperparameter tuning procedures for the algorithms

## Confidence

- High confidence: The core OTN framework construction and regret decomposition methodology
- Medium confidence: The logarithmic improvement claims for kernelized games, pending verification of kernel parameter sensitivity
- Low confidence: The robustness claims against non-stationary opponents, given limited empirical validation

## Next Checks

1. **Parameter Sensitivity Analysis**: Systematically vary kernel hyperparameters (lengthscale, smoothness) and the number of optimistic samples M to quantify their impact on regret performance across different game types.

2. **Non-Stationary Opponent Testing**: Implement a comprehensive suite of opponent strategy evolution patterns (abrupt shifts, gradual drift, periodic changes) to stress-test the algorithms' adaptability claims beyond the reported self-play and best-response scenarios.

3. **Scaling Experiments**: Evaluate algorithm performance on games with significantly larger action spaces (100+ actions per player) to verify the claimed logarithmic dependence on action space size holds under practical computational constraints.