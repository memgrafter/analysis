---
ver: rpa2
title: The Limited Impact of Medical Adaptation of Large Language and Vision-Language
  Models
arxiv_id: '2411.08870'
source_url: https://arxiv.org/abs/2411.08870
tags:
- medical
- prompt
- llms
- datasets
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether domain-adaptive pretraining (DAPT)
  on biomedical corpora improves the performance of large language and vision-language
  models for medical applications. The authors conduct a rigorous head-to-head comparison
  between ten "medical" LLMs and two VLMs against their corresponding base models
  across various medical question-answering tasks, including clinical note-based QA
  and visual medical QA.
---

# The Limited Impact of Medical Adaptation of Large Language and Vision-Language Models

## Quick Facts
- **arXiv ID**: 2411.08870
- **Source URL**: https://arxiv.org/abs/2411.08870
- **Reference count**: 24
- **Primary result**: Medical LLMs and VLMs generally fail to consistently improve over their base models across various medical question-answering tasks

## Executive Summary
This study rigorously evaluates whether domain-adaptive pretraining (DAPT) on biomedical corpora improves the performance of large language and vision-language models for medical applications. Through head-to-head comparisons between ten "medical" LLMs and two VLMs against their corresponding base models, the authors find that medical models generally fail to consistently outperform their base models in zero-/few-shot prompting and supervised fine-tuning regimes. The research suggests that state-of-the-art general-domain models may already exhibit strong medical knowledge and reasoning capabilities, highlighting the need for rigorous experimental design in future studies of medical AI systems.

## Method Summary
The authors conducted a comprehensive evaluation comparing ten medical LLMs and two medical VLMs against their base models across 22 textual and 8 visual medical question-answering datasets. They implemented model-specific prompt selection and optimization, using bootstrap resampling to calculate 95% confidence intervals for relative accuracy. The evaluation included both zero-/few-shot prompting with model-specific prompt formats and few-shot examples, as well as supervised fine-tuning using LoRA methods. Statistical significance testing was employed to determine whether performance differences were meaningful rather than due to chance.

## Key Results
- Medical LLMs outperformed their base models in only 26.7% of cases on clinical-note-based QA tasks in the 3-shot setting
- Performance gains from medical DAPT were not consistent across different tasks or model pairs
- Recent general-domain models showed strong medical knowledge capabilities even without medical adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Medical domain-adaptive pretraining (DAPT) improves performance on medical QA tasks
- Mechanism: By continuing pretraining on biomedical corpora, the model gains domain-specific knowledge and representations that improve performance on downstream medical tasks
- Core assumption: The biomedical corpora contain relevant, high-quality medical knowledge not present in the general-domain pretraining data
- Evidence anchors: Weak - corpus analysis only shows 25 related papers with no citation data, suggesting limited direct evidence for this mechanism

### Mechanism 2
- Claim: Medical LLMs show limited improvement over base models after proper experimental controls
- Mechanism: When accounting for prompt sensitivity and statistical uncertainty, medical models do not consistently outperform their base models on medical QA tasks
- Core assumption: Previous studies overestimated performance improvements by not optimizing prompts separately for each model and not accounting for statistical uncertainty
- Evidence anchors: [abstract]: "Our findings reveal that medical LLMs and VLMs generally fail to consistently improve over their base models in zero-/few-shot prompting and supervised fine-tuning regimes."

### Mechanism 3
- Claim: Recent general-domain models already possess strong medical knowledge capabilities
- Mechanism: State-of-the-art general-domain models may have been trained on sufficient diverse data that includes medical knowledge, reducing the need for additional medical adaptation
- Core assumption: The general-domain pretraining corpora already include substantial medical content from sources like PubMed
- Evidence anchors: [abstract]: "Our findings suggest that state-of-the-art general-domain models may already exhibit strong medical knowledge and reasoning capabilities."

## Foundational Learning

- **Concept: Statistical significance testing**
  - Why needed here: To determine whether observed performance differences between models are due to chance or represent real improvements
  - Quick check question: What statistical test is used to determine if performance differences are significant, and what threshold is applied?

- **Concept: Prompt sensitivity in LLMs**
  - Why needed here: Different prompt formats and few-shot examples can significantly affect model performance, making fair comparisons essential
  - Quick check question: Why is it important to optimize prompt format and few-shot examples separately for each model being compared?

- **Concept: Domain adaptation in NLP**
  - Why needed here: Understanding how continuing pretraining on domain-specific data affects model performance on related tasks
  - Quick check question: What is the theoretical basis for expecting domain-adaptive pretraining to improve performance on domain-specific tasks?

## Architecture Onboarding

- **Component map**: Model pairs (Medical LLMs/VLMs → Base models) → Evaluation regimes (Zero/few-shot prompting, SFT) → Tasks (Medical QA, Clinical note QA, Visual medical QA) → Metrics (Exact-match accuracy, Relative accuracy, Confidence intervals)

- **Critical path**: For each model pair and task: 1) Select best prompt format and few-shot examples for each model independently 2) Generate predictions and calculate exact-match accuracy 3) Use bootstrap resampling to calculate 95% confidence intervals for relative accuracy 4) Determine statistical significance of performance differences

- **Design tradeoffs**: Computational cost vs. thoroughness (evaluating all model pairs is expensive but necessary); Prompt optimization complexity (adds complexity but ensures fair comparisons); Statistical testing vs. simplicity (provides more reliable conclusions but requires more computation)

- **Failure signatures**: Overly optimistic conclusions from using fixed prompts for all models; False positives from not accounting for statistical uncertainty; Model-specific performance degradation after medical DAPT

- **First 3 experiments**:
  1. Compare MediTron-70B vs. Llama-2-70B on MedQA using optimized prompts and statistical testing
  2. Evaluate LLaVA-Med-7B vs. LLaVA-v0-7B on VQA-RAD in zero-shot setting with constrained decoding
  3. Fine-tune BioMedGPT-LM-7B and Llama-2-7B-Chat on MedMCQA and compare performance with statistical testing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of medical DAPT vary significantly across different medical specialties or types of clinical data?
- Basis in paper: Explicit - The paper notes variability in results across different clinical note datasets and suggests syntax, length, and format differences may contribute to performance variations
- Why unresolved: The study does not systematically investigate how medical DAPT performance differs across medical specialties (e.g., radiology vs. pathology) or specific clinical data types
- What evidence would resolve it: Conducting targeted evaluations of medical DAPT models across a diverse range of medical specialties and clinical data types, controlling for other variables

### Open Question 2
- Question: How does the performance of medical DAPT models compare to general-domain models when dealing with noisy or unstructured clinical text?
- Basis in paper: Inferred - The paper mentions that real-world clinical notes often contain grammatical errors and jargon, but does not directly compare model performance on clean vs. noisy clinical text
- Why unresolved: The study focuses on standard medical QA tasks without specifically addressing the impact of text quality on model performance
- What evidence would resolve it: Evaluating both medical and general-domain models on clinical note datasets with varying levels of noise and structure, measuring performance differences

### Open Question 3
- Question: What is the long-term clinical utility of medical DAPT models compared to their general-domain counterparts in real-world medical applications?
- Basis in paper: Explicit - The paper acknowledges that strong performance on medical knowledge QA tasks may not necessarily imply high clinical utility, but does not investigate real-world clinical outcomes
- Why unresolved: The study is limited to benchmark evaluations and does not examine how these models perform in actual clinical settings or their impact on patient care
- What evidence would resolve it: Conducting longitudinal studies in clinical environments to assess the practical benefits and limitations of medical DAPT models in real-world medical decision-making and patient care

## Limitations

- Limited model coverage: The study tested only 12 model pairs, excluding some notable medical models like Med-PaLM and BioMedLM
- Task and dataset constraints: While 30 medical QA datasets were used, the performance landscape across all possible medical applications remains incompletely characterized
- Evaluation methodology sensitivity: Findings remain sensitive to evaluation choices including specific prompt formats, few-shot examples, and decoding strategies

## Confidence

- **High Confidence**: Medical LLMs and VLMs do not consistently outperform their base models when proper experimental controls are applied
- **Medium Confidence**: General-domain models may already possess strong medical knowledge capabilities
- **Low Confidence**: Specific claims about which medical adaptation strategies are ineffective or which model pairs show the most promise

## Next Checks

1. Expand model and task coverage: Test additional medical LLMs (including previously inaccessible models like Med-PaLM and BioMedLM) and evaluate performance on a broader range of medical applications beyond question-answering

2. Analyze pretraining corpus overlap: Conduct detailed analysis of the pretraining corpora used for both general-domain and medical models to determine the extent of medical content overlap

3. Investigate model-specific adaptation effects: Perform deeper analysis of cases where medical adaptation either succeeded or failed dramatically, examining whether specific model architectures, adaptation strategies, or task types show systematic patterns