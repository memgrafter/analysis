---
ver: rpa2
title: Simple and Scalable Strategies to Continually Pre-train Large Language Models
arxiv_id: '2403.08763'
source_url: https://arxiv.org/abs/2403.08763
tags:
- pile
- learning
- replay
- pre-training
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how to efficiently update large language
  models (LLMs) when new data becomes available, without the cost of retraining from
  scratch. The authors propose a simple and scalable approach: combining learning
  rate re-warming, learning rate re-decaying, and replay of previous data.'
---

# Simple and Scalable Strategies to Continually Pre-train Large Language Models

## Quick Facts
- arXiv ID: 2403.08763
- Source URL: https://arxiv.org/abs/2403.08763
- Authors: Adam Ibrahim; Benjamin Thérien; Kshitij Gupta; Mats L. Richter; Quentin Anthony; Timothée Lesort; Eugene Belilovsky; Irina Rish
- Reference count: 40
- One-line primary result: Continual pre-training of LLMs using LR re-warming, LR re-decaying, and replay matches retraining from scratch performance

## Executive Summary
This paper investigates efficient strategies for updating large language models when new data becomes available, without the computational expense of retraining from scratch. The authors propose a simple yet effective approach combining learning rate re-warming, learning rate re-decaying, and replay of previous data during continual pre-training. They demonstrate that this combination enables LLMs to match the performance of models retrained on all available data across various benchmarks, for both weak and strong distribution shifts at the 405M parameter scale, and also for a 10B parameter model in the weak shift case.

## Method Summary
The method involves pre-training a transformer-based LLM on an initial dataset using a linear warmup and cosine decay schedule. When new data becomes available, training continues using a combination of learning rate re-warming (from low post-pretraining value to higher value), learning rate re-decaying (after adaptation to new data), and replay of previous data. The approach is evaluated on two distribution shift scenarios: a weak shift (English→English) using Pile and SlimPajama datasets, and a strong shift (English→German) using Pile and German Common Crawl. The continually pre-trained models are evaluated on several language model benchmarks and compared against baselines including retraining from scratch and naive continuation of training.

## Key Results
- Continual pre-training with LR re-warming, LR re-decaying, and replay matches performance of fully retraining from scratch
- All baselines and continually pre-trained models improve with increased parameter count
- Model scale has limited negative influence on forgetting-reduction from replay
- Infinite learning rate schedules prevent forgetting by avoiding re-warming between tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Re-warming and re-decaying the learning rate improves adaptation to new data without catastrophic forgetting when combined with replay
- Mechanism: Re-warming the learning rate from a low post-pretraining value to a higher value allows the model to escape flat minima and adapt to new data more effectively. Re-decaying the learning rate after adaptation stabilizes learning. Replay of previous data prevents forgetting by periodically refreshing older knowledge.
- Core assumption: The model retains sufficient representational capacity to learn new data without overwriting old knowledge, and replay samples are representative of the original distribution
- Evidence anchors: [abstract] "a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data"

### Mechanism 2
- Claim: Increasing model size reduces catastrophic forgetting during continual pre-training
- Mechanism: Larger models have more representational capacity, allowing them to learn new tasks without significantly disrupting existing knowledge. They also have flatter minima, making them more robust to distribution shifts.
- Core assumption: The increase in model size is accompanied by an increase in the size of the pre-training dataset, as suggested by scaling laws
- Evidence anchors: [section 6.4] "we observe that all baselines and continually pre-trained models consistently improve in perplexity on both datasets from increasing parameter count"

### Mechanism 3
- Claim: Infinite learning rate schedules prevent forgetting by avoiding the need to re-warm the learning rate between tasks
- Mechanism: By maintaining a constant learning rate after an initial warm-up and cooldown, the model avoids the instability and forgetting associated with re-warming. This allows for smoother transitions between tasks and better retention of previous knowledge.
- Core assumption: The constant learning rate is not so high as to cause instability, and the model can still converge to good minima with this schedule
- Evidence anchors: [section 7.4] "We observe that all schedules perform relatively similarly, however, the two infinite schedules have the advantage that we can start annealing at any time during the constant learning rate phase on each split"

## Foundational Learning

- Concept: Learning Rate Schedules
  - Why needed here: The choice of learning rate schedule significantly impacts the model's ability to adapt to new data and avoid catastrophic forgetting during continual pre-training
  - Quick check question: What is the difference between a cosine decay schedule and an infinite learning rate schedule, and when would you use each?

- Concept: Catastrophic Forgetting
  - Why needed here: Understanding the mechanisms of catastrophic forgetting is crucial for designing effective continual learning strategies and evaluating their performance
  - Quick check question: What are the main causes of catastrophic forgetting in neural networks, and how can they be mitigated?

- Concept: Replay Buffers
  - Why needed here: Replay buffers are a key component of many continual learning strategies, as they allow the model to revisit previous data and prevent forgetting
  - Quick check question: How does the size and composition of a replay buffer affect its ability to mitigate forgetting, and what are some strategies for selecting replay samples?

## Architecture Onboarding

- Component map: Transformer-based LLM with pre-LN -> AdamW optimizer -> Replay buffer for previous data -> Learning rate scheduler (cosine decay or infinite schedule)
- Critical path: 1. Pre-train model on initial dataset (D0) using cosine decay schedule 2. Continue training on new dataset (D1) using re-warmed and re-decayed learning rate schedule 3. Incorporate replay of D0 data during training on D1
- Design tradeoffs:
  - Re-warming vs. infinite schedule: Re-warming may cause initial forgetting but allows for more aggressive adaptation, while infinite schedules prevent forgetting but may hinder adaptation
  - Replay percentage: Higher replay percentages reduce forgetting but may slow adaptation to new data
  - Model size: Larger models forget less but are more computationally expensive
- Failure signatures: Increasing validation loss on D0 during training on D1 (forgetting), Stagnant or increasing validation loss on D1 (poor adaptation), Oscillating validation loss (instability)
- First 3 experiments:
  1. Train model on D0, then continue on D1 with re-warmed learning rate but no replay. Measure forgetting on D0.
  2. Train model on D0, then continue on D1 with re-warmed learning rate and 5% replay. Measure forgetting on D0 and adaptation on D1.
  3. Train model on D0, then continue on D1 with infinite learning rate schedule and 5% replay. Compare forgetting and adaptation to experiment 2.

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of comprehensive analysis of hyperparameters involved in the proposed continual pre-training strategy
- Focus on only two specific types of distribution shifts (weak and strong) without exploring other scenarios
- No investigation of the impact on the model's ability to perform zero-shot or few-shot learning tasks

## Confidence
**High Confidence:**
- The combination of learning rate re-warming, learning rate re-decaying, and replay of previous data is effective in mitigating catastrophic forgetting during continual pre-training
- Increasing model size reduces catastrophic forgetting during continual pre-training

**Medium Confidence:**
- The proposed strategy enables continual pre-training to match the performance of models retrained on all available data
- Infinite learning rate schedules prevent forgetting by avoiding the need to re-warm the learning rate between tasks

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Conduct a thorough analysis of the sensitivity of the proposed continual pre-training strategy to the key hyperparameters (learning rate, replay percentage, re-warming and re-decaying schedules)

2. **Generalization to Other Distribution Shifts**: Evaluate the proposed strategy on a wider range of distribution shifts, including shifts in data distribution, task distribution, and domain

3. **Impact on Zero-Shot and Few-Shot Learning**: Investigate the impact of the proposed continual pre-training strategy on the model's ability to perform zero-shot and few-shot learning tasks