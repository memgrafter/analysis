---
ver: rpa2
title: Bridge the Modality and Capability Gaps in Vision-Language Model Selection
arxiv_id: '2403.13797'
source_url: https://arxiv.org/abs/2403.13797
tags:
- dataset
- target
- image
- datasets
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses two key challenges in Language-Only Vision-Language\
  \ Model (VLM) Selection: the Modality Gap (text embeddings poorly substitute for\
  \ image embeddings) and the Capability Gap (a model\u2019s average ranking does\
  \ not reflect its dataset-specific ranking). The authors propose SWAB (VLM Selection\
  \ With gAp Bridging), which uses optimal transport to transfer useful statistics\
  \ (gap vectors and performance rankings) from open-source datasets to the target\
  \ dataset."
---

# Bridge the Modality and Capability Gaps in Vision-Language Model Selection

## Quick Facts
- arXiv ID: 2403.13797
- Source URL: https://arxiv.org/abs/2403.13797
- Reference count: 40
- Proposes SWAB, a method that bridges modality and capability gaps in LOVM selection, achieving 14.8% improvement over SoTA

## Executive Summary
This paper addresses two fundamental challenges in Language-Only Vision-Language Model (VLM) Selection: the Modality Gap (text embeddings poorly substitute for image embeddings) and the Capability Gap (a model's average ranking does not reflect its dataset-specific ranking). The authors propose SWAB (VLM Selection With gAp Bridging), which uses optimal transport to transfer useful statistics from open-source datasets to the target dataset. By aligning text and image embeddings and leveraging semantically relevant class rankings, SWAB bridges both gaps. Evaluated on 43 VLMs and 23 datasets, SWAB significantly outperforms prior methods, achieving an R5 + τ score of 0.822, a 14.8% improvement over the previous state-of-the-art (ModelGPT).

## Method Summary
SWAB addresses LOVM selection by bridging the modality gap through optimal transport-based transfer of class-specific gap vectors from open-source datasets, and the capability gap through transfer of VLM performance rankings based on semantic relevance. The method constructs a transport matrix using textual similarity between class names, estimates target dataset gap vectors and rankings by transferring statistics from open-source datasets, and ensembles predictions from both bridging approaches using weighted Borda count. The approach requires only text data from the target dataset and access to a VLM zoo, making it practical for real-world deployment.

## Key Results
- SWAB achieves an R5 + τ score of 0.822, representing a 14.8% improvement over the previous state-of-the-art (ModelGPT at 0.716)
- Outperforms all baseline methods across 23 diverse datasets and 43 VLMs
- Successfully bridges both the modality gap and capability gap, with each bridging method contributing to the final performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal transport bridges the modality gap by aligning text and image features within the VLM embedding space
- Mechanism: SWAB uses optimal transport to construct a transport matrix γ* that captures semantic relevance between classes in open-source and target datasets. This matrix is then used to transfer class-specific modality gap vectors from open-source datasets to estimate the target dataset's gap vectors. These estimated gap vectors are added to text features to align them with corresponding image features, effectively bridging the modality gap
- Core assumption: Classes with high textual semantic similarity have similar modality gap vectors, making it valid to transfer gap statistics between them
- Evidence anchors:
  - [abstract]: "SWAB first adopts optimal transport to capture the relevance between open-source and target datasets with a transportation matrix"
  - [section 3.2]: "We estimate the target dataset's gap vectors based on the open-source datasets' gap vectors with γ*"

### Mechanism 2
- Claim: Optimal transport bridges the capability gap by transferring VLM performance rankings based on class semantic relevance
- Mechanism: SWAB calculates VLM performance rankings on open-source dataset classes, then uses the transport matrix γ* to transfer these rankings to the target dataset. The transport matrix weights the contribution of each open-source class ranking based on its semantic similarity to target classes, producing a weighted sum that estimates the VLM's ranking on the target dataset
- Core assumption: A VLM's relative performance ranking across classes remains consistent when comparing semantically similar classes
- Evidence anchors:
  - [abstract]: "SWAB first adopts optimal transport to capture the relevance between open-source and target datasets with a transportation matrix"
  - [section 3.2]: "By re-organizing the ranking values...we perform a weighted sum of ranking values in rS m and assign larger weights to those classes related to the target dataset"

### Mechanism 3
- Claim: Ensembling predictions from both modality gap and capability gap bridging yields superior model selection performance
- Mechanism: SWAB generates two separate ranking predictions - one from bridging the modality gap (using modified text embeddings in ModelGPT) and one from bridging the capability gap (using class-specific ranking transfer). These predictions are combined using weighted Borda count (α = 0.5) to produce the final ranking
- Core assumption: Both gaps contribute independently to model selection accuracy, and combining their predictions captures complementary information
- Evidence anchors:
  - [section 3.3]: "We ensemble two predictions together and achieve a more accurate model ranking estimation. We utilize the weighted Borda Count to aggregate two rankings"
  - [section 4.1]: "Our final performance of R5 + τ (0.822) represents a significant improvement of 14.8% over the original SoTA method ModelGPT (0.716)"

## Foundational Learning

- Concept: Optimal transport theory and its application to cross-domain statistics transfer
  - Why needed here: The paper relies on optimal transport to construct a bridge matrix that enables transferring useful statistics (gap vectors and rankings) from open-source to target datasets based on semantic similarity
  - Quick check question: What is the primary objective function minimized when solving the optimal transport problem in this context?

- Concept: Modality gap in vision-language models and its impact on zero-shot classification
  - Why needed here: Understanding why text embeddings cannot directly substitute for image embeddings is crucial to grasping the problem SWAB addresses
  - Quick check question: How does the paper define the modality gap vector, and what does its magnitude indicate?

- Concept: Transfer learning and meta-learning concepts for model selection
  - Why needed here: SWAB's approach builds on learning-based model selection methods that predict model performance on target tasks using statistics from related tasks
  - Quick check question: How does the paper position SWAB in relation to learning-based LOVM methods like ModelGPT?

## Architecture Onboarding

- Component map:
  Optimal Transport Module -> Gap Vector Estimator -> Ranking Predictor -> Ensemble Module -> VLM Zoo Interface

- Critical path:
  1. Calculate textual similarity between class names of open-source and target datasets
  2. Solve optimal transport to obtain transport matrix γ*
  3. Estimate target dataset's modality gap vectors using γ* and open-source gap vectors
  4. Modify text embeddings by adding estimated gap vectors
  5. Generate two ranking predictions (modality gap bridging and capability gap bridging)
  6. Ensemble predictions to obtain final VLM ranking

- Design tradeoffs:
  - Computational cost vs. accuracy: Using partial optimal transport (mass = 0.9) reduces computation time while maintaining performance
  - Class filtering threshold: Setting λ = 0.5 filters out semantically dissimilar classes to reduce optimal transport problem size
  - Feature normalization: Applying z-score normalization to text and image features improves gap estimation accuracy

- Failure signatures:
  - High variance in gap vectors across semantically similar classes indicates poor transfer quality
  - Large discrepancy between the two ranking predictions suggests one bridging method is failing
  - Low Kendall's τ correlation indicates poor ranking similarity between predicted and ground truth rankings

- First 3 experiments:
  1. Verify optimal transport matrix construction by checking that semantically similar classes have higher transport weights
  2. Test gap vector estimation by comparing estimated vs. actual gap vectors on a dataset with available images
  3. Validate ranking prediction by computing Kendall's τ correlation between predictions and ground truth rankings on a validation split

## Open Questions the Paper Calls Out

### Open Question 1  
- Question: How does the choice of text encoder (e.g., MPNet) for calculating semantic similarity between class names affect the optimal transport matrix and subsequent performance in SWAB?  
- Basis in paper: [explicit] The paper states that SWAB uses a pre-trained text encoder (e.g., MPNet) to extract text features for class names and calculate cosine similarity for the optimal transport matrix.  
- Why unresolved: The paper does not explore the impact of different text encoders on the quality of the transport matrix and the final model selection performance.  
- What evidence would resolve it: Experimentally comparing SWAB's performance using different text encoders (e.g., BERT, RoBERTa) and analyzing the resulting transport matrices and selection accuracy.  

### Open Question 2  
- Question: Can SWAB be extended to handle datasets with hierarchical or fine-grained class structures, where class relationships are more complex than simple semantic similarity?  
- Basis in paper: [inferred] SWAB relies on textual similarity between class names to construct the transport matrix, which may not capture hierarchical relationships or fine-grained distinctions.  
- Why unresolved: The paper focuses on flat class structures and does not address the challenges posed by hierarchical or fine-grained class relationships.  
- What evidence would resolve it: Evaluating SWAB on datasets with hierarchical or fine-grained class structures and developing methods to incorporate hierarchical information into the transport matrix.  

### Open Question 3  
- Question: How sensitive is SWAB to the scaling factor used in the gap vector estimation (Equation 6), and are there more adaptive ways to determine this factor for different datasets or VLMs?  
- Basis in paper: [explicit] The paper mentions using a scaling factor of |CT| in Equation 6 to ensure the sum of the transport matrix columns equals 1, but does not explore the impact of different scaling factors.  
- Why unresolved: The choice of scaling factor may affect the effectiveness of bridging the modality gap, and a fixed scaling factor may not be optimal for all scenarios.  
- What evidence would resolve it: Experimenting with different scaling factors and developing adaptive methods to determine the optimal scaling factor based on dataset or VLM characteristics.  

### Open Question 4  
- Question: Can SWAB be adapted to handle multi-label classification tasks, where images can belong to multiple classes simultaneously?  
- Basis in paper: [inferred] SWAB is designed for single-label classification tasks and relies on cosine similarity for class prediction, which may not be suitable for multi-label scenarios.  
- Why unresolved: The paper does not address the challenges of multi-label classification and how SWAB can be extended to handle this case.  
- What evidence would resolve it: Modifying SWAB to handle multi-label classification by incorporating appropriate loss functions and evaluation metrics, and testing its performance on multi-label datasets.  

### Open Question 5  
- Question: How does the performance of SWAB scale with the number of VLMs in the zoo and the number of datasets in the open-source collection?  
- Basis in paper: [explicit] The paper evaluates SWAB on a VLM zoo of 43 models and 23 datasets, but does not explore the scalability of the method with larger numbers of models or datasets.  
- Why unresolved: The computational complexity of SWAB, particularly the optimal transport step, may increase with the size of the VLM zoo and open-source collection.  
- What evidence would resolve it: Analyzing the computational complexity of SWAB and conducting experiments with larger VLM zoos and open-source collections to assess its scalability.

## Limitations

- Effectiveness depends on availability of semantically relevant open-source datasets with similar visual concepts
- Assumes class-level statistics can be meaningfully transferred based on textual similarity alone, without considering visual semantic relationships
- Evaluation focuses primarily on image classification tasks, leaving questions about performance on other vision-language tasks

## Confidence

**High Confidence:**
- Experimental methodology and evaluation metrics (R5, τ, R5 + τ) are well-defined and consistently applied
- Core intuition that modality and capability gaps exist and impact LOVM selection is supported by both theory and empirical results
- Significant performance improvements over baseline methods (14.8% improvement over ModelGPT) are clearly demonstrated

**Medium Confidence:**
- Optimal transport mechanism for bridging both gaps is mathematically sound, but the assumption about class-level statistic transferability could benefit from further validation
- Choice of α = 0.5 for ensembling is reasonable but not rigorously justified
- Generalizability of results across different VLM architectures and dataset distributions

**Low Confidence:**
- Paper does not address potential biases introduced by the class-related text generation process using ChatGPT
- Scalability to very large-scale VLM zoos (>100 models) or extremely diverse datasets is not explored
- Computational overhead of optimal transport calculation for very large datasets is not quantified

## Next Checks

1. **Transferability Validation**: Conduct controlled experiments to measure how gap vector and ranking transfer accuracy varies with class semantic similarity scores. This would validate whether the optimal transport assumption holds across different similarity thresholds.

2. **Hyperparameter Sensitivity Analysis**: Perform ablation studies systematically varying λ (class filtering threshold), α (ensemble weight), and transport mass to identify optimal settings for different dataset categories (e.g., fine-grained vs. coarse-grained classification).

3. **Cross-Task Generalization**: Evaluate SWAB on non-classification vision-language tasks such as object detection or visual question answering to assess whether the modality and capability gap bridging approach generalizes beyond the image classification setting used in the current experiments.