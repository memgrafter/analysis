---
ver: rpa2
title: Learning When to Retrieve, What to Rewrite, and How to Respond in Conversational
  QA
arxiv_id: '2409.15515'
source_url: https://arxiv.org/abs/2409.15515
tags:
- retrieval
- conversation
- response
- history
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SELF-multi-RAG, an extension of the SELF-RAG
  framework for conversational question answering. The method enables large language
  models to adaptively decide when to retrieve passages based on conversational context,
  summarize conversation history for effective retrieval, and judge the relevance
  and quality of both retrieved passages and generated responses.
---

# Learning When to Retrieve, What to Rewrite, and How to Respond in Conversational QA

## Quick Facts
- arXiv ID: 2409.15515
- Source URL: https://arxiv.org/abs/2409.15515
- Authors: Nirmal Roy; Leonardo F. R. Ribeiro; Rexhina Blloshmi; Kevin Small
- Reference count: 25
- One-line primary result: SELF-multi-RAG improves conversational QA by enabling LLMs to adaptively decide when to retrieve passages, summarize conversation history, and evaluate response quality

## Executive Summary
This paper introduces SELF-multi-RAG, an extension of the SELF-RAG framework for conversational question answering. The method enables large language models to adaptively decide when to retrieve passages based on conversational context, summarize conversation history for effective retrieval, and judge the relevance and quality of both retrieved passages and generated responses. SELF-multi-RAG is trained to understand multi-turn dialogue, determine retrieval necessity, summarize context when retrieval is needed, and evaluate response quality. Experiments on three conversational QA datasets show that SELF-multi-RAG outperforms single-turn baselines, with approximately 13% improvement in human-annotated response quality and 13.5% improvement in retrieval effectiveness.

## Method Summary
SELF-multi-RAG is a three-component framework consisting of a Critic (decides retrieval necessity and judges relevance/groundedness/utility), a Generator (generates responses and conversation summaries), and a Retriever (off-the-shelf dense retrieval model). The system is trained using synthetic data from QReCC and UltraChat datasets with GPT-4 labels, employing special reflection tokens for self-reflection tasks. The key innovation is using conversational summaries instead of single rewritten questions for retrieval, enabling better handling of multi-turn dialogue context and more effective use of retrieved passages.

## Key Results
- SELF-multi-RAG achieves approximately 13% improvement in human-annotated response quality over single-turn baselines
- Retrieval effectiveness improves by 13.5% using conversational summaries compared to traditional query rewriting methods
- The system demonstrates better handling of conversational context and more effective use of retrieved passages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SELF-multi-RAG improves retrieval effectiveness by generating conversational summaries instead of single questions
- Mechanism: The model learns to distill multi-turn conversational context into a concise summary that captures all relevant information, which is then used as a query for retrieval
- Core assumption: A well-summarized conversational context contains more relevant retrieval signals than a single rewritten question while avoiding noise from irrelevant conversation history
- Evidence anchors: [abstract] and [section] statements about improved retrieval with summarized context

### Mechanism 2
- Claim: SELF-multi-RAG reduces unnecessary retrieval calls by better understanding conversational context
- Mechanism: The model learns to analyze both the current question and entire conversation history to determine if retrieval is needed, or if the answer can be found in previously retrieved passages or the conversation itself
- Core assumption: Many conversational questions can be answered using context already available in the conversation history
- Evidence anchors: [abstract] statements about better understanding of conversational context

### Mechanism 3
- Claim: Training on both single-turn and multi-turn data creates a more robust critic model
- Mechanism: By training on both STC and QU-MTC data, the critic model learns to handle both short questions and longer conversational contexts
- Core assumption: Exposure to both single-turn and multi-turn contexts during training improves generalization across different conversational scenarios
- Evidence anchors: [section] statements about critic accuracy with mixed training data

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: The paper builds on RAG framework and extends it to conversational settings
  - Quick check question: What are the main components of a RAG system and how do they work together?

- Concept: Conversational context understanding
  - Why needed here: The model needs to understand multi-turn dialogues to decide when to retrieve and how to summarize
  - Quick check question: What are the key challenges in understanding multi-turn conversational context compared to single-turn questions?

- Concept: Query rewriting and summarization
  - Why needed here: The model generates summaries of conversations for retrieval instead of rewriting them into single questions
  - Quick check question: What are the tradeoffs between query rewriting and summarization for conversational retrieval?

## Architecture Onboarding

- Component map: Conversation History → Critic → (if needed) Generator → Retriever → Generator → Critic → Select best response

- Critical path: Conversation History → Critic (decide retrieval) → (if needed) Generator (summarize) → Retriever → Generator (generate response) → Critic (evaluate) → Select best response

- Design tradeoffs:
  - Using summaries vs. rewritten queries for retrieval: Summaries capture more context but may be noisier
  - Including previous passages in context: Improves understanding but increases computational cost
  - Training on single-turn vs. multi-turn data: Multi-turn improves conversational understanding but may reduce performance on simple questions

- Failure signatures:
  - Poor retrieval effectiveness despite having summaries: May indicate summarization is missing key information
  - High retrieval call rate: May indicate poor understanding of conversational context
  - Low coherence scores: May indicate poor integration of retrieved information

- First 3 experiments:
  1. Test retrieval effectiveness with different conversation history representations (full conversation, T5QR rewrite, SELF-multi-RAG summary)
  2. Compare response quality with and without previous passages in the conversation context
  3. Evaluate critic performance on retrieval decisions with different conversation lengths

## Open Questions the Paper Calls Out
None

## Limitations
- Relies heavily on synthetic training data created through GPT-4 labeling, introducing potential biases
- Human evaluation sample size (50 conversations) is relatively small for drawing strong conclusions
- Doesn't explore computational costs or efficiency trade-offs of the multi-component system

## Confidence
- High confidence: Framework architecture and training methodology are clearly specified and follow from SELF-RAG approach
- Medium confidence: The 13% improvement claims, as they rely on relatively small evaluation samples and synthetic training data
- Medium confidence: Mechanism explanations for why summarization outperforms query rewriting, as the paper lacks ablation studies isolating specific contributions

## Next Checks
1. Conduct ablation studies comparing retrieval effectiveness using different conversation history representations (full conversation, T5QR rewrite, SELF-multi-RAG summary) on the same dataset
2. Test critic performance on retrieval decisions across varying conversation lengths (short 2-3 turn vs. long 10+ turn conversations)
3. Evaluate response quality when previous passages are included vs. excluded from the conversation context to measure practical benefits of utilizing retrieved passages