---
ver: rpa2
title: 'AFFAKT: A Hierarchical Optimal Transport based Method for Affective Facial
  Knowledge Transfer in Video Deception Detection'
arxiv_id: '2412.08965'
source_url: https://arxiv.org/abs/2412.08965
tags:
- deception
- dataset
- knowledge
- source
- facial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes AFFAKT, a method that transfers facial expression
  knowledge to improve video deception detection when labeled data is scarce. It uses
  a hierarchical optimal transport approach to map facial expression classes to deception
  samples, transferring appropriate knowledge.
---

# AFFAKT: A Hierarchical Optimal Transport based Method for Affective Facial Knowledge Transfer in Video Deception Detection

## Quick Facts
- arXiv ID: 2412.08965
- Source URL: https://arxiv.org/abs/2412.08965
- Authors: Zihan Ji; Xuetao Tian; Ye Liu
- Reference count: 16
- The paper proposes AFFAKT, a method that transfers facial expression knowledge to improve video deception detection when labeled data is scarce.

## Executive Summary
This paper introduces AFFAKT, a novel approach for video deception detection that leverages facial expression knowledge from large datasets to address data scarcity. The method employs a hierarchical optimal transport framework to map facial expression categories to deception samples, transferring relevant knowledge while maintaining invariant class-level relations through correlation prototypes. Experiments on two deception detection datasets demonstrate superior performance compared to existing methods, with interpretability studies revealing meaningful associations between deception and negative affections.

## Method Summary
AFFAKT addresses video deception detection with limited labeled data by transferring knowledge from large-scale facial expression recognition datasets. The method uses a hierarchical optimal transport approach (H-OTKT) to quantify semantic similarity between expression classes and deception samples, transferring appropriate knowledge. A sample-specific re-weighting strategy with correlation prototypes (SRKB) maintains invariant class-level relations and enhances robustness during inference. The framework extracts visual and audio features from videos, computes transport plans to align source and target domains, and uses an MLP classifier for final deception prediction.

## Key Results
- Superior performance compared to other methods on two deception detection datasets (RLT, DOLOS)
- Improved F1 score, accuracy (ACC), and area under the curve (AUC) metrics
- Interpretable results showing associations between deception and negative affections (disgust, fear, anger, sadness)
- Effective knowledge transfer from large facial expression datasets (DFEW, FERV39K, MAFW) to enhance deception detection accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: H-OTKT uses hierarchical optimal transport to align facial expression classes with deception samples based on class-level semantic similarity.
- Mechanism: Low-level OT computes sample-to-sample transport costs within each source class; high-level OT aggregates these into class-to-sample costs to determine how much knowledge to transfer from each expression class to each deception sample.
- Core assumption: Facial expression categories have meaningful semantic correspondence with deception behaviors that can be quantified via transport cost.
- Evidence anchors:
  - [abstract] "the optimal relation mapping between facial expression classes and deception samples is firstly quantified using proposed H-OTKT module"
  - [section] "high-level OT learn the optimal correlation between classes of VFER dataset and samples of deception dataset with a given cost matrix, where the cost matrix depends on the total low-level OT distance between each target deception sample and all samples from each class of VFER dataset"
- Break condition: If the semantic relationship between expression categories and deception is not meaningful or too weak, the transport plan T will be noisy and uninformative.

### Mechanism 2
- Claim: SRKB maintains invariant class-level relations through momentum-updated correlation prototypes to improve robustness during inference.
- Mechanism: During training, SRKB accumulates the weighted average of transport plans T for each deception class into a correlation prototype B using momentum updating. During testing, samples with unreliable transport plans (low standard deviation) use their class's prototype instead.
- Core assumption: There exists stable, invariant correlation patterns between facial expression categories and deception categories across different data batches and samples.
- Evidence anchors:
  - [abstract] "a correlation prototype within another proposed module SRKB is well designed to retain the invariant correlations between facial expression classes and deception classes through momentum updating"
  - [section] "During training phase, momentum updating (Laine and Aila 2016) is introduced to update the correlation prototype B by: Bl = αBl + (1− α) 1Pn i Iyt i=l nXi=1 TiIyt i=l"
- Break condition: If the invariant relations don't exist or vary significantly across training data, momentum updating will converge to meaningless averages.

### Mechanism 3
- Claim: Sample-specific re-weighting strategy in SRKB fine-tunes correlation mappings during inference to handle noise and improve accuracy.
- Mechanism: For each test sample, if the standard deviation of its transport plan T is below threshold ν, the correlation prototype BIi (from the class I it most closely resembles) replaces T to determine knowledge transfer weights.
- Core assumption: Test samples may have unreliable transport plans due to noise, and using class-level prototypes provides more robust guidance.
- Evidence anchors:
  - [abstract] "During inference, the transferred knowledge is fine-tuned with the correlation prototype using a sample-specific re-weighting strategy"
  - [section] "ˆTi = σiTi + (1− σi)BIi s.t. σ i = { 0, std(Ti) < ν std(Ti) − ν, otherwise"
- Break condition: If transport plans are consistently reliable (high std), the re-weighting strategy adds unnecessary complexity without benefit.

## Foundational Learning

- Concept: Optimal Transport Theory
  - Why needed here: H-OTKT module fundamentally relies on OT to quantify semantic similarity between facial expression classes and deception samples
  - Quick check question: Can you explain how entropy regularization in OT makes the problem computationally tractable?

- Concept: Momentum Updating in Deep Learning
  - Why needed here: SRKB module uses momentum updating to maintain running averages of correlation prototypes across training batches
  - Quick check question: How does momentum updating help stabilize learning compared to simple averaging?

- Concept: Curriculum Learning Strategy
  - Why needed here: The paper gradually increases ξ' during training to transition from relying on transferred knowledge to using learned target features
  - Quick check question: What's the benefit of gradually increasing the weight of transferred features rather than using them from the start?

## Architecture Onboarding

- Component map: Encoder layer -> H-OTKT module -> SRKB module -> Classification layer
- Critical path: Extract features from target samples → Compute transport plans via H-OTKT → Transfer knowledge from source classes → Fuse with target features → Classify with MLP
- Design tradeoffs:
  - H-OTKT vs direct feature matching: OT provides probabilistic alignment but is computationally heavier
  - SRKB vs no prototype: Adds robustness but requires maintaining and updating additional state
  - Multi-modal fusion: Simple averaging used, but more sophisticated fusion could improve performance
- Failure signatures:
  - Transport plans T with very low variance across all classes (no meaningful alignment)
  - Correlation prototypes B converging to uniform distributions (no learned patterns)
  - Performance degradation when switching from training to testing (SRKB not working properly)
- First 3 experiments:
  1. Test H-OTKT alone: Run AFFAKT without SRKB to verify knowledge transfer improves baseline accuracy
  2. Test SRKB ablation: Compare with and without SRKB to measure robustness improvement
  3. Test different source datasets: Verify that source domain quality affects performance as expected from interpretability studies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of pre-trained encoder affect the quality of transferred knowledge in AFFAKT?
- Basis in paper: [explicit] The paper mentions that different pre-trained encoders like MAE-DFER and Former-DFER are employed, and the results vary across different facial expression datasets due to different representation abilities.
- Why unresolved: The paper only briefly mentions the influence of the pre-trained encoder in ablation studies without providing a comprehensive analysis of how different encoders affect the overall performance and robustness of the method.
- What evidence would resolve it: A detailed comparison of AFFAKT's performance using various pre-trained encoders on multiple deception detection datasets, along with an analysis of the feature space structures they produce.

### Open Question 2
- Question: Can the hierarchical optimal transport knowledge transfer (H-OTKT) module be adapted for other domains beyond video deception detection?
- Basis in paper: [inferred] The H-OTKT module is designed to quantify the correlation mapping between facial expression classes and deception samples, suggesting a general framework for knowledge transfer that could potentially be applied to other tasks with different source and target domains.
- Why unresolved: The paper focuses solely on video deception detection and does not explore the applicability of the H-OTKT module to other domains or tasks.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of the H-OTKT module in transferring knowledge from a source domain to a target domain in tasks other than video deception detection, such as sentiment analysis or action recognition.

### Open Question 3
- Question: How does the sample-specific re-weighting strategy in the SRKB module handle noisy or ambiguous data in the target domain?
- Basis in paper: [explicit] The SRKB module uses a sample-specific re-weighting strategy to fine-tune the correlation mapping during inference, which is expected to improve robustness when dealing with noisy or ambiguous data.
- Why unresolved: The paper does not provide a detailed analysis of how the sample-specific re-weighting strategy performs in the presence of noise or ambiguity in the target domain data.
- What evidence would resolve it: Experiments evaluating the performance of AFFAKT with the SRKB module on target domain datasets with varying levels of noise or ambiguity, and a comparison with methods that do not use the sample-specific re-weighting strategy.

## Limitations

- The semantic validity of the assumed relationship between facial expression categories and deception behaviors is not empirically validated
- The effectiveness of momentum-updating depends on the existence of stable, invariant correlation patterns that may not generalize across datasets
- The computational complexity of hierarchical optimal transport may limit scalability to very large datasets

## Confidence

- **High confidence**: The mathematical framework of hierarchical optimal transport and its implementation are sound
- **Medium confidence**: The experimental results showing performance improvements are valid, but interpretability analysis could benefit from more rigorous validation
- **Low confidence**: The fundamental assumption that facial expression categories meaningfully correspond to deception behaviors across diverse contexts

## Next Checks

1. **Semantic validation study**: Conduct ablation experiments where source expression classes with minimal theoretical connection to deception are removed to test if H-OTKT is learning semantically meaningful mappings.

2. **Cross-dataset consistency test**: Train AFFAKT on one deception dataset, then evaluate on another with different expression-to-deception mappings to assess whether learned correlations are dataset-specific.

3. **Transport plan stability analysis**: For a subset of test samples, visualize and analyze the transport plans T across multiple runs to identify instability in the knowledge transfer mechanism.