---
ver: rpa2
title: Jailbreaking Attack against Multimodal Large Language Model
arxiv_id: '2402.02309'
source_url: https://arxiv.org/abs/2402.02309
tags:
- attack
- image
- jailbreaking
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a maximum likelihood-based approach for jailbreaking
  multimodal large language models (MLLMs) by finding an image-based jailbreaking
  prompt (imgJP). The method exhibits strong data-universal properties, enabling jailbreaks
  across multiple unseen prompts and images, and demonstrates notable model-transferability,
  successfully attacking various models in a black-box manner.
---

# Jailbreaking Attack against Multimodal Large Language Model

## Quick Facts
- **arXiv ID**: 2402.02309
- **Source URL**: https://arxiv.org/abs/2402.02309
- **Reference count**: 40
- **Primary result**: Achieves 93% attack success rate on LLaMA2 using only 20 reversed text-based jailbreaking prompts

## Executive Summary
This paper introduces a maximum likelihood-based approach for jailbreaking multimodal large language models (MLLMs) by finding image-based jailbreaking prompts (imgJP). The method demonstrates strong data-universal properties, enabling attacks across multiple unseen prompts and images, and shows notable model-transferability, successfully attacking various models in a black-box manner. The authors also establish a connection between MLLM- and LLM-jailbreaks, introducing a construction-based method that achieves higher efficiency than state-of-the-art approaches for LLM jailbreaking.

## Method Summary
The authors propose a maximum likelihood-based approach that modifies the objective function of adversarial attacks to maximize the likelihood of generating target outputs for harmful prompts. They integrate universal adversarial attack strategies to achieve image-universal properties and introduce a construction-based method that leverages the connection between MLLM and LLM jailbreaks. The approach uses adversarial optimization techniques like Projected Gradient Descent to find perturbations that maximize the likelihood of generating affirmative responses to harmful queries.

## Key Results
- Achieves 93% attack success rate (ASR) on LLaMA2 with only 20 reversed text-based jailbreaking prompts
- Demonstrates strong data-universal properties, generalizing to 300 unseen prompts from 25 training prompts
- Shows notable model-transferability, successfully attacking various models in a black-box manner
- Outperforms existing methods in LLM jailbreaking efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Jailbreaking MLLMs via maximum likelihood maximization of target outputs bypasses alignment by leveraging the generative nature of the model.
- **Mechanism**: The attack modifies the objective function from traditional adversarial classification tasks to a generative task where the model is encouraged to produce responses starting with a positive affirmation (e.g., "Sure, here is a...") when given harmful prompts.
- **Core assumption**: The MLLM's generative output is directly influenced by the likelihood of generating specific target responses, and the model's alignment guardrails can be bypassed by steering the likelihood toward affirmative responses.
- **Evidence anchors**:
  - [abstract] "we propose a maximum likelihood-based approach by modifying the objective function of adversarial attacks... we attempt to maximize the likelihood of generating the corresponding target outputs"
  - [section 3.1] "When faced with a harmful query, a LLM equipped with alignment guardrails will decline it... Thus, a common strategy in LLM-jailbreaks is to encourage the LLM to respond with a positive affirmation"
  - [corpus] "Weak evidence - the corpus does not provide additional supporting details for this specific mechanism"
- **Break condition**: If the MLLM's alignment mechanism detects and blocks responses that start with positive affirmations in the context of harmful queries, the attack would fail.

### Mechanism 2
- **Claim**: The image-universal property enables the same perturbation to jailbreak the model across multiple input images, extending the attack's applicability.
- **Mechanism**: By integrating universal adversarial attack strategies, the attack finds a perturbation that, when added to any input image from a specific distribution, enables successful jailbreaking across multiple unseen images.
- **Core assumption**: There exists a universal perturbation that can consistently modify the visual input's representation in a way that bypasses the MLLM's safety mechanisms regardless of the specific image content.
- **Evidence anchors**:
  - [section 3.2] "we integrate the universal adversarial attack strategy into our deltaJP-based attack... we extend Eq.(2) as follows, max_δ Σxj∈D MΣi=0 log(p(ai|qi, exj)) s.t. fxj ∈ [0, 255]d, fxj = xj + δ ||δ||p < ϵ"
  - [section 4.2] "From Table 3, we observe that our approach exhibits a certain image-universal property"
  - [corpus] "Weak evidence - the corpus does not provide additional supporting details for this specific mechanism"
- **Break condition**: If the MLLM's visual processing module has input-dependent safeguards that detect and neutralize universal perturbations, the attack would fail.

### Mechanism 3
- **Claim**: The connection between MLLM-jailbreaks and LLM-jailbreaks allows efficient LLM-jailbreaking through construction-based methods.
- **Mechanism**: Since MLLMs contain LLMs, finding an image jailbreaking prompt (imgJP) that jailbreaks an MLLM reveals features (embJP) that, when converted back to text space, can jailbreak the contained LLM.
- **Core assumption**: The features extracted from the visual input and fed to the LLM component are the critical pathway for jailbreaking, and these features can be accurately reverse-engineered from the imgJP to text.
- **Evidence anchors**:
  - [abstract] "we reveal a connection between MLLM- and LLM-jailbreaks... we introduce a construction-based method to harness our approach for LLM-jailbreaks"
  - [section 3.4] "To jailbreak a target LLM, we first construct a MLLM that encapsulates it... we perform our MLLM-jailbreak to acquire imgJP, while concurrently recording the embedding embJP... the embJP is reversed into text space through De-embedding and De-tokenizer operations"
  - [section 4.4] "Our approach can jailbreak not only MLLM models but also LLM models... we achieve the train ASR of 92% and the test ASR of 93%"
- **Break condition**: If the mapping between visual features and text embeddings is not invertible or if the LLM's internal processing significantly alters the features before making decisions, the construction-based attack would fail.

## Foundational Learning

- **Concept**: Adversarial attack optimization techniques (e.g., Projected Gradient Descent)
  - Why needed here: The attack uses adversarial optimization to find perturbations that maximize the likelihood of generating target outputs
  - Quick check question: What is the key difference between standard adversarial attacks and this jailbreaking approach in terms of their objective functions?

- **Concept**: Multimodal model architecture (vision encoder + LLM)
  - Why needed here: Understanding how visual inputs are processed and combined with text is crucial for grasping how the attack manipulates the model
  - Quick check question: In a typical MLLM, at what stage are visual features combined with text embeddings before being fed to the LLM component?

- **Concept**: Embedding spaces and tokenizers in LLMs
  - Why needed here: The construction-based attack relies on converting between image-based embeddings and text tokens, requiring understanding of how these mappings work
  - Quick check question: What is the purpose of the de-embedding operation in the construction-based attack, and how does it differ from standard embedding?

## Architecture Onboarding

- **Component map**: Input image → Vision encoder → Fusion with text → LLM → Output text
- **Critical path**: The attack modifies the input image to influence the entire downstream generation process through the vision encoder, fusion module, and LLM
- **Design tradeoffs**: The attack balances perturbation strength (to remain visually similar to original) with effectiveness (to maximize jailbreaking success). Stronger perturbations may be more effective but more detectable.
- **Failure signatures**: Attack fails if the MLLM's alignment mechanisms detect the perturbation as adversarial, if the visual features are too distorted to be processed, or if the generated responses are blocked by content filters.
- **First 3 experiments**:
  1. Implement the imgJP-based attack on a simple MLLM (e.g., MiniGPT-4) with a small set of harmful prompts to verify the basic mechanism
  2. Test the deltaJP-based attack with a single input image category to confirm the image-universal property
  3. Attempt the construction-based attack by building a simple MLLM wrapper around an LLM and converting a successful imgJP to txtJP

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important questions remain unaddressed based on the limitations and scope of the current work:

## Limitations

- **Model Architecture Dependencies**: The effectiveness of the attack relies heavily on the specific MLLM architecture (vision encoder + LLM + fusion module). The paper doesn't fully explore how different architectural choices might affect vulnerability or attack transferability.

- **Dataset Specificity**: While the authors claim data-universal properties, the training and evaluation datasets are not fully described. The 30 images per category in AdvBench-M could introduce bias if not properly sampled across diverse image distributions.

- **Perturbation Visibility**: The paper claims visually-similar perturbations, but doesn't provide detailed perceptual analysis of how noticeable these perturbations are to human observers, which is critical for real-world applicability.

## Confidence

**High Confidence**: The core mechanism of maximum likelihood-based optimization for jailbreaking is well-established in the literature and the mathematical formulation is sound. The connection between MLLM and LLM jailbreaks through embedding spaces is theoretically valid.

**Medium Confidence**: The experimental results showing 93% ASR on LLaMA2 are impressive, but the limited model diversity (5 MLLMs tested) and relatively small scale of harmful prompts (only 20 used for the construction-based method) reduce confidence in generalizability.

**Low Confidence**: The claim of strong data-universal properties across unseen prompts and images is based on limited testing. The evidence from Table 3 and the corpus indicates weak support for the universal perturbation mechanism.

## Next Checks

1. **Cross-Architecture Transferability Test**: Evaluate the attack against MLLMs with fundamentally different architectures (e.g., those using different vision encoders like CLIP vs. BLIP) to verify the claimed model-transferability.

2. **Perceptual Analysis of Perturbations**: Conduct a user study to quantify the visual detectability of deltaJP perturbations across different image categories and perturbation strengths.

3. **Adversarial Defense Evaluation**: Test the attack against common adversarial defense mechanisms (input preprocessing, adversarial training) to establish robustness and identify potential countermeasures.