---
ver: rpa2
title: 'Controllable Game Level Generation: Assessing the Effect of Negative Examples
  in GAN Models'
arxiv_id: '2410.23108'
source_url: https://arxiv.org/abs/2410.23108
tags:
- playable
- levels
- examples
- negative
- segments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the effectiveness of incorporating negative\
  \ examples into Generative Adversarial Networks (GANs) for controllable game level\
  \ generation. It compares three GAN variants\u2014vanilla GAN, Conditional GAN (CGAN),\
  \ and Rumi-GAN\u2014in generating playable and feature-controlled levels for two\
  \ 2D tile-based games."
---

# Controllable Game Level Generation: Assessing the Effect of Negative Examples in GAN Models

## Quick Facts
- arXiv ID: 2410.23108
- Source URL: https://arxiv.org/abs/2410.23108
- Reference count: 5
- Primary result: Rumi-GAN outperforms vanilla GAN in generating playable levels by leveraging negative examples

## Executive Summary
This study evaluates three GAN variants—vanilla GAN, Conditional GAN (CGAN), and Rumi-GAN—for controllable game level generation in two 2D tile-based games. The research focuses on how incorporating negative examples (unplayable levels) affects the generation of playable and feature-controlled levels. Rumi-GAN, which explicitly leverages negative examples in its loss function, demonstrates superior performance in generating playable levels compared to the vanilla GAN. However, the study finds mixed results for controllability, with CGAN showing improvements in some cases but not consistently across all game types.

## Method Summary
The study trains three GAN models (Vanilla GAN, CGAN, and Rumi-GAN) using WGAN algorithm with RMSprop optimizer, batch size 32, learning rate 0.00005 for 200 iterations. The models generate 14×32 Mario and 14×14 Cave level segments from one-hot encoded data. CGAN conditions on feature labels (pipe/treasure counts), while Rumi-GAN uses both positive (playable) and negative (unplayable) examples. Evaluation measures playability through pathfinding between start and end points, and controllability by checking correct feature counts.

## Key Results
- Rumi-GAN outperforms vanilla GAN in generating playable levels by leveraging negative examples
- CGAN shows improved controllability in some cases but results are inconsistent across game types
- Models using negative examples (CGAN and Rumi-GAN) generally outperform vanilla GAN in playability generation
- Controllability improvements are less consistent when combining negative examples with other constraints

## Why This Works (Mechanism)

### Mechanism 1
Rumi-GAN's loss function helps the generator avoid producing unplayable levels by explicitly penalizing generation of negative examples. Rumi-GAN splits the real data distribution into positive (playable) and negative (unplayable) components in its discriminator loss, giving higher weight to avoiding negative samples. This creates a stronger signal for the generator to stay within the playable subspace. Core assumption: Negative examples are sufficiently representative of the "bad" regions the generator should avoid, and the discriminator can effectively distinguish these from positive examples.

### Mechanism 2
CGAN improves controllability by conditioning both generator and discriminator on label information about level features. By providing explicit labels (e.g., number of pipes) during training, the CGAN learns to map specific latent codes to desired feature counts, creating a direct path from control parameters to generated content. Core assumption: The relationship between latent space and feature counts is learnable and stable across the training distribution.

### Mechanism 3
The WGAN training algorithm provides more stable learning dynamics than standard GANs for game level generation. WGAN uses Earth Mover distance and gradient penalties instead of Jensen-Shannon divergence, reducing mode collapse and providing smoother gradients for both generator and discriminator. Core assumption: The level generation problem benefits from the improved gradient stability and reduced vanishing gradient issues of WGAN.

## Foundational Learning

- Concept: Understanding of basic GAN architecture (generator, discriminator, adversarial loss)
  - Why needed here: The paper builds on vanilla GAN and extends it with conditioning and negative examples, so understanding the base architecture is essential
  - Quick check question: What is the fundamental minimax game between generator and discriminator in a vanilla GAN?

- Concept: Conditional modeling and label conditioning
  - Why needed here: CGAN relies on conditioning both networks on additional information to achieve controllability
  - Quick check question: How does conditioning the discriminator differently from the generator affect training dynamics?

- Concept: Negative sampling and contrastive learning principles
  - Why needed here: Rumi-GAN's core innovation is leveraging negative examples to improve learning from positive examples
  - Quick check question: What is the theoretical justification for using negative examples to enhance learning of positive examples in GANs?

## Architecture Onboarding

- Component map: Generator -> Discriminator -> Training Loop -> Evaluation
- Critical path: Data preparation (one-hot encoding, splitting into positive/negative) -> Model initialization (DCGAN architecture with ReLU/LeakyReLU activations) -> Training loop (alternating generator and discriminator updates) -> Evaluation (playability checking with pathfinding, feature counting)
- Design tradeoffs: WGAN vs standard GAN (more stable training but requires careful weight clipping/gradient penalty tuning), negative examples (provide stronger avoidance signals but require careful selection to be representative), conditioning approach (CGAN's explicit labels provide direct control but require accurate labeling, while Rumi-GAN's implicit approach may generalize better)
- Failure signatures: Mode collapse (generator produces very similar levels regardless of input), discriminator overpowering (generator fails to make progress), vanishing gradients (generator produces random noise), feature instability (generated levels have inconsistent feature counts)
- First 3 experiments: 1) Train vanilla GAN on only playable levels, evaluate playability percentage, 2) Train CGAN with playability labels, compare playability improvement, 3) Train Rumi-GAN with positive playable and negative unplayable examples, evaluate playability and feature controllability

## Open Questions the Paper Calls Out

### Open Question 1
Does the quality of negative examples significantly impact the performance of Rumi-GAN and CGAN in generating both playable and controllable game levels? The paper suggests that the surprising decrease in performance of models using negative examples in Experiment Two may be due to the quality of the negative examples. Experiments comparing Rumi-GAN and CGAN performance using high-quality versus low-quality negative examples for both playability and controllability constraints would resolve this.

### Open Question 2
Can multi-stage training approaches with high-quality negative examples improve the controllability of GAN models when generating game levels with specific features? The authors suggest exploring multi-stage training approaches with high-quality negative examples in fine-tuning steps as future work. Implementing and testing multi-stage training approaches using Rumi-GAN and CGAN to assess improvements in generating levels with specific features would resolve this.

### Open Question 3
How does the integration of active learning methods with minimal training levels affect the performance of Rumi-GAN and CGAN in generating playable and controllable game levels? The authors propose exploring active learning methods with minimal training levels as a future direction. Experiments applying active learning techniques to Rumi-GAN and CGAN, measuring their effectiveness in generating levels with desired constraints would resolve this.

## Limitations

- The study relies on synthetic level data generated by constraint solvers rather than human-designed levels, which may not capture the full complexity of real game level design
- The Rumi-GAN architecture details are not fully specified, making exact replication challenging
- Training was limited to 200 iterations with fixed hyperparameters, leaving open questions about optimal training duration and parameter tuning

## Confidence

- **High Confidence**: The observation that negative examples improve playability generation in GANs is well-supported by comparative results across all tested game types
- **Medium Confidence**: The claim that CGAN improves controllability in some cases is supported but inconsistent across game types, suggesting context-dependent effectiveness
- **Low Confidence**: The assertion that negative examples consistently enhance controllability when combined with other constraints lacks strong empirical support and requires further investigation

## Next Checks

1. **Architectural Verification**: Implement and test multiple variants of Rumi-GAN's negative example integration to determine which specific design choices most impact performance

2. **Generalization Testing**: Evaluate model performance on human-designed level segments to assess whether synthetic training data limitations affect real-world applicability

3. **Training Dynamics Analysis**: Conduct ablation studies varying training duration, learning rates, and batch sizes to identify optimal hyperparameter settings for each model variant