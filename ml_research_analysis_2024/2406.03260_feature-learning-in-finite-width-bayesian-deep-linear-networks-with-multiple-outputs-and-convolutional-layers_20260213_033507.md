---
ver: rpa2
title: Feature learning in finite-width Bayesian deep linear networks with multiple
  outputs and convolutional layers
arxiv_id: '2406.03260'
source_url: https://arxiv.org/abs/2406.03260
tags:
- networks
- learning
- deep
- https
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides exact non-asymptotic integral representations
  for the statistics of finite-width Bayesian deep linear networks, including fully-connected
  architectures with multiple outputs and convolutional layers. The authors show that
  the prior over outputs is a mixture of Gaussians with explicit covariance matrices
  formed from Wishart ensembles, and the posterior retains this mixture structure.
---

# Feature learning in finite-width Bayesian deep linear networks with multiple outputs and convolutional layers

## Quick Facts
- arXiv ID: 2406.03260
- Source URL: https://arxiv.org/abs/2406.03260
- Reference count: 40
- Key outcome: Exact non-asymptotic integral representations for finite-width Bayesian deep linear networks, showing prior and posterior as Wishart-based Gaussian mixtures with parametric hidden layer dependence

## Executive Summary
This paper provides exact integral representations for the statistics of finite-width Bayesian deep linear networks with multiple outputs and convolutional layers. The authors show that the prior over outputs is a mixture of Gaussians with explicit covariance matrices formed from Wishart ensembles, and the posterior retains this mixture structure. A key insight is that all hidden layer widths appear parametrically, enabling dimensional reduction. Using large deviation theory, the authors characterize the feature learning infinite-width limit, where Wishart ensembles concentrate around data-dependent solutions that differ from lazy-training regime. The approach unifies and extends previous results, providing a comprehensive framework for understanding feature learning in Bayesian deep linear networks at finite width.

## Method Summary
The method develops exact non-asymptotic integral representations for Bayesian deep linear networks by expressing the prior and posterior distributions as mixtures of Gaussians with Wishart-based covariance matrices. The analysis leverages matrix normal distributions for weight initialization and large deviation theory to characterize asymptotic behavior. The key innovation is showing how hidden layer widths appear parametrically in the representations, enabling dimensional reduction. For convolutional architectures, the approach adapts to the specific covariance structures and renormalization mechanisms. The results provide closed-form expressions for both prior and posterior distributions under Gaussian likelihood, with the posterior maintaining the mixture structure of the prior.

## Key Results
- Exact prior distribution over outputs represented as mixture of Gaussians with Wishart-based covariance matrices
- Closed-form posterior predictive distribution for squared error loss that inherits the mixture structure
- Large deviation characterization of feature learning infinite-width limit showing concentration around data-dependent solutions
- Dimensional reduction achieved through parametric dependence on hidden layer widths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The output prior is an exactly computable mixture of Gaussians with explicit covariance matrices formed from Wishart ensembles.
- Mechanism: At finite width, the prior over the outputs is represented as a mixture of Gaussians where the covariance is modified by L random matrices with Wishart distribution. The hidden layer widths appear parametrically in this representation, enabling dimensional reduction.
- Core assumption: The weights are independent normally distributed with zero means and variance 1/λℓ at each layer, and the input data dimension N0 is sufficiently large.
- Evidence anchors:
  - [abstract] "exact and elementary non-asymptotic integral representation for the joint prior distribution over the outputs, given in terms of a mixture of Gaussians"
  - [section 3.1] "the prior distribution over S1:P is a mixture of Gaussians where the covariance matrix is an explicit function of random Wishart matrices"
  - [corpus] Weak - related papers discuss finite-width neural tangent kernels but don't provide the specific Wishart ensemble representation
- Break condition: If the assumption of independent normally distributed weights fails, or if the hidden layer widths are not sufficiently large relative to the output dimension D.

### Mechanism 2
- Claim: The posterior predictive distribution in the case of squared error loss function (Gaussian likelihood) is also a mixture of Gaussians with closed form mixing distribution.
- Mechanism: When using quadratic loss function, the posterior distribution inherits the properties of the prior. The posterior remains a mixture of Gaussians, leading to standard equations for bias and variance of a Gaussian Process, but with random variables due to the mixture structure.
- Core assumption: The matrix [x0, x1, ..., xP] is full rank, ensuring invertibility of the covariance matrices.
- Evidence anchors:
  - [abstract] "an analytical formula for the posterior distribution in the case of squared error loss function (Gaussian likelihood)"
  - [section 3.2] "the posterior is again a mixture of Gaussians and this leads to the rather standard equations for the bias and variance of a Gaussian Process"
  - [corpus] Weak - related papers discuss Bayesian inference but don't provide the specific mixture representation for the posterior
- Break condition: If the training data matrix is not full rank, or if the likelihood is not Gaussian.

### Mechanism 3
- Claim: In the feature learning infinite-width limit, large deviation theory shows non-trivial explicit dependence on training inputs and labels.
- Mechanism: Using large deviation theory, the authors characterize the feature learning infinite-width limit where the Wishart ensembles concentrate around data-dependent solutions that differ from the lazy-training regime. The mean-field parametrization provides a way to escape lazy training.
- Core assumption: The mean-field parametrization is valid, where the loss and output functions are rescaled appropriately.
- Evidence anchors:
  - [abstract] "Using large deviation theory, the authors characterize the feature learning infinite-width limit, where the Wishart ensembles concentrate around data-dependent solutions that differ from the lazy-training regime"
  - [section 3.3] "we precisely show how it provides a way to escape lazy training"
  - [corpus] Weak - related papers discuss feature learning but don't provide the specific large deviation characterization
- Break condition: If the mean-field scaling is not appropriate for the specific architecture or training regime.

## Foundational Learning

- Concept: Matrix normal distribution
  - Why needed here: The weights are arranged as rectangular matrices with law MN(0, 1Nℓ, λ−1ℓ−1 1Nℓ−1), which is essential for deriving the prior distribution.
  - Quick check question: What is the Laplace functional of a Wishart distribution, and how is it used in deriving the prior representation?

- Concept: Large deviation theory
  - Why needed here: Used to characterize the feature learning infinite-width limit, showing how the Wishart ensembles concentrate around data-dependent solutions.
  - Quick check question: What is the rate function for the sequence of measures QL,N in the feature learning infinite-width limit?

- Concept: Kernel shape renormalization
  - Why needed here: From a physics perspective, deep architectures with multiple outputs or convolutional layers represent different manifestations of kernel shape renormalization.
  - Quick check question: How does kernel shape renormalization differ from scalar renormalization in fully-connected networks with a single output?

## Architecture Onboarding

- Component map:
  Input layer (N0 × C0) -> Hidden layers (Nℓ width for ℓ = 1, ..., L) -> Output layer (D dimension)
  Weights: W(ℓ) for ℓ = 0, ..., L
  Wishart matrices: Qℓ for ℓ = 1, ..., L

- Critical path:
  1. Initialize weights as independent normal distributions
  2. Compute the joint prior distribution over outputs using the Wishart ensemble representation
  3. For squared error loss, compute the posterior distribution using the mixture representation
  4. For feature learning analysis, apply large deviation theory to the Wishart ensembles

- Design tradeoffs:
  - Finite width vs. infinite width: Finite width provides more realistic feature learning but loses the Gaussian process equivalence
  - Multiple outputs vs. single output: Multiple outputs require matrix-valued Wishart distributions instead of scalar
  - Convolutional vs. fully-connected: Convolutional architectures have different covariance structures and renormalization mechanisms

- Failure signatures:
  - If the prior covariance matrices are not positive definite, check that Nℓ > D for all layers
  - If the posterior predictive distribution is not well-defined, verify that the training data matrix is full rank
  - If the large deviation analysis fails, ensure the mean-field parametrization is appropriate

- First 3 experiments:
  1. Verify the prior distribution: Compute the empirical covariance of outputs for a small network and compare with the theoretical Wishart mixture
  2. Test the posterior: Train a network on a simple dataset with squared error loss and check if the posterior predictive matches the theoretical mixture
  3. Explore feature learning: Vary the width of hidden layers and observe how the Wishart ensembles concentrate around data-dependent solutions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between the feature learning infinite-width limit (Proposition 10) and the proportional limit studied in Li and Sompolinsky (2021) and Pacelli et al. (2023)?
- Basis in paper: [explicit] The paper states that both limits involve concentration of Wishart ensembles around data-dependent solutions, but the reasons differ: mean-field scaling introduces additional data-dependent terms in the rate function, while the proportional limit has data entering through saddle point equations in the action.
- Why unresolved: The paper establishes qualitative similarities but does not provide a rigorous mathematical comparison or bridge between these two asymptotic regimes.
- What evidence would resolve it: A rigorous derivation showing how the mean-field scaling in the feature learning limit reduces to the proportional limit under appropriate parameter choices, or vice versa.

### Open Question 2
- Question: How do the results extend to networks with non-linear activation functions?
- Basis in paper: [inferred] The paper focuses exclusively on deep linear networks, which are noted as simplified models. The authors mention that non-linear networks display complex phenomena not captured by linear approximations.
- Why unresolved: The analytical tractability that makes deep linear networks amenable to exact analysis disappears with non-linearities, and no generalization of the mixture-of-Gaussians representation is provided.
- What evidence would resolve it: An extension of the integral representation to include non-linear activation functions, perhaps through perturbative methods or by identifying new tractable cases.

### Open Question 3
- Question: What are the implications of the dimensional reduction provided by the Wishart ensemble representation for computational efficiency in Bayesian inference?
- Basis in paper: [explicit] The authors emphasize that all hidden layer widths appear parametrically in the prior representation, providing explicit dimensional reduction compared to the direct representation using weight matrices.
- Why unresolved: While the reduction is noted, the paper does not explore practical algorithms or computational advantages that could be gained from this representation in Bayesian inference tasks.
- What evidence would resolve it: Development and testing of Bayesian inference algorithms that exploit the Wishart ensemble representation, with benchmarks comparing computational efficiency against standard methods.

## Limitations

- The analysis relies on several technical assumptions that may limit practical applicability, including the requirement that Nℓ > D for all layers
- Computational complexity of evaluating high-dimensional integrals in the mixture representations grows rapidly with network depth and width
- Large deviation analysis provides asymptotic characterizations that may not fully capture finite-width behavior near the transition between lazy-training and feature-learning regimes

## Confidence

- **High confidence**: The exact integral representations for prior and posterior distributions (Mechanisms 1 and 2) are mathematically rigorous with explicit formulas derived from well-established matrix theory
- **Medium confidence**: The large deviation characterization of the feature learning limit (Mechanism 3) is technically sound but relies on asymptotic assumptions that may not fully describe intermediate-width behavior
- **High confidence**: The computational tractability claims are supported by the explicit parametric forms, though practical implementation complexity remains to be fully validated

## Next Checks

1. **Empirical verification of Wishart mixture**: Generate synthetic data and train small networks to empirically verify that the output prior matches the theoretical Wishart mixture distribution across different width configurations
2. **Posterior predictive accuracy**: Compare theoretical posterior predictive distributions with Monte Carlo sampling from trained networks on benchmark datasets to validate the mixture representation
3. **Feature learning transition**: Numerically study the transition between lazy-training and feature-learning regimes by varying hidden layer widths and measuring concentration properties of Wishart ensembles against theoretical predictions