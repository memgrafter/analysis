---
ver: rpa2
title: Probabilistic Forecasting of Irregular Time Series via Conditional Flows
arxiv_id: '2402.06293'
source_url: https://arxiv.org/abs/2402.06293
tags:
- profiti
- time
- flows
- series
- normalizing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ProFITi, a new normalizing flow model for
  probabilistic forecasting of irregularly sampled multivariate time series with missing
  values. ProFITi learns joint distributions over future values conditioned on past
  observations and queried channels and times, without assuming any fixed shape of
  the underlying distribution.
---

# Probabilistic Forecasting of Irregular Time Series via Conditional Flows

## Quick Facts
- arXiv ID: 2402.06293
- Source URL: https://arxiv.org/abs/2402.06293
- Authors: Vijaya Krishna Yalavarthi; Randolf Scholz; Stefan Born; Lars Schmidt-Thieme
- Reference count: 40
- Primary result: 4x higher likelihood compared to the previously best model

## Executive Summary
This paper introduces ProFITi, a novel normalizing flow model for probabilistic forecasting of irregularly sampled multivariate time series with missing values. The model learns joint distributions over future values conditioned on past observations and queried channels and times without assuming any fixed shape of the underlying distribution. Key innovations include a sorted invertible triangular attention layer (SITA) that ensures permutation invariance and a new invertible non-linear activation function called Shiesh. Experiments on four real-world datasets demonstrate significant improvements in forecasting performance.

## Method Summary
ProFITi is a conditional normalizing flow model designed for irregularly sampled multivariate time series. It uses a GraFITi encoder to embed observations and queries, followed by multiple flow layers consisting of sorted invertible triangular attention (SITA), Shiesh activation, and elementwise linear transformation (EL) layers. The model is trained using normalized joint negative log-likelihood and evaluated on held-out data using njNLL, mNLL, and MSE metrics.

## Key Results
- ProFITi achieves 4x higher likelihood compared to the previously best model
- Demonstrates superior performance across four diverse datasets: MIMIC-III, MIMIC-IV, Physionet'12, and USHCN
- Effectively handles irregularly sampled multivariate time series with missing values
- Provides flexible modeling of conditional distributions without distributional assumptions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Sorted invertible triangular attention (SITA) enables permutation invariance for conditional normalizing flows.
- **Mechanism**: By sorting the conditioner inputs before applying attention and reversing the output order after transformation, the model ensures that any permutation of the conditioner results in an equivalent transformed output. This breaks the permutation equivariance into a sequence where the sorted order is invariant.
- **Core assumption**: Sorting the conditioner inputs uniquely orders the elements, and the inverse operation restores the original query ordering.
- **Evidence anchors**:
  - [abstract]: "SITA" is introduced as a novel model component.
  - [section 4]: Detailed explanation of SITA with sorting and triangular attention matrix.
  - [corpus]: No direct mention, but FMR scores indicate related work on permutation invariance.
- **Break condition**: If conditioner inputs contain duplicate elements, sorting may not be unique, breaking the inverse mapping.

### Mechanism 2
- **Claim**: Shiesh activation function enables unconstrained non-linearity in normalizing flows while maintaining invertibility.
- **Mechanism**: Shiesh is derived from an explicit ODE solution, ensuring invertibility and differentiability. Its bounded gradient prevents gradient explosion, and the analytic inverse simplifies training.
- **Core assumption**: The ODE solution for Shiesh is stable and covers the entire real line.
- **Evidence anchors**:
  - [abstract]: "Shiesh, that is on and onto whole real line."
  - [section 5]: Detailed derivation and properties of Shiesh.
  - [corpus]: No direct mention, but FMR scores suggest related work on activation functions.
- **Break condition**: If the parameter b is set too high, numerical instability may occur during implementation.

### Mechanism 3
- **Claim**: Elementwise linear transformation layer (EL) provides shift and scaling to the transformed distribution.
- **Mechanism**: EL applies learned scaling and translation to each dimension independently, ensuring the flow can model arbitrary shifts and scales in the data distribution.
- **Core assumption**: The neural networks for scaling and translation are flexible enough to capture the necessary transformations.
- **Evidence anchors**:
  - [section 6]: Description of EL layer in the model architecture.
  - [abstract]: Mention of EL layer in the model components.
  - [corpus]: No direct mention, but FMR scores indicate related work on flow components.
- **Break condition**: If the scaling network outputs values too close to zero, the inverse may become unstable.

## Foundational Learning

- **Concept**: Permutation invariance in normalizing flows
  - Why needed here: To ensure the model treats different orderings of the same conditioner inputs equivalently, crucial for forecasting multiple future values.
  - Quick check question: If we swap two query points in the conditioner, should the output distribution change? (Answer: No)

- **Concept**: Invertible neural networks
  - Why needed here: To ensure the normalizing flow can be trained via maximum likelihood, requiring an explicit inverse for the transformation.
  - Quick check question: Can a ReLU activation function be used in a normalizing flow? (Answer: No, because it's not invertible)

- **Concept**: Triangular Jacobian matrices
  - Why needed here: To enable efficient determinant computation and linear system solving in the flow, reducing computational complexity.
  - Quick check question: What is the computational complexity of inverting a dense KxK matrix vs a triangular KxK matrix? (Answer: O(K^3) vs O(K^2))

## Architecture Onboarding

- **Component map**: GraFITi encoder -> SITA layers -> Shiesh layers -> EL layers -> Final linear layer
- **Critical path**: Conditioner -> GraFITi encoder -> SITA -> Shiesh -> EL -> Base distribution
- **Design tradeoffs**:
  - SITA vs dense attention: SITA is more efficient but requires sorting
  - Shiesh vs other activations: Shiesh is more complex but provides better gradient properties
  - Number of flow layers: More layers increase expressiveness but also training difficulty
- **Failure signatures**:
  - Poor training loss: Check if conditioner sorting is correct
  - Unstable gradients: Check Shiesh implementation and parameter values
  - Memory issues: Check attention matrix size and number of flow layers
- **First 3 experiments**:
  1. Train on a simple synthetic dataset with known joint distribution to verify the model can learn basic relationships
  2. Compare performance with and without SITA to validate permutation invariance
  3. Test different values of the Shiesh parameter b to find the optimal setting for your data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ProFITi scale with increasing number of channels (C) in the time series?
- Basis in paper: [inferred] The paper demonstrates ProFITi's performance on datasets with varying numbers of channels (37, 96, 102) but does not systematically explore the impact of channel count on performance.
- Why unresolved: The paper focuses on demonstrating ProFITi's superiority over baselines on specific datasets rather than conducting a systematic study of how performance varies with different numbers of channels.
- What evidence would resolve it: Conducting experiments on datasets with a wider range of channel counts and analyzing how ProFITi's performance (e.g., njNLL) changes as C increases.

### Open Question 2
- Question: Can ProFITi's SITA layer be made more efficient for very long forecast horizons?
- Basis in paper: [explicit] The paper mentions that the determinant computation for the triangular attention matrix has O(K) complexity, which is more efficient than the O(K^3) complexity of the dense attention matrix used in Areg. However, it also notes that Areg performs worse with increasing forecast lengths.
- Why unresolved: While the paper demonstrates that Atri is more efficient than Areg, it does not explore whether further optimizations to SITA could improve its efficiency for very long forecast horizons.
- What evidence would resolve it: Developing and testing alternative implementations or approximations of SITA that maintain performance while reducing computational complexity for long forecast horizons.

### Open Question 3
- Question: How does the choice of sorting criterion in SITA affect ProFITi's performance?
- Basis in paper: [explicit] The paper mentions that sorting in SITA is unique only when x has unique elements and provides examples of different sorting criteria (e.g., sorting by timestamp then channel, or by channel then timestamp).
- Why unresolved: The paper does not systematically investigate how different sorting criteria impact ProFITi's performance or whether there is an optimal sorting strategy for different types of time series data.
- What evidence would resolve it: Conducting experiments where ProFITi is trained and evaluated using different sorting criteria on various datasets and analyzing the resulting performance differences.

## Limitations

- The permutation invariance mechanism assumes unique sorting of conditioner inputs, which may not hold for real-world data with duplicate or near-duplicate values
- The Shiesh activation function requires careful hyperparameter tuning, with potential numerical instability when parameter b is set too high
- Computational complexity may become prohibitive for high-dimensional time series or long forecasting horizons

## Confidence

**High confidence**: Sorted Invertible Triangular Attention (SITA) - The mechanism is well-explained with clear mathematical justification and logical flow.

**Medium confidence**: Shiesh Activation Function - While theoretically sound, practical implementation details and parameter sensitivity require more exploration.

**Medium confidence**: Overall Model Performance - Experimental results are promising, but lack of detailed baseline implementation information and hyperparameter tuning procedures limits full confidence.

## Next Checks

1. **Permutation Invariance Stress Test**: Systematically test the model's behavior with conditioner inputs containing duplicates and near-duplicates to quantify the impact on forecast accuracy and identify failure thresholds.

2. **Cross-Dataset Shiesh Parameter Sensitivity**: Conduct a comprehensive study varying the Shiesh parameter b across all four datasets to establish optimal ranges and identify any dataset-specific considerations.

3. **Computational Complexity Analysis**: Perform detailed profiling of memory usage and training/inference time as a function of forecasting horizon and time series dimensionality to establish practical limits and guide model architecture decisions for large-scale applications.