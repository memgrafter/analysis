---
ver: rpa2
title: Survey and Evaluation of Converging Architecture in LLMs based on Footsteps
  of Operations
arxiv_id: '2410.11381'
source_url: https://arxiv.org/abs/2410.11381
tags:
- attention
- size
- input
- generation
- stage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a survey and performance evaluation of large
  language model (LLM) architectures, focusing on how different architectural choices
  affect runtime behavior. The authors trace the evolution from RNNs through transformers
  to modern LLMs, then analyze four open-source models (Llama2-7B, Llama3-8B, Gemma-2B,
  Gemma-7B) under various hyperparameter settings using NVIDIA RTX 6000 Ada GPU.
---

# Survey and Evaluation of Converging Architecture in LLMs based on Footsteps of Operations

## Quick Facts
- arXiv ID: 2410.11381
- Source URL: https://arxiv.org/abs/2410.11381
- Reference count: 28
- This paper surveys LLM architectures and evaluates four open-source models (Llama2-7B, Llama3-8B, Gemma-2B, Gemma-7B) under various hyperparameter settings, finding that batch size and attention mechanisms significantly impact kernel composition and runtime ratios.

## Executive Summary
This paper conducts a comprehensive survey and performance evaluation of large language model architectures, tracing their evolution from RNNs through transformers to modern LLMs. The authors analyze how different architectural choices affect runtime behavior by examining four open-source models under various hyperparameter settings using NVIDIA RTX 6000 Ada GPU. They categorize model execution into summarization and generation stages, revealing that batch size and attention mechanisms (MHA vs GQA vs MQA) significantly impact kernel composition and runtime ratios.

The study demonstrates that for small batch sizes, GEMV operations dominate, while larger batches shift to GEMM operations. Models using grouped query attention (GQA) and multi-query attention (MQA) show reduced memory requirements compared to multi-head attention (MHA). The authors conclude that different hyperparameter settings and deployment environments lead to different optimal hardware configurations, suggesting specialized hardware may be needed for different LLM use cases.

## Method Summary
The authors conducted a survey tracing the evolution of LLM architectures from RNNs through transformers to modern models, then performed empirical evaluations on four open-source models (Llama2-7B, Llama3-8B, Gemma-2B, Gemma-7B) using NVIDIA RTX 6000 Ada GPU. They systematically varied hyperparameters including batch size, sequence length, and attention mechanisms, then profiled kernel composition and runtime behavior. The analysis categorized model execution into summarization and generation stages, examining how different architectural choices affected compute patterns and memory usage across various deployment scenarios.

## Key Results
- Batch size significantly affects operation dominance: small batches favor GEMV operations while large batches favor GEMM operations
- Attention mechanisms (MHA, GQA, MQA) impact memory requirements and compute patterns differently
- GQA and MQA reduce memory requirements compared to MHA by sharing key and value weights
- Model behavior shifts between compute-bound and memory-bound depending on hyperparameter settings and deployment environment

## Why This Works (Mechanism)
The effectiveness of different LLM architectures depends on the alignment between computational patterns and hardware characteristics. Batch size influences whether models operate primarily on matrix-vector (GEMV) or matrix-matrix (GEMM) operations, affecting memory access patterns and computational intensity. Attention mechanisms determine how information flows through the model and how weights are shared, directly impacting memory bandwidth requirements and computational load. The convergence toward specific architectural patterns (like MQA/GQA) represents optimization for particular deployment constraints, where memory efficiency becomes prioritized over fine-grained attention heads.

## Foundational Learning

1. **Multi-Head Attention (MHA)**: Each head has separate key, query, and value projections
   - Why needed: Provides fine-grained attention patterns for better model performance
   - Quick check: Verify each attention head has distinct projection matrices

2. **Grouped Query Attention (GQA)**: Groups heads share key/value projections
   - Why needed: Reduces memory bandwidth by sharing projections across multiple heads
   - Quick check: Count unique key/value projection matrices versus total heads

3. **Multi-Query Attention (MQA)**: All heads share single key/value projections
   - Why needed: Minimizes memory requirements by using one projection for all heads
   - Quick check: Confirm only one key and one value projection matrix exists

4. **GEMV vs GEMM Operations**: Matrix-vector versus matrix-matrix multiplications
   - Why needed: Different operation types have different memory access patterns and computational intensities
   - Quick check: Profile kernel types to identify dominant operation patterns

5. **Batch Size Impact**: Determines computational intensity and memory access patterns
   - Why needed: Small batches favor memory-efficient GEMV, large batches benefit from compute-efficient GEMM
   - Quick check: Vary batch size and observe shift between GEMV and GEMM dominance

## Architecture Onboarding

**Component Map**: Input Sequence -> Embedding Layer -> Attention Mechanism (MHA/GQA/MQA) -> Feed-Forward Network -> Output Layer

**Critical Path**: Input embedding → Attention computation → Feed-forward processing → Output generation, with attention mechanisms being the primary bottleneck

**Design Tradeoffs**: 
- MHA provides better performance but higher memory usage
- MQA/GQA reduce memory but may sacrifice some attention quality
- Larger batch sizes improve throughput but increase memory requirements
- Different deployment environments require different architectural optimizations

**Failure Signatures**:
- Memory-bound behavior: Low GPU utilization despite high memory bandwidth usage
- Compute-bound behavior: High GPU utilization but limited by arithmetic throughput
- Poor attention quality: Reduced model accuracy when using MQA instead of MHA
- Suboptimal batch sizing: Inefficient resource utilization when batch size doesn't match hardware capabilities

**First Experiments**:
1. Profile kernel composition across different batch sizes to identify GEMV vs GEMM dominance thresholds
2. Compare memory usage between MHA, GQA, and MQA implementations under identical workloads
3. Measure inference latency and throughput across different deployment scenarios (server vs edge)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different attention mechanisms (MHA, GQA, MQA) impact model performance across various deployment environments?
- Basis in paper: [explicit] The paper explicitly compares Multi-Head Attention (MHA), Grouped Query Attention (GQA), and Multi-Query Attention (MQA) and their effects on memory requirements and operations per byte.
- Why unresolved: While the paper shows that GQA and MQA reduce memory usage compared to MHA, it does not provide a comprehensive analysis of how these attention mechanisms affect overall model performance metrics like accuracy, latency, or energy efficiency across different deployment scenarios.
- What evidence would resolve it: Comparative studies measuring key performance indicators (accuracy, latency, energy consumption) for models using different attention mechanisms across various deployment environments (server, edge, mobile).

### Open Question 2
- Question: What are the optimal hardware configurations for different LLM architectures based on hyperparameter settings?
- Basis in paper: [explicit] The paper concludes that different hyperparameter settings and deployment environments lead to different optimal hardware configurations.
- Why unresolved: The study identifies that models exhibit different behaviors depending on hyperparameters and deployment environments, suggesting specialized hardware may be needed, but does not specify what those optimal configurations are.
- What evidence would resolve it: Systematic benchmarking of various hardware configurations (CPU, GPU, specialized accelerators) against different LLM architectures and hyperparameter settings to determine optimal pairings.

### Open Question 3
- Question: How does the choice of activation function impact model performance and computational efficiency?
- Basis in paper: [inferred] The paper discusses various activation functions (ReLU, ELU, GeLU, Swish) and their development, but does not provide empirical comparisons of their impact on model performance.
- Why unresolved: While the paper mentions several activation functions and their theoretical advantages, it does not present experimental results comparing their effects on model accuracy, training speed, or inference efficiency.
- What evidence would resolve it: Controlled experiments measuring model performance metrics (accuracy, training time, inference latency) when using different activation functions within the same model architecture.

## Limitations

- The analysis focuses on only four specific models, limiting generalizability across the broader LLM landscape
- Evaluation conducted on a single GPU platform makes it unclear how results translate to other hardware configurations or cloud-based deployments
- Lack of statistical significance testing for performance differences across hyperparameter settings
- Kernel composition analysis presented as relative percentages without absolute performance metrics

## Confidence

- Categorization of model execution into summarization and generation stages: High
- Observation that batch size affects GEMV vs GEMM operation dominance: Medium
- Claim that GQA and MQA reduce memory requirements compared to MHA: Medium
- Assertion that different deployment environments require specialized hardware: Low

## Next Checks

1. Replicate the kernel composition analysis across at least 10 additional models spanning different architectural families (RNN, transformer, state-space models) to test the generalizability of the observed patterns.

2. Conduct the same experiments on multiple GPU architectures (including consumer, data center, and mobile GPUs) to validate whether the compute-bound vs memory-bound conclusions hold across hardware platforms.

3. Perform ablation studies by systematically varying attention mechanisms while keeping all other hyperparameters constant to isolate the specific impact of attention choices on memory usage and runtime performance.