---
ver: rpa2
title: 'WorldValuesBench: A Large-Scale Benchmark Dataset for Multi-Cultural Value
  Awareness of Language Models'
arxiv_id: '2404.16308'
source_url: https://arxiv.org/abs/2404.16308
tags:
- answer
- value
- demographic
- question
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WorldValuesBench, a large-scale dataset derived
  from the World Values Survey Wave 7 containing over 20 million examples of demographic
  attributes paired with value questions and human responses. The dataset enables
  studying how language models can predict rating answers to value questions based
  on demographic contexts.
---

# WorldValuesBench: A Large-Scale Benchmark Dataset for Multi-Cultural Value Awareness of Language Models

## Quick Facts
- **arXiv ID**: 2404.16308
- **Source URL**: https://arxiv.org/abs/2404.16308
- **Reference count**: 0
- **Primary result**: Language models achieve Wasserstein 1-distance below 0.2 from human answer distributions on only 75-72% of questions

## Executive Summary
This paper introduces WorldValuesBench, a large-scale dataset derived from the World Values Survey Wave 7 containing over 20 million examples of demographic attributes paired with value questions and human responses. The dataset enables studying how language models can predict rating answers to value questions based on demographic contexts. A case study using 36 value questions and 48 demographic groups shows that even powerful models like GPT-3.5 Turbo and Mixtral-8x7B achieve Wasserstein 1-distance below 0.2 from human answer distributions on only 75% and 72% of questions respectively, indicating substantial room for improvement in multi-cultural value awareness.

## Method Summary
The authors construct WorldValuesBench by pairing demographic attributes (country, residential area, education level) from the World Values Survey Wave 7 with value questions and human responses. They derive a probe set (WVB-PROBE) with 8,280 examples across 36 questions and 48 demographic groups. Models are evaluated using prompt-based inference where demographic attributes are provided as conditioning context, and answers are generated in JSON format. Performance is measured using Wasserstein 1-distance between human and model answer distributions, with the goal of achieving distances below 0.2 to indicate good multi-cultural value awareness.

## Key Results
- GPT-3.5 Turbo achieves Wasserstein 1-distance below 0.2 on 75% of questions, while Mixtral-8x7B achieves this on 72% of questions
- Alpaca-7B and Vicuna-7B-v1.5 perform worse than baseline models when demographic attributes are provided
- Urban and rural groups show different performance patterns, with urban groups generally being easier for models to predict

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Demographic attributes serve as conditioning signals that enable models to generate culturally-aware value predictions.
- Mechanism: By providing demographic information (country, residential area, education level) in prompts, the model conditions its generation on these attributes, adjusting its prediction of value question answers accordingly.
- Core assumption: Language models can effectively use demographic context to modify their output distributions to better match human cultural patterns.
- Evidence anchors:
  - [abstract] "we define as generating a rating answer to a value question based on the available demographic attributes"
  - [section] "GPT-3.5 and Mixtral-8x7B benefit from the availability of demographic attributes"
  - [corpus] Weak - no direct corpus evidence found for demographic conditioning effectiveness
- Break condition: If the model fails to properly interpret or utilize demographic attributes in the prompt, or if cultural patterns are too complex for the model to capture through simple conditioning.

### Mechanism 2
- Claim: Wasserstein 1-distance provides a meaningful metric for comparing human and model answer distributions.
- Mechanism: By normalizing answers to [0,1] and computing the earth mover's distance between human and model distributions, the metric captures both the shape and location of the distributions.
- Core assumption: Wasserstein distance appropriately captures distributional differences for ordinal Likert-scale data when normalized.
- Evidence anchors:
  - [section] "To evaluate whether the model exhibits awareness of the human answer distribution, we compute the Wasserstein 1-distance"
  - [section] "A lower distance indicates better value awareness"
  - [corpus] Weak - no direct corpus evidence found for Wasserstein distance suitability for this specific application
- Break condition: If the ordinal nature of the data is not well-represented by the continuous metric, or if the normalization step distorts meaningful differences.

### Mechanism 3
- Claim: Large language models can predict culturally-specific value distributions when properly prompted.
- Mechanism: By framing the task as predicting what a specific person would answer based on their demographics, the model can leverage its knowledge of cultural patterns to generate appropriate distributions.
- Core assumption: Pre-trained LMs have internalized enough cultural knowledge to make reasonable predictions about value distributions.
- Evidence anchors:
  - [abstract] "even powerful models like GPT-3.5 Turbo and Mixtral-8x7B achieve Wasserstein 1-distance below 0.2 from human answer distributions on only 75% and 72% of questions respectively"
  - [section] "On merely 11.1%, 25.0%, 72.2%, and 75.0% of the questions, Alpaca-7B, Vicuna-7B-v1.5, Mixtral-8x7B-Instruct-v0.1, and GPT-3.5 Turbo can respectively achieve < 0.2 Wasserstein 1-distance"
  - [corpus] Weak - no direct corpus evidence found for LMs' ability to predict cultural value distributions
- Break condition: If the model lacks sufficient cultural knowledge in its training data, or if the demographic attributes are too coarse to capture nuanced cultural differences.

## Foundational Learning

- Concept: Wasserstein (Earth Mover's) Distance
  - Why needed here: To measure the similarity between human answer distributions and model-generated distributions in a way that accounts for the ordinal nature of the Likert scale data
  - Quick check question: Why is Wasserstein distance preferred over KL divergence for comparing these distributions?
  - Answer: KL divergence can't handle cases where the model assigns zero probability to events that occur in the human distribution, and doesn't account for the distance between different answer choices.

- Concept: Ordinal Data Handling
  - Why needed here: The value questions use Likert scales where answers have a natural order (e.g., "Very important" to "Not at all important")
  - Quick check question: How does treating ordinal data as interval data affect the analysis?
  - Answer: It allows for meaningful distance calculations between answers, assuming equal intervals between scale points, though this may not always be strictly valid.

- Concept: Demographic Conditioning in NLP
  - Why needed here: The task requires models to adjust their predictions based on demographic attributes to reflect cultural differences
  - Quick check question: What makes demographic conditioning different from other types of context in language models?
  - Answer: Demographic attributes provide structured, categorical information that should systematically influence the model's understanding of cultural context and value orientations.

## Architecture Onboarding

- Component map: Dataset construction → Probe set creation → Prompt engineering → Model inference → Distribution comparison → Evaluation
- Critical path: The most critical path is from prompt engineering through to evaluation, as this determines whether the task is well-defined and measurable
- Design tradeoffs: The choice of demographic attributes vs. value questions represents a key tradeoff between granularity and coverage; using more attributes increases specificity but may reduce the number of available examples
- Failure signatures: Models performing worse with demographic attributes (like Alpaca and Vicuna in the results) indicates failure to properly condition on provided context; uniformly poor performance across all questions suggests the task framing or metric may be flawed
- First 3 experiments:
  1. Run baseline models (uniform and majority) on the probe set to establish minimum performance thresholds
  2. Test a single strong model (like GPT-3.5 Turbo) on a small subset of questions with and without demographic attributes to validate the prompt format
  3. Perform ablation studies by removing one demographic attribute at a time to understand which attributes are most informative for the model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications or training strategies could improve multi-cultural value awareness in language models?
- Basis in paper: Explicit - The paper concludes that "recent, powerful LLMs exhibit substantial room for improvement on the multi-cultural value prediction task" and suggests this as future work
- Why unresolved: The paper only demonstrates the current limitations of existing models but does not propose or test specific architectural or training improvements to address these limitations
- What evidence would resolve it: Empirical results showing improved Wasserstein 1-distance scores when implementing specific architectural changes (like cultural context embeddings) or training strategies (like culturally-aware fine-tuning) on the WorldValuesBench dataset

### Open Question 2
- Question: How does model performance on multi-cultural value prediction vary across different demographic subgroups, and what are the underlying reasons for these variations?
- Basis in paper: Explicit - The paper notes that "Model awareness of the overall human answer distribution doesn't imply the awareness of the answer distribution of any demographic subgroups" and observes performance differences between urban and rural groups
- Why unresolved: While the paper identifies that performance varies across subgroups, it doesn't systematically analyze which demographic combinations pose the greatest challenges or investigate the reasons behind these variations
- What evidence would resolve it: Detailed analysis showing performance patterns across all demographic combinations, identifying which specific attribute combinations are most challenging, and analysis of whether these challenges stem from data sparsity, model bias, or other factors

## Limitations

- **Geographic and temporal coverage limitations**: The dataset represents only Wave 7 of the World Values Survey with 94,728 participants across 64 countries, potentially missing diverse cultural perspectives
- **Metric applicability concerns**: Wasserstein 1-distance application to ordinal Likert-scale data may not perfectly capture cultural value differences due to normalization assumptions
- **Model architecture bias**: The evaluation focuses on transformer-based models which may not be optimal for capturing culturally-dependent patterns

## Confidence

**High Confidence (8/10)**: Dataset construction methodology and basic evaluation framework are well-specified and reproducible
**Medium Confidence (6/10)**: Effectiveness of demographic conditioning is demonstrated but inconsistent results across models suggest limitations
**Low Confidence (4/10)**: Claim about Wasserstein 1-distance suitability for ordinal data has limited validation in the paper

## Next Checks

1. **Metric Validation Study**: Compare Wasserstein 1-distance against alternative metrics (KL divergence, ordinal-specific distance measures) on synthetic datasets with known cultural patterns
2. **Demographic Attribute Ablation Analysis**: Systematically remove individual demographic attributes from prompts and measure impact on model performance across different question types
3. **Cross-Cultural Generalization Test**: Evaluate models trained on one cultural region using test data from different regions to quantify cultural overfitting