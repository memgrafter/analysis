---
ver: rpa2
title: Understanding Aggregations of Proper Learners in Multiclass Classification
arxiv_id: '2410.22749'
source_url: https://arxiv.org/abs/2410.22749
tags:
- which
- multiclass
- learners
- learning
- dimension
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the limitations and possibilities of using
  simple aggregations of proper learners in multiclass classification. While binary
  classification has seen recent advances in optimal learning through aggregations
  of proper learners like ERM, multiclass classification faces a properness barrier
  where some learnable classes cannot be learned by any proper learner.
---

# Understanding Aggregations of Proper Learners in Multiclass Classification

## Quick Facts
- **arXiv ID:** 2410.22749
- **Source URL:** https://arxiv.org/abs/2410.22749
- **Reference count:** 7
- **Key outcome:** The paper establishes that while aggregations of proper learners can improve sample complexity for multiclass classification problems with finite Graph dimension, there exist fundamental limitations where certain learnable classes require infinite properness numbers.

## Executive Summary
This paper explores whether simple aggregations of proper learners (like ERM) can overcome the properness barrier in multiclass classification. The authors show that for classes with finite Graph dimension, majority voting over ERM learners achieves sample complexity of O((dG + ln(1/δ))/ε), improving upon ERM's complexity. However, they also prove that for general multiclass learning, some learnable classes cannot be learned to constant error by any aggregation of a finite number of proper learners, demonstrating fundamental limitations of these strategies.

## Method Summary
The paper investigates multiclass classification through the lens of properness barriers and aggregation strategies. The core approach involves converting multiclass problems to binary classification using characteristic functions, then applying various splitting schemes (Hanneke's deterministic scheme, Larsen's bagging, and Aden-Ali's three-way split) to ERM learners. These learners are combined via majority voting to create an improper learner. The authors establish both upper and lower bounds on the sample complexity of these majority voting schemes and demonstrate that certain learnable classes inherently require infinite properness numbers.

## Key Results
- For classes with finite Graph dimension dG, majority voting over ERM learners achieves sample complexity O((dG + ln(1/δ))/ε), improving upon ERM's O((dG ln(1/ε) + ln(1/δ))/ε)
- The authors establish a matching lower bound of Ω((dG + ln(1/δ))/ε for majority voting ERM on certain classes
- They prove the existence of learnable classes with DS dimension 1 that cannot be learned to constant error by any aggregation of a finite number of proper learners, demonstrating fundamental limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Properness barrier exists in multiclass classification where some learnable classes cannot be learned by any proper learner.
- Mechanism: The properness barrier arises because multiclass classification problems with finite DS dimension may have infinite Graph dimension, preventing proper learners from achieving optimal error rates.
- Core assumption: Multiclass classification fundamentally differs from binary classification in terms of learnability constraints.
- Evidence anchors:
  - [abstract]: "Multiclass learnability is known to exhibit a properness barrier: there are learnable classes which cannot be learned by any proper learner."
  - [section 1]: "Daniely and Shalev-Shwartz (2014) showed that there exist learnable multiclass problems which can only be learned by improper learners"
- Break condition: If a multiclass problem has finite DS dimension AND finite Graph dimension simultaneously, proper learners might overcome the barrier.

### Mechanism 2
- Claim: Aggregations of proper learners (like majority voting over ERM) can overcome the properness barrier for classes with finite Graph dimension.
- Mechanism: The aggregation strategy effectively creates an improper learner by combining multiple proper learners, achieving sample complexity of O((dG + ln(1/δ))/ε) which improves upon ERM's complexity.
- Core assumption: The aggregation strategy preserves the learnability properties while improving sample efficiency.
- Evidence anchors:
  - [abstract]: "We give a positive answer to this question for classes which have finite Graph dimension, dG... achieve sample complexity O((dG + ln(1/δ))/ε)"
  - [section 3.1]: "Our first set of main results concerns majorities of ERMs for classes with finite Graph dimension"
- Break condition: If the class has infinite Graph dimension, this aggregation strategy fails regardless of the number of proper learners used.

### Mechanism 3
- Claim: There exists a fundamental limit to learning with aggregations of proper learners - some learnable classes require infinite properness numbers.
- Mechanism: The properness number measures how many distinct hypotheses must be "pieced together" to construct a function. For certain classes with DS dimension 1 but arbitrary Graph dimension, any learner achieving constant error must have infinite properness number.
- Core assumption: The properness number provides a valid measure of the complexity of constructing classifiers from a hypothesis class.
- Evidence anchors:
  - [section 4]: "there exist learnable classification problems that cannot be learned by any combination of a finite number of proper learners"
  - [section 4]: "We now demonstrate the central result of the section: there exists an H ⊆ Y^X with DS dimension 1 and the property that any PAC learner A for H must have propH(A) = ∞"
- Break condition: If the hypothesis class structure allows finite properness number constructions, this limitation doesn't apply.

## Foundational Learning

- Concept: Graph dimension vs DS dimension distinction
  - Why needed here: The paper hinges on understanding why Graph dimension matters for aggregation strategies while DS dimension characterizes general learnability
  - Quick check question: Can you explain why a class might have finite DS dimension but infinite Graph dimension, and what this means for proper learning?

- Concept: Proper vs improper learners
  - Why needed here: The properness barrier and properness numbers are central to understanding the paper's main results
  - Quick check question: What's the key difference between proper and improper learners, and why does this distinction matter in multiclass classification?

- Concept: Majority voting and aggregation strategies
  - Why needed here: The paper's positive results rely on understanding how majority voting over proper learners can create effective improper learners
  - Quick check question: How does majority voting over ERM learners improve sample complexity compared to using a single ERM?

## Architecture Onboarding

- Component map: Realizable multiclass distribution -> Characteristic function reduction -> Splitting scheme -> ERM learners -> Majority voting -> Classifier
- Critical path: The most important theoretical contribution is the connection between Graph dimension and sample complexity of majority voting, which provides a concrete answer to whether simple aggregations can overcome the properness barrier.
- Design tradeoffs: The authors trade algorithmic simplicity (using ERM and majority voting) for theoretical guarantees, accepting that this approach won't work for all learnable multiclass problems.
- Failure signatures: The paper identifies two failure modes: (1) classes with infinite Graph dimension, (2) classes requiring infinite properness numbers for constant error.
- First 3 experiments:
  1. Implement the majority voting strategy over ERM for a synthetic multiclass problem with known Graph dimension and verify the O((dG + ln(1/δ))/ε) sample complexity
  2. Construct a class with finite DS dimension but infinite Graph dimension and demonstrate that majority voting fails to achieve constant error
  3. Build a hypothesis class with DS dimension 1 and verify that any learner achieving error < 1/2 requires infinite properness number

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the logarithmic factor in the upper bound for majority voters be eliminated for classes with finite Graph dimension?
- Basis in paper: [explicit] The paper shows that majority voters of ERM achieve sample complexity O((dG + ln(1/δ))/ε), improving upon ERM's O((dG ln(1/ε) + ln(1/δ))/ε). The authors ask whether this improvement is tight.
- Why unresolved: The lower bound demonstrates that certain classes require Ω((dG + ln(1/δ))/ε) samples, matching the upper bound, but it's unclear if this bound is achievable for all classes with finite Graph dimension.
- What evidence would resolve it: Either a matching lower bound for all classes with finite Graph dimension, or an improved upper bound showing better sample complexity for majority voters.

### Open Question 2
- Question: Is there a combinatorial dimension that characterizes the existence of optimal proper learners for multiclass problems?
- Basis in paper: [explicit] The authors note that while the Dual-Helly number characterizes optimality of proper learners for binary classification, it remains an open question whether such a dimension exists for multiclass learning.
- Why unresolved: The paper demonstrates fundamental limitations of proper learning in multiclass settings but doesn't identify what conditions would allow for optimal proper learners.
- What evidence would resolve it: Either a proof that no such dimension exists, or identification of a combinatorial dimension whose finiteness guarantees existence of optimal proper learners.

### Open Question 3
- Question: How do properness numbers scale with sample size or error parameters in learnable multiclass problems?
- Basis in paper: [inferred] The authors introduce properness numbers and show they must be infinite for certain learnable classes, but don't explore how they behave in cases where finite properness numbers are possible.
- Why unresolved: The paper only establishes the existence of problems requiring infinite properness numbers, leaving open the question of how properness numbers behave in more tractable settings.
- What evidence would resolve it: Analysis of specific learnable classes showing whether properness numbers can remain bounded as sample size or error tolerance changes.

## Limitations

- The results apply only to realizable multiclass classification problems, limiting applicability to more general settings
- The upper bounds depend on specific splitting schemes with potentially large universal constants that aren't explicitly characterized
- The infinite properness number construction requires careful verification and may not be constructive

## Confidence

- **Upper and lower bounds on majority voting ERM for classes with finite Graph dimension:** High
- **Properness barrier claims and infinite properness number result:** Medium
- **Novelty and broader impact:** Medium (weak corpus connections with average neighbor FMR = 0.49 and no citations)

## Next Checks

1. Construct explicit examples of classes with DS dimension 1 but arbitrary Graph dimension to verify the properness barrier claims
2. Implement and test the three-way splitting scheme to verify the O((dG + ln(1/δ))/ε) sample complexity bound
3. Reproduce the lower bound construction showing Ω((dG + ln(1/δ))/ε) samples are necessary for majority voting ERM