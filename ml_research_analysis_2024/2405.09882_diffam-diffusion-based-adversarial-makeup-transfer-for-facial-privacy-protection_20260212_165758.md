---
ver: rpa2
title: 'DiffAM: Diffusion-based Adversarial Makeup Transfer for Facial Privacy Protection'
arxiv_id: '2405.09882'
source_url: https://arxiv.org/abs/2405.09882
tags:
- makeup
- face
- adversarial
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiffAM addresses facial privacy protection against unauthorized
  face recognition systems by generating high-quality adversarial face images with
  makeup transferred from reference images. The method leverages diffusion models
  for precise, natural-looking makeup generation.
---

# DiffAM: Diffusion-based Adversarial Makeup Transfer for Facial Privacy Protection

## Quick Facts
- arXiv ID: 2405.09882
- Source URL: https://arxiv.org/abs/2405.09882
- Authors: Yuhao Sun; Lingyun Yu; Hongtao Xie; Jiaming Li; Yongdong Zhang
- Reference count: 40
- Key outcome: Achieves 12.98% improvement in attack success rate under black-box settings compared to state-of-the-art methods while maintaining superior visual quality

## Executive Summary
DiffAM addresses facial privacy protection against unauthorized face recognition systems by generating high-quality adversarial face images with makeup transferred from reference images. The method leverages diffusion models for precise, natural-looking makeup generation. It uses a two-stage approach: first removing makeup from reference images via text-guided diffusion, then transferring adversarial makeup back to source images using image-guided diffusion with a CLIP-based makeup loss and ensemble attack strategy. Experiments on CelebA-HQ and LADN datasets show DiffAM achieves a 12.98% improvement in attack success rate under black-box settings compared to state-of-the-art methods, while maintaining superior visual quality with higher PSNR and SSIM scores.

## Method Summary
DiffAM is a two-stage diffusion-based approach for facial privacy protection that generates adversarial face images with transferred makeup. In the first stage, a fine-tuned diffusion model removes makeup from reference images using text prompts in CLIP space to establish a deterministic alignment between makeup and non-makeup domains. In the second stage, another diffusion model transfers adversarial makeup back to source images using image-guided generation with a CLIP-based makeup loss that combines directional and pixel-level components. An ensemble attack strategy with multiple surrogate face recognition models optimizes for universal adversarial perturbations that transfer across different architectures. The method fine-tunes ADM diffusion models with CLIP-based losses and ensemble attack strategy to generate protected face images that achieve high attack success rates while maintaining natural appearance.

## Key Results
- Achieves 12.98% improvement in attack success rate under black-box settings compared to state-of-the-art methods
- Maintains superior visual quality with higher PSNR and SSIM scores than baseline approaches
- Demonstrates strong transferability across commercial APIs like Face++ and Aliyun
- Successfully transfers adversarial makeup across different FR model architectures including IR152, IRSE50, FaceNet, and MobileFace

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-guided makeup removal creates a deterministic alignment between makeup and non-makeup domains in CLIP space.
- Mechanism: The method uses a fine-tuned diffusion model with text prompts ("face with makeup" vs "face without makeup") to remove makeup from reference images. The resulting non-makeup images and their original makeup counterparts establish a clear directional vector in CLIP space (EI(y) - EI(Ë†y)) that guides subsequent adversarial makeup transfer.
- Core assumption: Makeup removal via text guidance is more reliable and deterministic than direct makeup generation via text, and the CLIP space provides meaningful semantic alignment for makeup styles.
- Evidence anchors:
  - [abstract]: "we first introduce a makeup removal module to generate non-makeup images utilizing a fine-tuned diffusion model with guidance of textual prompts in CLIP space"
  - [section 3.2.1]: "the difference between the latent codes of makeup and non-makeup images of reference image in CLIP space indicates the accurate direction from non-makeup domain to makeup domain"
  - [corpus]: Weak evidence - the corpus papers focus on different aspects of facial privacy protection and makeup transfer, with no direct discussion of deterministic domain alignment via makeup removal.

### Mechanism 2
- Claim: CLIP-based makeup loss with directional and pixel-level components enables precise control over adversarial makeup generation.
- Mechanism: The method combines a makeup direction loss (Ldir_MT) that aligns the direction between source and protected images with the pre-computed makeup direction from removal, and a pixel-level makeup loss (Lpx_MT) that controls the intensity and color of transferred makeup via histogram matching.
- Core assumption: CLIP space captures semantic makeup information effectively, and histogram matching provides sufficient pixel-level control for natural-looking results.
- Evidence anchors:
  - [abstract]: "a CLIP-based makeup loss along with an ensemble attack strategy is introduced to jointly guide the direction of adversarial makeup domain"
  - [section 3.2.2]: "By aligning image pairs in CLIP space, makeup direction loss controls precise direction for makeup transfer" and "a pixel-level makeup loss Lpx_MT[30] is employed to constrain makeup transfer distance in pixel space"
  - [corpus]: Weak evidence - corpus papers discuss makeup transfer and adversarial attacks but do not provide evidence for the specific combination of CLIP-based directional loss and pixel-level histogram matching.

### Mechanism 3
- Claim: Ensemble attack strategy with multiple surrogate models improves black-box transferability of adversarial makeup.
- Mechanism: The method fine-tunes the diffusion model using an ensemble of pre-trained FR models (e.g., IR152, IRSE50, FaceNet, MobileFace), optimizing for universal adversarial perturbations that transfer across different model architectures.
- Core assumption: The surrogate models represent a diverse enough set of FR architectures to capture universal vulnerabilities, and the ensemble objective produces transferable perturbations.
- Evidence anchors:
  - [abstract]: "a CLIP-based makeup loss along with an ensemble attack strategy is introduced to jointly guide the direction of adversarial makeup domain"
  - [section 3.2.2]: "We choose K pre-trained FR models with high recognition accuracy as surrogate models for fine-tuning, aiming to find the direction towards a universal adversarial makeup domain"
  - [corpus]: Weak evidence - while ensemble attacks are common in adversarial literature, the corpus papers do not specifically validate this approach for makeup-based attacks on FR systems.

## Foundational Learning

- Concept: Diffusion models and denoising processes
  - Why needed here: DiffAM relies on diffusion models for both makeup removal and adversarial makeup transfer, requiring understanding of forward/reverse diffusion, noise schedules, and sampling methods like DDIM.
  - Quick check question: What is the key difference between DDPM and DDIM sampling, and why is deterministic DDIM inversion important for makeup removal?

- Concept: CLIP space and cross-modal alignment
  - Why needed here: The method uses CLIP encoders to align images and text in a shared semantic space for both makeup removal direction computation and makeup transfer guidance.
  - Quick check question: How does the makeup direction loss (Ldir_MT) use CLIP space to ensure semantic consistency between source and protected images?

- Concept: Adversarial attack transferability
  - Why needed here: DiffAM aims for black-box attacks, requiring understanding of white-box vs black-box settings, surrogate models, and factors affecting transferability.
  - Quick check question: Why is an ensemble attack strategy used, and how does it improve transferability compared to single-model attacks?

## Architecture Onboarding

- Component map: Text-guided makeup removal module (diffusion model + CLIP text encoder + identity/perceptual losses) -> Image-guided adversarial makeup transfer module (diffusion model + CLIP image encoder + makeup direction loss + pixel-level makeup loss + ensemble attack loss + information preservation losses) -> Evaluation pipeline (FR models + commercial APIs)

- Critical path:
  1. Fine-tune diffusion model for makeup removal using text prompts and CLIP loss
  2. Generate non-makeup images from reference images
  3. Compute makeup direction vector in CLIP space
  4. Fine-tune second diffusion model for adversarial makeup transfer using CLIP-based makeup loss, ensemble attack loss, and preservation losses
  5. Generate protected face images and evaluate against FR models

- Design tradeoffs:
  - Text vs image guidance: Text provides coarse-grained control but is unreliable for fine makeup details; image guidance via makeup removal provides precise semantic alignment but requires additional computation
  - DDIM inversion steps: Fewer steps preserve more non-makeup information but may reduce generation quality; more steps improve quality but risk information loss
  - Ensemble size: Larger ensembles improve transferability but increase computational cost and risk overfitting

- Failure signatures:
  - Makeup artifacts or unnatural appearance: Indicates issues with makeup direction loss or pixel-level control
  - Poor attack success rates: Suggests problems with ensemble attack strategy or surrogate model selection
  - Identity preservation failure: Points to insufficient information preservation during fine-tuning

- First 3 experiments:
  1. Verify makeup removal quality: Test the text-guided makeup removal module on a small set of reference images and visually inspect the non-makeup outputs
  2. Validate makeup direction computation: Check that the computed makeup direction vectors in CLIP space are consistent and meaningful
  3. Test transfer learning: Fine-tune the adversarial makeup transfer model on a small dataset and evaluate the quality and attack success of generated images on a single FR model before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DiffAM change when using different CLIP models (e.g., CLIP-ViT-B-32 vs CLIP-ViT-L-14) for text and image encoding?
- Basis in paper: [inferred] The paper mentions using CLIP encoders but doesn't explore different CLIP model variants.
- Why unresolved: The choice of CLIP model could significantly impact the alignment between makeup and non-makeup domains, potentially affecting the quality and transferability of adversarial makeup.
- What evidence would resolve it: Comparative experiments using different CLIP models to evaluate their impact on attack success rates and image quality metrics like FID, PSNR, and SSIM.

### Open Question 2
- Question: What is the effect of using different makeup styles (e.g., natural vs. dramatic) on the attack success rate and transferability of DiffAM?
- Basis in paper: [inferred] The paper mentions evaluating robustness on different makeup styles but doesn't analyze the specific impact of style intensity or type.
- Why unresolved: Different makeup styles might have varying degrees of effectiveness in adversarial attacks, and understanding this relationship could lead to more targeted privacy protection strategies.
- What evidence would resolve it: Systematic experiments varying makeup style intensity and type, measuring attack success rates across different FR models and commercial APIs.

### Open Question 3
- Question: How does the choice of reference makeup image affect the transferability of adversarial makeup across different FR models?
- Basis in paper: [explicit] The paper mentions evaluating robustness on different makeup styles but doesn't explore the impact of specific reference images.
- Why unresolved: The reference makeup image serves as the source for adversarial makeup transfer, and its characteristics might influence how well the adversarial makeup transfers across different FR models.
- What evidence would resolve it: Experiments using multiple reference makeup images per target identity, analyzing the correlation between reference image features and attack success rates across different FR models.

### Open Question 4
- Question: Can DiffAM be extended to other types of adversarial attacks beyond targeted attacks, such as non-targeted attacks or transferability-based attacks?
- Basis in paper: [explicit] The paper focuses on targeted attacks but mentions non-targeted attacks in the problem formulation section.
- Why unresolved: Exploring other attack types could broaden the applicability of DiffAM and provide insights into its versatility as a privacy protection tool.
- What evidence would resolve it: Adapting the DiffAM framework to generate adversarial makeup for non-targeted attacks or transferability-based attacks, and evaluating its effectiveness compared to existing methods.

## Limitations

- Diffusion Model Architecture Details: The paper lacks specific details about the diffusion model architecture used, making faithful reproduction challenging and potentially leading to performance differences.
- Hyperparameter Sensitivity: While key hyperparameters are mentioned, the complete hyperparameter space is not fully specified, and the method's performance may be sensitive to these choices.
- Transferability Generalization: The paper demonstrates strong transferability on specific commercial APIs and chosen surrogate models, but generalizability to other FR systems or makeup styles is not thoroughly explored.

## Confidence

- **High Confidence**: The core two-stage diffusion approach and the use of CLIP-based losses for semantic alignment are well-supported by the experimental results and ablation studies. The improvement in ASR (12.98%) over state-of-the-art methods is substantial and convincingly demonstrated.

- **Medium Confidence**: The effectiveness of the ensemble attack strategy and the specific combination of CLIP-based directional loss with pixel-level histogram matching are supported by experiments, but could benefit from additional ablation studies isolating each component's contribution.

- **Low Confidence**: The paper claims the method produces "high-quality" and "natural-looking" results, but this is primarily supported by FID, PSNR, and SSIM metrics. A more comprehensive human perceptual study would strengthen these claims.

## Next Checks

1. **Architecture Fidelity Test**: Implement the diffusion models with varying architectural complexities (e.g., different U-Net depths, channel sizes) and evaluate how these changes affect makeup quality and attack success rate. This will help identify the minimum viable architecture and quantify the impact of architectural choices.

2. **Ablation of Loss Components**: Systematically remove or modify individual loss components (e.g., remove makeup direction loss, change histogram matching to other pixel-level losses) to quantify their individual contributions to final performance. This should include testing the sensitivity of results to loss weight values.

3. **Cross-Domain Transferability**: Test the method on a wider range of face recognition systems beyond the ones evaluated in the paper, including different architectures (e.g., ArcFace variants, ResNet-based models) and makeup styles not present in the training data. This will validate the claimed generalizability of the approach.