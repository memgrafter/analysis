---
ver: rpa2
title: 'ACDC: Autoregressive Coherent Multimodal Generation using Diffusion Correction'
arxiv_id: '2410.04721'
source_url: https://arxiv.org/abs/2410.04721
tags:
- diffusion
- generation
- acdc
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ACDC (Autoregressive Coherent Multimodal
  Generation using Diffusion Correction), a zero-shot method that combines autoregressive
  models (ARMs) and diffusion models (DMs) at inference time without fine-tuning.
  The approach addresses the problem of exponential error accumulation in ARMs by
  using DMs as local correctors through SDEdit, with a memory module implemented via
  LLM to preserve global context information.
---

# ACDC: Autoregressive Coherent Multimodal Generation using Diffusion Correction

## Quick Facts
- **arXiv ID**: 2410.04721
- **Source URL**: https://arxiv.org/abs/2410.04721
- **Reference count**: 40
- **Primary result**: A zero-shot method combining autoregressive models with diffusion models for coherent multimodal generation, achieving ↑10.4% frame consistency improvement

## Executive Summary
This paper introduces ACDC (Autoregressive Coherent Multimodal Generation using Diffusion Correction), a zero-shot approach that addresses error accumulation in autoregressive models by combining them with diffusion models at inference time. The method uses ARMs for global context generation and DMs as local correctors via SDEdit, with an LLM memory module that preserves global context by refining DM conditioning texts. ACDC is evaluated on coherent multi-frame story generation and autoregressive video generation tasks, demonstrating significant improvements in frame consistency and image quality metrics while remaining agnostic to specific ARM and DM architectures.

## Method Summary
ACDC is a zero-shot method that combines autoregressive models (ARMs) with diffusion models (DMs) at inference time without fine-tuning. The approach leverages ARMs for global context generation and memory-conditioned DMs for local correction of artifacts in multimodal tokens. A memory module based on LLMs dynamically adjusts the conditioning texts for the DMs, preserving crucial global context information. At inference, ARMs generate multimodal tokens, which are decoded via visual detokenizer, corrected by DMs through SDEdit with LLM-refined conditioning texts, then re-encoded for ARM continuation. The method uses DDIM sampling with CFG scale 7.5 for images and 5.0/1.0 for video frames.

## Key Results
- Achieves 10.4% improvement in frame consistency over baselines on coherent multi-frame story generation
- Demonstrates superior performance on video generation tasks with improved temporal coherence
- Shows effectiveness across multiple ARM architectures (Show-o, UIO-2XXL, LWM) and DM models (Stable Diffusion, AnimateDiff, VideoCrafter2)
- Maintains zero-shot approach without requiring fine-tuning of any component

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Combining ARMs and DMs at inference time reduces exponential error accumulation in ARMs while preserving their global context modeling capability.
- **Mechanism**: ARMs generate global context through causal attention, but their discrete token generation accumulates errors exponentially. ACDC uses DMs as local correctors via SDEdit to fix artifacts in each chunk of generated tokens without altering the ARM's global context understanding.
- **Core assumption**: The local correction from DMs doesn't disrupt the global context established by ARMs when properly conditioned.
- **Evidence anchors**: [abstract], [section 3], [corpus]

### Mechanism 2
- **Claim**: The LLM memory module preserves global context by refining DM conditioning texts for each correction step.
- **Mechanism**: The LLM causally summarizes previous prompts to generate refined conditioning texts for DMs, ensuring local corrections maintain awareness of the broader narrative context without requiring retraining.
- **Core assumption**: LLMs can effectively distill global context information into concise prompts that DMs can use for contextually appropriate corrections.
- **Evidence anchors**: [abstract], [section 3], [corpus]

### Mechanism 3
- **Claim**: Text-to-video DMs provide superior temporal coherence compared to autoregressive models for video generation.
- **Mechanism**: Video DMs inherently model temporal dynamics through their continuous latent space representation, enabling them to maintain frame-to-frame consistency that autoregressive models struggle with due to independent decoding.
- **Core assumption**: The temporal modeling capability of video DMs translates to better motion consistency when used for correction.
- **Evidence anchors**: [section 4.2], [section 4.2], [corpus]

## Foundational Learning

- **Concept**: Diffusion models and score matching
  - **Why needed**: Understanding how DMs work as score estimators is crucial for grasping the SDEdit correction mechanism
  - **Quick check**: What mathematical operation does a diffusion model perform when generating samples?

- **Concept**: Autoregressive modeling and error accumulation
  - **Why needed**: The core motivation for ACDC is addressing the exponential error accumulation in ARMs
  - **Quick check**: Why do errors in autoregressive models tend to compound rather than remain localized?

- **Concept**: Multimodal tokenization and detokenization
  - **Why needed**: ACDC operates on discrete tokens from ARMs and requires understanding how they map to continuous representations for DM correction
  - **Quick check**: How do multimodal ARMs typically represent visual information in their discrete token space?

## Architecture Onboarding

- **Component map**: ARM (global context generator) → Visual detokenizer → DM (local corrector with LLM memory) → Re-tokenizer → ARM (continuation)
- **Critical path**: Text prompt → ARM generation → Visual detokenization → LLM memory refinement → DM correction → Re-tokenization → ARM continuation
- **Design tradeoffs**: Zero-shot approach vs. potential performance gains from fine-tuning; inference-time computation vs. training-time optimization
- **Failure signatures**: Loss of global consistency despite local corrections; artifacts persisting after DM correction; semantic drift in corrected frames
- **First 3 experiments**:
  1. Apply ACDC to a simple story generation task with 3-4 frames to verify basic functionality
  2. Test ACDC with different t' values in SDEdit to find optimal perturbation level
  3. Compare ACDC performance with and without the LLM memory module on a small dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the choice of LLM architecture and size affect the performance of ACDC's memory module?
- **Basis in paper**: The paper mentions using gpt-4o-mini and gemma-9b-it for the memory module, but doesn't provide systematic comparisons across different LLM sizes or architectures.
- **Why unresolved**: The paper only briefly mentions that different LLMs can be used without extensive experimentation to determine optimal choices.
- **What evidence would resolve it**: Systematic experiments comparing ACDC performance with different LLM architectures (e.g., gemma-9b-it vs gpt-4o-mini vs larger models) and sizes across various tasks would clarify the impact on memory module effectiveness.

### Open Question 2
- **Question**: What is the optimal frequency of applying diffusion correction in autoregressive video generation?
- **Basis in paper**: The paper mentions controlling "the number of frames that the correction is applied" but only briefly explores this with ACDC #2 (correcting only first two frames) versus correcting all frames.
- **Why unresolved**: The paper doesn't provide comprehensive analysis of how correction frequency affects quality metrics or computational efficiency across different sequence lengths.
- **What evidence would resolve it**: Detailed ablation studies varying the correction frequency (e.g., every frame, every 2 frames, every 4 frames, only initial frames) across different video lengths would determine optimal correction strategies.

### Open Question 3
- **Question**: How does ACDC perform when applied to non-story or non-video generation tasks?
- **Basis in paper**: The paper focuses exclusively on story generation and autoregressive video generation, leaving unexplored how ACDC might perform in other multimodal generation scenarios.
- **Why unresolved**: The evaluation is limited to specific tasks without exploring the general applicability of ACDC to other multimodal generation domains.
- **What evidence would resolve it**: Applying ACDC to diverse multimodal generation tasks such as code generation with visual outputs, scientific visualization generation, or interactive design generation would demonstrate its broader applicability.

### Open Question 4
- **Question**: What is the computational overhead introduced by ACDC compared to baseline ARMs?
- **Basis in paper**: While the paper emphasizes that ACDC is "training-free and agnostic to the model class," it doesn't provide detailed analysis of inference-time computational costs or latency.
- **Why unresolved**: The paper focuses on qualitative and quantitative performance improvements but doesn't address the practical efficiency trade-offs of the correction process.
- **What evidence would resolve it**: Comprehensive benchmarking of inference time, memory usage, and GPU requirements for ACDC versus baseline ARMs across different hardware configurations would quantify the practical overhead.

## Limitations

- **Generalizability concerns**: Performance consistency across different architectural families of ARMs and DMs remains uncertain due to the zero-shot nature of the approach
- **Computational overhead**: The multi-stage correction process introduces significant inference-time computation that may not scale well with sequence length
- **Evaluation scope**: Current metrics may not fully capture qualitative aspects of coherence and aesthetic quality that the method aims to improve

## Confidence

- **High Confidence**: The core claim that combining ARMs with DMs at inference time can improve output quality without fine-tuning is well-supported by experimental results
- **Medium Confidence**: The claim that the LLM memory module is essential for preserving global context is supported by ablation studies, but the specific mechanism could benefit from more detailed analysis
- **Low Confidence**: The claim about superior temporal coherence when using video DMs for autoregressive video generation correction lacks direct comparative evidence against other temporal consistency methods

## Next Checks

1. **Architecture Transferability Test**: Implement ACDC with a different set of ARM and DM architectures (e.g., LLaVA for multimodal ARM and Kandinsky for DM) to verify that the approach generalizes beyond the specific models used in the paper. Measure whether similar improvements in frame consistency and quality metrics are observed.

2. **Ablation on Memory Module**: Conduct a systematic ablation study varying the size and type of LLM used for the memory module (e.g., GPT-3.5, LLaMA-2-7B, Claude Haiku) to determine the minimum effective model size and whether smaller, more efficient models can provide comparable performance.

3. **Long-Range Coherence Analysis**: Generate sequences of 20+ frames using ACDC and conduct both quantitative (CLIP-based consistency metrics) and qualitative (human evaluation of narrative coherence) assessments to determine whether the benefits persist over longer sequences or if error accumulation reemerges despite the correction mechanism.