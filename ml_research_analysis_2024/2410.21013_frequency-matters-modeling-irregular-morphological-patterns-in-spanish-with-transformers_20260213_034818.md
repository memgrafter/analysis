---
ver: rpa2
title: 'Frequency matters: Modeling irregular morphological patterns in Spanish with
  Transformers'
arxiv_id: '2410.21013'
source_url: https://arxiv.org/abs/2410.21013
tags:
- l-shaped
- verbs
- frequency
- morphological
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how transformer models learn morphomic
  patterns in Spanish verbs, specifically the L-shaped pattern where the first-person
  singular present indicative stem matches the present subjunctive stem. The authors
  formulate this as a morphological reinflection task and systematically manipulate
  the input frequency distributions of regular versus L-shaped verbs.
---

# Frequency matters: Modeling irregular morphological patterns in Spanish with Transformers

## Quick Facts
- arXiv ID: 2410.21013
- Source URL: https://arxiv.org/abs/2410.21013
- Reference count: 40
- Key outcome: Transformer models learn L-shaped morphomic patterns better than regular patterns, especially when L-shaped verbs are frequent, but rely on frequency-based pattern matching rather than abstract morphological competence.

## Executive Summary
This paper investigates how transformer models learn morphomic patterns in Spanish verbs, specifically the L-shaped pattern where the first-person singular present indicative stem matches the present subjunctive stem. The authors formulate this as a morphological reinflection task and systematically manipulate the input frequency distributions of regular versus L-shaped verbs. Four key findings emerge: models perform better on L-shaped verbs compared to regular verbs, especially in uneven frequency conditions; robust primacy effects are observed with better performance when the first source cell is within the L-shaped pattern; memorization becomes more prominent as the proportion of L-shaped verbs increases; and models tend to regularize L-shaped verbs when their consonant alternation pairs are rare or absent in training data.

## Method Summary
The study uses a morphological reinflection task with Spanish verbs from the UniMorph dataset, converting entries to two-source form-tag pairs for a transformer model with 4 layers, 4 attention heads, embedding size 256, and hidden layer size 1,024. The authors systematically manipulate the frequency distribution of L-shaped versus regular (NL-shaped) verbs across three conditions (10%L-90%NL, 50%L-50%NL, 90%L-10%NL) using character-level IPA transcriptions. Models are trained with Adam optimizer (learning rate 0.001) and evaluated using sequence accuracy and stem-only accuracy metrics across multiple randomized runs.

## Key Results
- Models perform better on L-shaped verbs compared to regular verbs, especially in uneven frequency conditions
- Robust primacy effects observed, with better performance when the first source cell is within the L-shaped pattern
- Memorization becomes more prominent as the proportion of L-shaped verbs increases
- Models tend to regularize L-shaped verbs when their consonant alternation pairs are rare or absent in training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer models capture morphomic patterns better than regular patterns when L-shaped verbs are frequent in training data.
- Mechanism: When L-shaped verbs dominate the training distribution, the model's attention heads align on the consistent stem alternation pattern, allowing it to memorize and generalize the L-shape pattern cells.
- Core assumption: The model's positional encoding and attention mechanism can detect the systematic mapping between source and target cells within the L-shape pattern.
- Evidence anchors:
  - [abstract] "Models perform better on L-shaped verbs compared to regular verbs, especially in uneven frequency conditions"
  - [section] "we find a learning advantage for L-shaped verbs in other conditions, suggesting the models might be learning specific characteristics of L-shaped verbs"
  - [corpus] Weak - no direct mention of frequency advantages in related work
- Break condition: If the training distribution becomes too skewed (e.g., 99% L-shaped), the model may overfit to L-shape and fail to generalize to novel L-shape forms with unseen consonant alternations.

### Mechanism 2
- Claim: Primacy effects in transformer models lead to better predictions when the first source cell is within the L-shaped pattern.
- Mechanism: The first token position receives higher attention weights during training, making the model more sensitive to the first source cell's morphological features, especially when that cell is part of the L-shape pattern.
- Core assumption: The model's self-attention architecture inherently biases toward earlier positions in the input sequence.
- Evidence anchors:
  - [abstract] "Robust primacy effects are observed, with models performing better when the first source cell is within the L-shaped pattern"
  - [section] "we observe a clear primacy effect in models' processing of L-shaped verbs, suggesting that the models are more influenced by the first source cell in making predictions"
  - [corpus] Weak - no direct mention of primacy effects in related work
- Break condition: If the model architecture is modified to eliminate positional bias (e.g., using relative positional encoding), the primacy effect may disappear.

### Mechanism 3
- Claim: Models rely on frequency-based pattern matching rather than acquiring true morphological competence for consonant alternations.
- Mechanism: The model learns to associate specific consonant pairs with high accuracy only when those pairs appear frequently in training data, defaulting to more frequent lemma consonants when encountering rare or unseen alternations.
- Core assumption: The model's embedding space clusters similar consonant patterns, but without abstract morphological rules, it falls back on frequency statistics.
- Evidence anchors:
  - [abstract] "Models tend to regularize L-shaped verbs when their consonant alternation pairs are rare or absent in training data"
  - [section] "The models perform worse for less frequent or unseen alternations (such as [s]-[g]) compared to more frequent alternations ([s]-[sk]). This indicates that the models have not fully acquired the abstract morphological patterns"
  - [corpus] Weak - no direct mention of regularization behavior in related work
- Break condition: If the model is trained with explicit morphological features or phonological rules, it may learn abstract patterns beyond frequency matching.

## Foundational Learning

- Concept: Paradigm Cell Filling Problem (PCFP)
  - Why needed here: Understanding PCFP is crucial because this work frames morphological reinflection as solving PCFP for L-shaped patterns, where speakers must generate missing inflected forms based on partial paradigms.
  - Quick check question: How does PCFP differ from standard morphological inflection tasks, and why is it particularly challenging for morphomic patterns?

- Concept: Morphomic patterns
  - Why needed here: The study focuses on L-shaped morphomic patterns, which are morphological patterns that exist independently of semantics or syntax, making them particularly interesting for computational modeling.
  - Quick check question: What distinguishes morphomic patterns from regular morphological patterns, and why might they be harder for models to learn?

- Concept: Type frequency vs token frequency
  - Why needed here: The paper manipulates type frequency of L-shaped verbs to study its effect on learnability, distinguishing between how many different verbs follow a pattern versus how often individual forms appear.
  - Quick check question: How might type frequency influence morphological productivity differently than token frequency in human language acquisition?

## Architecture Onboarding

- Component map: Spanish verb data -> Character-level IPA transcription -> Two-source form-tag pairs -> 4-layer transformer encoder-decoder -> Sequence accuracy/stem-only accuracy evaluation
- Critical path: Data preprocessing (IPA transcription and triple formation) → Model training with varying frequency distributions → Post-hoc analysis of primacy effects, memorization/generalization balance, and consonant pair sensitivity
- Design tradeoffs: Using character-level input captures phonological alternations but loses morphological boundaries; multi-source setup better captures L-shape patterns but increases input complexity; varying frequency distributions reveals learning dynamics but requires careful data sampling
- Failure signatures: Poor performance on L-shaped verbs indicates failure to capture morphomic patterns; systematic errors on rare consonant alternations suggest frequency-based regularization; inconsistent recency effects may indicate architectural limitations in handling positional information
- First 3 experiments:
  1. Train baseline model on natural 10%L-90%NL distribution and evaluate on held-out test set to establish performance baseline.
  2. Train model on 90%L-10%NL distribution to test whether increased exposure to L-shaped verbs improves their prediction accuracy.
  3. Conduct controlled experiment varying only the first source cell's position relative to the L-shape pattern to measure primacy effects systematically.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do transformer models' performance patterns on morphomic patterns compare to human speakers' acquisition of these patterns?
- Basis in paper: [inferred] The paper mentions that models' superior performance on regular verbs in the 10%L-90%NL condition "validates the results from Nevins et al. (2015)'s study" where human participants preferred NL-shaped responses over L-shaped responses 71.9% of the time. The authors state "This similarity in behavior between transformers and human participants motivates a deeper comparison."
- Why unresolved: The paper acknowledges this as a future direction but does not actually compare the models' performance to human experimental results from studies like Nevins et al. (2015) and Cappellaro et al. (2024).
- What evidence would resolve it: Direct comparison of model performance metrics with human experimental data on morphomic pattern acquisition, including error patterns and response preferences.

### Open Question 2
- Question: Why does the balanced 50%L-50%NL condition show better generalization performance compared to both skewed conditions (10%L-90%NL and 90%L-10%NL)?
- Basis in paper: [explicit] The results show "in the condition where the frequency distribution of verb types is equal (50%L-50%NL condition), we observe a slight advantage for generalization over memorization, with predicted probabilities for generalized stem-final consonant pairs being 0.113 higher than for memorized stem-final consonant pairs." This pattern is described as "non-linear."
- Why unresolved: The paper identifies this pattern but does not provide an explanation for why equal distribution leads to better generalization than either skewed condition.
- What evidence would resolve it: Analysis of model internal representations, attention patterns, or training dynamics across different frequency conditions to identify what changes in the balanced condition that enables better generalization.

### Open Question 3
- Question: What specific aspects of the transformer architecture make them prone to primacy effects when processing morphomic patterns?
- Basis in paper: [explicit] The paper observes "robust primacy effects" where models perform better when the first source cell is within the L-shaped pattern, and states "Transformers are prone to position bias, disproportionately focusing on specific token positions due to architectural constraints."
- Why unresolved: While the paper identifies the primacy effect, it does not investigate which architectural components (attention mechanisms, positional encodings, etc.) contribute to this bias.
- What evidence would resolve it: Systematic ablation studies of transformer components, analysis of attention weight distributions across positions, or comparison with alternative architectures that do not show the same primacy bias.

## Limitations

- The study uses character-level IPA transcriptions which may obscure morphological boundaries relevant for pattern learning
- Findings are limited to Spanish L-shaped morphomic patterns, restricting generalizability to other languages or morphological patterns
- The controlled manipulation of frequency distributions may not capture the complexity of how type and token frequencies interact in natural language use

## Confidence

**High Confidence**: The findings regarding primacy effects and frequency-based regularization of consonant alternations are well-supported by the experimental design and statistical analysis.

**Medium Confidence**: The claim that models perform better on L-shaped verbs compared to regular verbs, particularly in uneven frequency conditions, is supported by the data but requires careful interpretation given the significant performance gaps that remain.

**Low Confidence**: The paper's conclusions about what the models have "not fully acquired" in terms of abstract morphological patterns are inferential and would benefit from additional probing experiments.

## Next Checks

1. Probe for Abstract Pattern Knowledge: Conduct targeted experiments using novel L-shaped verbs with consonant alternations that were present in training but in different morphological contexts to test whether the model has learned abstract phonological rules.

2. Compare with Human Performance: Replicate the experimental paradigm with human subjects to establish whether the primacy effects and frequency-based learning patterns observed in transformers mirror human cognitive processing of morphomic patterns.

3. Cross-linguistic Generalization Test: Apply the same experimental design to a different language with L-shaped morphomic patterns (e.g., Italian or Portuguese) to determine whether the observed phenomena are language-specific or reflect general properties of transformer architectures.