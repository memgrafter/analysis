---
ver: rpa2
title: Reducing Oversmoothing through Informed Weight Initialization in Graph Neural
  Networks
arxiv_id: '2410.23830'
source_url: https://arxiv.org/abs/2410.23830
tags:
- initialization
- g-init
- weight
- graph
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a graph-informed weight initialization method
  (G-Init) for Graph Neural Networks (GNNs) that addresses the oversmoothing problem
  in deep architectures. The authors generalize Kaiming initialization to GNNs by
  analyzing theoretically the variance of forward and backward signals in convolutional
  GNNs, deriving formulas that account for graph topology.
---

# Reducing Oversmoothing through Informed Weight Initialization in Graph Neural Networks

## Quick Facts
- arXiv ID: 2410.23830
- Source URL: https://arxiv.org/abs/2410.23830
- Reference count: 40
- Authors: Dimitrios Kelesis; Dimitris Fotakis; Georgios Paliouras
- Key outcome: G-Init method enables deeper GCNs (up to 64 layers) to outperform standard initialization methods, achieving up to 74.04% accuracy on Cora and 49.75% on CiteSeer

## Executive Summary
This paper addresses the oversmoothing problem in deep Graph Neural Networks by introducing a graph-informed weight initialization method called G-Init. The authors generalize Kaiming initialization to GCNs by theoretically analyzing the variance of forward and backward signals in convolutional GNNs, deriving formulas that account for graph topology. By initializing weights with a zero-mean Gaussian distribution using standard deviation of 2di/√nl (where di is node degree), G-Init maintains higher variance in node representations and reduces oversmoothing. Experimental results on 8 node classification datasets demonstrate that G-Init enables deeper GCNs to achieve superior performance compared to standard initialization methods.

## Method Summary
G-Init is a graph-informed weight initialization method for GCNs that generalizes Kaiming initialization by incorporating graph topology information. The method initializes weights using a zero-mean Gaussian distribution with standard deviation σ = 2di/√nl, where di is a degree parameter (typically 2) and nl is the number of layers. This initialization maintains higher variance in node representations across layers, preventing the exponential reduction of signal magnitudes that leads to oversmoothing. The theoretical foundation involves analyzing the variance propagation through GCN layers and deriving bounds that account for the graph structure. The method is specifically designed for GCNs using asymmetric normalization and can be extended to other convolutional GNN architectures.

## Key Results
- G-Init enables deeper GCNs (up to 64 layers) to outperform standard initialization methods
- Achieves up to 74.04% accuracy on Cora and 49.75% on CiteSeer node classification tasks
- Excels in "cold start" scenarios where node features are only available for labeled nodes
- T-SNE visualizations confirm G-Init reduces oversmoothing by maintaining distinguishable node representations at greater depths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: G-Init reduces oversmoothing by initializing weight matrices with larger maximum singular values compared to Kaiming initialization.
- Mechanism: The circular law conjecture states that eigenvalues of normalized random matrices converge to a uniform distribution over the unit disk. By using a larger standard deviation (σstd = √(4/nl) vs √(2/nl)), G-Init places eigenvalues further from the origin, resulting in larger maximum singular values. These larger initial singular values influence the final singular values after training, which affects the convergence rate to the oversmoothing subspace.
- Core assumption: The initial largest singular values of weight matrices influence the final singular values after training convergence.
- Evidence anchors:
  - [abstract]: "These initial values may influence the largest singular values of the weight matrices after convergence, which, in turn, affect the extent to which oversmoothing can be reduced, based on Theorem 1."
  - [section]: "In fact, He et al. [5] propose an initialization using a zero-mean Gaussian with a standard deviation equal to √2/√nl. Based on Theorem 2, we conclude that that approach creates weight matrices, whose eigenvalues lie on a disk with a radius equal to √2, while G-Init does the same on a disk of a greater radius (i.e., √4). Hence, G-Init initializes the model with weight matrices having larger maximum singular values than those produced by Kaiming initialization."
- Break condition: If the training dynamics cause weight matrices to deviate significantly from their initial singular value distribution, or if other architectural components (like normalization layers) dominate the singular value behavior.

### Mechanism 2
- Claim: G-Init maintains higher variance in node representations across layers, preventing information collapse.
- Mechanism: The theoretical analysis derives bounds on the variance of forward signals and backward gradients. G-Init sets the initialization variance to 4/di which balances the variance across layers according to the derived conditions. This prevents the exponential reduction or magnification of signal magnitudes that occurs with standard initialization methods.
- Core assumption: The variance of node representations and gradients needs to be stabilized across layers to prevent oversmoothing.
- Evidence anchors:
  - [abstract]: "Our method aims to stabilize the variance and capitalize on its relationship with oversmoothing reduction."
  - [section]: "Variance stability: The first goal of our experimentation was to validate our theoretical results regarding the effect of G-Init on the variance within the model, before training. Specifically, we measured the average node variance as the signal flows forward in an 8-layer GCN and the average variance of the gradients flowing backward. Figure 1 presents the results for the Cora dataset. Both in the forward and in the backward pass, G-Init maintains a larger variance than the other methods."
- Break condition: If the graph topology changes significantly during training (e.g., in dynamic graphs) or if adaptive normalization layers are added that could override the variance stabilization effect.

### Mechanism 3
- Claim: G-Init enables deeper GCN architectures to perform effectively in cold start scenarios where unlabeled nodes lack feature information.
- Mechanism: By maintaining higher variance and reducing oversmoothing, G-Init allows deeper networks to recover meaningful information from distant nodes even when local features are missing. The experiments show that G-Init achieves superior performance compared to other methods in cold start scenarios where only labeled nodes have features.
- Core assumption: Deeper networks can recover information from distant nodes when local features are missing, but only if oversmoothing is controlled.
- Evidence anchors:
  - [abstract]: "Additionally, we demonstrate the benefits of G-Init, in the presence of the 'cold start' situation, where node features are available only for the labeled nodes."
  - [section]: "Performance under the 'cold start' scenario: In order to further explore the impact of G-Init on oversmoothing, we conducted a set of experiments aiming to highlight the value of using deep architectures that are not prone to oversmoothing. Specifically, we introduced a 'cold start' situation in the datasets (details about 'cold start' are available in Appendix C), by replacing the feature vectors of the unlabeled nodes with all-zero vectors."
- Break condition: If the graph structure doesn't provide meaningful long-range connections, or if the task requires primarily local feature information.

## Foundational Learning

- Concept: Graph Convolutional Networks and the message passing framework
  - Why needed here: G-Init is specifically designed for GCNs and related convolutional GNNs. Understanding how GCNs aggregate neighborhood information through message passing is essential to grasp why standard initialization methods fail and why G-Init's graph-informed approach works.
  - Quick check question: What is the difference between symmetric normalization (using 1/√(didj)) and asymmetric normalization (using 1/di) in GCNs, and how might this affect initialization?

- Concept: Variance propagation in deep neural networks
  - Why needed here: The core of G-Init's design is based on controlling the variance of signals flowing forward and gradients flowing backward through the network. This requires understanding how variance changes across layers and why uncontrolled variance leads to vanishing or exploding gradients.
  - Quick check question: Given a layer with input variance σ²_in, weight variance σ²_w, and activation function variance scaling factor c, what is the output variance σ²_out?

- Concept: Singular value decomposition and its relationship to matrix norms
  - Why needed here: The theoretical justification for G-Init's effectiveness relies on understanding how singular values affect oversmoothing. The circular law conjecture connects random matrix initialization to eigenvalue distributions, which relate to singular values through SVD.
  - Quick check question: How do the largest singular values of a weight matrix affect the amplification or attenuation of signals passing through a linear transformation?

## Architecture Onboarding

- Component map:
  - GCN layer: H^(l+1) = σ(ÂH^(l)W^(l))
  - Weight initialization: Zero-mean Gaussian with std = √(4/di/nl)
  - Variance stabilization: Theorems 4 and 6 provide bounds
  - Overfitting control: Standard L2 regularization (5×10^-4)
  - Optimization: Adam optimizer with learning rate 10^-3

- Critical path:
  1. Initialize weights using G-Init (zero-mean Gaussian, std = √(4/di/nl))
  2. Forward pass through GCN layers
  3. Compute loss (Cross Entropy for node classification)
  4. Backward pass with gradient computation
  5. Update weights using Adam optimizer
  6. Repeat for 200 epochs (1200 for Arxiv)

- Design tradeoffs:
  - Using di = 2 provides good balance between graph topology awareness and numerical stability, but di = 1.6 works better for Arxiv dataset
  - Higher variance initialization (G-Init) vs. lower variance (Kaiming) - tradeoff between information preservation and training stability
  - Deep architectures (up to 64 layers) enabled by G-Init vs. shallower networks with standard initialization

- Failure signatures:
  - Training instability (exploding/vanishing gradients) if di is too large
  - Oversmoothing still occurring if di is too small (approaching 1)
  - Performance degradation if graph topology is not well-represented by degree distribution

- First 3 experiments:
  1. Implement GCN with G-Init initialization and test on Cora dataset with depths 2, 4, 8, 16, 32, 64 layers to verify oversmoothing reduction
  2. Compare variance propagation in forward and backward passes for G-Init vs. Kaiming initialization on a small graph
  3. Test cold start scenario by zeroing out features of unlabeled nodes and measuring performance degradation across different initialization methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does G-Init initialization scale to extremely deep GNNs (100+ layers) and what are the theoretical limits of oversmoothing reduction?
- Basis in paper: [inferred] The paper shows G-Init works up to 64 layers but doesn't test deeper architectures. The theoretical analysis suggests potential for deeper models.
- Why unresolved: The experiments only tested up to 64 layers. Deeper architectures may have different convergence properties or encounter new challenges.
- What evidence would resolve it: Experimental results showing performance and variance stability on GNNs with 100+ layers, along with analysis of how singular values behave at extreme depths.

### Open Question 2
- Question: Can G-Init be extended to layer-specific initialization where each layer has a different standard deviation?
- Basis in paper: [explicit] The paper mentions plans to "explore layer-specific variations of G-Init" in the conclusion.
- Why unresolved: The current G-Init uses a uniform initialization across all layers. Different layers might benefit from different initialization scales based on their position in the network.
- What evidence would resolve it: Experimental comparison showing whether layer-specific initialization improves performance over the uniform approach, along with theoretical analysis of the benefits.

### Open Question 3
- Question: How does G-Init perform on graphs with heterogeneous node degrees and varying graph structures?
- Basis in paper: [inferred] The paper uses a fixed degree parameter (di=2 or 1.6 for Arxiv) across experiments but doesn't explore how varying graph structures affect performance.
- Why unresolved: The theoretical analysis assumes a general graph structure but doesn't examine how different degree distributions or graph topologies impact G-Init's effectiveness.
- What evidence would resolve it: Experiments on graphs with diverse degree distributions (scale-free, random, small-world) and theoretical analysis of how di should be adapted for different graph types.

## Limitations

- The generalizability of the circular law conjecture to GCN weight matrices remains theoretical and requires empirical validation across diverse graph structures
- The optimal choice of di = 2 as a universal parameter may not hold for graphs with highly irregular degree distributions or different sparsity patterns
- The long-term stability of G-Init's advantages beyond 64 layers is untested

## Confidence

**Low Confidence Claims:**
- The generalizability of the circular law conjecture (Theorem 2) to GCN weight matrices remains theoretical and requires empirical validation across diverse graph structures
- The optimal choice of di = 2 as a universal parameter may not hold for graphs with highly irregular degree distributions or different sparsity patterns
- The long-term stability of G-Init's advantages beyond 64 layers is untested

**Moderate Confidence Claims:**
- The variance stabilization mechanism shows consistent empirical support but relies on assumptions about graph regularity
- Performance improvements in cold-start scenarios are demonstrated but the specific dataset characteristics may influence the magnitude of benefits

## Next Checks

1. **Graph Structure Sensitivity**: Test G-Init on graphs with extreme degree distributions (e.g., power-law networks) to verify the robustness of the di parameter choice across different graph topologies

2. **Cross-Domain Generalization**: Evaluate G-Init on non-social network domains (e.g., molecular graphs, knowledge graphs) to assess whether the initialization benefits transfer beyond academic citation networks

3. **Integration with Other Techniques**: Combine G-Init with established oversmoothing mitigation methods like residual connections and JK networks to determine if multiplicative benefits exist or if certain combinations are redundant