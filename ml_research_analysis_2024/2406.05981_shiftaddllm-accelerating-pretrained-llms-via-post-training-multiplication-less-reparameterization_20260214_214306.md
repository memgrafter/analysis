---
ver: rpa2
title: 'ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less
  Reparameterization'
arxiv_id: '2406.05981'
source_url: https://arxiv.org/abs/2406.05981
tags:
- quantization
- arxiv
- llms
- ours
- reparameterization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ShiftAddLLM, a post-training reparameterization
  method for accelerating large language models (LLMs) by replacing costly multiplications
  with hardware-friendly shift-and-add operations. The approach uses binary coding
  quantization (BCQ) with group-wise scaling factors, reparameterizing multiplications
  into shifts and LUT-based additions.
---

# ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization

## Quick Facts
- arXiv ID: 2406.05981
- Source URL: https://arxiv.org/abs/2406.05981
- Reference count: 40
- Key outcome: ShiftAddLLM achieves 5.6 and 22.7 average perplexity improvements over 3-bit and 2-bit quantized LLMs respectively, with >80% memory and energy savings

## Executive Summary
ShiftAddLLM introduces a novel post-training reparameterization method that accelerates large language models by replacing expensive multiplications with hardware-friendly shift-and-add operations. The approach uses binary coding quantization with group-wise scaling factors to minimize both weight and activation reparameterization errors. By combining a multi-objective optimization method with an automated bit allocation strategy, ShiftAddLLM achieves significant performance improvements over existing 2-bit and 3-bit quantization methods while substantially reducing memory and energy consumption.

## Method Summary
ShiftAddLLM employs binary coding quantization (BCQ) with group-wise scaling factors to reparameterize LLM weights and activations, replacing multiplications with shifts and look-up-table-based additions. The method introduces a multi-objective optimization that jointly minimizes weight and activation reparameterization errors, and incorporates an automated bit allocation strategy that adapts bit-widths per layer based on sensitivity. This post-training approach achieves multiplication-less inference by decomposing matrix multiplications into sequences of bit-wise operations, shifts, and additions that are more efficient on modern hardware.

## Key Results
- Achieves 5.6 average perplexity improvement over most competitive 3-bit quantized LLMs
- Achieves 22.7 average perplexity improvement over most competitive 2-bit quantized LLMs
- Reduces memory and energy usage by over 80% compared to original models

## Why This Works (Mechanism)
The method works by transforming computationally expensive multiplications into sequences of simple bit operations, shifts, and additions that map efficiently to hardware. Binary coding quantization represents weights using binary codes, while group-wise scaling factors capture the remaining information through shared scaling coefficients. The multi-objective optimization simultaneously optimizes both weight and activation representations, ensuring that neither component becomes a bottleneck. The automated bit allocation strategy adapts precision per layer based on sensitivity, allocating more bits where they're most needed while conserving bits in less sensitive layers.

## Foundational Learning

**Binary Coding Quantization (BCQ)**: Represents weights using binary codes instead of floating-point values, dramatically reducing storage and computation requirements. Why needed: Enables the fundamental transformation from multiplications to bit operations. Quick check: Verify that binary representations can be decoded accurately using shift-and-add operations.

**Group-wise Scaling Factors**: Partitions weight matrices into groups and shares scaling factors within each group. Why needed: Preserves accuracy while reducing the number of unique scaling parameters. Quick check: Confirm that group size selection balances accuracy preservation with parameter efficiency.

**Multi-Objective Optimization**: Simultaneously optimizes weight and activation representations rather than optimizing them sequentially. Why needed: Prevents one component from becoming a bottleneck that limits overall performance. Quick check: Verify that joint optimization produces better results than sequential optimization.

**Automated Bit Allocation**: Dynamically assigns different bit-widths to different layers based on their sensitivity. Why needed: Allows precision to vary across the model, optimizing the trade-off between accuracy and efficiency. Quick check: Confirm that the allocation strategy correctly identifies and prioritizes sensitive layers.

## Architecture Onboarding

**Component Map**: Input weights -> Binary coding quantization -> Group-wise scaling -> Multi-objective optimization -> Automated bit allocation -> Shift-and-add reparameterization -> Output activations

**Critical Path**: The transformation pipeline from original weights through binary encoding to final shift-and-add operations represents the critical path for inference acceleration.

**Design Tradeoffs**: The method trades some accuracy for significant computational efficiency gains, balancing the granularity of group-wise scaling against parameter count, and determining optimal bit allocation across layers.

**Failure Signatures**: Poor performance may manifest as excessive quantization error in sensitive layers, inadequate scaling factor granularity leading to accuracy loss, or suboptimal bit allocation that underserves critical components.

**Three First Experiments**:
1. Apply binary coding quantization to a single layer and verify shift-and-add decoding accuracy
2. Test group-wise scaling with varying group sizes to find optimal balance between accuracy and parameter count
3. Implement the multi-objective optimization and compare against sequential optimization on a small model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of group size in the block-wise scaling factor design affect the trade-off between accuracy and latency?
- Basis in paper: The paper mentions using block-wise scaling factors to group 8 columns and 1/8 of the original rows to share a scaling factor, but does not explore the impact of different group sizes.
- Why unresolved: The paper only uses one group size and does not provide a sensitivity analysis or comparison with other group sizes.
- What evidence would resolve it: Experimental results comparing the accuracy and latency of ShiftAddLLM with different group sizes for the block-wise scaling factor design.

### Open Question 2
- Question: How does the multi-objective optimization approach compare to other methods that combine weight and activation objectives, such as those that use a weighted sum of the two objectives?
- Basis in paper: The paper introduces a multi-objective optimization approach that combines weight and activation objectives, but does not compare it to other methods that use a weighted sum of the two objectives.
- Why unresolved: The paper does not provide a comparison with other methods that use a weighted sum of the weight and activation objectives.
- What evidence would resolve it: Experimental results comparing the performance of ShiftAddLLM's multi-objective optimization approach to other methods that use a weighted sum of the weight and activation objectives.

### Open Question 3
- Question: How does the automated bit allocation strategy perform when the average bit budget per layer is very low, such as 1.5 bits?
- Basis in paper: The paper mentions that the automated bit allocation strategy can handle average bit budgets as low as 2 bits, but does not explore the performance at even lower bit budgets.
- Why unresolved: The paper only evaluates the automated bit allocation strategy with average bit budgets of 2 and 2.2 bits, and does not provide results for lower bit budgets.
- What evidence would resolve it: Experimental results evaluating the performance of ShiftAddLLM's automated bit allocation strategy with average bit budgets lower than 2 bits, such as 1.5 bits.

## Limitations
- Evaluation focuses primarily on perplexity metrics rather than broader task performance or real-world deployment scenarios
- Memory and energy savings claims lack detailed hardware-specific validation and empirical measurements
- Automated bit allocation strategy's effectiveness across diverse LLM architectures beyond those tested remains unproven

## Confidence

High confidence: The core methodology of replacing multiplications with shift-and-add operations through binary coding quantization is technically sound and well-supported by the results. The 5.6 and 22.7 perplexity improvements over 3-bit and 2-bit baselines are clearly demonstrated across multiple models and tasks.

Medium confidence: The claimed memory and energy reductions by over 80% are based on theoretical calculations rather than empirical hardware measurements, which could vary significantly depending on the specific inference hardware and implementation details.

Low confidence: The automated bit allocation strategy's effectiveness across diverse LLM architectures beyond those tested remains unproven, as does the method's robustness when applied to models trained with different objectives or pretraining approaches.

## Next Checks
1. Implement ShiftAddLLM on actual hardware accelerators (TPUs, GPUs, or custom ASICs) to verify the claimed memory and energy savings through empirical measurements rather than theoretical estimates.

2. Test the automated bit allocation strategy on a broader range of LLM architectures, including those with different attention mechanisms, activation functions, or training objectives to assess generalization beyond the current evaluation set.

3. Conduct comprehensive task-specific evaluations beyond perplexity, including fine-tuning stability, zero-shot task performance, and robustness to distribution shifts to validate practical deployment readiness.