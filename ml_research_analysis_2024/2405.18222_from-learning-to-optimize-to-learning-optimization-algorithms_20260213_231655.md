---
ver: rpa2
title: From Learning to Optimize to Learning Optimization Algorithms
arxiv_id: '2405.18222'
source_url: https://arxiv.org/abs/2405.18222
tags:
- learning
- algorithm
- algorithms
- bfgs
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of making learned optimization
  algorithms (L2O) widely applicable beyond their training distribution, a key limitation
  of current L2O methods. The authors identify five key principles that classical
  optimization algorithms follow, including equivariance to geometric transformations,
  adaptation to individual problems, and robustness to scaling.
---

# From Learning to Optimize to Learning Optimization Algorithms

## Quick Facts
- arXiv ID: 2405.18222
- Source URL: https://arxiv.org/abs/2405.18222
- Authors: Camille Castera; Peter Ochs
- Reference count: 40
- Key outcome: Learning-enhanced BFGS algorithm outperforms vanilla BFGS on both training and out-of-distribution test problems without retraining

## Executive Summary
This paper addresses the fundamental limitation of learned optimization algorithms (L2O) - their inability to generalize beyond their training distribution. The authors identify five key principles that classical optimization algorithms follow, including equivariance to geometric transformations, adaptation to individual problems, and robustness to scaling. They propose a general design pipeline that enforces these principles through careful design of the oracle, model, update function, and storage components. As a demonstration, they enhance the BFGS quasi-Newton method with a learned model that predicts the search direction while preserving BFGS's theoretical properties. Experiments show their learned BFGS consistently outperforms vanilla BFGS on both training and out-of-distribution test problems, including logistic regression and ridge regression on real-world datasets, without requiring retraining.

## Method Summary
The paper proposes a general L2O pipeline that enforces five key principles: equivariance to geometric transformations, adaptation to individual problems, robustness to scaling, and preservation of classical algorithm properties. The approach involves designing an oracle that collects problem information in an equivariant manner, a model that predicts modifications while respecting permutation equivariance, an update function that maintains classical algorithm properties, and storage that tracks necessary state. As an application, they enhance BFGS by learning to predict a modified search direction while preserving the BFGS update structure and positive definiteness. The model is trained on quadratic functions and tested on logistic regression, ridge regression, and higher-dimensional problems without retraining.

## Key Results
- Learned BFGS consistently outperforms vanilla BFGS on both training and out-of-distribution test problems
- The algorithm achieves better performance than other L2O baselines on real-world datasets
- Novel initialization strategy starting from exact BFGS dramatically stabilizes training
- The learned algorithm generalizes from n=100 to n=500 dimensions with only 216 parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Preserving equivariance to geometric transformations enables generalization beyond training distributions.
- **Mechanism**: Oracle outputs differences instead of absolute values for translation invariance, model uses coordinate-wise operations with averaging for permutation equivariance, and update maintains equivariance through careful matrix construction.
- **Core assumption**: Equivariance properties ensure consistent performance across different representations of the same optimization problem.
- **Evidence anchors**: [abstract]: "identify key principles that classical algorithms obey... enabling a synergy between classical optimization and L2O" [section]: "we identify specific key equivariance properties and study how to handle them in every step of a general L2O pipeline"
- **Break condition**: If transformations during testing differ significantly from training, or if equivariance requirements conflict with computational constraints.

### Mechanism 2
- **Claim**: Learning to enhance classical algorithms preserves theoretical convergence guarantees while adding learned improvements.
- **Mechanism**: Learned model predicts modified search direction while maintaining BFGS update structure, preserving positive definiteness and secant equation properties.
- **Core assumption**: Classical algorithm properties are robust to learned components that preserve mathematical structure.
- **Evidence anchors**: [abstract]: "demonstrate the success of these novel principles by designing a new learning-enhanced BFGS algorithm and provide numerical experiments evidencing its adaptation to many settings" [section]: "we use it precisely at this stage. We use a model Mθ that predicts directly a different yk than that of BFGS"
- **Break condition**: If learned component introduces instability violating convergence assumptions.

### Mechanism 3
- **Claim**: Initializing the learned model to match the classical algorithm stabilizes learning.
- **Mechanism**: Initialize final layer weights to zero and linear layer to represent classical algorithm behavior, starting from stable known-good configuration.
- **Core assumption**: Starting from working classical algorithm provides better training trajectory than random initialization.
- **Evidence anchors**: [section]: "By initializing the weights of the last FC layer to zero and the linear layer to be (0, 0, 0, 0, 1, 0) one can check that our algorithm is initialized to exactly coincide with BFGS before training" [abstract]: "A novel initialization strategy that starts the model as exact BFGS is shown to dramatically stabilize training"
- **Break condition**: If initialization becomes incompatible with architecture or classical algorithm performs poorly.

## Foundational Learning

- **Concept**: Equivariance in machine learning
  - Why needed here: Ensures learned optimization algorithm behaves consistently under geometric transformations, enabling generalization to out-of-distribution problems.
  - Quick check question: Can you explain why a translation-equivariant optimization algorithm would perform the same on f(x) and f(x-v) for any vector v?

- **Concept**: Quasi-Newton methods and the secant equation
  - Why needed here: Understanding how BFGS approximates inverse Hessian using gradient differences is crucial for knowing where to insert learned components while preserving convergence properties.
  - Quick check question: What is the secant equation that quasi-Newton methods aim to satisfy, and why is it important for convergence?

- **Concept**: Under-parameterization vs over-parameterization in learning
  - Why needed here: LOA approach operates in under-parameterized regime where model size is much smaller than problem dimension, requiring careful design rather than memorization.
  - Quick check question: How does the under-parameterized setting affect the generalization capabilities of a learned optimization algorithm compared to an over-parameterized one?

## Architecture Onboarding

- **Component map**: Oracle (C) → Model (Mθ) → Update (U) → Storage (S)
- **Critical path**: Problem input → Oracle transformation → Model prediction → Update computation → State storage → Next iteration
- **Design tradeoffs**: Permutation coordinate interactions vs computational cost, theoretical properties vs model expressivity, initialization stability vs training flexibility
- **Failure signatures**: Loss of positive definiteness in matrix updates, violation of equivariance properties leading to inconsistent behavior, numerical instability in matrix operations, poor generalization indicated by performance degradation on test problems
- **First 3 experiments**:
  1. Verify equivariance properties by applying transformations to test problems and checking if algorithm behavior is preserved
  2. Test convergence on ill-conditioned quadratic problems to validate theoretical guarantees
  3. Evaluate out-of-distribution performance on logistic regression and ridge regression problems without retraining

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed principles generalize to stochastic optimization algorithms beyond deterministic ones?
- Basis in paper: [explicit] The authors explicitly state in the conclusion that "An important next step is to adapt the principles and the recipe to the case of stochastic algorithms."
- Why unresolved: The paper only demonstrates the principles on deterministic optimization problems. Stochastic settings introduce additional challenges like noise in gradient estimates and require different design considerations.
- What evidence would resolve it: Empirical demonstrations of the principles applied to stochastic algorithms like SGD with momentum or Adam, showing comparable performance gains on stochastic optimization tasks.

### Open Question 2
- Question: What is the theoretical convergence guarantee when the learned model Mθ produces predictions that occasionally violate the positive definiteness of Bk?
- Basis in paper: [inferred] Theorem 2 assumes "for all k∈N, Bk is positive definite with eigenvalues lower and upper-bounded by c, C > 0" but this is based on strong assumptions about the model's output that aren't enforced by design.
- Why unresolved: The paper notes that Theorem 2 is "more restrictive than usual convergence theorems" and that "the failure can only come from the learning part of the algorithm." No analysis is provided for what happens when these assumptions are violated.
- What evidence would resolve it: Theoretical analysis showing convergence rates when the eigenvalues of Bk are not uniformly bounded, or empirical evidence showing performance degradation when the model occasionally produces destabilizing predictions.

### Open Question 3
- Question: How does the performance of the learned BFGS algorithm scale with problem dimension n when p (number of parameters) remains fixed at 216?
- Basis in paper: [explicit] The paper shows experiments in dimensions n=100 and n=500, noting that "p = 216 is much smaller than 20 × 100 = 2000" and that "even for a single problem p < 500."
- Why unresolved: While the paper demonstrates successful generalization from n=100 to n=500, it doesn't explore whether this scaling continues to hold for much larger dimensions, or what the theoretical limits are.
- What evidence would resolve it: Systematic experiments across a wider range of dimensions (e.g., n=1000, n=5000, n=10000) and/or theoretical analysis of the VC dimension or generalization bounds for the model as n increases.

## Limitations
- Limited generalizability to non-convex, non-smooth, or discrete optimization problems
- Computational overhead may offset performance gains on problems where classical BFGS already performs well
- Initialization strategy's dramatic impact may not generalize to other classical algorithms or problem families

## Confidence

**High Confidence**: The mechanism of preserving equivariance properties for generalization has strong theoretical grounding in equivariant machine learning literature and is directly observable in the algorithm's behavior on transformed problems.

**Medium Confidence**: The approach of learning to enhance classical algorithms while preserving their theoretical properties is well-supported by the experimental results, though the long-term stability and convergence behavior on more complex problems requires further validation.

**Low Confidence**: The initialization strategy's dramatic impact on training stability is primarily demonstrated on the specific BFGS enhancement and may not generalize to other classical algorithms or different problem families.

## Next Checks
1. **Equivariance Verification**: Systematically apply various geometric transformations (translations, rotations, permutations) to test problems and verify that the learned BFGS algorithm produces consistent optimization trajectories, quantifying the degree of equivariance achieved.

2. **Computational Overhead Analysis**: Measure the wall-clock time per iteration for the learned BFGS versus vanilla BFGS across different problem dimensions and compare the total optimization time required to reach target accuracy levels.

3. **Generalization to Non-Convex Problems**: Evaluate the learned BFGS algorithm on benchmark non-convex optimization problems (e.g., Rosenbrock function, neural network training) to assess whether the equivariance and adaptation principles remain beneficial outside the convex setting.