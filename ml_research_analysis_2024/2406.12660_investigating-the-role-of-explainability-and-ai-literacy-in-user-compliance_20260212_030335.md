---
ver: rpa2
title: Investigating the Role of Explainability and AI Literacy in User Compliance
arxiv_id: '2406.12660'
source_url: https://arxiv.org/abs/2406.12660
tags:
- compliance
- literacy
- users
- participants
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how explainability and AI literacy influence
  user compliance with AI recommendations. An experiment with 562 participants tested
  two types of explainable AI (XAI) while measuring compliance, mental models, and
  AI literacy.
---

# Investigating the Role of Explainability and AI Literacy in User Compliance

## Quick Facts
- arXiv ID: 2406.12660
- Source URL: https://arxiv.org/abs/2406.12660
- Authors: Niklas Kühl; Christian Meske; Maximilian Nitsche; Jodie Lobana
- Reference count: 0
- This study examines how explainability and AI literacy influence user compliance with AI recommendations.

## Executive Summary
This study investigates how explainable AI (XAI) and AI literacy affect user compliance with AI recommendations through changes in mental models. An experiment with 562 participants tested two types of XAI while measuring compliance, mental models, and AI literacy. Results show that XAI increases compliance and changes mental models, while AI literacy directly and indirectly affects compliance through mental models. However, for users with low AI literacy, the type of XAI does not impact compliance. These findings suggest the need for personalized explanations tailored to users' backgrounds and experience, highlighting mental models as a key factor in user-AI interaction design.

## Method Summary
The study employed an online experiment with 562 participants who completed three phases: training without AI support, baseline with AI support but no explanations, and treatment with AI support and XAI. Participants performed an age estimation task using an AI model, with two XAI conditions tested (probability distribution charts and LIME-based image overlays). The researchers measured compliance through prediction accuracy, assessed mental models before and after treatment, and evaluated AI literacy. Structural equation modeling was used to analyze the relationships between variables and test mediation effects.

## Key Results
- XAI increases user compliance and changes mental models of AI
- AI literacy directly affects compliance and indirectly through mental model changes
- For low AI literacy users, XAI type does not impact compliance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XAI increases user compliance by altering mental models of AI
- Mechanism: Providing explanations gives users new information about how AI works, which updates their internal representation (mental model). A better mental model leads to greater trust and acceptance of AI recommendations.
- Core assumption: Users can integrate new explanatory information into their existing mental models and that this updated model affects their decision-making behavior.
- Evidence anchors:
  - [abstract] "Results show that XAI increases compliance and changes mental models"
  - [section] "the introduction of XAI changes users' MMs of AI"
  - [corpus] Weak - corpus neighbors don't directly discuss mental model changes
- Break condition: If users cannot process or integrate the explanations due to low AI literacy, the mental model update won't occur and compliance won't improve.

### Mechanism 2
- Claim: AI literacy directly affects compliance and moderates XAI effectiveness
- Mechanism: Users with higher AI literacy have better baseline understanding of AI capabilities and limitations. This allows them to better evaluate explanations and adjust their trust accordingly. Low AI literacy users cannot effectively use explanations to update their mental models.
- Core assumption: AI literacy provides the cognitive foundation needed to interpret and benefit from XAI.
- Evidence anchors:
  - [abstract] "AI literacy directly and indirectly affects compliance through mental models"
  - [section] "users with low AI literacy, the type of XAI does not impact compliance"
  - [corpus] Weak - corpus neighbors don't directly discuss AI literacy moderation effects
- Break condition: If explanations are too complex or use technical language that low AI literacy users cannot understand, no compliance improvement occurs regardless of explanation type.

### Mechanism 3
- Claim: Mental models mediate the relationship between AI literacy and compliance
- Mechanism: AI literacy shapes mental models, which then influence compliance. This creates an indirect pathway where AI literacy affects compliance through mental model changes rather than directly.
- Core assumption: Mental models are the psychological mechanism through which AI literacy influences behavior.
- Evidence anchors:
  - [abstract] "relationships between AI literacy, XAI, and users' compliance are mediated by the users' mental model of AI"
  - [section] "the relationships between AI literacy XAI and users' compliance are mediated by the users' mental model"
  - [corpus] Weak - corpus neighbors don't discuss mediation effects
- Break condition: If mental models don't actually influence decision-making behavior, the mediation pathway breaks down.

## Foundational Learning

- Concept: Mental models
  - Why needed here: The study's core mechanism relies on mental models as the psychological construct that changes with XAI and affects compliance. Understanding what mental models are and how they form is essential to grasp the study's theoretical framework.
  - Quick check question: What is a mental model and how does it differ from general knowledge or beliefs?

- Concept: Explainable AI (XAI) types and methods
  - Why needed here: The study compares two different XAI approaches (probability distributions vs. image overlays). Understanding these methods and their differences is crucial for interpreting why different types might affect different user groups differently.
  - Quick check question: What are the key differences between pre-model, in-model, and post-model XAI approaches?

- Concept: Mediation analysis in statistics
  - Why needed here: The study uses structural equation modeling to test mediation effects. Understanding what mediation means statistically is essential for interpreting the results about how AI literacy affects compliance through mental models.
  - Quick check question: In a mediation model, what does it mean when we say variable X affects Y through mediator M?

## Architecture Onboarding

- Component map: Image → AI prediction → Participant guess → XAI explanation (for treatment groups) → Mental model assessment → Compliance measurement → Data analysis
- Critical path: Image → AI prediction → Participant guess → XAI explanation (for treatment groups) → Mental model assessment → Compliance measurement → Data analysis
- Design tradeoffs: The study chose age estimation as a simple, domain-agnostic task to isolate XAI effects, but this limits generalizability to complex domain-specific tasks. The MORPH dataset was chosen for quality over IMDB-WIKI despite smaller size.
- Failure signatures: If compliance doesn't change with XAI, it could indicate either explanations are ineffective or the mental model construct is not measuring what we think. If AI literacy shows no effect, it might mean the literacy measures are not capturing relevant skills or the task is too simple to require them.
- First 3 experiments:
  1. Test age prediction AI accuracy on holdout set to ensure model reliability
  2. Pilot XAI explanations with small user group to check comprehension
  3. Validate mental model questionnaire with known AI experts vs novices to establish construct validity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can personalized explanations be effectively designed for users with varying levels of AI literacy to maximize compliance?
- Basis in paper: [explicit] The paper suggests the need for personalized explanations (PXAI) tailored to users' backgrounds and experience, especially for users with low AI literacy who do not benefit from standard explanations.
- Why unresolved: While the paper identifies the need for PXAI, it does not provide specific design guidelines or methods for creating such personalized explanations that cater to different levels of AI literacy.
- What evidence would resolve it: Empirical studies testing various PXAI designs across different user groups with varying AI literacy levels, measuring compliance and user satisfaction.

### Open Question 2
- Question: What are the long-term effects of using XAI on users' compliance and mental models over time?
- Basis in paper: [inferred] The study examines the immediate effects of XAI on compliance and mental models, but does not address potential changes in these effects over extended periods of interaction with AI systems.
- Why unresolved: Longitudinal studies are needed to understand how users' compliance and mental models evolve with continued exposure to XAI, which is not covered in the current research.
- What evidence would resolve it: Longitudinal studies tracking user compliance and mental model changes over time with regular interaction with XAI systems.

### Open Question 3
- Question: How do different types of XAI explanations affect user trust and reliance on AI recommendations in critical domains such as healthcare?
- Basis in paper: [explicit] The paper mentions the importance of XAI in critical AI-based information systems but does not explore the specific impact of XAI types on trust and reliance in high-stakes domains.
- Why unresolved: The study focuses on general compliance but does not delve into the nuanced effects of XAI on trust and reliance in domains where the consequences of AI decisions are significant.
- What evidence would resolve it: Studies in specific critical domains like healthcare, comparing the effects of different XAI explanations on user trust and reliance, with controlled experiments and real-world applications.

## Limitations

- The study's use of age estimation as a task may not capture the complexity of real-world AI decision-making scenarios
- Reliance on self-reported measures of AI literacy and mental models may not accurately capture true understanding
- The finding that XAI type doesn't matter for low AI literacy users could result from measurement issues rather than true equivalence

## Confidence

*High Confidence:* The finding that XAI increases compliance and changes mental models is supported by direct measurement and consistent with existing literature on the relationship between explanations and user behavior. The structural equation modeling approach provides robust statistical support for these core relationships.

*Medium Confidence:* The mediation effect of mental models between AI literacy and compliance, while statistically significant, requires careful interpretation given potential measurement issues with the mental model construct. The moderation effect of AI literacy on XAI effectiveness is plausible but the boundary conditions need further exploration.

*Low Confidence:* The claim that different XAI types have no differential impact on low AI literacy users is based on null findings that could result from insufficient statistical power or measurement issues rather than true equivalence.

## Next Checks

1. **Replicate with domain-specific tasks**: Conduct the same experiment using healthcare diagnosis or financial decision-making tasks to test whether the mental model and literacy effects hold when consequences are more significant and domain knowledge is more relevant.

2. **Longitudinal tracking of explanation effectiveness**: Follow participants over multiple sessions to determine whether the initial compliance boost from XAI persists or whether users develop fatigue or dependency on explanations over time.

3. **Think-aloud protocol validation**: Pair the survey-based mental model assessment with concurrent think-aloud protocols during the XAI interaction phase to directly observe how users process explanations and whether their reported mental models match their actual reasoning processes.