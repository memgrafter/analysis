---
ver: rpa2
title: Variational quantization for state space models
arxiv_id: '2404.11117'
source_url: https://arxiv.org/abs/2404.11117
tags:
- time
- hidden
- series
- proposed
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel time series forecasting model that
  combines discrete state space hidden Markov models with neural network architectures
  inspired by vector quantized variational autoencoders. The approach introduces a
  variational discrete posterior distribution of latent states given observations
  and uses a two-stage training procedure to alternatively train the parameters of
  latent states and emission distributions.
---

# Variational quantization for state space models

## Quick Facts
- arXiv ID: 2404.11117
- Source URL: https://arxiv.org/abs/2404.11117
- Authors: Etienne David; Jean Bellot; Sylvain Le Corff
- Reference count: 27
- Outperforms state-of-the-art methods with MASE of 0.684 on fashion dataset

## Executive Summary
This paper introduces a novel time series forecasting model that combines discrete state space hidden Markov models with neural network architectures inspired by vector quantized variational autoencoders. The approach introduces a variational discrete posterior distribution of latent states given observations and uses a two-stage training procedure to alternatively train the parameters of latent states and emission distributions. The method is designed to handle large datasets with thousands of heterogeneous time series and leverage external signals.

## Method Summary
The proposed method combines discrete state space hidden Markov models with neural networks to create a probabilistic forecasting model. It introduces a variational discrete posterior distribution of latent states given observations and employs a two-stage training procedure: first training emission laws and the posterior variational law with a uniform prior, then training the prior law guided by the learned posterior. The model uses LSTM layers for the fashion dataset and fully connected layers for reference datasets, incorporating external signals in both emission laws and hidden state transitions.

## Key Results
- Achieves MASE of 0.684 on the fashion dataset, outperforming state-of-the-art methods
- Shows competitive performance on eight reference datasets (ETT, ECL, Exchange-Rate, Traffic, Weather, ILI)
- Demonstrates improved forecasting accuracy when external signals are included in the model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage training procedure decouples learning of hidden state dynamics from emission laws, improving convergence and accuracy.
- Mechanism: First, fix the prior distribution of hidden states and train the emission laws and variational posterior jointly. Once emission laws are learned, freeze them and train the prior distribution using the learned posterior as guidance. This alternation prevents interference between the two learning tasks.
- Core assumption: The posterior distribution can effectively guide the learning of the prior once the emission laws are sufficiently trained.
- Evidence anchors:
  - [abstract]: "We introduce a variational discrete posterior distribution of the latent states given the observations and a two-stage training procedure to alternatively train the parameters of the latent states and of the emission distributions."
  - [section]: "Inspired by ideas brought with Vector quantized V AE model (van den Oord et al., 2018), the ELBO loss presented above is optimized in two steps. Whilst training the emission laws and the posterior variational law, the prior is not learnt and kept constant and uniform. After the convergence of the emission laws, they are frozen and the prior model is trained, guided by the learned posterior variational law."
  - [corpus]: Weak - No direct mention of two-stage training or alternation in neighbors.

### Mechanism 2
- Claim: The hidden Markov structure allows modeling of heterogeneous time series by learning specialized emission laws for different regimes.
- Mechanism: The discrete hidden states represent different regimes or behaviors in the time series. Each hidden state activates a dedicated neural network that outputs the parameters of a Gaussian emission distribution. This specialization allows capturing diverse patterns across thousands of heterogeneous sequences.
- Core assumption: Time series exhibit regime-dependent behavior that can be captured by discrete hidden states.
- Evidence anchors:
  - [abstract]: "By learning a collection of emission laws and temporarily activating them depending on the hidden process dynamics, the proposed method allows to explore large datasets and leverage available external signals."
  - [section]: "Depending on the values taken, K different predictions can be computed for a same time series, all representing behaviours linked to the hidden regime of the time series."
  - [corpus]: Weak - Neighbors mention volatility-aware forecasting and regime changes but not discrete hidden state structures.

### Mechanism 3
- Claim: The variational posterior approximation enables scalable training for large datasets by avoiding exact inference.
- Mechanism: Instead of computing the intractable true posterior over hidden states, a neural network (the variational posterior) approximates it given observations. This approximation is differentiable and can be optimized via the ELBO, enabling efficient gradient-based training on thousands of sequences.
- Core assumption: The neural network can learn a sufficiently accurate approximation to the true posterior.
- Evidence anchors:
  - [abstract]: "We introduce a variational discrete posterior distribution of the latent states given the observations."
  - [section]: "As (xi t)t∈Z is never observed, the log likelihood is no longer computable for the proposed model and specific losses and algorithms have to be used. A relevant approach is to substitute the log likelihood by the Evidence Lower BOund (ELBO)."
  - [corpus]: Weak - No explicit mention of variational inference in neighbors.

## Foundational Learning

- Concept: Evidence Lower BOund (ELBO)
  - Why needed here: The model uses a latent discrete state variable, making exact maximum likelihood intractable. ELBO provides a tractable lower bound on the log-likelihood that can be optimized via variational inference.
  - Quick check question: What is the relationship between ELBO and the true log-likelihood when the variational posterior equals the true posterior?

- Concept: Hidden Markov Models (HMMs)
  - Why needed here: The framework extends classical HMMs by replacing discrete emission distributions with neural networks, allowing flexible modeling of complex time series while retaining interpretability of hidden regimes.
  - Quick check question: In a standard HMM, what are the two main components that define the joint distribution of observations and hidden states?

- Concept: Vector Quantized Variational Autoencoders (VQ-VAE)
  - Why needed here: The training procedure is inspired by VQ-VAE, using discrete latent variables and a two-stage optimization that first learns the decoder (emission laws) then the encoder (prior).
  - Quick check question: In VQ-VAE, what role does the "codebook" play and how is it analogous to the emission laws in this framework?

## Architecture Onboarding

- Component map:
  - Prior model (fθx) -> Hidden state sequence -> Selected emission model(s) -> Gaussian parameters -> Forecast distribution
  - Variational posterior (fϕ) -> Approximates hidden state distribution given observations
  - External signals (wi) -> Combined with observations for prior and posterior models

- Critical path: Data → Prior model → Hidden state sequence → Selected emission model(s) → Gaussian parameters → Forecast distribution

- Design tradeoffs:
  - Number of hidden states (K): More states increase flexibility but risk redundancy and overfitting; too few limit expressiveness.
  - Neural network architectures: LSTM layers capture temporal dependencies but are slower than fully connected layers for long sequences.
  - External signal integration: Can be included in prior, emissions, or both, affecting model capacity and training complexity.

- Failure signatures:
  - Poor accuracy: Emission laws not specialized enough, hidden states not well separated, or variational posterior inaccurate.
  - Slow convergence: Two-stage training not properly coordinated, or learning rates mismatched between stages.
  - Overfitting: Too many parameters relative to dataset size, or insufficient regularization in neural networks.

- First 3 experiments:
  1. Train with K=2 hidden states on a small subset of the fashion dataset, compare MASE with and without external signals.
  2. Vary the number of hidden states (K=1,2,3,4) on the Traffic dataset, plot MASE vs K to find optimal value.
  3. Replace LSTM layers with fully connected layers in emission models, measure impact on accuracy and training speed on the Weather dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the optimal number of hidden states (K) be automatically determined for different time series forecasting tasks?
- Basis in paper: [explicit] The paper mentions that future work will focus on providing an automatic selection of K and notes that increasing the number of hidden states does not always lead to improved accuracy.
- Why unresolved: The current approach requires manual selection of K through experimentation, which is not scalable for real-world applications with diverse datasets.
- What evidence would resolve it: Development and testing of an automated method to select K, such as a validation-based approach or a complexity penalty criterion, that consistently improves forecasting accuracy across various datasets.

### Open Question 2
- Question: What theoretical guarantees can be provided for the variational distribution and the overall model performance?
- Basis in paper: [explicit] The conclusion suggests extending recent results on variational learning of hidden Markov models to obtain theoretical guarantees on the variational distribution.
- Why unresolved: The paper relies on empirical results to demonstrate model effectiveness but lacks theoretical analysis of convergence, consistency, or bounds on the variational approximation.
- What evidence would resolve it: Proofs of convergence properties for the variational inference procedure, consistency of the parameter estimates, or bounds on the approximation error compared to the true posterior distribution.

### Open Question 3
- Question: How can the model be adapted to handle more complex external signals beyond simple inclusion in the emission laws and hidden state transitions?
- Basis in paper: [inferred] The paper shows improved performance when external signals are included but does not explore advanced techniques for integrating complex or high-dimensional external data.
- Why unresolved: The current implementation only considers simple incorporation of external signals, which may not capture intricate relationships or dependencies between the main time series and external variables.
- What evidence would resolve it: Experiments demonstrating improved forecasting accuracy when incorporating advanced techniques such as attention mechanisms, graph neural networks, or multi-modal learning to process complex external signals.

### Open Question 4
- Question: How does the proposed method compare to state-of-the-art approaches in terms of computational efficiency and scalability for very large datasets?
- Basis in paper: [inferred] While the paper shows competitive accuracy, it does not provide a detailed analysis of the computational requirements or scalability limitations compared to other methods.
- Why unresolved: The paper focuses on accuracy but does not address the practical considerations of implementing the model in real-world scenarios with massive datasets or limited computational resources.
- What evidence would resolve it: Empirical comparisons of training and inference times, memory usage, and scalability benchmarks against other state-of-the-art methods on large-scale datasets with varying numbers of time series.

## Limitations
- Exact architectural details for neural network components are not fully specified, allowing for dataset-dependent adjustments
- Two-stage training procedure may be sensitive to hyperparameter tuning and coordination between stages
- Limited analysis of computational efficiency and scalability for very large datasets

## Confidence
- High confidence: The variational inference framework and ELBO optimization are well-established techniques, and the paper correctly applies them to the discrete state space model context.
- Medium confidence: The experimental results showing improved MASE on the fashion dataset and competitive performance on reference datasets, though the ablation studies could be more comprehensive.
- Low confidence: The generalization of the approach to datasets with very long-term dependencies or highly similar time series patterns, where the benefits of discrete hidden states may be limited.

## Next Checks
1. Conduct ablation studies varying the number of hidden states (K) systematically across all datasets to quantify the impact of state discretization on forecasting accuracy.
2. Implement a version using only the posterior variational network (no prior learning stage) to assess whether the two-stage procedure provides significant benefits over simpler alternatives.
3. Test the model on datasets with known regime changes but no external signals to evaluate its ability to discover hidden states without supervision.