---
ver: rpa2
title: 'M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating
  Foundation Models'
arxiv_id: '2411.04075'
source_url: https://arxiv.org/abs/2411.04075
tags:
- question
- visual
- context
- reasoning
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces M3SciQA, a multi-modal, multi-document scientific
  question answering benchmark designed to evaluate foundation models' abilities in
  handling complex research workflows. The benchmark contains 1,452 expert-annotated
  questions spanning 70 NLP paper clusters, each requiring reasoning across text,
  figures, and tables while integrating information from multiple documents.
---

# M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models

## Quick Facts
- **arXiv ID**: 2411.04075
- **Source URL**: https://arxiv.org/abs/2411.04075
- **Reference count**: 26
- **Primary result**: Current foundation models significantly underperform human experts on multi-modal, multi-document scientific QA, with GPT-4o achieving MRR 0.488 vs human 0.796 and Command R+ achieving accuracy 33.25 vs human 76.56

## Executive Summary
This paper introduces M3SciQA, a benchmark designed to evaluate foundation models' abilities to handle complex scientific research workflows requiring multi-modal reasoning across text, figures, and tables while integrating information from multiple documents. The benchmark contains 1,452 expert-annotated questions spanning 70 NLP paper clusters from EMNLP 2023, each requiring reasoning across scientific diagrams and synthesizing information from multiple papers. When evaluated on 18 foundation models, current models significantly underperform compared to human experts, revealing fundamental limitations in both multi-modal comprehension of scientific diagrams and long-range document ranking capabilities.

## Method Summary
The M3SciQA benchmark construction pipeline involves curating recent NLP papers as anchor documents, generating visual context questions from figures and tables, identifying reference papers cited in anchor documents, and creating comprehensive questions requiring both visual reasoning and cross-document synthesis. The evaluation proceeds in two stages: first assessing LMMs' ability to retrieve and rank relevant reference papers given visual context questions and scientific images using metrics like MRR and Recall@k, then evaluating LLMs' ability to answer reference-based questions using the top-ranked papers as context with accuracy scoring.

## Key Results
- GPT-4o achieved Mean Reciprocal Rank of 0.488 versus human expert score of 0.796 for visual context retrieval
- Command R+ achieved accuracy of 33.25 versus human expert score of 76.56 for reference-based reasoning
- Current models struggle significantly with multi-modal comprehension of scientific diagrams and long-range document ranking

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: M3SciQA's multi-modal multi-document design creates a more realistic evaluation by requiring interpretation of scientific figures and tables while connecting information across multiple research papers.
- **Mechanism**: The benchmark construction pipeline creates visual context questions from figures/tables, generates reference-based questions from cited papers, and combines them into comprehensive questions requiring both visual reasoning and cross-document synthesis.
- **Core assumption**: Scientific research workflows inherently involve interpreting visual data and aggregating information from multiple sources, making this dual requirement essential for evaluating real-world model performance.
- **Evidence anchors**: [abstract] "However, they often fail to fully capture the complexity of research workflows, which typically involve interpreting non-textual data and gathering information across multiple documents." [section 2.1] "Our objective is to develop a challenging yet realistic QA benchmark that necessitates both multi-modal and multi-document reasoning over scientific papers."

### Mechanism 2
- **Claim**: Using recent EMNLP 2023 papers as anchor documents minimizes data contamination risk and ensures models must analyze provided scientific images rather than relying on pre-trained knowledge.
- **Mechanism**: By curating anchor papers from papers released after October 2023, the benchmark ensures that visual context questions cannot be answered from models' pre-existing knowledge, forcing actual image analysis.
- **Core assumption**: Models trained before late 2023 would not have seen these specific papers during training, making their performance on these questions indicative of true visual reasoning capability rather than memorization.
- **Evidence anchors**: [section 2.3] "To mitigate the risk of data contamination, where models might rely on pre-trained knowledge to answer the visual context questions rather than analyzing the provided scientific images, we curate anchor papers from a recent NLP conference, EMNLP 2023." [section 2.3] "Among the 1,047 papers accepted by EMNLP 2023, we select 441 papers that were released on arXiv after October 1st, 2023 as candidate anchor papers."

### Mechanism 3
- **Claim**: The dual-stage evaluation process effectively isolates and measures distinct capabilities of foundation models in handling complex scientific documents.
- **Mechanism**: First stage measures models' ability to correctly identify relevant reference papers from visual context questions and figures; second stage measures their ability to answer detailed questions using the retrieved papers as context.
- **Core assumption**: Visual context retrieval and reference-based reasoning are separable skills that can be independently evaluated, allowing for granular performance analysis across different aspects of document understanding.
- **Evidence anchors**: [section 3.1] "At the visual context evaluation stage, we assess LMMs' ability to accurately retrieve and rank the correct reference paper from a complete list of reference papers." [section 3.2] "At the reference-based evaluation stage, we assesses how LLMs perform on reference-based questions using the top three ranked papers identified from the visual context evaluation stage as context."

## Foundational Learning

- **Information retrieval metrics (MRR, Recall@k, nDCG@k)**: Essential for evaluating how well models rank relevant documents in the visual context evaluation stage. Quick check: Given a model that ranks the correct document at position 1 for 40% of queries, position 2 for 30%, and position 3 for 20%, what is its MRR score?

- **Visual reasoning types (comparison, data extraction, location, visual understanding)**: Crucial for analyzing model performance breakdowns and identifying specific areas where models struggle with scientific image interpretation. Quick check: Which visual reasoning type would be required to answer "What method is shown in the first row of the table?" and why?

- **Cohen's Kappa for inter-rater reliability**: Used to validate the agreement between LLM-based evaluators and human expert annotators when scoring reference-based question answers. Quick check: If two raters agree on 90 out of 100 ratings with 80 expected by chance, what is Cohen's Kappa and what does it indicate about their agreement?

## Architecture Onboarding

- **Component map**: Anchor paper curation → Visual context question generation → Reference paper identification → Reference-based question generation → Question combination → Model evaluation (visual context stage) → Model evaluation (reference-based stage) → Performance analysis

- **Critical path**: 1) Select anchor papers from recent conference (EMNLP 2023) 2) Expert annotators create visual context questions from figures/tables 3) Identify reference papers cited in anchor documents 4) Use GPT-4 to generate reference-based questions 5) Combine questions into comprehensive multi-modal queries 6) Evaluate models on visual context retrieval stage 7) Evaluate models on reference-based reasoning stage 8) Analyze performance breakdowns by reasoning type

- **Design tradeoffs**: Recent papers vs. larger dataset (using EMNLP 2023 minimizes data contamination but limits dataset size), GPT-4 generation vs. fully human-annotated (balances efficiency with quality control), dual-stage evaluation vs. single combined task (allows granular analysis but adds complexity)

- **Failure signatures**: Low MRR but high Recall@k (model ranks correct paper highly but not at top position), high performance on one reasoning type but low on others (indicates specific visual reasoning weaknesses), poor reference-based performance despite good retrieval (suggests models rely on pre-trained knowledge rather than context)

- **First 3 experiments**: 1) Run GPT-4o through visual context evaluation on small subset (10 questions) to verify retrieval pipeline works 2) Test LLM-based evaluator agreement with human experts on 50 reference-based answers to validate scoring methodology 3) Compare performance of models with different context window sizes on visual context questions to understand context limitations

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can we design specialized image embedding models that effectively capture the nuances of scientific diagrams, plots, and tables rather than relying on general-purpose vision models?
- **Basis in paper**: [explicit] The paper discusses that current image embedding models like LLaVA and CLIP, while proficient with natural images, are not trained on scientific images, limiting their effectiveness in scientific diagram interpretation.
- **Why unresolved**: Despite acknowledging the limitation, the paper does not propose or evaluate any specific architecture or training approach for scientific image understanding.
- **What evidence would resolve it**: Empirical results showing superior performance of specialized scientific image embedding models compared to general-purpose models on benchmarks like M3SciQA, along with ablation studies demonstrating the importance of scientific-specific training data.

### Open Question 2
- **Question**: What is the optimal context window size for multi-modal models handling multi-document scientific literature analysis, and how can we efficiently process documents that exceed current context limits?
- **Basis in paper**: [inferred] The paper highlights significant performance differences between models with different context window sizes and discusses the challenges of ranking large paper clusters within token limitations.
- **Why unresolved**: While the paper identifies the context window disparity as a limitation, it does not explore the theoretical or empirical relationship between context window size and performance, nor does it propose solutions for handling documents beyond current context limits.
- **What evidence would resolve it**: Systematic experiments varying context window sizes on M3SciQA to identify the point of diminishing returns, along with proposed architectures or methodologies for processing documents that exceed context limits.

### Open Question 3
- **Question**: How can we develop more reliable evaluation methodologies for multi-modal scientific question answering that better align with human expert judgment?
- **Basis in paper**: [explicit] The paper discusses the limitations of LLM-based evaluators, noting that while they achieve a Cohen's Kappa of 0.520 (weak agreement) with human experts, there is room for improvement in evaluation reliability.
- **Why unresolved**: The paper acknowledges the limitations of current evaluation approaches but does not propose alternative methodologies or explore why the alignment between LLM-evaluators and human experts remains imperfect.
- **What evidence would resolve it**: Development and validation of new evaluation methodologies that demonstrate improved alignment with human expert judgments on M3SciQA.

## Limitations

- Potential for data contamination despite using recent EMNLP 2023 papers, as some models may have been trained on preprint versions of these papers
- Dataset size (1,452 questions across 70 paper clusters) is relatively modest for a benchmark intended to evaluate state-of-the-art foundation models
- Evaluation methodology relies heavily on LLM-based scoring for reference-based questions, which introduces potential bias and limits robustness

## Confidence

- **High confidence**: The benchmark construction methodology and dual-stage evaluation design are well-documented and logically sound
- **Medium confidence**: The claim that current models significantly underperform human experts is supported by the results, though the human expert baseline consists of only two annotators
- **Low confidence**: The assertion that data contamination has been fully mitigated by using papers from a single recent conference

## Next Checks

1. Re-run the visual context evaluation stage with a holdout set of papers from a different conference (e.g., ACL 2024) to verify that performance differences are not specific to EMNLP 2023 domain
2. Conduct a blind human evaluation of 100 randomly selected reference-based answers to independently verify the LLM-based scoring accuracy and Cohen's Kappa agreement
3. Test model performance on a subset of questions after removing all contextual information (forcing pure visual reasoning) to quantify the extent of pre-trained knowledge utilization