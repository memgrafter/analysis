---
ver: rpa2
title: Implicit Bias of Mirror Flow for Shallow Neural Networks in Univariate Regression
arxiv_id: '2410.03988'
source_url: https://arxiv.org/abs/2410.03988
tags:
- networks
- mirror
- function
- have
- implicit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the implicit bias of mirror flow in univariate
  regression with wide shallow neural networks. For a broad class of potential functions,
  it proves that mirror flow exhibits lazy training and shares the same implicit bias
  as ordinary gradient flow when the network width tends to infinity.
---

# Implicit Bias of Mirror Flow for Shallow Neural Networks in Univariate Regression

## Quick Facts
- arXiv ID: 2410.03988
- Source URL: https://arxiv.org/abs/2410.03988
- Authors: Shuang Liang; Guido Montúfar
- Reference count: 40
- Key outcome: Mirror flow with unscaled potentials exhibits lazy training and shares implicit bias with gradient flow; scaled potentials induce rich biases not captured by RKHS norms

## Executive Summary
This paper analyzes the implicit bias of mirror flow in univariate regression with wide shallow neural networks. For a broad class of potential functions, it proves that mirror flow exhibits lazy training and shares the same implicit bias as ordinary gradient flow when the network width tends to infinity. For ReLU networks, the bias is characterized as minimizing a weighted L2 norm of the second-order derivative, plus two terms controlling the first-order derivative. The analysis recovers prior results for gradient flow and removes limitations requiring data adjustment or skip connections. Scaled potentials are introduced, showing that mirror flow remains lazy but is not in the kernel regime, with the bias depending strongly on the potential function. For absolute value activations, the bias cannot be captured by an RKHS norm, with the potential function determining how different magnitudes of curvature are penalized.

## Method Summary
The study uses two-layer neural networks with one input unit, n hidden units, and one output unit, employing either ReLU or absolute value activation functions. Parameters are initialized with sub-Gaussian random variables. Mirror flow optimization is implemented with both unscaled potentials (quadratic, hypentropy) and scaled potentials (ϕ(x) = 1/(1+ω)(ψ(x) + ωx²)). The analysis focuses on the infinite-width limit where lazy training occurs, with learning rate η = Θ(1/n). The implicit bias is characterized through variational problems in function space, comparing trained network functions to initial functions. Theoretical results are complemented by numerical experiments on synthetic data, though the validation is primarily qualitative.

## Key Results
- Mirror flow with unscaled potentials exhibits lazy training and converges to the same implicit bias as ordinary gradient flow as network width tends to infinity
- For ReLU networks, the implicit bias minimizes a weighted L2 norm of the second-order derivative plus two terms controlling the first-order derivative
- Scaled potentials induce implicit biases that generally cannot be captured by an RKHS norm, with the bias depending on both the potential function and parameter initialization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mirror flow with unscaled potentials exhibits lazy training and converges to the same implicit bias as ordinary gradient flow as network width tends to infinity.
- Mechanism: In the infinite width limit, the potential function's effect reduces to a quadratic form, Φ(θ) ≃ c∥θ∥²₂, so the mirror descent dynamics become equivalent to gradient descent.
- Core assumption: The potential function satisfies Assumption 1 (separable, twice differentiable, positive definite Hessian) and network width is sufficiently large.
- Evidence anchors:
  - [abstract]: "For a broad class of potential functions, we show that mirror flow exhibits lazy training and has the same implicit bias as ordinary gradient flow when the network width tends to infinity."
  - [section]: "We show for networks with standard parametrization that, under a broad class of potential functions, mirror flow displays lazy training, meaning that even though the loss converges to zero the parameters remain nearly unchanged."
  - [corpus]: Weak - the cited papers focus on privacy attacks and adversarial robustness, not the specific lazy training mechanism described here.
- Break condition: If the potential function is not separable or the Hessian is not positive definite, or if the network width is not sufficiently large for the lazy regime to manifest.

### Mechanism 2
- Claim: For ReLU networks, mirror flow minimizes a weighted L2 norm of the second-order derivative plus two terms controlling the first-order derivative.
- Mechanism: The implicit bias is characterized through a variational problem in function space where the difference between trained and initial network functions minimizes a functional combining G₁ (weighted L2 norm of second derivative), G₂ (slope constraints at infinity), and G₃ (boundary condition terms).
- Core assumption: The network uses ReLU activation, parameter initialization follows (2), and mirror flow converges to zero training error.
- Evidence anchors:
  - [abstract]: "For ReLU networks, we characterize this bias through a variational problem in function space."
  - [section]: "We characterize the implicit bias of mirror flow for wide and shallow ReLU networks in function space...by expressing the difference between the network functions before and after training, g(x) = ftrained(x) − finitial(x), as the solution to a variational problem."
  - [corpus]: Weak - the related papers discuss privacy attacks and robustness, not the specific variational characterization for ReLU networks.
- Break condition: If the activation function is not ReLU, or if the parameter initialization does not satisfy the required conditions, or if the variational problem has no solution.

### Mechanism 3
- Claim: Scaled potentials induce a rich class of implicit biases that generally cannot be captured by an RKHS norm.
- Mechanism: Scaled potentials Φ(θ) = 1/n² Σ ϕ(n(θk - θ̂k)) introduce a Bregman divergence regularization in the parameter space that translates to a non-homogeneous functional in function space, depending on both the potential function and parameter initialization.
- Core assumption: The potential function satisfies Assumption 7 (scaled, separable with convex component and quadratic component) and the network uses absolute value activation.
- Evidence anchors:
  - [abstract]: "For networks with absolute value activations, we show that mirror flow with scaled potentials induces a rich class of biases, which generally cannot be captured by an RKHS norm."
  - [section]: "To explore training algorithms capable of implementing different forms of regularization depending on the parameter geometry, we introduce mirror descent with scaled potentials...For networks with absolute value activations, we show that mirror flow with scaled potentials induces a rich class of implicit regularizations."
  - [corpus]: Weak - the related papers focus on privacy and robustness, not the specific scaled potential mechanism described here.
- Break condition: If the potential function does not satisfy Assumption 7, or if the activation function is not absolute value, or if the parameter initialization does not meet the required conditions.

## Foundational Learning

- Concept: Lazy training regime
  - Why needed here: Understanding when parameters remain nearly unchanged during training is crucial for characterizing the implicit bias of mirror flow, as it determines whether the dynamics reduce to a kernel model.
  - Quick check question: What distinguishes the lazy training regime from the active regime in overparameterized neural networks?

- Concept: Variational problems in function space
  - Why needed here: The implicit bias is characterized by expressing the difference between trained and initial network functions as the solution to a variational problem, requiring understanding of function space optimization.
  - Quick check question: How does the solution to a variational problem in function space relate to the parameter space optimization in mirror flow?

- Concept: Bregman divergence
  - Why needed here: Mirror descent uses Bregman divergence to determine update directions, and understanding its properties is essential for analyzing how different potentials affect the implicit bias.
  - Quick check question: How does the choice of potential function in mirror descent affect the Bregman divergence and consequently the training dynamics?

## Architecture Onboarding

- Component map: Network architecture (shallow, univariate) -> Mirror flow optimization (unscaled/scaled potentials) -> Implicit bias characterization (variational problems in function space)
- Critical path: 1) Define the network architecture and initialization, 2) Implement mirror flow with different potential functions, 3) Analyze training dynamics to verify lazy regime, 4) Characterize implicit bias through variational problems, 5) Validate results through numerical experiments
- Design tradeoffs: The choice between unscaled and scaled potentials affects whether the implicit bias is equivalent to gradient flow or can be tailored through the potential function. Using ReLU vs. absolute value activations affects the complexity of the function space characterization.
- Failure signatures: If the potential function does not satisfy the required assumptions, the theoretical analysis may not hold. If the network width is not sufficiently large, lazy training may not manifest. If the activation function is not properly specified, the variational characterization may be incorrect.
- First 3 experiments:
  1. Implement mirror flow with a simple quadratic potential (ϕ(x) = x²) and verify lazy training behavior by checking if parameters remain close to initialization.
  2. Train a ReLU network with mirror flow and compare the trained function to the solution of the corresponding variational problem (7).
  3. Implement mirror flow with a scaled hypentropy potential and observe how the implicit bias changes compared to unscaled potentials, verifying Theorem 8.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the implicit bias of mirror descent with scaled potentials change for ReLU networks when the potential function is non-homogeneous?
- Basis in paper: [explicit] The paper shows that for absolute value activations, scaled potentials induce a rich class of implicit biases that cannot be captured by RKHS norms. It mentions this is challenging for ReLU networks due to the even-odd decomposition technique no longer applying.
- Why unresolved: The paper successfully characterizes the implicit bias for absolute value activations but notes that extending this to ReLU networks is challenging due to the decomposition technique no longer working for general scaled potentials.
- What evidence would resolve it: A closed-form solution for the minimal representation cost problem for ReLU networks with general scaled potentials, or a proof that such a solution cannot be obtained in closed form.

### Open Question 2
- Question: What is the effect of parameter initialization distribution on the implicit bias of mirror descent with scaled potentials for ReLU networks?
- Basis in paper: [explicit] The paper states that "whereas the choice of parameter initialization distribution determines how strongly the curvature is penalized at different locations in the input domain via p, the choice of the potential function ϕ determines how the different magnitudes of the curvature are penalized."
- Why unresolved: The paper only considers uniformly initialized input biases for ReLU networks and does not explore how different initialization distributions affect the implicit bias.
- What evidence would resolve it: Numerical experiments or theoretical analysis showing how different initialization distributions (e.g., Gaussian, truncated Gaussian) affect the implicit bias of mirror descent with scaled potentials for ReLU networks.

### Open Question 3
- Question: How does the implicit bias of mirror descent with scaled potentials change for multivariate ReLU networks with general input weight initialization?
- Basis in paper: [inferred] The paper discusses the challenges of solving the minimal representation cost problem for multivariate networks with general input weight initialization in Appendix E.2, noting that determining the even component of the parameter becomes challenging.
- Why unresolved: The paper only considers univariate networks and notes that extending the results to multivariate networks with general input weight initialization is challenging due to the need for stronger theoretical tools.
- What evidence would resolve it: A characterization of the implicit bias for multivariate ReLU networks with general input weight initialization, either through numerical experiments or theoretical analysis.

## Limitations
- The analysis relies heavily on the infinite-width limit, which may not fully capture finite-width behavior even though lazy training occurs for moderate widths (n≥100)
- Theoretical framework assumes smooth activations for unscaled potentials and absolute value for scaled potentials, limiting generalizability to other common activation functions
- Minimal numerical validation, with only qualitative comparisons to gradient flow provided rather than quantitative error bounds

## Confidence
- **High Confidence**: Claims about lazy training behavior with unscaled potentials and equivalence to gradient flow in the infinite-width limit
- **Medium Confidence**: Variational characterization of implicit bias for ReLU networks, as this depends on solving specific optimization problems that may have numerical challenges
- **Low Confidence**: Claims about rich implicit biases from scaled potentials, particularly for absolute value activations, due to limited numerical validation

## Next Checks
1. **Finite-width validation**: Implement mirror flow with unscaled potentials for various network widths (n=50, 100, 500, 1000) and quantify how closely the implicit bias approaches the theoretical prediction as width increases.

2. **Numerical solution verification**: Develop a reliable numerical method to solve the variational problems characterizing implicit bias for ReLU networks, and validate against the trained network functions across multiple random seeds.

3. **Extended activation study**: Test mirror flow with scaled potentials for additional activation functions beyond absolute value (e.g., Leaky ReLU, ELU) to verify whether the rich class of implicit biases is unique to absolute value or more general.