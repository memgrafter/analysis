---
ver: rpa2
title: Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic
  Compositionality
arxiv_id: '2410.05210'
source_url: https://arxiv.org/abs/2410.05210
tags:
- loss
- image
- texts
- multi-modal
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FSC-CLIP, a fine-tuning framework designed
  to enhance vision-language compositionality while preserving multi-modal task performance
  in pre-trained VLMs. The method addresses the degradation caused by global hard
  negative losses, which harm multi-modal representations by pushing away hard negatives
  that are encoded similarly to original texts.
---

# Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality

## Quick Facts
- arXiv ID: 2410.05210
- Source URL: https://arxiv.org/abs/2410.05210
- Reference count: 39
- Key outcome: FSC-CLIP framework achieves state-of-the-art vision-language compositionality scores while preserving multi-modal capabilities, outperforming baselines that sacrifice general task performance.

## Executive Summary
This paper addresses a critical challenge in VLM fine-tuning: improving vision-language compositionality without degrading multi-modal performance. The authors identify that global hard negative mining in existing methods harms multi-modal representations by pushing away negatives that are semantically similar to original texts. FSC-CLIP introduces two innovations - Local Hard Negative loss for fine-grained patch-level supervision and Selective Calibrated Regularization to focus on challenging negatives while reducing confusion. The framework achieves compositionality scores on par with state-of-the-art models while maintaining strong performance across zero-shot classification and retrieval tasks.

## Method Summary
FSC-CLIP is a fine-tuning framework that enhances vision-language compositionality while preserving multi-modal capabilities in pre-trained VLMs. The method consists of two key components: Local Hard Negative (LHN) loss, which computes similarity at the patch-token level to provide fine-grained supervision, and Selective Calibrated Regularization (SCR), which combines focal loss and label smoothing to focus on challenging negatives while reducing confusion. This approach mitigates the degradation caused by global hard negative losses, which harm multi-modal representations by pushing away hard negatives that are encoded similarly to original texts.

## Key Results
- FSC-CLIP achieves compositionality scores on par with state-of-the-art models (53.5 average across compositionality tasks)
- Maintains strong multi-modal performance (55.9 average across 21 zero-shot classification tasks)
- Outperforms baselines that sacrifice multi-modal capabilities for compositionality gains
- Validated across 11 compositionality, 21 zero-shot classification, and 3 retrieval tasks

## Why This Works (Mechanism)
The framework addresses the fundamental tension between compositionality improvement and multi-modal capability preservation by recognizing that global hard negative mining harms multi-modal representations. The LHN loss provides more precise supervision by operating at the patch-token level rather than the global image level, ensuring that only semantically relevant negatives are pushed away. SCR's combination of focal loss and label smoothing creates a more nuanced learning signal that focuses on genuinely challenging negatives while reducing confusion from semantically similar but irrelevant pairs. This dual approach allows the model to learn compositionality patterns without disrupting its ability to handle general multi-modal tasks.

## Foundational Learning
- Vision-Language Compositionality: The ability to understand and generate complex relationships between visual and textual elements (why needed: core capability being improved)
- Hard Negative Mining: Identifying challenging negative examples during contrastive learning (why needed: improves discrimination but can harm multi-modal representations)
- Patch-Level Representation: Breaking images into tokens for fine-grained matching (why needed: enables more precise supervision than global image-level matching)
- Focal Loss: Adaptive weighting that focuses on hard examples (why needed: prevents easy negatives from dominating the learning signal)
- Label Smoothing: Technique that prevents overconfident predictions (why needed: reduces confusion from semantically similar negatives)
- Regularization in Contrastive Learning: Methods to prevent overfitting and maintain generalization (why needed: preserves multi-modal capabilities during compositionality-focused fine-tuning)

## Architecture Onboarding

**Component Map:** Pre-trained VLM -> LHN Loss (patch-level similarity) + SCR (focal loss + label smoothing) -> Fine-tuned VLM

**Critical Path:** Image/text input → VLM encoding → Patch-level similarity computation → Hard negative selection → LHN/SCR losses → Backpropagation

**Design Tradeoffs:** The framework trades computational efficiency for precision, as patch-level operations increase overhead but provide superior supervision compared to global approaches.

**Failure Signatures:** Degradation in multi-modal performance (classification/retrieval tasks) while compositionality improves, indicating the model has learned compositionality at the expense of general capabilities.

**First Experiments:**
1. Ablation study removing LHN loss to measure impact on compositionality vs multi-modal performance
2. Comparison of global vs local hard negative mining on semantic similarity preservation
3. Hyperparameter sensitivity analysis for α and ε in SCR across different task types

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead from patch-level operations in LHN loss could hinder scalability to larger VLMs
- SCR effectiveness depends on hyperparameter tuning that may require task-specific optimization
- Evaluation focuses primarily on CLIP-based models, leaving uncertainty about generalizability to other VLM architectures

## Confidence

**High confidence:** FSC-CLIP preserves multi-modal capabilities while improving compositionality (supported by consistent performance across 21 classification and 3 retrieval tasks)

**Medium confidence:** LHN loss provides superior fine-grained supervision (strong ablation evidence but lacks qualitative analysis of patch-level matching)

**Medium confidence:** SCR effectively reduces negative confusion (quantitative improvements shown but mechanism not fully explained)

## Next Checks

1. Conduct runtime and memory profiling to quantify the computational overhead of LHN loss across different input resolutions and VLM sizes

2. Test FSC-CLIP transferability by applying the framework to non-CLIP architectures (e.g., BLIP, SigLIP) and measuring compositionality gains

3. Perform qualitative analysis of patch-level matching in LHN to verify that it captures semantically relevant negative pairs rather than superficial similarities