---
ver: rpa2
title: Eliciting Personality Traits in Large Language Models
arxiv_id: '2402.08341'
source_url: https://arxiv.org/abs/2402.08341
tags:
- personality
- language
- traits
- questions
- trait
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the personality traits of large language
  models (LLMs) using a novel elicitation approach based on common interview questions
  and trait-activating prompts. The researchers used fine-tuned text classifiers to
  analyze the language models' outputs and measure their Big Five personality traits.
---

# Eliciting Personality Traits in Large Language Models

## Quick Facts
- arXiv ID: 2402.08341
- Source URL: https://arxiv.org/abs/2402.08341
- Authors: Airlie Hilliard; Cristian Munoz; Zekun Wu; Adriano Soares Koshiyama
- Reference count: 40
- Primary result: Novel elicitation approach reveals LLMs exhibit high openness and low extraversion, with larger models showing broader personality ranges

## Executive Summary
This study investigates the personality traits of large language models (LLMs) using a novel elicitation approach based on common interview questions and trait-activating prompts. The researchers used fine-tuned text classifiers to analyze LLM outputs and measure their Big Five personality traits. The findings reveal that all LLMs exhibit high openness and low extraversion, with larger models demonstrating increased agreeableness, emotional stability, and openness compared to smaller models. Fine-tuned models show minor modulations in personality traits depending on the dataset used. The results have implications for the use of generative AI in recruitment contexts, as LLM personality may influence candidate evaluations during interviews.

## Method Summary
The study employs a novel elicitation approach using prompts derived from common interview questions and trait-activating prompts designed to elicit Big Five personality traits. LLMs of varying parameter sizes (Llama-2, Falcon, Mistral, Bloom, GPT, OPT, and XLNet) are prompted repeatedly with different questions, and their responses are analyzed using fine-tuned text classifiers trained on the myPersonality dataset. The classifiers predict personality traits based on the language used in the LLM outputs, allowing for comparison of personality traits across different models and parameter sizes.

## Key Results
- All LLMs exhibited high openness and low extraversion personality traits
- Larger language models demonstrated increased agreeableness, emotional stability, and openness compared to smaller models
- Fine-tuned models showed minor modulations in personality traits depending on the dataset used for fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The study successfully elicits personality traits from LLMs using trait-activating prompts derived from common interview questions.
- Mechanism: Fine-tuned text classifiers analyze LLM outputs and measure Big Five personality traits based on language patterns in responses.
- Core assumption: Language used in LLM responses is indicative of personality traits, similar to human language.
- Evidence anchors: [abstract] Novel elicitation approach using prompts derived from common interview questions... to measure their personality based on the language used in their outputs.
- Break condition: If LLMs do not respond to trait-activating prompts similarly to humans, or if classifiers are inaccurate in measuring traits.

### Mechanism 2
- Claim: Larger language models demonstrate a broader range of personality traits compared to smaller models.
- Mechanism: Study compares personality trait scores of LLMs with different parameter sizes.
- Core assumption: Parameter size correlates with ability to exhibit wider range of personality traits.
- Evidence anchors: [abstract] Large language models with more parameters demonstrated increased agreeableness, emotional stability, and openness compared to smaller models.
- Break condition: If correlation between parameter size and personality trait diversity is not statistically significant.

### Mechanism 3
- Claim: Fine-tuned language models exhibit minor modulations in personality traits depending on the dataset used for fine-tuning.
- Mechanism: Study investigates personality traits of fine-tuned LLMs and impact of fine-tuning datasets.
- Core assumption: Fine-tuning dataset can influence personality traits, though effect is relatively small.
- Evidence anchors: [abstract] Fine-tuned models show minor modulations in their personality traits depending on the dataset.
- Break condition: If fine-tuning dataset has no measurable impact on personality traits, or if effect is significant rather than minor.

## Foundational Learning

- Concept: Big Five personality traits (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism/Emotional Stability)
  - Why needed here: Study measures LLM personality traits using Big Five model, crucial for interpreting results.
  - Quick check question: What are the five dimensions of the Big Five personality model, and how are they typically measured in human subjects?

- Concept: Natural Language Processing (NLP) and text analysis techniques
  - Why needed here: Study uses NLP techniques to analyze language in LLM responses and measure personality traits.
  - Quick check question: What are some common NLP techniques used for text analysis, and how can they be applied to measure personality traits from language?

- Concept: Machine learning classifiers and their application in personality trait prediction
  - Why needed here: Study uses fine-tuned machine learning classifiers to analyze LLM outputs and predict personality traits.
  - Quick check question: How do machine learning classifiers work, and what are some common algorithms used for text classification tasks like personality trait prediction?

## Architecture Onboarding

- Component map: LLMs with varying parameter sizes -> Trait-activating prompts -> Fine-tuned text classifiers -> Text generation and analysis pipeline -> Performance evaluation framework

- Critical path:
  1. Generate text responses from LLMs using trait-activating prompts
  2. Preprocess and normalize the generated text
  3. Apply fine-tuned text classifiers to predict personality traits
  4. Compare personality trait scores across different LLMs and parameter sizes
  5. Analyze the impact of fine-tuning datasets on personality trait expression

- Design tradeoffs:
  - Model size vs. computational efficiency: Larger models may exhibit more diverse personality traits but require more resources
  - Prompt design vs. trait elicitation: Effectiveness depends on prompt design and relevance to target traits
  - Classifier accuracy vs. interpretability: Fine-tuned classifiers may be more accurate but less interpretable

- Failure signatures:
  - Low variance in personality trait scores across different LLMs and parameter sizes
  - Inconsistent or unreliable personality trait predictions from fine-tuned classifiers
  - Lack of correlation between parameter size and personality trait diversity

- First 3 experiments:
  1. Compare personality trait scores of small and large LLM using same trait-activating prompts
  2. Fine-tune text classifier on different dataset and compare performance in predicting traits
  3. Generate responses from LLM using diverse prompts and analyze variance in personality trait scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of fine-tuning datasets (e.g., literary, conversational, technical) systematically affect the personality traits exhibited by LLMs?
- Basis in paper: [explicit] The paper discusses that fine-tuned models exhibit minor modulations in personality traits contingent on the dataset used for fine-tuning.
- Why unresolved: Study provides initial observations on specific datasets but does not comprehensively explore systematic effects of various dataset types.
- What evidence would resolve it: Experiments with wide range of fine-tuning datasets representing different domains and analysis of resulting personality trait changes.

### Open Question 2
- Question: Can trait-activation techniques be improved to effectively elicit specific personality traits in LLMs, similar to their impact on human subjects?
- Basis in paper: [inferred] The paper notes that LLMs are not influenced by trait-activation, unlike humans, possibly due to absence of social cues.
- Why unresolved: Study highlights lack of response to trait-activation in LLMs but does not explore potential methods to enhance effectiveness.
- What evidence would resolve it: Developing and testing new trait-activation approaches specifically designed for LLMs and measuring their impact on exhibited personality traits.

### Open Question 3
- Question: How does the personality of LLMs influence their performance and outputs in various tasks, such as creative writing, problem-solving, and social interaction?
- Basis in paper: [explicit] The paper discusses implications of LLM personality on their use in recruitment contexts and potential impact on candidate evaluations.
- Why unresolved: While study explores relationship between LLM personality and recruitment, it does not extensively investigate how personality traits affect performance across different tasks.
- What evidence would resolve it: Experiments where LLMs with different personality profiles are evaluated on variety of tasks, measuring performance and characteristics of outputs.

## Limitations
- Reliance on myPersonality dataset for classifier training may introduce domain-specific biases
- Study does not address potential confounds from prompt engineering or generation parameters
- "Minor modulations" in fine-tuned models are not quantified, making practical significance difficult to assess

## Confidence
- High confidence in elicitation methodology and general effectiveness
- Medium confidence in relationship between model size and personality trait diversity
- Medium confidence in observed personality trait patterns across models

## Next Checks
1. Conduct ablation studies on prompt engineering and generation parameters to assess impact on personality trait measurements
2. Test classifiers on human-written text to establish baseline performance and validate trait measurement approach
3. Investigate correlation between personality traits and task performance across different model sizes to assess practical implications