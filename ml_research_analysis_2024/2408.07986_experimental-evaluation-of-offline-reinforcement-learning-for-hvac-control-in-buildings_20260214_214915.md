---
ver: rpa2
title: Experimental evaluation of offline reinforcement learning for HVAC control
  in buildings
arxiv_id: '2408.07986'
source_url: https://arxiv.org/abs/2408.07986
tags:
- hvac
- offline
- control
- learning
- controllers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates offline reinforcement learning (RL) algorithms
  for HVAC control in buildings, addressing the challenge of leveraging historical
  data for model-free control. The authors propose integrating observation history
  modeling to capture long-term dependencies, enhancing both performance and stability.
---

# Experimental evaluation of offline reinforcement learning for HVAC control in buildings

## Quick Facts
- arXiv ID: 2408.07986
- Source URL: https://arxiv.org/abs/2408.07986
- Authors: Jun Wang; Linyan Li; Qi Liu; Yu Yang
- Reference count: 40
- Key outcome: Offline RL algorithms with observation history modeling achieve 28.5% reduction in temperature violations and 12.1% energy savings compared to rule-based controllers

## Executive Summary
This paper evaluates offline reinforcement learning algorithms for HVAC control in buildings, proposing observation history modeling to capture long-term dependencies. Through experiments in simulated building environments, the authors investigate how dataset quality (measured by regret ratio) and quantity affect performance. Results demonstrate that offline RL algorithms outperform online methods when trained on high-quality data, and surprisingly, that suboptimal trajectories and smaller datasets can effectively train well-performing controllers. The study provides practical insights for deploying RL-based HVAC control using historical operational data.

## Method Summary
The method involves simulating building environments (MixedUse and DataCenter) using EnergyPlus-based frameworks, generating offline datasets with controlled quality via exploration probability and noise, and training CQL-based offline RL controllers with Transformer-based observation history modeling. The controllers are evaluated on held-out weather conditions against rule-based baselines and off-policy methods (SAC, TD3), measuring performance through reward values, temperature violation ratios, and power consumption metrics.

## Key Results
- Offline RL algorithms achieve up to 28.5% reduction in temperature violation ratios compared to rule-based controllers
- Suboptimal trajectories in training data improve learning outcomes by providing diverse state-action coverage
- Observation history modeling with sequence lengths of 20-30 sufficiently captures thermal trends for effective control
- High-quality offline data enables offline RL to outperform online methods in terms of both stability and performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Observation history modeling reduces indoor temperature violations by capturing temporal dependencies that single-step state representations miss.
- Mechanism: The paper replaces partially observable states with full observation histories (sequences of past observations), enabling the agent to anticipate thermal trends and make proactive adjustments rather than reactive ones.
- Core assumption: Indoor thermal dynamics are sufficiently predictable from recent historical observations, and the sequence length chosen captures the relevant temporal patterns.
- Evidence anchors:
  - [abstract] "The ability of observation history modeling to reduce violations and enhance performance is subsequently studied."
  - [section 6.2] "We found that increasing the sequence length of the observation horizon up to 50 does not significantly benefit the reward performance as anticipated. Therefore, we deduce that the chosen sequence lengths (20 for MixedUse and 30 for DataCenter) sufficiently capture past trends."
  - [corpus] Weak - no direct comparison of history modeling vs single-step in the corpus papers.
- Break condition: If the thermal dynamics have long-term dependencies beyond the chosen history window, or if the environment has high-frequency noise that overwhelms historical patterns.

### Mechanism 2
- Claim: Offline RL algorithms outperform online methods when trained on high-quality data because they avoid distribution shift through regularization.
- Mechanism: Offline RL algorithms (CQL, BCQ, TD3+BC) include explicit regularization terms that constrain the learned policy to stay close to the behavior policy in the dataset, preventing extrapolation to unseen state-action pairs that would occur in standard off-policy learning.
- Core assumption: The offline dataset contains sufficient coverage of relevant state-action space for the target task, and the regularization strength is appropriately tuned.
- Evidence anchors:
  - [abstract] "Results show that offline RL algorithms outperform online methods when trained on high-quality data, achieving up to 28.5% reduction in temperature violation ratios and 12.1% energy savings."
  - [section 4.3] "CQL [45], the current state-of-the-art algorithm, just stems from this insight and proposes to lower bound the estimated Q values of state-action pairs ( s, a) under the current policy while maximizing the values of those contained in D at the same time."
  - [section 6.1] "When it comes to high-quality data with insufficient coverage (i.e., Scenario 2), the state-of-the-art off-policy algorithm-based controllers (TD3 and SAC) perform quite worse... By contrast, HVAC controllers relying on offline RL algorithms (CQL, BCQ, and TD3+BC) still exhibit a compelling performance."
- Break condition: If the dataset quality is too poor (highly suboptimal trajectories) or if the regularization is too strong, preventing learning of better-than-behavior policies.

### Mechanism 3
- Claim: Suboptimal trajectories in offline datasets can improve learning outcomes by providing diverse state-action coverage that enables policy stitching.
- Mechanism: Rather than requiring only expert trajectories, the offline RL algorithms can learn to combine elements from different suboptimal trajectories to construct better policies than any single trajectory in the dataset.
- Core assumption: The dataset contains a diverse set of trajectories with varying levels of suboptimality, and the offline RL algorithm can effectively combine these elements.
- Evidence anchors:
  - [abstract] "The study also reveals that suboptimal trajectories and smaller datasets can effectively train well-performing controllers."
  - [section 6.3] "Surprisingly, a high-level conclusion from Figure 7 reveals that offline datasets with trajectories having small δτ values near 0 are less suitable for training RL algorithm-based controllers, which might conflict with our intuition. In contrast, a mixture of suboptimal trajectories is beneficial to the reward performance in terms of offline-trained HVAC policies."
  - [corpus] Assumption: The corpus papers focus on DRL evaluation but don't explicitly discuss the benefits of suboptimal data for offline learning.
- Break condition: If the suboptimal trajectories are too far from optimal behavior, making it impossible to extract useful patterns, or if the dataset is too small to provide sufficient diversity.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation
  - Why needed here: The entire reinforcement learning framework is built on MDP theory, and understanding state, action, reward, and transition dynamics is essential for implementing HVAC control algorithms.
  - Quick check question: What are the five components of an MDP and how do they map to HVAC control (state = indoor conditions, action = setpoint adjustments, etc.)?

- Concept: Off-policy vs. on-policy learning
  - Why needed here: The paper explicitly compares off-policy algorithms (TD3, SAC) with offline RL algorithms, and understanding the difference is crucial for grasping why offline methods work better with static datasets.
  - Quick check question: What is the key difference between off-policy and on-policy learning in terms of data collection and policy evaluation?

- Concept: Function approximation with neural networks
  - Why needed here: Both the actor and critic networks use neural networks to approximate policies and value functions, and understanding how these work is essential for implementing and debugging the algorithms.
  - Quick check question: How do actor-critic methods use neural networks to represent both the policy (actor) and value function (critic)?

## Architecture Onboarding

- Component map: Environment simulator (EnergyPlus-based) → Observation history encoder (Transformer) → Actor network → Critic network → Replay buffer → Training loop with regularization
- Critical path: Data generation → Offline training with CQL → Online evaluation → Performance metrics collection
- Design tradeoffs: Longer observation histories improve performance but increase computational cost; stronger regularization prevents distribution shift but may limit learning potential
- Failure signatures: High temperature violation ratios indicate poor policy performance; training instability suggests incorrect regularization strength; poor reward values indicate inadequate dataset quality
- First 3 experiments:
  1. Verify that observation history modeling improves temperature stability by comparing single-step vs. history-based policies on a simple test environment
  2. Test offline RL performance with varying dataset quality (δτ ranges) to confirm the suboptimal trajectory benefit
  3. Evaluate the impact of observation history length on both performance and training efficiency to find the optimal sequence length

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of offline RL algorithms scale with the size of the training dataset beyond the 10^6 transition limit explored in the study?
- Basis in paper: [inferred] The paper investigates the performance impact of reducing dataset size from 10^6 to 10^5 transitions, but does not explore larger datasets or the scaling behavior beyond this point.
- Why unresolved: The study focuses on demonstrating the effectiveness of offline RL with relatively small datasets, leaving the upper bound of dataset size and its impact on performance unclear.
- What evidence would resolve it: Experiments training offline RL controllers with datasets larger than 10^6 transitions, comparing performance metrics like reward, energy consumption, and temperature violation ratios.

### Open Question 2
- Question: How does the performance of offline RL algorithms compare to online RL algorithms when both are given access to the same total amount of interaction data with the environment?
- Basis in paper: [inferred] The paper demonstrates that offline RL can achieve comparable performance to online methods with smaller datasets, but does not directly compare their performance when given equal interaction time or data volume.
- Why unresolved: The study highlights the data efficiency of offline RL but does not provide a head-to-head comparison of learning speed or final performance against online RL under identical data collection constraints.
- What evidence would resolve it: Experiments where both offline and online RL algorithms are trained for the same total number of environment interactions, measuring and comparing their learning curves and final performance.

### Open Question 3
- Question: How sensitive is the performance of offline RL algorithms to the specific distribution of regret ratios (δτ) in the training dataset, beyond the exploration probability and noise variance explored in the study?
- Basis in paper: [explicit] The paper investigates the impact of regret ratio distribution on performance by varying exploration probability and noise, but does not explore other methods of shaping the regret ratio distribution.
- Why unresolved: While the study shows that a mix of suboptimal trajectories is beneficial, it does not explore the full space of possible regret ratio distributions or their specific effects on learning.
- What evidence would resolve it: Experiments generating datasets with various regret ratio distributions (e.g., bimodal, skewed) and measuring the resulting performance of offline RL algorithms across different distribution shapes.

## Limitations
- The study relies on simulated environments rather than real-world deployment, which may not capture all practical challenges of HVAC control in actual buildings.
- The observation history modeling approach shows performance improvements, but lacks detailed comparisons against single-step state representations within the same experimental framework.
- While the study demonstrates benefits of suboptimal trajectories, the specific mechanisms by which offline RL algorithms effectively combine these trajectories are not fully explained.

## Confidence
- High confidence: The core finding that offline RL algorithms outperform online methods with high-quality data is well-supported by experimental results and established theoretical foundations (CQL regularization prevents distribution shift).
- Medium confidence: The claim about suboptimal trajectories improving learning outcomes is supported by empirical results but lacks detailed theoretical justification for why this occurs.
- Medium confidence: The observation history modeling mechanism is demonstrated effective, but the exact contribution of the Transformer architecture versus simpler sequence modeling approaches is not isolated.

## Next Checks
1. **Real-world validation**: Deploy the offline RL controller with observation history modeling in an actual building testbed to verify that simulation results translate to real-world performance improvements.
2. **Ablation study on history modeling**: Compare the performance of the Transformer-based observation history model against simpler sequence models (e.g., LSTM) and single-step state representations using identical experimental conditions.
3. **Dataset diversity analysis**: Systematically vary the types and distributions of suboptimal trajectories in offline datasets to better understand which patterns of suboptimal data are most beneficial for learning effective control policies.