---
ver: rpa2
title: Better Alignment with Instruction Back-and-Forth Translation
arxiv_id: '2408.04614'
source_url: https://arxiv.org/abs/2408.04614
tags:
- data
- responses
- instructions
- instruction
- dolma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes instruction back-and-forth translation as a
  method for constructing high-quality synthetic instruction-tuning data for large
  language models. The approach involves backtranslating web-scraped text into synthetic
  instructions, then rewriting the responses using an aligned LLM to improve their
  quality and better match the instructions.
---

# Better Alignment with Instruction Back-and-Forth Translation

## Quick Facts
- arXiv ID: 2408.04614
- Source URL: https://arxiv.org/abs/2408.04614
- Authors: Thao Nguyen; Jeffrey Li; Sewoong Oh; Ludwig Schmidt; Jason Weston; Luke Zettlemoyer; Xian Li
- Reference count: 37
- One-line primary result: Instruction back-and-forth translation with response rewriting improves Llama-2 alignment by 3.2-3.6% on AlpacaEval

## Executive Summary
This paper introduces instruction back-and-forth translation as a method for generating high-quality synthetic instruction-tuning data for large language models. The approach involves backtranslating web-scraped text into synthetic instructions, then rewriting the responses using an aligned LLM to improve quality and better match the instructions. Fine-tuning Llama-2 models on the resulting instruction-response pairs achieves 3.2-3.6% higher win rates on AlpacaEval compared to using other common instruction datasets. The authors demonstrate that their rewriting approach produces responses that interpolate between web text and distilled text distributions while being more diverse and complex than distillation alone.

## Method Summary
The method uses instruction back-and-forth translation to create synthetic instruction-response pairs. First, a backward model fine-tuned on seed instruction data generates instructions from web text extracted from the Dolma corpus. These instruction-text pairs are then filtered using a forward model to remove poorly aligned pairs. Finally, an aligned LLM (Llama-2-chat) rewrites the responses based on both the generated instructions and the original web text. The resulting instruction-response pairs are used to fine-tune Llama-2 models, with the rewriting step shown to be more effective than filtering alone for improving data quality.

## Key Results
- Fine-tuning Llama-2 on rewritten data achieves 3.2-3.6% higher win rates on AlpacaEval compared to using backtranslation data from previous work
- Rewritten responses show significantly different text distributions from distilled responses (MAUVE scores indicate interpolation between web text and distilled output distributions)
- Response rewriting is more effective than filtering for improving instruction-response pair quality
- The best performance comes from datasets that combine backtranslation with response rewriting (Dolma + filtering + rewriting)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rewriting responses using an aligned LLM interpolates between web-scraped text and distilled text distributions, improving alignment quality.
- Mechanism: The rewriting process conditions on both the initial web text and the generated instruction, allowing the model to incorporate information from the web while improving response structure and relevance.
- Core assumption: The aligned LLM has sufficient knowledge to improve response quality without simply distilling its own knowledge.
- Evidence anchors:
  - [abstract] "we obtain a distribution of responses that interpolates between the original web text distribution and the distribution of outputs distilled from the same model"
  - [section] "By asking a model to rewrite responses based on initial texts extracted from Dolma, we obtain a distribution of responses that interpolates between the original web text distribution and the distribution of outputs distilled from the same model"
- Break condition: If the aligned LLM cannot effectively incorporate web information, rewriting becomes equivalent to distillation.

### Mechanism 2
- Claim: Backtranslated instructions from Dolma yield higher quality than other synthetic methods.
- Mechanism: Backtranslation fine-tunes a model on seed instruction-response pairs, then generates instructions for web text, creating synthetic instruction-response pairs that maintain diversity while being well-aligned.
- Core assumption: The seed instruction data (Open Assistant) is high-quality enough to train a backward model that generates useful instructions.
- Evidence anchors:
  - [abstract] "our backtranslated instructions are of higher quality than other sources of synthetic instructions"
  - [section] "Fine-tuning Llama-2-70B on the instruction-response pairs from our data generation pipeline improves the AlpacaEval win rate by 3.6% compared to using the backtranslation data from previous work"
- Break condition: If the seed instruction data is too limited or biased, backtranslation quality degrades significantly.

### Mechanism 3
- Claim: Filtering is less effective than rewriting for improving instruction-response pair quality.
- Mechanism: Filtering removes poorly aligned pairs based on forward model scoring, while rewriting actively improves response quality by conditioning on instructions and web text.
- Core assumption: An aligned LLM can improve response quality more effectively than simply removing bad examples.
- Evidence anchors:
  - [abstract] "we find that the rewriting step is more effective than the filtering step at improving the quality of instruction-tuning data"
  - [section] "fine-tuning on rewritten responses from unfiltered instruction-response pairs (Dolma + rewriting) outperforms fine-tuning on web-scraped responses that have passed the filter but have not been rewritten"
- Break condition: If rewriting introduces new errors or hallucinations, filtering might become more valuable.

## Foundational Learning

- Concept: Backtranslation for instruction generation
  - Why needed here: Enables scalable creation of synthetic instruction-response pairs from web text
  - Quick check question: What are the two models needed for backtranslation, and what does each do?

- Concept: Response rewriting as alignment enhancement
  - Why needed here: Improves response quality beyond what filtering alone can achieve
  - Quick check question: What two inputs does the rewriting process condition on?

- Concept: Distribution interpolation for text quality
  - Why needed here: Balances the diversity of web text with the quality of aligned model outputs
  - Quick check question: How does the MAUVE score help demonstrate that rewriting is distinct from distillation?

## Architecture Onboarding

- Component map:
  Seed data (Open Assistant) → Backward model training → Instruction generation (backtranslation) → Pair filtering/scoring → Response rewriting → Fine-tuning → Evaluation

- Critical path: Dolma extraction → Backtranslation → Rewriting → Fine-tuning → Evaluation

- Design tradeoffs:
  - Filtering vs rewriting: Filtering is cheaper but less effective; rewriting is more computationally expensive but yields better results
  - Dolma vs ClueWeb: Dolma is open-access but less curated; ClueWeb is higher quality but requires paid access
  - Response length: Longer responses may contain more information but also more noise

- Failure signatures:
  - Low AlpacaEval win rates despite good data quantity → poor instruction quality or rewriting effectiveness
  - High similarity between rewritten and distilled responses → rewriting is just distillation
  - Filtering removes too many pairs → web text quality issues or instruction generation problems

- First 3 experiments:
  1. Run backtranslation on a small Dolma sample and compare instruction diversity to Alpaca/Self-instruct
  2. Compare filtering vs rewriting on a subset of data using MAUVE scores
  3. Fine-tune a small model (7B) on filtered-only vs rewritten data to validate performance difference

## Open Questions the Paper Calls Out
- How does the quality of rewritten responses compare to responses directly distilled from the same LLM when controlling for instruction diversity and complexity?
- Does the improvement in AlpacaEval win rates from using rewritten data persist across different model families or is it specific to Llama-2 models?
- How does the instruction back-and-forth translation approach scale with increasing amounts of instruction-tuning data, and is there a point of diminishing returns?

## Limitations
- The paper doesn't provide direct human evaluation of instruction quality or response relevance
- No comparison of computational cost between filtering-only and rewriting approaches
- The claim about Dolma being "more cost-effective" than ClueWeb lacks quantitative evidence
- The rewriting prompt details are underspecified, making replication challenging

## Confidence
- High: AlpacaEval win rate improvements (3.2-3.6%) are clearly demonstrated with multiple dataset variants
- Medium: Claims about distribution interpolation and diversity improvements are well-supported by MAUVE analysis
- Low: The absolute quality of backtranslated instructions compared to other synthetic methods is asserted but not directly benchmarked against alternatives

## Next Checks
1. Conduct human evaluation comparing the absolute quality of backtranslated instructions from Dolma versus instructions from Self-instruct and other synthetic methods
2. Perform an ablation study testing whether both filtering and rewriting are necessary by comparing four conditions: raw Dolma, Dolma+filtering, Dolma+rewriting, and Dolma+both
3. Measure the computational cost (GPU hours) of each pipeline stage and calculate cost-per-instruction pair for different approaches