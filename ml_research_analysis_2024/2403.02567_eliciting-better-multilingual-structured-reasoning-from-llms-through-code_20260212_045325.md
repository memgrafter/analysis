---
ver: rpa2
title: Eliciting Better Multilingual Structured Reasoning from LLMs through Code
arxiv_id: '2403.02567'
source_url: https://arxiv.org/abs/2403.02567
tags:
- reasoning
- facts
- code
- fact
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces xSTREET, a multilingual structured reasoning
  benchmark covering 6 languages and 4 tasks. It finds that current multilingual LLMs
  struggle with non-English reasoning tasks.
---

# Eliciting Better Multilingual Structured Reasoning from LLMs through Code

## Quick Facts
- arXiv ID: 2403.02567
- Source URL: https://arxiv.org/abs/2403.02567
- Authors: Bryan Li; Tamer Alkhouli; Daniele Bonadiman; Nikolaos Pappas; Saab Mansour
- Reference count: 22
- Primary result: Code-like prompt formats (SIM) and fine-tuning on code with multilingual comments (TCC) improve multilingual structured reasoning performance without regression on non-reasoning tasks

## Executive Summary
This paper addresses the challenge of multilingual structured reasoning in large language models by introducing xSTREET, a benchmark covering 6 languages and 4 reasoning tasks. The authors find that current multilingual LLMs struggle with non-English reasoning tasks and propose two methods to address this: fine-tuning on a dataset of code with multilingual comments (TCC), and using a novel code-like prompt format (SIM) that mimics reasoning task structure. Their methods show improved multilingual performance on xSTREET, particularly for scientific commonsense reasoning, demonstrating that code can effectively elicit better multilingual structured reasoning abilities in LLMs.

## Method Summary
The authors propose two complementary approaches to improve multilingual structured reasoning: (1) TCC - fine-tuning a base LLM (BLOOMZ) on a dataset of code with machine-translated multilingual comments, and (2) SIM - using a code-like prompt format that represents reasoning steps as function calls with multilingual comments. They evaluate these methods on xSTREET, a multilingual benchmark derived from the STREET dataset, using LoRA fine-tuning and comparing against baseline approaches like direct and linearized prompts. The methods are tested across 6 languages and 4 reasoning tasks including ARC, GSM8K, AQUA_RAT, and AR_LSAT.

## Key Results
- BLOOMZ-TCC and SIM code prompts improve multilingual performance on xSTREET benchmark
- SIM shows consistent improvements on ARC scientific reasoning task across all 6 languages
- TCC fine-tuning outperforms English-only and no-comment variants, showing benefit of multilingual diversity
- Methods improve multilingual reasoning without regression on non-reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Code prompts (SIM) improve multilingual reasoning by separating reasoning structure (code) from language understanding (comments)
- Mechanism: SIM uses function calls to capture reasoning steps while comments provide multilingual input, creating a structured interface that aligns with the LLM's training on code
- Core assumption: LLMs trained on code develop reasoning abilities that can be better elicited through code-like structures than natural language prompts
- Evidence anchors: [abstract] "building on the insight that LLMs trained on code are better reasoners", [section] "We hypothesize that structure, when applied to reasoning problems formulated in different languages, can abstract away some of the language-specific details"
- Break condition: If the LLM hasn't been trained on sufficient code data, or if the reasoning tasks are too symbolic where multilingual understanding is less relevant

### Mechanism 2
- Claim: TCC (translated code comments) dataset provides indirect supervision for multilingual reasoning
- Mechanism: By translating code comments into multiple languages while keeping code intact, the model learns to associate multilingual text with code structure, which transfers to reasoning tasks
- Core assumption: The code and reasoning hypothesis extends to multilingual settings when code is paired with multilingual comments
- Evidence anchors: [abstract] "augment a code dataset with multilingual comments using machine translation while keeping program code as-is", [section] "we investigate whether the code & reasoning hypothesis holds multilingually"
- Break condition: If the code comments are too generic or don't contain meaningful reasoning-related information, or if the translation quality is too poor

### Mechanism 3
- Claim: Instruction-following improves with code prompt formats due to better alignment with training distribution
- Mechanism: Linearized formats are task-specific and diverge from LLM's training data, while code prompts use familiar function-call structures that the model can better follow
- Core assumption: LLMs are better at following instructions when the format matches what they've seen during pre-training
- Evidence anchors: [section] "Ribeiro et al. (2022) find that for linearized (and their model), 62% of generations fail to generate a parsable answer", [section] "Our findings concur, in that linearized has 66% (223/340) invalid generations. In contrast, SIM has only ~19% invalid"
- Break condition: If the model is already highly capable at instruction-following, or if the reasoning tasks are too simple to require complex formatting

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: Understanding how step-by-step reasoning improves performance is fundamental to grasping why the code prompt format works
  - Quick check question: What's the difference between direct prompting and chain-of-thought prompting for arithmetic problems?

- Concept: Multilingual transfer learning
  - Why needed here: The paper leverages multilingual data (translated comments) to improve reasoning across languages, so understanding how multilingual models transfer knowledge is crucial
  - Quick check question: Why might translating code comments be more effective than translating reasoning tasks directly?

- Concept: Low-rank adaptation (LoRA)
  - Why needed here: The finetuning approach uses LoRA to efficiently adapt the base model without losing general capabilities
  - Quick check question: What's the advantage of LoRA over full fine-tuning when adapting a model for a specific task?

## Architecture Onboarding

- Component map: Base LLM (BLOOMZ) → Code Comment Translation → TCC Dataset → LoRA Fine-tuning → xSTREET Evaluation → Code Prompt Inference
- Critical path: TCC generation → LoRA fine-tuning → SIM prompt generation → xSTREET evaluation
- Design tradeoffs: Using 7B model (resource constraints) vs larger models, LoRA (efficient but limited capacity) vs full fine-tuning, machine translation (scalable but lower quality) vs human translation
- Failure signatures: Poor performance on GSM8K/AQUA_RAT (tasks too hard for 7B model), context length issues with Falcon, translation quality issues with TCC
- First 3 experiments:
  1. Evaluate base BLOOMZ on xSTREET ARC task with different prompt formats to establish baseline
  2. Generate TCC dataset and fine-tune BLOOMZ-TCC, then evaluate on xSTREET ARC
  3. Apply SIM code prompts to GPT-3 and compare performance across tasks and languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of the code & reasoning hypothesis vary across different types of reasoning tasks beyond those tested in the paper?
- Basis in paper: [explicit] The paper notes that BLOOMZ and BLOOMZ-TCC struggled with GSM8K, AQUA_RAT, and AR_LSAT tasks, which are arithmetic and logical reasoning tasks, while performing better on ARC (science commonsense reasoning)
- Why unresolved: The paper suggests that the reasoning tasks may be "too hard" for the 7B BLOOMZ model and that truly complex reasoning capabilities are emergent with model scale. However, it doesn't provide a comprehensive analysis of how the hypothesis holds across a wider range of reasoning task types
- What evidence would resolve it: Testing the proposed methods on a broader set of reasoning tasks, including more complex arithmetic, logical, and other types of reasoning tasks, using larger models would provide insights into the hypothesis's generalizability

### Open Question 2
- Question: To what extent does the inclusion of multilingual comments in the training data improve reasoning performance across different languages, including English?
- Basis in paper: [explicit] The ablation study shows that finetuning on TCC (which includes multilingual comments) outperforms finetuning on TCC-en (English-only comments) and TCC-del (no comments), suggesting that the diversity of code comments boosts performance
- Why unresolved: While the ablation study provides initial evidence, it doesn't explore the impact of different levels of multilingual comment diversity or compare the performance across all languages, including English, in a systematic way
- What evidence would resolve it: Conducting experiments with varying degrees of multilingual comment diversity in the training data and evaluating the performance across all languages, including English, would clarify the role of multilingual comments in reasoning performance

### Open Question 3
- Question: How do the proposed methods perform on lower-resource languages not included in the study?
- Basis in paper: [inferred] The paper acknowledges the limitation of studying only 6 languages and suggests that more community effort is needed to expand the study to lower-resource languages
- Why unresolved: The current study focuses on 6 languages from different families and scripts, but doesn't address the performance on lower-resource languages, which may have different linguistic characteristics and challenges
- What evidence would resolve it: Applying the proposed methods to a dataset of reasoning tasks in lower-resource languages and evaluating the performance would provide insights into the methods' effectiveness and limitations in these languages

### Open Question 4
- Question: How does the quality of machine-translated training data affect the performance of the proposed methods?
- Basis in paper: [inferred] The paper uses machine translation for the training and development splits of xSTREET and TCC, with human post-editing only for the test set. It acknowledges that the lower quality of machine-translated exemplars may worsen performance compared to gold standard exemplars
- Why unresolved: The paper doesn't explore the impact of the quality of machine-translated training data on the performance of the proposed methods, which could be significant given the reliance on machine translation
- What evidence would resolve it: Conducting experiments with varying qualities of machine-translated training data, including gold standard translations, and evaluating the performance of the proposed methods would clarify the importance of translation quality in the training process

## Limitations

- Limited to 6 languages, excluding many lower-resource languages that may present different challenges
- Reliance on machine translation for training data may introduce noise that affects learning quality
- Evaluation only on 7B parameter model limits generalizability to larger models with potentially different scaling properties
- SIM prompt format lacks complete specification of how structured reasoning graphs are converted to code-like format

## Confidence

**High Confidence**: The observation that multilingual LLMs struggle with non-English reasoning tasks is well-supported by the xSTREET benchmark results across multiple models (BLOOMZ, GPT-3, Falcon). The performance gap between English and other languages is consistently demonstrated.

**Medium Confidence**: The effectiveness of the SIM code-like prompt format is reasonably well-supported, particularly for the ARC scientific reasoning task where it shows consistent improvements. However, the benefits are less clear for other tasks (GSM8K, AQUA_RAT, AR_LSAT) where results are mixed or show regression.

**Low Confidence**: The claim that code training inherently improves reasoning abilities in LLMs lacks strong empirical support. The paper shows correlation between code fine-tuning and improved reasoning, but doesn't establish causation or rule out alternative explanations like improved instruction-following capabilities.

## Next Checks

1. **Mechanism Dissection Experiment**: Compare SIM prompts against equivalent natural language chain-of-thought prompts on the same tasks to isolate whether the benefits come from the code structure itself or simply from providing clearer reasoning instructions.

2. **Translation Quality Control**: Evaluate model performance when using human-translated versus machine-translated code comments in the TCC dataset to quantify the impact of translation quality on downstream reasoning performance.

3. **Scaling Study**: Test the proposed methods (TCC fine-tuning and SIM prompts) across multiple model scales (1B, 7B, 13B, 30B parameters) to determine whether the observed improvements are consistent across different model sizes or specific to the 7B regime.