---
ver: rpa2
title: 'The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio Benchmarks
  and Novel Data'
arxiv_id: '2403.14048'
source_url: https://arxiv.org/abs/2403.14048
tags:
- audio
- vocal
- emotion
- dataset
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces several open-source datasets to address the
  challenge of limited audio data in specialized domains like human-computer interaction
  and human behavior analysis. It presents four datasets: HUME-PROSODY for emotional
  speech recognition, HUME-VOCAL BURST for vocal burst classification, MODULATE-SONATA
  for acted emotional speech, and MODULATE-STREAM for in-game streamer audio.'
---

# The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio Benchmarks and Novel Data

## Quick Facts
- arXiv ID: 2403.14048
- Source URL: https://arxiv.org/abs/2403.14048
- Authors: Alice Baird; Rachel Manzelli; Panagiotis Tzirakis; Chris Gagne; Haoqi Li; Sadie Allen; Sander Dieleman; Brian Kulis; Shrikanth S. Narayanan; Alan Cowen
- Reference count: 40
- One-line primary result: Introduces four open-source audio datasets and establishes baseline models for speech emotion recognition and generation tasks.

## Executive Summary
This paper presents four novel open-source datasets designed to address the challenge of limited audio data in specialized domains like human-computer interaction and human behavior analysis. The datasets include HUME-PROSODY for emotional speech recognition, HUME-VOCAL BURST for vocal burst classification, MODULATE-SONATA for acted emotional speech, and MODULATE-STREAM for in-game streamer audio. The authors establish baselines using Wav2Vec2 and HuBERT embeddings with various classifiers, demonstrating strong performance across tasks. This work provides valuable resources for advancing research in speech emotion recognition and generation, with potential applications in human-computer interaction and behavior analysis.

## Method Summary
The paper introduces four datasets: HUME-PROSODY (41h audio, 1,004 speakers, 9 emotion classes), HUME-VOCAL BURST (37h audio, 1,702 speakers, 10 emotion classes), MODULATE-SONATA (6h acted emotional speech with 25 classes), and MODULATE-STREAM (7,000h gaming streams with 940 speakers). For baseline models, the authors use Wav2Vec2 and HuBERT embeddings extracted from audio files, followed by various classifiers such as Support Vector Machines, Logistic Regression, and regression models. The performance is evaluated using metrics like Unweighted Average Recall (UAR) and Concordance Correlation Coefficient (CCC) for classification tasks, and FrÃ©chet Inception Distance (FID) and Human Evaluation of Emotional Power (HEEP) for generation tasks. The paper also explores late fusion approaches combining Wav2Vec2 and HuBERT embeddings to improve performance across tasks.

## Key Results
- HUME-PROSODY achieved a Unweighted Average Recall (UAR) of 0.64 and a Concordance Correlation Coefficient (CCC) of 0.53 using Wav2Vec2 embeddings with a Support Vector Regressor.
- HUME-VOCAL BURST showed a UAR of 0.50 and a CCC of 0.44 with HuBERT embeddings and a Support Vector Regressor.
- MODULATE-SONATA achieved a UAR of 0.32 and a CCC of 0.37 using Wav2Vec2 embeddings with a Support Vector Regressor.
- MODULATE-STREAM demonstrated a UAR of 0.45 and a CCC of 0.41 with HuBERT embeddings and a Support Vector Regressor.

## Why This Works (Mechanism)
None

## Foundational Learning
- **Wav2Vec2 and HuBERT embeddings**: Self-supervised learning models that extract meaningful representations from raw audio, crucial for capturing emotional content in speech.
  - Why needed: These models provide robust feature representations that capture emotional nuances in speech, which are essential for accurate emotion recognition and generation.
  - Quick check: Compare performance of models using Wav2Vec2 and HuBERT embeddings to those using traditional handcrafted features.

- **Unweighted Average Recall (UAR)**: A performance metric that averages recall across all classes, providing a balanced evaluation for imbalanced datasets.
  - Why needed: UAR ensures that the model's performance is not biased towards majority classes, which is important in emotion recognition where some emotions may be underrepresented.
  - Quick check: Calculate UAR for each class separately to identify any significant performance disparities.

- **Concordance Correlation Coefficient (CCC)**: A measure of the agreement between predicted and true values, considering both precision and accuracy.
  - Why needed: CCC provides a comprehensive evaluation of the model's ability to predict continuous emotional dimensions, capturing both the direction and magnitude of predictions.
  - Quick check: Compare CCC scores with other correlation metrics like Pearson correlation to validate the model's performance.

## Architecture Onboarding
- **Component map**: Raw audio -> Wav2Vec2/HuBERT embeddings -> Classifier/Regressor -> Predicted emotion labels
- **Critical path**: The extraction of embeddings from raw audio is critical, as it directly influences the quality of the input features for the classifier.
- **Design tradeoffs**: The choice between Wav2Vec2 and HuBERT embeddings involves balancing between model size and performance, as HuBERT may provide better results but at the cost of increased computational resources.
- **Failure signatures**: Poor performance on minority emotion classes may indicate class imbalance issues, while low CCC scores could suggest that the model struggles with predicting continuous emotional dimensions accurately.
- **3 first experiments**:
  1. Train a baseline model using Wav2Vec2 embeddings and a Support Vector Regressor on the HUME-PROSODY dataset to verify the reported UAR and CCC scores.
  2. Experiment with different classifiers (e.g., Logistic Regression, Random Forest) using HuBERT embeddings on the HUME-VOCAL BURST dataset to explore alternative approaches.
  3. Perform a late fusion of Wav2Vec2 and HuBERT embeddings on the MODULATE-STREAM dataset to assess potential performance improvements.

## Open Questions the Paper Calls Out
- **Open Question 1**: What is the optimal embedding method and classifier combination for achieving the highest performance on the HUME-PROSODY dataset?
  - Basis in paper: [explicit] The paper provides baseline results using Wav2Vec2 embeddings with a Support Vector Regressor, but suggests that other methods may be explored.
  - Why unresolved: The paper only presents initial baseline results and encourages researchers to test their approaches against established benchmarks, indicating that further experimentation is needed to determine the optimal method.
  - What evidence would resolve it: Comparative results of different embedding methods (e.g., HuBERT, Wav2Vec2) and classifiers (e.g., Logistic Regression, Support Vector Regressor) on the HUME-PROSODY dataset would provide insights into the optimal combination.

- **Open Question 2**: How does the performance of emotion recognition models vary across different cultural contexts when using the HUME-VOCAL BURST dataset?
  - Basis in paper: [explicit] The HUME-VOCAL BURST dataset includes data from speakers in China, South Africa, the U.S., and Venezuela, with ratings for 10 emotions.
  - Why unresolved: While the dataset includes cross-cultural data, the paper does not provide results on how models perform across different cultural contexts, leaving this as an area for further investigation.
  - What evidence would resolve it: Performance metrics of emotion recognition models trained on the HUME-VOCAL BURST dataset, evaluated separately for each cultural group, would clarify how cultural context affects model performance.

- **Open Question 3**: What are the potential applications of the MODULATE-STREAM dataset in real-time emotion analysis and human-computer interaction?
  - Basis in paper: [inferred] The MODULATE-STREAM dataset contains over 7000 hours of in-game streamer audio, suggesting its potential use in analyzing real-time emotional responses in interactive environments.
  - Why unresolved: The paper does not explore specific applications of the MODULATE-STREAM dataset, focusing instead on providing the dataset and encouraging its use for various tasks.
  - What evidence would resolve it: Studies demonstrating the use of the MODULATE-STREAM dataset for real-time emotion analysis in gaming or other interactive contexts would highlight its practical applications and effectiveness.

## Limitations
- The paper lacks detailed experimental analysis and comprehensive hyperparameter tuning, which limits the evaluation of the true significance of the reported results.
- The quality and diversity of annotations across different datasets may vary, potentially impacting downstream model performance and generalization.
- The absence of comparisons to state-of-the-art methods in the field makes it challenging to assess the relative performance of the proposed baselines.

## Confidence
- **Medium confidence** in the datasets' utility and general methodology - The paper provides clear dataset descriptions and establishes reasonable baselines, but lacks detailed experimental analysis.
- **Medium confidence** in the reported baseline results - While the metrics are sound, the absence of comprehensive hyperparameter tuning and architectural exploration limits confidence in the absolute performance claims.
- **High confidence** in the contribution of open-source datasets - The paper clearly describes four new datasets with appropriate splits and metadata, which is a valuable contribution regardless of the specific baseline results.

## Next Checks
1. Reproduce baseline results: Train Wav2Vec2 and HuBERT-based classifiers on each dataset using the provided splits and compare performance against reported metrics to verify baseline claims.
2. Dataset quality assessment: Analyze class distributions, speaker diversity, and annotation consistency across all four datasets to evaluate their potential for robust model training.
3. Cross-dataset generalization: Test whether models trained on one dataset (e.g., HUME-PROSODY) can generalize to another (e.g., MODULATE-STREAM) to assess domain transferability and identify potential biases.