---
ver: rpa2
title: 'CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and Selective
  Sparsification'
arxiv_id: '2409.01366'
source_url: https://arxiv.org/abs/2409.01366
tags:
- activation
- chess
- sparsification
- sparsity
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying large language
  models (LLMs) on edge devices by optimizing activation sparsity during inference.
  The authors propose CHESS, a method that combines channel-wise thresholding for
  feed-forward network (FFN) layers and selective sparsification for attention modules
  to reduce the number of activated parameters while minimizing performance degradation.
---

# CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification

## Quick Facts
- arXiv ID: 2409.01366
- Source URL: https://arxiv.org/abs/2409.01366
- Reference count: 12
- One-line primary result: CHESS achieves up to 1.27x end-to-end speedup with lower performance degradation compared to existing methods while activating fewer parameters.

## Executive Summary
This paper addresses the challenge of deploying large language models (LLMs) on edge devices by optimizing activation sparsity during inference. The authors propose CHESS, a method that combines channel-wise thresholding for feed-forward network (FFN) layers and selective sparsification for attention modules to reduce the number of activated parameters while minimizing performance degradation. The method also includes efficient sparse kernel implementations to accelerate inference. Experiments on eight downstream tasks with various LLMs show that CHESS achieves up to 1.27x end-to-end speedup with lower performance degradation compared to existing methods, while activating fewer parameters. The approach is effective across different sparsity levels and offers a promising solution for efficient LLM deployment on resource-constrained devices.

## Method Summary
CHESS optimizes LLM inference through a three-pronged approach. First, channel-wise thresholding assigns unique thresholds to each activation channel in FFN layers based on importance scores computed from expected activation magnitudes. Second, selective sparsification applies thresholding-based activation sparsification to specific submodules in attention layers, targeting rows with similar ℓ2 norms for efficient pruning. Finally, the method implements custom sparse kernels (spvmm and vmmsp) to efficiently handle sparse activations during inference. The approach is designed to be training-free and can be applied to quantized models as well, making it suitable for edge deployment scenarios.

## Key Results
- Achieves up to 1.27x end-to-end speedup compared to baseline models
- Reduces performance degradation compared to existing sparsification methods
- Activates fewer parameters while maintaining competitive accuracy on 8 downstream tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Channel-wise thresholding improves sparsity by assigning unique thresholds to each activation channel based on importance scores.
- Mechanism: Importance scores are computed per channel using the expected magnitude of the corresponding activation tensor, allowing finer-grained pruning decisions that minimize performance degradation.
- Core assumption: Within each channel, the magnitude of the activation tensor (aup) remains relatively consistent across different inputs.
- Evidence anchors:
  - [abstract] "First, channel-wise thresholding assigns a unique threshold to each activation channel in the feed-forward network (FFN) layers."
  - [section] "Based on this observation, this paper estimates the |aup_i| using the expectation of |aup_i| over sampled training data... the importance score is further estimated as, ˆscorei = E|aup_i||agate_i|"
  - [corpus] Weak evidence; related work (WINA, La RoSA) suggests channel-wise approaches improve sparsity but not specific to this importance score formulation.
- Break condition: If the assumption about consistent activation magnitudes within channels fails, the estimated importance scores would be inaccurate, leading to suboptimal pruning.

### Mechanism 2
- Claim: Selective sparsification targets specific attention submodules to reduce memory access and computation while minimizing performance degradation.
- Mechanism: By analyzing the weight matrices, the method identifies that rows within the same weight tensor have similar ℓ2 norms, allowing simplification of the pruning objective to focus on input activation magnitudes.
- Core assumption: All rows from the same weight tensor exhibit similar ∥Wi,:∥2^2, allowing the coefficient to be eliminated from the pruning objective.
- Evidence anchors:
  - [abstract] "Then, selective sparsification involves applying thresholding-based activation sparsification to specific layers within the attention modules."
  - [section] "Figure 2 shows the distribution of ∥Wi,:∥2^2 of different rows in projection weights... we can eliminate this coefficient from Equation 17 and derive the simplified final objective: arg min P Σ|xi|"
  - [corpus] Weak evidence; related work (CATS) applies thresholding but doesn't demonstrate the weight norm similarity analysis for selective targeting.
- Break condition: If weight row norms vary significantly within a tensor, the simplified objective would be inaccurate, leading to suboptimal pruning decisions.

### Mechanism 3
- Claim: Custom sparse kernels achieve end-to-end speedup by efficiently handling sparse activations during inference.
- Mechanism: Two specialized kernels (spvmm and vmmsp) are implemented to bypass unnecessary computations and memory accesses when processing sparse activation tensors.
- Core assumption: The overhead of managing sparse data structures is offset by the reduction in unnecessary computations and memory accesses.
- Evidence anchors:
  - [abstract] "Finally, we detail the implementation of sparse kernels to accelerate LLM inference."
  - [section] "Algorithm 3.1 and Algorithm 3.2 show the detailed steps of spvmm and vmmsp... Both algorithms reduce the latency by bypassing unnecessary weight reads and computations."
  - [corpus] Moderate evidence; related work (Turbo Sparse) also mentions custom kernels but doesn't detail the specific implementations.
- Break condition: If the sparsity levels are too low, the overhead of managing sparse data structures may outweigh the benefits of bypassing computations.

## Foundational Learning

- Concept: Activation functions and their impact on sparsity
  - Why needed here: Understanding how different activation functions (ReLU, GeLU, SiLU, SwiGLU) affect activation sparsity is crucial for appreciating the motivation behind CHESS.
  - Quick check question: How does the choice of activation function influence the sparsity of activation tensors in neural networks?

- Concept: Linear algebra operations in neural networks
  - Why needed here: CHESS relies on understanding matrix multiplications and their relationship to model performance when activations are pruned.
  - Quick check question: How does pruning elements from an activation tensor affect the output of subsequent linear layers?

- Concept: Statistical analysis and estimation
  - Why needed here: CHESS uses statistical methods to estimate importance scores and thresholds based on sampled data, requiring an understanding of these techniques.
  - Quick check question: How can expected values and cumulative distribution functions be used to estimate thresholds for pruning activations?

## Architecture Onboarding

- Component map:
  - FFN layers with channel-wise thresholding
  - Attention modules with selective sparsification
  - Custom sparse kernels (spvmm and vmmsp)
  - Threshold estimation module
  - Integration with existing LLM inference pipeline

- Critical path:
  1. Estimate thresholds for channel-wise thresholding using sampled data
  2. Apply channel-wise thresholding to FFN activations
  3. Apply selective sparsification to attention module activations
  4. Use custom sparse kernels to perform matrix multiplications with sparse activations
  5. Compute final model outputs

- Design tradeoffs:
  - Granularity vs. overhead: Channel-wise thresholding provides finer-grained control but requires computing importance scores per channel.
  - Sparsity level vs. performance: Higher sparsity levels lead to greater speedup but may cause more performance degradation.
  - Custom kernels vs. general purpose: Custom kernels are optimized for specific sparsity patterns but require more development effort.

- Failure signatures:
  - Inaccurate threshold estimation: Model performance degrades more than expected.
  - Inefficient kernel implementation: End-to-end speedup is lower than predicted.
  - Poor selection of attention submodules: Performance degradation is higher than necessary for the achieved sparsity.

- First 3 experiments:
  1. Compare the accuracy of CHESS with different sparsity levels on a small downstream task.
  2. Measure the end-to-end inference speedup of CHESS compared to the baseline model.
  3. Analyze the distribution of activation magnitudes within channels to validate the assumption of consistent magnitudes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CHESS perform when fine-tuned compared to training-free scenarios?
- Basis in paper: [explicit] The paper mentions that Relufication achieves competitive performance when fine-tuned but struggles in training-free scenarios.
- Why unresolved: The paper only evaluates CHESS in a training-free context and does not explore its performance with fine-tuning.
- What evidence would resolve it: Experiments comparing CHESS performance with and without fine-tuning across various sparsity levels and downstream tasks.

### Open Question 2
- Question: What is the impact of CHESS on LLM performance when applied to different batch sizes beyond 1?
- Basis in paper: [inferred] The paper notes that CHESS performs optimally with a batch size of 1, which is suitable for edge deployment, but its effectiveness diminishes with larger batch sizes.
- Why unresolved: The paper does not provide experimental results or analysis for batch sizes greater than 1.
- What evidence would resolve it: Performance and speedup metrics for CHESS across a range of batch sizes, particularly in data center deployment scenarios.

### Open Question 3
- Question: How does CHESS compare to other activation sparsification methods when applied to quantized models?
- Basis in paper: [explicit] The paper states that CHESS can be applied to quantized models as well, but does not provide comparative results.
- Why unresolved: The paper does not include experiments or analysis on quantized models, leaving the effectiveness of CHESS in this context unexplored.
- What evidence would resolve it: Comparative studies of CHESS against other sparsification methods on quantized LLMs, focusing on performance and efficiency metrics.

## Limitations
- The paper relies on statistical estimation of importance scores and thresholds based on sampled data, which may not generalize perfectly to all inputs.
- The effectiveness of channel-wise thresholding depends on the assumption that activation magnitudes within channels remain relatively consistent across different inputs.
- The simplified pruning objective for selective sparsification assumes that all rows from the same weight tensor exhibit similar norms.

## Confidence
- **High**: The paper provides sufficient evidence for the claim that CHESS achieves up to 1.27x end-to-end speedup with lower performance degradation compared to existing methods.
- **Medium**: The paper presents a reasonable mechanism for channel-wise thresholding and selective sparsification, but the exact implementation details are not fully specified.
- **Low**: The paper relies on several assumptions (consistent activation magnitudes within channels, similar weight row norms) that are not extensively validated.

## Next Checks
1. Analyze the distribution of activation magnitudes within channels across different inputs to validate the assumption of consistent magnitudes.
2. Measure the impact of the simplified pruning objective on the achieved sparsity and performance degradation by comparing it with the full objective that accounts for varying weight row norms.
3. Evaluate the effectiveness of the custom sparse kernels on different sparsity levels and patterns to assess their robustness and generalizability.