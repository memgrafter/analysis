---
ver: rpa2
title: 'MoExtend: Tuning New Experts for Modality and Task Extension'
arxiv_id: '2408.03511'
source_url: https://arxiv.org/abs/2408.03511
tags:
- experts
- moextend
- arxiv
- layer
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoExtend is a framework that extends Mixture-of-Experts (MoE) models
  to multimodal data by introducing new modality-specific experts and calibration
  modules, avoiding full fine-tuning of pretrained models. It aligns visual and text
  modalities via a pretrained CLIP encoder, uses an adaptive extender to select which
  MoE layers to augment based on gate probability shifts, and fine-tunes only the
  new experts and routers to mitigate catastrophic forgetting.
---

# MoExtend: Tuning New Experts for Modality and Task Extension

## Quick Facts
- arXiv ID: 2408.03511
- Source URL: https://arxiv.org/abs/2408.03511
- Reference count: 15
- Primary result: MoExtend extends MoE models to multimodal data by adding modality-specific experts and calibration modules, avoiding full fine-tuning of pretrained models.

## Executive Summary
MoExtend is a framework designed to extend Mixture-of-Experts (MoE) models to multimodal data, particularly vision-language tasks, without the computational burden and risk of catastrophic forgetting associated with full fine-tuning. It introduces new modality-specific experts and calibration modules while freezing pretrained parameters, and uses an adaptive extender to select which MoE layers to augment based on gate probability shifts. The framework aligns visual and text modalities via a pretrained CLIP encoder and fine-tunes only the new experts and routers. Experimental results demonstrate that MoExtend achieves training speed acceleration of approximately 6× faster than full fine-tuning while delivering performance close to state-of-the-art multimodal models like LLaVA-1.5, with reduced parameter usage and maintained performance on pure text tasks.

## Method Summary
MoExtend is a three-stage framework for extending pretrained MoE language models to multimodal tasks. First, it aligns vision and text modalities using a pretrained CLIP encoder and adds an MLP projection layer to map visual features into the model's embedding space. Second, it uses an Extender module to select which MoE layers to augment by measuring shifts in gate probability distributions when fine-tuning on new modality data. Third, it initializes new experts and routers by copying from the most responsive existing expert and fine-tunes only these new components along with calibration modules to mitigate catastrophic forgetting. The framework avoids tuning pretrained parameters, focusing adaptation on new experts and routers.

## Key Results
- MoExtend achieves training speed acceleration of approximately 6× faster than full fine-tuning.
- Delivers performance close to state-of-the-art multimodal models like LLaVA-1.5.
- Maintains performance on pure text tasks while adding multimodal capabilities.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding new modality-specific experts without tuning existing parameters preserves prior knowledge and avoids catastrophic forgetting.
- Mechanism: MoExtend inserts new experts into selected MoE layers and freezes pretrained parameters, so only the new experts and their routers are fine-tuned. This isolates adaptation to new modalities from interference with existing capabilities.
- Core assumption: The pretrained MoE model already encodes strong task-specific knowledge, and adding complementary experts does not disrupt the gating decisions for original modalities.
- Evidence anchors:
  - [abstract] "MoExtend avoids tuning pretrained models, thus mitigating the risk of catastrophic forgetting."
  - [section] "MoExtend maintains the original MoE model parameters unchanged, while only trains the newly added experts and the corresponding gate layer."
- Break condition: If the pretrained model has severe overfitting or the new modality is too dissimilar, the new experts may not integrate smoothly and performance may degrade.

### Mechanism 2
- Claim: Adaptive layer selection via the Extender reduces computational cost and overfitting risk.
- Mechanism: The Extender compares gate probability distributions before and after fine-tuning on new modality data; layers with large shifts are selected for expert addition. This focuses adaptation where it is most needed.
- Core assumption: Changes in gate distributions indicate where the model would benefit from additional expert capacity.
- Evidence anchors:
  - [section] "We rank the MoE layers based on dj and introduce a new expert FFNm+1 to the top ⌊pL⌋ layers..."
  - [section] "This approach not only increases the parameter count significantly, leading to greater computational costs during training but also poses a potential risk of overfitting due to blindly adding a large number of parameters."
- Break condition: If the new modality data is not representative or the validation set is too small, the layer selection may misidentify where to add experts, reducing efficiency gains.

### Mechanism 3
- Claim: Proper initialization of new experts and routers via copying from the most responsive existing expert improves integration and performance.
- Mechanism: New experts are initialized by copying weights from the existing expert with the highest gate probability on the new modality data; routers are copied correspondingly, ensuring the new experts are immediately useful.
- Core assumption: The existing expert with highest response to the new modality is the best template for a new modality-specific expert.
- Evidence anchors:
  - [section] "We consider directly copying the expert and router parameters corresponding to max(rκ1j, rκ2j, · · · , rκmj), as initialization for the new parameters."
  - [section] "Models initialized with Zero and Mean are both unbalanced in expert selection, while MoExtend is more balanced."
- Break condition: If the most responsive existing expert is not truly optimal for the new modality, copying may propagate suboptimal behavior to the new expert.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why naive fine-tuning on new modalities degrades performance on original tasks.
  - Quick check question: What happens to a model's performance on previous tasks when it is fine-tuned only on new data without any preservation strategy?

- Concept: Mixture-of-Experts (MoE) routing and gating
  - Why needed here: Knowing how MoE layers dynamically select experts based on input tokens is key to understanding how MoExtend inserts new experts without breaking existing routing.
  - Quick check question: How does an MoE router decide which expert(s) to activate for a given token?

- Concept: Layer selection and pruning techniques
  - Why needed here: The Extender uses a similar idea to pruning—identifying which layers are most sensitive to changes—to decide where to add new experts.
  - Quick check question: In neural network pruning, what metric is often used to determine which layers or neurons are least important?

## Architecture Onboarding

- Component map:
  - Pretrained MoE backbone (frozen)
  - Vision encoder (frozen CLIP)
  - New modality-specific experts (trainable)
  - New router columns for new experts (trainable)
  - Calibration modules (trainable, one per expert)
  - Extender module (inference-only, for layer selection)

- Critical path:
  1. Alignment: map new modality into model space.
  2. Extension: use Extender to pick layers and initialize new experts.
  3. Fine-tuning: train only new experts, routers, and calibration modules.

- Design tradeoffs:
  - Adding experts to more layers increases capacity but also cost and overfitting risk.
  - Copying initialization speeds integration but may propagate suboptimal behavior.
  - Calibration modules reduce forgetting but add parameters and complexity.

- Failure signatures:
  - Large drop in original task performance → forgetting not fully mitigated.
  - Slow convergence or unstable loss → poor initialization or miscalibrated gates.
  - No performance gain on new modality → wrong layers chosen or new experts not trained.

- First 3 experiments:
  1. Ablation: add new experts to all layers vs. only selected layers (measure accuracy and training time).
  2. Ablation: try different initialization schemes for new experts (copy vs. zero vs. mean).
  3. Ablation: train with and without calibration modules (measure original vs. new modality performance).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effectively can MoExtend be extended to non-visual modalities like speech or text?
- Basis in paper: [inferred] The authors state MoExtend is a general approach but only validate it on visual tasks due to GPU resource constraints.
- Why unresolved: No experiments or ablation studies on modalities beyond vision, leaving scalability to other data types unproven.
- What evidence would resolve it: Empirical results showing successful adaptation of MoExtend to speech or text modalities with comparable performance to vision tasks.

### Open Question 2
- Question: What is the optimal balance between the number of new experts added and model performance?
- Basis in paper: [explicit] The authors discuss adding experts to only a subset of layers but do not explore the trade-off between number of experts and performance.
- Why unresolved: The paper does not conduct extensive experiments varying the number of new experts to determine the optimal configuration.
- What evidence would resolve it: Systematic ablation studies varying the number of new experts and measuring performance impact across different tasks.

### Open Question 3
- Question: How does MoExtend handle catastrophic forgetting in scenarios with frequent modality additions?
- Basis in paper: [explicit] The authors claim MoExtend mitigates catastrophic forgetting but only evaluate it in a single modality addition scenario.
- Why unresolved: No experiments testing the model's performance when new modalities are added sequentially or frequently, which could reveal limitations.
- What evidence would resolve it: Longitudinal studies tracking performance degradation or stability when multiple modalities are added over time.

### Open Question 4
- Question: How does the initialization strategy for new experts affect long-term model stability and performance?
- Basis in paper: [explicit] The authors discuss the importance of initialization but do not explore its long-term effects on model behavior.
- Why unresolved: The paper focuses on initial performance but does not investigate how different initialization strategies impact model stability or performance over extended training periods.
- What evidence would resolve it: Long-term training experiments comparing different initialization strategies and their effects on model convergence and stability.

## Limitations
- Experimental evaluation focuses primarily on image-to-text generation benchmarks, with limited coverage of other multimodal tasks.
- The claim of "approximately 6× faster" training speed is not benchmarked against other efficient fine-tuning methods like LoRA or adapters.
- The method's effectiveness may depend heavily on the quality of the initial alignment stage and the representativeness of the layer selection data.

## Confidence
- **High confidence**: Claims about catastrophic forgetting mitigation and the basic MoE extension mechanism.
- **Medium confidence**: Claims about training speed acceleration and overall performance gains.
- **Low confidence**: Claims about the generality of the Extender's layer selection method.

## Next Checks
1. **Ablation on Layer Selection**: Compare MoExtend's adaptive layer selection with naive extension (adding experts to all layers) on a held-out multimodal task to quantify the trade-off between efficiency and performance.
2. **Cross-Modal Generalization**: Test MoExtend on a non-vision modality (e.g., audio or video) to evaluate whether the framework generalizes beyond image-language tasks.
3. **Comparison with Efficient Fine-Tuning**: Benchmark MoExtend against other parameter-efficient methods (e.g., LoRA, adapters) on both training time and task performance to contextualize the claimed speed-up.