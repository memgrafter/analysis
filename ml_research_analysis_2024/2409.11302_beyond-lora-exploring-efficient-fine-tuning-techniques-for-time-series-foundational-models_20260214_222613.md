---
ver: rpa2
title: 'Beyond LoRA: Exploring Efficient Fine-Tuning Techniques for Time Series Foundational
  Models'
arxiv_id: '2409.11302'
source_url: https://arxiv.org/abs/2409.11302
tags:
- peft
- parameters
- lora
- chronos
- fourierft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores Parameter-Efficient Fine-Tuning (PEFT) techniques
  for adapting Time Series Foundation Models (TSFMs) to healthcare applications, specifically
  ICU vitals forecasting for sepsis patients. The authors evaluate two selective PEFT
  methods (BitFit, LayerNorm Tuning) and two additive PEFT methods (VeRA, FourierFT)
  on multiple Chronos TSFM configurations.
---

# Beyond LoRA: Exploring Efficient Fine-Tuning Techniques for Time Series Foundational Models

## Quick Facts
- arXiv ID: 2409.11302
- Source URL: https://arxiv.org/abs/2409.11302
- Reference count: 33
- This paper demonstrates that FourierFT applied to Chronos (Tiny) variant surpasses the SOTA model while fine-tuning only 2,400 parameters compared to the 700K parameters of the benchmark.

## Executive Summary
This paper investigates Parameter-Efficient Fine-Tuning (PEFT) techniques for adapting Time Series Foundation Models (TSFMs) to healthcare applications, specifically ICU vitals forecasting for sepsis patients. The authors evaluate selective PEFT methods (BitFit, LayerNorm Tuning) and additive PEFT methods (VeRA, FourierFT) on multiple Chronos TSFM configurations. The comparative analysis reveals that certain PEFT methods outperform LoRA in parameter efficiency and domain adaptation, achieving state-of-the-art results in ICU vital forecasting tasks. Notably, FourierFT achieves superior performance while fine-tuning only 2,400 parameters, demonstrating the potential of spectral coefficient-based adaptation for time series models.

## Method Summary
The study focuses on adapting Time Series Foundation Models (TSFMs) for ICU vital sign forecasting in sepsis patients, specifically MeanBP and HR prediction. The authors use the eICU Collaborative Research Database with 4,020 samples from 1,442 patients, preprocessed with forward-fill imputation, 9-hour windows (6h context, 3h horizon), 5-minute sampling, low-pass filtering, and min-max scaling. They evaluate Parameter-Efficient Fine-Tuning (PEFT) techniques including BitFit, LayerNorm Tuning, VeRA, and FourierFT applied to Chronos TSFM variants (Tiny, Small, Base) with learning rates 1e-2 to 1e-5 using Adam optimizer. The performance is measured using Mean Squared Error (MSE), Dynamic Time Warping (DTW), and Mean Average Percentage Error (MAPE) for both MeanBP and HR forecasting, with 10-run averaging for robustness.

## Key Results
- FourierFT applied to Chronos (Tiny) variant surpasses the SOTA model while fine-tuning only 2,400 parameters compared to the 700K parameters of the benchmark
- Fine-tuning as few as 256 parameters (BitFit) can yield competitive results in ICU vitals forecasting
- Certain PEFT methods outperform LoRA in parameter efficiency and domain adaptation for time series tasks

## Why This Works (Mechanism)

### Mechanism 1
FourierFT encodes model parameters into a small set of spectral coefficients, which are learned and shared across layers. By reducing the number of trainable parameters in the spectral domain, the model achieves competitive performance with minimal computational overhead. The core assumption is that a small number of sparse coefficients in the spectral domain are sufficient to capture the essential domain adaptation needed for ICU vitals forecasting.

### Mechanism 2
BitFit adjusts only the bias parameters within the model architecture, allowing for task-specific adaptations with minimal changes to the overall model structure. The core assumption is that bias terms contain sufficient information to adapt the model to the specific task without modifying the weights.

### Mechanism 3
VeRA introduces fixed, randomly initialized rank matrices shared across layers, with learnable scaling vectors that modulate the adaptation. The higher rank compensates for the limited fine-tuning to the scaling vectors. The core assumption is that the scaling vectors can effectively modulate the fixed rank matrices to achieve the desired domain adaptation.

## Foundational Learning

- Concept: Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: PEFT allows adaptation of large pre-trained models to specific tasks with minimal computational resources, crucial for healthcare applications with limited data
  - Quick check question: What is the main advantage of using PEFT over full fine-tuning in the context of this paper?

- Concept: Transformer Architecture
  - Why needed here: The Chronos TSFM uses a transformer architecture, which is essential for understanding how PEFT methods interact with the model
  - Quick check question: How does the transformer architecture facilitate the application of PEFT techniques?

- Concept: Time Series Foundation Models (TSFMs)
  - Why needed here: TSFMs are the backbone for forecasting tasks in this paper, and understanding their properties is crucial for applying PEFT effectively
  - Quick check question: What are the key challenges in applying TSFMs to healthcare data, as mentioned in the paper?

## Architecture Onboarding

- Component map: Chronos TSFM (Tiny, Small, Base) -> PEFT techniques (BitFit, LayerNorm Tuning, VeRA, FourierFT) -> eICU dataset -> Evaluation metrics (MSE, DTW, MAPE)
- Critical path: Preprocess the eICU dataset → Apply PEFT to Chronos TSFM → Evaluate performance on MeanBP and HR forecasting
- Design tradeoffs: Balancing parameter efficiency with performance; choosing between selective and additive PEFT methods; selecting appropriate hyperparameters for each PEFT technique
- Failure signatures: Poor performance on forecasting metrics; overfitting to training data; high computational costs despite using PEFT
- First 3 experiments:
  1. Apply BitFit to Chronos (Tiny) and evaluate on MeanBP forecasting
  2. Apply FourierFT to Chronos (Small) and compare performance with LoRA
  3. Experiment with different ranks for VeRA on Chronos (Base) and analyze the impact on performance

## Open Questions the Paper Calls Out

### Open Question 1
How do hybrid approaches that combine multiple PEFT techniques perform compared to individual methods for TSFM adaptation in healthcare applications? The paper only evaluates individual PEFT methods and explicitly plans to investigate hybrid approaches in future work.

### Open Question 2
What is the optimal rank for VeRA adapters when applied to different sizes of Chronos models and other TSFMs? The paper only tested rank 16 for VeRA on Chronos (Tiny) variant and acknowledges the need to explore this further.

### Open Question 3
How do PEFT techniques perform when applied to more complex TSFMs beyond the Chronos family? The study only evaluates PEFT methods on Chronos variants and expresses interest in testing more complex TSFMs.

## Limitations

- The findings are primarily constrained by focus on a single healthcare dataset (eICU) and two specific vital signs (MeanBP and HR), limiting generalizability to other medical domains or time series tasks
- The evaluation of PEFT methods is based on a relatively small parameter range (2,400 to 49,000 parameters), which may not fully represent the capabilities of these methods when applied to larger models or different architectures
- The paper does not address potential overfitting risks when fine-tuning with minimal parameters, particularly for FourierFT which showed sensitivity to initialization

## Confidence

- **High Confidence**: The claim that FourierFT applied to Chronos (Tiny) achieves state-of-the-art performance with minimal parameters is well-supported by the empirical results and the specific metric comparisons provided in the paper
- **Medium Confidence**: The assertion that BitFit can yield competitive results by fine-tuning only bias terms is supported by the evidence, but the broader applicability of this approach across different tasks and model scales remains to be fully established
- **Medium Confidence**: The claim about VeRA requiring higher ranks to match LoRA's performance is supported by the experimental data, but the underlying reasons for this rank difference and its implications for other tasks are not fully explored

## Next Checks

1. Apply the evaluated PEFT methods to additional healthcare datasets or other time series domains (e.g., financial forecasting, sensor data) to assess the robustness and generalizability of the findings beyond the eICU dataset

2. Conduct a thorough sensitivity analysis on the initialization and hyperparameters of FourierFT and VeRA to determine the stability of these methods across different settings and to identify potential failure modes

3. Extend the comparison to include other PEFT techniques such as prefix tuning or soft-prompting to provide a more comprehensive evaluation of the landscape and to identify potential trade-offs between different approaches