---
ver: rpa2
title: Computation-Aware Kalman Filtering and Smoothing
arxiv_id: '2405.08971'
source_url: https://arxiv.org/abs/2405.08971
tags:
- state
- space
- time
- kalman
- filtering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the scalability challenges of Kalman filtering
  and smoothing in high-dimensional state spaces, particularly for spatiotemporal
  regression problems. The authors propose computation-aware Kalman filters (CAKFs)
  and smoothers (CAKSs) that use low-rank projections of observations and iterative
  matrix-free algorithms to reduce computational cost and memory requirements.
---

# Computation-Aware Kalman Filtering and Smoothing

## Quick Facts
- arXiv ID: 2405.08971
- Source URL: https://arxiv.org/abs/2405.08971
- Authors: Marvin Pförtner; Jonathan Wenger; Jon Cockayne; Philipp Hennig
- Reference count: 40
- Primary result: CAKF/CAKS achieve better predictive performance than standard methods on large-scale spatiotemporal problems while using significantly less memory and computation time

## Executive Summary
This paper addresses the scalability challenges of Kalman filtering and smoothing in high-dimensional state spaces, particularly for spatiotemporal regression problems. The authors propose computation-aware Kalman filters (CAKFs) and smoothers (CAKSs) that use low-rank projections of observations and iterative matrix-free algorithms to reduce computational cost and memory requirements. The method introduces approximation error, which is explicitly quantified and incorporated into uncertainty estimates. Theoretical analysis provides worst-case error bounds that scale with the predicted variance. Experiments on synthetic data and a large-scale climate dataset (up to 4 million observations) demonstrate that CAKF/CAKS achieve better predictive performance compared to standard methods while using significantly less memory and computation time.

## Method Summary
The computation-aware Kalman filter and smoother use low-rank projections of observations to reduce the time and memory complexity of the Kalman filter update step. By projecting observations onto a low-dimensional subspace, the algorithm replaces the inversion of full innovation matrices with smaller matrices. The method employs matrix-free implementation with covariance truncation, avoiding explicit storage of large covariance matrices while maintaining efficient matrix-vector products. The algorithm explicitly quantifies approximation error from both projection and truncation, incorporating these into uncertainty estimates to provide "computation-aware" inference that accounts for both epistemic uncertainty and computational approximation error.

## Key Results
- CAKF/CAKS achieve lower mean squared error and negative log-likelihood compared to standard methods on spatiotemporal regression problems
- The method scales to problems with up to 4 million observations using significantly less memory (reducing from O(d²) to O(dˇn))
- Different projection policies (coordinate, random, CG/Lanczos) show varying performance, with CG/Lanczos typically performing best
- The theoretical error bounds scale with predicted variance and are independent of state dimension, though they may be loose in practice

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-rank projection of observations reduces the time and memory complexity of the Kalman filter update step.
- Mechanism: By projecting observations onto a low-dimensional subspace using matrix $S_k$, the algorithm replaces the inversion of the full innovation matrix $G_k \in \mathbb{R}^{n_k \times n_k}$ with the inversion of a much smaller matrix $\hat{G}_k \in \mathbb{R}^{\hat{n}_k \times \hat{n}_k}$ where $\hat{n}_k \ll n_k$.
- Core assumption: The projection matrix $S_k$ retains the most informative parts of the observation while reducing dimensionality.
- Evidence anchors:
  - [abstract] "use low-rank projections of observations and iterative matrix-free algorithms to reduce computational cost and memory requirements"
  - [section] "To reduce both the time and memory complexity of the update step, we project both sides of the observation onto a low-dimensional subspace"
- Break condition: If the projection matrix $S_k$ discards too much information, the approximation error becomes too large and the predictive performance degrades significantly.

### Mechanism 2
- Claim: Matrix-free implementation with covariance truncation enables scalable inference by avoiding explicit storage of large covariance matrices.
- Mechanism: Instead of storing full $d \times d$ covariance matrices, the algorithm maintains low-rank downdates and uses matrix-vector products with structured covariance matrices that can be computed efficiently without materializing the full matrix.
- Core assumption: Matrix-vector products with the prior covariance, dynamics matrix, observation matrix, and noise covariance can be computed efficiently without explicit matrix storage.
- Evidence anchors:
  - [abstract] "Our matrix-free iterative algorithm leverages GPU acceleration"
  - [section] "In many cases, such matrix-vector products can be efficiently implemented or accurately approximated in a 'matrix-free' fashion"
- Break condition: If the structure of the covariance matrices doesn't allow for efficient matrix-vector products, the matrix-free implementation loses its computational advantage.

### Mechanism 3
- Claim: The algorithm explicitly quantifies approximation error and incorporates it into uncertainty estimates, making the inference "computation-aware."
- Mechanism: By treating the projection error as part of the observation model and truncation error as additional computational uncertainty, the algorithm provides uncertainty estimates that account for both epistemic uncertainty and approximation error.
- Core assumption: The approximation errors introduced by projection and truncation can be modeled as additional uncertainty sources that are independent of the true state uncertainty.
- Evidence anchors:
  - [abstract] "they do not model the error introduced by the computational approximation, their predictive uncertainty estimates can be overly optimistic"
  - [section] "they return a combined uncertainty estimate quantifying both epistemic uncertainty and the approximation error"
  - [section] "the truncation of the downdate can be interpreted as the addition of independent computational uncertainty"
- Break condition: If the approximation errors are correlated with the true state uncertainty or if the model for computational uncertainty is misspecified, the uncertainty estimates may become unreliable.

## Foundational Learning

- Concept: Linear Gaussian State Space Models (LGSSM)
  - Why needed here: The paper builds on Kalman filtering and smoothing for LGSSMs as the foundation for their computation-aware approach
  - Quick check question: What are the two main steps in the Kalman filter recursion for LGSSMs?

- Concept: Gaussian Process Regression and Spatiotemporal Modeling
  - Why needed here: The paper applies their method to spatiotemporal regression problems where state space dimension scales with spatial observations
  - Quick check question: How does spatiotemporal GP regression relate to state space models when the GP is a space-time separable Gauss-Markov process?

- Concept: Probabilistic Linear Solvers and Matrix-Free Methods
  - Why needed here: The computation-aware approach uses iterative, matrix-free algorithms similar to those used in probabilistic linear solvers
  - Quick check question: What is the key idea behind matrix-free methods for linear algebra operations?

## Architecture Onboarding

- Component map: CAKF forward pass with projection and truncation -> CAKS backward pass (for inference) or CAKF pass with caching -> sampling module using Matheron's rule (for sampling). The CAKF uses a POLICY component to select projection matrices and a TRUNCATE component to control memory usage.

- Critical path: For inference, the critical path is: CAKF forward pass with projection and truncation, followed by CAKS backward pass. For sampling, the critical path is: CAKF pass with caching of intermediate quantities, then sampling using cached values without needing the smoother.

- Design tradeoffs: The main tradeoff is between computational cost and approximation error, controlled by the number of projection directions and truncation rank. More projection directions reduce approximation error but increase computation time. Higher truncation rank improves accuracy but increases memory usage.

- Failure signatures: If the approximation error becomes too large, the predictive performance will degrade as shown by increasing MSE and NLL. If the truncation rank is too low, the uncertainty estimates may become overly conservative. If the projection policy is poor, the algorithm may waste computation on uninformative directions.

- First 3 experiments:
  1. Run CAKF with different numbers of projection directions on a small synthetic dataset to observe the tradeoff between computational cost and predictive accuracy.
  2. Compare different projection policies (coordinate, random, CG/Lanczos) on a moderate-sized spatiotemporal problem to identify which policy performs best.
  3. Test the scalability of CAKF/CAKS on increasingly large climate datasets, measuring memory usage and computation time while monitoring predictive performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of policy affect the trade-off between computational cost and approximation error in the CAKF/CAKS?
- Basis in paper: [explicit] The paper discusses various policy choices (coordinate, random, CG/Lanczos) and their impact on the algorithm's performance.
- Why unresolved: The paper provides an empirical comparison of policies but does not offer a theoretical framework for understanding the optimal policy selection for a given problem.
- What evidence would resolve it: A theoretical analysis of the impact of different policy choices on the error bounds and computational complexity of the CAKF/CAKS, possibly using techniques from Bayesian experimental design or information theory.

### Open Question 2
- Question: Can the truncation of downdate matrices in the CAKF/CAKS be optimized to further reduce memory requirements without significantly increasing approximation error?
- Basis in paper: [explicit] The paper introduces downdate truncation as a method to control memory usage, but acknowledges that it introduces additional approximation error.
- Why unresolved: The paper does not provide a rigorous analysis of the optimal truncation strategy or its impact on the overall error bound.
- What evidence would resolve it: A theoretical analysis of the trade-off between truncation rank and approximation error, possibly using techniques from randomized numerical linear algebra or low-rank matrix approximation.

### Open Question 3
- Question: How can the CAKF/CAKS be extended to handle non-linear state space models and non-Gaussian observation noise?
- Basis in paper: [inferred] The paper focuses on linear Gaussian state space models, but many real-world applications involve non-linear dynamics or non-Gaussian noise.
- Why unresolved: The paper does not discuss potential extensions to handle non-linear models or non-Gaussian noise.
- What evidence would resolve it: Development of a non-linear extension of the CAKF/CAKS, possibly using techniques from sequential Monte Carlo methods or variational inference, and empirical evaluation on a dataset with non-linear dynamics or non-Gaussian noise.

## Limitations
- The worst-case error bounds are loose and may not be achievable in practice, with actual approximation error depending heavily on observation matrix structure
- Theoretical analysis assumes Gaussian observation noise and linear dynamics, which may not hold in all real-world applications
- Performance benefits demonstrated on climate data may not generalize to all spatiotemporal regression problems with different covariance structures

## Confidence

**Confidence labels**:
- High confidence: The core computational improvements (reduced time and memory complexity through projection and matrix-free methods) are well-established and directly verifiable through runtime and memory usage measurements
- Medium confidence: The theoretical error bounds, while providing useful worst-case guarantees, may not reflect practical performance, and the actual approximation error can be much smaller than predicted
- Medium confidence: The uncertainty quantification approach that incorporates computational error is novel and theoretically sound, but its practical effectiveness depends on the accuracy of the error models

## Next Checks
1. **Bound tightness validation**: Run experiments to empirically measure the actual approximation error and compare it against the theoretical worst-case bounds across different problem structures and projection policies
2. **Non-Gaussian noise robustness**: Test the CAKF/CAKS framework on datasets with non-Gaussian observation noise (e.g., heavy-tailed distributions) to evaluate the robustness of uncertainty estimates
3. **Alternative covariance structures**: Apply the method to spatiotemporal problems with different covariance kernels (e.g., exponential, periodic) beyond the Matérn kernels used in experiments to assess generalizability