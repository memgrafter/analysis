---
ver: rpa2
title: 'Aqulia-Med LLM: Pioneering Full-Process Open-Source Medical Language Models'
arxiv_id: '2406.12182'
source_url: https://arxiv.org/abs/2406.12182
tags:
- medical
- data
- dataset
- aquila-med-chat
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Aquila-Med, a bilingual medical LLM addressing
  the challenges of specialized medical knowledge through continued pre-training,
  supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF).
  The authors construct large-scale Chinese and English medical datasets for pre-training
  and high-quality SFT and DPO datasets covering 15+ departments and 100+ disease
  specialties.
---

# Aqulia-Med LLM: Pioneering Full-Process Open-Source Medical Language Models

## Quick Facts
- arXiv ID: 2406.12182
- Source URL: https://arxiv.org/abs/2406.12182
- Reference count: 6
- Key outcome: Aquila-Med achieves notable results across single-turn and multi-turn medical dialogues and multiple-choice questions, outperforming baseline Aquila on various medical benchmarks

## Executive Summary
This paper presents Aquila-Med, a bilingual medical LLM addressing the challenges of specialized medical knowledge through continued pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF). The authors construct large-scale Chinese and English medical datasets for pre-training and high-quality SFT and DPO datasets covering 15+ departments and 100+ disease specialties. Aquila-Med achieves notable results across single-turn and multi-turn medical dialogues and multiple-choice questions, demonstrating the effectiveness of their approach. The model outperforms baseline Aquila on various medical benchmarks, with Aquila-Med-Chat (RL) showing further improvements in alignment and human-style responses. The authors open-source the datasets and training process, contributing valuable resources to the research community.

## Method Summary
Aquila-Med employs a multi-stage approach to develop a bilingual medical LLM. The process begins with continue pre-training using a large-scale medical corpus, followed by two-stage fine-tuning with high-quality medical dialogue data. The model then undergoes supervised fine-tuning (SFT) on instruction-response pairs, and finally reinforcement learning from human feedback (RLHF) using Direct Preference Optimization (DPO). The authors construct and filter datasets at each stage, using context relevance scoring for multi-turn dialogues and combined subjective-objective DPO preference data to mitigate alignment tax while preserving medical knowledge.

## Key Results
- Aquila-Med outperforms baseline Aquila on various medical benchmarks including MMLU, C-Eval, CMB-Exam, MedQA, MedMCQA, and PubMedQA
- Aquila-Med-Chat (RL) shows improvements in alignment and human-style responses compared to non-RL versions
- The model demonstrates strong performance across single-turn and multi-turn medical dialogues covering 15+ departments and 100+ disease specialties

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-stage medical domain pre-training (general medical corpus → high-quality filtered corpus) improves medical task performance beyond SFT alone
- Mechanism: The two-stage continue pre-training approach first builds medical domain foundation using broader medical data (Stage 1), then refines with high-quality curated medical data (Stage 2), preventing catastrophic forgetting while specializing knowledge
- Core assumption: Medical knowledge requires both breadth (Stage 1) and depth (Stage 2) for effective learning
- Evidence anchors:
  - [abstract] "addressing these challenges through continue pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF)"
  - [section 2.1.2] "Stage 1: The aim is to prevent the model capability from being significantly degraded...Stage 2: The aim is to further improve the capability of the medical domain model"
  - [corpus] Weak evidence - related works use similar two-stage approaches but lack specific validation
- Break condition: If quality filtering removes too much medical-specific context, or if Stage 1 data is too general to establish domain relevance

### Mechanism 2
- Claim: Multi-turn dialogue data filtering using Context Relevance (CR) score captures realistic doctor-patient interaction patterns
- Mechanism: CR score measures how well historical context supports current turn generation, filtering out dialogues with low context correlation or excessive redundancy
- Core assumption: Real medical consultations have optimal context relevance that balances information flow and avoids redundancy
- Evidence anchors:
  - [section 2.2.1] "We propose a Context Relevance (CR) score...to evaluate the impact of historical information on each turn"
  - [abstract] "covering extensive medical specialties" and "high-quality SFT dataset"
  - [corpus] Weak evidence - no direct comparison of CR-filtered vs unfiltered multi-turn dialogue performance
- Break condition: If CR threshold is too strict, filtering out valuable complex cases; if too lenient, retaining redundant information

### Mechanism 3
- Claim: Combined subjective and objective DPO preference data prevents alignment tax while improving human-style responses
- Mechanism: Subjective DPO aligns with human preferences, while objective DPO maintains medical knowledge through ground truth comparisons, preventing performance degradation on factual tasks
- Core assumption: Alignment methods can cause catastrophic forgetting of pre-trained knowledge without targeted preservation
- Evidence anchors:
  - [section 2.3.1] "we construct subjective preference data and objective preference data...to mitigate this issue"
  - [abstract] "Aquila-Med-Chat (RL) showing further improvements in alignment and human-style responses"
  - [corpus] Moderate evidence - alignment tax is well-documented in literature, but specific objective DPO approach appears novel
- Break condition: If objective DPO samples are too limited or biased, failing to preserve diverse medical knowledge

## Foundational Learning

- Concept: Supervised fine-tuning (SFT) on instruction-response pairs
  - Why needed here: Builds conversational ability and domain-specific instruction following
  - Quick check question: What distinguishes SFT from pre-training in terms of learning objectives?

- Concept: Reinforcement learning from human feedback (RLHF) with Direct Preference Optimization (DPO)
  - Why needed here: Aligns model outputs with human preferences while preserving factual knowledge
  - Quick check question: How does DPO differ from traditional RLHF approaches?

- Concept: Context relevance scoring for multi-turn dialogue filtering
  - Why needed here: Ensures medical dialogues capture realistic information flow and doctor-patient interaction patterns
  - Quick check question: What metrics would you use to evaluate context relevance in multi-turn conversations?

## Architecture Onboarding

- Component map: Data pipeline → Continue pre-training (2 stages) → SFT → DPO → Evaluation
- Critical path: Data construction → Model training → Benchmark evaluation
- Design tradeoffs:
  - Quality vs quantity in data filtering
  - General vs specialized knowledge in pre-training stages
  - Alignment depth vs knowledge preservation
- Failure signatures:
  - Performance degradation on factual benchmarks
  - Overly generic or repetitive dialogue responses
  - Context confusion in multi-turn conversations
- First 3 experiments:
  1. Compare single-stage vs two-stage pre-training on medical benchmarks
  2. Test different CR score thresholds on multi-turn dialogue quality
  3. Evaluate DPO with only subjective vs combined subjective-objective preferences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Aquila-Med model perform on long-form medical dialogues compared to short-form dialogues?
- Basis in paper: [inferred] The paper evaluates single-turn and multi-turn dialogues but does not explicitly compare performance between long-form and short-form medical dialogues.
- Why unresolved: The paper focuses on evaluating the model's performance across different types of dialogues (single-turn and multi-turn) but does not provide a specific comparison of long-form versus short-form dialogues.
- What evidence would resolve it: A direct comparison of the model's performance on long-form medical dialogues versus short-form dialogues, possibly using metrics such as coherence, relevance, and completeness.

### Open Question 2
- Question: What is the impact of the alignment tax on the Aquila-Med model's performance in medical tasks?
- Basis in paper: [explicit] The paper mentions the alignment tax and its potential to cause the model to forget abilities acquired during pre-training and SFT stages, but does not quantify its impact on medical tasks.
- Why unresolved: While the paper acknowledges the existence of the alignment tax, it does not provide specific data on how this affects the model's performance in medical tasks.
- What evidence would resolve it: Empirical data showing the difference in performance on medical tasks before and after alignment, with and without measures to mitigate the alignment tax.

### Open Question 3
- Question: How does the Aquila-Med model handle rare or uncommon medical conditions?
- Basis in paper: [inferred] The paper discusses the model's performance on various medical benchmarks and its ability to handle different types of medical dialogues, but does not specifically address its performance on rare or uncommon medical conditions.
- Why unresolved: The paper does not provide information on the model's ability to accurately diagnose or provide information on rare or uncommon medical conditions.
- What evidence would resolve it: A detailed analysis of the model's performance on a dataset containing rare or uncommon medical conditions, with metrics such as accuracy, precision, and recall.

## Limitations

- Lack of direct comparisons with state-of-the-art medical LLMs like Med-PaLM 2 or Meditron on the same benchmarks
- Multi-stage pre-training approach validated only through internal benchmarking without ablation studies
- Context Relevance (CR) score for dialogue filtering described but not empirically validated against human judgments

## Confidence

**High Confidence**: The core methodology of multi-stage pre-training followed by SFT and DPO is well-established in the literature and the implementation details are sufficiently specified for reproduction. The reported benchmark results on standard medical datasets (MMLU, MedQA, etc.) are presented with appropriate metrics and are internally consistent.

**Medium Confidence**: The claim that Aquila-Med outperforms baseline Aquila on medical benchmarks is supported by provided evidence, but the magnitude of improvement and its practical significance would benefit from more extensive testing. The effectiveness of the two-stage pre-training approach is plausible based on the mechanism described but lacks direct comparative validation.

**Low Confidence**: The assertion that the combined subjective-objective DPO approach effectively prevents alignment tax while improving human-style responses is the weakest claim, as it relies on indirect evidence and lacks head-to-head comparisons with standard DPO or RLHF approaches. The CR score's effectiveness in filtering multi-turn dialogues is described but not empirically validated.

## Next Checks

1. Conduct ablation studies comparing single-stage vs two-stage pre-training on medical benchmarks to quantify the specific contribution of each pre-training stage to final performance.

2. Perform human evaluation studies to validate the Context Relevance (CR) score's effectiveness in filtering multi-turn dialogues, comparing CR-filtered dialogues against both unfiltered dialogues and human-curated dialogues.

3. Test the objective DPO approach against standard DPO and traditional RLHF on medical knowledge preservation tasks to empirically verify claims about preventing alignment tax while maintaining conversational quality.