---
ver: rpa2
title: Optimizing Latent Goal by Learning from Trajectory Preference
arxiv_id: '2412.02125'
source_url: https://arxiv.org/abs/2412.02125
tags:
- learning
- task
- preference
- tasks
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Preference Goal Tuning (PGT) fine-tunes the latent goal representation
  in goal-conditioned policies using preference-learned trajectories while keeping
  the policy backbone frozen. This approach improves performance by 72.0% and 81.6%
  on average over 17 tasks in two foundation policies, outperforming both human-selected
  prompts and full fine-tuning.
---

# Optimizing Latent Goal by Learning from Trajectory Preference

## Quick Facts
- arXiv ID: 2412.02125
- Source URL: https://arxiv.org/abs/2412.02125
- Authors: Guangyu Zhao; Kewei Lian; Haowei Lin; Haobo Fu; Qiang Fu; Shaofei Cai; Zihao Wang; Yitao Liang
- Reference count: 30
- Key outcome: PGT improves performance by 72.0% and 81.6% on average over 17 tasks in two foundation policies, outperforming both human-selected prompts and full fine-tuning

## Executive Summary
Preference Goal Tuning (PGT) is a parameter-efficient method for fine-tuning goal-conditioned policies by optimizing only the latent goal representation while keeping the policy backbone frozen. The approach collects trajectories from environment interaction, categorizes them into positive and negative samples based on preference, and uses preference learning to fine-tune the initial goal latent representation. PGT achieves significant performance improvements (72.0% and 81.6% average gains) across 17 Minecraft tasks while demonstrating strong out-of-distribution generalization and enabling efficient continual learning without catastrophic forgetting.

## Method Summary
PGT fine-tunes the latent goal representation in goal-conditioned policies using preference-learned trajectories while keeping the policy backbone frozen. The method collects ~300 trajectories under an initial prompt, labels them as positive/negative based on preference or reward, and uses preference learning algorithms like DPO to update the goal latent representation. This parameter-efficient approach (512 parameters vs 86M for full fine-tuning) activates pre-trained abilities through minimal data while maintaining generalization across different environments and preventing catastrophic forgetting.

## Key Results
- Average relative improvement of 72.0% (GROOT) and 81.6% (STEVE-1) in in-distribution settings across 17 tasks
- Strong out-of-distribution generalization with 73.8% and 36.9% improvement over baseline methods
- Avoids catastrophic forgetting by storing single latent representations for each task

## Why This Works (Mechanism)

### Mechanism 1
PGT improves performance by fine-tuning only the latent goal representation while keeping the policy backbone frozen. The goal latent space contains rich semantic meanings learned during pre-training, but human instructions may not map optimally to this space. PGT finds the optimal latent representation through preference learning on collected trajectories, activating pre-trained abilities without altering the underlying policy structure.

### Mechanism 2
PGT achieves better generalization by avoiding full fine-tuning, which tends to overfit to environment-specific details. With limited data, full fine-tuning memorizes environment-specific features rather than learning generalizable task representations. PGT's parameter-efficient approach (512 parameters vs 86M for full fine-tuning) prevents overfitting and maintains generalization across different environments.

### Mechanism 3
Incorporating negative samples through preference learning is crucial for distinguishing desired from undesired behaviors. Pure behavior cloning on positive samples teaches the policy what to do but not what to avoid. Preference learning with negative samples explicitly penalizes undesired behaviors, leading to better policy optimization.

## Foundational Learning

- **Goal-conditioned policies and latent goal representations**: Understanding how instructions are encoded into latent representations and how policies decode these representations into actions is fundamental to grasping PGT's approach.
- **Preference learning and contrastive training**: PGT relies on preference learning algorithms to fine-tune the latent goal representation using positive and negative trajectory pairs.
- **Catastrophic forgetting and continual learning**: PGT's approach of storing single latent representations for each task avoids catastrophic forgetting, making it suitable for continual learning.

## Architecture Onboarding

- **Component map**: Encoder → Initial latent goal → Policy backbone (frozen) → Action sequence
- **Critical path**: Instruction → Encoder → Initial latent goal → Environment interaction → Trajectory collection → Preference labeling → Preference learning → Updated latent goal → Improved performance
- **Design tradeoffs**: Parameter efficiency vs. expressivity (512 parameters vs 86M for full fine-tuning); Generalization vs. specificity (avoiding full fine-tuning maintains OOD generalization); Data efficiency vs. sample quality (PGT requires collecting trajectories but uses minimal data overall)
- **Failure signatures**: No improvement after fine-tuning (policy backbone may lack necessary skills); Worsening performance (incorrect preference labeling or hyperparameter issues); Slow convergence (insufficient trajectory diversity or poor initial latent representation)
- **First 3 experiments**: 1) Implement PGT on a simple task with synthetic preferences to verify basic functionality; 2) Compare PGT with behavior cloning baseline on the same task to validate importance of negative samples; 3) Test PGT on an out-of-distribution environment to confirm generalization benefits over full fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
How does PGT performance scale with larger foundation policies (e.g., 100B+ parameters) compared to current models? The paper tests PGT on relatively small foundation policies and mentions potential for robotics applications but doesn't explore scaling to larger models.

### Open Question 2
What is the minimum amount of preference data needed for PGT to achieve significant improvements? The paper uses ~300 trajectories for data collection but doesn't systematically explore the relationship between data quantity and performance.

### Open Question 3
How does PGT compare to reinforcement learning approaches for fine-tuning foundation policies in terms of sample efficiency and final performance? The paper focuses on preference learning without comparing to RL-based fine-tuning methods, despite mentioning RL in related work.

## Limitations
- Claims based on only two foundation policies (GROOT and STEVE-1) in Minecraft environments
- Does not address potential biases in human preference labeling
- Scalability of trajectory collection for more complex tasks not investigated

## Confidence
- **High Confidence**: The mechanism of fine-tuning only latent goal representations while freezing the policy backbone is technically sound
- **Medium Confidence**: The claimed generalization benefits and avoidance of catastrophic forgetting are plausible but need more extensive validation
- **Low Confidence**: The assertion that PGT "activates pre-trained abilities through minimal data" is somewhat speculative without direct analysis of latent space modifications

## Next Checks
1. Apply PGT to a non-Minecraft domain (e.g., robotic manipulation) using a different foundation policy to verify generalizability
2. Visualize and analyze changes in latent goal representations before and after PGT fine-tuning to understand semantic modifications
3. Systematically vary the number of collected trajectories (50, 100, 300, 500) to determine minimum data requirements for PGT