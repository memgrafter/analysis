---
ver: rpa2
title: 'Order of Compression: A Systematic and Optimal Sequence to Combinationally
  Compress CNN'
arxiv_id: '2403.17447'
source_url: https://arxiv.org/abs/2403.17447
tags:
- compression
- uni00000013
- uni0000001c
- uni0000004c
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the optimal sequence for combining multiple
  model compression techniques, including knowledge distillation, pruning, quantization,
  and early exit. It proposes the Order of Compression, a systematic approach to determine
  the best order of applying these methods.
---

# Order of Compression: A Systematic and Optimal Sequence to Combinationally Compress CNN

## Quick Facts
- arXiv ID: 2403.17447
- Source URL: https://arxiv.org/abs/2403.17447
- Authors: Yingtao Shen; Minqing Sun; Jianzhe Lin; Jie Zhao; An Zou
- Reference count: 9
- One-line primary result: Systematic exploration of optimal sequence for combining multiple model compression techniques, achieving up to 859× computational cost reduction on ResNet34 with negligible accuracy loss.

## Executive Summary
This paper investigates the optimal sequence for combining multiple model compression techniques—knowledge distillation, pruning, quantization, and early exit—to achieve maximum computational efficiency with minimal accuracy loss in CNNs. The authors propose a systematic approach using topological sorting to derive the optimal order, following the principle of transitioning from static to dynamic methods and from large to small granularity. Extensive experiments across popular CNN architectures and diverse datasets demonstrate significant compression ratios while maintaining high accuracy, outperforming state-of-the-art multi-compression techniques.

## Method Summary
The study systematically explores the optimal sequence for combining four compression techniques: knowledge distillation, pruning, quantization, and early exit. The authors first conduct pairwise evaluations of all possible combinations, then use topological sorting to derive the global optimal sequence from static to dynamic methods (distillation, pruning, quantization) followed by dynamic early exit. The methodology involves fine-tuning with 200 epochs after each compression step, using a learning rate of 1/10 of the initial value. Experiments are conducted on VGG19, ResNet34, and MobileNetV2 architectures across CIFAR-10, CIFAR-100, SVHN, CINIC-10, and ImageNet-1k datasets, measuring BitOpsCR and accuracy.

## Key Results
- Achieved up to 859× computational cost reduction on ResNet34 with only -0.09% accuracy loss on CIFAR10
- Outperformed state-of-the-art multi-compression techniques while maintaining high accuracy
- Validated that adding additional compression methods does not disrupt the established sequence order
- Demonstrated optimal sequence follows static-to-dynamic and large-to-small granularity principles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The optimal sequence is static → dynamic and large → small granularity.
- Mechanism: Each compression method reduces model complexity at different levels. Static methods (distillation, pruning, quantization) act before inference and target architecture, neurons, and sub-neuron values respectively. Dynamic methods (early exit) act during inference. Applying them in this order preserves more useful information for each subsequent step.
- Core assumption: Earlier compressions do not destroy the representational structure needed for later steps; later steps can exploit reduced dimensions without loss of effectiveness.
- Evidence anchors:
  - [abstract] "the optimal sequence is derived with topological sorting, which is from static to dynamic and granularity large to small."
  - [section 5] "The topological sorting of the compression approach reveals a consistent adherence to the principles of transitioning from static to dynamic and progressing from a large granularity perspective to a small granularity one."
  - [corpus] No direct evidence in corpus; this is the primary contribution of the paper.
- Break condition: If a later method's performance depends on fine-grained structure removed by an earlier method, the sequence fails.

### Mechanism 2
- Claim: Inserting additional compressions between two steps does not change their relative order.
- Mechanism: The pairwise order is determined by their complementary effects. Adding a method between them only changes the intermediate state, not the directional dependency between the two.
- Core assumption: Pairwise interactions are monotonic; adding more steps does not invert their relative efficiency.
- Evidence anchors:
  - [section 4] "regardless of whether we introduce quantization, pruning, or early exit as an additional compression step, the established sequence remains compatible with that presented in the previous section."
  - [abstract] "we demonstrate inserting additional compression between any two compressions will not break the order of the two compression approaches."
  - [corpus] No corpus evidence; result is specific to this paper.
- Break condition: If an inserted method changes the resource or accuracy profile enough to invert pairwise efficiency.

### Mechanism 3
- Claim: Knowledge distillation followed by other methods consistently outperforms the reverse order.
- Mechanism: Knowledge distillation first creates a compact teacher-student pair; pruning or quantization then applied to the student is more efficient because the student already has reduced complexity and distilled knowledge.
- Core assumption: Distillation's student model is better suited for further compression than a pruned teacher or quantized teacher.
- Evidence anchors:
  - [section 3.1] "the sequence of knowledge distillation followed by pruning consistently yields a better BitOps compression ratio while maintaining a comparable inference accuracy."
  - [section 3.1] "the sequence of knowledge distillation followed by quantization consistently yields a significantly better BitOps compression ratio while maintaining comparable inference accuracy."
  - [corpus] No corpus evidence; derived from paper's ablation studies.
- Break condition: If the student model's architecture is too fragile for subsequent aggressive compression.

## Foundational Learning

- Concept: Topological sorting of directed acyclic graphs.
  - Why needed here: To determine the global optimal order from pairwise orderings.
  - Quick check question: Can you list the steps to perform a topological sort on a DAG?

- Concept: BitOps compression ratio as a hardware-aware metric.
  - Why needed here: To compare compression effectiveness across static and dynamic methods on a common basis.
  - Quick check question: How does BitOpsCR differ from model size reduction metrics?

- Concept: Knowledge distillation mechanics (teacher-student training).
  - Why needed here: Distillation is the first step in the optimal sequence and its effectiveness sets the stage for later methods.
  - Quick check question: What loss function is typically used in distillation?

## Architecture Onboarding

- Component map: Data loader → Model (baseline) → Distillation trainer → Pruning module → Quantization module → Early exit trainer → Evaluation
- Critical path: Baseline → Distillation → Pruning → Quantization → Early Exit
- Design tradeoffs:
  - More aggressive early steps may reduce later method gains.
  - Fine-tuning after each step adds compute but preserves accuracy.
  - Uniform channel pruning chosen for hardware compatibility over accuracy.
- Failure signatures:
  - Accuracy collapse after a step indicates over-aggressive hyperparameters.
  - No BitOps reduction after a step indicates the method was ineffective for the model.
  - Early exit thresholds not triggering suggests poor placement or confidence calibration.
- First 3 experiments:
  1. Apply only distillation to ResNet34 on CIFAR10; record accuracy and BitOpsCR.
  2. Apply distillation then pruning; compare to step 1.
  3. Apply distillation then quantization; compare to step 1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the optimal sequence law for compression techniques be extended to other types of neural networks beyond CNNs, such as transformers or recurrent neural networks?
- Basis in paper: [explicit] The study focuses on CNNs and acknowledges the need for broader applicability but does not explore other network architectures.
- Why unresolved: The research is limited to CNN architectures, and the interaction between compression techniques may differ for other network types due to their unique structures and operations.
- What evidence would resolve it: Empirical studies applying the proposed order of compression to transformers or RNNs, measuring performance and comparing with existing methods.

### Open Question 2
- Question: How does the optimal sequence change when considering different hardware platforms, such as GPUs, TPUs, or edge devices, which may have varying support for quantization and pruning operations?
- Basis in paper: [inferred] The study mentions hardware compatibility as a factor in compression performance but does not investigate the impact of different hardware platforms.
- Why unresolved: Hardware-specific optimizations and constraints can significantly influence the effectiveness of compression techniques, and the optimal sequence may vary depending on the target hardware.
- What evidence would resolve it: Experiments comparing the proposed order of compression on different hardware platforms, evaluating compression ratios, accuracy, and inference speed.

### Open Question 3
- Question: Is there a theoretical framework that can predict the optimal sequence of compression techniques for a given neural network architecture and dataset, without relying on extensive empirical testing?
- Basis in paper: [explicit] The study relies on empirical testing and topological sorting to determine the optimal sequence, but does not provide a theoretical basis for predicting the sequence.
- Why unresolved: Developing a theoretical framework would require a deep understanding of the interactions between compression techniques and their impact on network performance, which is complex and not fully explored in the study.
- What evidence would resolve it: A mathematical model or algorithm that can accurately predict the optimal sequence based on network architecture, dataset characteristics, and hardware constraints, validated through extensive testing.

## Limitations
- The study relies heavily on BitOpsCR as the primary metric, which may not fully capture practical deployment considerations such as memory bandwidth constraints or real-world inference latency on diverse hardware platforms.
- The optimal sequence was validated primarily on popular CNN architectures (VGG19, ResNet34, MobileNetV2) and may not generalize to newer architectures like Vision Transformers or specialized models for specific tasks.
- The superiority of knowledge distillation as the first step is supported by ablation studies but could vary with different teacher-student configurations or datasets.

## Confidence

- High confidence: The topological sorting approach to determine optimal sequence from pairwise interactions is well-established and the experimental methodology is rigorous.
- Medium confidence: The claim that adding intermediate compression steps does not disrupt pairwise orderings is demonstrated but may depend on specific hyperparameter choices and model characteristics.
- Medium confidence: The superiority of knowledge distillation as the first step is supported by ablation studies but could vary with different teacher-student configurations or datasets.

## Next Checks

1. Validate the optimal sequence on a Vision Transformer architecture to test generalizability beyond CNNs.
2. Measure actual inference latency and memory usage on target hardware platforms, not just theoretical BitOpsCR calculations.
3. Test the sequence sensitivity by varying compression hyperparameters (pruning ratios, quantization bits) to identify breaking points where the established order fails.