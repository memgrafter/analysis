---
ver: rpa2
title: 'Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language
  Model from a Causal Mediation Perspective'
arxiv_id: '2407.02814'
source_url: https://arxiv.org/abs/2407.02814
tags:
- bias
- image
- text
- encoder
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses gender bias in vision-language models (VLMs),
  which can learn societal biases by correlating gender with specific objects or scenarios.
  The authors propose a framework using causal mediation analysis to measure and map
  bias generation and propagation within VLMs.
---

# Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective

## Quick Facts
- **arXiv ID**: 2407.02814
- **Source URL**: https://arxiv.org/abs/2407.02814
- **Reference count**: 10
- **Primary result**: Image features contribute more to gender bias in vision-language models than text features, with image encoders accounting for 32.57% and 12.63% of bias in MSCOCO and PASCAL-SENTENCE datasets respectively

## Executive Summary
This paper addresses gender bias in vision-language models (VLMs) by introducing a causal mediation analysis framework to measure and map how bias is generated and propagated within these models. The authors find that image features contribute significantly more to bias than text features, with image encoders being the primary source of bias propagation. Based on these findings, they propose a mitigation strategy that involves blurring gender representations within the image encoder, which successfully reduces bias by 22.03% and 9.04% in the respective datasets while maintaining minimal performance loss.

## Method Summary
The authors develop a causal mediation analysis framework to dissect how gender bias propagates through VLMs by identifying mediators such as image encoders, text encoders, and fusion encoders. They conduct quantitative measurements of bias contributions from each component and analyze the alignment between modality contributions. The mitigation strategy involves applying image transformations to blur gender-specific features in the image encoder, which is then evaluated for its effectiveness in reducing bias while preserving model performance.

## Key Results
- Image encoders contribute 32.57% and 12.63% of bias in MSCOCO and PASCAL-SENTENCE datasets respectively
- Image encoder contributions surpass those of text and fusion encoders in propagating gender bias
- Contributions from both image and text modalities are aligned and non-conflicting
- Blurring gender representations in image encoders reduces bias by 22.03% and 9.04% with minimal performance loss

## Why This Works (Mechanism)
The paper demonstrates that visual information in VLMs carries a disproportionate amount of gender bias compared to textual information. The causal mediation analysis reveals that image encoders serve as the primary pathway for bias propagation, suggesting that visual features contain stronger gender stereotypes than textual features. By targeting the image encoder through blurring techniques, the model can reduce bias propagation while maintaining overall functionality since the mitigation is focused on the primary source of bias.

## Foundational Learning
- **Causal Mediation Analysis**: Needed to understand how bias propagates through different model components; quick check: verify that identified mediators capture all relevant bias pathways
- **Vision-Language Model Architecture**: Required to understand how image and text features interact; quick check: confirm understanding of encoder-fusion relationships
- **Bias Measurement in VLMs**: Essential for quantifying bias contributions; quick check: validate measurement metrics against established benchmarks
- **Image Feature Manipulation**: Important for implementing mitigation strategies; quick check: test that blurring preserves relevant visual information while removing gender cues
- **Model Performance Evaluation**: Critical for ensuring mitigation doesn't harm functionality; quick check: verify that performance metrics remain stable post-mitigation

## Architecture Onboarding
- **Component Map**: Input Images -> Image Encoder -> Fusion Module <- Text Encoder <- Input Text -> Output
- **Critical Path**: Image Encoder -> Fusion Module -> Output (primary bias pathway)
- **Design Tradeoffs**: The study prioritizes bias reduction over complete preservation of gender-specific visual information, accepting some information loss for fairness gains
- **Failure Signatures**: If mitigation causes significant performance degradation or if bias shifts to other modalities, the approach may need refinement
- **First Experiments**: 1) Measure baseline bias contributions from each encoder, 2) Apply image blurring and measure bias reduction, 3) Evaluate performance impact across downstream tasks

## Open Questions the Paper Calls Out
None

## Limitations
- The causal mediation analysis relies on assumptions about mediator completeness that may not capture complex interactions
- Focus on gender bias using occupation-attribute associations may not generalize to other bias types
- Long-term effectiveness and potential unintended consequences of blurring approach remain unclear

## Confidence
- **High Confidence**: Image features contribute more to gender bias than text features in tested VLMs
- **Medium Confidence**: Contributions from both modalities are aligned and non-conflicting
- **Medium Confidence**: Blurring gender representations effectively reduces bias with minimal performance loss

## Next Checks
1. Test the causal mediation framework on different types of biases (racial, cultural, intersectional) to determine if image features consistently contribute more than text features across various bias dimensions

2. Evaluate the sustainability of the blurring mitigation approach over extended model training periods and across diverse downstream tasks to identify any performance degradation or emergent biases

3. Implement alternative causal inference methods (such as front-door adjustment or instrumental variable approaches) to validate the robustness of the observed mediation effects and ensure the findings are not artifacts of the specific methodology employed