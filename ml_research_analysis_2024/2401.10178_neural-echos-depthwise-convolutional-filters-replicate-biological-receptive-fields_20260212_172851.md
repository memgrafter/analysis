---
ver: rpa2
title: 'Neural Echos: Depthwise Convolutional Filters Replicate Biological Receptive
  Fields'
arxiv_id: '2401.10178'
source_url: https://arxiv.org/abs/2401.10178
tags:
- uni00000048
- uni00000003
- kernels
- uni00000057
- initialization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates depthwise convolutional kernels and finds
  that many trained kernels exhibit center-surround patterns resembling biological
  receptive fields. To exploit this observation, the authors propose a biologically
  inspired initialization method based on the difference-of-Gaussians (DoG) model
  of retinal ganglion cells.
---

# Neural Echos: Depthwise Convolutional Filters Replicate Biological Receptive Fields

## Quick Facts
- arXiv ID: 2401.10178
- Source URL: https://arxiv.org/abs/2401.10178
- Authors: Zahra Babaiee; Peyman M. Kiasari; Daniela Rus; Radu Grosu
- Reference count: 40
- Primary result: DoG-based initialization of depthwise convolutional kernels improves ImageNet accuracy across multiple architectures compared to Kaiming initialization

## Executive Summary
This paper investigates depthwise convolutional kernels and finds that many trained kernels exhibit center-surround patterns resembling biological receptive fields. To exploit this observation, the authors propose a biologically inspired initialization method based on the difference-of-Gaussians (DoG) model of retinal ganglion cells. Experiments on ImageNet with ConvNeXt, HorNet, and ConvMixer models show that this initialization consistently improves accuracy compared to Kaiming initialization, with improvements ranging from marginal to over 2%. The results suggest that incorporating biological principles can enhance the performance of deep neural networks.

## Method Summary
The method involves replacing Kaiming initialization with a Difference-of-Gaussians (DoG) model for depthwise convolutional kernels. The DoG function generates center-surround patterns where γ parameter controls the ratio between center and surround regions. The initialization is applied to depthwise convolutions in ConvNeXt, HorNet, and ConvMixer architectures, with equal probability for excitatory (On-center) and inhibitory (Off-center) patterns. Models are trained on ImageNet using Adam optimizer with standard data augmentations for 50 epochs.

## Key Results
- Center-surround patterns emerge in depthwise convolutional kernels trained on ImageNet but not in regular convolutions
- DoG-based initialization improves accuracy over Kaiming initialization by 0.5-2.0% across tested architectures
- Kernel patterns cluster into three groups: On-center, Off-center, and others using k-means clustering
- Pattern emergence is less pronounced in CIFAR-10 compared to ImageNet, suggesting dataset size matters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Depthwise convolutional kernels develop center-surround receptive field patterns that mirror biological retinal ganglion cells.
- Mechanism: During training on natural images, the network's optimization process favors spatial weight distributions that enhance edge and contrast detection, leading to patterns resembling biological center-surround antagonism.
- Core assumption: Natural image statistics contain sufficient structure for the optimization to converge toward biologically plausible patterns without explicit regularization.
- Evidence anchors:
  - [abstract] "we present evidence suggesting that depthwise convolutional kernels are effectively replicating the structural intricacies of the biological receptive fields observed in the mammalian retina."
  - [section 3.1] "we observed similar patterns in other variants of these models trained on ImageNet, too. However, kernels of regular convolutions do not show such observable patterns."
- Break condition: If trained on non-natural images (e.g., synthetic noise) where center-surround patterns don't improve performance, the mechanism would fail.

### Mechanism 2
- Claim: Initializing depthwise kernels with biologically-inspired Difference of Gaussians (DoG) weights accelerates convergence and improves final accuracy.
- Mechanism: Providing kernels with pre-structured weights that match the learned patterns reduces the optimization burden, allowing the network to focus on higher-level feature learning rather than discovering basic spatial structures.
- Core assumption: The center-surround structure is universally beneficial across different depthwise convolutional architectures and tasks.
- Evidence anchors:
  - [abstract] "Experimental analysis of the ImageNet dataset with multiple CNN architectures featuring depthwise convolutions reveals a marked enhancement in the accuracy of the learned model when initialized with biologically derived weights."
  - [section 4.2] "Across all configurations, our initialization consistently outperforms the Kaiming initialization, with improvements ranging from marginal to substantial."
- Break condition: If the center-surround structure proves detrimental for certain tasks (e.g., texture analysis where uniform sensitivity is preferred), the initialization would harm performance.

### Mechanism 3
- Claim: The prevalence of center-surround patterns is specific to depthwise convolutions, not regular convolutions.
- Mechanism: Depthwise convolutions operate independently on each channel, allowing each filter to specialize in specific spatial patterns without interference from cross-channel mixing, leading to more biologically-plausible specialization.
- Core assumption: Regular convolutions' cross-channel mixing prevents the emergence of distinct center-surround patterns by forcing each kernel to capture multi-channel features.
- Evidence anchors:
  - [section 3.1] "Interestingly, such patterns were exclusively observed in depthwise convolutions, eluding their regular convolution counterparts."
  - [section 3.1] "Upon inspection of these kernels, we can identify recurring patterns among the depthwise convolutions."
- Break condition: If depthwise convolution architectures change fundamentally (e.g., adding cross-channel connections), the pattern emergence might be disrupted.

## Foundational Learning

- Concept: Difference of Gaussians (DoG) function
  - Why needed here: Provides the mathematical foundation for modeling biological center-surround receptive fields and generating initialization weights.
  - Quick check question: How does varying the γ parameter in the DoG function affect the size ratio between the center and surround regions?

- Concept: K-means clustering
  - Why needed here: Used to analyze and categorize the patterns in trained depthwise convolutional kernels to validate the presence of center-surround structures.
  - Quick check question: What would happen to the clustering results if we increased the number of clusters beyond three?

- Concept: Depthwise separable convolutions
  - Why needed here: The specific type of convolution where this biological pattern replication occurs, distinguishing it from regular convolutions.
  - Quick check question: How do depthwise convolutions differ computationally from regular convolutions in terms of parameter count and operations?

## Architecture Onboarding

- Component map:
  - Depthwise convolutional layers with biologically-inspired initialization
  - Kaiming initialization (baseline comparison)
  - Standard CNN architectures (ConvNeXt, HorNet, ConvMixer)
  - Data augmentation pipeline (RandAugment, mixup, CutMix, random erasing)

- Critical path:
  1. Identify depthwise convolutional layers in the model
  2. Replace Kaiming initialization with DoG-based initialization
  3. Train model on target dataset
  4. Evaluate accuracy improvement compared to baseline

- Design tradeoffs:
  - DoG initialization vs Kaiming: Slight increase in setup complexity for potentially significant accuracy gains
  - Equal probability of On/Off centers vs task-specific balance: General approach vs potentially optimized for specific tasks
  - γ sampled from uniform distribution vs fixed values: Broader exploration of spatial scales vs potentially more focused initialization

- Failure signatures:
  - No improvement or degradation in accuracy compared to Kaiming initialization
  - Emergence of checkerboard artifacts in feature maps
  - Unstable training dynamics (exploding/vanishing gradients)

- First 3 experiments:
  1. Apply DoG initialization to ConvNeXt tiny with kernel size 7x7 and compare accuracy to Kaiming baseline
  2. Test different γ ranges (0-0.5 vs 0-1) to determine optimal center-surround ratio distribution
  3. Compare results on Cifar10 to confirm pattern emergence requires larger datasets (ImageNet vs Cifar10)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the center-surround pattern in depthwise kernels contribute to the model's ability to generalize across different visual domains?
- Basis in paper: [explicit] The paper mentions that models with our initialization showed improved accuracy on ImageNet, but it does not test generalization to other datasets.
- Why unresolved: The paper only tested on ImageNet and CIFAR-10, which have different characteristics. It's unclear if the biological initialization helps models adapt to new visual tasks.
- What evidence would resolve it: Testing the initialized models on diverse datasets (e.g., medical imaging, satellite imagery) and comparing their performance to Kaiming-initialized models would clarify if the biological patterns improve generalization.

### Open Question 2
- Question: What is the impact of varying the ratio of excitatory to inhibitory centers in the kernels?
- Basis in paper: [inferred] The paper uses a 50-50 split for excitatory and inhibitory centers but doesn't explore other ratios or their effects.
- Why unresolved: The paper assumes equal proportions are optimal but doesn't provide evidence or explore if imbalanced ratios could yield better results.
- What evidence would resolve it: Conducting experiments with different ratios (e.g., 70-30, 30-70) and measuring their impact on accuracy and convergence speed would reveal the optimal balance.

### Open Question 3
- Question: Do the non-center-surround patterns in the "Others" cluster have biological relevance or are they artifacts of the training process?
- Basis in paper: [explicit] The paper notes that the "Others" cluster contains patterns that don't resemble center-surround structures but doesn't investigate their origins or significance.
- Why unresolved: The paper focuses on center-surround patterns and doesn't analyze the alternative patterns in the third cluster.
- What evidence would resolve it: Analyzing the "Others" cluster using techniques like t-SNE or UMAP to visualize their relationships and comparing them to known biological receptive field models would clarify their relevance.

## Limitations

- Limited Generalization Evidence: The biological pattern replication claim is primarily based on visual inspection of kernel clusters rather than quantitative validation against actual biological receptive field measurements.
- Task Dependency Unclear: While improvements are demonstrated on ImageNet classification, the paper does not investigate whether the biological initialization benefits extend to other vision tasks or non-natural image domains.
- Architecture-Specific Results: The reported improvements are demonstrated only on three specific architectures, lacking validation on architectures with different design principles.

## Confidence

- High Confidence: Depthwise convolutions exhibit center-surround patterns and DoG initialization improves accuracy on tested architectures
- Medium Confidence: The center-surround patterns are functionally similar to biological receptive fields (structural similarity demonstrated but functional correspondence not rigorously established)
- Medium Confidence: The initialization approach will generalize to other depthwise convolutional architectures (supported by diverse architecture testing but not comprehensive validation)

## Next Checks

1. **Biological Functional Validation**: Measure the actual functional response properties of the trained kernels (e.g., orientation selectivity, spatial frequency tuning) and compare quantitatively to measured properties of biological retinal ganglion cells, not just visual inspection of weight patterns.

2. **Cross-Domain Generalization Test**: Apply the DoG initialization to depthwise convolutions in architectures trained on non-natural image datasets (medical imaging, satellite imagery, synthetic data) to determine whether the center-surround structure remains beneficial outside natural image statistics.

3. **Architecture Diversity Validation**: Test the initialization approach on depthwise convolutional architectures with fundamentally different design principles (e.g., MobileNetV3 with squeeze-and-excitation, EfficientNet with compound scaling) to verify the claimed broad applicability across depthwise convolution variants.