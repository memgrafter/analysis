---
ver: rpa2
title: 'SemioLLM: Evaluating Large Language Models for Diagnostic Reasoning from Unstructured
  Clinical Narratives in Epilepsy'
arxiv_id: '2407.03004'
source_url: https://arxiv.org/abs/2407.03004
tags:
- reasoning
- clinical
- seizure
- performance
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated six state-of-the-art large language models
  (GPT-3.5, GPT-4, Mixtral-8x7B, Qwen-72B, LlaMa2, LlaMa3) on a diagnostic task in
  epilepsy using unstructured clinical narratives. The task involved predicting seizure
  onset zones in the brain from free-text descriptions of seizure symptoms.
---

# SemioLLM: Evaluating Large Language Models for Diagnostic Reasoning from Unstructured Clinical Narratives in Epilepsy

## Quick Facts
- arXiv ID: 2407.03004
- Source URL: https://arxiv.org/abs/2407.03004
- Reference count: 40
- Models approach clinician-level performance on epilepsy diagnostic reasoning after expert-guided prompt engineering

## Executive Summary
This study evaluates six state-of-the-art large language models on their ability to predict seizure onset zones in the brain from unstructured clinical narratives describing seizure symptoms. The task involves mapping symptom descriptions to one of seven brain regions, requiring complex diagnostic reasoning. Through systematic prompt engineering, including chain-of-thought reasoning and clinical persona adaptation, most models achieved performance approaching that of clinical experts. The research demonstrates that structured prompting can significantly enhance LLM diagnostic capabilities, though expert analysis revealed that correct predictions may sometimes rely on hallucinated knowledge rather than genuine understanding.

## Method Summary
The study used 1,269 seizure descriptions from the Semio2Brain dataset, each annotated with one of seven brain regions. Six LLMs (GPT-3.5, GPT-4, Mixtral-8x7B, Qwen-72B, LlaMa2-70B, LlaMa3-70B) were evaluated using five prompting strategies: zero-shot, few-shot (K=5), chain-of-thought, few-shot CoT with expert-curated reasoning patterns, and self-consistency. Performance was measured using F1 scores for seizure onset zone localization, entropy-based confidence scores, calibration metrics, and expert-annotated reasoning quality. Models were also tested across different language conditions and with clinical persona adaptation.

## Key Results
- Most models achieved F1 scores approaching clinician-level performance (GPT-4: 0.844 vs clinician baseline: 0.872)
- Task-specific chain-of-thought reasoning led to the largest performance improvements (9.62-10.22% F1 gains)
- Clinical in-context impersonation improved both accuracy and confidence (up to 13.68% performance variation)
- Correct predictions were sometimes based on hallucinated knowledge, highlighting interpretability concerns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clinical narrative-based reasoning can be effectively translated into probabilistic diagnostic outputs by LLMs when prompted with structured chain-of-thought reasoning.
- Mechanism: Structured prompts (CoT and FS-CoT) guide models to decompose symptom descriptions into diagnostic reasoning steps, improving F1 scores by 9.62% to 10.22% over zero-shot.
- Core assumption: Models retain sufficient domain knowledge in pretraining to map symptom patterns to brain regions when prompted to reason explicitly.
- Evidence anchors:
  - [abstract] "expert-guided chain-of-thought reasoning leading to the most consistent improvements"
  - [section] "FS-CoT and SC both demonstrate the highest positive impact even though relying on different strategies -with FS-CoT providing expert-curated reasoning patterns that guide model outputs"
- Break condition: If reasoning step accuracy drops significantly, as observed in Mixtral's 34.48% incorrect reasoning rate, the model fails to integrate clinical inference despite good comprehension.

### Mechanism 2
- Claim: In-context persona adaptation improves both confidence and accuracy in clinical diagnostic tasks.
- Mechanism: Prompting models to adopt increasingly specialized clinical roles (e.g., "expert epileptologist") aligns their output with domain-specific reasoning styles, improving performance by up to 13.68% for GPT-4.
- Core assumption: LLMs respond to persona framing by activating relevant knowledge structures associated with that role.
- Evidence anchors:
  - [abstract] "Performance was further strongly modulated by clinical in-context impersonation... (13.7% performance variation)"
  - [section] "prompting models to impersonate clinical experts markedly improves both accuracy and confidence"
- Break condition: If persona cues are ignored or misinterpreted, models revert to generic reasoning, as seen with minimal gains for LlaMa models under impersonation.

### Mechanism 3
- Claim: Entropy-based confidence scores derived from output likelihoods align with model calibration when using task-specific prompts.
- Mechanism: Normalized Shannon entropy transforms output likelihoods into confidence scores (C = 1 - H), which improve with prompt engineering, especially FS-CoT (33.3% gain) and SC.
- Core assumption: Model likelihoods reflect internal uncertainty estimates that can be meaningfully extracted via entropy.
- Evidence anchors:
  - [abstract] "confidence similarly improves with prompt-engineering"
  - [section] "confidence scores were lowest in the zero-shot condition and consistently improved with prompt engineering"
- Break condition: If likelihoods are uniform or noisy (entropy near maximum), confidence measures fail to reflect true reliability.

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: Decompose symptom-to-region mapping into interpretable diagnostic steps.
  - Quick check question: Does the prompt instruct the model to "think step-by-step" and explain reasoning?

- Concept: In-context learning via few-shot examples
  - Why needed here: Provide exemplars that demonstrate the expected input-output format and reasoning style.
  - Quick check question: Are 5 representative (semiology, region) pairs included in the prompt?

- Concept: Entropy-based uncertainty quantification
  - Why needed here: Convert model likelihoods into interpretable confidence scores for clinical trust.
  - Quick check question: Is confidence computed as (1 - normalized Shannon entropy) over the 7-region likelihood vector?

## Architecture Onboarding

- Component map: Data preprocessing -> Prompt generation -> LLM inference -> Output parsing -> Evaluation (F1, confidence, calibration) -> Clinical reasoning assessment
- Critical path: Raw symptom description -> Prompt (persona + task) -> LLM output (likelihoods + reasoning) -> Parsed JSON -> Performance metrics
- Design tradeoffs: Use general-purpose models for broader applicability vs. fine-tuned medical models for higher baseline accuracy; prompt engineering vs. model architecture changes.
- Failure signatures: Low F1 with high confidence (overconfidence), inconsistent reasoning chains, hallucinated citations, U-shaped performance vs. description length.
- First 3 experiments:
  1. Compare zero-shot vs. CoT prompting on a small symptom set to measure reasoning impact.
  2. Test persona adaptation (AI assistant vs. expert epileptologist) to measure confidence gains.
  3. Evaluate cross-lingual performance by translating symptom descriptions while keeping prompts in English.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can retrieval-augmented generation (RAG) effectively ground LLM clinical reasoning in accurate, up-to-date knowledge?
- Basis in paper: [explicit] The paper identifies that Mixtral-8x7B has deficient source citation accuracy (18.3%) and that correct predictions can be based on hallucinated knowledge, noting that RAG "may help ground LLM reasoning in accurate, up-to-date knowledge, possibly improving reliability without retraining."
- Why unresolved: The study didn't test RAG methods, only evaluating base LLM performance with prompt engineering.
- What evidence would resolve it: Comparative experiments testing RAG-enhanced models against base models on the same seizure localization task, measuring accuracy, hallucination rates, and citation accuracy.

### Open Question 2
- Question: How do symptom description length and content complexity interact to influence LLM diagnostic performance?
- Basis in paper: [inferred] The study found a U-shaped relationship between description length and accuracy, but doesn't explain why very short descriptions sometimes outperform intermediate-length ones, suggesting content complexity and redundancy may play roles.
- Why unresolved: The analysis only examined length as a proxy for complexity without characterizing the actual content structure or semantic richness of descriptions.
- What evidence would resolve it: Detailed content analysis of descriptions across length bins, including redundancy metrics, presence of contradictory information, and semantic coherence measures, correlated with model performance.

### Open Question 3
- Question: How do multilingual capabilities of LLMs differ for clinical reasoning tasks when both prompts and inputs are non-English versus when only inputs are non-English?
- Basis in paper: [explicit] The study found GPT-4 maintained stable performance across language conditions while Mixtral-8x7B declined substantially when both prompts and inputs were non-English, suggesting limited cross-lingual generalization.
- Why unresolved: The study tested only three languages and didn't explore the underlying mechanisms of cross-lingual transfer in clinical contexts.
- What evidence would resolve it: Systematic testing across more languages and language families, including analysis of instruction tuning data and evaluation of cross-lingual transfer mechanisms.

## Limitations
- Model performance heavily depends on task-specific prompt engineering rather than general understanding
- Correct predictions may be based on hallucinated knowledge rather than genuine clinical reasoning
- Performance improvements may not generalize to different clinical tasks or real-world variations

## Confidence
- **High confidence**: Models approach clinician-level performance after prompt engineering - supported by direct F1 score comparisons showing GPT-4 achieving 0.844 F1 versus clinician baseline of 0.872
- **Medium confidence**: Clinical in-context impersonation improves performance - while statistically significant, the mechanism relies on unproven assumptions about persona alignment in LLMs
- **Medium confidence**: Entropy-based confidence scores align with calibration - calibration curves show improvement but the relationship between entropy and true uncertainty requires further validation

## Next Checks
1. **Reasoning authenticity validation**: Compare model reasoning chains against blinded clinical expert reviews to determine whether correct predictions stem from genuine understanding versus sophisticated pattern matching or hallucination

2. **Cross-task generalizability test**: Apply the same prompt engineering strategies to different clinical diagnostic tasks (e.g., radiology report interpretation) to assess whether improvements are task-specific or represent general reasoning capabilities

3. **Uncertainty calibration under distribution shift**: Evaluate model confidence and calibration when presented with symptom descriptions from different languages or cultural contexts to test the robustness of entropy-based confidence measures across real-world variations