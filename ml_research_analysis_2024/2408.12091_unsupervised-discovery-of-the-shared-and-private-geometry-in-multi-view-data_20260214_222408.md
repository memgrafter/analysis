---
ver: rpa2
title: Unsupervised discovery of the shared and private geometry in multi-view data
arxiv_id: '2408.12091'
source_url: https://arxiv.org/abs/2408.12091
tags:
- shared
- private
- data
- latent
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SPLICE, a neural network-based method for
  disentangling shared and private latent variables from paired high-dimensional data
  while preserving intrinsic geometry. SPLICE uses a "crossed butterfly" autoencoder
  architecture combined with predictability minimization to separate shared and private
  information, then employs a geometry-preserving loss based on geodesic distances
  to maintain the intrinsic structure of each latent space.
---

# Unsupervised discovery of the shared and private geometry in multi-view data

## Quick Facts
- arXiv ID: 2408.12091
- Source URL: https://arxiv.org/abs/2408.12091
- Reference count: 40
- Primary result: SPLICE disentangles shared and private latents while preserving intrinsic geometry, outperforming competing methods on simulated and experimental neural data.

## Executive Summary
This paper introduces SPLICE, a neural network-based method for disentangling shared and private latent variables from paired high-dimensional data while preserving intrinsic geometry. SPLICE uses a "crossed butterfly" autoencoder architecture combined with predictability minimization to separate shared and private information, then employs a geometry-preserving loss based on geodesic distances to maintain the intrinsic structure of each latent space. The method is validated on simulated LGN-V1 neural data, rotated MNIST digits, and experimental neurophysiology data. SPLICE outperforms competing methods in disentangling shared and private representations, preserving geometry, and robustness to noise.

## Method Summary
SPLICE employs a two-stage alternating optimization process. First, it trains a crossed butterfly autoencoder with four encoders (FA, FB, FA→B, FB→A) and two decoders (GA, GB) to minimize reconstruction loss while also minimizing the variance of measurement networks that predict one dataset from the other's private latents. This ensures shared latents contain no private information and private latents are first-order independent. Second, SPLICE estimates geodesic distances along submanifolds formed by varying one latent while fixing others, then adds a geometry preservation loss that aligns Euclidean distances in latent space with these geodesic distances. The network is retrained with this additional constraint to maintain interpretable intrinsic geometry.

## Key Results
- Correctly identifies 2-dimensional shared latents in simulated LGN-V1 data (versus 75 for RRR)
- Cleanly separates rotation angle from digit class in rotated MNIST
- Summarizes shared information in 2 dimensions versus 12 for RRR in experimental neurophysiology data
- Outperforms competing methods (RRR, DeepCCA, Lyu2021) in disentanglement quality and geometry preservation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Crossed butterfly architecture guarantees that shared latents cannot contain private information.
- Mechanism: By design, shared latents are computed from one dataset and used to reconstruct the other dataset. Any private information about the dataset being reconstructed would be inaccessible since the shared latents don't depend on that dataset's input.
- Core assumption: The encoder networks are sufficiently expressive to capture all shared information while excluding private information when computing shared latents.
- Evidence anchors:
  - [abstract]: "crossed butterfly" autoencoder architecture combined with predictability minimization to separate shared and private information
  - [section]: "the shared latents bsB→A and bsA→B are computed from the dataset they are not being used to reconstruct. Consequently, they are guaranteed to carry no private information about the view they are reconstructing"

### Mechanism 2
- Claim: Predictability minimization enforces first-order independence between private latents and the other dataset.
- Mechanism: Measurement networks predict one dataset from the private latents of the other. Minimizing the variance of these predictions encourages the private latents to be first-order independent from the other dataset.
- Core assumption: For well-trained measurement networks, minimizing output variance leads to first-order independence between private latents and the other dataset.
- Evidence anchors:
  - [abstract]: "predictability minimization to minimize how well dataset A can be predicted from the private latents for B, and vice versa"
  - [section]: "For well-trained measurement networks we have I(bzB; xA) = 0 ⇒ Var[MB→A(bzB)] = 0"

### Mechanism 3
- Claim: Geometry preservation loss ensures Euclidean distances in latent space match geodesic distances in data space.
- Mechanism: After disentangling, SPLICE traces submanifolds by fixing one latent while varying the other, then computes geodesic distances along these submanifolds. A loss term encourages latent space Euclidean distances to match these geodesic distances.
- Core assumption: The method from [42] for estimating geodesic distances is accurate enough for the geometry preservation loss to be effective.
- Evidence anchors:
  - [abstract]: "employs a geometry-preserving loss based on geodesic distances to maintain the intrinsic structure of each latent space"
  - [section]: "we retrain our SPLICE network, while incorporating a loss term that is minimized when Euclidean distances between pairs of points in the inferred latent spaces match the estimated geodesic distances"

## Foundational Learning

- Concept: Autoencoder architectures and training
  - Why needed here: SPLICE is fundamentally built on autoencoder principles for learning compressed representations that can reconstruct input data
  - Quick check question: What is the purpose of the reconstruction loss in an autoencoder, and how does SPLICE extend this with its crossed architecture?

- Concept: Canonical correlation analysis and its extensions
  - Why needed here: SPLICE builds on concepts from CCA and deep CCA, extending them to handle private information and geometry preservation
  - Quick check question: How does SPLICE differ from deep CCA in terms of what it can extract from multi-view data?

- Concept: Manifold learning and geodesic distance estimation
  - Why needed here: SPLICE's geometry preservation step relies on estimating intrinsic geometry of data manifolds through geodesic distances
  - Quick check question: Why is preserving geodesic distances important for maintaining interpretable geometry in the latent space?

## Architecture Onboarding

- Component map:
  Four encoders (FA, FB, FA→B, FB→A) compute shared and private latents; Two decoders (GA, GB) reconstruct inputs from latents; Two measurement networks (MA→B, MB→A) predict one dataset from the other's private latents; Loss functions include reconstruction loss, predictability minimization loss, geometry preservation loss

- Critical path:
  1. Encode inputs to obtain shared and private latents
  2. Decode latents to reconstruct inputs
  3. Use measurement networks to predict one dataset from the other's private latents
  4. Compute all loss terms and update networks

- Design tradeoffs:
  - Expressiveness vs. interpretability: More complex networks can capture better representations but may be harder to interpret
  - Disentanglement vs. reconstruction: Stricter disentanglement may reduce reconstruction quality if some shared information is incorrectly relegated to private latents
  - Geometry preservation vs. disentanglement: Adding geometry preservation loss may slightly degrade disentanglement if the losses conflict

- Failure signatures:
  - Poor reconstruction quality: May indicate insufficient disentanglement or overly strict disentanglement constraints
  - Private latents still predicting the other dataset: Measurement networks may not be well-trained or the disentanglement constraint is insufficient
  - Distorted geometry in latent space: Geometry preservation may be failing due to inaccurate geodesic distance estimation or inappropriate loss weighting

- First 3 experiments:
  1. Verify basic autoencoder functionality with reconstruction loss only
  2. Test disentanglement capability on synthetic data with known ground truth (like LGN-V1 simulation)
  3. Validate geometry preservation on a simple manifold (like rotated MNIST with clear circular geometry in private latents)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does SPLICE guarantee full statistical independence between latent variables, or only first-order independence as currently implemented?
- Basis in paper: [explicit] The paper states "we have found that first-order independence appears sufficient to lead networks towards solutions that also seem to satisfy full statistical independence" but acknowledges this is based on empirical observation rather than proof.
- Why unresolved: The paper explicitly notes that first-order independence is not full independence, and that higher-order independence could be necessary for more complex data. The authors call for future work to verify higher-order independence holds after training SPLICE.
- What evidence would resolve it: Mathematical proof that first-order independence constraints in SPLICE guarantee full statistical independence, or systematic testing showing when higher-order constraints become necessary.

### Open Question 2
- Question: How would SPLICE perform on multi-view data with temporal dependencies between views?
- Basis in paper: [explicit] The authors state "we limit SPLICE to minimizing the first-order independence" and "a second limitation of SPLICE is the omission of temporal dynamics. SPLICE specifically targets the geometry, and not the temporal evolution of the data."
- Why unresolved: The current SPLICE architecture treats each paired sample independently without modeling temporal structure, which may be crucial for many neuroscience applications where brain activity shows strong temporal dependencies.
- What evidence would resolve it: Application of SPLICE to time series data where temporal structure is known to be important (e.g., sequential decision-making tasks) compared against methods that incorporate temporal dynamics.

### Open Question 3
- Question: Can SPLICE be extended to handle more than two views simultaneously, or is the pairwise approach fundamentally limited?
- Basis in paper: [explicit] The authors state "One of the current primary limitations of SPLICE is that it can only account for two views at once. Analyzing three or more would require pair-wise runs of SPLICE on each pair of views."
- Why unresolved: The authors acknowledge this limitation but don't explore whether the pairwise approach is sufficient or if a true multi-view extension is possible. They note this is "likewise faced by CCA, and all extensions or generalizations thereof" but don't prove this is inherent to the problem.
- What evidence would resolve it: Mathematical analysis of whether pairwise SPLICE applications capture all shared information across multiple views, or development and testing of a true multi-view SPLICE architecture.

## Limitations

- Generalizability uncertainty to domains beyond tested neuroscience applications
- Computational cost of two-stage training and geometry preservation may limit scalability
- Performance on noisy real-world data with unknown ground truth remains uncertain

## Confidence

- Confidence in core disentanglement mechanism: High
- Confidence in geometry preservation component: Medium
- Confidence in overall superiority over competing methods: Medium-High

## Next Checks

1. Test SPLICE on additional multi-view datasets with different characteristics (e.g., natural images, time series) to assess generalizability.
2. Conduct ablation studies to quantify the contribution of each component (crossed architecture, predictability minimization, geometry preservation) to overall performance.
3. Evaluate SPLICE's robustness to varying levels of noise and missing data in the input views.