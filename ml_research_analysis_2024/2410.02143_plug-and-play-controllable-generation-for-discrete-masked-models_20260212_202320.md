---
ver: rpa2
title: Plug-and-Play Controllable Generation for Discrete Masked Models
arxiv_id: '2410.02143'
source_url: https://arxiv.org/abs/2410.02143
tags:
- masked
- generation
- protein
- distribution
- discrete
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a plug-and-play framework for controllable
  generation of discrete masked models using importance sampling and mean-field approximation.
  The approach enables sampling from distributions satisfying constraints or optimizing
  reward functions without fine-tuning, handling both differentiable and non-differentiable
  criteria.
---

# Plug-and-Play Controllable Generation for Discrete Masked Models

## Quick Facts
- arXiv ID: 2410.02143
- Source URL: https://arxiv.org/abs/2410.02143
- Authors: Wei Guo; Yuchen Zhu; Molei Tao; Yongxin Chen
- Reference count: 40
- Primary result: A plug-and-play framework for controllable generation of discrete masked models using importance sampling and mean-field approximation, achieving controlled generation while maintaining high sample quality with 10-1000 Monte Carlo samples

## Executive Summary
This paper presents a plug-and-play framework for controllable generation of discrete masked models that enables sampling from distributions satisfying constraints or optimizing reward functions without fine-tuning. The approach uses importance sampling and mean-field approximation to iteratively unmask and remask positions in sequences, allowing generation from target distributions q(x) ∝ r(x)p(x) where r(x) is a reward function and p(x) is the data distribution learned by a pretrained masked model. Extensive experiments on tasks including protein design demonstrate the method's effectiveness across domains, handling both differentiable and non-differentiable criteria while maintaining high sample quality.

## Method Summary
The framework operates by iteratively unmasking and remasking positions in sequences using a pretrained masked model that provides conditional probabilities for masked positions. At each unmasking step, K candidate completions are sampled from the masked model, their rewards are evaluated using r(x), and importance sampling is used to select one sample based on normalized weights. A portion of newly generated positions are then remasked according to a schedule γ(t/T), and the process repeats until all positions are unmasked. The method leverages mean-field approximation to factorize the joint distribution over unmasked positions, enabling tractable sampling even for long sequences. This plug-and-play approach requires no fine-tuning of the pretrained model and can handle both differentiable and non-differentiable reward functions.

## Key Results
- Achieves controlled generation while maintaining high sample quality across multiple domains including protein design
- Requires only 10-1000 Monte Carlo samples for effective generation, demonstrating computational efficiency
- Handles both differentiable and non-differentiable reward functions without fine-tuning the pretrained masked model
- Shows versatility across different sequence generation tasks while preserving the quality of generated samples

## Why This Works (Mechanism)
The method works by leveraging the pretrained masked model's ability to capture the data distribution p(x) while using importance sampling to steer generation toward distributions that satisfy desired constraints or optimize reward functions. The mean-field approximation enables tractable computation by factorizing the joint distribution over unmasked positions, making the iterative unmasking process computationally feasible. By remasking portions of generated sequences and repeating the process, the method explores the space effectively while maintaining coherence. The plug-and-play nature means the framework can work with any pretrained masked model and arbitrary reward functions, making it highly flexible.

## Foundational Learning

**Masked Language Models**: Learn to predict masked tokens given context, capturing data distribution p(x)
*Why needed*: Provides the base distribution from which controlled samples are drawn
*Quick check*: Can the model reconstruct masked tokens accurately on held-out data?

**Importance Sampling**: Reweights samples from one distribution to estimate expectations under another
*Why needed*: Enables sampling from q(x) ∝ r(x)p(x) without directly sampling from q
*Quick check*: Do importance weights properly concentrate on high-reward samples?

**Mean-Field Approximation**: Assumes independence between variables to factorize joint distributions
*Why needed*: Makes computation tractable when many positions are unmasked simultaneously
*Quick check*: Does the approximation error remain bounded as sequence length increases?

**Iterative Unmasking**: Sequentially reveals positions in sequences to build up complete samples
*Why needed*: Enables controlled generation by steering the unmasking process with rewards
*Quick check*: Does the method converge to stable distributions after sufficient iterations?

## Architecture Onboarding

**Component Map**: Pretrained Masked Model -> Iterative Unmasking Loop -> Importance Sampling -> Reward Evaluation -> Sample Selection

**Critical Path**: The core computational path involves: (1) masked model inference to get conditional probabilities, (2) K candidate sampling, (3) reward evaluation, (4) importance weight calculation and normalization, (5) sample selection, and (6) remasking according to schedule. Each iteration depends on the previous one's output.

**Design Tradeoffs**: The framework trades off between exploration (via K Monte Carlo samples) and computational efficiency. Larger K improves approximation quality but increases cost. The unmasking schedule γ(t/T) balances between preserving generated content and allowing exploration. The mean-field approximation sacrifices exactness for tractability.

**Failure Signatures**: Poor sample quality when K is too small, failure to converge when the reward function is too restrictive, or getting stuck in local optima when the unmasking schedule doesn't allow sufficient exploration. The method may also struggle when the masked model's learned distribution p(x) is very different from the target q(x).

**3 First Experiments**:
1. Generate samples from a simple discrete distribution with a known reward function and verify that generated samples match the expected distribution
2. Apply the framework to a toy sequence task (e.g., generating strings with specific character constraints) and measure constraint satisfaction rates
3. Test with different values of K (1, 10, 100, 1000) to empirically determine the minimum samples needed for quality results

## Open Questions the Paper Calls Out
**Open Question 1**: How does the mean-field approximation error scale with sequence length and vocabulary size in the proposed framework?
The paper acknowledges that mean-field approximation introduces some error, especially when many positions are unmasked simultaneously, but does not provide theoretical bounds or empirical analysis of this error. This question remains unresolved as the paper mentions the approximation introduces error but doesn't quantify or bound this error, which is crucial for understanding when the method breaks down.

**Open Question 2**: What are the optimal remasking strategies beyond uniform remasking, and how do they affect sample quality?
The paper states "uniform remasking strategy performs well" but notes "exploring more advanced remasking strategies is a promising area for future research." This question remains unresolved as the paper only experiments with uniform remasking despite acknowledging the potential for better strategies, leaving the question of optimal remasking unanswered.

**Open Question 3**: How does the choice of hyperparameters (w, α, A) in the reward function affect the trade-off between constraint satisfaction and sample diversity?
The paper notes that "there may exist an optimal choice of the hyperparameters w1 and A1 that maximizes the helix% of the generated sequences while keeping low instability index" but doesn't provide systematic analysis. This question remains unresolved as while the paper experiments with different hyperparameters, it doesn't provide a comprehensive analysis of the trade-offs or guidelines for hyperparameter selection.

## Limitations
- The framework's performance depends on the quality of the pretrained masked model's learned distribution
- Mean-field approximation introduces error that may compound with sequence length
- No theoretical guarantees on convergence or approximation quality are provided
- Implementation details like exact unmasking schedule and masked model architecture are underspecified

## Confidence
The confidence in these claims is Medium-High, as the theoretical framework is well-grounded and the experimental results are extensive, though some implementation details are underspecified.

## Next Checks
1. Implement the framework with different masked model architectures to test generality across various pretrained models
2. Systematically vary K (Monte Carlo samples) to find the minimum needed for quality results across different domains and sequence lengths
3. Compare against existing controllable generation methods on the same protein design tasks to benchmark performance and identify relative strengths/weaknesses