---
ver: rpa2
title: 'InstructionCP: A fast approach to transfer Large Language Models into target
  language'
arxiv_id: '2405.20175'
source_url: https://arxiv.org/abs/2405.20175
tags:
- language
- inscp
- data
- llms
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Instruction Continual Pre-training (InsCP),
  a novel approach to adapt Large Language Models (LLMs) to target languages. InsCP
  integrates instruction tags into the continual pre-training (CP) process, allowing
  LLMs to retain conversational and Reinforcement Learning from Human Feedback (RLHF)
  abilities while acquiring new languages.
---

# InstructionCP: A fast approach to transfer Large Language Models into target language

## Quick Facts
- arXiv ID: 2405.20175
- Source URL: https://arxiv.org/abs/2405.20175
- Authors: Kuang-Ming Chen; Hung-yi Lee
- Reference count: 9
- Primary result: Introduces InsCP, a method to adapt LLMs to target languages using 0.1B tokens while retaining RLHF abilities

## Executive Summary
This paper presents Instruction Continual Pre-training (InsCP), a novel approach to adapt Large Language Models to target languages by integrating instruction tags into the continual pre-training process. The method requires only 0.1 billion tokens of high-quality instruction-following data, significantly reducing resource consumption compared to traditional adaptation methods. InsCP enables LLMs to retain conversational and Reinforcement Learning from Human Feedback (RLHF) abilities while acquiring new languages.

The approach demonstrates effectiveness in aligning models with Traditional Chinese and Japanese prompts while maintaining RLHF capabilities. Experimental results show that LLaMA3-InsCP achieves MT-Bench scores of 7.6 for English and 6.7 for Traditional Chinese, outperforming baseline models. The method is designed to be generalizable to other languages with limited resources, offering a practical solution for multilingual LLM deployment.

## Method Summary
InsCP integrates instruction tags into the continual pre-training process of Large Language Models to adapt them to target languages. The method combines language-specific data with instruction-following data in a single pre-training phase, rather than requiring separate translation or alignment steps. By using only 0.1 billion tokens of high-quality instruction-following data, InsCP reduces the computational resources needed compared to traditional approaches. The instruction tags help maintain the model's conversational and RLHF abilities while acquiring new language capabilities. This approach is particularly effective for languages with limited resources and demonstrates strong performance on knowledge benchmarks while preserving the model's ability to follow instructions in multiple languages.

## Key Results
- InsCP effectively aligns LLaMA3 model with Traditional Chinese and Japanese prompts
- Achieves MT-Bench scores of 7.6 (English) and 6.7 (Traditional Chinese) with LLaMA3-InsCP
- Outperforms baseline models while using only 0.1 billion tokens of instruction-following data
- Successfully retains RLHF capabilities during language adaptation

## Why This Works (Mechanism)
InsCP works by integrating instruction tags directly into the continual pre-training process, which allows the model to learn language-specific patterns while maintaining its instruction-following capabilities. The instruction tags act as a bridge between the original model's conversational abilities and the new language data, preventing catastrophic forgetting of RLHF-tuned behaviors. By using a relatively small amount of high-quality instruction-following data (0.1B tokens) mixed with target language data, the model can efficiently acquire new language skills without extensive retraining. This approach leverages the existing knowledge and alignment of the base model, making the adaptation process more efficient than starting from scratch or using separate translation alignment steps.

## Foundational Learning

**Continual Pre-training (CP)**: Why needed - Allows models to learn from new data without forgetting previous knowledge. Quick check - Verify that the model maintains performance on original tasks while learning new language capabilities.

**Reinforcement Learning from Human Feedback (RLHF)**: Why needed - Ensures models align with human preferences and instructions. Quick check - Test that instruction-following capabilities are preserved after language adaptation.

**Instruction Tagging**: Why needed - Provides explicit signals to help models understand task requirements. Quick check - Confirm that models can correctly interpret and respond to instruction-tagged prompts in target languages.

**Token Efficiency**: Why needed - Reduces computational resources required for adaptation. Quick check - Compare training time and resources used against baseline adaptation methods.

**Multilingual Alignment**: Why needed - Enables models to understand and generate content in multiple languages. Quick check - Test model performance across different language pairs and tasks.

## Architecture Onboarding

**Component Map**: Base LLM -> InsCP Pre-training -> Target Language + Instruction Data -> Aligned Multilingual Model

**Critical Path**: The most critical components are the integration of instruction tags with target language data and the quality of the 0.1B token instruction-following dataset. These elements determine whether the model can successfully maintain RLHF capabilities while acquiring new language skills.

**Design Tradeoffs**: The main tradeoff is between the amount of instruction-following data used (0.1B tokens) and the quality of language adaptation. Using more data could improve language capabilities but would increase resource requirements, while using less might compromise either language learning or RLHF retention.

**Failure Signatures**: The model may fail to retain RLHF capabilities if the instruction-following data quality is poor or if the instruction tags are not properly integrated. Language adaptation may be incomplete if the target language data lacks diversity or if the token count is insufficient for effective learning.

**First Experiments**: 1) Test InsCP on a third language (e.g., Arabic) to validate generalizability. 2) Compare resource usage (training time, compute) against traditional adaptation methods. 3) Evaluate the impact of varying the instruction-following data quality on model performance.

## Open Questions the Paper Calls Out

None

## Limitations

- The approach's generalizability to languages beyond Traditional Chinese and Japanese has not been thoroughly tested
- The quality and diversity of the 0.1 billion tokens of instruction-following data used needs more empirical validation
- The claim of significant resource reduction compared to other methods lacks detailed quantification of computational costs

## Confidence

- **High confidence** in experimental results for Traditional Chinese and Japanese: MT-Bench scores demonstrate effective alignment and RLHF retention
- **Medium confidence** in resource efficiency claim: The 0.1B token requirement is stated but computational costs are not fully detailed
- **Low confidence** in generalizability claim: Limited testing to only two target languages without broader language diversity validation

## Next Checks

1. Conduct experiments with InsCP on a diverse set of languages, including those with different linguistic structures (e.g., Arabic, Finnish, Vietnamese) to validate the approach's generalizability.

2. Perform a detailed analysis of the computational resources required for InsCP compared to other adaptation methods, including training time and hardware specifications, to substantiate the resource efficiency claim.

3. Evaluate the quality and diversity of the instruction-following data used in InsCP, ensuring it is representative of the target languages and tasks, to confirm that the method can retain RLHF abilities while acquiring new languages.