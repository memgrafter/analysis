---
ver: rpa2
title: 'CAGE: Causality-Aware Shapley Value for Global Explanations'
arxiv_id: '2404.11208'
source_url: https://arxiv.org/abs/2404.11208
tags:
- causal
- feature
- features
- importance
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CAGE, a causality-aware global explanation
  method for AI models based on Shapley values. CAGE addresses the limitation of existing
  methods that assume feature independence and overlook causal relations.
---

# CAGE: Causality-Aware Shapley Value for Global Explanations

## Quick Facts
- arXiv ID: 2404.11208
- Source URL: https://arxiv.org/abs/2404.11208
- Authors: Nils Ole Breuer; Andreas Sauter; Majid Mohammadi; Erman Acar
- Reference count: 39
- The paper introduces CAGE, a causality-aware global explanation method for AI models based on Shapley values.

## Executive Summary
This paper presents CAGE (Causality-Aware Shapley Value for Global Explanations), a novel approach that addresses a critical limitation in traditional Shapley value-based explanation methods. The method incorporates causal knowledge into global feature importance calculations by sampling from post-interventional distributions rather than assuming feature independence. CAGE demonstrates improved faithfulness and intuitive explanations compared to baseline methods on both synthetic and real-world datasets, correctly handling causal relationships like indirect effects and redundant features.

## Method Summary
CAGE modifies the traditional Shapley value framework by incorporating causal structure through post-interventional sampling. Instead of sampling feature values independently as in conventional methods, CAGE samples out-coalition features from distributions that respect the causal dependencies specified in a causal graph. This approach ensures that when explaining a feature's contribution, the values of its descendants in the causal graph are appropriately adjusted to reflect their interventional distributions. The method satisfies key causal properties including the local and global Markov properties, ensuring that explanations respect the underlying causal structure of the data.

## Key Results
- On synthetic data, CAGE correctly assigns zero importance to features that can be completely explained by others, while traditional methods fail to do so
- In Alzheimer's disease dataset analysis, CAGE reduces the importance of features that are effects of other features, aligning with causal reasoning
- The method demonstrates improved faithfulness and more intuitive explanations compared to existing global explanation approaches

## Why This Works (Mechanism)
CAGE works by fundamentally changing how feature values are sampled during the Shapley value calculation. Traditional methods assume all features are independent and sample values from marginal distributions. CAGE instead samples from post-interventional distributions that respect the causal structure, meaning that when a feature is "intervened upon" during the explanation process, its descendants in the causal graph are sampled from their interventional distributions rather than their observational ones. This ensures that the explanations properly account for causal relationships rather than just statistical correlations.

## Foundational Learning

**Shapley Values** - A cooperative game theory concept for fairly distributing payoffs among players
*Why needed*: Forms the mathematical foundation for feature attribution
*Quick check*: Can you explain how Shapley values handle feature interactions?

**Causal Graphs** - Directed acyclic graphs representing causal relationships between variables
*Why needed*: Provides the structural knowledge that CAGE leverages
*Quick check*: Can you identify parent-child relationships in a simple causal graph?

**Post-Interventional Distributions** - Probability distributions after intervening on variables in a causal system
*Why needed*: Allows proper conditioning of descendant features during explanation
*Quick check*: What's the difference between P(Y|do(X)) and P(Y|X)?

**Global vs Local Explanations** - Global explanations capture overall feature importance across the dataset, while local explanations are instance-specific
*Why needed*: CAGE focuses on global explanations but the distinction matters for evaluation
*Quick check*: When would you prefer global over local explanations?

## Architecture Onboarding

**Component Map**: Causal Graph -> Post-Interventional Sampler -> Shapley Value Calculator -> Global Importance Scores

**Critical Path**: The causal graph is used to determine the sampling strategy for each coalition during the Shapley value calculation. For each subset of features being evaluated, descendant features are sampled from their interventional distributions based on the intervened features.

**Design Tradeoffs**: CAGE trades computational efficiency for causal faithfulness - the post-interventional sampling is more expensive than independent sampling but produces more accurate causal explanations. The method also requires access to a causal graph, which may not always be available or may be uncertain.

**Failure Signatures**: If the assumed causal graph is incorrect, CAGE may produce misleading explanations. The method may also struggle with highly complex causal structures or when there are hidden confounders not captured in the graph.

**First Experiments**: 
1. Test CAGE on a simple linear Gaussian causal model with known structure
2. Compare explanations on a synthetic dataset where some features are deterministic functions of others
3. Evaluate sensitivity to errors in the causal graph by systematically corrupting the assumed structure

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided.

## Limitations

- Computational complexity implications of the causal sampling approach are not fully explored
- Real-world causal structure identification is assumed to be available, which is often challenging in practice
- The approximation algorithm's accuracy across different types of causal structures remains unclear

## Confidence

- Theoretical framework and mathematical formulation: High
- Experimental results on synthetic data: Medium
- Practical applicability to real-world scenarios: Low
- Comparison with baseline methods: Medium

## Next Checks

1. Test the method on datasets with known causal structures but more complex relationships (e.g., cyclic dependencies or hidden confounders)
2. Evaluate the computational overhead compared to traditional Shapley methods across varying dataset sizes and feature counts
3. Assess the method's sensitivity to errors in the assumed causal structure - how do explanations degrade when the causal graph is partially incorrect?