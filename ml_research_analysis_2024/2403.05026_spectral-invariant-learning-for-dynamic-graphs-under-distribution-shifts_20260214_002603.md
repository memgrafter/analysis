---
ver: rpa2
title: Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts
arxiv_id: '2403.05026'
source_url: https://arxiv.org/abs/2403.05026
tags:
- dynamic
- graph
- invariant
- distribution
- spectral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies distribution shifts on dynamic graphs in the
  spectral domain, a novel perspective not previously explored. The key challenges
  are capturing different graph patterns driven by various frequency components entangled
  in the spectral domain, and handling distribution shifts with the discovered spectral
  patterns.
---

# Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts

## Quick Facts
- arXiv ID: 2403.05026
- Source URL: https://arxiv.org/abs/2403.05026
- Reference count: 40
- Key outcome: SILD achieves up to 5% improvement over the best baseline on the strongest distribution shift level

## Executive Summary
This paper introduces Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (SILD), a novel approach that addresses distribution shifts in dynamic graphs by operating in the spectral domain. The method uses Fourier transform to decompose dynamic graph trajectories into separate frequency components, then applies disentangled spectrum masking to identify invariant and variant spectral patterns. By encouraging the model to rely on invariant patterns through invariant spectral filtering, SILD demonstrates superior performance on both node classification and link prediction tasks under distribution shifts, outperforming existing baselines by up to 5% on the strongest distribution shift levels.

## Method Summary
SILD processes dynamic graph snapshots through a DyGNN backbone with Fourier transform to obtain ego-graph trajectory spectrums in the spectral domain. A disentangled spectrum mask, using both amplitude and phase information, separates invariant (MI) and variant (MV) spectral patterns. The method then applies invariant spectral filtering, minimizing prediction variance when exposed to variant patterns, to encourage reliance on invariant patterns. The overall framework is trained with a combination of task-specific losses and an invariance loss, with hyperparameters tuned for each dataset.

## Key Results
- Achieves up to 5% improvement over the best baseline on the strongest distribution shift level
- Demonstrates effectiveness on both synthetic and real-world datasets
- Outperforms existing methods for both node classification and link prediction tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SILD can capture invariant spectral patterns by disentangling amplitude and phase information in the Fourier domain.
- Mechanism: The method transforms dynamic graph trajectories into the spectral domain via FFT, then uses a disentangled spectrum mask that separately processes amplitude and phase to extract invariant (MI) and variant (MV) spectrum masks. Filtering with these masks isolates patterns with stable label correlations.
- Core assumption: Invariant and variant graph dynamics have non-overlapping frequency bandwidths (Proposition 2), allowing clean separation.
- Evidence anchors:
  - [abstract] "discover the invariant and variant spectral patterns"
  - [section 4.2] "we obtain the spectrum masks by leveraging both the amplitude and phase information"
  - [corpus] weak/no direct evidence
- Break condition: If invariant and variant patterns share frequency components, masks cannot cleanly separate them and model fails.

### Mechanism 2
- Claim: Minimizing variance of predictions under exposure to variant patterns forces the model to rely on invariant patterns.
- Mechanism: The invariant spectral filtering loss (LINV) measures variance of predictions when different variant patterns are mixed in. By minimizing this loss, the model learns to ignore variant patterns and depend only on invariant ones.
- Core assumption: Variant patterns do not causally determine labels (Assumption 1), so predictions should not change when variant patterns vary.
- Evidence anchors:
  - [section 4.3] "minimize the variance of predictions with exposure to various variant patterns"
  - [section 3] Proposition 2 shows bounded error when using invariant classifier in spectral domain
  - [corpus] weak/no direct evidence
- Break condition: If variant patterns do contain label-relevant information, minimizing variance may hurt performance.

### Mechanism 3
- Claim: Fourier transform enables separation of mixed graph dynamics into distinct frequency components, making invariant patterns observable.
- Mechanism: By applying FFT to ego-graph trajectory signals, the method transforms entangled temporal patterns into separate frequency components (Z), where each frequency band can be analyzed independently for invariant/variant properties.
- Core assumption: Graph dynamics are representable in frequency domain and mixed dynamics can be decomposed into orthogonal frequency components.
- Evidence anchors:
  - [section 4.1] "Fourier transform has the following advantages: 1) we can use fast Fourier transform (FFT)... 2) Each basis has clear semantics"
  - [section 2] "we discover that there exist cases with distribution shifts unobservable in the time domain while observable in the spectral domain"
  - [corpus] weak/no direct evidence
- Break condition: If graph dynamics are not periodic or global, FFT decomposition may not yield meaningful separation.

## Foundational Learning

- Concept: Fourier Transform for signal decomposition
  - Why needed here: To convert entangled temporal graph dynamics into separate frequency components where invariant patterns become observable
  - Quick check question: Why use FFT instead of raw temporal features when analyzing distribution shifts?
- Concept: Disentangled representation learning
  - Why needed here: To separate invariant and variant spectral patterns so model can focus on stable patterns
  - Quick check question: How does the mask ensure invariant and variant patterns are mutually exclusive?
- Concept: Invariant risk minimization
  - Why needed here: To ensure model predictions are stable across different variants of the data distribution
  - Quick check question: What happens if the variant patterns actually contain useful label information?

## Architecture Onboarding

- Component map: Input (Xt, At) -> Message Passing -> Spectral Transform (FFT) -> Spectrum Masking -> Filtering (ZI, ZV) -> Prediction (fI, fV) -> Loss (LI, LV, LINV)
- Critical path: Message Passing → Spectral Transform → Spectrum Masking → Invariant Filtering → Prediction
- Design tradeoffs:
  - FFT complexity vs. spatial-temporal methods
  - λ hyperparameter balancing predictive vs. invariant performance
  - Mask temperature τ controlling separation sharpness
- Failure signatures:
  - High variance in LINV but low task loss → model overfitting to variants
  - Poor performance on both training and test → incorrect mask separation
  - Degraded performance when λ→0 → invariance component not helping
- First 3 experiments:
  1. Run with λ=0 (SILD w/o I) to confirm invariance loss is necessary
  2. Test with random masks (vs. learned) to verify learned masks add value
  3. Compare FFT vs. no FFT (spectral vs. temporal only) to validate spectral approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SILD's performance compare to other methods on datasets with more complex and realistic distribution shifts, such as those caused by the COVID-19 pandemic or the emergence of deep learning?
- Basis in paper: [explicit] The paper mentions that the real-world datasets used in the experiments have distribution shifts caused by the COVID-19 pandemic and the emergence of deep learning, but it does not provide a detailed analysis of how SILD performs on these specific types of distribution shifts.
- Why unresolved: The paper only provides a general analysis of SILD's performance on real-world datasets with distribution shifts, but it does not provide a detailed analysis of how SILD performs on specific types of distribution shifts, such as those caused by the COVID-19 pandemic or the emergence of deep learning.
- What evidence would resolve it: A detailed analysis of SILD's performance on datasets with more complex and realistic distribution shifts, such as those caused by the COVID-19 pandemic or the emergence of deep learning.

### Open Question 2
- Question: How does SILD's performance compare to other methods on datasets with a larger number of nodes and edges?
- Basis in paper: [inferred] The paper does not provide any information on how SILD's performance scales with the size of the dataset.
- Why unresolved: The paper only provides results on datasets with a relatively small number of nodes and edges, so it is not clear how SILD would perform on larger datasets.
- What evidence would resolve it: Experiments on datasets with a larger number of nodes and edges, to see how SILD's performance scales with the size of the dataset.

### Open Question 3
- Question: How does SILD's performance compare to other methods on datasets with different types of graph structures, such as graphs with high clustering coefficient or graphs with a power-law degree distribution?
- Basis in paper: [inferred] The paper does not provide any information on how SILD's performance varies with different types of graph structures.
- Why unresolved: The paper only provides results on datasets with relatively simple graph structures, so it is not clear how SILD would perform on graphs with more complex structures.
- What evidence would resolve it: Experiments on datasets with different types of graph structures, such as graphs with high clustering coefficient or graphs with a power-law degree distribution, to see how SILD's performance varies with different types of graph structures.

## Limitations
- Validation relies heavily on synthetic datasets with constructed distribution shifts
- Method requires hyperparameter tuning (λ, τ) for optimal performance
- Effectiveness on real-world distribution shifts beyond synthetic scenarios remains less certain

## Confidence

- **High Confidence:** The Fourier transform approach for spectral decomposition and the overall architecture design
- **Medium Confidence:** The effectiveness of the disentangled spectrum mask for separating invariant/variant patterns
- **Low Confidence:** Generalization to real-world distribution shifts beyond synthetic scenarios

## Next Checks

1. Test SILD on additional real-world datasets with known distribution shifts (e.g., temporal splits with domain gaps) to validate robustness beyond synthetic scenarios
2. Perform ablation studies varying the temperature parameter τ in spectrum masking to quantify its impact on separation quality
3. Compare SILD against domain generalization methods that don't use spectral decomposition to isolate the benefit of the frequency-domain approach