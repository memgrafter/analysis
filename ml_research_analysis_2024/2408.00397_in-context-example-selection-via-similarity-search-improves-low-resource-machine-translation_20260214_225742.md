---
ver: rpa2
title: In-Context Example Selection via Similarity Search Improves Low-Resource Machine
  Translation
arxiv_id: '2408.00397'
source_url: https://arxiv.org/abs/2408.00397
tags:
- sonar
- random
- example
- language
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies whether selecting in-context examples based on
  similarity to the input sentence improves low-resource machine translation with
  large language models. While prior work on high-resource language pairs found minimal
  gains from similarity-based selection over random selection, this study systematically
  evaluates multilingual sentence embeddings across multiple LLMs and language directions,
  including low-resource pairs.
---

# In-Context Example Selection via Similarity Search Improves Low-Resource Machine Translation

## Quick Facts
- arXiv ID: 2408.00397
- Source URL: https://arxiv.org/abs/2408.00397
- Authors: Armel Zebaze; BenoÃ®t Sagot; Rachel Bawden
- Reference count: 33
- Primary result: Similarity-based in-context example selection significantly improves low-resource machine translation with large language models compared to random selection

## Executive Summary
This paper investigates whether selecting in-context examples based on similarity to the input sentence can improve low-resource machine translation when using large language models (LLMs). Prior research on high-resource language pairs showed minimal gains from similarity-based selection over random selection. The authors systematically evaluate multilingual sentence embeddings across multiple LLMs and language directions, including low-resource pairs, to determine if similarity-based selection offers advantages in these more challenging translation scenarios.

The study finds that similarity-based example selection does significantly improve translation quality for low-resource languages, with these gains observable across different LLM scales and example pool sizes. The research also highlights evaluation challenges specific to LLM-based machine translation, such as translation into wrong languages or empty outputs, and proposes a modified COMET metric (laCOMET) to address these issues.

## Method Summary
The authors evaluate similarity-based in-context example selection for low-resource machine translation using large language models. They systematically test multilingual sentence embeddings across multiple LLMs and language directions, comparing similarity-based selection against random selection. The evaluation includes low-resource language pairs to determine if the benefits observed in high-resource settings extend to more challenging translation scenarios. The study also addresses evaluation challenges in LLM-based MT by proposing laCOMET, a modified version of the COMET metric designed to handle issues like translation into wrong languages or empty outputs.

## Key Results
- Similarity-based example selection significantly improves translation quality for low-resource languages compared to random selection
- Gains from similarity-based selection are observable across different LLM scales and example pool sizes
- The proposed laCOMET metric addresses evaluation challenges in LLM-based machine translation, particularly for handling wrong language outputs or empty translations

## Why This Works (Mechanism)
Similarity-based selection works by choosing in-context examples that are semantically and stylistically close to the input sentence, providing the LLM with more relevant translation patterns. This is particularly beneficial for low-resource languages where training data is limited, as similar examples can help the model generalize better from sparse examples. The approach leverages the semantic relationships captured in multilingual sentence embeddings to identify examples that share contextual and linguistic features with the target translation, effectively augmenting the limited training data available for low-resource language pairs.

## Foundational Learning
**Multilingual sentence embeddings** - Vector representations that capture semantic meaning across multiple languages, enabling cross-lingual similarity comparisons
*Why needed*: Essential for measuring similarity between sentences in different languages when selecting in-context examples
*Quick check*: Verify that embeddings maintain semantic similarity across language pairs by testing parallel sentences

**In-context learning** - The ability of LLMs to perform tasks by conditioning on input-output examples provided in the prompt without parameter updates
*Why needed*: Forms the basis of the translation approach being evaluated
*Quick check*: Confirm that the LLM can perform basic translation with randomly selected examples

**Low-resource machine translation** - Translation between language pairs with limited parallel training data available
*Why needed*: Defines the target scenario where similarity-based selection is expected to provide the most benefit
*Quick check*: Verify data scarcity by comparing available parallel sentences to high-resource language pairs

## Architecture Onboarding

**Component map**: Input sentence -> Multilingual sentence embeddings -> Similarity search -> Example selection -> LLM prompt -> Translation output -> laCOMET evaluation

**Critical path**: The core workflow involves embedding the input sentence, searching for similar examples in the pool, constructing the prompt with selected examples, and generating the translation. The embedding and similarity search steps are critical for the effectiveness of the approach.

**Design tradeoffs**: The method trades computational overhead from embedding generation and similarity search against potential translation quality improvements. Using more sophisticated embeddings may improve selection quality but increase computation time. The number of examples selected must balance prompt token limits with the need for sufficient context.

**Failure signatures**: Translation into wrong languages, empty outputs, or degraded quality compared to random selection indicate problems with either the example selection process or the LLM's ability to leverage the selected examples. Poor embedding quality or inappropriate distance metrics can lead to irrelevant example selection.

**First experiments**:
1. Test basic translation with randomly selected examples to establish baseline performance
2. Verify that multilingual embeddings capture semantic similarity across target language pairs
3. Compare translation quality with similarity-based versus random example selection on a small validation set

## Open Questions the Paper Calls Out
The study leaves open questions about the optimal embedding models and distance metrics for different language pairs, the scalability of similarity-based selection to extremely large example pools and its computational efficiency compared to random selection, and how example selection strategies interact with different prompting techniques for LLMs.

## Limitations
- The optimal embedding models and distance metrics for different language pairs remain unclear
- Scalability to extremely large example pools and computational efficiency compared to random selection is unexplored
- The interaction between example selection strategies and different prompting techniques for LLMs warrants further investigation

## Confidence

**High**: The observation that similarity-based example selection significantly improves low-resource MT compared to random selection across multiple LLMs and language directions

**Medium**: The claim that gains from similarity-based selection are observable across different LLM scales and example pool sizes

**Medium**: The effectiveness of laCOMET as a modified evaluation metric for LLM-based MT

## Next Checks

1. Test the proposed similarity-based selection method on additional low-resource language pairs not covered in the study, particularly focusing on distant language families

2. Conduct ablation studies to determine the optimal number of in-context examples when using similarity-based selection versus random selection

3. Compare the computational efficiency and translation latency of similarity-based selection against random selection across different example pool sizes and embedding models