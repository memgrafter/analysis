---
ver: rpa2
title: 'Quati: A Brazilian Portuguese Information Retrieval Dataset from Native Speakers'
arxiv_id: '2404.06976'
source_url: https://arxiv.org/abs/2404.06976
tags:
- human
- passages
- query
- dataset
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Quati is a new Brazilian Portuguese information retrieval dataset
  designed to address the scarcity of high-quality evaluation resources for this widely
  spoken language. It includes human-written queries and a curated corpus from high-quality
  Brazilian Portuguese websites.
---

# Quati: A Brazilian Portuguese Information Retrieval Dataset from Native Speakers

## Quick Facts
- arXiv ID: 2404.06976
- Source URL: https://arxiv.org/abs/2404.06976
- Reference count: 40
- Primary result: Creates first high-quality Brazilian Portuguese IR dataset with human-written queries and LLM-annotated relevance scores, establishing baseline performance for multiple retrieval systems

## Executive Summary
Quati is a new Brazilian Portuguese information retrieval dataset designed to address the scarcity of high-quality evaluation resources for this widely spoken language. It includes human-written queries and a curated corpus from high-quality Brazilian Portuguese websites. The dataset is created using a semi-automated pipeline that leverages a large language model (GPT-4) for cost-effective relevance annotation, achieving Cohen's Kappa correlations of 0.31 compared to human annotations—comparable to typical human-human agreement in crowdsourced settings. Quati enables evaluation of diverse retrieval systems, showing balanced performance across multiple IR models and establishing a solid baseline for future research. It is publicly available in two sizes (10M and 1M passages) with 50 queries and an average of 97.78 and 38.66 annotated passages per query, respectively.

## Method Summary
The dataset creation pipeline involves collecting a Portuguese corpus from the ClueWeb22 dataset, filtering for Brazilian Portuguese URLs, and segmenting documents into passages. Human native speakers create 50-200 test queries using taxonomy-guided or document-based approaches. Multiple retrieval systems (BM25, E5, ColBERT-X, SPLADE, etc.) retrieve top-k passages per query, which are then annotated using GPT-4 with Chain-of-Thought prompting at $0.03 per query-passage pair. The pipeline produces two dataset versions (10M and 1M passages) with LLM-generated relevance scores and human validation on a subset.

## Key Results
- LLM annotations achieve Cohen's Kappa correlations of 0.31 with human annotations, comparable to human-human agreement in crowdsourced settings
- Diverse IR systems retrieve varied passages, with 61.96% of query-passage combinations returned by a single system
- Retrieval systems show balanced performance across both dataset versions, establishing solid baselines for future research
- Dataset creation cost was $140.19, demonstrating cost-effective semi-automated annotation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM annotations achieve inter-annotator agreement comparable to human annotators
- Mechanism: The LLM uses a Chain-of-Thought (CoT) prompt with in-context examples from a similar Portuguese dataset, enabling it to produce consistent relevance scores aligned with human judgment
- Core assumption: The LLM's reasoning capability is sufficient to mimic human judgment patterns on relevance scoring
- Evidence anchors:
  - [abstract]: "Cohen's Kappa correlations of 0.31 compared to human annotations—comparable to typical human-human agreement in crowdsourced settings"
  - [section 3.4]: Describes the CoT prompt with TREC 2021 DL track 4-score grading system and in-context examples
  - [corpus]: Weak - only mentions using mMARCO pt-BR as examples, no direct evaluation of dataset quality
- Break Condition: LLM's understanding of language nuances deteriorates, or prompt engineering fails to align its scoring with human judgment

### Mechanism 2
- Claim: Diverse IR systems contribute to a robust dataset by retrieving a varied set of passages
- Mechanism: Multiple IR systems (BM25, E5, ColBERT-X, SPLADE, etc.) are used to retrieve passages, ensuring that the union of results covers a wide range of potentially relevant documents
- Core assumption: Different IR systems have complementary strengths, so their combined results are more diverse than any single system
- Evidence anchors:
  - [section 3.3]: Lists the variety of IR systems and their configurations used for retrieval
  - [section 5.1]: Shows that 61.96% of query-passage combinations were returned by a single system, indicating diversity
  - [corpus]: Weak - no explicit evidence that the ClueWeb22 corpus contains diverse content relevant to the queries
- Break Condition: All IR systems converge on the same set of passages, reducing diversity and limiting generalizability

### Mechanism 3
- Claim: The semi-automated pipeline reduces labeling costs while maintaining dataset quality
- Mechanism: By using LLM for initial annotations and only validating a subset with human annotators, the dataset can be scaled efficiently without sacrificing too much quality
- Core assumption: LLM annotations are sufficiently accurate that full human annotation is unnecessary, and cost savings outweigh any quality loss
- Evidence anchors:
  - [abstract]: "cost-effective relevance annotation" and mentions the total cost of $140.19
  - [section 5.2]: Human annotators achieved Cohen's Kappa of 0.4256, comparable to LLM-human agreement of 0.31
  - [corpus]: Weak - no direct comparison of dataset quality metrics between fully manual vs. semi-automated approaches
- Break Condition: LLM annotations become too inaccurate, requiring extensive human review and negating cost benefits

## Foundational Learning

- Concept: Inter-annotator agreement metrics (Cohen's Kappa, Pearson, Spearman)
  - Why needed here: To evaluate the quality of LLM annotations by comparing them to human judgments
  - Quick check question: What does a Cohen's Kappa of 0.31 indicate about agreement between annotators?

- Concept: Information retrieval evaluation metrics (nDCG@10)
  - Why needed here: To assess the effectiveness of different IR systems using the annotated dataset
  - Quick check question: How does nDCG@10 measure the quality of ranked retrieval results?

- Concept: Chain-of-Thought prompting
  - Why needed here: To guide the LLM in producing consistent and accurate relevance judgments
  - Quick check question: Why are in-context examples important in CoT prompting for LLMs?

## Architecture Onboarding

- Component map: ClueWeb22 corpus (10M and 1M passages) -> Human-written queries -> BM25, E5, ColBERT-X, SPLADE, commercial models -> GPT-4 LLM annotation -> Human validation -> Dataset release
- Critical path: Corpus preparation → Query creation → Passage retrieval → LLM annotation → Human validation → Dataset release
- Design tradeoffs:
  - Cost vs. annotation quality: LLM reduces cost but may be less accurate than full human annotation
  - Dataset size vs. manageability: Two versions (10M and 1M) balance comprehensiveness and computational efficiency
  - Retrieval system diversity vs. agreement: Diverse systems increase coverage but may retrieve less relevant passages
- Failure signatures:
  - Low inter-annotator agreement between LLM and humans
  - High overlap in passages retrieved by all IR systems (low diversity)
  - Poor performance of retrieval systems when evaluated on the dataset
- First 3 experiments:
  1. Evaluate inter-annotator agreement between LLM and human annotators on a sample set
  2. Assess diversity of passages retrieved by different IR systems by measuring exclusive query-passage combinations
  3. Test retrieval system performance using nDCG@10 on both dataset versions to establish baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the correlation between LLM-generated relevance scores and human annotations in Brazilian Portuguese IR datasets?
- Basis in paper: [explicit] The paper notes that while GPT-4's annotations correlate with human annotations at a level comparable to human-human agreement, the correlation is still below that of human annotators. The paper suggests that this could be improved as LLMs improve and through additional prompt engineering.
- Why unresolved: The current state of LLMs and prompt engineering techniques may not yet be sufficient to achieve a higher correlation. Additionally, the subjective nature of relevance annotation tasks makes it challenging to reach perfect agreement.
- What evidence would resolve it: Further research into prompt engineering techniques, larger and more diverse datasets, and improved LLM models could provide evidence for improving the correlation between LLM-generated and human annotations.

### Open Question 2
- Question: What is the impact of using LLM-generated relevance scores on the performance of IR systems in Brazilian Portuguese?
- Basis in paper: [explicit] The paper evaluates the effectiveness of various IR systems using the LLM-generated relevance scores and finds that the results are in line with expectations, suggesting that the dataset is of good quality. However, the paper does not explore the impact of using LLM-generated scores on the performance of IR systems.
- Why unresolved: The paper focuses on the creation and evaluation of the dataset but does not delve into the implications of using LLM-generated scores for IR system performance.
- What evidence would resolve it: Conducting experiments that compare the performance of IR systems using LLM-generated scores versus human-generated scores could provide insights into the impact of using LLM-generated relevance scores.

### Open Question 3
- Question: How can we ensure the generalizability of IR datasets created using LLM-generated relevance scores to new retrievers?
- Basis in paper: [explicit] The paper notes that the diversity of retrieved passages is important for ensuring the generalizability of the dataset to new retrievers. However, it does not provide a clear solution for ensuring generalizability.
- Why unresolved: Ensuring generalizability is a complex issue that requires careful consideration of the diversity of retrieved passages and the agreement among different IR systems.
- What evidence would resolve it: Developing and testing strategies for ensuring the diversity of retrieved passages and the agreement among different IR systems could provide evidence for ensuring the generalizability of IR datasets created using LLM-generated relevance scores.

## Limitations
- Limited evaluation scope with only 50 queries and relatively small passage collections
- Reliance on a single LLM (GPT-4) without exploring alternative models or prompting strategies
- No assessment of dataset stability over time or across different LLM versions

## Confidence
- **High**: The dataset creation pipeline and basic evaluation methodology are well-documented and reproducible
- **Medium**: The quality of LLM annotations and their agreement with human judgments
- **Medium**: The diversity of retrieved passages across different IR systems

## Next Checks
1. Conduct cross-validation of LLM annotations using different prompting strategies or alternative LLMs to assess consistency
2. Evaluate retrieval system performance on a held-out query set to test generalization beyond the original 50 queries
3. Analyze the semantic diversity of passages per query using clustering or embedding-based metrics to quantify coverage beyond exclusive retrieval counts