---
ver: rpa2
title: Enrolment-based personalisation for improving individual-level fairness in
  speech emotion recognition
arxiv_id: '2406.06665'
source_url: https://arxiv.org/abs/2406.06665
tags:
- emotion
- enrolment
- speaker
- utility
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of individual-level fairness in
  speech emotion recognition (SER), where performance can vary significantly across
  speakers despite good aggregate performance metrics. The authors propose a speaker
  adaptation method that uses minimal enrolment utterances (one per class) to personalize
  a pre-trained SER model for each new speaker through a dot-product attention mechanism.
---

# Enrolment-based personalisation for improving individual-level fairness in speech emotion recognition

## Quick Facts
- arXiv ID: 2406.06665
- Source URL: https://arxiv.org/abs/2406.06665
- Reference count: 0
- One-line primary result: Speaker adaptation using minimal enrolment utterances improves both global performance and individual-level fairness in speech emotion recognition

## Executive Summary
This paper addresses individual-level fairness in speech emotion recognition (SER), where performance can vary significantly across speakers despite good aggregate metrics. The authors propose a speaker adaptation method that uses minimal enrolment utterances (one per class) to personalize a pre-trained SER model for each new speaker through a dot-product attention mechanism. They evaluate their approach on two datasets using both traditional aggregated metrics and novel individual-level fairness metrics including speaker-level UAR, Gini coefficient, and Isoelastic Social Welfare Functions. Results show that the proposed enrolment-based personalization method improves both global performance and fairness across speakers, with the best results achieved when using all available enrolment utterances.

## Method Summary
The method uses a pre-trained emotion encoder (WAV2VEC 2.0 fine-tuned for dimensional SER) with shared weights for both enrolment and emotion encoding. For each new speaker, one enrolment utterance per emotion class is selected alphabetically and encoded. A dot-product attention mechanism combines these enrolment embeddings with the target utterance embedding, where the target acts as a query attending to the enrolment embeddings (keys and values). The resulting combined embedding is passed through an MLP classifier for final prediction. The system is trained end-to-end for 50 epochs using Adam optimizer.

## Key Results
- The enrolment-based personalization method improves both global UAR and individual-level fairness metrics compared to baseline
- Using all available enrolment utterances (both neutral and emotional) yields the best performance
- Individual-level fairness metrics (Gini coefficient, ISWFs) reveal performance disparities hidden by aggregate metrics
- The dot-product attention mechanism effectively transfers emotional information from enrolment to target utterances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dot-product attention mechanism effectively transfers emotional information from enrolment utterances to the target utterance.
- Mechanism: The target utterance embedding acts as a query that attends to enrolment utterance embeddings (keys and values), allowing the model to focus on emotionally relevant aspects from the speaker's reference samples.
- Core assumption: Emotional expression patterns are consistent enough within a speaker that a few reference samples can provide useful adaptation signals.
- Evidence anchors:
  - [abstract]: "uses minimal enrolment utterances (one per class) to personalize a pre-trained SER model for each new speaker through a dot-product attention mechanism"
  - [section]: "We then combine the enrolment embeddings{ee} with the target embedding et. We choose a multihead, dot-product attention mechanism for this combination"
  - [corpus]: Weak - corpus neighbors don't discuss attention mechanisms for personalization
- Break condition: If emotional expression patterns vary too much within a speaker or enrolment samples are not representative of the speaker's typical emotional expression.

### Mechanism 2
- Claim: Using all available enrolment utterances (both neutral and emotional) provides better personalization than using only neutral or only emotional samples.
- Mechanism: A comprehensive set of enrolment samples captures a wider range of the speaker's emotional expression patterns, allowing the attention mechanism to learn more robust speaker-specific adaptations.
- Core assumption: Speakers exhibit consistent enough emotional expression patterns across different emotion classes that all classes contribute valuable information.
- Evidence anchors:
  - [abstract]: "the best results achieved when using all available enrolment utterances (both neutral and emotional)"
  - [section]: "Finally, we use all available enrolment utterances (both neutral and emotional)"
  - [corpus]: Weak - corpus neighbors don't discuss multi-class enrollment strategies
- Break condition: If certain emotion classes are not expressed consistently by a speaker, or if enrolment samples are too limited to capture meaningful patterns.

### Mechanism 3
- Claim: Speaker-level evaluation metrics (UARSP, Gini coefficient, Isoelastic Social Welfare Functions) better capture individual fairness issues than aggregate metrics.
- Mechanism: By evaluating performance separately for each speaker, these metrics reveal performance disparities that are hidden when averaging across all speakers, enabling targeted improvements in fairness.
- Core assumption: Individual-level performance varies significantly across speakers and this variation is meaningful for fairness assessment.
- Evidence anchors:
  - [abstract]: "our findings show that aggregated evaluation metrics may obfuscate fairness issues on the individual-level, which are uncovered by our evaluation"
  - [section]: "To define individual-level fairness, we begin by computing the performance on a speaker-level"
  - [section]: "We examine this utility under three different perspectives: I) Statistics... II) Gini coefficient... III) Isoelastic social welfare functions"
- Break condition: If speaker-level performance variation is minimal or not meaningful for the application context.

## Foundational Learning

- Concept: Attention mechanisms in deep learning
  - Why needed here: The personalization method relies on dot-product attention to combine enrolment and target embeddings
  - Quick check question: What is the difference between self-attention and cross-attention in transformer architectures?

- Concept: Fairness metrics in machine learning
  - Why needed here: The work introduces novel evaluation schemes for measuring individual-level fairness
  - Quick check question: How does the Gini coefficient differ from other fairness metrics like demographic parity or equalized odds?

- Concept: Few-shot learning and personalization
  - Why needed here: The method adapts models to new speakers using minimal enrolment data (one utterance per class)
  - Quick check question: What are the key challenges in few-shot learning that make it different from standard supervised learning?

## Architecture Onboarding

- Component map:
  - Emotion encoder (Eemo): WAV2VEC 2.0 model fine-tuned for dimensional SER
  - Enrolment encoder (Eenr): Shares weights with emotion encoder
  - Attention mechanism: Multihead dot-product attention combining enrolment and target embeddings
  - MLP classifier: Final classification layer
  - Enrolment utterance processor: Handles speaker-specific enrolment samples

- Critical path:
  1. Process enrolment utterances through shared encoder to generate embeddings
  2. Process target utterance through shared encoder to generate query embedding
  3. Apply attention mechanism to combine enrolment and target information
  4. Pass combined embedding through MLP classifier for final prediction

- Design tradeoffs:
  - Shared vs separate encoders: Weight sharing reduces parameters but may limit specialization
  - Number of enrolment samples: More samples improve adaptation but increase computational cost
  - Attention vs concatenation: Attention allows selective focus but adds complexity

- Failure signatures:
  - Poor performance on speakers with few or inconsistent emotional expressions
  - Overfitting to enrolment samples when dataset is small
  - Degradation in global performance while improving individual fairness

- First 3 experiments:
  1. Test baseline vs PERS A on FAU-AIBO 2-class problem to verify attention mechanism improves performance
  2. Compare PERS N, PERS E, and PERS A on MSP-Podcast to understand enrollment sample strategy impact
  3. Analyze Gini coefficient changes across models to validate individual-level fairness improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the personalisation method when given erroneous enrolment utterances, and what are the failure modes when incorrect samples are provided?
- Basis in paper: [explicit] The authors acknowledge that their method "depends on providing accurate enrolment samples, and may fail if (intentionally) given erroneous ones."
- Why unresolved: The paper identifies this as a limitation but does not empirically investigate how the system performs with noisy or incorrect enrolment data, or what types of errors cause the most significant degradation.
- What evidence would resolve it: Experiments systematically varying the quality and accuracy of enrolment samples (intentional mislabeling, noise injection, speaker mismatch) to characterize performance degradation patterns and identify failure modes.

### Open Question 2
- Question: What is the minimum number of enrolment utterances required per class to achieve meaningful personalisation improvements, and how does performance scale with enrolment sample size?
- Basis in paper: [explicit] The authors state they "always include a single enrolment utterance per class" but note that "some variants of our approach do not make use of all of them" and suggest this as a direction for future work.
- Why unresolved: While the paper demonstrates benefits of using enrolment utterances, it does not systematically explore the relationship between enrolment sample size and performance gains, or identify the point of diminishing returns.
- What evidence would resolve it: Experiments varying the number of enrolment utterances per class (from zero to multiple) to quantify performance improvements and determine optimal sample sizes for different dataset characteristics.

### Open Question 3
- Question: How does the proposed personalisation method affect model interpretability, and can we understand how enrolment information is integrated into the final predictions?
- Basis in paper: [explicit] The authors mention this as a future direction: "explainability methods can be used to understand how the network is using the additional enrolment information, e.g., by visualising and probing the embedding space before and after the enrolment information is injected."
- Why unresolved: The paper demonstrates performance improvements but does not investigate the mechanism by which enrolment utterances influence predictions or provide insights into the learned representations.
- What evidence would resolve it: Analysis using techniques like attention visualization, embedding space probing, or feature importance methods to characterize how enrolment information modifies the model's internal representations and decision-making process.

## Limitations
- Small enrolment utterance set (one per emotion class) may not capture speaker variability
- Reliance on dot-product attention assumes consistent emotional expression patterns within speakers
- Evaluation limited to two datasets (German children's robot interactions and podcast data)

## Confidence
- **High**: The attention mechanism improves performance when enrolment samples are available (Section 4.1 results show consistent gains over baseline)
- **Medium**: The claim that individual-level fairness metrics reveal problems hidden by aggregate metrics (supported by Gini coefficient improvements but correlation with real-world fairness impacts unclear)
- **Low**: The generalizability of results across different languages, age groups, and recording conditions (only German and English data tested)

## Next Checks
1. **Cross-dataset robustness test**: Evaluate the personalization method on additional emotion datasets with different recording conditions, languages, and speaker demographics to assess generalizability.

2. **Enrolment sample sensitivity analysis**: Systematically vary the number and selection criteria of enrolment utterances (e.g., 2-5 samples per class) to quantify the tradeoff between adaptation quality and enrolment data requirements.

3. **Longitudinal speaker consistency study**: Track individual speaker performance across multiple sessions to validate the assumption that emotional expression patterns remain stable enough for effective personalization.