---
ver: rpa2
title: 'CAREL: Instruction-guided reinforcement learning with cross-modal auxiliary
  objectives'
arxiv_id: '2411.19787'
source_url: https://arxiv.org/abs/2411.19787
tags:
- instruction
- learning
- carel
- babyai
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of grounding natural language instructions
  within reinforcement learning environments to improve sample efficiency and generalization
  in instruction-following tasks. The authors propose CAREL, a framework that incorporates
  auxiliary cross-modal contrastive loss functions inspired by video-text retrieval
  literature to align observation sequences with instructions at multiple granularities.
---

# CAREL: Instruction-guided reinforcement learning with cross-modal auxiliary objectives

## Quick Facts
- arXiv ID: 2411.19787
- Source URL: https://arxiv.org/abs/2411.19787
- Authors: Armin Saghafian; Amirmohammad Izadi; Negin Hashemi Dijujin; Mahdieh Soleymani Baghshah
- Reference count: 20
- Primary result: CAREL improves sample efficiency and systematic generalization in instruction-following RL tasks through cross-modal auxiliary losses and instruction tracking.

## Executive Summary
This paper addresses the challenge of grounding natural language instructions within reinforcement learning environments to improve sample efficiency and generalization in instruction-following tasks. The authors propose CAREL, a framework that incorporates auxiliary cross-modal contrastive loss functions inspired by video-text retrieval literature to align observation sequences with instructions at multiple granularities. Additionally, they introduce an instruction tracking mechanism that masks completed sub-tasks during training to focus the agent on remaining objectives. Experiments on MiniGrid and BabyAI environments demonstrate that CAREL improves both sample efficiency and systematic generalization compared to baselines, with instruction tracking further enhancing performance in complex multi-step tasks.

## Method Summary
CAREL combines cross-modal auxiliary loss functions with instruction tracking to improve grounding between natural language instructions and observation sequences in RL environments. The method computes similarity scores at multiple granularities (episode-level, word-level) between encoded observations and instructions, then applies contrastive loss on successful trajectories. Instruction tracking monitors sub-task completion using these similarity scores and probabilistically masks completed sub-tasks during training to reduce distraction and improve focus on remaining objectives.

## Key Results
- CAREL demonstrates superior sample efficiency compared to baselines in MiniGrid and BabyAI environments
- Instruction tracking further improves performance in complex multi-step tasks
- The framework shows improved systematic generalization to unseen tasks during validation
- Cross-modal auxiliary loss improves alignment between observation sequences and instructions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The auxiliary cross-modal contrastive loss improves grounding between instruction and observations at multiple granularities.
- Mechanism: The auxiliary loss computes four similarity scores (Episode-Instruction, Episode-Word, Observation-Instruction, Observation-Word) using encoded representations of observations and instruction tokens. These scores are aggregated through attention mechanisms (AOSM) to create a final similarity score that is used in a contrastive loss function. This loss is applied only to successful trajectories, ensuring alignment between behavior and instruction.
- Core assumption: Successful trajectories provide meaningful alignment between observation sequences and instructions, making them suitable for contrastive learning.
- Evidence anchors:
  - [abstract] "The authors propose CAREL, a framework that incorporates auxiliary cross-modal contrastive loss functions inspired by video-text retrieval literature to align observation sequences with instructions at multiple granularities."
  - [section 2.1] "The auxiliary loss in CAREL is applied only to successful trajectories to ensure that the loss signals align with goal-relevant behavior."
- Break condition: If the success threshold is too low, contrastive learning may be applied to misaligned trajectory-instruction pairs, potentially degrading representations.

### Mechanism 2
- Claim: Instruction tracking masks completed sub-tasks to focus the agent on remaining objectives.
- Mechanism: At each time step, similarity scores between partial episodes and sub-tasks are tracked. When similarity spikes significantly above the moving average, the corresponding sub-task is masked from the instruction with a probability that increases over training time. This prevents the agent from being distracted by completed sub-tasks and encourages focus on remaining objectives.
- Core assumption: Similarity scores between observations and sub-tasks can reliably indicate task completion status.
- Evidence anchors:
  - [abstract] "Additionally, they introduce an instruction tracking mechanism that masks completed sub-tasks during training to focus the agent on remaining objectives."
  - [section 2.2] "Once calculated at each time step of the episode, this matching can signal the agent about the status of the sub-task accomplishments."
- Break condition: If similarity scores are not well-calibrated (e.g., early in training), masking may remove uncompleted sub-tasks, disrupting task completion.

### Mechanism 3
- Claim: The combination of auxiliary loss and instruction tracking creates synergistic effects that improve both sample efficiency and systematic generalization.
- Mechanism: The auxiliary loss improves the quality of cross-modal representations, which in turn improves the reliability of similarity scores used for instruction tracking. Better similarity scores lead to more accurate sub-task masking, which reduces distraction and improves focus on remaining tasks. This creates a positive feedback loop where improved representations lead to better tracking, which leads to better learning signals.
- Core assumption: Improvements in cross-modal representations will translate to improvements in instruction tracking performance.
- Evidence anchors:
  - [abstract] "The results of our experiments suggest superior sample efficiency and systematic generalization for this framework in multi-modal reinforcement learning problems."
  - [section 2.2] "These two techniques can be applied jointly, as the auxiliary loss improves the similarity scores through time, and the improved similarities enhance the instruction tracking."
- Break condition: If the auxiliary loss creates representations that are too general or not task-specific, the improved representations may not translate to better instruction tracking.

## Foundational Learning

- Concept: Cross-modal contrastive learning
  - Why needed here: The problem requires aligning two different modalities (visual/symbolic observations and natural language instructions) that don't share a common representation space by default.
  - Quick check question: How does contrastive learning distinguish between matching and non-matching pairs in a cross-modal setting?

- Concept: Attention mechanisms for aggregation
  - Why needed here: The similarity scores need to be aggregated across different levels of granularity (episode-level, word-level) to create meaningful similarity measures for the contrastive loss.
  - Quick check question: What is the difference between the bi-level attention used for Observation-Word similarity versus the simpler attention used for other similarity scores?

- Concept: Partial observability in reinforcement learning
  - Why needed here: The agent only observes partial states of the environment, and the instruction tracking mechanism must work with these partial observations to infer task progress.
  - Quick check question: How does the instruction tracking mechanism handle situations where the agent's partial observation doesn't provide enough information to determine sub-task completion?

## Architecture Onboarding

- Component map:
  - Observation encoder (CNN for BabyAI, potentially CLIP for RGB environments)
  - Instruction encoder (token embedding + GRU for BabyAI, BERT tokenizer + Multihead-Attention for SHELM)
  - Local representation extraction (observation-action pairs with positional embeddings)
  - Global representation aggregation (mean-pooling or attention over local representations)
  - Auxiliary loss computation module (similarity score calculation + AOSM aggregation + contrastive loss)
  - Instruction tracking module (sub-task extraction + similarity monitoring + masking mechanism)

- Critical path:
  1. Encode observations and instructions into local representations
  2. Aggregate local representations into global representations
  3. Compute similarity scores at multiple granularities
  4. Apply auxiliary loss on successful trajectories
  5. Track instruction completion using similarity scores
  6. Mask completed sub-tasks with increasing probability over training

- Design tradeoffs:
  - Using only successful trajectories for auxiliary loss provides clean alignment signals but reduces the amount of data available for training the auxiliary objective
  - The instruction tracking mechanism adds computational overhead at each time step but can significantly improve sample efficiency
  - The choice between mean-pooling and attention for global representation aggregation affects both performance and computational cost

- Failure signatures:
  - If the auxiliary loss coefficient is too high, the model may overfit to the auxiliary objective at the expense of the primary RL task
  - If the instruction tracking threshold is too aggressive, the model may mask uncompleted sub-tasks, leading to task failure
  - If the success threshold for auxiliary loss is too low, the contrastive learning may be applied to misaligned trajectory-instruction pairs

- First 3 experiments:
  1. Verify the auxiliary loss implementation by running on a simple environment with clear success criteria and comparing success rates with and without the auxiliary loss
  2. Test the instruction tracking mechanism in isolation by running on a single trajectory with known sub-task completion order and verifying that masking occurs at the correct times
  3. Validate the interaction between auxiliary loss and instruction tracking by running on a multi-step environment and checking that improvements in representation quality lead to more accurate sub-task detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CAREL vary with different granularities of instruction parsing (e.g., word-level vs. phrase-level vs. sub-task level) and what is the optimal granularity for maximizing sample efficiency?
- Basis in paper: [explicit] The paper mentions using SpaCy or Large Language Models to break instructions down into subtasks, suggesting multiple levels of granularity are possible. The authors also track similarity scores at different levels (Episode-Instruction, Episode-Word, Observation-Instruction, Observation-Word).
- Why unresolved: The paper uses a rule-based parsing approach and focuses on sub-task level masking, but doesn't systematically compare performance across different parsing granularities or identify the optimal granularity.
- What evidence would resolve it: Experiments comparing CAREL performance using different parsing granularities (e.g., word-level, phrase-level, sub-task level) on the same tasks, with quantitative metrics showing which granularity yields the best sample efficiency and generalization.

### Open Question 2
- Question: What is the impact of CAREL on instruction-following performance in more complex, real-world environments beyond MiniGrid and BabyAI, particularly those with noisy observations, partial observability, and longer instruction sequences?
- Basis in paper: [inferred] The paper demonstrates CAREL's effectiveness in controlled benchmark environments but doesn't test it in more realistic settings. The authors mention future directions including "further experiments on more complex environments."
- Why unresolved: The current experiments are limited to relatively simple grid-world environments, and it's unclear whether CAREL's benefits would scale to more complex scenarios with real-world challenges.
- What evidence would resolve it: Experiments applying CAREL to more complex environments (e.g., 3D navigation tasks, robotic manipulation with real sensor data) showing performance metrics compared to baselines, particularly in scenarios with increased complexity and noise.

### Open Question 3
- Question: How does the masking probability schedule (currently using a tanh function) affect CAREL's performance, and could alternative scheduling strategies lead to better results?
- Basis in paper: [explicit] The paper mentions using a hyperbolic tangent function for masking probability that "starts very low in the initial stages of training and rises over time." However, the authors don't explore alternative scheduling strategies.
- Why unresolved: The choice of tanh function is arbitrary, and different scheduling strategies (e.g., linear, exponential, adaptive) could potentially yield better performance by optimizing when and how aggressively instruction tracking is applied.
- What evidence would resolve it: Experiments comparing CAREL performance using different masking probability schedules (e.g., linear increase, exponential increase, adaptive based on validation performance) with quantitative results showing which schedule maximizes sample efficiency and final performance.

## Limitations

- The paper lacks ablation studies isolating the individual contributions of auxiliary loss and instruction tracking mechanisms.
- Implementation details for key components like AOSM attention mechanism are not fully specified.
- Hyperparameter values for critical components like similarity threshold and masking probability are not provided.

## Confidence

- **High**: The technical framework is well-defined and the core mechanisms (cross-modal contrastive loss, instruction tracking) are clearly explained.
- **Medium**: The experimental results show improvements, but the lack of ablations and failure mode analysis reduces confidence in the robustness of the claims.
- **Low**: The paper does not provide sufficient detail on hyperparameter sensitivity or computational requirements, making it difficult to assess practical applicability.

## Next Checks

1. **Ablation Studies**: Run experiments with only the auxiliary loss, only the instruction tracking, and both mechanisms to isolate their individual contributions to the reported improvements.
2. **Failure Mode Analysis**: Test the model with low success thresholds for auxiliary loss and poorly calibrated similarity scores early in training to identify potential failure modes.
3. **Computational Overhead Evaluation**: Measure the computational overhead of instruction tracking in complex environments and assess its impact on training time and resource requirements.