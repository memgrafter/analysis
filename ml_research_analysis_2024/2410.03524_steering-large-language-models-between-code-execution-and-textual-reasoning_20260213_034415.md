---
ver: rpa2
title: Steering Large Language Models between Code Execution and Textual Reasoning
arxiv_id: '2410.03524'
source_url: https://arxiv.org/abs/2410.03524
tags:
- code
- task
- text
- arxiv
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the problem of steering large language models
  (LLMs) to choose between code execution and textual reasoning based on task complexity.
  Through experiments across 14 diverse tasks and 6 LLMs, the authors find that no
  single method optimally decides when to use code or text.
---

# Steering Large Language Models between Code Execution and Textual Reasoning

## Quick Facts
- arXiv ID: 2410.03524
- Source URL: https://arxiv.org/abs/2410.03524
- Authors: Yongchao Chen; Harsh Jhamtani; Srinagesh Sharma; Chuchu Fan; Chi Wang
- Reference count: 40
- One-line primary result: No single method optimally decides when to use code or text; proposed methods achieve notable performance improvements

## Executive Summary
This paper investigates the challenge of steering large language models to choose between code execution and textual reasoning based on task complexity. Through systematic experiments across 14 diverse tasks and 6 LLMs, the authors reveal that forcing code-only or text-only approaches can be suboptimal, with smaller models sometimes outperforming larger ones when using Code Interpreter. The study proposes three novel methods—Code Interpreter+, Code + Text + Sum., and Self-estimate Score—that significantly improve steering performance by better matching the reasoning modality to task requirements.

## Method Summary
The paper systematically evaluates 10 methods for steering LLMs between code execution and textual reasoning across 14 diverse tasks using 6 different LLM models. The methods include 7 baselines (Only Question, All Text, All Code, All Code + CoT, AutoGen Conca., AutoGen System, Code Interpreter) and 3 proposed approaches. Experiments measure success rates, token lengths, and runtime, with performance evaluated using Average Normalized Score across tasks. The Code + Text + Sum. method implements a multi-agent framework that combines answers from both code and text approaches, achieving notable performance improvements over all baselines.

## Key Results
- No single method optimally decides when to use code or text across all tasks
- Smaller models with Code Interpreter can outperform larger models due to less overconfidence in textual reasoning
- Forcing LLMs to always answer with code can lead to worse accuracy than mixed approaches
- Code + Text + Sum. method achieves notable performance improvements across 6 models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller models with Code Interpreter can outperform larger models due to less overconfidence in textual reasoning.
- Mechanism: GPT-4o tends to rely more on textual reasoning due to higher confidence, leading to incorrect decisions on medium-difficulty tasks where code would be more effective. Smaller models like GPT-3.5 are more conservative and default to code more often.
- Core assumption: The decision to use code vs. text is influenced by the model's confidence in its textual reasoning ability.
- Evidence anchors:
  - [abstract]: "we also discover that results from LLM written code are not always better than using textual reasoning, even if the task could be solved through code."
  - [section]: "Based on the above property, we also see that in some tasks smaller models outperform larger models when all augmented with CI, which is inverse to the well-known scaling law in LLMs (Kaplan et al., 2020)."
  - [corpus]: No direct corpus evidence for this specific inverse scaling behavior with Code Interpreter.

### Mechanism 2
- Claim: Forcing LLMs to always answer with code can lead to worse accuracy than allowing mixed code/text approaches.
- Mechanism: Code format imposes stricter constraints on thought processes, limiting reasoning diversity and potentially degrading performance on tasks requiring complex logical reasoning. Additionally, LLMs may generate code that resembles textual reasoning rather than functional implementations.
- Core assumption: The constraints of code format can hinder the reasoning ability of LLMs in certain tasks.
- Evidence anchors:
  - [abstract]: "We also discover that results from LLM written code are not always better than using textual reasoning, even if the task could be solved through code."
  - [section]: "The coding format will limit the space of generated tokens so that the reasoning ability is undermined. In logical reasoning tasks like Date Understanding, LLM's reasoning ability is degraded when using code..."
  - [corpus]: No direct corpus evidence for this specific claim about code format limiting reasoning ability.

### Mechanism 3
- Claim: Combining code and textual reasoning through multi-agent frameworks improves performance across diverse tasks.
- Mechanism: The Code + Text + Sum. method implements a multi-agent framework that first queries LLMs to answer the question with both All Text and All Code methods, then combines and summarizes both versions of the answers. This approach leverages the strengths of both modalities.
- Core assumption: Different tasks have varying optimal modalities (code vs. text), and combining both approaches can capture the best of each.
- Evidence anchors:
  - [abstract]: "To mitigate the above issues, we propose three methods to better steer LLM code/text generation and achieve a notable improvement. The costs of token lengths and runtime are thoroughly discussed for all the methods."
  - [section]: "The experimental results also show that the method Code + Text + Sum. achieves notable performance improvements over all the other 9 methods in both average scores and ranks."
  - [corpus]: No direct corpus evidence for this specific multi-agent combination approach.

## Foundational Learning

- Concept: Task complexity scaling
  - Why needed here: Understanding how task complexity affects model behavior is crucial for developing effective steering methods. The paper reveals that model decisions to use code vs. text depend on task complexity levels.
  - Quick check question: How does task complexity influence whether LLMs choose code or text as their primary reasoning modality?

- Concept: Inverse scaling behavior
  - Why needed here: The paper demonstrates that smaller models can outperform larger ones in certain setups, which is contrary to traditional scaling laws. Understanding this phenomenon is essential for developing effective model selection strategies.
  - Quick check question: Under what conditions might smaller language models outperform larger ones when using Code Interpreter?

- Concept: Multi-modal reasoning
  - Why needed here: The paper shows that combining code and textual reasoning can improve performance. Understanding how to effectively integrate multiple reasoning modalities is crucial for developing advanced LLM applications.
  - Quick check question: What are the benefits and challenges of combining code-based and text-based reasoning approaches?

## Architecture Onboarding

- Component map: Input question → Method selection → Model processing → Code execution (if applicable) → Answer evaluation → Cost calculation
- Critical path: Question → Method selection → Model processing → Code execution (if applicable) → Answer evaluation → Cost calculation
- Design tradeoffs: Higher performance methods like Code + Text + Sum. consume more tokens and runtime compared to simpler approaches like Only Question.
- Failure signatures: Models getting stuck in infinite loops during code execution, incorrect code generation that resembles textual reasoning, overconfidence in textual reasoning leading to wrong decisions.
- First 3 experiments:
  1. Test Number Multiplying task with varying complexity levels to observe code/text usage patterns
  2. Compare performance of different model sizes (GPT-3.5 vs GPT-4o) on Game 24 task
  3. Evaluate the Code + Text + Sum. method against baseline approaches on a diverse set of tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a more efficient method that improves performance with fewer resources compared to Code + Text + Sum. and All Code with multi-turn refinement?
- Basis in paper: [explicit] Section 3.5 "We still need a more efficient method that improves performance with fewer resources."
- Why unresolved: The paper identifies that Code + Text + Sum. and All Code with multi-turn refinement achieve higher performance but consume more tokens and runtime. While these methods show promise, they are resource-intensive and there's no clear path to making them more efficient.
- What evidence would resolve it: A new method that achieves comparable or better accuracy than Code + Text + Sum. while using fewer tokens and less runtime. Empirical comparison showing improved performance-per-resource metrics.

### Open Question 2
- Question: Can we build an extra scoring model or train the LLM to learn when to use code/text more effectively than the Self-estimate Score method?
- Basis in paper: [explicit] Section 4 "Inspired by the method Self-estimate Score, whether we can build an extra scoring model or train the LLM to learn when to use code/text more effectively."
- Why unresolved: The Self-estimate Score method requires the LLM to evaluate its own confidence in solving tasks using either code or text. While this shows improvement, it's unclear if this self-evaluation process can be enhanced through additional training or external models.
- What evidence would resolve it: Development and testing of an enhanced scoring mechanism that outperforms the current Self-estimate Score method in terms of accuracy and efficiency. Comparative analysis showing statistically significant improvements.

### Open Question 3
- Question: What is the underlying mechanism causing the inverse scaling behavior where smaller models outperform larger ones in certain tasks?
- Basis in paper: [explicit] Section 2.2 "We hypothesize that GPT-4o will increasingly use code as task complexity rises. However, our additional tests (not included in the paper) indicate that GPT-4o only begins using code to solve Game 24 problems when the complexity of the tasks becomes exceedingly high."
- Why unresolved: The paper observes that smaller models like GPT-3.5 sometimes outperform larger models like GPT-4o when augmented with Code Interpreter, particularly in tasks like Game 24 and Number Multiplying. The reasons for this phenomenon are hypothesized but not definitively proven.
- What evidence would resolve it: Detailed analysis of model confidence levels and decision-making processes across different model sizes, including investigation into how models assess task complexity and choose between code and text. Empirical data showing consistent patterns in when and why smaller models outperform larger ones.

## Limitations

- Evidence gaps exist as some claims lack direct corpus evidence, particularly regarding inverse scaling behavior and code format constraints
- Methodological constraints arise from reliance on commercial LLM APIs with potential implementation variations
- Evaluation framework depends on predefined rules for assessing code correctness and answer quality that are not fully detailed

## Confidence

**High Confidence**: The experimental methodology is sound, with proper controls and multiple baseline comparisons. The observation that forcing code-only answers can degrade performance is well-supported by the data. The overall finding that no single method works optimally across all tasks is robust and reproducible.

**Medium Confidence**: The mechanisms explaining why smaller models sometimes outperform larger ones (inverse scaling) are plausible but could benefit from more rigorous theoretical grounding. The specific implementation details of the proposed methods, particularly Code Interpreter+ and the self-estimate score mechanism, could affect reproducibility.

**Low Confidence**: The exact formulation of the predefined rules for answer evaluation and format adjustment is not fully specified, which could impact the exact performance numbers achieved.

## Next Checks

1. Reproduce inverse scaling behavior: Run experiments comparing GPT-3.5 vs GPT-4o on a subset of tasks (particularly Game 24 and Number Multiplying) with and without Code Interpreter to verify the claimed performance patterns.

2. Test code format constraints: Design experiments that systematically vary the complexity of logical reasoning tasks to measure how code format constraints affect reasoning performance compared to text-only approaches.

3. Validate multi-agent combination: Implement the Code + Text + Sum. method with different answer combination strategies (weighted averaging, majority voting, etc.) to test the robustness of the claimed performance improvements and identify optimal combination approaches.