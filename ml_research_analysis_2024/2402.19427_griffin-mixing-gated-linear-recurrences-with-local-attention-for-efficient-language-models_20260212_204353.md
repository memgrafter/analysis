---
ver: rpa2
title: 'Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient
  Language Models'
arxiv_id: '2402.19427'
source_url: https://arxiv.org/abs/2402.19427
tags:
- attention
- griffin
- arxiv
- linear
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Griffin, a hybrid language model combining
  gated linear recurrences (Hawk) with local attention. The authors propose the RG-LRU
  layer, a gated recurrent unit that can scale efficiently to long sequences.
---

# Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models

## Quick Facts
- **arXiv ID**: 2402.19427
- **Source URL**: https://arxiv.org/abs/2402.19427
- **Reference count**: 40
- **Primary result**: Griffin matches Llama-2 performance despite being trained on 6x fewer tokens by combining gated linear recurrences with local attention

## Executive Summary
Griffin introduces a hybrid language model architecture that combines Hawk's gated linear recurrences (RG-LRU) with local attention to achieve efficient language modeling. The key innovation is RG-LRU, a recurrent layer with gating mechanisms that can scale efficiently to long sequences while maintaining fixed memory overhead. By interleaving RG-LRU blocks with local attention, Griffin leverages the long-range modeling capability of recurrences while retaining the precise local context capture of attention. The model demonstrates competitive performance with Llama-2 despite significantly less training data, and shows strong extrapolation capabilities on sequences longer than seen during training.

## Method Summary
The paper introduces Hawk, a model using RG-LRU layers with a novel gating mechanism that enables efficient training and inference on long sequences. RG-LRU uses diagonal recurrence with element-wise gates (input gate and recurrence gate) that depend only on the input and are not influenced by the recurrent state, enabling efficient implementation. Griffin builds on Hawk by mixing RG-LRU blocks with local attention layers in a residual architecture. The model is trained on the MassiveText dataset with sequence length 2048, though evaluation shows performance on much longer sequences. The architecture is designed to be memory-bound rather than compute-bound on TPU-v3 hardware, enabling efficient scaling to long sequences.

## Key Results
- Griffin matches Llama-2 performance despite being trained on 6x fewer tokens
- RG-LRU enables efficient inference on sequences significantly longer than training length
- Griffin outperforms MQA Transformers on MMLU, HellaSwag, PIQA, ARC-E, ARC-C, and WinoGrande benchmarks
- The hybrid architecture provides both long-range modeling via recurrence and precise local context via attention

## Why This Works (Mechanism)

### Mechanism 1
RG-LRU's gated recurrence reduces memory overhead during decoding because the recurrence state is fixed-size, unlike Transformer KV cache which grows linearly with sequence length. The recurrence gate and input gate control state updates without full dependence on prior hidden state. This enables extrapolation on sequences longer than training length. The gating mechanism breaks associativity, preventing parallelization via associative scans. If gates fail to properly filter inputs, the fixed state becomes saturated and cannot represent long-range dependencies.

### Mechanism 2
Mixing RG-LRU with local attention gives Griffin both long-range and short-range modeling. Local attention captures recent tokens precisely while RG-LRU propagates information across long sequences via the fixed state. The recurrence can transmit information across longer distances than the local attention window alone. If the local attention window is too small or RG-LRU gates over-filter, long-range information may not propagate effectively.

### Mechanism 3
RG-LRU's diagonal recurrence and element-wise gates make it memory-bound rather than compute-bound on TPU-v3. Low FLOPs-to-byte ratio means runtime is dominated by memory transfers between HBM and VMEM rather than computation. This occurs because element-wise operations with low arithmetic intensity cannot fully utilize MXU units. If recurrence operations become compute-bound due to higher arithmetic intensity or if custom kernel optimizations fail, this characterization may change.

## Foundational Learning

- **Linear recurrences and associative scans**: RG-LRU uses linear recurrence that can theoretically be parallelized, but gating breaks associativity. Why needed: Understanding why parallelization techniques don't apply to RG-LRU despite its linear nature. Quick check: Why can't we use associative scans for RG-LRU despite the recurrence being linear?

- **Memory hierarchy in accelerators**: Understanding HBM vs VMEM vs MXU memory levels is crucial for RG-LRU's memory-bound characterization. Why needed: RG-LRU's performance depends critically on data movement bottlenecks. Quick check: What determines whether an operation is memory-bound or compute-bound on TPU-v3?

- **Gating mechanisms in RNNs**: RG-LRU introduces a novel recurrence gate distinct from standard LSTM/GRU gating. Why needed: Understanding how RG-LRU's gating differs from other architectures like Mamba. Quick check: How does RG-LRU's recurrence gate differ from Mamba's input-dependent selection mechanism?

## Architecture Onboarding

- **Component map**: Input → Residual block → (MLP block + Temporal mixing block) → Output. Temporal mixing can be MQA, local MQA, or RG-LRU. Griffin interleaves RG-LRU and local MQA.
- **Critical path**: Token embedding → N residual blocks → RMSNorm → Output projection. Each residual block: RMSNorm → Temporal mixing → Skip-add → RMSNorm → Gated MLP → Skip-add.
- **Design tradeoffs**: RG-LRU vs full attention: RG-LRU has fixed state size (good for inference) but may struggle with copying/retrieval tasks. Local attention window size vs model performance: larger windows improve local context but increase memory usage.
- **Failure signatures**: If Hawk/Griffin trains slower than baseline, check RG-LRU kernel implementation and memory transfers. If extrapolation fails, verify recurrence gates and local attention window size. If copying/retrieval tasks fail, suspect insufficient recurrence capacity or attention coverage.
- **First 3 experiments**: 1) Compare training step time of Hawk vs MQA Transformer on TPU-v3 with varying sequence lengths. 2) Measure inference latency and throughput of Griffin vs MQA Transformer with increasing sequence lengths. 3) Test Griffin's extrapolation by evaluating on sequences 4x longer than training sequence length.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the choice of local attention window size affect Griffin's performance across different sequence lengths and training contexts? The paper only tests limited window sizes for a single model size. A comprehensive ablation study varying window sizes across multiple scales would reveal optimal strategies.

- **Open Question 2**: What is the impact of complex-valued RG-LRU on language modeling performance compared to the real-valued version? The paper mentions complex-valued RG-LRU exists but provides no empirical comparison. Training both versions would determine if complex recurrences offer benefits.

- **Open Question 3**: How does Griffin's performance on copying and retrieval tasks compare to other state-space model architectures like Mamba when evaluated on pre-trained models? The paper shows Griffin outperforms pre-trained models on phone number lookup but doesn't directly compare to Mamba on pre-trained models for these tasks.

## Limitations

- Architecture scalability to 100B+ parameter models remains untested, with fixed recurrence state potentially becoming a bottleneck
- Performance on copying and retrieval intensive tasks may be limited without additional architectural modifications
- Hardware portability is uncertain as memory-bound characterization is specific to TPU-v3

## Confidence

**High Confidence**:
- RG-LRU provides efficient inference on long sequences due to fixed state size
- Griffin achieves competitive performance with Llama-2 despite fewer training tokens
- Hybrid architecture effectively combines recurrence and local attention benefits

**Medium Confidence**:
- RG-LRU's memory-bound characterization on TPU-v3 will generalize to other hardware
- Scaling laws identified will hold at larger scales
- Extrapolation capability extends reliably to much longer sequences

**Low Confidence**:
- Performance on copying/retrieval intensive tasks without modifications
- Scaling to 100B+ parameter models without fundamental changes
- Direct performance comparison across different hardware platforms

## Next Checks

- **Validation Check 1**: Implement and compare RG-LRU performance on both TPU-v4 and A100 GPU hardware to verify whether memory-bound characterization holds across different accelerator architectures and memory hierarchies.

- **Validation Check 2**: Design systematic experiments testing Griffin's performance on tasks with varying copying and retrieval requirements (e.g., code completion, document QA, multi-document summarization) to quantify claimed limitations for these specific task types.

- **Validation Check 3**: Train Griffin models at intermediate scales (20B, 50B parameters) to test whether identified scaling laws hold beyond 14B parameter upper bound and whether fixed recurrence state becomes limiting at larger scales.