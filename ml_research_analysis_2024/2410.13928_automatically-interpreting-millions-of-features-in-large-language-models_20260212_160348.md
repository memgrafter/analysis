---
ver: rpa2
title: Automatically Interpreting Millions of Features in Large Language Models
arxiv_id: '2410.13928'
source_url: https://arxiv.org/abs/2410.13928
tags:
- features
- feature
- examples
- interpretation
- interpretations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of automatically interpreting
  millions of features in sparse autoencoders (SAEs) of large language models. The
  authors develop an automated pipeline using LLMs to generate and evaluate natural
  language interpretations for SAE features.
---

# Automatically Interpreting Millions of Features in Large Language Models

## Quick Facts
- arXiv ID: 2410.13928
- Source URL: https://arxiv.org/abs/2410.13928
- Authors: Gonçalo Paulo; Alex Mallen; Caden Juang; Nora Belrose
- Reference count: 40
- Key outcome: Automated pipeline generates and evaluates natural language interpretations for millions of SAE features using LLM-based scoring methods that are 5-30× more efficient than prior approaches

## Executive Summary
This work addresses the challenge of automatically interpreting millions of features in sparse autoencoders (SAEs) of large language models. The authors develop an automated pipeline using LLMs to generate and evaluate natural language interpretations for SAE features. They introduce five new scoring techniques—detection, fuzzing, surprisal, embedding, and intervention scoring—that are cheaper than prior simulation-based methods. Fuzzing and detection achieve correlation with human evaluations of ~0.69 and ~0.59 respectively, while being 5-30× more efficient than simulation. The study finds SAE features significantly more interpretable than neurons (even when neurons are sparsified), with larger SAEs and later layers showing better interpretability.

## Method Summary
The method collects SAE activations from Gemma 2 9b and Llama 3.1 8b models over 10M tokens from the RedPajama-v2 dataset. For each feature, 40 activating examples (32 tokens each) are selected using stratified sampling from activation deciles and shown to Llama 3.1 70b Instruct to generate concise interpretations. The pipeline evaluates interpretations using five scoring techniques: detection (AUROC on activating vs non-activating examples), fuzzing (context retention after token masking), surprisal (likelihood ratio of activating vs non-activating examples), embedding (cosine similarity to activation embeddings), and intervention (KL-divergence from output distribution changes). The authors release millions of interpretations along with scoring data for research use.

## Key Results
- Fuzzing and detection scoring achieve correlation with human evaluations of ~0.69 and ~0.59 respectively
- SAE features are significantly more interpretable than neurons (even when neurons are sparsified)
- Larger SAEs and later layers show better interpretability scores
- Intervention scoring identifies "output features" whose effects are better explained by downstream impact rather than input correlations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAE features are more interpretable than neurons because they disentangle polysemantic activations into sparse, monosemantic features.
- Mechanism: The encoder in SAEs transforms dense activation vectors into a sparse higher-dimensional latent space where each feature activates on a narrower, more coherent set of contexts. The decoder reconstructs the original activations from these sparse features, minimizing reconstruction error while enforcing sparsity. This transformation separates concepts that would be conflated in individual neurons.
- Core assumption: The linear representation hypothesis holds - that human-interpretable concepts are encoded in linear combinations of neurons, and SAEs can learn these combinations effectively.
- Evidence anchors:
  - [abstract] "SAE features significantly more interpretable than neurons (even when neurons are sparsified)"
  - [section] "SAEs consist of two parts: an encoder that transforms activation vectors into a sparse, higher-dimensional latent space, and a decoder that projects the features back into the original space"
  - [corpus] Weak - corpus neighbors don't directly address the interpretability mechanism, though they mention SAE interpretation frameworks
- Break condition: If the underlying representations are not linearly separable or if the training objective fails to properly enforce sparsity and reconstruction accuracy.

### Mechanism 2
- Claim: Automated interpretability pipelines can generate and evaluate explanations at scale by leveraging LLMs to process millions of features.
- Mechanism: The pipeline collects activations over a broad dataset, generates interpretations by showing the LLM examples from different activation quantiles, and evaluates interpretations using multiple scoring techniques (detection, fuzzing, surprisal, embedding, intervention). This creates a feedback loop where poor interpretations can be filtered out efficiently.
- Core assumption: LLMs can recognize patterns in activating contexts and generate coherent natural language explanations that capture the feature's behavior.
- Evidence anchors:
  - [abstract] "We introduce five new scoring techniques... that are cheaper to run than the previous state of the art"
  - [section] "Our approach to generating interpretations follows Bricken et al. (2023) in showing the explainer model examples sampled from different quantiles"
  - [corpus] Weak - corpus neighbors discuss SAE interpretation but don't validate the specific LLM-based automation approach
- Break condition: If LLMs fail to capture the nuanced patterns in SAE activations or if the scoring methods don't correlate with human judgments of interpretability.

### Mechanism 3
- Claim: Intervention scoring captures a different dimension of interpretability by measuring causal effects rather than correlations.
- Mechanism: Intervention scoring evaluates features based on how intervening on them changes the model's output, measuring the KL-divergence between intervened and clean output distributions. This identifies "output features" whose effects are better explained by downstream impact rather than input correlations.
- Core assumption: Some features' behavior is better characterized by their causal influence on output rather than by patterns in input contexts.
- Evidence anchors:
  - [abstract] "One of these techniques, intervention scoring, evaluates the interpretability of the effects of intervening on a feature, which we find explains features that are not recalled by existing methods"
  - [section] "We define an output feature as a feature that causes a property of the model's output that can be easily explained in natural language"
  - [corpus] Weak - corpus neighbors don't address intervention-based interpretability
- Break condition: If intervention strength tuning is imprecise or if the scorer model cannot reliably predict the effects of interventions.

## Foundational Learning

- Concept: Sparse Autoencoders (SAEs) and their training objectives
  - Why needed here: Understanding how SAEs work is fundamental to interpreting their features and designing the automated pipeline
  - Quick check question: What are the two main components of an SAE and what are their respective roles?

- Concept: Feature activation sparsity and distribution analysis
  - Why needed here: The pipeline relies on analyzing activation distributions and sampling strategies to generate effective interpretations
  - Quick check question: Why does sampling from different activation quantiles matter for interpretation quality?

- Concept: Correlation vs causation in interpretability evaluation
  - Why needed here: Different scoring methods measure different aspects of interpretability, with intervention scoring capturing causal effects
  - Quick check question: What is the key difference between correlational and intervention-based scoring methods?

## Architecture Onboarding

- Component map:
  - Data collection: Gather SAE activations from models over broad datasets
  - Interpretation generation: Use LLM to generate natural language explanations from activation examples
  - Scoring pipeline: Apply multiple scoring techniques (detection, fuzzing, surprisal, embedding, intervention)
  - Evaluation framework: Compare scores against human judgments and analyze correlations
  - Output: Release interpretations and scoring data for research use

- Critical path:
  1. Collect SAE activations from target model
  2. Generate interpretations using LLM with stratified sampling of activating contexts
  3. Apply scoring methods to evaluate interpretation quality
  4. Filter and refine interpretations based on scores
  5. Release interpretations for downstream use

- Design tradeoffs:
  - Context length vs. example diversity: Shorter contexts allow more examples but may miss long-range features
  - Sampling strategy: Top examples give specificity but may miss broader patterns; stratified sampling captures distribution but may be too broad
  - Scoring method selection: Different methods have different costs and capture different aspects of interpretability

- Failure signatures:
  - Low correlation between different scoring methods suggests interpretation issues
  - Poor human correlation indicates the explanations don't capture meaningful patterns
  - Intervention scoring disagreements suggest causal vs correlational disconnect

- First 3 experiments:
  1. Test interpretation quality with different sampling strategies (top-only vs. stratified) on a small feature set
  2. Compare fuzzing vs. detection scores to understand context vs. token-level interpretability
  3. Evaluate intervention scoring on a subset of features to identify "output features"

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does intervention scoring perform compared to correlational scoring for features with complex downstream effects that are not easily explained by their input patterns?
- Basis in paper: [explicit] The paper introduces intervention scoring as a method to evaluate features whose behavior is better explained by their downstream effects rather than input correlations, finding a slight negative correlation between fuzzing score and intervention score.
- Why unresolved: The study provides initial evidence that some features are better explained by their effects on output rather than input patterns, but does not systematically characterize the types of features that benefit most from intervention scoring or quantify the trade-offs between correlational and intervention approaches.
- What evidence would resolve it: A comprehensive analysis comparing intervention and correlational scores across different feature categories (e.g., output features, input-correlated features, and ambiguous features) would clarify when each scoring method is most appropriate and reveal systematic patterns in feature interpretability.

### Open Question 2
- Question: What is the optimal balance between specificity and generality in feature interpretations when generating explanations from different quantiles of the activation distribution?
- Basis in paper: [explicit] The paper observes that interpretations based on top activating examples are more concise and specific but fail to capture the whole distribution, while interpretations based on stratified sampling from the entire activation distribution are too broad and fail to capture meaningful interpretations.
- Why unresolved: The study identifies a trade-off between specificity and generality in interpretations but does not provide guidelines for determining the optimal sampling strategy or characterize how this balance affects downstream applications of the interpretations.
- What evidence would resolve it: Systematic experiments varying the proportion of examples from different activation quantiles and measuring the resulting interpretation quality across multiple evaluation metrics would identify the optimal balance for different use cases.

### Open Question 3
- Question: How do different SAE architectures and hyperparameters affect the interpretability of their features, and can these relationships be predicted before training?
- Basis in paper: [explicit] The paper finds that SAEs with more features have higher interpretability scores and that residual stream SAEs have slightly better scores than MLP SAEs, but does not explore the full space of architectural choices or their interactions.
- Why unresolved: The study provides initial evidence that SAE size and location affect interpretability but does not systematically explore the impact of other architectural choices (e.g., activation functions, loss functions, sparsity levels) or develop predictive models for interpretability based on architectural parameters.
- What evidence would resolve it: A comprehensive study varying SAE architectures and hyperparameters while measuring interpretability would reveal which factors most strongly influence feature interpretability and enable prediction of interpretability before training.

## Limitations

- The evaluation framework relies heavily on LLM-based scoring methods without extensive independent human validation
- The study doesn't fully explore the effectiveness of neuron sparsification as a comparison baseline
- The causal interpretation of intervention scoring could be confounded by other factors

## Confidence

- SAE feature interpretability claims: Medium confidence (depends on LLM-based scoring correlations)
- SAE vs neurons interpretability: Medium-High confidence (supported by direct comparison experiments)
- Intervention scoring as distinct dimension: Medium confidence (identifies output features but causal interpretation uncertain)
- Scalability claims: High confidence for implementation, Medium for interpretation quality

## Next Checks

1. **Human validation sample**: Select 100 random features from the released interpretations and have independent human annotators rate their interpretability to verify the LLM-based scoring correlations.

2. **Intervention stability test**: Perform repeated intervention experiments on the same features to measure score consistency and determine if the identified "output features" are robust to different intervention strengths.

3. **Cross-model generalization**: Apply the automated interpretation pipeline to a different SAE architecture (different sparsity level or layer location) to test whether the observed interpretability patterns generalize beyond the specific models studied.