---
ver: rpa2
title: Revisiting MoE and Dense Speed-Accuracy Comparisons for LLM Training
arxiv_id: '2405.15052'
source_url: https://arxiv.org/abs/2405.15052
tags:
- dense
- step
- expert
- time
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the conventional practice of using FLOPs
  or activated parameters to measure MoE model complexity and proposes using step
  time instead, which accounts for communication overhead in sparse layers. The authors
  implement a 3D sharding strategy to optimize MoE training on modern accelerators
  and design MoE models with smaller dense backbones under Chinchilla's compute-optimal
  setting.
---

# Revisiting MoE and Dense Speed-Accuracy Comparisons for LLM Training

## Quick Facts
- arXiv ID: 2405.15052
- Source URL: https://arxiv.org/abs/2405.15052
- Reference count: 9
- Key outcome: MoE models consistently outperform dense models on speed-accuracy trade-off curves across three model scales, achieving up to 3.55× faster training while maintaining comparable or better performance

## Executive Summary
This paper challenges conventional MoE training practices by proposing step time as a more accurate complexity measure than FLOPs or activated parameters. The authors implement a 3D sharding strategy that effectively controls communication overhead in sparse layers while enabling efficient scaling of experts. Across three model scales (6.4B, 12.6B, 29.6B), MoE models consistently outperform dense models on speed-accuracy trade-offs, achieving up to 3.55× faster training while maintaining comparable or better performance on benchmarks including CoreEN, MMLU, and GSM8K.

## Method Summary
The authors implement a 3D sharding strategy that partitions MoE training across Data, Expert, and Model axes to optimize different types of parallelism while keeping dense-to-MoE step time increase within 20%. They design MoE models with smaller dense backbones under Chinchilla's compute-optimal setting of a 20:1 token-to-parameter ratio. The method involves training dense baseline models at each scale, then designing MoE models with varying expert numbers and layers, and finally evaluating performance on benchmark tasks including 9 0-shot and 2 1-shot English tasks, MMLU 5-shot, and GSM8K 8-shot across the three model scales.

## Key Results
- MoE models achieve up to 3.55× faster training than dense counterparts
- Step time measure captures communication overhead that FLOPs and activated parameters miss
- 3D sharding keeps dense-to-MoE step time increase within 20%
- Scaling number of experts monotonically improves performance without affecting train step time much

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Step time is a more accurate measure of model complexity than FLOPs or activated parameters for MoE models.
- Mechanism: FLOPs and activated parameters do not capture communication overhead from sparse routing, while step time includes all computation and communication costs.
- Core assumption: Communication overhead becomes significant as MoE sparsity grows and dominates total training time.
- Evidence anchors: Abstract states "FLOPs and activated parameters do not accurately measure the communication overhead in sparse layers"

### Mechanism 2
- Claim: Using Chinchilla's 20:1 token-to-parameter ratio provides a fair compute budget comparison between MoE and dense models.
- Mechanism: This ratio ensures MoE and dense models receive equivalent total compute budgets rather than identical training steps.
- Core assumption: The 20:1 ratio is compute-optimal for both dense and MoE architectures when training tokens are adjusted accordingly.
- Evidence anchors: Abstract mentions "Adopt the Chinchilla compute-optimal setting of a 20:1 token-to-parameter ratio"

### Mechanism 3
- Claim: 3D sharding strategy effectively controls communication overhead while enabling efficient scaling of experts.
- Mechanism: Partitioning across Data, Expert, and Model axes optimizes different types of parallelism while keeping dense-to-MoE step time increase within 20%.
- Core assumption: Modern accelerators can efficiently handle the communication patterns created by this 3D partitioning.
- Evidence anchors: Section states "the 3D sharding strategy keeps the dense-to-MoE step time increase within 20%"

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Understanding how MoE replaces dense FFN layers with sparse counterparts and the routing mechanism is fundamental to grasping the paper's contributions.
  - Quick check question: What are the two main components of an MoE layer and how do they interact during the forward pass?

- Concept: Compute-optimal scaling laws
  - Why needed here: The paper uses Chinchilla's token-to-parameter ratio to determine fair training budgets, so understanding compute-optimal scaling is essential.
  - Quick check question: What is the relationship between model size, training tokens, and compute budget according to Chinchilla's scaling laws?

- Concept: Parallelization strategies for large models
  - Why needed here: The 3D sharding method combines data, expert, and model parallelism, so understanding these concepts is crucial for the implementation details.
  - Quick check question: How do data parallelism, model parallelism, and expert parallelism differ in their communication patterns and hardware requirements?

## Architecture Onboarding

- Component map: Dense transformer backbone (LLaMA2-based) -> Sparse MoE layers replacing FFNs -> 3D sharding across Data, Expert, and Model axes -> Communication primitives (all2all, allreduce) -> Routing mechanism (Top-2 with load balancing loss)

- Critical path: Token input → Data parallelism sharding → Attention computation → Model parallelism sharding → MoE layer processing → Expert parallelism sharding → Communication (all2all/allreduce) → Expert and Data axis coordination → Output combination → Model axis synchronization

- Design tradeoffs: More experts vs. communication overhead; Smaller dense backbone vs. total parameter count; Expert capacity vs. load balancing efficiency; Sharding granularity vs. device memory constraints

- Failure signatures: High step time increase (>20%) indicates poor sharding configuration; Training instability suggests insufficient expert capacity or load balancing issues; Memory overflow indicates suboptimal mesh shape selection

- First 3 experiments: 1) Implement basic MoE layer with 2 experts and measure step time vs. dense baseline; 2) Add 3D sharding with varying expert counts (16, 64, 256) while monitoring step time; 3) Compare performance across different dense backbone sizes (1.2B, 1.6B, 4.8B) with fixed expert count

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal compute-optimal frontier for MoE models, and how does it differ from the Chinchilla frontier for dense models?
- Basis in paper: [explicit] The authors note that designing MoEs from smaller backbones gives better results than using the same scale dense backbone, suggesting that the equivalent dense scale of an MoE lies between its activated and total parameters.
- Why unresolved: The paper acknowledges that the full study on the compute-optimal frontier for MoE is left as future work, and current comparisons use the Chinchilla setting optimized for dense models.
- What evidence would resolve it: Systematic experiments varying both the dense backbone scale and training tokens for MoE models across multiple scales, then plotting the speed-accuracy trade-off curves to identify the optimal frontier.

### Open Question 2
- Question: How does the performance of MoE models change when scaling the number of experts beyond 256, and what are the practical limits?
- Basis in paper: [explicit] The authors note that existing work shows scaling beyond 256 experts gives diminished returns, but their 3D sharding method suggests scaling to the limit of device efficiency and model architecture.
- Why unresolved: The paper experiments with up to 256 experts but doesn't push beyond this limit to determine if the diminishing returns observation holds with their efficient implementation.
- What evidence would resolve it: Training MoE models with 512, 1024, or more experts using the 3D sharding method, then measuring performance gains and step time increases to identify the practical scaling limits.

### Open Question 3
- Question: How do different routing methods (Top-1, Top-K, expert-choice) compare in terms of performance and efficiency when using the 3D sharding implementation?
- Basis in paper: [explicit] The authors use Top-2 routing but mention other methods like Top-1 and expert-choice routing used in other work, without comparing them under their optimized implementation.
- Why unresolved: The paper only evaluates one routing method, leaving open whether alternative routing strategies could provide better speed-accuracy trade-offs with their 3D sharding approach.
- What evidence would resolve it: Implementing and comparing multiple routing methods (Top-1, Top-2, Top-K, expert-choice) using the same 3D sharding implementation and measuring their performance, efficiency, and load balancing characteristics.

## Limitations

- Communication overhead generalization remains uncertain across different hardware architectures and interconnect technologies
- Chinchilla ratio applicability to MoE architectures hasn't been validated as truly optimal for sparse models
- Benchmark representativeness may not capture full performance characteristics across diverse domains and languages

## Confidence

**High Confidence**: The claim that step time is a more comprehensive complexity measure than FLOPs or activated parameters for MoE models.

**Medium Confidence**: The assertion that 3D sharding effectively controls communication overhead while enabling efficient scaling.

**Medium Confidence**: The claim that Chinchilla's 20:1 ratio provides fair compute budget comparisons between MoE and dense models.

## Next Checks

**Validation Check 1**: Test the 3D sharding strategy across different hardware configurations (varying GPU counts, network bandwidth, and interconnect technologies) to verify that the 20% step time increase bound holds across diverse cluster setups.

**Validation Check 2**: Experiment with different token-to-parameter ratios (e.g., 15:1, 25:1) for MoE models to determine whether Chinchilla's 20:1 ratio remains optimal or if MoE architectures benefit from different scaling relationships.

**Validation Check 3**: Evaluate the MoE speed-accuracy advantages on a broader and more diverse set of benchmarks including multilingual tasks, long-context scenarios, specialized domain tasks, and different task formats beyond the standard configurations used in the paper.