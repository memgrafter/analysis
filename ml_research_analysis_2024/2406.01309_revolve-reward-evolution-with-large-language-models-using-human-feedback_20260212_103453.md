---
ver: rpa2
title: 'REvolve: Reward Evolution with Large Language Models using Human Feedback'
arxiv_id: '2406.01309'
source_url: https://arxiv.org/abs/2406.01309
tags:
- reward
- speed
- function
- revolve
- driving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REvolve, an evolutionary framework using
  large language models (LLMs) guided by human feedback to design reward functions
  for reinforcement learning. The approach evolves a population of reward functions
  via mutation, crossover, and selection, leveraging human feedback to guide the evolutionary
  search.
---

# REvolve: Reward Evolution with Large Language Models using Human Feedback

## Quick Facts
- arXiv ID: 2406.01309
- Source URL: https://arxiv.org/abs/2406.01309
- Authors: Rishi Hazra; Alkis Sygkounas; Andreas Persson; Amy Loutfi; Pedro Zuidberg Dos Martires
- Reference count: 40
- Primary result: Evolutionary framework using LLMs guided by human feedback to design reward functions outperforms state-of-the-art baselines across autonomous driving, humanoid locomotion, and dexterous manipulation tasks.

## Executive Summary
REvolve introduces an evolutionary framework that leverages large language models (LLMs) guided by human feedback to design reward functions for reinforcement learning. The approach evolves a population of reward functions using mutation, crossover, and selection, with human evaluators providing pairwise preferences that are converted to Elo ratings for fitness scoring. Experiments demonstrate that REvolve significantly outperforms iterative approaches like Eureka and expert-designed rewards, achieving higher fitness scores and better policy performance while eliminating the need for additional reward model training.

## Method Summary
REvolve is an evolutionary framework for reward function design that uses GPT-4 as intelligent genetic operators guided by human feedback. The method maintains a population of 16 reward functions, applies mutation and crossover operations to generate new candidates, and trains policies in simulation environments to collect human preferences through pairwise comparisons. These preferences are converted to Elo ratings serving as fitness scores, which guide the evolutionary search toward reward functions that generate human-preferred behaviors. The approach eliminates the need for additional reward model training while demonstrating strong generalizability across different task domains.

## Key Results
- REvolve achieves higher fitness scores than Eureka and expert-designed rewards across all tested environments
- The framework demonstrates successful generalization from autonomous driving to humanoid locomotion and dexterous manipulation tasks
- Human feedback effectively guides the evolutionary search, translating implicit human knowledge into explicit reward functions without requiring additional reward model training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Evolutionary search with LLMs as operators outperforms greedy search for reward function design.
- **Mechanism**: REvolve maintains a population of reward functions and applies mutation, crossover, and selection across generations. LLMs generate intelligent variations based on human feedback, preserving genetic diversity and avoiding premature convergence that occurs when only the best candidate is mutated (as in Eureka).
- **Core assumption**: Reward function design space is non-convex and benefits from population-based exploration rather than greedy exploitation.
- **Evidence anchors**:
  - [abstract] "evolutionary framework that uses LLMs for reward design in RL" and "considerably outperforms iterative frameworks like Eureka"
  - [section 1] "REvolve utilizes meta-heuristic optimization through EAs (De Jong, 2017) to search for the best candidates"
  - [corpus] "Reward Evolution with Graph-of-Thoughts: A Bi-Level Language Model Framework for Reinforcement Learning" suggests population-based approaches exist in literature
- **Break condition**: If the reward design space is actually convex or if human feedback becomes too noisy to guide meaningful population diversity

### Mechanism 2
- **Claim**: Human feedback directly mapped to fitness scores enables alignment of learned behaviors with human implicit knowledge.
- **Mechanism**: Human evaluators provide pairwise preferences on policy rollouts, which are converted to Elo ratings serving as fitness scores. This guides the evolutionary search toward reward functions that generate human-preferred behaviors without requiring explicit reward model training.
- **Core assumption**: Human pairwise preferences can effectively capture complex notions of "good" behavior that are hard to quantify explicitly.
- **Evidence anchors**:
  - [abstract] "human feedback to guide the evolution process, effectively translating implicit human knowledge into explicit reward functions"
  - [section 3.3] "humans can still evaluate learned behaviors, effectively acting as fitness functions" and "Elo rating system (Elo, 1978)"
  - [corpus] "Understanding Impact of Human Feedback via Influence Functions" indicates active research on human feedback in RL
- **Break condition**: If human evaluators become inconsistent or if the pairwise comparison format cannot capture the necessary behavioral nuances

### Mechanism 3
- **Claim**: LLMs as intelligent genetic operators generate more effective reward function variations than random perturbations.
- **Mechanism**: GPT-4 uses natural language feedback to intelligently modify reward components through mutation (adjusting single components) and crossover (combining components from two parents), leveraging commonsense priors to suggest improvements aligned with human feedback.
- **Core assumption**: LLMs possess sufficient commonsense understanding to generate meaningful reward function variations based on feedback.
- **Evidence anchors**:
  - [abstract] "LLMs, guided by human feedback, can be used to formulate reward functions that reflect human implicit knowledge"
  - [section 3.2] "we use GPT-4 as mutation and crossover operators with appropriate prompts" and "LLMs can leverage their commonsense priors to suggest changes that are intelligent"
  - [corpus] "REVOLVE: Optimizing AI Systems by Tracking Response Evolution in Textual Optimization" suggests LLMs can track and optimize response evolution
- **Break condition**: If LLM-generated variations become repetitive or if the model lacks sufficient domain knowledge for the specific task

## Foundational Learning

- **Concept**: Markov Decision Process (MDP) formalization
  - Why needed here: The paper formalizes reward design as an optimization problem over MDPs, requiring understanding of state, action, reward, and policy spaces
  - Quick check question: How does the reward design problem differ from standard RL when the reward function itself is unknown?

- **Concept**: Evolutionary Algorithms (EAs) and genetic programming
  - Why needed here: REvolve uses EAs with genetic operators (mutation, crossover, selection) to evolve reward functions, requiring understanding of population-based search
  - Quick check question: What distinguishes REvolve's evolutionary approach from traditional EAs that use random perturbations?

- **Concept**: Human-in-the-loop reinforcement learning
  - Why needed here: The framework relies on human feedback for fitness evaluation, requiring understanding of how human preferences integrate with RL training loops
  - Quick check question: How does the Elo rating system convert pairwise human preferences into fitness scores for the evolutionary algorithm?

## Architecture Onboarding

- **Component map**: GPT-4 (reward designer/operator) -> Human feedback interface -> RL training pipeline -> Evolutionary algorithm -> Fitness evaluation
- **Critical path**: Human feedback → GPT-4 mutation/crossover → Reward function generation → RL training → Policy evaluation → Elo scoring → Population update
- **Design tradeoffs**:
  - Population size vs. computational cost (16 individuals per generation chosen as balance)
  - Human feedback frequency vs. training efficiency (collected after each generation)
  - LLM quality vs. open-source accessibility (GPT-4 used despite closed-source limitations)
  - Interpretability of reward functions vs. potential performance gains from black-box approaches

- **Failure signatures**:
  - Population converges too quickly (premature convergence - increase mutation rate or diversity maintenance)
  - Human feedback becomes inconsistent (noisy fitness signals - implement feedback aggregation or quality checks)
  - LLM-generated variations become repetitive (model exhaustion - refresh prompts or adjust temperature)
  - RL training fails to converge (poor reward functions - increase population size or adjust selection pressure)

- **First 3 experiments**:
  1. Implement REvolve with synthetic human feedback (automated fitness function) on a simple MuJoCo task to verify evolutionary mechanics work
  2. Add human feedback interface and test with 2-3 human evaluators on a single generation to validate preference collection and Elo scoring
  3. Scale to full 7-generation run on the humanoid locomotion task, comparing against expert-designed rewards to establish baseline performance improvements

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on human feedback introduces variability that could affect reproducibility and scalability
- Computational cost of running multiple generations with human feedback and RL training remains unclear
- The paper does not extensively address how to handle inconsistent human preferences or scale the feedback collection process

## Confidence

- **High confidence**: The core evolutionary algorithm mechanics and the use of GPT-4 as genetic operators are well-specified and theoretically sound
- **Medium confidence**: The effectiveness of human feedback in guiding reward evolution, as results depend heavily on evaluator quality and consistency
- **Medium confidence**: Generalization claims across tasks, as the paper shows success on three specific domains but doesn't extensively test on diverse or novel environments

## Next Checks

1. **Noise sensitivity test**: Run REvolve with synthetic noisy human feedback to quantify how preference inconsistency affects evolutionary outcomes and determine robustness thresholds
2. **Baseline comparison refinement**: Implement Eureka and expert-designed rewards under identical conditions to verify the claimed performance improvements are not due to implementation differences
3. **Generalization stress test**: Apply REvolve to completely new task domains (e.g., robotic manipulation tasks not in the original paper) to validate cross-domain applicability beyond the three demonstrated environments