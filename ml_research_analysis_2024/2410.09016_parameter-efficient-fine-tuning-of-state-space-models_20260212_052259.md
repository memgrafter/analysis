---
ver: rpa2
title: Parameter-Efficient Fine-Tuning of State Space Models
arxiv_id: '2410.09016'
source_url: https://arxiv.org/abs/2410.09016
tags:
- lora
- state
- linear
- fine-tuning
- projection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks and improves parameter-efficient fine-tuning
  (PEFT) methods for State Space Models (SSMs) like Mamba. It finds that LoRA-based
  methods outperform other PEFT approaches on SSMs, but LoRA struggles when applied
  directly to SSM modules.
---

# Parameter-Efficient Fine-Tuning of State Space Models

## Quick Facts
- **arXiv ID:** 2410.09016
- **Source URL:** https://arxiv.org/abs/2410.09016
- **Reference count:** 40
- **Primary result:** SDT + LoRA achieves state-of-the-art performance on Mamba/Jamba across NLP/vision tasks

## Executive Summary
This paper benchmarks and improves parameter-efficient fine-tuning (PEFT) methods for State Space Models (SSMs) like Mamba. The authors find that LoRA-based methods outperform other PEFT approaches on SSMs, but LoRA struggles when applied directly to SSM modules. They propose Sparse Dimension Tuning (SDT), which selectively updates SSM channels and state dimensions based on parameter magnitude during a warmup phase, while applying LoRA to linear projections. Experiments show that combining SDT for SSM modules with LoRA for linear projections achieves state-of-the-art performance with strong theoretical guarantees.

## Method Summary
The paper introduces Sparse Dimension Tuning (SDT), a parameter-efficient fine-tuning method specifically designed for SSM-based models. SDT works by first running a warmup epoch to measure parameter changes, then categorizing dimensions as trainable or frozen based on magnitude. The method combines LoRA adapters for linear projection matrices (Win, Wout) with selective dimension tuning for SSM modules, creating an effective hybrid approach that maintains parameter efficiency while achieving strong performance.

## Key Results
- LoRA and its variants consistently outperform all other PEFT methods on SSM-based models
- LoRA is highly effective for linear projection matrices but less suitable for SSM modules
- Combining SDT for SSM modules with LoRA for linear projections achieves state-of-the-art performance
- Theoretical analysis provides convergence guarantees for the proposed approach

## Why This Works (Mechanism)

### Mechanism 1
LoRA and its variants outperform all other PEFT methods when applied to SSM-based models by modifying weight matrices through low-rank updates, reducing trainable parameters from D² to 2RD. This effectively captures essential parameter changes while maintaining model stability, assuming low-rank updates are sufficient to capture downstream task adaptation for SSM models.

### Mechanism 2
LoRA is highly effective for linear projection matrices but less suitable for SSM modules because linear projections control input-output mappings that can be efficiently approximated by low-rank updates, while SSM modules contain input-dependent parameters (∆, B, C) that require more complex updates, assuming the structure of SSM modules differs fundamentally from linear projections.

### Mechanism 3
Sparse Dimension Tuning (SDT) selectively updates SSM channels and state dimensions based on parameter magnitude during warmup by performing a warmup phase to measure parameter changes, then categorizing dimensions as trainable or frozen. This selective approach targets only essential parameters while maintaining efficiency, assuming parameter magnitude during warmup correlates with importance for downstream task adaptation.

## Foundational Learning

- **Concept:** State Space Models (SSMs) and their structure
  - Why needed here: Understanding SSM architecture is fundamental to grasping why standard PEFT methods fail and how SDT addresses these limitations
  - Quick check question: What are the three main components of an SSM module and their roles?

- **Concept:** Low-rank matrix approximation and LoRA
  - Why needed here: LoRA's effectiveness (and limitations) depends on understanding how low-rank updates can approximate weight matrix changes
  - Quick check question: How does LoRA reduce trainable parameters from D² to 2RD?

- **Concept:** Dimension selection and parameter pruning
  - Why needed here: SDT's core innovation relies on selecting which dimensions to update based on parameter magnitude
  - Quick check question: What criteria does SDT use to classify dimensions as trainable vs. frozen?

## Architecture Onboarding

- **Component map:** Input projection matrices (Win, Wout) -> SSM module (A, B, C, ∆) -> LoRA adapters (W↓, W↑) -> SDT dimension selector (warmup + magnitude-based selection)

- **Critical path:** Pretrained SSM model initialization → LoRA applied to linear projections (Win, Wout) → Warmup phase for SDT on SSM modules → Dimension selection based on parameter magnitude changes → Fine-tuning with selected parameters

- **Design tradeoffs:** SDT vs full fine-tuning (trades performance for parameter efficiency), LoRA rank selection (higher rank improves expressivity but increases parameters), Warmup duration (longer warmup improves selection but increases overhead)

- **Failure signatures:** Poor downstream performance despite training (inadequate LoRA rank or SDT dimension selection), Memory issues (LoRA matrix multiplications can be memory-intensive), Slow convergence (may require learning rate tuning or rank adjustment)

- **First 3 experiments:** 1) Apply LoRA to linear projections only on a small SSM model and measure performance vs full fine-tuning, 2) Implement SDT dimension selection on a single SSM layer and verify correct classification of trainable vs frozen dimensions, 3) Combine LoRA (linear projections) + SDT (SSM modules) on a medium-sized task and compare to LoRA-only baseline

## Open Questions the Paper Calls Out

### Open Question 1
Does the proposed Sparse Dimension Tuning (SDT) method generalize to other State Space Model (SSM) architectures beyond S4, S6, and S5? The paper states "Our theory can be extended to S5 (if needed) with similar results" and mentions applying SDT to Mamba-II despite its different state matrix constraints, but only provides theoretical guarantees and experimental validation for S4, S6, and S5 architectures.

### Open Question 2
What is the optimal strategy for channel and state dimension selection beyond the magnitude-based approach proposed in the paper? The paper acknowledges "Our approach, based on a warmup stage and parameter magnitude, might not be optimal" and suggests future research could "explore the impact of channel/state selection and improve dimension selection algorithms."

### Open Question 3
How does SDT perform on large-scale real-world applications compared to full fine-tuning, considering practical constraints like memory and computational efficiency? While the paper demonstrates SDT's effectiveness on various benchmark tasks and shows memory/runtime advantages over LoRA in controlled experiments, it does not address deployment in large-scale production scenarios.

## Limitations
- Focus on narrow set of SSM variants (Mamba and Jamba) without exploring full spectrum of SSM architectures
- Dimension selection criteria based solely on parameter magnitude during warmup may not capture all relevant aspects of parameter importance
- Theoretical analysis relies on assumptions about smoothness and boundedness of loss landscape that may not hold in practice

## Confidence

- **High Confidence:** LoRA's effectiveness on linear projection matrices is well-established and consistently demonstrated across experiments
- **Medium Confidence:** SDT algorithm's selective dimension tuning approach shows strong empirical results but depends heavily on warmup phase quality
- **Low Confidence:** Claim that SDT achieves "state-of-the-art" performance requires broader comparison with recent PEFT methods

## Next Checks
1. Apply SDT to a diverse set of SSM variants (e.g., structured state spaces with different recurrence patterns) to validate generalization beyond Mamba/Jamba architectures
2. Systematically vary the warmup duration (from 0.1 to 5 epochs) and evaluate how dimension selection quality and downstream performance change
3. Implement SDT with alternative dimension selection metrics (e.g., gradient magnitude, Fisher information) to test whether parameter magnitude during warmup is the most effective criterion