---
ver: rpa2
title: A Greedy Hierarchical Approach to Whole-Network Filter-Pruning in CNNs
arxiv_id: '2409.03777'
source_url: https://arxiv.org/abs/2409.03777
tags:
- pruning
- layer
- filter
- filters
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a two-level hierarchical approach for whole-network
  filter pruning in CNNs that is efficient and uses classification loss as the final
  criterion. The lower-level filter-pruning algorithm uses a sparse-approximation
  formulation based on linear approximation of filter weights, with two variants:
  an orthogonal matching pursuit-based greedy selection and a greedy backward pruning
  approach using a novel closed-form error criterion.'
---

# A Greedy Hierarchical Approach to Whole-Network Filter-Pruning in CNNs

## Quick Facts
- **arXiv ID**: 2409.03777
- **Source URL**: https://arxiv.org/abs/2409.03777
- **Reference count**: 18
- **Primary result**: Achieves 94% reduction in FLOPS without losing accuracy on CIFAR-10

## Executive Summary
This paper proposes a two-level hierarchical approach for whole-network filter pruning in CNNs that combines efficiency with classification loss as the final criterion. The method uses a sparse-approximation formulation based on linear approximation of filter weights, with two variants: an orthogonal matching pursuit-based greedy selection and a greedy backward pruning approach using a novel closed-form error criterion. The hierarchical backward greedy search (HBGS) and hierarchical backward greedy tree search (HBGTS) algorithms select layers for pruning based on relative reconstruction error or final classification error respectively. The approach achieves state-of-the-art results on ResNet18, ResNet32, ResNet56, VGG16, and ResNext101, with a 94% reduction in FLOPS without accuracy loss on CIFAR-10.

## Method Summary
The proposed method consists of two main components: a filter-pruning algorithm that uses sparse approximation to select filters within each layer, and a layer-selection algorithm that determines which layers to prune. The filter pruning is performed using either FP-OMP (forward greedy selection) or FP-Backward (backward elimination with closed-form error calculation). The layer selection is done through HBGS (layer-wise relative error) or HBGTS (final classification error). The hierarchical approach iteratively prunes filters and selects layers until the desired compression ratio is achieved, followed by fine-tuning for 300 epochs to recover accuracy.

## Key Results
- Achieves 94% reduction in FLOPS without losing accuracy on CIFAR-10
- Reduces RAM requirement for ResNext101 from 7.6 GB to 1.5 GB
- Outperforms state-of-the-art pruning methods on ResNet18, ResNet32, ResNet56, VGG16, and ResNext101
- Provides two orders of magnitude speedup over forward greedy OMP approach through FP-Backward

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The backward elimination strategy (FP-Backward) provides two orders of magnitude speedup over the forward greedy OMP approach by avoiding repeated fine-tuning and leveraging a closed-form error criterion.
- Mechanism: FP-Backward iteratively removes the filter whose removal causes the minimal increase in approximation error, using block matrix inversion to efficiently compute the error increase for each candidate filter.
- Core assumption: The increase in approximation error when removing a filter can be computed in closed form without re-computing the full regression solution, and the error increase for each filter can be compared directly.
- Evidence anchors:
  - [section] "The time complexity of the HBGS algorithm depends on the training set size N and the average number of filters per layer n. In many cases, when the time complexity of the filter pruning step (O(TCn⁴)) is substantially larger than the error computation step O(TCN), the complexity of the filter pruning algorithm becomes substantially larger than that of the fine-tuning algorithm on the training dataset."
  - [section] "Hence, assuming a constant number of loop executions (lines 6 - 17), the overall complexity of Algorithm 3 is O(n²), which is two orders of magnitude improvement over using algorithm 1."
- Break condition: The closed-form error calculation assumes the matrix ATA remains well-conditioned. If the filter weight matrix becomes ill-conditioned (e.g., highly correlated filters), the error estimates may become unreliable.

### Mechanism 2
- Claim: The hierarchical backward greedy search (HBGS) selects layers based on relative reconstruction error, which correlates with the actual impact on network performance.
- Mechanism: HBGS computes the relative reconstruction error of each layer's output using the training dataset, then greedily selects the layer with the smallest relative error to prune from, iteratively repeating until the desired pruning ratio is achieved.
- Core assumption: The relative reconstruction error of a layer's output is a good proxy for the impact of pruning that layer on the overall network performance.
- Evidence anchors:
  - [section] "However, the prediction error of a layer's output is not always indicative of the ultimate predictive performance of the overall network."
  - [section] "Hence, we use reconstruction error of filter outputs using input training dataset as the criteria."
- Break condition: If later layers in the network have highly nonlinear transformations that amplify small errors from earlier layers, the reconstruction error of intermediate layers may not accurately predict final performance degradation.

### Mechanism 3
- Claim: The hierarchical backward greedy tree search (HBGTS) directly optimizes classification error by computing the error in the final layer output for each candidate layer pruning, providing better accuracy than HBGS despite higher computational cost.
- Mechanism: HBGTS computes the error in the final classification layer output when each layer is pruned, using a clever data structure to avoid redundant computations across the network, and selects the layer that minimizes this final error.
- Core assumption: Directly minimizing the error in the final classification output leads to better overall accuracy than minimizing intermediate layer reconstruction errors.
- Evidence anchors:
  - [section] "A key idea here is to calculate the error in final layer output etj, when layer j∈{1,...,C} is pruned, for each input training example."
  - [abstract] "Our suite of algorithms outperforms state-of-the-art pruning methods on ResNet18, ResNet32, ResNet56, VGG16, and ResNext101."
- Break condition: The computational overhead of HBGTS may become prohibitive for very deep networks or large datasets, making it impractical despite its accuracy advantages.

## Foundational Learning

- Concept: Sparse approximation and orthogonal matching pursuit (OMP)
  - Why needed here: The filter pruning problem is formulated as finding a sparse linear combination of filters that approximates each output filter, which is a classic sparse approximation problem solvable by OMP.
  - Quick check question: Given a vector v and a matrix A, what does OMP do and what guarantees does it provide under the restricted isometry property?

- Concept: Block matrix inversion and recursive update formulas
  - Why needed here: FP-Backward uses block matrix inversion to efficiently update the error calculation when removing filters, avoiding full recomputation of the regression solution.
  - Quick check question: How does block matrix inversion allow efficient updates when removing a column from a matrix in a least-squares problem?

- Concept: Forward vs backward greedy selection
  - Why needed here: Understanding the tradeoff between forward selection (adding elements) and backward elimination (removing elements) is crucial for appreciating why FP-Backward is more efficient than FP-OMP in this context.
  - Quick check question: In what scenarios is backward elimination more efficient than forward selection, and why?

## Architecture Onboarding

- Component map:
  - FP-OMP: Filter pruning using orthogonal matching pursuit (forward greedy)
  - FP-Backward: Filter pruning using backward elimination with closed-form error
  - HBGS: Layer selection based on relative reconstruction error
  - HBGTS: Layer selection based on final classification error
  - HBGTS-B: HBGTS with FP-Backward for filter pruning
  - Weight compensation: 1x1 convolution layer to compensate for pruned filters

- Critical path:
  1. Pre-trained model loading
  2. Iterative pruning loop (HBGS/HBGTS)
  3. Filter pruning (FP-OMP or FP-Backward)
  4. Weight compensation
  5. Fine-tuning after each layer selection
  6. Output pruned model

- Design tradeoffs:
  - Accuracy vs speed: HBGTS provides better accuracy but is slower than HBGS
  - Uniform vs non-uniform pruning: Non-uniform pruning (HBGS/HBGTS) generally performs better than uniform pruning
  - Forward vs backward filter selection: FP-Backward is faster but may be slightly less accurate than FP-OMP

- Failure signatures:
  - Poor accuracy retention: May indicate inappropriate layer selection criteria or insufficient fine-tuning
  - Excessive runtime: Could suggest using HBGS instead of HBGTS, or increasing the number of filters pruned per iteration
  - Numerical instability: May occur if the filter weight matrix becomes ill-conditioned during backward elimination

- First 3 experiments:
  1. Test FP-Backward on a single layer with a small number of filters to verify the closed-form error calculation matches brute-force computation
  2. Compare HBGS vs HBGTS on a small network (e.g., ResNet18) to observe accuracy-speed tradeoff
  3. Validate weight compensation by pruning a layer and checking that the 1x1 convolution layer correctly reconstructs the output

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed hierarchical pruning approach be extended to transformer-based models, particularly for pruning the feed-forward network (FFN) layers?
- Basis in paper: [explicit] The authors mention in the conclusion that the method can in principle be applied to any linear approximation-based pruning technique, with one way being pruning filters in the feed-forward network (FFN) of transformer-based models.
- Why unresolved: The paper focuses on convolutional neural networks and does not provide details on how to adapt the approach for transformers or FFN layers.
- What evidence would resolve it: A demonstration of applying the HBGS/HBGTS framework to prune FFN layers in transformer models, with experimental results showing improved efficiency or accuracy compared to existing transformer pruning methods.

### Open Question 2
- Question: What is the theoretical limit of parameter reduction that can be achieved without significant loss in accuracy using the proposed pruning methods?
- Basis in paper: [inferred] The authors show impressive results with up to 98% parameter reduction on various models without losing accuracy, but do not explore the theoretical limits of this approach.
- Why unresolved: The paper does not provide a theoretical analysis of the maximum pruning ratio achievable before accuracy degradation becomes significant.
- What evidence would resolve it: A theoretical analysis or extensive experimental study pushing the pruning ratio to its limits across different architectures and datasets to identify the point of diminishing returns.

### Open Question 3
- Question: How does the proposed method perform on large-scale datasets like ImageNet compared to smaller datasets like CIFAR-10/100?
- Basis in paper: [explicit] The authors mention using Tiny-Imagenet in addition to CIFAR datasets, but do not provide results on the full ImageNet dataset.
- Why unresolved: The paper focuses on smaller image classification datasets and does not demonstrate the method's effectiveness on larger, more complex datasets commonly used in the field.
- What evidence would resolve it: Experimental results applying the HBGS/HBGTS pruning methods to large-scale models (e.g., ResNet50, EfficientNet) on ImageNet, comparing performance to state-of-the-art pruning techniques and the original models.

## Limitations
- The closed-form error criterion in FP-Backward assumes the filter weight matrix remains well-conditioned during iterative removal, which may not hold for highly correlated filters
- The computational overhead of HBGTS, while justified by accuracy gains, may become prohibitive for very deep networks or large datasets
- The method's effectiveness on datasets beyond CIFAR-10, CIFAR-100, and Tiny-Imagenet remains unverified

## Confidence
- **High confidence**: The core mechanism of using sparse approximation for filter selection (Mechanism 1) is well-established in the literature and the closed-form error calculation is mathematically sound
- **Medium confidence**: The layer selection criteria (HBGS and HBGTS) show strong empirical results, but the theoretical justification for why relative reconstruction error correlates with final performance could be strengthened
- **Medium confidence**: The claim of 94% FLOPs reduction without accuracy loss on CIFAR-10 is supported by experiments, but reproducibility depends on specific implementation details

## Next Checks
1. Verify the closed-form error calculation in FP-Backward against brute-force computation on a small test case with known ground truth
2. Implement a scaled-down version of HBGTS on ResNet18 to empirically measure the accuracy-speed tradeoff and identify computational bottlenecks
3. Test the pruning approach on a different architecture (e.g., MobileNet) and dataset (e.g., ImageNet) to assess generalizability beyond the reported experiments