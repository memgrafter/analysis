---
ver: rpa2
title: Online Zero-Shot Classification with CLIP
arxiv_id: '2408.13320'
source_url: https://arxiv.org/abs/2408.13320
tags:
- online
- learning
- vision
- zero-shot
- proxy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel online zero-shot transfer learning
  framework that improves upon CLIP's vanilla zero-shot performance by leveraging
  the distribution of streaming unlabeled data. The method, OnZeta, addresses the
  challenge of classifying images on-the-fly without storing their representations.
---

# Online Zero-Shot Classification with CLIP

## Quick Facts
- arXiv ID: 2408.13320
- Source URL: https://arxiv.org/abs/2408.13320
- Authors: Qi Qian; Juhua Hu
- Reference count: 38
- Key outcome: OnZeta achieves 78.94% accuracy on ImageNet with 3%+ improvement over baseline

## Executive Summary
This paper introduces OnZeta, a novel online zero-shot transfer learning framework that improves upon CLIP's vanilla zero-shot performance by leveraging streaming unlabeled data. The method addresses the challenge of classifying images on-the-fly without storing their representations, which is critical for memory-constrained scenarios. By combining online label learning with online proxy learning, OnZeta achieves an average improvement of over 3% across 14 benchmark datasets, demonstrating both effectiveness and efficiency.

## Method Summary
OnZeta consists of two key components: online label learning and online proxy learning. The online label learning component models the target data distribution by balancing class assignments using Lagrange dual variables, while the online proxy learning component refines class proxies in the vision space to mitigate the modality gap between images and text. Both components are theoretically guaranteed to converge and work together to provide robust predictions by combining text and vision space outputs. The method processes each image as it arrives, updating class assignments and vision proxies incrementally without storing image representations.

## Key Results
- Achieves 78.94% accuracy on ImageNet with 3%+ improvement over baseline
- Demonstrates strong performance across 14 diverse benchmark datasets
- Shows robustness to different vision encoders (ResNet-50, ViT variants) and text prompts
- Provides theoretical convergence guarantees for both online components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed online label learning balances class assignments by leveraging the distribution of streaming unlabeled data, improving prediction accuracy over vanilla zero-shot classification.
- Mechanism: The method introduces Lagrange dual variables to formulate an optimization problem that balances the assignment between different classes globally. As each image arrives, the algorithm computes a probability distribution over classes and updates dual variables to adjust class assignments based on the target data distribution.
- Core assumption: The distribution of the target data can be effectively captured through streaming unlabeled data without storing individual image representations.
- Evidence anchors:
  - [abstract] "The method, OnZeta, addresses the challenge of classifying images on-the-fly without storing their representations. It consists of two key components: online label learning, which models the target data distribution by balancing class assignments..."
  - [section] "To capture the distribution of all data, we develop an online label learning algorithm to balance the assignment between different classes globally."
- Break condition: If the streaming data does not represent the true target data distribution, the balancing mechanism may lead to incorrect class assignments.

### Mechanism 2
- Claim: The online proxy learning refines class proxies in the vision space to mitigate the modality gap between images and text, improving classification accuracy.
- Mechanism: The method learns class proxies directly in the vision space by optimizing a convex loss function using streaming images. The vision proxy is updated incrementally with each arriving image, and a combination strategy trades off bias from text space and variance from vision space.
- Core assumption: Learning class proxies in the vision space is more effective for visual classification than relying solely on text proxies.
- Evidence anchors:
  - [abstract] "...and online proxy learning, which refines class proxies in the vision space to mitigate the modality gap."
  - [section] "Given the whole set of images, vision proxy learning can be cast as an optimization problem... When solving the problem in an online manner, only a single example will be received at each iteration."
- Break condition: If the streaming images are not representative or if the learning rate is not properly tuned, the vision proxy may not converge to an effective solution.

### Mechanism 3
- Claim: The combination of online label learning and online proxy learning reduces variance in the target vision space by leveraging biased predictions from the text space.
- Mechanism: The method combines pseudo labels from both text and vision spaces using a trade-off ratio that adjusts based on the training dynamics. This combination helps stabilize predictions when the vision proxy is not fully trained.
- Core assumption: Combining predictions from text and vision spaces can reduce overall prediction variance compared to using either space alone.
- Evidence anchors:
  - [abstract] "By combining the predicted label from the online label learning and proxy learning, our online zero-shot transfer method (OnZeta) achieves 78.94% accuracy on ImageNet..."
  - [section] "Therefore, these predictions can be mixed to trade-off bias from text space and variance from vision space... We find that the bias can be considered as a constant but the variance will be reduced with the learning of w..."
- Break condition: If the combination ratio is not properly tuned or if one space becomes significantly more reliable than the other, the combination may not provide the intended variance reduction.

## Foundational Learning

- Concept: Online Learning
  - Why needed here: The scenario requires classifying each image as it arrives without storing representations, necessitating incremental updates to the model.
  - Quick check question: How does online learning differ from batch learning in terms of data access and model updates?

- Concept: Zero-Shot Transfer
  - Why needed here: The method aims to classify images into categories without any labeled examples from the target task, relying on pre-trained vision-language models.
  - Quick check question: What is the main advantage of zero-shot transfer over traditional supervised learning?

- Concept: Modality Gap
  - Why needed here: The vision and text representations come from different modalities, and the method aims to mitigate the gap between them for better classification.
  - Quick check question: Why might representations from vision and text modalities differ, and how can this gap affect classification performance?

## Architecture Onboarding

- Component map: Vision encoder -> Text encoder -> Online label learning module -> Online proxy learning module -> Combination strategy module
- Critical path: 1. Receive streaming image 2. Extract vision representation 3. Compute class probabilities using text proxies 4. Update class assignments using online label learning 5. Update vision proxies using online proxy learning 6. Combine predictions from text and vision spaces 7. Output final classification
- Design tradeoffs:
  - Memory efficiency vs. accuracy: Storing image representations could improve accuracy but violates the online constraint.
  - Learning rate vs. convergence: Higher learning rates may lead to faster updates but could cause instability.
  - Combination ratio vs. robustness: The ratio for combining text and vision predictions affects the balance between bias and variance.
- Failure signatures:
  - Accuracy plateaus early: May indicate insufficient learning rate or poor convergence.
  - High variance in predictions: Could suggest the vision proxy is not well-trained or the combination strategy is ineffective.
  - Class imbalance in assignments: Might indicate the online label learning is not properly balancing class distributions.
- First 3 experiments:
  1. Test the online label learning component in isolation on a small dataset to verify class balancing.
  2. Evaluate the online proxy learning with a fixed set of images to check convergence behavior.
  3. Combine both components and test on a benchmark dataset to assess overall performance improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of OnZeta scale with increasingly larger vision encoders beyond ViT-L/14@336, such as when using newer architectures like ConvNeXt or with further scaling up the model size?
- Basis in paper: [explicit] The paper mentions that OnZeta achieves more gains with a large backbone and shows its potential for large models, but does not explore beyond ViT-L/14@336.
- Why unresolved: The experiments are limited to CLIP's vision encoders, and the paper does not investigate the upper limits of performance improvement with larger or alternative architectures.
- What evidence would resolve it: Testing OnZeta with larger vision encoders or different architectures like ConvNeXt, and comparing the performance gains to those observed with ViT-L/14@336.

### Open Question 2
- Question: How robust is OnZeta to different random orders of image arrival, and what is the variance in performance across multiple trials with different image sequences?
- Basis in paper: [explicit] The paper mentions that the images arrive in a random order and the performance is robust to the order of input images, but it does not provide a detailed analysis of performance variance across different random sequences.
- Why unresolved: While the paper reports averaged results over 5 trials, it does not delve into the variance or robustness analysis across different random image sequences.
- What evidence would resolve it: Conducting extensive experiments with multiple random sequences and reporting the variance in performance to assess the robustness of OnZeta to different image arrival orders.

### Open Question 3
- Question: Can OnZeta be effectively extended to handle multimodal inputs or tasks that require integrating additional types of side information beyond text prompts and unlabeled images?
- Basis in paper: [inferred] The paper focuses on leveraging text prompts and streaming unlabeled images for online zero-shot transfer, but does not explore the integration of other modalities or additional side information.
- Why unresolved: The current framework is designed for vision-language models and does not address the potential benefits of incorporating other modalities or side information types.
- What evidence would resolve it: Developing and testing extensions of OnZeta that incorporate additional modalities or side information, and evaluating the impact on performance across various tasks.

## Limitations
- The method's effectiveness on more diverse or challenging domains beyond the 14 tested benchmark datasets remains to be validated.
- The relative importance and interaction effects between online label learning and online proxy learning components are not fully characterized.
- The method requires careful tuning of hyperparameters (learning rates, combination ratios) for optimal performance.

## Confidence

- High confidence in the theoretical framework and convergence guarantees
- Medium confidence in the empirical performance improvements
- Medium confidence in the practical utility for streaming data scenarios

## Next Checks

1. Conduct extensive ablation studies varying the combination ratio between text and vision predictions across different datasets to identify optimal settings and understand failure modes.
2. Test the method on datasets with highly imbalanced class distributions or significant domain shift to evaluate robustness under challenging conditions.
3. Implement a streaming variant where images arrive in random order (rather than dataset order) to validate the method's effectiveness in truly online scenarios.