---
ver: rpa2
title: Rubric-based Learner Modelling via Noisy Gates Bayesian Networks for Computational
  Thinking Skills Assessment
arxiv_id: '2408.01221'
source_url: https://arxiv.org/abs/2408.01221
tags:
- skills
- assessment
- competence
- learner
- skill
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a Bayesian network-based learner modelling approach
  that improves upon prior work by incorporating constraints on skill ordering and
  supplementary skills into the model. The authors use noisy-OR gates to simplify
  parameter elicitation and inference.
---

# Rubric-based Learner Modelling via Noisy Gates Bayesian Networks for Computational Thinking Skills Assessment

## Quick Facts
- arXiv ID: 2408.01221
- Source URL: https://arxiv.org/abs/2408.01221
- Reference count: 31
- Primary result: Enhanced Bayesian network model with skill ordering constraints and supplementary skills provides more accurate computational thinking assessment than baseline

## Executive Summary
This paper presents a Bayesian network-based learner modeling approach for assessing computational thinking skills that addresses limitations in prior work by incorporating constraints on skill ordering and supplementary skills. The authors use noisy-OR gates to simplify parameter elicitation and inference while maintaining model expressiveness. Applied to the Cross Array Task with 109 K-12 pupils, the enhanced model demonstrates improved accuracy and provides more detailed assessments compared to baseline approaches, with strong correlation between model-based scores and expert assessments.

## Method Summary
The method extends a Bayesian network with noisy-OR gates to model computational thinking skills assessment, incorporating dummy nodes to enforce skill ordering constraints and a two-layer gate structure for supplementary skills. The model takes as input task performance data and an assessment rubric, then computes posterior probabilities for target skills. Parameter elicitation involves defining inhibition parameters for skill-task pairs, with tasks grouped by difficulty level. The approach is implemented using the CREMA Java library and validated on the Cross Array Task dataset.

## Key Results
- Enhanced model with skill ordering constraints improves assessment accuracy over baseline BN approach
- Two-layer gate structure effectively models both disjunctive (OR) and conjunctive (AND) relationships between skills
- Model-based CAT scores show high correlation with expert assessments while providing detailed skill-level insights
- The approach remains computationally efficient and interpretable for practical use in intelligent tutoring systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model improves upon prior work by strictly enforcing the ordering of competence levels using dummy observed nodes.
- Mechanism: Auxiliary variables (Dik) are introduced for each implied skill relation, enforcing Xi → Xk by making P(Dik = 1 | Xi = 1, Xk = 0) = 0. This updates prior probabilities so that higher-level skills can only be present if all lower-level skills are also present.
- Core assumption: The assessment rubric encodes a partial order of competence levels that must be preserved in the learner model.
- Evidence anchors:
  - [abstract] "...we address issue (i) by introducing dummy observed nodes, strictly enforcing the skills ordering without changing the network's structure."
  - [section II-C.1] "To solve this issue, we enrich the model by adding an auxiliary variable Dik for each relation Xi =⇒ Xk defined by the rubric."
  - [corpus] Weak: corpus contains related works but none directly address skill ordering enforcement in BN-based learner models.
- Break condition: If the rubric does not encode a strict partial order, or if the dummy nodes introduce inconsistencies with observed data.

### Mechanism 2
- Claim: Supplementary skills are modeled using a two-layer gate structure that combines disjunctive (OR) and conjunctive (AND) logic.
- Mechanism: Group nodes Gi aggregate interchangeable supplementary skills via noisy-OR gates; these group nodes then connect to answer nodes through logical AND gates, allowing both disjunctive and conjunctive relationships between target and supplementary skills.
- Core assumption: Some tasks require multiple supplementary skills to be present simultaneously (AND) while other skills within a group are interchangeable (OR).
- Evidence anchors:
  - [abstract] "...we design a network with two layers of gates, one performing disjunctive operations by noisy-OR gates and the other conjunctive operations through logical ANDs."
  - [section II-C.2] "Supplementary skills are described by additional skill nodes S1, ..., Sm... combined with a logic function to allow for the inclusion of a suitable set of supplementary skills."
  - [corpus] Weak: corpus neighbors focus on assessment and Bayesian networks but do not explicitly discuss two-layer gate structures.
- Break condition: If the task structure does not require both AND and OR relationships, or if the supplementary skills are not clearly grouped.

### Mechanism 3
- Claim: Noisy-OR gates reduce parameter elicitation and inference complexity from exponential to linear in the number of parent skills.
- Mechanism: Instead of specifying a full CPT for each answer node given all combinations of parent skills, noisy-OR gates introduce inhibition parameters λi,j representing the probability that skill i is not expressed in task j, reducing the number of parameters to be elicited.
- Core assumption: Skills are interchangeable in contributing to an answer, and failure to express a skill is independent of other skills' expressions.
- Evidence anchors:
  - [section II-A.1] "To reduce the number of parameters, the noisy-OR defines the state of Yj as the logical disjunction (OR) of the auxiliary parent nodes, removing the need to specify the CPT of the answer node given the state of its parent nodes."
  - [section II-A.1] "In [15], a leak node was added to the model to represent the possibility of a random guess."
  - [corpus] Weak: corpus includes related works on Bayesian networks but does not explicitly discuss noisy-OR gates in the context of learner modeling.
- Break condition: If skills are not interchangeable or if their failures are correlated, the noisy-OR assumption breaks down.

## Foundational Learning

- Concept: Bayesian Networks (BNs)
  - Why needed here: BNs provide the probabilistic framework to model dependencies between latent skills and observable behaviors in learner assessment.
  - Quick check question: What is the Markov condition in a Bayesian network, and how does it relate to the factorization of the joint probability distribution?

- Concept: Noisy-OR gates
  - Why needed here: Noisy-OR gates simplify the modeling of disjunctive skill expressions and reduce the number of parameters that need to be elicited from experts.
  - Quick check question: How does a noisy-OR gate differ from a deterministic OR gate in terms of modeling skill expression?

- Concept: Assessment Rubrics
  - Why needed here: Rubrics define the competence components and levels to be assessed, which are then translated into the BN model structure.
  - Quick check question: How does an assessment rubric encode the ordering of competence levels, and why is this ordering important for the learner model?

## Architecture Onboarding

- Component map:
  - Skill nodes: Latent binary variables representing learner competencies
  - Answer nodes: Observable binary variables representing task performance
  - Dummy observed nodes (Dik): Enforce skill ordering constraints
  - Group nodes (Gi): Aggregate supplementary skills via noisy-OR
  - Leakage node: Models random guessing

- Critical path:
  1. Define skill and answer nodes from the assessment rubric
  2. Add dummy nodes to enforce skill ordering
  3. Group supplementary skills and connect via noisy-OR to group nodes
  4. Connect group nodes to answer nodes via AND gates
  5. Elicit inhibition parameters for skill-answer pairs
  6. Perform inference to update skill probabilities based on observed answers

- Design tradeoffs:
  - Simplicity vs. expressiveness: Noisy-OR gates simplify the model but assume independence of skill failures
  - Constraint enforcement vs. flexibility: Dummy nodes enforce ordering but may reduce model flexibility if the rubric's implied order is not strict

- Failure signatures:
  - Inconsistent posterior probabilities: May indicate incorrect inhibition parameters or violation of model assumptions
  - Slow inference: May indicate too many nodes or complex CPTs; consider simplifying the model or using approximate inference

- First 3 experiments:
  1. Implement a simple BN model with only target skills and no supplementary skills or constraints. Compare posterior probabilities with expert assessments.
  2. Add dummy nodes to enforce skill ordering and re-evaluate the model's performance.
  3. Introduce supplementary skills and the two-layer gate structure. Compare the model's ability to explain task failures with and without supplementary skills.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's performance change when using more granular inhibition parameters for each skill-task pair, rather than grouping tasks by difficulty levels?
- Basis in paper: [explicit] The paper mentions that the enhanced model (Model E) uses different values of inhibition parameters to describe different task complexities, but this was done by grouping tasks into eight categories of increasing difficulty. It suggests that further refinements could be introduced by removing some of the assumptions.
- Why unresolved: The paper does not explore the performance impact of using more granular inhibition parameters for each skill-task pair individually.
- What evidence would resolve it: Experimental results comparing the performance of the enhanced model with more granular inhibition parameters to the current model with grouped tasks.

### Open Question 2
- Question: How does the model's accuracy and interpretability change when applied to different domains beyond computational thinking and programming?
- Basis in paper: [explicit] The paper discusses the potential for applying the approach to other educational contexts and teachers without a technical background, but does not provide evidence for domains beyond computational thinking.
- Why unresolved: The model's performance and interpretability in other domains are not evaluated or discussed in the paper.
- What evidence would resolve it: Experimental results applying the model to assessment rubrics and tasks from different educational domains, along with an analysis of the model's accuracy and interpretability in those contexts.

### Open Question 3
- Question: How does the model's performance change when using data-driven parameter elicitation instead of expert-based elicitation?
- Basis in paper: [explicit] The paper mentions that ongoing work explores the possibility of improving the model by updating its parameters based on evidence collected during user interaction, suggesting a data-driven approach.
- Why unresolved: The paper does not compare the performance of the model with expert-based and data-driven parameter elicitation.
- What evidence would resolve it: Experimental results comparing the performance of the model with parameters elicited by experts to the performance with parameters learned from data collected during user interactions.

## Limitations
- The model is evaluated on a single computational thinking task, limiting generalizability to other domains
- Expert parameter elicitation remains a potential bottleneck despite noisy-OR gate simplification
- The model's performance under violation of its key assumptions (independence of skill failures, strict skill ordering) is not thoroughly explored

## Confidence

- **High confidence**: The mechanism of using dummy nodes to enforce skill ordering and the use of noisy-OR gates to reduce parameter complexity are well-supported by the paper's evidence and are consistent with established practices in Bayesian network modeling.
- **Medium confidence**: The two-layer gate structure for supplementary skills is described in the paper but lacks extensive validation or comparison with alternative approaches.
- **Low confidence**: The generalizability of the model to other assessment tasks and the impact of model assumptions on assessment accuracy are not thoroughly explored in the paper.

## Next Checks
1. Apply the model to a different computational thinking task or subject area and compare its performance with the Cross Array Task. Assess the model's ability to generalize across tasks.
2. Perform a sensitivity analysis to determine how violations of the noisy-OR assumption and skill ordering constraints affect the model's assessment accuracy. Identify the conditions under which the model is most robust.
3. Conduct a study comparing the model's assessments with those of multiple human experts on the same dataset. Analyze the sources of disagreement and potential improvements to the model or expert rubric.