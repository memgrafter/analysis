---
ver: rpa2
title: A Provably Efficient Option-Based Algorithm for both High-Level and Low-Level
  Learning
arxiv_id: '2406.15124'
source_url: https://arxiv.org/abs/2406.15124
tags:
- regret
- learning
- high-level
- options
- low-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning both high-level and
  low-level policies in hierarchical reinforcement learning from scratch, without
  pre-trained options. The authors propose a meta-algorithm called High-Level/Low-level
  Meta-Learning (HLML) that alternates between learning the high-level policy (selecting
  options) using a novel algorithm called Options-UCBVI and learning the low-level
  policies (inner to the options) using UCBVI.
---

# A Provably Efficient Option-Based Algorithm for both High-Level and Low-Level Learning

## Quick Facts
- arXiv ID: 2406.15124
- Source URL: https://arxiv.org/abs/2406.15124
- Reference count: 40
- Proposes HLML, a meta-algorithm that alternately learns high-level and low-level policies in hierarchical RL without pre-trained options

## Executive Summary
This paper addresses the challenge of learning both high-level and low-level policies in hierarchical reinforcement learning from scratch. The authors propose HLML (High-Level/Low-level Meta-Learning), an algorithm that alternates between learning option selection policies and option execution policies. The key innovation is handling the non-stationarity that arises from simultaneously learning at both levels by dividing the learning process into multiple phases with alternating updates. Theoretical analysis shows that HLML achieves sublinear regret bounds and identifies conditions under which hierarchical approaches provably outperform flat reinforcement learning.

## Method Summary
HLML is a meta-algorithm that alternates between two phases: (1) learning high-level option selection using Options-UCBVI while keeping low-level policies fixed, and (2) learning low-level option policies using UCBVI while keeping the high-level policy fixed. The algorithm uses a doubling schedule for phase lengths (K_H^n = K_L^n = ⌊2^(n-1)⌋) to ensure sufficient learning time at each level. Options-UCBVI extends UCBVI to finite-horizon semi-MDPs by introducing a backward-forward mechanism that handles the random duration of options. The theoretical analysis provides regret bounds of O(CLH√SOKd + CHHO√OSAKHO) where CL and CH are concentrability coefficients.

## Key Results
- HLML achieves regret bounds of O(CLH√SOKd + CHHO√OSAKHO) for jointly learning high-level and low-level policies
- The regret bound is compared with the lower bound for non-hierarchical problems to characterize when HRL is provably preferable
- The algorithm handles the non-stationarity in joint optimization by alternating between levels in phases rather than keeping one fixed throughout
- Theoretical analysis shows that hierarchical approaches are beneficial when concentrability coefficients are small

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alternating phases between high-level and low-level learning mitigates non-stationarity while enabling joint policy optimization.
- Mechanism: The meta-algorithm alternates between learning high-level policies with fixed low-level policies (Options-UCBVI) and learning low-level policies with fixed high-level policies (UCBVI). Each phase converges to a locally optimal policy given the fixed counterpart, and alternating allows both levels to gradually improve toward global optimality.
- Core assumption: The structural assumption (Assumption 4.1) ensures that local optimal low-level policies are consistent with global optimal policies, preventing the high-level policy from locking onto suboptimal options.
- Evidence anchors:
  - [abstract] "The key idea involves dividing the learning process of the two levels into multiple phases, rather than just two, and consistently switching between them by keeping one level fixed while the other is learning."
  - [section 4] "This induces a bias in both optimizations. To make this clear, we provide a convenient decomposition of the regret, which highlights the contributions of the two phases of learning in each stage."
- Break condition: If Assumption 4.1 is violated, the low-level learning process could converge to policies that are not part of any optimal joint policy, causing the high-level policy to learn incorrectly and the algorithm to fail.

### Mechanism 2
- Claim: Options-UCBVI extends UCBVI to handle the random duration of temporally extended actions in FH-SMDPs.
- Mechanism: The backward-forward mechanism computes optimistic value functions by propagating values not only to the previous state-step pair but to any reachable state-step pair where an option could be selected, accounting for the stochastic termination times of options.
- Core assumption: The options are admissible (Assumption 2.1), ensuring that whenever an option terminates, there is always another option that can start from that state-stage pair.
- Evidence anchors:
  - [section 3] "To address this issue, first, an estimate of the transition model is computed solely with the data collected from the SMDP, generating an estimate of a multi-step dynamic, thereby ignoring the primitive state-action pairs visited during option execution."
  - [section 3] "To handle this problem, we introduce a backward-forward mechanism presented in lines 7-15."
- Break condition: If options are not admissible or have very long durations that make d approach H, the advantage of temporal abstraction diminishes and the regret bound approaches that of flat algorithms.

### Mechanism 3
- Claim: Concentrability coefficients quantify the distribution shift cost when keeping one level fixed during learning.
- Mechanism: The concentrability coefficients (CH and CL) bound the bias introduced by not playing optimal policies at the other level. They measure how much the state visitation distributions under the current policy differ from those under the optimal policy.
- Core assumption: All state-stage pairs are visited with non-zero probability under any policy, ensuring finite concentrability coefficients.
- Evidence anchors:
  - [section 4] "Lemma 4.2. Let us define the concentrability coefficients: CH := max n∈ [N] inf µ∗ optimal max (s,h)∈S× [H] dµ∗ s1,1(s, h ) dµn s1,1(s, h ) ."
  - [section 4] "Then, it holds that: V ∗ ∗ (s1, 1) − V ∗ πn− 1 (s1, 1) ≤ CH ( V µn ∗ (s1, 1) − V µn πn− 1 (s1, 1) )."
- Break condition: If some state-stage pairs have very low probability under the current policies but high probability under optimal policies, the concentrability coefficients become very large, potentially negating the benefits of the hierarchical approach.

## Foundational Learning

- Concept: Finite-Horizon Semi-Markov Decision Processes (FH-SMDPs)
  - Why needed here: The high-level problem operates over options (temporally extended actions) which naturally form an SMDP where transitions depend on both the current state and the duration of option execution.
  - Quick check question: In an FH-SMDP, how does the transition model differ from a standard MDP transition model?

- Concept: Upper Confidence Bounds for Value Iteration (UCBVI)
  - Why needed here: Both the low-level (UCBVI) and high-level (Options-UCBVI) learning components are based on UCBVI's principle of optimism in the face of uncertainty, which is crucial for achieving sublinear regret bounds.
  - Quick check question: What is the main difference between the exploration bonus in standard UCBVI and Options-UCBVI?

- Concept: Regret minimization and concentrability coefficients
  - Why needed here: The theoretical analysis requires decomposing regret into terms involving the regret of the individual algorithms and bias terms that depend on concentrability coefficients, which quantify the distribution shift when policies are kept fixed.
  - Quick check question: How do concentrability coefficients relate to the distribution shift between current and optimal policies?

## Architecture Onboarding

- Component map:
  - Options-UCBVI (Algorithm 1) -> learns SMDP policy over options
  -> UCBVI (Algorithm 2) -> learns primitive action policies for each option
  -> HLML (Algorithm 2) -> orchestrates alternating phases between high and low levels
  -> Transition model estimates, visit counts for state-option-stage triples, optimistic value functions

- Critical path:
  1. Initialize random high-level and low-level policies
  2. Run Options-UCBVI for K H n episodes with fixed low-level policies
  3. Sample a high-level policy from the sequence generated by Options-UCBVI
  4. Run UCBVI for K L n episodes with fixed high-level policy
  5. Sample low-level policies from the sequence generated by UCBVI
  6. Repeat until K total episodes are reached

- Design tradeoffs:
  - Fixed options vs learned options: The framework assumes a fixed set of options with known initiation sets and termination functions but unknown policies, trading off flexibility for theoretical tractability
  - Phase length doubling: The exponential schedule for phase lengths ensures that both levels get sufficient learning time while maintaining sublinear regret
  - Concentrability coefficients: While necessary for the analysis, large concentrability coefficients could indicate that the algorithm is inefficient when the optimal and current policies have very different state visitation distributions

- Failure signatures:
  - If regret grows linearly instead of sublinearly, check whether the phase length schedule is being followed correctly
  - If low-level learning consistently produces poor policies, verify that Assumption 4.1 holds for the problem structure
  - If concentrability coefficients become very large, examine whether the problem has states that are rarely visited under current policies but frequently under optimal policies

- First 3 experiments:
  1. Test on a simple grid world with options corresponding to navigating to specific locations - verify that the algorithm learns to use options correctly and achieves lower regret than flat UCBVI
  2. Test on a taxi problem where options correspond to driving, picking up, and dropping off - check that the algorithm can learn the hierarchical structure and verify Assumption 4.1 holds
  3. Test on a problem where Assumption 4.1 is violated - observe how the algorithm performs and whether the failure mode matches theoretical predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions on the MDP structure do the concentrability coefficients CH and CL become negligible, making the hierarchical approach clearly preferable to flat RL?
- Basis in paper: [explicit] The authors note that the advantage of HRL emerges when CH and CL are negligible, but do not characterize when this occurs.
- Why unresolved: The paper establishes that CH and CL can diminish the advantage of HRL, but does not provide concrete conditions on the MDP structure that guarantee their negligibility.
- What evidence would resolve it: Formal analysis showing when state-action visitation distributions under optimal policies are similar to those under learned policies, possibly through bounds on the minimum state-action visitation probabilities or structural properties of the MDP.

### Open Question 2
- Question: Can the alternation strategy between high-level and low-level learning be optimized beyond the simple doubling schedule used in HLML?
- Basis in paper: [explicit] The authors use a doubling schedule for the number of episodes at each level but acknowledge this is a design choice that could be improved.
- Why unresolved: While the doubling schedule achieves theoretical guarantees, it may not be optimal in practice, and the paper does not explore alternative scheduling strategies.
- What evidence would resolve it: Empirical studies comparing different scheduling strategies (e.g., adaptive schedules based on learning progress) and theoretical analysis of their regret bounds.

### Open Question 3
- Question: How does the performance of HLML compare to other hierarchical RL approaches that use pre-trained options or different hierarchical structures?
- Basis in paper: [inferred] The paper focuses on the scenario where no pre-trained options are available, but does not compare HLML to methods that assume pre-trained options or use different hierarchical structures.
- Why unresolved: The paper establishes that HLML can learn both levels from scratch, but does not benchmark it against other hierarchical RL approaches.
- What evidence would resolve it: Empirical comparisons of HLML with other hierarchical RL algorithms on standard benchmark tasks, including those that use pre-trained options or different hierarchical structures.

## Limitations

- The theoretical guarantees rely heavily on Assumption 4.1, which requires that locally optimal low-level policies are consistent with globally optimal joint policies, a condition that may not hold in many practical scenarios
- Concentrability coefficients CH and CL can grow arbitrarily large when state visitation distributions under current policies differ significantly from those under optimal policies, potentially negating the benefits of the hierarchical approach
- The paper does not address how to construct admissible options that satisfy Assumption 2.1, which is necessary for faithful reproduction of the theoretical guarantees

## Confidence

**High Confidence:**
- The mechanism by which alternating phases between high and low-level learning mitigates non-stationarity
- The backward-forward mechanism in Options-UCBVI for handling random option durations
- The role of concentrability coefficients in quantifying distribution shift costs

**Medium Confidence:**
- The tightness of the regret bound O(CLH√SOKd + CHHO√OSAKHO) compared to practical performance
- The practical significance of the conditions under which hierarchical approaches are provably preferable

**Low Confidence:**
- The practical applicability of Assumption 4.1 in real-world hierarchical RL problems
- The effectiveness of the algorithm when concentrability coefficients become large

## Next Checks

1. **Assumption Validation**: Create a suite of test environments with varying degrees of compliance with Assumption 4.1 and measure how performance degrades as the assumption becomes violated. This will quantify the practical limitations of the theoretical framework.

2. **Concentrability Analysis**: Compute the concentrability coefficients for benchmark hierarchical RL problems and compare them against flat MDP counterparts. Determine whether the hierarchical approach provides meaningful improvements when CH and CL are bounded versus when they grow large.

3. **Option Construction Study**: Develop methods for automatically constructing admissible options that satisfy Assumption 2.1 for common benchmark problems, and evaluate whether the theoretical advantages translate to practical performance gains with automatically generated options.