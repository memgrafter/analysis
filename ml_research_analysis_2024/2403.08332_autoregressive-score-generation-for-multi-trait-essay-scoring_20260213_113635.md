---
ver: rpa2
title: Autoregressive Score Generation for Multi-trait Essay Scoring
arxiv_id: '2403.08332'
source_url: https://arxiv.org/abs/2403.08332
tags:
- trait
- scoring
- traits
- essay
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes ArTS, an autoregressive multi-trait essay scoring
  method using a pre-trained T5 model. Instead of predicting each trait score independently,
  ArTS generates a sequence of trait scores autoregressively, allowing subsequent
  trait predictions to benefit from preceding ones.
---

# Autoregressive Score Generation for Multi-trait Essay Scoring

## Quick Facts
- arXiv ID: 2403.08332
- Source URL: https://arxiv.org/abs/2403.08332
- Reference count: 8
- ArTS achieves over 5% average improvements in both prompt and trait-level QWK scores compared to baseline multi-task learning models

## Executive Summary
This paper introduces ArTS, an autoregressive multi-trait essay scoring method that uses a pre-trained T5 model to generate sequences of trait scores. Instead of predicting each trait score independently, ArTS generates trait scores autoregressively, allowing subsequent trait predictions to benefit from preceding ones. The method demonstrates significant improvements in scoring quality while using a single model for all traits and prompts, achieving training efficiency by avoiding redundant modules.

## Method Summary
ArTS fine-tunes the T5-Base encoder-decoder model to generate sequences of trait scores autoregressively. The input format includes the essay text prefixed with "score the essay of the prompt N:", and the output is a text sequence of trait names and scores. The model is trained using five-fold cross-validation on the ASAP and ASAP++ datasets, with QWK as the evaluation metric. The autoregressive generation order predicts rare traits first and comprehensive traits last, leveraging trait dependencies through causal self-attention in the decoder.

## Key Results
- ArTS achieves over 5% average improvements in both prompt and trait-level QWK scores compared to baseline multi-task learning models
- The method demonstrates training efficiency by using a single model for multi-trait scoring across all prompts
- ArTS generalizes well to other datasets and benefits from larger model sizes, particularly in low-resource settings

## Why This Works (Mechanism)

### Mechanism 1
The autoregressive generation order (from rare to frequent traits) improves scoring quality by leveraging trait dependencies. By predicting less common, more independent traits first, and more comprehensive, dependent traits later, the model can condition subsequent predictions on already-generated trait scores. The causal self-attention in the decoder allows later trait predictions to attend to earlier ones. This assumes trait scores are not independent and later traits (like Overall) are influenced by earlier ones.

### Mechanism 2
Using a single model for all traits and prompts is more efficient than duplicating trait-specific models. The T5 model is fine-tuned to generate a sequence of trait scores in one forward pass, avoiding the need to train and run multiple separate models for each trait. This assumes a single model can capture the shared and task-specific features needed for all traits without loss of accuracy.

### Mechanism 3
Providing the prompt number as a prefix helps the model disambiguate trait score ranges and scoring criteria. Including "score the essay of the prompt N:" as input allows the model to condition its predictions on the specific prompt context, ensuring trait scores fall within the correct range for that prompt. This assumes different prompts have different score ranges and rubric interpretations for the same trait names.

## Foundational Learning

- **Encoder-decoder transformer architectures (e.g., T5)**: ArTS uses T5's encoder-decoder structure to generate sequences of trait scores autoregressively, unlike encoder-only BERT models. Quick check: What is the key difference between encoder-only and encoder-decoder transformer models?

- **Causal self-attention in the decoder**: Allows the model to attend only to previous tokens in the sequence, enabling autoregressive trait score generation where later traits condition on earlier ones. Quick check: How does causal masking in the decoder prevent information leakage from future tokens?

- **Quadratic Weighted Kappa (QWK) as evaluation metric**: Measures agreement between model predictions and human scores, accounting for the distance between rating categories; it's the official metric for the ASAP dataset. Quick check: Why is QWK preferred over simple accuracy for ordinal score prediction tasks?

## Architecture Onboarding

- **Component map**: Essay text + prompt number prefix -> T5 encoder -> T5 decoder with causal attention -> Text sequence of trait scores -> Extract numeric scores by trait name

- **Critical path**: 1) Tokenize input (essay + prompt prefix) 2) Pass through T5 encoder 3) Generate sequence autoregressively via T5 decoder 4) Parse output text to extract trait scores 5) Compute QWK per prompt, average across folds

- **Design tradeoffs**: Single model vs. multiple trait-specific models (simpler, more efficient, but may lose trait-specific optimizations); Autoregressive generation vs. parallel prediction (captures dependencies but slower at inference); T5-base vs. larger models (good balance of performance and efficiency; larger models help in low-resource settings)

- **Failure signatures**: Out-of-range predictions (often due to missing prompt context); Degraded performance on rare traits (insufficient training data even with autoregressive benefits); Lower Overall score (model balances all traits rather than focusing on Overall like baselines)

- **First 3 experiments**: 1) Ablation: Remove prompt prefix to test its impact on out-of-range predictions and QWK 2) Ordering: Reverse trait prediction order to see if dependent traits benefit more when predicted earlier 3) Scale: Try T5-small and T5-large to assess impact of model size on low-resource trait performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the ArTS model perform on extremely low-resource traits (e.g., traits with fewer than 500 labeled examples)? The paper mentions that the Voice trait with 723 samples showed some performance degradation, but it does not explore traits with fewer than 723 examples or test the limits of ArTS's performance in extremely low-resource scenarios.

### Open Question 2
What is the optimal trait prediction order for ArTS, and how does it impact performance? The paper discusses the trait prediction order, stating that predicting general traits later in the sequence improves results. However, it does not explore alternative ordering strategies or their impact on performance.

### Open Question 3
How does ArTS compare to other pre-trained models (e.g., GPT, RoBERTa) for multi-trait essay scoring? The paper mentions that ArTS outperforms the baseline model and ArTS-Llama2, but it does not compare ArTS to other pre-trained models.

## Limitations

- The method's reliance on specific trait ordering (least to most data) may not generalize to datasets with different trait structures or scoring rubrics
- The autoregressive ordering assumes that rare traits influence common ones, but this relationship isn't empirically validated beyond observed performance improvements
- The paper doesn't address potential bias in the ordering strategy or explore alternative dependency structures that might better capture trait relationships

## Confidence

- **High Confidence**: The claim that ArTS achieves 5% average QWK improvements over MTL baselines is well-supported by experimental results across multiple datasets and metrics
- **Medium Confidence**: The mechanism explanation that autoregressive ordering improves scores through trait dependency modeling is plausible but not definitively proven
- **Low Confidence**: The generalization claims to other datasets beyond ASAP/ASAP++ are based on limited experiments with the Feedback Prize dataset

## Next Checks

1. **Dependency Structure Validation**: Conduct controlled experiments that systematically vary trait prediction order (random, frequency-based, dependency-based) while holding all other factors constant to isolate the impact of autoregressive ordering on performance

2. **Cross-dataset Robustness**: Apply ArTS to at least three additional essay scoring datasets with different trait structures, prompt formats, and score ranges to test the method's generalization beyond the ASAP family of datasets

3. **Ablation on Prompt Context**: Systematically remove the prompt prefix in controlled experiments to quantify its contribution to out-of-range prediction reduction versus its impact on overall scoring accuracy, particularly for traits with overlapping score ranges across prompts