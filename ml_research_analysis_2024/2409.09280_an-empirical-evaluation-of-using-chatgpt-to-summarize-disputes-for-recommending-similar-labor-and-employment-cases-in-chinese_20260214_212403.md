---
ver: rpa2
title: An empirical evaluation of using ChatGPT to summarize disputes for recommending
  similar labor and employment cases in Chinese
arxiv_id: '2409.09280'
source_url: https://arxiv.org/abs/2409.09280
tags:
- disputes
- cases
- similar
- were
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a hybrid approach combining clustering and
  convolutional neural networks to recommend similar labor and employment cases in
  Chinese. The method uses itemized disputes between litigants, either manually prepared
  or generated by GPT-3.5/GPT-4, as input features.
---

# An empirical evaluation of using ChatGPT to summarize disputes for recommending similar labor and employment cases in Chinese

## Quick Facts
- **arXiv ID:** 2409.09280
- **Source URL:** https://arxiv.org/abs/2409.09280
- **Reference count:** 28
- **Primary result:** Hybrid approach combining clustering and CNN classification using GPT-generated disputes achieves competitive performance for recommending similar labor cases

## Executive Summary
This study introduces a hybrid approach combining clustering and convolutional neural networks to recommend similar labor and employment cases in Chinese. The method uses itemized disputes between litigants, either manually prepared or generated by GPT-3.5/GPT-4, as input features. Disputes are vectorized using Sentence-BERT (Lawformer or RoBERTa) and transformed into grey-level images for CNN classification. Experiments show that using Lawformer embeddings with fine-tuning improves results over standard RoBERTa, and GPT-4-generated disputes yield better performance than GPT-3.5. While GPT-generated disputes slightly underperform manually prepared ones, the gap is small, suggesting practical viability.

## Method Summary
The method processes Chinese labor and employment cases by first extracting dispute statements between litigants. These disputes are vectorized using Sentence-BERT models (Lawformer or RoBERTa), which were fine-tuned on legal sentence pairs from clustered data. Cosine similarity matrices are computed between dispute vectors, then transformed into 32√ó32 grayscale images. A CNN with two convolution and max-pooling layers classifies these images as indicating similar or not-similar cases. The approach was evaluated using 2,288 labeled case pairs, with 30 random resplits for training, validation, and testing.

## Key Results
- The hybrid approach combining clustering and CNN classification outperforms prior clustering-only methods
- GPT-4-generated disputes achieve better classification performance than GPT-3.5, with the gap shrinking compared to previous results
- Fine-tuned Lawformer embeddings improve classification accuracy over standard RoBERTa embeddings
- GPT-generated disputes achieve only slightly lower performance than manually prepared disputes, suggesting practical viability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using Sentence-BERT embeddings with fine-tuning improves classification performance over standard embeddings.
- **Mechanism:** Fine-tuning Sentence-BERT on domain-specific legal sentence pairs ("same" vs "diff" clusters) adapts the model to capture legal semantics more precisely, improving the quality of dispute similarity vectors used in CNN classification.
- **Core assumption:** Legal disputes contain domain-specific semantic nuances that are not captured by general embeddings but can be learned from clustered legal sentences.
- **Evidence anchors:**
  - [abstract] "Using Lawformer embeddings with fine-tuning improves results over standard RoBERTa"
  - [section] "We fine-tuned the Lawformer and RoBERTa along with the Sentence-BERT... created a database of 3875115 'diff' pairs and 716850 'same' pairs"
  - [corpus] No direct corpus evidence found; core evidence from experiment design and abstract.

### Mechanism 2
- **Claim:** GPT-4 generates more effective dispute summaries than GPT-3.5 for case similarity tasks.
- **Mechanism:** GPT-4 produces more accurate and complete itemizations of disputes by better understanding legal context and relationships between claims and counterclaims, leading to higher quality input features for the classifier.
- **Core assumption:** The quality of generated disputes directly influences downstream classification performance.
- **Evidence anchors:**
  - [abstract] "GPT-4-generated disputes yield better performance than GPT-3.5"
  - [section] "GPT-4 was better than GPT-3.5 in the final results" and "the gap shrank when we changed from GPT-3.5 to GPT-4"
  - [corpus] No direct corpus evidence; relies on internal experiment results.

### Mechanism 3
- **Claim:** Converting dispute similarity matrices into grayscale images enables effective CNN-based similarity classification.
- **Mechanism:** By projecting cosine similarities into a visual format, the CNN can leverage spatial feature extraction to identify patterns indicating case similarity, which may be less apparent in raw numerical vectors.
- **Core assumption:** Similarity patterns between disputes exhibit spatial regularity that CNNs can detect when represented as images.
- **Evidence anchors:**
  - [section] "We projected a value of ùëßùëßùëöùëö,ùë¶ùë¶ ùëñùëñ,ùëóùëó in ùëçùëçùëñùëñ,ùëóùëó to the range [0, 255]... creating a squared grey-level image"
  - [section] "The reordering operations... allow us to figure out the clusters that included more dispute statements, thereby creating contextual information"
  - [corpus] No corpus evidence; mechanism derived from method description.

## Foundational Learning

- **Concept:** Cosine similarity for measuring semantic similarity between dispute embeddings
  - **Why needed here:** Used to quantify how closely two disputes align semantically, forming the core input for similarity matrices.
  - **Quick check question:** What is the range of cosine similarity values, and what does a value of 1 signify?

- **Concept:** Sentence-BERT and its fine-tuning process
  - **Why needed here:** Provides sentence-level embeddings tailored to legal text after fine-tuning, improving downstream classification accuracy.
  - **Quick check question:** Why might fine-tuning Sentence-BERT on legal sentence pairs outperform using a general pre-trained model?

- **Concept:** Convolutional neural networks for image-based classification
  - **Why needed here:** Enables spatial pattern recognition in grayscale images derived from dispute similarity matrices, allowing automated similarity judgments.
  - **Quick check question:** How does a CNN's convolution operation help detect features in an image that may indicate similarity?

## Architecture Onboarding

- **Component map:** Data preprocessing ‚Üí Sentence embedding (Lawformer/RoBERTa + fine-tuning) ‚Üí Cosine similarity matrix ‚Üí Grayscale image conversion ‚Üí CNN classification ‚Üí Output (similar/ not similar)
- **Critical path:** Embedding quality ‚Üí Similarity matrix accuracy ‚Üí Image transformation ‚Üí CNN model performance
- **Design tradeoffs:**
  - Fine-tuning embeddings improves domain relevance but requires additional labeled data and computation.
  - Image representation simplifies CNN input but may lose some numerical precision.
  - Using GPT for dispute generation expands dataset size but may introduce noise or inconsistencies.
- **Failure signatures:**
  - Low F1/accuracy scores ‚Üí Possible issues in embeddings, similarity calculation, or CNN architecture.
  - GPT-generated disputes underperform ‚Üí Likely prompt quality or context length limitations.
  - Overfitting on training data ‚Üí Need regularization or more diverse training samples.
- **First 3 experiments:**
  1. Compare classification performance using standard RoBERTa embeddings vs. fine-tuned Lawformer embeddings on the same dataset.
  2. Evaluate GPT-3.5 vs. GPT-4 generated disputes by measuring F1 score and macro-recall ROUGE scores.
  3. Test different CNN architectures (e.g., varying filter sizes or layer depths) on the grayscale image representations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do ROUGE scores compare between GPT-3.5 and GPT-4 generated disputes when evaluated against court-prepared dispute references?
- **Basis in paper:** [explicit] The paper explicitly mentions that reviewers recommended calculating ROUGE scores for LLM-generated disputes using court-prepared disputes as references, and provides macro-averaged ROUGE scores for GPT-3.5 and GPT-4.
- **Why unresolved:** While macro-averaged ROUGE scores are provided, the paper doesn't conduct a detailed analysis of how these scores correlate with the classification performance or which ROUGE variant (R-1, R-2, R-L) best predicts classification success.
- **What evidence would resolve it:** A detailed statistical analysis correlating ROUGE scores with classification accuracy, and an analysis of which ROUGE metric (precision, recall, or F1) best predicts classification performance.

### Open Question 2
- **Question:** Does fine-tuning Lawformer with Taiwanese legal documents improve classification performance compared to using the pre-trained Chinese version?
- **Basis in paper:** [explicit] The paper states that Lawformer was pre-trained on simplified Chinese legal documents from China, and the authors fine-tuned it with Taiwanese legal documents written in traditional Chinese, but notes they were "surprised that the results of using the Lawformer did not necessarily outperform those of using the Chinese RoBERTa."
- **Why unresolved:** The paper only reports comparative results between Lawformer and RoBERTa, but doesn't isolate the effect of fine-tuning Lawformer specifically with Taiwanese legal documents versus using the pre-trained version.
- **What evidence would resolve it:** Controlled experiments comparing classification performance using: 1) pre-trained Lawformer, 2) Lawformer fine-tuned with Taiwanese legal documents, and 3) RoBERTa with and without fine-tuning.

### Open Question 3
- **Question:** How does the choice of clustering algorithm (HDBSCAN vs. previous method) affect the quality of similarity recommendations?
- **Basis in paper:** [explicit] The paper mentions switching from a previous clustering method to HDBSCAN, stating "we switched to using the HDBSCAN of scikit-learn for clustering" but doesn't provide a direct comparison of results between the two methods.
- **Why unresolved:** While the paper claims the hybrid approach outperformed their previous recommender, it doesn't isolate whether improvements came from the clustering method change or from the CNN classification component.
- **What evidence would resolve it:** Comparative experiments using the same CNN classifier but with disputes clustered using both the previous method and HDBSCAN, measuring classification performance and recommendation quality for each.

## Limitations
- Evaluation limited to Taiwanese labor and employment cases, limiting generalizability to other legal domains
- CNN-based approach requires image transformation that may introduce information loss compared to direct numerical processing
- GPT-generated disputes still underperform manually prepared ones, though the gap is small

## Confidence

**High confidence:** The hybrid approach combining clustering and CNN classification outperforms prior methods; GPT-4 consistently outperforms GPT-3.5; fine-tuned Lawformer embeddings improve over standard RoBERTa

**Medium confidence:** GPT-generated disputes perform nearly as well as manually prepared ones; the image transformation approach effectively captures similarity patterns

**Low confidence:** The specific prompt templates for GPT generation would yield optimal results in other contexts; the approach generalizes beyond Chinese labor cases

## Next Checks

1. Test the approach on a different legal domain (e.g., criminal or contract law) and a different language to assess generalizability
2. Systematically vary GPT prompt engineering parameters (temperature, context window, specific instructions) to identify optimal settings
3. Compare the image-based CNN approach against direct numerical similarity processing using the same embeddings and similarity matrices