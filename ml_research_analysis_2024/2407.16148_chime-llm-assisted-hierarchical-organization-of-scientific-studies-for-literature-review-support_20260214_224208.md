---
ver: rpa2
title: 'CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature
  Review Support'
arxiv_id: '2407.16148'
source_url: https://arxiv.org/abs/2407.16148
tags:
- categories
- claim
- category
- hierarchies
- studies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work explores using LLMs to generate hierarchical organizations
  of scientific studies to assist researchers with literature review. The proposed
  LLM-based pipeline decomposes hierarchy generation into three steps: compressing
  study findings into claims, generating root categories, and completing the hierarchy
  with claim assignment.'
---

# CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support

## Quick Facts
- arXiv ID: 2407.16148
- Source URL: https://arxiv.org/abs/2407.16148
- Authors: Chao-Chun Hsu; Erin Bransom; Jenna Sparks; Bailey Kuehl; Chenhao Tan; David Wadden; Lucy Lu Wang; Aakanksha Naik
- Reference count: 40
- Dataset contains 2,174 LLM-generated hierarchies covering 472 topics, with expert-corrected hierarchies for 100 topics

## Executive Summary
This work introduces CHIME, a novel approach using large language models (LLMs) to generate hierarchical organizations of scientific studies to assist researchers with literature reviews. The system decomposes the complex task of hierarchy generation into three manageable steps: compressing study findings into claims, generating root categories, and completing the hierarchy through claim assignment. The authors address LLM performance limitations by collecting expert-curated corrections through a systematic three-step process, resulting in a substantial dataset that captures both LLM-generated and human-corrected hierarchies. This dual approach enables the training of corrector models that significantly improve the quality of automated literature organization.

## Method Summary
The proposed LLM-based pipeline tackles hierarchical organization through a decomposition strategy that breaks down the complex task into three sequential steps: first compressing individual study findings into concise claims, then generating appropriate root categories for organizing these claims, and finally completing the full hierarchy through systematic claim assignment. To evaluate and improve LLM performance, the authors collected CHIME, an expert-curated dataset where domain specialists correct errors in LLM-generated hierarchies through a three-step correction process focusing on parent-child category links, sibling category coherence, and claim categorization. This human-in-the-loop approach enables the training of corrector models that learn from expert feedback, achieving a 12.6 F1 point improvement in study assignment accuracy.

## Key Results
- LLM excels at generating and organizing categories but achieves only 61.5% F1 score on study assignment accuracy
- Training corrector models with human feedback improves study assignment by 12.6 F1 points
- CHIME dataset contains 2,174 LLM-generated hierarchies covering 472 topics, with expert-corrected hierarchies for 100 topics
- The three-step correction process (parent-child links, sibling coherence, claim categorization) effectively identifies and resolves hierarchy errors

## Why This Works (Mechanism)
The approach works by decomposing a complex hierarchical organization task into simpler, sequential subtasks that LLMs can handle more reliably. By first extracting claims from studies, then organizing those claims into categories, and finally assigning studies to the appropriate categories, the system leverages LLM strengths in pattern recognition and text organization while minimizing the cognitive load of handling the entire hierarchy simultaneously. The human-in-the-loop correction mechanism provides high-quality training data that enables the system to learn from expert domain knowledge, creating a feedback loop that progressively improves automated performance.

## Foundational Learning
- **Hierarchical decomposition**: Breaking complex tasks into simpler subtasks is essential because LLMs perform better on focused, specific prompts rather than attempting to generate entire structures at once. Quick check: Verify that each decomposition step produces coherent outputs independently.
- **Claim extraction**: Converting study findings into concise claims provides a common representation that enables systematic comparison and organization across diverse research. Quick check: Ensure extracted claims capture the core contribution of each study without introducing bias.
- **Expert correction protocols**: Structured human feedback is necessary because raw LLM outputs contain systematic errors that require domain expertise to identify and correct. Quick check: Validate that correction steps comprehensively cover all types of hierarchy errors.
- **Corrector model training**: Learning from expert-corrected hierarchies enables the system to generalize beyond the specific examples seen during correction. Quick check: Test corrector performance on topics not included in the training data.

## Architecture Onboarding

Component map: Study Extraction -> Claim Generation -> Root Category Generation -> Hierarchy Completion -> Expert Correction -> Corrector Training

Critical path: The system follows a sequential pipeline where each component depends on the successful completion of previous steps. Studies are first processed into claims, which then inform category generation, leading to hierarchy completion. The expert correction phase provides feedback that trains corrector models, which can be applied to new topics.

Design tradeoffs: The decomposition approach trades off some contextual understanding for improved reliability, as handling each step separately may miss cross-cutting relationships that would be apparent in a holistic analysis. The human-in-the-loop correction provides high-quality data but limits scalability.

Failure signatures: Common failure modes include overly broad or narrow categories, misclassification of studies due to ambiguous claims, and failure to capture interdisciplinary connections. The system may also struggle with rapidly evolving fields where category structures are still developing.

First experiments:
1. Generate hierarchies for well-established research areas with clear category structures to establish baseline performance
2. Test the correction pipeline on hierarchies with known systematic errors to validate the three-step correction process
3. Evaluate the corrector model's ability to generalize from expert-corrected examples to new, unseen topics

## Open Questions the Paper Calls Out
The paper acknowledges several important open questions regarding the generalizability of LLM-generated hierarchies beyond the specific domains covered in CHIME, the reliability of fully automated approaches given the 61.5% F1 score for study assignment, and the scalability of the human-in-the-loop correction approach for larger literature review tasks.

## Limitations
- Generalizability remains uncertain as the dataset represents specific domains that may not capture the full diversity of scientific literature
- The 61.5% F1 score for study assignment indicates significant room for improvement in automated study categorization
- The three-step correction process relies heavily on expert judgment, raising questions about subjectivity and scalability
- The hierarchical organization approach may not be optimal for interdisciplinary research requiring different organizational structures

## Confidence

High confidence:
- Dataset collection methodology and three-step pipeline design are well-documented and reproducible
- Performance metrics and improvement from human feedback are clearly presented with appropriate statistical support

Medium confidence:
- Generalizability of findings to broader scientific domains
- Scalability of human-in-the-loop correction approach for larger literature review tasks

Low confidence:
- Assumption that expert-corrected hierarchies represent absolute ground truth
- Potential bias introduced by specific domains and topics included in CHIME

## Next Checks
1. Test the pipeline on interdisciplinary research topics spanning multiple domains to evaluate cross-domain generalization
2. Conduct inter-rater reliability studies with multiple experts correcting the same LLM-generated hierarchies to quantify subjectivity in the correction process
3. Evaluate the system's performance on literature reviews requiring synthesis across different types of studies to test handling of heterogeneous research outputs