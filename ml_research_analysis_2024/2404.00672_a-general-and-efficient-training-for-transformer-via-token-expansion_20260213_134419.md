---
ver: rpa2
title: A General and Efficient Training for Transformer via Token Expansion
arxiv_id: '2404.00672'
source_url: https://arxiv.org/abs/2404.00672
tags:
- training
- token
- tokens
- transformer
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel token growth scheme called Token Expansion
  (ToE) to achieve consistent training acceleration for Vision Transformers (ViTs).
  The core idea is to maintain the integrity of the intermediate feature distribution
  during the accelerated training process by introducing an "initialization-expansion-merging"
  pipeline.
---

# A General and Efficient Training for Transformer via Token Expansion

## Quick Facts
- arXiv ID: 2404.00672
- Source URL: https://arxiv.org/abs/2404.00672
- Authors: Wenxuan Huang; Yunhang Shen; Jiao Xie; Baochang Zhang; Gaoqi He; Ke Li; Xing Sun; Shaohui Lin
- Reference count: 40
- Key outcome: Proposes Token Expansion (ToE) method that accelerates ViT training by 1.3× with negligible accuracy drop or even improvement

## Executive Summary
This paper introduces Token Expansion (ToE), a novel training acceleration method for Vision Transformers that maintains feature distribution integrity while progressively reducing token count. The method employs an "initialization-expansion-merging" pipeline that starts with a small token set, grows it during training, and merges close tokens to avoid information loss. Extensive experiments demonstrate ToE can accelerate DeiT-Tiny and DeiT-Small by about 1.3× with minimal accuracy degradation or even performance gains over full-token training baselines.

## Method Summary
ToE implements an "initialization-expansion-merging" pipeline applied to the output tokens of the first transformer block. The method begins with uniform spatial sampling to select an initial small token set, then uses widest feature-distribution expansion to iteratively add tokens farthest from the selected set based on cosine distance. Finally, unselected tokens are merged into their nearest selected tokens through averaging. This process progressively increases token count across training stages while maintaining feature distribution integrity, enabling significant computational savings from reduced attention operations.

## Key Results
- Achieves 1.3× faster training for DeiT-Tiny and DeiT-Small with negligible accuracy drop
- Maintains or improves accuracy compared to full-token training baselines
- Demonstrates consistent acceleration across DeiT, LV-ViT, and EfficientTrain frameworks
- Validated on ImageNet-1K and CIFAR-10/100 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Widest feature-distribution token expansion preserves intermediate feature space integrity by ensuring selected tokens cover diverse feature regions.
- Mechanism: Computes pairwise cosine distances between unselected and selected tokens, repeatedly selecting the token farthest from any selected token to maximize coverage.
- Core assumption: Feature distributions can be meaningfully compared using cosine distance, and distant tokens contain complementary information.
- Evidence anchors: [abstract] "widest feature-distribution token expansion is introduced to make the feature distributions of the selected token set as wide as possible"; [section] "we propose the widest feature-distribution token expansion strategy... seeks to maintain the integrity of the intermediate feature distribution"

### Mechanism 2
- Claim: Feature-distribution token merging retains information from unselected tokens by averaging close tokens into selected ones.
- Mechanism: After expansion, unselected tokens are merged into the nearest selected token based on feature distribution distance, preserving their information.
- Core assumption: Merging close tokens doesn't significantly distort feature distribution and captures essential information.
- Evidence anchors: [abstract] "we merge the unselected token set (blue boxes) into the selected one (red boxes) with the close feature distributions"; [section] "we merge the unselected tokens that the feature distributions are close to the selected ones"

### Mechanism 3
- Claim: Spatial-distribution token initialization ensures broad spatial coverage of image patches in the initial token set.
- Mechanism: Tokens are selected at regular spatial intervals (every ⌊1/r₀⌋ tokens), ensuring the initial set covers the entire image patch space.
- Core assumption: Early spatial diversity is critical for maintaining downstream feature integrity.
- Evidence anchors: [section] "This initialization selection strategy is based on spatial distribution... choose one token out of every ⌊1/r₀⌋ tokens"; [abstract] "we initially select a significantly small number of tokens, then progressively grow to the final full-token same as the original Transformer"

## Foundational Learning

- Concept: Cosine similarity as a distance metric for high-dimensional feature vectors
  - Why needed here: Used to measure feature distribution distances between tokens during expansion and merging
  - Quick check question: Why is cosine distance preferred over Euclidean distance for comparing token features in this context?

- Concept: Progressive training with staged token growth
  - Why needed here: ToE divides training into stages where token count increases gradually to maintain feature integrity
  - Quick check question: What is the advantage of dividing training into multiple stages versus a single growth phase?

- Concept: Self-attention computational complexity scaling with token count
  - Why needed here: The method reduces tokens to accelerate training, relying on the quadratic scaling of attention
  - Quick check question: How does reducing token count affect the FLOPs of self-attention layers?

## Architecture Onboarding

- Component map: Input -> Spatial-distribution initialization -> Widest feature-distribution expansion (repeated δ times) -> Feature-distribution merging -> Output
- Critical path: Initialization → Expansion (repeated δ times) → Merging → Forward pass
- Design tradeoffs:
  - Token reduction vs. information loss: Too aggressive reduction degrades accuracy
  - Number of expansion stages vs. training time: More stages improve accuracy but add overhead
  - Parallel expansion step size k vs. computational efficiency: Larger k reduces iterations but may miss optimal selections
- Failure signatures:
  - Accuracy drop during early training stages indicates insufficient token diversity
  - Instability in later stages suggests improper merging causing feature drift
  - No training speedup indicates incorrect token reduction implementation
- First 3 experiments:
  1. Validate spatial initialization by visualizing token coverage on sample images
  2. Test expansion strategy with synthetic feature distributions to confirm widest coverage
  3. Measure accuracy impact of merging threshold on a small model before full integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of ToE on models with different attention mechanisms, such as those using local attention or sparse attention?
- Basis in paper: [inferred] The paper focuses on Vision Transformers with standard self-attention, but does not explore the effect of ToE on models with different attention mechanisms.
- Why unresolved: The paper does not provide experimental results or analysis for models with alternative attention mechanisms.
- What evidence would resolve it: Experimental results comparing the performance of ToE on models with different attention mechanisms, such as local attention or sparse attention, would provide insights into the generalizability of the method.

### Open Question 2
- Question: How does the choice of the feature distribution distance metric (e.g., Manhattan, Euclidean, or Cosine distance) affect the performance of ToE in different scenarios?
- Basis in paper: [explicit] The paper compares the performance of different distance metrics in Table 9, but does not provide a comprehensive analysis of their impact on various scenarios.
- Why unresolved: The paper does not explore the effects of distance metrics in different contexts, such as varying model architectures or datasets.
- What evidence would resolve it: A thorough analysis of the performance of ToE with different distance metrics across various scenarios, including different model architectures and datasets, would provide insights into the optimal choice of distance metric.

### Open Question 3
- Question: Can ToE be effectively applied to models with different tokenization strategies, such as those using patch-based or point-based tokenization?
- Basis in paper: [inferred] The paper focuses on Vision Transformers with patch-based tokenization, but does not explore the effect of ToE on models with different tokenization strategies.
- Why unresolved: The paper does not provide experimental results or analysis for models with alternative tokenization strategies.
- What evidence would resolve it: Experimental results comparing the performance of ToE on models with different tokenization strategies, such as patch-based or point-based tokenization, would provide insights into the generalizability of the method.

## Limitations
- Implementation-specific sensitivity to precise details of cosine distance computation and merging thresholds
- Limited generalizability testing beyond ImageNet-1K and CIFAR datasets with specific ViT architectures
- Potential overstatement of practical benefits without full accounting of computational overhead

## Confidence
- **High Confidence**: The general concept that reducing token count through intelligent selection and merging can accelerate ViT training is well-supported by experimental results
- **Medium Confidence**: The specific "widest feature-distribution" expansion strategy and its superiority over other selection methods is moderately supported
- **Low Confidence**: The claim that ToE can achieve "lossless" acceleration with "even performance gains" is based on limited experimental evidence and may be overstated

## Next Checks
1. Implement ToE on additional transformer architectures (e.g., Swin, PVT) and datasets (e.g., COCO, ADE20K) to verify generalizability beyond reported models
2. Systematically compare "widest feature-distribution" expansion against random selection, entropy-based selection, and other distance metrics to validate superiority
3. Measure and report complete training pipeline time including all expansion/merging operations to provide accurate acceleration metrics accounting for computational overhead