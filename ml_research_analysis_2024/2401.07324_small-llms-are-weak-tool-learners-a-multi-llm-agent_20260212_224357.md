---
ver: rpa2
title: 'Small LLMs Are Weak Tool Learners: A Multi-LLM Agent'
arxiv_id: '2401.07324'
source_url: https://arxiv.org/abs/2401.07324
tags:
- name
- category
- electronics
- caller
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of training small language\
  \ models (LLMs) to act as effective tool-learning agents, a task that demands multiple\
  \ complex capabilities such as planning, tool invocation, and result summarization.\
  \ To overcome the limitations of small LLMs in single-agent frameworks, the authors\
  \ propose a novel multi-LLM agent framework called \u03B1-UMi, which decomposes\
  \ the agent into three specialized components: a planner, caller, and summarizer."
---

# Small LLMs Are Weak Tool Learners: A Multi-LLM Agent

## Quick Facts
- arXiv ID: 2401.07324
- Source URL: https://arxiv.org/abs/2401.07324
- Reference count: 40
- Small LLMs significantly improve tool-learning performance when decomposed into specialized components

## Executive Summary
This paper addresses the challenge of training small language models to act as effective tool-learning agents. The authors propose α-UMi, a multi-LLM agent framework that decomposes the agent into three specialized components: planner, caller, and summarizer. Each component is implemented as a dedicated LLM trained for a specific role. The framework uses a two-stage global-to-local progressive fine-tuning strategy and demonstrates significant performance improvements over single-LLM baselines on ToolBench and ToolAlpaca benchmarks, with a 7B backbone matching or exceeding a 13B single-LLM agent.

## Method Summary
The method involves training small LLMs as tool-learning agents by decomposing the task into specialized components (planner, caller, summarizer) and using a progressive fine-tuning strategy. The process starts with fine-tuning a backbone LLM on the full dataset, then splits it into three specialized LLMs and fine-tunes each on its sub-task dataset. The approach uses LLaMA-2 7B/13B models and evaluates performance on ToolBench and ToolAlpaca benchmarks using metrics like planning accuracy, tool invocation quality, and overall task success.

## Key Results
- α-UMi with 7B backbone matches or exceeds performance of 13B single-LLM agent
- Significant improvements in planning accuracy (Plan ACC) and tool invocation quality (Act. EM, Arg. F1, Hallu.)
- Overall task success metrics (R-L, Proc., Ans.) show superior performance
- The modular approach reduces hallucinations and improves robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing complex tool-learning tasks into specialized sub-tasks reduces cognitive load on small LLMs
- Mechanism: Each component focuses on a narrow capability, allowing the LLM to specialize rather than juggle multiple roles simultaneously
- Core assumption: Small LLMs have insufficient capacity to simultaneously excel at reasoning, tool invocation, and summarization
- Evidence anchors: [abstract], [section 3.2]
- Break condition: If model capacity scales sufficiently, the need for decomposition may diminish

### Mechanism 2
- Claim: The global-to-local progressive fine-tuning strategy bridges comprehensive understanding with specialized optimization
- Mechanism: First-stage fine-tuning provides holistic task comprehension, while second-stage fine-tuning optimizes each component for its specific sub-task
- Core assumption: Understanding the full task context enhances component performance when later specialized
- Evidence anchors: [abstract], [section 3.3]
- Break condition: If the global fine-tuning stage introduces harmful biases or if components can learn effectively without holistic context

### Mechanism 3
- Claim: Specialized prompts for each component reduce interference and improve focus on task-specific requirements
- Mechanism: Different system prompts guide each LLM toward its designated role, minimizing cross-task confusion
- Core assumption: Task-specific prompting can significantly improve performance by reducing irrelevant information processing
- Evidence anchors: [abstract], [section 3.2]
- Break condition: If prompts become too restrictive or if the model requires broader context to perform effectively

## Foundational Learning

- Concept: Task decomposition and modular design patterns
  - Why needed here: Understanding how to break down complex workflows into manageable components is essential for implementing the multi-LLM architecture
  - Quick check question: Can you identify three distinct sub-tasks in a typical tool-learning workflow and explain how they might benefit from separate LLM specialization?

- Concept: Progressive fine-tuning strategies
  - Why needed here: The global-to-local approach requires understanding how staged training can improve model performance compared to direct fine-tuning
  - Quick check question: What are the potential benefits and risks of first training a model on a comprehensive dataset before specializing it for specific sub-tasks?

- Concept: Prompt engineering for role specialization
  - Why needed here: Each component requires carefully crafted prompts to guide its specific function without interference from other tasks
  - Quick check question: How would you design prompts for a planner versus a caller component, and what key differences would you include?

## Architecture Onboarding

- Component map: Planner → Caller → Summarizer
- Critical path: Instruction → Planner → (Caller ↔ Tool)* → Planner → Summarizer → Answer
  - The planner-caller loop continues until the planner decides to transition to summarization or termination
- Design tradeoffs:
  - Storage: Three models require 3x storage vs single model
  - Latency: Sequential processing may increase response time
  - Flexibility: Components can be updated independently
  - Performance: Specialized components typically outperform general ones
- Failure signatures:
  - Planner fails to decide: Check if planner is receiving proper context and has appropriate termination criteria
  - Caller generates invalid actions: Verify tool documentation and action format requirements
  - Summarizer produces incomplete answers: Ensure summarizer has access to complete trajectory and appropriate prompt guidance
- First 3 experiments:
  1. Single-component baseline: Implement only the planner component to verify it can generate appropriate rationales and decisions
  2. Two-component pipeline: Add the caller component to test the planner-caller loop with a fixed summarizer (echo response)
  3. Full pipeline with mock tools: Implement all three components with simulated tool responses to verify end-to-end flow before real tool integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper bound of performance improvement when decomposing a single LLM into multiple specialized LLMs for tool learning tasks?
- Basis in paper: [inferred] The paper shows that α-UMi with a 7B backbone outperforms a single-LLM agent with a 13B backbone
- Why unresolved: The paper demonstrates empirical improvements but does not explore the theoretical limits of performance gains from decomposition
- What evidence would resolve it: A comprehensive theoretical analysis comparing representational capacity and optimization efficiency of single vs. multi-LLM architectures

### Open Question 2
- Question: How does the global-to-local progressive fine-tuning (GLPFT) strategy perform on other multi-task scenarios beyond tool learning?
- Basis in paper: [explicit] The paper proposes GLPFT specifically for tool learning but mentions it as a novel training strategy
- Why unresolved: The paper focuses solely on tool learning benchmarks and does not explore whether the two-stage training approach generalizes to other domains
- What evidence would resolve it: Experiments applying GLPFT to other multi-task domains and comparing its effectiveness against standard fine-tuning approaches

### Open Question 3
- Question: What is the optimal number and granularity of specialized LLMs for decomposing complex tasks?
- Basis in paper: [inferred] The paper uses three specialized components but does not explore whether this is optimal
- Why unresolved: The paper assumes three components as a reasonable decomposition for tool learning but does not investigate whether more granular decomposition or fewer components might yield better performance
- What evidence would resolve it: Systematic experiments varying the number of specialized LLMs across different task types and analyzing trade-offs

## Limitations
- Lack of ablation studies comparing α-UMi to alternative decomposition strategies and component counts
- Evaluation relies on established benchmarks without testing on more diverse or real-world tool environments
- Progressive fine-tuning strategy lacks sensitivity analysis for epochs, learning rates, and alternative schedules

## Confidence
- High confidence: Core empirical finding that multi-LLM architecture outperforms single-LLM baselines on tested benchmarks
- Medium confidence: Claimed mechanism that decomposition reduces cognitive load (reasonable explanation but not directly measured)
- Low confidence: Assertion that the specific 3-component decomposition is optimal or that alternatives wouldn't perform similarly or better

## Next Checks
1. **Ablation study on component count**: Test whether 2, 3, or 4 component architectures perform differently on the same benchmarks, keeping other factors constant
2. **Generalization test**: Evaluate the trained models on a new tool environment not seen during training to assess whether the specialization approach generalizes beyond the specific benchmarks
3. **Capacity sensitivity analysis**: Train α-UMi with different backbone sizes (3B, 13B, 30B) to determine whether the performance gains persist across the full spectrum of small-to-medium LLMs