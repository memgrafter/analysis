---
ver: rpa2
title: GFlowNet Pretraining with Inexpensive Rewards
arxiv_id: '2409.09702'
source_url: https://arxiv.org/abs/2409.09702
tags:
- a-gfn
- molecular
- property
- molecules
- gflownets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Atomic GFlowNets (A-GFNs), a novel generative
  model for molecular design that uses individual atoms as building blocks rather
  than predefined fragments, enabling broader exploration of chemical space. The authors
  propose an unsupervised pretraining approach using offline drug-like molecule datasets,
  conditioning A-GFNs on inexpensive molecular descriptors such as drug-likeliness,
  topological polar surface area, and synthetic accessibility scores as proxy rewards.
---

# GFlowNet Pretraining with Inexpensive Rewards

## Quick Facts
- arXiv ID: 2409.09702
- Source URL: https://arxiv.org/abs/2409.09702
- Reference count: 40
- Key outcome: Atomic GFlowNets with atom-based building blocks outperform fragment-based approaches in molecular exploration and property optimization tasks

## Executive Summary
This paper introduces Atomic GFlowNets (A-GFNs), a novel generative model for molecular design that uses individual atoms as building blocks rather than predefined fragments, enabling broader exploration of chemical space. The authors propose an unsupervised pretraining approach using offline drug-like molecule datasets, conditioning A-GFNs on inexpensive molecular descriptors such as drug-likeliness, topological polar surface area, and synthetic accessibility scores as proxy rewards. This pretraining strategy guides A-GFNs toward regions of chemical space with desirable pharmacological properties. The authors also implement a goal-conditioned fine-tuning process to adapt A-GFNs for specific target properties. Experimental results demonstrate that A-GFNs outperform fragment-based GFlowNets in exploring drug-like chemical space during pretraining and achieve superior performance in property optimization, targeting, and constrained optimization tasks during fine-tuning.

## Method Summary
The authors develop Atomic GFlowNets with an atom-based action space (addNode, addEdge, addNodeAttribute, addEdgeAttribute, stop) parameterized by graph neural networks. They propose a hybrid online-offline off-policy pretraining strategy using the ZINC250K dataset with inexpensive molecular descriptors (TPSA, QED, SAS, number of rings) as proxy rewards. The model is trained using trajectory balance loss with MLE regularization to handle the small trajectory length constraint inherent to atom-based approaches. After pretraining, the model undergoes goal-conditioned fine-tuning for specific property optimization tasks using task-specific reward functions. The architecture balances exploration of chemical space with maintaining synthesis feasibility through careful regularization.

## Key Results
- A-GFNs outperform fragment-based GFlowNets in exploring drug-like chemical space during pretraining
- A-GFNs achieve higher success rates, better convergence speed, and improved diversity in property optimization tasks
- The pretraining approach provides significant computational advantages while maintaining high molecular validity and novelty
- A-GFNs demonstrate superior performance in property targeting and constrained optimization compared to training from scratch

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Atomic GFlowNets can explore a larger chemical space than fragment-based GFlowNets because the action space includes all individual atoms rather than predefined fragments.
- **Mechanism:** By using atoms as building blocks, the model is not constrained by the limited vocabulary of molecular fragments, allowing it to construct molecules that may not be representable with any combination of available fragments. This increases the coverage of the chemical space and enables the discovery of novel molecular structures.
- **Core assumption:** The atom-based action space is sufficiently large and diverse to represent all relevant drug-like molecules, and the model can learn to navigate this space without getting lost in invalid or unproductive regions.
- **Evidence anchors:**
  - [abstract]: "leveraging individual atoms as building blocks to explore drug-like chemical space more comprehensively"
  - [section 1]: "A truly explorative generative policy, tapping into the full potential of GFlowNet would be realizable when using atoms instead of fragments as the action space"
  - [corpus]: No direct evidence found in corpus - assumption is based on theoretical argument
- **Break condition:** The model fails to learn valid molecular structures or gets stuck in regions of chemical space with low drug-likeness properties, suggesting the exploration is not effectively guided.

### Mechanism 2
- **Claim:** Pretraining with offline drug-like molecule datasets provides expert trajectories that help A-GFNs overcome the small trajectory length constraint for atom-based GFlowNets.
- **Mechanism:** The offline dataset provides expert demonstrations in the form of valid drug-like molecules. These trajectories give the model a starting point in chemically relevant regions of the state space, reducing the risk of collapse during training and enabling the generation of larger molecules with drug-like characteristics.
- **Core assumption:** The offline dataset contains sufficient diversity and coverage of drug-like chemical space to provide meaningful guidance, and the model can effectively learn from these expert trajectories to generalize to new regions.
- **Evidence anchors:**
  - [section 1]: "we propose to mitigate the small trajectory length constraint for atom-based GFNs by pretraining them with expert demonstrations, coming in the form of offline drug-like molecules"
  - [section 4.2]: "we propose leveraging the vast amounts of readily available inexpensive and unlabelled molecular data to perform a hybrid online-offline off-policy pretraining of A-GFN"
  - [corpus]: No direct evidence found in corpus - assumption is based on standard pretraining principles
- **Break condition:** The model overfits to the offline dataset and fails to generate novel molecules, or the offline data is not representative enough to guide effective exploration.

### Mechanism 3
- **Claim:** Using inexpensive molecular descriptors as proxy rewards during pretraining guides A-GFNs toward regions of chemical space with desirable pharmacological properties while remaining computationally efficient.
- **Mechanism:** Properties like drug-likeness (QED), topological polar surface area (TPSA), and synthetic accessibility (SAS) can be computed quickly and serve as proxies for more complex properties like binding affinity or toxicity. By optimizing for these inexpensive rewards during pretraining, the model learns to generate molecules that are likely to have good pharmacological profiles.
- **Core assumption:** The inexpensive molecular descriptors are sufficiently correlated with the more complex properties that matter for drug discovery, and optimizing for them will lead to molecules with good overall drug-like characteristics.
- **Evidence anchors:**
  - [section 4.1]: "inexpensive molecular rewards—such as Topological Polar Surface Area (TPSA), Quantitative Estimate of Drug-likeness (QED), synthetic accessibility (SAS)...These rewards are computationally cheap to evaluate and serve as proxies for more complex properties"
  - [section 4.1.1]: "In order to pre-train a goal-conditioned A-GFN for learning molecular properties p ∈ P , we need to define the property-specific conditional ranges cp"
  - [corpus]: No direct evidence found in corpus - assumption is based on standard cheminformatics principles
- **Break condition:** The inexpensive rewards do not correlate well with the actual drug-like properties, leading to molecules that score well on the proxies but perform poorly on the true objectives.

## Foundational Learning

- **Concept:** GFlowNets and their connection to reinforcement learning
  - **Why needed here:** Understanding GFlowNets is essential to grasp how the model learns to generate molecules proportional to reward distributions, which is the foundation of this approach.
  - **Quick check question:** What is the key difference between GFlowNets and traditional RL approaches in terms of the objective function?

- **Concept:** Molecular graph representation and SMILES notation
  - **Why needed here:** The model operates on molecular graphs and generates SMILES strings, so understanding these representations is crucial for working with the data and interpreting results.
  - **Quick check question:** How does the atomic-based action space differ from fragment-based approaches in terms of the molecular graphs that can be constructed?

- **Concept:** Molecular property calculation and their relevance to drug discovery
  - **Why needed here:** The model uses molecular properties as rewards, so understanding how these properties are calculated and why they matter for drug-like characteristics is essential for proper implementation and evaluation.
  - **Quick check question:** Why are properties like TPSA, QED, and SAS commonly used as indicators of drug-likeness in molecular design?

## Architecture Onboarding

- **Component map:** Atom-based action space (addNode, addEdge, addNodeAttribute, addEdgeAttribute, stop) -> Graph neural network policy parameterization -> Forward and backward policies (PF and PB) -> Trajectory balance loss function -> Offline dataset integration (ZINC250K) -> Property reward functions with conditional ranges

- **Critical path:**
  1. Load and preprocess offline molecular dataset
  2. Initialize GFlowNet policy with GNN
  3. Generate online and offline trajectories
  4. Compute property rewards for trajectories
  5. Optimize trajectory balance loss
  6. Evaluate and save model checkpoints

- **Design tradeoffs:**
  - Atom-based vs fragment-based action space: Larger space but harder to train vs smaller space but limited exploration
  - Property complexity vs computational cost: Expensive properties provide better guidance but slow training
  - Online vs offline training balance: Pure online is more flexible but may miss good regions; pure offline is stable but less exploratory

- **Failure signatures:**
  - High invalid molecule rate: Action space masking or GNN parameterization issues
  - Low diversity: Overfitting to offline dataset or insufficient exploration
  - Poor property optimization: Reward function misalignment or inadequate fine-tuning

- **First 3 experiments:**
  1. **Sanity check:** Train A-GFN on a small subset of ZINC with simple QED reward to verify basic functionality
  2. **Exploration test:** Compare fragment-based vs atom-based GFlowNets on scaffold coverage with same property rewards
  3. **Pretraining validation:** Evaluate pretraining A-GFN on novelty and diversity metrics against baseline methods

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the limitations section and discussion imply several areas for future research, including systematic comparison with state-of-the-art generative models, exploration of alternative molecular descriptors for pretraining, and investigation of different regularization strategies.

## Limitations
- The empirical validation is somewhat limited, with results primarily benchmarked against other GFlowNet variants rather than state-of-the-art diffusion or autoregressive models
- The claim that atom-based exploration is "more comprehensive" than fragment-based approaches is supported by scaffold coverage metrics but lacks direct comparison of molecular property optimization performance
- The specific benefits of offline pretraining for GFlowNets with long trajectories need more systematic ablation studies

## Confidence
- **High confidence** in the general framework and architectural choices
- **Medium confidence** in the specific pretraining benefits and comparative advantages

## Next Checks
1. **Ablation study on pretraining data quality**: Systematically vary the size and diversity of the offline pretraining dataset to quantify its impact on final performance, isolating the contribution of pretraining from the atom-based architecture.

2. **Direct comparison with state-of-the-art generative models**: Benchmark A-GFNs against leading molecular generation approaches (e.g., diffusion models, VAEs, autoregressive models) on standard property optimization and constrained optimization tasks to establish relative performance.

3. **Property correlation analysis**: Conduct a systematic study measuring the correlation between the inexpensive proxy rewards (TPSA, QED, SAS) and more complex, computationally expensive properties to validate the effectiveness of the pretraining objective.