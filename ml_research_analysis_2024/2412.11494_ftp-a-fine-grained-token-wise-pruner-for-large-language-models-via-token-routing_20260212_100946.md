---
ver: rpa2
title: 'FTP: A Fine-grained Token-wise Pruner for Large Language Models via Token
  Routing'
arxiv_id: '2412.11494'
source_url: https://arxiv.org/abs/2412.11494
tags:
- sparsity
- router
- token
- pruning
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a fine-grained token-wise pruning approach
  for large language models (LLMs) that dynamically identifies and skips less important
  tokens during inference to reduce computational cost. The method uses a token router
  with four low-dimensional input factors and three training losses to make compute/skip
  decisions, combined with a sparsity scheduler to allocate pruning ratios across
  model blocks.
---

# FTP: A Fine-grained Token-wise Pruner for Large Language Models via Token Routing

## Quick Facts
- arXiv ID: 2412.11494
- Source URL: https://arxiv.org/abs/2412.11494
- Reference count: 23
- Key outcome: Achieves up to 99.21% accuracy retention on LLaMA2-7B at 22% sparsity and 92.26% at 40% sparsity, with 1.28-1.61× inference speedup

## Executive Summary
This paper introduces FTP, a fine-grained token-wise pruning method for large language models that dynamically identifies and skips less important tokens during inference. The approach uses a token router with four low-dimensional input factors and three training losses to make compute/skip decisions, combined with a sparsity scheduler to allocate pruning ratios across model blocks. Experiments demonstrate superior performance compared to state-of-the-art pruning methods across multiple model sizes (7B, 8B, 13B parameters) and benchmarks.

## Method Summary
FTP employs a three-step framework: (1) initial sparsity search using a static router based on token position priority, (2) dynamic router training using four low-dimensional factors (token position, absolute attention scores, relative attention score rank, and sparsity requirements) with three losses (guide loss, sparsity constraint loss, knowledge distillation loss), and (3) sparsity scheduler fine-tuning using a genetic algorithm to optimize block-wise sparsity ratios. The router uses a simple two-layer MLP to make binary compute/skip decisions for each token in each transformer block, with modifications for KV cache compatibility during autoregressive decoding.

## Key Results
- Achieves 99.21% accuracy retention on LLaMA2-7B at 22% sparsity and 92.26% at 40% sparsity
- Outperforms state-of-the-art methods like DynamicV2 and DST across multiple benchmarks
- Provides 1.28-1.61× inference speedup while maintaining high accuracy
- Works across multiple model sizes (7B, 8B, 13B parameters) and maintains performance with KV cache integration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic token routing with low-dimensional input factors achieves better performance than high-dimensional hidden state approaches.
- Mechanism: The router uses four low-dimensional factors instead of full hidden states to make compute/skip decisions, reducing computational overhead while maintaining or improving accuracy.
- Core assumption: Low-dimensional factors capture sufficient information about token importance while being easier to learn than full hidden states.
- Evidence anchors:
  - [abstract] "we propose four low-dimensional factors as input to the router, making the model easier to train"
  - [section 3.2] "Previous work uses hidden states from each block as the sole feature for the router's decision-making. However, we argue that this approach is not effective"
- Break condition: If low-dimensional factors fail to capture sufficient token importance information, performance would degrade compared to hidden state approaches.

### Mechanism 2
- Claim: Sparsity scheduler with genetic algorithm optimization effectively allocates pruning ratios across model blocks.
- Mechanism: A genetic algorithm searches for optimal block-wise sparsity ratios that maximize accuracy while meeting overall sparsity constraints.
- Core assumption: Different blocks have varying sensitivity to token pruning, and optimal sparsity ratios can be found through evolutionary search.
- Evidence anchors:
  - [section 3.2] "we observe that the redundancy levels of different blocks are inconsistent, and the redundancy patterns of tokens within the same block are not fixed"
  - [section 4.3] "We utilize Genetic Algorithm to search for the optimal sparsity scheduler for each block"
- Break condition: If block redundancy patterns are too uniform or if the search space is too complex for genetic algorithms to find good solutions.

### Mechanism 3
- Claim: Decoupling sparsity allocation from router training improves optimization efficiency and performance.
- Mechanism: The method separates the problem into three steps: initial sparsity search with static router, dynamic router training with three proposed losses, and sparsity scheduler fine-tuning with trained router.
- Core assumption: Separating these optimization problems simplifies the overall optimization and leads to better solutions than joint optimization.
- Evidence anchors:
  - [section 3.2] "we divide the problem into several steps... Our method decouples the sparsity allocation and router tuning processes"
  - [section 4.3] "Table 3 demonstrates the effectiveness of our sparsity allocation compared to other strategies"
- Break condition: If the solutions found in each step are not compatible, or if joint optimization would find better solutions.

## Foundational Learning

- Concept: Transformer architecture and attention mechanism
  - Why needed here: The paper builds on transformer block structure (MHA + FFN) and leverages attention score patterns for token importance
  - Quick check question: How does the computational complexity of multi-head attention scale with sequence length?

- Concept: Genetic algorithms and evolutionary optimization
  - Why needed here: The sparsity scheduler uses a genetic algorithm to search for optimal block-wise sparsity ratios
  - Quick check question: What are the key components of a genetic algorithm (population, selection, crossover, mutation)?

- Concept: Knowledge distillation and transfer learning
  - Why needed here: The method uses knowledge distillation loss to align predictions between original and pruned models
  - Quick check question: How does knowledge distillation differ from standard supervised learning?

## Architecture Onboarding

- Component map:
  - Input: Token embeddings (L × d)
  - Router: 2-layer MLP (4 → 64 → 2) with softmax output
  - Losses: Guide loss (BCE), sparsity constraint loss, distillation loss (MSE)
  - Scheduler: Genetic algorithm for block-wise sparsity allocation
  - Output: Binary gate (compute/skip) for each token in each block

- Critical path: Token → Router factors → Router output → Gate decision → Block computation

- Design tradeoffs:
  - Router complexity vs accuracy: Simple 2-layer MLP vs more complex networks
  - Static vs dynamic routing: Fixed patterns vs input-dependent decisions
  - Decoupled vs joint optimization: Simpler optimization vs potentially better joint solutions

- Failure signatures:
  - Router fails to learn: Tokens with high importance scores still get pruned
  - Scheduler fails: Blocks with high redundancy get pruned too aggressively
  - Loss imbalance: Router focuses on one loss at expense of others

- First 3 experiments:
  1. Ablation study: Remove each router input factor and measure performance impact
  2. Router architecture comparison: Test recurrent router vs local router vs global router
  3. Sparsity allocation comparison: Uniform allocation vs BI score based vs GA-based vs fine-tuned

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sparsity ratio allocation across blocks change when using the dynamic router versus the static router?
- Basis in paper: [explicit] The paper discusses the difference in sparsity ratios obtained from the initial sparsity search using a static router versus the sparsity ratios obtained after fine-tuning with a trained dynamic router, as shown in Table 8.
- Why unresolved: The paper does not provide a detailed comparison of the sparsity ratio distributions before and after training the dynamic router.
- What evidence would resolve it: A detailed analysis of the sparsity ratios allocated to each block by the static router versus the dynamic router.

### Open Question 2
- Question: What is the impact of using different input factors (e.g., token position, absolute attention scores, relative attention score rank, sparsity requirements) on the performance of the dynamic router?
- Basis in paper: [explicit] The paper discusses the design of the dynamic router and the use of four low-dimensional factors as input, but does not provide an ablation study on the impact of each factor.
- Why unresolved: The paper does not provide a detailed ablation study on the impact of each input factor on the performance of the dynamic router.
- What evidence would resolve it: An ablation study that systematically removes each input factor and measures the impact on the performance of the dynamic router.

### Open Question 3
- Question: How does the performance of the dynamic router change when using different loss weights (λd, λs, λg) in the training objective?
- Basis in paper: [explicit] The paper discusses the use of three training losses with different loss weights, but does not provide a sensitivity analysis on the impact of each loss weight.
- Why unresolved: The paper does not provide a sensitivity analysis on the impact of each loss weight on the performance of the dynamic router.
- What evidence would resolve it: A sensitivity analysis that systematically varies each loss weight and measures the impact on the performance of the dynamic router.

## Limitations

- Limited evaluation on a relatively small set of datasets despite testing on four different model architectures
- No ablation study isolating the contribution of each of the four router input factors
- Missing analysis of computational overhead during training (router inference cost)

## Confidence

- Core mechanism validation: Medium
  - Strong experimental results but limited empirical validation of key design choices
- Method generalizability: Medium
  - Works across multiple model sizes but limited to specific model families
- Reproducibility: Medium
  - Clear methodology but some implementation details unspecified

## Next Checks

1. Compare router performance using low-dimensional factors versus full hidden states to validate the claimed computational advantage

2. Test alternative sparsity allocation strategies (e.g., uniform, gradient-based) against the genetic algorithm approach to isolate its contribution

3. Evaluate the method's performance on long-context benchmarks to assess whether token position-based routing remains effective as sequence length increases