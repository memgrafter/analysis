---
ver: rpa2
title: 'Towards Unifying Interpretability and Control: Evaluation via Intervention'
arxiv_id: '2411.04430'
source_url: https://arxiv.org/abs/2411.04430
tags:
- intervention
- methods
- coherence
- interpretability
- lens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of evaluating interpretability
  methods for their ability to control model behavior through interventions. It introduces
  a unifying encoder-decoder framework for four popular methods (sparse autoencoders,
  Logit Lens, Tuned Lens, and probing) and proposes two new metrics: intervention
  success rate and coherence-intervention tradeoff.'
---

# Towards Unifying Interpretability and Control: Evaluation via Intervention

## Quick Facts
- **arXiv ID:** 2411.04430
- **Source URL:** https://arxiv.org/abs/2411.04430
- **Reference count:** 40
- **Primary result:** Unifies four interpretability methods (SAEs, Logit Lens, Tuned Lens, probing) and proposes new metrics (intervention success rate, coherence-intervention tradeoff), finding lens-based methods outperform others for simple interventions but underperform prompting baselines

## Executive Summary
This paper addresses the critical challenge of evaluating interpretability methods not just for explanation quality but for their ability to control model behavior through interventions. The authors introduce a unifying encoder-decoder framework that brings together four popular interpretability approaches and proposes novel evaluation metrics to measure both intervention success and output coherence. The key finding is that while all methods allow for intervention, lens-based approaches (Logit Lens and Tuned Lens) achieve superior performance for simple interventions, though even the best methods fall short of simple prompting baselines, highlighting fundamental limitations in current interpretability approaches for practical control applications.

## Method Summary
The paper introduces a unified encoder-decoder framework where interpretable features z = xD are decoded back to latent representations x̂ via inverse mapping D⁻¹ to enable interventions. Four interpretability methods are evaluated: sparse autoencoders (SAEs), Logit Lens, Tuned Lens, and probing. Interventions are performed by directly altering feature activations, and their effectiveness is measured using two new metrics: intervention success rate (whether increasing feature activation increases feature presence in outputs) and coherence-intervention tradeoff (measuring grammatical correctness and consistency). The evaluation uses 210 open-ended prompts and 10 simple intervention topics across GPT2-small and Gemma2-2b models, comparing against prompting baselines.

## Key Results
- Logit Lens and Tuned Lens achieve the highest intervention success rates across all methods for simple interventions
- All interpretability methods underperform simple prompting baselines in achieving desired control
- Intervention effectiveness is inconsistent across features and models, with coherence degradation as a major challenge
- Lens-based methods excel at simple interventions due to lower reconstruction error and more direct feature-to-output mapping

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Intervention via interpretability methods works by decoding interpretable features back into latent space to control model outputs
- **Mechanism:** The paper unifies four interpretability methods (SAE, Logit Lens, Tuned Lens, probing) into an encoder-decoder framework where interpretable features z = xD are decoded back to latent representations x̂ via inverse mapping D⁻¹ to enable interventions
- **Core assumption:** The decoder/inverse mapping accurately reconstructs latent representations from interpretable features without significant information loss
- **Evidence anchors:** [abstract] "unify and extend four popular interpretability methods... into an abstract encoder-decoder framework, enabling interventions on interpretable features that can be mapped back to latent representations to control model outputs"

### Mechanism 2
- **Claim:** Intervention success is measured by whether increasing feature activation results in increased presence of that feature in model outputs
- **Mechanism:** The paper proposes "intervention success rate" as a metric measuring if increasing activation zi results in appropriate increase of feature i in model outputs, operationalized as checking if the target word/phrase appears in generated text
- **Core assumption:** Simple presence/absence of target words/phrases adequately captures intervention success for evaluating interpretability methods
- **Evidence anchors:** [abstract] "intervention success rate and coherence-intervention tradeoff, designed to measure the accuracy of explanations and their utility in controlling model behavior"

### Mechanism 3
- **Claim:** Lens-based methods outperform SAEs and probes for simple interventions due to lower reconstruction error and more direct feature-to-output mapping
- **Mechanism:** Logit Lens and Tuned Lens directly map to vocabulary space, requiring only simple linear transformations for decoding, while SAEs suffer from feature labeling noise and probes from spurious correlations
- **Core assumption:** Direct mapping to vocabulary space provides more reliable intervention than learned feature spaces with post-hoc labeling
- **Evidence anchors:** [abstract] "lens-based methods outperform SAEs and probes in achieving simple, concrete interventions, likely due to the spurious correlation learned by probes and steering vectors and the high error rate in SAE feature labeling pipelines"

## Foundational Learning

- **Concept: Encoder-decoder frameworks for interpretability**
  - Why needed here: Understanding how interpretability methods can be unified and extended for intervention requires grasping the bidirectional mapping between latent representations and interpretable features
  - Quick check question: What is the mathematical relationship between latent representations x, interpretable features z, and the decoder D in the proposed framework?

- **Concept: Causal intervention in neural networks**
  - Why needed here: Evaluating whether interpretability methods can actually control model behavior requires understanding how interventions propagate through the model architecture
  - Quick check question: How does the paper ensure that interventions on interpretable features causally affect the final model outputs rather than just correlating with them?

- **Concept: Evaluation metrics for interpretability methods**
  - Why needed here: The paper introduces novel metrics (intervention success rate, coherence-intervention tradeoff) that require understanding what makes interpretability methods useful for control applications
  - Quick check question: Why is coherence an important metric to measure alongside intervention success rate, and what does it tell us about the quality of interventions?

## Architecture Onboarding

- **Component map:** Input text → model encoding → latent representation → interpretability method encoding → interpretable features → intervention editing → decoding to latents → model generation → output evaluation

- **Critical path:** For each interpretability method, the critical path is: input text → model encoding → latent representation → interpretability method encoding → interpretable features → intervention editing → decoding to latents → model generation → output evaluation

- **Design tradeoffs:** The framework trades off between interpretability (more interpretable features vs. reconstruction accuracy) and intervention effectiveness (simple linear transformations vs. learned complex mappings), with lens-based methods favoring simplicity and SAEs favoring feature diversity

- **Failure signatures:** Interventions fail when: decoder reconstruction error is too high, interpretable features don't causally influence outputs, edited features get overridden by model's internal representations, or interventions damage output coherence beyond acceptable thresholds

- **First 3 experiments:**
  1. Implement Logit Lens intervention on GPT2-small with "coffee" feature to verify basic framework functionality and measure reconstruction error
  2. Compare intervention success rates across all four methods on simple features (beauty, chess, coffee, dogs) to establish baseline performance differences
  3. Measure coherence-intervention tradeoff curves for Logit Lens vs. prompting baseline to validate that interpretability methods can achieve comparable control without degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Logit Lens and Tuned Lens achieve superior intervention success rates compared to other methods like SAEs and probes?
- Basis in paper: [explicit] The paper states that Logit Lens and Tuned Lens generally outperform all other methods in achieving simple, concrete interventions.
- Why unresolved: The paper does not provide a detailed analysis of the underlying mechanisms that give lens-based methods an advantage, leaving this as an open question for further research.
- What evidence would resolve it: A comparative study analyzing the feature spaces and intervention mechanisms of lens-based methods versus SAEs and probes, potentially including experiments with different model architectures or datasets.

### Open Question 2
- Question: Can the coherence-intervention tradeoff be optimized for real-world applications where maintaining output quality is critical?
- Basis in paper: [inferred] The paper highlights that current interpretability methods often compromise model coherence, underperforming simpler alternatives like prompting.
- Why unresolved: The paper identifies the issue but does not explore potential solutions or strategies to balance intervention success with output coherence in practical scenarios.
- What evidence would resolve it: Development and testing of hybrid approaches that combine interpretability methods with prompting or other techniques to maintain coherence while achieving desired interventions.

### Open Question 3
- Question: How does the effectiveness of interpretability methods vary across different model architectures and layers?
- Basis in paper: [explicit] The paper finds that intervention effectiveness is inconsistent across features and models, and performs additional experiments to show intervention efficacy across model layers.
- Why unresolved: While the paper provides some insights, it does not offer a comprehensive framework for predicting or optimizing intervention effectiveness for specific model architectures or layers.
- What evidence would resolve it: A systematic study examining the performance of interpretability methods across a diverse range of models and layers, potentially leading to guidelines for selecting the most effective methods for different scenarios.

## Limitations

- The evaluation focuses on a limited set of simple, concrete features that may not represent the full complexity of interpretable features in larger models
- Coherence metrics rely on automated evaluation through Llama3.1-8b as an oracle judge, introducing potential bias from the oracle model's own capabilities
- The comparison to prompting baselines may be unfair since simple prompting exploits model training rather than mechanistic understanding

## Confidence

**High confidence** in the finding that lens-based methods (Logit Lens and Tuned Lens) achieve higher intervention success rates than SAEs and probes for simple interventions. This conclusion is well-supported by the unified framework and systematic evaluation across multiple models and intervention strengths.

**Medium confidence** in the coherence-intervention tradeoff analysis. While the methodology is sound, the reliance on automated coherence evaluation and the limited exploration of different coherence metrics introduce uncertainty about the robustness of these findings.

**Medium confidence** in the overall superiority of lens-based methods. The paper provides strong evidence for simple interventions, but doesn't adequately address whether this advantage extends to more complex, real-world interpretability tasks or whether SAEs might be superior for features requiring learned representations.

## Next Checks

1. **Feature complexity validation**: Systematically test the same intervention framework on progressively more complex features (from simple concrete concepts to abstract ideas) to determine whether the observed method differences persist across the full spectrum of interpretability.

2. **Alternative coherence metrics**: Re-run the coherence-intervention tradeoff analysis using multiple independent coherence evaluation methods (including human evaluation for a subset) to verify that coherence degradation isn't an artifact of the automated evaluation approach.

3. **Prompting baseline refinement**: Compare against more sophisticated prompting baselines that incorporate mechanistic understanding (e.g., chain-of-thought prompting, structured prompting templates) to determine whether the observed gap between interpretability interventions and prompting can be narrowed.