---
ver: rpa2
title: Towards Building a Robust Knowledge Intensive Question Answering Model with
  Large Language Models
arxiv_id: '2409.05385'
source_url: https://arxiv.org/abs/2409.05385
tags:
- information
- context
- answer
- knowledge
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a framework to improve the robustness of large
  language models (LLMs) in knowledge-intensive question answering tasks. It addresses
  the challenge of model performance degradation due to noisy or conflicting external
  information during retrieval-augmented generation.
---

# Towards Building a Robust Knowledge Intensive Question Answering Model with Large Language Models

## Quick Facts
- arXiv ID: 2409.05385
- Source URL: https://arxiv.org/abs/2409.05385
- Reference count: 25
- Key outcome: Improves LLM robustness in knowledge-intensive QA through data augmentation and contrastive learning, achieving WSCORE of 68.5 compared to 61.9 baseline

## Executive Summary
This paper addresses the challenge of improving large language model (LLM) robustness in knowledge-intensive question answering (QA) tasks where performance degrades due to noisy or conflicting external information during retrieval-augmented generation. The authors propose a two-stage fine-tuning framework: first using data augmentation with context masking and word swapping to improve generalization, then applying contrastive learning to enhance the model's ability to reject unanswerable questions. Experiments with Baichuan2-13B-Chat demonstrate significant improvements in weighted scores, approaching the performance of GPT-3.5-Turbo.

## Method Summary
The framework consists of two stages: (1) Data augmentation fine-tuning where answer-containing spans are randomly masked (~40%) and words are swapped within spans to simulate noise, forcing the LLM to rely more on internal knowledge; (2) Contrastive learning fine-tuning that trains the model to distinguish between answerable and unanswerable contexts by constructing pairs of samples with appropriate rejection phrases. The approach is implemented using LoRA fine-tuning with rank 8 and learning rate 1e-4, evaluated on constructed datasets simulating various noise and conflict scenarios.

## Key Results
- Weighted score (WSCORE) improves from 61.9 to 68.5 with contrastive learning approach
- Model achieves performance approaching GPT-3.5-Turbo on knowledge-intensive QA tasks
- Data augmentation improves generalization to incomplete contexts (SSIncomp samples)
- Contrastive learning enhances discrimination capability for rejecting unanswerable questions

## Why This Works (Mechanism)

### Mechanism 1
- Masking spans in training data improves LLM generalization by forcing the model to rely on internal knowledge when answer-relevant context is incomplete.
- During fine-tuning, randomly masking answer-containing spans removes explicit answer cues. The LLM must then infer answers using remaining context and its pre-trained knowledge, reducing over-reliance on noisy external data.
- Core assumption: LLMs retain sufficient internal knowledge to infer answers even when explicit answer spans are masked.
- Evidence anchors:
  - [section] "Masking is performed with spans, typically short sentences whose removal do not significantly affect semantic integrity. ... This motivates LLM to distinguish the relevance of information and stick to its own knowledge when confronted with inadequate context..."
  - [abstract] "We propose a data augmentation-based fine-tuning method to enhance LLM’s robustness against noise."

### Mechanism 2
- Contrastive learning enhances the model's ability to reject unanswerable questions by training it to distinguish between answerable and unanswerable contexts.
- By constructing pairs of samples where one has sufficient context and one does not, the model learns to assign higher probability to answering only when context is adequate, otherwise outputting a rejection phrase.
- Core assumption: The model can learn to associate specific rejection phrases with inadequate context through contrastive training.
- Evidence anchors:
  - [section] "To reinforce the model’s ability to refrain from providing answers beyond its scope rather than generating inaccurate responses, we introduce a contrastive learning-based approach."
  - [abstract] "contrastive learning approach is utilized to preserve the model’s discrimination capability of external information."

### Mechanism 3
- Data augmentation through word swapping introduces controlled noise to improve robustness against irrelevant or conflicting external information.
- Randomly swapping adjacent words within spans creates syntactic noise that simulates real-world retrieval noise, training the model to maintain performance despite minor context corruption.
- Core assumption: Word swapping preserves semantic meaning sufficiently that the model can still extract correct information while learning robustness.
- Evidence anchors:
  - [section] "As disorder of words between adjacent words does not alter its overall meaning significantly, the switch of words can be regarded as adding noise to the context."
  - [abstract] "To address the issue of model accuracy decline caused by noisy external information, we propose a data augmentation-based fine-tuning method..."

## Foundational Learning

- **Concept: Retrieval-augmented generation (RAG)**
  - Why needed here: The paper builds on RAG as the baseline framework being improved for robustness
  - Quick check question: What are the main limitations of RAG that this paper addresses?

- **Concept: Contrastive learning**
  - Why needed here: Used to train the model to distinguish between answerable and unanswerable contexts
  - Quick check question: How does the loss function in this contrastive approach differ from standard contrastive learning?

- **Concept: Data augmentation techniques**
  - Why needed here: Masking and swapping are core methods for creating robust training data
  - Quick check question: Why might masking be more effective than complete context removal for training?

## Architecture Onboarding

- **Component map**: Base LLM → Fine-tuning stage 1 (masking+swapping augmentation) → Fine-tuning stage 2 (contrastive learning) → Robust QA model
- **Critical path**: Data augmentation → Contrastive learning → Robustness evaluation
- **Design tradeoffs**: Masking improves generalization but may reduce performance on questions needing explicit context; contrastive learning adds rejection capability but requires carefully constructed negative samples
- **Failure signatures**: Accuracy drops on SSIncomp samples indicate insufficient internal knowledge; increased rejection rate without accuracy improvement suggests over-aggressive contrastive training
- **First 3 experiments**:
  1. Run baseline evaluation on SS samples to establish performance without augmentation
  2. Evaluate SSIncomp performance after stage 1 fine-tuning to measure generalization improvement
  3. Compare rejection rates between contrastive learning and direct preference optimization variants

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do different masking strategies (e.g., span length, semantic vs random masking) impact the model's ability to handle incomplete or noisy contexts in knowledge-intensive question answering?
- **Basis in paper**: [explicit] The paper discusses using span masking to enhance reasoning and generalization abilities, but does not explore different masking strategies.
- **Why unresolved**: The paper only mentions that spans are usually delimited by punctuation marks and every span has equal possibility of being eliminated, without specifying the optimal masking strategy.
- **What evidence would resolve it**: Experiments comparing different masking strategies (e.g., fixed vs variable span lengths, semantic vs random masking) and their impact on model performance in various noisy scenarios.

### Open Question 2
- **Question**: Can the proposed contrastive learning approach be effectively applied to other types of knowledge-intensive tasks beyond question answering, such as summarization or information extraction?
- **Basis in paper**: [inferred] The paper focuses on question answering tasks and demonstrates the effectiveness of contrastive learning in enhancing model discrimination capability, but does not explore its applicability to other tasks.
- **Why unresolved**: The paper does not provide any evidence or experiments to support the generalizability of the contrastive learning approach to other knowledge-intensive tasks.
- **What evidence would resolve it**: Experiments applying the contrastive learning approach to other knowledge-intensive tasks (e.g., summarization, information extraction) and evaluating its effectiveness in enhancing model performance.

### Open Question 3
- **Question**: How does the model's performance on the proposed datasets correlate with its performance on real-world knowledge-intensive question answering tasks that involve noise and conflicts?
- **Basis in paper**: [explicit] The paper constructs datasets simulating various scenarios, including critical information absence, noise, and conflicts, but does not evaluate the model's performance on real-world tasks.
- **Why unresolved**: The paper does not provide any evidence or experiments to support the external validity of the proposed datasets and evaluation methods.
- **What evidence would resolve it**: Experiments evaluating the model's performance on real-world knowledge-intensive question answering tasks that involve noise and conflicts, and comparing the results with its performance on the proposed datasets.

## Limitations

- The constructed noisy datasets may not fully capture the diversity of real-world retrieval noise patterns
- The data augmentation approach relies on the assumption that masking and swapping preserve sufficient semantic information
- The contrastive learning method's effectiveness depends on the quality of negative samples and the model's ability to generalize rejection behavior

## Confidence

- **High Confidence**: The effectiveness of data augmentation through masking and swapping for improving generalization to incomplete contexts
- **Medium Confidence**: The contrastive learning approach for improving rejection of unanswerable questions
- **Medium Confidence**: The claim that the proposed framework approaches GPT-3.5-Turbo performance

## Next Checks

1. **Robustness to Novel Noise Patterns**: Evaluate the fine-tuned model on a held-out test set with synthetic noise patterns not seen during training (e.g., more severe word swaps, grammatical errors, or semantic contradictions) to verify generalization of the robustness improvements.

2. **Rejection Precision and Recall Analysis**: Conduct a detailed analysis of the model's rejection behavior by computing precision and recall for rejection decisions across different sample types, particularly focusing on false positives (rejecting answerable questions) and false negatives (answering unanswerable questions).

3. **Ablation Study on Data Augmentation**: Perform an ablation study comparing the effects of masking-only, swapping-only, and combined augmentation strategies on model performance to quantify the individual contributions of each technique to the overall robustness improvement.