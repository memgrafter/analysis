---
ver: rpa2
title: Memory Mosaics
arxiv_id: '2405.06394'
source_url: https://arxiv.org/abs/2405.06394
tags:
- memory
- training
- figure
- attention
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Memory Mosaics are networks of associative memories that achieve
  prediction tasks through a process called predictive disentanglement. Unlike transformers,
  which are opaque, Memory Mosaics provide transparent compositional and in-context
  learning capabilities.
---

# Memory Mosaics

## Quick Facts
- arXiv ID: 2405.06394
- Source URL: https://arxiv.org/abs/2405.06394
- Reference count: 40
- Memory Mosaics are networks of associative memories that achieve prediction tasks through a process called predictive disentanglement

## Executive Summary
Memory Mosaics are networks of associative memories that achieve prediction tasks through a process called predictive disentanglement. Unlike transformers, which are opaque, Memory Mosaics provide transparent compositional and in-context learning capabilities. The core idea is to use associative memories to predict future information based on past observations, with the training process splitting the overall prediction task into disentangled sub-tasks assigned to each memory unit. Experiments show that Memory Mosaics match or outperform transformers on medium-scale language modeling tasks, including in-context learning. The predictive disentanglement principle explains how training decomposes the task into independent, efficiently memorized fragments that can be recombined.

## Method Summary
Memory Mosaics use associative memories implemented with kernel regression to predict future information based on past observations. The training process splits the overall prediction task into disentangled sub-tasks assigned to each memory unit through gradient-based optimization. The architecture consists of multiple blocks, each containing contextual and persistent memory units, with key and value extraction functions that use leaky averages and future peeking. The model is trained using AdamW optimizer with cosine learning rate scheduler on medium-scale language modeling tasks.

## Key Results
- Memory Mosaics match or outperform transformers on medium-scale language modeling tasks
- Demonstrated superior in-context learning abilities compared to transformers on out-of-distribution tasks
- Three moons experiment shows predictive disentanglement mechanism works for compositional learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Memory Mosaics use predictive disentanglement to split a prediction task into independently memorizable subtasks.
- **Mechanism**: The gradient training algorithm minimizes total prediction loss across time steps, which can be viewed as minimizing the context length needed for accurate predictions. Since each memory unit retrieves values using only past key/value pairs, the training process favors dividing the overall task into subtasks that each unit can handle independently.
- **Core assumption**: Each memory unit's retrieval function (kernel regression) converges to the true conditional expectation as more data is seen.
- **Evidence anchors**:
  - [abstract]: "Memory Mosaics achieve these capabilities in comparatively transparent way ('predictive disentanglement')."
  - [section 4]: "The training process minimizes the total prediction cost, that is the area under the curve... We can also view this area as a collection of horizontal slices, each representing the context length required to drive the prediction cost below a certain threshold."
  - [corpus]: Weak - related work discusses Hopfield networks and dense associative memories but doesn't directly address predictive disentanglement as described here.
- **Break condition**: If the kernel regression fails to converge to true conditional expectations, or if the prediction task cannot be meaningfully decomposed into independent subtasks.

### Mechanism 2
- **Claim**: Memory Mosaics perform as well or better than transformers on medium-scale language modeling tasks.
- **Mechanism**: Memory Mosaics use associative memories implemented with kernel regression that are closely related to self-attention. They achieve compositional and in-context learning capabilities through their transparent predictive disentanglement mechanism, while maintaining similar parameter counts and architecture depth to transformers.
- **Core assumption**: The kernel regression implementation of associative memories can approximate the behavior of self-attention sufficiently well for language modeling tasks.
- **Evidence anchors**:
  - [abstract]: "Memory Mosaics perform as well or better than transformers on medium-scale language modeling tasks."
  - [section 7]: "Memory Mosaic slightly outperforms the transformer for small depth networks, but this effect disappears when the depth increases and both the training and validation losses become indistinguishable."
  - [corpus]: Weak - related work discusses transformer alternatives but doesn't provide direct evidence comparing Memory Mosaics to transformers on language modeling.
- **Break condition**: If the kernel regression becomes computationally prohibitive at larger scales, or if the lack of position encoding significantly degrades performance on long sequences.

### Mechanism 3
- **Claim**: Memory Mosaics demonstrate superior in-context learning abilities compared to transformers.
- **Mechanism**: The three moons example demonstrates that Memory Mosaics can combine individual predictions for each moon to accurately predict configurations that differ from previously seen ones. This suggests Memory Mosaics learn compositional structures that can be recombined in novel ways during in-context learning.
- **Core assumption**: The architecture can learn to decompose complex prediction tasks into independently memorizable components that can be recombined during inference.
- **Evidence anchors**:
  - [section 5]: "The learned weight matrices... produce accurate predictions after a time equal to the period max(p1, p2, p3) of the slowest moon... In this interval, accurate predictions are returned for moon configurations that can be very different from the previously observed ones."
  - [section 7]: "On the out-of-distribution Simple English Wikipedia dataset, Memory Mosaics substantially outperform transformers after about 50 tokens, suggesting superior in-context learning abilities."
  - [corpus]: Weak - related work discusses in-context learning but doesn't specifically address the compositional decomposition mechanism described here.
- **Break condition**: If the in-context learning task requires maintaining dependencies between components that were decomposed during training, or if the context window exceeds the architecture's effective memory capacity.

## Foundational Learning

- **Concept**: Kernel regression and its relationship to attention mechanisms
  - Why needed here: Understanding how associative memories implemented with kernel regression relate to self-attention is crucial for grasping the core architecture
  - Quick check question: How does the Gaussian kernel smoothing in Memory Mosaics relate to the attention mechanism in transformers?

- **Concept**: Meta-learning and the distinction between training-time and inference-time processes
  - Why needed here: Memory Mosaics involve different processes at training and inference time - the training process determines what to predict, while inference involves memorizing specific key/value pairs
  - Quick check question: What is the difference between the meta-learning process during training and the memory-based learning during inference in Memory Mosaics?

- **Concept**: Disentanglement in machine learning and its relationship to compositional learning
  - Why needed here: The paper's key contribution is "predictive disentanglement" - understanding what disentanglement means and why it's valuable is essential
  - Quick check question: How does predictive disentanglement differ from traditional approaches to achieving disentanglement in neural networks?

## Architecture Onboarding

- **Component map**: Input → Embedding → Block 1 (Contextual memory → Persistent memory) → ... → Block N (Contextual memory → Persistent memory) → Output

- **Critical path**: Input → Embedding → Block 1 (Contextual memory → Persistent memory) → ... → Block N (Contextual memory → Persistent memory) → Output

- **Design tradeoffs**:
  - No position encoding vs. standard transformers - saves parameters but may limit long-range context handling
  - Single memory unit can implement induction heads vs. transformers needing two layers
  - Kernel regression (quadratic cost) vs. efficient transformer alternatives

- **Failure signatures**:
  - Training instability with very small depths (Nb=1 or Nb=4)
  - Performance degradation on tasks requiring long-range dependencies
  - Inability to maintain context across multiple stories in language modeling

- **First 3 experiments**:
  1. Replicate the three moons experiment to verify predictive disentanglement mechanism
  2. Train a 1-block Memory Mosaic on a simple language modeling task and compare with transformer baseline
  3. Test in-context learning capability on a simple out-of-distribution task to verify the three moons mechanism applies to language

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the exact mechanism by which predictive disentanglement occurs in transformer architectures, as suggested in the paper?
  - Basis in paper: [explicit] The paper conjectures that something similar to predictive disentanglement occurs in standard transformers.
  - Why unresolved: The paper only conjectures this similarity without providing a detailed explanation or evidence of how transformers achieve this effect.
  - What evidence would resolve it: Experimental results showing the internal workings of transformers that lead to a similar disentanglement effect, possibly through ablation studies or analysis of attention patterns.

- **Open Question 2**: How do Memory Mosaics perform on larger-scale language modeling tasks compared to transformers?
  - Basis in paper: [inferred] The paper demonstrates Memory Mosaics' performance on medium-scale tasks and suggests that further work is needed to replicate observations at far greater scale.
  - Why unresolved: The paper focuses on medium-scale tasks, leaving the performance on larger-scale tasks unverified.
  - What evidence would resolve it: Training and evaluating Memory Mosaics on large-scale datasets like the full Wikipedia or large-scale language modeling benchmarks.

- **Open Question 3**: Can Memory Mosaics effectively implement position encoding without relying on traditional methods?
  - Basis in paper: [explicit] The paper mentions that Memory Mosaics do not use positional encoding and rely on mechanisms like peeking into the future and leaky integration.
  - Why unresolved: The paper does not provide a comprehensive evaluation of these mechanisms' effectiveness in handling position-dependent tasks.
  - What evidence would resolve it: Experiments testing Memory Mosaics on tasks that heavily depend on positional information, such as machine translation or tasks requiring precise ordering.

## Limitations

- Memory Mosaics' performance advantages over transformers are most pronounced on specific tasks but become negligible or disappear on larger-scale language modeling tasks
- The "predictive disentanglement" mechanism lacks extensive empirical validation beyond the three moons toy example
- Kernel regression has quadratic cost in sequence length, making computational efficiency claims unclear

## Confidence

- High confidence: The mathematical framework and architectural description are well-specified and reproducible
- Medium confidence: The three moons experiment convincingly demonstrates the predictive disentanglement principle
- Low confidence: Claims about Memory Mosaics being "transparent" compositional models compared to transformers, and performance advantages on medium-scale tasks

## Next Checks

1. **Cross-task disentanglement validation**: Test whether the predictive disentanglement mechanism generalizes beyond periodic functions by applying Memory Mosaics to tasks with different types of compositional structure (e.g., hierarchical sequences, multi-modal data).

2. **Scaling behavior analysis**: Systematically compare Memory Mosaics and transformers across different sequence lengths and model depths to precisely characterize when and why performance differences emerge or disappear.

3. **Interpretability benchmark**: Develop quantitative metrics to measure the "transparency" claim by analyzing what Memory Mosaics actually learn during predictive disentanglement versus what transformers learn through attention patterns.