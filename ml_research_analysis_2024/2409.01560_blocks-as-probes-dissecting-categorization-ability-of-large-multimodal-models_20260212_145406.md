---
ver: rpa2
title: 'Blocks as Probes: Dissecting Categorization Ability of Large Multimodal Models'
arxiv_id: '2409.01560'
source_url: https://arxiv.org/abs/2409.01560
tags:
- primitive
- secondary
- primary
- image
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a benchmark called ComBo to evaluate the
  categorization ability of large multimodal models (LMMs). Inspired by human cognitive
  categorization, the benchmark assesses three key abilities: low-level pattern recognition,
  abstraction alignment with human concepts, and generalization to unseen abstract
  categories.'
---

# Blocks as Probes: Dissecting Categorization Ability of Large Multimodal Models

## Quick Facts
- arXiv ID: 2409.01560
- Source URL: https://arxiv.org/abs/2409.01560
- Authors: Bin Fu; Qiyang Wan; Jialin Li; Ruiping Wang; Xilin Chen
- Reference count: 40
- Primary result: LMMs struggle with abstract reasoning and generalization to new categories despite strong pattern recognition, with humans significantly outperforming them across all tasks.

## Executive Summary
This paper introduces ComBo, a benchmark for evaluating the categorization ability of large multimodal models (LMMs) using synthetic composite blocks. Inspired by human cognitive categorization, the benchmark assesses three key abilities: low-level pattern recognition, abstraction alignment with human concepts, and generalization to unseen abstract categories. The experiments reveal that while LMMs perform well on pattern recognition, they struggle with abstract reasoning and generalizing to new categories, especially as task difficulty increases. Human participants significantly outperform LMMs across all tasks, highlighting the need for further development of LMMs in fundamental visual perception and conceptual reasoning.

## Method Summary
The ComBo benchmark uses synthetic composite blocks to avoid data leakage and enable controlled evaluation of categorization ability. The benchmark consists of three tasks: Pattern Perception (low-level pattern recognition), Abstraction Alignment (alignment of category concepts with human mental representations), and Category Building (generalization to unseen abstract categories). The dataset includes 190,080 images composed of two geometric primitives with different shapes, materials, colors, and contact points. Pre-trained LMMs are evaluated without fine-tuning using multiple-choice questions and manual scoring for chain-of-thought responses.

## Key Results
- LMMs demonstrate strong pattern recognition abilities but struggle with abstract conceptual reasoning and learning new categories
- Human participants significantly outperform LMMs across all tasks, especially in spatial detail perception and abstract category understanding
- Performance gaps between humans and LMMs widen as task difficulty increases, particularly in the Category Building task

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: ComBo's synthetic dataset avoids data leakage and enables controlled evaluation of categorization ability.
- **Mechanism**: By using entirely synthetic composite blocks with no real-world visual content, the benchmark ensures that LMMs cannot rely on memorized patterns or concepts from their training data. The dataset is fully controllable, allowing researchers to manipulate difficulty and generate ground truth labels inexpensively.
- **Core assumption**: The synthetic dataset is sufficiently complex to test fundamental categorization skills while remaining unseen during training.
- **Evidence anchors**:
  - [abstract] The benchmark uses "synthetic composite blocks to avoid data leakage and enable controlled evaluation."
  - [section] "The objects and categories in ComBo are entirely unseen to LMMs, meeting the need of preventing data leakage."
- **Break condition**: If LMMs were exposed to similar synthetic data during training, or if the synthetic complexity is insufficient to probe true categorization ability.

### Mechanism 2
- **Claim**: The three-task framework comprehensively evaluates the full categorization process from perception to abstract reasoning.
- **Mechanism**: Each task targets a distinct stage: low-level pattern recognition, alignment of learned concepts with human mental representations, and generalization to novel abstract categories. This mirrors the human cognitive process of categorization (learning → use) and provides disentangled evaluation.
- **Core assumption**: Human-like categorization can be decomposed into these three stages and that each stage is necessary and sufficient for complete evaluation.
- **Evidence anchors**:
  - [abstract] "Inspired by this, we propose a novel, challenging, and efficient benchmark based on composite blocks, called ComBo, which provides a disentangled evaluation framework and covers the entire categorization process from learning to use."
  - [section] "We design three evaluation tasks...corresponding to different stages of the categorization process, in order to conduct a comprehensive evaluation of the categorization capability of LMMs."
- **Break condition**: If the tasks fail to capture essential aspects of categorization, or if the disentanglement introduces artificial constraints that do not reflect real-world categorization.

### Mechanism 3
- **Claim**: LMMs' superior performance on high-level visual tasks does not imply strong fundamental categorization ability.
- **Mechanism**: By evaluating LMMs on a pure, low-level categorization benchmark, the study reveals gaps in spatial detail perception, abstract conceptual reasoning, and learning new categories. This shows that high-level task performance may rely on superficial pattern matching rather than deep understanding.
- **Core assumption**: High-level visual tasks do not necessarily require robust fundamental categorization skills, or that LMMs can perform them through alternative mechanisms.
- **Evidence anchors**:
  - [abstract] "Although LMMs exhibit acceptable generalization ability in learning new categories, there are still gaps compared to humans in many ways, such as fine-grained perception of spatial relationship and abstract category understanding."
  - [section] "we find that although LMMs demonstrate enhanced categorization capability over traditional CV models, they continue to be stuck in spatial detail recognition, abstract conceptual reasoning, and learning unseen categories in some scenarios."
- **Break condition**: If high-level task performance is shown to depend on the same fundamental categorization skills tested in ComBo, or if the benchmark tasks are not representative of real-world categorization.

## Foundational Learning

- **Concept**: Cognitive categorization process (learning → use).
  - **Why needed here**: The benchmark is explicitly designed to mirror human categorization, so understanding this process is crucial for interpreting results and designing tasks.
  - **Quick check question**: What are the two main stages of human categorization, and how do they relate to the tasks in ComBo?

- **Concept**: Disentangled evaluation.
  - **Why needed here**: ComBo aims to isolate specific abilities (pattern recognition, abstraction alignment, category building) to understand LMMs' strengths and weaknesses. This requires understanding what disentangled evaluation means and why it's important.
  - **Quick check question**: Why is disentangled evaluation important for understanding LMMs' categorization ability, and how does ComBo achieve this?

- **Concept**: Synthetic data for benchmarking.
  - **Why needed here**: ComBo uses synthetic composite blocks to avoid data leakage. Understanding the benefits and limitations of synthetic data is crucial for evaluating the benchmark's validity.
  - **Quick check question**: What are the advantages and disadvantages of using synthetic data for evaluating LMMs' categorization ability, and how does ComBo address potential limitations?

## Architecture Onboarding

- **Component map**: Synthetic dataset generator -> Task definition module -> Evaluation pipeline
- **Critical path**: Generate synthetic composite blocks → Define evaluation tasks → Run LMM through each task → Score LMM's performance → Analyze results
- **Design tradeoffs**: Synthetic data ensures controlled evaluation but may not fully capture real-world complexity; three-task framework provides comprehensive evaluation but may introduce artificial constraints
- **Failure signatures**: Poor Pattern Perception suggests weaknesses in low-level visual processing; poor Abstraction Alignment indicates issues with conceptual reasoning; poor Category Building suggests difficulties in generalizing to novel categories
- **First 3 experiments**:
  1. Run an LMM through the Pattern Perception task and analyze its performance on each pattern type (shape, color, material, contact point)
  2. Run the same LMM through the Abstraction Alignment task and compare its performance on Img2Text vs. Text2Img questions
  3. Run the LMM through the Category Building task at different difficulty levels and analyze its decision-making process using Chain-of-Thought outputs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do LMMs perform on categorization tasks with more complex, real-world objects and images compared to synthetic composite blocks?
- **Basis in paper**: [inferred] The paper mentions that current LMMs struggle with spatial detail perception, abstract concept reasoning, and learning new categories using synthetic composite blocks. It also notes the need to use more complex, real-world images for further evaluation.
- **Why unresolved**: The ComBo benchmark uses synthetic data, which may not fully represent the complexity of real-world scenarios. LMMs' performance on real-world images could differ significantly.
- **What evidence would resolve it**: Testing LMMs on categorization tasks using real-world images and comparing their performance to that on synthetic composite blocks.

### Open Question 2
- **Question**: What are the specific architectural or training improvements needed for LMMs to better perceive spatial details, reason about abstract concepts, and learn new categories?
- **Basis in paper**: [explicit] The paper identifies these as key weaknesses of current LMMs and suggests that studying categorization can inspire further development in these areas.
- **Why unresolved**: The paper does not provide specific recommendations for architectural or training improvements. It only highlights the areas where LMMs need improvement.
- **What evidence would resolve it**: Proposing and testing specific architectural or training modifications aimed at improving LMMs' performance in spatial detail perception, abstract concept reasoning, and learning new categories.

### Open Question 3
- **Question**: How does the categorization ability of LMMs compare to other AI models, such as specialized computer vision models or large language models, on the ComBo benchmark?
- **Basis in paper**: [explicit] The paper mentions comparing LMMs to specialized models like ResNet-50 and ViT-B/16 on the Category Building task, showing that specialized models can achieve near-perfect performance with fine-tuning.
- **Why unresolved**: The comparison is limited to a single task and does not provide a comprehensive evaluation across all tasks in the ComBo benchmark.
- **What evidence would resolve it**: Testing and comparing the performance of LMMs, specialized computer vision models, and large language models across all tasks in the ComBo benchmark.

## Limitations

- The synthetic nature of the dataset may not fully capture the complexity of real-world categorization scenarios
- The evaluation methodology relies on multiple-choice questions and manual scoring, which may not capture the full spectrum of LMMs' categorization capabilities
- The benchmark's effectiveness depends on the assumption that LMMs were not exposed to similar synthetic data during training, which is difficult to verify

## Confidence

- **High Confidence**: The benchmark design and methodology are clearly specified and reproducible
- **Medium Confidence**: The claim that LMMs struggle with fundamental categorization despite high-level performance is supported by empirical results
- **Medium Confidence**: The assertion that ComBo provides a comprehensive evaluation of categorization ability is reasonable given the three-task framework

## Next Checks

1. **Cross-dataset validation**: Test whether LMMs that perform poorly on ComBo also show similar weaknesses on established categorization benchmarks with real-world images, such as ImageNet or CLEVR

2. **Ablation study on task components**: Systematically remove or modify individual task components to determine which aspects of the benchmark most strongly influence LMM performance differences

3. **Human benchmarking across difficulty levels**: Conduct a more comprehensive human evaluation across all difficulty levels and task types to establish clearer baselines and identify whether the performance gap between humans and LMMs is consistent or varies significantly with task complexity