---
ver: rpa2
title: An In-Depth Examination of Risk Assessment in Multi-Class Classification Algorithms
arxiv_id: '2412.04166'
source_url: https://arxiv.org/abs/2412.04166
tags:
- calibration
- probability
- invcp
- prediction
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a conformal prediction (CP)-based approach\
  \ to estimate the miss-classification probability for multi-class classification\
  \ models in safety-critical applications. The core idea is to interpret a model\u2019\
  s output as a prediction interval and estimate the probability that the true label\
  \ lies outside this interval using CP techniques."
---

# An In-Depth Examination of Risk Assessment in Multi-Class Classification Algorithms

## Quick Facts
- **arXiv ID:** 2412.04166
- **Source URL:** https://arxiv.org/abs/2412.04166
- **Authors:** Disha Ghandwani; Neeraj Sarna; Yuanyuan Li; Yang Lin
- **Reference count:** 40
- **Key outcome:** Proposes a conformal prediction (CP)-based approach to estimate miss-classification probability for multi-class classification models in safety-critical applications, showing reasonable results across various datasets.

## Executive Summary
This paper addresses the critical need for reliable risk assessment in multi-class classification models, particularly for safety-critical applications. The authors propose a novel conformal prediction-based approach that estimates miss-classification probability by interpreting model outputs as prediction intervals. The method is compared against traditional calibration techniques (histogram binning, isotonic regression, Platt scaling) across multiple datasets including CIFAR-100, CIFAR-10, Flowers102, ImageNet-V1, and Places365. The CP-based approach demonstrates model-agnostic applicability, simple implementation, and generally provides more accurate and conservative risk estimates compared to existing methods.

## Method Summary
The proposed method uses conformal prediction to estimate miss-classification probability by constructing a prediction interval for each input and calculating the miss-coverage level. The approach approximates the model's output interval with a CP interval that satisfies two properties: inclusion of the original interval and minimal size. This is achieved by computing a miss-coverage level α(X) for each input such that the CP interval T(X; α(X)) contains the model's prediction interval I(X) and has the smallest possible size. The method is compared with traditional calibration techniques including histogram binning, isotonic regression, and Platt scaling. Experiments are conducted on various datasets using pre-trained CNN models (resNets, denseNets, AlexNet, VGGs) and tree-based models (LightGBM, AdaBoost, XGBoost, RandomForest).

## Key Results
- The CP-based approach provides reasonable risk estimates across various datasets and models while maintaining conservativeness (δ ≥ 0)
- No single method consistently outperforms others, but the CP-based approach generally offers better accuracy and conservativeness
- For datasets with fewer classes, the CP-based approach performs particularly well
- Traditional calibration techniques (Platt scaling, histogram binning, isotonic regression) show varying performance depending on the dataset and model characteristics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The CP-based approach improves risk assessment by approximating a model's output interval with a CP interval that satisfies two properties: inclusion of the original interval and minimal size.
- **Mechanism:** The method computes a miss-coverage level α(X) for each input such that the CP interval T(X; α(X)) contains the model's prediction interval I(X) and has the smallest possible size. This α(X) is then averaged over a test set to estimate the miss-classification probability.
- **Core assumption:** The approximation error introduced by using a CP interval instead of the exact prediction interval is small enough to provide reasonable risk estimates.
- **Evidence anchors:** The abstract states the approach is "model and data-distribution agnostic, simple to implement, and provides reasonable results for a variety of use-cases." Section III.B details the problem reduction to estimating miss-coverage using CP intervals with properties P1 (contains I(X)) and P2 (smallest size).

### Mechanism 2
- **Claim:** Calibration techniques improve risk assessment by adjusting model output probabilities to better reflect true probabilities.
- **Mechanism:** These techniques transform raw model output probabilities into calibrated probabilities that satisfy the calibration condition P(Y = ˆY(X)|ˆp = p) = p, which are then used to estimate miss-classification probability.
- **Core assumption:** Calibration techniques can effectively correct the overconfidence bias in modern classification models.
- **Evidence anchors:** The abstract mentions considering calibration techniques that calibrate output probabilities for accurate probability outputs. Section II notes that model probabilities could be calibrated to better reflect true probabilities.

### Mechanism 3
- **Claim:** The method maintains conservativeness (δ ≥ 0) across different models and datasets, desirable for safety-critical applications.
- **Mechanism:** The InvCP approach tends to overestimate risk by constructing CP intervals larger than original prediction intervals, ensuring estimated miss-classification probability doesn't underestimate true value.
- **Core assumption:** Being conservative in risk assessment is preferable to being accurate but potentially under-prepared for failures in safety-critical applications.
- **Evidence anchors:** The abstract highlights CP as a non-parametric method that doesn't make assumptions on functional form of true class probabilities. Section III.B relates this to regression works that compute provably conservative risk-assessment.

## Foundational Learning

- **Concept:** Conformal prediction
  - **Why needed here:** The paper's main contribution is a CP-based approach to estimate miss-classification probability. Understanding CP is crucial for grasping how the method works.
  - **Quick check question:** What are the three steps to construct a CP interval, and what property does it guarantee?

- **Concept:** Model calibration
  - **Why needed here:** The paper compares the CP-based approach with traditional calibration techniques. Understanding calibration is necessary to evaluate strengths and weaknesses of different methods.
  - **Quick check question:** What is the calibration condition P(Y = ˆY(X)|ˆp = p) = p, and why is it important for risk assessment?

- **Concept:** Multi-class classification
  - **Why needed here:** The paper focuses on risk assessment in multi-class classification algorithms. Understanding challenges and techniques specific to multi-class classification is essential for applying methods to real-world problems.
  - **Quick check question:** How does the one-versus-all approach extend binary calibration techniques to multi-class classification?

## Architecture Onboarding

- **Component map:** Input -> Score function -> Quantile computation -> Prediction interval -> Risk estimate
- **Critical path:**
  1. Compute scores for calibration points
  2. Sort scores and find quantiles
  3. For each test point, compute s(X, I(X)) and find γ(X)
  4. Calculate α(X) = 1 - γ(X)/(n+1)
  5. Average α(X) over test set to get risk estimate
- **Design tradeoffs:** CP-based approach is model-agnostic and requires no optimization but may be overly conservative for datasets with many classes. Calibration techniques can be more accurate for specific models but require optimization and hyperparameter tuning.
- **Failure signatures:** High variance in risk estimates across different splits of calibration and test sets, consistent underestimation of risk (δ < 0) indicating non-conservativeness, poor performance on datasets with many classes.
- **First 3 experiments:**
  1. Compare CP-based approach with softmax outputs (SMX) on CIFAR-10 using VGG11 to evaluate performance on a simple dataset with a known overconfident model.
  2. Evaluate effect of varying calibration points (n) on accuracy and conservativeness of CP-based approach to understand scalability.
  3. Test robustness of CP-based approach to different score functions (e.g., Least Ambiguous Set-valued Classifier) on CIFAR-100 with many classes.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the proposed conformal prediction (CP)-based approach perform under covariate shift, and can it be extended to maintain its accuracy and conservativeness in such scenarios?
- **Basis in paper:** The paper discusses the need for future improvements, particularly mentioning that "most practical problems have data-drift and the proposed method needs to be extended to such scenarios."
- **Why unresolved:** The current study does not address performance under covariate shift, leaving a gap in understanding robustness in real-world applications where data distributions may change over time.
- **What evidence would resolve it:** Conducting experiments on datasets with simulated or real covariate shift and comparing performance with other methods under these conditions would provide insights into robustness and potential extensions.

### Open Question 2
- **Question:** What are the theoretical guarantees for conservativeness in the CP-based approach when the prediction interval is larger than the model's output interval?
- **Basis in paper:** The paper mentions that "it is unclear as of yet if the conservativeness can be guaranteed theoretically" when considering an interval larger than the model's output interval.
- **Why unresolved:** The paper does not provide theoretical analysis of conservativeness for the CP-based approach, particularly when the interval is expanded beyond the model's output.
- **What evidence would resolve it:** Developing and proving theoretical bounds or conditions under which the CP-based approach maintains conservativeness when using larger intervals would clarify its reliability.

### Open Question 3
- **Question:** How does the performance of the CP-based approach scale with the number of classes in the dataset, and what are the implications for datasets with a very large number of labels?
- **Basis in paper:** The paper notes that "for datasets with large number of labels (ImageNet and Places365), the calibration technique of histogram-binning provided the best results," suggesting CP-based approach may become overly conservative with more classes.
- **Why unresolved:** The paper does not provide detailed analysis of how CP-based approach's performance changes with number of classes, leaving questions about scalability and effectiveness for large-scale classification tasks.
- **What evidence would resolve it:** Analyzing CP-based approach's accuracy and conservativeness across datasets with varying numbers of classes would help understand scalability and identify potential improvements for large-scale applications.

## Limitations
- The CP-based approach may be overly conservative for datasets with many classes, leading to poor accuracy
- The method relies on approximating the model's prediction interval with a CP interval, introducing approximation error
- Effectiveness of calibration techniques depends on specific model and dataset characteristics, and they may fail to correct overconfidence bias in some cases

## Confidence
- **High confidence:** The CP-based approach is model-agnostic and simple to implement. The method maintains conservativeness (δ ≥ 0) across different models and datasets, which is desirable for safety-critical applications.
- **Medium confidence:** The CP-based approach provides reasonable results for a variety of use-cases. The method is generally more accurate and maintains conservativeness compared to traditional calibration techniques.
- **Low confidence:** The CP-based approach is consistently superior to calibration techniques for all models and datasets. The approximation error introduced by using a CP interval instead of the exact prediction interval is negligible.

## Next Checks
1. Conduct ablation studies to isolate the impact of the CP interval approximation on the accuracy and conservativeness of the risk estimates
2. Evaluate the robustness of the CP-based approach to different score functions and their computational efficiency on large datasets
3. Investigate the use of ensemble methods combining CP-based approach and calibration techniques to leverage their respective strengths and mitigate their weaknesses