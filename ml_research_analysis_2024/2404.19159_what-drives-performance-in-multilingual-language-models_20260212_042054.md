---
ver: rpa2
title: What Drives Performance in Multilingual Language Models?
arxiv_id: '2404.19159'
source_url: https://arxiv.org/abs/2404.19159
tags:
- data
- languages
- language
- pretrain
- script
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the factors influencing the performance
  of multilingual large language models (MLLMs) across diverse languages. The researchers
  analyze 6 MLLMs, including masked language models, autoregressive models, and instruction-tuned
  LLMs, on the SIB-200 dataset, a topic classification dataset encompassing 204 languages.
---

# What Drives Performance in Multilingual Language Models?

## Quick Facts
- arXiv ID: 2404.19159
- Source URL: https://arxiv.org/abs/2404.19159
- Authors: Sina Bagheri Nezhad; Ameeta Agrawal
- Reference count: 9
- One-line primary result: Pretraining data size is the most influential factor for seen languages, while script type and language family are crucial for unseen languages in multilingual language models.

## Executive Summary
This study investigates the factors influencing the performance of multilingual large language models (MLLMs) across diverse languages. The researchers analyze 6 MLLMs on the SIB-200 dataset, a topic classification dataset encompassing 204 languages. They examine the impact of factors such as pretraining data size, general resource availability, language family, and script type on model performance under three scenarios: ALL languages, SEEN languages (present in the model's pretraining data), and UNSEEN languages (not present in the pretraining data). Decision tree analysis reveals that pretraining data size is the most influential factor for SEEN languages, while script type and language family are crucial for UNSEEN languages, highlighting the importance of cross-lingual transfer learning.

## Method Summary
The study analyzes 6 MLLMs, including masked language models, autoregressive models, and instruction-tuned LLMs, on the SIB-200 dataset for text classification. Decision tree analysis is used to identify the most influential factors affecting model performance across three scenarios: ALL languages, SEEN languages, and UNSEEN languages. The factors examined include pretraining data size, general resource availability, language family, and script type. Mann-Whitney U test is conducted to validate the significance of the features identified by the decision tree. The study evaluates model performance using F1 score and investigates the impact of model size and architecture on the identified factors.

## Key Results
- Pretraining data size is the most influential factor for SEEN languages.
- Script type and language family are crucial for UNSEEN languages, highlighting the importance of cross-lingual transfer learning.
- Model size and architecture do not significantly alter the most important features identified.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining data size is the most influential factor for SEEN languages.
- Mechanism: Models perform better on languages they have encountered during pretraining because they have learned the relevant linguistic patterns and representations.
- Core assumption: The amount of language-specific pretraining data directly correlates with model performance on that language.
- Evidence anchors:
  - [abstract] "Decision tree analysis reveals that pretraining data size is the most influential factor for SEEN languages."
  - [section] "Pretraining data size consistently emerges as a crucial factor, but the distinction is less along the quantity of data but rather whether the languages have been encountered during training or not."
  - [corpus] Weak - corpus neighbors don't directly discuss pretraining data size as a factor, but they do mention model scaling and multilingual data.

### Mechanism 2
- Claim: Script type and language family are crucial for UNSEEN languages.
- Mechanism: When models encounter unfamiliar languages, they rely on similarities in writing systems (scripts) and linguistic relationships (language families) to generalize from their existing knowledge.
- Core assumption: Languages sharing the same script or belonging to the same family have similar enough features that knowledge can be transferred.
- Evidence anchors:
  - [abstract] "However, interestingly, script type and language family are crucial for UNSEEN languages, highlighting the importance of cross-lingual transfer learning."
  - [section] "For UNSEEN languages, script type and language family are influential, suggesting that MLLMs rely on cross-lingual transfer learning to generalize to unfamiliar languages."
  - [corpus] Weak - corpus neighbors don't directly discuss script type or language family as factors for unseen languages.

### Mechanism 3
- Claim: General resource availability plays a less prominent role overall but can be significant for specific models and settings.
- Mechanism: Models may leverage general knowledge about a language's resource level to adjust their expectations and performance, even if they haven't encountered the language during pretraining.
- Core assumption: Resource levels are a reasonable proxy for the amount and quality of linguistic data available for a language.
- Evidence anchors:
  - [section] "General resource availability plays a less prominent role overall but appears to be important for one specific model under one setting (Bloom in zero-shot for seen languages)."
  - [section] "Interestingly, general resource availability based on linguistic diversity taxonomy (Joshi et al., 2020) appears to be the most important factor for Bloom models in the zero-shot setup..."
  - [corpus] Weak - corpus neighbors don't directly discuss general resource availability as a factor.

## Foundational Learning

- Concept: Decision tree analysis
  - Why needed here: To understand the complex interplay of multiple factors influencing MLLM performance and identify the most important features.
  - Quick check question: How does decision tree analysis handle factors of different types (categorical, ordinal, numeric)?

- Concept: Cross-lingual transfer learning
  - Why needed here: To explain how models can generalize to unseen languages by leveraging similarities in scripts and language families.
  - Quick check question: What are the main mechanisms of cross-lingual transfer in multilingual language models?

- Concept: Resource level classification
  - Why needed here: To understand how general resource availability is quantified and its potential impact on model performance.
  - Quick check question: How are languages categorized into different resource levels, and what factors are considered?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training -> Evaluation -> Analysis
- Critical path: Data preprocessing → Model training → Evaluation → Analysis
- Design tradeoffs:
  - Model size vs. performance: Larger models may perform better but require more computational resources.
  - Pretraining data diversity vs. quantity: A more diverse pretraining corpus may improve cross-lingual transfer but could reduce the amount of data for individual languages.
- Failure signatures:
  - Poor performance on unseen languages with unique scripts or distant language families.
  - Over-reliance on pretraining data size as a predictor of performance, neglecting other important factors.
- First 3 experiments:
  1. Analyze the correlation between pretraining data size and performance on SEEN languages for different model architectures.
  2. Investigate the impact of script type and language family on performance for UNSEEN languages across various model sizes.
  3. Evaluate the importance of general resource availability for different models and training settings.

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses on a single text classification task (SIB-200), which may not generalize to other NLP tasks or domains.
- The analysis of only 6 MLLMs may not capture the full diversity of current model architectures and training approaches.
- The reliance on pretraining data size as a proxy for language exposure assumes that all data is equally valuable, which may not hold true for low-resource languages or domains with specialized terminology.

## Confidence
- **High Confidence**: The finding that pretraining data size is the most influential factor for SEEN languages is well-supported by the decision tree analysis and statistical tests.
- **Medium Confidence**: The importance of script type and language family for UNSEEN languages is supported by the analysis, but the underlying mechanisms of cross-lingual transfer are complex and may vary across models and language pairs.
- **Low Confidence**: The role of general resource availability as a factor influencing performance is the least well-established, with significant effects observed only for one specific model under one setting.

## Next Checks
1. Replicate findings on additional tasks: Test the identified factors' importance on other NLP tasks (e.g., named entity recognition, machine translation) to assess the generalizability of the results beyond text classification.
2. Analyze individual language performance: Conduct a detailed analysis of model performance on specific languages, particularly those with unique scripts or distant language families, to better understand the mechanisms of cross-lingual transfer and identify potential failure modes.
3. Investigate resource level impact: Perform a more comprehensive analysis of the role of general resource availability, including a larger sample of models and training settings, to determine if this factor consistently influences performance across different scenarios and to develop a more robust theoretical framework for its importance.