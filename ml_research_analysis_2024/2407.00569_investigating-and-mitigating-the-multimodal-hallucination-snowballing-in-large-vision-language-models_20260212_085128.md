---
ver: rpa2
title: Investigating and Mitigating the Multimodal Hallucination Snowballing in Large
  Vision-Language Models
arxiv_id: '2407.00569'
source_url: https://arxiv.org/abs/2407.00569
tags:
- image
- hallucination
- arxiv
- answer
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates Multimodal Hallucination Snowballing, where
  Large Vision-Language Models (LVLMs) are misled by their own generated hallucinations
  and produce incorrect responses. The authors propose MMHalSnowball, a framework
  to evaluate this phenomenon by constructing hallucinatory conversations.
---

# Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models

## Quick Facts
- arXiv ID: 2407.00569
- Source URL: https://arxiv.org/abs/2407.00569
- Authors: Weihong Zhong; Xiaocheng Feng; Liang Zhao; Qiming Li; Lei Huang; Yuxuan Gu; Weitao Ma; Yuan Xu; Bing Qin
- Reference count: 30
- One-line primary result: Multimodal hallucination snowballing causes LVLMs to suffer 31% performance drop, which can be mitigated by RVD to reduce hallucination snowballing by 24% while maintaining contextual abilities.

## Executive Summary
This paper investigates a critical failure mode in Large Vision-Language Models (LVLMs) called Multimodal Hallucination Snowballing, where models generate incorrect responses by over-relying on their own previously generated hallucinations rather than visual evidence. The authors construct a novel evaluation framework (MMHalSnowball) to measure this phenomenon and demonstrate that open-source LVLMs experience significant performance degradation when confronted with hallucinatory conversations. To address this issue, they propose Residual Visual Decoding (RVD), a training-free method that dynamically emphasizes visual information during inference to mitigate the accumulation of multimodal hallucinations.

## Method Summary
The paper introduces MMHalSnowball, a framework for constructing hallucinatory conversations to evaluate hallucination snowballing in LVLMs. The framework creates conversations where models first generate hallucinatory descriptions of images, then answer questions based on these hallucinations rather than the actual visual content. To mitigate this issue, the authors propose Residual Visual Decoding (RVD), which revises the output distribution by blending it with predictions derived from residual visual input. The blending ratio is dynamically adjusted based on the Jensen-Shannon divergence between visual-only and query-only predictions, allowing the model to emphasize visual information when needed while preserving contextual abilities.

## Key Results
- LVLMs suffer a 31% performance drop when encountering hallucinatory conversations compared to factual descriptions
- RVD reduces multimodal hallucination snowballing by more than 24% while maintaining contextual abilities
- The phenomenon is specifically tied to hallucinations rather than context length or general conversational complexity
- Different LVLM architectures show varying susceptibility to hallucination snowballing

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multimodal hallucination snowballing occurs because LVLMs over-rely on text context when hallucinations are present, ignoring visual information that contradicts the hallucination.
- **Mechanism:** The model conditions on the hallucinatory conversation, which creates a strong textual prior that biases the output distribution. When asked about visual content that contradicts this prior, the model fails to access the visual information and instead generates responses aligned with the hallucination.
- **Core assumption:** The model's language capabilities create overconfidence in the textual context, causing it to discount contradictory visual information during inference.
- **Evidence anchors:**
  - [abstract] "their strong language capabilities make them prone to be over-confident in the hallucinated context, thereby generating false claims that they normally would not support"
  - [section 2.7] "When presented with a query relevant to the previously generated hallucination that contradicts the visual information, can models make the correct judgment when they could have given a correct answer independently?"
  - [corpus] Weak evidence - most related work focuses on detecting or mitigating hallucinations generally, but few explicitly model the accumulation effect in conversation

### Mechanism 2
- **Claim:** Residual Visual Decoding (RVD) mitigates hallucination snowballing by dynamically blending visual predictions with the original output distribution based on how much the query depends on visual information.
- **Mechanism:** RVD constructs an input that residual connects visual input with the current text query, derives an output distribution from this visual-only path, and then blends it with the original distribution using a dynamically adjusted scaling parameter. This emphasizes visual information when needed while preserving contextual ability.
- **Core assumption:** The Jensen-Shannon divergence between visual predictions and query-only predictions can effectively measure how much the response depends on visual information.
- **Evidence anchors:**
  - [abstract] "we revise the output distribution of LVLMs with the one derived from the residual visual input, providing models with direct access to the visual information"
  - [section 3.2] "By residual connecting the visual information and the current user instruction, distributions that emphasizing the visual information are derived to revise the original output distribution"
  - [section 3.3] "We derive an output distribution pθ(y|x) given the current user query x only, and calculate the Jensen-Shannon divergence (JSD) between it and residual visual predictions"

### Mechanism 3
- **Claim:** The hallucination snowballing phenomenon is specific to the presence of hallucinatory context rather than simply increased context length or general conversational complexity.
- **Mechanism:** Experiments show that when the hallucinatory content is replaced with factual image descriptions (FactConv) or irrelevant conversations (IrrConv), the performance drop disappears, indicating that the issue is specifically tied to hallucinations rather than context length.
- **Core assumption:** LVLMs can benefit from longer context when it contains useful visual information, but suffer when that context contains contradictory hallucinations.
- **Evidence anchors:**
  - [section 2.8] "all the models benefit a lot from a correct image description, which further proves that LVLMs tend to rely on text context when there is text format visual information"
  - [section 2.8] "when the context provides no useful information, the models' abilities are not severely influenced by the context, which further indicates the performance drop in HalluConv. setting is caused by hallucination snowballing, not the context length"
  - [corpus] Moderate evidence - related work on context length effects exists but doesn't specifically isolate hallucination effects

## Foundational Learning

- **Concept: Multimodal Hallucinations in LVLMs**
  - Why needed here: Understanding what constitutes multimodal hallucinations is fundamental to recognizing when and why snowballing occurs.
  - Quick check question: What distinguishes an object hallucination from an attribute hallucination in vision-language models?

- **Concept: Zero-shot prompting and inference**
  - Why needed here: The evaluation framework uses zero-shot settings to avoid bias, and understanding inference parameters is crucial for reproducing results.
  - Quick check question: What inference parameters (temperature, top_p, etc.) are used consistently across experiments in this paper?

- **Concept: Output distribution manipulation and decoding strategies**
  - Why needed here: RVD fundamentally works by manipulating output distributions, so understanding techniques like contrastive decoding is essential.
  - Quick check question: How does the scaling parameter α in RVD control the balance between visual and contextual information?

## Architecture Onboarding

- **Component map:** Visual input + dialog history + current text query -> LVLM backbone -> RVD module (residual connection + dynamic blending) -> Generated response

- **Critical path:**
  1. Construct hallucinatory conversation using MMHalSnowball framework
  2. Generate model responses under HalluConv and CleanConv settings
  3. Compare accuracy and flip rates to measure snowballing
  4. Apply RVD during inference to mitigate effects
  5. Evaluate mitigation effectiveness and contextual ability preservation

- **Design tradeoffs:**
  - RVD vs prompt engineering: RVD preserves model autonomy while prompts can be brittle
  - Dynamic vs fixed blending: Dynamic adaptation preserves contextual ability but adds complexity
  - Zero-shot vs fine-tuning: Zero-shot avoids dataset contamination but may be less effective than specialized training

- **Failure signatures:**
  - If accuracy improvement is minimal despite high flip rate reduction, the model may be ignoring context too aggressively
  - If contextual ability degrades significantly (WPI task performance drops), the visual emphasis is too strong
  - If flip rate remains high, the JSD-based adaptation may not be accurately measuring visual dependence

- **First 3 experiments:**
  1. Replicate the HalluConv vs CleanConv accuracy comparison on a smaller subset to verify hallucination snowballing exists
  2. Implement basic RVD with fixed α=0.5 and test on HalluConv setting to confirm mechanism works
  3. Add JSD-based dynamic α adjustment and compare against fixed α on both HalluConv and WPI task

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation framework relies on ChatGPT-generated hallucinatory conversations, introducing potential variability in test case construction
- RVD's effectiveness depends on accurate JSD measurement, which may have edge cases where it performs suboptimally
- Findings are limited to open-source LVLMs, leaving uncertainty about proprietary model behavior

## Confidence
- **High Confidence:** The existence of multimodal hallucination snowballing and its measurable impact on model performance (31% drop in accuracy) is well-supported by controlled experiments comparing HalluConv vs CleanConv settings.
- **Medium Confidence:** The effectiveness of Residual Visual Decoding in mitigating hallucination snowballing (24% reduction) is demonstrated, but the results depend on specific implementation details not fully disclosed.
- **Low Confidence:** The generalizability of findings across all LVLM architectures and the long-term stability of RVD under varied conversational contexts remain uncertain.

## Next Checks
1. **Reproducibility Check:** Implement the MMHalSnowball framework using the exact ChatGPT prompts provided in the appendix (if available) and verify that similar hallucination snowballing patterns emerge across multiple runs with different random seeds.

2. **RVD Parameter Sensitivity:** Systematically vary the JSD threshold and scaling parameter α in RVD across the full range of possible values to identify optimal settings and understand failure modes when the adaptation mechanism over- or under-emphasizes visual information.

3. **Cross-Model Generalization:** Test RVD on additional LVLM architectures not included in the original study (e.g., proprietary models or newer open-source variants) to assess whether the mitigation technique's effectiveness generalizes beyond the specific models examined.