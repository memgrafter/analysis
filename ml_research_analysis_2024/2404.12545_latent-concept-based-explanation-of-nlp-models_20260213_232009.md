---
ver: rpa2
title: Latent Concept-based Explanation of NLP Models
arxiv_id: '2404.12545'
source_url: https://arxiv.org/abs/2404.12545
tags:
- layer
- latent
- concept
- concepts
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LACOAT generates explanations of NLP model predictions using latent
  concepts discovered in training data. It maps salient input representations to training
  latent concepts, which capture multifaceted aspects of words depending on context.
---

# Latent Concept-based Explanation of NLP Models

## Quick Facts
- **arXiv ID**: 2404.12545
- **Source URL**: https://arxiv.org/abs/2404.12545
- **Reference count**: 40
- **Primary result**: LACOAT generates explanations of NLP model predictions using latent concepts discovered in training data, with human evaluation showing it ranks higher than other methods and ablation tests confirming up to 46% prediction change.

## Executive Summary
LACOAT is a novel framework for explaining NLP model predictions by discovering latent concepts in training data and mapping salient input representations to these concepts. The approach uses hierarchical clustering of contextualized representations to identify multifaceted aspects of words, then maps important input features to these discovered concepts to provide enriched explanations. Human evaluation shows LACOAT explanations are more useful for understanding predictions compared to other methods, and faithfulness tests demonstrate that removing salient latent concepts significantly impacts model predictions.

## Method Summary
LACOAT operates through four main modules: ConceptDiscoverer extracts contextualized representations from training data and clusters them using agglomerative hierarchical clustering to discover latent concepts; PredictionAttributor uses Integrated Gradients to identify salient input representations; ConceptMapper trains a logistic regression classifier to map these salient representations to the discovered latent concepts; and PlausiFyer generates human-friendly explanations using an LLM. The framework is evaluated across multiple NLP tasks including POS tagging, toxicity classification, sentiment classification, and natural language inference using pre-trained models like BERT, RoBERTa, XLM-Roberta, and Llama-2.

## Key Results
- Human evaluation shows LACOAT explanations help understand model predictions and rank higher than other methods
- Faithfulness tests confirm ablation of salient latent concepts changes predictions up to 46% of the time
- LACOAT successfully generates explanations across multiple NLP tasks and model architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Latent concepts discovered via hierarchical clustering of contextualized representations capture facets of words that the model uses for predictions.
- Mechanism: ConceptDiscoverer extracts contextualized representations of words from training data, clusters them using agglomerative hierarchical clustering with Ward's criterion, and each cluster becomes a latent concept representing a specific facet of words in context.
- Core assumption: The model's learned representations naturally form meaningful clusters in the high-dimensional space that correspond to distinct semantic/morphological/syntactic facets.
- Evidence anchors:
  - [abstract] "Our foundational intuition is that a word can exhibit multiple facets, contingent upon the context in which it is used."
  - [section 2.1] "The words are grouped in the high-dimensional space based on various latent relations such as semantic, morphology, and syntax... these groupings evolve into dynamically formed clusters that represent a unique facet of the words called latent concept"
  - [corpus] Weak - related work focuses on concept-based explanations but doesn't specifically validate hierarchical clustering for capturing word facets in NLP models.

### Mechanism 2
- Claim: Mapping salient input representations to training latent concepts provides enriched explanations of predictions.
- Mechanism: PredictionAttributor uses Integrated Gradients to identify the most salient words/representations, ConceptMapper trains a logistic regression classifier to map these representations to the training latent concepts, and the resulting concepts explain the prediction.
- Core assumption: The most salient representations align with latent concepts that capture the facets the model actually uses for prediction.
- Evidence anchors:
  - [abstract] "LACOAT functions by mapping the representations of salient input words into the training latent space, allowing it to provide latent context-based explanations of the prediction."
  - [section 2.4] "The shortlisted latent concepts serve as an explanation of the prediction."
  - [corpus] Moderate - the approach is novel but related work validates concept-based explanations in other domains.

### Mechanism 3
- Claim: Human-friendly explanations generated by PlausiFyer improve interpretability of latent concept explanations.
- Mechanism: PlausiFyer prompts an LLM to explain the relationship between input sentence and latent concept, providing natural language descriptions of why the concept relates to the prediction.
- Core assumption: LLMs can effectively summarize and explain the relationship between complex latent concepts and input instances.
- Evidence anchors:
  - [section 2.4] "PlausiFyer offers a user-friendly summary and explanation of the latent concept and its relationship to the input instance using a Large Language Model (LLM)."
  - [section 4.1.2] "It is worth noting that both models are fine-tuned on identical data." - implies comparison between different model explanations.
  - [corpus] Moderate - related work validates LLM-based explanations but specific effectiveness for latent concepts is not established.

## Foundational Learning

- Concept: Agglomerative hierarchical clustering with Ward's criterion
  - Why needed here: Groups contextualized representations into meaningful clusters that represent word facets
  - Quick check question: What distance metric and linkage criterion are used to ensure clusters minimize within-cluster variance?

- Concept: Integrated Gradients for saliency attribution
  - Why needed here: Identifies which input representations are most important for the prediction
  - Quick check question: How many approximation steps are used in the IG computation and why?

- Concept: Logistic regression for concept mapping
  - Why needed here: Maps input representations to training latent concepts using learned decision boundaries
  - Quick check question: What regularization and solver are used when training the concept mapper?

## Architecture Onboarding

- Component map: ConceptDiscoverer → PredictionAttributor → ConceptMapper → PlausiFyer (pipeline flow from training data to explanation generation)
- Critical path: ConceptDiscoverer (once per dataset) → ConceptMapper training → PredictionAttributor + ConceptMapper (at inference)
- Design tradeoffs: Hierarchical clustering is computationally expensive but produces meaningful clusters vs. faster alternatives like k-means that may not capture irregular cluster boundaries
- Failure signatures: Poor concept discovery (no coherent clusters), bad saliency attribution (wrong features highlighted), mapping failures (low classifier accuracy), unhelpful LLM explanations
- First 3 experiments:
  1. Run ConceptDiscoverer on small training sample and visualize resulting clusters
  2. Test PredictionAttributor on sample input to verify saliency detection works
  3. Validate ConceptMapper accuracy on held-out data before integrating into pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LACOAT be adapted to scale to larger corpora with millions of tokens while maintaining computational efficiency?
- Basis in paper: [inferred] The paper discusses the limitations of hierarchical clustering for large corpora and mentions potential alternatives like k-means and leaders.
- Why unresolved: The paper does not provide empirical evidence or a detailed methodology for scaling LACOAT to larger datasets.
- What evidence would resolve it: Experiments comparing the performance and efficiency of LACOAT using different clustering algorithms (hierarchical, k-means, leaders) on datasets of increasing size would demonstrate scalability.

### Open Question 2
- Question: Can the PlausiFyer module be improved to generate more precise and task-relevant explanations, especially for incorrect predictions?
- Basis in paper: [explicit] The paper acknowledges that the human-friendly explanations generated by PlausiFyer can be prone to errors due to limitations of LLMs, such as positional bias and verbosity bias.
- Why unresolved: The paper does not explore methods to improve the quality of the explanations generated by PlausiFyer.
- What evidence would resolve it: Comparing the effectiveness of explanations generated by different LLM prompting strategies or using multiple LLMs (as suggested in the paper) would show if more precise explanations can be achieved.

### Open Question 3
- Question: How can LACOAT handle tasks that require reasoning over multiple sentences, where the prediction might depend on complex relationships between latent concepts?
- Basis in paper: [inferred] The paper mentions that LACOAT explanations might not be clearly indicative of the reason for a prediction in tasks requiring multi-sentence reasoning.
- Why unresolved: The paper does not provide a solution for addressing this limitation or explore how hierarchical relationships between latent concepts could be utilized.
- What evidence would resolve it: Experiments comparing the effectiveness of LACOAT explanations for multi-sentence reasoning tasks using different approaches (e.g., considering hierarchical relationships between latent concepts) would demonstrate if the limitation can be overcome.

## Limitations
- Hierarchical clustering may not scale well to larger datasets, requiring alternative clustering approaches
- The quality of explanations depends heavily on the LLM's ability to accurately relate latent concepts to predictions
- Explanations may not be clearly indicative for tasks requiring reasoning over multiple sentences

## Confidence
- **Mechanism 1 (Medium)**: The foundational assumption that hierarchical clustering captures semantically meaningful facets requires more rigorous validation through ablation studies.
- **Mechanism 2 (Medium)**: The mapping between salient representations and latent concepts is novel but lacks direct evaluation of the concept mapper's accuracy.
- **Mechanism 3 (Medium)**: LLM-based explanations are well-specified but introduce uncertainty about the quality and accuracy of generated explanations.

## Next Checks

1. **Cluster Ablation Study**: Replace the discovered latent concepts with randomly generated clusters of the same size and distribution. If LACOAT performs similarly, this would suggest the clustering itself isn't providing unique value.

2. **Concept Mapper Generalization Test**: Hold out a subset of training instances and their associated concepts, then evaluate whether the logistic regression classifier can accurately map representations to concepts on this held-out set. Current evaluation focuses on overall faithfulness rather than mapper accuracy.

3. **Alternative Clustering Comparison**: Implement and compare LACOAT using different clustering approaches (k-means, DBSCAN, or topic modeling) to determine if hierarchical clustering provides specific advantages for capturing model-relevant facets.