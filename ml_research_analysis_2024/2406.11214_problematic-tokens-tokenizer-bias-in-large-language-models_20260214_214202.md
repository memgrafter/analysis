---
ver: rpa2
title: 'Problematic Tokens: Tokenizer Bias in Large Language Models'
arxiv_id: '2406.11214'
source_url: https://arxiv.org/abs/2406.11214
tags:
- tokens
- data
- token
- sentences
- long
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates tokenizer bias in large language models
  (LLMs) by examining the impact of token length on model performance. The authors
  conduct experiments using GPT-4 and GPT-4o to evaluate how well these models handle
  long Chinese tokens versus their segmented shorter counterparts.
---

# Problematic Tokens: Tokenizer Bias in Large Language Models

## Quick Facts
- arXiv ID: 2406.11214
- Source URL: https://arxiv.org/abs/2406.11214
- Reference count: 9
- Primary result: GPT-4 outperforms GPT-4o in handling long Chinese tokens, with 80.72% retention accuracy versus 45.18% for GPT-4o

## Executive Summary
This paper investigates tokenizer bias in large language models by examining how token length affects model performance, particularly in Chinese language processing. The authors conduct experiments comparing GPT-4 and GPT-4o on their ability to handle long Chinese tokens versus their segmented shorter counterparts. Results demonstrate significant performance differences between the models, with GPT-4 consistently outperforming GPT-4o in token retention and sentence generation quality. The study also reveals that shorter tokens lead to improved performance for both models, and that most long tokens in the dataset are related to gaming or pornography, introducing ethical concerns. These findings highlight the importance of token length in model performance and the need for better tokenization frameworks to ensure more equitable and secure AI technologies.

## Method Summary
The authors collected long Chinese tokens and their segmented shorter counterparts from the o200k base tokenizer library, then used GPT-4 and GPT-4o to generate sentences incorporating these tokens. Human raters evaluated sentence relevance and accuracy on a 0-5 scale, while GPT-4 assessed privacy and security aspects of generated sentences. The study measured Token Retention Accuracy (TRA) to determine the percentage of sentences containing the given tokens, and evaluated consistency in GPT-4o's explanations and translations of long tokens using GPT-4 as judge. The experiments compared performance across different token lengths, from 1 to 11 tokens, to identify patterns in model behavior.

## Key Results
- GPT-4 achieved 80.72% token retention accuracy for long tokens compared to 45.18% for GPT-4o
- Both models showed improved performance with shorter tokens: GPT-4 reached 90.96% retention versus 83.73% for GPT-4o
- GPT-4o's performance plateaus when token size exceeds 6, indicating limitations in handling longer tokens
- Most high-quality long tokens in the dataset were related to gaming or pornography, introducing ethical concerns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 retains long tokens more accurately than GPT-4o because its tokenizer vocabulary is more comprehensive and better trained on diverse linguistic data.
- Mechanism: GPT-4 uses a larger, more diverse vocabulary that reduces the likelihood of unknown or under-trained tokens. This allows it to retain and generate sentences with long tokens more effectively than GPT-4o, which has a simplified token-handling approach that amplifies risks of misinterpretation.
- Core assumption: GPT-4's tokenizer vocabulary is sufficiently large and diverse to cover a wide range of linguistic expressions, including long tokens from under-resourced languages.
- Evidence anchors:
  - [abstract] "Specifically, it explores how the tokenizers vocabulary, often used to speed up the tokenization process and reduce tokens but constructed independently of the actual model training data, inadequately represents non-English languages."
  - [section] "GPT-4 demonstrates a superior capacity to construct sentences incorporating both the original long tokens and their shorter counterparts. Approximately 80.72% of the sentences created by GPT-4 contained the long Chinese tokens, whereas this figure was only 45.18% for GPT-4o."
  - [corpus] Found 25 related papers on tokenizer performance in Indian languages and low-resource languages, indicating this is a recognized issue in multilingual NLP.

### Mechanism 2
- Claim: Segmenting long tokens into shorter tokens improves model performance because shorter tokens are more common and easier for LLMs to handle accurately.
- Mechanism: Breaking down long, uncommon tokens into shorter, more common tokens increases the likelihood that the model has encountered these sub-tokens during training. This improves the model's ability to understand and generate sentences with these tokens, as evidenced by the improved performance of both GPT-4 and GPT-4o when using shorter tokens.
- Core assumption: LLMs are trained on datasets that contain a higher frequency of shorter tokens compared to longer, less common tokens, making them more familiar with shorter tokens.
- Evidence anchors:
  - [section] "Interestingly, both models showed enhanced performance with the shorter tokens: 90.96% of sentences from GPT-4 included the shorter tokens, compared to 83.73% for GPT-4o. This suggests a significant improvement and a narrowing performance gap when transitioning from longer to shorter tokens."
  - [section] "Our results validate this hypothesis: both models exhibit improved performance when long tokens are segmented into shorter tokens, with GPT-4 maintaining a slight edge. This indicates that shorter tokens are easier for LLMs to handle accurately."
  - [corpus] Comparative analysis of subword tokenization approaches for Indian languages suggests that breaking down complex tokens improves model performance.

### Mechanism 3
- Claim: The presence of low-quality content in training data, such as gambling and pornography links, negatively impacts the ethical alignment and quality of LLM outputs.
- Mechanism: Training LLMs on datasets that include inappropriate or low-quality content can lead to the generation of biased or unethical outputs. The study found that most long tokens were related to gaming or pornography, which introduces ethical concerns and affects model outputs, particularly in GPT-4o, which struggled more with these tokens.
- Core assumption: The quality of training data directly influences the ethical alignment and output quality of LLMs, and the presence of inappropriate content can bias the model's behavior.
- Evidence anchors:
  - [abstract] "This misrepresentation results in the propagation of under-trained or untrained tokens, which perpetuate biases and pose serious concerns related to data security and ethical standards."
  - [section] "Most of the high-quality, contextually rich data in Chinese remains locked within app servers and proprietary databases... This situation necessitates reliance on publicly available data sources, which, unfortunately, are often riddled with low-quality content such as gambling and pornography links."
  - [section] "Sentences containing long tokens related to sensitive topics such as gaming and pornography scored lower in privacy and security evaluations."
  - [corpus] Explaining and Mitigating Crosslingual Tokenizer Inequities discusses how dataset quality affects model performance, supporting the idea that low-quality data leads to poor outputs.

## Foundational Learning

- Concept: Tokenization
  - Why needed here: Understanding how tokenization works is crucial because it directly impacts the performance and accuracy of LLMs, especially when dealing with languages that have complex morphological structures or limited resources.
  - Quick check question: What is the primary purpose of tokenization in natural language processing, and how does it affect the performance of LLMs?

- Concept: Data quality and bias in AI
  - Why needed here: Recognizing the impact of data quality and bias is essential for developing ethical and effective AI systems, as poor-quality data can lead to biased or unethical outputs.
  - Quick check question: How can the presence of low-quality or biased data in training datasets affect the performance and ethical alignment of LLMs?

- Concept: Evaluation metrics for NLP models
  - Why needed here: Understanding evaluation metrics is important for assessing the performance of LLMs in handling different types of tokens and ensuring that the models meet the desired quality and ethical standards.
  - Quick check question: What are some common evaluation metrics used to assess the performance of LLMs in generating accurate and contextually relevant sentences?

## Architecture Onboarding

- Component map: Tokenizer -> LLM sentence generation -> Human evaluation -> GPT-4 privacy/security assessment -> Consistency checking
- Critical path: Tokenization → LLM sentence generation → Evaluation of token retention, relevance, and ethical alignment → Mitigation strategies for improving data quality and model performance
- Design tradeoffs: Using a larger tokenizer vocabulary can improve performance but may increase computational costs. Segmenting long tokens into shorter ones can enhance understanding but may lose some contextual meaning.
- Failure signatures: Poor token retention, generation of irrelevant or biased sentences, and low scores in privacy and security evaluations indicate issues with tokenization or data quality.
- First 3 experiments:
  1. Compare token retention accuracy of GPT-4 and GPT-4o with long and short tokens to identify performance gaps.
  2. Evaluate the impact of token segmentation on model performance by testing with both segmented and non-segmented tokens.
  3. Assess the ethical alignment of generated sentences by analyzing the presence of inappropriate content and biases in the outputs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-4 and GPT-4o vary with token length beyond the tested range (up to 11 tokens)?
- Basis in paper: [explicit] The paper mentions that GPT-4o's performance plateaus when token size exceeds 6, indicating potential limitations in handling longer tokens.
- Why unresolved: The study only tested token sizes up to 11, leaving uncertainty about model performance with even longer tokens.
- What evidence would resolve it: Testing the models with tokens longer than 11 characters to determine if performance continues to degrade or stabilizes.

### Open Question 2
- Question: How do the identified biases in tokenization impact the overall ethical alignment of LLMs in real-world applications?
- Basis in paper: [explicit] The paper highlights that most long tokens in the dataset are related to gaming or pornography, introducing ethical concerns and affecting model outputs.
- Why unresolved: While the paper identifies the presence of biases, it does not explore the extent to which these biases affect real-world LLM applications.
- What evidence would resolve it: Conducting case studies or simulations to observe how biased tokenization impacts LLM outputs in practical scenarios.

### Open Question 3
- Question: What specific data filtering techniques can effectively mitigate the biases introduced by problematic tokens in LLMs?
- Basis in paper: [inferred] The paper suggests that advanced data filtering techniques are crucial for enhancing LLM robustness and ethical alignment but does not detail specific methods.
- Why unresolved: The paper proposes the need for data filtering but lacks a detailed exploration of effective techniques.
- What evidence would resolve it: Implementing and evaluating various data filtering methods to determine their effectiveness in reducing biases from problematic tokens.

## Limitations

- The study focuses exclusively on Chinese tokens from a single tokenizer library (o200k base), limiting generalizability to other languages and tokenization approaches.
- Human evaluation relies on subjective scoring (0-5 scale) without detailed scoring criteria, introducing potential rater bias.
- The privacy and security evaluation methodology using GPT-4 as judge lacks transparency in assessment criteria.

## Confidence

- High Confidence: The empirical finding that GPT-4 outperforms GPT-4o in long token retention (80.72% vs 45.18%) is well-supported by direct experimental results.
- Medium Confidence: The mechanism attributing GPT-4's superior performance to a more comprehensive tokenizer vocabulary is plausible but not directly verified.
- Low Confidence: The assertion that most long tokens in the dataset relate to gaming or pornography is based on qualitative assessment without clear statistical backing.

## Next Checks

1. **Vocabulary Size Verification**: Conduct controlled experiments comparing GPT-4 and GPT-4o with identical tokenizer vocabularies to isolate whether performance differences stem from vocabulary size versus other architectural factors.

2. **Cross-Lingual Generalization Test**: Replicate the long token vs. short token experiment across 3-5 additional languages with different morphological complexity (e.g., Japanese, Arabic, Finnish) to assess whether findings generalize beyond Chinese.

3. **Data Quality Impact Quantification**: Systematically categorize generated outputs by content type and measure performance degradation rates, establishing statistical correlation between inappropriate content exposure and token retention/accuracy metrics.