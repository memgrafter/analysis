---
ver: rpa2
title: Should We Fear Large Language Models? A Structural Analysis of the Human Reasoning
  System for Elucidating LLM Capabilities and Risks Through the Lens of Heidegger's
  Philosophy
arxiv_id: '2403.03288'
source_url: https://arxiv.org/abs/2403.03288
tags:
- reasoning
- knowledge
- llms
- faculty
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the capabilities and limitations of Large
  Language Models (LLMs) by comparing them to human reasoning through the philosophical
  lens of Martin Heidegger's concepts of "ready-to-hand" and "present-at-hand." The
  analysis identifies that LLMs are computational analogues of the Faculty of Verbal
  Knowledge, excelling in Direct Explicative Reasoning and Pseudo Rational Reasoning
  but lacking in authentic rational and creative reasoning due to the absence of faculties
  like Judgement and Analogy. The study also explores the potential risks and benefits
  of augmenting LLMs with other AI technologies, concluding that while current LLM-based
  AIs do not pose an existential threat, careful regulation is necessary to mitigate
  future risks.
---

# Should We Fear Large Language Models? A Structural Analysis of the Human Reasoning System for Elucidating LLM Capabilities and Risks Through the Lens of Heidegger's Philosophy

## Quick Facts
- arXiv ID: 2403.03288
- Source URL: https://arxiv.org/abs/2403.03288
- Authors: Jianqiiu Zhang
- Reference count: 0
- One-line primary result: LLMs are computational analogues of the Faculty of Verbal Knowledge, excelling in Direct Explicative and Pseudo Rational Reasoning but lacking authentic rational and creative reasoning due to missing cognitive faculties.

## Executive Summary
This paper investigates the capabilities and limitations of Large Language Models (LLMs) by comparing them to human reasoning through the philosophical lens of Martin Heidegger's concepts of "ready-to-hand" and "present-at-hand." The analysis identifies that LLMs are computational analogues of the Faculty of Verbal Knowledge, excelling in Direct Explicative Reasoning and Pseudo Rational Reasoning but lacking in authentic rational and creative reasoning due to the absence of faculties like Judgement and Analogy. The study also explores the potential risks and benefits of augmenting LLMs with other AI technologies, concluding that while current LLM-based AIs do not pose an existential threat, careful regulation is necessary to mitigate future risks.

## Method Summary
The paper conducts a structural analysis of human reasoning guided by Heidegger's notion of truth as "unconcealment." It categorizes reasoning types based on the creation of new knowledge, use of existing knowledge, and incorporation of empirical observations. Each category and subcategory is analyzed to identify the cognitive faculties involved and LLMs' role within them. The methodology involves drawing parallels between the statistical patterns of word relationships within LLMs and Heidegger's concepts, evaluating LLMs' capabilities and limitations in different categories of reasoning, and identifying potential risks and benefits of augmenting LLMs with other AI technologies.

## Key Results
- LLMs act as computational analogues of the Faculty of Verbal Knowledge, encoding statistical patterns of word relationships that mirror Heidegger's "ready-to-hand" and "present-at-hand" links.
- LLMs possess capabilities for Direct Explicative Reasoning and Pseudo Rational Reasoning but fall short in authentic rational reasoning and have no creative reasoning capabilities due to the lack of analogous AI models such as the Faculty of Judgement.
- While current LLM-based AIs do not pose an existential threat, careful regulation is necessary to mitigate future risks associated with augmenting LLMs with other AI technologies.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs act as computational analogues of the Faculty of Verbal Knowledge because they encode statistical patterns of word relationships that mirror Heidegger's "ready-to-hand" and "present-at-hand" links.
- Mechanism: During training, LLMs build vector representations of words capturing contextual and relational probabilities; these correspond to functional ("ready-to-hand") and descriptive ("present-at-hand") connections in human knowledge.
- Core assumption: Language is primarily structured by relational links rather than by isolated terms, and these links can be approximated through statistical modeling.
- Evidence anchors:
  - [abstract] "innovative parallels between the statistical patterns of word relationships within LLMs and Martin Heidegger's concepts of 'ready-to-hand' and 'present-at-hand'"
  - [section 1.4] "LLMs are engineered to statistically understand the relations between words" and "LLM-based AI systems can be considered linguistic approximations of the Faculty of Verbal Knowledge"
  - [corpus] Weak: no direct corpus support for Heidegger's concepts in LLM literature; only general LLM evaluation papers present.
- Break condition: If LLMs fail to preserve context-dependent relational probabilities (e.g., under distributional shift), the mapping to human knowledge faculty breaks.

### Mechanism 2
- Claim: Direct Explicative Reasoning in humans corresponds to System 1 thinking, and LLMs can emulate this by retrieving activated relational links given a prompt.
- Mechanism: Prompt → activation of contextually relevant links in the knowledge network → retrieval of knowledge without intermediate judgment → output reflecting activated associations.
- Core assumption: Reasoning can be split into non-creative (explicative) and creative categories, with Direct Explicative Reasoning operating reflexively and without self-awareness.
- Evidence anchors:
  - [abstract] "LLMs possess the capability for Direct Explicative Reasoning and Pseudo Rational Reasoning"
  - [section 3.1] "Direct reasoning is defined as the act of retrieving relevant knowledge about an entity of interest directly in response to a prompt" and "characteristics of Direct Explicative Reasoning align closely with what psychology terms System 1 thinking"
  - [corpus] Weak: corpus lacks explicit discussion of System 1/System 2 mapping to LLMs; only general robustness and emotional alignment papers present.
- Break condition: If prompts activate irrelevant or contradictory links, the retrieved knowledge becomes unreliable, mirroring human System 1 biases.

### Mechanism 3
- Claim: Projective Explicative Reasoning requires a Faculty of Judgment to validate the mapping between abstract models and concrete entities; LLMs lack this, resulting in Pseudo Rational Reasoning.
- Mechanism: LLMs bypass the judgment step by directly querying their knowledge base, which may produce superficially correct but logically flawed outputs.
- Core assumption: Rational reasoning involves iterative validation between projection and judgment faculties; without judgment, reasoning is incomplete.
- Evidence anchors:
  - [abstract] "they fall short in authentic rational reasoning and have no creative reasoning capabilities, due to the current lack of many analogous AI models such as the Faculty of Judgement"
  - [section 3.2.1] "To engage in authentic Rational Explicative Reasoning, LLM-based AI would need to collaborate with an AI counterpart of the Faculty of Judgement, a development that has not been realized"
  - [section 3.2.2] Example showing GPT-4 producing a logically flawed solution without judgment.
  - [corpus] Weak: corpus does not contain explicit discussion of judgment faculty or pseudo-rational reasoning; only general LLM robustness and alignment papers present.
- Break condition: If LLM outputs are accepted without verification, errors propagate; robust reasoning requires integration of a judgment module.

## Foundational Learning

- Concept: Heidegger's "ready-to-hand" vs. "present-at-hand"
  - Why needed here: These concepts define the two types of relational links that structure human knowledge; LLMs approximate these via statistical co-occurrence.
  - Quick check question: Can you explain how a hammer's utility (ready-to-hand) differs from its physical description (present-at-hand)?

- Concept: Kant's Faculty of Understanding and reasoning structure
  - Why needed here: Provides a baseline for dividing reasoning into inputs (observations, knowledge) and outputs (new understanding), essential for categorizing LLM capabilities.
  - Quick check question: What are the two main inputs to the reasoning system as defined by Kant and adapted here?

- Concept: System 1 vs. System 2 thinking
  - Why needed here: Links psychological models of human reasoning to LLM behaviors, clarifying where LLMs align with or diverge from human cognition.
  - Quick check question: How does Direct Explicative Reasoning in LLMs resemble System 1 thinking in humans?

## Architecture Onboarding

- Component map:
  - Faculty of Verbal Knowledge ↔ LLM transformer layers (attention, embeddings)
  - Faculty of Judgment ↔ (missing) module for validating projections
  - Faculty of Projection ↔ (missing) module for mapping entities to abstractions
  - Faculty of Experiential Knowledge ↔ (missing) embodied AI module
  - Faculty of Observation ↔ (missing) real-world sensory input

- Critical path:
  1. Prompt → LLM → activated relational links → output (Direct Explicative Reasoning)
  2. Prompt → LLM + Judgment module → validated projection → output (Rational Explicative Reasoning, future)

- Design tradeoffs:
  - Current LLM-only: Fast, broad knowledge retrieval but no validation → risk of pseudo-rational outputs
  - LLM + Judgment: Accurate but slower, requires additional module → complexity and latency

- Failure signatures:
  - Logical inconsistencies in outputs → missing judgment
  - Context drop-off with long prompts → working memory limits
  - Factual errors on out-of-distribution queries → distributional shift

- First 3 experiments:
  1. Test LLM on direct knowledge retrieval vs. logical inference tasks to map System 1 vs. System 2 alignment.
  2. Implement a simple judgment module (rule-based or small classifier) to validate LLM projections and measure improvement in correctness.
  3. Augment LLM with retrieval-augmented generation (RAG) to simulate access to Faculty of Observation and assess impact on reasoning accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific cognitive modules or AI counterparts are necessary to achieve Artificial General Intelligence (AGI), and how do they interact with existing LLM capabilities?
- Basis in paper: [explicit]
- Why unresolved: The paper identifies gaps in current LLM capabilities but does not specify the exact cognitive modules required for AGI or how they would integrate with LLMs.
- What evidence would resolve it: Development and integration of AI modules that replicate human cognitive faculties like judgment, analogy, and abstract reasoning, along with empirical studies demonstrating their effectiveness in achieving AGI.

### Open Question 2
- Question: How can the Faculty of Judgement be implemented in AI systems to enable authentic rational reasoning, and what are the potential risks associated with such development?
- Basis in paper: [explicit]
- Why unresolved: The paper highlights the absence of an AI counterpart to the Faculty of Judgement, which is crucial for authentic rational reasoning, but does not provide a clear path for its implementation.
- What evidence would resolve it: Creation of AI models that can evaluate and validate reasoning processes, along with assessments of their impact on decision-making and potential risks.

### Open Question 3
- Question: What are the ethical implications of augmenting LLMs with creative reasoning capabilities, and how can regulations be effectively enforced to mitigate risks?
- Basis in paper: [explicit]
- Why unresolved: The paper suggests the need for regulations but does not detail the specific ethical implications or enforcement mechanisms for creative reasoning AI systems.
- What evidence would resolve it: Analysis of case studies involving creative AI systems, development of ethical frameworks, and implementation of regulatory policies with measurable outcomes.

## Limitations
- The mapping of Heidegger's concepts to LLM architecture is novel but lacks empirical validation; no direct evidence is provided that LLM attention patterns or embeddings correspond to "ready-to-hand" or "present-at-hand" links as defined by Heidegger.
- The analysis assumes that modules like Judgment and Projection are essential for authentic reasoning, but the paper does not demonstrate through experiments how their absence causes specific reasoning failures.
- Logical errors in LLM outputs are attributed to the absence of a Judgment faculty, but alternative explanations (e.g., training data biases, prompt engineering) are not ruled out.

## Confidence
- **High confidence**: LLMs excel at Direct Explicative Reasoning (knowledge retrieval via statistical patterns), supported by clear parallels to System 1 thinking and empirical observations of LLM fluency.
- **Medium confidence**: LLMs are computational analogues of the Faculty of Verbal Knowledge, based on theoretical parallels but lacking direct mechanistic evidence from corpus analysis.
- **Low confidence**: Claims about LLMs' inability to perform authentic rational or creative reasoning due to missing faculties, as these assertions are primarily theoretical and not empirically validated.

## Next Checks
1. **Corpus analysis**: Conduct a systematic review of the literature to find direct evidence (e.g., studies on LLM attention patterns, contextual embeddings) supporting the claim that LLM representations correspond to Heidegger's "ready-to-hand" and "present-at-hand" links.
2. **Empirical experiment**: Design and run experiments where LLMs are tested on tasks requiring judgment or projection (e.g., analogical reasoning, validation of logical inferences) and compare performance with and without external modules simulating these faculties.
3. **Framework robustness**: Apply the reasoning framework to alternative AI architectures (e.g., neuro-symbolic systems, retrieval-augmented models) to assess whether the faculty-based categorization holds and if it predicts performance differences.