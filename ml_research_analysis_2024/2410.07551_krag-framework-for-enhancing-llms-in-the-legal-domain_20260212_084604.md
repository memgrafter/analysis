---
ver: rpa2
title: KRAG Framework for Enhancing LLMs in the Legal Domain
arxiv_id: '2410.07551'
source_url: https://arxiv.org/abs/2410.07551
tags:
- legal
- knowledge
- krag
- llms
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KRAG, a framework to enhance LLM reasoning
  in legal contexts through structured knowledge representation. The proposed Soft
  PROLEG model integrates knowledge graphs to improve comprehension of legal scenarios
  and deliver structured reasoning.
---

# KRAG Framework for Enhancing LLMs in the Legal Domain

## Quick Facts
- arXiv ID: 2410.07551
- Source URL: https://arxiv.org/abs/2410.07551
- Authors: Nguyen Ha Thanh; Ken Satoh
- Reference count: 7
- Primary result: KRAG improves LLM legal reasoning accuracy by up to 20% and stability by up to 20% on Japanese Bar Exam questions

## Executive Summary
This paper introduces KRAG (Knowledge Representation Augmented Generation), a framework that enhances large language models' legal reasoning capabilities by integrating structured knowledge graphs with retrieval-augmented generation. The proposed Soft PROLEG model uses inference graphs to decompose legal conditions into subconditions and exceptions, guiding LLMs through structured reasoning paths. When evaluated on the English version of the Japanese Bar Exam using GPT-3.5 and GPT-4 backbones, KRAG achieved significant improvements in both accuracy (up to 20%) and stability (up to 20%) compared to vanilla models.

## Method Summary
The KRAG framework combines knowledge retrieval with structured generation to address LLM limitations in specialized legal domains. It uses inference graphs that represent legal reasoning as structured relationships between conditions, subconditions, and exceptions. The system retrieves relevant legal texts and applies them to specific queries through graph decomposition guided by the Presupposed Ultimate Fact Theory (JUF). The framework integrates with existing LLMs like GPT-3.5 and GPT-4, using the graph structure to constrain generation and improve consistency across repeated queries.

## Key Results
- KRAG-3.5 and KRAG-4 achieved up to 20% accuracy improvement over vanilla models on Japanese Bar Exam questions
- Stability improvements of up to 20% in consistent answers across repeated queries
- Integration of KRAG with retrieval augmented generation (RAG) further enhances legal text navigation and problem-solving

## Why This Works (Mechanism)

### Mechanism 1
KRAG improves LLM legal reasoning by integrating structured knowledge graphs that decompose legal conditions into subconditions and exceptions. The system uses derivational analogy to match past legal cases (represented as graphs) to new queries, ensuring precise matching of conditions and exceptions. This works when the legal query aligns with existing graph structures, but breaks when novel conditions aren't represented in the Knowledge Set.

### Mechanism 2
KRAG enhances stability by providing structured reasoning paths that constrain the LLM's output space. By forcing the model to follow predefined graph structures (condition/subcondition/exception), the framework reduces variability between repeated queries. This mechanism is effective when the graph structure captures the essential legal reasoning, but may limit handling of nuanced scenarios that don't fit the predefined template.

### Mechanism 3
KRAG improves accuracy by combining retrieval of relevant legal texts with structured application of those texts to specific query contexts. The system retrieves legal articles (R) and related context (Q), then uses graph structure (G) to decompose and apply these texts to legal scenarios. This works well when retrieved texts are unambiguous and align with the graph decomposition, but fails when texts are contradictory or the decomposition doesn't match actual legal reasoning requirements.

## Foundational Learning

- **Knowledge Representation Augmented Generation (KRAG)**: Combines knowledge retrieval with structured generation to address LLM limitations in specialized domains. Quick check: What are the two main components that KRAG integrates to improve LLM performance in legal reasoning?

- **Presupposed Ultimate Fact Theory (JUF)**: Provides theoretical foundation for how legal burdens of proof are allocated, which KRAG models through its graph structure. Quick check: How does the JUF theory guide the allocation of burden of proof in legal disputes?

- **Derivational Analogy**: Cognitive science concept underlying how KRAG matches past legal cases to new queries through graph similarity. Quick check: How does derivational analogy differ from transformational analogy in problem-solving approaches?

## Architecture Onboarding

- **Component map**: Query Submission → Embedding Conversion → Vector Database Search → Context Retrieval → Knowledge Set → LLM → Graph Generation → Response Preparation → Response Delivery

- **Critical path**: Query → Embedding → Vector DB Search → Context Retrieval → Knowledge Set Matching → LLM Processing → Graph Generation → Response
  - Most time-sensitive components are vector database search and LLM inference
  - Graph generation can be parallelized with response preparation

- **Design tradeoffs**: Accuracy vs. Speed (structured reasoning improves accuracy but adds computational overhead), Flexibility vs. Consistency (graph constraints improve consistency but may limit novel scenarios), Knowledge Set Size vs. Query Performance (larger sets improve coverage but slow retrieval)

- **Failure signatures**: Low stability scores indicate ineffective graph constraints, poor accuracy suggests knowledge-query mismatch, slow response times may indicate inefficient graph matching algorithms

- **First 3 experiments**: 1) Measure baseline accuracy and stability of vanilla GPT-3.5 and GPT-4 on Japanese Bar Exam questions, 2) Implement basic graph structure matching without LLM integration to validate knowledge representation approach, 3) Compare KRAG-enhanced GPT-3.5 (KRAG-3.5) against vanilla GPT-3.5 on a subset of bar exam questions to measure improvement in both accuracy and stability

## Open Questions the Paper Calls Out

### Open Question 1
How does the KRAG framework's integration with graph search methodologies compare to current vector-based search techniques in terms of retrieval precision and efficiency? The paper discusses plans to incorporate graph search methodologies in future iterations, suggesting current reliance on vector-based search, but provides no empirical data comparing the two approaches.

### Open Question 2
To what extent can the Soft PROLEG system be scaled to handle more complex legal scenarios without compromising accuracy and interpretability? While the system demonstrates improved accuracy and interpretability, there is no discussion on its performance with increasingly complex legal scenarios or the potential impact on computational resources.

### Open Question 3
How does the integration of structured knowledge graphs with LLMs in the KRAG framework affect the interpretability and trustworthiness of AI-generated legal advice? The paper emphasizes the role of structured knowledge graphs in enhancing interpretability and trustworthiness, but does not provide empirical evidence or user studies on the perceived interpretability and trustworthiness of AI-generated advice.

## Limitations

- Inference graph construction details are underspecified, making reproducibility challenging
- Evaluation limited to English version of Japanese Bar Exam, raising generalizability concerns
- Stability metric measures consistency but not necessarily correctness of outputs

## Confidence

**High Confidence (80-100%)**
- General framework architecture (KRAG + Soft PROLEG) is well-specified and technically feasible
- Use of graph structures to decompose legal conditions and exceptions is theoretically sound
- Accuracy improvements (up to 20%) over vanilla models are likely real

**Medium Confidence (50-80%)**
- Stability improvements (up to 20%) are harder to verify due to unclear methodology
- Generalizability beyond Japanese Bar Exam to other legal domains remains uncertain
- Specific implementation details of inference graph matching algorithm are underspecified

**Low Confidence (0-50%)**
- Exact prompt engineering and methodology for generating legal reasoning paths
- Knowledge set construction process and its coverage of diverse legal scenarios
- Long-term robustness when encountering novel or ambiguous legal cases

## Next Checks

1. Replicate the 20% accuracy improvement claim using Japanese Bar Exam dataset with both GPT-3.5 and GPT-4 backbones, measuring accuracy and stability across multiple runs with different random seeds.

2. Test KRAG framework on a different legal domain (e.g., contract law or criminal law) to assess whether 20% improvement generalizes beyond bar exam format.

3. Conduct knowledge set coverage analysis by measuring percentage of legal concepts in bar exam represented in inference graphs, and test performance degradation when removing key knowledge points.