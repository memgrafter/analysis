---
ver: rpa2
title: '"Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization'
arxiv_id: '2411.02355'
source_url: https://arxiv.org/abs/2411.02355
tags:
- quantization
- arxiv
- w4a16-int
- bf16
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the most comprehensive empirical study to date
  on LLM quantization accuracy-performance trade-offs, evaluating FP8, INT8, and INT4
  formats across academic and real-world benchmarks on the entire Llama-3.1 model
  family (8B, 70B, 405B). Through over 500,000 evaluations, the study finds that FP8
  (W8A8-FP) is essentially lossless, INT8 (W8A8-INT) achieves surprisingly low 1-3%
  accuracy degradation, and INT4 weight-only (W4A16-INT) is more competitive than
  expected, rivaling 8-bit quantization.
---

# "Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization

## Quick Facts
- arXiv ID: 2411.02355
- Source URL: https://arxiv.org/abs/2411.02355
- Authors: Eldar Kurtic; Alexandre Marques; Shubhra Pandit; Mark Kurtz; Dan Alistarh
- Reference count: 20
- One-line primary result: FP8 is essentially lossless, INT8 achieves 1-3% accuracy degradation, and INT4 is surprisingly competitive for LLM quantization.

## Executive Summary
This comprehensive empirical study evaluates quantization accuracy and performance trade-offs for Llama-3.1 models (8B, 70B, 405B) across FP8, INT8, and INT4 formats. Through over 500,000 evaluations spanning academic benchmarks, real-world tasks, and vLLM performance testing on three GPU architectures, the research establishes that FP8 (W8A8-FP) is essentially lossless while INT8 (W8A8-INT) achieves surprisingly low 1-3% accuracy degradation. The study also finds that INT4 weight-only quantization (W4A16-INT) is more competitive than expected, rivaling 8-bit quantization in certain deployment scenarios. Additionally, the research corrects prior claims about AWQ vs GPTQ performance, demonstrating GPTQ's superiority on real-world tasks through MSE-optimal clipping.

## Method Summary
The study evaluates quantization across the entire Llama-3.1 model family using GPTQ and AWQ algorithms with dynamic per-token activation quantization and SmoothQuant for 70B models. Academic benchmarks (Open LLM Leaderboard V1/V2) and real-world tasks (Arena-Hard-Auto-v0.1, HumanEval, HumanEval+, RULER) are used alongside vLLM performance benchmarking on A6000, A100, and H100 GPUs. The research employs specific calibration datasets for different model sizes and measures accuracy recovery, text similarity metrics, and inference performance across synchronous/asynchronous deployment scenarios.

## Key Results
- FP8 (W8A8-FP) quantization is essentially lossless, preserving accuracy while enabling efficient inference
- INT8 (W8A8-INT) achieves surprisingly low 1-3% accuracy degradation across all model sizes
- W4A16-INT weight-only quantization rivals 8-bit quantization in cost-efficiency for synchronous deployments
- GPTQ with MSE-optimal clipping outperforms AWQ on real-world tasks, correcting prior claims
- Larger quantized models closely adhere to full-precision counterparts in text generation while smaller models show moderate structural variability

## Why This Works (Mechanism)

### Mechanism 1
Dynamic per-token activation quantization in W8A8-FP preserves accuracy while enabling efficient inference. Activations are quantized per token using symmetric weight quantization via round-to-nearest assignment, avoiding the need for calibration data and maintaining computational efficiency even at large scales. Core assumption: Dynamic quantization of activations does not introduce significant error accumulation across long sequences. Evidence anchors: "This result is achieved with a simple yet robust approach: dynamic per-token activation quantization combined with symmetric weight quantization via round-to-nearest assignment." Break condition: If dynamic quantization accumulates significant error over long sequences, accuracy would degrade beyond the negligible margin observed.

### Mechanism 2
SmoothQuant mitigates activation quantization challenges by shifting complexity to weights. SmoothQuant precomputes outlier features across the model and shifts some activation complexity onto weights, which are easier to quantize, enabling 8-bit activation quantization without significant accuracy loss. Core assumption: Outlier features are stable across the model and can be effectively precomputed. Evidence anchors: "SmoothQuant improves upon this by noticing that outliers are stable across the model and can be precomputed using a calibration set." Break condition: If outlier features are not stable across the model or cannot be effectively precomputed, SmoothQuant would fail to mitigate activation quantization challenges.

### Mechanism 3
GPTQ with MSE-optimal clipping outperforms AWQ on real-world tasks despite similar academic benchmark performance. GPTQ applies second-order weight adjustments using calibration data with MSE-optimal clipping, while AWQ uses outlier-aware quantization. The difference becomes significant on real-world tasks due to GPTQ's more effective handling of weight quantization errors. Core assumption: MSE-optimal clipping provides better weight quantization than outlier-aware approaches for real-world tasks. Evidence anchors: "We attribute this to three key factors: (1) we use GPTQ with MSE-optimal clipping (the AWQ comparison used absmax); this has no overhead and yields consistently better results." Break condition: If MSE-optimal clipping does not provide better weight quantization than outlier-aware approaches, GPTQ would not outperform AWQ on real-world tasks.

## Foundational Learning

- Concept: Quantization and its impact on model accuracy and performance
  - Why needed here: Understanding quantization is essential for grasping the trade-offs between accuracy, performance, and cost in LLM deployment.
  - Quick check question: What is the primary challenge in balancing efficiency and accuracy when using quantization for LLMs?

- Concept: Dynamic vs static quantization methods
  - Why needed here: The paper contrasts dynamic per-token activation quantization with static approaches, highlighting their respective advantages.
  - Quick check question: How does dynamic per-token activation quantization differ from static activation quantization in terms of calibration requirements and computational efficiency?

- Concept: Hardware-specific quantization formats (FP8, INT8, INT4)
  - Why needed here: The paper evaluates different quantization formats across GPU architectures, requiring understanding of their characteristics and performance implications.
  - Quick check question: What are the key differences between FP8, INT8, and INT4 quantization formats in terms of precision, hardware support, and typical use cases?

## Architecture Onboarding

- Component map: Model quantization (GPTQ/AWQ) → Academic benchmarking (Open LLM Leaderboard) → Real-world benchmarking (Arena-Hard-Auto-v0.1, HumanEval, RULER) → vLLM performance analysis → Deployment recommendations
- Critical path: Quantization → Benchmarking → Performance Analysis → Deployment Recommendations
- Design tradeoffs: Accuracy vs. performance vs. cost, with different quantization formats optimized for different deployment scenarios (synchronous vs. asynchronous)
- Failure signatures: Accuracy degradation beyond acceptable thresholds, performance bottlenecks in specific use cases, or cost inefficiencies in certain hardware configurations
- First 3 experiments:
  1. Implement dynamic per-token activation quantization for a small LLM and measure accuracy on a simple benchmark.
  2. Compare GPTQ with MSE-optimal clipping against AWQ on a real-world task to verify the claimed performance difference.
  3. Benchmark W4A16-INT vs W8A8-INT on different GPU architectures to identify optimal deployment configurations.

## Open Questions the Paper Calls Out

### Open Question 1
How do quantization trade-offs vary across different languages and multilingual benchmarks? The study focused on English-language benchmarks and did not evaluate multilingual performance, leaving uncertainty about whether the observed quantization patterns hold across different languages. Systematic evaluation of quantized models across multilingual benchmarks would reveal whether accuracy recovery rates, text similarity metrics, and performance trade-offs differ significantly from English-only results.

### Open Question 2
What is the impact of quantizing additional model components beyond weights and activations (e.g., KV-cache, embeddings, language modeling head)? The investigation was limited to linear operators and activation functions, without exploring whether compressing these additional components could further improve efficiency without significant accuracy loss. Comparative experiments measuring accuracy, inference speed, and memory usage when quantizing KV-cache, embeddings, and output layers alongside weights and activations would clarify the potential benefits and trade-offs of more comprehensive quantization strategies.

### Open Question 3
How do quantization performance patterns change when using alternative calibration data sources or training data distributions? The study used specific calibration datasets but did not systematically explore how different calibration data distributions affect quantization accuracy and performance trade-offs. Experiments comparing quantization accuracy and performance using various calibration datasets (random vs. curated vs. domain-specific) across different model sizes and tasks would reveal whether calibration data choice significantly impacts quantization effectiveness.

## Limitations

- Calibration data quality and representativeness remain unclear, particularly for the 70B model using "Lee et al. 2023" without detailed specification
- Hardware-specific quantization kernels and firmware optimizations are not fully accounted for, potentially influencing performance results
- Real-world task generalization is limited to specific benchmarks, with uncertain applicability to broader task domains

## Confidence

- **High Confidence**: FP8 being essentially lossless and INT8 achieving 1-3% accuracy degradation are well-supported by extensive evaluation across academic benchmarks and the Llama-3.1 model family.
- **Medium Confidence**: W4A16-INT being more competitive than expected depends on specific deployment scenarios, with performance benefits varying significantly across GPU architectures and use cases.
- **Low Confidence**: GPTQ consistently outperforming AWQ on real-world tasks may not generalize to all real-world scenarios, as the study acknowledges this is a correction to prior claims without comprehensive evidence across diverse task domains.

## Next Checks

1. **Cross-Dataset Calibration Validation**: Re-quantize the 70B model using multiple calibration datasets and measure accuracy degradation across the same academic and real-world benchmarks to verify calibration data quality impact.
2. **Hardware Kernel Dependency Analysis**: Re-run vLLM performance benchmarks on A100 and H100 GPUs with and without specific quantization kernel optimizations enabled to determine hardware-specific optimization effects.
3. **Task Diversity Generalization Test**: Evaluate quantized models on an expanded set of real-world tasks spanning different domains beyond the current benchmark suite to assess GPTQ vs AWQ generalization.