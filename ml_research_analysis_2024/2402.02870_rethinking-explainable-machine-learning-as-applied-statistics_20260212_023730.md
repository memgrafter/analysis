---
ver: rpa2
title: Rethinking Explainable Machine Learning as Applied Statistics
arxiv_id: '2402.02870'
source_url: https://arxiv.org/abs/2402.02870
tags:
- statistics
- learning
- machine
- explainable
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper argues that explainable machine learning should
  be understood as applied statistics for high-dimensional functions. Post-hoc explanation
  algorithms are statistics of functions, taking complex learned models and mapping
  them to simpler interpretable forms.
---

# Rethinking Explainable Machine Learning as Applied Statistics

## Quick Facts
- arXiv ID: 2402.02870
- Source URL: https://arxiv.org/abs/2402.02870
- Authors: Sebastian Bordt; Eric Raidl; Ulrike von Luxburg
- Reference count: 27
- Primary result: Post-hoc explanation algorithms are statistics of functions, analogous to traditional statistics of probability distributions

## Executive Summary
This position paper reframes explainable machine learning as applied statistics for high-dimensional functions. The authors argue that post-hoc explanation algorithms are fundamentally statistics of the learned functions, mapping complex models to simpler interpretable forms. They identify a critical gap between the mathematical formalism of many explanation methods and their intuitive interpretations, arguing that this disconnect leads to frequent misinterpretations. The paper draws lessons from traditional statistics, emphasizing the need for clear interpretations, appropriate benchmarks, and recognizing that working with explanations requires expertise similar to working with statistical methods.

## Method Summary
The paper establishes a formal framework by defining statistics of functions as functionals that map complex learned functions to interpretable outputs. It distinguishes between the mathematical form of explanation algorithms and their interpretations - the connection to intuitive human concepts. The authors argue that many post-hoc methods lack clear interpretations because they cannot be connected to intuitive concepts through either convergence arguments or derivation arguments. They propose that explanations should be designed to answer specific, well-defined questions rather than serving as general-purpose tools.

## Key Results
- Post-hoc explanation algorithms are fundamentally statistics of functions, analogous to traditional statistics of probability distributions
- Many explanation algorithms lack clear interpretations - the connection between their mathematical form and intuitive human concepts
- Working with explanations requires expertise similar to working with traditional statistics
- Explanations should be designed to answer specific questions rather than serving as general-purpose tools

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-hoc explanation algorithms are fundamentally statistics of functions, analogous to traditional statistics of probability distributions
- Mechanism: By treating explanations as functionals that map complex learned functions to simpler interpretable forms, we establish a direct parallel to how statistics map complex data distributions to interpretable quantities like means and p-values
- Core assumption: The mathematical structure of explanation algorithms (functionals) is sufficiently similar to traditional statistical measures to warrant direct analogy
- Evidence anchors:
  - [abstract] "explanations are statistics of high-dimensional functions"
  - [section] "Definition 2.3 (Statistic of a Function)" introduces the formal framework
  - [corpus] Weak - no direct statistical analogy papers found, but related papers discuss bridging statistics and XAI
- Break condition: If explanation algorithms cannot be formally expressed as functionals, or if their output cannot be meaningfully compared to traditional statistical quantities

### Mechanism 2
- Claim: The interpretation problem in explainable ML stems from treating explanation algorithms as black boxes without understanding their relationship to intuitive human concepts
- Mechanism: By recognizing that statistics (including explanations) must have an interpretation - a clear relationship to intuitive concepts - we can identify why many post-hoc methods fail and how to design better ones
- Core assumption: Intuitive human concepts can be formalized through careful conceptual analysis, similar to how traditional statistics formalized concepts like "average" or "significant difference"
- Evidence anchors:
  - [abstract] "many explanation algorithms lack a clear interpretation"
  - [section] "Definition 4.1 (Interpretation)" and discussion of conceptual analysis
  - [corpus] Moderate - papers like "Explainable AI needs formalization" support this view
- Break condition: If intuitive human concepts cannot be formalized, or if the formalization process proves impossible for most explanation scenarios

### Mechanism 3
- Claim: Expertise is required to properly work with explanations, similar to how expertise is required for traditional statistics
- Mechanism: The technical complexity and nuanced interpretation requirements of explanation algorithms mean they are primarily useful tools for experts, not end users
- Core assumption: The technical properties and limitations of explanation algorithms are sufficiently complex to require specialized training for proper use
- Evidence anchors:
  - [section] "Working with explanations requires expertise" and discussion of technical properties
  - [abstract] "statistics without interpretation lead to errors in interpretation"
  - [corpus] Weak - no direct evidence about expertise requirements found
- Break condition: If explanation algorithms can be made sufficiently simple for end-user interpretation, or if automated interpretation tools become available

## Foundational Learning

- Concept: Statistics of functions as functionals
  - Why needed here: Understanding that explanations are functionals mapping functions to simpler objects is the foundation for the entire analogy with applied statistics
  - Quick check question: Can you express SHAP values as a functional F(f, D, x) that maps a function to a vector of feature attributions?

- Concept: Interpretation vs mathematical form
  - Why needed here: Distinguishing between the mathematical properties of an explanation algorithm and its interpretation is crucial for understanding why many methods fail in practice
  - Quick check question: Given a feature attribution method, can you articulate what intuitive human concept it formalizes, or explain why it cannot formalize any intuitive concept?

- Concept: Conceptual analysis and formalization
  - Why needed here: Understanding how intuitive concepts are formalized through convergence arguments and derivations explains why some statistics have clear interpretations while others do not
  - Quick check question: Can you explain the difference between a convergence argument and an argument by derivation in the context of formalizing an intuitive concept?

## Architecture Onboarding

- Component map:
  - Function space F(Rd): The domain of all possible learned functions
  - Probability space P(Rd): The input data distribution
  - Explanation functional F: Maps functions to interpretable outputs
  - Interpretation layer: Connects mathematical outputs to intuitive concepts
  - Expertise interface: Handles the technical complexity for proper use

- Critical path:
  1. Define the function and data distribution
  2. Select appropriate explanation functional
  3. Compute explanation output
  4. Apply interpretation layer to connect to intuitive concepts
  5. Validate through expertise interface

- Design tradeoffs:
  - Mathematical simplicity vs interpretability: More complex functionals may capture nuanced behavior but be harder to interpret
  - Generality vs specificity: General-purpose explanations may lack clear interpretations, while specific ones may only answer narrow questions
  - Automation vs expertise: More automated systems reduce expertise requirements but may sacrifice accuracy or reliability

- Failure signatures:
  - No clear interpretation: The explanation functional cannot be connected to any intuitive human concept
  - Multiple conflicting interpretations: Different functionals purport to explain the same concept but give different answers
  - Expert-only usability: The explanation requires extensive technical knowledge to interpret correctly
  - Benchmark dependency: The explanation's validity depends entirely on benchmark performance rather than theoretical grounding

- First 3 experiments:
  1. Take a simple interpretable model (GAM) and compute both the model output and an explanation functional. Verify that the explanation matches the model's inherent interpretability.
  2. Apply SHAP values to a black-box model and attempt to connect the output to an intuitive concept like "feature importance." Document any gaps in interpretation.
  3. Design a new explanation functional for a specific intuitive question (e.g., "what would need to change to get a different prediction?") and test it on multiple models to verify consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which specific intuitive human questions can be reliably answered by post-hoc explanation methods in explainable machine learning?
- Basis in paper: [explicit] The paper emphasizes that explanations should be designed to answer specific questions and argues that many current methods lack clear interpretations connecting their mathematical form to intuitive concepts.
- Why unresolved: While the paper identifies this as a key issue, it does not provide a comprehensive framework for determining which intuitive questions are formalizable or which methods successfully address them.
- What evidence would resolve it: Empirical studies demonstrating that specific explanation methods consistently provide accurate answers to well-defined human questions, along with formal proofs that certain methods satisfy convergence arguments or derivation arguments for specific intuitive concepts.

### Open Question 2
- Question: Can the analogy between explainable machine learning and applied statistics be extended to fundamentally change how we evaluate and benchmark explanation algorithms?
- Basis in paper: [explicit] The paper argues that explainable machine learning should be understood as applied statistics for high-dimensional functions, suggesting that research practices should draw from statistical traditions.
- Why unresolved: While the paper makes this analogy, it doesn't provide concrete alternative evaluation frameworks that would replace current benchmarking approaches or demonstrate their superiority.
- What evidence would resolve it: Development and validation of evaluation frameworks for explanation methods based on statistical principles rather than task-specific performance metrics, showing improved reliability and interpretability of results.

### Open Question 3
- Question: What is the relationship between the complexity of explanation methods and the expertise required to correctly interpret them?
- Basis in paper: [explicit] The paper argues that working with explanations requires expertise similar to working with traditional statistics, noting that many popular methods have complex technical properties.
- Why unresolved: The paper establishes this relationship but doesn't quantify it or identify specific thresholds where explanation methods become too complex for non-expert interpretation.
- What evidence would resolve it: Systematic studies mapping explanation method complexity to interpretation accuracy across different user expertise levels, establishing guidelines for when expert interpretation is necessary versus when explanations are accessible to general users.

## Limitations

- The core analogy between post-hoc explanations and statistics of functions remains largely theoretical without concrete empirical validation
- The claim that expertise is required for proper use of explanations lacks supporting evidence from real-world applications or user studies
- The framework doesn't provide specific criteria for determining which intuitive human questions can be formalized and answered by explanation methods

## Confidence

- **High**: The mathematical framework for defining statistics of functions (Section 2) is rigorous and well-established
- **Medium**: The analogy between post-hoc explanations and applied statistics is compelling but untested empirically
- **Low**: The claim about expertise requirements is largely speculative without empirical validation

## Next Checks

1. Conduct a user study comparing expert vs non-expert interpretation of common explanation methods to test the expertise requirement claim
2. Implement the functional framework on multiple explanation algorithms and attempt to formalize their intuitive interpretations to validate the interpretation problem
3. Design benchmark datasets where different explanation methods with the same mathematical form yield different intuitive interpretations, testing whether the formalism captures real interpretive differences