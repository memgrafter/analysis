---
ver: rpa2
title: 'MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT'
arxiv_id: '2402.16840'
source_url: https://arxiv.org/abs/2402.16840
tags:
- mobillama
- arxiv
- feat
- training
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MobiLlama, a 0.5B parameter Small Language
  Model (SLM) designed for resource-constrained environments. The core innovation
  is a shared feed-forward network (FFN) across transformer layers, reducing parameters
  by 60% compared to standard designs while maintaining model capacity.
---

# MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT

## Quick Facts
- arXiv ID: 2402.16840
- Source URL: https://arxiv.org/abs/2402.16840
- Reference count: 25
- Primary result: 0.5B parameter SLM with shared FFN achieves 2.4% better average performance than existing 0.5B models

## Executive Summary
MobiLlama is a 0.5B parameter Small Language Model designed for resource-constrained environments. Its core innovation is sharing a single feed-forward network (FFN) across all transformer layers, reducing parameters by 60% while maintaining model capacity. The model is trained on 1.2T tokens from the LLM360 Amber dataset and achieves state-of-the-art performance among sub-1B parameter models. The paper also introduces a 0.8B variant and a multimodal extension, MobiLlama-V, demonstrating strong efficiency in both pre-training and on-device deployment. All training data, code, model weights, and over 300 checkpoints are publicly released for full transparency.

## Method Summary
MobiLlama introduces a shared FFN design where a single MLP is used across all transformer layers, reducing parameter count by 60% compared to standard designs. The model is initialized from a 1.2B base configuration, then parameter sharing is applied to create the 0.5B version. Training uses the LLM360 Amber dataset (1.2T tokens) with a 22-layer transformer architecture, hidden size of 2048, and intermediate size of 5632. A 0.8B variant is created by widening the shared FFN to hidden size 2532 and intermediate size 11,080. The model uses Rotary Positional Embeddings (RoPE) and Swish activation functions.

## Key Results
- 0.5B MobiLlama achieves 2.4% better average performance than existing 0.5B models across nine benchmarks
- Parameter reduction of 60% compared to standard transformer designs
- 0.8B variant maintains efficiency while scaling to larger hidden dimensions
- Strong performance on both language and multimodal tasks (MobiLlama-V)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sharing feed-forward networks (FFNs) across transformer layers significantly reduces total parameters while maintaining model capacity
- Mechanism: Standard transformers have dedicated MLPs per layer (65% of parameters). Sharing a single MLP across all layers enables deeper/wider architecture without increasing training cost
- Core assumption: Shared MLP provides sufficient representational diversity when model is initialized from larger base and fine-tuned
- Evidence anchors: Abstract states MobiLlama "maintains efficiency" with shared FFN; paper describes FFN accounting for 65% of parameters and proposes sharing scheme
- Break condition: If shared MLP cannot capture layer-specific transformations, performance degrades significantly

### Mechanism 2
- Claim: Initializing from larger model and applying parameter sharing reduces pre-training cost while maintaining accuracy
- Mechanism: Start with 1.2B base design, apply parameter sharing to reduce to 0.5B, inheriting learned representations
- Core assumption: Knowledge from larger base transfers effectively to smaller shared-architecture model
- Evidence anchors: Abstract mentions initiating from larger model with careful parameter sharing; paper describes constructing SLM by initiating from large-base then reducing size
- Break condition: If initialization doesn't transfer well, shared 0.5B model underperforms models trained from scratch

### Mechanism 3
- Claim: Wider shared FFN enables scaling to 0.8B without proportional parameter increase
- Mechanism: 0.8B variant increases hidden dimension to 2532 and intermediate size to 11,080 while maintaining single shared MLP
- Core assumption: Shared MLP scales to larger hidden dimensions without efficiency loss
- Evidence anchors: Abstract mentions 0.8B SLM achieving top performance; paper describes widening shared FFN while keeping configuration same
- Break condition: If wider shared MLP becomes bottleneck, scaling may not yield proportional performance gains

## Foundational Learning

- Concept: Transformer architecture and role of FFN layers
  - Why needed here: Understanding FFN contribution to model capacity and parameter count is essential to grasp why sharing them is effective
  - Quick check question: What percentage of parameters in standard transformer block are typically in FFN layers?

- Concept: Parameter sharing in neural networks
  - Why needed here: Core innovation relies on sharing weights across layers; knowing when/why this works is key
  - Quick check question: In what scenarios does parameter sharing in neural networks improve efficiency without hurting performance?

- Concept: Pre-training data scale and model performance
  - Why needed here: MobiLlama uses 1.2T tokens; understanding data size vs performance relationship is important
  - Quick check question: How does increasing pre-training data size affect performance of small vs large language models?

## Architecture Onboarding

- Component map: Token IDs → Embedding → RoPE → LayerNorm → Self-Attention → Add & Norm → Shared MLP → Add & Norm → Next layer (repeated 22×) → Output projection
- Critical path: Token → Embedding → RoPE → LayerNorm → Self-Attention → Add & Norm → Shared MLP → Add & Norm → Next layer (repeated 22×) → Output projection
- Design tradeoffs:
  - Sharing MLP reduces parameters by 60% but may limit layer-specific feature extraction
  - Wider hidden size (2048) compensates for shared capacity
  - 22 layers provide depth without proportional parameter growth
- Failure signatures:
  - If MLP sharing is too aggressive, model may fail to learn hierarchical representations
  - If hidden size is too small, model may underfit
  - If pre-training data is insufficient, model may not converge properly
- First 3 experiments:
  1. Train baseline 0.5B model with dedicated MLPs per layer on 100B tokens and compare average benchmark score to MobiLlama
  2. Train MobiLlama 0.5B from scratch (without initializing from large-base) to test if initialization is critical
  3. Train 0.8B model with dedicated MLPs to compare parameter efficiency vs shared MLP design

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MobiLlama's shared FFN design impact its ability to learn hierarchical representations compared to traditional dedicated FFN designs?
- Basis in paper: Inferred from discussion on baseline models struggling to balance accuracy and efficiency, and MobiLlama's approach to alleviate redundancy
- Why unresolved: Paper doesn't provide detailed analysis of how shared FFN design affects capacity to learn hierarchical representations
- What evidence would resolve it: Experiments comparing MobiLlama's performance on tasks requiring hierarchical understanding against models with dedicated MLPs

### Open Question 2
- Question: What is optimal number of transformer layers for MobiLlama to achieve best balance between computational efficiency and model performance?
- Basis in paper: Paper mentions reducing hidden layers affects depth and ability to learn hierarchical representations
- Why unresolved: Paper doesn't explore impact of varying number of layers on MobiLlama's performance
- What evidence would resolve it: Conducting experiments with different numbers of transformer layers while keeping other hyperparameters constant

### Open Question 3
- Question: How does MobiLlama's performance on multimodal tasks compare to larger, more resource-intensive models like GPT-4 or CLIP?
- Basis in paper: Paper introduces multimodal extension (MobiLlama-V) and demonstrates capabilities but doesn't compare to state-of-the-art multimodal models
- Why unresolved: Paper doesn't provide comparison with larger multimodal models
- What evidence would resolve it: Evaluating MobiLlama-V on same benchmarks used to assess larger multimodal models

### Open Question 4
- Question: How does MobiLlama's parameter sharing scheme affect its ability to generalize to unseen tasks or domains?
- Basis in paper: Paper highlights parameter sharing as key innovation but doesn't explore impact on generalization capabilities
- Why unresolved: Paper doesn't investigate how shared FFN design influences ability to adapt to new tasks or domains
- What evidence would resolve it: Testing MobiLlama's performance on few-shot learning tasks or domain adaptation scenarios

### Open Question 5
- Question: What are potential biases and misrepresentations in MobiLlama's training data, and how do they affect model's outputs?
- Basis in paper: Paper mentions use of LLM360 Amber dataset but doesn't discuss potential biases or misrepresentations
- Why unresolved: Paper doesn't address issue of biases in training data
- What evidence would resolve it: Conducting thorough analysis of training data for biases and performing fairness evaluations on MobiLlama's outputs

## Limitations

- Lack of ablation studies showing performance degradation when varying degree of parameter sharing or training from scratch without initialization
- Statistical significance of 2.4% average improvement across nine benchmarks is not established
- Multimodal extension (MobiLlama-V) and chat variants lack detailed performance metrics or training procedures

## Confidence

- High Confidence: Architectural design of shared FFNs and parameter reduction calculation are well-specified and verifiable
- Medium Confidence: 2.4% average performance improvement claim, as it's based on benchmark results but lacks statistical significance testing
- Low Confidence: Assertion that knowledge effectively transfers from larger 1.2B base model to smaller shared-architecture model, as no ablation studies compare MobiLlama trained from scratch versus with initialization

## Next Checks

1. **Ablation Study**: Train MobiLlama variants with different degrees of parameter sharing (e.g., sharing only every 2nd layer vs. all layers) to determine optimal sharing strategy and identify performance degradation thresholds
2. **Statistical Significance**: Perform paired t-tests or bootstrap confidence intervals on benchmark score differences between MobiLlama and baseline models to establish whether 2.4% improvement is statistically significant
3. **From-Scratch Training**: Train a MobiLlama 0.5B model from scratch without initialization from larger base model to validate whether initialization step is truly necessary for maintaining accuracy