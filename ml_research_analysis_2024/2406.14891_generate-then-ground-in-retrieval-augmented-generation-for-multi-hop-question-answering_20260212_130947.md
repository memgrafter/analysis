---
ver: rpa2
title: Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question
  Answering
arxiv_id: '2406.14891'
source_url: https://arxiv.org/abs/2406.14891
tags:
- answer
- llms
- question
- documents
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of Multi-Hop Question Answering
  (MHQA) tasks, where complex questions require multi-step reasoning using intensive
  knowledge. The authors propose a novel "generate-then-ground" (GenGround) framework
  that leverages the parametric knowledge of large language models (LLMs) and external
  documents.
---

# Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering

## Quick Facts
- arXiv ID: 2406.14891
- Source URL: https://arxiv.org/abs/2406.14891
- Authors: Zhengliang Shi; Weiwei Sun; Shen Gao; Pengjie Ren; Zhumin Chen; Zhaochun Ren
- Reference count: 16
- Primary result: Novel GenGround framework achieves SOTA performance on multi-hop QA tasks by alternating answer deduction and instructional grounding phases

## Executive Summary
This paper addresses the challenge of Multi-Hop Question Answering (MHQA) by proposing a novel "generate-then-ground" (GenGround) framework that leverages both the parametric knowledge of large language models (LLMs) and external retrieved documents. The framework alternates between two phases: answer deduction, where LLMs generate simpler sub-questions and produce immediate answers, and instructional knowledge grounding, where LLMs cite evidence from retrieved documents to revise answers. The authors also introduce a batch grounding strategy for efficiency and an instructional grounding distillation method to generalize the approach to smaller models. Extensive experiments on four datasets demonstrate the framework's superiority over existing methods.

## Method Summary
The GenGround framework addresses MHQA by decomposing complex questions into simpler sub-questions through an alternating process of answer deduction and instructional knowledge grounding. First, LLMs use their parametric knowledge to generate immediate answers for formulated sub-questions. Then, retrieved documents are used to revise these answers through evidence citation. The process repeats until a final answer is derived. A batch grounding strategy processes retrieved documents in batches to improve efficiency, stopping when relevant evidence is found. To scale the method to smaller models, an instructional grounding distillation technique transfers the grounding capability from large LLMs to smaller models through synthetic data generation and instruction tuning.

## Key Results
- GenGround framework achieves state-of-the-art performance across four MHQA benchmarks (HotpotQA, MuSiQue, 2WikiMultiHopQA, StrategyQA)
- Batch grounding strategy reduces token consumption while maintaining accuracy
- Instructional grounding distillation successfully transfers capabilities to smaller 1.3B parameter models
- The alternating mechanism between deduction and grounding phases outperforms retrieve-then-read approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Alternating between answer deduction and instructional knowledge grounding reduces reliance on imperfect retrievers.
- **Mechanism:** The framework first uses LLMs to generate answers using parametric knowledge, then selectively incorporates evidence from retrieved documents to revise answers, mitigating the impact of irrelevant or noisy documents.
- **Core assumption:** LLMs possess sufficient parametric knowledge to generate reasonable initial answers for at least some sub-questions.
- **Evidence anchors:**
  - [abstract] "Our framework enables LLMs to alternate two phases until the final answer is derived: (1) formulate a simpler, single-hop question and directly generate the answer; (2) ground the question-answer pair in retrieved documents, amending any wrong predictions in the answer."
  - [section] "The LLMs then produce an immediate answer for each sub-question... we guide the LLMs to revise the answer in the grounding phase, using documents retrieved from an external corpus like Wikipedia."
- **Break condition:** If LLMs cannot generate any reasonable initial answers, the framework loses its advantage over retrieve-then-read approaches.

### Mechanism 2
- **Claim:** Batch grounding strategy improves efficiency by reducing token consumption compared to sequentially processing all retrieved documents.
- **Mechanism:** Instead of processing all retrieved documents at once, the framework processes them in batches, stopping when relevant evidence is found.
- **Core assumption:** Relevant evidence, if present, is likely to be found within the first few retrieved documents.
- **Evidence anchors:**
  - [abstract] "We also propose a batch grounding strategy for efficient document use."
  - [section] "We first utilize the LLMs to revise the generated answer ai using the (1, b)-th documents. If relevant evidence can be cited to revise the answer, we end our grounding phase for the current iteration and move to the next iteration."
- **Break condition:** If relevant evidence is consistently found only in later documents, the batch approach may miss it and fall back to using the initial answer.

### Mechanism 3
- **Claim:** Instructional grounding distillation transfers the grounding capability from large LLMs to smaller models.
- **Mechanism:** The framework uses a large LLM (e.g., ChatGPT) to generate a revision trajectory for single-hop questions, then trains a smaller model to mimic this behavior.
- **Core assumption:** The revision trajectory generated by large LLMs can be effectively distilled into smaller models.
- **Evidence anchors:**
  - [abstract] "We also propose an instructional grounding distillation method to generalize our method into smaller models."
  - [section] "We use 50k single-hop questions... each with ground-truth and noise documents. We guide ChatGPT to generate and adjust an answer for each question, then distill ChatGPT's process into a student model using instruction tuning."
- **Break condition:** If the distilled smaller model cannot effectively learn the grounding behavior, performance will degrade.

## Foundational Learning

- **Concept:** Multi-hop reasoning
  - Why needed here: The framework addresses multi-hop question answering by decomposing complex questions into simpler sub-questions.
  - Quick check question: Can you explain why answering a complex question might require multiple reasoning steps rather than a single inference?

- **Concept:** Retrieval-augmented generation (RAG)
  - Why needed here: The framework builds on RAG by adding an answer deduction phase before grounding, which is a novel contribution.
  - Quick check question: What is the main limitation of traditional RAG approaches that this framework aims to address?

- **Concept:** Instruction tuning
  - Why needed here: The framework uses instruction tuning to distill the grounding capability from large LLMs to smaller models.
  - Quick check question: How does instruction tuning differ from standard supervised fine-tuning, and why might it be more effective for this task?

## Architecture Onboarding

- **Component map:** Question → Answer Deduction → Retrieval → Instructional Grounding → Revised Answer → Next Iteration (or Finish)

- **Critical path:** Question → Answer Deduction → Retrieval → Instructional Grounding → Revised Answer → Next Iteration (or Finish)

- **Design tradeoffs:**
  - Token efficiency vs. completeness: Batch grounding saves tokens but may miss relevant evidence in later documents
  - Accuracy vs. latency: Using larger LLMs improves accuracy but increases inference cost
  - Distillation quality vs. data requirements: More synthetic data improves distillation but increases training time

- **Failure signatures:**
  - High failure rate in answer deduction phase: LLMs lack sufficient parametric knowledge
  - High error rate in grounding phase: Retrieved documents contain misleading information
  - Distillation performance degradation: Synthetic data generation or training process issues

- **First 3 experiments:**
  1. Ablation study removing the answer deduction phase to measure its contribution
  2. Comparison of different batch sizes to optimize token efficiency
  3. Evaluation of distillation quality by comparing performance with and without distillation on smaller models

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the generate-then-ground framework handle questions that cannot be decomposed into simpler sub-questions, and what is the impact on performance?
- **Basis in paper:** [inferred] The paper discusses decomposing complex questions into simpler ones, but does not explicitly address cases where this is not possible.
- **Why unresolved:** The paper focuses on the effectiveness of the framework when decomposition is feasible, but does not explore scenarios where questions cannot be broken down.
- **What evidence would resolve it:** Experimental results comparing performance on questions that can and cannot be decomposed, along with qualitative analysis of the framework's behavior in such cases.

### Open Question 2
- **Question:** What are the limitations of the batch grounding strategy in terms of computational efficiency and accuracy, especially for longer documents?
- **Basis in paper:** [explicit] The paper mentions the batch grounding strategy but does not provide detailed analysis of its efficiency or limitations.
- **Why unresolved:** While the strategy is introduced, its impact on computational resources and accuracy is not thoroughly examined.
- **What evidence would resolve it:** Performance metrics comparing batch grounding with other strategies, along with analysis of computational costs and accuracy trade-offs.

### Open Question 3
- **Question:** How does the instructional grounding distillation method perform with different sizes of synthetic datasets, and what is the optimal dataset size for training?
- **Basis in paper:** [explicit] The paper discusses using a synthetic dataset for distillation but does not explore the impact of dataset size.
- **Why unresolved:** The effectiveness of the distillation method is shown, but the influence of dataset size on performance is not investigated.
- **What evidence would resolve it:** Experimental results showing performance with varying dataset sizes, along with analysis of the relationship between dataset size and model accuracy.

## Limitations

- The framework's performance depends heavily on LLMs' ability to generate reasonable initial answers in the deduction phase
- Batch grounding may miss relevant evidence if it's located in later-retrieved documents
- The effectiveness of instructional grounding distillation may not generalize to all model architectures and sizes

## Confidence

- **High Confidence:** The core alternating mechanism between answer deduction and grounding phases is well-supported by the paper's experiments and ablation studies.
- **Medium Confidence:** The efficiency gains from batch grounding are demonstrated but not extensively validated across different scenarios.
- **Medium Confidence:** The effectiveness of instructional grounding distillation for smaller models is shown but may not generalize to all model architectures.

## Next Checks

1. **Ablation Study on Deduction Phase:** Remove the answer deduction phase entirely and compare performance to validate whether this phase contributes meaningfully beyond standard RAG approaches.

2. **Batch Size Sensitivity Analysis:** Systematically test different batch sizes (1, 3, 5, 10 documents) to determine the optimal balance between efficiency and completeness of evidence retrieval.

3. **Cross-Model Distillation Evaluation:** Apply the instructional grounding distillation method to a wider range of model sizes (e.g., LLaMA-7B, LLaMA-13B) to assess generalization beyond the reported results with 1.3B parameter models.