---
ver: rpa2
title: 'DIPPER: Direct Preference Optimization to Accelerate Primitive-Enabled Hierarchical
  Reinforcement Learning'
arxiv_id: '2406.10892'
source_url: https://arxiv.org/abs/2406.10892
tags:
- policy
- learning
- dipper
- preference
- hierarchical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DIPPER introduces a hierarchical approach for learning robotics
  policies from human preference data, combining direct preference optimization (DPO)
  for higher-level policy learning with reinforcement learning for lower-level policy
  learning. The method addresses two key challenges in hierarchical reinforcement
  learning: non-stationarity due to changing lower-level policies and infeasible subgoal
  generation by higher-level policies.'
---

# DIPPER: Direct Preference Optimization to Accelerate Primitive-Enabled Hierarchical Reinforcement Learning

## Quick Facts
- arXiv ID: 2406.10892
- Source URL: https://arxiv.org/abs/2406.10892
- Reference count: 40
- Primary result: Achieves >40% success rates on complex robotic control tasks using human preference data

## Executive Summary
DIPPER introduces a hierarchical approach for learning robotics policies from human preference data, combining direct preference optimization (DPO) for higher-level policy learning with reinforcement learning for lower-level policy learning. The method addresses two key challenges in hierarchical reinforcement learning: non-stationarity due to changing lower-level policies and infeasible subgoal generation by higher-level policies. DIPPER introduces a novel primitive-informed regularization scheme derived from a bi-level optimization formulation of the HRL problem, which conditions the higher-level policy to generate feasible subgoals. Experimental results demonstrate that DIPPER achieves greater than 40% success rates on complex robotic control tasks where baseline methods typically fail, significantly outperforming both hierarchical and non-hierarchical alternatives across maze navigation, pick and place, push, and kitchen manipulation tasks.

## Method Summary
DIPPER uses a hierarchical structure where a higher-level policy generates subgoals and a lower-level policy executes primitive actions to achieve those subgoals. The higher-level policy is trained using Direct Preference Optimization (DPO) on human preference data, which directly optimizes the policy without learning an intermediate reward model. The lower-level policy is trained using standard reinforcement learning (SAC). To address the challenge of infeasible subgoal generation, DIPPER introduces a primitive-informed reference policy that conditions subgoal generation on the achievability of subgoals by the lower-level primitive. This reference policy is derived from a bi-level optimization formulation and uses the lower-level value function to estimate subgoal feasibility.

## Key Results
- Achieves greater than 40% success rates on complex robotic control tasks where baseline methods typically fail
- Outperforms hierarchical baselines (HAC, DAC, RAPS) and non-hierarchical baselines (DPO-FLAT, FLAT) across four robotic tasks
- Demonstrates improved computational efficiency compared to RLHF-based approaches by eliminating the reward modeling step

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DIPPER mitigates non-stationarity in hierarchical reinforcement learning by decoupling the higher-level policy optimization from the changing lower-level primitive behavior.
- Mechanism: The paper introduces a bi-level optimization formulation where the higher-level policy is optimized independently using a KL-regularized maximum likelihood objective derived from human preference data, while the lower-level policy is optimized using standard reinforcement learning. This separation prevents the higher-level policy from being affected by the non-stationary behavior of the evolving lower-level primitive.
- Core assumption: The human preference data provides a stable signal that can guide the higher-level policy regardless of how the lower-level policy changes during training.
- Evidence anchors:
  - [abstract] "DIPPER enjoys improved computational efficiency due to its use of direct preference optimization instead of standard preference-based approaches such as reinforcement learning from human feedback, while it also mitigates the well-known hierarchical reinforcement learning issues of non-stationarity and infeasible subgoal generation"
  - [section] "Concretely, due to continuously changing lower-level primitive behavior, the higher-level replay buffer experience is rendered obsolete. Some prior works deal with this issue by either simulating an optimal lower-level primitive [24], or relabeling replay buffer transitions using a maximum likelihood-based approach [26, 36]. In contrast, we deal with non-stationarity by using preference-based learning [6, 21]."

### Mechanism 2
- Claim: DIPPER addresses infeasible subgoal generation by introducing a primitive-informed reference policy that conditions the higher-level policy to predict feasible subgoals.
- Mechanism: The paper derives a reference policy formulation based on the difference between the lower-level value function and its optimal value. This reference policy assigns higher probability to subgoals where the lower-level policy's value function is close to optimal, effectively ensuring that the higher-level policy only predicts subgoals that the lower-level primitive can realistically achieve.
- Core assumption: The lower-level value function provides a reliable estimate of subgoal achievability, and this estimate remains meaningful even as the lower-level policy improves during training.
- Evidence anchors:
  - [abstract] "DIPPER introduces a novel primitive-informed regularization scheme derived from a bi-level optimization formulation of the HRL problem, which conditions the higher-level policy to generate feasible subgoals"
  - [section] "For a particular subgoal gt, VπL(st, gt) provides an estimate of the achievability of subgoal gt from current State st, since a high value of VπL(st, gt) implies that the lower level expects to achieve high reward for subgoal gt. Since πref assigns high probability to subgoals with large VπL(st, gt), πref produces achievable subgoals"

### Mechanism 3
- Claim: DIPPER achieves computational efficiency by replacing the three-tier RLHF+HRL approach with a direct preference optimization framework that avoids learning a separate reward model.
- Mechanism: Instead of first learning a reward model from human preferences and then using reinforcement learning to optimize policies based on that reward, DIPPER directly optimizes the higher-level policy using a KL-regularized maximum likelihood objective that incorporates the preference data. This eliminates the need for an intermediate reward modeling step and the associated computational overhead.
- Core assumption: The direct optimization of the policy using preference data can achieve comparable or better performance than the two-step approach of reward modeling followed by RL.
- Evidence anchors:
  - [abstract] "DIPPER enjoys improved computational efficiency due to its use of direct preference optimization instead of standard preference-based approaches such as reinforcement learning from human feedback"
  - [section] "Our key idea... we introduce a DPO-based approach to directly learn higher-level policies from preference data, replacing the two-tier RLHF component in the scheme described above with a simpler, more efficient single-tier approach"

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO provides the computational efficiency and direct policy optimization framework that makes DIPPER practical for complex robotics tasks, avoiding the overhead of reward modeling and separate RL training.
  - Quick check question: How does DPO differ from standard reinforcement learning from human feedback in terms of the optimization objective and computational steps?

- Concept: Hierarchical Reinforcement Learning (HRL)
  - Why needed here: HRL is the fundamental framework for decomposing complex robotics tasks into manageable sub-tasks, which is essential for achieving the long-horizon control required in the navigation and manipulation tasks DIPPER addresses.
  - Quick check question: What are the two main challenges in off-policy HRL that DIPPER specifically addresses, and how do these challenges typically manifest in practice?

- Concept: Bi-level optimization
  - Why needed here: The bi-level optimization formulation provides the theoretical foundation for deriving the primitive-informed reference policy, which is the key mechanism for addressing infeasible subgoal generation in DIPPER.
  - Quick check question: In the bi-level optimization formulation for HRL, what is the relationship between the optimal lower-level policy and the higher-level policy's objective?

## Architecture Onboarding

- Component map:
  - Human annotator → Preference dataset → Higher-level policy optimization (DPO) → Subgoal generation → Lower-level policy execution → Environment interaction → Experience collection → Lower-level policy optimization (SAC) → Repeat

- Critical path: Human annotator → Preference dataset → Higher-level policy optimization (DPO) → Subgoal generation → Lower-level policy execution → Environment interaction → Experience collection → Lower-level policy optimization (SAC) → Repeat

- Design tradeoffs:
  - Computational efficiency vs. potential accuracy: DPO avoids reward modeling overhead but may be less sample-efficient with sparse preference data
  - Decoupling vs. coordination: Separating higher and lower level training simplifies optimization but may miss opportunities for tighter coordination
  - Approximation vs. exactness: Using approximate value functions instead of optimal ones makes the algorithm practical but may reduce the quality of subgoal conditioning

- Failure signatures:
  - Non-stationarity not resolved: Lower-level policy changes rapidly, causing higher-level policy to generate increasingly poor subgoals despite preference optimization
  - Infeasible subgoals persist: Lower-level value function estimation is poor, leading to reference policy that doesn't accurately reflect subgoal achievability
  - Poor sample efficiency: Sparse preference data makes direct optimization difficult, resulting in slow learning or suboptimal policies

- First 3 experiments:
  1. Compare DIPPER vs. DIPPER-No-V (without primitive regularization) on maze navigation to verify that primitive-informed regularization improves performance
  2. Compare DIPPER vs. single-level DPO implementation on pick and place task to verify that hierarchical structure provides benefits
  3. Analyze the effect of primitive regularization weight λ by running DIPPER with different λ values on the push task and measuring success rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DIPPER perform in high-dimensional subgoal spaces compared to its performance in the tested environments?
- Basis in paper: [inferred] The paper mentions that applying DIPPER to high-dimensional subgoal spaces would be challenging.
- Why unresolved: The experiments only tested DIPPER in environments with relatively low-dimensional subgoal spaces (e.g., 3D positions). The paper does not provide any results or analysis for high-dimensional subgoal spaces.
- What evidence would resolve it: Experimental results comparing DIPPER's performance in high-dimensional subgoal spaces (e.g., images, point clouds) to its performance in low-dimensional spaces.

### Open Question 2
- Question: How does DIPPER's performance compare to hierarchical RLHF-based methods in terms of out-of-distribution generalization?
- Basis in paper: [inferred] The paper raises the question of whether DIPPER generalizes better on out-of-distribution states and actions compared to reward model-based RL formulations, but does not provide a direct comparison.
- Why unresolved: The paper does not include experiments comparing DIPPER to hierarchical RLHF-based methods, which would be necessary to assess their relative out-of-distribution generalization capabilities.
- What evidence would resolve it: Experimental results comparing DIPPER's performance on out-of-distribution states and actions to that of hierarchical RLHF-based methods.

### Open Question 3
- Question: What is the impact of the choice of reference policy on DIPPER's performance?
- Basis in paper: [explicit] The paper mentions that the reference policy is typically unavailable in complex robotics tasks and proposes a primitive-enabled reference policy, but does not explore the impact of different reference policy choices.
- Why unresolved: The paper only experiments with the proposed primitive-enabled reference policy and does not investigate how alternative reference policy choices might affect DIPPER's performance.
- What evidence would resolve it: Experimental results comparing DIPPER's performance using different reference policy choices (e.g., learned reference policies, uniform policies) to assess the impact on overall performance.

## Limitations
- The effectiveness of the non-stationarity mitigation depends on the stability of human preference data as a signal, which may not hold if preferences become correlated with specific lower-level primitive behaviors
- The primitive-informed regularization relies on accurate lower-level value function estimation, which may be challenging in practice and could lead to infeasible subgoal generation
- The method has not been tested in high-dimensional subgoal spaces, which limits its applicability to more complex robotics tasks involving vision or point cloud inputs

## Confidence

- **High confidence**: The computational efficiency claims (avoiding reward modeling) are well-supported by the algorithm description and the DPO formulation is standard
- **Medium confidence**: The empirical results showing >40% success rates on complex tasks are convincing, but the comparison to baselines doesn't isolate the contribution of individual components (primitive regularization vs. DPO vs. hierarchical structure)
- **Low confidence**: The theoretical guarantees about addressing non-stationarity and infeasible subgoal generation rely on assumptions about preference data stability and value function accuracy that aren't empirically validated

## Next Checks

1. **Sensitivity analysis**: Run DIPPER with systematically degraded lower-level primitive performance to test whether the higher-level policy maintains stable performance, directly validating the non-stationarity mitigation claim.

2. **Ablation of primitive regularization**: Implement a version where π_ref is replaced with a uniform distribution over subgoals, keeping all other aspects of DIPPER identical, to isolate the contribution of the primitive-informed regularization.

3. **Value function quality assessment**: Measure the correlation between V^L_π estimates and actual subgoal achievement rates across different lower-level policy stages to validate whether the reference policy reliably identifies feasible subgoals.