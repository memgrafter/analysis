---
ver: rpa2
title: Multi-modal Evidential Fusion Network for Trustworthy PET/CT Tumor Segmentation
arxiv_id: '2406.18327'
source_url: https://arxiv.org/abs/2406.18327
tags:
- segmentation
- tumor
- uncertainty
- fusion
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of accurately segmenting tumors
  in PET/CT images, where the varying quality of the two modalities leads to uncertainty
  in the extracted information. To tackle this challenge, the authors propose a Multi-modal
  Evidential Fusion Network (MEFN) consisting of two core stages: Cross-Modal Feature
  Learning (CFL) and Multi-modal Trustworthy Fusion (MTF).'
---

# Multi-modal Evidential Fusion Network for Trustworthy PET/CT Tumor Segmentation

## Quick Facts
- arXiv ID: 2406.18327
- Source URL: https://arxiv.org/abs/2406.18327
- Reference count: 30
- Primary result: 3.10% and 3.23% improvements in DSC scores on AutoPET and Hecktor datasets respectively

## Executive Summary
This paper addresses the challenge of accurately segmenting tumors in PET/CT images by proposing a Multi-modal Evidential Fusion Network (MEFN). The approach tackles the uncertainty inherent in combining two modalities with varying quality by implementing a two-stage process: Cross-Modal Feature Learning (CFL) to align features and learn robust representations, followed by Multi-modal Trustworthy Fusion (MTF) that combines features based on uncertainty using mutual attention mechanisms and Dempster-Shafer Theory. The method achieves significant improvements over state-of-the-art approaches while providing clinically useful uncertainty estimates for radiologists.

## Method Summary
The proposed Multi-modal Evidential Fusion Network consists of two core stages. First, the Cross-Modal Feature Learning (CFL) stage uses GAN-based back-and-forth modality conversion with auxiliary tumor decoders to learn modality-specific and shared tumor features, which are then transferred as initial weights to the segmentation backbone. Second, the Multi-modal Trustworthy Fusion (MTF) stage fuses modality features based on uncertainty using Dual-attention Feature Calibrating (DFC) for feature-level fusion and Dempster-Shafer Theory Based Trusted Fusion (DBTF) for decision-level fusion. An Uncertainty Perceptual Loss is introduced to force the network to focus on uncertain regions during training, improving its ability to extract trusted modality information.

## Key Results
- Achieved 3.10% improvement in DSC score on AutoPET dataset compared to state-of-the-art methods
- Achieved 3.23% improvement in DSC score on Hecktor dataset compared to state-of-the-art methods
- Provides credible uncertainty estimates for segmentation results, enhancing clinical applicability
- Demonstrated effectiveness across two publicly available PET/CT datasets with different tumor types

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CFL improves initialization by learning modality-specific and shared tumor features before segmentation
- **Mechanism**: GAN-based back-and-forth modality conversion with auxiliary tumor decoders extracts both modality-specific and general tumor features, which are transferred as initial weights
- **Core assumption**: Features learned during modality conversion contain transferable knowledge relevant to segmentation
- **Evidence anchors**: [abstract], [section], weak corpus support
- **Break condition**: If modality conversion doesn't capture tumor-relevant features or transferred weights are incompatible with segmentation network

### Mechanism 2
- **Claim**: MTF combines feature-level and decision-level fusion guided by uncertainty to improve segmentation accuracy
- **Mechanism**: Dual-attention Feature Calibrating adjusts encoder features via mutual attention and calibrates decoder features based on uncertainty; decision-level fusion uses Dempster-Shafer Theory weighted by uncertainties
- **Core assumption**: Uncertainty estimates are reliable and positively correlated with prediction quality
- **Evidence anchors**: [abstract], [section], weak corpus support
- **Break condition**: If uncertainty estimates are poorly calibrated or Dempster-Shafer combination produces counterintuitive results

### Mechanism 3
- **Claim**: Uncertainty Perceptual Loss forces network to focus on uncertain regions during training
- **Mechanism**: Loss function multiplies cross-entropy loss by uncertainty mass of each sample, weighting training gradients by prediction uncertainty
- **Core assumption**: Uncertainty mass is meaningful signal indicating where model should focus during training
- **Evidence anchors**: [abstract], [section], weak corpus support
- **Break condition**: If uncertainty estimates are unreliable during early training, loss may amplify noise and degrade performance

## Foundational Learning

- **Concept**: Dempster-Shafer Theory of Evidence
  - **Why needed here**: Provides formal framework for combining evidence from multiple sources while accounting for uncertainty
  - **Quick check question**: How does Dempster-Shafer Theory differ from simple averaging when combining segmentation results from two modalities?

- **Concept**: Dirichlet distribution for uncertainty quantification
  - **Why needed here**: Allows model to output both probability distribution over classes and associated uncertainty measure
  - **Quick check question**: What is the relationship between Dirichlet concentration parameters and belief mass assigned to each class?

- **Concept**: GAN-based modality conversion
  - **Why needed here**: Enables network to learn cross-modal feature representations useful for task-driven initialization
  - **Quick check question**: Why might learning to convert between modalities help network learn features useful for segmentation?

## Architecture Onboarding

- **Component map**: CFL module (GAN + MTGA attention) → Segmentation Backbone (initialized with CFL weights) → DFC (CMA + UC) → DBTF → Final output

- **Critical path**: CFL → Segmentation Backbone → DFC (feature fusion) → DBTF (decision fusion) → Final output

- **Design tradeoffs**:
  - CFL adds computational overhead but provides better initialization
  - Dempster-Shafer fusion is more complex than averaging but accounts for uncertainty
  - Uncertainty Perceptual Loss focuses training but may amplify noise if uncertainty is poorly calibrated

- **Failure signatures**:
  - Poor convergence: CFL initialization may not be transferring useful features
  - Overconfident uncertainty: Uncertainty estimates may not correlate with actual prediction quality
  - Degraded performance with perturbations: DFC or DBTF may not be robust to input noise

- **First 3 experiments**:
  1. **Ablation study**: Train with and without CFL initialization to verify convergence speed and final performance improvement
  2. **Uncertainty calibration**: Test whether uncertainty estimates increase with input perturbations as expected
  3. **Fusion comparison**: Compare Dempster-Shafer fusion against simple averaging to quantify benefit of uncertainty-weighted fusion

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the proposed MEFN be adapted to handle multimodal data with significant domain shifts, such as different scanners or imaging protocols?
- **Basis in paper**: [inferred] Paper mentions use of GANs for cross-modal feature learning which could address domain shifts, but evaluation is limited to single scanner/protocol per dataset
- **Why unresolved**: Paper does not provide evidence on model's performance with multimodal data from different sources
- **What evidence would resolve it**: Evaluating MEFN on multimodal datasets with different scanners or imaging protocols and comparing performance to baseline methods

### Open Question 2
- **Question**: Can uncertainty estimates provided by MEFN be used to improve model's performance during training, rather than just being used for decision-making?
- **Basis in paper**: [explicit] Paper introduces uncertainty perceptual loss to force model to focus on uncertain features during training but does not explore other ways to leverage uncertainty estimates
- **Why unresolved**: Paper only scratches surface of how uncertainty estimates can be used to improve performance
- **What evidence would resolve it**: Investigating other ways to utilize uncertainty estimates such as data augmentation, curriculum learning, or active learning strategies

### Open Question 3
- **Question**: How can proposed MEFN be extended to handle multimodal data with more than two modalities?
- **Basis in paper**: [inferred] Paper focuses on fusing two modalities (PET and CT) and does not provide insights into extension to more modalities
- **Why unresolved**: Paper does not discuss scalability of MEFN to handle multimodal data with more than two modalities
- **What evidence would resolve it**: Evaluating MEFN on multimodal datasets with more than two modalities and investigating modifications to model architecture or fusion strategies

## Limitations

- Implementation details for critical components (CFL network architecture, DFC attention mechanisms) are not fully specified, making exact reproduction challenging
- Evaluation relies on only two datasets, limiting generalizability to other tumor types and imaging conditions
- Uncertainty estimates' calibration and reliability are not extensively validated across different scenarios

## Confidence

- **High confidence**: The core hypothesis that cross-modal feature learning and uncertainty-aware fusion can improve PET/CT tumor segmentation is well-supported by reported results
- **Medium confidence**: Specific mechanisms of CFL initialization and Uncertainty Perceptual Loss are plausible but rely on assumptions about feature transferability and uncertainty reliability that require further validation
- **Low confidence**: Exact implementation details necessary for faithful reproduction are not provided, particularly for attention mechanisms and uncertainty calibration components

## Next Checks

1. **Ablation study**: Train with and without CFL initialization to verify convergence speed and final performance improvement, measuring both training efficiency and segmentation accuracy
2. **Uncertainty calibration test**: Systematically perturb input images and measure whether uncertainty estimates increase proportionally, validating reliability of uncertainty estimates
3. **Cross-dataset generalization**: Evaluate trained model on additional PET/CT datasets with different tumor types and imaging protocols to assess robustness beyond two evaluated datasets