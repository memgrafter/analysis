---
ver: rpa2
title: TTSDS -- Text-to-Speech Distribution Score
arxiv_id: '2407.12707'
source_url: https://arxiv.org/abs/2407.12707
tags:
- speech
- systems
- score
- factor
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces TTSDS (Text-to-Speech Distribution Score),
  a novel benchmark for evaluating synthetic speech quality by measuring the distance
  between real and synthetic speech distributions across multiple factors: general
  speech characteristics, environment, speaker identity, prosody, and intelligibility.
  The method uses Wasserstein distance to compare distributions of features extracted
  from both synthetic and real speech datasets, employing representations from self-supervised
  models, pitch extraction, speaker verification systems, and automatic speech recognition.'
---

# TTSDS -- Text-to-Speech Distribution Score

## Quick Facts
- arXiv ID: 2407.12707
- Source URL: https://arxiv.org/abs/2407.12707
- Reference count: 0
- Primary result: Introduces TTSDS benchmark measuring synthetic speech quality via Wasserstein distance between real and synthetic speech distributions across 5 factors

## Executive Summary
TTSDS (Text-to-Speech Distribution Score) introduces a novel benchmark for evaluating synthetic speech quality by measuring the distance between real and synthetic speech distributions across five factors: general speech characteristics, environment, speaker identity, prosody, and intelligibility. The method uses Wasserstein distance to compare distributions of features extracted from both synthetic and real speech datasets, employing representations from self-supervised models, pitch extraction, speaker verification systems, and automatic speech recognition. The benchmark was evaluated on 35 TTS systems from 2008 to 2024, showing strong correlation (0.60-0.83 Spearman coefficient) with human evaluations across different time periods.

## Method Summary
TTSDS evaluates synthetic speech by extracting features representing five quality factors using pre-trained models (Hubert, wav2vec 2.0, Whisper, etc.), then computing Wasserstein distances between synthetic and reference speech distributions. For each factor, distances are calculated to both real speech datasets and noise datasets, then normalized to produce scores between 0-100. The overall TTSDS score is computed as an unweighted average of all five factor scores. The benchmark uses 80-100 samples per TTS system and reference datasets including LibriTTS, LJSpeech, VCTK, and various noise datasets.

## Key Results
- TTSDS shows strong correlation (0.60-0.83 Spearman) with human evaluations across 35 TTS systems from 2008-2024
- Outperforms traditional MOS prediction methods, particularly for modern TTS systems
- Individual factor scores provide interpretable breakdowns of synthesis quality
- TTSDS score, computed as unweighted average of all factors, provides comprehensive quality assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TTSDS captures overall speech quality by measuring Wasserstein distance between real and synthetic speech distributions across multiple factors.
- Mechanism: The benchmark extracts high-dimensional and scalar features representing speech characteristics, then computes Wasserstein distance (Earth Mover's Distance) to quantify how far synthetic speech deviates from real speech distributions.
- Core assumption: Speech features extracted from self-supervised models and algorithmic methods accurately represent the underlying factors that contribute to perceived speech quality.
- Evidence anchors: [abstract] "Our approach assesses how well synthetic speech mirrors real speech by obtaining correlates of each factor and measuring their distance from both real speech datasets and noise datasets."
- Break condition: If the feature extraction methods fail to capture the true underlying distributions, or if Wasserstein distance assumptions are violated.

### Mechanism 2
- Claim: Combining multiple factors into an unweighted average provides more robust evaluation than single-factor approaches.
- Mechanism: Individual factor scores are computed independently and then averaged to produce the overall TTSDS score, capturing different aspects of speech quality.
- Core assumption: Different factors contribute equally to overall speech quality perception, and averaging captures the holistic evaluation better than any single metric.
- Evidence anchors: [section] "The TTSDS score has higher correlation than any single factor for all datasets included in our study"
- Break condition: If certain factors become significantly more or less important over time, or if some factors are redundant.

### Mechanism 3
- Claim: Using real speech datasets and noise datasets as references provides a relative scoring system that adapts to evolving TTS quality.
- Mechanism: For each feature, TTSDS computes distances to both real speech datasets (Dreal) and noise datasets (Dnoise), then normalizes the score based on these distances to produce values between 0-100.
- Core assumption: Real speech datasets represent the target distribution while noise datasets represent the worst-case scenario, providing meaningful bounds for evaluation.
- Evidence anchors: [section] "We find the smallest W2 among the real and noise datasets respectively as Wreal = min W2(ˆPsyn, ˆPreal) and Wnoise = min W2(ˆPsyn, ˆPnoise)"
- Break condition: If the selected reference datasets don't adequately represent the speech quality spectrum.

## Foundational Learning

- Concept: Wasserstein distance (Earth Mover's Distance)
  - Why needed here: Provides a way to measure distribution similarity without assuming specific parametric forms, crucial when comparing complex speech feature distributions
  - Quick check question: What's the key advantage of Wasserstein distance over KL divergence when comparing distributions?

- Concept: Self-supervised learning representations for speech
  - Why needed here: These models learn rich, general-purpose speech representations that can capture multiple aspects of speech quality in a single embedding space
  - Quick check question: Why might self-supervised representations be more effective than handcrafted features for capturing speech quality?

- Concept: Distribution comparison in high-dimensional spaces
  - Why needed here: Speech features often exist in high-dimensional spaces, and understanding how to compare distributions in these spaces is fundamental to the benchmark's methodology
  - Quick check question: What assumption does the TTSDS make about the distribution of self-supervised speech representations?

## Architecture Onboarding

- Component map: Feature extraction (Hubert, wav2vec 2.0, Whisper, VoiceFixer, PESQ, pitch extractors, speaker verification, ASR) -> Distribution comparison (Wasserstein distance) -> Reference dataset management (real and noise datasets) -> Scoring (normalization and aggregation) -> Evaluation (MOS prediction integration)

- Critical path: 1. Extract features from synthetic speech 2. Compute Wasserstein distances to real and noise datasets 3. Normalize scores using Wreal and Wnoise 4. Aggregate factor scores into TTSDS score 5. Compare with human evaluation benchmarks

- Design tradeoffs: More features increase robustness but computational cost; broader reference dataset coverage improves generalization but increases complexity; equal weighting is simple but may not reflect true importance; more samples improve reliability but increase runtime

- Failure signatures: Low correlation with human evaluations indicates feature extraction or scoring methodology issues; unstable scores across random seeds suggests reference dataset or sampling problems; factor scores diverging from overall score may indicate aggregation issues

- First 3 experiments: 1. Test feature extraction consistency by running extraction on same synthetic speech multiple times 2. Validate Wasserstein distance implementation by comparing computed distances on synthetic distributions with known ground truth 3. Benchmark correlation sensitivity by measuring how TTSDS correlation changes as number of reference samples varies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the TTSDS benchmark be extended to evaluate long-form speech synthesis quality effectively?
- Basis in paper: [explicit] The paper mentions that adding more complex domains such as long-form speech synthesis would require running the evaluation from scratch.
- Why unresolved: The current benchmark is validated on short utterances (100 samples per system) and factors like prosody, speaker identity, and intelligibility are measured on these shorter samples. Long-form speech synthesis introduces additional challenges like maintaining consistency over time, handling discourse-level prosody, and managing speaker consistency across extended passages.
- What evidence would resolve it: Developing and validating TTSDS metrics specifically designed for long-form synthesis, testing them on datasets with extended speech passages, and demonstrating correlation with human evaluations of long-form quality.

### Open Question 2
- Question: What is the optimal weighting scheme for combining individual factor scores into the overall TTSDS score?
- Basis in paper: [explicit] The current TTSDS score uses an unweighted average of all factors, but the paper notes that human evaluators show varying priorities over time.
- Why unresolved: Different applications may prioritize different factors (e.g., character voices in video games may need higher prosody scores, while language learning apps may prioritize intelligibility). The unweighted average may not capture these domain-specific requirements.
- What evidence would resolve it: Empirical studies comparing weighted versus unweighted combinations across different application domains, analysis of how different weighting schemes affect correlation with human evaluations in specific use cases.

### Open Question 3
- Question: How can the TTSDS benchmark be made more sample-efficient while maintaining its strong correlation with human evaluations?
- Basis in paper: [explicit] The paper notes that the current benchmark uses only 80-100 samples per system, and that finding statistically significant differences between TTS systems has been difficult even with previous subjective evaluation methods.
- Why unresolved: The low number of samples required is advantageous for practical evaluation, but it may limit the benchmark's ability to detect subtle quality differences between high-performing systems. The paper shows that while worst-performing systems can be distinguished from best-performing ones, there is no statistically significant difference between better-performing systems.
- What evidence would resolve it: Developing techniques to reduce sample requirements further while maintaining or improving correlation with human evaluations, or demonstrating that increasing sample size significantly improves the benchmark's ability to distinguish between high-quality systems.

## Limitations
- The unweighted averaging of factor scores may not reflect the true perceptual importance of different quality aspects
- Benchmark relies heavily on pre-trained models for feature extraction, introducing potential biases from their training data
- Limited sample size (80-100 per system) may prevent detection of subtle differences between high-quality systems

## Confidence
- High confidence: Technical implementation of Wasserstein distance computation and feature extraction pipeline
- Medium confidence: Correlation results with human evaluations across time periods
- Low confidence: Assumption that equal weighting of all factors is optimal for capturing perceived speech quality

## Next Checks
1. Reference Dataset Sensitivity Analysis: Systematically vary the reference speech datasets and measure how TTSDS scores and correlations with human evaluation change
2. Factor Weighting Optimization: Conduct ablation studies by varying weights assigned to each factor and measure impact on correlation with human evaluations
3. Temporal Robustness Testing: Apply TTSDS to TTS systems from different time periods and analyze whether the benchmark maintains consistent performance across these eras