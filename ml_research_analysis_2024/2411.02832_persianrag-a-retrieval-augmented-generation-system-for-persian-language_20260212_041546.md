---
ver: rpa2
title: 'PersianRAG: A Retrieval-Augmented Generation System for Persian Language'
arxiv_id: '2411.02832'
source_url: https://arxiv.org/abs/2411.02832
tags:
- language
- persian
- retrieval
- system
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents PersianRAG, a retrieval-augmented generation
  system specifically designed for the Persian language, addressing the challenges
  of implementing RAG in low-resource languages. The authors tackle issues including
  Persian PDF processing, optimal embedding model selection, prompt engineering, and
  hallucination reduction.
---

# PersianRAG: A Retrieval-Augmented Generation System for Persian Language

## Quick Facts
- **arXiv ID:** 2411.02832
- **Source URL:** https://arxiv.org/abs/2411.02832
- **Reference count:** 39
- **Primary result:** 89% accuracy on PersianQuAD dataset

## Executive Summary
PersianRAG is a retrieval-augmented generation system specifically designed for the Persian language, addressing the challenges of implementing RAG in low-resource languages. The authors tackle issues including Persian PDF processing, optimal embedding model selection, prompt engineering, and hallucination reduction. Through systematic evaluation across multiple Persian benchmark datasets, PersianRAG achieved 89% accuracy on the PersianQuAD dataset, representing a significant improvement over baseline approaches. The system demonstrates effective handling of Persian linguistic characteristics and provides a framework for deploying RAG technology in languages with limited available resources.

## Method Summary
PersianRAG implements a comprehensive RAG pipeline for Persian language processing, starting with specialized OCR for Persian PDFs that handles character distortion issues with half-spaces and specific encodings. The system uses a hybrid retriever approach combining BM25 and dense retrievers, with a document joiner that removes duplicates and prioritizes higher-scoring documents. Embedding models are carefully selected and evaluated, with cohere-embed-multilingual-v3.0 showing strong performance. The pipeline includes a reranker model (cohere-rerank-multilingual-v3.0) to improve retrieval precision, prompt engineering optimization with Persian language prompts for multilingual models, and hallucination reduction techniques. The system uses the Command-R model and Elasticsearch for vector storage, with extensive hyperparameter optimization throughout the pipeline.

## Key Results
- Achieved 89% accuracy on PersianQuAD dataset
- Hybrid retriever approach improved accuracy by up to 4% over single-method approaches
- Reranker models improved Top-1 accuracy from 93.5% to 98.7%
- Prompt language selection significantly impacted response quality, with Persian prompts reducing English word generation

## Why This Works (Mechanism)

### Mechanism 1
PersianRAG's hybrid retriever approach combining BM25 and dense retrievers improves accuracy by up to 4% over single-method approaches. The system uses BM25 (sparse retrieval) for exact keyword matching and a dense retriever for semantic similarity, then merges results with a document joiner that removes duplicates and prioritizes higher-scoring documents. Core assumption: Persian text contains both explicit keywords and semantic relationships that benefit from complementary retrieval methods. Evidence anchors: [section] "This approach includes using search-based retrievers like BM25 along with dense retrievers. By adjusting various top_k values and adding a document_joiner at the end of the indexing pipeline, better accuracy could be achieved compared to using either method alone" [abstract] "Through systematic evaluation across multiple Persian benchmark datasets, PersianRAG achieved 89% accuracy".

### Mechanism 2
Reranker models significantly improve retrieval precision by re-ranking chunks based on semantic relevance. After initial retrieval using cohere-embed-multilingual-v3.0, the cohere-rerank-multilingual-v3.0 model reorders results to place the most relevant chunks first, improving Top-1 accuracy from 93.5% to 98.7%. Core assumption: Initial dense retrieval produces near-relevant results that can be better ordered through learned re-ranking. Evidence anchors: [section] "Our experiments demonstrate that after retrieving results from a vector database using the 'cohere-embed-multilingual-v3.0' embedding model, the addition of the 'cohere-rerank-multilingual-v3.0' as a Reranker substantially improves performance" [abstract] "Our experimental results demonstrate the capability of the PersianRAG framework to enhance question answering task in Persian".

### Mechanism 3
Prompt language selection based on model type significantly impacts response quality. For monolingual or limited-support models, using English prompts improves comprehension; for multilingual models like Aya-101, using Persian prompts reduces irrelevant word generation and improves output smoothness. Core assumption: Different language models have varying proficiency levels across languages, affecting their instruction-following ability. Evidence anchors: [section] "In this study, changing the prompt language from English to Persian led to a significant reduction in the occurrence of English words in the output" [section] "Language models that support only a limited number of languages react differently to the input prompt language compared to multilingual models".

## Foundational Learning

- **Concept:** Persian text preprocessing challenges
  - Why needed here: Persian PDFs often contain character distortion issues with half-spaces and specific encodings that can completely scramble text extraction
  - Quick check question: What are the three main character-level issues that must be handled when extracting Persian text from PDFs?

- **Concept:** Semantic embedding for low-resource languages
  - Why needed here: Finding effective embedding models for Persian requires testing multiple approaches since most open-source models are optimized for English
  - Quick check question: Why did the PersianRAG team test over 40 embedding models specifically for Persian text?

- **Concept:** Chunking strategy optimization
  - Why needed here: PersianRAG found that adjusting chunk size and overlap significantly impacted retrieval accuracy, requiring systematic experimentation
  - Quick check question: How does chunk size affect the trade-off between retrieval precision and the ability to capture complete answers?

## Architecture Onboarding

- **Component map:** Knowledge Base → Text Extractor → Preprocessor → Embedder → Vector Database → User Query → Embedder (query) → Retriever → Related Chunks → Prompt Builder → Language Model → Response, with Hyperparameters Optimizer tuning parameters throughout
- **Critical path:** Text extraction → Preprocessing → Embedding → Retrieval → Prompt building → Generation
- **Design tradeoffs:** PersianRAG balances between using specialized Persian models versus multilingual models, choosing between extractive vs generative approaches, and deciding on hybrid vs single retrieval methods
- **Failure signatures:** Poor accuracy suggests issues in text extraction or embedding; irrelevant responses indicate prompt engineering problems; slow performance points to vector database or retrieval configuration issues
- **First 3 experiments:**
  1. Test Persian text extraction from sample PDFs using Tesseract OCR and compare output quality against ground truth
  2. Evaluate 3-4 different embedding models on a small Persian question-answering task to identify the best performer
  3. Implement a simple BM25 retriever and measure baseline retrieval accuracy on PersianQuAD dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal chunk size and overlap configuration for Persian PDF documents containing complex tables and mixed text formats?
- Basis in paper: [explicit] The paper discusses PDF reading challenges and chunking optimization as key challenges
- Why unresolved: The authors mention PDF processing challenges and chunking optimization but do not provide specific optimal configurations for Persian documents with complex layouts
- What evidence would resolve it: Systematic testing of different chunk sizes (e.g., 256, 512, 1024 tokens) and overlap percentages (e.g., 10%, 25%, 50%) on Persian PDF datasets with various document structures, measuring retrieval accuracy and hallucination rates

### Open Question 2
- Question: How does the performance of PersianRAG compare to fine-tuned Persian-specific LLMs when answering questions that require both retrieved context and model reasoning?
- Basis in paper: [explicit] The paper evaluates PersianRAG against several Persian fine-tuned models and finds they suffer from hallucination errors
- Why unresolved: While the paper shows PersianRAG achieves 89% accuracy on PersianQuAD, it doesn't compare performance on questions requiring reasoning beyond direct retrieval
- What evidence would resolve it: Head-to-head comparison on a benchmark containing both extractive and reasoning questions, measuring accuracy, hallucination rates, and response coherence

### Open Question 3
- Question: What is the minimum training data requirement for effective fine-tuning of multilingual LLMs on Persian language tasks?
- Basis in paper: [explicit] The authors found that fine-tuned models required at least 0.02% of total training corpus to be in Persian to achieve desirable performance
- Why unresolved: The 0.02% threshold is mentioned but not empirically validated across different model sizes or tested for optimal performance
- What evidence would resolve it: Systematic fine-tuning experiments varying Persian corpus percentage (0.005%, 0.02%, 0.05%, 0.1%) across different model sizes, measuring performance on Persian language tasks

### Open Question 4
- Question: How does prompt language selection impact PersianRAG performance across different model architectures (monolingual vs. multilingual)?
- Basis in paper: [explicit] The authors found that using English prompts in multilingual models caused confusion and English word insertion in Persian outputs
- Why unresolved: The paper identifies this issue but doesn't explore the full spectrum of prompt language combinations or develop systematic guidelines
- What evidence would resolve it: Comprehensive testing of prompt language combinations (Persian-only, English-only, bilingual) across various monolingual and multilingual models, measuring response quality, hallucination rates, and language mixing

## Limitations

- Hyperparameter optimization process lacks specific details about search space and criteria, making independent replication challenging
- Hallucination reduction techniques are described only at high level without concrete implementation details or quantitative evaluation
- Evaluation focuses primarily on question-answering tasks without exploring broader language understanding capabilities or robustness to different Persian dialects

## Confidence

- **Hybrid retrieval improvement (4% accuracy gain):** Medium confidence - The mechanism is well-described, but specific implementation details and hyperparameter values are missing
- **Reranker model effectiveness (98.7% Top-1 accuracy):** Low confidence - While the general approach is clear, the paper doesn't provide sufficient detail about the reranker training process
- **Prompt engineering impact:** Medium confidence - The concept is well-supported by evidence, but specific prompt templates and variations are not fully disclosed

## Next Checks

1. **Text extraction validation:** Test PersianRAG's OCR pipeline on 10 diverse Persian PDF documents containing known text, comparing extracted output against ground truth to verify the claimed character distortion handling capabilities

2. **Embedding model benchmarking:** Implement a controlled experiment comparing the recommended cohere-embed-multilingual-v3.0 model against 3-5 other multilingual embedding models on a held-out PersianQuAD subset, measuring Top-1 accuracy with and without reranking

3. **Prompt language ablation study:** Conduct a systematic comparison using the same Persian questions but varying prompt languages (Persian vs English) across 50 representative queries, measuring hallucination rates and relevance scores to verify the claimed prompt language effects