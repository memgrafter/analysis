---
ver: rpa2
title: Generative Retrieval as Multi-Vector Dense Retrieval
arxiv_id: '2404.00684'
source_url: https://arxiv.org/abs/2404.00684
tags:
- retrieval
- alignment
- document
- mvdr
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Generative retrieval directly generates document identifiers for
  a given query using a sequence-to-sequence model, but its relationship to dense
  retrieval methods is not well understood. This work demonstrates that generative
  retrieval and multi-vector dense retrieval share the same framework for measuring
  query-document relevance: both compute relevance as a sum of products of query and
  document vectors aligned by a matrix.'
---

# Generative Retrieval as Multi-Vector Dense Retrieval

## Quick Facts
- arXiv ID: 2404.00684
- Source URL: https://arxiv.org/abs/2404.00684
- Reference count: 40
- Primary result: Generative retrieval and multi-vector dense retrieval share the same framework for computing query-document relevance

## Executive Summary
This paper establishes a theoretical connection between generative retrieval (GR) and multi-vector dense retrieval (MVDR), showing they share a unified framework for measuring query-document relevance. Both paradigms compute relevance as a sum of products between query and document vectors with an alignment matrix. The work reveals that GR employs distinct strategies for document encoding (simple embeddings vs. contextualized tokens) and alignment (dense learned matrix vs. sparse heuristics). Through experimental verification, the authors demonstrate commonalities in term matching behavior between these approaches, positioning GR as a special case of MVDR.

## Method Summary
The authors analyze generative retrieval through the lens of multi-vector dense retrieval by examining the attention layer and prediction head of GR models. They derive a unified relevance computation framework where both methods calculate rel(d,q) = sum(D^T Q âŠ™ A), with D and Q as token vectors and A as an alignment matrix. The study implements PAWA (Position-wise Attention Weight Averaging) and NP-decoding to enhance GR's document encoding, bridging the gap between simple embeddings and contextualized representations. Experiments use T5-SEAL for GR and T5-ColBERT for MVDR, trained on NQ and MS MARCO datasets with in-batch negatives.

## Key Results
- Generative retrieval and multi-vector dense retrieval share identical relevance computation frameworks
- GR uses simple word embeddings while MVDR uses contextualized token vectors for document encoding
- Both paradigms exhibit similar term matching behavior in their alignment matrices despite different sparsity patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative retrieval and multi-vector dense retrieval share a unified framework for computing relevance.
- Mechanism: Both methods compute relevance as a sum of products of query and document vectors aligned by a matrix. The core computation is `rel(d, q) = sum(D^T Q âŠ™ A)` where D and Q are token vectors from document and query, and A is an alignment matrix.
- Core assumption: The final logits in the generative retrieval loss function can be reformulated as the product of document word embeddings, query token vectors, and an attention matrix.
- Evidence anchors:
  - [abstract] "Specifically, we examine the attention layer and prediction head of generative retrieval, revealing that generative retrieval can be understood as a special case of multi-vector dense retrieval. Both methods compute relevance as a sum of products of query and document vectors and an alignment matrix."
  - [section] "rel(d, q) = sum(ð·âŠ¤ð‘„ âŠ™ ð´)" derived in Section 4.2 showing the framework equivalence.
  - [corpus] Weak - no direct citation supporting the framework equivalence in existing literature.
- Break condition: If the attention mechanism fails to capture meaningful alignment between query and document tokens, the framework equivalence breaks down.

### Mechanism 2
- Claim: Generative retrieval employs a distinct document encoding strategy compared to multi-vector dense retrieval.
- Mechanism: GR uses simple word embeddings for document tokens, while MVDR uses contextualized token vectors. PAWA and NP-decoding methods enhance GR's document encoding to bridge this gap.
- Core assumption: Simple embeddings can be enhanced with contextualized information through PAWA or replaced with contextualized embeddings via NP-decoding.
- Evidence anchors:
  - [abstract] "We then explore how generative retrieval applies this framework, employing distinct strategies for computing document token vectors and the alignment matrix."
  - [section] Section 5.1 discusses PAWA and NP-decoding methods for enhancing document encoding in GR.
  - [corpus] Weak - limited corpus evidence on the effectiveness of these specific encoding enhancements in GR.
- Break condition: If the enhanced document encoding methods (PAWA, NP-decoding) fail to improve performance or introduce significant computational overhead, the distinct encoding strategy becomes a liability.

### Mechanism 3
- Claim: Generative retrieval and multi-vector dense retrieval exhibit commonalities in term matching behavior within their alignment matrices.
- Mechanism: Both methods show a preference for exact term matching in their alignment matrices, despite differences in sparsity and direction.
- Core assumption: The alignment matrix, whether dense (GR) or sparse (MVDR), captures term matching behavior that can be analyzed through exact match rates.
- Evidence anchors:
  - [abstract] "We have conducted experiments to verify our conclusions and show that both paradigms exhibit commonalities of term matching in their alignment matrix."
  - [section] Section 7.2 presents experiments showing exact term matching rates in both GR and MVDR alignment matrices.
  - [corpus] Weak - no direct citation supporting the term matching behavior commonality in existing literature.
- Break condition: If the term matching behavior is not consistent across different datasets or if the alignment matrices fail to capture meaningful term relationships, the commonality assumption breaks down.

## Foundational Learning

- Concept: Attention mechanism in transformer models
  - Why needed here: Understanding how the attention mechanism computes the alignment matrix in generative retrieval is crucial for grasping the framework equivalence.
  - Quick check question: How does the attention mechanism compute the alignment between query and document tokens in generative retrieval?

- Concept: Dense retrieval and multi-vector dense retrieval
  - Why needed here: Familiarity with dense retrieval concepts and the multi-vector approach is essential for understanding the comparison and connection to generative retrieval.
  - Quick check question: What is the main difference between single-vector dense retrieval and multi-vector dense retrieval in terms of query-document relevance computation?

- Concept: Token embeddings and contextualized representations
  - Why needed here: Understanding the difference between simple word embeddings and contextualized token vectors is key to grasping the document encoding strategies in generative retrieval and multi-vector dense retrieval.
  - Quick check question: How do contextualized token representations differ from simple word embeddings in terms of capturing semantic information?

## Architecture Onboarding

- Component map:
  - Encoder -> Decoder -> Alignment Matrix -> Prediction Head
  - Query tokens -> Document identifiers -> Cross-attention scores -> Word embedding lookup

- Critical path:
  1. Encode the query to generate token vectors.
  2. Use the decoder to generate document identifiers autoregressively.
  3. At each generation step, compute the alignment between query and document tokens using cross-attention.
  4. Use the prediction head to select the most relevant document token based on the alignment scores.

- Design tradeoffs:
  - Simple embeddings vs. contextualized representations: GR uses simple embeddings for efficiency, while MVDR uses contextualized representations for better semantic capture. PAWA and NP-decoding methods aim to bridge this gap.
  - Dense vs. sparse alignment: GR uses a dense alignment matrix for expressiveness, while MVDR uses a sparse matrix for efficiency. The choice affects the inference procedure and computational requirements.

- Failure signatures:
  - Poor alignment scores: If the attention mechanism fails to capture meaningful alignment between query and document tokens, the generated document identifiers may be irrelevant.
  - Overfitting to training data: If the model memorizes the training data instead of learning generalizable patterns, it may perform poorly on unseen queries or documents.

- First 3 experiments:
  1. Reproduce the framework equivalence: Verify that the relevance computation in GR can be reformulated as `rel(d, q) = sum(D^T Q âŠ™ A)`.
  2. Compare document encoding strategies: Implement and compare the performance of simple embeddings, PAWA-enhanced embeddings, and NP-decoding-based contextualized embeddings in GR.
  3. Analyze term matching behavior: Compute and compare the exact match rates in the alignment matrices of GR and MVDR to validate the commonality in term matching behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do multi-layer cross-attention interactions between query and document tokens affect the framework's relevance computation in generative retrieval?
- Basis in paper: [inferred] The paper explicitly states that multi-layer interactions in the cross-attention between query and document are not considered in their framework for simplicity.
- Why unresolved: The authors simplified their analysis by focusing on single-layer attention, but did not investigate how deeper transformer layers might modify or complicate the relevance computation framework.
- What evidence would resolve it: Experiments comparing single-layer versus multi-layer attention implementations in generative retrieval models, measuring how relevance scores and alignment matrices change with depth.

### Open Question 2
- Question: What are the generalization properties differences between dense retrieval and generative retrieval when encountering new documents, and how can these differences be explained by the proposed framework?
- Basis in paper: [inferred] The paper mentions that differences in generalization properties for new documents between DR and GR are an important aspect deserving further investigation, but this is not explored.
- Why unresolved: While the paper establishes the theoretical connection between GR and MVDR, it does not empirically investigate how well each paradigm generalizes to unseen documents.
- What evidence would resolve it: Controlled experiments testing both retrieval methods on datasets with novel documents, measuring performance degradation and analyzing how the alignment matrices and document encodings contribute to generalization.

### Open Question 3
- Question: How do different identifier designs and document encoding strategies in generative retrieval affect alignment and generalization during inference?
- Basis in paper: [explicit] The paper explicitly states they aim to study how different architectures and identifier designs will affect alignment and generalization during inference in future work.
- Why unresolved: The paper provides theoretical analysis but does not empirically test how various identifier types (atomic, hierarchical, semantic) and encoding strategies (PAWA, NP-decoding) impact retrieval performance and alignment behavior.
- What evidence would resolve it: Comparative experiments across multiple identifier designs and encoding methods, measuring retrieval accuracy, alignment matrix characteristics, and computational efficiency to determine optimal combinations for different use cases.

## Limitations

- Weak corpus evidence supporting the framework equivalence between generative retrieval and multi-vector dense retrieval
- Limited empirical validation of the proposed document encoding enhancements (PAWA and NP-decoding) in generative retrieval
- Insufficient evidence for the claimed commonalities in term matching behavior across different datasets

## Confidence

- Framework equivalence between generative retrieval and multi-vector dense retrieval: Medium
- Distinct document encoding strategies in generative retrieval: Low
- Commonalities in term matching behavior: Low

## Next Checks

1. Conduct a comprehensive literature review to gather more evidence supporting the framework equivalence between generative retrieval and multi-vector dense retrieval, focusing on the attention mechanism and relevance computation.
2. Implement and evaluate the proposed document encoding enhancements (PAWA and NP-decoding) in generative retrieval on a wider range of datasets to assess their effectiveness and generalizability.
3. Analyze the term matching behavior in the alignment matrices of generative retrieval and multi-vector dense retrieval on multiple datasets, comparing exact match rates and investigating the impact of different alignment strategies on retrieval performance.