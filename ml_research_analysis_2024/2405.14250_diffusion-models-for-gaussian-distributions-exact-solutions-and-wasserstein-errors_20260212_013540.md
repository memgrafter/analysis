---
ver: rpa2
title: 'Diffusion models for Gaussian distributions: Exact solutions and Wasserstein
  errors'
arxiv_id: '2405.14250'
source_url: https://arxiv.org/abs/2405.14250
tags:
- gaussian
- error
- wasserstein
- diffusion
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies diffusion models when the data distribution
  is Gaussian. The authors derive exact analytical solutions for both the backward
  SDE and probability flow ODE, proving these solutions and their discretizations
  are all Gaussian processes.
---

# Diffusion models for Gaussian distributions: Exact solutions and Wasserstein errors

## Quick Facts
- arXiv ID: 2405.14250
- Source URL: https://arxiv.org/abs/2405.14250
- Authors: Emile Pierret; Bruno Galerne
- Reference count: 40
- One-line primary result: Derives exact analytical solutions for backward SDE and probability flow ODE under Gaussian assumption, enabling exact Wasserstein error computation for any numerical scheme.

## Executive Summary
This paper provides a comprehensive analytical study of diffusion models when the data distribution is Gaussian. The authors derive exact solutions for both the backward SDE and probability flow ODE, proving these solutions and their discretizations are all Gaussian processes. By computing exact Wasserstein errors between target and sampled distributions for any numerical scheme, they enable direct error analysis in data space. The analysis reveals that for Gaussian data, the SDE sampler is more resilient to initialization error than the ODE sampler, with Heun's method achieving the lowest discretization error.

## Method Summary
The paper analyzes diffusion models under the assumption that the data distribution is Gaussian. It derives exact analytical solutions for the backward SDE and probability flow ODE, showing both solutions are Gaussian processes. The authors then compute exact Wasserstein errors between target and sampled distributions for any numerical scheme (EM, EI, DDPM for SDE; Euler, Heun, RK4 for ODE) by leveraging the fact that all discretized processes remain Gaussian with simultaneously diagonalizable covariance matrices. This enables closed-form error computation and comparison of different sampling approaches.

## Key Results
- Exact analytical solutions for backward SDE and probability flow ODE are derived under Gaussian assumption
- All numerical discretization schemes (EM, EI, DDPM, Euler, Heun, RK4) preserve Gaussianity, enabling exact Wasserstein error computation
- For Gaussian data with exact score function, SDE sampler is more resilient to initialization error than ODE sampler
- Heun's method achieves the lowest discretization error among the tested schemes

## Why This Works (Mechanism)

### Mechanism 1
Under Gaussian assumption, both the backward SDE and probability flow ODE have exact analytical solutions that are Gaussian processes. The score function becomes a linear operator under Gaussian assumption, enabling explicit integration of the stochastic and deterministic equations. The solutions are expressed in terms of matrix exponentials and the covariance matrix Σ.

### Mechanism 2
The discretization of backward SDE and probability flow ODE under Gaussian assumption preserves Gaussianity, allowing exact computation of Wasserstein errors. Each numerical scheme reduces to a linear transformation of Gaussian variables, producing Gaussian processes whose covariance matrices are simultaneously diagonalizable with Σ. This enables closed-form Wasserstein distance calculation.

### Mechanism 3
The SDE sampler is more resilient to initialization error than the ODE sampler when the exact score is known. The stochastic noise added at each SDE step helps mitigate accumulated errors from imperfect initialization, while the deterministic ODE propagates initialization errors more directly.

## Foundational Learning

- **Concept: Gaussian processes and their properties**
  - Why needed here: The paper relies on the fact that both the continuous solutions and their discretizations remain Gaussian processes, enabling exact analysis
  - Quick check question: If x ~ N(0, Σ) and A is a matrix, what is the distribution of Ax?

- **Concept: Wasserstein distance for Gaussian distributions**
  - Why needed here: The paper computes exact Wasserstein errors using the closed-form formula that depends only on eigenvalues of covariance matrices
  - Quick check question: For two centered Gaussians N(0, Σ1) and N(0, Σ2), what is the 2-Wasserstein distance in terms of their eigenvalues?

- **Concept: Stochastic differential equations and their numerical discretization**
  - Why needed here: Understanding how Euler-Maruyama, exponential integrator, and other schemes approximate the backward SDE is crucial for analyzing discretization errors
  - Quick check question: What is the Euler-Maruyama update for a SDE dy = f(t,y)dt + g(t,y)dw?

## Architecture Onboarding

- **Component map**: Forward SDE -> Backward SDE solution -> ODE solution -> Discretization schemes (EM, EI, DDPM, Euler, Heun, RK4) -> Error computation (Wasserstein distance)
- **Critical path**: 1) Verify Gaussian assumption on data 2) Compute covariance matrix Σ and its eigendecomposition 3) Choose numerical scheme based on error tolerance 4) Compute exact Wasserstein errors at each time step 5) Compare initialization, discretization, and truncation errors
- **Design tradeoffs**: SDE vs ODE: SDE more robust to initialization error but introduces additional stochastic noise; Discretization order: Higher order methods (Heun, RK4) reduce discretization error but may be unstable near t=0; Time truncation: Using ε > 0 avoids numerical instability but introduces truncation error
- **Failure signatures**: Non-Gaussian data: Analytical solutions break down, score function becomes nonlinear; Ill-conditioned covariance: Small eigenvalues cause numerical instability in matrix inversions; Large truncation time: Significant discrepancy between pε and p0 distributions
- **First 3 experiments**: 1) Implement forward SDE solution and verify covariance evolution matches Σt = e^(-2Bt)Σ + (1-e^(-2Bt))I 2) Compare Wasserstein errors for EM vs Heun schemes on synthetic Gaussian data 3) Test robustness to initialization by varying y0 ~ N(0,σ²I) with different σ values

## Open Questions the Paper Calls Out

### Open Question 1
Can the exact Wasserstein error analysis be extended to Gaussian mixture models (GMMs) for diffusion models? The authors discuss that while GMMs have known analytical scores, exact solutions for backward SDE/ODE and closed-form Wasserstein distances are unknown. This remains unresolved because GMMs lack closed-form solutions for backward SDE/ODE, no closed-form Wasserstein distance between GMMs exists, and discretized processes may not follow known distributions.

### Open Question 2
How does the score approximation error compare between deterministic and stochastic sampling schemes for non-Gaussian distributions? The authors found that for Gaussian microtextures, stochastic Euler-Maruyama sampling was more resilient to score approximation error than deterministic Heun's method, but this was only tested on Gaussian data. This is unresolved because the experiments were limited to Gaussian distributions, and it's unclear if the same robustness pattern holds for more complex, non-Gaussian data distributions.

### Open Question 3
What is the theoretical impact of different β parameterizations on eigenvalue contributions to Wasserstein error? The authors observed that eigenvalue contributions to error vary with different β parameterizations, but did not provide theoretical analysis of this relationship. This remains unresolved because while empirical observations were made, no theoretical framework explains how β parameterization choices systematically affect the error distribution across eigenvalues.

## Limitations
- All analytical results depend critically on the Gaussian assumption, which may not hold for real-world data distributions
- The exact solutions assume the score function is known analytically, which is only true in synthetic Gaussian cases - neural network approximations introduce unmodeled errors
- The analysis focuses on theoretical error bounds without extensive empirical validation on complex non-Gaussian datasets

## Confidence

- **High Confidence**: The analytical derivations for Gaussian processes under the exact score function assumption are mathematically rigorous and well-supported by the proofs provided.
- **Medium Confidence**: The comparison between SDE and ODE samplers for initialization error resilience is supported by theoretical analysis but would benefit from more extensive empirical validation across different initialization scales.
- **Medium Confidence**: The ranking of discretization schemes (Heun's method being optimal) is theoretically sound but assumes no approximation error in the score function, which may not hold in practical scenarios.

## Next Checks

1. **Gaussian Assumption Validation**: Test the analytical solutions on non-Gaussian synthetic distributions (e.g., mixture of Gaussians, Laplace) to quantify the breakdown of theoretical predictions and measure the error introduced by the Gaussian approximation.

2. **Approximation Error Characterization**: Implement the neural network score approximation for Gaussian microtextures and measure how the approximation error affects the relative performance of SDE vs ODE samplers, particularly testing whether the initialization error advantage of SDE persists under realistic approximation conditions.

3. **Eigenvalue Sensitivity Analysis**: Conduct a systematic study of how the condition number of the covariance matrix Σ affects the numerical stability and accuracy of different discretization schemes, particularly focusing on the ill-conditioning near t=0 that requires truncation time ε > 0.