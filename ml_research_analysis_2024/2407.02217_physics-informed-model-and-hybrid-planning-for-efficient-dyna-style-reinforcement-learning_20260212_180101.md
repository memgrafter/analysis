---
ver: rpa2
title: Physics-Informed Model and Hybrid Planning for Efficient Dyna-Style Reinforcement
  Learning
arxiv_id: '2407.02217'
source_url: https://arxiv.org/abs/2407.02217
tags:
- phihp
- learning
- performance
- planning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PhIHP addresses the trade-off between sample efficiency, time efficiency,
  and performance in reinforcement learning by leveraging partial physical knowledge
  of system dynamics. The method learns a physics-informed model by combining an approximate
  physical model with a learned residual, then trains a model-free policy and Q-function
  in imagination using trajectories generated from this model.
---

# Physics-Informed Model and Hybrid Planning for Efficient Dyna-Style Reinforcement Learning

## Quick Facts
- arXiv ID: 2407.02217
- Source URL: https://arxiv.org/abs/2407.02217
- Authors: Zakariae El Asri; Olivier Sigaud; Nicolas Thome
- Reference count: 40
- Key outcome: PhIHP achieves 5-15x better sample efficiency than TD-MPC, reaches higher asymptotic performance, and reduces inference time by 7-8x while maintaining competitive performance on classic control tasks.

## Executive Summary
This paper introduces PhIHP, a method that addresses the trade-off between sample efficiency, time efficiency, and performance in reinforcement learning by leveraging partial physical knowledge of system dynamics. The approach combines an approximate physical model with a learned residual to create a physics-informed model, then trains a model-free policy and Q-function in imagination using trajectories generated from this model. A hybrid planning strategy that combines the learned model with the policy and Q-function optimizes the performance vs. time efficiency trade-off. Experiments on 6 classic control tasks demonstrate that PhIHP achieves superior sample efficiency compared to TD-MPC while reducing inference time and reaching higher asymptotic performance.

## Method Summary
PhIHP learns a physics-informed model by combining an analytical physical model (describing idealized dynamics) with a learned residual network (capturing unmodeled phenomena like friction). This model is trained on real transition data and used to generate synthetic trajectories. A model-free policy and Q-function are then trained using TD3 on these imagined trajectories. During inference, a hybrid planning strategy combines the learned model with the policy and Q-function, using CEM to sample trajectories while incorporating policy guidance and Q-function evaluation to reduce planning horizon and inference time.

## Key Results
- PhIHP achieves 5-15x better sample efficiency than TD-MPC in terms of samples needed to reach 90% of maximum performance
- The method reduces inference time by 7-8x compared to TD-MPC while maintaining competitive performance
- PhIHP reaches higher asymptotic performance on all tested environments compared to both TD-MPC and pure model-free approaches
- The physics-informed model enables superior generalization to unseen states compared to fully data-driven approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The physics-informed model improves generalization to unseen states by combining an analytical physical model with a learned residual.
- Mechanism: The residual network captures phenomena not represented in the physical model (e.g., friction), while the physical model provides a stable prior that generalizes better than purely data-driven approaches.
- Core assumption: The physical model captures most of the system dynamics correctly, and the residual only needs to account for unmodeled phenomena.
- Evidence anchors:
  - [abstract] "The physics-informed model enables superior generalization to unseen states compared to fully data-driven approaches."
  - [section 4.1] "The physical modelFpθp is described by an ODE and the residual partFrθr as a neural network"
  - [corpus] Weak evidence - no direct citations found for generalization benefits
- Break condition: If the physical model is significantly wrong, the residual may need to learn too much, defeating the generalization advantage.

### Mechanism 2
- Claim: Learning a policy through imagination using the physics-informed model provides better sample efficiency than learning from real data.
- Mechanism: The agent generates synthetic trajectories from the learned model and trains a model-free policy on this data, reducing the need for expensive real-world interactions.
- Core assumption: The physics-informed model is accurate enough to generate useful training data that generalizes to the real environment.
- Evidence anchors:
  - [abstract] "PhIHP achieves 5-15x better sample efficiency than TD-MPC"
  - [section 4.2] "we preserve the sample efficiency by training a policy in an actor-critic fashion, using TD3 on trajectories generated from the learned model"
  - [corpus] Moderate evidence - Dyna-style approaches show similar benefits but with data-driven models
- Break condition: If the model has significant bias, the learned policy will perform poorly when deployed in the real environment.

### Mechanism 3
- Claim: The hybrid planning strategy combines the learned policy, Q-function, and model to optimize the performance vs. inference time tradeoff.
- Mechanism: The planner uses CEM to evaluate trajectories but incorporates the learned policy to generate informative candidates and the Q-function to evaluate long-term rewards, reducing the planning horizon needed.
- Core assumption: The learned policy and Q-function are sufficiently accurate to guide planning effectively.
- Evidence anchors:
  - [abstract] "reduces inference time by 7-8x compared to TD-MPC while maintaining competitive performance"
  - [section 4.3] "We incorporate the learned policy and Q-function in planning with the learned model"
  - [corpus] Strong evidence - TD-MPC and related hybrid approaches demonstrate similar benefits
- Break condition: If the policy or Q-function is inaccurate, the hybrid planning will produce suboptimal actions.

## Foundational Learning

- Concept: Ordinary Differential Equations (ODEs) for modeling continuous-time dynamics
  - Why needed here: The method uses ODE solvers to integrate state changes predicted by the dynamics model
  - Quick check question: Why would you model dynamics as dst/dt = f(st, at) rather than predicting st+1 directly?

- Concept: Residual learning and model decomposition
  - Why needed here: The physics-informed model combines an analytical model with a learned residual to correct for unmodeled effects
  - Quick check question: What advantage does decomposing the dynamics model into physical and residual parts provide over learning a single model?

- Concept: Model-free actor-critic methods (specifically TD3)
  - Why needed here: The policy and Q-function are learned using TD3 on trajectories generated from the learned model
  - Quick check question: How does using a learned model to generate training data affect the bias-variance tradeoff in policy learning?

## Architecture Onboarding

- Component map: Physical model (ODE) -> Residual network (MLP) -> Model integrator (ODE solver) -> Synthetic data generator -> Policy/Q learners (TD3) -> Hybrid planner (CEM with policy/Q guidance)
- Critical path: Model learning → Synthetic data generation → Policy/Q learning → Hybrid planning at inference
- Design tradeoffs:
  - Physical model accuracy vs. residual network complexity
  - Model learning time vs. policy learning time
  - Planning horizon length vs. inference time
  - Population size in CEM vs. planning quality
- Failure signatures:
  - Poor performance despite good training: likely model bias issues
  - High inference time: planning horizon or population size too large
  - Unstable learning: learning rates or model regularization need adjustment
- First 3 experiments:
  1. Validate physical model accuracy on a simple pendulum environment
  2. Test synthetic data generation quality by comparing model predictions vs. real trajectories
  3. Benchmark sample efficiency of imagination-based policy learning vs. real-data training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PhIHP scale to more complex continuous control tasks with higher-dimensional state and action spaces?
- Basis in paper: [inferred] The paper demonstrates PhIHP on 6 classic control tasks but notes these are simpler environments compared to real-world applications.
- Why unresolved: The experiments only cover relatively simple ODE-governed environments with low-dimensional states and actions.
- What evidence would resolve it: Empirical results on benchmark continuous control tasks like MuJoCo or real robotic manipulation tasks showing maintained sample efficiency and time efficiency improvements.

### Open Question 2
- Question: What is the theoretical guarantee for the performance of the hybrid planning strategy when the physics-informed model has residual errors?
- Basis in paper: [explicit] The paper mentions that compounding errors from inaccurate models can degrade solutions, and that the physics-informed model reduces but doesn't eliminate bias.
- Why unresolved: The paper provides empirical results but doesn't establish theoretical bounds on how model inaccuracies propagate through the hybrid planning.
- What evidence would resolve it: Formal analysis of error propagation through the planning pipeline, or empirical studies on how varying model accuracy affects the final performance bounds.

### Open Question 3
- Question: How sensitive is PhIHP to the choice of physical prior model, and can it automatically adapt when the true dynamics deviate significantly from the assumed physical laws?
- Basis in paper: [explicit] The paper discusses using an approximate physical model combined with a learned residual, but doesn't explore scenarios where the physical prior is substantially wrong.
- Why unresolved: The experiments use environments where the physical prior is close to correct, just missing friction terms.
- What evidence would resolve it: Experiments on environments with dynamics that violate the assumed physical laws, or methods for automatically adjusting the physical prior when learned residuals become large.

## Limitations

- The experiments are limited to 6 classic control tasks from the Gymnasium suite, which are relatively simple environments compared to real-world applications.
- The generalization benefits of the physics-informed model are demonstrated but not extensively validated across diverse physical systems or under significant model misspecification.
- While the paper mentions ablation studies, the specific contributions of each component (physical model, residual, hybrid planning) to the overall performance are not fully isolated.

## Confidence

- **High Confidence**: The claims about 5-15x better sample efficiency compared to TD-MPC and 7-8x reduction in inference time are well-supported by the experimental results presented in Tables 1 and 2, with clear statistical comparisons across multiple tasks.
- **Medium Confidence**: The claim about superior generalization to unseen states is supported by the experimental results but would benefit from more extensive testing on out-of-distribution states or different physical configurations to fully validate the mechanism.
- **Medium Confidence**: The effectiveness of the hybrid planning strategy in balancing performance and inference time is demonstrated but the specific contribution of each component (policy guidance, Q-function evaluation, model-based planning) to this balance is not fully decomposed.

## Next Checks

1. **Generalization Test**: Evaluate PhIHP on a modified version of the control tasks where the friction parameters are changed significantly from those used during training to test whether the physics-informed model truly generalizes better than purely data-driven approaches.

2. **Component Ablation**: Systematically disable the physical model component, the residual component, and the hybrid planning strategy individually to quantify their specific contributions to sample efficiency, asymptotic performance, and inference time.

3. **Model Bias Analysis**: Compare the performance degradation when using PhIHP's learned model versus a fully data-driven model when the physical model assumptions are deliberately made incorrect (e.g., wrong system parameters or incorrect ODE formulation).