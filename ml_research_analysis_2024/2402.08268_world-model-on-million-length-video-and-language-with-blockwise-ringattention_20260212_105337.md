---
ver: rpa2
title: World Model on Million-Length Video And Language With Blockwise RingAttention
arxiv_id: '2402.08268'
source_url: https://arxiv.org/abs/2402.08268
tags:
- video
- context
- training
- arxiv
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of scaling sequence models to
  process long contexts of up to 1 million tokens, a crucial capability for developing
  generally intelligent models that can handle extended temporal sequences in both
  language and video. The authors present a comprehensive approach that includes data
  curation from books and long videos, progressive context extension from 4K to 1M
  tokens, and an efficient implementation using Blockwise RingAttention for scalable
  training.
---

# World Model on Million-Length Video And Language With Blockwise RingAttention

## Quick Facts
- arXiv ID: 2402.08268
- Source URL: https://arxiv.org/abs/2402.08268
- Authors: Hao Liu; Wilson Yan; Matei Zaharia; Pieter Abbeel
- Reference count: 33
- Primary result: 7B parameter model processing 1M token contexts with competitive performance on long video understanding and language retrieval

## Executive Summary
This paper tackles the challenge of scaling sequence models to process long contexts of up to 1 million tokens, a crucial capability for developing generally intelligent models that can handle extended temporal sequences in both language and video. The authors present a comprehensive approach that includes data curation from books and long videos, progressive context extension from 4K to 1M tokens, and an efficient implementation using Blockwise RingAttention for scalable training. They also introduce techniques like masked sequence packing and model-generated question-answering to improve long-context performance. The resulting family of 7B parameter models, called Large World Model (LWM), achieves competitive results on long video understanding benchmarks and language retrieval tasks, outperforming state-of-the-art models at multi-needle retrieval in contexts up to 1M tokens.

## Method Summary
The authors develop LWM through a two-stage training approach. Stage I progressively extends context from 32K to 1M tokens using Blockwise RingAttention with FlashAttention for efficient long-context computation. Stage II integrates vision-language capabilities through joint training on text-image, text-video, and chat datasets using masked sequence packing. The model uses VQGAN for image tokenization, RoPE positional encodings with scaled parameters, and is trained on curated datasets including Books3, LAION-2B-en, COYO-700M, WebVid10M, and model-generated question-answer pairs from Books3 chunks.

## Key Results
- Achieves competitive performance on long video understanding benchmarks (Video-MME, MSVD, MSRVTT, TGIF)
- Outperforms state-of-the-art models at multi-needle retrieval in contexts up to 1M tokens
- Demonstrates strong performance on hour-long video comprehension tasks
- Can generate both images and videos from text, showcasing multimodal long-context capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Blockwise RingAttention enables efficient training on long sequences by reducing memory and computational costs while maintaining exact attention computation.
- Mechanism: Blockwise RingAttention partitions the attention computation across devices, allowing parallel processing of attention blocks without approximations. This is combined with FlashAttention for memory-efficient attention computation.
- Core assumption: The communication overhead during sequence parallelism is fully overlapped by computation when there are enough tokens per device.
- Evidence anchors:
  - [abstract] "We address these challenges by leveraging Blockwise RingAttention (Liu et al., 2024), a technique that scales context size without approximations or overheads, enabling efficient training on long sequences."
  - [section] "To address these computational limitations, we leverage recent advancements in scaling context window size, particularly Blockwise RingAttention (Liu et al., 2024). This approach theoretically allows for an infinite context, bounded only by available devices."
  - [corpus] "Average neighbor FMR=0.407" - Weak corpus evidence, no direct citations found.

### Mechanism 2
- Claim: Progressive training from shorter to longer context lengths improves model performance on long sequences while being cost-effective.
- Mechanism: The model is first trained on shorter sequences (32K tokens) to learn shorter-range dependencies, then gradually extended to longer sequences (up to 1M tokens) in powers of two.
- Core assumption: Learning shorter-range dependencies first provides a better foundation for learning longer-range dependencies later.
- Evidence anchors:
  - [section] "To mitigate computational costs, we gradually extended context size from an initial 4K tokens to 1M tokens, achieving a cost-effective and scalable approach for long-context modeling."
  - [section] "For better efficiency, we adopt a training approach inspired by prior research on extending context (Jin et al., 2023a), where our model is trained on progressively longer sequence lengths, starting from 32K tokens and ending at 1M tokens in increasing powers of two."
  - [corpus] "Average neighbor FMR=0.407" - Weak corpus evidence, no direct citations found.

### Mechanism 3
- Claim: Masked sequence packing with attention masking and loss reweighting enables effective training on mixed-length sequences while preserving performance on short-context tasks.
- Mechanism: During training on sequences of varying lengths, attention is masked so that each text-vision pair only attends to itself, and losses are reweighted to maintain computation identical to non-packed training.
- Core assumption: Properly masking attention and reweighting losses prevents degradation of performance on short-context tasks during mixed-length training.
- Evidence anchors:
  - [section] "During packing, we found it crucial to mask out the attention so that each text-vision pair only attends to itself, as well as re-weighting losses to make computation identical to training in a non-packed + padding training regime."
  - [section] "Naively packing shows large degradation in accuracy across image understanding tasks. We hypothesize naive packing degrades performance due to down-weighting text token answers which are shorter."
  - [corpus] "Average neighbor FMR=0.407" - Weak corpus evidence, no direct citations found.

## Foundational Learning

- Concept: Positional encodings for long sequences
  - Why needed here: Standard positional encodings are limited to fixed context lengths, but this work requires extending to 1M tokens
  - Quick check question: How does scaling the θ parameter in RoPE positional encodings enable extension to longer context lengths?

- Concept: Attention mechanisms and their computational complexity
  - Why needed here: Understanding the quadratic complexity of attention and how Blockwise RingAttention reduces this cost is crucial for implementing efficient long-context models
  - Quick check question: What is the computational complexity of standard self-attention, and how does Blockwise RingAttention reduce this complexity?

- Concept: Multimodal learning and data curation
  - Why needed here: The model needs to handle both text and visual modalities, requiring understanding of how to effectively combine and process different types of data
  - Quick check question: How does the model distinguish between text and vision tokens during generation, and what mechanisms are used to mark the end of text and vision sequences?

## Architecture Onboarding

- Component map:
  Base transformer architecture (Llama2 7B) -> Blockwise RingAttention for efficient long-context computation -> FlashAttention for memory-efficient attention -> RoPE positional encodings with scaled θ parameter -> VQGAN for image tokenization -> Byte-Pair Encoding tokenizer for text -> Masked sequence packing mechanism -> Model-generated question-answering data pipeline

- Critical path:
  1. Data curation and preprocessing (text and video)
  2. Progressive context extension training (Stage I)
  3. Multimodal joint training (Stage II)
  4. Model evaluation on benchmarks

- Design tradeoffs:
  - Using discrete tokens from VQGAN vs continuous embeddings for visual input
  - Progressive training vs. training on full context length from the start
  - Masked sequence packing vs. independent packing for mixed-length sequences

- Failure signatures:
  - Memory errors during training on long sequences
  - Degradation in short-context task performance after long-context training
  - Poor performance on video understanding tasks due to insufficient frame sampling

- First 3 experiments:
  1. Implement Blockwise RingAttention with FlashAttention and verify MFU improvement on a small-scale long-context task
  2. Test progressive training from 32K to 128K context on text-only data and measure performance on short-context benchmarks
  3. Implement masked sequence packing and compare performance on image understanding tasks against naive packing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LWM scale with larger model sizes (e.g., 70B or 70B+ parameters) while maintaining long-context capabilities?
- Basis in paper: [inferred] The paper discusses LWM as a 7B parameter model and mentions that being much smaller than current large language models (100B+ parameters), findings may not directly apply to them.
- Why unresolved: The paper only evaluates the 7B parameter version of LWM and does not explore how scaling to larger model sizes would affect long-context performance.
- What evidence would resolve it: Training and evaluating LWM models with 70B or more parameters on the same benchmarks, comparing their performance to the 7B version and other large language models.

### Open Question 2
- Question: What is the impact of using continuous CLIP embeddings as input in addition to discrete VQGAN tokens on LWM's performance across various tasks?
- Basis in paper: [explicit] The paper mentions that LWM uses discrete tokens from an off-the-shelf model (Patil et al., 2024) and states "A straightforward approach to improving benchmark scores would be to incorporate CLIP embeddings as additional input."
- Why unresolved: The paper does not experiment with or evaluate the use of CLIP embeddings alongside VQGAN tokens.
- What evidence would resolve it: Training LWM with both CLIP embeddings and VQGAN tokens as input, then comparing performance on image understanding, video understanding, and generation tasks against the original VQGAN-only version.

### Open Question 3
- Question: How does the quality and diversity of the model-generated question-answering data affect LWM's long-context chat capabilities?
- Basis in paper: [explicit] The paper describes creating model-generated question-answering data by prompting a short-context model to generate Q&A pairs from books, and notes this enhances chat capabilities over long sequences.
- Why unresolved: The paper does not explore variations in the quality or diversity of the generated Q&A data and their effects on model performance.
- What evidence would resolve it: Systematically varying the quality and diversity of the model-generated Q&A data (e.g., using different prompts, models, or data sources) and measuring the resulting impact on LWM's performance in long-context chat tasks.

## Limitations

- Lack of direct comparison against other long-context architectures like π-Attention or FlashAttention 2 on identical benchmarks
- No detailed computational cost comparisons between different training strategies
- Model-generated question-answering data pipeline effectiveness not empirically validated against simpler alternatives

## Confidence

**High Confidence:**
- The LWM models can process sequences up to 1M tokens with competitive performance on retrieval and understanding tasks
- Progressive training from shorter to longer contexts is feasible and improves long-context performance
- Blockwise RingAttention with proper sequence parallelism enables efficient training on long sequences
- Masked sequence packing with attention masking and loss reweighting is necessary to maintain short-context performance

**Medium Confidence:**
- The model-generated question-answering data significantly improves long sequence conversation abilities
- The specific RoPE θ scaling schedule (2.0, 1.8, 1.6, 1.4, 1.2) is optimal for progressive training
- The combination of Blockwise RingAttention and FlashAttention provides superior efficiency compared to alternative approaches

## Next Checks

1. **Ablation study on progressive training**: Train two models from scratch - one using progressive context extension (32K → 1M) and another trained directly on 1M tokens from initialization. Compare both final performance and total training compute requirements to validate the claimed efficiency benefits of progressive training.

2. **Independent validation of Blockwise RingAttention efficiency**: Implement the same 1M context model using FlashAttention 2 with sparse attention patterns (e.g., sliding window + global attention) and compare wall-clock training time and memory usage against the Blockwise RingAttention implementation on identical hardware.

3. **Evaluation of model-generated data quality**: Create a human-annotated subset of Books3 chunks with question-answer pairs and compare model performance on these human-annotated questions versus the model-generated questions. This would validate whether the automatic question generation process produces high-quality training data for long-context reasoning.