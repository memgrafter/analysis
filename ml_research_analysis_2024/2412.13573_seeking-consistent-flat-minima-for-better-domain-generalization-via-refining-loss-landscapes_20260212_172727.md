---
ver: rpa2
title: Seeking Consistent Flat Minima for Better Domain Generalization via Refining
  Loss Landscapes
arxiv_id: '2412.13573'
source_url: https://arxiv.org/abs/2412.13573
tags:
- loss
- domain
- domains
- training
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses domain generalization (DG) by proposing a
  Self-Feedback Training (SFT) framework that refines loss landscapes to achieve consistent
  flat minima across training domains. The key innovation is an iterative two-phase
  approach: (1) a feedback phase that measures inconsistency between domain-specific
  loss landscapes, and (2) a refinement phase that uses a landscape refiner to generate
  soft labels, encouraging flatter and more consistent loss surfaces.'
---

# Seeking Consistent Flat Minima for Better Domain Generalization via Refining Loss Landscapes

## Quick Facts
- arXiv ID: 2412.13573
- Source URL: https://arxiv.org/abs/2412.13573
- Reference count: 40
- Primary result: SFT improves domain generalization accuracy by +2.6% over SAM with ResNet-50 on five benchmarks

## Executive Summary
This paper addresses domain generalization (DG) by proposing a Self-Feedback Training (SFT) framework that refines loss landscapes to achieve consistent flat minima across training domains. The key innovation is an iterative two-phase approach: (1) a feedback phase that measures inconsistency between domain-specific loss landscapes, and (2) a refinement phase that uses a landscape refiner to generate soft labels, encouraging flatter and more consistent loss surfaces. Experiments on five DG benchmarks (PACS, VLCS, OfficeHome, TerraIncognita, DomainNet) show SFT improves over state-of-the-art sharpness-aware methods: +2.6% average accuracy over SAM with ResNet-50, and +1.5% with ViT-B/16. The method is also effective with large-scale vision transformers, demonstrating strong generalization performance and the importance of landscape consistency for DG.

## Method Summary
SFT is an iterative two-phase framework that alternately generates feedback signals by measuring loss landscape inconsistency across domains and refines these landscapes for greater consistency. In each iteration, training domains are randomly split into two sets: one for training and one for validation. The feedback phase computes sharpness differences between domains using zero-th and first-order flatness measures. The refinement phase optimizes a landscape refiner (sharing architecture with the main model) using a custom Projection Cross-Entropy (PCE) loss that encourages soft labels to be close to the true label space while promoting consistency. This process continues until convergence, producing models with flatter and more consistent minima across domains.

## Key Results
- SFT achieves 62.1% average accuracy on PACS with ResNet-50, outperforming SAM by 2.6% and ERM by 5.3%
- With ViT-B/16, SFT reaches 66.5% average accuracy on PACS, a 1.5% improvement over SAM
- SFT maintains strong performance across all five benchmarks (VLCS, OfficeHome, TerraIncognita, DomainNet) and is particularly effective with larger vision transformers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Landscape inconsistency across domains degrades generalization.
- Mechanism: Domain shifts create different loss geometries for each domain, so flat minima in one domain are not flat in others.
- Core assumption: Training loss landscapes can be compared and quantified via sharpness measures.
- Evidence anchors:
  - [abstract] "However, existing methods often neglect the consistency of loss landscapes in different domains, resulting in models that are not simultaneously in the optimal flat minima in all domains, which limits their generalization ability."
  - [section 2] "the domain shifts contribute to the drastic discrepancy of loss landscapes [9], and thus loss landscapes in test domains may not exhibit similar geometries as in training domains."
- Break condition: If domain shift is negligible, landscape inconsistency will be minimal and this mechanism won't be the bottleneck.

### Mechanism 2
- Claim: Self-feedback training reduces landscape inconsistency.
- Mechanism: The feedback phase measures sharpness differences between randomly held-out training domains; the refinement phase uses a refiner to generate soft labels that encourage flatter and more consistent landscapes.
- Core assumption: The refiner can generate soft labels that improve consistency without losing label correctness.
- Evidence anchors:
  - [abstract] "It alternatively generates a feedback signal by measuring the inconsistency of loss landscapes in different domains and refines these loss landscapes for greater consistency using this feedback signal."
  - [section 3.2.1] "we use the difference in loss sharpness, |LCES−SL Dd (θ, ϕ) − LCES−SL Dd′ (θ, ϕ)|, to quantify the landscape consistency, which serves as a feedback signal to the landscape refiner."
- Break condition: If the refiner overfits to the one-hot label space, the feedback signal may lead to worse generalization.

### Mechanism 3
- Claim: Projection cross-entropy (PCE) loss preserves label correctness while refining landscapes.
- Mechanism: PCE minimizes KL divergence between the refiner's output and the true label space, ensuring that soft labels stay close to the correct class while allowing for consistency.
- Core assumption: The KL divergence minimization problem can be solved efficiently and leads to valid soft labels.
- Evidence anchors:
  - [section 3.2.2] "we define the 'distance' between the output ˜y = gϕ(x) and the label space C1 as: miny KL(y || ˜y) subject to y ∈ C1"
  - [section 7] "we provide a detailed derivation of Algorithm 2, which is capable of finding the exact optimal solution to this optimization problem."
- Break condition: If the hyperparameter α is too small or too large, the soft labels may either be too close to one-hot (losing refinement benefit) or too spread out (losing correctness).

## Foundational Learning

- Concept: Loss landscape geometry and sharpness.
  - Why needed here: The paper's core contribution is refining loss landscapes to achieve flatter and more consistent minima; understanding flatness and sharpness is essential to grasp why this matters for generalization.
  - Quick check question: What is the difference between zeroth-order and first-order flatness, and why might both be relevant for domain generalization?

- Concept: Domain generalization (DG) vs domain adaptation.
  - Why needed here: SFT operates in a DG setting where test domains are unseen during training; this constraint drives the training-domain split scheme and the need for transferability of landscape consistency.
  - Quick check question: How does the absence of test data during training affect the design of methods that aim to improve out-of-domain performance?

- Concept: PAC-Bayesian generalization bounds.
  - Why needed here: The theoretical analysis in the supplement uses PAC-Bayesian framework to show that maintaining landscape consistency in training domains bounds the sharpness on test domains.
  - Quick check question: In the PAC-Bayesian bound, what role does the KL divergence between posterior and prior play in controlling generalization?

## Architecture Onboarding

- Component map:
  - Model fθ (ResNet-50 or ViT) -> Landscape refiner gϕ (same architecture) -> Soft labels -> Loss function (PCE + sharpness terms)

- Critical path:
  1. Initialize model and refiner
  2. Loop until convergence:
     - Select two domains Dd and Dd′
     - Update model on Dd using SAM with soft labels from refiner
     - Compute sharpness difference between Dd and Dd′ as feedback
     - Update refiner to minimize PCE + consistency penalty
  3. Return trained model

- Design tradeoffs:
  - Refiner architecture: Sharing architecture with model is simple but may be suboptimal; custom small refiner could be more efficient
  - Hyperparameters α, λ1, λ2: Control label space tightness and balance between label correctness and consistency; poor choices can hurt performance
  - Training-domain split: Using only two domains per iteration is efficient but may miss broader inconsistency patterns

- Failure signatures:
  - Model performance drops sharply when λ2 is too large (over-emphasis on consistency)
  - Refiner output becomes nearly one-hot even with large λ2 (fails to refine landscapes)
  - Landscape consistency improves but accuracy does not (feedback signal may not be well-calibrated)

- First 3 experiments:
  1. Toy dataset visualization: Verify that SFT produces more consistent loss surfaces across domains compared to one-hot labels
  2. Ablation on PCE loss: Remove PCE and use label smoothing instead; compare consistency and accuracy
  3. Hyperparameter sweep on λ2: Observe the tradeoff between landscape consistency and label correctness on a validation set

## Open Questions the Paper Calls Out

- Question: How does the landscape refiner's architecture (shared vs. separate from the model) affect performance and computational efficiency across different DG benchmarks?
  - Basis in paper: [explicit] The paper states "Unless otherwise specified, the landscape refiner shares the same architecture with the model f θ" but does not systematically compare different architectures.
  - Why unresolved: The paper only uses the shared architecture setting and does not explore alternatives or analyze trade-offs between architecture choices.
  - What evidence would resolve it: Systematic ablation studies comparing landscape refiners with different architectures (shared, separate, smaller/larger) across multiple DG benchmarks with corresponding efficiency measurements.

- Question: What is the theoretical relationship between the label space consistency promoted by PCE loss and the consistency of flat minima in loss landscapes?
  - Basis in paper: [inferred] The paper introduces PCE loss to maintain label correctness and mentions that soft labels help improve consistency of refined loss landscapes, but lacks theoretical justification for this connection.
  - Why unresolved: The paper provides empirical results showing effectiveness but does not establish a theoretical framework linking label space properties to landscape geometry consistency.
  - What evidence would resolve it: Formal theoretical analysis connecting KL divergence minimization over label spaces to properties of loss landscape flatness and consistency, potentially through PAC-Bayesian bounds or information-theoretic arguments.

- Question: How does the performance of SFT vary with different perturbation strengths ρ across diverse DG scenarios and model architectures?
  - Basis in paper: [explicit] The paper mentions "as the perturbation strength ρ increases, the accuracy peak on the test domain shifts to the left" and shows results for different ρ values in toy experiments, but does not provide comprehensive analysis across real datasets.
  - Why unresolved: The paper only shows limited results for ρ variation in toy experiments and uses fixed ρ values for real datasets without analyzing sensitivity.
  - What evidence would resolve it: Comprehensive sensitivity analysis of SFT performance across multiple ρ values on all five real DG benchmarks for both ResNet-50 and ViT architectures, identifying optimal ranges and failure modes.

## Limitations

- The method's effectiveness depends on the assumption that domain shifts create inconsistent loss landscapes, which may not hold for all DG scenarios
- The landscape refiner's architecture is fixed to share the main model's architecture, potentially limiting efficiency and performance
- The theoretical analysis in the supplement makes simplifying assumptions about domain shift and loss landscape geometry that may not generalize

## Confidence

- High Confidence: The empirical results showing SFT's improvement over SAM and ERM on five DG benchmarks, and the mechanism of using feedback to refine loss landscapes
- Medium Confidence: The theoretical analysis in the supplement and the claim that landscape consistency is a primary driver of DG performance
- Low Confidence: The exact mechanism by which the landscape refiner generates soft labels that improve consistency without losing label correctness, and the generalizability of the method to other architectures

## Next Checks

1. **Ablation on Refiner Architecture**: Replace the refiner with a simpler architecture (e.g., linear layer) and compare landscape consistency and accuracy. This will test whether the refiner's architecture is crucial for performance.

2. **Theoretical Analysis Extension**: Extend the PAC-Bayesian analysis to include a bound on the KL divergence between the refiner's output and the true label space. This will provide a more complete understanding of the method's theoretical guarantees.

3. **Visualization of Loss Landscapes**: Generate 2D slices of the loss landscapes for SFT, SAM, and ERM on a held-out domain. Compare the flatness and consistency of the minima to provide visual evidence for the paper's claims.