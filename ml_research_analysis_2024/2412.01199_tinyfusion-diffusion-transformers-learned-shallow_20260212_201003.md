---
ver: rpa2
title: 'TinyFusion: Diffusion Transformers Learned Shallow'
arxiv_id: '2412.01199'
source_url: https://arxiv.org/abs/2412.01199
tags:
- pruning
- diffusion
- layer
- arxiv
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TinyFusion, a learnable depth pruning method
  for compressing diffusion transformers. The core idea is to optimize both the layer
  mask sampling process and weight updates end-to-end, explicitly targeting recoverability
  after fine-tuning rather than immediate post-pruning loss.
---

# TinyFusion: Diffusion Transformers Learned Shallow
## Quick Facts
- arXiv ID: 2412.01199
- Source URL: https://arxiv.org/abs/2412.01199
- Authors: Gongfan Fang; Kunjun Li; Xinyin Ma; Xinchao Wang
- Reference count: 40
- Primary result: Achieves up to 2× speedup with minimal quality loss by optimizing layer mask sampling and weight updates end-to-end for diffusion transformer compression

## Executive Summary
TinyFusion introduces a learnable depth pruning method for compressing diffusion transformers by jointly optimizing layer mask sampling and parameter updates. Unlike traditional pruning methods that focus on immediate post-pruning performance, TinyFusion explicitly targets recoverability after fine-tuning by integrating differentiable sampling with LoRA-based parameter updates. The method identifies high-recoverability layer pruning patterns efficiently and demonstrates strong performance across multiple architectures including DiT-XL, MAR-Large, and SiT-XL, achieving significant speedups with minimal quality degradation.

## Method Summary
TinyFusion employs a two-stage optimization approach: first, it learns a binary mask to identify which transformer layers to prune through differentiable sampling, then co-optimizes this mask with weight updates using LoRA to ensure recoverability after fine-tuning. The method introduces a masked knowledge distillation strategy where the pruned model learns from the full model during training, and uses a differentiable sampling technique to make the pruning process end-to-end trainable. By focusing on the recoverability of the pruned model rather than immediate post-pruning performance, TinyFusion can identify optimal layer combinations that maintain quality while achieving significant computational savings.

## Key Results
- Achieves up to 2× inference speedup on DiT-XL while maintaining FID of 2.86 after fine-tuning
- Outperforms existing layer pruning methods across multiple architectures with minimal quality loss
- Reduces pre-training cost to just 7% while recovering most of the original model's performance

## Why This Works (Mechanism)
The method works by recognizing that traditional pruning focuses on immediate performance degradation rather than long-term recoverability. By co-optimizing the layer mask with parameter updates through differentiable sampling, TinyFusion can identify layer combinations that are structurally easier to recover during fine-tuning. The LoRA-based update mechanism allows for efficient parameter adaptation while maintaining the pruned architecture's integrity, and the masked knowledge distillation ensures that the pruned model learns to mimic the full model's behavior effectively.

## Foundational Learning
- **Diffusion Transformers**: Why needed - Core architecture being compressed; quick check - Understand U-Net structure with transformer blocks
- **Layer-wise Pruning**: Why needed - Fundamental compression technique; quick check - Know difference between structured vs unstructured pruning
- **Differentiable Sampling**: Why needed - Enables end-to-end training of pruning decisions; quick check - Understand Gumbel-Softmax trick
- **LoRA (Low-Rank Adaptation)**: Why needed - Efficient parameter update mechanism; quick check - Know how low-rank decomposition works
- **Knowledge Distillation**: Why needed - Helps pruned model learn from full model; quick check - Understand teacher-student training paradigm
- **Recoverability Optimization**: Why needed - Key insight distinguishing this method; quick check - Compare pre-pruning vs post-fine-tuning performance metrics

## Architecture Onboarding
- **Component Map**: Input -> Layer Selection Module -> Pruned Transformer Blocks -> Output
- **Critical Path**: Data flows through selected transformer layers only, with skipped layers bypassed via learned mask
- **Design Tradeoffs**: Structured layer pruning vs. flexible attention/channel pruning; end-to-end optimization vs. two-stage approaches
- **Failure Signatures**: Poor recoverability after fine-tuning, inconsistent performance across different pruning ratios, failure to generalize to new architectures
- **First Experiments**: 1) Verify differentiable sampling produces reasonable layer masks, 2) Test recoverability on simplified toy model, 3) Compare pre-pruning vs post-fine-tuning performance curves

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the performance of TinyFusion compare when applied to text-to-image diffusion models like Stable Diffusion?
- Basis in paper: [inferred] The paper mentions that TinyFusion has been tested on class-conditional image generation models like DiT-XL, MAR-Large, and SiT-XL, but does not discuss its application to text-to-image models.
- Why unresolved: The paper focuses on compressing models for class-conditional image generation and does not explore the effectiveness of TinyFusion on text-to-image tasks, which are more complex and widely used in practice.
- What evidence would resolve it: Conducting experiments to apply TinyFusion to text-to-image models like Stable Diffusion and comparing the performance metrics (e.g., FID, inference speed) with the original models.

### Open Question 2
- Question: What is the impact of removing attention layers and MLP layers independently in TinyFusion, rather than removing entire transformer blocks?
- Basis in paper: [inferred] The paper mentions that TinyFusion removes entire transformer layers, but does not explore the possibility of independently removing attention layers and MLP layers.
- Why unresolved: The paper does not investigate whether selectively removing attention or MLP layers could lead to better performance or efficiency compared to removing entire transformer blocks.
- What evidence would resolve it: Performing experiments to compare the performance of TinyFusion when removing attention layers and MLP layers independently versus removing entire transformer blocks, and analyzing the trade-offs in terms of accuracy and speed.

### Open Question 3
- Question: How does the choice of local pruning scheme (e.g., 1:2, 2:4, 7:14) affect the performance and efficiency of TinyFusion?
- Basis in paper: [explicit] The paper discusses different local pruning schemes (1:2, 2:4, 7:14) and their impact on the search space and optimization difficulty.
- Why unresolved: While the paper provides some insights into the effects of different pruning schemes, it does not fully explore how these choices impact the overall performance and efficiency of the compressed models.
- What evidence would resolve it: Conducting a comprehensive study to evaluate the performance and efficiency of TinyFusion using different local pruning schemes, and determining the optimal scheme for various model architectures and tasks.

## Limitations
- Method focuses exclusively on layer pruning, missing opportunities from attention head or channel pruning
- Limited evaluation scope to text-to-image diffusion transformers and two autoregressive models
- Computational savings claims based on theoretical FLOPs rather than actual hardware measurements

## Confidence
- Core algorithm: High
- Performance claims: Medium
- Generalization claims: Medium

## Next Checks
1. Benchmark wall-clock inference time reductions on GPU/CPU to verify claimed speedup claims beyond theoretical FLOPs analysis
2. Test the method on additional transformer architectures including language models and vision transformers to assess true generalization capability
3. Compare against state-of-the-art unstructured pruning methods to establish whether structured layer pruning is the optimal approach for diffusion transformers