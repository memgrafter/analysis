---
ver: rpa2
title: Active Preference Learning for Large Language Models
arxiv_id: '2402.08114'
source_url: https://arxiv.org/abs/2402.08114
tags:
- preference
- fine-tuning
- learning
- data
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of efficiently fine-tuning large
  language models using human or AI preference data. The authors propose an active
  learning approach for Direct Preference Optimization (DPO) that iteratively selects
  the most informative prompt/completion pairs for preference labeling, rather than
  randomly sampling a large fixed dataset upfront.
---

# Active Preference Learning for Large Language Models

## Quick Facts
- **arXiv ID**: 2402.08114
- **Source URL**: https://arxiv.org/abs/2402.08114
- **Reference count**: 14
- **Primary result**: Active learning approach for DPO improves win-rate by 1-6% compared to random sampling

## Executive Summary
This paper introduces an active learning framework for Direct Preference Optimization (DPO) that efficiently fine-tunes large language models using human or AI preference data. The authors propose acquisition functions that prioritize prompt/completion pairs where the model is confidently wrong about preferences, combining predictive entropy with preference certainty metrics. Experiments on IMDB and TLDR datasets using 1-billion parameter models demonstrate 1-6% average improvements in win-rate performance compared to random sampling approaches.

## Method Summary
The authors develop an active learning framework for DPO that iteratively selects the most informative examples for preference labeling. Their approach uses acquisition functions combining predictive entropy of the language model with certainty metrics from DPO's implicit preference model. The framework employs GPT-4 as an oracle for preference labeling and evaluation, with active learning loops that acquire batches of examples, fine-tune the model, and repeat until budget exhaustion. The method targets examples where the model is confidently wrong about preferences, aiming to maximize learning efficiency from limited preference data.

## Key Results
- Preference certainty acquisition strategy achieves 1-6% average win-rate improvement over random sampling
- Hybrid approach combining entropy and preference certainty performs consistently well across datasets
- GPT-4 oracle demonstrates >90% self-consistency in preference judgments versus ~60% for GPT-3.5-turbo

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Active sampling based on preference certainty improves DPO fine-tuning by targeting confidently wrong implicit preference predictions
- **Mechanism**: The preference certainty acquisition function identifies prompt/completion pairs where the model's implicit Bradley-Terry reward model is highly confident in an incorrect ranking, then prioritizes these examples for oracle labeling and fine-tuning
- **Core assumption**: Examples where the model is confidently wrong about preferences provide more informative gradient updates than randomly sampled examples
- **Evidence anchors**: [abstract]: "We propose a practical acquisition function for prompt/completion pairs based on the predictive entropy of the language model and a measure of certainty of the implicit preference model optimized by DPO." [section 3.1.2]: "We define a function we refer to as the certainty of the implicit preference model using y1, y2 ~ pθt(y|x) that is maximised when the difference between the implicit rewards for y1 and y2 is large and minimised when it's small."

### Mechanism 2
- **Claim**: Combining entropy and preference certainty acquisition creates a hybrid approach that balances uncertainty and preference model correctness
- **Mechanism**: First rank prompts by predictive entropy to identify uncertain examples, then generate completions and rank by preference certainty to select the most informative subset for fine-tuning
- **Core assumption**: High entropy prompts are more likely to contain completions that the preference model ranks incorrectly, making them valuable learning opportunities
- **Evidence anchors**: [section 3.1.3]: "In practise we can combine both entropy and preference certainty as complimentary metrics for scoring data to exploit the strengths of both." [section 3.1.1]: "Prior work has shown the predictive entropy to be a well calibrated measure of uncertainty in LLMs."

### Mechanism 3
- **Claim**: Online active learning with incremental fine-tuning is more computationally efficient than re-initializing and fine-tuning from scratch each iteration
- **Mechanism**: Instead of resetting model parameters θt to θ0 at each acquisition step, perform a single gradient update on the most recently acquired batch, reducing fine-tuning time and GPU memory requirements
- **Core assumption**: Incremental updates on the latest data are sufficient to maintain learning progress without catastrophic forgetting of previously acquired preferences
- **Evidence anchors**: [section 6]: "A promising direction of future work is to integrate approaches from online learning... This could significantly improve computational efficiency by allowing us to not re-initialise the parameters at each time step."

## Foundational Learning

- **Concept**: Bradley-Terry model for pairwise preference ranking
  - **Why needed here**: Forms the theoretical foundation for the implicit preference model in DPO and the certainty acquisition metric
  - **Quick check question**: How does the Bradley-Terry model assign probabilities to preference orderings, and what assumptions does it make about the underlying utility function?

- **Concept**: Kullback-Leibler (KL) divergence regularization
  - **Why needed here**: Used in DPO to prevent the fine-tuned model from deviating too far from the pre-trained model's distribution
  - **Quick check question**: What role does the β parameter play in the KL regularization term, and how does it balance between following preferences and staying close to the original model?

- **Concept**: Monte Carlo approximation for predictive entropy
  - **Why needed here**: Enables practical computation of the entropy acquisition metric by sampling completions from the model
  - **Quick check question**: Why is Monte Carlo approximation necessary for computing predictive entropy, and how does the number of samples affect the accuracy of the approximation?

## Architecture Onboarding

- **Component map**: prompt pool → completion generation → scoring → oracle labeling → preference dataset → DPO fine-tuning → evaluation
- **Critical path**: The sequence from data acquisition to oracle labeling to fine-tuning is the most time-sensitive part, as it directly affects the total budget consumption and learning progress
- **Design tradeoffs**: 
  - Batch size vs. frequency: Larger acquisition batches reduce oracle queries but may slow learning; smaller batches increase query frequency but improve responsiveness
  - Model re-initialization vs. online updates: Re-initialization ensures isolation of acquisition effects but is computationally expensive; online updates are efficient but may accumulate bias
  - Oracle choice: GPT-4 provides high-quality labels but is expensive; smaller models are cheaper but may have lower consistency
- **Failure signatures**: 
  - If win-rate plateaus early, the acquisition function may not be selecting informative examples
  - If performance degrades over iterations, the fine-tuning procedure may be causing catastrophic forgetting
  - If oracle costs exceed budget, the acquisition function may be too selective or the completion generation too expensive
- **First 3 experiments**:
  1. Implement and validate the preference certainty acquisition function on a small dataset to confirm it surfaces confidently wrong examples
  2. Compare random, entropy, certainty, and hybrid acquisition strategies on IMDB to establish baseline performance differences
  3. Test the online fine-tuning variant (single gradient step) vs. full re-initialization to measure efficiency gains and performance trade-offs

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the preference certainty acquisition strategy work equally well for online learning where parameters are not reset between fine-tuning steps?
  - **Basis in paper**: [explicit] The authors mention that re-initializing parameters at each step is a limitation and suggest online learning as future work, with preliminary results showing preference certainty still outperforms random in online setting
  - **Why unresolved**: The online learning results were preliminary with only 3 random seeds and a single gradient step per batch, lacking comprehensive evaluation across different dataset sizes and multiple epochs
  - **What evidence would resolve it**: A comprehensive study comparing random vs preference certainty acquisition in online learning across multiple dataset sizes, different numbers of fine-tuning epochs, and multiple random seeds showing consistent performance differences

- **Open Question 2**: Would incorporating diversity measures into the acquisition function improve performance beyond the preference certainty approach?
  - **Basis in paper**: [inferred] The authors suggest this as a promising direction in the conclusion, noting that diversity within batches could be an additional factor to consider
  - **Why unresolved**: The current study focused on simple acquisition functions (entropy, preference certainty, hybrid) without exploring diversity-based approaches or combinations with other active learning techniques
  - **What evidence would resolve it**: Experiments comparing the current acquisition strategies against diversity-enhanced variants across multiple datasets, showing whether diversity measures provide additional performance gains

- **Open Question 3**: Can smaller, more economical LLM oracles be engineered to provide preference judgments with quality comparable to GPT-4?
  - **Basis in paper**: [explicit] The authors note that GPT-4 was far more consistent (>90%) than GPT-3.5-turbo (only ~60%) in providing self-consistent preference labels, but suggest this could be improved with prompt engineering
  - **Why unresolved**: The analysis used the same prompt for both models without extensive prompt engineering, and the study was limited to consistency rather than overall quality or correlation with human preferences
  - **What evidence would resolve it**: A comprehensive study testing different prompt engineering techniques on smaller models (GPT-3.5, LLaMA, etc.) measuring both self-consistency and correlation with human judgments across multiple datasets and tasks

## Limitations
- Heavy reliance on GPT-4 as oracle introduces potential bias and consistency concerns
- Computational efficiency claims for online learning remain theoretical (re-initialization used instead)
- Results demonstrated only on 1-billion parameter models, leaving scalability uncertain
- Active learning may amplify existing biases in preference data

## Confidence
- **High confidence**: Theoretical foundation of DPO and preference certainty acquisition metrics
- **Medium confidence**: Empirical improvements (1-6% win-rate gains) but may not generalize beyond IMDB/TLDR
- **Low confidence**: Computational efficiency claims for online learning variants (not implemented)

## Next Checks
1. **Oracle consistency validation**: Run multiple GPT-4 preference labeling rounds on the same examples to quantify inter-rater reliability and identify potential bias patterns
2. **Scalability assessment**: Implement active learning framework on larger models (7B+ parameters) to verify performance improvements scale and test practicality of acquisition computation
3. **Online learning implementation**: Develop and test incremental fine-tuning approach (single gradient step vs. full re-initialization) to measure actual computational savings and identify performance degradation