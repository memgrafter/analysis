---
ver: rpa2
title: Transformers Use Causal World Models in Maze-Solving Tasks
arxiv_id: '2412.11867'
source_url: https://arxiv.org/abs/2412.11867
tags:
- features
- figure
- maze
- head
- right
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies causal world models in transformers trained
  on maze-solving tasks. The authors use sparse autoencoders (SAEs) to extract interpretable
  features from the residual stream, and compare these with attention patterns in
  early layers.
---

# Transformers Use Causal World Models in Maze-Solving Tasks

## Quick Facts
- arXiv ID: 2412.11867
- Source URL: https://arxiv.org/abs/2412.11867
- Reference count: 40
- This paper identifies causal world models in transformers trained on maze-solving tasks using sparse autoencoders and attention analysis.

## Executive Summary
This paper demonstrates that transformers trained on maze-solving tasks develop interpretable world models (WMs) of maze connectivity. The authors use sparse autoencoders (SAEs) and attention pattern analysis to identify and validate these world models, showing that maze connectivity information is consolidated into specific features that causally influence maze-solving behavior. They further show that these models can generalize to larger mazes through latent space interventions even when they fail on longer input sequences. The work also reveals that positional encoding schemes influence how world models are structured within the transformer's residual stream.

## Method Summary
The researchers trained two transformer models (Stan with learned positional embeddings and Terry with rotary positional embeddings) on maze-solving tasks using the maze-dataset library. They then trained sparse autoencoders on the residual stream after the first layer to extract interpretable features. Through attention analysis, decision tree classification, and causal interventions, they validated that these features represent maze connectivity information and causally influence maze-solving behavior. The intervention experiments involved toggling feature activations to test their causal impact on model performance.

## Key Results
- Transformers develop structured representations of maze connectivity that are causally linked to maze-solving behavior
- Models can generalize to larger mazes through latent space interventions even when they fail on longer input sequences
- Different positional encoding schemes lead to different world model representation strategies (compositional vs. direct encoding)
- Activating features tends to be more effective than removing them for altering model behavior

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transformers develop causally efficacious world models through structured attention patterns that consolidate maze connectivity information.
- **Mechanism:** Early layer attention heads attend from semicolon tokens back to coordinate tokens, constructing representations of maze connections that are causally linked to maze-solving behavior.
- **Core assumption:** The attention patterns observed are not random but represent deliberate feature construction for maze-solving.
- **Evidence anchors:**
  - [abstract] "we identify WMs in transformers trained on maze-solving tasks. By using Sparse Autoencoders (SAEs) and analyzing attention patterns, we examine the construction of WMs"
  - [section] "we found that connection information was consolidated into semicolon tokens by a subset of attention heads"
  - [corpus] Weak evidence - related work focuses on graph WMs but doesn't directly address transformer attention mechanisms

### Mechanism 2
- **Claim:** Sparse autoencoders (SAEs) can isolate disentangled features that represent maze connectivity, enabling causal interventions.
- **Mechanism:** SAEs trained on residual stream activations after layer 0 extract sparse features that encode maze connections, which can be manipulated to alter model behavior predictably.
- **Core assumption:** The SAE latent space captures the same features as the attention-based construction but in a more accessible, disentangled form.
- **Evidence anchors:**
  - [abstract] "By using Sparse Autoencoders (SAEs) and analyzing attention patterns, we examine the construction of WMs and demonstrate consistency between SAE feature-based and circuit-based analyses"
  - [section] "we trained Sparse Autoencoders to attempt to find disentangled features in our models...the hope is that the SAE will recover interpretable features"
  - [corpus] Weak evidence - related work on SAEs for LLMs but not specifically for maze-solving tasks

### Mechanism 3
- **Claim:** Positional encoding schemes influence how world models are structured within the transformer's residual stream.
- **Mechanism:** Models using learned positional embeddings develop compositional codes for connections, while those using rotary positional embeddings encode connections more directly.
- **Core assumption:** The choice of positional encoding scheme affects the internal representation strategy of the model.
- **Evidence anchors:**
  - [abstract] "Finally, we demonstrate that positional encoding schemes appear to influence how World Models are structured within the model's residual stream"
  - [section] "It is also interesting to note that Terry encoded connection information very cleanly into single features...Stan's WM consisted of two features for each connection"
  - [corpus] Weak evidence - related work mentions positional encodings but doesn't specifically address their impact on world model structure

## Foundational Learning

- **Concept:** Sparse Autoencoders (SAEs)
  - Why needed here: SAEs can isolate disentangled features from the residual stream that represent maze connectivity, enabling causal interventions.
  - Quick check question: What is the primary advantage of using SAEs over linear probes for identifying world model features?

- **Concept:** Attention mechanisms in transformers
  - Why needed here: Understanding how attention patterns consolidate information is crucial for identifying how world models are constructed.
  - Quick check question: What specific pattern do the attention heads show when consolidating maze connectivity information?

- **Concept:** Positional encoding schemes
  - Why needed here: Different positional encoding schemes (learned vs. rotary) affect how the model structures its world model representations.
  - Quick check question: How do the world model representations differ between models using learned versus rotary positional encodings?

## Architecture Onboarding

- **Component map:** Input maze (adjacency list, origin, target, path) -> Transformer (6 layers, varying heads, positional encodings) -> SAE (residual stream after layer 0) -> Output path prediction
- **Critical path:** 1. Attention heads in layer 0 consolidate maze connectivity information at semicolon tokens 2. SAEs extract sparse features from residual stream representing these connections 3. Interventions on these features alter model behavior predictably
- **Design tradeoffs:**
  - Learned positional embeddings allow compositional encoding but limit generalization to longer sequences
  - Rotary positional embeddings enable better sequence generalization but produce less structured world models
  - SAE sparsity vs. reconstruction fidelity tradeoff affects feature isolation quality
- **Failure signatures:**
  - Attention patterns that don't consistently consolidate connectivity information
  - SAE features that don't activate predictably or fail to reconstruct the residual stream accurately
  - Interventions that don't produce predictable changes in maze-solving behavior
- **First 3 experiments:**
  1. Analyze attention patterns in layer 0 to identify heads consolidating connectivity information
  2. Train SAEs on residual stream and identify features corresponding to maze connections
  3. Perform interventions by activating/removing specific SAE features and observe changes in maze-solving accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise computational mechanism by which transformers with learned positional embeddings fail to generalize to longer input sequences?
- Basis in paper: [explicit] "the fact that activating connections in the space of the SAE worked at all means that Stan's maze-solving behaviour was at least partially able to generalize in the latent space, where it was decoupled from the positional embeddings"
- Why unresolved: The paper demonstrates that Stan fails on longer input sequences but can generalize in latent space through SAE interventions, but does not explain the underlying computational reason for this failure.
- What evidence would resolve it: Detailed analysis of positional encoding mechanisms and how they interact with the world model representations, potentially through ablation studies isolating positional embedding effects.

### Open Question 2
- Question: Are the intervention asymmetries (activating vs. removing features) universal across different transformer architectures and tasks?
- Basis in paper: [explicit] "interventions that activate features tend to be more effective in altering the model's behavior compared to those that remove features"
- Why unresolved: The paper only demonstrates this asymmetry in two maze-solving transformer models, leaving open whether this is a general phenomenon or specific to these architectures and tasks.
- What evidence would resolve it: Systematic testing of activation vs. removal interventions across diverse transformer models and tasks, with quantitative comparison of intervention efficacy.

### Open Question 3
- Question: What determines whether a transformer uses a compositional code (like Stan) or a direct encoding (like Terry) for world model features?
- Basis in paper: [explicit] "it is also interesting to note that Terry encoded connection information very cleanly into single features for each connection - i.e., a compositional code"
- Why unresolved: The paper observes different encoding strategies but does not identify the factors that determine which strategy a transformer will adopt.
- What evidence would resolve it: Controlled experiments varying model architecture parameters, training procedures, and data characteristics to identify which factors influence encoding strategy choice.

### Open Question 4
- Question: How do the different attention patterns in Stan and Terry heads contribute to their respective world model representations?
- Basis in paper: [explicit] "Terry's attention heads appeared to operate in a more entangled fashion than those of Stan" and the detailed attention pattern analysis in figures 3 and 14
- Why unresolved: While the paper identifies different attention patterns and their effects on world model construction, it does not explain how these patterns mechanistically contribute to the different representation strategies.
- What evidence would resolve it: Detailed mechanistic analysis tracing how specific attention patterns lead to particular feature representations, potentially through circuit analysis or intervention studies on attention mechanisms.

## Limitations
- Analysis is constrained to two specific transformer architectures and simple maze tasks
- SAE feature identification relies heavily on correlation-based analysis rather than purely causal methods
- Intervention experiments focus primarily on binary feature activation rather than exploring continuous feature values or their interactions

## Confidence
- **High confidence**: The core finding that transformers develop interpretable world models for maze connectivity is well-supported by multiple converging lines of evidence
- **Medium confidence**: The distinction between compositional versus direct encoding strategies based on positional encoding schemes is plausible but requires more systematic investigation
- **Medium confidence**: The claim about activating versus removing features being differentially effective is supported but the underlying mechanism remains incompletely explained

## Next Checks
1. Test whether the identified world model features generalize to transformers with different depths, widths, or attention mechanisms
2. Evaluate whether the SAE-identified features maintain their causal efficacy when applied to mazes of different sizes or topologies not seen during training
3. Conduct targeted experiments to distinguish between attention-based consolidation versus SAE-based feature extraction as the primary mechanism for world model construction