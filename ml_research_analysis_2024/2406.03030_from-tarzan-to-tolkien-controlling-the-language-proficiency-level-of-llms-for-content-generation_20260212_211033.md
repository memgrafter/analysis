---
ver: rpa2
title: 'From Tarzan to Tolkien: Controlling the Language Proficiency Level of LLMs
  for Content Generation'
arxiv_id: '2406.03030'
source_url: https://arxiv.org/abs/2406.03030
tags:
- language
- proficiency
- story
- cefr
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how to control the proficiency level of text
  generated by large language models, which is important for applications like language
  learning. The authors define a new framework for evaluating proficiency control,
  measuring both how well the model controls language level and how relevant the content
  is.
---

# From Tarzan to Tolkien: Controlling the Language Proficiency Level of LLMs for Content Generation

## Quick Facts
- arXiv ID: 2406.03030
- Source URL: https://arxiv.org/abs/2406.03030
- Reference count: 30
- Primary result: Fine-tuning open-source LLMs on GPT-4-generated data and applying reinforcement learning enables proficiency control comparable to GPT-4 at much lower cost

## Executive Summary
This paper addresses the challenge of controlling the language proficiency level of text generated by large language models, which is crucial for applications like language learning. The authors introduce a framework for evaluating proficiency control that measures both the accuracy of proficiency level control (ControlError) and content relevance. They find that while GPT-4 excels at this task, open-source models like Llama2-7B and Mistral-7B struggle. To bridge this gap, they develop a method that combines fine-tuning on GPT-4-generated data with reinforcement learning, resulting in a model called CALM that achieves proficiency control comparable to GPT-4 but at significantly lower computational cost.

## Method Summary
The authors define a new task called Proficiency Control Task (PCT), where models must generate text at a specified CEFR level. They evaluate several prompting strategies and find GPT-4 outperforms open-source models. To improve open-source models, they generate a dataset (TinyTolkien) of stories at different CEFR levels using GPT-4, then fine-tune Llama2-7B and Mistral-7B on this data with control tokens. They further refine these models using reinforcement learning with PPO, optimizing for alignment with an automatic CEFR scorer. They also introduce a top-k sampling strategy that can arbitrarily reduce ControlError by selecting the best output from k samples.

## Key Results
- GPT-4 significantly outperforms open-source models on proficiency control via prompting alone
- Fine-tuning open-source models on GPT-4-generated data substantially improves proficiency control
- Reinforcement learning with PPO further aligns outputs with target proficiency levels
- The resulting CALM model achieves proficiency control comparable to GPT-4 at much lower cost
- Top-k sampling can arbitrarily reduce ControlError but increases computational expense

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning on GPT-4-generated data improves open-source models' ability to control language proficiency level. GPT-4, when prompted effectively, generates stories at target CEFR levels with high quality and relevance. This data is used to directly train open-source models via supervised fine-tuning, teaching them to associate specific control tokens with desired proficiency levels. Core assumption: GPT-4's outputs at target CEFR levels are both high quality and accurately reflect the intended proficiency level, making them suitable training data for fine-tuning.

### Mechanism 2
Reinforcement learning with Proximal Policy Optimization (PPO) aligns model outputs more closely with target proficiency levels. After initial fine-tuning, the model's outputs may be misaligned with the automatic CEFR scorer's predictions. PPO is used with a reward function based on the negative ControlError (the squared difference between predicted and target proficiency levels). This incentivizes the model to generate text that scores closer to the desired CEFR level according to the automatic scorer. Core assumption: The automatic CEFR scoring function accurately reflects human perception of proficiency level, and optimizing for its output will lead to human-aligned results.

### Mechanism 3
Top-k sampling with an existing proficiency control model can arbitrarily reduce ControlError at the cost of increased computational expense. For any given prompt and target proficiency, a proficiency control model can generate multiple responses with varying ControlError due to stochastic sampling. By generating k responses and selecting the one with the lowest ControlError, the overall error of the strategy can be reduced. Core assumption: The model's sampling distribution contains some samples close to the target proficiency, and increasing k increases the likelihood of finding them.

## Foundational Learning

- Concept: CEFR (Common European Framework of Reference) proficiency levels
  - Why needed here: The entire task is defined around controlling text generation to match specific CEFR levels (A1-C2). Understanding what each level represents is crucial for both implementing the control and evaluating results.
  - Quick check question: What are the six CEFR levels in order from lowest to highest proficiency?

- Concept: Automatic text readability assessment
  - Why needed here: The paper uses an automatic scorer to evaluate how closely generated text matches target CEFR levels. Understanding how such scorers work (e.g., linguistic features, regression models) is important for interpreting results and potential limitations.
  - Quick check question: What are some common linguistic features used in automatic readability assessment?

- Concept: Reinforcement learning with PPO
  - Why needed here: The paper uses PPO to further align model outputs with target proficiency levels after initial fine-tuning. Understanding the basics of PPO, reward functions, and policy optimization is necessary to implement and debug this component.
  - Quick check question: In PPO, what is the role of the KL divergence penalty?

## Architecture Onboarding

- Component map: TinyStories dataset → TinyTolkien generation (GPT-4) → Fine-tuning data → Supervised fine-tuning (CEFR control tokens) → PPO alignment (ControlError reward) → Evaluation (Automatic CEFR scoring → GPT-4 quality assessment → Human evaluation) → Deployment (CALM model with optional top-k sampling)

- Critical path: TinyTolkien generation → Fine-tuning → PPO → Evaluation → Deployment
  - Each step depends on the successful completion of the previous one.

- Design tradeoffs:
  - Model size vs. cost: GPT-4 vs. open-source models (Mistral-7B, Llama2-7B)
  - Accuracy vs. efficiency: Top-k sampling reduces error but increases cost
  - Automatic vs. human evaluation: Automatic scoring is scalable but may not perfectly align with human perception

- Failure signatures:
  - High ControlError after fine-tuning: Likely issues with training data quality or control token implementation
  - Degraded text quality after PPO: Over-optimization for the automatic scorer, reward hacking
  - Unstable PPO training: Learning rate too high, reward scaling issues, or insufficient KL penalty

- First 3 experiments:
  1. Generate TinyTolkien dataset: Use GPT-4 with the best prompting strategy to create fine-tuning data at all CEFR levels.
  2. Fine-tune open-source model: Train Llama2-7B or Mistral-7B on TinyTolkien with control tokens.
  3. Evaluate ControlError: Test the fine-tuned model on a held-out set and measure ControlError against the automatic scorer.

## Open Questions the Paper Calls Out
- How well does CALM generalize to other languages beyond English?
- Can the CALM model's performance be further improved through additional fine-tuning techniques beyond PPO?

## Limitations
- Reliance on an automatic CEFR scorer that may not align with human perception of proficiency levels
- Potential instability of the PPO training process that could lead to degenerate outputs
- Limited evaluation to English language, leaving cross-lingual generalization unexplored

## Confidence
- High Confidence: GPT-4 outperforms open-source models at proficiency control via prompting
- Medium Confidence: Fine-tuning on GPT-4-generated data improves open-source models' proficiency control
- Low Confidence: PPO alignment significantly improves proficiency control

## Next Checks
1. **Human Evaluation of CEFR Scorer Alignment**: Conduct a human evaluation study where raters assess the proficiency level of a set of texts. Compare these human judgments with the automatic scorer's predictions to quantify the alignment between the two.

2. **PPO Training Stability Analysis**: Implement the PPO training process with a range of hyperparameter settings and monitor the training curves. Identify settings that lead to stable training and compare the performance of models trained with stable vs. unstable PPO.

3. **Generalization Test of Fine-tuned Models**: Evaluate the fine-tuned open-source models on a held-out set of prompts and CEFR levels that were not present in the TinyTolkien training data. This will test whether the models have learned to generalize their proficiency control to new contexts.