---
ver: rpa2
title: 'Find The Gap: Knowledge Base Reasoning For Visual Question Answering'
arxiv_id: '2404.10226'
source_url: https://arxiv.org/abs/2404.10226
tags:
- knowledge
- visual
- question
- reasoning
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies knowledge-based visual question answering (KB-VQA),\
  \ where a model must ground a question in both visual content and an external knowledge\
  \ base (KB) to answer correctly. The authors focus on two key challenges: (1) retrieving\
  \ relevant knowledge from a large KB and the image\u2019s scene graph, and (2) integrating\
  \ these sources with the question for multi-hop reasoning."
---

# Find The Gap: Knowledge Base Reasoning For Visual Question Answering

## Quick Facts
- arXiv ID: 2404.10226
- Source URL: https://arxiv.org/abs/2404.10226
- Reference count: 20
- Key outcome: Task-specific neural model achieves 44.36% accuracy on KRVQA, outperforming LLM-based approach (40.50%) and prior methods, with supervised retrieval significantly improving performance over unsupervised methods.

## Executive Summary
This paper addresses knowledge-based visual question answering (KB-VQA) by developing a framework that grounds questions in both visual content and an external knowledge base. The authors focus on two key challenges: retrieving relevant knowledge from large KBs and scene graphs, and integrating these sources with questions for multi-hop reasoning. They propose a supervised retrieval module trained with contrastive loss and compare two reasoning approaches: a task-specific neural architecture and an LLM-based method. Experiments on the KRVQA dataset demonstrate that supervised retrieval significantly outperforms unsupervised methods, with the task-specific neural model achieving state-of-the-art performance.

## Method Summary
The method consists of a supervised retrieval module trained with contrastive loss to extract relevant triplets from both the knowledge base and scene graph, conditioned on the question. For reasoning, the authors implement two approaches: a task-specific neural architecture with iterative cross-attention over retrieved facts, and an LLM approach that incorporates retrieved facts into prompts. The model is evaluated on the KRVQA dataset, which contains 32,910 images and 157,201 QA pairs, with supporting facts provided in metadata. The framework integrates external knowledge from DBpedia, ConceptNet, and WebChild, along with scene graphs from the Visual Genome dataset.

## Key Results
- Task-specific neural model achieves 44.36% accuracy, outperforming prior methods and LLM-based approach (40.50%)
- Supervised retrieval significantly improves performance over unsupervised retrieval methods
- LLMs excel at 1-hop reasoning but struggle with 2-hop reasoning compared to the task-specific neural model
- LLMs perform better on KB-related questions, leveraging implicit knowledge, but still require explicit KB retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised retrieval using contrastive loss improves retrieval accuracy over unsupervised methods.
- Mechanism: The retrieval module is trained to maximize similarity between question embeddings and supporting fact embeddings while minimizing similarity to irrelevant facts through contrastive loss.
- Core assumption: The contrastive loss formulation effectively separates relevant from irrelevant facts in the embedding space.
- Evidence anchors: [abstract] "Our results demonstrate the positive impact of empowering task-specific and LLM models with supervised external and visual knowledge retrieval models."
- Break condition: If supporting facts in the dataset are noisy or incomplete, the contrastive loss may learn incorrect associations.

### Mechanism 2
- Claim: LLM models perform better on 1-hop reasoning tasks compared to 2-hop tasks, while the task-specific neural model performs better on 2-hop reasoning.
- Mechanism: LLMs rely more on implicit knowledge and pattern matching, effective for simpler reasoning, while the task-specific model's iterative cross-attention better handles multi-hop complexity.
- Core assumption: Implicit knowledge in LLMs is sufficient for 1-hop reasoning but not for complex multi-step reasoning.
- Evidence anchors: [abstract] "Our findings show that though LLMs are stronger in 1-hop reasoning, they suffer in 2-hop reasoning in comparison with our fine-tuned NN model."
- Break condition: If LLMs are provided with explicit, step-by-step reasoning chains, they might perform better on 2-hop tasks.

### Mechanism 3
- Claim: LLM models perform better on KB-related questions compared to non-KB-related questions.
- Mechanism: LLM's implicit knowledge, accumulated during pre-training, is more relevant for questions requiring external knowledge from the knowledge base.
- Core assumption: Implicit knowledge in LLMs is more relevant for KB-related questions compared to explicitly retrieved facts.
- Evidence anchors: [abstract] "Moreover, we observed that LLM models outperform the NN model for KB-related questions which confirms the effectiveness of implicit knowledge in LLMs."
- Break condition: If explicitly retrieved facts for KB-related questions are of significantly higher quality, the task-specific neural model might outperform the LLM model.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: To train the retrieval module to distinguish between relevant and irrelevant facts in the knowledge base and scene graph.
  - Quick check question: What is the objective of the contrastive loss function used in the retrieval module?

- Concept: Multi-hop reasoning
  - Why needed here: To answer questions that require combining information from multiple sources in a step-by-step manner.
  - Quick check question: How does the task-specific neural model perform multi-hop reasoning?

- Concept: Knowledge graphs and scene graphs
  - Why needed here: To represent external knowledge and visual information in structured formats that can be easily queried and reasoned over.
  - Quick check question: What is the difference between a knowledge graph and a scene graph?

## Architecture Onboarding

- Component map: Retriever -> Reasoning Module (Task-specific or LLM) -> Answer Generator
- Critical path: Retriever extracts relevant facts from KB and scene graph -> Reasoning module integrates facts with question for multi-hop reasoning -> Answer generator produces final response
- Design tradeoffs:
  - Supervised vs. unsupervised retrieval: Supervised retrieval improves accuracy but requires labeled data
  - Task-specific vs. LLM reasoning: Task-specific models are better for complex reasoning, while LLMs leverage implicit knowledge
  - High-level vs. low-level visual representation: High-level scene graphs improve reasoning accuracy
- Failure signatures:
  - Low retrieval accuracy: Indicates issues with retriever or quality of knowledge base/scene graph
  - Poor reasoning performance: Suggests problems with reasoning module or integration of retrieved facts
  - Imbalanced performance across question types: Highlights weaknesses in handling specific types of questions
- First 3 experiments:
  1. Evaluate retrieval accuracy on a held-out set of questions with known supporting facts
  2. Compare performance of task-specific neural model and LLM model on 1-hop and 2-hop questions
  3. Analyze performance of both models on KB-related and non-KB-related questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does dynamic retrieval, where facts are refined after each reasoning step, compare to the static retrieval approach used in this paper?
- Basis in paper: Explicit - Mentioned as a limitation and future work direction, noting that current method trains and executes retrieval and reasoning parts separately.
- Why unresolved: The paper does not explore or provide experimental results comparing static versus dynamic retrieval methods.
- What evidence would resolve it: Experimental results comparing performance of static retrieval versus dynamic retrieval on the KRVQA dataset.

### Open Question 2
- Question: Can the proposed retrieval and reasoning framework be effectively adapted to other knowledge-based VQA datasets like OK-VQA, which lacks supporting facts/reasons for analysis?
- Basis in paper: Explicit - Authors acknowledge that OK-VQA lacks supporting metadata for analysis and did not provide results on this dataset.
- Why unresolved: Framework is only evaluated on KRVQA, and adapting it to OK-VQA would require modifications to handle absence of supporting facts.
- What evidence would resolve it: Experimental results demonstrating framework's performance on OK-VQA after modifying it to handle dataset's characteristics.

### Open Question 3
- Question: What is the impact of using automatically generated scene graphs versus human-annotated scene graphs on the performance of the retrieval and reasoning model?
- Basis in paper: Explicit - Authors attempted to use approximated scene graphs but found current scene graph approximation methods have low accuracy, leading to poor retrieval performance.
- Why unresolved: Paper relies on human-annotated scene graphs from Visual Genome dataset to avoid error propagation, limiting scalability and generalizability.
- What evidence would resolve it: Experimental results comparing performance of model using human-annotated scene graphs versus automatically generated scene graphs with improved accuracy.

## Limitations
- The framework's performance may not generalize to other KB-VQA benchmarks beyond KRVQA
- Analysis of LLM weaknesses in 2-hop reasoning lacks granular breakdown of specific failure cases
- Evaluation of implicit knowledge utilization by LLMs is limited to binary KB/non-KB categorization without deeper analysis of knowledge type effectiveness

## Confidence
- **High Confidence**: Claims about supervised retrieval improving performance over unsupervised methods
- **Medium Confidence**: Claims about task-specific model superiority in 2-hop reasoning
- **Medium Confidence**: Claims about LLM implicit knowledge effectiveness for KB-related questions

## Next Checks
1. Conduct ablation studies removing different components of the supervised retrieval module to quantify the specific contribution of contrastive loss vs. architectural choices
2. Test both the task-specific and LLM approaches on additional KB-VQA datasets to assess generalizability of observed performance patterns
3. Perform error analysis categorizing LLM failures in 2-hop reasoning to identify whether issues stem from retrieval, reasoning, or knowledge integration limitations