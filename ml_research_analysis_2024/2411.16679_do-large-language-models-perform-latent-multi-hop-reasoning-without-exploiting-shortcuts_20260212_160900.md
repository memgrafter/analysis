---
ver: rpa2
title: Do Large Language Models Perform Latent Multi-Hop Reasoning without Exploiting
  Shortcuts?
arxiv_id: '2411.16679'
source_url: https://arxiv.org/abs/2411.16679
tags:
- latent
- entity
- multi-hop
- composability
- bridge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates how well large language models (LLMs) perform
  latent multi-hop reasoning without exploiting shortcuts. The authors address a key
  challenge in evaluating such reasoning: models may guess answers based on frequent
  co-occurrences of entities or relation patterns in training data, bypassing true
  reasoning.'
---

# Do Large Language Models Perform Latent Multi-Hop Reasoning without Exploiting Shortcuts?

## Quick Facts
- **arXiv ID:** 2411.16679
- **Source URL:** https://arxiv.org/abs/2411.16679
- **Reference count:** 40
- **Primary result:** While some LLMs demonstrate promising latent multi-hop reasoning abilities (up to 80% latent composability for country-related queries), performance varies significantly by bridge entity type, with only 5% composability for year-based queries.

## Executive Summary
This paper evaluates how well large language models perform latent multi-hop reasoning without exploiting shortcuts. The authors construct a dataset called SOCRATES that excludes queries where head and answer entities might co-occur in training, and apply rigorous evaluation procedures to minimize shortcut exploitation. Their results show that while some LLMs demonstrate promising latent multi-hop reasoning abilities, performance varies substantially by bridge entity type. The study reveals a substantial gap between latent and Chain-of-Thought reasoning abilities, and identifies that latent representations of bridge entities are constructed more often for query types with higher composability.

## Method Summary
The authors construct the SOCRATES dataset through a multi-step filtering process to ensure shortcut-free evaluation of latent multi-hop reasoning. They start by extracting facts from Wikidata and generating relation compositions, then construct natural language queries. To prevent shortcut exploitation, they apply co-occurrence-based filtering using proxy corpora, guessability filtering, and usability filtering. Models are evaluated using greedy decoding with CoT-suppressing instructions, and latent composability is measured as the ratio of cases where models correctly answer multi-hop queries while also correctly answering both corresponding single-hop queries. The evaluation is designed to be shortcut-free by excluding cases where head and answer entities co-occur in training data.

## Key Results
- State-of-the-art models demonstrate strong latent composability of over 80% when bridge entity is a country, but only 5-7% when bridge entity is a year
- There is a significant gap between latent and Chain-of-Thought reasoning abilities (e.g., GPT-4o achieves 92.8% composability with CoT vs 7.6% with latent reasoning)
- Models that know more single-hop facts and larger models show dramatic improvements in composability for CoT reasoning compared to latent reasoning
- Latent composability measured with shortcut-free data and evaluation procedure is three times lower than the shortcut-prone counterpart

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Latent multi-hop reasoning emerges when models have sufficient exposure to facts connected through certain types of bridge entities during pretraining
- Core assumption: Frequency and manner of exposure to specific relation compositions during pretraining influences emergence of latent reasoning abilities
- Evidence: Models show 82.6% latent composability for country-type bridge entities vs 6.7% for year-type bridge entities

### Mechanism 2
- Claim: Explicit Chain-of-Thought reasoning outperforms latent reasoning because it forces clear, early representations of bridge entities
- Core assumption: Explicit generation of bridge entities ensures their clear representation in model's internal state
- Evidence: GPT-4o achieves 92.8% composability with CoT reasoning vs 7.6% with latent reasoning

### Mechanism 3
- Claim: Shortcut-free evaluation methodology is crucial because it prevents models from exploiting correlations between entities or relations
- Core assumption: Models can develop shortcuts through frequent co-occurrences of subject-object or relation-object pairs in training data
- Evidence: Latent composability is three times lower with shortcut-free evaluation compared to shortcut-prone counterpart

## Foundational Learning

- **Concept: Entity co-occurrence and its impact on model shortcuts**
  - Why needed here: Understanding how frequent co-occurrence of entities in training data can lead to shortcut learning is crucial for interpreting why certain evaluation methodologies are necessary
  - Quick check question: What are the two types of shortcuts mentioned in the paper that models can exploit when answering multi-hop queries?

- **Concept: Latent versus explicit reasoning in LLMs**
  - Why needed here: The distinction between how models perform reasoning internally (latent) versus through explicit intermediate steps (Chain-of-Thought) is central to understanding the paper's main findings
  - Quick check question: According to the paper, what is the main difference in performance between latent and Chain-of-Thought reasoning?

- **Concept: Dataset construction for evaluation**
  - Why needed here: The methodology for constructing evaluation datasets that minimize shortcut exploitation is a key contribution of the paper
  - Quick check question: What is the main challenge in constructing a dataset for evaluating latent multi-hop reasoning, and how does the paper address it?

## Architecture Onboarding

- **Component map:** Knowledge graph extraction -> Fact pair selection -> Test case construction -> Co-occurrence filtering -> Guessability filtering -> Usability filtering -> Model evaluation
- **Critical path:** The most critical steps are co-occurrence-based filtering and guessability filtering, as these directly address the paper's main concern about shortcut exploitation
- **Design tradeoffs:** Using proxy corpora instead of actual pretraining data is necessary due to data inaccessibility but introduces approximation error that is mitigated through Google Search validation
- **Failure signatures:** If latent composability appears high across all bridge entity types, it may indicate insufficient filtering of shortcuts
- **First 3 experiments:**
  1. Replicate latent composability evaluation on a subset of models to verify reported performance gap between country-type and year-type bridge entities
  2. Implement Google Search filter on subset of test queries to measure impact on latent composability
  3. Use Patchscopes framework to examine whether bridge entity representations are constructed more often for query types with higher latent composability

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the discussion section, several important open questions emerge:

- What specific architectural or pretraining modifications could significantly improve latent multi-hop reasoning abilities in LLMs?
- How do different pretraining corpora distributions affect the emergence of latent multi-hop reasoning capabilities?
- Can latent multi-hop reasoning abilities be transferred or adapted when models are fine-tuned on new knowledge or tasks?

## Limitations

- The use of proxy corpora instead of actual pretraining data introduces approximation error in co-occurrence filtering
- The evaluation focuses on factual knowledge and may not generalize to more complex reasoning tasks
- The methodology assumes that successful latent composability requires both facts to be individually recalled, which may not capture all forms of latent reasoning

## Confidence

- **High confidence:** The methodology for constructing shortcut-free evaluation datasets and the observed performance gap between latent and CoT reasoning
- **Medium confidence:** The interpretation that bridge entity construction frequency explains variation in latent composability across entity types
- **Medium confidence:** The finding that certain LLMs demonstrate strong latent reasoning for country-related queries

## Next Checks

1. Replicate the latent composability evaluation on a subset of models to verify the reported performance gap between country-type and year-type bridge entities
2. Implement the Google Search filter on a subset of test queries to measure its impact on latent composability and validate the approximation of co-occurrence statistics
3. Use the Patchscopes framework to examine whether bridge entity representations are constructed more often for query types with higher latent composability, as hypothesized in the paper