---
ver: rpa2
title: 'MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds'
arxiv_id: '2402.01706'
source_url: https://arxiv.org/abs/2402.01706
tags:
- world
- llms
- jailbreak
- language
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MULTIVERSE, a novel technique to expose vulnerabilities
  in large language model (LLM) alignment by automatically generating jailbreak prompts
  using a domain-specific language (DSL) to describe diverse virtual worlds (e.g.,
  time, location, characters, actions, and languages). By embedding malicious questions
  within nested contexts, MULTIVERSE achieves a jailbreak success rate of over 85%
  across three datasets and various aligned LLMs, including GPT-4 and Llama-2-70B.
---

# MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds

## Quick Facts
- arXiv ID: 2402.01706
- Source URL: https://arxiv.org/abs/2402.01706
- Authors: Xiaolong Jin; Zhuo Zhang; Xiangyu Zhang
- Reference count: 24
- Key outcome: Jailbreak success rate over 85% across three datasets and various aligned LLMs, including GPT-4 and Llama-2-70B

## Executive Summary
This paper introduces MULTIVERSE, a novel technique to expose vulnerabilities in large language model (LLM) alignment by automatically generating jailbreak prompts using a domain-specific language (DSL) to describe diverse virtual worlds (e.g., time, location, characters, actions, and languages). By embedding malicious questions within nested contexts, MULTIVERSE achieves a jailbreak success rate of over 85% across three datasets and various aligned LLMs, including GPT-4 and Llama-2-70B. The method is highly effective and efficient, significantly outperforming state-of-the-art techniques. Results indicate that LLMs are particularly vulnerable to prompts involving nested worlds and programming language contexts, suggesting that current alignment training focuses on real-world scenarios and lacks robustness in virtual environments.

## Method Summary
MULTIVERSE uses a World Description Language (WDL) to define world parameters (scenario, time, location, characters, actions, language), compiles them into natural descriptions, and injects malicious questions with jailbreak instructions and competing objectives. The system iteratively updates configurations until successful jailbreak or maximum iterations reached. The method is evaluated on three datasets (AdvBench, GPTFuzzer, TDC Redteaming) using metrics like Jailbreak Success Rate (JSR), Top-1 Jailbreak Success Rate, and Average Number of Queries per Question (AQQ).

## Key Results
- Jailbreak success rate exceeds 85% across three datasets and multiple aligned LLMs
- MULTIVERSE significantly outperforms state-of-the-art jailbreak techniques in both effectiveness and efficiency
- LLMs show particular vulnerability to nested worlds and programming language contexts
- Current alignment training appears focused on real-world scenarios, lacking robustness in virtual environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM alignment is context-sensitive and degrades when prompts embed malicious questions within nested fictional or virtual worlds.
- Mechanism: By constructing a nested world hierarchy via a DSL, the malicious query is disguised in a benign outer context (e.g., a Python or novel world), reducing the salience of the harmful content to the alignment system.
- Core assumption: Alignment training focuses primarily on real-world conversational contexts, so LLMs are less protected against prompts that mix virtual and real-world scenarios.
- Evidence anchors:
  - [abstract] "LLMs are extremely vulnerable to nesting worlds and programming language worlds. They imply that existing alignment training focuses on the real-world and is lacking in various (virtual) worlds where LLMs can be exploited."
  - [section] "LLMs are well-protected in the vanilla real world. However, the protection degrades when the created world diverges from the reality."
- Break condition: If alignment training explicitly includes virtual and nested-world scenarios, the context-sensitivity advantage disappears.

### Mechanism 2
- Claim: Competing objectives in LLMs (instruction-following vs. safety) can be exploited by first having the model generate harmless background content before injecting the malicious query.
- Mechanism: The compiler injects a "Competing Objectives" step where the LLM first elaborates on a benign world-building task, then the prompt pivots to the harmful question, leveraging the model's task-completion bias.
- Core assumption: LLMs trained with multiple objectives can be confused when a benign-sounding task masks a harmful follow-up.
- Evidence anchors:
  - [section] "Jailbreak Instruction and Competing Objectives ... we enforce LLMs to produce harmless content initially, such as irrelevant background information."
- Break condition: If the model is explicitly trained to detect and block jailbreak patterns that combine benign context-setting with harmful payloads, this exploit fails.

### Mechanism 3
- Claim: LLM pronoun resolution weakness can be exploited for multi-round jailbreak by issuing follow-up prompts with neutral pronouns that reference previously generated harmful instructions.
- Mechanism: After the first jailbreak yields harmful instructions, subsequent prompts use non-malicious phrasing like "provide more details of Step 3" to elicit additional harmful content without triggering safety filters.
- Core assumption: LLMs have strong pronoun resolution capabilities but weak contextual alignment when pronouns reference earlier malicious content.
- Evidence anchors:
  - [section] "Interestingly, we discovered that LLMs exhibit weak alignment with pronouns, although they are proficient in pronoun resolution ... we continuously jailbreak for more detailed content by utilizing neutral and benign pronouns such as 'Provide more details of Step 3' instead of malicious phrases like 'Provide more details of Disabling Security.'"
- Break condition: If the alignment model incorporates pronoun-based context checks for malicious instruction continuation, this multi-round approach fails.

## Foundational Learning

- Concept: Domain-Specific Language (DSL) for structured world description.
  - Why needed here: Enables systematic generation of diverse, semantically meaningful jailbreak prompts by encoding world parameters (scenario, time, location, characters, actions) in a structured format.
  - Quick check question: Can you explain how a DSL helps automate the generation of varied jailbreak contexts compared to manually writing prompts?

- Concept: Context-sensitivity in alignment training.
  - Why needed here: Understanding that LLMs' safety mechanisms are less robust in non-real-world contexts explains why nested or virtual-world jailbreaks succeed.
  - Quick check question: Why might a malicious query hidden inside a "Python world" be less likely to trigger safety filters than the same query asked directly?

- Concept: Competing objectives in reinforcement learning from human feedback (RLHF).
  - Why needed here: Recognizing that LLMs balance multiple goals (helpfulness, harmlessness, instruction-following) reveals how conflicting directives can be exploited to bypass safety.
  - Quick check question: How does instructing the model to first produce harmless content before a harmful query exploit its competing objectives?

## Architecture Onboarding

- Component map: DSL config -> Compiler -> Jailbreak prompt -> LLM query -> Evaluation -> (if fail) Parameter update -> repeat
- Critical path: DSL configuration -> Compiler transforms into natural-language prompts -> Jailbreak prompt generation -> LLM query execution -> Evaluation of jailbreak success -> Parameter update for failed attempts -> Iteration
- Design tradeoffs:
  - Structured DSL vs. free-form prompt generation: DSL provides reproducibility and systematic coverage but may miss unforeseen jailbreak patterns
  - Single vs. multiple nested layers: More layers increase success but also increase prompt complexity and cost
  - Automated vs. manual evaluation: Manual evaluation is accurate but costly; automated evaluation with GPT-4 is scalable but may have false positives/negatives
- Failure signatures:
  - Low jailbreak success rate despite varied configurations: May indicate strong baseline alignment or overly restrictive DSL
  - High success in specific parameter combinations: Suggests alignment gaps in those contexts; focus further exploration there
  - Model refusal to follow jailbreak instructions: Indicates strong alignment in the tested world configurations
- First 3 experiments:
  1. Test a simple single-layer prompt with a virtual world (e.g., "Python world") embedding a direct harmful question to confirm basic context-sensitivity
  2. Test a two-layer nested prompt combining a virtual world and a real-world context to measure the impact of nesting on success rate
  3. Test parameter sensitivity by varying the "Language" field (e.g., natural language vs. programming language) to identify which contexts are most vulnerable

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design defense methods that effectively detect and filter jailbreak prompts generated by MULTIVERSE, particularly those that combine multiple virtual worlds and embed malicious questions within nested contexts?
- Basis in paper: [inferred] The paper demonstrates that existing defense methods, such as Perplexity Filter and OpenAI Moderation Endpoint, are ineffective against MULTIVERSE jailbreak prompts, highlighting the need for more robust defenses.
- Why unresolved: The paper does not propose or evaluate any new defense mechanisms specifically designed to counter MULTIVERSE attacks. It merely shows the inadequacy of existing methods, leaving a gap in understanding how to protect against this type of jailbreak.
- What evidence would resolve it: Development and evaluation of a new defense method that can accurately identify and filter MULTIVERSE jailbreak prompts, achieving a significant reduction in jailbreak success rate compared to current methods.

### Open Question 2
- Question: How can we incorporate diverse contexts and languages comprehensively during the alignment training process of LLMs to enhance their robustness against jailbreak attacks like MULTIVERSE?
- Basis in paper: [explicit] The paper's results indicate that LLMs are particularly vulnerable to prompts involving nested worlds and programming language contexts, suggesting that current alignment training focuses on real-world scenarios and lacks robustness in virtual environments.
- Why unresolved: The paper does not provide specific strategies or techniques for incorporating diverse contexts and languages into the alignment training process. It merely highlights the need for such improvements, leaving open the question of how to effectively implement them.
- What evidence would resolve it: A comprehensive study comparing the jailbreak success rate of LLMs trained with diverse contexts and languages against those trained with only real-world scenarios, demonstrating a significant reduction in vulnerability to MULTIVERSE-like attacks.

### Open Question 3
- Question: How can we automate the evaluation process of jailbreak prompts to assess their effectiveness in manipulating LLMs into generating harmful content, reducing the reliance on manual checking and improving scalability?
- Basis in paper: [explicit] The paper mentions that evaluating the success of jailbreaking an LLM is a well-known challenge and employs manual checking for most experiments, with GPT-4 used for evaluation in Section 4.6 due to the extensive workload.
- Why unresolved: The paper does not provide a detailed description of the evaluation prompt used for GPT-4 or explore alternative automated evaluation methods. It leaves open the question of how to develop a reliable and scalable automated evaluation process.
- What evidence would resolve it: Development and validation of an automated evaluation method that accurately assesses the jailbreak success rate of prompts, achieving high agreement with manual evaluation and demonstrating scalability for large-scale experiments.

## Limitations

- Evaluation relies heavily on manual assessment for a small sample of prompts, raising questions about generalizability
- The study does not investigate whether jailbroken outputs represent genuine vulnerabilities or merely demonstrate the LLM's ability to roleplay within fictional contexts
- The DSL approach may miss jailbreak patterns that emerge from more nuanced or unexpected prompt formulations outside predefined world parameters

## Confidence

**High Confidence**: The mechanism by which nested fictional contexts can reduce the effectiveness of alignment training is well-supported by experimental results across multiple datasets and LLM models.

**Medium Confidence**: The claim that current alignment training focuses primarily on real-world scenarios is inferred from results but not directly validated through analysis of training data or alignment procedures.

**Low Confidence**: The assertion that LLMs have "weak alignment with pronouns" as an exploitable vulnerability is based on limited observations and requires more systematic investigation.

## Next Checks

1. **Context Transferability Test**: Take successful jailbreak prompts from virtual worlds and remove the fictional framing to see if the same malicious queries succeed without the nested context.

2. **Training Data Analysis**: Examine whether the alignment training data actually lacks virtual or nested-world scenarios by analyzing the proportion of training examples containing fictional contexts versus real-world scenarios.

3. **Pronoun Resolution Control**: Design a controlled experiment testing whether the same malicious content, when presented with different pronoun references (or no pronouns), yields different jailbreak success rates.