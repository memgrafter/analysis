---
ver: rpa2
title: How Much Can We Forget about Data Contamination?
arxiv_id: '2410.03249'
source_url: https://arxiv.org/abs/2410.03249
tags:
- data
- training
- contamination
- forgetting
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies how the impact of data contamination on language
  model evaluation changes with training scale. Experiments with models up to 1.6B
  parameters and training runs up to 15x Chinchilla show that contamination effects
  can grow with model size, shrink with more training tokens, and scale with repetition
  frequency.
---

# How Much Can We Forget about Data Contamination?

## Quick Facts
- **arXiv ID**: 2410.03249
- **Source URL**: https://arxiv.org/abs/2410.03249
- **Reference count**: 40
- **Primary result**: Contamination effects can grow with model size but shrink with more training tokens; even 144 repetitions can be forgotten with sufficient clean training.

## Executive Summary
This work investigates how data contamination impacts language model evaluation accuracy and how these effects change with training scale. Through experiments with models up to 1.6B parameters and training runs up to 15x Chinchilla, the authors show that contamination effects depend non-linearly on model size, training tokens, and repetition frequency. Surprisingly, they find that contamination impacts can vanish with sufficient additional clean training, attributed to gradient descent forgetting dynamics. Analysis of large-scale runs (OLMo-7B, Llama 3 405B) reveals that early training data is most likely to be forgotten, suggesting that for many modern LLMs, early contamination may no longer influence final evaluations.

## Method Summary
The authors conduct controlled experiments by inserting exact benchmark questions at random positions in training data, with contamination occurring at 4, 12, 36, or 144 repetitions. They train GPT-3-style models (124M to 1.6B parameters) on FineWeb-Edu and OLMo checkpoints, monitoring accuracy differences between contaminated and clean evaluation examples. To study forgetting, they insert benchmark data mid-training and continue training to observe how accuracy differences decay. They also analyze the impact of weight decay on forgetting by computing cumulative weight decay bounds and comparing them to empirical forgetting rates. The study includes large-scale runs on OLMo-7B and Llama 3 405B to validate extrapolation claims.

## Key Results
- Contamination effects can grow with model size but shrink with more training tokens
- Even 144 repetitions of contamination can be forgotten with sufficient additional clean training
- Weight decay parameter accelerates forgetting, with empirical forgetting occurring faster than cumulative weight decay bounds
- Early training data is most likely to be forgotten in large-scale runs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The impact of data contamination depends on the joint scaling of model, data, and contamination.
- Mechanism: Contamination effects can grow with model size, shrink with more training tokens, and scale with repetition frequency. This creates a non-linear interaction where the relative scale of each factor determines whether contamination matters.
- Core assumption: The Chinchilla scaling laws apply to contamination effects, meaning that doubling model size requires doubling training tokens for similar contamination impact.
- Evidence anchors:
  - [abstract] "Experiments with models up to 1.6B parameters and training runs up to 15x Chinchilla show that contamination effects can grow with model size, shrink with more training tokens, and scale with repetition frequency."
  - [section] "First, similar to many other works, we find that the tendency of a model to overfit increases in the number of parameters... Second, and this is also expected, we find a clear scaling in the number of repetitions... More surprisingly, we find that the effect of contamination can vanish as we increase the number of training tokens"
- Break condition: If the Chinchilla scaling laws don't apply to the specific model architecture or if the contamination pattern differs significantly from uniform random insertion.

### Mechanism 2
- Claim: Empirical forgetting occurs faster than the cumulative weight decay bound.
- Mechanism: The weight decay parameter in AdamW optimizer accelerates forgetting of past training examples. The cumulative weight decay provides a bound on how much past gradient updates contribute to current model weights.
- Core assumption: Weight decay is the primary driver of forgetting, and its effect can be modeled through the cumulative weight decay formula.
- Evidence anchors:
  - [abstract] "Next, we study the impact of the weight decay parameter on example forgetting, showing that empirical forgetting occurs faster than the cumulative weight decay."
  - [section] "We find that the weight decay parameter and learning rate schedule of the AdamW optimizer play a key part in forgetting past training examples. Concretely, we demonstrate that the cumulative weight decay bounds empirical forgetting."
- Break condition: If other factors (like learning rate scheduling or gradient clipping) dominate the forgetting process, or if the model architecture significantly differs from standard transformers.

### Mechanism 3
- Claim: Forgetting dynamics explain why increasing training tokens alleviates contamination impact.
- Mechanism: Training on novel data causes the model to gradually forget previously seen contaminated examples. This forgetting is driven by exposure to new information rather than catastrophic forgetting.
- Core assumption: The forgetting process is gradual and depends on the amount of novel data seen after contamination occurs.
- Evidence anchors:
  - [abstract] "Our investigation reveals that the forgetting dynamics of gradient descent is the reason why increasing the number of tokens alleviates the impact of contamination."
  - [section] "To study the effect of forgetting, we train a 124M parameter model at 15x Chinchilla... Figure 2a depicts the development of the difference in cross-entropy loss between contaminated and clean benchmark questions over the course of training. We see a strong peak after 2 Chinchilla, which is expected and shows the effect of contamination. What is interesting to us is the rate at which the cross-entropy loss difference decays as we continue training."
- Break condition: If the forgetting process is interrupted (e.g., multi-epoch training on limited data) or if the contamination is too frequent to be forgotten.

## Foundational Learning

- Concept: Chinchilla scaling laws
  - Why needed here: The paper's experimental design and conclusions heavily rely on understanding how model parameters and training tokens should scale together.
  - Quick check question: If a model has 10B parameters, how many tokens should it be trained on according to Chinchilla scaling?

- Concept: Cross-entropy loss and accuracy metrics
  - Why needed here: The paper measures contamination impact through accuracy differences and cross-entropy loss differences between contaminated and clean examples.
  - Quick check question: What does a negative cross-entropy loss difference between contaminated and clean examples indicate about the model's performance?

- Concept: Weight decay in AdamW optimizer
  - Why needed here: Weight decay is shown to be a key factor in the forgetting process, and understanding its mathematical formulation is crucial for interpreting the results.
  - Quick check question: How does weight decay in AdamW differ from L2 regularization in standard SGD?

## Architecture Onboarding

- Component map: Data preprocessing -> Contamination insertion -> Model training -> Evaluation -> Forgetting analysis -> Weight decay analysis
- Critical path: 1) Preprocess benchmarks and remove duplicates, 2) Prepare training data with contamination insertion, 3) Train model while monitoring contamination effects, 4) Continue training to observe forgetting, 5) Analyze weight decay impact on forgetting, 6) Compute cumulative weight decay for large-scale models.
- Design tradeoffs: The paper uses small models (up to 1.6B parameters) for controlled experiments but must extrapolate to larger models. Contamination is inserted uniformly at random, which may not reflect real-world contamination patterns. The choice to use exact contamination simplifies analysis but may underestimate effects of near-duplicate contamination.
- Failure signatures: If contamination effects don't scale as expected with model size or training tokens, if forgetting doesn't occur as predicted by weight decay analysis, or if duplicate filtering fails to remove all problematic questions leading to spurious contamination effects.
- First 3 experiments:
  1. Train a small model (124M parameters) with 4x contamination and observe the accuracy difference compared to clean evaluation.
  2. Train the same model with increasing training tokens (2x to 15x Chinchilla) while keeping contamination constant to observe forgetting dynamics.
  3. Modify the weight decay parameter in AdamW and observe how it affects the rate of forgetting for contaminated examples.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the rate of forgetting scale with model size for contamination data when training with continuous novel data versus multiple epochs of the same data?
- Basis in paper: [explicit] The paper shows that forgetting behaves differently when training on a continuous stream of novel data versus multi-epoch training on a limited dataset (Tirumala et al., 2022; Muennighoff et al., 2023).
- Why unresolved: While the paper demonstrates this difference exists, it doesn't provide a quantitative model for how forgetting rates scale with model size across these two training regimes.
- What evidence would resolve it: Systematic experiments varying model size and training regime while measuring forgetting rates would establish scaling laws for both continuous novel data and multi-epoch training.

### Open Question 2
- Question: What is the relationship between the type of data contamination (exact vs non-exact) and the degree of overfitting across different model sizes?
- Basis in paper: [inferred] The paper only considers exact contamination, noting that non-exact contamination (re-worded questions, translation into a different language) can also affect benchmark performance (Yang et al., 2023).
- Why unresolved: The paper explicitly limits itself to exact contamination due to the smaller model sizes used, leaving the impact of non-exact contamination unexplored.
- What evidence would resolve it: Controlled experiments with both exact and non-exact contamination across a range of model sizes would reveal how contamination type influences overfitting.

### Open Question 3
- Question: How does the forgetting of uniquely identifiable information (random strings) compare to the forgetting of benchmark questions during LLM pre-training?
- Basis in paper: [explicit] The paper acknowledges this as a limitation, noting that empirical forgetting might behave differently for random strings or other uniquely identifiable information (Carlini et al., 2019; Jagielski et al., 2023).
- Why unresolved: All experiments were conducted with benchmark questions, which may have different memorization and forgetting dynamics than random strings.
- What evidence would resolve it: Direct experiments comparing forgetting rates for benchmark questions versus random strings of similar length and frequency would clarify these differences.

## Limitations
- The paper uses controlled contamination experiments with exact duplicates rather than realistic partial overlaps or near-duplicates
- Conclusions are based on models up to 1.6B parameters, requiring extrapolation to frontier models
- The weight decay analysis doesn't account for other factors like learning rate scheduling or architectural differences
- Assumes Chinchilla scaling laws apply to contamination effects without comprehensive validation

## Confidence

**High confidence**: The core observation that contamination effects can diminish with sufficient clean training tokens is well-supported by controlled experiments. The relationship between weight decay and forgetting is mathematically sound and empirically validated on small models.

**Medium confidence**: The extrapolation from 1.6B parameter models to 405B parameter models introduces uncertainty. While the weight decay analysis provides bounds, the exact forgetting dynamics at scale remain uncertain due to potential architectural differences and training variations.

**Low confidence**: The claim that most modern LLMs have forgotten early contamination may overstate the generality of findings. Real-world contamination often involves partial overlaps rather than exact duplicates, and different contamination patterns may not exhibit the same forgetting behavior.

## Next Checks

1. **Real-world contamination validation**: Test the forgetting hypothesis using realistic contamination patterns (partial overlaps, near-duplicates, paraphrased questions) rather than exact duplicates to verify if the observed dynamics generalize beyond controlled experiments.

2. **Scale-up empirical validation**: Train models between 7B and 70B parameters with mid-training contamination insertion to empirically validate the extrapolation claims before applying them to 405B+ parameter models.

3. **Alternative optimizer comparison**: Repeat key experiments using SGD with momentum and other optimizers to isolate whether weight decay specifically or optimizer choice broadly drives the observed forgetting dynamics.