---
ver: rpa2
title: 'TopoLM: brain-like spatio-functional organization in a topographic language
  model'
arxiv_id: '2410.11516'
source_url: https://arxiv.org/abs/2410.11516
tags:
- language
- fmri
- topolm
- spatial
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TopoLM introduces a two-dimensional spatial representation into
  a transformer language model, training it with both a next-token prediction objective
  and a spatial smoothness loss. This encourages neighboring units to develop correlated
  activation profiles, resulting in clusters that align with semantically interpretable
  groupings and replicate functional organization observed in the human language system.
---

# TopoLM: brain-like spatio-functional organization in a topographic language model

## Quick Facts
- arXiv ID: 2410.11516
- Source URL: https://arxiv.org/abs/2410.11516
- Authors: Neil Rathi; Johannes Mehrer; Badr AlKhamissi; Taha Binhuraib; Nicholas M. Blauch; Martin Schrimpf
- Reference count: 40
- TopoLM achieves performance on downstream tasks and brain alignment comparable to a non-topographic baseline, with only slight decreases on some linguistic benchmarks, demonstrating that brain-like spatial organization can emerge without sacrificing task performance.

## Executive Summary
TopoLM introduces a two-dimensional spatial representation into a transformer language model, training it with both a next-token prediction objective and a spatial smoothness loss. This encourages neighboring units to develop correlated activation profiles, resulting in clusters that align with semantically interpretable groupings and replicate functional organization observed in the human language system. The model successfully predicts spatially organized cortical language networks and fine-grained linguistic clusters (e.g., verb- and noun-selective), matching brain activity patterns.

## Method Summary
TopoLM is a transformer language model that incorporates two-dimensional spatial coordinates into each attention and MLP layer. During training, a spatial smoothness loss is applied alongside the standard next-token prediction objective, encouraging nearby units to develop correlated activation profiles. The model uses random spatial encoding permutations per layer to prevent shortcut solutions and applies fMRI-like readout sampling with Gaussian smoothing to enable meaningful comparison with brain data. Training uses a 10B-token subset of FineWeb-Edu with AdamW optimization for approximately 5 days on 4xNVIDIA 80GB A100s.

## Key Results
- TopoLM achieves performance on downstream tasks and brain alignment comparable to a non-topographic baseline, with only slight decreases on some linguistic benchmarks
- The model successfully predicts spatially organized cortical language networks and fine-grained linguistic clusters (e.g., verb- and noun-selective), matching brain activity patterns
- Spatial organization emerges in TopoLM that closely matches the functional organization in the brain's language system

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The spatial smoothness loss creates brain-like functional organization by encouraging nearby units to develop correlated activation profiles.
- Mechanism: The spatial loss term penalizes dissimilarity between nearby units' activation patterns during training. This creates pressure for units with similar response profiles to cluster together spatially, mirroring the topographic organization observed in cortical tissue.
- Core assumption: Minimizing wiring length and encouraging local correlations during training leads to functionally meaningful spatial organization in neural networks.
- Evidence anchors:
  - [abstract] "By combining a next-token prediction objective with a spatial smoothness loss, representations in this model assemble into clusters that correspond to semantically interpretable groupings of text and closely match the functional organization in the brain's language system."
  - [section] "The spatial loss for layer k is given by SLk = 1/2(1 − corr(rk, dk)), where dk is a vector of pairwise inverse distances between units, based on their spatial encoding, and corr is Pearson's r. This means that nearby units (i.e. high inverse distance) should have highly correlated activations on the same inputs, and that distant units should be less correlated; this gives us a notion of spatial smoothness."

### Mechanism 2
- Claim: Random spatial encoding permutations per layer prevent shortcut solutions and encourage genuine learning of spatial organization.
- Mechanism: By randomly permuting spatial positions for each layer, the model cannot simply propagate the same spatial pattern through layers. This forces each layer to learn its own meaningful spatial organization from scratch, rather than relying on hierarchical propagation of a single spatial pattern.
- Core assumption: Without randomization, models can minimize spatial loss by propagating spatial patterns without learning meaningful representations.
- Evidence anchors:
  - [section] "We randomly permute these positions for each layer such that each layer has a unique spatial encoding. 2 Our goal is to abstract away from feed-forward propagation as much as possible, as the hierarchical organization of the brain is quite different from that of a language model. This random permutation prevents the model from exploiting the feed-forward nature of the Transformer."

### Mechanism 3
- Claim: fMRI-like readout sampling is crucial for comparing model outputs to brain data and revealing brain-like organization.
- Mechanism: Applying Gaussian smoothing with FWHM 2.0 mm to model activations simulates the coarse spatial sampling of fMRI voxels, which aggregate responses from populations of neurons. This allows meaningful comparison between model and brain data.
- Core assumption: fMRI voxels represent aggregated neural responses, so direct comparison requires simulating this aggregation process.
- Evidence anchors:
  - [section] "Due to the coarse spatial sampling in fMRI neuroimaging work, voxels contain the aggregated response of a large population of neurons (Kriegeskorte et al., 2010, Figure 1C). In all following analyses, we thus apply a simulated version of fMRI readout sampling to model activations, consisting of smoothing with a Gaussian kernel, to imitate the locally aggregated responses of fMRI voxels."

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: TopoLM builds directly on transformer architecture, adding spatial components to existing attention and MLP layers
  - Quick check question: What is the role of multi-head attention in transformer models, and how does it affect neural alignment?

- Concept: Functional magnetic resonance imaging (fMRI) and spatial autocorrelation
  - Why needed here: Understanding how fMRI data is collected and analyzed is crucial for interpreting TopoLM's brain alignment results and the use of Moran's I statistic
  - Quick check question: What does Moran's I measure, and what values indicate clustering versus dispersion in spatial data?

- Concept: Language neuroscience and cortical organization
  - Why needed here: The model is explicitly designed to replicate brain-like organization, so understanding the core language system and known cortical clusters is essential
  - Quick check question: What are the main characteristics of the core language system in the human brain, and how is it typically localized?

## Architecture Onboarding

- Component map:
  Input embedding layer -> 12 Transformer blocks (each with attention layer + MLP layer + spatial encoding) -> Output projection to vocabulary -> Spatial correlation loss computation module

- Critical path:
  1. Forward pass through transformer layers
  2. Compute pairwise correlations between unit activations
  3. Calculate spatial loss using inverse distances
  4. Combine spatial loss with task loss for backpropagation
  5. Update model parameters

- Design tradeoffs:
  - Spatial loss weight (αk): Too high degrades task performance, too low insufficient clustering
  - Neighborhood size: Larger neighborhoods capture more global structure but are computationally expensive
  - Hidden dimension (784): Must be perfect square for 2D grid, affects model capacity

- Failure signatures:
  - Low clustering with high task performance: Spatial loss weight too low
  - High clustering with poor task performance: Spatial loss weight too high
  - No meaningful clusters: Random permutation may not be working, or spatial loss computation has bugs
  - Poor brain alignment: Could indicate issues with fMRI sampling simulation or model architecture

- First 3 experiments:
  1. Train with αk = 0 (no spatial loss) and compare clustering metrics to full model
  2. Vary spatial loss weight αk across layers and measure impact on clustering vs. performance
  3. Test different neighborhood sizes (radius 3, 5, 7) and evaluate computational cost vs. clustering quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different initialization strategies for the spatial encoding (e.g., learned vs. random permutations) affect the emergence and stability of spatio-functional organization in TopoLM?
- Basis in paper: [inferred] The paper uses randomly permuted spatial encodings for each layer to avoid exploiting the feed-forward nature of the Transformer, but notes that training without this randomization leads to the spatial pattern propagating through the network. This suggests the initialization strategy impacts the model's learning dynamics.
- Why unresolved: The paper only compares random permutation with no permutation, leaving the effects of other strategies unexplored. Different initializations might lead to different degrees of clustering or affect task performance.
- What evidence would resolve it: Systematic experiments varying initialization strategies (e.g., learned, structured, or multiple random restarts) and measuring their impact on clustering metrics (Moran's I), task performance (BLiMP, GLUE), and brain alignment (Brain-Score) would clarify the role of initialization.

### Open Question 2
- Question: Can TopoLM predict novel spatio-functional organizations in the language network that have not yet been observed in neuroimaging studies, and how can these predictions be experimentally validated?
- Basis in paper: [explicit] The paper suggests TopoLM could be used to discover new candidates for spatial clustering in linguistic processing and inform stimulus selection for further neuroimaging experiments, leveraging its success in predicting existing patterns.
- Why unresolved: While TopoLM successfully replicates known clustering patterns (verb/noun, concrete/abstract), it's unclear whether it can extrapolate to entirely new organizational principles. Validation requires empirical neuroimaging data.
- What evidence would resolve it: Using TopoLM to generate predictions for novel contrasts or linguistic features, designing experiments to test these predictions with fMRI or ECoG, and comparing the observed clustering patterns to the model's predictions would demonstrate its predictive power.

### Open Question 3
- Question: How does the spatial smoothness loss in TopoLM relate to other potential principles of cortical organization, such as minimizing wiring cost or energy consumption, and can these principles be unified in a single objective function?
- Basis in paper: [explicit] The paper notes that the spatial smoothness loss serves as an efficient proxy for neural wiring length, encouraging local correlation. It also mentions that wiring length minimization corresponds to brain size and power consumption in neuroscience terms.
- Why unresolved: The paper establishes a connection between spatial smoothness and wiring length but doesn't explore whether other organizational principles (e.g., energy efficiency, information theory) could be incorporated or whether they would yield different or complementary results.
- What evidence would resolve it: Experiments combining spatial smoothness with other objectives (e.g., sparsity-inducing regularization, energy-based penalties) and comparing the resulting model's clustering, performance, and alignment to brain data would clarify the relationships between these principles.

## Limitations
- The computational cost of training topographic models with spatial loss is significant, requiring 5 days on 4xNVIDIA 80GB A100s, which limits extensive hyperparameter exploration
- While the model shows brain-like clustering, the causal relationship between spatial smoothness loss and actual language processing capabilities remains underexplored
- The specific choice of FWHM 2.0 mm for fMRI sampling simulation is justified but not thoroughly tested across different resolutions

## Confidence
**High Confidence**: The spatial smoothness loss mechanism successfully creates correlated activation patterns between neighboring units, as evidenced by Moran's I clustering metrics and brain alignment scores. The architectural implementation details are clearly specified and reproducible.

**Medium Confidence**: The claim that TopoLM's brain alignment is comparable to non-topographic baselines while achieving similar task performance is supported by benchmark results, though the slight decreases on some linguistic tasks suggest potential tradeoffs that weren't fully characterized.

**Low Confidence**: The assertion that random spatial encoding permutations per layer are essential for preventing shortcut solutions is theoretically plausible but lacks direct empirical validation. The specific choice of FWHM 2.0 mm for fMRI sampling simulation is justified but not thoroughly tested across different resolutions.

## Next Checks
1. **Spatial Loss Ablation Study**: Train models with αk = 0, αk = 0.1, and αk = 1.0 across all layers, then measure clustering metrics (Moran's I) and brain alignment scores to establish the precise relationship between spatial loss magnitude and brain-like organization.

2. **Randomization Robustness Test**: Implement multiple random spatial encoding schemes (different seeds, different grid sizes, different permutation patterns) and evaluate whether the same clustering patterns emerge, testing the claim that randomization prevents shortcut solutions.

3. **fMRI Sampling Resolution Sweep**: Train TopoLM with fMRI sampling at FWHM = 1.0 mm, 2.0 mm, and 4.0 mm, then measure brain alignment and clustering metrics to determine the optimal resolution for meaningful comparison with actual brain data.