---
ver: rpa2
title: Neuro-Symbolic Data Generation for Math Reasoning
arxiv_id: '2412.04857'
source_url: https://arxiv.org/abs/2412.04857
tags:
- math
- data
- problem
- problems
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a neuro-symbolic framework to address the
  scarcity of high-quality mathematical datasets for training large language models
  (LLMs). The key idea is to mutate existing math problems in a formal symbolic space,
  ensuring both diversity and validity through systematic sampling and symbolic solvers.
---

# Neuro-Symbolic Data Generation for Math Reasoning

## Quick Facts
- arXiv ID: 2412.04857
- Source URL: https://arxiv.org/abs/2412.04857
- Reference count: 40
- Primary result: Neuro-symbolic framework generates 860K math problems, improving LLM reasoning accuracy by up to 10.6% on GSM8K and MATH benchmarks

## Executive Summary
This paper introduces a neuro-symbolic framework to address the scarcity of high-quality mathematical datasets for training large language models (LLMs). The key idea is to mutate existing math problems in a formal symbolic space, ensuring both diversity and validity through systematic sampling and symbolic solvers. The mutated problems are then translated back to natural language using LLMs. Empirical results show that models fine-tuned on the generated data significantly outperform state-of-the-art open-source models on benchmarks like GSM8K and MATH, with improvements up to 10.6% in accuracy. The method also demonstrates scalability and generalizability to out-of-domain datasets.

## Method Summary
The framework formalizes math problems into SMT-LIB, mutates them in symbolic space using projected Markov chain Monte Carlo sampling and simplification/complication strategies, validates mutated problems with symbolic solvers, and translates them back to natural language using GPT-4. The generated dataset (860K examples) is used to fine-tune LLaMA-2 and Mistral models, which are evaluated on GSM8K, MATH, and other benchmarks. The approach systematically generates problems at multiple difficulty levels to improve reasoning capability.

## Key Results
- Models fine-tuned on generated data achieve up to 10.6% improvement in accuracy on GSM8K and MATH benchmarks
- Generated data shows significant diversity gain compared to original datasets
- Approach generalizes well to out-of-domain datasets like SVAMP and ASDiv
- Graduated difficulty training improves reasoning capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mutating math problems in symbolic space improves both diversity and validity compared to natural language mutation.
- Mechanism: Formalization to SMT-LIB → systematic sampling in symbolic space → symbolic solver validation → LLM-based informalization back to natural language.
- Core assumption: The symbolic space is more structured and constraint-aware than natural language, allowing controlled mutations that preserve validity.
- Evidence anchors:
  - [abstract]: "mutates existing math problems in a formal symbolic space, ensuring both diversity and validity through systematic sampling and symbolic solvers"
  - [section 2]: "the key feature of our framework lies in the mutation of math problems within the symbolic space, enabling systematic sampling and symbolic solvers"
  - [corpus]: No direct evidence, but corpus shows related neuro-symbolic approaches support this direction.

### Mechanism 2
- Claim: Projected Markov Chain Monte Carlo sampling in symbolic space generates diverse and valid auxiliary variable solutions.
- Mechanism: Perturb a subset of variables → symbolic solver resolves the remaining subset → ensures both diversity (through perturbation) and validity (through solving).
- Core assumption: The projected MCMC approach can effectively sample valid solutions in the highly irregular symbolic space.
- Evidence anchors:
  - [abstract]: "projected Markov chain Monte Carlo sampling in the highly-irregular symbolic space"
  - [section 2.3]: "we instead opt for auxiliary variable solution generation via the projected Markov chain Monte Carlo (projected MCMC)"
  - [corpus]: Evidence is weak; no direct corpus support for projected MCMC in symbolic spaces.

### Mechanism 3
- Claim: Graduating difficulty levels in training data improves LLM reasoning capability.
- Mechanism: Generate problems at multiple difficulty levels (level-0 to level-5), starting with simpler problems and progressively incorporating more complex ones.
- Core assumption: Exposure to increasingly difficult problems forces the model to develop more sophisticated reasoning strategies.
- Evidence anchors:
  - [abstract]: "the exposure to more complex math problems can improve the LLM's reasoning capability"
  - [section 2.1]: "Our mutation mechanism can properly adjust the complexity of generated problems"
  - [corpus]: No direct evidence in corpus for graduated difficulty improving reasoning.

## Foundational Learning

- Concept: SMT-LIB language and formal problem representation
  - Why needed here: The entire framework relies on formalizing math problems into a structured symbolic language that can be manipulated and validated by solvers.
  - Quick check question: Can you express a simple math problem like "John has 5 apples and gives 2 to Mary. How many does he have left?" in SMT-LIB format?

- Concept: Symbolic solvers and their limitations
  - Why needed here: Solvers are used to validate the correctness of mutated problems and ensure they remain solvable with the intended constraints.
  - Quick check question: What types of mathematical problems can Z3 solve, and where does it struggle?

- Concept: Markov Chain Monte Carlo sampling and projected sampling
  - Why needed here: Projected MCMC is used to generate diverse valid solutions for auxiliary variables in the symbolic space.
  - Quick check question: How does projected MCMC differ from standard MCMC, and why is it suitable for this problem?

## Architecture Onboarding

- Component map:
  - Formalization module → SMT-LIB converter
  - Mutation engine → simplification and complication strategies
  - Solver integration → Z3, CVC4, MathSAT, SymPy, SciPy
  - Projected MCMC sampler → diversity generation
  - Informalization module → GPT-4 prompt templates
  - Training pipeline → fine-tuning on generated data

- Critical path: Formalization → Mutation → Solver validation → MCMC sampling → Informalization → Training

- Design tradeoffs:
  - Symbolic vs. natural language mutation: Symbolic ensures validity but may be less expressive; natural language is more flexible but risks invalid problems.
  - Solver choice: Different solvers have different strengths (e.g., Z3 for logic, SymPy for symbolic math); combining them increases coverage but adds complexity.
  - GPT-4 dependency: GPT-4 is used for informalization and reasoning path generation; replacing it with smaller models could reduce costs but may impact quality.

- Failure signatures:
  - Invalid problems: Solver cannot find solutions or returns unsatisfiable.
  - Low diversity: MCMC sampling converges too quickly or the perturbation space is too constrained.
  - Poor informalization: GPT-4 generates inconsistent natural language problems or reasoning paths that don't match solver answers.

- First 3 experiments:
  1. Formalize a small set of GSM8K problems to SMT-LIB and verify solver compatibility.
  2. Apply simplification and complication strategies to a sample problem and validate solver results.
  3. Implement projected MCMC for auxiliary variable sampling and test diversity of generated solutions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of neuro-symbolic data generation scale with increasing problem complexity beyond high-school level mathematics?
- Basis in paper: [inferred] The paper mentions limitations in handling higher-order mathematical concepts and suggests future work to introduce more mutation operators.
- Why unresolved: The current framework's mutation operators are limited, and the paper does not provide empirical evidence on performance with more complex problems.
- What evidence would resolve it: Empirical results showing performance metrics on datasets with problems of increasing complexity, such as college-level or IMO-level mathematics.

### Open Question 2
- Question: Can the dependence on GPT-4 for informalization and reasoning path generation be fully eliminated while maintaining data quality and consistency?
- Basis in paper: [explicit] The paper discusses potential solutions to reduce reliance on GPT-4, including fine-tuning a new LLM for informalization and using curriculum learning instead of supervised fine-tuning.
- Why unresolved: The proposed solutions are theoretical and have not been tested or validated in the paper.
- What evidence would resolve it: Experimental results comparing data quality and consistency when using alternative methods to GPT-4 for informalization and reasoning path generation.

### Open Question 3
- Question: What are the long-term societal impacts of widespread adoption of neuro-symbolic data generation frameworks in educational and professional settings?
- Basis in paper: [inferred] The paper discusses potential societal consequences and broader impacts, but does not delve into long-term effects.
- Why unresolved: The paper acknowledges potential impacts but does not provide a detailed analysis of long-term societal changes.
- What evidence would resolve it: Longitudinal studies or expert analyses on the effects of neuro-symbolic data generation on education, workforce development, and societal norms.

## Limitations
- Heavy reliance on GPT-4 for informalization and reasoning path generation limits scalability and reproducibility
- Effectiveness of projected MCMC sampling in highly irregular symbolic spaces remains somewhat unproven
- Scalability to more complex mathematical domains beyond arithmetic and algebra is uncertain

## Confidence
- **High Confidence**: The core claim that symbolic space mutation improves validity over natural language mutation is well-supported by the formalization and solver validation process.
- **Medium Confidence**: The effectiveness of projected MCMC for generating diverse valid solutions in symbolic space, as the evidence is primarily from the paper's methodology rather than external validation.
- **Medium Confidence**: The graduated difficulty training approach, as it aligns with curriculum learning principles but lacks direct empirical support in the paper or corpus.

## Next Checks
1. **Solver Expressivity Test**: Verify that Z3 and other solvers can express and solve a representative sample of GSM8K and MATH problems in SMT-LIB format. Identify any mathematical constructs that cannot be formalized.
2. **MCMC Sampling Diversity Analysis**: Implement projected MCMC sampling on a simplified symbolic problem set and analyze the diversity of generated solutions. Check for convergence issues or constraint violations.
3. **Informalization Consistency Check**: Use GPT-4 to translate a small set of formal problems to natural language and verify that the generated problems are solvable by humans and match the solver's answers. Test different prompt templates to optimize consistency.