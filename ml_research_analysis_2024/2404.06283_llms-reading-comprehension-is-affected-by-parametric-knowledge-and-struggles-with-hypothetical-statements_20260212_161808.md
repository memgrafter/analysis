---
ver: rpa2
title: LLMs' Reading Comprehension Is Affected by Parametric Knowledge and Struggles
  with Hypothetical Statements
arxiv_id: '2404.06283'
source_url: https://arxiv.org/abs/2404.06283
tags:
- knowledge
- parametric
- context
- contexts
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of evaluating language models'\
  \ text understanding capabilities while mitigating the influence of parametric knowledge.\
  \ The authors propose using \"imaginary\" data\u2014contexts and questions based\
  \ on fictitious entities and facts\u2014to create a knowledge-conflict-free benchmark."
---

# LLMs' Reading Comprehension Is Affected by Parametric Knowledge and Struggles with Hypothetical Statements

## Quick Facts
- arXiv ID: 2404.06283
- Source URL: https://arxiv.org/abs/2404.06283
- Authors: Victoria Basmov; Yoav Goldberg; Reut Tsarfaty
- Reference count: 15
- Primary result: LLMs struggle with hypothetical statements involving modality and conditionals, especially when parametric knowledge conflicts with context information

## Executive Summary
This paper addresses the challenge of evaluating language models' text understanding capabilities while mitigating the influence of parametric knowledge. The authors propose using "imaginary" data—contexts and questions based on fictitious entities and facts—to create a knowledge-conflict-free benchmark. This approach allows for more reliable assessment of models' comprehension abilities, especially for non-affirmative constructions like negation, modality, and conditionals. Experiments with ChatGPT, GPT-4, LLaMA 2, and Mixtral reveal that while models handle simple affirmative and negative contexts well, they struggle significantly with hypothetical statements involving modality and conditionals. The study also finds that these semantic complexities trigger vulnerability to knowledge conflicts, even in models that appear context-faithful on simpler tasks.

## Method Summary
The authors create a controlled evaluation framework using imaginary datasets derived from WebQuestions and SQuAD2.0, where real entities and facts are replaced with fictitious ones. They generate five semantic variation sets for each context: affirmative, negated, unlikely, modals, and conditionals (both "if" and "would" forms). Using zero-shot extractive QA with different prompt templates, they test models across three conditions: supported (context supports answer), contradicting (context contradicts parametric knowledge), and imaginary (no external knowledge to draw from). Performance is measured using F1 scores from the SQuAD2.0 evaluation script, with a controlled setting that only accepts exact context spans or "None" answers.

## Key Results
- Models handle simple affirmative and negative contexts well but struggle significantly with hypothetical statements involving modality and conditionals
- Semantic complexity triggers vulnerability to knowledge conflicts, even in models that appear context-faithful on simpler tasks
- Parametric knowledge interferes with reading comprehension, causing models to output incorrect answers based on memorized information rather than context

## Why This Works (Mechanism)
Models rely heavily on parametric knowledge when processing text, which can override or conflict with information provided in the context. This parametric knowledge becomes particularly problematic when dealing with non-affirmative constructions that require careful semantic interpretation. The imaginary data approach works by eliminating the possibility of knowledge conflicts, forcing models to rely solely on context for answers. This reveals the true extent of models' comprehension capabilities and exposes vulnerabilities that are masked when parametric knowledge happens to align with the correct answer.

## Foundational Learning

**Extractive QA Task**: Understanding how models extract answers from text using start and end positions; needed to grasp the evaluation methodology and metrics used.

**Parametric Knowledge**: Knowledge acquired during model training that exists independently of any given context; critical for understanding how memorized information can interfere with context-based reasoning.

**Non-affirmative Constructions**: Linguistic forms like negation, modality, and conditionals that modify or qualify the truth conditions of statements; essential for understanding the specific semantic challenges tested.

**Knowledge Conflict**: When a model's parametric knowledge contradicts information in the provided context; key concept for interpreting why imaginary data is necessary for fair evaluation.

**Zero-shot Evaluation**: Testing models without any task-specific training, using only prompts; important for understanding how the experiments isolate comprehension abilities from task familiarity.

## Architecture Onboarding

**Component Map**: Imaginary data generation -> Context variations (5 types) -> Prompt templates -> Model inference -> F1 evaluation with SQuAD2.0 script

**Critical Path**: Context creation → Semantic variation application → Zero-shot prompting → Answer extraction → Performance measurement

**Design Tradeoffs**: Using imaginary data eliminates knowledge conflicts but may reduce real-world applicability; controlled evaluation ensures fairness but doesn't reflect typical usage scenarios.

**Failure Signatures**: Models outputting parametric knowledge instead of context answers on supported data; failure to abstain on non-affirmative contexts; consistent performance degradation on modality and conditional constructions.

**First Experiments**:
1. Generate imaginary data from a small subset of WebQuestions and create all five semantic variations
2. Test a single model (e.g., GPT-3.5) on the imaginary dataset with one prompt template
3. Compare performance across the three conditions (supported, contradicting, imaginary) for affirmative constructions

## Open Questions the Paper Calls Out
None

## Limitations
- Manual crafting of imaginary data introduces potential inconsistencies in how non-affirmative variations are constructed across contexts
- The controlled setting that accepts only exact context spans or "None" answers may not reflect real-world usage where some knowledge integration is expected
- Reliance on fictitious entities limits external validity and generalizability to real-world applications

## Confidence

**High confidence**: Models struggle with hypothetical statements involving modality and conditionals is well-supported by consistent performance degradation across multiple model families.

**Medium confidence**: Parametric knowledge triggers vulnerability to knowledge conflicts in non-affirmative contexts, while supported by error analysis, could benefit from additional systematic investigation.

**Medium confidence**: Imaginary data is necessary for accurate NLU evaluation is compelling but requires broader validation across different task types and model architectures.

## Next Checks

1. Replicate experiments with a larger sample of non-affirmative variations created using standardized transformation rules to assess consistency across different constructions.

2. Test additional model families including open-source models with different training approaches (instruction-tuned vs. base models) to determine whether patterns are universal or architecture-specific.

3. Conduct ablation studies varying the complexity and type of knowledge conflicts in non-affirmative contexts to better understand the interaction between semantic complexity and parametric knowledge interference.