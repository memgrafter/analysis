---
ver: rpa2
title: 'Dubo-SQL: Diverse Retrieval-Augmented Generation and Fine Tuning for Text-to-SQL'
arxiv_id: '2404.12560'
source_url: https://arxiv.org/abs/2404.12560
tags:
- dubo-sql
- performance
- examples
- fine
- text-to-sql
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two methods, Dubo-SQL v1 and v2, for automated
  text-to-SQL generation. Dubo-SQL v1 uses low-cost fine-tuning of GPT-3.5 Turbo to
  achieve record-breaking execution accuracy (60.71%) on the BIRD-SQL benchmark at
  lower cost than competitors.
---

# Dubo-SQL: Diverse Retrieval-Augmented Generation and Fine Tuning for Text-to-SQL

## Quick Facts
- arXiv ID: 2404.12560
- Source URL: https://arxiv.org/abs/2404.12560
- Reference count: 6
- Key outcome: Dubo-SQL v1 achieves 60.71% execution accuracy on BIRD-SQL benchmark at lower cost than competitors

## Executive Summary
This paper introduces Dubo-SQL, a novel approach for automated text-to-SQL generation that combines diverse retrieval-augmented generation with fine-tuning. The authors propose two methods: Dubo-SQL v1, which uses low-cost fine-tuning of GPT-3.5 Turbo, and Dubo-SQL v2, which employs GPT-4 Turbo with diverse RAG without fine-tuning. Both methods significantly improve execution accuracy on the BIRD-SQL benchmark while reducing inference costs compared to prior work. Key innovations include diverse RAG for selecting informative few-shot examples, presenting examples in conversation history format, and requesting JSON output format.

## Method Summary
Dubo-SQL addresses the text-to-SQL generation problem using two complementary approaches. Dubo-SQL v1 fine-tunes GPT-3.5 Turbo on the BIRD-SQL training set (9,424 examples, 17M tokens) for 2 epochs to generate SQL queries from natural language questions and database schemas. Dubo-SQL v2 uses GPT-4 Turbo with diverse retrieval-augmented generation, which selects informative few-shot examples to improve learning from limited context. Both methods format prompts using a conversation history approach and request JSON output format. The evaluation uses execution accuracy (EX) on the BIRD-SQL benchmark, which tests whether generated SQL queries return correct results when executed against actual databases.

## Key Results
- Dubo-SQL v1 achieves 60.71% execution accuracy on BIRD-SQL benchmark
- Dubo-SQL v2 achieves 61.47% execution accuracy without fine-tuning
- Both methods outperform existing approaches while reducing inference costs

## Why This Works (Mechanism)
The diverse RAG method improves few-shot learning by selecting examples that maximize information diversity, helping the model generalize better to unseen queries. The conversation history format provides contextual understanding that mirrors real-world interactions, improving the model's ability to handle multi-turn dialogues. The JSON output format reduces parsing errors and ensures structured responses that are easier to validate programmatically.

## Foundational Learning
- **BIRD-SQL benchmark**: A text-to-SQL dataset with 95 databases from 37 industries used for evaluating Dubo-SQL
  - Why needed: Provides standardized evaluation framework for comparing text-to-SQL methods
  - Quick check: Verify the benchmark includes diverse schemas and sample data
- **Execution accuracy (EX)**: Metric measuring whether generated SQL queries return correct results when executed
  - Why needed: Directly measures practical utility of text-to-SQL systems
  - Quick check: Confirm queries return expected results on test databases
- **Few-shot learning**: Technique using limited examples to improve model performance
  - Why needed: Enables high performance without extensive fine-tuning
  - Quick check: Test model performance with varying numbers of examples
- **Retrieval-augmented generation**: Method combining information retrieval with language model generation
  - Why needed: Improves generation quality by incorporating relevant examples
  - Quick check: Verify retrieved examples are contextually relevant
- **Conversation history formatting**: Present examples as multi-turn dialogues rather than isolated queries
  - Why needed: Mirrors real-world usage patterns and improves contextual understanding
  - Quick check: Ensure formatting preserves logical flow between turns
- **JSON output format**: Request structured JSON responses instead of raw text
  - Why needed: Reduces parsing errors and ensures consistent output structure
  - Quick check: Validate JSON schema matches expected format

## Architecture Onboarding

**Component Map**: Database Schema -> Diverse RAG Example Selection -> Prompt Formatting -> Model Generation -> SQL Validation

**Critical Path**: The generation pipeline starts with database schema input, uses diverse RAG to select informative examples, formats these into conversation history prompts, generates SQL queries through the language model, and validates results through execution against the database.

**Design Tradeoffs**: The authors chose diverse RAG over simple random sampling to maximize information coverage in limited context windows. They opted for conversation history formatting over isolated examples to better capture real-world usage patterns. JSON output was preferred over plain text to reduce parsing complexity and errors.

**Failure Signatures**: SQL syntax errors typically indicate issues with column selection or table joins. Low execution accuracy suggests problems with prompt formatting or insufficient example diversity. Context window overflow may occur with very large schemas.

**First Experiments**:
1. Test diverse RAG example selection on a small database to verify improved coverage over random sampling
2. Validate conversation history formatting by comparing performance with isolated example prompts
3. Verify JSON output parsing works correctly across different database schemas

## Open Questions the Paper Calls Out

### Open Question 1
How does the execution accuracy of Dubo-SQL v1 compare to human performance on the BIRD-SQL benchmark?
- Basis in paper: [explicit] The paper states that human performance on the BIRD-SQL benchmark is 92.96%, while Dubo-SQL v1 achieves 60.71% execution accuracy.
- Why unresolved: The paper does not provide a direct comparison or analysis of the gap between Dubo-SQL v1's performance and human performance, nor does it explore the specific types of errors or limitations that prevent Dubo-SQL v1 from achieving human-level accuracy.
- What evidence would resolve it: A detailed error analysis of Dubo-SQL v1's predictions on the BIRD-SQL benchmark, comparing the types and frequency of errors to those made by humans, would provide insights into the remaining challenges and potential areas for improvement.

### Open Question 2
How does the performance of Dubo-SQL v1 and v2 scale with the size and complexity of the database schema and sample data?
- Basis in paper: [inferred] The paper mentions that BIRD-SQL databases are small compared to common corporate databases, and that Dubo-SQL v2 uses GPT-4 Turbo with a longer context window (128,000 tokens) compared to Dubo-SQL v1 (4,096 tokens). However, the paper does not provide experimental results or analysis of how the methods perform on larger, more complex databases.
- Why unresolved: The scalability of the methods to real-world, large-scale databases is a critical factor in their practical applicability, but the paper does not address this issue directly.
- What evidence would resolve it: Experiments evaluating Dubo-SQL v1 and v2 on a diverse set of databases with varying sizes and complexities, including real-world corporate databases, would provide insights into the scalability and limitations of the methods.

### Open Question 3
How do the diverse retrieval-augmented generation (RAG) and conversation history formatting techniques in Dubo-SQL v2 contribute to its improved performance compared to Dubo-SQL v1?
- Basis in paper: [explicit] The paper introduces diverse RAG and conversation history formatting as key innovations in Dubo-SQL v2, claiming that they help the LLM learn more from few-shot examples. However, the paper does not provide a detailed ablation study or analysis of the individual contributions of these techniques to the overall performance improvement.
- Why unresolved: While the paper presents the overall performance of Dubo-SQL v2, it does not provide insights into the specific impact of the diverse RAG and conversation history formatting techniques on the model's accuracy.
- What evidence would resolve it: An ablation study that systematically evaluates the performance of Dubo-SQL v2 with and without the diverse RAG and conversation history formatting techniques, as well as other possible variations, would provide a clearer understanding of their individual contributions to the improved performance.

## Limitations
- The paper lacks detailed implementation information for the diverse RAG algorithm, making it difficult to verify the claimed improvements
- Cost comparisons rely on estimated API pricing rather than actual usage data, which may not reflect real-world efficiency differences
- The BIRD-SQL benchmark uses relatively small databases, limiting conclusions about scalability to enterprise-scale schemas

## Confidence

**Execution accuracy claims**: High - The reported results are verifiable through the BIRD-SQL benchmark, though independent reproduction would strengthen confidence

**Cost efficiency claims**: Medium - While API pricing is documented, real-world efficiency depends on implementation details not fully disclosed

**Methodological innovations**: Medium - The diverse RAG approach and conversation history formatting are novel, but the paper lacks sufficient implementation details for complete verification

## Next Checks

1. Verify the exact schema format and foreign key specifications used in the BIRD-SQL benchmark to ensure faithful reproduction
2. Replicate the diverse retrieval-augmented generation process independently to confirm it consistently improves few-shot example selection
3. Conduct cost analysis using actual API usage logs rather than estimated pricing to validate efficiency claims