---
ver: rpa2
title: Reinforcement Learning Gradients as Vitamin for Online Finetuning Decision
  Transformers
arxiv_id: '2410.24108'
source_url: https://arxiv.org/abs/2410.24108
tags:
- online
- decision
- reward
- transformer
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the under-explored problem of online finetuning
  for decision transformers, which struggle when pretrained with low-reward offline
  data. The authors theoretically analyze this limitation, showing that conditioning
  on high return-to-go values far from the expected return hampers policy improvement.
---

# Reinforcement Learning Gradients as Vitamin for Online Finetuning Decision Transformers

## Quick Facts
- arXiv ID: 2410.24108
- Source URL: https://arxiv.org/abs/2410.24108
- Reference count: 40
- Primary result: Adding TD3 gradients to decision transformers significantly improves online finetuning performance, especially on low-quality offline data

## Executive Summary
This work addresses the challenge of online finetuning decision transformers when pretrained on low-quality offline data. The authors theoretically analyze why decision transformers struggle when conditioning on high return-to-go values far from the expected return, showing this hampers policy improvement. They propose a simple yet effective solution: adding TD3 gradients to the decision transformer training process. This approach combines the transformer architecture with RL gradients, achieving significant improvements over baselines like ODT, TD3+BC, and IQL across multiple environments including Adroit, Antmaze, MuJoCo, and Maze2D.

## Method Summary
The authors propose combining decision transformers with TD3 gradients during online finetuning. The method uses a standard decision transformer architecture (4 layers, 4 heads, ~13M parameters) pretrained on offline data, then adds an RL gradient term to the actor loss during online training. Specifically, they train the actor with both the original ODT loss and an additional TD3 gradient term with coefficient α=0.1, while training a separate TD3 critic with MSE loss. The training involves collecting online transitions, updating the actor for 300 steps and critic for 600 steps per epoch, with batch size 256 and learning rates of 1e-3 (actor) and 1e-4/1e-3 (critic).

## Key Results
- ViDT outperforms ODT, TD3+BC, and IQL baselines across Adroit, Antmaze, and MuJoCo environments
- Significant performance gains on low-quality datasets (human/cloned/random) where decision transformers typically struggle
- Achieves high normalized returns with fewer online samples compared to baselines
- Ablation studies show α=0.1 provides optimal balance between stability and performance

## Why This Works (Mechanism)
The core insight is that decision transformers conditioned on high return-to-go values during online finetuning can suffer from policy degradation when the offline data is of low quality. By adding TD3 gradients, the method provides additional exploration guidance and more accurate value estimation, helping the policy avoid poor actions that might be reinforced by the transformer's autoregressive predictions. The RL gradients effectively act as a "vitamin" that supplements the transformer's learned behavior, particularly in the exploration phase where the return-to-go conditioning may be misleading.

## Foundational Learning

**Decision Transformer Architecture**
- Why needed: Enables autoregressive modeling of trajectories conditioned on desired returns
- Quick check: Verify transformer layers, attention heads, and positional encodings match paper specifications

**Offline-to-Online Transfer**
- Why needed: Critical for understanding the performance gap between offline pretraining and online finetuning
- Quick check: Confirm dataset splits and pretraining procedures match D4RL standards

**TD3 Algorithm**
- Why needed: Provides the gradient-based RL component that supplements the transformer
- Quick check: Validate critic architecture and target network updates match TD3 specifications

## Architecture Onboarding

**Component Map**
Offline Dataset -> Decision Transformer Pretraining -> Online Buffer -> ViDT Training (Actor + Critic)

**Critical Path**
Transformer Actor (ODT Loss + RL Gradient) -> TD3 Critic (MSE Loss) -> Online Data Collection

**Design Tradeoffs**
- Computational overhead: +20% training time vs ODT due to additional critic and gradient calculations
- Stability vs performance: α=0.1 balances exploration benefits against training instability
- Architecture complexity: Separate actor/critic networks vs unified approach

**Failure Signatures**
- Critic instability leading to NaN Q-values (indicates learning rate or architecture issues)
- Poor exploration with high RTGeval mismatch (suggests incorrect conditioning implementation)
- Degraded performance on high-quality data (may indicate over-reliance on RL gradients)

**First Experiments**
1. Verify decision transformer pretraining matches ODT performance on offline data
2. Test actor loss with RL gradient term while keeping critic frozen
3. Evaluate online performance with varying α values (0.01, 0.1, 1.0) to find optimal coefficient

## Open Questions the Paper Calls Out

**Open Question 1**
How can we theoretically analyze the performance of decision transformers without relying on strong assumptions about the return-to-go distribution?
- Basis: The paper's theoretical analysis relies on assumptions about Laplace distribution of return-to-go and Lipschitzness of RTG density function
- Why unresolved: These assumptions are strong and impractical for real-life applications
- Resolution: Develop new theoretical framework without these strong assumptions, potentially through more general probabilistic or information-theoretic approaches

**Open Question 2**
What are the optimal hyperparameters for combining TD3 gradients with decision transformers, and how do they vary across different environments and datasets?
- Basis: Limited ablation studies on hyperparameters like RL coefficient α and evaluation context length Teval
- Why unresolved: Only explores subset of possible hyperparameters without systematic study across environments
- Resolution: Comprehensive hyperparameter search across wider range of environments using Bayesian optimization or meta-learning

**Open Question 3**
How can we design a more efficient architecture for combining decision transformers with RL gradients, potentially reducing computational overhead?
- Basis: Method adds 20% extra training time compared to ODT
- Why unresolved: Does not explore alternative architectures or techniques for reducing computational overhead
- Resolution: Develop and evaluate new architectures or techniques achieving similar performance with lower computational cost

## Limitations
- Performance gains primarily demonstrated on low-quality datasets, effectiveness on expert data uncertain
- Theoretical analysis relies on strong assumptions about return-to-go distribution that may not hold in practice
- Additional computational overhead of ~20% compared to baseline ODT method

## Confidence

**High confidence**: Experimental findings showing ViDT outperforming ODT and other baselines on D4RL benchmarks across multiple environments and dataset qualities.

**Medium confidence**: Theoretical analysis explaining why decision transformers struggle with low-quality offline data and how RL gradients help, though mathematical derivations could benefit from more rigorous proofs.

**Medium confidence**: Claim that RL gradients specifically address exploration challenges during online finetuning, as ablation studies provide supporting evidence but don't isolate exploration effect from other benefits.

## Next Checks

1. Implement and test the RTG conditioning mechanism during training to verify that conditioning on RTG values outside expected return range is problematic and that the proposed solution addresses this issue.

2. Conduct ablation study varying TD3 gradient coefficient α (0.01, 0.1, 1.0) to determine optimal trade-off between stability and performance, verifying α=0.1 is not arbitrary.

3. Test method on high-quality expert datasets (like D4RL's "expert" splits) to validate whether approach provides benefits beyond correcting poor offline data quality, or introduces unnecessary complexity when good data is available.