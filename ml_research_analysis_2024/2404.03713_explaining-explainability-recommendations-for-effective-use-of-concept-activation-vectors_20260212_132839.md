---
ver: rpa2
title: 'Explaining Explainability: Recommendations for Effective Use of Concept Activation
  Vectors'
arxiv_id: '2404.03713'
source_url: https://arxiv.org/abs/2404.03713
tags:
- cavs
- concept
- concepts
- dataset
- tcav
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates three properties of Concept Activation
  Vectors (CAVs): inconsistency across layers, entanglement with other concepts, and
  spatial dependency. CAVs are vector representations of concepts found in the activation
  space of neural networks using probe datasets.'
---

# Explaining Explainability: Recommendations for Effective Use of Concept Activation Vectors

## Quick Facts
- arXiv ID: 2404.03713
- Source URL: https://arxiv.org/abs/2404.03713
- Reference count: 40
- This work investigates three properties of Concept Activation Vectors (CAVs): inconsistency across layers, entanglement with other concepts, and spatial dependency.

## Executive Summary
This paper examines three critical properties of Concept Activation Vectors (CAVs) that can lead to misleading model explanations: inconsistency across layers, entanglement with other concepts, and spatial dependency. Through experiments on synthetic (Elements) and real datasets (ImageNet, ISIC 2019), the authors demonstrate how these properties affect TCAV scores and model interpretation. They introduce tools to detect these properties and provide practical recommendations for their mitigation. The study emphasizes that understanding these properties is essential for effective use of CAVs in practice and releases the Elements dataset to facilitate further research in this area.

## Method Summary
The method involves training binary linear classifiers to find CAVs in activation space using probe datasets containing concept exemplars. TCAV is then used to measure model sensitivity to concepts. The authors analyze consistency across layers using a consistency error metric, assess concept entanglement through cosine similarity matrices, and examine spatial dependence by visualizing spatial norms of CAVs. Experiments are conducted on both synthetic (Elements) and real-world datasets to demonstrate the impact of these properties on model interpretation.

## Key Results
- CAVs exhibit inconsistency across layers, with empirical evidence showing that the standard approach to finding CAVs does not yield optimally layer-consistent results
- Entanglement between CAVs can cause misleadingly high TCAV scores, with the choice of negative probe set substantially impacting the meaning of a CAV
- Spatially dependent CAVs reveal that models are not translation invariant with respect to specific concepts, with TCAV scores varying by concept location

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CAVs trained on the same concept with different negative probe sets can encode entangled concepts, leading to misleading TCAV scores.
- Mechanism: When the negative probe set contains samples from related concepts, the resulting CAVs become more similar to each other than to unrelated concepts, encoding not just the target concept but also "not-concept" information.
- Core assumption: The negative probe set significantly influences the direction of the CAV in activation space, and related concepts share similar activation patterns.
- Evidence anchors:
  - [abstract] "we demonstrate how entanglement can lead to uninterpretable results and that the choice of negative probe set can have a substantial impact on the meaning of a CAV"
  - [section 6.2] "The association between CAVs causes a misleadingly high TCAV score for the red concept"

### Mechanism 2
- Claim: CAVs are inconsistent across layers because neural network transformations are not linear combinations of the form f(al1) = g(al1) + Mal1 + b where g is periodic with period equal to the CAV.
- Mechanism: The layer transformation f cannot simultaneously preserve vector addition and represent periodic functions with CAV periods, making CAVs in different layers encode different aspects of the same concept.
- Core assumption: Neural network layers use common activation functions that cannot satisfy the mathematical conditions for layer consistency.
- Evidence anchors:
  - [section 3.1] "we prove the result that Eq. 12 can hold if and only if f is composed of a periodic function with period vc,l1 and a non-zero linear term M"
  - [section 6.1] "the consistency error for the optimised CAVs is lower, implying that the standard approach to find CAVs does not find optimally layer consistent CAVs"

### Mechanism 3
- Claim: Spatially dependent CAVs can detect translation invariance by exploiting the fact that CNNs have activation spatial dependence - different spatial locations affect activations differently.
- Mechanism: When probe datasets contain concepts at specific locations, the resulting CAVs develop spatial norms that peak at corresponding activation locations, revealing whether the model treats the concept equally regardless of position.
- Evidence anchors:
  - [abstract] "we introduce spatially dependent CAVs to test if a model is translation invariant with respect to a specific concept and class"
  - [section 6.3] "we use spatially dependent CAVs to show that a model is not translation invariant with respect to striped or triangle in fig. 5b"

## Foundational Learning

- Concept: Vector representation of concepts in neural network activation space
  - Why needed here: Understanding how CAVs are mathematical objects that capture concept directions in high-dimensional activation space is fundamental to grasping why they have the properties studied in this paper
  - Quick check question: If a CAV is a vector in activation space, what does it mean for two CAVs to be "entangled" from a geometric perspective?

- Concept: TCAV score calculation and interpretation
  - Why needed here: The paper's findings about inconsistency, entanglement, and spatial dependence all affect how TCAV scores should be interpreted
  - Quick check question: How does the directional derivative Sc,k,l(x) in equation 4 relate to the CAV's direction in activation space?

- Concept: Probe dataset construction and its impact
  - Why needed here: The paper demonstrates that different probe dataset choices dramatically affect CAV properties
  - Quick check question: What would happen to a CAV if the negative probe set contained images with similar concepts to the positive set?

## Architecture Onboarding

- Component map: Trained neural network model → Concept probe datasets (positive/negative) → Linear classifier training for CAV generation → TCAV score calculation → Analysis tools (consistency/entanglement/spatial) → Interpretation/recommendations
- Critical path: Probe dataset → CAV generation (linear classifier) → TCAV score calculation → Property analysis (consistency/entanglement/spatial) → Interpretation/recommendations
- Design tradeoffs: Using multiple layers increases computational cost but provides robustness to inconsistency; choosing appropriate negative probe sets affects CAV quality but may require domain expertise; visualizing spatial norms adds interpretability but increases complexity
- Failure signatures: Inconsistent TCAV scores across layers suggest the model represents concepts differently at different depths; high similarity between CAVs for opposing concepts indicates entanglement; uniform spatial norms suggest lack of spatial dependence when it should exist
- First 3 experiments:
  1. Train a simple CNN on the Elements dataset and generate CAVs for a concept across multiple layers, measuring consistency error
  2. Create two probe datasets for the same concept but with different negative sets and compare resulting CAV similarity matrices
  3. Generate spatially dependent CAVs using probe datasets with concepts at specific locations and visualize their spatial norms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can neural networks be designed or trained to have layer-consistent concept activation vectors (CAVs) across multiple layers?
- Basis in paper: [explicit] The paper proves that layer consistency is theoretically possible only if the function mapping activations between layers has a specific form (periodic function plus non-zero linear term), but empirical evidence suggests this does not occur naturally in standard networks
- Why unresolved: The paper demonstrates that layer inconsistency is common in practice but does not explore whether architectural modifications or training procedures could encourage consistency
- What evidence would resolve it: Experiments showing whether specific architectural choices or training techniques can produce CAVs with minimal consistency error across layers compared to baseline networks

### Open Question 2
- Question: How can we quantitatively measure and mitigate concept entanglement in concept activation vectors (CAVs) to ensure they represent only the intended concept?
- Basis in paper: [explicit] The paper introduces cosine similarity matrices to visualize entanglement and shows that entangled CAVs can lead to misleading TCAV scores, but does not provide a comprehensive solution for mitigation
- Why unresolved: While the paper demonstrates the problem of entanglement and suggests verifying expected dependencies, it does not offer a systematic method to reduce entanglement or ensure CAVs are disentangled
- What evidence would resolve it: Development and validation of methods that either preprocess probe datasets to minimize concept correlation, modify CAV training to explicitly discourage entanglement, or provide quantitative metrics to assess and correct entanglement post-hoc

### Open Question 3
- Question: How do concept activation vectors (CAVs) behave in transformer-based architectures compared to convolutional neural networks (CNNs), particularly regarding spatial dependence and translation invariance?
- Basis in paper: [explicit] The paper provides preliminary evidence that spatially dependent CAVs can be created for Vision Transformers (ViTs) and that TCAV scores vary by concept location, but does not conduct a comprehensive analysis
- Why unresolved: The experiments are limited to a single ViT model and a synthetic dataset, leaving open questions about how CAV properties generalize to different transformer architectures and real-world datasets
- What evidence would resolve it: Extensive experiments comparing CAV properties across multiple transformer architectures on diverse datasets, examining spatial dependence, translation invariance, and consistency in a manner analogous to the CNN experiments

## Limitations

- The generalizability of findings to complex real-world models remains uncertain despite using both synthetic and real datasets
- The mathematical proof of layer inconsistency relies on specific assumptions about neural network architectures that may not hold for all model types
- The impact of these properties on practical model interpretation in high-stakes domains like medical diagnosis needs further validation

## Confidence

- High confidence in demonstrating CAV inconsistency across layers (empirical evidence with multiple datasets and benchmarks)
- Medium confidence in entanglement claims (supported by cosine similarity analysis but limited by probe dataset design choices)
- Medium confidence in spatial dependence findings (shown for CNNs but transformers remain under-explored)

## Next Checks

1. Replicate the consistency error analysis on a pre-trained ResNet-50 on ImageNet to verify layer inconsistency in established models
2. Conduct a controlled experiment varying negative probe set composition systematically to quantify entanglement effects on TCAV scores
3. Test spatially dependent CAVs on transformer architectures to determine if spatial dependence findings generalize beyond CNNs