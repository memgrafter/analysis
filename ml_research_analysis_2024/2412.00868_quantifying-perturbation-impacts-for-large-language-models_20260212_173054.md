---
ver: rpa2
title: Quantifying perturbation impacts for large language models
arxiv_id: '2412.00868'
source_url: https://arxiv.org/abs/2412.00868
tags:
- perturbation
- output
- input
- distributions
- outputs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Distribution-Based Perturbation Analysis
  (DBPA), a statistical framework for quantifying how input perturbations affect large
  language model outputs. The core method reformulates perturbation analysis as a
  frequentist hypothesis testing problem by constructing empirical null and alternative
  output distributions within a low-dimensional semantic similarity space via Monte
  Carlo sampling.
---

# Quantifying perturbation impacts for large language models

## Quick Facts
- arXiv ID: 2412.00868
- Source URL: https://arxiv.org/abs/2412.00868
- Reference count: 35
- Primary result: Introduces DBPA, a statistical framework for quantifying how input perturbations affect LLM outputs through Monte Carlo sampling and permutation testing

## Executive Summary
This paper presents Distribution-Based Perturbation Analysis (DBPA), a novel statistical framework for quantifying the impact of input perturbations on large language model outputs. The method reformulates perturbation analysis as a frequentist hypothesis testing problem, using Monte Carlo sampling to construct empirical output distributions and permutation testing to assess statistical significance without restrictive distributional assumptions. DBPA enables tractable inference through low-dimensional semantic similarity spaces while providing interpretable p-values and effect sizes.

## Method Summary
DBPA constructs empirical output distributions for original and perturbed inputs through Monte Carlo sampling, then quantifies distributional changes using pairwise cosine similarity metrics in embedding space. The framework employs permutation testing to assess statistical significance, computing p-values while only assuming exchangeability in the similarity space. Jensen-Shannon divergence serves as the primary effect size measure, and the method supports multiple testing with controlled error rates. The approach is model-agnostic and can evaluate arbitrary perturbations on any black-box LLM.

## Key Results
- Effectively measures answer divergence across different perturbation types and LLM outputs
- Quantifies LLM robustness to irrelevant prompt changes with controlled error rates
- Evaluates alignment between different models through statistical comparison of output distributions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Monte Carlo sampling resolves computational intractability of comparing full output distributions
- **Mechanism**: By sampling finite outputs from the LLM's output distribution for both original and perturbed inputs, the method constructs empirical approximations that are computationally feasible
- **Core assumption**: The finite sample size k is sufficiently large to capture the essential characteristics of the true output distributions
- **Evidence anchors**:
  - [section] "We address the computational complexity by Monte Carlo sampling. We define the stochastic approximations of the output distributions for an input x and its perturbation ∆(x) as: ˆDx = {yi}k i=1, yi i.i.d. ∼ S (x) and ˆD∆(x) = {y′i}k i=1, y′i i.i.d. ∼ S (∆(x))"
  - [corpus] Weak evidence - the corpus contains related work on perturbation analysis but no specific discussion of Monte Carlo sampling for LLM output distributions
- **Break condition**: If k is too small relative to the diversity of LLM outputs, the empirical distributions will poorly approximate the true distributions, leading to unreliable statistical inference

### Mechanism 2
- **Claim**: Pairwise cosine similarity in embedding space provides interpretable semantic comparison
- **Mechanism**: Embeddings of LLM outputs are compared using cosine similarity to construct null and alternative distributions in a low-dimensional semantic space, avoiding high-dimensional curse of dimensionality
- **Core assumption**: Cosine similarity in the embedding space correlates with semantic similarity of LLM outputs
- **Evidence anchors**:
  - [section] "To quantify the distributional changes induced by input perturbations, we introduce the similarity metric s : Y × Y → [0, 1] (discussed more in Sec. 3.2). This metric allows us to construct empirical distributions of pairwise similarities"
  - [section] "We find the cosine similarity s(yi, yj) = ⟨e(yi),e(yj)⟩/∥e(yi)∥∥e(yj)∥ to be a natural choice, where e(·) is an embedding function"
- **Break condition**: If the embedding space does not preserve semantic relationships relevant to the task, or if cosine similarity poorly captures the meaningful differences between outputs

### Mechanism 3
- **Claim**: Permutation testing enables statistical inference without restrictive distributional assumptions
- **Mechanism**: By permuting samples between null and alternative distributions under the null hypothesis, the method constructs a reference distribution for the test statistic without assuming specific distributional forms
- **Core assumption**: Under the null hypothesis, samples from S(x) and S(∆(x)) are exchangeable
- **Evidence anchors**:
  - [section] "To assess statistical significance, we employ a permutation test. This approach allows us to compute p-values while only assuming exchangeability in the similarity space"
  - [section] "p = P(ω(P∗0, P∗1) ≥ ω(P0,P1)|H0) where P∗0 and P∗1 are permuted versions of P0 and P1 under the null hypothesis"
- **Break condition**: If the exchangeability assumption is violated (i.e., S(x) ≠ S(∆(x)) under the null), the permutation test will produce incorrect p-values

## Foundational Learning

- **Concept**: Frequentist hypothesis testing
  - Why needed here: The framework reframes perturbation analysis as testing H0: S(x) = S(∆(x)) against H1: S(x) ≠ S(∆(x)), requiring understanding of null/alternative hypotheses, test statistics, and p-values
  - Quick check question: What is the difference between the null hypothesis and the alternative hypothesis in this context?

- **Concept**: Monte Carlo methods
  - Why needed here: The approach uses Monte Carlo sampling to approximate intractable output distributions, requiring understanding of random sampling and convergence properties
  - Quick check question: Why does increasing the sample size k improve the approximation of the true output distribution?

- **Concept**: Embedding spaces and similarity metrics
  - Why needed here: The framework projects high-dimensional LLM outputs into a semantic space using embeddings and cosine similarity, requiring understanding of vector representations and distance metrics
  - Quick check question: How does cosine similarity differ from Euclidean distance when comparing embeddings?

## Architecture Onboarding

- **Component map**: LLM interface -> Embedding function -> Similarity metric calculator -> Distribution constructor -> JSD calculator -> Permutation testing engine
- **Critical path**: Sample generation -> embedding -> similarity calculation -> distribution construction -> JSD computation -> permutation testing -> p-value calculation
- **Design tradeoffs**: The method trades computational efficiency (finite sampling) for statistical power (larger k needed), and interpretability (cosine similarity) for potentially missing non-linear semantic relationships
- **Failure signatures**: High variance in p-values across runs indicates insufficient sampling; consistently low JSD values suggest the perturbation has minimal effect; unexpectedly high p-values with large JSD may indicate violation of exchangeability assumptions
- **First 3 experiments**:
  1. Test with a simple prompt and perturbation where the effect is known (e.g., changing "cat" to "dog" in a sentence about animals)
  2. Vary the sample size k to observe convergence of p-values and JSD values
  3. Compare results using different embedding models (e.g., sentence-BERT vs. original LLM embeddings)

## Open Questions the Paper Calls Out

- **Open Question 1**: How do different similarity metrics (e.g., BERTScore, MoverScore, ROUGE) affect the sensitivity and specificity of DBPA in detecting meaningful changes in LLM outputs?
- **Open Question 2**: How can DBPA be extended to evaluate the robustness of LLMs to adversarial perturbations beyond simple prompt paraphrasing?
- **Open Question 3**: How can the insights from DBPA be translated into actionable strategies for improving LLM robustness and aligning outputs with human preferences?

## Limitations

- Sample size sensitivity affects statistical reliability and computational costs
- Embedding space dependence may not preserve task-relevant semantic relationships
- Exchangeability assumption under null hypothesis lacks extensive empirical validation

## Confidence

- Sample Size Sensitivity: Medium
- Embedding Space Dependence: Medium
- Exchangeability Assumption: Low

## Next Checks

1. **Sample Size Sensitivity Analysis**: Systematically vary k across multiple orders of magnitude (e.g., 10, 100, 1000, 10000) for different perturbation types and LLM outputs to quantify the tradeoff between computational cost and statistical reliability.

2. **Embedding Space Robustness**: Compare results using different embedding models (e.g., sentence-BERT, original LLM embeddings, task-specific embeddings) and similarity metrics (cosine, Euclidean, learned metrics) to assess sensitivity to the semantic representation choice.

3. **Exchangeability Validation**: Design experiments with perturbations known to have minimal semantic impact (e.g., synonym substitutions, minor syntactic changes) and verify that p-values are uniformly distributed under the null hypothesis, as expected from valid permutation tests.