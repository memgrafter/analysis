---
ver: rpa2
title: 'PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations
  for Zero-Shot Document Retrieval'
arxiv_id: '2404.18424'
source_url: https://arxiv.org/abs/2404.18424
tags:
- retrieval
- dense
- sparse
- document
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PromptReps, a zero-shot document retrieval
  method that prompts large language models (LLMs) to generate both dense and sparse
  representations for hybrid retrieval. The method uses a prompt to ask LLMs to represent
  text with a single word, then extracts the last token's hidden states as dense embeddings
  and the logits as sparse bag-of-words representations.
---

# PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval

## Quick Facts
- arXiv ID: 2404.18424
- Source URL: https://arxiv.org/abs/2404.18424
- Authors: Shengyao Zhuang; Xueguang Ma; Bevan Koopman; Jimmy Lin; Guido Zuccon
- Reference count: 40
- Key outcome: PromptReps achieves similar or higher retrieval effectiveness compared to state-of-the-art trained LLM embedding methods on BEIR, MSMARCO, and TREC datasets, particularly when using larger LLMs like Llama3-70B, without requiring additional training.

## Executive Summary
This paper introduces PromptReps, a zero-shot document retrieval method that prompts large language models (LLMs) to generate both dense and sparse representations for hybrid retrieval. The method uses a carefully crafted prompt to ask LLMs to represent text with a single word, then extracts the last token's hidden states as dense embeddings and the logits as sparse bag-of-words representations. PromptReps demonstrates that prompt engineering alone is sufficient to generate robust representations for retrieval tasks, achieving competitive performance across multiple benchmark datasets without the need for contrastive training.

## Method Summary
PromptReps is a zero-shot document retrieval method that leverages LLMs to generate both dense and sparse representations for hybrid retrieval. The approach prompts LLMs with a carefully crafted instruction to represent input text with a single word, then extracts two types of representations: dense representations from the last token's hidden states, and sparse representations from the logits over the vocabulary. The sparse representations are processed through a sparsification pipeline involving tokenization, filtering, ReLU activation, log-saturation, and top-k selection before being indexed in an inverted index. Dense representations are indexed in an ANN vector index, and both are combined using linear interpolation for final retrieval scores. The method requires no additional training and can be applied directly to full corpus retrieval.

## Key Results
- PromptReps achieves similar or higher retrieval effectiveness compared to state-of-the-art trained LLM embedding methods on BEIR, MSMARCO, and TREC datasets
- Larger LLMs (Llama3-70B) significantly improve both dense and sparse retrieval effectiveness compared to smaller models (Llama3-8B)
- The hybrid approach combining dense and sparse representations consistently outperforms using either representation type alone
- PromptReps demonstrates competitive performance without requiring any additional training or fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can directly generate effective retrieval representations without contrastive training by leveraging their zero-shot language understanding capabilities.
- Mechanism: The prompt engineering guides the LLM to produce a single-word representation of text, from which both dense (last hidden state) and sparse (logits) representations are extracted for hybrid retrieval.
- Core assumption: The LLM's ability to understand and compress text meaning into a single word translates to meaningful dense and sparse representations for retrieval.
- Evidence anchors:
  - [abstract] "Our method only requires prompts to guide an LLM to generate query and document representations for effective document retrieval."
  - [section 3] "we develop a sparse + dense hybrid document retrieval system by utilizing both the next token logits and the last layer hidden states outputted by the LLM"
  - [corpus] Weak evidence - no direct citations of similar prompt-based representation extraction approaches
- Break condition: If the LLM fails to generate meaningful single-word representations for the input text, the extracted dense and sparse representations will not capture relevant semantic information.

### Mechanism 2
- Claim: Sparse representations derived from logits provide robust exact term matching while dense representations enable semantic search.
- Mechanism: Logits are sparsified through filtering, ReLU activation, and quantization to create inverted index-compatible sparse vectors that emphasize important terms.
- Core assumption: The distribution of logits over vocabulary tokens reflects the relative importance of terms for representing the input text.
- Evidence anchors:
  - [section 3] "To sparsify the logit representations for sparse retrieval, we perform the following steps:..."
  - [section 3] "the logits are quantized by multiplying the original values by 100 and taking the integer operation on that, and the obtained values represent the weights of corresponding tokens."
  - [corpus] Weak evidence - SPLADE recipe mentioned but not specifically for LLM-derived logits
- Break condition: If the sparsification process removes too many relevant terms or the quantization loses critical information, retrieval effectiveness will degrade.

### Mechanism 3
- Claim: Scaling LLM size improves retrieval effectiveness by generating more discriminative representations.
- Mechanism: Larger LLMs produce more nuanced single-word representations, leading to better dense and sparse vectors for retrieval.
- Core assumption: The representational capacity of larger LLMs translates to improved encoding of text meaning into single-word representations.
- Evidence anchors:
  - [abstract] "especially when using a larger LLM"
  - [section 5.1] "When changing Llama3-8B-Instruction to Llama3-70B-Instruction, the dense and sparse retrieval effectiveness of PromptReps further improves"
  - [corpus] Weak evidence - scaling laws mentioned but not specifically for prompt-based representation generation
- Break condition: If the relationship between model size and representation quality plateaus or if larger models overfit to prompt patterns rather than general text understanding.

## Foundational Learning

- Concept: Prompt engineering and its role in guiding LLM behavior
  - Why needed here: The entire method relies on carefully crafted prompts to extract meaningful representations
  - Quick check question: What happens if you remove the phrase "Make sure your word is in lowercase" from the prompt?
- Concept: Dense vs sparse retrieval paradigms
  - Why needed here: PromptReps creates both representation types and combines them in hybrid retrieval
  - Quick check question: How would you modify the system to use only dense representations?
- Concept: Tokenization and subword representations
  - Why needed here: The method relies on understanding how LLMs split words into tokens for representation extraction
  - Quick check question: Why might using first-token representations be less effective than whole-word representations?

## Architecture Onboarding

- Component map: LLM inference → prompt processing → representation extraction (dense from hidden states, sparse from logits) → sparsification pipeline → inverted index + ANN index → hybrid scoring
- Critical path: Document encoding → index construction → query encoding → hybrid retrieval → score combination
- Design tradeoffs: Single-word representation vs multi-word (simplicity vs potential information loss), dense-only vs hybrid (latency vs effectiveness), model size vs computational cost
- Failure signatures: Poor retrieval when LLM generates generic words, sparse representations dominated by stop words, dense representations not capturing semantic similarity
- First 3 experiments:
  1. Run PromptReps with a simple prompt on a small dataset and verify dense/sparse representations are extracted correctly
  2. Test retrieval effectiveness with dense-only vs sparse-only vs hybrid on a validation set
  3. Experiment with different prompt variations (single vs multiple words, different instructions) to find optimal prompt design

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal prompt engineering strategy to maximize the retrieval effectiveness of PromptReps across different domains and tasks?
- Basis in paper: [inferred] The paper discusses the impact of different prompts on retrieval effectiveness and suggests that future work could explore customized instructions for PromptReps to generate embeddings specific to different domains, tasks, or even to multi-lingual and cross-lingual IR settings.
- Why unresolved: The paper only explored a limited set of prompts and did not conduct an exhaustive search for the optimal prompt strategy across different domains and tasks.
- What evidence would resolve it: Conducting experiments with a wide range of prompts, including domain-specific and task-specific instructions, and evaluating their impact on retrieval effectiveness across various datasets and domains.

### Open Question 2
- Question: How can PromptReps be further improved by incorporating recent advancements in prompt compression and multi-vector dense retrieval techniques?
- Basis in paper: [inferred] The paper mentions that PromptReps has higher query latency compared to other LLM-based dense retrievers due to the longer input length. It also suggests that techniques like prompt compression could mitigate this limitation. Additionally, the paper briefly explores alternative representation methods, including multi-vector dense retrieval, but does not fully investigate their potential.
- Why unresolved: The paper only mentions these techniques as potential areas for future work and does not provide experimental results on their effectiveness when applied to PromptReps.
- What evidence would resolve it: Implementing prompt compression techniques and multi-vector dense retrieval methods on PromptReps and evaluating their impact on query latency and retrieval effectiveness.

### Open Question 3
- Question: Can PromptReps be effectively adapted for multi-lingual and cross-lingual information retrieval tasks?
- Basis in paper: [inferred] The paper suggests that future work could explore how to customize instructions for PromptReps to generate embeddings specific to different domains, tasks, or even to multi-lingual and cross-lingual IR settings.
- Why unresolved: The paper does not provide any experimental results on the performance of PromptReps in multi-lingual or cross-lingual IR tasks.
- What evidence would resolve it: Conducting experiments on multi-lingual and cross-lingual IR datasets, such as those used in the TREC 2022 Deep Learning Track, and evaluating the retrieval effectiveness of PromptReps compared to other multi-lingual and cross-lingual IR methods.

## Limitations

- The method's effectiveness heavily depends on prompt engineering quality, which was not thoroughly explored through ablation studies
- The sparsification pipeline for converting logits to sparse representations relies on heuristic steps without clear justification for parameter choices
- The claim that prompt engineering alone is sufficient for robust zero-shot retrieval is premature given the limited prompt variation experiments

## Confidence

*High Confidence*: The core methodology of extracting dense representations from LLM hidden states and combining them with sparse representations for hybrid retrieval is technically sound and well-implemented.

*Medium Confidence*: The claim that larger LLMs consistently produce better representations has empirical support but lacks theoretical grounding.

*Low Confidence*: The assertion that prompt engineering alone is sufficient for robust zero-shot retrieval is premature given the limited prompt variation experiments.

## Next Checks

1. Conduct systematic prompt engineering ablation studies comparing single-word vs multi-word representations, different instruction phrasings, and explicit vs implicit prompting strategies to isolate the impact of prompt design on retrieval effectiveness.

2. Perform detailed analysis of the sparse representation pipeline by varying the top-k selection threshold, quantization strategies, and sparsification parameters to determine their individual contributions to retrieval performance.

3. Test the method's robustness across diverse document types and retrieval scenarios (long documents, technical domains, different languages) to validate whether the approach generalizes beyond the standard BEIR benchmark datasets.