---
ver: rpa2
title: Representing Rule-based Chatbots with Transformers
arxiv_id: '2407.10949'
source_url: https://arxiv.org/abs/2407.10949
tags:
- eliza
- each
- template
- input
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes ELIZA, a rule-based chatbot, as a testbed\
  \ for formal mechanistic analysis of Transformers in conversational settings. It\
  \ presents theoretical constructions for how Transformers can implement ELIZA\u2019\
  s pattern matching and long-term memory mechanisms, building on prior work on automata\
  \ simulation."
---

# Representing Rule-based Chatbots with Transformers

## Quick Facts
- arXiv ID: 2407.10949
- Source URL: https://arxiv.org/abs/2407.10949
- Authors: Dan Friedman; Abhishek Panigrahi; Danqi Chen
- Reference count: 40
- Key outcome: ELIZA rule-based chatbot testbed shows Transformers struggle with precise copying and memory queue tracking, favoring induction-head mechanisms

## Executive Summary
This paper introduces ELIZA, a rule-based chatbot, as a testbed for analyzing how Transformers implement conversational mechanisms. The authors present theoretical constructions showing how Transformers can simulate ELIZA's pattern matching through finite-state automata and use intermediate outputs as implicit scratchpads for long-term memory tracking. Empirical results reveal that models have difficulty with precise copying tasks and memory queue operations, while showing a strong preference for content-based copying mechanisms over position-based ones. The work provides a framework for formal mechanistic analysis of conversational agents in a controlled setting.

## Method Summary
The paper proposes theoretical mechanisms for implementing ELIZA's rule-based conversation system in Transformers. For pattern matching, they use finite-state automata simulation where each Transformer layer corresponds to one state in the automaton, recognizing star-free regular expressions through uniform and position-based attention. For long-term memory tracking, they show how models can process their own intermediate outputs to track cycling and memory queue states, avoiding complex automata through implicit scratchpad mechanisms. The empirical evaluation trains 8-layer decoder-only Transformers on synthetic ELIZA conversation datasets, measuring turn-level exact match accuracy across different task types and analyzing attention patterns to understand the copying mechanisms learned.

## Key Results
- Transformers struggle most with precise copying and memory queue tracking subtasks in ELIZA implementation
- Models show strong bias toward induction-head copying mechanisms rather than precise position-based copying
- Using intermediate outputs as implicit scratchpads is more efficient than simulating complex automata for memory tracking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers can implement ELIZA's pattern matching by simulating finite-state automata for each template.
- Mechanism: Each Transformer layer corresponds to one state in the automaton. At layer ℓ, the model determines whether the input matches the first ℓ symbols of each template using two attention heads per layer—one attending uniformly to the prefix, one attending to the previous position.
- Core assumption: ELIZA templates are equivalent to star-free regular expressions, which can be recognized by finite-state automata.
- Evidence anchors:
  - [abstract]: "Building on prior constructions, particularly those for simulating finite-state automata, we show how simpler mechanisms can be composed and extended to produce more sophisticated behavior."
  - [section 3.1]: "We build on the constructions of (Liu et al., 2023; Yang et al., 2024). At a high level, we can recognize a template with L symbols using a Transformer with L layers."
  - [corpus]: Found 25 related papers; weak signal for automata construction evidence, but relevant papers like "Script-Strategy Aligned Generation" suggest dialogue script alignment is a real challenge.
- Break condition: Templates longer than the number of Transformer layers, or templates with n-gram literals requiring more context than the model can handle with available attention heads.

### Mechanism 2
- Claim: Transformers use intermediate outputs to simulate data structures for long-term memory tracking.
- Mechanism: Instead of simulating complex automata for cycling and memory queues, models examine their own previous outputs to track state. For cycling, the model identifies previous responses to the same template and increments the response index. For memory queues, the model counts previous dequeue operations by examining earlier outputs.
- Core assumption: Models can reliably identify and process their own intermediate outputs as part of the computation.
- Evidence anchors:
  - [abstract]: "using intermediate generations to simulate recurrent data structures, akin to an implicit scratchpad or Chain-of-Thought"
  - [section 3.2]: "the model can avoid modular arithmetic by making use of its earlier outputs... This mechanism works regardless of the cycle number."
  - [section 3.3]: "we can instead identify dequeue operations by examining earlier ELIZA outputs... This construction uses fewer attention heads and does not limit the size of the memory queue"
- Break condition: If the same reassembly rule appears multiple times in the list, or if memory queue operations are too numerous to embed in the output sequence.

### Mechanism 3
- Claim: Transformers show strong bias toward induction-head copying mechanisms rather than precise position-based copying.
- Mechanism: When copying segments from the input, models prefer content-based attention that matches n-gram prefixes rather than calculating exact positions based on decomposition group counts and generation step.
- Core assumption: Induction heads are easier to learn than position arithmetic mechanisms, even at the cost of reduced precision.
- Evidence anchors:
  - [abstract]: "models favor an induction head mechanism over a more precise, position-based copying mechanism"
  - [section 4.2]: "Models trained on the most repetitive data (α = 0.01) generalize poorly to higher values of α. The best-generalizing model is trained with an α = 0.1"
  - [section 4.2]: "we plot the difference between these scores for different n-gram windows, averaging over attention heads, with positive values indicating that the model assigns higher scores to content than position"
- Break condition: When the same n-gram appears multiple times in the copying segment, causing the induction head to select the wrong position.

## Foundational Learning

- Concept: Finite-state automata simulation with Transformers
  - Why needed here: ELIZA's pattern matching requires recognizing regular expressions, which can be implemented by simulating finite-state automata layer by layer in the Transformer.
  - Quick check question: How many Transformer layers are needed to match a template with L symbols, and what attention patterns are used at each layer?

- Concept: Intermediate output processing for state tracking
  - Why needed here: Long-term memory mechanisms like cycling and memory queues require tracking conversation state across turns, which can be implemented by processing the model's own previous outputs.
  - Quick check question: How does the model distinguish between enqueue and dequeue operations by examining its own previous responses?

- Concept: Content-based vs position-based copying mechanisms
  - Why needed here: Generating responses requires copying segments from the input, and the model must choose between induction-head content matching or precise position arithmetic.
  - Quick check question: Under what conditions does content-based copying fail, and how can you detect this failure in the model's attention patterns?

## Architecture Onboarding

- Component map: Input Segmentation -> Template Matching (Finite-State Automata Simulation) -> State Identification -> Response Generation -> Long-Term Memory Tracking (via Intermediate Outputs)
- Critical path: Input → Segmentation → Template Matching → State Identification → Response Generation, with long-term memory requiring access to previous outputs
- Design tradeoffs: Position-based copying is more precise but harder to learn and generalizes poorly to longer sequences; content-based copying is easier to learn but fails with repeated n-grams; intermediate output processing avoids complex automata but requires sufficient output length to embed state information.
- Failure signatures: Poor copying accuracy correlated with segment length and n-gram repetition; decreased accuracy on null inputs with more memory operations; preference for content over position in attention patterns.
- First 3 experiments:
  1. Train on datasets with varying n-gram repetition (α parameters) and test generalization to evaluate copying mechanism learned.
  2. Conduct counterfactual experiments by editing intermediate outputs to test whether model uses intermediate outputs for state tracking.
  3. Measure attention patterns to compare content-based vs position-based copying preferences across different training distributions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Transformer models actually implement the ELIZA program mechanistically, and how closely do they align with the theoretical constructions proposed?
- Basis in paper: [explicit] The paper states that their constructions illustrate "some of the ways that Transformers could implement ELIZA" but acknowledges they "are not exhaustive" and "might not correspond to the solutions that Transformers actually learn."
- Why unresolved: The paper conducts empirical analysis showing which subtasks are harder to learn, but does not perform exhaustive mechanistic analysis using interpretability techniques like causal methods to fully understand how the mechanisms are encoded in model weights.
- What evidence would resolve it: Comprehensive mechanistic analysis using techniques like causal mediation analysis or attribution patching to map the actual mechanisms learned by models to the proposed symbolic constructions.

### Open Question 2
- Question: How does the data distribution affect which copying mechanism (induction head vs position-based) Transformers learn and generalize with?
- Basis in paper: [explicit] The paper finds that models trained on less repetitive data (α ≥ 1) prefer content-based copying but generalize poorly to repetitive data, while models trained on very repetitive data generalize poorly to non-repetitive data. Models trained on moderate repetition (α = 0.1) generalize best and show mixed preferences.
- Why unresolved: While the paper shows correlations between data properties and mechanism preference, it doesn't establish causal relationships or fully explain why certain distributions lead to different learning dynamics.
- What evidence would resolve it: Systematic experiments varying data properties while controlling for other factors, combined with analysis of attention patterns across training to track mechanism formation.

### Open Question 3
- Question: Can the ELIZA framework be extended to include more sophisticated conversational behaviors like semantic parsing, in-context learning, and explicit chain-of-thought reasoning?
- Basis in paper: [inferred] The paper discusses ELIZA as a "rich but controlled setting" and suggests future work could "gradually extend the rule-based chatbot framework to include more of the key phenomena observed in modern language models."
- Why unresolved: The paper focuses on ELIZA's specific mechanisms but doesn't explore how to systematically incorporate more complex behaviors while maintaining interpretability.
- What evidence would resolve it: Development of extended rule-based chatbot frameworks that incorporate these behaviors, with corresponding theoretical constructions showing how Transformers could implement them.

## Limitations

- Data generation complexity: The paper relies on synthetic ELIZA datasets with specific statistical properties that are not fully specified, creating uncertainty about faithful reproduction.
- Theoretical construction verification: The proposed mechanisms are plausible extensions of prior work but lack exhaustive proofs that they are the only or optimal solutions.
- Generalization claims: Empirical results are limited to the synthetic ELIZA domain, leaving unclear how findings extend to naturalistic dialogue or other rule-based systems.

## Confidence

**High confidence**: The core finding that Transformers struggle with precise copying and memory queue tracking is well-supported by empirical results across multiple experimental conditions. The observation that models favor induction-head mechanisms over position-based copying is consistently observed in attention pattern analysis.

**Medium confidence**: The theoretical constructions for implementing ELIZA's pattern matching using finite-state automata simulation are plausible extensions of prior work, but practical implementation details and their exact correspondence to learned mechanisms are not fully verified.

**Low confidence**: The claim that intermediate output processing is the primary mechanism for long-term memory tracking, while supported by results, is based on limited ablation experiments and could benefit from more direct intervention studies.

## Next Checks

1. **Counterfactual intervention study**: Modify the intermediate outputs in the model's attention pattern analysis to test whether the model genuinely relies on these outputs for state tracking. By editing or removing specific intermediate outputs and measuring the impact on accuracy, we can more directly verify the intermediate scratchpad hypothesis.

2. **Cross-architecture comparison**: Replicate the key experiments using different Transformer variants (e.g., with positional encodings, different attention patterns) to determine whether the observed limitations are inherent to the architecture or specific to the decoder-only GPT-2 style models used in the study.

3. **Naturalistic dialogue extension**: Apply the mechanistic analysis framework to a small-scale naturalistic dialogue dataset with explicit rules or templates to test whether the same copying and memory tracking limitations observed in ELIZA generalize to more realistic conversational settings.