---
ver: rpa2
title: An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal
  Models
arxiv_id: '2411.06048'
source_url: https://arxiv.org/abs/2411.06048
tags:
- reasoning
- spatial
- lmms
- bounding
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Spatial-MM, a new benchmark to evaluate
  the spatial reasoning capabilities of large multimodal models (LMMs). The benchmark
  includes two subsets: Spatial-Obj with 2,000 multiple-choice questions focusing
  on spatial relationships between objects, and Spatial-CoT with 310 multi-hop questions
  requiring chain-of-thought reasoning.'
---

# An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models

## Quick Facts
- arXiv ID: 2411.06048
- Source URL: https://arxiv.org/abs/2411.06048
- Authors: Fatemeh Shiri; Xiao-Yu Guo; Mona Golestan Far; Xin Yu; Gholamreza Haffari; Yuan-Fang Li
- Reference count: 12
- Key outcome: This paper introduces Spatial-MM, a new benchmark to evaluate the spatial reasoning capabilities of large multimodal models (LMMs).

## Executive Summary
This paper introduces Spatial-MM, a comprehensive benchmark for evaluating spatial reasoning capabilities of large multimodal models (LMMs). The benchmark includes two subsets: Spatial-Obj with 2,000 multiple-choice questions focusing on spatial relationships between objects, and Spatial-CoT with 310 multi-hop questions requiring chain-of-thought reasoning. Through extensive experiments on four top-performing LMMs (GPT-4 Vision, GPT-4o, Gemini 1.5 Pro, and MiniGPT-v2), the authors identify key limitations in current LMMs' spatial reasoning abilities, including significant performance gaps between object detection and complex spatial reasoning, challenges with human perspective questions, and the ineffectiveness of chain-of-thought prompting for multi-hop spatial tasks.

## Method Summary
The authors evaluate four LMMs on the Spatial-MM benchmark, which includes Spatial-Obj (2,000 multiple-choice questions) and Spatial-CoT (310 multi-hop questions). Models are tested with standard prompting, bounding box enrichment, and scene graph enrichment. For Spatial-CoT, the authors also evaluate reasoning path correctness using a scene graph format. The study uses GPT-4o as a judge for multi-hop questions and conducts perturbation analysis on the GQA-spatial dataset to examine model performance on different spatial relationship types. The evaluation includes comparisons between camera and human perspective questions, as well as analysis of how different visual information formats affect performance.

## Key Results
- Bounding boxes and scene graphs significantly improve LMMs' spatial reasoning, with scene graphs being particularly effective for complex two-object questions
- LMMs perform significantly worse on questions from human perspective compared to camera perspective
- Chain-of-thought prompting does not improve performance on complex multi-hop spatial questions
- LMMs are much stronger at basic object detection than complex spatial reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bounding boxes and scene graphs significantly improve LMMs' spatial reasoning by providing structured visual grounding.
- Mechanism: These symbolic visual information formats anchor object locations and relationships, reducing ambiguity in interpreting spatial relations from raw pixel data.
- Core assumption: LMMs can effectively integrate structured symbolic visual information when provided as additional context.
- Evidence anchors:
  - [abstract] "bounding boxes and scene graphs, even synthetic ones, can significantly enhance LMMs' spatial reasoning"
  - [section 5.4] "When given the synthesized bounding boxes information on GQA dataset, the LMMs improve their answer accuracy significantly, with the average increase over 'stan' of 33 points"
  - [corpus] Weak - no direct corpus evidence available
- Break condition: If LMMs cannot effectively parse or utilize the structured visual information format, or if the information provided is noisy or inconsistent with the actual image content.

### Mechanism 2
- Claim: LMMs perform better on questions from camera perspective than human perspective within images.
- Mechanism: Camera perspective questions align with how LMMs typically process visual information (external viewpoint), while human perspective requires understanding the scene from an embedded viewpoint, which is less common in training data.
- Core assumption: Training data for LMMs contains predominantly camera-perspective visual information.
- Evidence anchors:
  - [abstract] "LMMs struggle more with questions posed from the human perspective than the camera perspective about the image"
  - [section 5.5] "Though all these LMMs receive explicit prompt 'from human/camera's perspective', they all show significant performance drop on human's perspective, when compared to the camera's perspective"
  - [corpus] Weak - no direct corpus evidence available
- Break condition: If LMMs are specifically trained on human-perspective visual data or if they develop the capability to switch viewpoints effectively.

### Mechanism 3
- Claim: Chain of thought prompting does not improve performance on complex multi-hop spatial questions.
- Mechanism: CoT prompting may generate reasoning paths that are not aligned with the actual reasoning needed for spatial tasks, leading to incorrect intermediate steps even when the final answer might be correct by chance.
- Core assumption: The reasoning paths generated by CoT prompting for spatial tasks are not necessarily valid or useful.
- Evidence anchors:
  - [abstract] "chain of thought (CoT) prompting does not improve model performance on complex multi-hop questions involving spatial relations"
  - [section 5.6] "the conventional CoT prompting (Wei et al., 2022b) may not be as effective for complex VQA tasks as it is for NLP tasks"
  - [corpus] Weak - no direct corpus evidence available
- Break condition: If CoT prompting is adapted specifically for spatial reasoning or if the reasoning paths are validated against ground truth spatial relationships.

## Foundational Learning

- Concept: Spatial relationships and prepositions
  - Why needed here: Understanding the 36 spatial relationships (right, left, attached to, touching, etc.) is fundamental to interpreting the benchmark questions and evaluating model performance.
  - Quick check question: What is the difference between "on" and "above" in spatial terms?

- Concept: Scene graphs and bounding boxes
  - Why needed here: These are the structured visual information formats used to enhance LMMs' spatial reasoning, so understanding their representation and purpose is crucial.
  - Quick check question: How is a bounding box typically represented (coordinates format)?

- Concept: Chain of thought prompting
  - Why needed here: CoT is a key technique being evaluated for its effectiveness on multi-hop spatial reasoning tasks.
  - Quick check question: What is the main difference between standard prompting and chain of thought prompting?

## Architecture Onboarding

- Component map: Image input -> LMM processing -> (optional) data enrichment (bounding boxes/scene graphs) -> answer generation -> evaluation (final answer accuracy and reasoning path correctness)
- Critical path: Image input → LMM processing → (optional) data enrichment (bounding boxes/scene graphs) → answer generation → evaluation (final answer accuracy and reasoning path correctness)
- Design tradeoffs: Using synthetic vs. ground truth bounding boxes/scene graphs (synthetic are more readily available but may be less accurate); focusing on camera vs. human perspective (camera is easier for LMMs but human is more challenging and realistic)
- Failure signatures: Poor performance on human perspective questions; no improvement with CoT prompting on multi-hop questions; significant gap between object detection and spatial reasoning abilities
- First 3 experiments:
  1. Test LMM performance on Spatial-Obj with and without bounding boxes to verify the improvement mechanism
  2. Compare LMM performance on human vs. camera perspective questions in Spatial-CoT to confirm the perspective difficulty finding
  3. Evaluate the correctness of reasoning paths generated by CoT prompting vs. standard prompting on Spatial-CoT to understand why CoT doesn't help

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of spatial relationships (e.g., orientation vs. positional vs. viewpoint) affect the performance of LMMs?
- Basis in paper: The paper mentions that they categorized spatial relationships into visual patterns like "object localization," "orientation and direction," "viewpoints," and "positional and relational context" using GPT-4o, and found these pose significant challenges for LMMs.
- Why unresolved: The paper identifies these patterns as challenging but doesn't provide a detailed analysis of how each type specifically impacts model performance or which types are most problematic.
- What evidence would resolve it: A comprehensive ablation study analyzing LMM performance on each visual pattern category separately, showing which types of spatial reasoning are most difficult for models.

### Open Question 2
- Question: Why do synthesized bounding boxes outperform ground truth bounding boxes in spatial reasoning tasks?
- Basis in paper: The paper explicitly states "Surprisingly, the LMMs perform better with synthesized bounding boxes than with ground-truth bounding boxes" but doesn't explain the underlying reason.
- Why unresolved: The authors note this unexpected finding but don't investigate the potential causes, such as differences in annotation quality, diversity, or how LMMs process different types of bounding box representations.
- What evidence would resolve it: Analysis comparing the characteristics of synthesized vs. ground truth bounding boxes (precision, recall, coverage) and how LMMs use this information during reasoning.

### Open Question 3
- Question: What specific aspects of human perspective questions make them significantly more challenging than camera perspective questions?
- Basis in paper: The paper finds that "LMMs struggle more with questions posed from the human perspective than the camera perspective" but doesn't explore the underlying cognitive or technical reasons.
- Why unresolved: While the performance gap is documented, the paper doesn't investigate whether this is due to perspective transformation complexity, viewpoint consistency issues, or other factors.
- What evidence would resolve it: Controlled experiments testing LMMs on progressively more complex perspective transformations, or analysis of reasoning paths to identify where perspective-based reasoning fails.

### Open Question 4
- Question: How can chain-of-thought prompting be modified to improve performance on multi-hop spatial reasoning tasks?
- Basis in paper: The paper finds that "chain of thought (CoT) prompting does not improve model performance on complex multi-hop questions involving spatial relations" and notes that "the rationales produced by the conventional CoT may not align well with the reasoning path needed to arrive at the answer."
- Why unresolved: The authors identify the ineffectiveness of standard CoT but don't propose or test alternative prompting strategies specifically designed for spatial reasoning.
- What evidence would resolve it: Development and evaluation of specialized spatial-aware CoT prompting techniques that incorporate visual grounding or perspective transformation steps.

### Open Question 5
- Question: What is the relationship between the number of reasoning hops and the accuracy of spatial vs. non-spatial reasoning steps?
- Basis in paper: The paper observes that "with the increase in the number of the hops, accuracy of the final answer drops" and finds that "91% of questions included at least one incorrect spatial reasoning step, while only 9.75% contained at least one incorrect Non-spatial reasoning step."
- Why unresolved: While the correlation between hop count and accuracy is documented, the paper doesn't explore whether spatial reasoning steps are disproportionately affected compared to non-spatial steps as complexity increases.
- What evidence would resolve it: Detailed analysis of reasoning step accuracy broken down by hop number and reasoning type, showing whether spatial reasoning degrades faster than non-spatial reasoning as task complexity increases.

## Limitations

- The benchmark uses a limited set of 36 spatial relationships and 8 spatial prepositions, which may not capture the full complexity of real-world spatial reasoning tasks
- The synthetic scene graphs and bounding boxes may not perfectly represent ground truth visual information, potentially overestimating the benefit of structured visual information
- The perspective analysis relies on explicit prompting rather than naturally occurring perspective differences in images

## Confidence

**High Confidence**: The finding that LMMs show significant performance gaps between object detection and spatial reasoning is well-supported by consistent results across multiple datasets and models. The effectiveness of scene graphs and bounding boxes in improving spatial reasoning is also highly reliable given the large performance differences observed.

**Medium Confidence**: The perspective analysis showing worse performance on human perspective questions has good empirical support but may be influenced by prompt engineering choices. The CoT prompting ineffectiveness finding is medium confidence because it only tests one specific CoT approach without exploring alternatives.

**Low Confidence**: The synthetic data generation process's impact on results is low confidence - we cannot be certain how closely synthetic scene graphs and bounding boxes match ground truth, which could affect the magnitude of reported improvements.

## Next Checks

1. **Ground Truth Validation**: Evaluate LMM performance using human-annotated ground truth scene graphs and bounding boxes rather than synthetic ones to verify that improvements aren't artifacts of the generation process.

2. **Real-World Dataset Testing**: Test the same LMMs on naturally occurring images with human perspective questions (rather than prompted perspective) from real-world datasets to validate the perspective difficulty findings.

3. **Alternative CoT Strategies**: Implement and test alternative chain-of-thought prompting approaches specifically designed for spatial reasoning (e.g., step-by-step visual grounding) to determine if the ineffectiveness is due to the specific CoT method used or a fundamental limitation of CoT for spatial tasks.