---
ver: rpa2
title: 'Attention Shift: Steering AI Away from Unsafe Content'
arxiv_id: '2410.04447'
source_url: https://arxiv.org/abs/2410.04447
tags:
- image
- diffusion
- unsafe
- prompts
- safe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a training-free attention reweighing method
  to restrict unsafe content generation in diffusion models, validated by LLM prompt
  sanitization. Experiments on Stable Diffusion v1.4 compare the approach against
  five ablation methods for removing "kids with guns" and "naked woman" concepts from
  both direct and adversarial jailbreak prompts.
---

# Attention Shift: Steering AI Away from Unsafe Content

## Quick Facts
- arXiv ID: 2410.04447
- Source URL: https://arxiv.org/abs/2410.04447
- Authors: Shivank Garg; Manyana Tiwari
- Reference count: 38
- One-line primary result: A training-free attention reweighing method removes unsafe concepts from Stable Diffusion v1.4 with competitive quality metrics

## Executive Summary
This paper introduces a novel training-free approach using attention reweighing to remove unsafe content from text-to-image diffusion models. The method dynamically adjusts cross-attention maps during inference by scaling safe concept token embeddings, validated through LLM prompt sanitization. Experiments demonstrate successful removal of "kids with guns" and "naked woman" concepts from both direct and adversarial jailbreak prompts while maintaining image quality comparable to baseline models.

## Method Summary
The approach combines LLM-based prompt sanitization with attention reweighing during the diffusion model's inference process. First, an LLM (Mistral-8x7B) validates and rewrites unsafe prompts into safe alternatives. Then, the safe token's embedding is scaled by a factor of 10 and used in the diffusion model's cross-attention mechanism. This training-free method avoids the computational cost of fine-tuning while achieving competitive performance in removing unsafe concepts, tested on Stable Diffusion v1.4 with both direct and adversarial prompts.

## Key Results
- Achieves 0/10 human evaluation scores for "naked woman" concepts across all prompt types
- Lowest or competitive CLIP scores and ImageReward metrics compared to five ablation methods
- Maintains FID scores comparable to baseline while removing unsafe content
- Successfully handles both direct and jailbreak prompts for tested concepts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention reweighing suppresses unsafe concepts by amplifying safe tokens in cross-attention maps
- Mechanism: After LLM validation replaces unsafe tokens with safe ones, the safe token's embedding is scaled (e.g., by 10) before being fed to the diffusion model, increasing its influence on the output
- Core assumption: Scaling the embedding vector of a safe token will proportionally increase its impact on the cross-attention weights and thus on the final image
- Break condition: If scaling a token's embedding does not translate to increased attention weight influence, the method fails to suppress unsafe content

### Mechanism 2
- Claim: LLM-based prompt sanitization prevents adversarial jailbreak prompts from reaching the diffusion model
- Mechanism: Unsafe prompts are detected and rewritten by the LLM before being processed by the diffusion model
- Core assumption: The LLM can accurately detect and rewrite unsafe prompts without generating new unsafe content or losing semantic coherence
- Break condition: If the LLM misclassifies prompts or rewrites them in a way that still produces unsafe content, the defense fails

### Mechanism 3
- Claim: Dynamic attention adjustment during inference preserves image quality while removing unsafe content
- Mechanism: Instead of fine-tuning or ablating concepts, attention weights are modified on-the-fly, preserving surrounding concept distributions
- Core assumption: Adjusting attention maps at inference time can selectively suppress target concepts without harming unrelated image features
- Break condition: If attention adjustments introduce artifacts or significantly degrade FID scores, the method fails to preserve image quality

## Foundational Learning

- Concept: Cross-attention mechanism in diffusion models
  - Why needed here: The method modifies cross-attention maps to suppress unsafe concepts; understanding how attention influences image generation is essential
  - Quick check question: In a text-to-image diffusion model, what role does the cross-attention layer play in mapping text tokens to image features?

- Concept: CLIP embeddings and semantic similarity
  - Why needed here: The paper references CLIP-based safety filters and CLIP scores as metrics; understanding how CLIP measures semantic similarity is necessary for interpreting results
  - Quick check question: How does CLIP compute similarity between a text prompt and an image, and why is this relevant for detecting unsafe content?

- Concept: Diffusion denoising process
  - Why needed here: The final safe image is generated via the standard diffusion denoising process after attention reweighing; knowing the denoising steps helps understand where and how the method intervenes
  - Quick check question: At which step in the diffusion denoising loop are attention maps typically computed, and how can they be modified without retraining?

## Architecture Onboarding

- Component map: Input Prompt → LLM Validator → Attention Reweigher → Diffusion UNet → Output Image
- Critical path: 1. Prompt → LLM validation, 2. Unsafe → Safe rewrite, 3. Safe token embedding scaling, 4. Cross-attention map modification, 5. Diffusion denoising with modified maps, 6. Output image + metrics
- Design tradeoffs: Training-free vs. fine-tuning (avoids retraining cost but relies on LLM accuracy), Attention scaling factor (higher values suppress more but risk artifacts), Multi-concept removal (scalable but requires careful token selection)
- Failure signatures: High CLIP scores for unsafe concepts (attention reweighing insufficient), Low ImageReward scores (image quality degradation), High FID scores (distribution shift from baseline), LLM misclassifications (unsafe content slips through)
- First 3 experiments: 1. Test attention scaling factor sweep (1x to 100x) on a single unsafe prompt; measure CLIP score drop and FID change, 2. Compare LLM-rewritten vs. original prompts for same scaling factor; check if rewrites alone are sufficient, 3. Run multi-concept removal on a prompt containing two unsafe concepts; verify simultaneous suppression without mutual interference

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on only two unsafe concepts with limited prompt variation, making generalization uncertain
- Method relies entirely on LLM accuracy for prompt sanitization - any misclassification could allow unsafe content
- Scaling factor of 10 is chosen empirically without systematic exploration of optimal values
- Performance against sophisticated adversarial prompts beyond tested jailbreak examples remains unknown

## Confidence

**High confidence**: The method successfully removes "kids with guns" concepts from both direct and jailbreak prompts, achieving 0/10 scores in human evaluations. The attention reweighing approach works as intended for the specific concepts tested.

**Medium confidence**: The approach generalizes to "naked woman" concepts and maintains reasonable image quality. The training-free nature of the method is confirmed. The method's scalability for multi-concept removal is plausible.

**Low confidence**: Claims about effectiveness against sophisticated adversarial attacks beyond tested jailbreak prompts. The assertion that attention reweighing preserves image quality across all content types. The method's performance on concepts not tested in the evaluation.

## Next Checks

1. **Scaling factor sensitivity analysis**: Systematically test attention scaling factors from 1x to 100x on the same prompts to identify the optimal balance between safety and image quality. Measure CLIP score reduction, FID score change, and qualitative image quality degradation at each factor level.

2. **Adversarial prompt robustness test**: Generate a diverse set of sophisticated adversarial prompts designed to evade LLM detection and evaluate whether the method still successfully removes unsafe content across these variations.

3. **Cross-concept generalization test**: Apply the same method to a broader range of unsafe concepts including violence, hate speech, and misinformation. Use the same evaluation framework to determine if the approach generalizes beyond the two tested concepts.