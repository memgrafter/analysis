---
ver: rpa2
title: Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized
  Rehearsal
arxiv_id: '2403.01244'
source_url: https://arxiv.org/abs/2403.01244
tags:
- uni00000013
- data
- uni00000036
- synthetic
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles catastrophic forgetting in large language models
  (LLMs) during continual learning, where models lose previously acquired knowledge
  when learning new tasks. The proposed Self-Synthesized Rehearsal (SSR) framework
  addresses this by generating synthetic training data for rehearsal instead of relying
  on real data from previous tasks.
---

# Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal

## Quick Facts
- **arXiv ID**: 2403.01244
- **Source URL**: https://arxiv.org/abs/2403.01244
- **Reference count**: 15
- **Key outcome**: SSR outperforms conventional rehearsal methods with 1% data utilization, achieving superior or comparable performance on SuperNI tasks while preserving generalization on AlpacaEval and MMLU benchmarks.

## Executive Summary
This paper addresses catastrophic forgetting in large language models during continual learning by proposing Self-Synthesized Rehearsal (SSR). The framework generates synthetic training data for rehearsal instead of relying on real data from previous tasks. SSR uses in-context learning with the base LLM to generate synthetic instances, refines these using the latest LLM to preserve acquired knowledge, and selects diverse high-quality synthetic instances for rehearsal. Experiments on SuperNI tasks demonstrate that SSR achieves superior or comparable performance to conventional rehearsal-based methods while using only 1% of the data, and successfully preserves generalization capabilities on external benchmarks.

## Method Summary
SSR is a rehearsal-based continual learning framework that mitigates catastrophic forgetting by generating synthetic data instead of storing real data from previous tasks. The method employs a three-step process: first, it uses the base LLM's in-context learning capability to generate synthetic instances from demonstrations; second, it refines these synthetic outputs using the latest LLM to preserve current knowledge; third, it clusters the synthetic instances using K-means and selects diverse high-quality instances near cluster centroids for rehearsal. This approach achieves data efficiency while maintaining task diversity and preventing overfitting to the latest task.

## Key Results
- SSR outperforms conventional rehearsal-based methods on SuperNI tasks with only 1% data utilization
- SSR preserves generalization capabilities, showing comparable or better performance on AlpacaEval and MMLU benchmarks
- The method demonstrates superior or comparable performance while maintaining robustness across different task orders

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data generation via in-context learning with base LLM preserves task diversity and avoids overfitting to latest task.
- Mechanism: Base LLM is used for in-context learning to generate synthetic instances from demonstrations, which are then refined by the latest LLM to preserve current knowledge. This two-stage generation ensures diversity while maintaining task relevance.
- Core assumption: Base LLM's in-context learning capability is preserved better than the latest LLM's after fine-tuning, and synthetic data distribution approximates real data.
- Evidence anchors:
  - [abstract]: "we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs"
  - [section 4]: "we use the base LLM θ(0) rather than the latest LLM θ(t−1) to conduct ICL. This is because the ICL ability of LLMs tends to exhibit a significant degradation after supervised fine-tuning"
  - [corpus]: No direct evidence; this is an assumption based on paper claims
- Break condition: If base LLM's ICL capability degrades significantly or synthetic data distribution diverges too much from real data, diversity and relevance would be lost.

### Mechanism 2
- Claim: K-means clustering of synthetic instances ensures rehearsal data diversity and quality.
- Mechanism: After synthetic instance refinement, K-means clustering groups instances, and instances near cluster centroids are selected for rehearsal, ensuring coverage of the solution space.
- Core assumption: K-means clustering effectively captures the underlying distribution of synthetic instances and selecting near centroids maintains diversity.
- Evidence anchors:
  - [abstract]: "we select diverse high-quality synthetic instances for rehearsal in future stages"
  - [section 4]: "we first adopt a clustering algorithm (e.g. K-means) to group {(ˆx, ¯y)} into C clusters. Then we calculate the distance between each synthetic instance and its corresponding cluster centroid, and finally select a certain amount of synthetic instances near cluster centroids"
  - [corpus]: No direct evidence; this is an assumption based on paper claims
- Break condition: If clustering fails to capture meaningful structure in synthetic data or selected instances don't represent task diversity, rehearsal effectiveness would degrade.

### Mechanism 3
- Claim: Synthetic output refinement by latest LLM preserves current knowledge in rehearsal data.
- Mechanism: Latest LLM refines synthetic instance outputs, ensuring rehearsal data contains current model's understanding while maintaining task structure from base LLM generation.
- Core assumption: Latest LLM can effectively refine base LLM outputs without introducing significant noise or drift.
- Evidence anchors:
  - [abstract]: "we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability"
  - [section 4]: "we use the latest LLM θ(t−1) to refine the output of each synthetic instance: ¯y = LLM(ˆx; θ(t−1))"
  - [corpus]: No direct evidence; this is an assumption based on paper claims
- Break condition: If refinement introduces significant noise or drift, rehearsal data quality would degrade and hurt performance.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: This is the core problem SSR addresses - LLMs losing previously learned knowledge when fine-tuned on new tasks
  - Quick check question: What happens to a neural network's performance on previous tasks when trained sequentially on new tasks without any mitigation strategy?

- Concept: Continual learning and rehearsal-based methods
  - Why needed here: SSR is a rehearsal-based continual learning method that maintains knowledge by rehearsing on synthetic data instead of real data
  - Quick check question: How do rehearsal-based continual learning methods typically prevent catastrophic forgetting?

- Concept: In-context learning capabilities of LLMs
  - Why needed here: SSR relies on base LLM's in-context learning to generate synthetic instances, which is a key differentiator from other approaches
  - Quick check question: What is in-context learning and how does it differ from traditional fine-tuning in LLMs?

## Architecture Onboarding

- Component map: Base LLM (θ(0)) -> In-context learning -> Synthetic instance generation -> Latest LLM (θ(t-1)) -> Output refinement -> K-means clustering -> Instance selection -> Rehearsal data -> Fine-tuning

- Critical path:
  1. Generate synthetic instances with base LLM using demonstrations
  2. Refine synthetic outputs with latest LLM
  3. Cluster synthetic instances and select diverse high-quality ones
  4. Fine-tune latest LLM on current data + selected synthetic instances
  5. Repeat for each new task

- Design tradeoffs:
  - Base LLM vs latest LLM for ICL: Base LLM preserves ICL capability but may lack current knowledge
  - Synthetic data quantity vs quality: More synthetic data provides better coverage but may introduce noise
  - K-means clusters vs random selection: Clustering ensures diversity but adds computational overhead
  - Demonstration quality: High-quality demonstrations improve synthetic data but require careful curation

- Failure signatures:
  - Degrading performance on previous tasks indicates forgetting
  - High variance in results across different task orders suggests instability
  - Synthetic data quality issues manifest as poor performance despite rehearsal
  - Clustering failures show up as suboptimal synthetic data selection

- First 3 experiments:
  1. Implement synthetic instance generation with base LLM using simple demonstrations, verify output quality
  2. Add refinement step with latest LLM, measure improvement in synthetic data quality
  3. Implement K-means clustering and selection, test diversity of selected instances

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of demonstration data affect the quality and diversity of synthetic instances in SSR, and what are the optimal strategies for selecting demonstrations?
- Basis in paper: [explicit] The paper mentions that demonstrations can be collected from previous data or manually constructed, but does not provide a detailed analysis of the impact of demonstration selection strategies.
- Why unresolved: The paper does not explore the effects of different demonstration selection strategies on the quality and diversity of synthetic instances, leaving a gap in understanding how to optimize this aspect of SSR.
- What evidence would resolve it: Experiments comparing different demonstration selection strategies (e.g., random sampling, clustering, manual construction) and their impact on synthetic instance quality and diversity would provide insights into optimal strategies.

### Open Question 2
- Question: How does SSR perform on longer sequences of tasks beyond the 10 tasks tested, and what are the scalability limits of the framework?
- Basis in paper: [inferred] The paper tests SSR on 10 tasks but does not explore its performance on longer sequences, raising questions about scalability and long-term effectiveness.
- Why unresolved: The paper does not provide data on SSR's performance with task sequences longer than 10, making it unclear how well the framework scales to more complex continual learning scenarios.
- What evidence would resolve it: Experiments with task sequences longer than 10 and analysis of performance metrics (e.g., AR, FWT, BWT) would help determine SSR's scalability and limitations.

### Open Question 3
- Question: What are the potential risks of using synthetic data generated by LLMs, such as the introduction of biases or unsafe content, and how can these risks be mitigated?
- Basis in paper: [explicit] The paper mentions that synthetic instances may contain unsafe content due to data bias during training, but does not discuss mitigation strategies.
- Why unresolved: The paper acknowledges the risk of bias and unsafe content in synthetic data but does not explore methods to address these issues, leaving a gap in understanding how to ensure the safety and reliability of SSR-generated data.
- What evidence would resolve it: Studies on the prevalence of biases and unsafe content in SSR-generated synthetic data, along with experiments testing mitigation strategies (e.g., content filtering, bias correction), would provide insights into ensuring data safety and reliability.

## Limitations
- The paper lacks empirical validation of the core assumption that base LLM's in-context learning capability degrades less than the latest LLM after fine-tuning.
- No ablation studies demonstrate the relative importance of each SSR component (base LLM generation, latest LLM refinement, K-means clustering).
- The experiments only evaluate on 10 SuperNI tasks and two generalization benchmarks, limiting understanding of scalability and robustness.

## Confidence

- **High confidence**: Effectiveness of synthetic rehearsal as a data-efficient approach to catastrophic forgetting
- **Medium confidence**: Specific two-stage generation mechanism (base LLM for ICL + latest LLM for refinement)
- **Low confidence**: Scalability and generalizability of SSR to diverse real-world continual learning scenarios

## Next Checks

1. Conduct ablation studies isolating each SSR component (ICL with base LLM only, refinement only, clustering only) to quantify individual contributions and validate the necessity of the two-stage generation approach.

2. Test SSR across a broader range of task types including natural language inference, coreference resolution, and multi-hop reasoning to evaluate domain robustness and identify failure modes.

3. Implement stress tests with task orderings that specifically challenge forgetting (e.g., alternating between dissimilar tasks, repeating similar tasks) to verify SSR's effectiveness under worst-case forgetting scenarios.