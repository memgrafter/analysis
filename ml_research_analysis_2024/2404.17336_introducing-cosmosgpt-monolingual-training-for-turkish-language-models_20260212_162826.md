---
ver: rpa2
title: 'Introducing cosmosGPT: Monolingual Training for Turkish Language Models'
arxiv_id: '2404.17336'
source_url: https://arxiv.org/abs/2404.17336
tags:
- language
- turkish
- dataset
- datasets
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces cosmosGPT, a set of Turkish-only language
  models (355M and 774M parameters) trained from scratch on Turkish corpora. The authors
  create new fine-tuning and evaluation datasets for Turkish instruction-following
  tasks and compare cosmosGPT to much larger multilingual models.
---

# Introducing cosmosGPT: Monolingual Training for Turkish Language Models

## Quick Facts
- arXiv ID: 2404.17336
- Source URL: https://arxiv.org/abs/2404.17336
- Reference count: 24
- Primary result: Turkish-only models (355M/774M params) achieve competitive performance vs 10x larger multilingual models

## Executive Summary
This paper introduces cosmosGPT, a family of Turkish language models (355M and 774M parameters) trained from scratch on monolingual Turkish data. The authors create new fine-tuning and evaluation datasets for Turkish instruction-following tasks and compare cosmosGPT to much larger multilingual models. Using human evaluation and automatic metrics, cosmosGPT models achieve performance close to models 10x their size, ranking competitively with ELO scores of 919-1069 versus 1107-1229 for 7B-parameter models. The models excel in general Turkish text generation but lag in logic and math tasks.

## Method Summary
The researchers trained GPT-2 models from scratch on Turkish-only data using a 275GB corpus (250GB CulturaX + 25GB custom). They created specialized instruction-following datasets (M: 51K, B: 67K, H: 16K, g: 5K) and fine-tuned the base models. Evaluation used both automatic metrics (ROUGE) and human ELO scoring. The training pipeline involved data preparation, tokenizer training, base model pre-training on TPUv3-8 clusters, fine-tuning with instruction datasets, and comprehensive evaluation.

## Key Results
- CosmosGPT Medium and Large models trained exclusively on Turkish achieve competitive performance vs 7B-parameter multilingual models
- Human evaluation shows cosmosGPT models rank closely with much larger models (ELO scores: 919-1069 vs 1107-1229)
- CosmosGPT excels in general Turkish text generation but shows relative weakness in logic and math tasks
- Monolingual training enables Turkish-specific nuance capture that multilingual models miss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Monolingual training on Turkish corpus improves language-specific nuance capture compared to multilingual models.
- Mechanism: By focusing exclusively on Turkish data, the model avoids dilution of language-specific features that occurs when training on multiple languages.
- Core assumption: Turkish language patterns are distinct enough from other languages that they benefit from dedicated modeling without interference.
- Evidence anchors: Abstract mentions "trained exclusively on Turkish, a relatively low-resource language" and models "surpassed larger-sized multilingual models."

### Mechanism 2
- Claim: Careful dataset curation and cleaning improves model performance more than dataset size alone.
- Mechanism: Using CulturaX, a meticulously cleaned dataset, provides higher quality training data that allows the model to learn more accurate language representations.
- Core assumption: Quality of training data has greater impact on model performance than quantity when dataset is sufficiently large.
- Evidence anchors: Section notes "opted for the meticulously cleaned and deduplicated CulturaX dataset" contrasting with "noisy data derived from general web sources."

### Mechanism 3
- Claim: Specialized instruction-following datasets tailored to Turkish improve model performance on task-oriented queries.
- Mechanism: Custom fine-tuning datasets that include Turkish-specific instructions and human-generated responses help the model better understand and execute user requests in Turkish.
- Core assumption: Turkish language tasks and user expectations differ sufficiently from other languages that dedicated datasets are necessary.
- Evidence anchors: Section states "New fine-tuning and evaluation datasets have been developed to enhance the models' adaptability to various instruction execution tasks in Turkish."

## Foundational Learning

- Concept: Tokenizer efficiency and vocabulary design
  - Why needed here: Turkish has agglutinative morphology with many suffixes, requiring careful vocabulary design to handle word formation efficiently
  - Quick check question: How does Turkish morphology affect tokenization strategy compared to English?

- Concept: Multilingual vs monolingual model architecture tradeoffs
  - Why needed here: Understanding when monolingual models outperform multilingual ones is crucial for resource allocation decisions
  - Quick check question: What are the key architectural differences between training a model on one language vs 100+ languages?

- Concept: Instruction tuning and preference optimization
  - Why needed here: The paper shows instruction-tuned models performing better, suggesting this is a critical capability for practical deployment
  - Quick check question: How does instruction tuning differ from standard language model pre-training?

## Architecture Onboarding

- Component map: Data preparation pipeline -> Tokenizer training -> Base model pre-training -> Fine-tuning with instruction datasets -> Evaluation and comparison
- Critical path: Data preparation → Tokenizer training → Base model pre-training → Fine-tuning with instruction datasets → Evaluation and comparison
- Design tradeoffs: Smaller monolingual models vs larger multilingual models; extensive data cleaning vs broader but noisier datasets; custom Turkish instruction datasets vs translated general instruction sets
- Failure signatures: Poor performance on Turkish-specific tasks while maintaining general language generation; overfitting to training corpus; inability to handle Turkish agglutinative morphology
- First 3 experiments:
  1. Compare tokenizer performance on Turkish morphology using different vocabulary sizes (25k vs 50k vs 100k)
  2. Test base model performance on Turkish-specific tasks before vs after fine-tuning with instruction datasets
  3. Evaluate model robustness by testing on Turkish dialects and informal language variants not well-represented in the training corpus

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of cosmosGPT models change when trained on Turkish corpora of different sizes and qualities, and what is the optimal balance between corpus size and quality?
- Basis in paper: [explicit] The paper discusses the use of CulturaX and additional datasets for training, noting the importance of data quality and size.
- Why unresolved: The study used a specific dataset size and quality but did not explore variations in these parameters to determine their impact on model performance.
- What evidence would resolve it: Systematic experiments varying corpus size and quality, followed by performance comparisons, would provide insights into the optimal balance for Turkish language models.

### Open Question 2
- Question: Can the inclusion of Direct Preference Optimization (DPO) or Reinforcement Learning further enhance the performance of cosmosGPT models in Turkish instruction-following tasks?
- Basis in paper: [explicit] The paper mentions future research plans to incorporate DPO and Reinforcement Learning to improve model performance.
- Why unresolved: These techniques were not applied in the current study, leaving their potential impact on the models unexplored.
- What evidence would resolve it: Implementing DPO or Reinforcement Learning in the training process and evaluating the resulting models against current benchmarks would clarify their effectiveness.

### Open Question 3
- Question: How do cosmosGPT models perform on specialized Turkish NLP tasks such as sentiment analysis, named entity recognition, and machine translation compared to existing models?
- Basis in paper: [inferred] The paper focuses on instruction-following capabilities and general text generation but does not delve into specific NLP tasks.
- Why unresolved: The evaluation datasets used do not cover specialized NLP tasks, and the paper does not compare cosmosGPT models on these tasks.
- What evidence would resolve it: Testing cosmosGPT models on established benchmarks for sentiment analysis, named entity recognition, and machine translation, and comparing results with existing models, would provide a comprehensive performance assessment.

## Limitations

- Data composition uncertainty: The exact composition and quality of the custom 25GB Turkish dataset remains unspecified, limiting assessment of result generalizability
- Evaluation scope limitations: Human evaluation methodology has potential limitations including relative rankings without absolute benchmarks and limited assessment of Turkish-specific linguistic challenges
- Statistical significance concerns: With only 400 and 1000 evaluation samples for validation and test sets respectively, statistical power to detect meaningful differences may be limited

## Confidence

**High confidence**: The core claim that monolingual Turkish models (355M and 774M parameters) can achieve competitive performance compared to much larger multilingual models (7B parameters) is well-supported by experimental results.

**Medium confidence**: The assertion that careful dataset curation (using CulturaX) significantly improves model performance is plausible but not definitively proven without direct comparisons to less curated datasets.

**Low confidence**: The claim that cosmosGPT "surpasses" larger multilingual models is somewhat overstated based on the data presented - ELO scores show competitive but not exceeding performance.

## Next Checks

**Validation Check 1**: Conduct a controlled experiment comparing cosmosGPT's performance using different dataset qualities - train identical model architectures on CulturaX vs a comparable-sized but less curated Turkish corpus to quantify the actual impact of dataset cleaning.

**Validation Check 2**: Expand evaluation methodology to include Turkish-specific linguistic challenges, particularly agglutinative morphology handling and dialect comprehension, to test whether monolingual training provides advantages beyond general language generation.

**Validation Check 3**: Perform ablation studies on the instruction-tuning datasets to determine which components (M, B, H, g) contribute most significantly to observed performance improvements, helping identify whether custom Turkish instruction datasets are truly necessary.