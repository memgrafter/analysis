---
ver: rpa2
title: 'NeuralDiffuser: Neuroscience-inspired Diffusion Guidance for fMRI Visual Reconstruction'
arxiv_id: '2402.13809'
source_url: https://arxiv.org/abs/2402.13809
tags:
- guidance
- visual
- image
- fmri
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reconstructing visual stimuli
  from functional Magnetic Resonance Imaging (fMRI) data with high fidelity to details.
  The authors propose NeuralDiffuser, a neuroscience-inspired diffusion guidance method
  that incorporates primary visual feature guidance to enhance the detail fidelity
  of reconstructed images.
---

# NeuralDiffuser: Neuroscience-inspired Diffusion Guidance for fMRI Visual Reconstruction

## Quick Facts
- arXiv ID: 2402.13809
- Source URL: https://arxiv.org/abs/2402.13809
- Reference count: 40
- Primary result: NeuralDiffuser achieves SSIM of 0.318 and two-way identification rate of 95.33% on AlexNet(2) for fMRI visual reconstruction

## Executive Summary
This paper addresses the challenge of reconstructing visual stimuli from functional Magnetic Resonance Imaging (fMRI) data with high fidelity to details. The authors propose NeuralDiffuser, a neuroscience-inspired diffusion guidance method that incorporates primary visual feature guidance to enhance the detail fidelity of reconstructed images. The method involves two key components: an fMRI decoder that aligns fMRI data with the latent space of Latent Diffusion Models (LDM), and a guidance strategy that integrates primary visual features into the diffusion process. The proposed method is evaluated on the Natural Senses Dataset (NSD) and demonstrates significant improvements in image reconstruction quality compared to baseline and state-of-the-art methods.

## Method Summary
NeuralDiffuser uses subject-specific voxel encoders to map voxels to a shared space, fMRI decoders to align with latent space of Latent Diffusion Models (LDM), and primary visual feature guidance to enhance detail fidelity. Training involves two stages: pre-training on other subjects and fine-tuning on target subject. The method employs CLIP text embeddings for semantic conditioning and CLIP image features for low-level visual guidance. Guidance hyperparameters include scale κ=300,000 and strength η=0.2 during the first 20% of reverse diffusion steps.

## Key Results
- Achieves SSIM of 0.318 and two-way identification rate of 95.33% on AlexNet(2)
- Demonstrates superior performance in brain correlation scores, indicating strong relationship between reconstructed images and brain activity
- Outperforms baseline and state-of-the-art methods on Natural Senses Dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Primary visual feature guidance improves detail fidelity by injecting low-level visual cues into the diffusion process.
- **Mechanism:** The method decodes hierarchical CLIP image features (layers 2, 4, 6, 8, 10, 12) from fMRI and uses them as a gradient-based guidance signal during early reverse diffusion steps.
- **Core assumption:** Low-level visual features can be reliably decoded from fMRI and meaningfully guide the diffusion process toward image details.
- **Break condition:** If decoded primary visual features are too noisy or misaligned with actual image details, the guidance will introduce artifacts rather than improve fidelity.

### Mechanism 2
- **Claim:** A two-stage training pipeline (pre-train on other subjects, then fine-tune on target subject) improves fMRI-to-embedding alignment.
- **Mechanism:** First, voxel encoders map subject-specific fMRI voxels into a shared space; then decoders are pre-trained across subjects to learn robust mappings to CLIP and VAE latent spaces; finally, target-subject-specific components are fine-tuned.
- **Core assumption:** Cross-subject fMRI patterns share enough structure to enable transfer learning, reducing the need for large per-subject datasets.
- **Break condition:** If subject-specific anatomical or functional differences are too large, pre-training may introduce bias and hurt fine-tuning performance.

### Mechanism 3
- **Claim:** CLIP text embeddings are used as high-level semantic conditions because they are cleaner and easier to decode than image embeddings.
- **Mechanism:** fMRI is mapped to CLIP text embedding space (77×768) instead of image embedding space (256×768), reducing modality mismatch and improving semantic coherence.
- **Core assumption:** Text embeddings represent pure high-level semantics without mixing in low-level visual details, making them easier to align with fMRI.
- **Break condition:** If fMRI contains latent visual semantics that text embeddings cannot capture, the semantic alignment will be incomplete.

## Foundational Learning

- **Concept: Diffusion Models and Score Matching**
  - Why needed here: The paper builds on latent diffusion models; understanding the reverse diffusion process and guidance is essential to grasp how primary visual features are injected.
  - Quick check question: In diffusion models, what role does the score function play during the reverse process?

- **Concept: fMRI Signal Processing and Voxel Encoding**
  - Why needed here: The method relies on mapping 3D voxel data to shared latent spaces; knowing how fMRI data is structured and preprocessed is critical.
  - Quick check question: Why is it necessary to flatten ROI voxels into vectors before encoding them?

- **Concept: Contrastive Learning and CLIP Embeddings**
  - Why needed here: The decoders use contrastive learning (MixCo, soft CLIP loss) to align fMRI with CLIP spaces; understanding this helps in debugging alignment quality.
  - Quick check question: What is the difference between hard and soft CLIP loss, and why does the paper switch to soft?

## Architecture Onboarding

- **Component map:** fMRI voxels -> Voxel Encoder -> Shared space -> fMRI Decoder -> CLIP text embeddings + CLIP image features -> Guided reverse diffusion -> Image

- **Critical path:**
  1. fMRI → voxel encoder → shared space
  2. Shared space → decoders → embeddings + guidance features
  3. Guidance features + embeddings → guided reverse diffusion → image

- **Design tradeoffs:**
  - Using CLIP text embeddings simplifies semantic decoding but loses direct visual grounding.
  - Primary visual feature guidance adds computational overhead and potential noise but improves detail fidelity.
  - Cross-subject pre-training improves generalization but may introduce subject-specific bias if not fine-tuned well.

- **Failure signatures:**
  - Blurry or inconsistent reconstructions → voxel encoder misalignment or weak guidance signal.
  - Distorted images with high guidance scale → over-amplification of noisy guidance features.
  - Poor semantic alignment → CLIP text decoder underfitting or modality gap too large.

- **First 3 experiments:**
  1. Test voxel encoder with synthetic fMRI-like data to verify shared space mapping works across subjects.
  2. Run decoder inference with and without guidance to confirm qualitative detail improvements.
  3. Perform ablation: set guidance scale κ = 0 and measure drop in SSIM and AlexNet(2) accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a universal voxel encoder be developed that effectively maps different subjects' voxel data into a shared space without requiring subject-specific fine-tuning?
- Basis in paper: The authors discuss the use of subject-specific voxel encoders to map different voxel sizes into a shared space, but note that individual-specific skull sizes lead to different numbers of voxels that cannot be fed into the model simultaneously.
- Why unresolved: Developing a universal encoder that can handle the variability in voxel sizes across subjects without the need for subject-specific fine-tuning remains a challenge due to the individual differences in brain anatomy.
- What evidence would resolve it: Demonstration of a voxel encoder that can accurately map voxel data from multiple subjects into a shared space, achieving comparable performance to subject-specific encoders without requiring fine-tuning for each subject.

### Open Question 2
- Question: How does the performance of primary visual feature guidance compare when using different types of guidance functions, such as brain encoding models or other computer vision tasks?
- Basis in paper: The authors propose a universal guidance method and demonstrate its effectiveness using brain encoding models as guidance functions, but suggest that alternative guidance functions are also acceptable.
- Why unresolved: While the authors show promising results with brain encoding models, the comparative performance of different guidance functions in terms of detail fidelity and semantic coherence remains unexplored.
- What evidence would resolve it: Comparative studies evaluating the performance of primary visual feature guidance using various guidance functions, such as brain encoding models, object detection, or semantic segmentation, and analyzing their impact on reconstruction quality.

### Open Question 3
- Question: What is the optimal balance between guidance scale and guidance strength for achieving the best trade-off between detail fidelity and image quality in visual reconstruction?
- Basis in paper: The authors discuss the trade-off between guidance scale and guidance strength, noting that increasing the guidance scale improves detail accuracy but may lead to a decline in overall image quality, as reflected in IS and FID metrics.
- Why unresolved: The optimal balance between guidance scale and guidance strength may vary depending on the specific task, dataset, and desired outcome, and finding the right balance remains an open question.
- What evidence would resolve it: Systematic experiments exploring the impact of different combinations of guidance scale and guidance strength on reconstruction quality, including both qualitative and quantitative evaluations, to identify the optimal balance for various scenarios.

## Limitations

- The effectiveness of cross-subject pre-training for fMRI-to-image alignment is assumed but not rigorously tested across different subject populations.
- The reliance on CLIP text embeddings for semantic conditioning may miss important visual semantics that are better captured by direct image embeddings.
- The quality of decoded primary visual features and their reliability for guidance is not extensively validated, despite being critical to the method's performance.

## Confidence

- High confidence: The overall architecture design and two-stage training pipeline are well-specified and implementable.
- Medium confidence: The claimed improvements in SSIM (0.318) and two-way identification rate (95.33%) are based on the NSD dataset, but the robustness across different datasets and subject populations is unclear.
- Low confidence: The effectiveness of primary visual feature guidance in improving detail fidelity is supported by qualitative claims but lacks quantitative ablation studies showing the specific contribution of this component.

## Next Checks

1. Perform an ablation study by removing the primary visual feature guidance component and measuring the drop in detail fidelity metrics (SSIM, AlexNet(2)) to quantify its specific contribution.
2. Test the method on multiple independent fMRI datasets with different acquisition protocols to assess generalization beyond the NSD dataset.
3. Conduct a controlled experiment comparing CLIP text embedding guidance versus CLIP image embedding guidance to validate the claim that text embeddings are easier to decode and provide better semantic alignment.