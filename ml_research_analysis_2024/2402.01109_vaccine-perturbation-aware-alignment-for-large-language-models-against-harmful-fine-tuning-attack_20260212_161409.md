---
ver: rpa2
title: 'Vaccine: Perturbation-aware Alignment for Large Language Models against Harmful
  Fine-tuning Attack'
arxiv_id: '2402.01109'
source_url: https://arxiv.org/abs/2402.01109
tags:
- fine-tuning
- alignment
- harmful
- arxiv
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Vaccine is a perturbation-aware alignment technique designed to
  mitigate the security risk of harmful fine-tuning attacks on large language models.
  It addresses the problem of a few harmful data uploaded by users tricking fine-tuning
  into producing alignment-broken models.
---

# Vaccine: Perturbation-aware Alignment for Large Language Models against Harmful Fine-tuning Attack

## Quick Facts
- arXiv ID: 2402.01109
- Source URL: https://arxiv.org/abs/2402.01109
- Authors: Tiansheng Huang; Sihao Hu; Ling Liu
- Reference count: 40
- Primary result: Reduces harmful scores by up to 9.8% while maintaining <2% accuracy loss on benign tasks

## Executive Summary
Vaccine addresses the security vulnerability of large language models (LLMs) to harmful fine-tuning attacks, where a few malicious data samples can break alignment and cause models to produce unsafe outputs. The approach introduces perturbation-aware alignment that creates invariant hidden embeddings resistant to harmful perturbations during subsequent fine-tuning. By adding bounded gradient-based perturbations during alignment, Vaccine forces the model to learn robust representations that withstand malicious data. Experimental results demonstrate significant reduction in harmful outputs while maintaining strong performance on benign tasks across multiple model architectures.

## Method Summary
Vaccine modifies the alignment stage by adding bounded gradient-based perturbations to hidden embeddings during training. For each training step, the method performs a dual forward-backward pass: first to compute the optimal perturbation that maximizes alignment loss, then to perform the actual update with this perturbation applied. This forces the model to learn representations robust to such changes. The method uses LoRA for efficient training and only requires modification of the alignment phase, making it computationally efficient compared to fine-tuning-stage defenses.

## Key Results
- Reduces harmful scores by up to 9.8% compared to standard supervised fine-tuning alignment
- Maintains negligible loss in fine-tune accuracy (up to 1.8%) on benign tasks
- Works across multiple model architectures including Llama2-7B, Opt-3.7B, and Vicuna-7B
- Only modifies the alignment stage, requiring minimal computational overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vaccine creates invariant hidden embeddings that resist harmful perturbation drift during fine-tuning.
- Mechanism: During alignment, Vaccine adds bounded gradient-based perturbations to each layer's hidden embeddings, forcing the model to learn representations robust to such changes.
- Core assumption: The drift in hidden embeddings is the main cause of alignment breakage during fine-tuning on harmful data.
- Evidence anchors:
  - [abstract] "produce invariant hidden embeddings by progressively adding crafted perturbation to them during the alignment phase"
  - [section 4.1] "find the optimal bounded perturbation on embedding that maximizes the alignment loss... then add the perturbation... to produce gradient that optimizes the model such that it can be robust to the perturbation"
- Break condition: If embedding drift is not the primary cause of alignment failure, or if perturbations exceed bounds that preserve task performance.

### Mechanism 2
- Claim: Vaccine reduces harmful score by up to 9.8% compared to standard alignment while maintaining fine-tune accuracy.
- Mechanism: By making embeddings robust to perturbation, Vaccine maintains alignment even when fine-tuning data contains harmful content, preventing the model from adopting harmful behaviors.
- Core assumption: The model can maintain alignment knowledge while being resistant to harmful perturbations introduced during fine-tuning.
- Evidence anchors:
  - [abstract] "Vaccine significantly reduces harmful scores (by up to 9.8% compared to standard alignment) while maintaining negligible loss in fine-tune accuracy (up to 1.8%)"
  - [section 5.2] "Vaccine significantly reduces the harmful score of the model (by up to 9.8% reduction compared to SFT)"
- Break condition: If the perturbation introduces excessive noise that degrades model performance on benign tasks.

### Mechanism 3
- Claim: Vaccine only modifies the alignment stage, requiring minimal computational overhead compared to fine-tuning-stage defenses.
- Mechanism: By applying perturbation-aware training once during alignment, Vaccine creates a robust model without needing to process user data or apply additional defenses during each fine-tuning request.
- Core assumption: Alignment only needs to be done once for all user fine-tuning requests, making the extra overhead during alignment acceptable.
- Evidence anchors:
  - [abstract] "only modifies the alignment stage. Vaccine finds the optimal bounded perturbation on embedding that maximizes the alignment loss"
  - [section 2] "Vaccine... only modifies the alignment stage with dual benefits: (i) small computation overhead (compared to solutions that require more computation for each fine-tuning request) and (ii) no assumption on accessing user data used for fine-tuning"
- Break condition: If alignment needs to be repeated frequently or if perturbation computation becomes prohibitively expensive for larger models.

## Foundational Learning

- Concept: Adversarial training and robustness
  - Why needed here: Vaccine uses adversarial-style perturbation to make embeddings robust to harmful data during fine-tuning.
  - Quick check question: How does adding adversarial perturbations during training improve model robustness to unseen attacks?

- Concept: Catastrophic forgetting and continual learning
  - Why needed here: Vaccine addresses the problem of models forgetting alignment knowledge when fine-tuned on harmful data, similar to catastrophic forgetting.
  - Quick check question: What are the main strategies in continual learning to prevent catastrophic forgetting, and how do they compare to Vaccine's approach?

- Concept: Embedding space and representation learning
  - Why needed here: Vaccine operates by perturbing hidden embeddings, requiring understanding of how embeddings encode information and how they can drift.
  - Quick check question: How do changes in hidden embeddings affect model behavior, and why is embedding drift particularly problematic for alignment?

## Architecture Onboarding

- Component map: Pre-trained model → Alignment with Vaccine (perturbation-aware) → Fine-tuning on user data (with or without harmful content) → Deployed model
- Critical path: The perturbation generation and application during alignment is the critical path that enables robustness during subsequent fine-tuning.
- Design tradeoffs: Vaccine trades increased alignment computation time and memory for improved robustness against harmful fine-tuning attacks.
- Failure signatures: If harmful score reduction is minimal or fine-tune accuracy drops significantly, the perturbation may be too aggressive or not properly calibrated.
- First 3 experiments:
  1. Run alignment with Vaccine on a small dataset and measure harmful score reduction compared to SFT baseline.
  2. Vary perturbation intensity (ρ) to find optimal balance between harmful score reduction and fine-tune accuracy.
  3. Test Vaccine on different fine-tuning sample sizes to understand scalability and robustness boundaries.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the perturbation-aware alignment technique generalize to RLHF-based alignment methods?
- Basis in paper: [explicit] The paper states that the proposed solution can potentially be extended to RLHF but focuses on supervised fine-tuning-based alignment.
- Why unresolved: The paper only evaluates the method on supervised fine-tuning and does not provide empirical evidence or theoretical justification for its effectiveness with RLHF.
- What evidence would resolve it: Experiments comparing the method's performance on RLHF-based alignment tasks versus supervised fine-tuning, along with theoretical analysis of how the perturbation-aware approach interacts with RLHF objectives.

### Open Question 2
- Question: What is the impact of harmful fine-tuning on the safety basin landscape of aligned models?
- Basis in paper: [inferred] The paper mentions recent research on safety basins but does not analyze how harmful fine-tuning affects the basin's structure or stability.
- Why unresolved: The paper identifies harmful embedding drift as a cause of alignment breakdown but does not explore how this relates to the broader concept of safety basins in model space.
- What evidence would resolve it: Visualization or analysis of safety basin changes before and after harmful fine-tuning, showing how the basin's shape or stability is affected.

### Open Question 3
- Question: How does the proposed method perform on conversational AI tasks compared to traditional classification tasks?
- Basis in paper: [inferred] The paper uses classification datasets (SST2, AGNEWS) for evaluation but acknowledges limitations in using these for conversational AI.
- Why unresolved: The paper's experiments focus on classification tasks, which may not fully represent the complexities of conversational AI alignment challenges.
- What evidence would resolve it: Experiments evaluating the method on conversational datasets, comparing performance on dialogue-based tasks versus classification tasks.

## Limitations

- The core claim that embedding invariance is the primary mechanism for effectiveness is not directly validated through causal analysis.
- Experimental results show relatively modest harmful score reductions (9.8%) and are limited to three model architectures.
- The method's effectiveness against sophisticated harmful fine-tuning attacks (beyond simple prompt injection) is not explored.
- Computational overhead of dual forward-backward passes could become prohibitive for larger models.

## Confidence

**High Confidence**: The claim that Vaccine modifies only the alignment stage and not the fine-tuning stage is well-supported by the methodology description and experimental setup.

**Medium Confidence**: The claim of 9.8% harmful score reduction is supported by reported experimental results, but the limited scope of evaluation reduces confidence in generalizability.

**Low Confidence**: The core mechanistic claim that creating "invariant hidden embeddings" is the primary reason for Vaccine's effectiveness is not directly validated.

## Next Checks

1. **Causal Mechanism Validation**: Design an ablation study that measures embedding drift separately from harmful score changes. Use techniques like representation similarity analysis to quantify how much embeddings actually change during fine-tuning with and without Vaccine, and correlate these changes with harmful score outcomes.

2. **Attack Robustness Scaling**: Test Vaccine against increasingly sophisticated harmful fine-tuning attacks beyond simple harmful prompt injection. Include adversarial data curation, multi-stage poisoning, and adaptive attacks that specifically target the perturbation mechanism.

3. **Broader Alignment Capability Assessment**: Evaluate Vaccine's impact on non-safety aspects of alignment, including truthfulness, helpfulness, and nuanced instruction following. Use established benchmarks like TruthfulQA, HELM, or AlpacaEval's full suite to ensure the perturbation-based defense doesn't create blind spots in other alignment dimensions.