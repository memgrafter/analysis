---
ver: rpa2
title: 'MlingConf: A Comprehensive Study of Multilingual Confidence Estimation on
  Large Language Models'
arxiv_id: '2402.13606'
source_url: https://arxiv.org/abs/2402.13606
tags:
- confidence
- language
- 'true'
- languages
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a comprehensive study of multilingual confidence
  estimation for Large Language Models (LLMs) across both language-agnostic and language-specific
  tasks. The authors construct a high-quality multilingual benchmark called MlingConf,
  including datasets for four language-agnostic tasks (TriviaQA, GSM8K, CommonsenseQA,
  SciQ) and one language-specific task (LSQA) in five languages: English, Japanese,
  Chinese, French, and Thai.'
---

# MlingConf: A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models

## Quick Facts
- arXiv ID: 2402.13606
- Source URL: https://arxiv.org/abs/2402.13606
- Authors: Boyang Xue; Hongru Wang; Rui Wang; Sheng Wang; Zezhong Wang; Yiming Du; Bin Liang; Wenxuan Zhang; Kam-Fai Wong
- Reference count: 32
- Primary result: Native-Tone Prompting strategy significantly improves multilingual confidence estimation by aligning response language with query context

## Executive Summary
This paper introduces MlingConf, a comprehensive study of multilingual confidence estimation for Large Language Models across language-agnostic and language-specific tasks. The authors construct a high-quality multilingual benchmark spanning five languages (English, Japanese, Chinese, French, Thai) and four language-agnostic tasks plus one language-specific task. Through systematic experiments on three major confidence estimation methods, they reveal that English exhibits linguistic dominance in confidence estimation performance on language-agnostic tasks, while on language-specific tasks, using the language related to the question context yields the best performance. Based on these findings, the authors propose a Native-Tone Prompting strategy that significantly improves both reliability and accuracy by aligning the response language with the linguistic context of the query.

## Method Summary
The study constructs the MlingConf benchmark by translating four language-agnostic datasets (TriviaQA, GSM8K, CommonsenseQA, SciQ) into four non-English languages using GPT-4 with consistency checks and human evaluation. Three confidence estimation methods are evaluated: probability-based (joint token probability normalized by sequence length), p(True)-based (verbalized true/false predictions), and self-verbalized (explicit confidence scores). The Native-Tone Prompting strategy is implemented for language-specific tasks by identifying the language context and using query-related languages for responses. Experiments are conducted on three LLMs (GPT-3.5, Llama-3.1, Llama-2, Vicuna) using accuracy (PREM), AUROC, and ECE metrics.

## Key Results
- English shows consistent linguistic dominance in confidence estimation performance on language-agnostic tasks across all three confidence estimation methods
- On language-specific tasks, using the question-related language yields the highest accuracy and confidence estimation performance
- Native-Tone Prompting strategy achieves better confidence estimation results than using any single language prompt
- Probability-based methods perform better on models with weaker instruction-following but stronger token probability calibration (Llama-3.1)
- Prompt-based methods (p(True) and verbalized) outperform probability-based methods on models with strong instruction-following abilities (GPT-3.5)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-specific prompting significantly improves confidence estimation reliability on language-specific tasks by leveraging linguistic dominance effects.
- Mechanism: When questions pertain to specific social, cultural, or geographical contexts tied to a language, prompting in that language taps into richer and more confident representations in the model's training data. This "native-tone" alignment increases both accuracy and calibration quality (lower ECE).
- Core assumption: The pre-training corpus contains richer, more contextually grounded data for language-specific domains in their native language than in cross-lingual translations.
- Evidence anchors:
  - [abstract] "on LS tasks, using question-related language to prompt LLMs demonstrates better linguistic dominance in multilingual confidence estimations"
  - [section] "Results on the LS task reveal a pronounced phenomenon of language dominance, indicating that, for questions related to specific linguistic contexts, utilizing the respective languages yields the highest accuracy and confidence estimation performance"
  - [corpus] Strong similarity with UAlign and BIRD papers suggesting linguistic dominance and calibration are key concerns in LLM reliability studies.
- Break condition: If the model's training data does not encode richer or more confident representations for the target language's domain, or if the domain is truly language-agnostic, the effect may not hold.

### Mechanism 2
- Claim: Probability-based confidence estimation methods perform better on models with weaker instruction-following abilities and stronger token probability calibration.
- Mechanism: Probability-based methods calculate joint token probabilities normalized by sequence length, leveraging the model's inherent calibration of token likelihoods. Models like Llama-3.1, which are not as strong in instruction-following, still maintain well-calibrated token probabilities during pre-training, making probability-based confidence estimation more reliable.
- Core assumption: Models with weaker instruction-following but better token-level probability calibration will yield more stable and calibrated confidence scores when using probability-based methods.
- Evidence anchors:
  - [abstract] "Experiments conducted on MlingConf datasets present valuable findings about confidence estimation uses in multilingual scenarios, language dominance effects of English on LA tasks"
  - [section] "In contrast, Llama-3.1 cannot stably generate verbalized confidence scores and perform relatively weak instruction-following abilities, but maintain well-calibrated likelihood probabilities during the pre-training stage for all languages"
  - [corpus] Position paper on diagnostic uncertainty notes that next-word probability is not always a reliable pre-test probability, indicating limitations of probability-based methods in some domains.
- Break condition: If the model's token probability calibration degrades or if the task requires stronger instruction-following for accurate confidence estimation, probability-based methods may underperform.

### Mechanism 3
- Claim: Prompt-based confidence estimation methods (p(True) and verbalized) outperform probability-based methods on models with strong instruction-following abilities in multilingual tasks.
- Mechanism: Models like GPT-3.5 with strong instruction-following can be prompted to self-evaluate or verbally express confidence, leveraging their ability to follow complex instructions across languages. This allows for more nuanced confidence estimation than simple token probability aggregation.
- Core assumption: Models with strong instruction-following abilities can reliably follow prompts to self-evaluate or verbally express confidence across multiple languages.
- Evidence anchors:
  - [abstract] "Experiments conducted on MlingConf datasets present valuable findings about confidence estimation uses in multilingual scenarios"
  - [section] "On GPT-3.5, both p(True) and Verb. yield the superior performances than Prob. across all languages averaged on four datasets"
  - [corpus] No direct corpus evidence for this mechanism; assumption based on model capability descriptions.
- Break condition: If the model's instruction-following ability degrades or if prompts are not well-crafted for the target language, prompt-based methods may fail to elicit reliable confidence estimates.

## Foundational Learning

- Concept: Calibration of confidence scores
  - Why needed here: Understanding how well a model's predicted confidence matches its actual accuracy is crucial for assessing reliability, especially in multilingual contexts where calibration may vary by language.
  - Quick check question: What does an Expected Calibration Error (ECE) of 0 indicate about a model's confidence predictions?

- Concept: Language dominance in pre-training data
  - Why needed here: Recognizing that different languages may have varying amounts of training data and domain coverage is essential for interpreting confidence estimation results across languages.
  - Quick check question: How might the quantity and quality of pre-training data for a language affect its performance on language-specific tasks?

- Concept: Prompt engineering for confidence estimation
  - Why needed here: Crafting effective prompts for self-evaluation or verbalized confidence expression is key to leveraging prompt-based confidence estimation methods.
  - Quick check question: What are the key elements of a well-designed prompt for eliciting confidence scores from a language model?

## Architecture Onboarding

- Component map:
  - Data Preparation -> Model Selection -> Confidence Estimation Methods -> Evaluation Metrics -> Analysis Pipeline
  - Data Preparation: MlingConf dataset construction (translation, consistency checks, human evaluation)
  - Model Selection: GPT-3.5, Llama-3.1, Llama-2, Vicuna for multilingual experiments
  - Confidence Estimation Methods: Probability-based, p(True)-based, Self-verbalized, Sampling-based, CoT-enhanced
  - Evaluation Metrics: Accuracy (PREM), AUROC, ECE
  - Analysis Pipeline: Language-specific vs. language-agnostic task comparison, linguistic dominance analysis, NTP strategy implementation

- Critical path:
  1. Construct or obtain high-quality multilingual datasets
  2. Select appropriate LLM and confidence estimation method
  3. Generate predictions and confidence scores
  4. Evaluate using accuracy, AUROC, and ECE metrics
  5. Analyze results for language dominance effects
  6. Implement and test NTP strategy if applicable

- Design tradeoffs:
  - Cost vs. Coverage: Using GPT-4 for translation and expert evaluation ensures quality but limits the number of languages due to expense
  - Model Capability vs. Method Suitability: Choosing between probability-based and prompt-based methods depends on the model's instruction-following and calibration strengths
  - Granularity vs. Generalization: Language-specific prompts improve reliability on LS tasks but may not generalize to LA tasks

- Failure signatures:
  - High ECE across all languages: Indicates poor calibration, possibly due to model limitations or task difficulty
  - Large performance gaps between languages: Suggests language dominance effects or imbalanced training data
  - Inconsistent confidence estimates for semantically equivalent questions: May indicate prompt sensitivity or model instability

- First 3 experiments:
  1. Compare probability-based vs. prompt-based confidence estimation methods on a single language-agnostic task (e.g., TriviaQA in English) using GPT-3.5 and Llama-3.1
  2. Test the Native-Tone Prompting strategy on a language-specific task (e.g., LSQA Japanese subset) using Llama-3.1
  3. Evaluate the effect of paraphrasing questions on confidence estimation reliability across multiple languages using GPT-3.5 on SciQ

## Open Questions the Paper Calls Out
- How do confidence estimation methods perform across a broader range of low-resource languages beyond the five investigated?
- Can confidence estimation methods be developed that are invariant to language-specific pre-training corpus biases?
- What is the optimal balance between prompt-based and probability-based confidence estimation methods for multilingual scenarios?

## Limitations
- Limited to five languages due to translation costs, raising questions about generalizability to other language families and low-resource languages
- Focus on instruction-tuned models leaves uncertainty about whether findings extend to base models or different training paradigms
- Translation process, while involving human evaluation, may still introduce subtle biases affecting confidence estimation differently across languages

## Confidence
- High Confidence: Comparative performance of different confidence estimation methods across models
- Medium Confidence: Linguistic dominance effect and superiority of native-tone prompting on language-specific tasks
- Low Confidence: Native-Tone Prompting strategy's effectiveness beyond tested language-specific task

## Next Checks
1. Test linguistic dominance hypothesis on additional language pairs, particularly between languages from different families (e.g., adding Arabic or Hindi)
2. Evaluate confidence estimation performance across different model architectures (base vs. instruction-tuned, different training objectives)
3. Conduct experiments on additional language-specific domains beyond LSQA (e.g., medical, legal, or technical domains) to test native-tone prompting across various specialized contexts