---
ver: rpa2
title: On Faster Marginalization with Squared Circuits via Orthonormalization
arxiv_id: '2412.07883'
source_url: https://arxiv.org/abs/2412.07883
tags:
- layer
- algorithm
- product
- squared
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the inefficiency of computing marginal probabilities
  in squared probabilistic circuits, which are expressive distribution estimators
  that require quadratic complexity due to the squaring operation. The key insight
  is inspired by tensor network canonical forms: by parameterizing squared circuits
  with orthonormal functions at input layers and (semi-)unitary matrices at sum layers,
  the resulting squared circuit becomes already normalized (partition function Z =
  1).'
---

# On Faster Marginalization with Squared Circuits via Orthonormalization

## Quick Facts
- arXiv ID: 2412.07883
- Source URL: https://arxiv.org/abs/2412.07883
- Authors: Lorenzo Loconte; Antonio Vergari
- Reference count: 40
- Key outcome: This paper addresses the inefficiency of computing marginal probabilities in squared probabilistic circuits, which are expressive distribution estimators that require quadratic complexity due to the squaring operation.

## Executive Summary
This paper tackles the computational inefficiency of marginalization in squared probabilistic circuits by introducing orthonormal parameterization inspired by tensor network canonical forms. The key insight is that by structuring circuits with orthonormal functions at input layers and (semi-)unitary matrices at sum layers, the resulting squared circuit automatically encodes a normalized distribution, eliminating the need for explicit partition function computation. This allows for faster marginalization algorithms that selectively avoid squaring operations on layers that depend only on marginalized variables, improving complexity from O(LS²) to O(|ϕY|S + |ϕY,Z|S²).

## Method Summary
The authors propose a method to parameterize squared circuits with orthonormal functions at input layers and (semi-)unitary matrices at sum layers, inspired by tensor network canonical forms. This orthonormalization is achieved through QR decomposition applied recursively to convert arbitrary parameter matrices to (semi-)unitary form while preserving circuit function. They then devise an algorithm to compute marginals by selectively avoiding squaring operations on layers that depend only on marginalized variables, significantly reducing computational complexity. The approach maintains expressiveness by proving that any tensorized circuit can be converted to an orthonormal one while preserving its function up to a scaling factor.

## Key Results
- Marginals in squared circuits can be computed in O(|ϕY|S + |ϕY,Z|S²) time instead of O(LS²) by using orthonormal parameterization
- Any tensorized circuit can be converted to an orthonormal one while preserving its function up to a scaling factor
- Orthonormal parameterization ensures the squared circuit already encodes a normalized distribution (Z=1), eliminating the need for explicit partition function computation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthonormal parameterization ensures the squared circuit already encodes a normalized distribution (Z=1), eliminating the need for explicit partition function computation.
- Mechanism: By parameterizing input layers with orthonormal functions and sum layer matrices with (semi-)unitary matrices, the squaring operation automatically produces identity matrices when integrated over marginalized variables, thus preserving normalization.
- Core assumption: The tensorized circuit is structured-decomposable and the orthonormal conditions (Definition 3) are strictly maintained.
- Evidence anchors:
  - [abstract] "Inspired by TN canonical forms, we show how to parameterize squared circuits to ensure they encode already normalized distributions."
  - [section 3] "Proposition 1. Let c be a structured-decomposable tensorized circuit over variables X. If c is orthonormal, then its squaring encodes a normalized distribution, i.e., Z = 1."
- Break condition: If the circuit is not structured-decomposable or the orthonormal constraints are violated during parameterization updates.

### Mechanism 2
- Claim: Marginalization complexity improves from O(LS²) to O(|ϕY|S + |ϕY,Z|S²) by selectively avoiding squaring layers that depend only on marginalized variables.
- Mechanism: During marginalization, layers whose scope depends solely on variables to be integrated out (Z) produce identity matrices when squared and integrated, so they can be skipped. Only layers depending on both Y and Z need to be squared, reducing computational overhead.
- Core assumption: The circuit maintains orthonormal structure during marginalization and the topological ordering correctly identifies layer dependencies.
- Evidence anchors:
  - [abstract] "We then use this parameterization to devise an algorithm to compute any marginal of squared circuits that is more efficient than a previously known one."
  - [section 4] "Theorem 1. Let c be a structured-decomposable orthonormal circuit over variables X. Let Z ⊆ X, Y = X \ Z. Computing the marginal likelihood p(y) requires time O(|ϕY|S +|ϕY,Z|S²)"
- Break condition: If variable dependency tracking fails or if the orthonormal structure is compromised during computation.

### Mechanism 3
- Claim: Expressiveness is preserved because any tensorized circuit can be converted to an orthonormal one while maintaining equivalence up to a scaling factor.
- Mechanism: Through QR decomposition applied recursively to sum layer parameter matrices, the circuit can be transformed to satisfy orthonormal constraints without losing representational power, with the scaling factor recovered at the output layer.
- Core assumption: QR decomposition can be efficiently computed and propagated through the circuit structure without numerical instability.
- Evidence anchors:
  - [abstract] "We conclude by formally showing the proposed parameterization comes with no expressiveness loss for many circuit classes."
  - [section 5] "Theorem 2. Let c be a tensorized circuit over variables X. Assume that each input layer in c encodes a set of orthonormal functions. Then, there exists an algorithm returning an orthonormal circuit c′ in polynomial time such that c′ is equivalent to c up to a multiplicative constant"
- Break condition: If QR decomposition fails due to numerical issues or if the circuit structure prevents proper propagation of the scaling factors.

## Foundational Learning

- Concept: Tensor networks and their canonical forms
  - Why needed here: The paper builds directly on tensor network canonical forms that use unitary matrices to simplify partition function computation, extending this concept to general circuits
  - Quick check question: How do unitary matrices in tensor network canonical forms simplify partition function computation?

- Concept: Structured-decomposability in circuits
  - Why needed here: The efficiency gains depend on the circuit being structured-decomposable, which allows tractable marginalization and the construction of squared circuits
  - Quick check question: What structural property must a circuit have to ensure tractable marginalization of any variable subset?

- Concept: QR decomposition and matrix parameterizations
  - Why needed here: The orthonormalization algorithm relies on QR decomposition to convert arbitrary parameter matrices to (semi-)unitary form while preserving circuit function
  - Quick check question: What is the key property of the Q matrix in a QR decomposition that makes it useful for enforcing orthonormal constraints?

## Architecture Onboarding

- Component map:
  - Input layers: Parameterized with orthonormal functions (Fourier series, Hermite polynomials, etc.)
  - Product layers: Hadamard (element-wise) or Kronecker (tensor) products between layer outputs
  - Sum layers: Parameterized by (semi-)unitary matrices, ensuring orthonormal structure
  - Circuit structure: Tree-like decomposition where each product layer factorizes scope to inputs

- Critical path:
  1. Build initial tensorized circuit with desired structure
  2. Apply orthonormalization algorithm to convert to orthonormal form
  3. Compute marginals using the optimized algorithm that skips identity-producing sub-circuits
  4. Optionally convert back to original form if needed for other operations

- Design tradeoffs:
  - Orthonormalization adds preprocessing overhead but pays off in faster marginalization
  - Restricting to (semi-)unitary matrices may limit some parameterization flexibility
  - Kronecker product layers in orthonormal circuits may have increased size compared to Hadamard-only alternatives

- Failure signatures:
  - Numerical instability in QR decomposition during orthonormalization
  - Incorrect variable dependency tracking leading to unnecessary squaring
  - Partition function deviating from 1 due to orthonormal constraint violations

- First 3 experiments:
  1. Compare marginalization time for a simple tree-structured circuit with and without orthonormalization on a fixed variable subset
  2. Test orthonormalization preservation during gradient-based learning updates
  3. Verify that orthonormalization doesn't change the circuit's output distribution by comparing samples before and after conversion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal way to parameterize squared orthonormal circuits for learning from data?
- Basis in paper: Explicit - Section 6 discusses future work on learning squared orthonormal PCs from data
- Why unresolved: The paper focuses on theoretical properties and efficiency gains but doesn't explore practical learning algorithms or compare different parameterizations
- What evidence would resolve it: Empirical studies comparing different unitary matrix parameterizations (Arjovsky et al., Huang et al., Bansal et al.) on various datasets and tasks

### Open Question 2
- Question: How do squared orthonormal circuits perform compared to squared circuits with unconstrained parameters on practical machine learning tasks?
- Basis in paper: Explicit - Section 6 mentions future work on comparing squared orthonormal PCs vs squared PCs with unconstrained parameters
- Why unresolved: The paper proves theoretical expressiveness equivalence but doesn't provide empirical performance comparisons
- What evidence would resolve it: Benchmark experiments on tasks like lossless compression, sampling, and density estimation comparing both approaches

### Open Question 3
- Question: Can neural network parameterizations be effectively adapted for squared orthonormal circuits?
- Basis in paper: Explicit - Section 6 suggests parameterizing squared orthonormal PCs similarly to neural network parameterizations of monotonic PCs
- Why unresolved: The paper only mentions this as future work without exploring specific architectures or training methods
- What evidence would resolve it: Implementations of neural network-based squared orthonormal circuits with experimental validation on real-world datasets

## Limitations
- The practical efficiency gains depend heavily on circuit structure and variable partitioning, which are not thoroughly analyzed
- Numerical stability of QR decomposition during orthonormalization is not discussed
- The extension to Kronecker product layers may increase computational overhead compared to Hadamard-only alternatives

## Confidence

**High Confidence**: The theoretical correctness of the orthonormalization approach and its ability to preserve expressiveness (Theorem 2)

**Medium Confidence**: The practical efficiency improvements, as they depend on specific circuit structures and variable dependencies that weren't extensively benchmarked

**Low Confidence**: The handling of Kronecker product layers in orthonormal circuits, as the paper acknowledges this requires "further care"

## Next Checks
1. **Numerical Stability Test**: Apply the orthonormalization algorithm to circuits with varying sizes and assess numerical stability of QR decompositions, particularly for circuits with many layers
2. **Efficiency Benchmarking**: Compare actual runtime performance across different circuit structures (tree vs. more complex topologies) and variable partitioning strategies to validate claimed complexity improvements
3. **Expressiveness Preservation**: Verify that orthonormalization preserves the circuit's ability to approximate target distributions by training on a simple dataset before and after orthonormalization