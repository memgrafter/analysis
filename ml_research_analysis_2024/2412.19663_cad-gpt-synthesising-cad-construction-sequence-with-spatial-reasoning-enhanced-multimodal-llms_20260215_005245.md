---
ver: rpa2
title: 'CAD-GPT: Synthesising CAD Construction Sequence with Spatial Reasoning-Enhanced
  Multimodal LLMs'
arxiv_id: '2412.19663'
source_url: https://arxiv.org/abs/2412.19663
tags:
- spatial
- sketch
- these
- modeling
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CAD-GPT, a novel approach for synthesizing
  CAD construction sequences using a spatial reasoning-enhanced multimodal large language
  model (MLLM). The method addresses the challenge of accurately inferring 3D spatial
  locations and orientations in CAD model generation, which existing MLLMs struggle
  with.
---

# CAD-GPT: Synthesising CAD Construction Sequence with Spatial Reasoning-Enhanced Multimodal LLMs

## Quick Facts
- arXiv ID: 2412.19663
- Source URL: https://arxiv.org/abs/2412.19663
- Authors: Siyu Wang; Cailian Chen; Xinyi Le; Qimin Xu; Lei Xu; Yanzhou Zhang; Jie Yang
- Reference count: 16
- Outperforms state-of-the-art methods, achieving 48% reduction in Chamfer Distance and 91% reduction in invalidity ratio for image input

## Executive Summary
This paper introduces CAD-GPT, a novel approach for synthesizing CAD construction sequences using a spatial reasoning-enhanced multimodal large language model (MLLM). The method addresses the challenge of accurately inferring 3D spatial locations and orientations in CAD model generation, which existing MLLMs struggle with. CAD-GPT employs a 3D Modeling Spatial Localization Mechanism that maps 3D spatial positions and sketch plane rotation angles into a 1D linguistic feature space using specialized spatial unfolding. The model takes either a single image or textual description as input and generates precise CAD modeling sequences. Extensive experiments demonstrate that CAD-GPT significantly outperforms state-of-the-art methods in both accuracy and failure rates.

## Method Summary
CAD-GPT fine-tunes LLaVA-1.5 7B with a 3D Modeling Spatial Localization Mechanism that converts 3D coordinates and orientation angles into discrete tokens. The approach uses specialized spatial unfolding to map continuous 3D positions and sketch plane rotation angles into a 1D linguistic feature space. The model incorporates learnable position embeddings to bridge the modality gap between spatial tokens and language tokens. Training uses a dataset of 162k image-CAD sequence pairs and 18k text-CAD sequence pairs from the DeepCAD dataset, with two-stage training (image-to-CAD followed by text-to-CAD).

## Key Results
- Achieves median Chamfer Distance of 9.77 for image input (48% reduction compared to HNC-CAD)
- Achieves invalidity ratio of 1.61 for image input (91% reduction compared to HNC-CAD)
- For text input, achieves median Chamfer Distance of 28.33 and invalidity ratio of 7.43

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mapping 3D spatial positions and sketch plane rotation angles into 1D linguistic feature space via spatial unfolding enables MLLMs to reason about 3D geometry as naturally as text.
- Mechanism: The 3D Modeling Spatial Localization Mechanism converts continuous 3D coordinates and orientation angles into discrete tokens by discretizing the coordinate space (K³ grids for positions, 9 levels per angle) and mapping them into a fixed vocabulary. These tokens are concatenated with regular text tokens, allowing the MLLM to process spatial information through its existing attention and reasoning mechanisms.
- Core assumption: Discretizing 3D space into finite tokens preserves enough geometric precision for accurate CAD reconstruction while remaining within the model's linguistic processing capacity.
- Evidence anchors:
  - [abstract] "This method maps 3D spatial positions and 3D sketch plane rotation angles into a 1D linguistic feature space using a specialized spatial unfolding mechanism"
  - [section] "We normalize each CAD model within a 1 × 1 × 1 cube. Next, we discretize the vertices' coordinates into K³ grids, where K = 36"
  - [corpus] Weak evidence - no corpus papers explicitly discuss spatial unfolding from 3D to 1D for CAD, though related work on 3D point cloud discretization exists
- Break condition: If discretization resolution (K=36) is insufficient for the geometric complexity of target CAD models, or if token vocabulary size exceeds model capacity, spatial reasoning accuracy degrades.

### Mechanism 2
- Claim: Learnable position embeddings bridge the modality gap between spatial tokens and language tokens, enabling coherent reasoning across both domains.
- Mechanism: After introducing spatial tokens into the vocabulary, four learnable position embedding matrices (Wangle, W3D_pos, W2D_sketch_x, W2D_sketch_y) are added to augment the embeddings of corresponding token types. These embeddings are trained alongside the MLLM to capture spatial relationships.
- Core assumption: The model can learn meaningful spatial relationships from position embeddings during fine-tuning, despite being initialized from language-only pretraining.
- Evidence anchors:
  - [section] "We introduced four distinct types of tokens to the vocabulary. Correspondingly, we expanded the embedding layers to accommodate these additional tokens and incorporated learnable position embedding layers"
  - [section] "The use of learnable position embeddings allows the model to understand the relative positioning and relationships within the spatial data"
  - [corpus] Weak evidence - no corpus papers explicitly discuss learnable position embeddings for bridging language and spatial modalities in CAD
- Break condition: If position embeddings fail to converge during training or if spatial relationships learned are insufficient for accurate 3D reasoning.

### Mechanism 3
- Claim: Fine-tuning on a specialized dataset pairing CAD sequences with natural language descriptions and rendered images enables the MLLM to learn the CAD construction domain.
- Mechanism: The approach constructs a dataset of 162k image-CAD sequence pairs and 18k text-CAD sequence pairs from the DeepCAD dataset, using rendered images and natural language captions. Two-stage training (image-to-CAD followed by text-to-CAD) adapts the MLLM to the CAD generation task.
- Core assumption: The DeepCAD dataset contains sufficient CAD modeling diversity and the synthetic natural language captions are adequate for teaching the model CAD domain knowledge.
- Evidence anchors:
  - [section] "We constructed a dataset that pairs CAD modeling sequences with natural language descriptions and single fixed-view rendered images of the CAD models"
  - [section] "Utilizing the DeepCAD dataset, we generated 160k fixed-viewpoint CAD model images and 18k corresponding natural language captions"
  - [corpus] Weak evidence - while related work exists on CAD generation datasets, the specific construction of image-text-CAD triplets for MLLM training is novel
- Break condition: If the dataset lacks coverage of complex CAD operations or if the natural language descriptions are too generic to capture necessary geometric details.

## Foundational Learning

- Concept: Multimodal Large Language Models (MLLMs) and their architecture
  - Why needed here: Understanding how MLLMs process visual and textual information jointly is essential for comprehending how CAD-GPT extends this to spatial reasoning
  - Quick check question: What are the three main modules of an efficient MLLM and what role does each play?

- Concept: 3D spatial reasoning and CAD construction sequences
  - Why needed here: CAD models are built through sequences of operations (sketching and extrusion) that require precise spatial reasoning about 3D positions and orientations
  - Quick check question: How are CAD models typically represented as construction sequences in parametric CAD systems?

- Concept: Tokenization and vocabulary expansion in LLMs
  - Why needed here: The 3D Modeling Spatial Localization Mechanism relies on adding spatial tokens to the LLM vocabulary and understanding how this affects model processing
  - Quick check question: What happens when new tokens are added to an LLM's vocabulary during fine-tuning?

## Architecture Onboarding

- Component map: Image/text → Visual encoder (ViT-L/14-336px) → Vision-language projector → LLM reasoning → Spatial tokenization → Position embedding augmentation → CAD sequence generation

- Critical path: Image/text → Visual encoder → Vision-language projector → LLM reasoning → Spatial tokenization → Position embedding augmentation → CAD sequence generation

- Design tradeoffs:
  - Tokenization resolution (K=36) vs. computational efficiency
  - Fixed vocabulary size vs. model capacity for spatial tokens
  - Two-stage training (image then text) vs. joint training
  - Discrete spatial representation vs. continuous spatial reasoning

- Failure signatures:
  - High invalidity ratio (IR) indicates spatial tokens aren't mapping to valid CAD operations
  - Poor Chamfer Distance suggests spatial discretization resolution is insufficient
  - Low command accuracy (ACCcmd) points to issues with token-to-operation mapping
  - Training instability may indicate position embeddings aren't learning meaningful spatial relationships

- First 3 experiments:
  1. Test spatial tokenization with synthetic 3D points - verify that discretizing known coordinates produces expected tokens and that position embeddings can recover spatial relationships
  2. Ablation study with and without spatial tokens - compare CAD generation accuracy to establish baseline impact of the 3D Modeling Spatial Localization Mechanism
  3. Visualization of learned position embeddings - examine whether spatial tokens cluster meaningfully in embedding space to validate the bridging mechanism

## Open Questions the Paper Calls Out

- Question: How can the 3D Modeling Spatial Localization Mechanism be extended to handle more complex geometric transformations beyond basic translations and rotations?
- Question: What is the optimal discretization strategy for 2D sketch coordinates to balance precision and computational efficiency?
- Question: How can the CAD-GPT model be adapted to handle real-time interactive CAD design sessions with user feedback?

## Limitations
- Relies on discretizing continuous 3D spatial information into finite tokens, which may lose precision for complex CAD geometries
- Uses synthetic natural language captions that may not capture all necessary geometric details for real-world CAD descriptions
- Assumes simple CAD construction sequences with only sketching and extrusion operations

## Confidence

**High Confidence:** The architectural integration of spatial tokens into the MLLM vocabulary and the effectiveness of learnable position embeddings in bridging language and spatial modalities.

**Medium Confidence:** The generalizability of the approach to CAD models beyond the DeepCAD dataset.

**Low Confidence:** The scalability of the approach to CAD workflows with more complex operations beyond sketching and extrusion.

## Next Checks

1. Systematically vary the discretization resolution (K parameter) and measure the trade-off between spatial precision and model performance. Test whether finer discretization (K>36) or adaptive discretization schemes improve reconstruction accuracy for complex CAD geometries.

2. Evaluate CAD-GPT on CAD models from different domains (mechanical engineering, architectural design, consumer product design) to assess whether the spatial reasoning mechanism generalizes beyond the DeepCAD dataset's CAD modeling patterns.

3. Modify the spatial tokenization scheme to incorporate additional CAD operations (filleting, chamfering, Boolean operations) and retrain the model to determine whether the current spatial unfolding mechanism can be extended to handle these more complex geometric transformations.