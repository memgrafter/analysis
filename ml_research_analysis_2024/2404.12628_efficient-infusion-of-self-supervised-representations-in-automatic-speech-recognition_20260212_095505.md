---
ver: rpa2
title: Efficient infusion of self-supervised representations in Automatic Speech Recognition
arxiv_id: '2404.12628'
source_url: https://arxiv.org/abs/2404.12628
tags:
- encoder
- representations
- layer
- speech
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to efficiently incorporate representations
  from pre-trained self-supervised learning (SSL) models like Wav2vec and HuBERT into
  end-to-end automatic speech recognition (ASR) systems. The proposed approach uses
  a fusion layer to combine the SSL representations with conventional acoustic features,
  avoiding the need to use SSL models during training.
---

# Efficient infusion of self-supervised representations in Automatic Speech Recognition

## Quick Facts
- arXiv ID: 2404.12628
- Source URL: https://arxiv.org/abs/2404.12628
- Reference count: 20
- Primary result: Framewise addition and cross-attention mechanisms to incorporate SSL representations improve ASR performance while maintaining efficiency

## Executive Summary
This paper proposes a method to efficiently incorporate representations from pre-trained self-supervised learning (SSL) models like Wav2vec and HuBERT into end-to-end automatic speech recognition (ASR) systems. The approach uses a fusion layer to combine SSL representations with conventional acoustic features, avoiding the need to use SSL models during training. Two fusion mechanisms are explored: subsampled framewise addition and cross-attention. Experiments on Librispeech and Tedlium datasets show significant performance gains compared to baseline conformer models, with faster convergence and comparable model sizes.

## Method Summary
The method involves pre-extracting SSL features from models like Wav2vec 2.0 and HuBERT, then fusing them with acoustic features (fbank) using either framewise addition or cross-attention mechanisms. The fused features are fed into a conformer encoder within a joint CTC-Attention architecture. This approach avoids running SSL models during training, reducing computational overhead. The fusion can occur at different encoder layers, with the first layer being most effective. The method is implemented using the ESPnet toolkit and tested on Librispeech-100h and Tedlium datasets.

## Key Results
- Framewise addition and cross-attention fusion mechanisms achieve significant WER improvements over baseline conformer models
- Cross-attention generally outperforms framewise addition in terms of WER and convergence speed
- Using HuBERT BASE representations at the first encoder layer provides optimal performance
- Pre-extracting SSL features reduces training time while maintaining performance gains

## Why This Works (Mechanism)

### Mechanism 1: Framewise Addition
- Claim: Framewise addition of subsampled SSL and acoustic features provides a parameter-free way to combine complementary representations
- Mechanism: The SSL and acoustic features are both subsampled by a factor of 2, aligning their temporal dimensions. The resulting sequences are added elementwise, producing a fused representation for the encoder
- Core assumption: The temporal alignment between SSL and acoustic features after subsampling is sufficient for effective fusion, and the additive combination captures complementary information
- Evidence anchors: [abstract] "framewise addition and (2) cross-attention mechanisms to efficiently incorporate the representations from the SSL model(s)"; [section] "Inspired by the work of Jialu et al. [9], we first propose a simple parameterless approach... we can then perform framewise addition of both these sequences to generate h0"
- Break condition: If the temporal alignment assumption fails (e.g., large frame rate mismatch) or if the additive combination does not capture the interaction between modalities

### Mechanism 2: Cross-Attention
- Claim: Cross-attention allows the model to learn optimal temporal alignment and fusion between SSL and acoustic features
- Mechanism: The acoustic features serve as queries, and the SSL features serve as keys and values. The cross-attention layer computes weighted sums of SSL features conditioned on acoustic features, producing a fused representation
- Core assumption: The attention mechanism can learn the correct alignment and weighting between the two modalities, and the resulting fused representation improves downstream ASR performance
- Evidence anchors: [abstract] "cross-attention mechanisms to efficiently incorporate the representations from the SSL model(s)"; [section] "we introduce a cross-attention layer... that uses the concept of attention to determine how each frame of ˆu wants to attend to the frames of ˆv"
- Break condition: If the attention mechanism fails to learn meaningful alignments (e.g., due to lack of diversity in training data) or if the computational overhead outweighs the performance gains

### Mechanism 3: Pre-extraction
- Claim: Pre-extraction of SSL features reduces computational overhead during training while maintaining performance gains
- Mechanism: SSL features are computed once during preprocessing and stored, eliminating the need to run the SSL model during each training iteration
- Core assumption: The cost of pre-extracting SSL features is offset by the savings in training time, and the frozen SSL features are still effective for the target ASR task
- Evidence anchors: [abstract] "avoiding the usage of SSL models during training"; [section] "The representations from SSL models are dumped beforehand to speed up the training"
- Break condition: If the pre-extraction process becomes a bottleneck (e.g., for very large datasets) or if the frozen SSL features are not representative of the target ASR domain

## Foundational Learning

- Concept: Self-supervised learning (SSL) for speech
  - Why needed here: SSL models like Wav2Vec and HuBERT learn rich speech representations from unlabeled data, which can improve ASR performance, especially with limited labeled data
  - Quick check question: What is the key difference between SSL models and traditional supervised models in the context of speech representation learning?

- Concept: End-to-end ASR with joint CTC-Attention
  - Why needed here: The proposed method integrates SSL features into an end-to-end ASR architecture that uses both CTC and attention-based decoding, requiring understanding of how these components interact
  - Quick check question: How do CTC and attention-based decoding differ in their approach to sequence modeling, and why might a joint model be beneficial?

- Concept: Conformer encoder architecture
  - Why needed here: The proposed method modifies the conformer encoder to incorporate SSL features, requiring understanding of the conformer's components (convolution, self-attention, feed-forward) and their roles
  - Quick check question: What are the key components of a conformer encoder, and how do they contribute to the model's ability to capture both local and global speech patterns?

## Architecture Onboarding

- Component map:
  Raw speech input → FBANK generator → subsampled FBANK features (ˆu)
  Raw speech input → SSL model → normalized SSL features (ˆv)
  Fusion layer (framewise addition or cross-attention) → fused features (h0)
  h0 + conformer encoder layers → contextualized speech representation (h)
  h → CTC decoder + Attention decoder → final ASR output

- Critical path:
  1. Raw speech input is processed in parallel by FBANK generator and SSL model
  2. The resulting features are normalized and subsampled as needed
  3. The fusion layer combines the features into a single representation
  4. The fused representation is processed by the conformer encoder
  5. The final representation is used by both CTC and attention decoders to produce the ASR output

- Design tradeoffs:
  - Framewise addition vs. cross-attention: Framewise addition is parameter-free but relies on fixed temporal alignment; cross-attention learns optimal alignment but adds parameters and computation
  - Using multiple SSL models vs. a single model: Multiple models may capture more diverse information but increase complexity and risk of overfitting
  - Pre-extracting SSL features vs. using them on-the-fly: Pre-extraction reduces training time but requires additional storage and preprocessing

- Failure signatures:
  - No improvement over baseline: Fusion layer may not be effective, SSL features may not be complementary to acoustic features, or the model may not be learning to use the fused representation
  - Degradation in performance: Fusion layer may be introducing noise, SSL features may be misaligned or of poor quality, or the model may be overfitting to the SSL features
  - Increased training time without performance gains: Pre-extraction may not be effective, or the fusion layer may be introducing unnecessary complexity

- First 3 experiments:
  1. Implement the framewise addition fusion layer and train a conformer model with HuBERT BASE features on Librispeech 100h; compare performance to the baseline conformer
  2. Implement the cross-attention fusion layer and train a conformer model with HuBERT BASE features on Librispeech 100h; compare performance to the framewise addition model and the baseline
  3. Pre-extract SSL features for the entire Librispeech 100h dataset and measure the time savings compared to computing SSL features on-the-fly during training; verify that the performance is unchanged

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed fusion layer architecture scale to even larger pre-trained SSL models like HuBERT-Large or beyond, and what are the practical computational limits?
- Basis in paper: [explicit] The paper briefly mentions using HuBERT-LARGE in experiments on Tedlium, showing improved performance, but does not explore scaling behavior or computational constraints in detail
- Why unresolved: The experiments focus on smaller-scale models and datasets; no systematic study of scaling to larger SSL models or the computational trade-offs involved is provided
- What evidence would resolve it: Detailed experiments comparing performance and computational cost (e.g., memory usage, training time) when integrating representations from increasingly larger SSL models (e.g., HuBERT-Large, HuBERT-XL, or even larger variants) on diverse datasets

### Open Question 2
- Question: How robust is the proposed method to domain shifts or low-resource scenarios where the pre-trained SSL model has not been exposed to the target domain or language?
- Basis in paper: [explicit] The paper tests performance on Tedlium, an unseen dataset, and notes the importance of normalization, but does not systematically explore robustness across varying degrees of domain shift or low-resource settings
- Why unresolved: The experiments are limited to two datasets (Librispeech and Tedlium) and do not vary the degree of domain mismatch or resource availability
- What evidence would resolve it: Experiments on multiple datasets with varying degrees of domain shift (e.g., accents, noise conditions, languages) and resource levels (e.g., 10h, 1h, few-shot settings) to evaluate robustness

### Open Question 3
- Question: What is the optimal fusion strategy (e.g., framewise addition vs. cross-attention) for different types of SSL representations or speech tasks beyond ASR, such as speech translation or speaker identification?
- Basis in paper: [explicit] The paper compares framewise addition and cross-attention on ASR tasks, finding cross-attention generally superior, but does not explore other tasks or SSL model types
- Why unresolved: The analysis is limited to ASR and specific SSL models (Wav2vec, HuBERT), without exploring task or model diversity
- What evidence would resolve it: Comparative studies applying both fusion strategies to other speech tasks (e.g., speech translation, speaker identification) and with different SSL models (e.g., APC, VQ-APC) to identify task- or model-specific optimal strategies

## Limitations
- The performance gains are primarily demonstrated on English datasets (Librispeech and Tedlium), leaving uncertainty about effectiveness for low-resource or multilingual scenarios
- The computational efficiency claims rely on pre-extracted SSL features, but the preprocessing overhead and storage requirements are not thoroughly characterized
- The analysis focuses on two specific fusion mechanisms without exploring the full design space of potential integration strategies

## Confidence
- High confidence: The empirical results showing WER improvements on Librispeech and Tedlium datasets
- Medium confidence: The claimed computational efficiency benefits and faster convergence rates
- Medium confidence: The effectiveness of the specific fusion mechanisms (framewise addition and cross-attention)

## Next Checks
1. Evaluate the approach on non-English datasets and low-resource languages to assess cross-linguistic generalization
2. Characterize the preprocessing overhead and storage requirements for SSL feature extraction at scale
3. Implement and compare additional fusion mechanisms (e.g., gated fusion, concatenation with projection) to establish whether the proposed approaches are optimal