---
ver: rpa2
title: 'Medical mT5: An Open-Source Multilingual Text-to-Text LLM for The Medical
  Domain'
arxiv_id: '2404.07613'
source_url: https://arxiv.org/abs/2404.07613
tags:
- medical
- language
- domain
- data
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Medical mT5, the first open-source multilingual
  text-to-text LLM for the medical domain. The authors compiled a 3B word multilingual
  corpus covering English, French, Italian, and Spanish, then continued pre-training
  mT5 on this domain-specific data.
---

# Medical mT5: An Open-Source Multilingual Text-to-Text LLM for The Medical Domain

## Quick Facts
- arXiv ID: 2404.07613
- Source URL: https://arxiv.org/abs/2404.07613
- Authors: Iker García-Ferrero; Rodrigo Agerri; Aitziber Atutxa Salazar; Elena Cabrio; Iker de la Iglesia; Alberto Lavelli; Bernardo Magnini; Benjamin Molinet; Johana Ramirez-Romero; German Rigau; Jose Maria Villa-Gonzalez; Serena Villata; Andrea Zaninello
- Reference count: 0
- Outperforms similarly-sized text-to-text models on Spanish, French, and Italian medical benchmarks

## Executive Summary
This paper introduces Medical mT5, the first open-source multilingual text-to-text LLM for the medical domain. The authors created a 3B word multilingual corpus covering English, French, Italian, and Spanish, then continued pretraining mT5 on this domain-specific data. They also developed new multilingual benchmarks for sequence labeling (Argument Mining) and abstractive question answering. Experimental results demonstrate that Medical mT5 outperforms similarly-sized text-to-text models on Spanish, French, and Italian benchmarks, while remaining competitive with state-of-the-art models in English.

## Method Summary
The authors compiled a multilingual medical corpus from diverse sources including PubMed abstracts, clinical notes, and medical websites, totaling 3B words across four languages. They continued pretraining mT5 checkpoints on this domain-specific data for one epoch to prevent overfitting. The model was evaluated on newly created multilingual benchmarks for sequence labeling and abstractive question answering, using both single-task and multi-task fine-tuning approaches. Zero-shot cross-lingual transfer was assessed by fine-tuning on English data and evaluating on other languages.

## Key Results
- Medical mT5 achieves best overall results for Spanish, French, and Italian in multi-task sequence labeling
- Outperforms any other model in zero-shot cross-lingual transfer from English to other languages
- Competitive with current state-of-the-art models on English medical benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific multilingual pretraining improves performance across languages by enriching the shared multilingual representation space with medical domain knowledge, enabling better cross-lingual transfer. The core assumption is that the multilingual model can effectively share learned medical representations across languages during fine-tuning.

### Mechanism 2
Multi-task training improves overall model performance compared to single-task training by forcing the model to learn more robust, generalized representations applicable across tasks. This works under the assumption that the tasks share enough underlying structure to benefit from multi-task learning.

### Mechanism 3
Zero-shot cross-lingual transfer works effectively from English to other languages because the shared multilingual representations learned during pretraining allow the model to generalize from English training data to other languages without task-specific training in those languages.

## Foundational Learning

- **Masked language modeling objective (span-corruption)**: Why needed - This is the pretraining objective used by mT5 and continued by Medical mT5, which teaches the model to reconstruct masked text spans, building bidirectional context understanding. Quick check - What is the difference between masked language modeling and causal language modeling?

- **Text-to-text framework**: Why needed - Medical mT5 is a text-to-text model, meaning it takes text as input and generates text as output, requiring all tasks to be reformulated as text generation problems. Quick check - How would you convert a sequence labeling task into a text-to-text format?

- **Cross-lingual transfer**: Why needed - Medical mT5 is evaluated on zero-shot cross-lingual transfer, demonstrating its ability to perform tasks in languages it wasn't explicitly trained on. Quick check - What factors contribute to successful cross-lingual transfer in multilingual models?

## Architecture Onboarding

- **Component map**: mT5 encoder-decoder architecture with medical domain-specific weights, using shared vocabulary across four languages (English, French, Italian, Spanish)
- **Critical path**: Pretraining (continuing mT5 on medical corpus) → Fine-tuning (single/multi-task) → Evaluation (sequence labeling, QA)
- **Design tradeoffs**: Smaller model size (738M parameters) vs. performance vs. larger models (3B parameters); multilingual coverage vs. depth in any single language
- **Failure signatures**: Overfitting on small domain corpus (evidenced by degradation with more pretraining epochs); poor cross-lingual transfer if representations are not language-agnostic
- **First 3 experiments**:
  1. Continue pretraining mT5 on medical corpus for 1 epoch; evaluate perplexity on held-out medical data
  2. Fine-tune on single sequence labeling task; evaluate F1 score
  3. Fine-tune on multi-task setting; compare performance to single-task baseline

## Open Questions the Paper Calls Out

### Open Question 1
How would Medical mT5's performance change with substantially larger domain-specific corpora across all languages? The authors note their corpus is relatively small compared to English datasets and mention potential overfitting with larger models, leaving the impact of larger datasets unexplored.

### Open Question 2
What is the optimal architecture for abstractive question answering in the medical domain given the evaluation difficulties observed? The authors found manual evaluation challenging due to similar answers across models and low inter-annotator agreement, suggesting T5 architecture may not be ideal.

### Open Question 3
How can evaluation metrics for medical text generation tasks be improved to better capture truthfulness and potential harm? The authors note that ROUGE is inadequate for medical domain tasks and that manual evaluation by doctors resulted in low agreement and difficulty distinguishing model outputs.

## Limitations
- Corpus size and domain coverage remain unclear, particularly for low-resource languages where medical domain data may be scarce
- Newly created evaluation benchmarks lack detailed validation of quality and representativeness
- Model uses standard mT5 architecture without modifications for medical domain specificity

## Confidence

**High Confidence**: The core finding that domain-specific pretraining improves medical NLP performance in multilingual settings is well-supported by experimental results.

**Medium Confidence**: Claims about zero-shot cross-lingual transfer effectiveness are supported by results but limited by the relatively small number of target languages and tasks.

**Low Confidence**: The assertion that Medical mT5 is "the first open-source multilingual text-to-text LLM for the medical domain" cannot be independently verified.

## Next Checks

1. Conduct systematic analysis of the multilingual medical corpus to quantify domain coverage, data quality distribution, and potential biases across languages.

2. Perform comprehensive validation of the newly created evaluation benchmarks through inter-annotator agreement studies and coverage analysis against established medical ontologies.

3. Systematically evaluate whether domain-specific architectural modifications provide performance gains over the standard mT5 architecture for medical tasks.