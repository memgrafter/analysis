---
ver: rpa2
title: Numeric Reward Machines
arxiv_id: '2404.19370'
source_url: https://arxiv.org/abs/2404.19370
tags:
- reward
- agent
- numeric
- boolean
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces two types of reward machines (RMs) that incorporate
  numeric features: numeric-Boolean and numeric. In numeric-Boolean RMs, numeric distances
  are emulated using Boolean features that track decreases and target reach, while
  numeric RMs use the distance values directly alongside Boolean terminal features.'
---

# Numeric Reward Machines

## Quick Facts
- arXiv ID: 2404.19370
- Source URL: https://arxiv.org/abs/2404.19370
- Authors: Kristina Levina; Nikolaos Pappas; Athanasios Karapantelakis; Aneta Vulgarakis Feljan; Jendrik Seipp
- Reference count: 5
- Key outcome: Numeric-Boolean and numeric reward machines outperform Boolean RM with automatic shaping in Craft domain using Q-learning, CRM, and HRM methods.

## Executive Summary
This paper introduces two types of reward machines (RMs) that incorporate numeric features: numeric-Boolean and numeric. Numeric-Boolean RMs emulate numeric distances using two Boolean features that track decreases and target reach, while numeric RMs use distance values directly alongside Boolean terminal features. The methods are evaluated against a baseline Boolean RM with automatic reward shaping in the Craft domain using cross-product Q-learning, Q-learning with counterfactual experiences, and the options framework. Experimental results demonstrate that numeric-Boolean and numeric RM approaches achieve faster learning and higher performance, validating the benefits of extending RMs with numeric features for inherently numeric tasks.

## Method Summary
The paper proposes numeric-Boolean and numeric reward machines to extend traditional Boolean RMs with numeric features for reinforcement learning in numeric tasks. In numeric-Boolean RMs, distance to target is emulated by two Boolean features: one tracking distance decrease and one indicating target reached. Numeric RMs use the distance value directly with a Boolean terminal feature. The methods are compared against a baseline Boolean RM with automatic reward shaping in the Craft domain using three learning approaches: cross-product Q-learning (QRM), Q-learning with counterfactual experiences (CRM), and the options framework (HRM). Experiments evaluate performance on tasks ranging from single-object visits to sequential multi-object visits in both 1a1b1c and 2a2b2c setups.

## Key Results
- Numeric-Boolean and numeric RMs outperform baseline Boolean RM with automatic reward shaping in all tested configurations
- Numeric-Boolean RMs achieve shortest-path guarantees under specific reward parameter constraints
- CRM and HRM methods benefit from RM structure exposure, simulating experiences at all RM states without environmental interaction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Numeric features allow direct reward shaping tied to environmental state, improving sample efficiency
- Mechanism: In numeric RMs, the agent receives reward `-d` where `d` is the distance to target, providing continuous feedback per step rather than sparse Boolean rewards. This enables smoother policy updates and faster learning convergence
- Core assumption: The numeric feature `d` is computable from the environment state and transitions monotonically toward zero as the agent approaches the target
- Evidence anchors:
  - [abstract]: "In a numeric reward machine, distance-to-gold is used directly alongside the Boolean feature robot-reached-gold, and the agent is rewarded with `-d` after each action."
  - [section]: "In a numeric reward machine, distance-to-gold is used directly together with the Boolean featured=0, and the agent is rewarded with `-d` after each action."

### Mechanism 2
- Claim: Numeric-Boolean RMs emulate numeric distances using two Boolean features to preserve shortest-path guarantees under parameter constraints
- Mechanism: The features `↓d` (distance decreased) and `d=0` (target reached) encode progress toward the target as Boolean transitions. Reward `r` is given when `↓d` is true, and `R` when `d=0` is true, ensuring the agent is incentivized to move closer while still reaching the target
- Core assumption: The agent's movement reduces distance monotonically toward the target, so `↓d` reliably signals progress
- Evidence anchors:
  - [abstract]: "In a numeric–Boolean reward machine, distance-to-gold is emulated by two Boolean features distance-to-gold-decreased and robot-reached-gold."
  - [section]: "Feature `↓da` becomes true if the distance between the agent and object a decreases."

### Mechanism 3
- Claim: Exposing RM structure to the agent via Boolean features enables hierarchical RL and counterfactual reasoning to boost learning speed
- Mechanism: The Boolean feature `d=0` marks terminal transitions in the RM, allowing CRM and HRM algorithms to simulate experiences at all RM states without environmental interaction, effectively multiplying training data
- Core assumption: The RM state transitions are Markovian given the Boolean features, enabling accurate simulation of counterfactual experiences
- Evidence anchors:
  - [abstract]: "RL algorithms with access to an RM can simulate experiences at all RM states using only a single interaction with the environment."
  - [section]: "In HRM, the problem is decomposed into sub-problems called options, each representing transitions between RM states."

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The problem is formalized as an MDP where states, actions, transitions, and rewards define the agent's environment
  - Quick check question: What tuple defines an MDP and which component is updated by Q-learning?

- Concept: Q-learning and Bellman optimality
  - Why needed here: Tabular Q-learning estimates optimal action-values Q(s,a) to derive policies without explicit transition probabilities
  - Quick check question: How does the Q-value update rule incorporate the learning rate and discount factor?

- Concept: Reward shaping
  - Why needed here: Numeric features allow direct reward shaping tied to environmental state, improving sample efficiency
  - Quick check question: What is the potential-based reward shaping formula and why does it preserve optimal policies?

## Architecture Onboarding

- Component map:
  - Environment (Craft domain grid with objects) -> Agent (RL controller) -> Reward Machine (Boolean/Numeric-Boolean/Numeric) -> Learning algorithm (QRM/CRM/HRM) -> Experience replay (counterfactual/hierarchical)

- Critical path:
  1. Agent interacts with environment → receives state, reward
  2. Labeling function maps state to Boolean/numeric features
  3. RM updates internal state and emits reward
  4. Learning algorithm updates Q-values or hierarchical policies
  5. Agent selects next action (ϵ-greedy)

- Design tradeoffs:
  - Numeric RM: Direct shaping but requires monotonic distance features
  - Numeric-Boolean RM: Guarantees shortest paths under parameter constraints but uses sparse Boolean rewards
  - Boolean RM: Simpler but relies on automatic shaping, slower learning

- Failure signatures:
  - Non-monotonic distance features → misleading rewards
  - Parameter mismatch (r too large vs R) → suboptimal paths in Numeric-Boolean
  - Sparse Boolean rewards → slow convergence

- First 3 experiments:
  1. Compare Q-learning on Boolean vs Numeric-Boolean RM in a simple 1-target task
  2. Test CRM on Numeric RM with one target, measure convergence speed
  3. Evaluate HRM on Numeric-Boolean RM with sequential tasks, check shortest-path adherence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do numeric reward machines perform on more complex environments with obstacles compared to their performance in obstacle-free environments?
- Basis in paper: [inferred] The paper mentions testing the developed methods in "tabular domains with obstacles" as a future work, indicating that current experiments were conducted in obstacle-free environments
- Why unresolved: The paper's experimental evaluation focuses on obstacle-free environments, leaving the performance in obstacle-rich environments unexplored
- What evidence would resolve it: Conducting experiments on a variety of grid worlds with different obstacle configurations and comparing the performance of numeric reward machines against baseline methods

### Open Question 2
- Question: Can the shortest-path guarantees for numeric reward machines be extended to sequential tasks with multiple target types?
- Basis in paper: [explicit] The paper proves shortest-path guarantees for tasks with one target type but mentions the need to extend these proofs for sequential tasks with several targets as future work
- Why unresolved: The theoretical analysis provided in the paper does not cover sequential tasks with multiple target types, leaving a gap in understanding the behavior of numeric reward machines in such scenarios
- What evidence would resolve it: Developing and proving theorems that extend the shortest-path guarantees to sequential tasks with multiple target types, followed by experimental validation

### Open Question 3
- Question: How does the performance of numeric reward machines compare to numeric-Boolean reward machines in environments with continuous state spaces?
- Basis in paper: [inferred] The paper suggests testing numeric reward machines in "continuous-space domains" as future work, implying that current experiments are limited to discrete state spaces
- Why unresolved: The experimental evaluation in the paper is conducted in discrete grid worlds, and there is no exploration of performance in continuous state spaces
- What evidence would resolve it: Implementing numeric reward machines in continuous control tasks and comparing their performance against numeric-Boolean reward machines and baseline methods

## Limitations

- Theoretical shortest-path guarantees for numeric-Boolean RMs depend on strict parameter constraints that may not hold in all environments
- Empirical evaluation is limited to a single grid-world domain, leaving generalization to other task types uncertain
- Numeric RM approach requires distance features to be monotonic and computationally accessible, which may not be feasible in complex environments with obstacles

## Confidence

- High: Numeric reward machines provide faster learning through direct shaping in numeric tasks
- Medium: Numeric-Boolean RMs preserve shortest-path guarantees under parameter constraints
- Low: Generalization of numeric reward machines to arbitrary domains

## Next Checks

1. Test numeric-Boolean RMs with varying r/R ratios to empirically verify shortest-path guarantees break down when parameters violate theoretical constraints
2. Implement numeric reward machines in a non-grid environment (e.g., robotic arm control) to evaluate distance feature computation feasibility
3. Compare learning curves of numeric RMs with potential-based reward shaping baselines to isolate the benefit of explicit numeric features versus general shaping techniques