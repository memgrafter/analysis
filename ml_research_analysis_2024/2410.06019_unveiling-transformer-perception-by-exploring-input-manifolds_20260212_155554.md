---
ver: rpa2
title: Unveiling Transformer Perception by Exploring Input Manifolds
arxiv_id: '2410.06019'
source_url: https://arxiv.org/abs/2410.06019
tags:
- class
- input
- simexp
- equivalence
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel mathematical framework for exploring
  equivalence classes in the input space of Transformer models using Riemannian geometry.
  The authors develop two algorithms - SiMEC (for finding inputs producing identical
  output distributions) and SiMEC (for navigating between different equivalence classes)
  - by analyzing the pullback of the output space metric through the model's Jacobian.
---

# Unveiling Transformer Perception by Exploring Input Manifolds

## Quick Facts
- arXiv ID: 2410.06019
- Source URL: https://arxiv.org/abs/2410.06019
- Reference count: 30
- One-line primary result: Novel mathematical framework using Riemannian geometry to systematically explore equivalence classes in Transformer input spaces

## Executive Summary
This paper introduces a novel mathematical framework for exploring equivalence classes in the input space of Transformer models using Riemannian geometry. The authors develop two algorithms - SiMEC and SiMExp - that leverage the pullback of the output space metric through the model's Jacobian to navigate between and within equivalence classes. Experiments across image and text datasets demonstrate that SiMEC maintains original class probabilities while SiMExp can shift predictions, with the latter exploring 100x larger input space volumes.

## Method Summary
The framework treats neural networks as maps between manifolds and uses geometric concepts to analyze input space structure. By computing the pullback of the output space metric through the model's Jacobian, the authors identify equivalence classes where the model produces identical outputs. Two algorithms are introduced: SiMEC for exploring within equivalence classes (preserving outputs) and SiMExp for navigating between classes (changing outputs). Both use eigendecomposition of the pullback metric, with SiMEC using zero-eigenvalue eigenvectors and SiMExp using non-zero eigenvectors. The method enables interpretable exploration by mapping modified embeddings back to human-readable formats.

## Key Results
- SiMExp algorithm explores 100x larger input space volumes compared to SiMEC
- SiMEC maintains original class probabilities while SiMExp can shift model predictions
- The framework successfully applies to both image (ViT on MNIST/CIFAR10) and text (BERT on WinoBias/MHS) datasets
- Interpretation algorithm enables visual inspection of equivalence class exploration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pullback of the output space metric through the model's Jacobian identifies equivalence classes in the input space.
- Mechanism: The model's layers deform the input manifold, and by pulling back the metric from the output space, the framework reconstructs a degenerate metric in the input space. Eigenvectors corresponding to zero eigenvalues define tangent directions along which the output remains unchanged.
- Core assumption: The model layers are smooth submersions and the output space has a well-defined metric.
- Evidence anchors:
  - [abstract] "Using eigendecomposition of the pullback of the distance metric defined on the output space through the Jacobian of the model"
  - [section] "Using eigendecomposition of the pullback of the distance metric defined on the output space through the Jacobian of the model, we are able to reconstruct equivalence classes in the input space"
  - [corpus] Weak - corpus lacks specific mentions of Riemannian metric pullbacks
- Break condition: If the model layers are not smooth submersions or if the Jacobian loses rank in a non-regular way, the pullback metric may not correctly identify equivalence classes.

### Mechanism 2
- Claim: SiMEC and SiMExp algorithms navigate between and within equivalence classes using null and non-null eigenvectors respectively.
- Mechanism: SiMEC uses eigenvectors with zero eigenvalues to move within an equivalence class without changing the output distribution, while SiMExp uses non-zero eigenvectors to shift to a different equivalence class and change the output distribution.
- Core assumption: The pullback metric has a clear spectral gap between zero and non-zero eigenvalues.
- Evidence anchors:
  - [abstract] "Our method enables two complementary exploration procedures: the first retrieves input instances that produce the same class probability distribution as the original instance-thus identifying elements within the same equivalence class-while the second discovers instances that yield a different class probability distribution, effectively navigating toward distinct equivalence classes"
  - [section] "SiMEC reconstructs the class of equivalence of the input via exploration of the input space by randomly selecting one of the eigenvectors related to the zero eigenvalue. On the opposite, in SiMExp, in order to move from a class of equivalence to another we consider the eigenvectors relative to the nonzero eigenvalues"
  - [corpus] Missing - corpus does not contain relevant content about navigation algorithms
- Break condition: If the spectral gap is not clear or if numerical errors cause small non-zero eigenvalues to be misclassified as zero, the algorithms may not correctly navigate equivalence classes.

### Mechanism 3
- Claim: Interpretation algorithm maps embeddings back to human-readable formats, enabling visual inspection of equivalence class exploration.
- Mechanism: The decoder reconstructs images or text from embeddings, allowing direct comparison between original and modified inputs to understand how equivalence classes relate to human perception.
- Core assumption: Either the embedding layer is invertible or a trained decoder can reconstruct the original modality with reasonable fidelity.
- Evidence anchors:
  - [abstract] "Finally, we demonstrate how the retrieved instances can be meaningfully interpreted by projecting their embeddings back into a human-readable format"
  - [section] "These methodologies facilitate the sequential acquisition of embedding matrices... The interpretation of an embedding vector depends on the operations performed by the Transformer's embedding module"
  - [corpus] Missing - corpus lacks content about interpretation methods
- Break condition: If the decoder introduces significant noise or if the reconstruction quality degrades with distance from the original embedding, the interpretation may not accurately reflect the equivalence class structure.

## Foundational Learning

- Concept: Riemannian geometry and differential manifolds
  - Why needed here: The framework treats neural networks as maps between manifolds and uses geometric concepts to analyze input space structure
  - Quick check question: What is the difference between a metric and a pseudometric in Riemannian geometry?
- Concept: Eigenvalue decomposition and spectral theory
  - Why needed here: The algorithms rely on decomposing the pullback metric to identify equivalence class structure
  - Quick check question: How do zero eigenvalues in the pullback metric relate to equivalence classes in the input space?
- Concept: Pullback of differential forms and metrics
  - Why needed here: The core mechanism uses pullback to transfer geometric information from output to input space
  - Quick check question: What is the mathematical operation that allows transferring geometric structure from the output space to the input space?

## Architecture Onboarding

- Component map: Input manifold → Transformer layers (smooth submersions) → Output manifold with metric → Jacobian → Pullback metric → Eigen decomposition → SiMEC/SiMExp navigation → Decoder for interpretation
- Critical path: Compute pullback metric → Eigen decomposition → Select appropriate eigenvectors → Generate new inputs → Decode and evaluate
- Design tradeoffs: Accuracy vs. computational cost (eigen decomposition is O(n³)), exploration speed vs. preservation of equivalence class (step size parameter η), decoder quality vs. interpretability
- Failure signatures: SiMEC produces outputs with different class probabilities than expected, SiMExp fails to change class predictions, decoder produces unrecognizable outputs, exploration becomes stuck in local regions
- First 3 experiments:
  1. Apply SiMEC to a simple 2D classification problem and visualize the equivalence classes
  2. Compare SiMEC vs. baseline Gaussian noise exploration on MNIST for class preservation
  3. Test interpretation quality by varying decoder architecture and measuring reconstruction fidelity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the SiMEC and SiMExp algorithms be scaled to handle very large Transformer models with embedding spaces of dimension 10^3 to 10^4?
- Basis in paper: [explicit] The authors mention that future work includes improving scalability for large-scale architectures, specifically noting that partial decompositions could be used.
- Why unresolved: The current implementation uses full eigendecomposition which is computationally expensive (O(n^3)), making it impractical for very large models.
- What evidence would resolve it: Implementation and testing of partial decomposition methods showing comparable performance to full eigendecomposition while significantly reducing computational complexity.

### Open Question 2
- Question: How does the choice of step size multiplier η affect the reliability and exploration pace of SiMEC versus SiMExp algorithms?
- Basis in paper: [explicit] The authors discuss that there is a trade-off between reliability and exploration pace, and they test different η values (1 and 10) in their experiments.
- Why unresolved: While the paper tests some η values, a comprehensive analysis of the parameter space and its effects on algorithm performance across different model architectures and datasets is not provided.
- What evidence would resolve it: Systematic experimentation varying η across a wider range of values, model architectures, and datasets, with quantitative measures of both exploration effectiveness and computational efficiency.

### Open Question 3
- Question: Can the Riemannian geometry framework be extended to provide local and task-agnostic explainability methods for both Computer Vision and Natural Language Processing tasks?
- Basis in paper: [explicit] The authors identify this as a future research direction, suggesting their methods could be applied to investigate Transformers' sensitivity and explainability with respect to input features.
- Why unresolved: The paper demonstrates the framework's potential but does not implement or validate it as an explainability method for real-world tasks.
- What evidence would resolve it: Development of explainability tools based on the framework that can provide interpretable explanations for model decisions across multiple CV and NLP tasks, validated through user studies or comparison with existing XAI methods.

## Limitations
- The framework assumes model layers behave as smooth submersions, which may not hold for all Transformer architectures
- Computational complexity of eigen-decomposition (O(n³)) limits scalability to high-dimensional input spaces
- Decoder quality is critical for interpretation but implementation details are sparse and reconstruction quality not thoroughly evaluated

## Confidence
- High confidence: The mathematical framework for pullback metric computation and its relationship to equivalence classes (supported by explicit equations and derivations)
- Medium confidence: The effectiveness of SiMEC and SiMExp algorithms in navigating equivalence classes (empirical results shown but limited ablation studies)
- Low confidence: The interpretability of the exploration results and the quality of the decoder implementation (decoder details are sparse and reconstruction quality not thoroughly evaluated)

## Next Checks
1. **Numerical stability verification**: Test the eigenvalue computation pipeline on synthetic manifolds with known equivalence structure to verify that zero eigenvalues correctly identify invariant directions.
2. **Decoder quality assessment**: Systematically vary decoder architecture and capacity for both ViT and BERT models, measuring reconstruction fidelity and exploring the trade-off between interpretability and exploration quality.
3. **Baseline comparison expansion**: Implement and compare against additional exploration baselines beyond Gaussian noise, such as gradient-based methods and adversarial attacks, to better contextualize the proposed approach's performance.