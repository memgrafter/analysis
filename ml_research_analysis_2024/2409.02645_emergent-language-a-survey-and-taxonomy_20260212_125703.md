---
ver: rpa2
title: 'Emergent Language: A Survey and Taxonomy'
arxiv_id: '2409.02645'
source_url: https://arxiv.org/abs/2409.02645
tags:
- language
- communication
- https
- agents
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey of emergent language
  (EL) research in artificial intelligence, focusing on multi-agent reinforcement
  learning settings. It reviews 181 scientific publications and introduces a systematic
  taxonomy for EL, categorizing communication settings, language games, language priors,
  and linguistic characteristics across six major structural levels: phonetics, phonology,
  morphology, syntax, semantics, and pragmatics.'
---

# Emergent Language: A Survey and Taxonomy

## Quick Facts
- **arXiv ID**: 2409.02645
- **Source URL**: https://arxiv.org/abs/2409.02645
- **Reference count**: 40
- **Primary result**: Comprehensive survey of 181 EL publications with systematic taxonomy covering communication settings, language games, language priors, and six linguistic structural levels (phonetics through pragmatics)

## Executive Summary
This survey provides a comprehensive overview of emergent language research in artificial intelligence, focusing on multi-agent reinforcement learning settings. The authors systematically reviewed 181 scientific publications and developed a unified framework that categorizes EL research across communication settings, language games, language priors, and linguistic characteristics spanning six structural levels from phonetics to pragmatics. The paper introduces standardized notation for existing evaluation metrics and identifies key research gaps, particularly in syntax evaluation and pragmatic language use. The resulting taxonomy and metrics framework serve as a foundation for future research in human-agent interaction and language alignment.

## Method Summary
The authors conducted a systematic literature review across multiple databases (ScienceDirect, IEEE Xplore, ACM Digital Library, WebOfScience, arXiv, and SemanticScholar) using keywords like "emergent language" and "emergent communication." They reviewed and categorized 181 publications based on communication settings, language games, language priors, and linguistic characteristics. The methodology involved developing a unified framework and notation for EL metrics by adapting existing metrics to this framework with appropriate references. The taxonomy was built around the semiotic cycle framework, organizing language characteristics along six major linguistic structural levels.

## Key Results
- Discrete communication channels dominate EL research due to their closer approximation to natural language structure and efficient information encoding
- Referential games are the most widely used language game paradigm in EL research
- Semantic metrics such as topographic similarity and zero-shot evaluation are most commonly used, while syntax evaluation remains underdeveloped
- The survey identifies significant research gaps in pragmatic language use and the need for standardized evaluation frameworks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Emergent language emerges because communication provides utility for coordination in multi-agent reinforcement learning
- Mechanism: Agents develop shared communication protocols when these protocols directly improve task performance and cooperation
- Core assumption: Communication must be rewarded or incentivized; without positive reinforcement, agents will not develop meaningful language
- Evidence anchors:
  - [abstract] "Early works examined narrowly defined questions regarding the characteristics of emergent communication (EC) via hand-crafted simulations [2–12]. These approaches mostly utilized supervised learning methods and non-situated settings, limiting them in their ability to examine the origins and development of complex linguistic features [2]. However, EL research experienced an upsurge in the period between 2016 and 2018 [13–20] with a focus on multi-agent reinforcement learning (MARL) approaches [21–32] to enable the examination of more complex features."
  - [section] "Accordingly, the level of cooperation is a defining element of the communication setting in EL research."
- Break condition: If the environment rewards do not depend on communication or if communication is penalized/costly without corresponding benefit

### Mechanism 2
- Claim: Discrete communication channels are preferred because they better approximate natural language structure and enable more efficient information encoding
- Mechanism: Discrete symbols allow agents to create combinatorial vocabularies that can express complex meanings with limited resources
- Core assumption: Discretization is necessary for establishing common ground and efficient communication
- Evidence anchors:
  - [abstract] "However, for EL research the discrete case is of particular importance, as it closely mirrors NL as we understand it [35]."
  - [section] "Table 5 provides an overview of the reviewed papers, categorized according to the continuous or discrete approach. Notably, some papers explore both approaches, providing valuable insights for researchers interested in the basic aspects of phonetics research in EL."
- Break condition: If continuous communication proves more effective for specific tasks or if discrete constraints become limiting

### Mechanism 3
- Claim: Language priors guide emergent language toward natural language-like structures by providing initial linguistic scaffolding
- Mechanism: By introducing structures from human NLs through supervised learning or divergence estimation, agents develop more interpretable and aligned communication
- Core assumption: Prior knowledge of NL structure can accelerate and direct the emergence of useful communication
- Evidence anchors:
  - [abstract] "A language prior is used to impose specific linguistic structures on the emerging language, making it easier to align with human NL and improve interpretability and performance."
  - [section] "EL research occasionally utilizes a concept known as a language prior to incorporate structures from human NLs into the emerging language."
- Break condition: If priors constrain language evolution too much, preventing agents from developing optimal communication for their specific tasks

## Foundational Learning

- Concept: Multi-agent reinforcement learning (MARL) fundamentals
  - Why needed here: The survey is built on MARL as the primary framework for studying emergent language
  - Quick check question: What distinguishes MARL from single-agent RL in terms of learning objectives and communication emergence?

- Concept: Semiotics and the semiotic cycle
  - Why needed here: The proposed notation system is organized around the semiotic cycle framework (cf. Figure 10)
  - Quick check question: How do the conceptualization and interpretation mappings connect the setting and language spaces in the semiotic cycle?

- Concept: Linguistic structural levels (phonetics through pragmatics)
  - Why needed here: The taxonomy categorizes language characteristics along these six major linguistic structural levels
  - Quick check question: Which linguistic level focuses on how context contributes to meaning and how language is used in interactions?

## Architecture Onboarding

- Component map: Environment → Observation → Meaning Representation → Message Production → Message Transmission → Message Comprehension → Action Selection → Environment feedback
- Critical path: This loop must be implemented for each agent role, with observation spaces, action spaces, and communication channels connecting agents to their environment
- Design tradeoffs: Discrete vs continuous communication channels (efficiency vs expressivity), fixed vs variable message length (simplicity vs flexibility), symmetric vs asymmetric agent roles (fairness vs specialization)
- Failure signatures: Agents develop non-communicative "gibberish" that achieves task completion without meaningful language (low positive signaling, low positive listening), agents fail to develop shared vocabulary (high inter-agent divergence), or agents develop overly complex communication that doesn't improve task performance
- First 3 experiments:
  1. Implement a simple referential game with two agents and discrete binary communication channel, measuring active words and message distinctness
  2. Add variable message length and measure average message length and compression metrics
  3. Introduce language prior and measure grounding divergence against natural language baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically quantify and compare the "goodness" of an emergent language system beyond task performance metrics?
- Basis in paper: [explicit] The paper states that "the optimality of these metrics is not straightforward" and that "what constitutes a 'good' EL system remains largely unanswered."
- Why unresolved: Current metrics focus on specific linguistic features (e.g., compositionality, consistency) but lack a holistic framework for evaluating overall language quality. There's no consensus on how to balance different metrics or account for trade-offs between linguistic properties.
- What evidence would resolve it: A standardized evaluation framework that incorporates multiple linguistic dimensions, with established weightings or optimization criteria for balancing competing features, validated across diverse EL systems.

### Open Question 2
- Question: What is the optimal balance between the evolution and acquisition approaches in emergent language development?
- Basis in paper: [explicit] The paper identifies the "Evolution-Acquisition Dilemma" where agents need autonomy to develop languages organically but must also align with natural language for human-agent communication.
- Why unresolved: The tension between organic language emergence and NL alignment creates a fundamental trade-off. Too much freedom may produce uninterpretable languages, while too much constraint may limit emergent properties and generalization.
- What evidence would resolve it: Empirical studies comparing different ratios of evolutionary versus acquisition-based approaches across various tasks, measuring both language interpretability and task performance.

### Open Question 3
- Question: How can we effectively evaluate the grounding of emergent languages without relying on ground-truth oracles or pre-trained language models?
- Basis in paper: [inferred] The paper notes that grounding metrics "require the existence of predefined ground-truth labels" and rely on "some form of oracle or a NL-grounded precursor," which limits their applicability.
- Why unresolved: Current grounding metrics are fundamentally limited by their dependence on external references, making them unsuitable for truly novel emergent languages or domains without established linguistic conventions.
- What evidence would resolve it: Development of metrics that assess grounding through behavioral and interaction-based measures, or methods to establish grounding through multi-agent consensus and environmental interaction without external references.

## Limitations
- The survey's reliance on published literature may introduce publication bias toward positive results and well-resourced research groups
- The taxonomy may not fully capture emerging EL paradigms that develop after the literature cutoff
- The standardized notation system assumes mathematical consistency across diverse implementations that may vary in practice

## Confidence
- **High Confidence**: The survey's core findings about discrete communication predominance and referential game prevalence are well-supported by the literature corpus
- **Medium Confidence**: Claims about language priors effectively guiding emergent language toward natural language structures require more empirical validation
- **Low Confidence**: Predictions about future research directions and their potential impact on human-agent interaction remain speculative

## Next Checks
1. Conduct a follow-up literature search using the same methodology to identify papers published after the original survey cutoff, validating whether identified trends persist
2. Implement a reproducibility study testing the standardized metric notation across at least three different EL research implementations to verify mathematical consistency
3. Design a controlled experiment comparing discrete versus continuous communication channels across multiple task types to empirically validate the claimed efficiency differences