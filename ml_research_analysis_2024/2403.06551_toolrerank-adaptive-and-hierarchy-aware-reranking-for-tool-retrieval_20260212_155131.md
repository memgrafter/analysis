---
ver: rpa2
title: 'ToolRerank: Adaptive and Hierarchy-Aware Reranking for Tool Retrieval'
arxiv_id: '2403.06551'
source_url: https://arxiv.org/abs/2403.06551
tags:
- retrieval
- tool
- tools
- reranking
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of retrieving external tools
  for large language models (LLMs), especially when dealing with unseen tools. Existing
  methods lack consideration for the differences between seen and unseen tools, as
  well as the hierarchical structure of the tool library, leading to suboptimal performance.
---

# ToolRerank: Adaptive and Hierarchy-Aware Reranking for Tool Retrieval

## Quick Facts
- **arXiv ID**: 2403.06551
- **Source URL**: https://arxiv.org/abs/2403.06551
- **Reference count**: 0
- **Primary result**: Improves Recall@5 by 5.0 points over baselines on ToolBench dataset

## Executive Summary
This paper addresses the challenge of retrieving external tools for large language models (LLMs), particularly when dealing with unseen tools. The authors propose ToolRerank, which combines Adaptive Truncation and Hierarchy-Aware Reranking to improve retrieval quality. The method shows significant improvements over baselines, achieving 5.0, 3.9, and 4.8 Recall@5 point gains on average across six test datasets.

## Method Summary
ToolRerank is an adaptive and hierarchy-aware reranking method for tool retrieval that addresses limitations in existing approaches. It consists of two key components: Adaptive Truncation, which truncates retrieval results differently for seen and unseen tools based on their performance characteristics; and Hierarchy-Aware Reranking, which applies different reranking strategies depending on whether queries require single or multiple tools. The method uses a dual-encoder retriever for initial retrieval, followed by a cross-encoder reranker for fine-grained matching, and incorporates a classifier to distinguish query types.

## Key Results
- Outperforms baselines by 5.0 Recall@5 points on average across six test datasets
- Improves retrieval quality leading to better execution results by LLM (Pass Rate, Win Rate)
- Shows particular effectiveness for unseen tools where traditional methods struggle

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive Truncation improves recall for both seen and unseen tools by truncating results at different positions.
- Mechanism: The method truncates the coarse-grained retrieval results based on whether a tool is seen or unseen in training data. Seen tools use a smaller truncation threshold (ms), while unseen tools use a larger one (mu), allowing the reranker to focus on relevant candidates.
- Core assumption: The reranker performs differently for seen and unseen tools, with seen tools benefiting from fewer candidates and unseen tools needing more.
- Evidence anchors:
  - [abstract] "Adaptive Truncation, which truncates the retrieval results related to seen and unseen tools at different positions"
  - [section] "Figure 1(a), the reranker performs better with fewer candidates for seen tools and with more candidates for unseen tools"
  - [corpus] Weak evidence - only 8 related papers found, none directly addressing adaptive truncation strategies
- Break condition: If the reranker's performance curve is similar for seen and unseen tools, or if the distribution of correct APIs doesn't vary significantly between seen and unseen tools in the top-k positions.

### Mechanism 2
- Claim: Hierarchy-Aware Reranking improves precision by concentrating results for single-tool queries and diversifying for multi-tool queries.
- Mechanism: The method uses a classifier to distinguish single-tool vs multi-tool queries, then applies different reranking algorithms. For single-tool queries, it concentrates results within the same tool family. For multi-tool queries, it diversifies across functionally different tools.
- Core assumption: The tool library has a hierarchical structure where tools contain multiple APIs, and queries can be classified as needing either single or multiple tools.
- Evidence anchors:
  - [abstract] "Hierarchy-Aware Reranking, which makes retrieval results more concentrated for single-tool queries and more diverse for multi-tool queries"
  - [section] "Figure 1(b), the tool library may have a hierarchy where a tool may have multiple APIs"
  - [corpus] Weak evidence - only 8 related papers found, none directly addressing hierarchy-aware reranking for tool retrieval
- Break condition: If queries cannot be reliably classified as single-tool vs multi-tool, or if the tool hierarchy is flat or irrelevant to query resolution.

### Mechanism 3
- Claim: Cross-encoder reranking improves over dual-encoder by allowing fine-grained interaction between query and document.
- Mechanism: The method uses a cross-encoder reranker after the initial dual-encoder retrieval, allowing token-level matching and semantic understanding that dual-encoder's vector similarity misses.
- Core assumption: Fine-grained interaction between query and document representations improves ranking quality compared to vector similarity alone.
- Evidence anchors:
  - [abstract] "a more computationally intensive but effective model is used to refine the retrieval results"
  - [section] "they lack fine-grained interaction between the query and the documents"
  - [corpus] Weak evidence - only 8 related papers found, but reranking is a well-established IR technique
- Break condition: If the computational cost of cross-encoder reranking outweighs the quality gains, or if the dual-encoder already captures sufficient semantic similarity.

## Foundational Learning

- Concept: Dual-encoder vs cross-encoder retrievers
  - Why needed here: ToolRerank builds on dual-encoder retrieval but adds cross-encoder reranking. Understanding the tradeoff between efficiency (dual-encoder) and effectiveness (cross-encoder) is crucial.
  - Quick check question: Why might a dual-encoder retriever be insufficient for tool retrieval tasks compared to a cross-encoder?

- Concept: Hierarchical tool libraries
  - Why needed here: The method assumes tools contain multiple APIs and queries can be single-tool or multi-tool. Understanding this hierarchy is essential for implementing Hierarchy-Aware Reranking.
  - Quick check question: How does the hierarchical structure of tools (tool → multiple APIs) affect the design of a reranking strategy?

- Concept: Seen vs unseen tool distinction
  - Why needed here: Adaptive Truncation specifically handles seen and unseen tools differently. Understanding why this distinction matters requires knowledge of few-shot learning and domain adaptation.
  - Quick check question: Why might retrieval performance differ between tools seen during training versus completely unseen tools?

## Architecture Onboarding

- Component map: Dual-encoder retriever → Adaptive Truncation → Cross-encoder reranker → Hierarchy-Aware Reranking → LLM execution
- Critical path: User query → Dual-encoder retriever → Adaptive Truncation → Cross-encoder reranker → Final ranking → Top-k APIs to LLM
- Design tradeoffs: Computational cost vs retrieval quality (cross-encoder is more expensive but more accurate), recall vs precision (Adaptive Truncation balances these), diversity vs concentration (Hierarchy-Aware Reranking handles this tradeoff)
- Failure signatures: Poor performance on unseen tools suggests Adaptive Truncation thresholds need adjustment; misclassification of query types indicates classifier issues; low win rate suggests LLM integration problems
- First 3 experiments:
  1. Test Adaptive Truncation with different ms/mu values on development set to find optimal thresholds
  2. Evaluate classifier accuracy on distinguishing single-tool vs multi-tool queries
  3. Compare recall@5 with and without cross-encoder reranking on both seen and unseen test sets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ToolRerank change when applied to different types of tools, such as tools with different levels of complexity or tools that require different types of inputs?
- Basis in paper: [inferred] The paper mentions that the tool library may have a hierarchy and that some queries should be resolved using different APIs of a single tool (single-tool queries), while others should be resolved using APIs of different tools (multi-tool queries). However, the paper does not explore how ToolRerank performs with different types of tools.
- Why unresolved: The paper focuses on evaluating ToolRerank on the ToolBench dataset, which may not cover all types of tools and their complexities. Therefore, it is unclear how ToolRerank would perform with other types of tools.
- What evidence would resolve it: Experiments comparing ToolRerank's performance on different types of tools, such as tools with varying levels of complexity or tools that require different types of inputs.

### Open Question 2
- Question: How does the performance of ToolRerank change when applied to different types of user queries, such as queries with different levels of specificity or queries that require different types of reasoning?
- Basis in paper: [inferred] The paper mentions that some queries should be resolved using different APIs of a single tool (single-tool queries), while others should be resolved using APIs of different tools (multi-tool queries). However, the paper does not explore how ToolRerank performs with different types of user queries.
- Why unresolved: The paper focuses on evaluating ToolRerank on the ToolBench dataset, which may not cover all types of user queries and their complexities. Therefore, it is unclear how ToolRerank would perform with other types of user queries.
- What evidence would resolve it: Experiments comparing ToolRerank's performance on different types of user queries, such as queries with varying levels of specificity or queries that require different types of reasoning.

### Open Question 3
- Question: How does the performance of ToolRerank change when applied to different types of large language models (LLMs), such as LLMs with different sizes or LLMs with different capabilities?
- Basis in paper: [inferred] The paper mentions that ToolRerank improves the quality of the retrieval results, leading to better execution results generated by the LLM. However, the paper does not explore how ToolRerank performs with different types of LLMs.
- Why unresolved: The paper focuses on evaluating ToolRerank on a specific LLM, which may not cover all types of LLMs and their capabilities. Therefore, it is unclear how ToolRerank would perform with other types of LLMs.
- What evidence would resolve it: Experiments comparing ToolRerank's performance on different types of LLMs, such as LLMs with varying sizes or LLMs with different capabilities.

## Limitations

- The method relies heavily on the hierarchical structure of the tool library, which may not generalize well to tool libraries with different organizational principles or flat structures.
- The distinction between seen and unseen tools requires careful calibration of truncation thresholds that may not transfer across different domains or tool distributions.
- The computational overhead of cross-encoder reranking may limit scalability for very large tool libraries or real-time applications.

## Confidence

- **High Confidence**: The effectiveness of cross-encoder reranking over dual-encoder retrieval is well-established in information retrieval literature, and the improvement in NDCG@5 and Recall@5 metrics is directly measurable and reported.
- **Medium Confidence**: The adaptive truncation strategy's effectiveness depends on the specific distribution of correct APIs across positions in the retrieval results, which may vary by domain. The reported improvement of 5.0 Recall@5 points is impressive but requires validation on additional datasets.
- **Low Confidence**: The assumption that tool libraries universally exhibit hierarchical structures with clear single-tool vs multi-tool query distinctions may not hold across all domains. The method's performance on highly diverse or unstructured tool collections remains uncertain.

## Next Checks

1. **Cross-Domain Generalization Test**: Apply ToolRerank to a different tool retrieval domain (e.g., software development APIs, medical diagnostic tools) to verify whether the adaptive truncation thresholds and hierarchy-aware reranking generalize beyond the ToolBench dataset. Measure performance degradation and identify domain-specific adjustments needed.

2. **Ablation Study on Query Classification**: Systematically remove or modify the single-tool vs multi-tool query classifier to determine its actual contribution to overall performance. Compare results using: (a) perfect classification, (b) random classification, (c) always treating as single-tool queries, and (d) always treating as multi-tool queries.

3. **Computational Cost Analysis**: Benchmark the end-to-end latency of ToolRerank versus baseline dual-encoder methods across varying tool library sizes (10K, 50K, 100K APIs). Calculate the tradeoff between improved recall and increased computational cost to determine practical deployment thresholds.