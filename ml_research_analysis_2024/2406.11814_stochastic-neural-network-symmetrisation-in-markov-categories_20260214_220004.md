---
ver: rpa2
title: Stochastic Neural Network Symmetrisation in Markov Categories
arxiv_id: '2406.11814'
source_url: https://arxiv.org/abs/2406.11814
tags:
- markov
- example
- group
- which
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of symmetrising neural networks
  along group homomorphisms in Markov categories, which allows for stochastic outputs.
  The core method uses a left adjoint functor to map H-equivariant morphisms to G-equivariant
  ones, with a weaker condition than requiring a full left adjoint.
---

# Stochastic Neural Network Symmetrisation in Markov Categories

## Quick Facts
- arXiv ID: 2406.11814
- Source URL: https://arxiv.org/abs/2406.11814
- Reference count: 40
- Key outcome: Provides a flexible framework for symmetrising neural networks along group homomorphisms in Markov categories, extending from deterministic to stochastic settings with improved performance on synthetic tasks.

## Executive Summary
This paper introduces a novel framework for symmetrising neural networks in Markov categories that allows for stochastic outputs. The approach uses a left adjoint functor to map H-equivariant morphisms to G-equivariant ones via group homomorphisms, providing a more general methodology than existing deterministic techniques. The framework recovers canonicalisation and averaging methods as special cases while extending to stochastic symmetrisation. Tested on a synthetic matrix inversion task, the recursive symmetrisation approach shows improved performance over intrinsic equivariant neural networks when the group action is too complex for intrinsic methods.

## Method Summary
The method uses a group homomorphism φ: H → G to define a restriction functor Rφ that maps G-equivariant morphisms to H-equivariant ones. A left adjoint E to Rφ provides a bijection between morphisms in different categories, which is composed with a precomposition morphism ω to obtain the symmetrisation procedure. The framework includes stability conditions ensuring the procedure restricts to identity on already equivariant morphisms, and allows recursive composition of multiple symmetrisation steps to build complex equivariance properties. The approach is tested on synthetic matrix inversion tasks using recursive symmetrisation with Haar measure base cases.

## Key Results
- The recursive symmetrisation approach outperforms intrinsic equivariant neural networks on synthetic matrix inversion tasks when group actions are complex
- Existing deterministic symmetrisation methods are recovered as special cases within the framework
- Stochastic symmetrisation provides flexibility beyond deterministic approaches while maintaining theoretical guarantees
- The framework successfully handles group actions involving GL(d,R) and O(d) in matrix inversion tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symmetrisation can be performed using a left adjoint functor to map H-equivariant morphisms to G-equivariant ones.
- Mechanism: The approach uses a functor Rφ that maps G-equivariant morphisms to H-equivariant ones by restricting actions via a group homomorphism φ. A left adjoint E would provide a bijection CH(RφX, RφY) ≅ CG(ERφX, Y), which is then composed with a precomposition morphism to obtain the desired symmetrisation procedure.
- Core assumption: The functor Rφ admits a left adjoint E that exists for all groups and actions when C is the Markov category of topological spaces and continuous Markov kernels.
- Evidence anchors:
  - [abstract]: "Our approach recovers existing canonicalisation and averaging techniques for symmetrising deterministic models, and extends to provide a novel methodology for stochastic symmetrisation."
  - [section 5.1]: "Suppose the restriction functor Rφ admits a left adjoint E. For any pair of objects X and Y in CG, this yields a bijection between the morphisms in CH and CG."
  - [corpus]: Weak - corpus neighbors do not provide specific evidence about the left adjoint mechanism.
- Break condition: The left adjoint E does not exist, or the bijection does not preserve composition.

### Mechanism 2
- Claim: Symmetrisation procedures can be made stable, ensuring they restrict to the identity on already equivariant morphisms.
- Mechanism: The symmetrisation procedure symγ is defined using a precomposition morphism ω of the form (38). This restriction ensures stability, which means symγ(k) = k for all k already G-equivariant. Stability implies surjectivity and idempotence.
- Core assumption: The choice of precomposition morphism ω follows the specific form (38), which is sufficient for stability in positive Markov categories.
- Evidence anchors:
  - [section 4.4]: "Definition 4.5. A symmetrisation procedure sym : CG(RφX, RφY) → CG(X, Y) is stable if it satisfies sym(k) = k for all k : X → Y in CG."
  - [section 5.3]: "A default choice for this that applies for any (X, αX) in CG is to take γ as the Haar measure of G."
  - [corpus]: Weak - corpus neighbors do not provide specific evidence about stability conditions.
- Break condition: The category C is not positive, or the precomposition morphism does not follow the required form.

### Mechanism 3
- Claim: Recursive symmetrisation allows building complex equivariance properties by composing multiple symmetrisation procedures.
- Mechanism: Multiple homomorphisms K → H → G can be composed, with symmetrisation procedures applied sequentially. This allows starting with a K-equivariant morphism and ending with a G-equivariant one. The recursive approach also applies to obtaining the precomposition morphism γ itself.
- Core assumption: The composition of symmetrisation procedures preserves stability and surjectivity, though idempotence may not be preserved.
- Evidence anchors:
  - [section 4.5]: "Symmetrisation procedures may be applied in sequence. For example, suppose we have two homomorphisms K → H → G."
  - [section 5.5]: "Various existing methods for deterministic symmetrisation can be expressed as instances of the framework we have described here."
  - [corpus]: Weak - corpus neighbors do not provide specific evidence about recursive composition.
- Break condition: The composition does not preserve the required properties, or the homomorphisms do not compose as expected.

## Foundational Learning

- Concept: Markov categories
  - Why needed here: The paper uses Markov categories as an abstraction framework to reason about stochastic neural networks without measure-theoretic details. Understanding Markov categories is essential to follow the theoretical development.
  - Quick check question: What are the key components of a Markov category, and how do they relate to Markov kernels?

- Concept: Group actions and equivariance
  - Why needed here: The paper deals with symmetrising neural networks along group homomorphisms, requiring a deep understanding of group actions and equivariance in abstract settings. This concept is central to formulating the problem and solution.
  - Quick check question: How does the definition of equivariance in Markov categories generalize the classical notion from group theory?

- Concept: Adjunctions and functors
  - Why needed here: The symmetrisation methodology relies on adjunctions between categories of equivariant morphisms. Understanding how functors and adjunctions work is crucial for grasping the theoretical framework.
  - Quick check question: What is the relationship between the restriction functor Rφ and its left adjoint E, and how does this enable the symmetrisation procedure?

## Architecture Onboarding

- Component map: The symmetrisation architecture consists of: 1) a group homomorphism φ: H → G, 2) a restriction functor Rφ, 3) a precomposition morphism ω (often parameterised by γ), 4) the symmetrisation procedure symγ that combines these components. The architecture can be applied recursively for complex equivariance properties.
- Critical path: 1) Define the group homomorphism φ and actions on objects, 2) Implement the restriction functor Rφ, 3) Choose or construct a precomposition morphism ω, 4) Apply the symmetrisation procedure symγ to convert H-equivariant morphisms to G-equivariant ones.
- Design tradeoffs: The choice between deterministic and stochastic symmetrisation affects the expressiveness and computational requirements. Recursive symmetrisation provides more flexibility but increases complexity. The stability of the procedure ensures it doesn't modify already equivariant morphisms but may limit expressiveness in some cases.
- Failure signatures: If the symmetrisation procedure fails to produce equivariant outputs, check: 1) the homomorphism φ is correctly defined, 2) the actions are properly restricted via Rφ, 3) the precomposition morphism ω has the correct form, 4) the category C admits the necessary orbit maps.
- First 3 experiments:
  1. Implement symmetrisation for a simple finite group (like S_n) on a basic neural network architecture, verifying equivariance through permutation tests.
  2. Compare deterministic vs. stochastic symmetrisation on a small dataset with known symmetries, measuring performance and robustness.
  3. Apply recursive symmetrisation along multiple homomorphisms (e.g., I → SO(2) → SE(2)) to build up complex equivariance properties, testing on a synthetic geometric task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Meas admit all orbit maps? The paper shows that TopStoch does, but the Meas case remains open.
- Basis in paper: Explicit - Section 3.6 states "We do not know whether Meas or Stoch admit all orbit maps."
- Why unresolved: The coequalizer construction exists in Meas but the preservation condition under the functor (−) ⊗ Y is not proven.
- What evidence would resolve it: Either a proof that the coequalizer in Meas is preserved by (−) ⊗ Y, or a counterexample showing it fails for some group action.

### Open Question 2
- Question: Can the recursive symmetrization approach be made more expressive by allowing multiple recursive steps or different base cases?
- Basis in paper: Explicit - Section 5.5 mentions "we can set γ := symγ1(m) for some m" but doesn't explore this fully.
- Why unresolved: The paper only briefly mentions this recursive approach and doesn't provide empirical comparisons.
- What evidence would resolve it: Empirical studies comparing single vs multiple recursive steps, and comparisons with different base case choices across various tasks.

### Open Question 3
- Question: How does stochastic symmetrization perform on real-world datasets compared to deterministic methods?
- Basis in paper: Inferred - Section 7 only provides synthetic matrix inversion results.
- Why unresolved: The paper focuses on theoretical development and one synthetic example, without testing on practical ML benchmarks.
- What evidence would resolve it: Performance evaluations on standard ML datasets (image classification, graph neural networks, etc.) comparing stochastic vs deterministic symmetrization.

## Limitations
- Theoretical framework relies heavily on abstract categorical constructions that may be difficult to implement in practice
- Empirical validation is limited to a single synthetic matrix inversion task, lacking real-world dataset testing
- Recursive symmetrisation introduces multiple composition steps that could accumulate numerical errors

## Confidence
**High Confidence**: The core mathematical framework involving restriction functors, adjunctions, and precomposition morphisms is well-established and internally consistent. The recovery of existing deterministic symmetrisation methods as special cases is mathematically rigorous.

**Medium Confidence**: The extension to stochastic symmetrisation and the recursive composition approach, while theoretically sound, requires more empirical validation across diverse tasks. The stability conditions and their implications for practical implementation are reasonably well-supported but could benefit from additional theoretical analysis.

**Low Confidence**: The empirical results from the matrix inversion task, while promising, are based on a single synthetic example. The scalability of the approach to larger, more complex tasks with multiple group actions remains unverified.

## Next Checks
1. **Empirical Scalability Test**: Apply the recursive symmetrisation framework to a real-world geometric computer vision task (e.g., point cloud processing with SE(3) symmetries) and compare against established equivariant architectures across multiple datasets.

2. **Numerical Stability Analysis**: Conduct a systematic study of the numerical stability of the recursive symmetrisation procedure across different group compositions, measuring error accumulation and identifying optimal composition strategies.

3. **Theoretical Necessity Proof**: Investigate whether orbit maps are truly necessary for the symmetrisation procedure or if weaker conditions suffice, potentially expanding the applicability of the framework to non-positive Markov categories.