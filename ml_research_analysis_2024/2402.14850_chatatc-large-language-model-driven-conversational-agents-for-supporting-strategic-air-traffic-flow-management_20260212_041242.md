---
ver: rpa2
title: 'CHATATC: Large Language Model-Driven Conversational Agents for Supporting
  Strategic Air Traffic Flow Management'
arxiv_id: '2402.14850'
source_url: https://arxiv.org/abs/2402.14850
tags:
- traffic
- hatatc
- such
- data
- management
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed CHATATC, a large language model trained on
  over 80,000 historical Ground Delay Program (GDP) issuances from 2000-2023 to assist
  air traffic managers in strategic traffic flow management. Using data from the FAA's
  Operational Information System, the researchers fine-tuned GPT-4 via Maizey to create
  a conversational agent that can rapidly retrieve and summarize GDP information through
  natural language queries.
---

# CHATATC: Large Language Model-Driven Conversational Agents for Supporting Strategic Air Traffic Flow Management

## Quick Facts
- **arXiv ID**: 2402.14850
- **Source URL**: https://arxiv.org/abs/2402.14850
- **Reference count**: 36
- **Primary result**: Developed CHATATC, an LLM fine-tuned on 80,000+ historical GDP issuances to assist air traffic managers with strategic traffic flow management

## Executive Summary
This study presents CHATATC, a conversational agent built by fine-tuning GPT-4 on over 80,000 historical Ground Delay Program (GDP) issuances from 2000-2023. The system enables air traffic managers to rapidly retrieve and summarize GDP information through natural language queries, providing accurate responses about program rates, durations, and reasons for specific airports. While demonstrating strong performance on standard queries, the model struggles with superlative questions and requires further evaluation before broader deployment.

## Method Summary
The researchers collected historical GDP data from the FAA's Operational Information System, parsing XML files to extract raw text and parameters. They fine-tuned GPT-4 via the Maizey platform using this data, creating separate instances for specific airports (SFO and EWR). The team also developed a graphical user interface prototype to enable user interaction with the model, organizing key information in a user-friendly format. Temperature hyperparameter tuning (set to 0.2) and system prompt engineering were employed to optimize response quality.

## Key Results
- CHATATC successfully retrieves specific GDP parameters (rates, durations, reasons) for individual airports when queried with standard questions
- The system provides more consistently formatted outputs that can be easily parsed by users compared to traditional database queries
- Temperature hyperparameter adjustment showed minimal impact on accuracy between 0.2 and 1.2 settings for ATM queries

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fine-tuning GPT-4 on historical GDP data enables accurate retrieval of specific program parameters for individual airports
- **Mechanism**: The model learns the structured format of GDP issuances and can extract relevant information when prompted with airport-specific queries
- **Core assumption**: Historical GDP text contains sufficient patterns for the model to learn extraction patterns
- **Evidence anchors**: [abstract] trained on over 80,000 historical GDP issuances; [section] LLM accurately extracts parameters from GDP issuance; [corpus] Weak - focuses on conversational agents generally

### Mechanism 2
- **Claim**: Temperature hyperparameter tuning controls output creativity vs. factual consistency
- **Mechanism**: Lower temperature values (0.2) produce more deterministic, fact-based responses while higher values introduce more variation
- **Core assumption**: Temperature directly controls the sampling distribution from the model's probability outputs
- **Evidence anchors**: [section] chose temperature parameter of 0.2; [section] no significant difference in accuracy between 0.2 and 1.2; [corpus] Weak - doesn't address temperature effects specifically

### Mechanism 3
- **Claim**: Structured system prompts guide LLM to format responses in airport manager-friendly formats
- **Mechanism**: Explicit instructions in the system prompt ensure consistent output structure (bulleted lists, key parameters) that matches user workflow
- **Core assumption**: LLMs can reliably follow complex formatting instructions when provided in system prompts
- **Evidence anchors**: [section] relied on prompt engineering by adding instructions; [section] CHATATC provided consistently formatted outputs; [corpus] Weak - focuses on conversational agents generally

## Foundational Learning

- **Concept**: Ground Delay Program (GDP) structure and parameters
  - **Why needed here**: Understanding GDP components (rates, durations, reasons) is essential for evaluating model outputs and designing effective prompts
  - **Quick check question**: What are the key parameters that define a Ground Delay Program, and why are they important for air traffic management?

- **Concept**: Fine-tuning vs. in-context learning for LLMs
  - **Why needed here**: The paper uses both approaches; understanding their differences is crucial for interpreting results and making design decisions
  - **Quick check question**: What is the fundamental difference between fine-tuning an LLM on a dataset versus using in-context learning with prompt examples?

- **Concept**: Temperature hyperparameter in LLM sampling
  - **Why needed here**: Temperature controls output diversity and is explicitly tuned in the experiments
  - **Quick check question**: How does adjusting the temperature parameter affect the distribution of words sampled by an LLM?

## Architecture Onboarding

- **Component map**: Data ingestion (XML parsing) -> Fine-tuning pipeline (Maizey with GPT-4) -> Query interface (U-M GPT) -> GUI prototype (web interface) -> Evaluation framework (manual verification)

- **Critical path**: 1. Data collection and preprocessing (XML â†’ text files) 2. Fine-tuning GPT-4 on airport-specific GDP subsets 3. Temperature parameter optimization 4. System prompt engineering for consistent formatting 5. GUI development for user interaction 6. Evaluation and baseline establishment

- **Design tradeoffs**: 
  - Model size vs. inference speed: GPT-4 provides accuracy but may be slow for real-time use
  - Fine-tuning scope: Airport-specific models provide better context but require more resources
  - Temperature setting: Low values ensure accuracy but may miss relevant related information
  - Data recency: Model trained through 2023 but needs periodic updates

- **Failure signatures**: 
  - Incorrect GDP dates despite date information being present in raw text
  - Failure on superlative questions (highest/lowest values)
  - Inconsistent formatting when system prompts are modified
  - Hallucinations when queried about airports not in training data

- **First 3 experiments**:
  1. Test fine-tuned model on held-out GDP data to establish baseline accuracy for rate/duration extraction
  2. Compare in-context learning vs. fine-tuning performance on same query set
  3. Evaluate temperature sensitivity by testing accuracy across range (0.1 to 1.0) on standardized queries

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal temperature hyperparameter setting for CHATATC to balance accuracy and user-friendliness when generating responses about GDP parameters?
- **Basis in paper**: [explicit] The paper discusses experimenting with temperature hyperparameter values and notes no significant difference in accuracy for ATM queries
- **Why unresolved**: Authors acknowledge this needs more systematic testing with larger user groups
- **What evidence would resolve it**: Controlled experiments comparing user satisfaction, response accuracy, and retrieval effectiveness across multiple temperature settings with statistically significant sample sizes of actual Traffic Managers

### Open Question 2
- **Question**: How can baseline performance metrics be established for evaluating LLM-based conversational agents in aviation settings to enable meaningful comparisons between different approaches?
- **Basis in paper**: [explicit] Authors explicitly state need to establish baseline by which to compare and evaluate performance of large language models in aviation
- **Why unresolved**: Paper does not propose specific metrics or methodologies for creating such baselines
- **What evidence would resolve it**: Development and validation of standardized evaluation frameworks that include accuracy metrics, response time, user satisfaction scores, and safety-critical performance measures specific to aviation applications

### Open Question 3
- **Question**: What GUI design features most effectively support Traffic Managers in retrieving and interpreting GDP information while minimizing cognitive load during high-stress operational periods?
- **Basis in paper**: [explicit] Authors developed a GUI wireframe and mention ongoing research will investigate dynamic GUI outputs
- **Why unresolved**: Current GUI design is described as a wireframe without empirical testing
- **What evidence would resolve it**: A/B testing of different GUI layouts with actual Traffic Managers under simulated operational conditions, measuring task completion time, error rates, and subjective workload assessments

## Limitations

- Model struggles with superlative questions (highest/lowest values), indicating incomplete semantic understanding of comparative relationships
- Current validation relies heavily on manual verification, which may not scale for broader deployment
- Claims about system's readiness for broader deployment are premature without baseline performance evaluation and expanded user testing

## Confidence

- **High Confidence**: The core mechanism of fine-tuning GPT-4 on historical GDP data for accurate parameter extraction is well-supported by the evidence
- **Medium Confidence**: The effectiveness of temperature tuning and system prompt engineering is demonstrated, but limited variation in accuracy suggests these optimizations may have marginal impact
- **Low Confidence**: Claims about the system's readiness for broader deployment are premature given the need for baseline evaluation, expanded user testing, and further prompt engineering

## Next Checks

1. **Benchmark Against Traditional Methods**: Compare CHATATC's performance against conventional database queries for the same set of GDP parameters to establish baseline accuracy and efficiency metrics
2. **User Experience Testing**: Conduct formal user studies with air traffic managers to evaluate the GUI prototype's effectiveness in real-world workflows and identify usability barriers
3. **Edge Case Analysis**: Systematically test the model's performance on edge cases including superlative queries, ambiguous language, and GDPs with missing or conflicting information to better understand failure modes