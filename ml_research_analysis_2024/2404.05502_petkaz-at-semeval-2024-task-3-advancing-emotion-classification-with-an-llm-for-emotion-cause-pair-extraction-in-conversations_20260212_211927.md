---
ver: rpa2
title: 'PetKaz at SemEval-2024 Task 3: Advancing Emotion Classification with an LLM
  for Emotion-Cause Pair Extraction in Conversations'
arxiv_id: '2404.05502'
source_url: https://arxiv.org/abs/2404.05502
tags:
- emotion
- utterance
- cause
- task
- utterances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a system for extracting emotion-cause pairs
  from conversational data, addressing the SemEval-2024 Task 3. The approach combines
  a fine-tuned GPT-3.5 model for emotion classification with a BiLSTM-based neural
  network for cause detection.
---

# PetKaz at SemEval-2024 Task 3: Advancing Emotion Classification with an LLM for Emotion-Cause Pair Extraction in Conversations

## Quick Facts
- arXiv ID: 2404.05502
- Source URL: https://arxiv.org/abs/2404.05502
- Reference count: 6
- Primary result: Achieved weighted-average proportional F1 score of 0.264, ranking 2nd out of 15 teams

## Executive Summary
This paper presents a system for extracting emotion-cause pairs from conversational data in the SemEval-2024 Task 3. The approach combines a fine-tuned GPT-3.5 model for emotion classification with a BiLSTM-based neural network for cause detection. The model achieved competitive performance with a weighted-average proportional F1 score of 0.264, placing second among 15 participating teams. Key challenges identified include accurately identifying causes of anger and the correlation between cause distance from the emotion and model performance.

## Method Summary
The method employs a two-stage pipeline: first, GPT-3.5 is fine-tuned for emotion classification using a specific prompt format; second, a BiLSTM-based neural network processes BERT embeddings of utterances to detect causes. The cause detection uses full utterances rather than specific spans to avoid annotation inconsistencies. The system processes the Friends TV series dataset with a 9:1 train-dev split, training the cause extractor for 200 epochs with Adam optimizer.

## Key Results
- Achieved weighted-average proportional F1 score of 0.264 on the test set
- Ranked 2nd out of 15 participating teams
- Fine-tuning GPT-3.5 significantly outperformed zero-shot and few-shot settings for emotion classification
- Model showed particular difficulty in accurately identifying causes of anger

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning GPT-3.5 for emotion classification improves performance over zero-shot and few-shot settings. Fine-tuning adapts the model's weights to the specific emotion labels and conversational style of the Friends dataset, enabling better generalization to domain-specific emotional cues. Core assumption: The emotional language in the Friends TV show differs enough from general web data that pre-training alone is insufficient. Evidence: "fine-tuning yields the best results on all emotion types and overall." Break condition: If the domain shifts significantly (e.g., medical conversations), fine-tuning on the original dataset would not generalize well.

### Mechanism 2
BiLSTM-based neural network effectively captures conversational context for cause extraction. BiLSTM layers process utterance embeddings in both forward and backward directions, allowing the model to encode dependencies from both past and future utterances relative to the target. Core assumption: The emotional cause is often found in nearby utterances, and bidirectional context is sufficient for identifying them. Evidence: "Bidirectional LSTM... can preserve context information in sequential settings using the content of the previous hidden state in encoding the current one." Break condition: If causes are highly non-local or depend on speaker identity changes, simple BiLSTM context may not be enough.

### Mechanism 3
Using full utterance as cause instead of span detection improves reliability given dataset annotation inconsistencies. By avoiding fine-grained span extraction, the model sidesteps ambiguous annotations (e.g., missing punctuation or partial utterance labeling). Core assumption: The dataset's span-level annotations are noisy or inconsistent enough that coarse utterance-level labels yield more stable results. Evidence: "We have decided not to extract specific spans... these spans often defy straightforward explanations, even from a human annotator perspective." Break condition: If future datasets provide cleaner span annotations, this simplification could lose fine-grained precision.

## Foundational Learning

- Concept: Bidirectional LSTM (BiLSTM) operation
  - Why needed here: The task requires understanding both preceding and following context to detect causes of emotions in conversations.
  - Quick check question: If an utterance at position t has a cause at position t-2, which direction in the BiLSTM captures that dependency first?

- Concept: Fine-tuning vs zero-shot learning
  - Why needed here: The Friends dataset contains domain-specific language and emotional expressions that require model adaptation.
  - Quick check question: What metric in the paper shows that fine-tuning outperforms zero-shot for emotion classification?

- Concept: Weighted-average F1 score for multi-label classification
  - Why needed here: The evaluation combines multiple emotion types with different class frequencies, requiring a balanced metric.
  - Quick check question: Why might macro-F1 be less appropriate than weighted-average F1 for this task?

## Architecture Onboarding

- Component map: GPT-3.5 (fine-tuned) → emotion classification → BERT-base-uncased → utterance embeddings → BiLSTM layers → context enrichment → Feed-forward network → binary cause detection → One-hot speaker vectors → speaker identity encoding

- Critical path: GPT-3.5 emotion classification → BiLSTM cause extraction → F1 evaluation
  - Bottleneck: Emotion classification accuracy directly limits cause extraction performance.

- Design tradeoffs:
  - Span extraction skipped to avoid annotation ambiguity vs potential precision loss
  - Fine-tuning GPT-3.5 adds cost but improves accuracy vs using zero-shot
  - BiLSTM chosen over transformers for simplicity vs potential context modeling depth

- Failure signatures:
  - Low emotion classification F1 → poor cause extraction downstream
  - High neutral classification error → most utterances misclassified
  - BiLSTM overfitting → poor generalization to new dialogs

- First 3 experiments:
  1. Run zero-shot GPT-3.5 emotion classification on dev set and compare to fine-tuned
  2. Replace BiLSTM with simple mean-pooling of BERT embeddings and measure impact on cause detection
  3. Try span-level cause extraction on a small manually cleaned subset to evaluate feasibility

## Open Questions the Paper Calls Out

### Open Question 1
How can the quality of the Emotion-Cause-in-Friends (ECF) dataset be improved to address the inconsistencies and ambiguities in emotion-cause annotations? Basis: The authors discuss challenges in accurately identifying causes, inconsistencies in dataset annotation, and complexity of the task. Unresolved: The paper identifies dataset issues but does not propose concrete solutions. What would resolve it: Experimental results demonstrating improved model performance after dataset refinement or development of a new annotation framework.

### Open Question 2
What are the potential benefits and challenges of incorporating multimodal information (e.g., visual and acoustic cues) into the emotion-cause extraction pipeline, and how would this affect model performance? Basis: The paper focuses on textual extraction, but the task includes a multimodal track that the authors did not participate in. Unresolved: The paper does not explore multimodal information's impact. What would resolve it: Comparative studies showing performance with and without multimodal information, along with analysis of specific challenges and advantages.

### Open Question 3
How can speaker representations be improved to enhance the understanding and processing of dialogues in emotion-cause extraction tasks? Basis: The authors mention that speaker representations can be improved as a potential future direction. Unresolved: The paper does not provide specific methods or experiments for improving speaker representations. What would resolve it: Experiments comparing performance using different speaker representation techniques, along with analysis of the impact on model accuracy.

## Limitations
- No ablation studies to quantify individual component contributions
- Performance on emotions other than anger and joy is not well-characterized
- No error analysis beyond overall F1 scores to understand failure modes

## Confidence

- **High**: The overall ranking (2nd place out of 15 teams) and the comparative advantage of fine-tuning over zero-shot settings
- **Medium**: The effectiveness of BiLSTM for context modeling, given no comparison to transformer-based alternatives
- **Low**: The decision to skip span-level cause extraction without empirical validation on a cleaned subset

## Next Checks

1. Conduct ablation studies comparing: (a) zero-shot vs fine-tuned GPT-3.5 emotion classification, (b) BiLSTM vs mean-pooling for cause detection, (c) utterance-level vs span-level cause extraction on manually cleaned data
2. Test the pipeline on out-of-domain conversational data (e.g., movie subtitles, social media conversations) to assess generalization
3. Analyze per-emotion performance in detail to identify whether the model has systematic weaknesses beyond the reported anger-specific issues