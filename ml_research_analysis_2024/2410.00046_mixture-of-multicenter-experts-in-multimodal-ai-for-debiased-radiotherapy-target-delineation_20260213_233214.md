---
ver: rpa2
title: Mixture of Multicenter Experts in Multimodal AI for Debiased Radiotherapy Target
  Delineation
arxiv_id: '2410.00046'
source_url: https://arxiv.org/abs/2410.00046
tags:
- center
- mome
- data
- training
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Mixture of Multicenter Experts (MoME) framework addresses AI
  bias in medical radiotherapy by enabling debiased training across diverse institutional
  practices without requiring full data sharing. It integrates a shared path with
  center-specific router paths, allowing adaptation to unique institutional treatment
  strategies using minimal data (10-20 samples).
---

# Mixture of Multicenter Experts in Multimodal AI for Debiased Radiotherapy Target Delineation

## Quick Facts
- arXiv ID: 2410.00046
- Source URL: https://arxiv.org/abs/2410.00046
- Reference count: 40
- Key outcome: MoME achieves Dice scores of 0.752-0.756 across five centers for prostate cancer radiotherapy, outperforming baselines through debiased training without full data sharing.

## Executive Summary
This paper presents the Mixture of Multicenter Experts (MoME) framework for debiased radiotherapy target delineation across multiple institutions. The framework addresses AI bias by enabling adaptation to diverse institutional practices without requiring full data sharing. MoME integrates a shared encoder/decoder architecture with center-specific router paths, allowing selective activation of expert modules based on institutional characteristics. Evaluated on prostate cancer radiotherapy target volume delineation across five centers, MoME demonstrates superior performance with Dice scores of 0.752-0.756, while maintaining equitable performance across institutions. The framework also enables effective few-shot fine-tuning in closed-center settings using minimal data (10-20 samples), achieving Dice scores of 0.605-0.712.

## Method Summary
MoME combines a shared 3D U-Net backbone with center-specific Mixture-of-Experts (MoE) router modules. The framework processes both CT images and clinical EMR data through a multimodal alignment module that uses a frozen LLM with learnable text prompts. During training, the router selects top-k expert modules conditioned on center flags, allowing adaptation to unique institutional treatment strategies. The model is trained on full data from one center while incorporating few-shot samples (1-3) from other centers. For closed-center adaptation, MoME uses pseudo-center selection based on expert activation similarity when the target center is unavailable. The framework employs statistical center similarity measures with layer-wise weights to select appropriate routers for inference on new centers.

## Key Results
- MoME achieves Dice scores of 0.752-0.756 across five centers for prostate cancer radiotherapy target delineation
- The framework enables effective few-shot fine-tuning with only 10-20 samples per center, achieving Dice scores of 0.605-0.712
- MoME demonstrates equitable performance across institutions while reducing bias compared to baseline models
- The multimodal integration with clinical notes improves generalization beyond imaging-only approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The center-specific router paths enable selective activation of expert modules conditioned on institutional characteristics, reducing bias from dominant data distributions.
- Mechanism: The router uses Gaussian noise injection to sparsely activate top-k expert modules for each input, conditioned on the center flag. This allows the model to adapt to each center's unique data distribution without full data sharing.
- Core assumption: Different centers have distinct data distributions that can be modeled through expert routing.
- Evidence anchors:
  - [abstract]: "integrates a shared path with center-specific router paths, allowing adaptation to unique institutional treatment strategies using minimal data"
  - [section]: "The only center-dependent component is the MoME router, which selects top-k expert modules conditioned on the center flag"
  - [corpus]: Weak evidence - no direct corpus support found for this specific mechanism
- Break condition: If expert modules are not sufficiently diverse to capture institutional differences, or if center flags are not properly encoded.

### Mechanism 2
- Claim: Multimodal integration with clinical notes improves model generalization by incorporating contextual factors beyond imaging.
- Mechanism: The framework uses a frozen LLM with learnable text prompts to process clinical data, which is then aligned with image embeddings through interactive attention mechanisms.
- Core assumption: Clinical context significantly impacts target delineation decisions and cannot be captured by imaging alone.
- Evidence anchors:
  - [abstract]: "integrates specialized expertise from diverse clinical strategies to enhance model generalizability and adaptability across medical centers"
  - [section]: "Multimodal AI models, which combine clinical context with imaging data, have demonstrated superior generalization capabilities"
  - [corpus]: Weak evidence - no direct corpus support found for this specific multimodal mechanism
- Break condition: If clinical data quality is poor or if the alignment between clinical and imaging modalities fails.

### Mechanism 3
- Claim: Few-shot fine-tuning with center-specific routers enables effective adaptation to new institutions without requiring large datasets.
- Mechanism: The model uses pre-trained weights and adapts to new centers through minimal data input (10-20 samples) by leveraging the center-specific router structure.
- Core assumption: Pre-trained models with center-specific routing can effectively transfer knowledge to new institutional settings with minimal additional data.
- Evidence anchors:
  - [abstract]: "enables effective few-shot fine-tuning in closed-center settings, achieving Dice scores of 0.605-0.712 with minimal data"
  - [section]: "Furthermore, MoME enables model customization to local clinical preferences without cross-institutional data exchange"
  - [corpus]: Weak evidence - no direct corpus support found for this specific fine-tuning mechanism
- Break condition: If the few-shot samples are not representative of the new center's distribution, or if the pre-trained model is too overfitted to training centers.

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: MoE allows the model to maintain a large effective capacity while keeping computational cost manageable through selective expert activation
  - Quick check question: What is the key difference between MoE and traditional ensemble methods?

- Concept: Multimodal learning integration
  - Why needed here: Target volume delineation requires both imaging and clinical context, making multimodal approaches essential for accurate predictions
  - Quick check question: How does the interactive alignment module combine clinical and imaging embeddings?

- Concept: Federated learning concepts
  - Why needed here: Understanding data privacy constraints and decentralized training approaches helps explain why MoME avoids full data sharing
  - Quick check question: What are the main privacy concerns that MoME addresses compared to traditional federated learning?

## Architecture Onboarding

- Component map:
  Input: 3D CT images + clinical EMR data + center flag
  Encoder: 3D Residual U-Net for images
  Router: Center-specific MoME modules with top-k expert selection
  Alignment: Interactive attention between image and text embeddings
  Decoder: 3D decoder for final segmentation output

- Critical path: Image → Encoder → MoME router selection → Expert combination → Interactive alignment → Decoder → Output

- Design tradeoffs:
  Computational cost vs performance: Using 8 experts with top-2 selection balances efficiency and effectiveness
  Shared vs center-specific parameters: Shared encoder/decoder prevents overfitting while center-specific routers enable adaptation
  Frozen LLM vs fine-tuning: Keeping LLM frozen reduces complexity while learnable prompts provide flexibility

- Failure signatures:
  Poor performance on new centers: Router selection may not be capturing institutional differences
  Overfitting to training centers: Shared parameters may not be sufficiently generalizable
  High computational cost: Too many experts or inefficient routing mechanism

- First 3 experiments:
  1. Test router selection effectiveness by comparing performance when using different center routers on validation data
  2. Evaluate few-shot adaptation by gradually increasing fine-tuning data size and measuring performance gains
  3. Assess multimodal impact by comparing performance with and without clinical note integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which the Mixture of Multicenter Experts (MoME) framework reduces bias across diverse institutional data distributions?
- Basis in paper: [explicit] The paper describes that MoME integrates specialized expertise from diverse clinical strategies to enhance model generalizability and adaptability across medical centers, and that it uses center-specific router paths to adapt to unique institutional treatment strategies.
- Why unresolved: The paper explains the MoME framework's structure and how it adapts to different centers but does not detail the specific mechanisms by which it reduces bias in the model's predictions.
- What evidence would resolve it: Detailed analysis of how the router networks and expert modules interact to mitigate bias, possibly including statistical evidence showing bias reduction in predictions across centers.

### Open Question 2
- Question: How does the closed center MoME fine-tuning approach perform compared to traditional centralized training methods in terms of accuracy and generalizability?
- Basis in paper: [inferred] The paper mentions that MoME enables effective few-shot fine-tuning in closed-center settings and maintains equitable performance across institutions, but does not provide a direct comparison with centralized training.
- Why unresolved: While the paper demonstrates the effectiveness of MoME in closed-center settings, it lacks a direct comparison with centralized training methods to assess relative performance.
- What evidence would resolve it: Comparative studies between MoME fine-tuning and centralized training on the same datasets, measuring accuracy and generalizability metrics.

### Open Question 3
- Question: Can the MoME framework be extended to handle other types of cancer and therapeutic tasks beyond prostate cancer and nasopharyngeal cancer?
- Basis in paper: [explicit] The paper states that MoME was validated on prostate cancer radiotherapy and nasopharyngeal cancer, suggesting potential for broader application.
- Why unresolved: The paper only tests MoME on two specific cancer types, leaving its effectiveness on other cancers and therapeutic tasks unexplored.
- What evidence would resolve it: Testing and validation of MoME on a wider range of cancer types and therapeutic tasks, demonstrating its generalizability and adaptability.

## Limitations

- Performance relies heavily on the quality and representativeness of few-shot samples used for adaptation, with only 10-20 samples per center
- The multimodal component's contribution is unclear due to incomplete specification of clinical data curation and specific features used
- Statistical center similarity measure for closed-center adaptation lacks complete specification, making pseudo-center selection verification difficult
- Generalizability across substantially different institutional practices or patient populations remains uncertain

## Confidence

**High Confidence:** The core architecture of MoME (shared encoder/decoder with center-specific router paths) is well-specified and the experimental results demonstrate clear performance improvements over baseline models. The mechanism of using top-k expert selection with center conditioning is technically sound.

**Medium Confidence:** The few-shot adaptation capability is supported by experimental results, but the generalizability across diverse institutional practices remains uncertain. The multimodal integration shows promise but lacks detailed validation of the clinical data contribution.

**Low Confidence:** The statistical center similarity measure for closed-center adaptation has insufficient detail for full verification. The clinical data curation process and the specific features that drive multimodal improvements are not fully specified.

## Next Checks

1. **Expert Activation Analysis:** Visualize and quantify the expert activation patterns across centers to verify that the top-k selection is effectively capturing institutional differences. Compare activation distributions between training and test centers to identify potential domain gaps.

2. **Clinical Data Ablation Study:** Conduct a controlled experiment to isolate the contribution of multimodal inputs by comparing performance with and without clinical data integration across all evaluation metrics. Analyze which clinical features most strongly correlate with segmentation accuracy.

3. **Robustness to Sample Size:** Systematically vary the number of few-shot samples (5, 10, 20, 50) for closed-center adaptation and measure performance degradation. Identify the minimum effective sample size and characterize failure modes when samples are not representative of the target center's distribution.