---
ver: rpa2
title: 'Retrieval-Augmented Code Generation for Situated Action Generation: A Case
  Study on Minecraft'
arxiv_id: '2406.17553'
source_url: https://arxiv.org/abs/2406.17553
tags:
- builder
- info
- language
- task
- color
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using large language models (LLMs) for action
  prediction in the Minecraft Collaborative Building Task, where an architect provides
  instructions to a builder to construct structures with 3D blocks. The authors frame
  action prediction as a code generation task, transforming builder actions into place()
  and pick() function calls.
---

# Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft

## Quick Facts
- arXiv ID: 2406.17553
- Source URL: https://arxiv.org/abs/2406.17553
- Reference count: 25
- GPT-4 achieves F1 score of 0.39 on Minecraft builder action prediction

## Executive Summary
This paper explores using large language models (LLMs) for action prediction in the Minecraft Collaborative Building Task, where an architect provides instructions to a builder to construct structures with 3D blocks. The authors frame action prediction as a code generation task, transforming builder actions into place() and pick() function calls. They use few-shot prompting with dynamically retrieved in-context examples and experiment with GPT-4 and Llama models. GPT-4 achieves the best F1 score of 0.39, outperforming previous baselines, while Llama-3-70b and Llama-3-8b (both fine-tuned) achieve 0.33 and 0.19, respectively. Analysis reveals that spatial prepositions, geometric shapes, and anaphora are key challenges, with models correctly predicting only 26.03%, 18.26%, and 25.53% of cases, respectively. Builder mistakes and underspecified instructions further complicate evaluation. The study highlights the potential of LLMs for action prediction while identifying areas for future improvement.

## Method Summary
The authors transform the Minecraft Dialogue dataset by converting builder actions into place() and pick() function calls, aggregating architect and builder utterances into single instructions per turn. They implement few-shot prompting by constructing prompts with system info, environment details (11x9x11 grid, 6 colors), task format, and 3 dynamically retrieved in-context examples based on instruction similarity using Sentence Transformers. The authors query instruction-tuned LLMs (GPT-4, Llama-3-70b, Llama-3-8b) with temperature 0 and max_new_tokens=500, then evaluate predictions against ground truth using the F1 metric. They also conduct fine-tuning experiments with Llama-3-8b using Q-LORA optimization on the training set.

## Key Results
- GPT-4 achieves the best F1 score of 0.39 on the test set
- Llama-3-70b (few-shot) achieves F1 score of 0.33
- Llama-3-8b (fine-tuned) achieves F1 score of 0.19, showing ~6% improvement over few-shot
- Spatial prepositions, geometric shapes, and anaphora are key challenges with accuracy rates of 26.03%, 18.26%, and 25.53% respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented few-shot prompting significantly improves performance over zero-shot baselines
- Mechanism: Dynamically retrieving similar in-context examples from training data based on instruction similarity helps the LLM generalize to new task instances
- Core assumption: Similar instructions in the training set contain relevant examples that can guide the model on the test instruction
- Evidence anchors: Using few-shot prompting techniques, that significantly improve performance over baseline methods; dynamically adapted in-context examples suitable to the current turn instruction from the training set
- Break condition: When test instructions are highly novel or dissimilar to any training examples, the retrieved examples may be irrelevant and performance degrades

### Mechanism 2
- Claim: Framing builder action prediction as code generation leverages LLM strengths
- Mechanism: LLMs are pre-trained on large code corpora and have strong code generation capabilities, making them well-suited for translating natural language instructions into structured action sequences
- Core assumption: The code representation (place() and pick() functions with parameters) is expressive enough to capture the builder actions
- Evidence anchors: Taking advantage of the code-generation capabilities of LLMs, we model the action prediction task as a code-generation task; Following standard prompting approaches (Brown et al., 2020; Liu et al., 2023; Wei et al., 2022; Wu et al., 2023), we adopt few-shot prompting to probe LLMs
- Break condition: When instructions involve complex spatial reasoning or ambiguous references that cannot be easily expressed in the code format, the model may fail

### Mechanism 3
- Claim: Fine-tuning further improves performance over few-shot prompting
- Mechanism: Fine-tuning adapts the LLM parameters specifically to the Minecraft action prediction task, allowing it to learn task-specific patterns
- Core assumption: The training data contains sufficient examples to learn meaningful patterns for the task
- Evidence anchors: The fine-tuned version of Llama-3-8b showed a âˆ¼ 6% improvement over the vanilla version; We use the training set of the Minecraft Dialogue corpus and fine-tune the Llama-3-8b model
- Break condition: When fine-tuning data is limited or not representative of the test distribution, the improvement may be marginal or even negative

## Foundational Learning

- Concept: In-context learning
  - Why needed here: The approach relies on providing the LLM with relevant examples in the prompt to guide its generation on the test instruction
  - Quick check question: What is the difference between in-context learning and fine-tuning in terms of how the model adapts to a new task?

- Concept: Code representation of actions
  - Why needed here: The builder actions are converted to place() and pick() function calls with parameters, which the LLM generates as output
  - Quick check question: How would you represent a builder action that involves multiple blocks being placed at different locations in the code format?

- Concept: Retrieval-augmented generation
  - Why needed here: Similar in-context examples are retrieved from the training set based on instruction similarity to improve the LLM's performance
  - Quick check question: What are the potential challenges in retrieving relevant examples for a test instruction?

## Architecture Onboarding

- Component map: Instruction preprocessing -> Example retrieval -> Prompt construction -> LLM inference -> Postprocessing
- Critical path: 1. Retrieve similar examples from training set 2. Construct the prompt with the retrieved examples 3. Query the LLM with the prompt 4. Parse and evaluate the generated code
- Design tradeoffs:
  - Number of in-context examples: More examples may provide better guidance but increase prompt length and cost
  - Example retrieval method: Similarity based on instructions vs. actions or other features
  - Code representation: More expressive format may capture complex actions better but increase generation difficulty
- Failure signatures:
  - Generated code does not match the expected format (missing functions, incorrect parameters)
  - Generated code does not correspond to the input instruction (hallucinations)
  - Performance degradation when instructions involve spatial prepositions, geometric shapes, or anaphora
- First 3 experiments:
  1. Ablation study on the number of in-context examples (0, 1, 2, 3, 4, 5) to find the optimal number
  2. Compare performance with and without environment information in the prompt
  3. Fine-tune Llama-3-8b on the training set and evaluate on the test set to measure the improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompting strategies (e.g., zero-shot vs. few-shot) impact the accuracy of LLM-generated code for builder actions?
- Basis in paper: The paper mentions using few-shot prompting techniques and conducts an ablation study on the impact of the parts of the prompt
- Why unresolved: The paper does not explicitly compare the performance of zero-shot prompting with few-shot prompting
- What evidence would resolve it: A direct comparison of LLM performance using zero-shot and few-shot prompting on the same dataset

### Open Question 2
- Question: What are the limitations of using LLMs for action prediction in real-world collaborative tasks beyond the Minecraft environment?
- Basis in paper: The paper discusses the limitations of the current approach, including challenges with spatial prepositions, geometric shapes, and anaphora, as well as the simulated nature of the Minecraft environment
- Why unresolved: The paper does not explore the applicability of LLMs for action prediction in real-world scenarios
- What evidence would resolve it: Experiments applying the same LLM approach to real-world collaborative tasks and analyzing the performance and limitations

### Open Question 3
- Question: How does fine-tuning LLMs on the Minecraft dataset affect their ability to generalize to new, unseen building tasks?
- Basis in paper: The paper mentions fine-tuning the Llama-3-8b model on the Minecraft training set and observing a marginal improvement in F1-score
- Why unresolved: The paper does not investigate the generalization capabilities of the fine-tuned model on new tasks
- What evidence would resolve it: Testing the fine-tuned model on a separate set of Minecraft building tasks not seen during training and comparing its performance to the pre-trained model

## Limitations
- The evaluation metric (F1 score) may not fully capture semantic correctness of generated actions
- The Minecraft Dialogue corpus may not represent the full diversity of possible building instructions
- The place() and pick() function representation may not capture all possible builder actions, particularly complex spatial operations
- The paper doesn't analyze the quality of retrieved examples or how often they are truly relevant

## Confidence
- High Confidence: Retrieval-augmented few-shot prompting improves over zero-shot baselines; Framing action prediction as code generation leverages LLM strengths; Fine-tuning Llama-3-8b improves performance over the base model
- Medium Confidence: Spatial prepositions, geometric shapes, and anaphora are key challenges for the model; The specific F1 scores achieved represent meaningful improvements
- Low Confidence: The exact contribution of each component to the overall performance; The generalizability of these results to other situated action generation tasks

## Next Checks
1. Conduct an ablation study to isolate the contribution of each component (few-shot prompting vs. zero-shot, retrieval vs. no retrieval, code representation vs. alternative formats)
2. Perform detailed error analysis with human judges to assess whether F1 metric captures true semantic correctness, particularly for challenging cases
3. Test the approach on a different situated action generation task (e.g., robotic manipulation instructions) to assess generalizability beyond the Minecraft domain