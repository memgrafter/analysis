---
ver: rpa2
title: 'RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning
  with Explanation'
arxiv_id: '2405.03064'
source_url: https://arxiv.org/abs/2405.03064
tags:
- agent
- explanation
- policy
- refining
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RICE, a refining scheme for reinforcement
  learning that uses explanation methods to overcome training bottlenecks. RICE identifies
  critical states in a pre-trained policy's trajectory and constructs a mixed initial
  state distribution combining default and critical states to encourage exploration.
---

# RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation

## Quick Facts
- arXiv ID: 2405.03064
- Source URL: https://arxiv.org/abs/2405.03064
- Reference count: 40
- Primary result: Improves RL agent performance by up to 68.2% in malware mutation tasks

## Executive Summary
RICE introduces a refining scheme for reinforcement learning that uses explanation methods to overcome training bottlenecks. The method identifies critical states in pre-trained policy trajectories and constructs a mixed initial state distribution combining default and critical states to encourage exploration. RICE theoretically guarantees a tighter sub-optimality bound and is evaluated across four MuJoCo games and four real-world applications, showing significant performance improvements over existing refining schemes.

## Method Summary
RICE works by first training a pre-trained policy using PPO, then applying a StateMask-based explanation method to identify critical states that drive policy success or failure. These critical states are combined with default initial states in a mixed distribution, which is used to initialize a new PPO training process with an RND-based exploration bonus. The approach allows the agent to break out of local optima by focusing exploration on states where the pre-trained policy struggled.

## Key Results
- Achieves up to 68.2% improvement in malware mutation tasks compared to baseline methods
- Outperforms existing refining schemes across four MuJoCo games (Hopper, Walker2d, Reacher, HalfCheetah)
- Demonstrates efficient training of explanation models compared to state-of-the-art methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RICE improves policy refinement by combining default and critical states in a mixed initial state distribution.
- Mechanism: The mixed initial state distribution prevents overfitting to critical states while maintaining exploration diversity, allowing the agent to break out of local optima.
- Core assumption: The pre-trained policy covers states visited by the optimal policy, ensuring critical states are informative for exploration.
- Evidence anchors:
  - [abstract] "The high-level idea of RICE is to construct a new initial state distribution that combines both the default initial states and critical states identified through explanation methods"
  - [section 3.3] "We then set the initial distribution µ as a mixture of the selected important states distribution dˆπρ(s) and the original initial distribution of interest ρ: µ(s) = βdˆπρ(s) + (1 − β)ρ(s)"
  - [corpus] Weak - corpus focuses on general DRL exploration methods rather than mixed initial distributions specifically
- Break condition: If the pre-trained policy has poor state coverage (Assumption 3.2 fails), the mixed distribution provides no benefit over random exploration.

### Mechanism 2
- Claim: StateMask-based explanation identifies truly critical states that drive policy success/failure.
- Mechanism: The mask network learns to "blind" the agent at non-critical steps, revealing which states are essential for achieving final rewards through randomized action comparison.
- Core assumption: Critical states can be identified by measuring their impact on final reward when actions are randomized at those steps.
- Evidence anchors:
  - [section 3.3] "StateMask learns a policy to 'blind' the target agent at certain steps without changing the agent's final reward"
  - [section 4.1] "The idea is to use a sliding window to step through all time steps and then choose the window with the highest average importance score"
  - [corpus] Weak - corpus papers focus on general DRL explanation rather than state masking specifically
- Break condition: If the mask network cannot distinguish between truly critical and non-critical states, the explanation becomes random and loses refinement value.

### Mechanism 3
- Claim: Exploration bonus via RND encourages state coverage expansion beyond local optima.
- Mechanism: Random Network Distillation provides intrinsic reward for visiting novel states, decaying as state coverage increases and allowing recovery of optimal policy.
- Core assumption: The state space has regions that the pre-trained policy has not explored, and these regions contain valuable information for policy improvement.
- Evidence anchors:
  - [section 3.3] "we adopt Random Network Distillation (RND) (Burda et al., 2018) which is proved to be an effective exploration bonus"
  - [section 3.3] "Along with the policy parameters, the RND predictor network ˆf is updated to regress to the target network f"
  - [corpus] Moderate - RND is well-established in DRL literature as an exploration method
- Break condition: If the pre-trained policy already has good state coverage, RND bonuses decay to zero and exploration becomes unnecessary.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formalism
  - Why needed here: RICE operates within the MDP framework, using state distributions, policies, and value functions for theoretical analysis
  - Quick check question: What is the relationship between dπρ(s) and dπρ(s,a) in terms of state-action occupancy measures?

- Concept: Policy gradient methods and advantage functions
  - Why needed here: RICE uses PPO for refinement and theoretical analysis relies on advantage function properties
  - Quick check question: How does the advantage function Aπ(s,a) relate to the policy improvement condition?

- Concept: Distribution mismatch coefficients and concentrability
  - Why needed here: Theoretical guarantees depend on bounding the ratio of state distributions between pre-trained and optimal policies
  - Quick check question: What does the infinity norm of dπ*ρ/dπρ represent in terms of policy performance bounds?

## Architecture Onboarding

- Component map:
  Pre-trained policy π -> StateMask explanation module -> Mixed initial state distribution constructor -> PPO refiner with RND exploration -> Refined policy

- Critical path:
  1. Sample trajectory from pre-trained policy
  2. Apply StateMask to identify critical states
  3. Construct mixed initial distribution
  4. Initialize PPO training with mixed distribution
  5. Apply RND bonus during training
  6. Output refined policy

- Design tradeoffs:
  - Exploration vs exploitation: Higher λ increases exploration but may slow convergence
  - State diversity vs overfitting: p controls balance between default and critical states
  - Explanation fidelity vs efficiency: StateMask provides good explanations but requires training time

- Failure signatures:
  - No performance improvement: Check if pre-trained policy has poor state coverage
  - Overfitting to critical states: Verify p is not too close to 1
  - Slow convergence: Consider increasing λ or checking RND implementation
  - Explanation quality issues: Validate mask network training or try alternative explanation methods

- First 3 experiments:
  1. Run with p=0 (only default initial states) vs p=1 (only critical states) to verify mixed distribution benefit
  2. Disable RND (λ=0) to confirm exploration contribution to performance
  3. Compare with random explanation baseline to validate StateMask quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RICE perform when the pre-trained policy violates Assumption 3.2 (poor state coverage)?
- Basis in paper: [explicit] The paper states "RICE essentially reduces to being equivalent to Random Network Distillation (RND) due to the lack of meaningful explanation" in cases where the original policy fails to learn an effective strategy.
- Why unresolved: The paper provides only one negative example (MountainCarContinuous-v0) and does not explore the full spectrum of how poor state coverage affects RICE's performance across different environments or degrees of violation.
- What evidence would resolve it: Empirical results showing RICE's performance degradation across a range of environments where the pre-trained policy has increasingly poor state coverage, along with quantitative measures of state coverage quality.

### Open Question 2
- Question: What is the optimal strategy for filtering critical states to maximize refinement efficiency?
- Basis in paper: [inferred] The paper mentions "Future work could explore additional filtering of critical states using metrics such as policy convergence or temporal difference (TD) errors" but does not implement or evaluate such strategies.
- Why unresolved: The current implementation uses all identified critical states without filtering, potentially wasting computational resources on states where the agent has already converged to optimal actions.
- What evidence would resolve it: Experimental comparison showing performance and efficiency gains when applying various filtering strategies (policy convergence metrics, TD errors, etc.) versus using all identified critical states.

### Open Question 3
- Question: How sensitive is RICE to the choice of exploration bonus method beyond RND?
- Basis in paper: [explicit] The paper states "The result is less sensitive to the specific value of λ" when varying the RND exploration bonus, suggesting exploration is important but not specifically tied to RND.
- Why unresolved: While the paper demonstrates RICE works well with RND, it does not explore whether other exploration methods (count-based bonuses, curiosity-driven exploration, etc.) could provide equal or better performance.
- What evidence would resolve it: Comparative experiments showing RICE's performance using different exploration bonus methods across multiple environments, with statistical analysis of their relative effectiveness.

## Limitations
- Theoretical guarantees rely heavily on Assumption 3.2, which may not hold in practice for many real-world scenarios
- StateMask's explanation quality is not directly validated against ground truth critical states
- Performance improvements are demonstrated primarily on relatively simple MuJoCo environments, with limited testing on more complex real-world applications

## Confidence

**High Confidence**: The mixed initial state distribution mechanism and its theoretical analysis are well-grounded

**Medium Confidence**: The effectiveness of StateMask for identifying critical states, as this requires validation on ground truth

**Medium Confidence**: The real-world application results, particularly the 68.2% improvement claim in malware mutation

## Next Checks
1. Test RICE on environments where the pre-trained policy has poor state coverage to verify Assumption 3.2 is necessary for performance gains
2. Compare StateMask explanations against human-annotated critical states or ablation studies to validate explanation quality
3. Evaluate RICE on more complex environments (e.g., robotic manipulation tasks) to assess scalability beyond MuJoCo benchmarks