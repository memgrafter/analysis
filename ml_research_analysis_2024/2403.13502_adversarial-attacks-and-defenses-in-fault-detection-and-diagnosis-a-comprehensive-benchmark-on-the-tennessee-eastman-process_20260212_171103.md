---
ver: rpa2
title: 'Adversarial Attacks and Defenses in Fault Detection and Diagnosis: A Comprehensive
  Benchmark on the Tennessee Eastman Process'
arxiv_id: '2403.13502'
source_url: https://arxiv.org/abs/2403.13502
tags:
- adversarial
- attacks
- attack
- training
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper benchmarks adversarial attacks and defenses in fault
  detection and diagnosis using the Tennessee Eastman Process dataset. It evaluates
  three neural network architectures (MLP, GRU, TCN) under six types of adversarial
  attacks and five defense methods.
---

# Adversarial Attacks and Defenses in Fault Detection and Diagnosis: A Comprehensive Benchmark on the Tennessee Eastman Process

## Quick Facts
- arXiv ID: 2403.13502
- Source URL: https://arxiv.org/abs/2403.13502
- Reference count: 40
- Primary result: Models are highly vulnerable to adversarial samples, with accuracy dropping significantly even for small perturbations

## Executive Summary
This paper benchmarks adversarial attacks and defenses in fault detection and diagnosis using the Tennessee Eastman Process dataset. The study evaluates three neural network architectures (MLP, GRU, TCN) under six types of adversarial attacks and five defense methods. The authors find that models are highly vulnerable to adversarial samples, with accuracy dropping significantly even for small perturbations. Black-box attacks like FGSM distillation prove particularly effective. While universal defense methods like adversarial training and autoencoders reduce vulnerability, they also degrade performance on normal data. The authors propose a novel defense strategy combining adversarial training with data quantization, which provides good protection against most attacks while maintaining accuracy.

## Method Summary
The study uses the Tennessee Eastman Process (TEP) dataset with 100 simulation runs for each of 28 fault types, 52 sensor values for 2000 timestamps each, and sliding window size 32. Three neural network architectures are implemented: MLP with 2 layers, GRU, and TCN. The models are trained for specified epochs (20 for MLP, 5 for GRU, 10 for TCN) and standardized data is used. Six attack methods (Random noise, FGSM, FGSM distillation, PGD, DeepFool, C&W) and five defense methods (Adversarial training, Autoencoder, Quantization, Regularization, Distillation) are implemented and evaluated. The novel combination approach is also tested.

## Key Results
- Models are highly vulnerable to adversarial samples, with accuracy dropping significantly even for small perturbations
- Black-box attacks like FGSM distillation prove particularly effective
- Defense methods like adversarial training reduce vulnerability but degrade performance on normal data
- Combining adversarial training with data quantization provides good protection while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial samples cause significant accuracy drops even for small perturbations
- Mechanism: Small input perturbations aligned with the gradient of the loss function maximize model error while remaining imperceptible
- Core assumption: The model's decision boundary is locally linear or nearly linear
- Evidence anchors:
  - [abstract] "models are highly vulnerable to adversarial samples, with accuracy dropping significantly even for small perturbations"
  - [section 4.2] "It decreases significantly with small shifts in the attacked data for ϵ values less than 0.05"
- Break condition: If the model's decision boundary is highly non-linear in the region near the input

### Mechanism 2
- Claim: Defense methods like adversarial training reduce vulnerability but degrade performance on normal data
- Mechanism: Training on adversarial examples forces the model to learn a more robust decision boundary, but this boundary is less optimized for clean data
- Core assumption: There exists a fundamental tradeoff between robustness to adversarial examples and accuracy on clean data
- Evidence anchors:
  - [abstract] "universal defense methods like adversarial training and autoencoders reduce vulnerability, they also degrade performance on normal data"
  - [section 4.3.1] "Adding more different perturbed data to the training process increases the average robustness of the model to adversarial attacks. But the quality with normal data decreases."
- Break condition: If a defense method can learn to distinguish between adversarial and clean inputs without sacrificing clean accuracy

### Mechanism 3
- Claim: Combining adversarial training with data quantization provides good protection while maintaining accuracy
- Mechanism: Quantization reduces the precision of input data, which limits the effectiveness of adversarial perturbations, while adversarial training on quantized data helps the model learn to generalize better despite the reduced precision
- Core assumption: Adversarial perturbations are more sensitive to small changes in input precision than legitimate features
- Evidence anchors:
  - [abstract] "authors propose a novel defense strategy combining adversarial training with data quantization, which provides good protection against most attacks while maintaining accuracy"
  - [section 3.3.6] "Quantization allows to reduce the strength of the attack, which in turn allows the model to generalize better during adversarial training."
- Break condition: If adversarial perturbations can be crafted to exploit the quantization grid itself

## Foundational Learning

- Concept: Adversarial examples and attacks
  - Why needed here: Understanding how small input perturbations can fool machine learning models is fundamental to the research problem
  - Quick check question: What is the difference between white-box and black-box adversarial attacks?

- Concept: Defense methods against adversarial attacks
  - Why needed here: The paper evaluates multiple defense strategies, so understanding their mechanisms and tradeoffs is crucial
  - Quick check question: How does adversarial training work, and what is its main limitation?

- Concept: Fault Detection and Diagnosis (FDD) in industrial processes
  - Why needed here: The study uses the Tennessee Eastman Process dataset for FDD, so understanding the problem domain is important
  - Quick check question: What are the three main categories of FDD methods according to the literature?

## Architecture Onboarding

- Component map: Data → FDD model → Prediction → Evaluation
- Critical path: Data → FDD model → Prediction → Evaluation (accuracy on clean and adversarial data)
- Design tradeoffs: Robustness vs. accuracy tradeoff in defense methods; computational cost vs. protection level
- Failure signatures: Significant accuracy drop on adversarial data; degradation of performance on clean data after applying defense methods
- First 3 experiments:
  1. Train FDD models on clean TEP data and evaluate accuracy
  2. Apply each adversarial attack to the trained models and measure accuracy degradation
  3. Apply each defense method to the models and evaluate both adversarial robustness and clean accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do autoencoders with more advanced architectures perform as universal defense mechanisms against adversarial attacks in FDD systems?
- Basis in paper: [explicit] The authors state that autoencoders show potential as universal defense mechanisms but require further research, and that the accuracy of the model protected by autoencoder on non-attacked data has noticeably decreased
- Why unresolved: The paper only tested a simple autoencoder with linear layers. More complex architectures might yield better results
- What evidence would resolve it: Experiments comparing various advanced autoencoder architectures on their effectiveness as universal defense mechanisms in FDD systems

### Open Question 2
- Question: What is the optimal combination of defense methods that provides the best balance between security against adversarial attacks and maintaining accuracy on normal data?
- Basis in paper: [explicit] The authors propose a novel defense strategy combining adversarial training with data quantization, which provides good protection against most attacks while maintaining accuracy. However, they acknowledge that other combinations can be explored in further research
- Why unresolved: The paper only tested one combination of defense methods. Other combinations might yield better results
- What evidence would resolve it: Systematic evaluation of various combinations of defense methods on their effectiveness and impact on accuracy in FDD systems

### Open Question 3
- Question: How do different quantization grid sizes (quantization frequencies) affect the effectiveness of the quantization defense method in FDD systems?
- Basis in paper: [explicit] The authors discuss the importance of the quantization grid size, stating that if the grid is too wide, it reduces the quality of fault diagnosis, and if the grid is too narrow, only a fraction of the data can be effectively recovered
- Why unresolved: The paper only tested quantization with a fixed number of discrete values. The optimal quantization grid size might vary depending on the specific FDD system and attack scenario
- What evidence would resolve it: Experiments varying the quantization grid size and evaluating its impact on the effectiveness of the quantization defense method and the accuracy of FDD systems under different attack scenarios

## Limitations

- The study's findings may not generalize to industrial processes with different noise characteristics and fault distributions
- The computational overhead of implementing the proposed defenses in real-time industrial control systems is not addressed
- The study does not consider potential adaptive attacks that could specifically target the quantization defense mechanism

## Confidence

- Vulnerability to adversarial attacks: High
- Defense effectiveness beyond TEP dataset: Medium
- Computational feasibility for real-time deployment: Low

## Next Checks

1. Test the quantization defense against adaptive attacks that specifically target the reduced precision of the input data
2. Evaluate the proposed defense methods on additional industrial process datasets to assess generalizability
3. Measure the computational overhead of each defense method to determine feasibility for real-time deployment in control systems