---
ver: rpa2
title: 'Inverted Activations: Reducing Memory Footprint in Neural Network Training'
arxiv_id: '2407.15545'
source_url: https://arxiv.org/abs/2407.15545
tags:
- tensor
- nonlinearity
- memory
- input
- backward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the memory footprint challenge in neural network
  training, specifically for pointwise nonlinearity layers that traditionally save
  the entire input tensor for the backward pass. The authors propose a modification
  where the output tensor is saved instead, reducing memory usage when the subsequent
  layer also saves its input tensor.
---

# Inverted Activations: Reducing Memory Footprint in Neural Network Training

## Quick Facts
- arXiv ID: 2407.15545
- Source URL: https://arxiv.org/abs/2407.15545
- Authors: Georgii Novikov; Ivan Oseledets
- Reference count: 4
- Primary result: A memory optimization technique that saves output tensors instead of input tensors in pointwise nonlinearity layers, reducing memory usage by up to 50% in transformer architectures without affecting training accuracy or computational performance.

## Executive Summary
This paper addresses the memory footprint challenge in neural network training, specifically for pointwise nonlinearity layers that traditionally save the entire input tensor for the backward pass. The authors propose a modification where the output tensor is saved instead, reducing memory usage when the subsequent layer also saves its input tensor. This optimization is particularly beneficial for transformer-based architectures like GPT, BERT, Mistral, and Llama. To enable this approach, they utilize the inverse function of the nonlinearity during the backward pass, constructing accurate approximations using simpler functions since analytical computation is not possible for most nonlinearities. Experimental results demonstrate that their method significantly reduces memory usage without affecting training accuracy or computational performance. The implementation is provided as a drop-in replacement for standard nonlinearity layers in the PyTorch framework, facilitating easy adoption without requiring architectural modifications.

## Method Summary
The method involves modifying pointwise nonlinearity layers (specifically GELU and SiLU) to save the output tensor instead of the input tensor during the forward pass. Since these functions are non-invertible analytically, the authors construct accurate approximations of their derivatives using simpler functions. For GELU and SiLU, which consist of two monotonic parts, they save an additional boolean indicator to track which monotonic part each element belongs to. During the backward pass, the inverse function is approximated using these simpler functions, and the boolean indicator is used to select the appropriate approximation. The implementation is provided as a drop-in replacement for standard nonlinearity layers in PyTorch, requiring no architectural modifications.

## Key Results
- Memory usage reduced by up to 50% in transformer architectures by storing only one tensor between two layers instead of two separate tensors
- Training accuracy remains indistinguishable from baseline implementations using standard nonlinearity layers
- No computational performance degradation observed during training
- Implementation available as drop-in replacement for GELU and SiLU layers in PyTorch

## Why This Works (Mechanism)

### Mechanism 1
Storing the output tensor instead of the input tensor reduces memory usage when the next layer also stores its input tensor. In a computation graph, each pointwise nonlinearity layer traditionally saves its input tensor S(i) for the backward pass. If we modify this to save the output tensor X(i+1) instead, and the next layer also saves its input tensor, we only store one tensor between two layers rather than two separate tensors. This mechanism breaks if the subsequent layer does not save its input tensor, or if there are architectural patterns where layers do not form these natural pairings.

### Mechanism 2
Using inverse function approximations enables backward computation when saving output tensors instead of input tensors. The backward pass for pointwise nonlinearities requires computing ∂L/∂x = ∂L/∂y · f'(x). When we save y (the output) instead of x (the input), we need to compute f'(x) where x = f⁻¹(y). Since analytical computation of f⁻¹ is not possible for most nonlinearities, we construct accurate approximations using simpler functions. This mechanism breaks if the approximations are not accurate enough to maintain training accuracy, or if the computational overhead of approximations negates memory benefits.

### Mechanism 3
Space-efficient boolean indicators enable inversion of non-invertible functions by separating them into monotonic parts. Both GELU and SiLU are non-invertible functions but consist of two monotonic parts that are individually invertible. We save an additional boolean indicator S that records which monotonic part each element belongs to, allowing disambiguation during the inverse computation. This mechanism breaks if the functions cannot be decomposed into monotonic parts, or if the boolean indicator storage overhead is not negligible compared to memory savings.

## Foundational Learning

- Concept: Computation graph and activation tensors in neural network training
  - Why needed here: Understanding how activation tensors are saved during forward pass and used during backward pass is fundamental to grasping why this optimization works
  - Quick check question: What is the difference between saving an input tensor versus an output tensor in the context of a pointwise nonlinearity layer?

- Concept: Inverse functions and function approximation
  - Why needed here: The method relies on computing f'(f⁻¹(y)) where analytical inverses are not available, requiring understanding of how to construct accurate approximations
  - Quick check question: Why can't we compute the inverse of GELU analytically, and what mathematical property allows us to approximate it instead?

- Concept: Monotonic functions and their invertibility
  - Why needed here: The approach exploits that non-invertible functions like GELU and SiLU can be split into monotonic segments that are individually invertible
  - Quick check question: What mathematical property makes a function invertible, and how does splitting a function into monotonic parts enable inversion?

## Architecture Onboarding

- Component map: Pointwise nonlinearity layer (GELU/SiLU) -> Save output tensor Y and boolean indicator S -> Next layer processes Y as input -> Backward pass computes gradient using ∂L/∂y · f'(f⁻¹(Y, S)) -> Use boolean indicator S to select appropriate approximation (qleft or qright) -> Return gradient to previous layer

- Critical path: Forward pass → Save output tensor Y and boolean indicator S → Next layer processes Y as input → Backward pass computes gradient using ∂L/∂y · f'(f⁻¹(Y, S)) → Use boolean indicator S to select appropriate approximation (qleft or qright) → Return gradient to previous layer

- Design tradeoffs: Memory vs. computation tradeoff - we save memory by storing one tensor instead of two, but add computational overhead for inverse function approximations. Accuracy vs. complexity tradeoff - more complex approximations yield better accuracy but may increase computation time. The implementation uses 1-bit boolean indicators to minimize storage overhead.

- Failure signatures: Training accuracy degradation (indicating approximation errors are too large), unexpected memory usage patterns (indicating the optimization isn't being applied correctly), or performance regressions (indicating computational overhead outweighs memory benefits).

- First 3 experiments:
  1. Implement the modified forward and backward passes for a single GELU layer and verify memory usage reduction while maintaining training accuracy on a small transformer model.
  2. Test the approximation accuracy by comparing f'(f⁻¹(y)) against numerical derivatives across the full range of y values for both GELU and SiLU.
  3. Benchmark the complete optimization on a medium-sized transformer model (e.g., BERT-base) to measure actual memory savings and any performance impact during training.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The optimization assumes subsequent layers save their input tensors, which may not hold in all architectural patterns
- The accuracy of inverse function approximations for GELU and SiLU is critical but not extensively validated across all possible input ranges
- The computational overhead of the approximations, while claimed to be negligible, is not quantified in detail
- The implementation's compatibility with other activation functions beyond GELU and SiLU remains unclear

## Confidence

- High Confidence: The core memory-saving mechanism of storing output instead of input tensors is well-established and the mathematical foundation for the optimization is sound.
- Medium Confidence: The approximation accuracy for inverse functions and the overall impact on training performance are demonstrated but could benefit from more extensive validation across different model sizes and tasks.
- Low Confidence: The generalizability of the approach to other activation functions and the precise quantification of computational overhead remain uncertain.

## Next Checks

1. **Benchmark on Diverse Architectures:** Test the optimization on a wider range of transformer architectures (e.g., Vision Transformers, smaller language models) to verify the consistency of memory savings and accuracy preservation.

2. **Quantify Computational Overhead:** Conduct detailed benchmarks to measure the exact computational overhead introduced by the inverse function approximations and compare it against the memory savings across different hardware configurations.

3. **Stress Test Approximation Accuracy:** Perform rigorous testing of the inverse function approximations across the full range of input values, including edge cases and extreme values, to ensure training stability and accuracy are maintained under all conditions.