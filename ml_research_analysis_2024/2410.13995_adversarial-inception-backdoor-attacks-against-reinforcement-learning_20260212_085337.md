---
ver: rpa2
title: Adversarial Inception Backdoor Attacks against Reinforcement Learning
arxiv_id: '2410.13995'
source_url: https://arxiv.org/abs/2410.13995
tags:
- attack
- agent
- attacks
- action
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new class of backdoor poisoning attacks against
  deep reinforcement learning (DRL) algorithms that operate under realistic reward
  constraints. Prior attacks assume unlimited control over reward magnitude, making
  them brittle and easily detected.
---

# Adversarial Inception Backdoor Attacks against Reinforcement Learning

## Quick Facts
- arXiv ID: 2410.13995
- Source URL: https://arxiv.org/abs/2410.13995
- Authors: Ethan Rathbun; Alina Oprea; Christopher Amato
- Reference count: 35
- This paper proposes a new class of backdoor poisoning attacks against deep reinforcement learning algorithms that operate under realistic reward constraints, achieving 100% attack success across multiple environments while maintaining benign task performance.

## Executive Summary
This paper addresses a critical vulnerability in deep reinforcement learning (DRL) systems by introducing adversarial inception attacks that overcome realistic reward constraints. Unlike prior backdoor attacks that assume unlimited control over reward magnitude, these attacks manipulate training data by inserting triggers into observations and replacing high-return actions with adversarial ones. The authors develop a novel framework that uses Deep Q-Learning-based approximation to identify optimal actions for replacement, achieving state-of-the-art performance across seven diverse environments while evading universal defenses.

## Method Summary
The adversarial inception framework formulates backdoor poisoning attacks as an adversarial MDP problem where the attacker manipulates both state observations and action selections during training. The core innovation is the Q-Incept algorithm, which uses a Deep Q-Learning-based approximation to identify high-return actions that can be replaced with target actions when trigger patterns are present. The attack maintains reward bounds within natural limits while guaranteeing both attack success and benign task performance through careful design of transition and reward functions. The method scales attack success with poisoning rate and requires no assumptions about trigger patterns, outperforming existing approaches like SleeperNets and TrojDRL.

## Key Results
- Q-Incept achieves 100% attack success rate across seven tested environments including Q*Bert, Frogger, Pacman, Breakout, CAGE-2, Highway Merge, and Safety Car
- The attack maintains better or comparable benign return scores compared to state-of-the-art baselines while evading universal state sanitization defenses
- Adversarial inception attacks overcome the fundamental limitations of reward-based poisoning by manipulating actions rather than rewards, operating within realistic constraint bounds

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adversarial inception attacks overcome reward constraints by manipulating actions rather than rewards.
- **Mechanism:** The attack inserts triggers into prior observations and replaces high-return actions with adversarial ones using a Deep Q-Learning-based approximation to ensure optimality.
- **Core assumption:** The adversary can approximate the Q-function well enough to identify high-return actions and replace them with the target action.
- **Evidence anchors:**
  - [abstract]: "These 'inception' attacks manipulate the agent's training data – inserting the trigger into prior observations and replacing high return actions with those of the targeted adversarial behavior."
  - [section 4.2]: "To achieve this, we design an adversarial MDP with a novel transition function T′ and reward function R′."
  - [corpus]: No direct corpus evidence found for this specific mechanism.
- **Break condition:** If the Q-function approximation is poor, the attack cannot identify appropriate high-return actions to replace.

### Mechanism 2
- **Claim:** The adversarial inception framework guarantees attack success and stealth under reward constraints.
- **Mechanism:** The framework uses a transition function that ensures optimal outcomes when the target action is chosen in poisoned states, combined with a reward function that stays within natural bounds.
- **Core assumption:** The transition and reward functions can be designed to work together to guarantee both objectives.
- **Evidence anchors:**
  - [section 4.3]: "Through these lemmas we now have a direct relationship between the value of a policy in the benign MDP M and the adversarial MDP M′."
  - [section 4.2]: "The goal of ϕ is to calculate and return the optimal action in state δ−1(sp) given π if the agent chooses the target action a+."
  - [corpus]: No direct corpus evidence found for this specific mechanism.
- **Break condition:** If the relationship between the value functions in M and M′ breaks down, the guarantees fail.

### Mechanism 3
- **Claim:** Q-Incept achieves state-of-the-art performance by scaling attack success with poisoning rate while maintaining benign task performance.
- **Mechanism:** Q-Incept uses the absolute value of F̂Q(s,a) as softmax logits to weigh sampling, biasing towards high or low value states while maintaining state space coverage.
- **Core assumption:** The sampling method effectively balances attack success and task performance across different environments.
- **Evidence anchors:**
  - [section 5]: "In Q-Incept we use the absolute value of F̂Q(s, a) as softmax logits to weigh how we sample H′ ⊆ H in step 4."
  - [section 6]: "Across all seven environments Q-Incept outperforms both SleeperNets and TrojDRL in terms of ASR while maintaining better or comparable BR scores."
  - [corpus]: No direct corpus evidence found for this specific mechanism.
- **Break condition:** If the sampling method fails to effectively balance objectives, performance degrades.

## Foundational Learning

- **Concept:** Markov Decision Processes (MDPs)
  - Why needed here: The attack framework is formulated in terms of MDPs, requiring understanding of states, actions, rewards, and transitions.
  - Quick check question: What are the four components of an MDP?

- **Concept:** Deep Q-Learning
  - Why needed here: The attack uses Deep Q-Learning to approximate the Q-function for identifying high-return actions.
  - Quick check question: How does Deep Q-Learning approximate the optimal action-value function?

- **Concept:** Backdoor Poisoning Attacks
  - Why needed here: The attack is a specific type of backdoor poisoning attack against DRL algorithms.
  - Quick check question: What are the two primary objectives of backdoor poisoning attacks?

## Architecture Onboarding

- **Component map:**
  - Victim DRL agent training on MDP M -> Adversary observes training episodes -> Q-Incept attack manipulates states/actions/rewards -> Agent trains on poisoned data

- **Critical path:**
  1. Adversary observes training episodes
  2. Q-Incept approximates Q-function using DQN
  3. Identifies high-return actions using F̂Q metric
  4. Replaces actions with target action in poisoned states
  5. Updates agent's replay memory with manipulated data
  6. Agent trains on poisoned data, learning to associate trigger with target action

- **Design tradeoffs:**
  - Poisoning rate vs. attack success vs. benign task performance
  - Q-function approximation accuracy vs. computational overhead
  - Trigger pattern visibility vs. evasion of defenses

- **Failure signatures:**
  - Low attack success rate despite high poisoning rate
  - Significant drop in benign task performance
  - Q-function approximation fails to converge

- **First 3 experiments:**
  1. Test Q-Incept on simple MDP with known optimal actions to verify mechanism
  2. Evaluate attack success vs. poisoning rate on Atari Q*bert environment
  3. Compare Q-Incept performance against baselines under different reward constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different Q-function approximation methods (beyond DQN) affect Q-Incept's attack success rate and computational efficiency?
- Basis in paper: [explicit] The paper mentions that better Q-function approximations lead to better performance, as demonstrated by Oracle-Incept's superior results compared to Q-Incept.
- Why unresolved: The paper only evaluates DQN for Q-function approximation. Exploring other methods like Monte Carlo Tree Search or model-based approaches could reveal more efficient or effective alternatives.
- What evidence would resolve it: Experiments comparing Q-Incept's performance using various Q-function approximation methods (DQN, MCTS, model-based RL) across multiple environments, measuring both attack success rate and computational overhead.

### Open Question 2
- Question: Can adaptive trigger patterns be developed that maintain high attack success under state sanitization defenses without sacrificing stealth?
- Basis in paper: [explicit] The paper shows that state sanitization defenses can be evaded by using "evasive triggers" that account for the defense's SVD-based filtering, but this requires prior knowledge of the defense mechanism.
- Why unresolved: The paper only demonstrates evasion using a simple SVD-based defense. More sophisticated defenses or adaptive trigger optimization methods could potentially close this gap.
- What evidence would resolve it: Testing Q-Incept against multiple state sanitization techniques (different SVD thresholds, PCA-based methods) using triggers optimized through adversarial training or genetic algorithms.

### Open Question 3
- Question: What is the relationship between environment reward structure (positive-only vs. mixed rewards) and the effectiveness of adversarial inception attacks?
- Basis in paper: [explicit] The paper notes that environments with only positive rewards (like Highway Merge) are particularly difficult to poison, as the attacker cannot penalize the agent for ignoring the target action.
- Why unresolved: The paper only provides anecdotal evidence from one environment. A systematic study across environments with varying reward structures could reveal general principles.
- What evidence would resolve it: Experiments comparing Q-Incept's performance across environments with different reward characteristics (positive-only, negative-only, mixed) while controlling for other factors like action space size and episode length.

## Limitations

- The paper's claims rely heavily on theoretical guarantees that may not fully translate to practical scenarios with imperfect Q-function approximations.
- The assumption that adversaries can perfectly observe training episodes may not hold in many practical settings where training data is private or distributed.
- The formalized reward constraints, while theoretically sound, may not reflect the full complexity of real-world detection mechanisms and defense strategies.

## Confidence

- **High confidence:** The paper's contribution of formalizing reward constraints and proving limitations of prior approaches is well-supported by theoretical analysis.
- **Medium confidence:** The adversarial inception framework's ability to overcome reward constraints while maintaining benign performance is demonstrated empirically but relies on assumptions about Q-function approximation quality.
- **Low confidence:** The claim that Q-Incept achieves 100% attack success rate across all tested environments may be overly optimistic, as this performance depends on the specific hyperparameters and environment configurations used.

## Next Checks

1. **Q-function approximation validation:** Test the sensitivity of attack success to Q-function approximation quality by systematically degrading the approximation accuracy and measuring the impact on attack performance.
2. **Reward constraint realism:** Evaluate whether the formalized reward constraints reflect realistic scenarios by testing the attack against defenders with varying detection capabilities and reward manipulation thresholds.
3. **Cross-environment generalizability:** Validate the attack's performance on a broader range of DRL environments, including those with continuous state and action spaces, to assess the framework's scalability and robustness.