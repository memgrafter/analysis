---
ver: rpa2
title: Exploring and Benchmarking the Planning Capabilities of Large Language Models
arxiv_id: '2406.13094'
source_url: https://arxiv.org/abs/2406.13094
tags:
- planning
- conn
- gemini
- language
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates planning capabilities of LLMs using PDDL
  and natural language benchmarks. The authors introduce a scalable benchmark suite,
  evaluate in-context learning with many-shot examples, explore inference-time techniques
  like MCTS and tree-of-thought, and assess fine-tuning with optimal plans.
---

# Exploring and Benchmarking the Planning Capabilities of Large Language Models

## Quick Facts
- arXiv ID: 2406.13094
- Source URL: https://arxiv.org/abs/2406.13094
- Reference count: 33
- One-line primary result: Fine-tuning LLMs on optimal plans achieves near-perfect accuracy while in-context learning improves with more examples; inference-time techniques enable smaller models to approach larger model performance.

## Executive Summary
This paper investigates the planning capabilities of large language models (LLMs) using both classical planning benchmarks (PDDL) and natural language planning tasks. The authors introduce a comprehensive benchmark suite and evaluate three approaches: many-shot in-context learning, fine-tuning with optimal plans, and inference-time chain-of-thought techniques. Their key finding is that while in-context learning performance scales with more examples, fine-tuning with optimal plans achieves superior accuracy even with smaller models. They also demonstrate that chain-of-thought methods like MCTS and Tree-of-Thought can help smaller models approach the performance of larger models on natural language tasks.

## Method Summary
The paper evaluates LLM planning capabilities through three main approaches: in-context learning with varying numbers of examples, supervised fine-tuning on optimal planning paths, and inference-time chain-of-thought reasoning methods (MCTS, Tree-of-Thought, debate-as-reasoning). The authors created benchmark datasets by generating PDDL planning problems (BlocksWorld, Logistics, Mini-Grid) with varying difficulty levels and mapping them to natural language. They also used native natural language datasets from NaturalPlan (Trip Planning, Calendar Scheduling). Performance was measured using PDDL verifiers for formal tasks and comparison to expected results for natural language tasks, with analysis of failure modes and generalization patterns.

## Key Results
- Fine-tuning with optimal plans achieves near-perfect accuracy even with smaller models, outperforming many-shot in-context learning on most benchmarks
- In-context learning performance improves significantly with increased context length, demonstrating strong scaling with more examples
- Chain-of-thought reasoning methods (MCTS, ToT, debate-as-reasoning) enable smaller models like Gemma2 27B to approach performance of larger models like GPT-4 and Gemini 1.5 Pro on natural language tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Many-shot in-context learning significantly improves LLM planning performance by providing sufficient examples for the model to learn task structure and constraints.
- **Mechanism:** When provided with multiple examples of planning problems and their solutions, the LLM can infer the underlying patterns and logic required for planning, including state transitions, action constraints, and goal satisfaction.
- **Core assumption:** The LLM can effectively extract and generalize planning patterns from the provided examples within its context window.
- **Evidence anchors:**
  - [abstract] "We investigate the use of many-shot in-context learning to enhance LLM planning, exploring the relationship between increased context length and improved planning performance."
  - [section 3.1.1] "Figure 2 shows the in-context learning performance on classical planning and natural language benchmarks as we vary the number of shots."
- **Break condition:** The model's context window becomes saturated, or the examples provided are too diverse to establish consistent patterns.

### Mechanism 2
- **Claim:** Fine-tuning with optimal plans enables smaller models to achieve near-perfect planning accuracy by directly learning the target task.
- **Mechanism:** Supervised fine-tuning on optimal planning paths allows the model to learn the specific reasoning and decision-making processes required for planning, bypassing the need for extensive in-context learning.
- **Core assumption:** The model can effectively learn the planning task from optimal examples without requiring the same level of generalization as in-context learning.
- **Evidence anchors:**
  - [abstract] "We demonstrate the positive impact of fine-tuning LLMs on optimal planning paths."
  - [section 3.2] "The results are shown in Table 1. We observe that SFT leads to high accuracy for some instances of both datasets and outperforms many-shot ICL."
- **Break condition:** The fine-tuning dataset is insufficient or not representative of the task complexity, leading to overfitting or poor generalization.

### Mechanism 3
- **Claim:** Chain-of-thought reasoning methods (MCTS, ToT, debate-as-reasoning) allow smaller models to approach performance of larger ones by explicitly modeling the planning process.
- **Mechanism:** These inference-time techniques construct a chain of reasoning, allowing the model to explore multiple solution paths and refine its plan iteratively, compensating for the smaller model's limited knowledge.
- **Core assumption:** The model can effectively use these reasoning procedures to simulate a more extensive planning process, even with limited inherent capabilities.
- **Evidence anchors:**
  - [abstract] "We also probe the efficacy of chain-of-thought reasoning methods to improve LLM planning performance."
  - [section 3.1.2] "In Figure 4 we provide experimental evidence that methods such as Debate-as-reasoning, MCTS, and ToT can augment Gemma2 27B... to be competitive with GPT-4, Gemini 1.5 Pro and Gemini 1.5 Flash."
- **Break condition:** The reasoning procedures become computationally expensive or fail to provide meaningful improvements for simpler planning tasks.

## Foundational Learning

- **Concept: PDDL (Planning Domain Definition Language)**
  - **Why needed here:** PDDL is the formal language used to represent classical planning problems, and understanding its structure is crucial for evaluating and generating plans.
  - **Quick check question:** What are the two main components of a PDDL problem definition?

- **Concept: In-context learning vs. Fine-tuning**
  - **Why needed here:** The paper compares these two approaches for improving LLM planning, and understanding their differences is key to interpreting the results.
  - **Quick check question:** What is the main difference between in-context learning and fine-tuning in terms of how the model learns the task?

- **Concept: Chain-of-thought reasoning**
  - **Why needed here:** This is a key technique explored in the paper for improving LLM planning performance, and understanding its principles is essential for evaluating its effectiveness.
  - **Quick check question:** How does chain-of-thought reasoning differ from standard prompting in terms of the model's inference process?

## Architecture Onboarding

- **Component map:** LLM models (Gemini 1.5 Pro, GPT-4 Turbo, Gemma 2 27B) -> Benchmark datasets (PDDL and natural language) -> Planning methods (ICL, SFT, CoT) -> Verifier evaluation -> Results analysis
- **Critical path:** Generate benchmark instances → Apply planning method (ICL, SFT, CoT) → Generate plan → Evaluate plan using verifier → Analyze results
- **Design tradeoffs:** In-context learning offers flexibility but may be limited by context window; fine-tuning provides better performance but requires labeled data; CoT methods improve smaller models but add computational overhead
- **Failure signatures:** Plan does not satisfy constraints, plan does not reach goal state, generated actions are invalid, poor generalization to out-of-domain instances
- **First 3 experiments:**
  1. Evaluate in-context learning performance with varying numbers of examples on BlocksWorld benchmark
  2. Compare fine-tuning performance with in-context learning on Logistics benchmark
  3. Assess plan generalization by training on easier instances and evaluating on harder ones

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do inference-time techniques (MCTS, ToT, debate-as-reasoning) scale with model size for planning tasks?
- Basis in paper: [explicit] The paper compares inference-time techniques on Gemma2 27B against larger models like GPT-4 and Gemini 1.5, finding competitive performance at smaller context lengths but inconsistent scaling.
- Why unresolved: The paper only tests one open-source model (Gemma2 27B) and doesn't explore scaling trends across multiple model sizes or benchmarks.
- What evidence would resolve it: Experiments testing multiple model sizes (e.g., Gemma variants, Llama models) with inference-time techniques across diverse planning benchmarks to establish scaling laws.

### Open Question 2
- Question: What are the specific failure patterns that distinguish out-of-distribution generalization failures from in-distribution ones in planning tasks?
- Basis in paper: [explicit] The paper analyzes failure modes (constraint violations, goal failure, invalid actions) and finds models fail earlier on unseen harder examples, but doesn't deeply characterize distribution-specific failure patterns.
- Why unresolved: While failure modes are categorized, the paper doesn't provide a detailed comparison of how these patterns differ between in-distribution and OOD scenarios.
- What evidence would resolve it: Detailed failure analysis comparing step-by-step error distributions and constraint violation types between in-distribution and OOD test sets.

### Open Question 3
- Question: How does the effectiveness of fine-tuning compare to inference-time techniques across different planning task complexities?
- Basis in paper: [explicit] The paper shows fine-tuning achieves near-perfect accuracy on simpler tasks but degrades on harder ones, while inference-time techniques help smaller models approach larger model performance.
- Why unresolved: The paper doesn't directly compare fine-tuning and inference-time techniques on the same tasks or explore hybrid approaches combining both.
- What evidence would resolve it: Head-to-head comparisons of fine-tuned vs. inference-time technique performance across tasks of varying complexity, including potential hybrid methods.

## Limitations
- The evaluation is constrained by relatively small-scale benchmarks that may not fully capture real-world planning complexity
- The comparison between in-context learning and fine-tuning doesn't account for computational costs and data requirements of fine-tuning
- Natural language tasks are still somewhat simplified compared to practical applications

## Confidence
- **High confidence:** Fine-tuning with optimal plans significantly improves planning accuracy, particularly for smaller models
- **Medium confidence:** Many-shot in-context learning improves performance as context length increases
- **Medium confidence:** Chain-of-thought reasoning methods can close the performance gap between smaller and larger models

## Next Checks
1. **Scale validation:** Test the proposed methods on larger, more complex planning benchmarks that better represent real-world scenarios, including longer plan horizons and more intricate constraints
2. **Resource analysis:** Conduct a comprehensive comparison of computational costs, including inference time and fine-tuning requirements, to provide a complete picture of the practical viability of each approach
3. **Robustness testing:** Evaluate model performance on out-of-distribution planning tasks and adversarial examples to assess the true generalization capabilities beyond the reported in-domain results