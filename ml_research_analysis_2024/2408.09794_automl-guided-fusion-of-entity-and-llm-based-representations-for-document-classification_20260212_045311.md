---
ver: rpa2
title: AutoML-guided Fusion of Entity and LLM-based Representations for Document Classification
arxiv_id: '2408.09794'
source_url: https://arxiv.org/abs/2408.09794
tags:
- representations
- embeddings
- text
- learning
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BabelFusion, a novel method that enhances
  LLM-based document representations by incorporating knowledge from semantic knowledge
  graphs. The approach leverages automated machine learning (AutoML) and dimensionality
  reduction techniques to fuse text-based representations with knowledge graph embeddings,
  resulting in more robust and efficient document representations for classification
  tasks.
---

# AutoML-guided Fusion of Entity and LLM-based Representations for Document Classification

## Quick Facts
- arXiv ID: 2408.09794
- Source URL: https://arxiv.org/abs/2408.09794
- Reference count: 40
- Primary result: BabelFusion improves document classification accuracy by 0.52% average across six datasets by fusing LLM text representations with knowledge graph embeddings

## Executive Summary
This paper introduces BabelFusion, a novel method that enhances LLM-based document representations by incorporating knowledge from semantic knowledge graphs. The approach leverages automated machine learning (AutoML) and dimensionality reduction techniques to fuse text-based representations with knowledge graph embeddings, resulting in more robust and efficient document representations for classification tasks. The method demonstrates superior performance compared to traditional text-based representations, achieving an average improvement of 0.52% in classification accuracy across six diverse datasets. Notably, BabelFusion enables effective learning in low-dimensional spaces, outperforming baseline methods even with projections as low as 2 dimensions. The approach is particularly effective for news genre classification and sentiment analysis, showcasing its versatility across different domains.

## Method Summary
BabelFusion fuses text-based representations from large language models with knowledge graph embeddings to enhance document classification. The method extracts document embeddings from chosen LLM models (Angle, OpenAI-small/large, mxbai, LLM2Vec-LLaMa3) using HuggingFace and sentence-transformers. Knowledge graph embeddings are generated using Babelfy entity linking on WikiData5m subgraph with RotatE algorithm, then aggregated per document. The text and KG embeddings are concatenated, reduced via SVD dimensionality reduction, and classified using AutoML (TPOT) with 5-fold cross-validation, compared against ridge regression baseline.

## Key Results
- BabelFusion achieves an average 0.52% improvement in classification accuracy across six diverse datasets
- The method demonstrates effectiveness in very low-dimensional spaces, outperforming baselines even with projections as low as 2 dimensions
- Superior performance particularly notable for news genre classification and sentiment analysis tasks

## Why This Works (Mechanism)
The fusion of LLM-based text representations with knowledge graph embeddings captures both contextual semantic information and structured factual knowledge, creating richer document representations. By integrating external knowledge through entity linking and leveraging AutoML for optimal classifier selection, BabelFusion can identify patterns and relationships that pure text-based representations might miss. The dimensionality reduction via SVD enables efficient learning in lower-dimensional spaces while preserving the most informative features from the fused representations.

## Foundational Learning
- **Entity Linking (Babelfy)**: Maps textual mentions to knowledge graph entities, enriching documents with structured knowledge from semantic networks. Why needed: Provides access to external knowledge that supplements language model understanding. Quick check: Verify Babelfy correctly identifies entities relevant to document domain.
- **Knowledge Graph Embeddings (RotatE)**: Generates vector representations of knowledge graph entities that capture semantic relationships. Why needed: Transforms discrete entity links into continuous vectors suitable for mathematical operations. Quick check: Ensure embedding quality by examining nearest neighbors for known entities.
- **Dimensionality Reduction (SVD)**: Projects high-dimensional fused representations into lower-dimensional spaces while preserving variance. Why needed: Reduces computational complexity and mitigates overfitting in downstream classification. Quick check: Monitor explained variance ratio to ensure sufficient information retention.
- **Automated Machine Learning (TPOT)**: Automatically searches for optimal machine learning pipelines and hyperparameters. Why needed: Eliminates manual hyperparameter tuning and discovers non-obvious model combinations. Quick check: Verify TPOT search completes successfully within specified time/memory constraints.
- **HuggingFace Embeddings**: Extracts contextual document representations from various LLM models. Why needed: Provides rich, pre-trained semantic representations as foundation for fusion. Quick check: Compare embedding similarity scores for semantically related documents.

## Architecture Onboarding

**Component Map**: LLM Embedding Extraction -> Entity Linking (Babelfy) -> KG Embedding Generation (RotatE) -> Concatenation -> SVD Dimensionality Reduction -> AutoML Classification (TPOT)

**Critical Path**: The most time-consuming steps are KG embedding generation (requires external API calls and graph processing) and TPOT AutoML search (can take hours depending on parameters). The SVD projection is computationally efficient even for large matrices.

**Design Tradeoffs**: Using WikiData5m subgraph balances coverage with computational feasibility, though larger knowledge graphs might improve performance. TPOT provides flexibility but introduces reproducibility challenges compared to fixed models.

**Failure Signatures**: 
- Low classification accuracy despite fusion suggests poor entity linking quality or mismatched embedding spaces
- SVD truncation leading to accuracy drops indicates loss of critical information
- TPOT optimization failure points to incompatible pipeline components or insufficient search resources

**3 First Experiments**:
1. Verify BabelFusion pipeline on a single dataset with known optimal SVD dimension to establish baseline functionality
2. Test entity linking quality by manually inspecting extracted entities for representative documents across different domains
3. Compare TPOT vs ridge regression baseline on concatenated (non-reduced) embeddings to validate AutoML contribution

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the performance of BabelFusion scale with the size and quality of the knowledge graph used for entity embeddings?
- Basis in paper: The paper mentions using WikiData5m subgraph of BabelNet, but suggests exploring the entire graph in future work.
- Why unresolved: The current study uses a limited subgraph, so the impact of using larger or more comprehensive knowledge graphs is unknown.
- What evidence would resolve it: Comparative experiments using different knowledge graph sizes and qualities, measuring classification accuracy and computational efficiency.

### Open Question 2
- Question: Can the fusion process be improved by incorporating advanced entity linking and word sense disambiguation techniques beyond the current Babelfy implementation?
- Basis in paper: The paper notes that the current approach "did not explore document representations from generative and large language models or apply sophisticated entity linking and word sense disambiguation."
- Why unresolved: The current method uses a basic entity linking approach, potentially leaving room for improvement with more advanced techniques.
- What evidence would resolve it: Experiments comparing classification performance using different entity linking and disambiguation methods while keeping other variables constant.

### Open Question 3
- Question: Is there an optimal dimension for the SVD projection that varies depending on the dataset characteristics, and can this be predicted or determined automatically?
- Basis in paper: The paper finds that the optimal projection dimension varies across datasets and embeddings, but does not explore automated dimension selection.
- Why unresolved: While the paper identifies the importance of dimension selection, it does not provide a method for determining the optimal dimension for a given dataset.
- What evidence would resolve it: Development and validation of a method to predict or automatically determine the optimal SVD projection dimension based on dataset characteristics.

## Limitations
- The 0.52% average accuracy improvement, while statistically meaningful, represents a relatively small practical improvement over strong baselines
- Performance improvements may be sensitive to specific implementation details and hyperparameter choices
- Reliance on entity linking quality and specific knowledge graph (WikiData5m) may limit generalizability to domains with different knowledge structures or languages

## Confidence

**High Confidence**: The core methodology of fusing text and KG embeddings is clearly described and reproducible, with well-defined steps for representation extraction and dimensionality reduction.

**Medium Confidence**: The reported performance improvements and their consistency across datasets, as the modest gains may be sensitive to specific implementation details and hyperparameter choices.

**Low Confidence**: The practical significance of the improvements in real-world applications, particularly given the minimal dimensionality reduction benefits in high-dimensional spaces and the computational overhead of the multi-stage pipeline.

## Next Checks

1. **Implementation Reproducibility**: Replicate the BabelFusion pipeline on at least two datasets using publicly available code for Babelfy entity linking and TPOT AutoML, verifying the reported accuracy improvements and checking for potential implementation-specific optimizations.

2. **Robustness to Entity Linking Quality**: Systematically evaluate the method's sensitivity to entity linking errors by artificially introducing noise in the Babelfy output and measuring degradation in classification accuracy across different levels of noise.

3. **Dimensionality Reduction Trade-offs**: Conduct a detailed ablation study examining the relationship between SVD truncation dimension, explained variance retention, and classification performance to identify optimal dimensionality reduction strategies for different dataset characteristics.