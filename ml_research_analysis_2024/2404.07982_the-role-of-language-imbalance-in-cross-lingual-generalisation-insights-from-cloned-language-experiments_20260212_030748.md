---
ver: rpa2
title: 'The Role of Language Imbalance in Cross-lingual Generalisation: Insights from
  Cloned Language Experiments'
arxiv_id: '2404.07982'
source_url: https://arxiv.org/abs/2404.07982
tags:
- languages
- language
- training
- performance
- cloned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how language imbalance during training
  affects cross-lingual generalization in multilingual language models. Through controlled
  experiments with perfectly equivalent cloned languages, the authors find that having
  a dominant language during training improves performance on lower-resource languages
  and leads to stronger alignment of model representations across languages.
---

# The Role of Language Imbalance in Cross-lingual Generalisation: Insights from Cloned Language Experiments

## Quick Facts
- **arXiv ID**: 2404.07982
- **Source URL**: https://arxiv.org/abs/2404.07982
- **Reference count**: 19
- **Primary result**: Language imbalance during training can improve cross-lingual generalization, with dominant languages leading to better alignment and performance across all languages.

## Executive Summary
This study investigates how language imbalance during training affects cross-lingual generalization in multilingual language models. Through controlled experiments with perfectly equivalent cloned languages, the authors find that having a dominant language during training improves performance on lower-resource languages and leads to stronger alignment of model representations across languages. Surprisingly, they observe that with large enough models or long enough training, a 90/10 language split yields better performance on both languages than a balanced 50/50 split. These insights enable the design of training schedules that improve performance across all languages without altering training data. When extending analysis to real languages (English and French), the authors find that infrequent languages still benefit from frequent ones, though the impact of language imbalance on cross-lingual generalization is less conclusive in this setting.

## Method Summary
The authors conduct controlled experiments using cloned languages - perfectly equivalent synthetic languages created by token permutation. They train multilingual language models on these languages in varying proportions (50/50, 90/10, etc.) and evaluate cross-lingual performance. The experiments are performed across different model sizes and training durations to test the robustness of findings. Additionally, the authors extend their analysis to real language pairs (English and French) to assess the generalizability of their results to practical scenarios.

## Key Results
- Language imbalance during training can improve cross-lingual generalization, with dominant languages leading to better alignment and performance across all languages
- A 90/10 language split can yield better performance on both languages than a balanced 50/50 split with large enough models or long enough training
- Infrequent languages benefit from frequent ones in real language settings, though the impact of imbalance is less conclusive

## Why This Works (Mechanism)
The mechanism behind these findings appears to involve the development of shared model components across languages when training with a dominant language. The presence of a main language during training may lead to stronger cross-lingual alignment of representations, enabling better transfer of knowledge to lower-resource languages. This shared component development could explain why both languages benefit from the 90/10 split despite the imbalance.

## Foundational Learning
- **Cross-lingual alignment**: The degree to which representations of different languages are positioned similarly in the model's embedding space. Why needed: Critical for understanding how models transfer knowledge between languages. Quick check: Measure representation similarity across languages using metrics like centered kernel alignment (CKA).
- **Language imbalance in training**: The distribution of training data across different languages, often with some languages being significantly more represented than others. Why needed: Directly relates to the study's core investigation. Quick check: Calculate the ratio of training examples between the most and least represented languages.
- **Cloned languages**: Synthetic languages created by permuting tokens in a base language to create perfectly equivalent but distinct languages. Why needed: Enables controlled experiments by eliminating confounding linguistic differences. Quick check: Verify that cloned languages have identical vocabulary sizes and token distributions.
- **Encoder-decoder models**: Transformer-based architectures with separate encoding and decoding components, commonly used in machine translation and sequence-to-sequence tasks. Why needed: The primary model architecture used in the study. Quick check: Count the number of encoder and decoder layers in the model architecture.

## Architecture Onboarding

### Component Map
Tokenizer -> Encoder -> Decoder -> Language-Specific Heads

### Critical Path
Input text → Tokenization → Encoding (cross-lingual representation) → Decoding (language-specific output)

### Design Tradeoffs
- **Pro**: Cloned languages enable perfect control over linguistic equivalence, isolating the effect of language imbalance
- **Con**: Results from synthetic languages may not fully generalize to real language pairs with genuine linguistic differences
- **Pro**: Testing across different model sizes and training durations provides robustness to findings
- **Con**: Focus on encoder-decoder models limits generalizability to other architectures

### Failure Signatures
- Performance degradation on the dominant language when using extreme imbalances (e.g., 99/1 splits)
- Weakened cross-lingual alignment in models trained on balanced language splits
- Inconsistent benefits of language imbalance across different model sizes or task types

### First 3 Experiments
1. Train a model on a 50/50 split of cloned languages and measure cross-lingual alignment using centered kernel alignment (CKA)
2. Compare performance on a 90/10 split versus 50/50 split for both languages across different model sizes
3. Extend the 90/10 vs 50/50 comparison to a real language pair (e.g., English-French) to assess practical applicability

## Open Questions the Paper Calls Out
None

## Limitations
- Results from perfectly cloned synthetic languages may not fully capture the complexities of real-world language relationships
- The surprising finding that 90/10 splits outperform 50/50 splits needs further validation across diverse settings
- The study focuses primarily on encoder-decoder models and token classification tasks, limiting generalizability to other architectures and task types

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Language imbalance during training can improve cross-lingual generalization | High |
| Shared model components across languages due to dominant language training | Medium |
| 90/10 splits can outperform 50/50 splits | Medium |

## Next Checks
1. Replicate the synthetic language experiments across different model architectures (e.g., encoder-only, decoder-only) to test generalizability
2. Test the 90/10 vs 50/50 split findings with actual linguistically related language pairs to verify real-world applicability
3. Investigate whether the improved cross-lingual performance comes at the cost of monolingual performance degradation for the dominant language