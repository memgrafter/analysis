---
ver: rpa2
title: Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking
  Advanced Quantitative Reasoning with Data
arxiv_id: '2402.17644'
source_url: https://arxiv.org/abs/2402.17644
tags:
- data
- reasoning
- causal
- questions
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Quantitative Reasoning with Data (QRData)
  benchmark to evaluate large language models' abilities in statistical and causal
  reasoning with real-world data. The benchmark comprises 411 questions from textbooks,
  online learning materials, and academic papers, along with 195 accompanying data
  sheets.
---

# Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data

## Quick Facts
- arXiv ID: 2402.17644
- Source URL: https://arxiv.org/abs/2402.17644
- Reference count: 16
- Best model GPT-4 achieves 58% accuracy on the benchmark

## Executive Summary
This paper introduces the Quantitative Reasoning with Data (QRData) benchmark to evaluate large language models' abilities in statistical and causal reasoning with real-world data. The benchmark comprises 411 questions from textbooks, online learning materials, and academic papers, along with 195 accompanying data sheets. Models are evaluated using various reasoning approaches including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants. The best-performing model GPT-4 achieves 58% accuracy, while the top open-source model Deepseek-coder-instruct reaches 37%. Analysis reveals that models struggle particularly with data analysis and causal reasoning tasks, especially when integrating causal knowledge with provided data. The benchmark highlights significant room for improvement in advanced quantitative reasoning capabilities of current language models.

## Method Summary
The QRData benchmark evaluates LLMs on statistical and causal reasoning tasks using 411 questions paired with 195 data sheets. The evaluation employs multiple reasoning approaches: Chain-of-Thought (CoT), Program-of-Thoughts (PoT), ReAct-style prompting, and code interpreter assistants. Models are tested across different sizes and architectures, including GPT-4, Deepseek-coder-instruct, and various open-source LLMs. Questions are categorized into statistical and causal reasoning types, with answers extracted through a unified pipeline. Accuracy is measured on multiple-choice and numerical questions, with numerical answers accepted within ±3% tolerance. The benchmark includes diverse data formats and reasoning complexities to assess comprehensive quantitative reasoning capabilities.

## Key Results
- GPT-4 achieves the highest accuracy at 58% across all reasoning approaches and question types
- Deepseek-coder-instruct outperforms other open-source models by more than 10% with 37% accuracy
- Models show significantly lower performance on causal reasoning (40% accuracy) compared to statistical reasoning (50% accuracy)
- Code-based reasoning approaches consistently outperform natural language approaches, particularly for numerical questions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multi-turn agent reasoning with code execution significantly improves performance on quantitative reasoning with data compared to single-turn approaches.
- **Mechanism**: Multi-turn reasoning allows models to iteratively refine their code generation based on execution results, fixing errors and adjusting approaches. This is particularly valuable for complex data analysis tasks where initial code may contain errors or require refinement.
- **Core assumption**: The model's ability to generate executable code in the first place, and that execution feedback provides meaningful information for subsequent refinement.
- **Evidence anchors**:
  - [abstract]: "Code and data are in https://github.com/xxxiaol/QRData" - provides empirical evidence through benchmark results
  - [section]: "Although the overall performance of several non-code LLMs drops with PoT compared to CoT, the accuracy of numerical questions improves with the help of code execution" - demonstrates mechanism effectiveness
  - [corpus]: Weak evidence - related papers focus on different aspects of LLM evaluation
- **Break condition**: If the model cannot generate executable code initially, or if execution results are not interpretable by the model for refinement.

### Mechanism 2
- **Claim**: Code generation capabilities are more critical than general mathematical reasoning abilities for quantitative reasoning with data.
- **Mechanism**: Data analysis requires precise calculations and data manipulation that are best performed through code execution rather than natural language reasoning. Models with strong code generation abilities can leverage data analysis libraries for accurate computation.
- **Core assumption**: The provided data can be effectively processed through code execution, and that code execution is more accurate than natural language mathematical reasoning for complex data analysis.
- **Evidence anchors**:
  - [abstract]: "The strongest model GPT-4 achieves an accuracy of 58%, which has much room for improvement" - indicates room for specialized approaches
  - [section]: "Deepseek-coder-instruct, which masters in code generation, outperforms all other models with similar sizes by more than 10%" - directly supports code capability importance
  - [corpus]: Moderate evidence - related work on code generation for data science tasks
- **Break condition**: If the data format is incompatible with standard code libraries, or if the model's code generation is too error-prone to be useful.

### Mechanism 3
- **Claim**: Causal reasoning is fundamentally more difficult than statistical reasoning for LLMs, even when controlling for data analysis complexity.
- **Mechanism**: Statistical reasoning involves pattern recognition and probability calculations that LLMs can handle reasonably well, while causal reasoning requires understanding counterfactuals and intervention effects that are more abstract and less represented in training data.
- **Core assumption**: The training data contains sufficient statistical reasoning examples but limited causal reasoning examples, and that the models' internal knowledge about causality is not easily integrated with provided data.
- **Evidence anchors**:
  - [abstract]: "Analysis reveals that models encounter difficulties in data analysis and causal reasoning" - highlights the distinction
  - [section]: "GPT-4 achieves 89% accuracy in statistical questions on QRTEXT, but only about half the accuracy in causal questions" - quantitative comparison
  - [corpus]: Strong evidence - related papers on LLM causal reasoning capabilities
- **Break condition**: If causal reasoning examples become more prevalent in training data, or if models develop better mechanisms for integrating internal knowledge with external data.

## Foundational Learning

- **Concept**: Conditional probability and Bayes' theorem
  - Why needed here: Essential for understanding statistical inference questions and calculating probabilities from data
  - Quick check question: If P(A|B) = 0.6 and P(B) = 0.5, what is P(A∩B)?

- **Concept**: Counterfactual reasoning and intervention
  - Why needed here: Core to causal reasoning - understanding what would happen under different treatment conditions
  - Quick check question: If a drug reduces mortality from 20% to 10% in treated patients, what is the average treatment effect?

- **Concept**: Data manipulation and analysis using pandas/Python
  - Why needed here: Required for implementing the program-based reasoning approaches that show best performance
  - Quick check question: Given a dataframe df with columns 'treatment' and 'outcome', write code to calculate the mean outcome for each treatment group.

## Architecture Onboarding

- **Component map**: Data ingestion → Code generation → Code execution → Result interpretation → Answer generation → Answer extraction
- **Critical path**: Code generation → Code execution → Result interpretation (errors here propagate through entire pipeline)
- **Design tradeoffs**: Single-turn vs multi-turn reasoning (simplicity vs accuracy), natural language vs code execution (flexibility vs precision), model size vs performance (cost vs capability)
- **Failure signatures**: Code generation failures (syntax errors, library import issues), execution failures (runtime errors, data type mismatches), interpretation failures (misreading execution output, incorrect answer extraction)
- **First 3 experiments**:
  1. Compare single-turn vs multi-turn reasoning on a subset of questions to quantify the benefit of iterative refinement
  2. Test different code libraries (pandas vs numpy vs custom implementations) to identify optimal data analysis approach
  3. Implement error handling and recovery mechanisms in the code generation pipeline to improve robustness

## Open Questions the Paper Calls Out
None

## Limitations
- Current LLMs show significant limitations in quantitative reasoning, with best models achieving only 58% accuracy on the benchmark
- Models struggle particularly with causal reasoning, often confusing correlation with causation and failing to integrate internal causal knowledge with provided data
- The reliance on code execution for optimal performance raises questions about whether models truly understand quantitative reasoning versus leveraging code generation capabilities

## Confidence
- **High confidence**: The overall finding that current LLMs struggle with quantitative reasoning tasks is well-supported by the empirical results across multiple models and reasoning approaches.
- **Medium confidence**: The specific claim that causal reasoning is more difficult than statistical reasoning is supported by the data but could be influenced by the particular question selection or data presentation format.
- **Medium confidence**: The superiority of code-based reasoning approaches is well-demonstrated, though the extent of improvement may vary with different model versions or prompt engineering techniques.

## Next Checks
1. **Generalization Test**: Evaluate models on a held-out subset of questions or entirely new data sets to assess whether performance gains from code execution generalize beyond the specific benchmark questions.
2. **Causal Reasoning Robustness**: Design experiments that explicitly separate causal knowledge from data presentation, testing whether models can apply causal reasoning when data is misleading or absent.
3. **Model Architecture Analysis**: Compare different model architectures (transformer variants, retrieval-augmented approaches) to determine whether performance limitations are fundamental to current architectures or can be addressed through architectural innovations.