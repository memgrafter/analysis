---
ver: rpa2
title: Abstractive summarization from Audio Transcription
arxiv_id: '2408.04639'
source_url: https://arxiv.org/abs/2408.04639
tags:
- training
- which
- summarization
- text
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents an end-to-end (E2E) audio summarization model
  that combines a fine-tuned Whisper ASR model with a fine-tuned text summarization
  model using parameter-efficient fine-tuning methods. The study evaluates two main
  approaches: LoRA and AdaLoRA for fine-tuning, along with quantization techniques
  to reduce model size and training time.'
---

# Abstractive summarization from Audio Transcription

## Quick Facts
- arXiv ID: 2408.04639
- Source URL: https://arxiv.org/abs/2408.04639
- Authors: Ilia Derkach
- Reference count: 0
- Best models achieved ROUGE-1 scores of 33.4 (Gazeta) and 36.1 (WikiHow)

## Executive Summary
This paper presents an end-to-end audio summarization model that combines a fine-tuned Whisper ASR model with a fine-tuned text summarization model using parameter-efficient fine-tuning methods. The study evaluates LoRA and AdaLoRA for fine-tuning, along with quantization techniques to reduce model size and training time. The AdaLoRA method consistently outperformed LoRA across all datasets and metrics, with optimal performance achieved at low adapter ranks (16-32).

## Method Summary
The study combines a fine-tuned Whisper ASR model with a fine-tuned text summarization model using parameter-efficient fine-tuning methods. For text summarization, MBart and T5 models were fine-tuned on Gazeta (Russian news) and WikiHow (English instructional content) datasets using LoRA and AdaLoRA methods. For ASR, Whisper models of varying sizes were fine-tuned using different quantization schemes (int4, int8) with and without AdaLoRA. The optimal AdaLoRA performance was achieved at adapter ranks of 16-32, while int8 quantization with AdaLoRA yielded the best WER results.

## Key Results
- AdaLoRA method consistently outperformed LoRA across all datasets and metrics
- Best models achieved ROUGE-1 scores of 33.4 (Gazeta) and 36.1 (WikiHow)
- int8 + AdaLoRA configuration yielded best WER: 9.8 (Large), 11.3 (Medium), 25.1 (Small), 36.5 (Base), and 32.1 (Tiny)

## Why This Works (Mechanism)

### Mechanism 1
Parameter-efficient fine-tuning (PEFT) methods like LoRA and AdaLoRA can achieve near full fine-tuning performance while drastically reducing training time and memory usage. LoRA approximates weight updates with low-rank matrices, reducing trainable parameters from O(n*k) to O((n+k)*r) where r << min(n,k). AdaLoRA extends this by adaptively allocating ranks based on layer importance using SVD pruning.

### Mechanism 2
Quantization (int4, int8) significantly reduces model size and inference cost with minimal accuracy loss. Weights are transformed from high-precision (float32) to lower-precision (int4/int8) using affine or distribution-aware quantization with separate scaling factors per tensor/block.

### Mechanism 3
Combining ASR (Whisper) and T2T summarization in an E2E pipeline yields performance comparable to cascaded approaches while capturing richer context. The E2E training optimizes both components jointly on paired audio/summary data, allowing the model to learn to implicitly handle ASR errors and leverage non-verbal/audio features.

## Foundational Learning

- **Concept:** Low-rank matrix approximation
  - Why needed here: Core to understanding how LoRA/AdaLoRA reduce parameters while preserving expressiveness
  - Quick check question: Why does representing ∆W as A·B (with r << min(n,k)) work better than expected?

- **Concept:** Quantization schemes (symmetric vs asymmetric, linear vs distribution-aware)
  - Why needed here: Understanding how int4/int8 compression preserves model accuracy
  - Quick check question: When would you choose symmetric vs asymmetric quantization for neural weights?

- **Concept:** Transformer encoder-decoder architecture
  - Why needed here: Both Whisper and T5/MBart use this, understanding attention mechanisms is critical
  - Quick check question: How does the attention mechanism differ between encoder and decoder layers?

## Architecture Onboarding

- **Component map:** Whisper ASR (quantized+PEFT) → text transcript → T5/MBart (PEFT) → text summary
- **Critical path:** Data → Whisper (quantized+PEFT) → transcript → T5/MBart (PEFT) → summary
- **Design tradeoffs:** Model size vs accuracy (quantization levels), training time vs performance (LoRA vs full fine-tuning), ASR quality vs end-to-end optimization
- **Failure signatures:** High WER but low ROUGE (ASR errors not compensated), low WER but poor ROUGE (summarization model not capturing content), training instability (learning rate too high for quantized models)
- **First 3 experiments:**
  1. Fine-tune Whisper Tiny with int8 + AdaLoRA on How2 dataset, measure WER after 3 epochs
  2. Fine-tune T5 base with LoRA (r=16) on Gazeta, compare ROUGE-1 vs full fine-tuning
  3. Run E2E pipeline on sample How2 videos, measure end-to-end latency and summary quality

## Open Questions the Paper Calls Out

### Open Question 1
How do the performance characteristics of the E2E audio summarization model compare to traditional cascade approaches (ASR + T2T) when evaluated on real-world datasets with varying audio quality and domain-specific content? The paper discusses limitations of cascade systems but lacks direct comparative results between the two approaches in end-to-end settings.

### Open Question 2
What is the optimal balance between model compression (through quantization and LoRA) and summarization quality, and how does this balance vary across different hardware constraints and use cases? While the paper demonstrates compression effectiveness, it doesn't explore the full spectrum of trade-offs between compression and quality across varying scenarios.

### Open Question 3
How can the E2E audio summarization model be adapted to handle multilingual and cross-lingual scenarios, and what are the challenges and opportunities in extending the model's capabilities beyond English and Russian? The paper focuses on specific languages without addressing challenges and opportunities in extending capabilities to handle diverse linguistic contexts.

## Limitations

- Evaluation relies primarily on ROUGE metrics, which may not fully assess semantic quality or factual consistency
- E2E system evaluation lacks direct comparison with cascaded baselines on identical test sets
- Results focus on instructional content datasets that may not generalize to other audio domains

## Confidence

- **High Confidence:** AdaLoRA effectiveness over LoRA for parameter-efficient fine-tuning, and int8 quantization outperforming int4
- **Medium Confidence:** Specific size reduction percentages (65% with int4, 30% with int8) may depend on implementation details
- **Low Confidence:** End-to-end system performance benefits due to limited quantitative evidence compared to individual components or cascaded approaches

## Next Checks

1. Implement a complete end-to-end pipeline using fine-tuned Whisper and T5/MBart models, evaluate on held-out How2 test data to measure WER → ROUGE correlations and identify failure modes.

2. Test fine-tuned models on alternative audio datasets (podcasts, meetings, lectures) to assess whether optimal adapter ranks (16-32) and quantization schemes transfer across different domains.

3. Conduct human assessments of summary quality focusing on coherence, relevance, and factual accuracy, comparing PEFT approaches against both full fine-tuning and cascaded systems.