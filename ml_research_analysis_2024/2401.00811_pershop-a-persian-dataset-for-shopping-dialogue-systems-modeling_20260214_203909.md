---
ver: rpa2
title: PerSHOP -- A Persian dataset for shopping dialogue systems modeling
arxiv_id: '2401.00811'
source_url: https://arxiv.org/abs/2401.00811
tags:
- dataset
- product
- dialogue
- system
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors developed PerSHOP, the first open Persian dataset for
  shopping dialogue systems, containing 21,925 utterances across 1,061 dialogues in
  15 domains. They used crowd-sourcing to collect human-to-human conversations between
  customers and a virtual seller, with automatic rule-based annotation supplemented
  by manual review.
---

# PerSHOP -- A Persian dataset for shopping dialogue systems modeling

## Quick Facts
- **arXiv ID**: 2401.00811
- **Source URL**: https://arxiv.org/abs/2401.00811
- **Reference count**: 0
- **Primary result**: First open Persian shopping dialogue dataset with 21,925 utterances across 1,061 dialogues in 15 domains

## Executive Summary
PerSHOP is the first open Persian dataset for shopping dialogue systems, containing 21,925 utterances across 1,061 dialogues spanning 15 domains. The dataset was collected through crowd-sourcing of human-to-human conversations between customers and a virtual seller, with automatic rule-based annotation supplemented by manual review. It includes intent labels (e.g., Purchase, Inform, User-Confirm) and entity annotations for product attributes such as brand, weight, and color. The authors established baseline NLU models using DIETClassifier and Conditional Random Field with ParsBERT and laBSE language models, achieving F1 scores around 91% for intent classification and 93% for entity extraction. The dataset and models are publicly available to support future research in Persian task-oriented dialogue systems.

## Method Summary
The dataset was created through crowd-sourcing human-to-human conversations between customers and a virtual seller in Persian. Automatic rule-based annotation systems were employed to label intents and entities, followed by manual review to ensure quality. The dataset covers 15 domains and includes annotations for both user intents (such as Purchase, Inform, and User-Confirm) and product-related entities (including brand, weight, color, and other attributes). Baseline models were developed using DIETClassifier and Conditional Random Field for intent classification and entity extraction, leveraging ParsBERT and laBSE language models. The authors report benchmark performance metrics and make both the dataset and trained models publicly available for research purposes.

## Key Results
- PerSHOP contains 21,925 utterances across 1,061 dialogues in 15 domains
- Baseline models achieve F1 scores of approximately 91% for intent classification
- Entity extraction models achieve F1 scores of approximately 93%
- Dataset includes 7 intent types and multiple product attribute entities

## Why This Works (Mechanism)
The dataset's effectiveness stems from its realistic human-to-human conversation collection method, which captures natural dialogue patterns and variations in shopping interactions. The rule-based annotation system, while automated, is supplemented by manual review to ensure accuracy and consistency in labeling intents and entities. The combination of ParsBERT and laBSE language models provides strong multilingual and monolingual capabilities for understanding Persian text. The dataset's coverage of 15 domains ensures sufficient diversity for training robust models that can generalize across different shopping scenarios. The public availability of both the dataset and baseline models creates opportunities for community-driven improvements and benchmarking.

## Foundational Learning
- **Persian language processing**: Understanding linguistic characteristics and tokenization specific to Persian is essential for effective model training and evaluation.
- **Task-oriented dialogue systems**: Knowledge of how intent classification and entity extraction work in conversational AI contexts is necessary for interpreting benchmark results.
- **Rule-based annotation**: Understanding the trade-offs between automated and manual annotation methods helps evaluate the dataset's quality and potential biases.
- **Language model selection**: Familiarity with multilingual models like laBSE and monolingual models like ParsBERT is important for understanding model architecture choices.
- **Dialogue system evaluation**: Knowledge of appropriate metrics and evaluation methodologies for NLU components in conversational systems is needed to assess the reported results.
- **Dataset construction**: Understanding best practices in dataset creation, including domain coverage and annotation consistency, is crucial for evaluating the dataset's utility.

## Architecture Onboarding

**Component Map**: Crowd-sourcing -> Conversation Collection -> Rule-based Annotation -> Manual Review -> Dataset -> NLU Models (DIETClassifier/CRF) -> ParsBERT/laBSE -> Evaluation

**Critical Path**: Conversation collection → annotation → model training → evaluation

**Design Tradeoffs**: The authors chose human-to-human conversations over simulated interactions to capture natural dialogue patterns, but this may introduce variability that differs from human-bot interactions. They used automatic rule-based annotation with manual review rather than full manual annotation to balance efficiency with quality.

**Failure Signatures**: Potential failure modes include inconsistent annotation due to rule-based automation, domain coverage gaps in the 15 domains, and model performance degradation when encountering domain shifts or out-of-distribution examples.

**First Experiments**:
1. Reproduce baseline intent classification results using the provided dataset and model configurations
2. Test entity extraction performance on a held-out validation set
3. Evaluate model performance across different domain subsets to identify coverage gaps

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset relies on crowd-sourced human-to-human conversations that may not fully represent customer-seller interactions in real commercial settings
- Automatic rule-based annotation system may contain systematic biases despite manual review supplementation
- Evaluation focuses only on intent classification and entity extraction, leaving uncertainty about utility for more complex dialogue system components

## Confidence
- **High confidence**: Dataset existence, basic statistics (1,061 dialogues, 21,925 utterances, 15 domains), and public availability
- **Medium confidence**: Reported benchmark performance metrics (F1 scores of ~91% for intent classification and ~93% for entity extraction)
- **Medium confidence**: Claim of being the "first open Persian dataset" for shopping dialogue systems

## Next Checks
1. Reproduce baseline results using the provided dataset and model configurations to verify benchmark accuracy
2. Evaluate model performance when training on a subset of domains and testing on unseen domains to assess generalization
3. Conduct systematic human evaluation of a random sample of utterances to verify annotation quality, particularly for edge cases or ambiguous conversations