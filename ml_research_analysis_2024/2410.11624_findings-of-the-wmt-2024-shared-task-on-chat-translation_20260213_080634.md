---
ver: rpa2
title: Findings of the WMT 2024 Shared Task on Chat Translation
arxiv_id: '2410.11624'
source_url: https://arxiv.org/abs/2410.11624
tags:
- translation
- evaluation
- language
- systems
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the third edition of the Chat Translation\
  \ Shared Task, focusing on translating bilingual customer support conversations.\
  \ The task aimed to evaluate the impact of conversation context on translation quality\
  \ and evaluation, expanding to include English\u2194Korean and English\u2194Dutch\
  \ language pairs alongside previous editions."
---

# Findings of the WMT 2024 Shared Task on Chat Translation

## Quick Facts
- arXiv ID: 2410.11624
- Source URL: https://arxiv.org/abs/2410.11624
- Reference count: 8
- Primary result: Third edition of Chat Translation Shared Task with 22 primary submissions across 5 language pairs, showing context incorporation improves quality but optimal methods remain unclear

## Executive Summary
This paper presents the third edition of the WMT Chat Translation Shared Task, focusing on translating bilingual customer support conversations. The task expanded to include English↔Korean and English↔Dutch language pairs alongside previous editions. The shared task received 22 primary submissions and 32 contrastive submissions from eight teams, with each language pair having participation from at least three teams. Systems were evaluated using automatic metrics (COMET, BLEU, and context-aware COMET QE), human judgments via a direct assessment framework, and LLM-based fine-grained error analysis following the MQM framework.

The official rankings were determined based on human evaluation scores, considering performance in both translation directions - agent and customer. Key findings indicate that while the systems excelled at translating individual turns, there is room for improvement in overall conversation-level translation quality. Incorporating contextual information from previous turns almost always improved translation quality, but the optimal method for introducing context still requires further investigation. The UNBABEL-IT submission achieved the best results across most language pairs and evaluation criteria.

## Method Summary
The shared task provided participants with the MAIA 2.0 corpus containing bilingual customer support conversations across five language pairs. Participants fine-tuned large language models on the provided chat datasets, with various approaches for incorporating conversation context. The evaluation framework combined automatic metrics (COMET, BLEU, CHR F, context-aware COMET QE), human evaluation via Direct Assessment, and LLM-based error analysis using the MQM framework. The best systems typically employed domain-specific fine-tuning on in-domain chat data, quality-aware decoding with MBR reranking, and various context incorporation strategies.

## Key Results
- UNBABEL-IT achieved the highest scores across most language pairs and evaluation criteria
- Incorporating conversational context improved translation quality for later turns
- Quality-aware decoding (MBR reranking) with appropriate utility functions enhanced results
- Systems performed better on individual turns than maintaining conversation-level coherence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating conversational context improves translation quality, especially for later turns
- Mechanism: Context provides coherence signals that help resolve ambiguity in short, fragmented chat turns where meaning is implied rather than explicit
- Core assumption: The model can effectively leverage context without being overwhelmed by excessive information
- Evidence anchors:
  - [abstract] "incorporating contextual information from previous turns almost always improved translation quality"
  - [section] "Different strategies were employed to incorporate conversation context into the translation process. UNBABEL-IT, SHEFFIELD GATE, and MULTITAN-GML utilized the previous turns of the conversation as context to maintain continuity and coherence in translations"
  - [corpus] Weak - corpus shows related papers mention context but not specific evidence for this mechanism
- Break condition: When context introduces noise or the model cannot distinguish relevant from irrelevant information

### Mechanism 2
- Claim: Quality-aware decoding (MBR reranking) significantly improves translation quality when paired with appropriate utility functions
- Mechanism: By generating multiple candidate translations and selecting the one that optimizes a quality metric, the system can overcome local optima in beam search
- Core assumption: The utility function (COMET or context-aware COMET QE) correlates well with human judgment
- Evidence anchors:
  - [abstract] "UNBABEL-IT's submission also achieves the highest scores across all settings according to CONTEXT COMET QE"
  - [section] "HW-TSC optimized for COMET, UNBABEL-IT used a context-aware COMET metric as a utility for selecting the best candidate"
  - [corpus] Weak - corpus shows related papers mention MBR but not specific evidence for this mechanism
- Break condition: When the utility function doesn't correlate with human judgment or when computational costs outweigh benefits

### Mechanism 3
- Claim: Finetuning LLMs on in-domain chat data and synthetic data improves translation quality more than general-purpose models
- Mechanism: Domain-specific training adapts the model's language patterns and translation strategies to the informal, context-dependent nature of customer support chats
- Core assumption: The training data captures the relevant linguistic features of the target domain
- Evidence anchors:
  - [abstract] "The best systems finetune strong pretrained LLMs using multilingual in-domain data"
  - [section] "HW-TSC went a step further by generating a synthetic parallel corpus. They did this by forward translating source-side monolingual data into target-side text and backtranslating target-side monolingual into source-side texts"
  - [corpus] Weak - corpus shows related papers mention finetuning but not specific evidence for this mechanism
- Break condition: When synthetic data introduces artifacts or when in-domain data is insufficient to cover the domain's complexity

## Foundational Learning

- Concept: Discourse phenomena in conversational translation (lexical cohesion, formality, pronoun resolution, verb forms)
  - Why needed here: The paper specifically evaluates systems on their ability to handle these phenomena using MUDA tagger
  - Quick check question: What are the four discourse phenomena evaluated in this task and why are they particularly challenging in chat translation?

- Concept: Quality-aware decoding and minimum Bayes risk (MBR) reranking
  - Why needed here: HW-TSC and UNBABEL-IT both used MBR reranking with different utility functions, and this approach contributed to their success
  - Quick check question: How does MBR reranking differ from standard beam search decoding and what are the computational trade-offs?

- Concept: Context representation strategies (raw context, summary, graph)
  - Why needed here: Different teams used different context representation methods, and the paper suggests the optimal method still requires investigation
  - Quick check question: What are the advantages and disadvantages of using raw conversation context versus summarized context versus graph-based context in translation?

## Architecture Onboarding

- Component map:
  - Base LLM model (Llama-3, Tower, Gemma, etc.) -> Context incorporation module (raw context, summary generation, graph construction) -> Training pipeline (finetuning on in-domain data, synthetic data generation) -> Inference pipeline (quality-aware decoding with MBR reranking) -> Evaluation pipeline (automatic metrics, human evaluation, LLM-based error analysis)

- Critical path: Training → Inference → Evaluation
  - Training: Load base LLM → Incorporate context strategy → Finetune on in-domain data → Generate synthetic data (optional) → Further finetuning
  - Inference: Generate candidates → Apply quality-aware decoding → Select best hypothesis
  - Evaluation: Run automatic metrics → Conduct human evaluation → Perform LLM-based error analysis

- Design tradeoffs:
  - Context incorporation: Raw context provides completeness but may overwhelm the model; summaries are concise but may lose important information; graphs capture relationships but add complexity
  - Decoding strategy: Quality-aware decoding improves quality but increases computational cost; standard beam search is faster but may miss optimal translations
  - Training data: More in-domain data improves performance but requires annotation resources; synthetic data is cheaper but may introduce artifacts

- Failure signatures:
  - Context incorporation failure: Translations lose coherence across turns, repeated information, or incorrect pronoun resolution
  - Decoding failure: Consistently suboptimal translations that could be improved by considering alternatives
  - Training failure: Poor performance on in-domain test data despite good performance on general translation tasks

- First 3 experiments:
  1. Compare baseline NLLB-3.3B with context-aware version using previous two turns as context on EN-DE development set
  2. Implement MBR reranking with COMET as utility function on top of the best context-aware system and measure improvement
  3. Generate synthetic in-domain data using back-translation and measure impact on translation quality for EN-NL language pair

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective methods for incorporating conversational context into chat translation systems?
- Basis in paper: [explicit] The paper states that "Incorporating contextual information from previous turns almost always improved translation quality. However, the optimal method for introducing context (whether through summary, graph, or raw context) still requires further investigation."
- Why unresolved: Different approaches were explored (summaries, graphs, raw context) but the paper concludes that the optimal method is still unclear.
- What evidence would resolve it: Systematic comparative studies testing various context incorporation methods (summaries, graphs, raw context) on multiple language pairs with standardized evaluation metrics.

### Open Question 2
- Question: How does turn-level translation quality impact overall conversation-level translation quality in chat translation?
- Basis in paper: [explicit] The paper notes that "conversation-level quality is lower than turn-level scores" and observes a Spearman correlation of 0.722 between average turn-level scores and conversation-level DA scores.
- Why unresolved: The relationship between turn-level and conversation-level quality needs deeper investigation, as current evaluation methods may not fully capture this relationship.
- What evidence would resolve it: Detailed analysis correlating individual turn errors with overall conversation quality, and development of metrics that better capture conversation-level properties.

### Open Question 3
- Question: What evaluation frameworks best capture dialogue-specific criteria in chat translation systems?
- Basis in paper: [explicit] The paper suggests that "future editions of the shared task could benefit from a) designing evaluation frameworks, both automatic and human, that specifically target dialogue-specific criteria to better understand system limitations."
- Why unresolved: Current evaluation methods may not fully capture all aspects of chat translation quality, particularly dialogue-specific properties like coherence and consistency across turns.
- What evidence would resolve it: Development and validation of evaluation frameworks specifically designed to assess dialogue-specific properties, including both automatic and human evaluation methods.

## Limitations

- Evaluation relies heavily on automatic metrics that may not fully capture conversational nuances
- Optimal context incorporation strategy remains unclear with no definitive best approach
- Human evaluation was limited to 100 test instances per language pair
- Synthetic data generation may introduce artifacts not reflective of real conversational patterns

## Confidence

**High Confidence Claims:**
- Incorporating context improves translation quality for later conversation turns
- Quality-aware decoding (MBR reranking) with appropriate utility functions enhances results
- Finetuning on in-domain chat data outperforms general-purpose models
- Systems perform better on individual turns than maintaining conversation-level coherence

**Medium Confidence Claims:**
- Context-aware COMET QE correlates well with human judgments for chat translation
- Synthetic data generation through back-translation effectively augments training data
- The MUDA framework adequately captures discourse phenomena in chat translation

**Low Confidence Claims:**
- Optimal context representation method (raw vs. summary vs. graph) for chat translation
- Generalizability of the best systems to other conversational domains
- Impact of model scale (parameter count) on chat translation quality beyond certain thresholds

## Next Checks

1. **Ablation Study on Context Length**: Systematically evaluate translation quality with varying amounts of context (previous 1, 2, 3, and 4 turns) on the EN-DE language pair to identify the optimal context window and determine when context becomes detrimental.

2. **Cross-Domain Generalization Test**: Evaluate the best-performing systems from this shared task on a different conversational domain (e.g., medical consultations or social media conversations) to assess their generalizability beyond customer support chats.

3. **Human vs. Automatic Metric Correlation Analysis**: Conduct a detailed correlation analysis between human judgments and automatic metrics (COMET, BLEU, CHR F) specifically for discourse phenomena like pronoun resolution and lexical cohesion to identify which metrics best predict human preferences for chat translation.