---
ver: rpa2
title: Supervised Fine-Tuning as Inverse Reinforcement Learning
arxiv_id: '2403.12017'
source_url: https://arxiv.org/abs/2403.12017
tags:
- learning
- arxiv
- dataset
- alignment
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper connects supervised fine-tuning (SFT) for language model
  alignment to inverse reinforcement learning (IRL) by framing token generation as
  a sequential decision-making problem in a Markov Decision Process. The author shows
  that SFT is equivalent to minimizing the forward KL divergence between trajectory
  distributions, explaining its mass-covering behavior.
---

# Supervised Fine-Tuning as Inverse Reinforcement Learning

## Quick Facts
- arXiv ID: 2403.12017
- Source URL: https://arxiv.org/abs/2403.12017
- Reference count: 40
- Primary result: Supervised fine-tuning for LLM alignment is equivalent to minimizing forward KL divergence, with alternative divergence-based approaches offering different alignment behaviors.

## Executive Summary
This paper establishes a formal connection between supervised fine-tuning (SFT) for large language model alignment and inverse reinforcement learning (IRL) by modeling token generation as a sequential decision-making problem within a Markov Decision Process framework. The author demonstrates that SFT is mathematically equivalent to minimizing the forward KL divergence between trajectory distributions, explaining its characteristic mass-covering behavior. The work introduces alternative alignment approaches using reverse KL divergence and Jensen-Shannon divergence, which produce mode-seeking and balanced behaviors respectively. Additionally, the paper critiques the Bradley-Terry model assumptions commonly used in reward modeling for preference-based alignment, highlighting limitations in handling domain heterogeneity and offline learning constraints.

## Method Summary
The paper reframes LLM alignment as an imitation learning problem by modeling token generation as sequential decision-making in a Markov Decision Process. Under this framework, the states represent context windows, actions represent next token predictions, and the policy is the language model itself. The author proves that standard supervised fine-tuning minimizes the forward KL divergence between expert and model trajectory distributions. Three alternative alignment approaches are proposed: reverse KL divergence for mode-seeking behavior, Jensen-Shannon divergence for balanced behavior via adversarial training, and various f-divergence formulations that generalize these approaches. The methods are evaluated theoretically through their mathematical properties rather than empirical experiments.

## Key Results
- SFT is mathematically equivalent to minimizing forward KL divergence between expert and model trajectory distributions
- Forward KL leads to mass-covering behavior while reverse KL produces mode-seeking behavior
- Jensen-Shannon divergence offers a balanced middle ground through adversarial training approaches
- Bradley-Terry model assumptions in reward modeling have significant limitations for preference-based alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised fine-tuning (SFT) for LLM alignment is equivalent to minimizing forward KL divergence between trajectory distributions
- Mechanism: SFT trains the model to maximize likelihood of expert demonstrations, which mathematically corresponds to minimizing KL divergence from expert trajectory distribution to model trajectory distribution
- Core assumption: Token generation can be modeled as a Markov Decision Process where states are context windows and actions are next tokens
- Evidence anchors:
  - [abstract] states "SFT is equivalent to minimizing the forward KL divergence between trajectory distributions"
  - [section 3.4.2] derives this equivalence showing "Minimizing the forward KL divergence of trajectories between demonstration and current policy leads to the same learning objective as SFT"
  - [corpus] includes "Inverse-RLignment: Large Language Model Alignment from Demonstrations through Inverse Reinforcement Learning" which supports this framing
- Break condition: If token dependencies extend beyond context window or if expert demonstrations contain conflicting preferences that cannot be reconciled through KL minimization

### Mechanism 2
- Claim: Different divergence measures lead to different alignment behaviors - forward KL produces mass-covering behavior while reverse KL produces mode-seeking behavior
- Mechanism: Forward KL divergence encourages the model to cover all modes in the expert distribution (avoiding any region with non-zero expert probability), while reverse KL encourages the model to focus on a single mode that explains most of the expert distribution
- Core assumption: The choice of divergence measure fundamentally changes the optimization landscape and resulting behavior
- Evidence anchors:
  - [abstract] mentions "mass-covering and mode-seeking behaviors of these different approaches"
  - [section 3.4.2] states "it is known that using the forward KL-Divergence will lead to mass-covering and using reverse KL-Divergence leads to mode-seeking behaviors"
  - [corpus] includes "Anomalous Decision Discovery using Inverse Reinforcement Learning" suggesting practical applications of different behaviors
- Break condition: If the expert distribution has many modes with similar probability mass, making mode-seeking behavior suboptimal

### Mechanism 3
- Claim: Adversarial imitation learning approaches using Jensen-Shannon divergence offer a middle ground between mass-covering and mode-seeking behaviors
- Mechanism: JS divergence is symmetric and bounded, combining properties of both forward and reverse KL divergences, leading to more balanced alignment behavior
- Core assumption: Symmetric divergences provide more stable optimization than asymmetric ones in sequential decision-making contexts
- Evidence anchors:
  - [abstract] lists "Jensen-Shannon divergence" as an alternative objective
  - [section 3.4.4] derives the practical objective for JS divergence showing "min π max ψ E(y|x)∼ dexp [log Dψ(y|x)] + E(y|x)∼ ρπ [log(1 − Dψ(y|x))]"
  - [corpus] includes "Imitating Language via Scalable Inverse Reinforcement Learning" supporting the adversarial framework
- Break condition: If computational resources are limited, as JS divergence requires training an additional discriminator network

## Foundational Learning

- Concept: Markov Decision Processes
  - Why needed here: The paper frames LLM generation as sequential decision-making within an MDP framework
  - Quick check question: How does the deterministic transition function in the proposed MDP differ from typical RL MDPs?

- Concept: KL Divergence Properties
  - Why needed here: Different KL divergence formulations (forward vs reverse) lead to fundamentally different alignment behaviors
  - Quick check question: Why does forward KL divergence encourage mass-covering behavior while reverse KL encourages mode-seeking?

- Concept: Adversarial Training in RL
  - Why needed here: The paper proposes adversarial imitation learning approaches that require training discriminative models alongside the policy
  - Quick check question: How does the optimal discriminator relate to the occupancy measures of expert and learned policies?

## Architecture Onboarding

- Component map: Policy network (LLM) -> Trajectory formation -> Divergence computation -> Policy update
- Critical path: Token generation → trajectory formation → divergence computation → policy update
- Design tradeoffs: Forward KL (SFT) is simpler but may produce bland outputs; reverse KL is more focused but may miss important variations; JS divergence balances both but requires more computation
- Failure signatures: Mode collapse (reverse KL), lack of specificity (forward KL), discriminator instability (adversarial methods)
- First 3 experiments:
  1. Implement SFT baseline and verify it minimizes forward KL divergence as claimed
  2. Implement reverse KL approach and compare output diversity to SFT
  3. Implement JS divergence approach and measure trade-off between faithfulness and diversity compared to other methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different f-divergence choices (beyond KL, reverse KL, and Jensen-Shannon) impact the performance and behavior of LLM alignment methods?
- Basis in paper: [explicit] The paper discusses AIRL, GAIL, FAIRL, and α-IRL as examples within the f-divergence framework, highlighting their different properties.
- Why unresolved: The paper only briefly mentions these alternatives without providing empirical comparisons or a comprehensive analysis of their trade-offs in the context of LLM alignment.
- What evidence would resolve it: Empirical studies comparing the performance of LLM alignment methods using various f-divergences on diverse tasks and datasets, analyzing their impact on factors like mode-seeking vs. mass-covering behavior, sample efficiency, and alignment quality.

### Open Question 2
- Question: How can we effectively address the compounding error problem in behavior cloning for LLM alignment, especially for long sequences?
- Basis in paper: [explicit] The paper mentions that behavior cloning is unreliable due to compounding errors, which is a known challenge in imitation learning.
- Why unresolved: The paper does not propose specific solutions to mitigate this issue in the context of LLM alignment, leaving it as a fundamental limitation of SFT-type methods.
- What evidence would resolve it: Research demonstrating novel techniques or architectures that effectively reduce compounding errors in LLM alignment, potentially through advanced training strategies, architectural modifications, or hybrid approaches combining BC with other methods.

### Open Question 3
- Question: What are the practical implications and limitations of using the Bradley-Terry model assumptions in reward modeling for LLM alignment, and how can alternative models improve alignment quality?
- Basis in paper: [explicit] The paper critiques the Bradley-Terry model assumptions in reward modeling, highlighting challenges like domain heterogeneity and limited error correction in offline learning.
- Why unresolved: The paper suggests potential alternatives but does not explore them in detail or provide empirical evidence of their effectiveness compared to the Bradley-Terry model.
- What evidence would resolve it: Comparative studies evaluating the performance of LLM alignment methods using different reward modeling approaches, including learned variance terms, direct preference objectives, and prospect theory objectives, on diverse tasks and datasets.

## Limitations

- The Markov property assumption for token generation may not hold for long-range dependencies beyond context windows
- Expert demonstrations are assumed to be representative and free from selection bias, which is rarely true in practice
- Alternative divergence-based approaches (reverse KL, JS divergence) face practical challenges in optimization stability and computational efficiency that aren't fully explored

## Confidence

**High Confidence**: The mathematical equivalence between SFT and forward KL minimization is well-established and the derivation appears sound. The characterization of mass-covering versus mode-seeking behaviors as properties of forward versus reverse KL divergences is a standard result in information theory with strong theoretical foundations.

**Medium Confidence**: The practical implications of choosing different divergence measures for alignment quality and the specific trade-offs between them are reasonably well-supported but would benefit from empirical validation across diverse datasets and model architectures.

**Low Confidence**: The claims about the limitations of Bradley-Terry models in reward modeling and the specific advantages of the proposed alternatives lack sufficient empirical backing and may depend heavily on the particular characteristics of preference datasets used in practice.

## Next Checks

1. **Empirical verification of divergence behaviors**: Implement and test all three divergence-based approaches (forward KL/SFT, reverse KL, and JS divergence) on the same alignment task with identical datasets to measure concrete differences in output diversity, faithfulness to demonstrations, and computational efficiency.

2. **Contextual dependency validation**: Design experiments to test the Markov assumption by varying context window sizes and measuring how this affects the alignment quality and the validity of the MDP framing, particularly for tasks requiring long-range coherence.

3. **Preference model comparison**: Systematically compare the proposed divergence-based alignment methods against state-of-the-art preference-based alignment approaches (like RLHF) across multiple benchmarks to quantify trade-offs in alignment quality, sample efficiency, and robustness to preference noise.