---
ver: rpa2
title: 'One Model is All You Need: ByT5-Sanskrit, a Unified Model for Sanskrit NLP
  Tasks'
arxiv_id: '2409.13920'
source_url: https://arxiv.org/abs/2409.13920
tags:
- sanskrit
- tasks
- data
- performance
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ByT5-Sanskrit, a byte-level pretrained language
  model designed to address NLP challenges in morphologically rich languages like
  Sanskrit. Sanskrit presents difficulties due to its rich morphology, free word order,
  heavy compounding, and phonetic merging of words (Sandhi).
---

# One Model is All You Need: ByT5-Sanskrit, a Unified Model for Sanskrit NLP Tasks

## Quick Facts
- arXiv ID: 2409.13920
- Source URL: https://arxiv.org/abs/2409.13920
- Reference count: 6
- One-line primary result: ByT5-Sanskrit achieves state-of-the-art results on Sanskrit word segmentation, Vedic dependency parsing, and OCR post-correction while generalizing to other morphologically rich languages

## Executive Summary
This paper introduces ByT5-Sanskrit, a byte-level pretrained language model designed to address NLP challenges in morphologically rich languages like Sanskrit. Sanskrit presents difficulties due to its rich morphology, free word order, heavy compounding, and phonetic merging of words (Sandhi). The proposed method combines pretraining a ByT5 model on a large Sanskrit corpus with fine-tuning on downstream tasks reformulated as sequence generation. Tasks are distinguished using prefix tokens and morphosyntactic tags are compressed for efficient prediction. ByT5-Sanskrit achieves state-of-the-art results on Sanskrit word segmentation, Vedic dependency parsing, and OCR post-correction, while also generalizing well to other morphologically rich languages.

## Method Summary
The approach combines byte-level pretraining on a large Sanskrit corpus with multitask fine-tuning using sequence generation reformulation. The model uses ByT5 base architecture (582M parameters) trained on OCR-based Sanskrit data from IndicLLMSuite, GRETIL, and DSBC. Downstream tasks (word segmentation, lemmatization, morphosyntactic tagging, dependency parsing, OCR post-correction) are reformulated as sequence generation problems with task-specific prefix tokens. The model is fine-tuned jointly on all tasks simultaneously, with training on pseudo-paragraph-level by concatenating adjacent sentences. Compressed morphosyntactic tags use IAST letter combinations for efficiency.

## Key Results
- ByT5-Sanskrit achieves state-of-the-art results on Sanskrit word segmentation, Vedic dependency parsing, and OCR post-correction
- The unified model performs all tasks simultaneously with high accuracy, benefiting from multitask training
- Generalizes well to other morphologically rich languages, achieving best results on lemmatization and dependency parsing for Bulgarian, Romanian, and Turkish

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Byte-level pretraining captures subword and morphological information without relying on language-specific tokenizers
- Mechanism: By operating on raw bytes, the model learns character and subword patterns directly, enabling it to handle complex morphology and compounding in Sanskrit without needing pre-tokenized units
- Core assumption: Morphological information is distributed across byte sequences in a way that is learnable by a sufficiently large model
- Evidence anchors:
  - [abstract]: "We thus demonstrate that byte-level pretrained language models can achieve excellent performance for morphologically rich languages, outperforming tokenizer-based models"
  - [section]: "For morphologically rich languages, language models that make use of character-level information show superior performance to those operating on word-level alone"
  - [corpus]: Weak - the corpus evidence focuses on neighbor papers but doesn't directly support this specific mechanism
- Break condition: If morphology is too complex for byte-level patterns to capture, or if language-specific tokenization rules are essential for downstream tasks

### Mechanism 2
- Claim: Reformulating NLP tasks as sequence generation with prefix tokens allows unified training across multiple tasks
- Mechanism: By converting segmentation, lemmatization, and tagging into sequence generation problems and using task-specific prefixes (S, L, M), the model learns to distinguish and perform all tasks simultaneously with shared parameters
- Core assumption: Task prefixes provide sufficient signal for the model to learn task-specific behavior while sharing representations
- Evidence anchors:
  - [abstract]: "We reformulate the central Sanskrit NLP tasks of word segmentation, lemmatization, and morphosyntactic tagging as sequence generation tasks"
  - [section]: "In order to distinguish between the different tasks, we use prefix letters at the beginning of the input sequence to indicate the task"
  - [corpus]: Weak - no direct corpus evidence for this specific mechanism
- Break condition: If task prefixes become ambiguous or if tasks are too dissimilar to benefit from shared representations

### Mechanism 3
- Claim: Joint multitask training improves performance across all tasks through transfer learning
- Mechanism: Training on multiple related tasks simultaneously allows the model to learn shared representations that benefit each individual task, as evidenced by ablation results showing performance drops when training tasks individually
- Core assumption: Tasks like segmentation, lemmatization, and tagging are related enough that learning one helps with the others
- Evidence anchors:
  - [abstract]: "We also show that jointly training on the tasks of SWS, lemmatization, and morphosyntactic tagging on top of the pretrained language model leads to the best performance"
  - [section]: "The results in Table 8 show that individual task training diminishes performance for both segmentation and lemmatization tasks. This confirms that transfer learning across different tasks contributes to enhanced overall performance."
  - [corpus]: Weak - corpus doesn't directly support this mechanism
- Break condition: If tasks are too dissimilar or if one task dominates the learning process, preventing effective transfer

## Foundational Learning

- Concept: Character-level vs word-level language modeling
  - Why needed here: Sanskrit's rich morphology and compounding make word-level tokenization challenging, while character-level modeling can capture subword patterns
  - Quick check question: Why might character-level models outperform word-level models for morphologically rich languages?

- Concept: Sequence-to-sequence modeling for structured prediction
  - Why needed here: Reformulating tasks like segmentation and tagging as sequence generation enables unified multitask training
  - Quick check question: How does converting structured prediction to sequence generation enable multitask learning?

- Concept: Pretraining-finetuning paradigm
  - Why needed here: Large-scale pretraining on Sanskrit data provides strong priors that improve performance on downstream tasks with limited labeled data
  - Quick check question: What advantages does pretraining on a large corpus provide for downstream fine-tuning tasks?

## Architecture Onboarding

- Component map: ByT5 base model → pretraining on Sanskrit corpus → multitask fine-tuning with prefix tokens → output sequence generation for all tasks
- Critical path: Pretraining (1 week) → fine-tuning individual tasks → joint multitask training → evaluation
- Design tradeoffs: Byte-level modeling vs tokenization complexity, multitask training vs task-specific optimization, model size vs training efficiency
- Failure signatures: Degraded performance on specific tasks, inconsistent outputs across similar inputs, failure to generalize to unseen morphological patterns
- First 3 experiments:
  1. Fine-tune ByT5-Sanskrit on word segmentation alone and evaluate on SIGHUM dataset
  2. Fine-tune ByT5-Sanskrit on lemmatization task and compare against baseline UDPipe
  3. Train multitask model on all three tasks and evaluate sentence-level perfect match improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do byte-level language models compare to subword-tokenization models for other morphologically rich languages beyond Sanskrit, Bulgarian, Romanian, and Turkish?
- Basis in paper: [explicit] The authors show byte-level models outperform subword models on three morphologically rich languages, but suggest further exploration is needed for other MRLs
- Why unresolved: The study only evaluated three additional MRLs, leaving open whether these advantages generalize across the full spectrum of morphologically rich languages
- What evidence would resolve it: Systematic benchmarking of byte-level vs subword models across a diverse set of morphologically rich languages with varying morphological complexity

### Open Question 2
- Question: What is the impact of different training data characteristics (e.g., noise levels, domain diversity) on the performance of byte-level language models for morphologically rich languages?
- Basis in paper: [inferred] The authors use a large corpus of noisy OCR data for pretraining and observe strong performance, suggesting training data characteristics may be important
- Why unresolved: While the authors demonstrate success with noisy OCR data, they don't systematically explore how different data characteristics affect model performance
- What evidence would resolve it: Controlled experiments varying training data characteristics while holding other factors constant

### Open Question 3
- Question: How can homonymy in morphologically rich languages be effectively addressed within the byte-level language model framework?
- Basis in paper: [explicit] The authors identify homonymy as a limitation, noting that 7.5% of lemmata have homonyms representing 57.5% of all words
- Why unresolved: The authors propose marking homonyms with numeric affixes but haven't implemented or tested this solution
- What evidence would resolve it: Implementation and evaluation of the proposed homonym marking system, along with comparison to alternative approaches

## Limitations

- Generalization uncertainty to languages beyond the tested set of morphologically rich languages
- Potential loss of critical linguistic information due to compressed morphosyntactic tag representation
- Uncertain coverage of Sandhi phenomena and compound words in pretraining data

## Confidence

**High Confidence**: The claim that ByT5-Sanskrit achieves state-of-the-art results on Sanskrit word segmentation, Vedic dependency parsing, and OCR post-correction is well-supported by direct comparisons with established baselines and detailed evaluation metrics.

**Medium Confidence**: The assertion that byte-level pretraining outperforms tokenizer-based models for morphologically rich languages is supported by experimental results but lacks extensive ablation studies comparing different tokenization strategies across multiple languages.

**Medium Confidence**: The claim that joint multitask training improves performance across all tasks through transfer learning is supported by ablation results, but the mechanism explaining why specific tasks benefit from each other could be more thoroughly analyzed.

## Next Checks

1. **Cross-linguistic generalization test**: Evaluate ByT5-Sanskrit on additional morphologically rich languages (e.g., Finnish, Hungarian, Arabic) across all three core tasks (segmentation, lemmatization, morphosyntactic tagging) to validate the claim that byte-level models generalize well beyond the tested languages.

2. **Pretraining data coverage analysis**: Conduct a systematic analysis of how pretraining data coverage of Sandhi phenomena and compound words correlates with downstream performance on these specific challenges, addressing the uncertainty about whether the model truly learns these patterns or simply memorizes frequent examples.

3. **Tag compression fidelity evaluation**: Compare the compressed morphosyntactic tag representation against the full tag set by measuring information loss and task performance degradation, determining whether the efficiency gains justify potential loss of linguistic detail.