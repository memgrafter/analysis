---
ver: rpa2
title: 'JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset'
arxiv_id: '2403.17319'
source_url: https://arxiv.org/abs/2403.17319
tags:
- dialogue
- task-oriented
- user
- japanese
- jmultiwoz
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JMultiWOZ, the first large-scale Japanese
  multi-domain task-oriented dialogue dataset. It contains 4,246 dialogues spanning
  six travel-related domains and provides benchmarks for dialogue state tracking (DST)
  and response generation (RG).
---

# JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset

## Quick Facts
- arXiv ID: 2403.17319
- Source URL: https://arxiv.org/abs/2403.17319
- Reference count: 0
- Primary result: First large-scale Japanese multi-domain task-oriented dialogue dataset with 4,246 dialogues spanning six travel-related domains

## Executive Summary
JMultiWOZ is the first large-scale Japanese multi-domain task-oriented dialogue dataset, containing 4,246 dialogues across six travel-related domains. The dataset was constructed using the Wizard-of-Oz methodology and provides benchmarks for dialogue state tracking (DST) and response generation (RG). The authors evaluate the dataset using state-of-the-art supervised fine-tuning (T5 models) and large language model (LLM) approaches, finding that supervised fine-tuning outperforms LLMs in Japanese task-oriented dialogue. Human evaluation reveals limitations in LLM capabilities for Japanese task-oriented dialogue, suggesting that even advanced models like GPT-4 struggle with dynamically changing dialogue contexts in Japanese.

## Method Summary
The authors constructed JMultiWOZ by translating and adapting the English MultiWOZ2.2 dataset to Japanese, maintaining similar domain structure and dialogue complexity. They fine-tuned T5-base and T5-large models for both DST and RG tasks using maximum likelihood estimation, and evaluated LLM performance (GPT-3.5 and GPT-4) in zero/few-shot settings. The evaluation used standard metrics including joint goal accuracy (JGA), Slot-F1 for DST, and BLEU score for RG. Human evaluation was conducted with crowd workers interacting with trained models to assess real-world performance.

## Key Results
- T5-large achieved JGA of 0.77 for DST on JMultiWOZ
- T5-large achieved BLEU score of 49.68 for RG on JMultiWOZ
- Supervised fine-tuning (T5 models) outperformed LLM-based methods for both DST and RG tasks
- LLM performance (GPT-3.5/4) significantly declined compared to English benchmarks, indicating limitations in Japanese task-oriented dialogue capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JMultiWOZ provides a benchmark comparable to MultiWOZ2.2 in complexity and annotation quality
- Mechanism: The dataset was constructed using the same Wizard-of-Oz method, similar number of domains and slots, and controlled dialogue collection to ensure quality
- Core assumption: Japanese cultural adaptation of domains maintains equivalent dialogue complexity
- Evidence anchors:
  - [abstract] "JMultiWOZ can provide a benchmark that is on par with MultiWOZ2.2"
  - [section] "Table 7 shows the statistics of JMultiWOZ compared with the major multi-domain task-oriented dialogue datasets in English and Chinese, MultiWOZ and CrossWOZ, respectively. Given that the number of domains, the number of slots, the average number of domains, and the average number of turns are roughly equivalent, the complexity of dialogues in JMultiWOZ can be considered to be on par with the existing datasets."
  - [corpus] "Average neighbor FMR=0.543, average citations=0.0" - indicates limited external validation but internal design alignment
- Break condition: If Japanese cultural adaptation reduces dialogue complexity or if annotation quality is lower than MultiWOZ2.2

### Mechanism 2
- Claim: T5-based supervised fine-tuning outperforms LLM-based methods on JMultiWOZ
- Mechanism: T5-base/large models trained on JMultiWOZ data learn the dialogue patterns better than zero/few-shot LLM prompting
- Core assumption: Japanese task-oriented dialogue requires domain-specific training rather than general LLM capabilities
- Evidence anchors:
  - [abstract] "the SFT method, namely T5-base/large, generally had the highest performance for both DST and RG, and there seemed to be certain limitations to the performance of LLMs"
  - [section] "Table 8 shows a comparison between the evaluation results reported in previous studies on MultiWOZ2.2 and those of JMultiWOZ. On JMultiWOZ, the SFT method, namely T5-base/large, generally had the highest performance for both DST and RG"
  - [corpus] Limited - corpus doesn't provide direct comparison evidence
- Break condition: If LLM models with better Japanese language capabilities or better prompting strategies outperform supervised fine-tuning

### Mechanism 3
- Claim: LLM performance in Japanese task-oriented dialogue is limited due to language capability differences
- Mechanism: GPT-3.5/4 show significant performance degradation in Japanese compared to English, likely due to lower Japanese language ability
- Core assumption: GPT-4's English task-oriented dialogue capabilities don't transfer well to Japanese
- Evidence anchors:
  - [abstract] "we confirmed that the capabilities of GPT-3.5/4 in particular are limited in Japanese"
  - [section] "The performance of the LLM models, i.e., GPT-3.5 and GPT-4, significantly declined compared to that of MultiWOZ2.2. This may be because even the latest LLMs, such as GPT-4, are not capable of handling dynamically changing dialogue contexts in Japanese"
  - [corpus] "Average neighbor FMR=0.543" - suggests moderate relatedness but no direct performance comparison
- Break condition: If future LLMs with improved Japanese capabilities show performance parity with English

## Foundational Learning

- Concept: Wizard-of-Oz dialogue collection methodology
  - Why needed here: Understanding how JMultiWOZ was collected helps in interpreting the dataset characteristics and limitations
  - Quick check question: What are the key differences between Wizard-of-Oz and machine-to-machine dialogue collection methods?

- Concept: Dialogue state tracking (DST) and response generation (RG) tasks
  - Why needed here: These are the two main tasks JMultiWOZ provides benchmarks for, understanding their mechanics is crucial
  - Quick check question: How does dialogue state tracking differ from response generation in task-oriented dialogue systems?

- Concept: Cross-lingual transfer learning limitations
  - Why needed here: The paper shows that even advanced LLMs struggle with Japanese task-oriented dialogue, understanding why is important
  - Quick check question: What factors might cause cross-lingual transfer learning to perform poorly for task-oriented dialogue?

## Architecture Onboarding

- Component map: Dialogue data → Backend database → Ontology definitions → Annotations → T5 models (DST/RG) or LLM models → Evaluation metrics (JGA, Slot-F1, BLEU)

- Critical path: Dialogue collection → Annotation → Model training → Evaluation → Human evaluation

- Design tradeoffs: The choice to use selection-based input for DB queries reduces annotation errors but may limit natural dialogue patterns. Using the same model for both DST and RG simplifies the pipeline but may introduce error propagation.

- Failure signatures: Low JGA scores indicate dialogue state tracking failures. Low BLEU scores suggest response generation issues. Significant performance gaps between T5 and LLM methods indicate language model limitations.

- First 3 experiments:
  1. Train T5-base on JMultiWOZ training set and evaluate on dev set to establish baseline performance
  2. Test LLM zero-shot performance on JMultiWOZ dev set to compare with supervised methods
  3. Conduct human evaluation with crowd workers interacting with trained models to assess real-world performance

## Open Questions the Paper Calls Out
- How would using JMultiWOZ for pre-training and then fine-tuning with GlobalWOZ affect the performance of task-oriented dialogue models in Japanese?
- How does the complexity and annotation accuracy of JMultiWOZ compare to that of MultiWOZ2.2, and what factors contribute to any differences?
- How do the capabilities of GPT-3.5 and GPT-4 in task-oriented dialogue differ between English and Japanese, and what factors contribute to these differences?

## Limitations
- Dataset comparison methodology relies heavily on matching MultiWOZ2.2 statistics without direct validation of dialogue complexity equivalence
- Human evaluation study lacks statistical significance testing and uses a small sample size (60 dialogues)
- LLM performance comparison is limited by the fixed GPT models used and doesn't explore alternative Japanese-capable models or more sophisticated prompting strategies

## Confidence

**High Confidence:** The dataset construction methodology and basic statistics are well-documented and reproducible. The T5 fine-tuning approach and evaluation metrics are standard and clearly specified.

**Medium Confidence:** The claim that JMultiWOZ is "on par with MultiWOZ2.2" is supported by structural comparisons but lacks direct complexity validation. The LLM performance limitations are well-observed but the attribution to "language capability differences" is speculative without controlled experiments.

**Low Confidence:** The cross-lingual transfer learning failure analysis is based on a single model comparison without exploring alternative approaches or Japanese-specific language models that might perform better.

## Next Checks

1. **Statistical Validation of Human Evaluation:** Conduct a larger-scale human evaluation with statistical significance testing (e.g., paired t-tests) comparing LLM and T5 model performance across at least 200 dialogues to verify the claimed performance differences.

2. **Alternative LLM Testing:** Evaluate Japanese-specific LLMs (e.g., Japanese GPT variants, LLaMA-based Japanese models) and test different prompting strategies to determine if the observed LLM limitations are model-specific or inherent to cross-lingual task-oriented dialogue.

3. **Complexity Validation Study:** Design a controlled study comparing dialogue complexity between JMultiWOZ and MultiWOZ2.2 using metrics like dialogue act diversity, context dependency measures, and human complexity ratings to empirically validate the claimed equivalence.