---
ver: rpa2
title: 'Think Before You Act: A Two-Stage Framework for Mitigating Gender Bias Towards
  Vision-Language Tasks'
arxiv_id: '2405.16860'
source_url: https://arxiv.org/abs/2405.16860
tags:
- gender
- bias
- image
- gama
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses gender bias in vision-language models (VLMs),
  where models tend to focus on salient or familiar attributes in images but ignore
  contextualized nuances, often relying on co-occurrence between specific objects
  and gender attributes to infer ignored features. To mitigate this, the authors propose
  GAMA, a task-agnostic generation framework with two stages: narrative generation
  and answer inference.'
---

# Think Before You Act: A Two-Stage Framework for Mitigating Gender Bias Towards Vision-Language Tasks

## Quick Facts
- arXiv ID: 2405.16860
- Source URL: https://arxiv.org/abs/2405.16860
- Authors: Yunqi Zhang; Songda Li; Chunyuan Deng; Luyi Wang; Hui Zhao
- Reference count: 40
- Key outcome: GAMA reduces gender bias in VLMs through a two-stage framework while maintaining task performance across image captioning, image search, and zero-shot gender bias benchmarks.

## Executive Summary
This paper addresses gender bias in vision-language models (VLMs) by identifying object hallucination as the root cause of gender bias. The authors propose GAMA, a task-agnostic generation framework with two stages: narrative generation and answer inference. During narrative generation, GAMA creates gender-obfuscated narratives to prevent premature concentration on localized image features. During answer inference, the model integrates the image, generated narrative, and task-specific question prompt to infer unbiased answers. Extensive experiments demonstrate GAMA's effectiveness in reducing gender bias while maintaining task performance across multiple vision-language tasks and benchmarks.

## Method Summary
GAMA is a two-stage task-agnostic generation framework that mitigates gender bias in VLMs. In the first stage, a narrative generation model creates comprehensive, gender-obfuscated narratives by combining image features (from frozen ViT) with text features (from T5 encoder) using attention and gating mechanisms, then applying contrastive learning to remove gender-related features. In the second stage, an answer inference model generates task-specific outputs by combining the original image, the obfuscated narrative, and the task-specific prompt. The framework is trained on Localized Narratives dataset and evaluated on image captioning (MSCOCO, Flickr30K) and image search tasks, with gender bias assessed using multiple metrics including LIC, Error, and BiasAmp.

## Key Results
- GAMA reduces gender bias metrics (LIC, Error, BiasAmp) by 20-30% compared to baseline VLMs while maintaining comparable task performance on BLEU-4, CIDEr, and Recall@K metrics
- Ablation studies show the gender obfuscation module and two-stage framework both contribute significantly to bias reduction
- Object hallucination metrics (CHAIRs, HRoC@K) decrease proportionally with gender bias reduction, supporting the claim that hallucination underlies gender bias
- Zero-shot evaluation on VisoGender benchmark demonstrates GAMA's generalization ability without task-specific fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GAMA's two-stage framework mitigates gender bias by separating context generation from answer generation.
- Mechanism: During narrative generation, GAMA creates gender-obfuscated narratives using contrastive learning to disentangle gender-related features from general context features. During answer inference, the model integrates the image, obfuscated narrative, and task-specific question to generate answers, encouraging the model to rethink gender attributes based on comprehensive image understanding rather than localized stereotypes.
- Core assumption: Gender bias in VLMs stems from premature concentration on localized gender attributes and reliance on object-gender co-occurrence patterns learned from training data.
- Evidence anchors:
  - [abstract] "During narrative generation, GAMA yields all-sided but gender-obfuscated narratives, which prevents premature concentration on localized image features, especially gender attributes."
  - [section 3.4] "We disentangle features through contrastive learning to obfuscate gender information in the generated narratives."
  - [corpus] Weak - neighbor papers discuss gender bias but don't directly address the two-stage narrative-generation approach.

### Mechanism 2
- Claim: Gender obfuscation through contrastive learning reduces gender-related feature prominence in generated narratives.
- Mechanism: The gender obfuscation module detects gender-related features using a gating mechanism, then applies contrastive loss to push gender-masked features closer to vision-language features while pushing gender-related features away. This process obscures gender information while preserving contextual information in narratives.
- Core assumption: Gender information can be effectively disentangled from general context information through contrastive learning, and reducing gender feature prominence in narratives will lead to more unbiased answer generation.
- Evidence anchors:
  - [section 3.4] "We employ a contrastive loss to keep the vision-language features H close to the gender-masked features ¯H and away from the gender-related features Hg."
  - [section 5.2] "The gender obfuscation module effectively obfuscates gender-related information during narrative generation, leading to a reduction in the number of narratives with gender words."
  - [corpus] Moderate - neighbor papers discuss gender fairness interventions but not specifically contrastive learning for feature disentanglement in VLMs.

### Mechanism 3
- Claim: Comprehensive image understanding before answer generation reduces object hallucination and associated gender bias.
- Mechanism: By generating all-sided narratives that cover the entire image rather than focusing on salient regions, GAMA prevents the model from prematurely concentrating on localized features, particularly gender attributes. This comprehensive understanding reduces the tendency to hallucinate objects based on gender stereotypes.
- Core assumption: Object hallucination in VLMs is closely related to gender bias, and both stem from focusing on salient or familiar attributes while ignoring contextualized nuances.
- Evidence anchors:
  - [abstract] "We identify object hallucination as the essence of gender bias in VLMs. Existing VLMs tend to focus on salient or familiar attributes in images but ignore contextualized nuances."
  - [section 5.2] "Object hallucination and gender bias exhibit a close correlation. As shown in Table 4, mitigating gender bias leads to a decrease in object hallucination."
  - [corpus] Weak - neighbor papers discuss bias in VLMs but don't explicitly connect object hallucination to gender bias mitigation strategies.

## Foundational Learning

- Concept: Contrastive learning for feature disentanglement
  - Why needed here: To separate gender-related features from general context features in generated narratives, allowing the model to obscure gender information while preserving useful context.
  - Quick check question: How does contrastive loss help the model learn to distinguish between gender-related and gender-neutral features in the narrative generation stage?

- Concept: Two-stage reasoning frameworks
  - Why needed here: To separate comprehensive context generation from answer generation, allowing the model to first understand the full image context before making gender-related decisions.
  - Quick check question: Why might separating narrative generation from answer generation help reduce premature focus on localized gender attributes compared to a single-stage approach?

- Concept: Vision-language feature fusion mechanisms
  - Why needed here: To effectively combine visual and textual information in both narrative generation and answer inference stages, enabling the model to reason about images in the context of generated narratives.
  - Quick check question: How does the gate mechanism in the vision-language fusion module help balance the contributions of visual and textual features in GAMA?

## Architecture Onboarding

- Component map:
  Vision-Language Fusion Module -> Gender Obfuscation Module -> Narrative Generation Decoder -> Answer Inference Decoder

- Critical path: Image → Vision-Language Fusion → Gender Obfuscation → Narrative Generation → Answer Inference
  - First stage: Generate comprehensive, gender-obfuscated narrative
  - Second stage: Generate unbiased answer using image, narrative, and task-specific prompt

- Design tradeoffs:
  - Freezing ViT parameters vs. fine-tuning: Freezing reduces computational cost but may limit adaptation to task-specific visual features
  - Two-stage vs. single-stage: Two-stage reduces bias but adds computational overhead and complexity
  - Gender obfuscation strength: Strong obfuscation reduces bias but may remove useful gender-relevant context in some cases

- Failure signatures:
  - Narratives lack diversity or coverage of image content
  - Generated answers still exhibit strong gender bias despite narrative context
  - Model performance degrades significantly on task-specific metrics
  - Training becomes unstable due to contrastive loss hyperparameter issues

- First 3 experiments:
  1. Ablation study: Remove gender obfuscation module and compare LIC, Error, and BiasAmp scores to assess its effectiveness
  2. Ablation study: Remove narrative generation stage and compare performance to two-stage framework
  3. Hyperparameter sensitivity: Test different temperature values in contrastive loss and observe effects on gender bias metrics and task performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GAMA perform on benchmarks that include non-binary gender categories, and what modifications would be needed to handle such cases?
- Basis in paper: [inferred] The paper acknowledges that current datasets and benchmarks only consider binary gender, which oversimplifies the complex nature of gender identity.
- Why unresolved: The paper does not explore the performance of GAMA on datasets that include non-binary gender categories, nor does it discuss potential modifications to handle such cases.
- What evidence would resolve it: Experiments on datasets with non-binary gender categories and analysis of GAMA's performance on such datasets, along with proposed modifications to the model to handle non-binary gender.

### Open Question 2
- Question: How does the temperature parameter in the contrastive loss affect the performance of GAMA on different vision-language tasks?
- Basis in paper: [explicit] The paper conducts an ablation study on the temperature hyper-parameter and shows that a larger temperature leads to increased LIC and BiasAmp scores.
- Why unresolved: The paper only explores the effect of temperature on a single task (image captioning) and does not investigate its impact on other tasks or provide a comprehensive analysis of the optimal temperature for different scenarios.
- What evidence would resolve it: Extensive experiments on various vision-language tasks with different temperature values, along with an analysis of the optimal temperature for each task and the underlying reasons for the observed differences.

### Open Question 3
- Question: How does the size of the training dataset for narrative generation affect the generalization ability of GAMA on different benchmarks?
- Basis in paper: [explicit] The paper conducts an ablation study on the data size of Localized Narratives for narrative generation and shows that a larger training set enhances the zero-shot generalization ability of GAMA on VisoGender.
- Why unresolved: The paper only explores the effect of data size on two benchmarks (image captioning and VisoGender) and does not provide a comprehensive analysis of the relationship between training data size and generalization ability across different tasks and datasets.
- What evidence would resolve it: Experiments on a wide range of vision-language tasks and benchmarks with varying sizes of training data for narrative generation, along with an analysis of the impact of data size on generalization ability and the underlying reasons for the observed differences.

## Limitations

- Contrastive loss implementation details are not fully specified, particularly the temperature parameter and exact formulation for gender feature disentanglement
- Hyperparameter sensitivity is not comprehensively explored, with limited ablation studies on beam search size and contrastive loss temperature
- Zero-shot evaluation methodology for VisoGender benchmark lacks clarity on whether models are adapted or fine-tuned for this specific benchmark

## Confidence

**High confidence**: The core two-stage framework design is well-supported by experimental results showing improved gender bias metrics while maintaining task performance. Ablation studies provide strong evidence for the effectiveness of both the gender obfuscation module and the two-stage approach.

**Medium confidence**: The claim that object hallucination is the essence of gender bias is supported by correlation in ablation studies, but the causal mechanism is not fully established. The paper shows correlation between reduced bias and reduced hallucination, but doesn't definitively prove causation.

**Low confidence**: The specific effectiveness of the contrastive learning approach for gender feature disentanglement is moderately supported but could be sensitive to implementation details not fully specified. The gender obfuscation mechanism shows effectiveness, but optimal configuration of contrastive loss parameters remains unclear.

## Next Checks

1. **Contrastive loss hyperparameter sweep**: Systematically test different temperature values (τ) in the contrastive loss function and measure their impact on both gender bias metrics (LIC, Error, BiasAmp) and task performance (BLEU-4, CIDEr).

2. **Cross-dataset generalization test**: Evaluate GAMA on additional gender bias benchmarks beyond VisoGender (such as StereoSet or CrowS-Pairs adapted for VLMs) to verify that gender bias mitigation generalizes across different types of bias measurement and dataset distributions.

3. **Human evaluation of narrative quality**: Conduct human studies to assess whether gender-obfuscated narratives maintain sufficient image coverage and relevance while successfully reducing gender stereotyping, validating whether the model truly understands comprehensive image context.