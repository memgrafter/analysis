---
ver: rpa2
title: Transfer Learning with Clinical Concept Embeddings from Large Language Models
arxiv_id: '2409.13893'
source_url: https://arxiv.org/abs/2409.13893
tags:
- embeddings
- learning
- clinical
- data
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the effectiveness of semantic embeddings from
  large language models (LLMs) for healthcare transfer learning tasks. The research
  compared domain-specific models like Med-BERT against generic models such as OpenAI
  embeddings in classifying influenza cases using electronic health records from two
  healthcare systems.
---

# Transfer Learning with Clinical Concept Embeddings from Large Language Models

## Quick Facts
- arXiv ID: 2409.13893
- Source URL: https://arxiv.org/abs/2409.13893
- Reference count: 34
- Primary result: Domain-specific LLM embeddings outperform generic ones in healthcare transfer learning tasks, particularly for influenza case classification

## Executive Summary
This study evaluates the effectiveness of semantic embeddings from large language models (LLMs) for healthcare transfer learning tasks. The research compares domain-specific models like Med-BERT against generic models such as OpenAI embeddings in classifying influenza cases using electronic health records from two healthcare systems. Domain-specific embeddings consistently outperform generic ones, particularly in local and direct transfer scenarios. While generic models require extensive fine-tuning to achieve optimal performance, domain-specific models like Med-BERT demonstrate robustness with minimal adjustment. However, excessive tuning of domain-specific models can reduce their effectiveness.

## Method Summary
The study uses electronic health records from Allegheny County and Salt Lake County containing 70 clinical findings, age groups, and temperature grades. Clinical concept embeddings are extracted using various LLMs including Med-BERT, BioBERT, BERT, and OpenAI models. A CNN architecture with 1D convolution layer (100 filters, size 1), max pooling, and fully connected layer is employed for classification. The research evaluates local performance and transfer learning scenarios with three tuning strategies: direct sharing, tuning only linear layers, and full tuning of CNN and linear layers.

## Key Results
- Domain-specific embeddings (Med-BERT) consistently outperform generic embeddings in local and direct transfer scenarios
- Excessive fine-tuning of BERT-based models can reduce performance, while baseline and OpenAI models benefit from full tuning
- Semantic embeddings improve classification performance over one-hot encoding by capturing relationships between clinical concepts

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific LLM embeddings (Med-BERT) outperform generic embeddings in local and transfer learning scenarios for clinical classification tasks. Med-BERT is trained on EHR data and medical claims, capturing clinical terminology and context better than generic LLMs like BERT or OpenAI embeddings, leading to improved semantic representations for clinical concepts. This mechanism relies on the assumption that clinical embeddings trained on domain-specific data provide richer semantic representations for clinical tasks compared to generic embeddings.

### Mechanism 2
Excessive fine-tuning of models with biomedical embeddings may reduce effectiveness, while minimal tuning preserves performance. Domain-specific models like BERT-based embeddings already have robust semantic representations for clinical concepts, so additional fine-tuning can lead to overfitting or degradation of learned embeddings. This mechanism assumes that pre-trained domain-specific embeddings have optimized representations that can be disrupted by excessive modification during fine-tuning.

### Mechanism 3
Using semantic embeddings improves the performance of classification tasks over one-hot encoding by capturing relationships between clinical concepts. Semantic embeddings represent clinical concepts as continuous vectors that encode similarities and relationships, allowing models to recognize synonyms and clinically related concepts, whereas one-hot encoding only indicates presence/absence. This mechanism relies on the assumption that continuous vector representations can capture semantic relationships better than discrete, sparse representations like one-hot encoding.

## Foundational Learning

- **Concept: Transfer Learning**
  - Why needed here: The study applies knowledge from one healthcare system to another, reducing the need for extensive data collection and addressing data scarcity
  - Quick check question: What is the difference between traditional machine learning and transfer learning in the context of this study?

- **Concept: Semantic Embeddings**
  - Why needed here: Embeddings represent clinical concepts as vectors, enabling models to understand semantic meanings and relationships, which is crucial for handling heterogeneous clinical languages
  - Quick check question: How do semantic embeddings differ from one-hot encodings in representing clinical concepts?

- **Concept: Convolutional Neural Networks (CNNs)**
  - Why needed here: CNNs are used to process the embedded clinical concepts and extract features for classification tasks, leveraging their ability to capture local patterns
  - Quick check question: Why might CNNs be suitable for processing embedded clinical concepts in this study?

## Architecture Onboarding

- **Component map:** Clinical concept embeddings → Convolutional layer → Max-pooling → Fully connected layer → Classification output
- **Critical path:** Clinical concept embeddings → Convolutional layer → Max-pooling → Fully connected layer → Classification output
- **Design tradeoffs:** Using pre-trained embeddings reduces training time and leverages existing knowledge but limits adaptability; freezing embeddings simplifies the model but may miss domain-specific nuances
- **Failure signatures:** Poor performance may indicate mismatched embeddings, insufficient fine-tuning, or inappropriate model architecture for the data characteristics
- **First 3 experiments:**
  1. Compare classification performance using one-hot encoding versus semantic embeddings from a generic LLM (e.g., BERT) on the local dataset
  2. Evaluate the impact of fine-tuning only the linear layers versus both convolutional and linear layers when transferring models between sites
  3. Test the performance of domain-specific embeddings (e.g., Med-BERT) against generic embeddings in both local and transfer learning scenarios

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of domain-specific embeddings like Med-BERT compare to general embeddings in tasks involving multimodal data or more complex clinical scenarios beyond binary classification? The paper mentions that One-Hot encoding may not perform well in complex clinical tasks such as disease stage classification or multimodal data analysis, suggesting the need for more sophisticated representations. This remains unresolved as the study focuses on binary classification of influenza cases using EHR data.

### Open Question 2
What is the optimal fine-tuning strategy for domain-specific embeddings like Med-BERT when transferring models between healthcare systems with varying levels of data similarity? The paper highlights that excessive tuning of domain-specific models can reduce effectiveness, but it does not provide a clear guideline on the optimal fine-tuning strategy for different levels of data similarity. This remains unresolved as the study does not explore the impact of varying degrees of data similarity between source and target domains on the fine-tuning process.

### Open Question 3
How do other pre-trained LLMs, such as Llama or its biomedical variant Me-Llama, perform in biomedical applications compared to BERT-based and OpenAI models? The paper acknowledges that there are many other LLMs, like Llama, that have demonstrated success in biomedical applications, but their transferability and sensitivity to the tuning process remain untested. This remains unresolved as the study only compares BERT-based and OpenAI models.

## Limitations

- The study relies on retrospective EHR data from only two healthcare systems, limiting generalizability
- The evaluation focuses on a single clinical task (influenza classification) and a fixed set of 70 clinical concepts
- The exact preprocessing and feature extraction methods are not fully specified, which could affect reproducibility

## Confidence

- **High confidence** in the finding that domain-specific embeddings (Med-BERT) outperform generic embeddings in local and transfer learning scenarios
- **Medium confidence** in the claim that excessive fine-tuning degrades BERT-based model performance
- **Low confidence** in the generalizability of results to other clinical tasks or datasets

## Next Checks

1. **External validation:** Test the transfer learning framework on additional healthcare systems and clinical tasks (e.g., sepsis detection) to assess robustness and generalizability
2. **Embedding sensitivity analysis:** Evaluate the impact of varying embedding dimensions and architectures on model performance to identify optimal configurations for different clinical tasks
3. **Fine-tuning ablation study:** Conduct a detailed analysis of fine-tuning strategies (e.g., layer-wise tuning, regularization) to determine the optimal balance between preserving pre-trained knowledge and adapting to new data