---
ver: rpa2
title: 'PRECISE: Pre-training Sequential Recommenders with Collaborative and Semantic
  Information'
arxiv_id: '2412.06308'
source_url: https://arxiv.org/abs/2412.06308
tags:
- training
- recommendation
- item
- embedding
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'PRECISE combines item IDs and textual descriptions from LLM models
  to generate unified item embeddings for sequential recommendation. The method uses
  an MoE network to selectively fuse token embeddings with ID embeddings, then trains
  a Transformer model in two stages: first on all-scene behaviors, then on target-task
  behaviors.'
---

# PRECISE: Pre-training Sequential Recommenders with Collaborative and Semantic Information

## Quick Facts
- arXiv ID: 2412.06308
- Source URL: https://arxiv.org/abs/2412.06308
- Authors: Chonggang Song; Chunxu Shen; Hao Gu; Yaoming Wu; Lingling Yi; Jie Wen; Chuan Chen
- Reference count: 40
- Primary result: Up to 14.89% improvement in recall and 18.5% improvement in NDCG over state-of-the-art methods

## Executive Summary
PRECISE is a pre-training framework for sequential recommendation that combines item IDs with textual descriptions to generate unified item embeddings. The method uses an MoE network to selectively fuse token embeddings from LLM models with ID embeddings, then trains a Transformer model in two stages: first on all-scene behaviors, then on target-task behaviors. The framework demonstrates significant improvements in both offline metrics and online A/B tests, particularly for long-tail items and cold-start scenarios.

## Method Summary
PRECISE combines item IDs and textual descriptions to generate unified item embeddings through an MoE network that selectively fuses token embeddings with ID embeddings. The model uses a two-stage training approach: Universal Training on all-scene behaviors followed by Targeted Training on specific task behaviors with periodic warm-up. The framework is designed to capture both collaborative signals from user interactions and semantic information from item descriptions, addressing limitations in handling long-tail items and cold-start scenarios.

## Key Results
- Achieves up to 14.89% improvement in recall and 18.5% improvement in NDCG over state-of-the-art methods
- Shows 14.5% improvement in recall@10 for long-tail items compared to baselines
- Online A/B tests demonstrate +1.961% clicks, +1.433% shares, and +0.884% reading time improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PRECISE's MoE network selectively fuses token embeddings with ID embeddings to balance semantic and collaborative information
- Mechanism: The MoE network uses a gating network to score token embeddings from LLM, then activates top-k experts to compute weighted semantic embeddings. These are concatenated with ID embeddings to form unified item representations
- Core assumption: Token embeddings contain complementary semantic information to ID embeddings, and selective fusion improves recommendation quality
- Evidence anchors:
  - [abstract]: "PRECISE employs a learning framework that initially models users' comprehensive interests across all recommendation scenarios"
  - [section]: "We introduce an expert network based on the Mixture of Experts (MoE) structure to balance the importance of token embeddings"
  - [corpus]: Weak - no direct corpus evidence for MoE effectiveness in this specific fusion approach

### Mechanism 2
- Claim: Universal Training followed by Targeted Training enables effective transfer of comprehensive interests to specific tasks
- Mechanism: The model first learns global user representations across all scenarios, then fine-tunes on target-specific behaviors while periodically warming up parameters from Universal Training
- Core assumption: Cross-scenario behavior patterns contain transferable knowledge that improves target-task performance
- Evidence anchors:
  - [abstract]: "PRECISE employs a learning framework that initially models users' comprehensive interests across all recommendation scenarios and subsequently concentrates on the specific interests of target-scene behaviors"
  - [section]: "We demonstrate that PRECISE precisely captures the entire range of user interests and effectively transfers them to the target interests"
  - [corpus]: Weak - limited corpus evidence for this specific two-stage training approach

### Mechanism 3
- Claim: Combining ID embeddings with LLM-generated semantic embeddings addresses cold-start and long-tail item limitations
- Mechanism: ID embeddings capture collaborative signals from user interactions, while LLM embeddings provide semantic context, creating comprehensive representations for items with sparse interactions
- Core assumption: Cold items lack sufficient collaborative signals but can be represented through semantic features
- Evidence anchors:
  - [abstract]: "Traditional pre-trained recommendation models mainly capture user interests by leveraging collaborative signals. Nevertheless, a prevalent drawback of these systems is their incapacity to handle long-tail items and cold-start scenarios"
  - [section]: "The second type of approach involves pre-training on user or item IDs. Due to reliance on ID representation, the majority of such studies pre-train models on target-domain datasets"
  - [corpus]: Weak - no direct corpus evidence for this specific cold-start improvement approach

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanisms
  - Why needed here: PRECISE uses decoder-only Transformer blocks for sequence modeling, requiring understanding of masked self-attention and positional encoding
  - Quick check question: How does masked self-attention differ from bi-directional attention in the context of next-item prediction?

- Concept: Mixture of Experts (MoE) networks
  - Why needed here: The MoE module selectively fuses token embeddings, requiring understanding of gating mechanisms and expert activation
  - Quick check question: What determines which experts are activated in an MoE network, and how does this affect model capacity?

- Concept: Large Language Model tokenization and embedding generation
  - Why needed here: PRECISE relies on LLM-generated token embeddings for semantic representation, requiring understanding of how text is processed and embedded
  - Quick check question: How does an LLM convert raw text into token embeddings, and what information do these embeddings capture?

## Architecture Onboarding

- Component map: Embedding Fusion → Universal Training → Targeted Training → Serving
- Critical path: Embedding Fusion → Universal Training → Targeted Training → Serving
- Design tradeoffs:
  - MoE vs. simple concatenation: MoE provides selective fusion but adds complexity
  - Universal vs. Targeted only: Universal captures broader patterns but requires more data
  - Periodic warm-up frequency: More frequent warm-up maintains consistency but increases compute
- Failure signatures:
  - Poor cold-start performance: Suggests semantic embeddings are inadequate
  - Target-task degradation: Indicates insufficient fine-tuning or warm-up frequency
  - High latency: May indicate inefficient embedding retrieval or similarity computation
- First 3 experiments:
  1. Validate MoE gating effectiveness by comparing semantic embedding quality with/without MoE
  2. Test warm-up frequency impact on target-task performance using controlled dataset splits
  3. Evaluate cold-start performance improvement by comparing long-tail item recommendations with baseline ID-only models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the periodical warm-up strategy impact the model's ability to handle real-time updates of newly published items in industrial recommendation systems?
- Basis in paper: [explicit] The paper discusses the challenge of newly published items not being trained by the Universal Training module if the Targeted Training module does not periodically warm up with the Universal Training model.
- Why unresolved: The paper provides an illustration of the problem but does not quantify the exact impact of periodical warm-up on recommendation performance for newly published items or determine the optimal warm-up cycle.
- What evidence would resolve it: Experiments measuring the performance difference between periodical and non-periodical warm-up strategies on newly published items, along with analysis of the optimal warm-up cycle frequency.

### Open Question 2
- Question: What is the impact of the MoE network's expert selection mechanism on the model's ability to balance collaborative and semantic information for long-tail items?
- Basis in paper: [explicit] The paper introduces an MoE network to selectively fuse token embeddings with ID embeddings, but does not explore how different expert selection mechanisms affect the model's performance on long-tail items.
- Why unresolved: The paper mentions the use of MoE but does not provide a detailed analysis of how the expert selection impacts the model's ability to handle long-tail items or whether alternative selection mechanisms could improve performance.
- What evidence would resolve it: Comparative studies of different expert selection mechanisms (e.g., top-k vs. softmax) on long-tail item recommendation performance.

### Open Question 3
- Question: How does the scalability of the PRECISE framework vary with the size of the LLM model and the transformer layers in terms of computational efficiency and recommendation accuracy?
- Basis in paper: [explicit] The paper discusses the scalability of PRECISE in terms of training data and model parameters, but does not provide a detailed analysis of the trade-off between computational efficiency and recommendation accuracy as the model scales.
- Why unresolved: While the paper shows that increasing the scale of the LLM model and transformer layers improves performance, it does not quantify the computational cost or explore the optimal balance between model size and efficiency.
- What evidence would resolve it: Experiments measuring the computational cost (e.g., training time, inference latency) and recommendation accuracy as the model scales, along with analysis of the optimal balance between model size and efficiency.

## Limitations

- Limited empirical validation of MoE gating effectiveness compared to simpler fusion methods
- Insufficient analysis of optimal warm-up frequency for the periodic refresh mechanism
- Claims about cold-start improvement lack detailed analysis of specific item categories or thresholds

## Confidence

- **High Confidence**: The two-stage training framework (Universal → Targeted) and its basic implementation details are well-specified and reproducible
- **Medium Confidence**: The general approach of combining ID and semantic embeddings is sound, but the specific MoE implementation details are sparse
- **Low Confidence**: Claims about MoE gating effectiveness and optimal warm-up frequency lack sufficient empirical backing

## Next Checks

1. Conduct ablation study comparing MoE-based fusion with simple concatenation and weighted averaging of ID and token embeddings to isolate the contribution of selective gating
2. Perform sensitivity analysis on warm-up frequency in Targeted Training by varying the periodic refresh intervals and measuring impact on target-task performance
3. Analyze cold-start performance across different item popularity thresholds to identify specific item categories where semantic embeddings provide the most benefit versus those where they may introduce noise