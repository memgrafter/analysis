---
ver: rpa2
title: Probabilistic Demand Forecasting with Graph Neural Networks
arxiv_id: '2401.13096'
source_url: https://arxiv.org/abs/2401.13096
tags:
- demand
- articles
- graph
- forecasting
- article
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of incorporating article relationships
  into demand forecasting for e-commerce retailers. It proposes GraphDeepAR, a novel
  approach that combines a Graph Neural Network (GNN) encoder with a DeepAR decoder
  to produce probabilistic forecasts.
---

# Probabilistic Demand Forecasting with Graph Neural Networks

## Quick Facts
- arXiv ID: 2401.13096
- Source URL: https://arxiv.org/abs/2401.13096
- Reference count: 37
- Combines GNN encoder with DeepAR decoder for probabilistic demand forecasting

## Executive Summary
This paper introduces GraphDeepAR, a novel approach for probabilistic demand forecasting in e-commerce that leverages article relationships through a Graph Neural Network (GNN) encoder combined with a DeepAR decoder. The method constructs graphs from article attribute similarity to capture relationships between products, then uses the GNN to aggregate demand patterns from related articles before generating probabilistic forecasts. Experiments on three real-world datasets demonstrate consistent improvements over non-graph baselines, particularly for connected articles and top-selling items.

## Method Summary
GraphDeepAR combines a GNN encoder with a DeepAR decoder to produce probabilistic demand forecasts. The GNN encoder constructs a graph from article attribute similarity using pairwise cosine similarity, then iteratively aggregates neighbor features to produce article embeddings that encode both attribute similarity and demand dynamics. The DeepAR decoder handles temporal dynamics by outputting mean and variance parameters for a Student's t-distribution, which is used to sample future demand values. The model is trained end-to-end using synchronized batching and a t-distribution loss function, with early stopping after 5 epochs without improvement.

## Key Results
- GraphDeepAR consistently outperforms DeepAR baseline across three real-world datasets
- Largest RMSE improvements of 4.24% for connected articles and 6.72% for cold-start items on retail dataset
- Model generates useful article embeddings that capture both attribute similarity and demand dynamics for downstream tasks

## Why This Works (Mechanism)

### Mechanism 1
The GNN encoder improves forecasting accuracy by aggregating demand patterns from related articles based on attribute similarity. Graph edges are constructed using pairwise cosine similarity on static article features, and the GNN iteratively aggregates neighbor features to produce article embeddings. Core assumption: Article attribute similarity correlates with demand relationship. Break condition: If attribute similarity does not correlate with demand relationships, the graph structure will be ineffective.

### Mechanism 2
GraphDeepAR produces probabilistic forecasts by sampling from a Student's t-distribution, which is more appropriate for retail demand than Gaussian. The DeepAR decoder outputs mean and variance parameters for the t-distribution, allowing uncertainty quantification. Core assumption: Retail demand has heavier tails than Gaussian distribution can capture. Break condition: If retail demand is well-modeled by Gaussian distribution, using t-distribution adds unnecessary complexity.

### Mechanism 3
GraphDeepAR improves cold-start forecasting by leveraging historical demand from similar articles when individual article history is limited. Articles with fewer than 5 historical demand values benefit from neighbor information aggregated by the GNN. Core assumption: Related articles have similar demand patterns that can inform cold-start predictions. Break condition: If cold-start articles are too dissimilar from neighbors, or if demand patterns are highly article-specific, the aggregation will provide little benefit.

## Foundational Learning

- Concept: Graph Neural Networks
  - Why needed here: GNNs encode article relationships in the graph structure, allowing the model to aggregate information from similar articles
  - Quick check question: How does a graph convolution layer aggregate information from neighboring nodes?

- Concept: Probabilistic forecasting with DeepAR
  - Why needed here: Understanding how DeepAR generates probabilistic forecasts via distribution parameters is critical for extending it with GNN components
  - Quick check question: What distribution does DeepAR use to model future demand, and why?

- Concept: Graph construction from similarity
  - Why needed here: The method for building the article graph from attribute similarity directly impacts model performance
  - Quick check question: What similarity threshold would you use to construct edges, and how would you validate this choice?

## Architecture Onboarding

- Component map: Graph construction → GNN encoder → DeepAR decoder → Demand predictions
- Critical path: 1. Build article graph from attribute similarity 2. Generate GNN embeddings from graph 3. Concatenate embeddings with demand history and features 4. Pass through DeepAR decoder to produce probabilistic forecasts
- Design tradeoffs:
  - Similarity threshold: Higher threshold → sparser graph but more meaningful connections; lower threshold → denser graph but potentially noisy relationships
  - Embedding dimensionality: Larger dimensions capture more information but increase computational cost
  - Neighbor sampling: Limits computational cost but may miss important relationships
- Failure signatures:
  - No improvement over DeepAR baseline: Graph construction or GNN architecture may be ineffective
  - Very slow training: Neighbor sampling ratio or embedding size may be too large
  - Overfitting on small datasets: Model complexity may exceed available data
- First 3 experiments:
  1. Train DeepAR baseline on retail dataset and record performance metrics
  2. Construct article graph with similarity threshold 0.95 and visualize neighbor distribution
  3. Train GraphDeepAR with GNN encoder (2 layers, embedding size 16→8) and compare to baseline

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of graph construction method (e.g., different similarity measures or graph sparsification techniques) affect the performance of GraphDeepAR compared to other graph-based forecasting methods? The paper focuses on a single graph construction approach and does not explore alternative methods or their impact on performance.

### Open Question 2
How does GraphDeepAR's performance scale with increasing numbers of articles and time series compared to non-graph baselines? The experiments are limited to three datasets with a maximum of 80,838 articles, and the paper does not discuss how performance would change with significantly larger datasets.

### Open Question 3
Can GraphDeepAR be extended to incorporate dynamic graph structures that change over time based on demand patterns? The paper uses a static graph structure based on article attributes but acknowledges the importance of demand dynamics.

### Open Question 4
How sensitive is GraphDeepAR's performance to the choice of GNN encoder architecture and hyperparameters? The paper uses a specific GNN encoder architecture and hyperparameters but does not perform extensive ablation studies or sensitivity analysis.

## Limitations
- Effectiveness depends heavily on the quality and relevance of article features for graph construction
- Assumes similar articles have correlated demand patterns, which may not hold for all product categories
- Cold-start performance improvements are dataset-dependent and may not generalize to domains with different product relationships

## Confidence

- Mechanism 1 (GNN improves accuracy): High confidence - well-supported by core paper results and general effectiveness of GNNs for relational data
- Mechanism 2 (t-distribution is better than Gaussian): Medium confidence - supported by theoretical motivation but lacks direct empirical validation in this specific context
- Mechanism 3 (cold-start improvement): Medium confidence - demonstrated on one dataset but requires validation across different cold-start scenarios

## Next Checks

1. **Graph Construction Sensitivity Analysis**: Systematically vary the similarity threshold (e.g., 0.9, 0.95, 0.98) and measure the impact on forecasting accuracy. Track graph density and neighbor distribution to understand the tradeoff between connectivity and relationship quality.

2. **Distribution Choice Validation**: Compare forecasting performance using Gaussian vs. t-distribution on the same datasets. Conduct statistical tests to determine if the heavier tails of t-distribution provide meaningful improvements for retail demand data.

3. **Cross-Domain Cold-Start Testing**: Apply GraphDeepAR to a dataset with fundamentally different product relationships (e.g., electronics vs. apparel) and evaluate cold-start performance. This tests whether the similarity-based approach generalizes beyond the retail domain used in the study.