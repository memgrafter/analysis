---
ver: rpa2
title: Efficient Sketches for Training Data Attribution and Studying the Loss Landscape
arxiv_id: '2402.03994'
source_url: https://arxiv.org/abs/2402.03994
tags:
- sketching
- dimension
- affd
- params
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the memory limitations of traditional sketching
  methods when applied to modern machine learning models that require storing vast
  quantities of gradients or Hessian vector products. The authors propose a novel
  framework for scalable gradient and Hessian vector product sketching, tailored for
  modern hardware like GPUs and TPUs.
---

# Efficient Sketches for Training Data Attribution and Studying the Loss Landscape

## Quick Facts
- arXiv ID: 2402.03994
- Source URL: https://arxiv.org/abs/2402.03994
- Authors: Andrea Schioppa
- Reference count: 40
- One-line primary result: Novel sketching algorithms (AFFD, AFJL, QK) overcome memory limitations in storing gradients/Hessian products for modern ML models, achieving 40-70% wall-time reductions on GPUs/TPUs

## Executive Summary
This paper addresses the critical memory limitations of traditional sketching methods when applied to modern machine learning models that require storing vast quantities of gradients or Hessian vector products. The authors propose a novel framework for scalable gradient and Hessian vector product sketching, tailored for modern hardware like GPUs and TPUs. They introduce three new algorithms (AFFD, AFJL, and QK) that overcome the performance bottlenecks of existing methods by optimizing design choices related to implicit vs explicit sketching and preconditioning strategies.

## Method Summary
The paper introduces three novel sketching algorithms: AFFD (Affine Fastfood Decomposition), AFJL (Affine Johnson-Lindenstrauss), and QK (Kronecker-based orthogonal matrices). These algorithms are designed to efficiently sketch gradients and Hessian vector products while addressing memory bottlenecks on modern accelerators. The methods leverage Kronecker product structures and explicit sketching approaches to achieve significant performance improvements over traditional methods like the Fastfood Transform. The algorithms are demonstrated on pre-trained language models for applications including training data attribution, Hessian spectrum analysis, and intrinsic dimension computation.

## Key Results
- Proposed algorithms achieve 40-70% wall-time reductions on GPUs/TPUs compared to traditional methods
- QK algorithm provides theoretical guarantees for sketching with random orthogonal matrices
- Intrinsic dimension analysis reveals high dimensionality for LLMs on generative tasks, challenging previous assumptions
- Hessian spectrum analysis shows distinct characteristics for pre-trained models compared to smaller networks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Removing lookup-based memory operations from sketching algorithms significantly improves performance on modern accelerators.
- **Mechanism:** Modern accelerators are optimized for dense linear algebra operations. Lookup-based memory operations introduce random memory access patterns that conflict with this optimization, causing performance bottlenecks. By restructuring the algorithms to use Kronecker products, memory access becomes more regular and predictable.
- **Core assumption:** The performance bottleneck in existing sketching algorithms is primarily due to memory access patterns rather than computational complexity.
- **Break condition:** If memory access patterns are not the primary bottleneck, or if accelerator architecture changes to optimize for sparse operations.

### Mechanism 2
- **Claim:** Using explicit gradient sketches instead of implicit gradient sketches provides significant performance advantages.
- **Mechanism:** Implicit gradient sketches require computing the gradient at perturbed parameters and then differentiating through the sketching operation. This introduces additional computational overhead and memory accesses. Explicit sketches compute the gradient first and then apply the sketching operation directly.
- **Core assumption:** The overhead of implicit gradient computation and differentiation outweighs any potential benefits of avoiding explicit gradient materialization.
- **Break condition:** If computational overhead of implicit gradient computation is negligible compared to cost of explicit gradient materialization.

### Mechanism 3
- **Claim:** Replacing Walsh-Hadamard transform with FFT or random orthogonal matrices as pre-conditioners offers significant performance gains.
- **Mechanism:** Walsh-Hadamard transform preconditions sparse inputs for sketching. FFT is computationally more efficient and provides similar preconditioning benefits. Random orthogonal matrices offer even more flexibility and can be implemented efficiently using Kronecker products.
- **Core assumption:** Preconditioning benefits of Hadamard transforms can be achieved with alternative transforms that have better computational properties.
- **Break condition:** If preconditioning benefits of Hadamard transforms are unique and cannot be replicated by alternatives.

## Foundational Learning

- **Concept: Sketching algorithms and their properties**
  - **Why needed here:** The paper introduces novel sketching algorithms and provides theoretical guarantees for them. Understanding properties like norm preservation and dimensionality reduction is crucial.
  - **Quick check question:** What is the key property that a sketching algorithm must satisfy to be useful for dimensionality reduction?

- **Concept: Hessian vector products (HVPs) and their role in machine learning**
  - **Why needed here:** The paper uses HVPs for applications like training data attribution and Hessian spectrum analysis. Understanding HVPs and their computational properties is essential.
  - **Quick check question:** What is a Hessian vector product, and why is it important in machine learning?

- **Concept: Intrinsic dimension and its estimation**
  - **Why needed here:** The paper proposes a novel algorithm for estimating the intrinsic dimension of neural networks. Understanding this concept and existing estimation methods is crucial.
  - **Quick check question:** What is the intrinsic dimension of a neural network, and why is it an important metric?

## Architecture Onboarding

- **Component map:** Sketching algorithms (AFFD, AFJL, QK) -> Pre-conditioners (Hadamard transforms, FFT, random orthogonal matrices) -> Applications (training data attribution, Hessian spectrum analysis, intrinsic dimension computation)

- **Critical path:**
  1. Implement the sketching algorithms (AFFD, AFJL, QK)
  2. Implement the pre-conditioners (Hadamard transforms, FFT, random orthogonal matrices)
  3. Integrate the sketching algorithms into the applications
  4. Verify the theoretical guarantees through experiments

- **Design tradeoffs:**
  - Explicit vs. implicit gradient sketches: Explicit sketches offer better performance but require more memory, while implicit sketches save memory but introduce computational overhead
  - Pre-conditioner choice: Different pre-conditioners offer different performance characteristics and theoretical properties
  - Target dimension: Higher target dimensions provide better approximation quality but increase computational cost

- **Failure signatures:**
  - Poor performance on modern accelerators: Indicates issues with memory access patterns or computational efficiency
  - Inaccurate results in applications: Indicates issues with the sketching algorithms or their theoretical guarantees
  - Memory overflow errors: Indicates issues with memory usage or scalability

- **First 3 experiments:**
  1. Benchmark the performance of the sketching algorithms on a simple dataset and compare with existing methods
  2. Verify the theoretical guarantees of the sketching algorithms through experiments on synthetic data
  3. Apply the sketching algorithms to a real-world machine learning task (e.g., training data attribution) and evaluate their effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the intrinsic dimensionality of large language models for generative tasks beyond summarization?
- **Basis in paper:** The paper demonstrates high intrinsic dimensionality for BART on XSUM summarization but suggests further study is needed for other generative tasks.
- **Why unresolved:** The paper only explores one generative task (summarization) and finds dimensionality close to full model size. It's unclear if this pattern holds for other generative tasks like translation, dialogue, or code generation.
- **What evidence would resolve it:** Empirical measurements of intrinsic dimensionality across diverse generative tasks using the proposed methods would reveal whether high dimensionality is a general property of LLM generative capabilities.

### Open Question 2
- **Question:** Can non-linear projection methods achieve significantly lower intrinsic dimensionality for generative tasks compared to linear sketching methods?
- **Basis in paper:** The paper notes that existing generalization bounds assume low intrinsic dimensionality and suggests "new non-linear projections" may be needed for generative tasks where dimensionality approaches full model size.
- **Why unresolved:** The paper uses linear sketching methods and finds high dimensionality for generative tasks. It conjectures that non-linear methods might help but doesn't explore this possibility.
- **What evidence would resolve it:** Comparative studies of intrinsic dimensionality using both linear and non-linear projection methods across various generative tasks would demonstrate whether non-linear approaches can achieve lower dimensionality.

### Open Question 3
- **Question:** How do Hessian spectral properties evolve during fine-tuning across different model architectures and tasks?
- **Basis in paper:** The paper finds deviations from previous observations about Hessian spectra in smaller networks when analyzing pre-trained LLMs, but only examines three specific models on two tasks.
- **Why unresolved:** The study is limited to Roberta, GPT-2L, and BART on SNLI and XSUM tasks. It's unclear if the observed Hessian characteristics are universal across architectures (CNNs, RNNs) or specific to Transformers, and whether they generalize across diverse tasks.
- **What evidence would resolve it:** Systematic analysis of Hessian spectra across multiple architectures (CNNs, RNNs, Transformers) and diverse tasks (classification, generation, reinforcement learning) would reveal whether the observed patterns are architecture- or task-specific.

## Limitations
- Performance improvements are empirically demonstrated but lack detailed architectural analysis
- Computational advantages of pre-conditioners are shown experimentally but without rigorous comparison to theoretical expectations
- Fastfood Transform limitations are identified but not comprehensively characterized

## Confidence

**High Confidence:** The theoretical guarantees for AFFD and QK are mathematically rigorous and the experimental methodology for benchmarking sketching algorithms is sound.

**Medium Confidence:** The claims about performance improvements on GPUs/TPUs are supported by experiments but the analysis could benefit from deeper architectural insights. The effectiveness of different pre-conditioners is demonstrated but comparative analysis with theoretical predictions is limited.

**Low Confidence:** The paper's assertions about the intrinsic dimensionality of LLMs challenge existing assumptions but the sample size and diversity of models studied may be insufficient for definitive conclusions.

## Next Checks

1. **Memory Access Pattern Analysis:** Conduct a detailed analysis of memory access patterns for the proposed algorithms versus traditional sketching methods on GPU/TPU architectures to validate the claimed performance improvements.

2. **Pre-conditioner Comparative Study:** Systematically compare the effectiveness of different pre-conditioners (Hadamard, FFT, random orthogonal matrices) against theoretical predictions for preconditioning quality and computational efficiency.

3. **Intrinsic Dimension Replication:** Replicate the intrinsic dimension experiments across a broader range of LLM architectures and tasks to verify the claim that intrinsic dimension increases with network size and is task-dependent.