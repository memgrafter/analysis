---
ver: rpa2
title: 'Lost in the Source Language: How Large Language Models Evaluate the Quality
  of Machine Translation'
arxiv_id: '2401.06568'
source_url: https://arxiv.org/abs/2401.06568
tags:
- translation
- source
- llms
- reference
- mode
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how Large Language Models (LLMs) leverage
  source and reference data in machine translation evaluation tasks. The authors conduct
  controlled experiments across various input modes and model types, using both coarse-grained
  and fine-grained prompts to discern the utility of source versus reference information.
---

# Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation

## Quick Facts
- arXiv ID: 2401.06568
- Source URL: https://arxiv.org/abs/2401.06568
- Authors: Xu Huang; Zhirui Zhang; Xiang Geng; Yichao Du; Jiajun Chen; Shujian Huang
- Reference count: 25
- Key outcome: Reference information significantly enhances machine translation evaluation accuracy while source information sometimes degrades performance, indicating LLMs' limited ability to leverage cross-lingual capabilities in this task.

## Executive Summary
This study investigates how Large Language Models (LLMs) leverage source and reference data in machine translation evaluation tasks. Through controlled experiments across various input modes and model types, the authors demonstrate that reference information consistently improves evaluation accuracy, while source information sometimes confuses the model and degrades performance. The findings reveal that LLMs primarily rely on reference translations for quality assessment rather than utilizing their cross-lingual capabilities to evaluate translations directly from source content.

## Method Summary
The authors conducted controlled experiments using the WMT22 test set with MQM annotations across En-De, Zh-En, and En-Ru language pairs. They evaluated four input modes (Translation-only, Source-Translation, Reference-Translation, and Source-Reference-Translation) using both closed models (GPT-3.5-turbo) and open models (Llama2 series) with coarse-grained and fine-grained prompts. Performance was measured through system-level accuracy and segment-level correlation metrics. Fine-tuning experiments were also conducted on open LLMs using MQM data to assess whether task-specific adaptation could overcome observed limitations.

## Key Results
- Reference information significantly enhances evaluation accuracy across all tested models and language pairs
- Source information sometimes degrades performance, particularly for segment-level correlation metrics
- LLMs in Translation-only mode perform reasonably well by default to assessing fluency, which correlates positively with translation quality
- Fine-tuning improves overall performance but does not eliminate the limitation of source information utilization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reference information provides context that LLMs can use to evaluate translation quality, while source information sometimes confuses the model.
- Mechanism: LLMs rely on reference translations to assess quality because references provide a direct target for comparison. Source information, however, introduces cross-lingual complexity that LLMs struggle to reconcile with the translation evaluation task.
- Core assumption: LLMs' cross-lingual capabilities are not fully utilized in translation evaluation tasks.
- Evidence anchors:
  - [abstract]: "reference information significantly enhances the evaluation accuracy, while surprisingly, source information sometimes is counterproductive, indicating LLMs' inability to fully leverage the cross-lingual capability when evaluating translations."
  - [section 3.2]: "The R-T mode surpasses other modes in system-level accuracy and segment-level correlations across models, particularly excelling with strong models like GPT-3.5 and Llama2-70B-Chat."
- Break condition: If future experiments show source information consistently improves evaluation accuracy, this mechanism would be invalidated.

### Mechanism 2
- Claim: LLMs evaluate translation quality based on fluency when only the translation is provided (T mode).
- Mechanism: Without source or reference information, LLMs default to assessing the fluency of the translation itself, which correlates positively with translation quality.
- Core assumption: Fluency is a reasonable proxy for translation quality in the absence of other information.
- Evidence anchors:
  - [section 3.2]: "the T mode can achieve much better performance than a random guess. We posit that LLMs evaluate translations solely based on fluency, which is positively correlated to the translation quality."
- Break condition: If future experiments show that fluency is not a reliable proxy for translation quality, this mechanism would be invalidated.

### Mechanism 3
- Claim: Fine-tuning LLMs with task-specific data improves their performance in translation evaluation but does not eliminate the limitation of source information.
- Mechanism: Fine-tuning adapts the LLM to the specific task of translation evaluation, but the inherent limitation in utilizing source information persists.
- Core assumption: Fine-tuning can improve task-specific performance but may not address fundamental architectural limitations.
- Evidence anchors:
  - [section 5.2]: "Firstly, the performance of the R-T mode remains significantly superior to that of the other two modes, indicating that the model still cannot make full use of the source information after naive fine-tuning."
- Break condition: If future experiments show that fine-tuning can eliminate the negative impact of source information, this mechanism would be invalidated.

## Foundational Learning

- Concept: Cross-lingual capabilities of LLMs
  - Why needed here: Understanding how LLMs handle different languages is crucial for interpreting their performance in translation evaluation tasks.
  - Quick check question: Can LLMs effectively utilize source information in translation evaluation tasks?

- Concept: Prompt engineering for LLMs
  - Why needed here: The study relies on different prompts to elicit the desired behavior from LLMs, highlighting the importance of prompt design.
  - Quick check question: How do different prompts affect the performance of LLMs in translation evaluation tasks?

- Concept: Fine-tuning for task-specific adaptation
  - Why needed here: The study explores the impact of fine-tuning on LLM performance, demonstrating the potential for task-specific adaptation.
  - Quick check question: Can fine-tuning eliminate the limitations of LLMs in utilizing source information for translation evaluation?

## Architecture Onboarding

- Component map: Data preprocessing -> Prompt engineering -> Model evaluation -> Fine-tuning (optional) -> Analysis
- Critical path: Data preparation → Prompt design → Model evaluation → Fine-tuning (optional) → Analysis
- Design tradeoffs:
  - Using open vs. closed LLMs: Open models offer flexibility for fine-tuning but may have lower baseline performance
  - Coarse-grained vs. fine-grained evaluation: Coarse-grained evaluation is faster but less interpretable, while fine-grained evaluation provides more detailed insights
  - Data distribution: Balanced data is crucial for effective fine-tuning, especially for underrepresented language pairs
- Failure signatures:
  - Poor performance in R-T mode: Indicates issues with the reference information or the LLM's ability to utilize it
  - Negative impact of source information: Suggests limitations in the LLM's cross-lingual capabilities for translation evaluation
  - Inconsistent performance across language pairs: May indicate issues with data quality or LLM's multilingual proficiency
- First 3 experiments:
  1. Evaluate the performance of different LLMs using the GEMBA-SQM prompt with all four input modes on the WMT22 test set
  2. Conduct a fine-grained evaluation using the AutoMQM prompt to assess error detection capabilities across different input modes
  3. Fine-tune an open LLM with MQM data and evaluate its performance using the same prompts and metrics as in the first experiment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do Large Language Models (LLMs) sometimes perform worse when source information is included in translation evaluation tasks?
- Basis in paper: [explicit] The authors found that source information sometimes is counterproductive, leading to performance degradation in evaluation accuracy and segment-level correlations.
- Why unresolved: The paper does not provide a definitive explanation for why LLMs struggle with source information, suggesting it may be due to the models' inability to fully leverage cross-lingual capabilities or confusion caused by the source.
- What evidence would resolve it: Experiments isolating specific aspects of cross-lingual understanding and controlled studies on how source information affects model confusion could provide insights.

### Open Question 2
- Question: How does the quality of reference translations impact the performance of reference-based metrics in machine translation evaluation?
- Basis in paper: [explicit] The authors note that poor human-generated reference translations can dramatically hurt the performance and reliability of reference-based metrics.
- Why unresolved: The paper suggests that low-quality references have a negative impact but does not explore the extent or mechanisms of this effect.
- What evidence would resolve it: A systematic study varying reference quality and measuring its impact on different metrics could quantify this relationship.

### Open Question 3
- Question: Can fine-tuning open LLMs with task-specific data eliminate the limitations observed in their ability to utilize source information for translation evaluation?
- Basis in paper: [explicit] The authors fine-tuned an open LLM with MQM data and found that while performance improved, the model still could not fully utilize source information.
- Why unresolved: The fine-tuning experiment showed some improvement but did not resolve the core issue of source information utilization.
- What evidence would resolve it: Further experiments with more sophisticated fine-tuning techniques or larger datasets could determine if this limitation can be overcome.

## Limitations
- The study is limited to three language pairs (En-De, Zh-En, En-Ru), which may not generalize to all language combinations
- The specific mechanisms explaining why source information confuses LLMs remain largely theoretical without deeper probing experiments
- The study uses relatively standard prompt templates without exploring the full space of prompt engineering possibilities

## Confidence
- High confidence in the core finding that reference information significantly improves evaluation accuracy across different models and prompts
- Medium confidence in the conclusion that source information is counterproductive, as this effect varies across models and language pairs
- Low confidence in the specific mechanisms proposed for why source information confuses LLMs, as these remain speculative without deeper probing experiments

## Next Checks
1. **Cross-linguistic generalization test**: Evaluate the same input mode hypotheses across 10+ additional language pairs spanning different language families to determine whether the source information limitation is universal or language-dependent.

2. **Architectural probing experiments**: Use activation analysis and attention visualization techniques to directly observe how LLMs process source versus reference information during evaluation tasks, testing the proposed mechanisms about cross-lingual capability utilization.

3. **Alternative prompt engineering study**: Systematically explore variations in prompt structure, formatting, and instruction wording to determine whether careful prompt design can eliminate or reduce the negative impact of source information on evaluation accuracy.