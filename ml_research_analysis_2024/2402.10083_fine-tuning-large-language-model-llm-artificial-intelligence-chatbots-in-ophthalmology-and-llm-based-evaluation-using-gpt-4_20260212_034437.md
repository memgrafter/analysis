---
ver: rpa2
title: Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in
  Ophthalmology and LLM-based evaluation using GPT-4
arxiv_id: '2402.10083'
source_url: https://arxiv.org/abs/2402.10083
tags:
- response
- your
- surgery
- which
- glaucoma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study fine-tuned five large language models (GPT-3.5, LLAMA2-7b,
  LLAMA2-7b-Chat, LLAMA2-13b, LLAMA2-13b-Chat) on ophthalmology questions and evaluated
  their responses using GPT-4. GPT-4-based evaluation aligned significantly with human
  clinician rankings, with Spearman and Kendall Tau correlation coefficients of 0.90
  and 0.80, respectively.
---

# Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4

## Quick Facts
- arXiv ID: 2402.10083
- Source URL: https://arxiv.org/abs/2402.10083
- Reference count: 0
- GPT-4-based evaluation aligned significantly with human clinician rankings (Spearman 0.90, Kendall Tau 0.80)

## Executive Summary
This study fine-tuned five large language models (GPT-3.5, LLAMA2-7b, LLAMA2-7b-Chat, LLAMA2-13b, LLAMA2-13b-Chat) on ophthalmology questions and evaluated their responses using GPT-4. The evaluation framework demonstrated strong alignment with human clinician rankings, with Spearman and Kendall Tau correlation coefficients of 0.90 and 0.80, respectively. GPT-3.5 fine-tuned responses were ranked highest by both GPT-4 and clinicians, while LLAMA2-7b-generated responses were least preferred. The study validates GPT-4's potential for automated clinical evaluation of LLM chatbot responses, offering an efficient alternative to manual clinician grading.

## Method Summary
The study utilized a dataset of 400 ophthalmology questions and answers, divided into fine-tuning (368 QnA pairs) and testing (40 QnA pairs) datasets. Five pre-trained LLMs were fine-tuned using domain-specific data: GPT-3.5-turbo-0613 via OpenAI's fine-tuning API and the LLAMA2 series models using the H2O.ai library. The fine-tuned models generated responses to the testing dataset questions, which were then evaluated by GPT-4 using a customized clinical evaluation rubric. Human clinicians also ranked the LLM-generated responses for comparison. Spearman, Kendall Tau, and Cohen's Kappa correlation coefficients were calculated to assess the alignment between GPT-4 and human evaluations.

## Key Results
- GPT-4-based evaluation aligned significantly with human clinician rankings (Spearman 0.90, Kendall Tau 0.80)
- GPT-3.5 fine-tuned responses were ranked highest by both GPT-4 and clinicians
- LLAMA2-7b-generated responses were least preferred by both evaluation methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fine-tuning large language models on ophthalmology-specific data improves their accuracy in responding to patient queries.
- **Mechanism**: Fine-tuning adapts a pre-trained model's weights using a curated dataset of ophthalmology questions and answers, aligning its language generation capabilities with domain-specific terminology and clinical accuracy.
- **Core assumption**: The fine-tuning dataset is representative of the real-world distribution of patient queries and contains accurate, high-quality answers.
- **Evidence anchors**:
  - "We find-tuned 5 different LLMs, including LLAMA2-7b, LLAMA2-7b-Chat, LLAMA2-13b, and LLAMA2-13b-Chat, based on the fine-tuning dataset as domain-specific knowledge."
  - "We utilized GPT-3.5-turbo-0613 via the OpenAI fine-tuning API, and the LLAMA2 series models were fine-tuned using the H2O.ai library."
- **Break condition**: Fine-tuning dataset lacks diversity or contains errors, leading to poor generalization or hallucination of incorrect medical information.

### Mechanism 2
- **Claim**: GPT-4 can serve as an effective automated evaluator for LLM-generated clinical responses.
- **Mechanism**: GPT-4, pre-trained on a vast corpus including medical literature, can assess responses based on clinical accuracy, relevance, patient safety, and ease of understanding when guided by a custom rubric.
- **Core assumption**: GPT-4's pre-training data includes sufficient medical knowledge to make clinically informed evaluations.
- **Evidence anchors**:
  - "GPT-4-based evaluation aligned significantly with human clinician rankings, with Spearman and Kendall Tau correlation coefficients of 0.90 and 0.80, respectively."
  - "A customized clinical evaluation rubric was used to guide GPT-4 evaluation, grounded on clinical accuracy, relevance, patient safety, and ease of understanding."
- **Break condition**: GPT-4's pre-training data lacks sufficient medical depth, or the rubric fails to capture critical clinical nuances.

### Mechanism 3
- **Claim**: LLM fine-tuning can degrade original knowledge base, leading to reduced performance on unseen tasks.
- **Mechanism**: Fine-tuning modifies a model's weights to specialize in a domain, potentially overwriting or distorting previously learned general knowledge, which is crucial for handling diverse or novel queries.
- **Core assumption**: The fine-tuning process is applied uniformly across all model parameters, affecting both domain-specific and general knowledge.
- **Evidence anchors**:
  - "For the same test question on laser peripheral iridotomy, the LLAMA2-13b and GPT-3.5 responses described the procedure requiring the patient to 'lie on your side,' which was incorrect as the procedure should be performed in upright sitting position."
  - "This was astutely identified by the GPT-4 evaluation and appropriately penalized."
- **Break condition**: Fine-tuning is applied selectively to specific model components, preserving general knowledge while adapting domain-specific capabilities.

## Foundational Learning

- **Concept**: Transformer-based neural networks with self-attention mechanisms
  - **Why needed here**: Understanding the underlying architecture of LLMs (like GPT-3.5 and LLAMA2) is crucial for comprehending how fine-tuning affects their performance and why certain architectural choices impact their ability to handle ophthalmology-specific queries.
  - **Quick check question**: What is the primary function of self-attention in transformer models, and how does it contribute to the model's ability to process long-range dependencies in text?

- **Concept**: Evaluation metrics for ranking correlation (Spearman, Kendall Tau, Cohen's Kappa)
  - **Why needed here**: Understanding these metrics is essential for interpreting the study's findings on the alignment between GPT-4's automated evaluations and human clinician rankings of LLM-generated responses.
  - **Quick check question**: How do Spearman and Kendall Tau correlation coefficients differ in their calculation and interpretation, and what does a high value for each metric indicate about the agreement between two rankings?

- **Concept**: Overfitting and underfitting in machine learning
  - **Why needed here**: Recognizing the risks of overfitting and underfitting is important for understanding why fine-tuning on a limited ophthalmology dataset might lead to poor generalization on unseen questions or degrade the model's original knowledge base.
  - **Quick check question**: What is the difference between overfitting and underfitting, and how might each scenario manifest in the performance of a fine-tuned LLM on ophthalmology-specific tasks?

## Architecture Onboarding

- **Component map**: Dataset Preparation -> Model Selection -> Fine-tuning Process -> Evaluation Framework -> Human Evaluation
- **Critical path**: Fine-tuning -> Evaluation (GPT-4) -> Comparison (Human Clinician Ranking) -> Analysis
- **Design tradeoffs**:
  - Dataset size vs. diversity: A larger dataset may improve model performance but could be harder to curate and ensure accuracy.
  - Model complexity vs. computational resources: More complex models (e.g., LLAMA2-13b) may perform better but require more computational power for fine-tuning and inference.
  - GPT-4 evaluation vs. human evaluation: Automated evaluation is faster and more scalable but may miss subtle clinical nuances that human experts can identify.
- **Failure signatures**:
  - Low correlation between GPT-4 and human rankings: Indicates GPT-4's evaluation may not align with clinical judgment.
  - High variance in human clinician rankings: Suggests inter-grader variability or lack of consensus on what constitutes a high-quality response.
  - LLM responses containing clinical inaccuracies: Indicates fine-tuning may have introduced errors or degraded the model's original knowledge.
- **First 3 experiments**:
  1. Evaluate the correlation between GPT-4's evaluations and human rankings on a small subset of LLM-generated responses to validate the evaluation framework.
  2. Compare the performance of different fine-tuned models (e.g., GPT-3.5 vs. LLAMA2-13b) on a held-out test set to identify the most effective model for the task.
  3. Analyze LLM responses for common types of errors (e.g., clinical inaccuracies, irrelevant information) to inform future fine-tuning efforts and rubric improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of fine-tuned GPT-3.5 versus native GPT-3.5 vary across different glaucoma subspecialties?
- Basis in paper: Explicit - Paper states "For the sub-analysis focusing on glaucoma-related test questions, we assessed the efficacy of LLMs in handling these subspecialty queries despite not being explicitly trained for it" and "a significant shift was noted with LLAMA2-13b and LLAMA2-13b-Chat exceeding the performance of GPT-3.5" for glaucoma test questions.
- Why unresolved: The paper only evaluated fine-tuned versus native GPT-3.5 for glaucoma test questions, not for other glaucoma subspecialties like angle-closure, secondary glaucoma, or pediatric glaucoma.
- What evidence would resolve it: A head-to-head comparison of native versus fine-tuned GPT-3.5 responses across multiple glaucoma subspecialty domains (angle-closure, secondary, pediatric, etc.) using the same GPT-4 evaluation framework.

### Open Question 2
- Question: What is the optimal fine-tuning strategy for LLAMA2 models in ophthalmology to minimize hallucinations while maximizing clinical accuracy?
- Basis in paper: Explicit - Paper states "our aim was to objectively tune the LLM models to reduce possible chances of hallucinations, thereby enhancing their reliability and accuracy in medical contexts" and notes "responses from the LLAMA2 series were noted on several occasions to contain unnecessary repetition of phrases with redundant or irrelevant content."
- Why unresolved: The paper used a single fine-tuning approach (3 epochs, specific parameters) but did not explore alternative strategies like different epoch numbers, learning rates, or domain-specific pre-training.
- What evidence would resolve it: Comparative analysis of different fine-tuning strategies (varying epochs, learning rates, domain pre-training) on hallucination rates and clinical accuracy scores for ophthalmology LLMs.

### Open Question 3
- Question: How does GPT-4 evaluation variability compare to inter-clinician variability in ranking LLM responses?
- Basis in paper: Explicit - Paper states "Further work would be needed to explore the variability of GPT-4 responses" and shows Cohen's Kappa of 0.50 between GPT-4 and human rankings, while noting "disparities in statistical correlations" between GPT-4 and different clinicians.
- Why unresolved: The paper acknowledges GPT-4 variability as a limitation but does not quantify it through repeated evaluations or compare it directly to human grader variability.
- What evidence would resolve it: Multiple independent evaluations by GPT-4 on the same LLM responses, compared statistically to multiple clinicians ranking the same responses, to quantify and compare variability patterns.

## Limitations

- The study's findings are based on a relatively small dataset of 400 ophthalmology questions, which may limit generalizability to broader clinical scenarios
- GPT-4's evaluation reliability depends heavily on the custom rubric design, though the specific rubric components are not fully detailed
- Fine-tuning may have introduced domain-specific knowledge at the cost of some general medical knowledge, as evidenced by clinical inaccuracies in some responses

## Confidence

- **High confidence** in GPT-4's ability to evaluate clinical responses reliably (supported by strong correlation coefficients of 0.90 and 0.80 with human clinicians)
- **Medium confidence** in the generalizability of findings due to limited dataset size
- **Medium confidence** in the long-term stability of fine-tuned models, as knowledge degradation patterns were observed

## Next Checks

1. Replicate the evaluation framework using a larger, more diverse ophthalmology dataset to test generalizability
2. Conduct longitudinal testing of fine-tuned models to assess knowledge stability over time
3. Implement a blind study where human clinicians evaluate responses without knowing the source model to eliminate potential bias