---
ver: rpa2
title: Measuring Meaning Composition in the Human Brain with Composition Scores from
  Large Language Models
arxiv_id: '2403.04325'
source_url: https://arxiv.org/abs/2403.04325
tags:
- composition
- scores
- brain
- score
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Composition Score, a computational metric
  for quantifying meaning composition in neural language models based on the key-value
  memory interpretation of transformer feed-forward networks. The metric measures
  the degree to which the output of an FFN block depends on multiple neurons versus
  a few, using Jensen-Shannon distances between vocabulary distributions.
---

# Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models

## Quick Facts
- arXiv ID: 2403.04325
- Source URL: https://arxiv.org/abs/2403.04325
- Reference count: 26
- Key outcome: Composition Scores from LLaMA2 models predict brain activity in left inferior frontal and anterior superior temporal regions during language comprehension

## Executive Summary
This paper introduces the Composition Score, a computational metric for quantifying meaning composition in neural language models based on the key-value memory interpretation of transformer feed-forward networks. The metric measures the degree to which the output of an FFN block depends on multiple neurons versus a few, using Jensen-Shannon distances between vocabulary distributions. Applied to LLaMA2 models during processing of "The Little Prince," the scores show increasing compositionality in higher layers and partial correlation with word frequency and syntactic node counts. When regressed against fMRI data from subjects listening to the same text, the Composition Score predicts neural activity in left inferior frontal and anterior superior temporal regions better than control variables, with regression scores up to 0.177.

## Method Summary
The authors compute Composition Scores using LLaMA2-7B models (both base and chat variants) on English text of "The Little Prince" (15,376 words). For each word and layer, they calculate Jensen-Shannon distances between the final FFN output distribution and each neuron's predicted distribution, then normalize by dividing the minimum distance by the maximum. These scores are aligned with fMRI data from 49 subjects listening to the audiobook and regressed against brain activity using ridge regression with automatic cross-validation. Statistical significance is tested using cluster-based permutation tests (10,000 permutations) with clusters defined as spatially contiguous vertices exceeding p < 0.05 and minimum size of 20 vertices.

## Key Results
- Composition Scores increase with layer depth in LLaMA2 models, plateauing after layer 6
- The metric shows partial correlation with word frequency and syntactic node counts
- Regression scores between Composition Scores and fMRI data reach up to 0.177 in left inferior frontal and anterior superior temporal regions
- These brain regions overlap with areas linked to word frequency, syntactic processing, and general word sensitivity
- Hidden layer activity correlates with a different left temporal-parietal network, indicating complementary information about compositional processing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Composition Score captures meaning composition by measuring the degree to which the FFN output depends on a distributed population of neurons rather than a few dominant ones.
- Mechanism: The score computes Jensen-Shannon distances between the final FFN output distribution and each neuron's predicted distribution. A high score indicates all neurons contribute similarly to the output, implying distributed composition; a low score indicates one or two neurons dominate.
- Core assumption: The final vocabulary prediction is a weighted mixture of individual neuron predictions, and the spread of distances reflects the degree of meaning composition.
- Evidence anchors:
  - [abstract] "the metric measures the degree to which the output of an FFN block depends on multiple neurons versus a few, using Jensen-Shannon distances between vocabulary distributions."
  - [section 3.1.2] Formal definition: Sl_comp = min dist / max dist.
  - [corpus] Weak evidence: no direct neural data correlating distance spread to human composition, but matches transformer memory interpretation.

### Mechanism 2
- Claim: Higher layers in the model show increased compositionality, aligning with deeper integration of context in meaning composition.
- Mechanism: As the input prefix to the FFN block includes more context, the key-value memory lookup becomes less likely to find exact matches, forcing the block to combine multiple memory entries.
- Core assumption: Residual connections do not fully cancel the contextual enrichment effect in higher layers.
- Evidence anchors:
  - [section 3.1.3] "both the LLaMA2-base and LLaMA2-chat models exhibit a similar pattern, with the mean Composition Score increasing in the first 6 layers and plateauing thereafter."
  - [corpus] Assumption: context integration increases with depth, but no direct proof the model is mimicking human composition.

### Mechanism 3
- Claim: The Composition Score captures brain activity in regions linked to conceptual and syntactic combination, reflecting multifaceted meaning composition.
- Mechanism: When the score is high, the distributed neural code predicts vocabulary that requires integrating multiple concepts, activating left frontal and anterior temporal areas involved in composition.
- Core assumption: The same distributed code in the model corresponds to distributed neural firing in humans.
- Evidence anchors:
  - [abstract] "Brain regions associated with the Composition Score encompass those underlying word frequency, structural processing, and general sensitivity to words."
  - [section 5.3.2] "The Composition Scores derived from LLaMA2-chat exhibit a significant association with a cluster in the LIFG and the LaSTG."
  - [corpus] Weak evidence: no causal proof that score drives activity, only correlation.

## Foundational Learning

- Transformer FFN as key-value memory
  - Why needed here: The Composition Score relies on interpreting each neuron's output as a memory entry that predicts a vocabulary distribution.
  - Quick check question: In the FFN equation FFl(x) = f(x · Kl^T) · V^l, what does each row of Kl represent?

- Jensen-Shannon divergence and distance
  - Why needed here: The score uses JS distances to quantify similarity between vocabulary distributions.
  - Quick check question: Why take the square root of JS divergence to get a distance?

- fMRI regression and cluster-based statistics
  - Why needed here: Validating the score requires regressing it against brain data and testing clusters for significance.
  - Quick check question: What does the noise ceiling represent in normalized regression scores?

## Architecture Onboarding

- Component map: Text → tokenizer → LLaMA2 layers → per-layer FFN output → vocabulary distributions → Jensen-Shannon distances → Composition Score → regression vs. fMRI
- Critical path: Tokenization → forward pass through model → FFN memory interpretation → distance computation → score aggregation → statistical analysis
- Design tradeoffs: Computing all dm distances is accurate but slow; using d'm << dm speeds computation but may miss weak contributors. The quotient form is robust to scale but sensitive to outliers.
- Failure signatures: Extremely low scores everywhere indicate memorization; extremely high scores everywhere suggest flat, uninformative distributions. Inconsistent layer patterns may indicate residual connection effects.
- First 3 experiments:
  1. Verify that higher layers produce higher average Composition Scores on a small test corpus.
  2. Confirm that approximating with d'm neurons yields scores within 5% of full computation on held-out data.
  3. Check that regressing scores against fMRI data produces significant clusters in expected language regions before scaling to full dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the patterns of high and low Composition Scores across layers relate to the memory efficiency optimization of LLMs and potential memory mechanisms in the human brain?
- Basis in paper: [explicit] The authors note that the varying patterns of Composition Scores across different layers may be attributed to the residual connection structure and the nature of model training, but state that they have yet to fully comprehend these patterns.
- Why unresolved: The paper acknowledges the patterns but doesn't provide a complete explanation for why specific prefixes receive high or low scores in different layers.
- What evidence would resolve it: Detailed analysis of how specific word combinations and syntactic structures affect Composition Scores across layers, and comparison with known memory mechanisms in neuroscience.

### Open Question 2
- Question: Does the Composition Score metric accurately capture the true population variance of memorized and predicted vocabulary distributions, especially when distances are sharply distributed rather than evenly distributed?
- Basis in paper: [explicit] The authors acknowledge that the metric may over-simplify the true population variance, especially when distances are sharply rather than evenly distributed.
- Why unresolved: The current metric uses a simple quotient of minimum and maximum distances, which may not fully capture complex distribution patterns.
- What evidence would resolve it: Testing the metric against known distributions and comparing it with more sophisticated information theory-based metrics like entropy or Gini Index.

### Open Question 3
- Question: How generalizable are the findings to other LLMs and languages beyond English?
- Basis in paper: [explicit] The authors state that they only employed LLaMA2-7B models and focused on English text stimuli, leaving potential for further exploration in multilingual experiments.
- Why unresolved: The study is limited to one model family and one language, which may not capture the full range of meaning composition processes.
- What evidence would resolve it: Testing the Composition Score across multiple model architectures (different sizes, training objectives) and multiple languages with varying morphological complexity.

## Limitations

- The key-value memory interpretation of transformers remains a hypothesis rather than established fact
- The study shows correlation but cannot establish causation between model scores and brain activity
- Brain regions associated with Composition Scores overlap with areas linked to multiple linguistic functions, raising questions about specificity
- The metric may capture low-level statistical regularities rather than higher-level composition

## Confidence

- High confidence: The Composition Score is a well-defined computational metric that can be consistently computed across transformer layers and shows expected patterns (increasing with layer depth, varying with word frequency and syntactic complexity)
- Medium confidence: The metric captures meaningful aspects of neural processing during language comprehension, as evidenced by brain-fMRI correlations in expected language regions
- Low confidence: The metric specifically captures meaning composition as opposed to other linguistic phenomena, and that the key-value memory interpretation is the correct mechanistic explanation

## Next Checks

1. Apply the Composition Score to parallel texts in multiple languages to test whether it captures language-independent aspects of composition or is biased toward English-specific patterns.

2. Design stimuli with varying levels of semantic compositionality (e.g., compound words, idiomatic expressions, novel combinations) and test whether the metric shows graded responses that match human behavioral measures of composition difficulty.

3. Compute the Composition Score using different transformer architectures (e.g., GPT, BERT, OPT) on the same text to determine whether the key-value memory interpretation is architecture-specific or a general property of transformer-based models.