---
ver: rpa2
title: Scaling Laws for Discriminative Classification in Large Language Models
arxiv_id: '2405.15765'
source_url: https://arxiv.org/abs/2405.15765
tags:
- language
- customer
- domain
- support
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a two-phase training pipeline for adapting
  large language models (LLMs) to discriminative classification tasks in closed domains,
  specifically for customer support template selection. The approach involves domain
  adaptation through autoregressive language modeling followed by fine-tuning with
  discrete labels.
---

# Scaling Laws for Discriminative Classification in Large Language Models

## Quick Facts
- arXiv ID: 2405.15765
- Source URL: https://arxiv.org/abs/2405.15765
- Authors: Dean Wyatte; Fatemeh Tahmasbi; Ming Li; Thomas Markovich
- Reference count: 40
- Key outcome: Two-phase training pipeline achieves 4-5% accuracy improvement over BERT baselines and 3.56% response selection time reduction

## Executive Summary
This paper introduces a two-phase training pipeline for adapting large language models (LLMs) to discriminative classification tasks in closed domains, specifically for customer support template selection. The approach involves domain adaptation through autoregressive language modeling followed by fine-tuning with discrete labels. Experiments with Pythia models (410M, 1.4B, and 2.8B parameters) show that classification loss scales linearly with both domain adaptation FLOPs and training tokens, while language modeling loss predicts classification performance. The method outperforms BERT-large baselines by 4-5% in accuracy and achieves a 3.56% reduction in customer support response selection time in online A/B tests.

## Method Summary
The paper presents a two-phase training pipeline for adapting LLMs to discriminative classification tasks in closed domains. Phase 1 involves domain adaptation through autoregressive language modeling on customer support transcripts, while Phase 2 conducts discriminative fine-tuning with discrete labels for template selection. The approach uses Pythia models pre-trained on domain-general web data, fine-tuning them first on domain-specific customer support data, then on labeled template selection data. The method employs a right-most token pooling strategy during fine-tuning and evaluates performance using Top-1, Top-3, and Top-5 accuracy metrics.

## Key Results
- Classification loss scales linearly with domain adaptation FLOPs and training tokens
- Language modeling loss during domain adaptation predicts classification performance after fine-tuning
- Method outperforms BERT-large baselines by 4-5% accuracy with 3.56% response selection time reduction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language modeling loss during domain adaptation linearly predicts classification loss after fine-tuning
- Mechanism: Fine-tuning uses the right-most token as classification input, reducing to language modeling objective over class token
- Core assumption: Class token treated as special "next token" in fine-tuning
- Evidence anchors: Linear relationship observed in Figure 3(c), abstract mentions scaling curves
- Break condition: Changes to pooling strategy or non-discrete classification classes

### Mechanism 2
- Claim: Domain adaptation significantly improves classification accuracy over BERT baselines
- Mechanism: Autoregressive pre-training on domain-specific data improves model's representation of domain-relevant text
- Core assumption: Domain-specific language modeling objective improves understanding of customer support patterns
- Evidence anchors: 4-5% accuracy improvement mentioned in abstract, section confirms regular improvements
- Break condition: Insufficient domain data or overfitting during adaptation

### Mechanism 3
- Claim: Larger models achieve lower classification loss for same compute budget
- Mechanism: Increased model capacity enables better representation of complex data patterns
- Core assumption: Scaling laws from generative modeling apply to discriminative fine-tuning
- Evidence anchors: Linear scaling observed across model sizes, overlap in scaling curves
- Break condition: Insufficient compute for larger models or overly simple tasks

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Entire pipeline relies on transformer-based LLMs
  - Quick check question: What is the role of causal masking in decoder-only transformers, and why is it important for this application?

- Concept: Domain adaptation techniques
  - Why needed here: Two-phase pipeline starts with domain adaptation
  - Quick check question: How does domain adaptation differ from standard pre-training, and why is it particularly effective for customer support applications?

- Concept: Scaling laws in machine learning
  - Why needed here: Paper's main contribution is observing scaling laws for this task
  - Quick check question: What are the key variables in neural scaling laws, and how do they relate to model performance in this context?

## Architecture Onboarding

- Component map: Raw transcript → domain adaptation → discriminative fine-tuning → template suggestion
- Critical path: Raw transcript → domain adaptation → discriminative fine-tuning → template suggestion
- Design tradeoffs:
  - Model size vs. latency: Larger models are more accurate but slower
  - Domain adaptation data volume vs. fine-tuning data volume: Balancing compute costs
  - Context length: 512 tokens for fair BERT comparison vs. 2048 for full Pythia capability
- Failure signatures:
  - High domain adaptation loss: Insufficient domain data or poor model initialization
  - Overfitting during fine-tuning: Too few fine-tuning examples or too many epochs
  - Poor latency: Model too large for inference budget or inefficient implementation
- First 3 experiments:
  1. Verify that domain adaptation improves classification accuracy over baseline BERT
  2. Test the linear relationship between language modeling loss and classification loss
  3. Measure latency vs. accuracy tradeoff for different model sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relationship between model size and data volume for optimal domain adaptation in closed domains?
- Basis in paper: Scaling laws focus on open domains, leaving closed domain adaptation unexplored
- Why unresolved: Only explores Pythia models up to 2.8B parameters and 30B tokens
- What evidence would resolve it: Systematic experiments varying both model size and data volume to derive closed-form scaling equation

### Open Question 2
- Question: How does choice of pooling strategy during discriminative fine-tuning affect observed linear relationship?
- Basis in paper: Only tests right-most token pooling, other strategies could yield different scaling relationships
- Why unresolved: Limited to one pooling strategy
- What evidence would resolve it: Comparative experiments using different pooling strategies while measuring both language modeling loss and classification loss

### Open Question 3
- Question: What is optimal balance between domain adaptation duration and discriminative fine-tuning duration for different model sizes?
- Basis in paper: Trains domain adaptation for 14,500 steps but only fine-tunes for one epoch
- Why unresolved: Doesn't explore trade-offs across model sizes
- What evidence would resolve it: Experiments varying both adaptation and fine-tuning durations for each model size while measuring final classification accuracy

### Open Question 4
- Question: How do scaling laws generalize to other closed-domain classification tasks?
- Basis in paper: Claims applicability to any structured conversation or text classification situation
- Why unresolved: Only validates on customer support template selection
- What evidence would resolve it: Application to diverse closed-domain tasks with varying numbers of classes and context lengths

## Limitations
- Lack of theoretical justification for linear relationships between language modeling loss and classification loss
- Domain adaptation data preprocessing pipeline (PII redaction) described only generally
- Focus exclusively on Pythia models, limiting generalizability to other LLM architectures

## Confidence
- High Confidence: Empirical observation that domain adaptation improves accuracy over BERT baselines
- Medium Confidence: Claim that larger models achieve lower classification loss for same compute budget
- Low Confidence: Generalizability of scaling laws to other domains or model architectures

## Next Checks
1. Develop formal theoretical framework explaining why language modeling loss during domain adaptation predicts classification loss
2. Replicate experiments using different LLM architectures (e.g., LLaMA, Mistral) to test architecture-agnostic scaling laws
3. Conduct comprehensive cost-benefit analysis comparing two-phase pipeline against simpler fine-tuning approaches across multiple customer support scenarios