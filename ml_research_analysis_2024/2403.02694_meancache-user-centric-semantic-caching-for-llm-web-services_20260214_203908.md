---
ver: rpa2
title: 'MeanCache: User-Centric Semantic Caching for LLM Web Services'
arxiv_id: '2403.02694'
source_url: https://arxiv.org/abs/2403.02694
tags:
- queries
- meancache
- cache
- query
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MeanCache is a user-centric semantic caching system for LLM-based
  services that leverages federated learning to train query similarity models locally
  on user devices, preserving privacy while improving accuracy. It addresses the limitations
  of server-side semantic caching by eliminating network costs, reducing storage needs
  by 83%, and accelerating cache hit-and-miss decisions by 11%.
---

# MeanCache: User-Centric Semantic Caching for LLM Web Services

## Quick Facts
- arXiv ID: 2403.02694
- Source URL: https://arxiv.org/abs/2403.02694
- Reference count: 40
- Primary result: 83% storage reduction and 11% faster cache decisions vs. server-side caching

## Executive Summary
MeanCache introduces a user-centric semantic caching system for LLM-based web services that addresses privacy and efficiency concerns through federated learning. By training query similarity models locally on user devices rather than centralizing data, the system preserves user privacy while maintaining high caching accuracy. The approach leverages MPNet for efficient query embedding, PCA for compression, and autonomous threshold determination to optimize semantic matching. Compared to GPTCache, MeanCache achieves a 17% higher F-score and 20% increase in precision while significantly reducing storage requirements and cache lookup latency.

## Method Summary
MeanCache implements a decentralized semantic caching architecture where user devices train local query similarity models using federated learning, then share only model updates rather than raw queries. The system uses MPNet to generate query embeddings, applies PCA compression to reduce dimensionality, and autonomously determines optimal cosine similarity thresholds for semantic matching. A local cache stores embeddings with their corresponding LLM responses, and similarity scores are computed using the compressed embeddings. The federated learning framework aggregates model updates across users to improve the global similarity model while preserving privacy.

## Key Results
- 83% reduction in storage requirements through PCA compression and embedding optimization
- 11% acceleration in cache hit-and-miss decisions compared to server-side semantic caching
- 17% higher F-score and 20% increase in precision compared to GPTCache baseline

## Why This Works (Mechanism)
MeanCache works by shifting the computational and storage burden from centralized servers to user devices while maintaining caching accuracy through federated learning. The system trains query similarity models locally, eliminating the need to transmit raw queries or store large embedding matrices centrally. MPNet provides rich semantic embeddings that capture query intent, while PCA compression reduces storage requirements without significant accuracy loss. The autonomous threshold determination adapts to query distribution changes, and federated learning aggregates improvements across the user population. This distributed approach eliminates network latency for cache lookups while preserving the semantic understanding necessary for effective caching.

## Foundational Learning

**Federated Learning**: Distributed machine learning approach where models train locally on devices and only model updates are shared. Needed to preserve privacy while improving similarity models across users. Quick check: Verify model convergence with increasing number of participating devices.

**MPNet Embeddings**: Multi-perspective sentence embeddings that capture semantic meaning of queries. Needed to represent queries in a space where semantically similar queries are close together. Quick check: Compare similarity scores between semantically related and unrelated queries.

**PCA Compression**: Dimensionality reduction technique that preserves variance while reducing storage requirements. Needed to make local caching feasible on devices with limited storage. Quick check: Measure F-score degradation as compression ratio increases.

**Cosine Similarity Thresholding**: Method for determining when two query embeddings are semantically similar enough to share cached responses. Needed to balance precision and recall in cache matching. Quick check: Plot precision-recall curve across different threshold values.

## Architecture Onboarding

**Component Map**: User Device -> MPNet Embedding -> PCA Compression -> Local Cache -> Cosine Similarity -> Response Retrieval

**Critical Path**: Query Input → MPNet → PCA → Cosine Similarity → Cache Lookup → Response Delivery

**Design Tradeoffs**: Privacy preservation vs. computational overhead on user devices; storage efficiency vs. embedding quality; federated learning benefits vs. model synchronization complexity.

**Failure Signatures**: High false positive rate indicates threshold too low; high false negative rate indicates threshold too high; poor convergence indicates insufficient training data or model capacity issues.

**First Experiments**: 1) Benchmark MPNet embedding quality on semantic similarity tasks; 2) Evaluate PCA compression impact on retrieval accuracy; 3) Test federated learning convergence with varying numbers of participating devices.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided content.

## Limitations

- Federated learning computational overhead on user devices not fully characterized in terms of battery impact and device compatibility
- Evaluation limited to MMLU dataset may not represent real-world query diversity
- Performance with multi-turn conversations and context-dependent queries remains unclear
- System robustness against malicious queries or attempts to game caching mechanisms not addressed

## Confidence

- High Confidence: 83% storage reduction and 11% faster cache decisions supported by concrete measurements
- Medium Confidence: 17% higher F-score may not generalize to all query distributions or real-world usage patterns
- Low Confidence: "No network costs" claim overstated due to initial model synchronization and update requirements

## Next Checks

1. Deploy MeanCache in production LLM web service over 3-month period to validate real-world performance metrics and measure actual battery impact across diverse devices

2. Systematically test MeanCache's robustness against adversarial queries designed to bypass caching mechanisms or trigger false positives

3. Evaluate MeanCache across representative range of consumer devices to quantify federated learning computational overhead and memory requirements under typical usage scenarios