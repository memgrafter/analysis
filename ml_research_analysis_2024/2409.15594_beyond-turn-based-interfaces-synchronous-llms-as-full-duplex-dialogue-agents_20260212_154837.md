---
ver: rpa2
title: 'Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents'
arxiv_id: '2409.15594'
source_url: https://arxiv.org/abs/2409.15594
tags:
- dialogue
- speech
- spoken
- chunk
- turn-taking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Synchronous LLMs (SyncLLM), a novel framework
  for full-duplex spoken dialogue modeling that addresses the challenge of modeling
  synchrony in pre-trained language models, which lack a sense of time. The method
  integrates time information into Llama3-8b through periodic synchronization tokens
  and trains the model on interleaved speech chunks, enabling it to generate two synchronized
  speech streams for natural dialogue with backchannels and overlaps.
---

# Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents

## Quick Facts
- arXiv ID: 2409.15594
- Source URL: https://arxiv.org/abs/2409.15594
- Authors: Bandhav Veluri; Benjamin N Peloquin; Bokai Yu; Hongyu Gong; Shyamnath Gollakota
- Reference count: 33
- Primary result: +2.2-point improvement in dialogue content Meaningfulness over dGSLM while maintaining turn-taking Naturalness

## Executive Summary
This paper introduces Synchronous LLMs (SyncLLM), a novel framework for full-duplex spoken dialogue modeling that addresses the challenge of modeling synchrony in pre-trained language models. The method integrates time information into Llama3-8b through periodic synchronization tokens and trains the model on interleaved speech chunks, enabling it to generate two synchronized speech streams for natural dialogue with backchannels and overlaps. A three-stage training approach leverages 212k hours of synthetic spoken dialogue data and 2k hours of real-world data to overcome limited real-world spoken dialogue datasets.

## Method Summary
SyncLLM extends Llama3-8b with periodic synchronization tokens and speaker tags [S0]/[S1] to model timing information. The model is trained on deduplicated HuBERT token sequences in interleaved chunks, allowing it to predict two synchronized speech streams. A three-stage training approach is employed: (1) turn-based spoken dialogue with synthetic speech data, (2) full-duplex dialogue assuming no overlaps, and (3) fine-tuning on real-world spoken dialogue data. The model uses HiFi-GAN for speech synthesis and demonstrates robustness to Internet-scale latencies up to 240 ms.

## Key Results
- SyncLLM achieves +2.2-point improvement in dialogue content Meaningfulness over state-of-the-art full-duplex model dGSLM
- Maintains turn-taking Naturalness while improving semantic quality
- Demonstrates robust performance with Internet-scale latencies up to 240 ms
- Generalizes well to out-of-distribution data, maintaining coherent conversations between models trained on different datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model achieves synchrony by integrating periodic synchronization tokens into the token sequence, providing a common time frame for both speakers.
- Mechanism: Speaker tags [S0] and [S1] are interleaved at fixed intervals (chunk size) to mark the start of each speaker's turn and maintain timing information even after deduplication.
- Core assumption: Deduplicated token sequences with coarse timing from periodic synchronization tokens can preserve semantic content while enabling synchronous generation.
- Evidence anchors:
  - [abstract] "We design a novel mechanism to integrate time information into Llama3-8b so that they run synchronously with the real-world clock."
  - [section 3.2] "To model dialog between two speakers 0 & 1, we define two special tokens [S0] and [S1], referred to as speaker tags, specifying the start of each speaker's token sequence, respectively."
  - [corpus] Weak - the corpus doesn't directly address the synchronization token mechanism.
- Break condition: If deduplication removes too much timing information or if the periodic synchronization becomes too coarse, the model may lose the ability to maintain precise synchrony with real-world timing.

### Mechanism 2
- Claim: The model predicts interleaved chunks of speech for both speakers, enabling full-duplex generation where both sides can speak and listen simultaneously.
- Mechanism: The model is trained to predict deduplicated HuBERT token sequences for both speakers in interleaved chunks, allowing it to generate two synchronized speech streams.
- Core assumption: An auto-regressive transformer can effectively model the interleaving of two speech streams when trained on appropriately formatted data.
- Evidence anchors:
  - [abstract] "We design a novel mechanism to integrate time information into Llama3-8b so that they run synchronously with the real-world clock."
  - [section 3] "SyncLLM is trained to predict interleaving chunks of speech units corresponding to both sides of the dialogue"
  - [corpus] Moderate - related papers discuss full-duplex modeling but don't specifically address the interleaved chunk prediction mechanism.
- Break condition: If the interleaving becomes too complex or the chunk size is not appropriately chosen, the model may struggle to maintain coherent dialogue generation for both speakers.

### Mechanism 3
- Claim: The three-stage training approach leverages synthetic spoken dialogue data to overcome the limited availability of real-world spoken dialogue data.
- Mechanism: Stage 1 uses synthetic speech from text dialogue datasets to achieve text-speech alignment, Stage 2 treats synthetic data as full-duplex without overlaps, and Stage 3 fine-tunes on real-world spoken dialogue data.
- Core assumption: Large-scale synthetic data can effectively bridge the gap to real-world data, especially when combined with a carefully designed training progression.
- Evidence anchors:
  - [abstract] "We also introduce a training recipe that uses 212k hours of synthetic spoken dialogue data generated from text dialogue data to create a model that generates meaningful and natural spoken dialogue, with just 2k hours of real-world spoken dialogue data."
  - [section 4] "We use supervised finetuning (SFT) datasets, as our source text-dialogue datasets. We used Bark TTS (AI, 2023) model to generate spoken versions of text-dialogue datasets"
  - [corpus] Moderate - related work mentions synthetic data usage but doesn't detail the specific three-stage approach.
- Break condition: If the synthetic data doesn't adequately capture the nuances of real spoken dialogue, or if the training stages don't properly build upon each other, the model may not generalize well to real-world interactions.

## Foundational Learning

- Concept: Auto-regressive transformer architecture
  - Why needed here: SyncLLM builds upon Llama3-8b, which uses an auto-regressive transformer decoder. Understanding this architecture is crucial for comprehending how the model generates sequences token by token.
  - Quick check question: How does an auto-regressive transformer generate sequences, and what are the implications for speech modeling?

- Concept: Discrete speech tokenization (HuBERT)
  - Why needed here: The model represents speech as discrete tokens using HuBERT, which is essential for integrating speech into the transformer architecture.
  - Quick check question: What is the role of HuBERT tokenization in spoken dialogue modeling, and how does it differ from text tokenization?

- Concept: Deduplication of token sequences
  - Why needed here: Deduplication is used to remove repeated tokens caused by silence, which helps maintain semantic content while preserving timing information through synchronization tokens.
  - Quick check question: Why is deduplication important for spoken dialogue modeling, and how does it affect the semantic capability of the model?

## Architecture Onboarding

- Component map: Llama3-8b base model -> HuBERT tokenizer -> Speaker tags [S0]/[S1] -> Deduplication -> Interleaving -> HiFi-GAN vocoder
- Critical path: Token generation → deduplication → synchronization → interleaving → speech synthesis
- Design tradeoffs: Balancing chunk size for latency tolerance vs. maintaining coherent dialogue generation; choosing between text-speech interleaving strategies at sentence vs. turn level.
- Failure signatures: Poor turn-taking naturalness (indicated by low correlation in turn-taking events); degraded semantic quality (high perplexity); inability to handle Internet-scale latencies.
- First 3 experiments:
  1. Evaluate semantic quality (perplexity) of generated spoken dialogues using ASR and text dialogue model.
  2. Measure turn-taking naturalness by computing Pearson correlation of turn-taking event durations between generations and ground-truth continuations.
  3. Test latency tolerance by simulating LLM-LLM interactions with different chunk sizes (corresponding to different latencies).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise effect of the 240 ms latency threshold on the semantic coherence of the generated dialogue, and why does performance degrade beyond this point?
- Basis in paper: [explicit] The paper shows that SyncLLM is robust to latencies up to 200 ms but experiences a performance drop at 240 ms in the LLM-LLM interaction setting (Fig. 8).
- Why unresolved: The paper demonstrates a performance drop at 240 ms but does not provide a detailed analysis of the underlying mechanisms causing this degradation in semantic coherence.
- What evidence would resolve it: Experiments comparing semantic coherence (e.g., ASR perplexity, human evaluation) at incremental latency points between 200-240 ms and beyond, with ablation studies on the model's ability to handle delayed user input.

### Open Question 2
- Question: How does the choice of tokenization strategy (HuBERT vs. other speech tokenizers like EnCodec) affect the naturalness and meaningfulness of the generated dialogue, especially in terms of capturing expressivity and non-verbal sounds?
- Basis in paper: [explicit] The paper mentions that the current implementation uses HuBERT and notes that more expressive multi-codebook tokenizers like EnCodec could be used but have higher token rates (Limitations section).
- Why unresolved: The paper uses HuBERT but does not explore the impact of using alternative tokenizers on the quality of the generated dialogue, particularly regarding expressivity and non-verbal sounds.
- What evidence would resolve it: Comparative experiments using different tokenizers (e.g., HuBERT, EnCodec) to generate dialogue and evaluating the resulting naturalness and meaningfulness through human evaluation and objective metrics.

### Open Question 3
- Question: What is the impact of using different datasets (e.g., Fisher vs. Candor) for training the two interacting models on the coherence and naturalness of the full-duplex conversation, and how does this generalize to other datasets?
- Basis in paper: [explicit] The paper demonstrates that SyncLLM can perform coherent conversations even when the user's side is generated by a model trained on a different dataset (Fisher vs. Candor) in the LLM-LLM interaction setting (Fig. 7).
- Why unresolved: The paper shows that SyncLLM-F-C (Fisher-trained interacting with Candor-trained) performs similarly to SyncLLM-F-F (both Fisher-trained), but does not extensively explore the impact of using different datasets or generalize this finding to other datasets.
- What evidence would resolve it: Experiments training interacting models on various combinations of datasets (e.g., Fisher-CANDOR, Fisher-SWITCHBOARD) and evaluating the coherence and naturalness of the resulting conversations through human evaluation and objective metrics.

## Limitations

- Heavy reliance on synthetic data (212k hours) to compensate for limited real-world spoken dialogue data (2k hours), raising questions about synthetic-to-real transfer fidelity
- Deduplication mechanism for token sequences is not fully specified, making it difficult to assess whether timing information is adequately preserved
- Evaluation relies on ASR-based perplexity measurements, which introduce additional uncertainty as ASR errors could affect semantic quality assessments

## Confidence

- **High confidence** in the core architectural contribution of integrating synchronization tokens into Llama3-8b for full-duplex spoken dialogue modeling
- **Medium confidence** in the three-stage training approach's effectiveness, as the paper provides empirical results but limited ablation studies on individual training stages
- **Medium confidence** in the +2.2 point improvement claim, as it's based on human evaluation but the evaluation protocol details are somewhat limited
- **Low confidence** in the model's generalization to real-world scenarios with higher latencies or more complex overlapping patterns, as the paper primarily tests controlled conditions

## Next Checks

1. Conduct ablation studies on the three-stage training approach to isolate the contribution of each stage and verify that synthetic data effectively bridges to real-world performance
2. Test the model's performance on more diverse spoken dialogue datasets with varying levels of overlap complexity and background noise to assess robustness beyond the Fisher dataset
3. Evaluate the model's ability to handle latencies beyond 240ms and measure the degradation in dialogue quality to establish practical limits of the full-duplex capability