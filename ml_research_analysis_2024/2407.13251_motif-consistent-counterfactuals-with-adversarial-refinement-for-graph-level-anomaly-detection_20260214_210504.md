---
ver: rpa2
title: Motif-Consistent Counterfactuals with Adversarial Refinement for Graph-Level
  Anomaly Detection
arxiv_id: '2407.13251'
source_url: https://arxiv.org/abs/2407.13251
tags:
- graph
- graphs
- counterfactual
- detection
- anomaly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses graph-level anomaly detection by introducing
  counterfactual graphs to improve generalization. The proposed MotifCAR framework
  generates counterfactual graphs by combining the discriminative motif (core subgraph)
  of one graph with the contextual subgraph of another, preserving identification
  information while incorporating environmental characteristics.
---

# Motif-Consistent Counterfactuals with Adversarial Refinement for Graph-Level Anomaly Detection

## Quick Facts
- arXiv ID: 2407.13251
- Source URL: https://arxiv.org/abs/2407.13251
- Authors: Chunjing Xiao; Shikang Pang; Wenxin Tai; Yanlong Huang; Goce Trajcevski; Fan Zhou
- Reference count: 40
- Key outcome: MotifCAR achieves F1 scores of 0.96, 0.94, 0.96, and 0.87 on IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, and REDDIT-MULTI-5K respectively

## Executive Summary
This paper introduces MotifCAR, a framework for graph-level anomaly detection that generates counterfactual graphs to improve model generalization. The approach combines discriminative motifs (core subgraphs containing identification information) from one graph with contextual subgraphs (environmental characteristics) from another, then refines these raw counterfactuals using a GAN-based optimizer. Experiments on four benchmark datasets demonstrate significant performance improvements over state-of-the-art methods.

## Method Summary
MotifCAR generates counterfactual graphs by extracting the discriminative motif from one graph and combining it with the contextual subgraph of another graph. This raw generation is followed by GAN-based refinement using three tailored losses: motif consistency loss ensures validity by maintaining identification information, contextual loss ensures proximity by matching environmental characteristics, and connection loss ensures sparsity by controlling edge density. The refined counterfactual graphs are then used to train an anomaly detection model, improving its ability to generalize to unseen data distributions.

## Key Results
- MotifCAR outperforms state-of-the-art baselines with F1 scores of 0.96, 0.94, 0.96, and 0.87 on four datasets
- The framework effectively generates high-quality counterfactual graphs meeting realism, validity, proximity, and sparsity properties
- GAN-based refinement significantly improves the quality of raw counterfactual graphs compared to baseline generation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MotifCAR improves graph-level anomaly detection by generating counterfactual graphs that preserve identification information while incorporating environmental characteristics.
- Mechanism: The framework combines the discriminative motif (core subgraph containing identification information) of one graph with the contextual subgraph of another graph to create raw counterfactual graphs, then refines them using a GAN-based optimizer with three tailored losses.
- Core assumption: The discriminative motif is the core subgraph that determines the category of a graph and exists in the graphon.
- Evidence anchors:
  - [abstract] "The proposed MotifCAR framework generates counterfactual graphs by combining the discriminative motif (core subgraph) of one graph with the contextual subgraph of another"
  - [section] "According to [15], the discriminative motif is a subgraph of a graph that decides the category of this graph. Hence, it can be regarded as the core subgraph containing the identification (category) information of a given graph."
  - [corpus] Weak evidence - the corpus doesn't discuss motif-based approaches specifically, but contains related work on graph anomaly detection and counterfactual learning.
- Break condition: If the discriminative motif doesn't exist in the graphon or doesn't capture sufficient category information, the framework would fail to preserve identification information.

### Mechanism 2
- Claim: The GAN-based graph optimizer refines raw counterfactual graphs to meet counterfactual properties (Realism, Validity, Proximity, Sparsity) through adversarial training and three tailored losses.
- Mechanism: The optimizer uses a generator to adjust edges and a discriminator to distinguish real from generated graphs. The motif consistency loss ensures validity, contextual loss ensures proximity, and connection loss ensures sparsity.
- Core assumption: Adversarial training with the discriminator can guide the generator to produce graphs close to realistic data.
- Evidence anchors:
  - [abstract] "we present a Generative Adversarial Network (GAN)-based graph optimizer to refine the raw counterfactual graphs. It adopts the discriminator to guide the generator to generate graphs close to realistic data, i.e., meet the property Realism."
  - [section] "To meet the desired counterfactual properties, we design three specific losses for the graph optimizer."
  - [corpus] No direct evidence - the corpus doesn't discuss GAN-based refinement for counterfactual graphs specifically.
- Break condition: If the adversarial training fails to converge or the losses don't effectively capture the desired properties, the generated graphs would not meet the counterfactual properties.

### Mechanism 3
- Claim: Combining motif and contextual subgraph from different graphs enlarges the training data distribution and helps the model learn transferable relations across different environments.
- Mechanism: By creating unseen combinations of motif and contextual subgraph from graphs that may have the same/different categories/clusters, the framework expands the data manifold.
- Core assumption: Different graphs have different contextual subgraphs that represent environmental characteristics.
- Evidence anchors:
  - [abstract] "These two source graphs can be from the same/different categories/clusters."
  - [section] "These unseen combinations can enlarge the distribution of training data and help the model learn transferable relations across different environments."
  - [corpus] Weak evidence - the corpus doesn't explicitly discuss environmental characteristics in graph contexts.
- Break condition: If the contextual subgraph doesn't capture meaningful environmental differences or if combining from different categories disrupts the identification information, the approach would fail.

## Foundational Learning

- Concept: Graphons and their estimation
  - Why needed here: Graphons are used to extract motifs and build the masked matrix for counterfactual generation
  - Quick check question: How does the step function estimation method approximate graphons from a set of graphs?

- Concept: Discriminative motifs and their relationship to graph categories
  - Why needed here: The discriminative motif is the core concept for preserving identification information in counterfactual generation
  - Quick check question: Why does Theorem 1 state that the discriminative motif of a graph exists in the graphon from which the graph was sampled?

- Concept: GAN-based graph generation and adversarial training
  - Why needed here: The GAN-based optimizer is central to refining raw counterfactual graphs to meet counterfactual properties
  - Quick check question: How does the motif consistency loss in the GAN optimizer ensure that generated graphs maintain their identification information?

## Architecture Onboarding

- Component map: Raw Graph Producer -> GAN-based Graph Optimizer -> Anomaly Detection Model
- Critical path:
  1. Input two graphs to raw graph producer
  2. Generate raw counterfactual graph by combining motif and contextual subgraph
  3. Feed raw graph to GAN optimizer
  4. Generator adjusts edges based on three losses while discriminator distinguishes real/fake
  5. Output refined counterfactual graphs
  6. Train anomaly detection model on combined dataset
- Design tradeoffs:
  - Using graphons for motif extraction provides theoretical guarantees but adds computational overhead
  - The random connection between motif and contextual subgraph in raw generation may create distorted structures requiring refinement
  - The GAN framework adds complexity but enables more sophisticated refinement than simple perturbation methods
- Failure signatures:
  - Poor F1 scores on test datasets (indicating failure to generalize)
  - High Realism scores but low Validity scores (generated graphs look real but don't preserve category information)
  - Generator and discriminator losses not converging during training
- First 3 experiments:
  1. Test raw graph producer alone: Generate counterfactual graphs without optimization and measure Realism, Validity, Proximity, and Sparsity scores to establish baseline
  2. Test each loss component: Remove one loss at a time (motif consistency, contextual, connection) and measure impact on counterfactual properties and detection performance
  3. Hyperparameter sensitivity: Vary 位g, 位1, 位2, 位3 values and plot F1 scores to identify optimal ranges and understand each parameter's influence on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MotifCAR scale with graph size and density compared to existing methods?
- Basis in paper: [inferred] The paper mentions that graphon estimation performs better on dense graphs and that MotifCAR achieves more significant advantages on IMDB-Binary and REDDIT-Binary datasets which are denser.
- Why unresolved: The paper only evaluates on four datasets with varying characteristics but doesn't systematically explore how performance scales with graph size or density.
- What evidence would resolve it: Systematic experiments varying graph size and density parameters, and comparing performance degradation/gains relative to baseline methods.

### Open Question 2
- Question: What is the theoretical relationship between the quality of graphon estimation and the effectiveness of the motif consistency loss?
- Basis in paper: [explicit] The paper uses graphons to extract motifs and mentions Theorem 1 about graphons preserving discriminative motifs.
- Why unresolved: The paper doesn't provide a theoretical analysis of how graphon estimation error propagates to the motif consistency loss and ultimately affects performance.
- What evidence would resolve it: A theoretical framework linking graphon estimation error bounds to motif consistency loss effectiveness and downstream detection performance.

### Open Question 3
- Question: How sensitive is MotifCAR to the choice of motif detection algorithm?
- Basis in paper: [inferred] The paper mentions that the discriminative motif is a subgraph that decides the category of a graph, but doesn't discuss or evaluate different motif detection approaches.
- Why unresolved: The paper assumes a specific motif definition but doesn't explore how different motif detection algorithms might affect performance.
- What evidence would resolve it: Experiments comparing MotifCAR performance using different motif detection algorithms (e.g., frequent subgraph mining, graphlet-based approaches, etc.).

## Limitations

- Theoretical guarantees for motif preservation across different graph categories remain unproven
- The framework's computational complexity due to graphon estimation and GAN training may limit scalability
- Performance evaluation is limited to four specific graph datasets, requiring validation on broader graph types

## Confidence

- High confidence: The framework architecture and experimental methodology are clearly specified
- Medium confidence: The counterfactual properties (Realism, Validity, Proximity, Sparsity) and their measurement approaches
- Low confidence: The theoretical guarantees for motif preservation across different graph categories and the generalization of the GAN-based refinement to unseen graph structures

## Next Checks

1. Conduct ablation studies by removing each of the three GAN losses individually to quantify their specific contributions to performance improvements
2. Test the framework on additional graph datasets with different structural properties (e.g., citation networks, biological networks) to assess generalizability
3. Implement controlled experiments where motif-contextual combinations are restricted to same-category graphs versus cross-category combinations to validate the environmental learning hypothesis