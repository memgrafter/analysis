---
ver: rpa2
title: 'MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation
  Question Answering'
arxiv_id: '2409.03171'
source_url: https://arxiv.org/abs/2409.03171
tags:
- question
- task
- system
- crag
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MARAGS, a multi-adapter retrieval-augmented
  generation system for Meta's Comprehensive RAG (CRAG) competition. The system addresses
  the challenge of building robust RAG systems capable of handling diverse question
  types, dynamic answers, and varying entity popularity while minimizing hallucinations.
---

# MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering

## Quick Facts
- arXiv ID: 2409.03171
- Source URL: https://arxiv.org/abs/2409.03171
- Authors: Mitchell DeHaven
- Reference count: 15
- Achieved 2nd place in Task 1 and 3rd place in Task 2 of Meta's CRAG competition

## Executive Summary
MARAGS is a multi-adapter retrieval-augmented generation system designed to handle diverse question types in Meta's Comprehensive RAG (CRAG) competition. The system combines document segmentation, API-based knowledge graph integration, candidate ranking, and task-specific LoRa adapters to generate accurate answers while minimizing hallucinations. It demonstrates strong performance across various question types, particularly excelling at false premise detection, though facing challenges with finance-related queries and dynamic answers.

## Method Summary
MARAGS employs a pipeline architecture that segments HTML documents into manageable chunks, generates API calls to access knowledge graphs for structured data retrieval, and uses a cross-encoder to rank candidate answers. The system leverages task-specific LoRa adapters fine-tuned on Llama 3 8B to handle different question requirements. A key innovation is the relabeling of training data to reduce hallucinations, where the model learns to identify when information is unavailable or when questions contain false premises. The multi-adapter approach allows the system to adapt to various task requirements while maintaining performance across the competition's diverse question types.

## Key Results
- Achieved 2nd place in Task 1 and 3rd place in Task 2 of Meta's CRAG competition
- Demonstrated strong performance on false premise questions
- Successfully minimized hallucinations through relabeling training data strategy
- Showed particular challenges with finance-related queries and dynamic answers

## Why This Works (Mechanism)
The MARAGS system succeeds by combining multiple complementary approaches to handle the complexity of real-world question answering. The document segmentation allows for more focused retrieval, while API integration with knowledge graphs provides access to structured data that traditional RAG systems might miss. The cross-encoder ranking ensures high-quality candidate selection before generation. The use of task-specific LoRa adapters enables the system to adapt its generation strategy based on the question type, whether it requires factual answers, handling of false premises, or dynamic information. The relabeling approach trains the model to recognize when it should decline to answer or indicate information unavailability, directly addressing the hallucination problem common in RAG systems.

## Foundational Learning
- **HTML Document Segmentation**: Breaking large documents into smaller, semantically coherent chunks improves retrieval precision by reducing noise and focusing on relevant content sections. Quick check: Verify that segmented chunks maintain topical coherence and that important context isn't split across boundaries.
- **Knowledge Graph API Integration**: Structured data retrieval through APIs complements unstructured document retrieval, providing precise answers for entity-based queries. Quick check: Test API response times and handle cases where the knowledge graph is unavailable or returns incomplete data.
- **Cross-Encoder Ranking**: Using cross-encoders to re-rank retrieved candidates improves answer quality by considering the full context-query relationship rather than just individual similarity scores. Quick check: Compare cross-encoder rankings against simpler similarity metrics to quantify performance gains.
- **LoRa Adapter Fine-tuning**: Task-specific adapters allow for efficient adaptation to different question types without full model retraining, enabling specialization while maintaining general capabilities. Quick check: Test adapter performance on out-of-distribution questions to assess generalization limits.
- **Hallucination Reduction through Relabeling**: Training data relabeling teaches the model to recognize information gaps and false premises, reducing the tendency to generate incorrect information. Quick check: Evaluate the model's confidence scores on questions where it should decline to answer versus those requiring generation.
- **Multi-Adapter Architecture**: Using multiple specialized adapters rather than a single general-purpose model allows for better handling of diverse task requirements while maintaining efficiency. Quick check: Compare performance of single vs. multi-adapter approaches on the full range of question types.

## Architecture Onboarding

Component Map: HTML Document Segmentation -> Knowledge Graph API Calls -> Cross-Encoder Ranking -> Task-Specific LoRa Adapters -> Answer Generation

Critical Path: The critical path flows from document ingestion through segmentation, retrieval, ranking, and finally generation. The most time-sensitive components are the API calls to knowledge graphs and the cross-encoder ranking, as these involve external dependencies and computational overhead. The LoRa adapters introduce minimal latency but require careful loading and switching between task types.

Design Tradeoffs: The system trades computational efficiency for accuracy by using multiple specialized components rather than a single monolithic model. The API integration adds complexity and potential failure points but provides access to structured data that significantly improves performance on entity-based queries. The relabeling approach increases training time but reduces hallucinations in production. The multi-adapter architecture requires more memory and management overhead but enables better task-specific performance.

Failure Signatures: Common failures include API timeouts or rate limiting when accessing knowledge graphs, cross-encoder failures due to malformed input from segmentation, adapter loading errors when switching between task types, and generation failures when the model encounters out-of-distribution prompts. The system may also produce incomplete answers if segmentation splits relevant information across chunks.

First Experiments:
1. Test the document segmentation pipeline on a sample of HTML documents to verify chunk quality and ensure important context isn't lost during segmentation.
2. Validate the knowledge graph API integration by querying a small set of entity-based questions and measuring response times and accuracy.
3. Evaluate the cross-encoder ranking component by comparing its output against baseline similarity metrics on a validation set of question-candidate pairs.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on task-specific LoRa adapters requires retraining for new task types, limiting scalability to unseen question categories
- Performance degradation on finance-related queries suggests domain-specific challenges not fully addressed
- Heavy dependence on API calls introduces potential latency and availability concerns not extensively evaluated
- Relabeling approach effectiveness depends on quality and representativeness of modified training data

## Confidence
- High confidence: Competitive ranking (2nd in Task 1, 3rd in Task 2) and technical architecture are well-documented and verifiable
- Medium confidence: Claims about hallucination reduction and false premise performance lack detailed ablation studies or error analysis
- Medium confidence: Limitations regarding finance queries and dynamic answers are noted but not thoroughly analyzed

## Next Checks
1. Conduct comprehensive ablation studies comparing MARAGS performance with and without the relabeling strategy, LoRa adapters, and API integration components to quantify their individual contributions.
2. Test the system on out-of-distribution question types not present in the competition dataset, particularly focusing on complex reasoning tasks and emerging domain-specific queries.
3. Evaluate the system's robustness to API failures or latency by simulating network interruptions and measuring performance degradation, including fallback mechanisms when knowledge graph access is unavailable.