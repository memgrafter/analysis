---
ver: rpa2
title: 'M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain Multi-Hop
  Dense Sentence Retrieval'
arxiv_id: '2403.14074'
source_url: https://arxiv.org/abs/2403.14074
tags:
- retrieval
- learning
- multi-hop
- dense
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of open-domain multi-hop dense
  sentence retrieval for fact verification. The authors propose M3, a recursive multi-hop
  dense sentence retrieval system that leverages a novel multi-task mixed-objective
  learning framework for dense text representation learning.
---

# M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain Multi-Hop Dense Sentence Retrieval

## Quick Facts
- arXiv ID: 2403.14074
- Source URL: https://arxiv.org/abs/2403.14074
- Authors: Yang Bai, Anthony Colas, Christan Grant, Daisy Zhe Wang
- Reference count: 0
- One-line primary result: Achieves state-of-the-art performance on FEVER dataset with document-level recall@5 of 0.790 and sentence-level recall@5 of 0.719 for multi-hop retrieval

## Executive Summary
This paper addresses the challenge of open-domain multi-hop dense sentence retrieval for fact verification by proposing M3, a recursive multi-hop dense sentence retrieval system. The authors introduce a novel multi-task mixed-objective learning framework that combines contrastive learning with claim classification objectives to improve dense text representation learning. The system demonstrates state-of-the-art performance on the FEVER dataset while introducing an efficient hybrid ranking algorithm for combining single-hop and multi-hop evidence.

## Method Summary
M3 is a recursive multi-hop dense sentence retrieval system built upon a novel multi-task mixed-objective learning framework. The system uses a bi-encoder architecture initialized from DPR-MultiData and implements iterative dense sentence retrieval with sentence reranking after each step. The multi-task learning combines contrastive loss with claim classification loss, while the mixed-objective framework allows sequential training across different datasets. A dynamic hybrid ranking algorithm combines single-hop and multi-hop retrievals by normalizing scores and applying step-wise scaling with factor γ.

## Key Results
- Achieves document-level recall@5 of 0.790 and sentence-level recall@5 of 0.719 for multi-hop retrieval on FEVER dataset
- Attains highest label accuracy (0.8054) and second-best FEVER score (0.7743) on blind FEVER test set
- Ablation studies show multi-task learning, mixed-objective learning, and hybrid ranking algorithm all contribute to improved performance

## Why This Works (Mechanism)

### Mechanism 1
Multi-task learning with contrastive and classification objectives reduces representation conflicts in dense retrieval. By jointly optimizing both contrastive loss and NLI classification loss, the model learns sentence representations that are both retrieval-friendly and semantically discriminative, reducing internal conflicts when passages contain multiple semantically distant sentences. Core assumption: The FEVER dataset's annotations provide reliable supervision for both retrieval and classification. Evidence: "bypass the conflicts in contrastive learning by performing dense sentence-level retrieval and combining it with multi-task and mixed-objective learning."

### Mechanism 2
Mixed-objective learning framework enables effective use of multiple datasets with different objectives for dense text representation learning. The framework allows sequential training across different datasets with user-defined intervals, maximizing the use of diverse supervision signals. Core assumption: Different datasets capture complementary aspects of text representation. Evidence: "To make maximum use of these valuable datasets... we developed a framework that allows dense encoders to be trained over these datasets with different user-defined objectives."

### Mechanism 3
Hybrid ranking algorithm combining single-hop and multi-hop evidence through normalized score combination and step-wise scaling improves overall recall. The algorithm first normalizes scores from single-hop and multi-hop retrievals, then combines them using weighted sum where multi-hop scores are scaled by factor γ. Core assumption: Both single-hop and multi-hop retrievals provide complementary evidence. Evidence: "we propose a dynamic hybrid ranking algorithm to jointly rank the single-hop and multi-hop retrievals."

## Foundational Learning

- Concept: Contrastive learning for dense retrieval
  - Why needed here: M3 builds on contrastive learning but identifies its limitations and extends it with multi-task and mixed-objective frameworks
  - Quick check question: What is the primary objective of contrastive learning in dense retrieval, and what limitation does M3 identify with this approach?

- Concept: Multi-hop reasoning in fact verification
  - Why needed here: M3 specifically addresses multi-hop evidence retrieval where evidence must be aggregated from multiple documents before logical reasoning
  - Quick check question: How does multi-hop retrieval differ from single-hop retrieval in the context of fact verification, and why is this distinction important for M3?

- Concept: Negative sampling strategies in contrastive learning
  - Why needed here: M3 uses BM25 for initial negative sampling followed by filtering with an attention-based model to reduce false negatives
  - Quick check question: Why does M3 filter BM25-sampled negatives with an attention-based model, and what problem does this solve in contrastive learning?

## Architecture Onboarding

- Component map: Dense Sentence Retriever (M3-DSR) with multi-task mixed-objective learning -> Sentence Reranker using pairwise classification -> Iterative retrieval pipeline with claim and evidence concatenation -> Hybrid ranking algorithm for combining single-hop and multi-hop results

- Critical path: Claim → DSR → Rerank → Hybrid Ranking → Claim Classifier
  - The retrieval component (DSR + Rerank) is most critical for overall performance

- Design tradeoffs:
  - Sentence-level vs document-level retrieval: M3 chooses finer granularity to reduce representation conflicts but increases computational cost
  - Iterative vs one-pass retrieval: M3 uses iterative approach for multi-hop evidence but requires multiple retrieval steps
  - Single-task vs multi-task learning: M3 uses multi-task learning but must balance competing objectives

- Failure signatures:
  - Low recall@5 despite high similarity scores: likely negative sampling issues or representation collapse
  - Poor performance on multi-hop claims: likely insufficient evidence aggregation or hybrid ranking issues
  - Training instability: likely conflicts between contrastive and classification objectives

- First 3 experiments:
  1. Test retrieval recall with different α/β ratios in multi-task learning to find optimal balance
  2. Compare mixed-objective learning with single-dataset training to verify performance gains
  3. Evaluate hybrid ranking algorithm against simple concatenation baselines on multi-hop recall

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of multi-task learning loss weights (α/β) for dense sentence retrieval?
- Basis in paper: The paper explores this question in Section 6.1, showing that α/β = 30 gives the highest retrieval recall.
- Why unresolved: The paper only tests a limited range of ratios and does not explore the full space of possible values.
- What evidence would resolve it: Further experiments testing a wider range of α/β ratios and identifying the global optimum.

### Open Question 2
- Question: How does the proposed M3-DSR method compare to other state-of-the-art dense retrieval methods on other datasets beyond FEVER?
- Basis in paper: The paper only evaluates M3-DSR on the FEVER dataset, leaving its performance on other datasets unexplored.
- Why unresolved: The paper does not provide any comparison with other methods on datasets other than FEVER.
- What evidence would resolve it: Experiments evaluating M3-DSR on a variety of other datasets, such as MS MARCO, Natural Questions, and TriviaQA.

### Open Question 3
- Question: How does the proposed hybrid ranking algorithm compare to other methods for combining single-hop and multi-hop retrieval results?
- Basis in paper: The paper proposes a novel hybrid ranking algorithm in Section 3.4 and compares it to two baseline methods in Table 4.
- Why unresolved: The paper only compares the proposed algorithm to two baseline methods and does not explore the full space of possible ranking algorithms.
- What evidence would resolve it: Further experiments testing a wider range of ranking algorithms and identifying the best approach for combining single-hop and multi-hop retrieval results.

## Limitations

- The paper only evaluates on the FEVER dataset, leaving performance on other datasets unexplored
- Limited ablation studies on the mixed-objective learning framework to quantify benefits of training on multiple datasets
- Specific implementation details for normalization and scaling factors in hybrid ranking algorithm are unclear

## Confidence

- Mechanism 1 (Multi-task learning): Medium
- Mechanism 2 (Mixed-objective learning): Medium
- Mechanism 3 (Hybrid ranking): High

## Next Checks

1. **Ablation Study on Multi-Task Learning**: Conduct an ablation study that trains M3 with only the contrastive objective, only the classification objective, and both objectives to quantify the contribution of each component and determine if there are conflicts between them.

2. **Mixed-Objective Learning Comparison**: Compare the performance of M3 trained with the mixed-objective learning framework against a version trained solely on the FEVER dataset with multi-task learning to determine if sequential training on multiple datasets provides significant benefits.

3. **Hybrid Ranking Algorithm Analysis**: Perform a detailed analysis of the hybrid ranking algorithm by varying the scaling factor γ and examining its impact on different types of claims (single-hop vs multi-hop) to ensure the algorithm is robust across claim types and not over-tuned to the validation set.