---
ver: rpa2
title: 'ANAH: Analytical Annotation of Hallucinations in Large Language Models'
arxiv_id: '2405.20315'
source_url: https://arxiv.org/abs/2405.20315
tags:
- reference
- hallucination
- question
- annotation
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ANAH, a large-scale Chinese-English dataset
  for fine-grained hallucination annotation in large language models (LLMs). ANAH
  consists of approximately 12k sentence-level annotations for about 4.3k LLM responses
  across over 700 topics.
---

# ANAH: Analytical Annotation of Hallucinations in Large Language Models

## Quick Facts
- **arXiv ID**: 2405.20315
- **Source URL**: https://arxiv.org/abs/2405.20315
- **Reference count**: 40
- **Key outcome**: Introduces ANAH, a large-scale Chinese-English dataset for fine-grained hallucination annotation in LLMs, achieving 81.01% accuracy with a generative annotator that rivals GPT-4 performance while being smaller and cheaper.

## Executive Summary
This paper introduces ANAH, a large-scale Chinese-English dataset for fine-grained hallucination annotation in large language models (LLMs). The dataset consists of approximately 12k sentence-level annotations for about 4.3k LLM responses across over 700 topics, enabling quantitative analysis of hallucination accumulation patterns. The authors demonstrate that a generative hallucination annotator trained on ANAH achieves 81.01% accuracy, surpassing open-source LLMs and GPT-3.5 while approaching GPT-4 performance. The dataset construction uses a human-in-the-loop pipeline and reveals that topic breadth is more important than question depth for generalization.

## Method Summary
The ANAH dataset is constructed using a human-in-the-loop pipeline that involves topic selection, question generation, answer generation from LLMs, and fine-grained sentence-level annotation. Each annotation includes retrieving reference fragments, judging hallucination type (No/Contradictory/Unverifiable/No Fact), and correcting hallucinated content. The dataset is used to train both generative and discriminative annotators using InternLM models, with data augmentation techniques and prompt engineering to improve performance. The evaluation employs multiple metrics including F1 score, accuracy, RougeL, and BERTScore to assess hallucination detection quality.

## Key Results
- ANAH-20B generative annotator achieves 81.01% accuracy, surpassing open-source models and rivaling GPT-4 (86.97%)
- Fine-grained annotations enable quantitative confirmation of progressive hallucination accumulation in LLM responses
- Annotators show better generalization on unseen questions than unseen topics, indicating importance of scaling data breadth
- Generative models provide corrections while being slower; discriminative models are faster but only classify types

## Why This Works (Mechanism)

### Mechanism 1
Fine-grained sentence-level hallucination annotation enables quantitative detection of hallucination accumulation in LLM responses. By annotating each sentence with reference fragments, hallucination type, and correction, the dataset allows tracking of hallucination probability progression across the response sequence. Core assumption: Sentence-level annotations provide sufficient granularity to detect incremental hallucination buildup. Evidence: Hallucination probability is significantly higher when previous sentences contain hallucinations compared to when they don't. Break condition: If sentence-level annotations fail to capture true context or dependencies between sentences, accumulation effects may not be accurately detected.

### Mechanism 2
Generative hallucination annotators trained on ANAH outperform open-source LLMs and approach GPT-4 performance while being smaller and cheaper. The dataset provides comprehensive training signals (reference retrieval, type classification, correction generation) that enable generative models to learn fine-grained hallucination detection and correction patterns. Core assumption: The diversity and scale of ANAH provides sufficient training coverage to learn robust hallucination detection patterns. Evidence: ANAH-20B achieves F1 of 80.49% and accuracy of 81.01%, surpassing open-source models and rivaling GPT-4. Break condition: If the dataset lacks sufficient diversity in hallucination patterns or training doesn't capture nuanced human decision-making, performance may plateau below GPT-4 levels.

### Mechanism 3
Prioritizing breadth of topics over number of questions under existing topics leads to better generalization for hallucination annotators. The annotators learn to transfer knowledge across different topics when exposed to diverse subject matter, while repeated questions on the same topic may lead to overfitting to specific question patterns. Core assumption: Topic diversity provides more robust learning signals than question quantity within topics for detecting hallucinations across varied domains. Evidence: Annotators exhibit better generalization on unseen questions than unseen topics, suggesting importance of scaling data breadth. Break condition: If topic diversity comes at cost of insufficient question coverage within each topic, annotators may fail to learn full range of hallucination patterns for specific subject areas.

## Foundational Learning

- Concept: Fine-grained hallucination types (No/Contradictory/Unverifiable/No Fact)
  - Why needed here: Different hallucination types require different handling strategies and provide more nuanced evaluation than binary classification
  - Quick check question: Can you distinguish between contradictory and unverifiable hallucinations based on reference evidence?

- Concept: Reference retrieval and matching strategies
  - Why needed here: Accurate reference fragments are essential for determining whether hallucinations exist and for providing meaningful corrections
  - Quick check question: What's the difference between hard matching (exact) and soft matching (semantic similarity) in reference retrieval?

- Concept: Data augmentation through multi-task training
  - Why needed here: Incorporating related tasks (question generation, selection, dialogue generation) helps models learn broader instruction-following capabilities that transfer to hallucination annotation
  - Quick check question: How does training on multiple tasks simultaneously improve performance on the primary hallucination annotation task?

## Architecture Onboarding

- Component map: Topic selection → Question generation → Answer generation → Fine-grained annotation → Retriever (BM25 + embedding models) → Annotator (generative or discriminative) → Evaluator (F1, accuracy, ROUGE, BERTScore)
- Critical path: 1. Retrieve relevant reference fragments for each sentence 2. Classify hallucination type (No/Contradictory/Unverifiable/No Fact) 3. Generate correction if hallucination exists 4. Evaluate using multiple metrics
- Design tradeoffs: Generative vs discriminative (corrections vs speed), reference availability (with vs without external documents), language coverage (bilingual support vs quality maintenance)
- Failure signatures: Low ROUGE scores (incorrect reference identification or poor correction generation), high false positive rate (too sensitive to supported content), poor generalization (significant performance drop on unseen topics vs questions)
- First 3 experiments: 1. Compare generative vs discriminative annotator performance on held-out test set with and without reference documents 2. Evaluate generalization by testing on unseen topics vs unseen questions to validate breadth vs depth hypothesis 3. Test robustness by applying prompt disturbance to evaluate model performance with instruction variations

## Open Questions the Paper Calls Out
- How does the performance of hallucination annotators vary when applied to tasks beyond generative question answering, such as summarization or dialogue generation?
- How do hallucination annotators perform when applied to responses generated by models that have been fine-tuned or adapted on specific domains or tasks?
- How does the performance of hallucination annotators vary when applied to responses generated by models of different sizes or architectures?

## Limitations
- Dataset construction relies heavily on human-in-the-loop verification, introducing potential bias and scalability limitations
- Evaluation focuses primarily on Chinese-English contexts, potentially limiting generalizability to other languages
- Comparison with GPT-4 uses GPT-3.5 as intermediate benchmark, with direct comparisons potentially influenced by API access differences and cost constraints

## Confidence
- High confidence: Dataset construction methodology and basic annotation framework are well-documented and reproducible
- Medium confidence: Performance claims relative to open-source models and GPT-3.5, given clear experimental setup
- Medium confidence: Generalization findings regarding topic vs question coverage, though requiring more extensive validation

## Next Checks
1. **Cross-linguistic validation**: Test trained annotator on hallucination detection tasks in languages beyond Chinese and English to assess true multilingual capability
2. **Real-world deployment testing**: Evaluate annotator on corpus of actual LLM outputs from production systems rather than controlled QA pairs to measure practical utility
3. **Bias and fairness audit**: Analyze annotation patterns for potential cultural or topical biases that could affect model's performance on diverse subject matter