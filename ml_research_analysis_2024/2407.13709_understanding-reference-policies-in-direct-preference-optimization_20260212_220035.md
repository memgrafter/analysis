---
ver: rpa2
title: Understanding Reference Policies in Direct Preference Optimization
arxiv_id: '2407.13709'
source_url: https://arxiv.org/abs/2407.13709
tags:
- reference
- policy
- reward
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the role of reference policies in Direct
  Preference Optimization (DPO), a widely used method for aligning large language
  models with human preferences. The study addresses three key questions: the optimal
  strength of the KL divergence constraint in DPO, the necessity of the reference
  policy as a regularization, and whether stronger reference policies improve DPO
  performance.'
---

# Understanding Reference Policies in Direct Preference Optimization

## Quick Facts
- arXiv ID: 2407.13709
- Source URL: https://arxiv.org/abs/2407.13709
- Reference count: 40
- Primary result: DPO is sensitive to KL divergence constraint strength, with optimal performance requiring careful tuning and compatible reference policies

## Executive Summary
This paper investigates the critical role of reference policies in Direct Preference Optimization (DPO), a method for aligning large language models with human preferences. Through extensive experiments on instruction-following tasks, the authors find that DPO performance is highly sensitive to the KL divergence constraint strength, with smaller values generally improving performance but risking model degradation. They demonstrate that reference policies serve as essential regularization, and that stronger, compatible reference policies can enhance DPO effectiveness. The study reveals the confounding effects of reference policies in DPO and calls for further theoretical analysis to guide best practices in model alignment.

## Method Summary
The authors conduct systematic experiments on instruction-following tasks using the UltraFeedback dataset with 64K examples. They fine-tune 7B parameter models (mistral-7b, tulu-2-7b, qwen2-1.5b) using DPO with varying KL constraint strength (β parameter) and different reference policies. Performance is evaluated using the AlpacaEval2 benchmark, with additional analysis of ranking accuracy and token-level log-probability differences. The study compares DPO against alternative reward parameterizations and examines the impact of stronger reference policies on fine-tuning outcomes.

## Key Results
- DPO performance is highly sensitive to KL constraint strength, with optimal β typically around 0.01 for 7B models
- Reference policies are necessary for effective DPO, outperforming reference-policy-free methods
- Stronger reference policies improve DPO performance when they are compatible with the fine-tuned model (similar architecture or pre-training data)
- Model degeneration occurs with too small KL constraints, leading to loss of coherence and generation of nonsensical text

## Why This Works (Mechanism)

### Mechanism 1
DPO's effectiveness depends on KL divergence constraint strength between the fine-tuned model and reference policy. The KL divergence constraint controls how much the fine-tuned model can deviate from the reference policy. Too strong a constraint limits learning from preferences, while too weak a constraint causes instability and degeneration. The reference policy provides necessary regularization that prevents model collapse.

### Mechanism 2
Reference policies act as implicit reward models that shape DPO training dynamics. The reference policy defines the reward function through the ratio pθ(y|x)/pref(y|x). This creates a baseline that the fine-tuned model must exceed to show preference, with the KL constraint controlling how aggressively this can happen. The reference policy's ranking accuracy affects the fine-tuned model's ability to learn correct preferences.

### Mechanism 3
Stronger reference policies can improve DPO performance when they are compatible with the fine-tuned model. A stronger reference policy provides a better baseline for comparison, allowing the fine-tuned model to learn more refined preferences. Compatibility (similar architecture, training data, or model family) ensures the reference policy's preferences are relevant.

## Foundational Learning

- Concept: KL divergence and its role in regularization
  - Why needed here: Understanding how KL divergence constrains the fine-tuned model's deviation from the reference policy is central to DPO's mechanics
  - Quick check question: What happens to the fine-tuned model if the KL divergence constraint is removed entirely?

- Concept: Bradley-Terry preference model
  - Why needed here: DPO derives its preference learning objective from the Bradley-Terry model, which compares pairs of outputs
  - Quick check question: How does the Bradley-Terry model convert reward differences into preference probabilities?

- Concept: Maximum Entropy reinforcement learning
  - Why needed here: The paper connects DPO alternatives to MaxEnt RL, showing how entropy regularization affects learning dynamics
  - Quick check question: What role does entropy play in preventing model degeneration during preference optimization?

## Architecture Onboarding

- Component map: User instruction -> Reference policy (SFT model) -> Fine-tuned model -> KL divergence constraint -> Preference objective -> Updated model parameters
- Critical path:
  1. Load reference policy (SFT model)
  2. Initialize fine-tuned model from reference policy
  3. For each batch, compute log probability ratios
  4. Apply binary cross-entropy loss with KL regularization
  5. Update model parameters
  6. Monitor ranking accuracy and degeneration

- Design tradeoffs:
  - KL constraint strength vs. learning speed: Stronger constraints slow learning but provide stability
  - Reference policy strength vs. compatibility: Stronger references help when compatible but hurt when incompatible
  - Training duration vs. overfitting: DPO tends to overfit but this doesn't always hurt performance

- Failure signatures:
  - Model degeneration: Loss of coherence, generation of nonsensical text
  - Sensitivity to KL constraint: Small changes in β causing large performance swings
  - Incompatible reference policies: Performance degradation when using mismatched reference models

- First 3 experiments:
  1. Vary β from 0.001 to 1.0 on a small dataset to observe sensitivity and identify optimal range
  2. Compare DPO with posterior probability and likelihood function objectives on the same dataset
  3. Test different reference policies (SFT, stronger compatible, stronger incompatible) with optimal β

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal KL constraint strength for DPO, and how does it vary across different model scales and datasets? The paper only experiments with 7B parameter models and a single dataset (UltraFeedback), leaving open questions about generalizability to other model sizes and data distributions.

### Open Question 2
How does the choice of reference policy affect the stability and convergence of DPO training? While the paper provides empirical evidence of sensitivity, it does not offer a theoretical framework for understanding the dynamics of DPO training with different reference policies.

### Open Question 3
What are the key factors determining the compatibility between a reference policy and the model being fine-tuned in DPO? The paper only explores compatibility in terms of model scale and architecture, leaving open questions about other factors such as the reference policy's training data, tokenization scheme, or reward function.

## Limitations

- The findings are primarily based on instruction-following tasks and may not generalize to other alignment domains like safety or factual accuracy
- The study relies heavily on AlpacaEval2 as the primary evaluation metric, which uses automated pairwise comparisons that may not fully capture nuanced aspects of model quality
- The empirical approach shows what works but not why it works in a generalizable way, lacking theoretical framework for predicting optimal KL constraint strength

## Confidence

- **High confidence**: DPO is sensitive to KL constraint strength (β) and very small values can lead to model degeneration
- **Medium confidence**: Stronger reference policies improve performance when compatible with the fine-tuned model
- **Medium confidence**: DPO outperforms reference-policy-free methods

## Next Checks

1. Evaluate the optimal KL constraint strength and reference policy choices on non-instruction-following tasks (e.g., safety alignment, reasoning tasks) to determine if findings generalize beyond the UltraFeedback dataset

2. Derive analytical bounds on optimal KL constraint strength as a function of dataset characteristics (preference strength distribution, instruction complexity) to move beyond purely empirical tuning

3. Systematically test different aspects of reference policy compatibility (architecture, tokenizer, training data distribution, parameter count) in isolation to identify which factors most strongly predict successful DPO performance