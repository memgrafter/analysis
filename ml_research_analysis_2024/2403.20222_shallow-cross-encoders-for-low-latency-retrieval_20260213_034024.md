---
ver: rpa2
title: Shallow Cross-Encoders for Low-Latency Retrieval
arxiv_id: '2403.20222'
source_url: https://arxiv.org/abs/2403.20222
tags:
- cross-encoders
- shallow
- training
- latency
- gbce
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Shallow cross-encoders with limited transformer layers can achieve
  higher retrieval effectiveness than full-scale models when constrained by low latency
  requirements. Training shallow models using the generalized Binary Cross-Entropy
  (gBCE) scheme with increased negative sampling improves effectiveness.
---

# Shallow Cross-Encoders for Low-Latency Retrieval

## Quick Facts
- arXiv ID: 2403.20222
- Source URL: https://arxiv.org/abs/2403.20222
- Reference count: 40
- Shallow cross-encoders with limited transformer layers can achieve higher retrieval effectiveness than full-scale models when constrained by low latency requirements

## Executive Summary
This paper demonstrates that shallow cross-encoders with limited transformer layers can achieve higher retrieval effectiveness than full-scale models when constrained by low latency requirements. The key insight is that reducing model depth decreases inference time per document, allowing more documents to be scored within the same latency window. The authors show that training shallow models using the generalized Binary Cross-Entropy (gBCE) scheme with increased negative sampling further improves effectiveness. For example, TinyBERT-gBCE achieved NDCG@10 of 0.652 on TREC DL 2019 with 25ms latency, a +51% gain over MonoBERT-Large.

## Method Summary
The authors propose using shallow cross-encoders (transformers with limited layers) for low-latency retrieval, specifically TinyBERT, MiniBERT, and SmallBERT models. They train these models using a generalized Binary Cross-Entropy (gBCE) scheme with increased negative sampling (up to 128 negatives per positive) and a calibration parameter t=0.75 to address overconfidence issues. The models are evaluated on TREC DL 2019 and 2020 datasets with BM25 candidate selection, measuring NDCG@10 and latency under various constraints. CPU-only inference is also tested for the smallest TinyBERT model to demonstrate practical deployment options.

## Key Results
- TinyBERT-gBCE achieved NDCG@10 of 0.652 on TREC DL 2019 with 25ms latency, a +51% gain over MonoBERT-Large
- Shallow cross-encoders are effective for CPU-only inference, with only 3% NDCG@10 degradation compared to GPU inference at 50ms latency
- The smallest TinyBERT model (2 layers) consistently provided the best effectiveness under very low latency (< 10ms) despite having the lowest memory footprint

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shallow cross-encoders achieve higher effectiveness under low-latency constraints because they can score more documents within the latency window.
- Mechanism: Reducing the number of transformer layers decreases inference time per document. With fixed latency, this allows scoring more candidate documents, increasing the chance of finding relevant ones.
- Core assumption: The benefit of ranking more documents outweighs the loss in accuracy per individual ranking decision.
- Evidence anchors:
  - [abstract]: "we show that weaker shallow transformer models (i.e., transformers with a limited number of layers) actually perform better than full-scale models when constrained to these practical low-latency settings since they can estimate the relevance of more documents in the same time budget."
  - [section 2]: "Equation (1) defines the tradeoff between the latency window ω and the number of scored documents K... Equation (1) also shows that the number of scored documents K can be increased if we decrease model inference time λ."
- Break condition: If latency constraints are relaxed (e.g., >50ms), full-scale models become more effective due to higher per-pair accuracy.

### Mechanism 2
- Claim: The gBCE training scheme mitigates overconfidence in shallow models, improving their effectiveness.
- Mechanism: gBCE loss with increased negative sampling prevents the model from predicting probabilities too close to 1, stabilizing training and improving ranking quality.
- Core assumption: Shallow models are more prone to overconfidence than full-scale models due to lack of architectural regularization.
- Evidence anchors:
  - [abstract]: "We further show that shallow transformers may benefit from the generalised Binary Cross-Entropy (gBCE) training scheme... Our experiments... demonstrate significant improvements in shallow and full-scale models in low-latency scenarios."
  - [section 3]: "A recent publication [29] has shown that negative sampling coupled with the BCE loss leads to the overconfidence problem... Our initial experiments have shown that overconfidence does not cause effectiveness degradation of full-scale Cross-Encoder models... However, our experiments show... that overconfidence is indeed a problem in shallow cross-encoders."
- Break condition: If negative sampling rate is too low or calibration parameter t is set incorrectly, overconfidence may persist.

### Mechanism 3
- Claim: CPU-only inference of shallow models is practical due to their small size and low computational requirements.
- Mechanism: TinyBERT's small checkpoint size (17MB) and reduced transformer depth allow efficient CPU inference with minimal effectiveness loss.
- Core assumption: Memory footprint and inference time scale favorably with model depth and embedding size.
- Evidence anchors:
  - [abstract]: "We also show that shallow Cross-Encoders are effective even when used without a GPU (e.g., with CPU inference, NDCG@10 decreases only by 3% compared to GPU inference with 50ms latency)"
  - [section 4.4]: "The fact that TinyBERT allows us to achieve relatively high performance even using CPU-only inference allows us to use it in cases where GPU inference is not feasible... Considering its low memory footprint (checkpoint is only 17 Mb)"
- Break condition: If CPU performance degrades significantly or memory constraints become tighter, even shallow models may become impractical.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how reducing transformer layers affects model capacity and inference speed.
  - Quick check question: How does the number of transformer layers impact the number of sequential operations during inference?

- Concept: Binary cross-entropy loss and calibration
  - Why needed here: Understanding why BCE can cause overconfidence and how gBCE addresses this.
  - Quick check question: What happens to BCE loss gradients when predicted probabilities are very close to 0 or 1?

- Concept: Negative sampling in training
  - Why needed here: Understanding how increasing negative samples affects model training and effectiveness.
  - Quick check question: How does increasing the number of negative samples per positive instance affect the training data distribution?

## Architecture Onboarding

- Component map: Input text -> Tokenizer -> Transformer encoder (shallow) -> Classification head -> Loss function (gBCE)
- Critical path: Input text -> Tokenization -> Transformer layers -> CLS token -> Feed-forward classification -> Probability calculation
- Design tradeoffs: Model depth vs. inference speed vs. per-pair accuracy vs. overall ranking effectiveness
- Failure signatures: Overconfidence (probabilities too close to 1), slow inference (unable to score enough documents), poor generalization (underfitting)
- First 3 experiments:
  1. Compare inference times of TinyBERT vs. MonoBERT-Large on identical hardware
  2. Evaluate NDCG@10 when varying number of candidates from 1 to 1000 for both models
  3. Test effectiveness of gBCE vs. BCE training on TinyBERT with varying numbers of negative samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of negative samples per positive instance for training shallow Cross-Encoders across different dataset sizes and query distributions?
- Basis in paper: [explicit] The paper shows that increasing negatives improves effectiveness, with best results at 8 negatives for TREC-DL2019 and 32 for TREC-DL2020, but effectiveness fluctuates beyond certain thresholds.
- Why unresolved: The optimal number appears dataset-dependent and the paper only tests a limited range (1-128 negatives) on two specific datasets.
- What evidence would resolve it: Systematic experiments varying negative sample counts across multiple datasets with different characteristics (query volume, document length distributions, relevance judgment sparsity) to identify generalizable patterns.

### Open Question 2
- How do shallow Cross-Encoders trained with gBCE compare to Bi-Encoders on the same hardware when latency is constrained?
- Basis in paper: [inferred] The paper focuses on shallow Cross-Encoders vs full-scale Cross-Encoders, but doesn't compare against Bi-Encoders which are specifically designed for low-latency scenarios.
- Why unresolved: The paper doesn't include Bi-Encoder baselines in its experiments, leaving the relative effectiveness unknown.
- What evidence would resolve it: Head-to-head experiments comparing shallow Cross-Encoders with state-of-the-art Bi-Encoders under identical latency constraints on the same datasets.

### Open Question 3
- What is the minimum viable number of transformer layers for shallow Cross-Encoders before effectiveness degrades below an acceptable threshold?
- Basis in paper: [explicit] The paper tests models with 2, 4, and 4 layers (TinyBERT, MiniBERT, SmallBERT) but doesn't systematically explore the boundary between effective and ineffective shallow models.
- Why unresolved: The experiments only cover a narrow range of model depths and don't establish where diminishing returns begin.
- What evidence would resolve it: Experiments testing models with 1, 2, 3, 4, and 5 layers across multiple datasets to identify the point where effectiveness drops below a specified threshold (e.g., 95% of full-scale model performance).

## Limitations

- Generalization to other datasets and domains remains uncertain, as results are demonstrated only on TREC DL 2019/2020 and MS MARCO datasets.
- Hardware dependency and scalability present practical limitations, with latency measurements dependent on specific GPU configurations (RTX 4090).
- Training procedure sensitivity to hyperparameter choices, particularly the gBCE calibration parameter and negative sampling rates, may impact reproducibility.

## Confidence

- **High confidence**: The core finding that shallow cross-encoders achieve higher effectiveness than full-scale models under strict latency constraints is well-supported by direct experimental comparisons.
- **Medium confidence**: The gBCE training scheme's effectiveness for shallow models shows improvement over BCE, but generalizability to other architectures and datasets requires further validation.
- **Medium confidence**: CPU-only inference viability is promising with only 3% NDCG@10 degradation, but practical deployment would need to consider memory constraints and real-world CPU performance variations.

## Next Checks

1. **Cross-dataset validation**: Evaluate the same shallow cross-encoder models on datasets outside the TREC/DL domain (e.g., web search, scientific literature) to assess generalizability of the latency-effectiveness tradeoff.

2. **Hardware sensitivity analysis**: Repeat latency and effectiveness measurements across different GPU/CPU configurations to quantify hardware dependency and identify minimum viable hardware requirements for practical deployment.

3. **Alternative training schemes comparison**: Systematically compare gBCE against other calibration methods (e.g., label smoothing, focal loss) across multiple model depths to isolate the specific benefits of the gBCE approach.