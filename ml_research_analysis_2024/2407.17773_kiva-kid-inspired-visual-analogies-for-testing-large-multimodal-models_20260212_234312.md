---
ver: rpa2
title: 'KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models'
arxiv_id: '2407.17773'
source_url: https://arxiv.org/abs/2407.17773
tags:
- visual
- reasoning
- verbal
- transformation
- extrapolation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces KiVA, a new benchmark for evaluating visual\
  \ analogical reasoning in large multimodal models (LMMs) using everyday objects.\
  \ Unlike prior benchmarks that use abstract shapes, KiVA tests LMMs on transformations\
  \ like color, size, rotation, reflection, and number changes\u2014tasks that even\
  \ three-year-olds can solve."
---

# KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models

## Quick Facts
- **arXiv ID**: 2407.17773
- **Source URL**: https://arxiv.org/abs/2407.17773
- **Reference count**: 40
- **Primary result**: LMMs struggle with visual analogical reasoning tasks that even 3-year-olds can solve, with performance varying significantly across transformation domains

## Executive Summary
This paper introduces KiVA, a new benchmark for evaluating visual analogical reasoning in large multimodal models (LMMs) using everyday objects. Unlike prior benchmarks that use abstract shapes, KiVA tests LMMs on transformations like color, size, rotation, reflection, and number changes—tasks that even three-year-olds can solve. The benchmark uses a three-stage evaluation: identifying what changed, specifying how it changed, and applying the rule to new scenarios.

Results show that while models like GPT-o1 and GPT-4V can identify changes, they struggle with specifying transformations and extrapolating rules to new objects. Children and adults outperform LMMs across all stages, especially in number and spatial transformations. GPT-o1 performs best but only reaches adult-level reasoning in the color domain. Models are more consistent in simpler tasks (color, size) and show reduced performance in complex tasks (rotation, reflection, number), mirroring human cognitive processing patterns.

## Method Summary
The KiVA benchmark uses real-world objects from household items and children's toys to create 4,300 transformations across five domains: color, size, rotation, reflection, and number. The evaluation follows a three-stage pipeline: verbal classification of what changed, verbal specification of how it changed, and visual extrapolation to apply the rule to new items. Human evaluations were conducted with both adults and children (ages 3-10), while LMMs including GPT-o1, GPT-4V, LLaVA-1.5, and MANTIS were tested using both single and multiple image inputs.

## Key Results
- GPT-o1 and GPT-4V outperform LLaVA-1.5 and MANTIS but only GPT-o1 approaches adult performance, and only in the color domain
- Models perform better on simpler transformations (color, size) than complex spatial tasks (rotation, reflection, number)
- Human performance exceeds LMM performance across all stages, with adults and children showing distinct patterns based on age and task complexity
- LMMs show domain-dependent performance patterns that correlate with human cognitive processing, with simpler features processed more successfully

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** KiVA benchmark isolates fundamental visual analogical reasoning skills by structuring evaluation into three progressive stages.
- **Mechanism:** The three-stage pipeline (verbal classification → verbal specification → visual extrapolation) systematically breaks down analogical reasoning into component processes, allowing precise identification of where models fail.
- **Core assumption:** Visual analogical reasoning can be decomposed into discrete cognitive steps that can be evaluated independently while maintaining task integrity.
- **Evidence anchors:**
  - [abstract] "We structure the evaluation into three stages: identifying what changed (e.g., color, number, etc.), how it changed (e.g., added one object), and applying the rule to new scenarios."
  - [section] "We break down our evaluation to examine the different steps involved in analogical reasoning to determine which steps a model can perform and where it may fail: 1) classifying the domain of a visual transformation, 2) specifying the transformation rule, and 3) extrapolating the inferred rule to a new item."
  - [corpus] Weak evidence - corpus contains related work on analogical reasoning but no direct evidence about three-stage decomposition effectiveness.
- **Break condition:** If models can succeed at visual extrapolation without correctly completing earlier verbal stages, the decomposition assumption would be violated.

### Mechanism 2
- **Claim:** Real-world object transformations in KiVA better align with both human cognitive development and model training distributions than abstract shape benchmarks.
- **Mechanism:** Using everyday objects from household items and children's toys creates stimuli that match natural visual experiences and common training data, while testing fundamental transformations that emerge early in human development.
- **Core assumption:** Models trained on 2D image-text data will perform better on tasks involving familiar real-world objects than abstract geometric shapes.
- **Evidence anchors:**
  - [abstract] "Our dataset utilizes real-world, physically grounded objects curated from established 3D datasets of common household items (Downs et al., 2022) and toys that are familiar to human children (Stojanov et al., 2021)"
  - [section] "We create a dataset of stimuli using everyday objects that better represent real-world visual data and better match the training data of computer vision models (and humans)."
  - [corpus] Weak evidence - corpus mentions related benchmarks but lacks direct comparison of real-world vs abstract shape performance.
- **Break condition:** If models show superior performance on abstract shape benchmarks compared to KiVA's real-world objects, the alignment assumption would be invalid.

### Mechanism 3
- **Claim:** Models show domain-dependent performance that correlates with human cognitive processing patterns, with simpler surface features processed more successfully than complex spatial transformations.
- **Mechanism:** GPT-o1 and other models perform better on color and size transformations (processed earlier in visual cortex and development) than on number, rotation, and reflection tasks (requiring more complex spatial reasoning).
- **Core assumption:** Performance differences across visual domains reflect underlying cognitive processing similarities between models and humans.
- **Evidence anchors:**
  - [abstract] "GPT-o1 and GPT-4V outperform LLaVA-1.5 and MANTIS but also demonstrates weaker performance in orientation and number changes than in size and color changes which are processed more quickly by humans, at an earlier age"
  - [section] "In KiVA-adults, the best-performing model GPT-o1 nears adult performance only in the color domain"
  - [corpus] Weak evidence - corpus contains related work on model limitations but no direct evidence of human-model correlation patterns.
- **Break condition:** If model performance patterns don't correlate with human processing times or developmental emergence, the cognitive similarity assumption would fail.

## Foundational Learning

- **Concept: Visual analogical reasoning**
  - Why needed here: Understanding that analogical reasoning involves mapping relations between source and target structures is fundamental to grasping why KiVA tests these specific capabilities
  - Quick check question: What distinguishes analogical reasoning from simple pattern matching in visual tasks?

- **Concept: Three-stage decomposition of cognitive tasks**
  - Why needed here: The benchmark's effectiveness relies on breaking complex reasoning into verbal classification, verbal specification, and visual extrapolation components
  - Quick check question: How might performance on verbal specification tasks predict success at visual extrapolation?

- **Concept: Developmental psychology benchmarks**
  - Why needed here: KiVA is explicitly inspired by developmental psychology tasks that even young children can solve, providing a baseline for evaluating model capabilities
  - Quick check question: Why might tasks solvable by three-year-olds be appropriate for testing large multimodal models?

## Architecture Onboarding

- **Component map:** KiVA dataset generation -> Three-stage evaluation pipeline -> Model testing framework -> Human evaluation protocols
- **Critical path:** Data generation → Evaluation pipeline implementation → Model testing configuration → Human subject recruitment and testing → Results analysis and correlation assessment
- **Design tradeoffs:** Real-world objects vs abstract shapes (ecological validity vs controlled complexity), verbal questions vs pure visual tasks (insight into reasoning process vs replication of existing benchmarks), single vs multiple image presentation (efficiency vs reduced binding errors)
- **Failure signatures:** Models failing verbal classification indicate inability to detect basic changes; failures at verbal specification suggest inability to quantify transformations; visual extrapolation failures indicate inability to apply learned rules to new contexts
- **First 3 experiments:**
  1. Test a simple model on KiVA with all three stages to establish baseline performance patterns
  2. Evaluate the same model on KiVA-adults to assess performance on more complex generalization tasks
  3. Run the model on KiVA-compositionality to probe compositional reasoning capabilities beyond single transformations

## Open Questions the Paper Calls Out
None

## Limitations

- Dataset Construction Limitations: The KiVA benchmark relies on real-world objects from existing 3D datasets, which may not fully capture the diversity of visual experiences models encounter in training
- Human Performance Comparison Issues: Direct comparison between model and human performance faces methodological challenges including strategy differences and developmental variability
- Generalization Validity Concerns: While KiVA demonstrates LMM limitations on specific visual analogical reasoning tasks, the extent to which these findings generalize to other reasoning domains remains unclear

## Confidence

- **High Confidence**: The finding that GPT-o1 and GPT-4V outperform LLaVA-1.5 and MANTIS on KiVA tasks, with performance differences correlating with model complexity and training approaches
- **Medium Confidence**: The observation that models show domain-dependent performance patterns mirroring human cognitive processing, with simpler transformations yielding higher accuracy than complex spatial tasks
- **Low Confidence**: The claim that KiVA's three-stage decomposition precisely isolates the cognitive components of visual analogical reasoning in a way that directly maps to human developmental processes

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate the same LMMs on KiVA using objects from entirely different visual domains (e.g., natural scenes vs household objects) to assess whether performance patterns hold across varied visual contexts

2. **Incremental Complexity Protocol**: Design a controlled experiment where models first master simple single-transformations, then progress to multi-step analogical reasoning tasks, measuring whether KiVA performance predicts success on more complex reasoning challenges

3. **Adversarial Transformation Generation**: Systematically create edge cases where transformations are ambiguous or overlapping (e.g., size changes that also affect perceived number) to identify whether model failures stem from genuine reasoning limitations or sensitivity to ambiguous visual inputs