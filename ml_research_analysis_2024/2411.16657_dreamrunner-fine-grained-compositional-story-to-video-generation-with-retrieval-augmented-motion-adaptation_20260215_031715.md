---
ver: rpa2
title: 'DreamRunner: Fine-Grained Compositional Story-to-Video Generation with Retrieval-Augmented
  Motion Adaptation'
arxiv_id: '2411.16657'
source_url: https://arxiv.org/abs/2411.16657
tags:
- motion
- video
- generation
- arxiv
- motions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of generating long-form, multi-scene
  videos from text stories, addressing issues of fine-grained motion synthesis, multi-object
  consistency, and smooth transitions. The proposed DREAM RUNNER approach combines
  hierarchical LLM planning (coarse scene-level and fine-grained frame-level), retrieval-augmented
  test-time adaptation for motion and character priors, and a novel spatial-temporal
  region-based 3D attention module (SR3AI) for precise object-motion binding.
---

# DreamRunner: Fine-Grained Compositional Story-to-Video Generation with Retrieval-Augmented Motion Adaptation

## Quick Facts
- arXiv ID: 2411.16657
- Source URL: https://arxiv.org/abs/2411.16657
- Authors: Zun Wang; Jialu Li; Han Lin; Jaehong Yoon; Mohit Bansal
- Reference count: 40
- Key outcome: Achieves 13.1% relative improvement in character consistency and 8.56% gain in text alignment on DreamStorySet benchmark

## Executive Summary
DreamRunner addresses the challenge of generating long-form, multi-scene videos from text stories by combining hierarchical LLM planning, retrieval-augmented motion adaptation, and a novel spatial-temporal region-based 3D attention module (SR3AI). The approach enables fine-grained control over object motions and character consistency across scenes while maintaining smooth transitions. By decomposing complex narratives into coarse scene-level and fine-grained frame-level plans, the system can generate videos with precise semantic control and compositional flexibility.

## Method Summary
DreamRunner uses a hierarchical LLM planning pipeline to generate both high-level scene descriptions and detailed frame-level layouts with entity-specific bounding boxes and motion captions. A retrieval-augmented motion adaptation system finds semantically relevant videos for each motion description, which are then used to fine-tune motion and character LoRA modules. The core innovation is the SR3AI module, which applies region-specific masking to align different spatial-temporal regions with their respective text descriptions while injecting character and motion priors only into corresponding regions. This enables frame-by-frame semantic control and avoids conflicts between multiple objects and motions.

## Key Results
- Achieves 13.1% relative improvement in character consistency (CLIP score) over prior methods on DreamStorySet
- Shows 8.56% gain in text alignment (ViCLIP score) compared to baselines
- Demonstrates 27.2% improvement in transition smoothness (DINO score) across scenes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-level LLM planning enables fine-grained object-motion binding
- Mechanism: The hierarchical structure decomposes complex narratives into high-level scene descriptions and detailed frame-level layouts, allowing precise spatial-temporal control of objects and their motions.
- Core assumption: LLM-generated fine-grained plans can accurately specify object motions and bounding box layouts for each frame.
- Evidence anchors:
  - [abstract]: "we structure the input script using a large language model (LLM) to facilitate both coarse-grained scene planning as well as fine-grained object-level layout and motion planning"
  - [section 3.1]: "detailed, entity-specific plans for each frame within a single scene" with "entity-level details specify each entity's description along with its motion and bounding box layout"
  - [corpus]: Weak - no direct corpus evidence for LLM planning effectiveness in this context
- Break condition: LLM fails to generate coherent fine-grained plans or produces inaccurate bounding box specifications.

### Mechanism 2
- Claim: Retrieval-augmented motion adaptation captures diverse motion priors
- Mechanism: Automatic video retrieval pipeline retrieves semantically relevant videos based on motion descriptions, which are then used to fine-tune motion LoRA modules for specific actions.
- Core assumption: Retrieved videos contain motions semantically aligned with target descriptions and can be used to learn effective motion priors.
- Evidence anchors:
  - [abstract]: "retrieval-augmented test-time adaptation to capture target motion priors for objects in each scene, supporting diverse motion customization based on retrieved videos"
  - [section 3.2]: "retrieve videos relevant to the desired motions from a video database" and "parameter-efficient fine-tune a video diffusion model [1,50] with LoRA"
  - [corpus]: Weak - no direct corpus evidence for retrieval-augmented motion adaptation effectiveness
- Break condition: Retrieved videos are not semantically aligned with target motions or retrieval pipeline fails to find relevant videos.

### Mechanism 3
- Claim: Spatial-temporal region-based 3D attention enables precise object-motion binding
- Mechanism: SR3AI module applies region-specific masks to align different spatial-temporal regions with their respective text descriptions, while injecting character and motion priors only into corresponding regions.
- Core assumption: Masking attention to unrelated regional latents can achieve fine-grained object-and-motion-level control without interference.
- Evidence anchors:
  - [abstract]: "propose a novel spatial-temporal region-based 3D attention and prior injection module SR3AI for fine-grained object-motion binding and frame-by-frame semantic control"
  - [section 3.3]: "we extend this module to enable region-specific conditioning via masking, aligning different regions with their respective text descriptions"
  - [corpus]: Weak - no direct corpus evidence for region-based 3D attention effectiveness
- Break condition: Masking strategy fails to maintain object-motion binding or causes conflicts between multiple objects and motions.

## Foundational Learning

- Concept: Test-time adaptation with LoRA
  - Why needed here: Enables efficient learning of motion and character priors without full model fine-tuning
  - Quick check question: How does LoRA maintain the base model's capabilities while adding new motion priors?

- Concept: Retrieval-augmented generation
  - Why needed here: Provides contextually relevant video examples for learning motion priors specific to each scene's requirements
  - Quick check question: What metrics are used to evaluate the semantic alignment between retrieved videos and target motions?

- Concept: Spatial-temporal attention in video diffusion
  - Why needed here: Enables frame-by-frame control over object motions while maintaining temporal consistency across the video
  - Quick check question: How does region-based masking in 3D attention differ from standard spatial-temporal attention mechanisms?

## Architecture Onboarding

- Component map:
  LLM planning module (coarse-grained and fine-grained planning) -> Retrieval pipeline (BM25 initial retrieval, attribute filtering, clip segmentation, CLIP/ViCLIP scoring) -> Motion prior learning module (LoRA fine-tuning on retrieved videos) -> Character prior learning module (LoRA fine-tuning on reference images) -> SR3AI module (region-based 3D attention and prior injection) -> Base video diffusion model (CogVideoX-2B)

- Critical path:
  1. LLM generates hierarchical video plan
  2. Retrieval pipeline finds relevant videos for each motion
  3. Motion and character priors are learned via LoRA fine-tuning
  4. SR3AI generates videos with region-specific control and prior injection

- Design tradeoffs:
  - Computational cost vs. fine-grained control (SR3AI adds complexity)
  - Retrieval quality vs. coverage (filtering ensures quality but may miss some relevant videos)
  - Motion prior learning vs. generalization (test-time adaptation is specific but may not generalize well)

- Failure signatures:
  - Poor character consistency: SR3AI masking or character LoRA injection failing
  - Incorrect motion generation: Retrieval pipeline not finding relevant videos or motion LoRA learning failing
  - Inconsistent scene transitions: Fine-grained planning not properly specifying frame layouts

- First 3 experiments:
  1. Test LLM planning module with simple single-motion scenes to verify fine-grained plan generation
  2. Evaluate retrieval pipeline on a small set of motions to verify semantic alignment scoring
  3. Validate SR3AI module with synthetic data to ensure region-based masking works as intended

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the spatial-temporal region-based 3D attention module handle occlusions between multiple objects in complex scenes?
- Basis in paper: [inferred] The paper mentions that SR3AI enables regional motion and character control within 3D attention layers, improving object-motion binding and reducing interference between multiple objects and motions. However, it doesn't explicitly address how occlusions are handled.
- Why unresolved: The paper focuses on object-motion binding and reducing interference but doesn't discuss occlusion handling, which is crucial for realistic video generation.
- What evidence would resolve it: Additional experiments showing how the model handles scenes with occlusions, or a detailed explanation of the occlusion handling mechanism in the SR3AI module.

### Open Question 2
- Question: What is the impact of varying the number of retrieved videos on the quality of motion priors and final video generation?
- Basis in paper: [explicit] The paper mentions that for each motion, 4-20 video clips are retrieved and used for learning motion priors. It also shows in ablation studies that retrieving a sufficient number of videos is critical.
- Why unresolved: The paper doesn't explore the optimal number of retrieved videos or the impact of different numbers on motion quality and final video generation.
- What evidence would resolve it: Experiments varying the number of retrieved videos (e.g., 1, 5, 10, 20) and comparing the quality of motion priors and final generated videos.

### Open Question 3
- Question: How does the hierarchical LLM planning approach scale to stories with more than 8 scenes or with significantly more complex narratives?
- Basis in paper: [explicit] The paper mentions that the LLM generates a sequence of 6-8 narrations spanning multiple scenes. It also states that each story comprises 5 to 8 scenes.
- Why unresolved: The paper doesn't explore the limitations or scalability of the hierarchical LLM planning approach for longer or more complex stories.
- What evidence would resolve it: Experiments generating videos for stories with more than 8 scenes or with significantly more complex narratives, and analyzing the quality and coherence of the generated videos.

## Limitations
- Effectiveness heavily depends on LLM's ability to generate accurate fine-grained plans with correct entity bounding boxes and motion descriptions
- Retrieval-augmented motion adaptation assumes retrieved videos contain semantically aligned motions, but quality of this alignment is not directly validated
- SR3AI's region-based masking approach for multi-object control is novel but lacks direct corpus validation of its effectiveness

## Confidence
- **Medium**: Hierarchical LLM planning effectiveness - while the mechanism is sound, there's limited direct evidence of LLM-generated plans producing accurate fine-grained specifications
- **Medium**: Retrieval-augmented motion adaptation - the approach is methodologically sound but lacks validation of semantic alignment between retrieved videos and target motions
- **Medium**: SR3AI module effectiveness - the spatial-temporal region-based masking is novel but lacks direct corpus evidence for its effectiveness in object-motion binding

## Next Checks
1. **Validate LLM planning accuracy**: Test the hierarchical planning module with simple single-motion scenes and verify that generated fine-grained plans contain accurate entity descriptions, bounding boxes, and motion captions
2. **Evaluate retrieval semantic alignment**: Measure CLIP/ViCLIP scores between retrieved videos and target motion descriptions across a diverse set of motion queries to quantify retrieval quality
3. **Test SR3AI region masking**: Create synthetic data with known object layouts and motions to verify that the region-based masking correctly isolates attention to designated spatial-temporal regions without cross-contamination