---
ver: rpa2
title: Personalized Audiobook Recommendations at Spotify Through Graph Neural Networks
arxiv_id: '2403.05185'
source_url: https://arxiv.org/abs/2403.05185
tags:
- audiobook
- user
- audiobooks
- users
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces 2T-HGNN, a scalable recommendation system
  for Spotify''s new audiobook product that combines Heterogeneous Graph Neural Networks
  (HGNNs) with a Two Tower (2T) model. The system addresses three main challenges:
  data sparsity from limited user interactions, high stakes for recommendation accuracy
  given the paywalled nature of audiobooks, and the need for scalability across millions
  of users.'
---

# Personalized Audiobook Recommendations at Spotify Through Graph Neural Networks

## Quick Facts
- arXiv ID: 2403.05185
- Source URL: https://arxiv.org/abs/2403.05185
- Reference count: 40
- Primary result: +46% increase in new audiobook start rate and +23% boost in streaming rates compared to baselines

## Executive Summary
This paper introduces 2T-HGNN, a scalable recommendation system for Spotify's new audiobook product that combines Heterogeneous Graph Neural Networks (HGNNs) with a Two Tower (2T) model. The system addresses three main challenges: data sparsity from limited user interactions, high stakes for recommendation accuracy given the paywalled nature of audiobooks, and the need for scalability across millions of users. The key innovation is decoupling users from the HGNN graph to create a co-listening graph between items only, which significantly reduces complexity while maintaining effectiveness. The model also introduces a balanced multi-link neighbor sampler to address data imbalance between edge types. Empirical evaluation shows substantial improvements in recommendation quality, with the model now in production serving millions of users.

## Method Summary
2T-HGNN combines a Heterogeneous Graph Neural Network with a Two Tower model to recommend audiobooks at Spotify scale. The approach constructs a co-listening graph connecting audiobooks and podcasts when the same users stream both, then trains an HGNN to generate item embeddings while a 2T model independently encodes users and items for real-time serving. The system incorporates weak signals (follows, previews, intent to pay) alongside explicit streams and uses a balanced multi-link neighbor sampler to address data imbalance between edge types. The model is trained on 90 days of streaming data with 800M+ streams and LLM embeddings from audiobook/podcast titles and descriptions.

## Key Results
- +46% increase in new audiobook start rate compared to baselines
- +23% boost in audiobook streaming rates
- Significant improvements in offline metrics including HR@10 and MRR

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The co-listening graph enables transfer of user preference signals from podcasts to audiobooks.
- **Mechanism:** By connecting audiobooks and podcasts when the same user streams both, the graph captures indirect similarity patterns. A user's podcast preferences become informative for audiobook recommendations through these shared edges.
- **Core assumption:** User preferences for podcasts are sufficiently correlated with audiobook preferences that podcast consumption patterns can serve as a proxy for audiobook taste.
- **Evidence anchors:**
  - [abstract] Users with similar audiobook tastes also show higher similarity in podcast preferences (Figure 2B)
  - [section] Audiobook pairs connected through shared podcast co-listenings show higher content similarity than random pairs (Figure 2D)
  - [corpus] Weak (no corpus evidence directly supports this mechanism, though it's implicit in the design)
- **Break condition:** If user preferences for podcasts and audiobooks are uncorrelated, the co-listening graph would fail to transfer meaningful signals between content types.

### Mechanism 2
- **Claim:** The balanced multi-link neighbor sampler addresses data imbalance between edge types, improving audiobook embedding quality.
- **Mechanism:** The sampler equalizes the number of audiobook-audiobook and audiobook-podcast edges during training, preventing the model from being dominated by the more abundant podcast-podcast edges. This ensures the HGNN learns meaningful audiobook representations rather than being biased toward podcast patterns.
- **Core assumption:** The imbalance in edge types (many podcast-podcast edges vs. few audiobook-audiobook edges) would otherwise cause the model to learn podcast-dominated representations that are less useful for audiobook recommendations.
- **Evidence anchors:**
  - [abstract] "Our multi-link neighborhood sampler... optimizes the HGNN training for multiple edge types by undersampling the majority edge types"
  - [section] "This sampling strategy ensures a predictable expected runtime for each training epoch" and "improved performance and produces more meaningful embeddings"
  - [corpus] Weak (no corpus evidence directly supports this mechanism, though it's described in the paper)
- **Break condition:** If edge type imbalance were not problematic, the sampler would provide no benefit and might even hurt performance by removing potentially useful edges.

### Mechanism 3
- **Claim:** Weak signals (follows, previews, intent to pay) capture user intent beyond explicit streams, improving cold-start recommendations.
- **Mechanism:** These implicit interactions provide early signals of user interest before purchase/streaming occurs. Incorporating them into the 2T model allows the system to make recommendations for users who haven't streamed audiobooks yet.
- **Core assumption:** Weak signals are predictive of future audiobook streams and contain information about user preferences not captured by explicit streams alone.
- **Evidence anchors:**
  - [abstract] "We use both streams and weak signals, such as follows and previews" and the model improves cold-start recommendations
  - [section] "a higher occurrence of 'follow' signals significantly boosts the odds of initiating a new stream (+118%)" and weak signals are "positively associated with stream initiation"
  - [corpus] Weak (no corpus evidence directly supports this mechanism, though it's described in the paper)
- **Break condition:** If weak signals were not predictive of future behavior, incorporating them would add noise without improving recommendation quality.

## Foundational Learning

- **Graph Neural Networks (GNNs):**
  - Why needed here: GNNs excel at learning representations from graph-structured data, capturing both content features and user preference patterns in the co-listening graph.
  - Quick check question: What is the key difference between message passing in GNNs and traditional neural networks?

- **Heterogeneous Graphs:**
  - Why needed here: The co-listening graph contains multiple node types (audiobooks, podcasts) and edge types, requiring HGNNs to properly handle different relationships and content modalities.
  - Quick check question: How does an HGNN differ from a homogeneous GNN in handling different node/edge types?

- **Two-Tower Models:**
  - Why needed here: The 2T component provides scalability and real-time serving capability by independently encoding users and items, then matching them via nearest neighbor search.
  - Quick check question: What is the main advantage of using a 2T architecture for recommendation systems serving millions of users?

## Architecture Onboarding

- **Component map:** Co-listening graph construction -> HGNN model -> 2T model -> Nearest neighbor index -> Training pipeline

- **Critical path:**
  1. Build co-listening graph from user streaming data
  2. Train HGNN to generate item embeddings
  3. Train 2T model using HGNN embeddings and user features
  4. Build nearest neighbor index from 2T item embeddings
  5. Generate real-time user embeddings and retrieve recommendations

- **Design tradeoffs:**
  - Decoupling users from the graph reduces complexity but loses direct user-user similarity signals
  - Using LLM features enables cold-start recommendations but may be less personalized than interaction-based embeddings
  - The balanced sampler improves audiobook embeddings but may discard some useful edges

- **Failure signatures:**
  - Poor performance on audiobook recommendations but good podcast performance suggests issues with audiobook-specific components
  - High training time or memory usage indicates neighbor sampling problems
  - Coverage issues suggest the model is recommending from too narrow a subset of the catalog

- **First 3 experiments:**
  1. Compare HR@10 with and without the balanced sampler to verify it improves audiobook recommendations
  2. Test model performance with only explicit streams (no weak signals) to measure their impact
  3. Evaluate ablation study removing podcast-podcast edges to confirm their importance in the graph structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do weak signals (follows, previews, intent to pay) differ in predictive value across different user segments (new vs. experienced audiobook listeners)?
- Basis in paper: [explicit] The paper analyzes weak signals' correlation with future streams but doesn't segment analysis by user experience level.
- Why unresolved: The paper treats all users uniformly when examining weak signal correlations, despite acknowledging significant differences between cold-start and warm-start users in recommendation performance.
- What evidence would resolve it: A stratified analysis comparing weak signal effectiveness across user experience levels, potentially revealing which signals are most valuable for different user segments.

### Open Question 2
- Question: Does the 2T-HGNN model's performance advantage persist when applied to other content types beyond audiobooks and podcasts?
- Basis in paper: [inferred] The paper notes the model's modularity and suggests it "can scale across various content types" but only empirically validates it on audiobooks and podcasts.
- Why unresolved: While the architecture appears generalizable, the paper only demonstrates its effectiveness within the specific context of audiobooks and podcasts, leaving open questions about its broader applicability.
- What evidence would resolve it: Application and evaluation of the 2T-HGNN architecture to other content types (e.g., music, videos) with similar sparsity challenges.

### Open Question 3
- Question: What is the optimal cadence for HGNN training versus 2T model training to balance freshness and computational efficiency?
- Basis in paper: [explicit] The paper mentions that "the modularity of 2T-HGNN allows us to train the HGNN at a different cadence from the 2T model training" but leaves this as future work.
- Why unresolved: The paper acknowledges this as a practical consideration but doesn't investigate the trade-offs between training frequency, model freshness, and computational costs.
- What evidence would resolve it: Systematic experiments varying HGNN and 2T training frequencies to identify optimal schedules for different operational scenarios.

## Limitations
- Implementation details for critical components like the balanced multi-link neighbor sampler and exact 2T model architecture are not fully specified
- Evaluation focuses primarily on short-term metrics without examining long-term user engagement or diversity impacts
- The assumption that podcast preferences correlate with audiobook preferences isn't extensively validated across different user segments

## Confidence
- **High confidence**: The core claim that 2T-HGNN improves audiobook recommendations (46% increase in new starts, 23% boost in streams) is well-supported by offline and online metrics
- **Medium confidence**: The mechanism of using podcast-audiobook edges to transfer preference signals is plausible given the empirical correlations, but the strength of this transfer across different user types is unclear
- **Medium confidence**: The balanced sampler's effectiveness in addressing data imbalance is supported by performance improvements, but the specific sampling ratios and their optimality are not detailed

## Next Checks
1. **Edge type importance ablation**: Systematically remove different edge types (audiobook-audiobook, podcast-podcast, audiobook-podcast) to quantify their individual contributions to recommendation quality and identify which connections are most critical for the model's success.

2. **Cross-user segment validation**: Evaluate model performance across different user cohorts (new vs. existing audiobook users, heavy vs. light podcast listeners) to determine if the podcast-to-audiobook preference transfer assumption holds uniformly or if certain segments benefit more than others.

3. **Long-term engagement analysis**: Track user retention and repeat usage patterns for 30-90 days post-recommendation to ensure the improvements in initial stream rates translate to sustained engagement rather than just short-term clicks.