---
ver: rpa2
title: Improving Rare Word Translation With Dictionaries and Attention Masking
arxiv_id: '2408.09075'
source_url: https://arxiv.org/abs/2408.09075
tags:
- translation
- dictionary
- rare
- definitions
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of rare word translation in
  neural machine translation, particularly in low-resource and out-of-domain settings.
  The authors propose a method that appends definitions from a bilingual dictionary
  to source sentences and uses attention masking to link rare words with their definitions.
---

# Improving Rare Word Translation With Dictionaries and Attention Masking

## Quick Facts
- arXiv ID: 2408.09075
- Source URL: https://arxiv.org/abs/2408.09075
- Reference count: 6
- Improves rare word translation performance by up to 1.0 BLEU and 1.6 MacroF1

## Executive Summary
This paper addresses the challenge of rare word translation in neural machine translation, particularly in low-resource and out-of-domain settings. The authors propose a method that appends definitions from a bilingual dictionary to source sentences and uses attention masking to link rare words with their definitions. The approach involves three main steps: headword selection based on word frequency, definition retrieval using lemmatization, and attention masking with learnable strengths. Experiments on German-to-English translation show that this method improves performance by up to 1.0 BLEU and 1.6 MacroF1, outperforming baseline models. The results demonstrate the effectiveness of incorporating dictionary definitions and attention masking in enhancing translation quality, especially for rare words.

## Method Summary
The method retrieves dictionary definitions for low-frequency words, appends them to source sentences after the <EOS> token, and uses two learnable attention masks to control information flow between source words and their definitions. The masks allow source subwords to attend to definitions (M1) and definitions to attend to the words they define (M2). A frequency threshold identifies rare words, and a lemmatizer finds dictionary entries for inflected forms. The approach uses a standard transformer architecture with 6 layers, 8 heads, and d_model=512, trained with negative log-likelihood loss.

## Key Results
- Improves German-to-English translation by up to 1.0 BLEU and 1.6 MacroF1
- Outperforms baseline models in both news domain (newstest2019/2022) and biomedical domain (Medline test sets)
- Shows consistent improvements across different frequency thresholds (5, 10, 15, 25, 50) and maximum definition counts (1, 5, 10, unbounded)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention masking allows the model to learn to associate rare words with their dictionary definitions by controlling information flow.
- Mechanism: The model uses two learnable attention masks. Mask M1 allows source subwords to attend to their definitions. Mask M2 allows definition subwords to attend to the word they define. The strengths of these masks are learned during training.
- Core assumption: The model can learn to use these masks effectively to incorporate dictionary definitions when helpful.
- Evidence anchors:
  - [abstract]: "using attention masking to link together rare words with their definitions"
  - [section]: "We construct two masks... Both masks allow all source subwords to attend to all source subwords, and all definition subwords to attend to all subwords in the same definition"
  - [corpus]: Weak - no direct corpus evidence, but the paper shows improved BLEU scores
- Break condition: If the model cannot learn to use the masks effectively, or if the dictionary definitions are not helpful for translation

### Mechanism 2
- Claim: Appending dictionary definitions to source sentences provides additional context that helps the model translate rare words.
- Mechanism: Definitions are retrieved for rare words and appended to the source sentence after the <EOS> token. The model learns to use this additional information.
- Core assumption: Dictionary definitions contain useful information for translating rare words.
- Evidence anchors:
  - [abstract]: "including definitions for rare words improves performance by up to 1.0 BLEU and 1.6 MacroF1"
  - [section]: "Our approach is to retrieve dictionary definitions for low-frequency words, append the definitions to source sentences containing rare words"
  - [corpus]: Weak - the corpus shows related work on dictionary use but no direct evidence for this specific mechanism
- Break condition: If the appended definitions do not provide useful information, or if they overwhelm the model with irrelevant data

### Mechanism 3
- Claim: Using a frequency threshold to identify rare words ensures that only words that need help are provided with definitions.
- Mechanism: Words are classified as rare if they have a frequency below a threshold and an entry in the dictionary, or if their lemmatized form meets these criteria.
- Core assumption: Frequency is a good indicator of whether a word will be difficult to translate.
- Evidence anchors:
  - [section]: "In order to classify a source word as rare, we compare the number of occurrences in the training data against a frequency threshold"
  - [section]: "we used frequency thresholds of 5, 10, 15, 25, and 50"
  - [corpus]: Weak - the corpus shows related work on rare word translation but no direct evidence for this specific mechanism
- Break condition: If the frequency threshold misclassifies words (e.g., compounds that can be translated via subword tokenization but are not in the training data)

## Foundational Learning

- Concept: Attention mechanisms in transformers
  - Why needed here: The model uses attention masking, which builds on standard attention mechanisms
  - Quick check question: What is the difference between standard attention and attention with masking?

- Concept: Subword tokenization (BPE)
  - Why needed here: The model uses BPE to segment both source sentences and dictionary definitions
  - Quick check question: How does BPE handle rare words differently than word-level tokenization?

- Concept: Lemmatization
  - Why needed here: The model uses a lemmatizer to find dictionary entries for inflected forms of words
  - Quick check question: Why might a lemmatizer fail to find the correct dictionary entry for a German adjective?

## Architecture Onboarding

- Component map:
  - Encoder: Standard transformer encoder with 6 layers, 8 heads, dmodel=512
  - Decoder: Standard transformer decoder with 6 layers, 8 heads, dmodel=512
  - Attention masking: Two learnable masks (M1 and M2) applied to attention weights
  - Dictionary retrieval: External component that retrieves definitions for rare words
  - Lemmatizer: External component that lemmatizes source words

- Critical path:
  1. Tokenize source sentence and append dictionary definitions
  2. Compute attention weights with masking
  3. Generate translation

- Design tradeoffs:
  - Longer source sentences vs. more information
  - Learnable mask strengths vs. fixed masks
  - Frequency threshold vs. other rare word identification methods

- Failure signatures:
  - BLEU/MacroF1 not improving over baseline
  - Attention masks not being used (mask strengths remain near zero)
  - Dictionary definitions not being used in translations

- First 3 experiments:
  1. Verify that attention masks are being applied by checking attention weights
  2. Test with a simple dictionary and known rare words to ensure definitions are being retrieved and appended
  3. Compare BLEU scores with and without attention masking to verify the masking is helping

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method compare to other dictionary-based approaches when applied to truly low-resource languages with minimal parallel training data?
- Basis in paper: [inferred] The paper primarily evaluates on German-English and mentions potential benefits for low-resource settings but doesn't test on genuinely low-resource languages.
- Why unresolved: The experiments use Europarl data with substantial training data (even the "limited" setting uses 190k+ sentences), which is not truly low-resource by many definitions.
- What evidence would resolve it: Experiments on languages with less than 100k parallel sentences, or zero-shot learning from monolingual dictionaries without parallel data.

### Open Question 2
- Question: What is the optimal strategy for handling compound words and phrases in the dictionary-based approach?
- Basis in paper: [explicit] The paper mentions this as a limitation, noting that compound words are treated as single entries and phrases are not considered.
- Why unresolved: The paper acknowledges this as an open issue but doesn't provide solutions or experimental results for handling compounds differently.
- What evidence would resolve it: Comparative experiments showing the impact of different compound word handling strategies (segmentation vs. whole-word lookup) on translation quality.

### Open Question 3
- Question: How does the masking strength (learnable weights) affect the model's ability to generalize to unseen rare words and domain shifts?
- Basis in paper: [explicit] The paper uses learnable mask strengths but doesn't analyze their behavior or impact on generalization.
- Why unresolved: While the paper mentions that the model can "adjust the strengths of the attention masks accordingly," there's no analysis of how these strengths behave or affect performance on unseen data.
- What evidence would resolve it: Analysis of mask strength distributions across different rare word categories and their correlation with translation accuracy on test sets.

## Limitations

- The approach relies heavily on dictionary coverage, which may not include all rare words in test data
- The method increases source sentence length by appending definitions, potentially introducing noise
- Compound words and phrases are not optimally handled, limiting effectiveness for certain word types

## Confidence

**High Confidence**
- The overall approach of appending dictionary definitions to source sentences improves rare word translation performance
- Attention masking with learnable strengths can effectively link rare words with their definitions
- The frequency threshold method successfully identifies rare words that benefit from dictionary assistance

**Medium Confidence**
- The specific choice of frequency thresholds (5, 10, 15, 25, 50) represents optimal values for this task
- The dictionary-based approach generalizes well to truly low-resource settings beyond the controlled experiments
- The attention masking mechanism learns optimal strengths for all rare word scenarios

**Low Confidence**
- The method's effectiveness would scale proportionally with dictionary size
- The approach would work equally well for all language pairs without modification
- The computational overhead is negligible compared to the translation quality gains

## Next Checks

1. **Dictionary Coverage Validation**: Systematically analyze test set rare words that lack dictionary entries to quantify the impact of dictionary coverage gaps on overall performance. This would involve creating a coverage matrix showing which rare words in test data have dictionary definitions available.

2. **Attention Mask Behavior Analysis**: Track the learned attention mask strengths throughout training for different frequency thresholds and word types. This would reveal whether the model consistently learns to use the masks effectively or if there are patterns where masking fails.

3. **Cross-Domain Generalization Test**: Evaluate the approach on additional out-of-domain test sets beyond biomedical data (e.g., legal, technical, conversational domains) to assess how well the dictionary-based method generalizes across different vocabulary distributions and writing styles.