---
ver: rpa2
title: Evaluation Ethics of LLMs in Legal Domain
arxiv_id: '2403.11152'
source_url: https://arxiv.org/abs/2403.11152
tags:
- legal
- language
- large
- llms
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of evaluating large language models
  (LLMs) for use in legal domains, highlighting the need for specialized assessments
  to ensure ethical and effective integration. The authors propose a novel evaluation
  methodology using authentic legal cases to assess fundamental language abilities,
  specialized legal knowledge, and legal robustness of LLMs.
---

# Evaluation Ethics of LLMs in Legal Domain

## Quick Facts
- arXiv ID: 2403.11152
- Source URL: https://arxiv.org/abs/2403.11152
- Authors: Ruizhe Zhang; Haitao Li; Yueyue Wu; Qingyao Ai; Yiqun Liu; Min Zhang; Shaoping Ma
- Reference count: 4
- Primary result: GPT4 and Qwen-Chat models demonstrate strong legal knowledge and instruction following but exhibit significant gender bias in sentencing

## Executive Summary
This paper addresses the critical need for specialized evaluation of large language models (LLMs) in legal domains, proposing a novel methodology using authentic legal cases to assess fundamental language abilities, specialized legal knowledge, and legal robustness. The authors evaluate mainstream and legal-specific LLMs on instruction following, bias detection, and resistance to inducement. Key findings reveal that while models like GPT4 achieve perfect instruction following rates, significant gender biases persist in sentencing decisions, with male defendants receiving nearly double the sentences of females in identical circumstances. The study concludes that LLMs are feasible for legal applications but require further optimization to address biases and improve robustness.

## Method Summary
The evaluation methodology uses 11 authentic Chinese criminal case descriptions from the LeCaRD dataset to assess LLMs across three dimensions: instruction following, bias detection, and legal robustness. Models are presented with case descriptions in varied formats, requiring numerical responses for conviction rates and prison terms. Bias is quantified by comparing outcomes across defendant backgrounds (gender, age, career) while keeping case facts constant. Legal robustness is evaluated through self-consistency testing (repeated queries) and resistance to inducement (statutory provisions presented before case queries). The study analyzes 13 mainstream and legal-specific LLMs, measuring instruction following rate (IFR), conviction rate (CR), average term (AT), and standard deviations across repeated trials.

## Key Results
- GPT4 achieves 100% instruction following rate and demonstrates superior legal knowledge compared to other models
- GPT4 shows significant gender bias in sentencing, with male defendants receiving sentences nearly double those of females in identical cases
- Qwen-Chat (14B/7B) models perform competitively with GPT4 in legal knowledge and instruction following while showing better bias resistance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The evaluation methodology uses authentic legal cases to assess both legal knowledge and legal robustness of LLMs.
- Mechanism: By extracting factual descriptions from real judicial documents and presenting them with varying inquiry styles and defendant backgrounds, the methodology tests whether models can distinguish legal elements from non-legal ones (e.g., gender, age) and maintain consistent responses under different inducements.
- Core assumption: Legal professionals are trained to exclude bias factors such as gender, age, and career when making judgments, and this capability can be quantitatively measured in LLMs.
- Evidence anchors:
  - [abstract] "utilizing authentic legal cases to evaluate the fundamental language abilities, specialized legal knowledge and legal robustness of large language models"
  - [section] "we extracted factual descriptions of cases. We presented the language models with varying inquiry styles, defendant backgrounds, and pre-set legal knowledge"
  - [corpus] Weak - related work focuses on benchmarking but doesn't specifically use authentic legal case data for bias/robustness testing
- Break condition: If the model fails to follow instructions or shows inconsistent behavior across repeated tests, the evaluation framework would not accurately capture legal robustness.

### Mechanism 2
- Claim: The evaluation captures bias through quantitative metrics comparing conviction rates and average prison terms across different demographic groups.
- Mechanism: By modifying defendant backgrounds (gender, age, career) while keeping case facts constant, the methodology quantifies bias through differences in conviction rates (CR) and average terms (AT), with lower differences indicating better exclusion of non-legal factors.
- Core assumption: The selected observational dimensions (gender, age, career) comprehensively cover potential bias factors that could affect legal judgments.
- Evidence anchors:
  - [section] "we examined the following aspects of bias: GB (Gender Bias), AB (Age Bias), CB (Career Bias)" and specific calculations for CR and AT
  - [section] "GPT4 and Qwen-Chat (14B) models exhibit perfect unbiased feedback on conviction outcomes"
  - [corpus] Weak - related work discusses bias evaluation but doesn't provide specific methodology for legal domain bias quantification
- Break condition: If the methodology fails to account for intersectional biases or other demographic factors not covered in the evaluation.

### Mechanism 3
- Claim: Legal robustness is evaluated through both self-consistency testing and resistance to inducement testing.
- Mechanism: Self-consistency is measured by the standard deviation of conviction rates and prison terms across repeated queries, while resistance to inducement is tested by presenting statutory provisions before case queries to see if they influence judgment.
- Core assumption: A robust legal model should maintain consistent answers when responding to the same question multiple times and should not be influenced by irrelevant inducements.
- Evidence anchors:
  - [section] "we evaluate the model by repeatedly testing it on the same case. This is described using the standard deviation of CR and the standard deviation of the prison terms"
  - [section] "We conducted a statistical analysis of the results provided by LLMs after inducement with the three different scenarios"
  - [corpus] Weak - related work mentions robustness evaluation but doesn't detail specific inducement testing methodology
- Break condition: If the inducement statements are too subtle or the model's training data already contains similar statements, the evaluation might not accurately measure resistance to inducement.

## Foundational Learning

- Concept: Legal instruction following
  - Why needed here: Models must correctly interpret and respond to legal queries in the required format to be useful in legal practice
  - Quick check question: Can the model provide numerical answers in the correct format when asked about conviction rates or prison terms?

- Concept: Bias detection and quantification
  - Why needed here: Legal systems require fairness and consistency, so models must be evaluated for demographic biases
  - Quick check question: Does the model show different conviction rates or sentencing patterns when defendant backgrounds are changed while case facts remain constant?

- Concept: Robustness to inducement
  - Why needed here: Legal professionals must not be swayed by irrelevant information, so models must maintain consistent judgments under different contextual cues
  - Quick check question: Does presenting statutory provisions before case queries significantly alter the model's conviction rates or sentencing decisions?

## Architecture Onboarding

- Component map: Data extraction (from legal documents) -> Query generation (varying styles and backgrounds) -> LLM evaluation -> Statistical analysis -> Bias/robustness quantification
- Critical path: Data extraction → Query generation → LLM evaluation → Statistical analysis → Bias/robustness quantification
- Design tradeoffs: Using authentic legal cases provides realism but limits the number of test cases. The methodology balances comprehensiveness with practical feasibility.
- Failure signatures: High instruction following failure rates, large standard deviations in repeated queries, significant differences in conviction/sentencing across demographic groups, or substantial changes in responses after inducement statements.
- First 3 experiments:
  1. Test instruction following capability with simple numerical queries
  2. Evaluate gender bias by comparing male vs female defendant outcomes
  3. Assess self-consistency by querying the same case multiple times and measuring response variance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific modifications to training data or fine-tuning procedures could reduce gender bias in sentencing for large language models?
- Basis in paper: [explicit] The paper explicitly identifies that GPT4 demonstrates significant gender bias in sentencing, with sentences for males nearly double those for females in identical circumstances.
- Why unresolved: The paper identifies the bias but does not explore technical solutions for mitigating it.
- What evidence would resolve it: Empirical results showing reduced gender bias in sentencing after implementing specific training data modifications or fine-tuning procedures.

### Open Question 2
- Question: How does the performance of large language models in legal ethics evaluation vary across different legal systems and jurisdictions?
- Basis in paper: [inferred] The paper acknowledges that the evaluation was conducted using Chinese legal cases and datasets, but notes this as a limitation without exploring cross-jurisdictional performance.
- Why unresolved: The study is limited to Chinese legal data and does not test the models' performance in other legal systems.
- What evidence would resolve it: Comparative performance data of the same models evaluated using legal cases and ethical standards from multiple jurisdictions.

### Open Question 3
- Question: What is the optimal balance between legal instruction following accuracy and the model's ability to maintain consistency in responses across repeated queries?
- Basis in paper: [explicit] The paper identifies both instruction following ability and self-consistency as important metrics, but notes that repeated questioning may lead to inaccuracies as the model perceives answers as being challenged.
- Why unresolved: The paper presents these as separate metrics without exploring the potential trade-off between them.
- What evidence would resolve it: Experimental results demonstrating the relationship between instruction following accuracy and response consistency across varying numbers of repeated queries.

## Limitations
- The study's reliance on 11 Chinese criminal cases from a single dataset limits generalizability across legal domains and jurisdictions.
- The binary gender classification and broad age/career categories might oversimplify complex bias patterns and fail to detect intersectional biases.
- The evaluation includes only a limited set of mainstream and legal-specific LLMs, potentially missing important performance variations across different model architectures.

## Confidence
- High Confidence: The instruction following evaluation methodology and basic bias quantification approach are well-established and reproducible. The findings about GPT4's strong performance in instruction following (100% IFR) and legal knowledge are consistent across multiple evaluation dimensions.
- Medium Confidence: The legal robustness evaluation through self-consistency and inducement resistance provides meaningful insights, though the specific methodology for inducement testing could be refined. The observed bias patterns are concerning but require further investigation with larger datasets.
- Low Confidence: The generalizability of findings across different legal systems, languages, and cultural contexts remains uncertain due to the limited scope of evaluated cases and models.

## Next Checks
1. **Cross-Jurisdictional Validation**: Replicate the evaluation methodology using legal cases from multiple jurisdictions (civil law, common law, etc.) to assess whether findings generalize beyond the Chinese criminal law context.
2. **Intersectional Bias Analysis**: Extend the bias evaluation to include intersectional dimensions (e.g., gender × age, gender × career) and additional demographic factors to capture more complex bias patterns not detected in the current analysis.
3. **Extended Model Comparison**: Include additional LLMs with different architectures and training approaches (including open-source models with legal fine-tuning) to better understand the relationship between model design choices and legal domain performance.