---
ver: rpa2
title: 'Beyond Relevance: Evaluate and Improve Retrievers on Perspective Awareness'
arxiv_id: '2405.02714'
source_url: https://arxiv.org/abs/2405.02714
tags:
- perspectives
- retrieval
- queries
- retrievers
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies perspective awareness in information retrieval
  systems, which are typically optimized for semantic relevance but often overlook
  the nuanced intents behind user queries. To address this, the authors create a new
  benchmark called PIR (Perspective-aware Information Retrieval), built by reformatting
  six existing datasets across domains like news, argument mining, and QA, each with
  explicit perspective labels (e.g., support/oppose, left/right-wing).
---

# Beyond Relevance: Evaluate and Improve Retrievers on Perspective Awareness

## Quick Facts
- arXiv ID: 2405.02714
- Source URL: https://arxiv.org/abs/2405.02714
- Reference count: 40
- Key outcome: Existing retrievers show bias toward certain perspectives; proposed PAP method improves perspective-aware retrieval and downstream task performance

## Executive Summary
This paper addresses a critical gap in information retrieval: while retrievers are optimized for semantic relevance, they often overlook the nuanced intents and perspectives behind user queries. The authors introduce a new benchmark (PIR) and metric (p-Recall) to evaluate perspective awareness, revealing that state-of-the-art retrievers are biased toward certain perspectives. To address this, they propose a simple zero-shot method (PAP) that consistently improves retrieval across tasks and models. The work also demonstrates that perspective-aware retrieval enhances downstream performance, such as more accurate answers and viewpoint-aligned essays, while highlighting social biases in existing systems.

## Method Summary
The authors create the PIR (Perspective-aware Information Retrieval) benchmark by reformatting six existing datasets across news, argument mining, and QA domains, each with explicit perspective labels (e.g., support/oppose, left/right-wing). They design a new metric, p-Recall, to measure consistent performance across different perspectives. To improve perspective awareness, they propose PAP (Perspective-aware Projection), a zero-shot method that projects query and corpus embeddings onto the perspective vector space. PAP is simple, requires no fine-tuning, and can be applied to any backbone retriever. The method is evaluated on multiple tasks, showing consistent improvements in retrieval and downstream performance.

## Key Results
- Existing retrievers (BM25, DPR, SimCSE) are biased toward certain perspectives and perform worse when perspectives are included in queries.
- PAP improves retrieval performance across tasks and backbone models, outperforming baseline methods.
- Perspective-aware retrieval enhances downstream task performance: GPT-3.5-Turbo answers AmbigQA questions with 4.2% higher accuracy and generates essays more aligned with target viewpoints (29.9% more correlation).
- Analysis reveals social biases in retrievers, such as favoring news from certain countries.

## Why This Works (Mechanism)
None

## Foundational Learning
- **Perspective-aware retrieval**: Why needed? To capture nuanced user intents beyond semantic relevance. Quick check: Can the system retrieve documents aligned with a specified perspective (e.g., pro vs. con)?
- **p-Recall metric**: Why needed? To measure consistent performance across different perspectives, unlike traditional recall. Quick check: Does the metric penalize retrievers for favoring one perspective over another?
- **Zero-shot projection methods**: Why needed? To enable perspective awareness without requiring task-specific fine-tuning. Quick check: Can the method be applied to any backbone retriever without additional training?

## Architecture Onboarding
- **Component map**: User query -> Perspective-aware projection (PAP) -> Retriever (e.g., BM25, DPR) -> Retrieved documents -> Downstream task (e.g., QA, essay generation)
- **Critical path**: Query embedding projection → Retrieval → Downstream evaluation
- **Design tradeoffs**: PAP is simple and zero-shot but depends on available perspective embeddings; balancing retrieval relevance and perspective alignment.
- **Failure signatures**: Retriever bias toward certain perspectives; degradation in performance when perspectives are included in queries.
- **First experiments**: 1) Test PAP on a held-out perspective-aware dataset. 2) Evaluate PAP’s impact on downstream tasks (e.g., AmbigQA, essay generation). 3) Analyze social biases in retrieval outputs.

## Open Questions the Paper Calls Out
None

## Limitations
- PIR benchmark relies on reformatting existing datasets, potentially limiting diversity and naturalness of queries.
- PAP depends on the availability of perspective embeddings or annotations, which may not always be available.
- Downstream evaluation using GPT-3.5-Turbo may not generalize to other generative models or tasks.
- Social bias analysis is limited to surface-level patterns (e.g., country-based news preferences).

## Confidence
- High: Core findings about retrievers' perspective bias and PAP’s effectiveness are well-supported by experiments.
- Medium: Improvements in downstream performance are promising but may depend on specific models and prompts.
- Low: Generalization of PAP to real-world, open-domain retrieval with diverse perspectives is not fully explored.

## Next Checks
1. Test PAP’s effectiveness on a newly collected, large-scale dataset of naturally occurring perspective-aware queries across multiple languages and domains.
2. Evaluate PAP’s performance when perspective embeddings are noisy or incomplete, simulating real-world conditions.
3. Assess the impact of perspective-aware retrieval on a wider range of downstream tasks, including those requiring multi-hop reasoning or handling conflicting perspectives.