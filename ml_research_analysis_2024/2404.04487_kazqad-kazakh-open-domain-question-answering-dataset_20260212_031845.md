---
ver: rpa2
title: 'KazQAD: Kazakh Open-Domain Question Answering Dataset'
arxiv_id: '2404.04487'
source_url: https://arxiv.org/abs/2404.04487
tags:
- kazakh
- questions
- dataset
- kazqad
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KazQAD is a new open-domain question answering dataset for Kazakh
  that includes 6,000 unique questions with short answers and 12,000 passage-level
  relevance judgements. The dataset was created using a combination of machine translation,
  Wikipedia search, and manual annotation, with questions sourced from the Natural
  Questions dataset (for training) and the Kazakh Unified National Testing exam (for
  development and testing).
---

# KazQAD: Kazakh Open-Domain Question Answering Dataset

## Quick Facts
- arXiv ID: 2404.04487
- Source URL: https://arxiv.org/abs/2404.04487
- Reference count: 0
- Primary result: 6,000 unique questions with 12,000 passage relevance judgements for Kazakh ODQA

## Executive Summary
KazQAD is a new open-domain question answering dataset for Kazakh that includes 6,000 unique questions with short answers and 12,000 passage-level relevance judgements. The dataset was created using a combination of machine translation, Wikipedia search, and manual annotation, with questions sourced from the Natural Questions dataset (for training) and the Kazakh Unified National Testing exam (for development and testing). Baseline models for retrieval, reading comprehension, and full ODQA settings achieved reasonable but not state-of-the-art performance, indicating room for improvement. The dataset is freely available under a Creative Commons license and can be used for various NLP and IR tasks in Kazakh.

## Method Summary
The KazQAD dataset was constructed using a hybrid approach combining machine translation of Natural Questions with manual annotation and original UNT exam questions. For training data, questions were machine-translated from English and paired with relevant Kazakh Wikipedia passages identified via langlinks. Manual annotators verified passage relevance and extracted short answers. Development and test sets used original UNT questions with answers found through Google search and Wikipedia, then manually verified. The resulting dataset contains 6,000 unique questions across 800,000+ passages from Kazakh Wikipedia.

## Key Results
- KazQAD contains 6,000 unique questions with 12,000 passage-level relevance judgments
- Retrieval baselines achieved NDCG@10 of 0.547 and R@100 of 0.547
- Reading comprehension models achieved F1 scores up to 51.8 on dev set
- Full ODQA systems achieved EM scores around 21.5 on test set

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining machine translation of existing QA datasets with manual annotation yields a higher-quality low-resource dataset than fully automated translation alone.
- Mechanism: Machine translation provides a scalable starting point for question and answer content, but manual annotation ensures passage relevance and answer accuracy in the target language.
- Core assumption: Machine-translated questions retain semantic fidelity enough to serve as annotation prompts for native speakers.
- Evidence anchors:
  - [abstract] "We use a combination of machine translation, Wikipedia search, and in-house manual annotation to ensure annotation efficiency and data quality."
  - [section] "We refrained from adopting a fully automated approach to dataset construction such as relying solely on machine translation to prevent an extensive presence of unrealistic synthetic data such as translationese."
- Break condition: If machine-translated questions are semantically inconsistent or ambiguous, manual annotation quality degrades or becomes infeasible.

### Mechanism 2
- Claim: Using national exam questions (UNT) for development/test data improves cultural and topical relevance compared to translated general QA data.
- Mechanism: UNT questions reflect local educational priorities and real-world user needs, ensuring the dataset covers domain-specific knowledge and cultural context.
- Core assumption: UNT questions are high-quality, unambiguous, and align with the information coverage of the chosen corpus (Kazakh Wikipedia).
- Evidence anchors:
  - [abstract] "The questions come from two sources: translated items from the Natural Questions (NQ) dataset (only for training) and the original Kazakh Unified National Testing (UNT) exam (for development and testing)."
  - [section] "UNT contains more 'localised' questions (at least in the subsections corresponding to history of Kazakhstan and Kazakh literature)."
- Break condition: If UNT questions are too narrow or domain-specific, they may not generalize to broader ODQA use cases.

### Mechanism 3
- Claim: Leveraging cross-lingual alignment (langlinks) between English Wikipedia and Kazakh Wikipedia passages enables efficient passage selection for training.
- Mechanism: For each NQ question with an English Wikipedia page, the corresponding Kazakh Wikipedia page provides candidate passages aligned by topic, reducing annotation effort.
- Core assumption: Wikipedia langlinks reliably map semantically equivalent articles between languages, ensuring topical consistency.
- Evidence anchors:
  - [section] "We extracted up to four passages from the parallel Kazakh page using Wikipedia langlinks."
  - [section] "The total count of question-passage pairs amounted to 34,660, as some pages contained minimal information."
- Break condition: If langlinks are missing or point to semantically different pages, passage relevance drops and annotation quality suffers.

## Foundational Learning

- Concept: Understanding the difference between reading comprehension (RC) and open-domain QA (ODQA).
  - Why needed here: KazQAD supports both RC and ODQA evaluation; models and metrics differ between settings.
  - Quick check question: In RC, the answer is guaranteed to be in one passage; in ODQA, the retriever must first find the relevant passage(s) from a large corpus. True or false?

- Concept: Familiarity with information retrieval evaluation metrics (NDCG, MRR, recall).
  - Why needed here: Baseline retriever performance is reported using NDCG@10, MRR, and R@100; understanding these metrics is essential for interpreting results.
  - Quick check question: Which metric measures the rank position of the first relevant document retrieved?

- Concept: Knowledge of transformer-based models and fine-tuning strategies for low-resource languages.
  - Why needed here: Baseline models use multilingual transformers (XLM-R, mBERT) and a monolingual Kazakh model; fine-tuning strategies differ for translated vs. original data.
  - Quick check question: If a model is fine-tuned on machine-translated NQ data, should you expect the same performance as fine-tuning on manually annotated KazQAD data?

## Architecture Onboarding

- Component map: Corpus -> Annotation Pipeline -> IR Retriever -> RC Reader -> ODQA Fusion -> Evaluation
- Critical path: Data → Annotation → IR Retriever → RC Reader → ODQA Fusion → Evaluation
- Design tradeoffs:
  - Fully manual vs. machine translation + manual annotation: cost vs. data quality
  - Single vs. multiple candidate passages per question: annotation effort vs. coverage
  - Fusion vs. top-1 answer selection: potential accuracy gain vs. added complexity
- Failure signatures:
  - Low retrieval recall: candidate generator or re-ranker not capturing relevant passages
  - Low RC F1: transformer model not generalizing from translated data; insufficient training examples
  - ODQA EM/F1 much lower than RC: retriever failing to surface correct passage
- First 3 experiments:
  1. Run BM25 candidate generation on a small subset of KazQAD dev questions; check recall@10
  2. Fine-tune XLM-R on NQ translated data only; evaluate on KazQAD dev (EM/F1)
  3. Combine best IR and RC models; test fusion vs. top-1 answer selection on dev set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does KazQAD compare in difficulty and quality to other low-resource language QA datasets like XQuAD or MKQA?
- Basis in paper: [explicit] The paper compares KazQAD to other datasets like SQuAD, Belebele, and MKQA, but doesn't provide a detailed comparative analysis of difficulty and quality metrics.
- Why unresolved: The paper focuses on introducing KazQAD and establishing baselines, rather than conducting a comprehensive comparative study with other low-resource datasets.
- What evidence would resolve it: A detailed analysis comparing performance metrics (EM, F1, NDCG, etc.) of models trained on KazQAD versus other low-resource datasets, along with qualitative assessments of dataset quality and diversity.

### Open Question 2
- Question: What is the impact of using machine-translated training data from NQ on the performance of KazQAD models compared to using original Kazakh questions?
- Basis in paper: [explicit] The paper uses machine-translated NQ questions for training and compares performance with the original UNT questions, but doesn't isolate the impact of translation quality on model performance.
- Why unresolved: The paper combines machine-translated and original data without specifically analyzing how translation quality affects model learning and generalization.
- What evidence would resolve it: Experiments comparing model performance using only original Kazakh questions versus machine-translated data, along with analysis of translation quality and its correlation with model performance.

### Open Question 3
- Question: How do current large language models perform on KazQAD compared to specialized QA models, and what are the limitations of using LLMs for low-resource languages?
- Basis in paper: [explicit] The paper tests ChatGPT on KazQAD and finds it performs poorly, but doesn't conduct a comprehensive comparison with specialized QA models or analyze LLM limitations for low-resource languages.
- Why unresolved: The paper only tests one LLM (ChatGPT) and doesn't explore the broader landscape of LLM performance on low-resource language tasks or identify specific limitations.
- What evidence would resolve it: A comprehensive study testing multiple LLMs on KazQAD, comparing their performance with specialized QA models, and analyzing factors like data quality, model architecture, and language-specific challenges that affect LLM performance.

## Limitations
- Dataset size is relatively small (6,000 questions) compared to major QA datasets
- Heavy reliance on machine translation may introduce semantic inconsistencies
- Limited evaluation to Wikipedia domain may not reflect broader ODQA capabilities

## Confidence
- **High confidence**: The dataset construction methodology and availability (Creative Commons license, public GitHub repository)
- **Medium confidence**: Baseline model performance comparisons and retrieval metrics, given the use of established IR evaluation methods
- **Low confidence**: Claims about state-of-the-art performance and generalizability beyond Kazakh Wikipedia domain

## Next Checks
1. Run BM25 candidate generation on KazQAD dev questions and measure recall@10 to verify the reported 78% recall is achievable with the provided corpus and whether it aligns with the 54.7% R@100 baseline.

2. Sample 50 machine-translated NQ questions and their corresponding manual relevance annotations to assess semantic fidelity and identify any systematic translation issues or annotation inconsistencies.

3. Select 20 questions with langlinked Wikipedia pages and verify that the corresponding Kazakh passages are topically relevant and semantically aligned with the original English passages, checking for potential langlink failures or topical mismatches.