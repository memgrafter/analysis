---
ver: rpa2
title: Debiased Multimodal Understanding for Human Language Sequences
arxiv_id: '2403.05025'
source_url: https://arxiv.org/abs/2403.05025
tags:
- causal
- subject
- suci
- multimodal
- subjects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the subject variation problem in multimodal
  language understanding (MLU), where models learn subject-specific spurious correlations
  due to idiosyncratic expression styles across different subjects. The authors introduce
  a causal graph to identify subjects as confounders and propose SuCI (Subject Causal
  Intervention), a plug-and-play module that implements backdoor adjustment to remove
  these confounding effects.
---

# Debiased Multimodal Understanding for Human Language Sequences

## Quick Facts
- **arXiv ID:** 2403.05025
- **Source URL:** https://arxiv.org/abs/2403.05025
- **Reference count:** 10
- **Primary result:** Introduces SuCI module that achieves 0.4% to 3.5% performance gains across multiple MLU benchmarks by addressing subject-specific spurious correlations through causal intervention

## Executive Summary
This paper addresses the subject variation problem in multimodal language understanding (MLU), where models learn subject-specific spurious correlations due to idiosyncratic expression styles across different subjects. The authors introduce a causal graph to identify subjects as confounders and propose SuCI (Subject Causal Intervention), a plug-and-play module that implements backdoor adjustment to remove these confounding effects. SuCI disentangles subject-specific features through adversarial training and applies normalized weighted geometric mean aggregation to achieve unbiased predictions. Experimental results show consistent performance improvements across multiple MLU benchmarks including MOSI (e.g., +1.2% accuracy), MOSEI, and UR FUNNY, with gains ranging from 0.4% to 3.5% across different metrics. The method demonstrates effectiveness in both in-distribution and cross-dataset evaluations, validating its ability to improve model generalizability and mitigate subject-related prediction bias.

## Method Summary
The SuCI (Subject Causal Intervention) framework treats subjects as observed confounders in multimodal language understanding tasks and applies backdoor adjustment through a plug-and-play module. The method consists of a subject generator that creates subject-specific feature representations through dynamic fusion of text, visual, and audio modalities, followed by adversarial training with a subject discriminator and task discriminator to disentangle subject-specific from task-relevant information. A confounder dictionary stores subject prototypes, and normalized weighted geometric mean (NWGM) aggregation approximates causal intervention for unbiased predictions. The approach is model-agnostic and can be integrated with existing MLU architectures to improve generalization across subjects while maintaining performance on target tasks.

## Key Results
- MOSI dataset: +1.2% accuracy improvement over baseline models
- MOSEI dataset: Consistent gains ranging from 0.4% to 3.5% across different metrics
- UR FUNNY dataset: Demonstrated effectiveness in humor detection task with binary accuracy improvements
- Cross-dataset evaluations show robust performance improvements across different MLU architectures including MulT, MISA, Self-MM, MMIM, and DMD

## Why This Works (Mechanism)

### Mechanism 1
The backdoor adjustment approach removes confounding effects from subject-specific expression styles by cutting the causal link between subjects and multimodal inputs. The causal graph identifies subjects (Z) as confounders that affect both multimodal inputs (X) and predictions (Y). By applying do(X) operator through backdoor adjustment, the model estimates P(Y|do(X)) instead of P(Y|X), effectively removing the backdoor paths X ← Z → Y and X ← Z → M → Y. This works under the core assumption that subjects can be treated as observed confounders that influence all three modalities equally and consistently across the dataset.

### Mechanism 2
The adversarial feature disentanglement successfully isolates subject-specific features from task-relevant semantic information. The subject discriminator (Ds) and task discriminator (Dt) create opposing forces on the subject feature representation s. The subject discriminator ensures s contains subject-specific information while the task discriminator pushes s to be equally distributed across all task categories, effectively removing task-relevant information. This relies on the assumption that subject-specific and task-relevant information can be effectively separated in the learned feature space without losing important multimodal semantic information.

### Mechanism 3
The Normalized Weighted Geometric Mean (NWGM) aggregation approximates causal intervention efficiently while maintaining theoretical guarantees. NWGM provides feature-level approximation of P(Y|do(X)) by computing a weighted geometric mean of the causal effects across all subject strata. This reduces computational overhead compared to full stratification while preserving the causal structure. This approach assumes the geometric mean aggregation can approximate the true causal effect without introducing significant bias, and that subject proportions can be accurately estimated from training data.

## Foundational Learning

- **Concept:** Causal inference and backdoor adjustment
  - Why needed here: The paper's core contribution relies on treating subjects as confounders and applying backdoor adjustment to remove their effects. Understanding the do-calculus and backdoor criterion is essential for grasping why this approach works.
  - Quick check question: What is the difference between P(Y|X) and P(Y|do(X)) in terms of causal interpretation?

- **Concept:** Adversarial learning and feature disentanglement
  - Why needed here: The subject de-confounding relies on adversarial training to separate subject-specific features from task-relevant features. Understanding how discriminators can enforce feature separation is crucial.
  - Quick check question: How does the task discriminator ensure that subject features don't contain task-relevant information?

- **Concept:** Multimodal fusion and representation learning
  - Why needed here: The method needs to work with text, visual, and audio modalities. Understanding how different modalities can be fused and how subject effects manifest across modalities is important for implementation.
  - Quick check question: Why is dynamic fusion preferred over simple averaging for capturing subject-specific semantics?

## Architecture Onboarding

- **Component map:** Feature extractors → Dynamic fusion → Subject generator → Confounder dictionary → NWGM aggregation → Vanilla MLU model → Output
- **Critical path:** Input → Feature extractors → Dynamic fusion → Subject generator → Confounder dictionary → NWGM aggregation → Vanilla MLU model → Output. The confounder dictionary is updated at the end of each epoch.
- **Design tradeoffs:** The method trades computational complexity (additional discriminators and confounder dictionary) for improved generalization. The confounder dictionary size grows with the number of subjects, and the adversarial training adds training instability risks.
- **Failure signatures:** If the subject discriminator cannot distinguish subjects effectively, the confounder dictionary will be poor quality. If the task discriminator fails to remove task information, subject de-confounding will be incomplete. If the NWGM approximation is poor, causal effects won't be properly estimated.
- **First 3 experiments:**
  1. Implement the dynamic fusion mechanism and verify that it produces different attention weights for different frames within the same modality.
  2. Test the subject discriminator alone on a subset of data to ensure it can distinguish between subjects before integrating with the full system.
  3. Validate the confounder dictionary construction by checking that subject prototypes are stable across epochs and that different subjects have distinguishable prototypes.

## Open Questions the Paper Calls Out
None

## Limitations

- The framework assumes subjects act as observed confounders affecting all modalities equally, but real-world subject variation may involve complex interactions between modalities that the current causal graph cannot capture.
- The adversarial disentanglement approach relies on the assumption that subject-specific and task-relevant information can be cleanly separated in feature space, which may not hold for subjects with overlapping expression patterns.
- The NWGM approximation introduces computational efficiency but may sacrifice accuracy compared to full stratification methods, and the confounder dictionary grows with the number of subjects, potentially limiting scalability.

## Confidence

- **High Confidence:** The performance improvements on benchmark datasets (MOSI, MOSEI, UR FUNNY) with consistent gains across multiple metrics are well-supported by the experimental results.
- **Medium Confidence:** The causal graph formulation and backdoor adjustment approach are theoretically sound, but their effectiveness depends on the assumption that subjects can be treated as observed confounders with uniform effects across modalities.
- **Low Confidence:** The effectiveness of the adversarial feature disentanglement and NWGM approximation in real-world scenarios remains somewhat uncertain, as these are relatively novel approaches in the MLU domain without extensive validation.

## Next Checks

1. Conduct ablation studies removing each component of SuCI (adversarial training, confounder dictionary, NWGM aggregation) to quantify their individual contributions to performance gains and verify that the improvements are not due to other factors.

2. Test the method on datasets with varying subject distributions (e.g., balanced vs. imbalanced subject counts) to assess robustness and identify potential failure modes when subject effects are not uniformly distributed.

3. Perform cross-modal intervention analysis to verify that the backdoor adjustment effectively removes subject confounding effects across all three modalities (text, visual, audio) and that no modality-specific biases remain after SuCI integration.