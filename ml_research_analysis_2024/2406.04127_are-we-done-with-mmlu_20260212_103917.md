---
ver: rpa2
title: Are We Done with MMLU?
arxiv_id: '2406.04127'
source_url: https://arxiv.org/abs/2406.04127
tags:
- mmlu
- questions
- correct
- question
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors manually analyzed 5,700 questions from the MMLU benchmark
  and found that 6.49% contain errors, with some subjects like Virology having error
  rates as high as 57%. They developed MMLU-Redux, a cleaned subset annotated with
  error types, and showed that these errors can significantly impact model rankings.
---

# Are We Done with MMLU?

## Quick Facts
- arXiv ID: 2406.04127
- Source URL: https://arxiv.org/abs/2406.04127
- Reference count: 40
- Key outcome: Authors manually analyzed 5,700 questions from MMLU and found 6.49% contain errors, with some subjects like Virology having error rates as high as 57%.

## Executive Summary
This paper critically examines the quality of the Massive Multitask Language Understanding (MMLU) benchmark, revealing significant errors in its question-answer pairs. Through manual annotation of 5,700 questions across all 57 subjects, the authors discovered that 6.49% of MMLU contains errors, with some subjects like Virology reaching 57% error rates. They created MMLU-Redux, a cleaned subset annotated with error types, and demonstrated that these errors significantly impact model rankings. The study also shows that automatic error detection remains challenging, with even the best models achieving only 41.9% F2 score.

## Method Summary
The authors developed a comprehensive error annotation protocol and applied it to 5,700 MMLU questions, creating MMLU-Redux. They evaluated leading LLMs on both original MMLU and MMLU-Redux to quantify error impact on model rankings. Automatic error detection was tested using various prompting strategies including zero-shot, few-shot, chain-of-thought, and retrieval-augmented generation. The study also explored fine-tuning approaches using LabelChaos to improve automatic error detection performance.

## Key Results
- 6.49% of MMLU questions contain errors, with error rates varying from 0.28% to 57.14% across subjects
- Automatic error detection remains challenging, with best performance at 41.9% F2 score (Claude 3 Opus)
- Re-evaluation on MMLU-Redux shows significant changes in model rankings compared to original MMLU results

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: MMLU-Redux enables accurate error detection by providing a manually annotated subset of questions with ground truth error types, overcoming the challenge of unreliable labels in the original MMLU.
- **Mechanism**: Manual annotation protocol identifies and categorizes errors across all 57 MMLU subjects, creating a high-quality reference set for evaluating automatic error detection methods.
- **Core assumption**: Human annotators can reliably identify and categorize errors better than automated methods, and this manual labeling effort is feasible at scale.
- **Evidence anchors**:
  - [abstract]: "We introduce a comprehensive framework for identifying dataset errors using a novel error annotation protocol."
  - [section]: "To create MMLU-Redux, we followed the proposed annotation protocol."
  - [corpus]: Weak - corpus shows related work on MMLU benchmarking but no direct evidence about manual annotation effectiveness.

### Mechanism 2
- **Claim**: Automatic error detection in MMLU remains challenging despite annotated error types, with even the best models achieving only 41.9% F2 score.
- **Mechanism**: LLMs struggle to distinguish between genuinely ambiguous questions and those with incorrect ground truth, requiring complex reasoning beyond simple pattern matching.
- **Core assumption**: The complexity of error types in MMLU requires sophisticated reasoning capabilities that current LLMs have not yet achieved.
- **Evidence anchors**:
  - [abstract]: "Automatic error detection proved challenging, with the best model (Claude 3 Opus) achieving only 41.9% F2 score."
  - [section]: "Despite the availability of annotated error types, fixing MMLU automatically proves to be a challenging task."
  - [corpus]: Moderate - corpus shows related work on MMLU benchmarking but limited evidence about automatic error detection challenges.

### Mechanism 3
- **Claim**: MMLU errors significantly impact model rankings and performance metrics, demonstrating the critical importance of dataset quality in LLM evaluation.
- **Mechanism**: Errors in MMLU create systematic biases in evaluation, leading to misleading comparisons between models and incorrect assessments of their true capabilities.
- **Core assumption**: Model performance differences on erroneous versus correct instances reflect genuine differences in model capabilities rather than random noise.
- **Evidence anchors**:
  - [abstract]: "Using MMLU-Redux, we demonstrate significant discrepancies with the model performance metrics that were originally reported."
  - [section]: "We re-evaluated leading LLMs on MMLU-Redux, and found the performance metrics notably altered, changing their ranking."
  - [corpus]: Weak - corpus shows related work on MMLU benchmarking but limited evidence about error impact on model rankings.

## Foundational Learning

- **Concept**: Error categorization taxonomy
  - Why needed here: The study develops a hierarchical annotation protocol to classify various error types in MMLU, which is fundamental to understanding the nature and distribution of errors.
  - Quick check question: What are the two primary groups of errors identified in the MMLU-Redux annotation protocol?

- **Concept**: Stratified sampling
  - Why needed here: The study uses stratified sampling to estimate that 6.49% of the complete MMLU dataset contains errors, making this statistical technique essential for generalization.
  - Quick check question: How does stratified sampling help in estimating error rates across the entire MMLU dataset?

- **Concept**: Cohen's Kappa inter-annotator agreement
  - Why needed here: The study assesses annotation reliability using Cohen's Kappa, which is crucial for validating the quality of the manual annotation process.
  - Quick check question: What does a Cohen's Kappa value exceeding 0.6 indicate about the annotation agreement?

## Architecture Onboarding

- **Component map**: MMLU-Redux dataset (manually annotated subset) -> Error detection models (LLMs with various prompting strategies) -> Evaluation framework (performance metrics comparison) -> Analysis pipeline (error type statistics and correlation analysis)
- **Critical path**: 1) Manual annotation of MMLU questions using error protocol, 2) Creation of MMLU-Redux dataset, 3) Evaluation of LLMs on both original and corrected datasets, 4) Analysis of error detection performance, 5) Correlation analysis between error rates and model performance.
- **Design tradeoffs**: Manual annotation provides high-quality labels but doesn't scale to the full dataset; automatic detection is scalable but currently inaccurate; retrieval augmentation improves detection but adds computational complexity.
- **Failure signatures**: Low inter-annotator agreement indicates protocol issues; poor automatic detection performance suggests fundamental challenges; inconsistent model rankings reveal dataset quality problems.
- **First 3 experiments**:
  1. Replicate the error detection experiments using the provided prompts and compare results with Table 3.
  2. Test the correlation analysis between error rates and model performance using the MMLU-Redux data.
  3. Implement the fine-tuning approach using LabelChaos and evaluate on MMLU-Redux to verify results in Table 9.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific factors contribute to the high error rates in certain MMLU subsets like Virology and Logical Fallacies?
- Basis in paper: Explicit - The paper identifies that Virology has a 57% error rate and Logical Fallacies has a 26% error rate, but doesn't fully explain the underlying causes.
- Why unresolved: The paper mentions possible causes like parsing errors and contextual issues, but doesn't provide a comprehensive analysis of why certain subjects are more prone to errors than others.
- What evidence would resolve it: A detailed analysis of the data collection and annotation processes for each subject, including interviews with the original annotators and examination of source materials, would help identify systematic patterns in error generation.

### Open Question 2
- Question: How can the error detection performance of LLMs be improved beyond the current 41.9% F2 score achieved by Claude 3 Opus?
- Basis in paper: Explicit - The paper demonstrates that even the best model (Claude 3 Opus) achieves only 41.9% F2 score in error detection, suggesting significant room for improvement.
- Why unresolved: The paper tests various prompting strategies and retrieval augmentation but doesn't explore more advanced techniques or model architectures specifically designed for error detection.
- What evidence would resolve it: Developing specialized error detection models, exploring ensemble methods, or creating domain-specific error detection models could potentially improve performance.

### Open Question 3
- Question: What is the long-term impact of MMLU errors on downstream applications and research that rely on these benchmarks?
- Basis in paper: Inferred - While the paper demonstrates that errors affect model rankings and evaluation metrics, it doesn't explore the broader implications for research and applications built on these benchmarks.
- Why unresolved: The paper focuses on the immediate effects of errors on LLM evaluation but doesn't investigate how these errors might propagate through the research ecosystem or affect real-world applications.
- What evidence would resolve it: Long-term studies tracking the impact of benchmark errors on published research, or experiments testing how error-riddled benchmarks affect model deployment in real-world scenarios, would provide insights into the broader consequences.

## Limitations
- Manual annotation scalability remains uncertain despite reliability on 5,700 questions
- Automatic detection performance is limited even with best models at 41.9% F2 score
- Correlation between error rates and performance changes needs stronger causal evidence

## Confidence
**High Confidence**:
- 6.49% of MMLU questions contain errors (based on manual annotation of 5,700 questions with established inter-annotator agreement)
- Error rates vary significantly across subjects (ranging from 0.28% to 57.14%)
- MMLU-Redux enables detection of error impact on model rankings (demonstrated through direct re-evaluation)

**Medium Confidence**:
- Automatic error detection remains challenging (supported by experimental results but limited by evaluation methodology)
- Model rankings change significantly when evaluated on MMLU-Redux (observed but not extensively validated across different model families)

**Low Confidence**:
- The proposed annotation protocol would scale effectively to the full MMLU dataset (extrapolated from smaller sample without validation)
- The 6.49% error rate represents the true error rate for all MMLU questions (based on stratified sampling assumptions)

## Next Checks
1. **Inter-annotator Agreement Validation**: Replicate the manual annotation process with new annotators using the described protocol to verify the reported Cohen's Kappa > 0.6 and assess consistency across different annotator groups.

2. **Error Detection Performance Replication**: Implement the exact prompting strategies described (zero-shot, few-shot, CoT, RAG) and evaluate on MMLU-Redux to verify the reported F2 scores of 41.9% for Claude 3 Opus and other models.

3. **Error Impact Correlation Analysis**: Conduct additional correlation analysis between error rates and model performance changes across different model families and evaluation metrics to strengthen the evidence for systematic impact.