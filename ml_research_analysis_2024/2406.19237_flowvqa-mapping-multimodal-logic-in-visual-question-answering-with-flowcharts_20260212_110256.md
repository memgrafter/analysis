---
ver: rpa2
title: 'FlowVQA: Mapping Multimodal Logic in Visual Question Answering with Flowcharts'
arxiv_id: '2406.19237'
source_url: https://arxiv.org/abs/2406.19237
tags:
- flowchart
- question
- questions
- visual
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FlowVQA is a new benchmark for testing vision language models on
  flowchart-based reasoning. It contains 2,272 human-verified flowcharts and 22,413
  question-answer pairs spanning four types of reasoning tasks.
---

# FlowVQA: Mapping Multimodal Logic in Visual Question Answering with Flowcharts

## Quick Facts
- arXiv ID: 2406.19237
- Source URL: https://arxiv.org/abs/2406.19237
- Reference count: 40
- Primary result: New benchmark reveals VLMs struggle with flowchart-based reasoning tasks

## Executive Summary
FlowVQA is a benchmark designed to evaluate vision language models' (VLMs) capabilities in visual logic and spatial reasoning through flowchart comprehension. The dataset contains 2,272 human-verified flowcharts and 22,413 question-answer pairs spanning four distinct reasoning types. When tested on this benchmark, even the best proprietary VLMs achieved only 68.42% accuracy, highlighting significant limitations in their ability to understand complex visual structures and logical relationships. The benchmark reveals that models struggle particularly with path-based and topological reasoning tasks, suggesting a need for improved visual logic capabilities in VLMs.

## Method Summary
The FlowVQA dataset was created using an automated pipeline that generates flowcharts from source texts using GPT-4, followed by rigorous human verification. The process involves collecting source texts from WikiHow articles and Instructables, generating structured representations and Mermaid.js scripts, creating diverse question-answer pairs covering four reasoning types, and implementing a multi-step human verification pipeline. The final dataset includes 2,272 flowcharts and 22,413 question-answer pairs, with up to 51% of samples discarded during verification to ensure quality and challenge level.

## Key Results
- Proprietary VLMs achieved only 68.42% accuracy on the benchmark
- Models consistently performed better on Fact Retrieval and Applied Scenario questions than on Flow-Referential and Topological questions
- Directional bias was observed, with models showing inconsistent performance on inverted flowcharts
- The best performance was achieved using text-only few-shot prompting with reasoning directives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset addresses a gap in existing VQA benchmarks by focusing on visual logic and spatial reasoning through flowcharts
- Mechanism: By using flowcharts as visual contexts, the benchmark requires models to understand both structural and semantic aspects, as well as macroscopic and granular context within visually complex yet straightforward flowcharts
- Core assumption: Flowcharts inherently emphasize sequential and logical reasoning, necessitating traversal of steps or decisions in a specific sequence, and their multi-directional flow represents various paths based on conditions or decisions
- Evidence anchors:
  - [abstract] "Most benchmarks evaluate pretrained model extraction capabilities, neglecting their ability to comprehend complex spatial relationships and visual logical reasoning."
  - [abstract] "Flowcharts emphasize sequential and logical reasoning, as they necessitate traversal of steps or decisions in a specific sequence."

### Mechanism 2
- Claim: The generate-and-test approach with rigorous human verification ensures high-quality, challenging, and accurate samples
- Mechanism: Machine generation of flowcharts and Q/A pairs enables rapid scaling, while stochasticity of LLMs helps create unbiased and diverse datasets. Human verification with detailed rubrics and a custom annotation platform ensures quality, complexity, and accuracy
- Core assumption: Human experts can effectively evaluate the quality of machine-generated flowcharts and Q/A pairs, ensuring they meet high standards of challenge, coherence, and insightfulness
- Evidence anchors:
  - [section 2.4] "To ensure strong validity of our work, we establish a robust human verification pipeline for our models and flowcharts."
  - [section 2.4] "The final samples... ensure appropriate complexity and correctness of flowcharts, questions and corresponding answers."

### Mechanism 3
- Claim: The dataset's complexity and variety of question types reveal limitations in existing VLMs' visual and logical reasoning abilities
- Mechanism: The dataset includes four distinct question types (Fact Retrieval, Applied Scenario, Flow Referential, and Topological) that test different aspects of flowchart comprehension, from simple fact localization to complex structural analysis and path-based reasoning
- Core assumption: Different question types will reveal different strengths and weaknesses in VLMs' abilities to handle various aspects of flowchart understanding
- Evidence anchors:
  - [section 2.3] "We curate four question types designed to analyze and test different aspects: Fact Retrieval, Applied Scenario, Flow Referential and Topological Question and Answer."
  - [section 3.3] "It is evident from the table that all models consistently perform better on Fact Retrieval (T1) and Applied Scenario (T2) based questions than on Flow-Referential (T3) and Topological (T4)."

## Foundational Learning

- Concept: Visual grounding in VQA
  - Why needed here: The dataset aims to address the issue of visual grounding in existing VQA systems, which often rely on irrelevant parts of images or disregard the visual modality entirely
  - Quick check question: How does the dataset ensure that models must rely on the visual modality and not just pre-trained knowledge to answer questions?

- Concept: Sequential and logical reasoning in flowcharts
  - Why needed here: Flowcharts require understanding of directional logic and multi-directional flow, representing various paths based on conditions or decisions
  - Quick check question: What aspects of flowchart structure make them inherently more challenging for VLMs compared to simple image comprehension tasks?

- Concept: Graph-based representation of flowcharts
  - Why needed here: Topological questions require analysis of the flowchart at a macroscopic level, involving parsing Mermaid.js scripts into adjacency matrices representing the flowchart as a graph
  - Quick check question: How does representing flowcharts as graphs enable the creation of template-based questions with quantitative answers?

## Architecture Onboarding

- Component map: Flowchart images (from Mermaid.js scripts) -> Question-Answer pairs (four reasoning types) -> Human verification pipeline -> Benchmark evaluation
- Critical path: Flowchart generation -> Q/A creation -> Human verification -> Model evaluation
- Design tradeoffs: Quality over quantity - up to 51% samples discarded during verification to ensure challenging, accurate samples
- Failure signatures: Poor performance across all question types suggests fundamental limitations in visual logic reasoning; directional bias indicates reliance on visual cues rather than true understanding
- First 3 experiments:
  1. Evaluate baseline VLMs on full dataset using various prompting strategies to establish performance baseline
  2. Analyze performance differences across question types to identify specific areas of strength and weakness
  3. Investigate presence of directional bias by evaluating models on inverted flowchart sets

## Open Questions the Paper Calls Out

## Question 1
- Question: How can VLMs be effectively trained to improve their understanding of flowchart structures and logical reasoning, particularly for tasks involving complex path reconstruction and multi-directional logic?
- Basis in paper: Explicit
- Why unresolved: The paper demonstrates that even the best VLMs struggle significantly with flowchart-based reasoning tasks, suggesting a fundamental limitation in their ability to comprehend complex structural and logical aspects. The authors highlight the need for advancements in architecture and prompting strategies to enhance visual logic and reasoning capabilities
- What evidence would resolve it: Developing and evaluating new VLM architectures or training techniques specifically designed for flowchart understanding, followed by rigorous testing on FlowVQA and similar benchmarks

## Question 2
- Question: What are the key factors contributing to the directional bias observed in VLMs when analyzing flowcharts, and how can this bias be mitigated?
- Basis in paper: Explicit
- Why unresolved: The paper identifies a significant directional bias in VLMs, where their performance drops notably when presented with inverted flowcharts. This suggests that models rely on directional cues rather than grounding their inferences in the visual context. The authors propose that augmenting pretraining mixtures with counterfactual examples might help, but this remains to be explored
- What evidence would resolve it: Conducting experiments with VLMs trained on datasets containing a balanced mix of normal and inverted flowcharts, and analyzing their performance on both versions to assess the impact of such interventions

## Question 3
- Question: How can the FlowVQA benchmark be extended to include more diverse and complex flowchart scenarios, such as those involving real-world data or more intricate decision-making processes?
- Basis in paper: Inferred
- Why unresolved: While FlowVQA is a significant step towards evaluating VLM reasoning capabilities on flowcharts, it relies on synthetically generated flowcharts based on process workflows. The paper acknowledges the limitations of this approach and suggests that future work could involve incorporating real-world flowcharts or exploring more complex subtasks
- What evidence would resolve it: Creating and evaluating VLMs on an expanded FlowVQA dataset that includes real-world flowcharts, more intricate decision-making scenarios, or additional subtasks like flowchart-to-code conversion

## Limitations

- The dataset relies on synthetically generated flowcharts rather than real-world examples
- The evaluation focuses primarily on accuracy metrics without deeper analysis of error patterns
- The human verification process, while rigorous, may not fully capture all forms of bias or systematic errors in the generated data

## Confidence

- High Confidence: The observation that VLMs struggle with FlowVQA tasks is well-supported by empirical results showing performance below 70% even for proprietary models
- Medium Confidence: The claim that this dataset fills a specific gap in VQA benchmarks is reasonable but could benefit from more direct comparison with existing flowchart-specific benchmarks
- Low Confidence: The assertion that directional bias is a primary factor in model performance is based on limited evidence from inverted flowchart experiments

## Next Checks

1. **Bias Analysis Validation**: Conduct a more comprehensive study of directional bias by systematically varying flowchart orientations (horizontal, vertical, diagonal) and analyzing performance patterns across all question types

2. **Error Pattern Analysis**: Perform detailed error analysis on the 30-40% of incorrect predictions to identify whether failures stem from visual comprehension, logical reasoning, or language understanding components

3. **Cross-dataset Generalization**: Test whether models trained on FlowVQA show improved performance on other flowchart and diagram understanding tasks, validating the dataset's effectiveness for improving visual logic reasoning capabilities