---
ver: rpa2
title: 'SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI
  and Low-Resource Environments'
arxiv_id: '2410.11331'
source_url: https://arxiv.org/abs/2410.11331
tags:
- shakti-llm
- shakti
- such
- language
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Shakti is a 2.5 billion parameter language model optimized for
  edge AI and low-resource environments. It integrates Variable Grouped Query Attention
  (VGQA), pre-normalization, SwiGLU activations, and Rotary Positional Embeddings
  (RoPE) to balance performance with low latency and memory efficiency.
---

# SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments

## Quick Facts
- arXiv ID: 2410.11331
- Source URL: https://arxiv.org/abs/2410.11331
- Reference count: 25
- A 2.5B parameter model optimized for edge deployment with competitive benchmark performance

## Executive Summary
Shakti is a compact language model designed specifically for edge AI deployment and low-resource environments. At 2.5 billion parameters, it integrates innovative architectural components including Variable Grouped Query Attention (VGQA), SwiGLU activations, and Rotary Positional Embeddings (RoPE) to balance performance with efficiency. Trained on 2.8 trillion tokens including vernacular languages, Shakti achieves competitive results on benchmarks like MMLU (71.7%) while maintaining low memory usage and inference latency suitable for deployment on smartphones, wearables, and IoT devices.

## Method Summary
Shakti employs a transformer architecture with 16 layers, 4096 model dimension, and 32 attention heads. The model integrates three key innovations: Variable Grouped Query Attention (VGQA) for memory-efficient attention computation, SwiGLU activation functions for stable training and better gradient flow, and Rotary Positional Embeddings (RoPE) for efficient sequence encoding. The training process involves three phases: causal language modeling pretraining, supervised fine-tuning on curated datasets, and direct preference optimization. The model is fine-tuned on vernacular languages and domain-specific datasets, enabling effective deployment in healthcare, finance, and customer service applications.

## Key Results
- Achieves 71.7% accuracy on MMLU benchmark and 86.2% on PIQA, outperforming larger models like Phi-3 Mini-4k and Gemma 7B in several tasks
- Quantized model reduces GPU memory consumption from 9GB to 4GB, enabling deployment on edge devices with limited resources
- Demonstrates strong performance on multilingual benchmarks including Hindi, Kannada, and Telugu, with MMLU-Hindi score of 55.8
- Optimized architecture achieves competitive performance while maintaining low latency suitable for real-time applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Variable Grouped Query Attention (VGQA) reduces memory usage while maintaining performance on edge devices.
- Mechanism: VGQA groups multiple queries per key during attention computations, which reduces the number of key-value pairs that need to be stored in memory. This decreases memory footprint and accelerates inference times compared to standard multi-head attention.
- Core assumption: Attention is the primary memory bottleneck in transformer models, and sharing keys across multiple queries does not significantly degrade model performance.
- Evidence anchors:
  - [abstract] "VGQA groups multiple queries per key during attention computations, significantly reducing the memory footprint and accelerating inference times."
  - [section] "VGQA allows multiple queries to share a single key during the attention process, significantly reducing the memory footprint while improving inference times."
  - [corpus] Weak evidence - only mentions similar mechanisms in related work but not direct performance data for VGQA specifically.
- Break condition: If the model requires fine-grained attention distinctions that are lost when grouping queries, performance may degrade despite memory savings.

### Mechanism 2
- Claim: SwiGLU activation function provides more stable training and better gradient flow than traditional ReLU activations.
- Mechanism: SwiGLU (Sigmoid-weighted Linear Units) provides smoother gradient transitions during backpropagation compared to ReLU, preventing vanishing or exploding gradients. This results in more efficient training and better convergence.
- Core assumption: The activation function significantly impacts training stability and gradient propagation in transformer architectures.
- Evidence anchors:
  - [abstract] "SwiGLU activations... improve the training process by stabilizing gradient flows and preventing issues like vanishing or exploding gradients."
  - [section] "SwiGLU activation functions enhance gradient flow, resulting in more efficient training [17]. These methods provide significant improvements over traditional activation functions like ReLU."
  - [corpus] Weak evidence - only references the original SwiGLU paper without specific comparative data for this model.
- Break condition: If the computational overhead of SwiGLU (requiring element-wise multiplication) outweighs the training stability benefits, it may not be optimal for edge deployment.

### Mechanism 3
- Claim: Rotary Positional Embeddings (RoPE) enable efficient handling of long sequences without increasing computational overhead.
- Mechanism: RoPE encodes positional information by rotating query and key vectors in the attention mechanism, allowing the model to capture sequence order without additional positional embeddings or increased memory usage.
- Core assumption: Positional information is critical for sequence modeling, and RoPE provides an efficient encoding method that scales well with sequence length.
- Evidence anchors:
  - [abstract] "To handle long text sequences without increasing computational overhead, Shakti integrates Rotary Positional Embeddings (RoPE)."
  - [section] "RoPE enhances the model's ability to process longer sequences efficiently, making it suitable for tasks such as document summarization and complex queries, all while maintaining low memory usage."
  - [corpus] Weak evidence - mentions RoPE in related work but lacks specific performance data for this implementation.
- Break condition: If the sequence length requirements exceed RoPE's effective range (typically around 10,000 tokens), the model may need alternative positional encoding methods.

## Foundational Learning

- Concept: Transformer attention mechanism
  - Why needed here: Understanding how attention works is crucial to grasp why VGQA and RoPE are effective optimizations.
  - Quick check question: What is the computational complexity of standard multi-head attention per layer?

- Concept: Quantization and model compression
  - Why needed here: The paper discusses GPU consumption differences between raw and quantized models (9GB vs 4GB), which is essential for edge deployment.
  - Quick check question: How does quantization reduce memory usage and what is the typical precision reduction?

- Concept: Fine-tuning methodologies (CPT, SFT, DPO)
  - Why needed here: The paper describes three distinct training phases that shape the final model performance and behavior.
  - Quick check question: What is the key difference between Supervised Fine-Tuning and Direct Preference Optimization?

## Architecture Onboarding

- Component map: 16 layers -> 4096 model dimension -> 32 attention heads -> VGQA -> SwiGLU -> RoPE -> Pre-normalization

- Critical path: VGQA → Attention computation → Memory savings → Faster inference → Edge deployment viability

- Design tradeoffs: Model size (2.5B) vs performance vs memory constraints; SwiGLU complexity vs training stability; RoPE effectiveness vs sequence length limitations

- Failure signatures:
  - High memory usage despite VGQA suggests incorrect implementation or excessive sequence lengths
  - Poor convergence indicates SwiGLU or pre-normalization issues
  - Inaccurate positional encoding for long sequences points to RoPE limitations

- First 3 experiments:
  1. Compare memory usage and inference speed between VGQA and standard multi-head attention on target edge hardware
  2. Benchmark SwiGLU vs ReLU activation on training stability and convergence speed
  3. Test RoPE positional encoding effectiveness across different sequence lengths (256, 1024, 4096 tokens)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Shakti's performance scale with additional training data in low-resource language domains?
- Basis in paper: [explicit] The paper mentions Shakti is fine-tuned on vernacular languages like Hindi, Kannada, and Telugu, but does not provide performance scaling analysis.
- Why unresolved: The paper does not explore how increasing training data volume for low-resource languages affects performance.
- What evidence would resolve it: Comparative benchmarks showing Shakti's performance across different sizes of vernacular language datasets.

### Open Question 2
- Question: What is the optimal quantization strategy for balancing model size and inference speed on different edge devices?
- Basis in paper: [explicit] The paper presents performance comparisons across different quantization types (Q4 KM, Q5 KM) but doesn't explore the optimization space.
- Why unresolved: The paper provides specific quantization results but doesn't analyze the trade-offs or optimal strategies for different hardware configurations.
- What evidence would resolve it: Comprehensive analysis of quantization effects on various edge devices with different memory and processing constraints.

### Open Question 3
- Question: How does Shakti's Variable Grouped Query Attention (VGQA) compare to other attention optimization techniques in terms of computational efficiency?
- Basis in paper: [explicit] The paper introduces VGQA as a key innovation but doesn't provide comparative analysis with other attention mechanisms.
- Why unresolved: While VGQA is described as beneficial, its relative performance advantages aren't quantified against alternative approaches.
- What evidence would resolve it: Head-to-head benchmarks comparing VGQA with other attention optimization techniques like grouped query attention and sliding window attention.

## Limitations
- No comparative ablation studies for individual architectural innovations (VGQA, SwiGLU, RoPE)
- Performance benchmarks primarily against relatively small models rather than established edge AI models
- Limited empirical data on actual edge hardware performance metrics (latency, power consumption, real-time inference)

## Confidence
- **Medium**: Overall performance claims on benchmarks - limited to comparisons with smaller models and lack of ablation studies
- **Low**: Edge deployment benefits - minimal empirical data on actual edge hardware performance metrics
- **Medium**: Architectural innovations - well-described mechanisms but insufficient empirical validation of their individual contributions

## Next Checks
1. Conduct ablation studies comparing Shakti with and without VGQA, SwiGLU, and RoPE to quantify individual architectural contributions
2. Benchmark against established edge AI models (NanoGPT, DistilBERT, MobileBERT) on the same hardware platforms and metrics
3. Deploy the quantized model on representative edge devices (smartphone, Raspberry Pi, microcontroller) to measure actual latency, throughput, and power consumption under realistic workloads