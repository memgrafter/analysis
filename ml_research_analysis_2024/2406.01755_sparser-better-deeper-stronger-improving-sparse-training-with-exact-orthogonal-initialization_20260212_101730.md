---
ver: rpa2
title: 'Sparser, Better, Deeper, Stronger: Improving Sparse Training with Exact Orthogonal
  Initialization'
arxiv_id: '2406.01755'
source_url: https://arxiv.org/abs/2406.01755
tags:
- sparse
- orthogonal
- initialization
- training
- density
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Exact Orthogonal Initialization (EOI), a method
  for creating sparse, orthogonal weight matrices in neural networks by composing
  random Givens rotations. Unlike previous approaches, EOI provides exact orthogonality
  and supports arbitrary sparsity levels, making it applicable to both fully-connected
  and convolutional layers.
---

# Sparser, Better, Deeper, Stronger: Improving Sparse Training with Exact Orthogonal Initialization

## Quick Facts
- arXiv ID: 2406.01755
- Source URL: https://arxiv.org/abs/2406.01755
- Reference count: 40
- Authors: Aleksandra Irena Nowak; Åukasz Gniecki; Filip Szatkowski; Jacek Tabor

## Executive Summary
This paper introduces Exact Orthogonal Initialization (EOI), a method for creating sparse, orthogonal weight matrices in neural networks by composing random Givens rotations. Unlike previous approaches, EOI provides exact orthogonality and supports arbitrary sparsity levels, making it applicable to both fully-connected and convolutional layers. The method samples both the mask and weights simultaneously, with an efficient O(n) computation per rotation.

Experiments show EOI consistently outperforms standard sparse initialization techniques across various architectures (MLP, CNN, ResNet, VGG, EfficientNet) on datasets like MNIST, CIFAR, Tiny ImageNet, and ImageNet. Notably, EOI enables training of extremely deep 1000-layer networks without residual connections or normalization layers, demonstrating the importance of weight initialization in static sparse training. EOI also improves training dynamics and signal propagation, especially in high-sparsity regimes, while being computationally efficient compared to approximate isometry methods.

## Method Summary
EOI generates exact orthogonal sparse matrices by composing random Givens rotations. The method samples both the sparsity mask and rotation parameters simultaneously, ensuring exact orthogonality while achieving arbitrary sparsity levels. For convolutional layers, EOI converts them to their fully-connected equivalents and applies the same principle. The computation is efficient, with O(n) complexity per rotation, making it scalable to deep networks. Unlike approximate methods, EOI guarantees exact orthogonality, which is crucial for maintaining stable signal propagation in extremely deep networks without normalization layers.

## Key Results
- EOI consistently outperforms standard sparse initialization techniques across multiple architectures (MLP, CNN, ResNet, VGG, EfficientNet)
- Enables training of 1000-layer networks without residual connections or normalization layers, demonstrating the importance of initialization
- Improves training dynamics and signal propagation, especially in high-sparsity regimes, while maintaining computational efficiency

## Why This Works (Mechanism)
EOI works by composing exact orthogonal transformations through Givens rotations, which are orthogonal matrices that zero out specific elements. By sampling both the sparsity mask and rotation parameters simultaneously, EOI ensures that the resulting matrix is both sparse and exactly orthogonal. This exact orthogonality is crucial for maintaining stable gradient flow in extremely deep networks, preventing the vanishing/exploding gradient problems that typically require normalization layers or residual connections.

## Foundational Learning
1. **Orthogonal initialization** - Ensures stable gradient flow in deep networks by preserving the norm of activations and gradients
   - Why needed: Prevents vanishing/exploding gradients in deep networks
   - Quick check: Verify that the weight matrix multiplied by its transpose equals the identity matrix

2. **Givens rotations** - Elementary orthogonal transformations that zero out specific elements while preserving orthogonality
   - Why needed: Building blocks for composing exact orthogonal sparse matrices
   - Quick check: Confirm that individual Givens matrices are orthogonal (Q^T Q = I)

3. **Static sparse training** - Training networks with fixed sparsity patterns throughout training
   - Why needed: EOI is specifically designed for static sparse training scenarios
   - Quick check: Ensure sparsity mask remains fixed during training

4. **Signal propagation in deep networks** - How information flows through layers, affected by weight initialization
   - Why needed: EOI aims to improve signal propagation in extremely deep networks
   - Quick check: Monitor activation and gradient norms across layers during training

## Architecture Onboarding
**Component Map:** Sparse mask generation -> Givens rotation sampling -> Matrix composition -> Weight initialization

**Critical Path:** The critical path involves generating the sparse mask, sampling Givens rotations, and composing them to form the final weight matrix. The efficiency of this process, particularly the O(n) computation per rotation, is crucial for scalability.

**Design Tradeoffs:** EOI trades computational complexity of matrix composition for exact orthogonality, unlike approximate methods that are faster but sacrifice orthogonality guarantees. The method also balances sparsity level with the number of rotations needed for composition.

**Failure Signatures:** If EOI fails, it may manifest as:
- Poor training dynamics despite exact orthogonality (indicating insufficient expressiveness)
- Computational bottlenecks when composing many rotations for extremely deep networks
- Degraded performance compared to dense initialization (suggesting sparsity is too aggressive)

**First Experiments:**
1. Compare EOI with standard sparse initialization on a 20-layer MLP on MNIST
2. Test EOI on a 50-layer CNN on CIFAR-10 with varying sparsity levels (50%, 90%, 95%)
3. Evaluate EOI's ability to train a 100-layer network without normalization layers on Tiny ImageNet

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead of composing many Givens rotations for extremely deep networks, though claimed to be O(n), may still impact training efficiency in practice
- Performance on recurrent neural networks and other specialized architectures remains untested
- Analysis focuses primarily on classification tasks, with limited exploration of regression or other problem domains

## Confidence
- **High confidence** in EOI's ability to produce exact orthogonal matrices and its superiority over standard sparse initialization methods
- **Medium confidence** in the claimed computational efficiency, as practical implementation details and benchmarks are limited
- **Medium confidence** in the generalizability across all architectures, given the focus on CNNs and MLPs in experiments

## Next Checks
1. Benchmark EOI's runtime performance against approximate isometry methods on extremely deep networks (1000+ layers) to verify computational efficiency claims
2. Test EOI on sequence models (RNNs/LSTMs) and natural language processing tasks to assess broader applicability
3. Conduct ablation studies varying the number of Givens rotations to determine the optimal balance between orthogonality and computational cost