---
ver: rpa2
title: 'CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese
  Public Security Domain'
arxiv_id: '2402.07234'
source_url: https://arxiv.org/abs/2402.07234
tags:
- llms
- public
- security
- tasks
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces CPSDBench, a comprehensive evaluation benchmark
  for Large Language Models (LLMs) in the Chinese public security domain. The benchmark
  addresses the gap in assessing LLM performance on specialized public security tasks
  such as sentiment analysis, fraud detection, and case summarization.
---

# CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain

## Quick Facts
- arXiv ID: 2402.07234
- Source URL: https://arxiv.org/abs/2402.07234
- Reference count: 34
- Large Language Models show varying performance across public security tasks, with Chinese-specific models excelling in language-specific tasks while proprietary models like GPT-4 lead in general tasks

## Executive Summary
CPSDBench introduces a comprehensive evaluation benchmark for Large Language Models (LLMs) in the Chinese public security domain. The benchmark addresses the gap in assessing LLM performance on specialized public security tasks such as sentiment analysis, fraud detection, and case summarization. By integrating real-world datasets across four dimensions—text classification, information extraction, question answering, and text generation—CPSDBench provides a more realistic assessment of LLM capabilities in handling complex, sensitive scenarios. The study evaluates ten mainstream LLMs, revealing that while proprietary models excel in general tasks, Chinese-specific models outperform in language-specific tasks, highlighting the importance of domain-specific training for specialized applications.

## Method Summary
The CPSDBench benchmark evaluates LLMs across four task dimensions: text classification (sentiment analysis, rumor detection, fraud detection), information extraction (drug-related case reports), question answering (case reading comprehension), and text generation (case summaries). The benchmark uses real-world public security datasets and introduces a hybrid evaluation metric for information extraction that combines exact match and fuzzy matching based on Levenshtein distance. Ten mainstream LLMs including GPT-4, ChatGLM-4, and open-source models are evaluated using tailored prompts across all tasks, with metrics including accuracy, precision, recall, F1-score, BLEU, ROUGE, and BERT Score.

## Key Results
- Proprietary models like GPT-4 excel in general tasks across all four dimensions
- Chinese-specific models like ChatGLM-4 outperform in language-specific tasks, particularly in text generation and question answering
- Models show significant limitations in processing sensitive data and adversarial samples
- Parameter size plays a key role in enhancing natural language understanding capabilities, but has less impact on text generation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark integrates real-world datasets across four dimensions to capture task-specific capabilities
- Mechanism: Sourcing data from actual public security scenarios reflects realistic operational complexity and adversarial conditions not present in general-purpose datasets
- Core assumption: Public security tasks involve unique data characteristics that standard benchmarks fail to represent
- Evidence anchors: [abstract] "CPSDBench integrates datasets related to public security collected from real-world scenarios"
- Break condition: If real-world public security datasets do not exhibit unique characteristics beyond general text data

### Mechanism 2
- Claim: Hybrid evaluation metric combines exact match and fuzzy matching based on Levenshtein distance
- Mechanism: This approach addresses semantic equivalence in LLM outputs, allowing correct predictions with minor lexical differences to be recognized as accurate
- Core assumption: LLM outputs may be semantically correct but differ literally from labels, particularly in Chinese language processing
- Evidence anchors: [section] "We have designed a hybrid evaluation metric... calculating the Levenshtein distance"
- Break condition: If threshold settings are too high or too low, the metric may miss valid predictions or accept incorrect ones

### Mechanism 3
- Claim: Chinese-specific models outperform general models in language-specific tasks due to pre-training on extensive Chinese corpora
- Mechanism: Models trained on domain-specific language data develop better understanding of linguistic nuances and context-specific expressions
- Core assumption: Language-specific pre-training significantly improves performance on tasks requiring deep cultural and linguistic understanding
- Evidence anchors: [abstract] "Chinese-specific models like ChatGLM-4 outperform in language-specific tasks"
- Break condition: If performance differences are primarily due to model size rather than language-specific training

## Foundational Learning

- Concept: Public security domain specificity
  - Why needed here: Understanding unique characteristics of public security tasks is crucial for evaluating LLM performance
  - Quick check question: What are the key differences between general NLP tasks and public security tasks that affect LLM evaluation?

- Concept: Hybrid evaluation metrics
  - Why needed here: Recognizing when exact match metrics are insufficient and understanding how fuzzy matching can improve evaluation accuracy
  - Quick check question: Why might an LLM output be semantically correct but fail exact string matching?

- Concept: Prompt engineering design
  - Why needed here: Effective prompts are essential for eliciting desired responses from LLMs in specialized tasks
  - Quick check question: What are the four key components of the prompt engineering framework used in this study?

## Architecture Onboarding

- Component map: Data collection pipeline -> Task categorization system -> Evaluation metric engine -> Model interface layer -> Result aggregation and reporting module
- Critical path: Dataset collection → Task categorization → Model evaluation → Metric calculation → Result analysis
- Design tradeoffs: Real-world dataset collection vs. synthetic data generation; exact match precision vs. fuzzy match flexibility; broad task coverage vs. deep specialization
- Failure signatures: Poor performance on adversarial samples indicates model safety limitations; format compliance issues suggest prompt engineering weaknesses
- First 3 experiments:
  1. Evaluate all baseline models on a small subset of each task type to establish baseline performance patterns
  2. Test the hybrid evaluation metric on cases with known semantic equivalence to validate threshold settings
  3. Compare Chinese-specific models vs. general models on language-specific subtasks to quantify the performance gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on adversarial samples in public security tasks compared to non-adversarial samples?
- Basis in paper: [explicit] The paper states that LLMs exhibit limitations in processing sensitive data and adversarial samples
- Why unresolved: The paper mentions this limitation but does not provide specific performance metrics or comparative analysis
- What evidence would resolve it: Empirical results showing LLM performance on adversarial vs. non-adversarial samples with specific accuracy metrics

### Open Question 2
- Question: What are the specific safety mechanisms that cause LLMs to trigger sensitive alerts in public security tasks?
- Basis in paper: [explicit] The paper discusses that LLMs tend to trigger safety mechanisms when processing sensitive content
- Why unresolved: The paper identifies the issue but does not detail the underlying safety mechanisms or adjustment methods
- What evidence would resolve it: Detailed analysis of safety mechanisms with case studies of specific instances and potential solutions

### Open Question 3
- Question: How does parameter size of LLMs affect their performance in public security tasks, particularly in text generation?
- Basis in paper: [explicit] The paper notes that parameter scale plays a key role in enhancing natural language understanding but has less impact on text generation
- Why unresolved: The paper suggests this relationship but does not provide comprehensive analysis across different parameter sizes
- What evidence would resolve it: Comparative performance data of LLMs with different parameter sizes across multiple public security tasks

## Limitations
- Benchmark relies on real-world public security datasets, restricting reproducibility and independent validation
- Current datasets lack sufficient adversarial samples critical for evaluating LLM robustness
- Performance evaluation conducted during early release stages of most models, potentially not reflecting current capabilities

## Confidence

**High Confidence Claims:**
- Benchmark successfully covers four key task dimensions as evidenced by comprehensive methodology
- GPT-4 demonstrates superior performance in general tasks compared to other evaluated models

**Medium Confidence Claims:**
- Chinese-specific models outperform GPT-4 in language-specific tasks based on reported performance differences
- Hybrid evaluation metric effectively addresses semantic equivalence issues, though exact threshold settings remain unclear

**Low Confidence Claims:**
- Extent to which real-world dataset characteristics uniquely benefit public security LLM evaluation
- Practical impact of current limitations on adversarial sample handling in real-world deployment

## Next Checks
1. Attempt to reproduce benchmark results using publicly available subsets of datasets or comparable open-source alternatives
2. Design and implement additional adversarial samples to evaluate model performance under more challenging conditions
3. Systematically test how different safety filtering thresholds affect model performance on legitimate public security tasks