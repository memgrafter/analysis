---
ver: rpa2
title: 'FuLG: 150B Romanian Corpus for Language Model Pretraining'
arxiv_id: '2407.13657'
source_url: https://arxiv.org/abs/2407.13657
tags:
- arxiv
- language
- fulg
- dataset
- romanian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FuLG, a 156 billion-token Romanian corpus
  extracted from CommonCrawl for language model pretraining. The authors developed
  a pipeline involving deduplication, content filtering, and quality filtering using
  thresholds optimized for Romanian text.
---

# FuLG: 150B Romanian Corpus for Language Model Pretraining

## Quick Facts
- arXiv ID: 2407.13657
- Source URL: https://arxiv.org/abs/2407.13657
- Reference count: 32
- Corpus size: 156 billion tokens (three times larger than existing Romanian datasets)

## Executive Summary
FuLG is a 156 billion-token Romanian corpus developed for language model pretraining, addressing the critical scarcity of high-quality training data for underrepresented languages. The corpus was extracted from CommonCrawl snapshots spanning 2013 to May 2024 using a pipeline that includes deduplication, content filtering, and quality filtering optimized for Romanian text. When compared against existing Romanian corpora (OSCAR, mC4) through ablation studies using identical 1B parameter models, FuLG achieved comparable or better performance with a perplexity of 16.06 versus 15.38 for mC4, along with improved qualitative story generation results.

## Method Summary
The FuLG corpus construction pipeline processes CommonCrawl WET-format snapshots through distributed processing using CCNet, followed by language identification with FastText (threshold 0.5). The pipeline includes deduplication reducing corpus size by 37% with exact matching and 50% with fuzzy matching (0.8 threshold). Content filtering removes HTML artifacts using regex patterns and controversial content via dictionary-based filtering while replacing PII with special tokens. Quality filtering applies thresholds from the Gopher paper including n-gram statistics, word count limits, median word length, and line pattern rules. The final corpus is three times larger than existing Romanian datasets while maintaining quality comparable to or better than OSCAR and mC4.

## Key Results
- FuLG corpus size: 156 billion tokens, three times larger than existing Romanian datasets
- Perplexity performance: 16.06 vs 15.38 for mC4 (comparable or better)
- Achieved through 37% reduction via exact deduplication and 50% via fuzzy deduplication (0.8 threshold)

## Why This Works (Mechanism)
None provided in source material

## Foundational Learning
None provided in source material

## Architecture Onboarding
**Component Map:**
CommonCrawl snapshots -> CCNet distributed processing -> FastText language identification -> Exact deduplication (37%) -> Fuzzy deduplication (50%) -> Content filtering (regex + dictionary) -> Quality filtering (Gopher thresholds) -> FuLG corpus

**Critical Path:**
The critical path involves successful language identification and deduplication, as these steps determine the corpus size and quality foundation. Content and quality filtering build upon this foundation.

**Design Tradeoffs:**
- WET format vs HTML parsing: WET provides cleaner text but potentially misses structured content; HTML parsing could yield more data but requires sophisticated processing
- English-derived thresholds vs Romanian-specific optimization: Using established thresholds ensures comparability but may not optimize for Romanian language characteristics

**Failure Signatures:**
- Insufficient hardware resources: Job array size limits exceeded during CCNet processing
- Quality filters too aggressive: Dramatic reduction in token count compared to baselines
- Quality filters too lenient: Perplexity and story generation performance not competitive with OSCAR/mC4

**First Experiments:**
1. Process a single CommonCrawl snapshot through the full pipeline to validate each step
2. Compare token counts and basic statistics after each filtering stage against OSCAR/mC4
3. Train a small model on processed data to verify language identification accuracy

## Open Questions the Paper Calls Out
**Open Question 1:** How does FuLG's performance compare to proprietary models trained on Romanian data when used for downstream tasks like question answering or summarization? The paper only evaluates perplexity and story generation, lacking comprehensive downstream task evaluation against state-of-the-art proprietary models.

**Open Question 2:** What is the optimal size threshold for quality filtering in Romanian corpora, and how does it differ from English-based thresholds? The paper acknowledges current thresholds may not be optimal for Romanian but doesn't explore alternative configurations.

**Open Question 3:** How much additional high-quality Romanian text remains extractable from future CommonCrawl snapshots using improved HTML parsing techniques? The paper only used WET format processing and didn't explore HTML parsing improvements.

## Limitations
- Quality filtering thresholds are derived from English language research rather than optimized for Romanian specifically
- Content filtering dictionary contents are not disclosed, making bias assessment difficult
- Limited evaluation scope focused on perplexity and story generation without comprehensive downstream task validation

## Confidence
- Corpus size and construction methodology: High
- Perplexity and story generation comparisons: Medium
- Generalizability and bias analysis: Low

## Next Checks
1. Conduct ablation studies varying the quality filtering thresholds to assess sensitivity and optimize for Romanian specifically, rather than relying on English-derived parameters.
2. Perform comprehensive downstream task evaluations on established Romanian NLP benchmarks (e.g., Romanian Named Entity Recognition, Part-of-Speech Tagging) to validate practical utility beyond perplexity and story generation.
3. Analyze the topical and domain distribution of the final corpus to identify potential biases and ensure balanced representation across Romanian language use cases.