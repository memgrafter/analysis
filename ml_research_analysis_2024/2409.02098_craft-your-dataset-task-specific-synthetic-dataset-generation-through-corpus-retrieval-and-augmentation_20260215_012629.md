---
ver: rpa2
title: 'CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus
  Retrieval and Augmentation'
arxiv_id: '2409.02098'
source_url: https://arxiv.org/abs/2409.02098
tags:
- task
- samples
- data
- craft
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CRAFT is a method for generating task-specific synthetic datasets
  from large-scale web corpora. Given a small set of user-written few-shot examples,
  CRAFT retrieves relevant human-written documents using similarity-based retrieval
  and then uses instruction-tuned LLMs to transform them into task-specific samples.
---

# CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation

## Quick Facts
- arXiv ID: 2409.02098
- Source URL: https://arxiv.org/abs/2409.02098
- Reference count: 35
- Models trained on CRAFT-generated datasets consistently outperformed or matched general LLMs on QA tasks, and summarization models trained on CRAFT data exceeded those trained on human-curated datasets by 46 preference points

## Executive Summary
CRAFT is a method for generating task-specific synthetic datasets from large-scale web corpora. Given a small set of user-written few-shot examples, CRAFT retrieves relevant human-written documents using similarity-based retrieval and then uses instruction-tuned LLMs to transform them into task-specific samples. This approach was evaluated across four diverse tasks: biology, medicine, commonsense question-answering, and summarization. Models trained on CRAFT-generated datasets consistently outperformed or matched general LLMs on QA tasks, and summarization models trained on CRAFT data exceeded those trained on human-curated datasets by 46 preference points. The method also showed strong out-of-domain generalization and required minimal human effort, though scaling issues were observed in one task (recipe generation) due to declining data quality.

## Method Summary
CRAFT generates task-specific synthetic datasets by first embedding user-provided few-shot examples and retrieving similar documents from large-scale web corpora using cosine similarity. The retrieved documents are then combined with the few-shots in a prompt template and fed to an instruction-tuned LLM (Mistral 7B Instruct) to generate task-specific samples in the desired format. The synthetic data undergoes quality control through deduplication, format validation, and similarity filtering before being used to fine-tune models via LoRA adapters. The method requires minimal human effort beyond writing few-shot examples and leverages existing web corpora rather than generating all content synthetically.

## Key Results
- Models trained on CRAFT-generated datasets showed improvements of 17% (BioQA), 12% (CSQA), 23% (MedQA), and 124% (Summarization) over few-shot baselines
- Summarization models trained on CRAFT data outperformed those trained on human-curated datasets by 46 preference points
- CRAFT demonstrates strong out-of-domain generalization capabilities, with a model trained on biology data achieving state-of-the-art performance on unseen tasks
- Quality control mechanisms effectively remove duplicates and format errors, though scaling issues emerged in recipe generation due to declining data quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Similarity-based retrieval finds relevant human-written documents that can be transformed into task-specific samples.
- Mechanism: The embedding database stores pre-computed embeddings of large-scale web corpora. Few-shot examples are embedded and used to retrieve top-k most similar documents based on cosine similarity. This ensures retrieved documents are semantically relevant to the task.
- Core assumption: The embedding space preserves semantic similarity between documents and task requirements.
- Evidence anchors:
  - [abstract]: "Given these examples, CRAFT uses large-scale public web-crawled corpora and similarity-based document retrieval to find other relevant human-written documents."
  - [section 3.4]: "Relevant text documents that contain similar latent features as the few-shots are retrieved from the corpora by calculating similarity scores based on the embedded few-shots and corpus samples."
  - [corpus]: Evidence is strong - the paper uses multiple diverse corpora (C4, Wikipedia, Stack Exchange, WikiHow) and reports successful retrieval across tasks.
- Break condition: If the embedding space does not capture task-relevant semantic features, or if corpora lack diversity, retrieval will fail to find appropriate documents.

### Mechanism 2
- Claim: Instruction-tuned LLMs can transform retrieved free-text documents into properly formatted task samples.
- Mechanism: The retrieved documents are combined with few-shot examples in a prompt template. The LLM is instructed to extract relevant information from the corpus sample and reformat it into the desired task format (e.g., question-answer pairs).
- Core assumption: Instruction-tuned LLMs have learned to follow formatting instructions and can extract task-relevant content from unstructured text.
- Evidence anchors:
  - [abstract]: "instruction-tuned large language models (LLMs) augment the retrieved documents into custom-formatted task samples"
  - [section 3.5]: "instruction-tuning prompt templates... augment the free-text documents into task-specific training data while simultaneously eliminating noise"
  - [corpus]: Evidence is strong - the paper shows this process works across multiple diverse tasks with consistent quality improvements.
- Break condition: If the LLM cannot follow the formatting instructions or if the corpus samples are too noisy, the transformation will fail.

### Mechanism 3
- Claim: Models trained on CRAFT-generated datasets can match or exceed performance of models trained on human-curated data.
- Mechanism: The synthetic datasets created through CRAFT's pipeline provide task-specific training data that captures domain knowledge and task patterns. Fine-tuning on this data adapts models to perform well on the target tasks.
- Core assumption: The synthetic data generated by CRAFT is of sufficient quality and diversity to effectively train models.
- Evidence anchors:
  - [abstract]: "models trained on CRAFT-generated datasets consistently outperformed or matched general LLMs on QA tasks, and summarization models trained on CRAFT data exceeded those trained on human-curated datasets by 46 preference points"
  - [section 5.2]: "models trained with only few-shot examples... improvements of 17% (from 66.9 to 78.1), 12% (from 55.3 to 62.1), 23% (from 39.1 to 48.0), and 124% (from 43.7 to 97.9) for BioQA, CSQA, MedQA, and Summarization"
  - [corpus]: Evidence is strong - the paper shows CRAFT outperforms or matches baselines across four diverse tasks and shows OOD generalization capabilities.
- Break condition: If the synthetic data quality degrades significantly with scale, or if the retrieval fails to capture task-relevant patterns, model performance will suffer.

## Foundational Learning

- Concept: Cosine similarity in embedding space
  - Why needed here: The retrieval mechanism relies on calculating cosine similarity between few-shot embeddings and corpus document embeddings to find relevant documents.
  - Quick check question: What does a cosine similarity score of 0.8 between two document embeddings indicate about their semantic relationship?

- Concept: Instruction tuning and in-context learning
  - Why needed here: CRAFT uses instruction-tuned LLMs to transform retrieved documents into task samples, and the transformation process relies on in-context learning with few-shot examples.
  - Quick check question: How does providing few-shot examples in the prompt help an instruction-tuned LLM generate properly formatted task samples?

- Concept: LoRA (Low-Rank Adaptation) fine-tuning
  - Why needed here: CRAFT uses LoRA to efficiently fine-tune large language models on the synthetic datasets, allowing adaptation without full model retraining.
  - Quick check question: What is the advantage of using LoRA over full fine-tuning when adapting large language models to new tasks?

## Architecture Onboarding

- Component map:
  Few-shot examples -> Embedding database -> Document retrieval system -> Task template generator -> Instruction-tuned LLM -> Fine-tuning pipeline -> Quality control

- Critical path:
  1. User provides few-shot examples
  2. Few-shots are embedded and used to retrieve similar corpus documents
  3. Retrieved documents are formatted into task templates with few-shots
  4. LLM augments templates into synthetic task samples
  5. Quality control filters and deduplicates samples
  6. Synthetic dataset is used to fine-tune a model via LoRA

- Design tradeoffs:
  - Retrieval vs generation: CRAFT retrieves human-written documents rather than generating everything synthetically, trading computational cost for quality
  - Quality vs quantity: The filtering process removes duplicates and low-quality samples, potentially reducing dataset size but improving quality
  - Embedding database size: Larger databases provide more diverse documents but increase storage and retrieval costs

- Failure signatures:
  - Low diversity in retrieved documents suggests corpus lacks coverage or embedding space doesn't capture task semantics
  - High format error rates indicate LLM struggles with instruction-following or task complexity
  - Poor model performance despite good synthetic data suggests retrieval is finding wrong type of documents or LLM transformation is losing important information

- First 3 experiments:
  1. Test retrieval quality: Provide few-shots for a simple task and manually inspect top-10 retrieved documents for relevance
  2. Test LLM transformation: Run the task template generation on a single retrieved document and verify the output format matches expectations
  3. Test end-to-end pipeline: Generate a small synthetic dataset (100 samples) and fine-tune a model to verify the full pipeline works before scaling up

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability concerns: Performance degradation when scaling synthetic dataset size, particularly for recipe generation where duplicate detection failed to prevent quality collapse
- Task generality: Limited evaluation scope covering only four diverse tasks, effectiveness on other task types remains untested
- Quality control effectiveness: Current pipeline relies on similarity filtering, format validation, and duplicate detection, but the exact false positive/negative rates are not reported

## Confidence

**High confidence** (Multiple strong evidence anchors, consistent results):
- The retrieval mechanism using embedding similarity finds relevant documents for transformation
- Instruction-tuned LLMs can transform retrieved documents into task-specific formats
- Models trained on CRAFT data show consistent improvements over few-shot baselines

**Medium confidence** (Evidence exists but with caveats):
- CRAFT matches or exceeds performance of general LLMs on QA tasks (limited to 4 tasks, some results show only marginal improvements)
- CRAFT outperforms human-curated data on summarization (only one task comparison, no ablation on summarization-specific factors)
- Out-of-domain generalization capabilities (only tested on summarization model applied to unseen tasks)

**Low confidence** (Limited evidence or single data points):
- Scalability limitations are fully understood (only one task shows degradation, no systematic study of when/why scaling fails)
- Quality control pipeline effectiveness (no detailed error analysis of what types of errors persist)

## Next Checks
1. **Scaling behavior analysis**: Generate synthetic datasets of increasing sizes (1k, 10k, 100k samples) for multiple tasks and track performance degradation patterns. This would validate whether the scaling issues observed in recipe generation generalize to other task types.

2. **Quality control ablation**: Remove individual quality control components (duplicate detection, similarity filtering, format validation) and measure their individual impact on final model performance and dataset quality. This would quantify the effectiveness of each control mechanism.

3. **Cross-task retrieval effectiveness**: For each task, measure the semantic similarity between few-shot examples and their top-10 retrieved documents, then correlate these similarity scores with downstream model performance. This would validate whether retrieval quality directly predicts task success.