---
ver: rpa2
title: Efficient Quantum One-Class Support Vector Machines for Anomaly Detection Using
  Randomized Measurements and Variable Subsampling
arxiv_id: '2407.20753'
source_url: https://arxiv.org/abs/2407.20753
tags:
- data
- quantum
- kernel
- measurements
- randomized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficient quantum anomaly
  detection using one-class support vector machines (OC-SVMs) for large datasets.
  The authors propose combining randomized measurements kernels, variable subsampling
  ensembles, and rotated feature bagging to achieve linear time complexity both with
  respect to data size and number of features.
---

# Efficient Quantum One-Class Support Vector Machines for Anomaly Detection Using Randomized Measurements and Variable Subsampling

## Quick Facts
- arXiv ID: 2407.20753
- Source URL: https://arxiv.org/abs/2407.20753
- Reference count: 23
- The paper demonstrates that combining randomized measurements kernels, variable subsampling ensembles, and rotated feature bagging achieves linear time complexity while maintaining or improving average precision for quantum anomaly detection.

## Executive Summary
This paper addresses the challenge of efficient quantum anomaly detection using one-class support vector machines (OC-SVMs) for large datasets. The authors propose combining randomized measurements kernels, variable subsampling ensembles, and rotated feature bagging to achieve linear time complexity both with respect to data size and number of features. Their method significantly reduces training and testing times while maintaining or improving average precision compared to classical and quantum baselines. The variable subsampling with rotated feature bagging using randomized measurements kernels (VS-RFB-RM) achieves the highest average precision (up to 0.25-0.35) and fastest computation times, reducing training time by up to 90% and testing time by up to 80% compared to quantum inversion test methods.

## Method Summary
The authors combine three techniques to create efficient quantum anomaly detection: randomized measurements kernels that sample in random bases to approximate fidelity calculations, variable subsampling ensembles that train on differently sized subsets to reduce variance, and rotated feature bagging that projects high-dimensional data onto lower-dimensional random orthogonal axes. The method preprocesses data with PCA dimensionality reduction and specific scaling (1/√M for randomized measurements), implements quantum feature maps (IQP-like), and trains OC-SVM ensembles with variable subsampling (c = ⌊n/100⌋ components) and rotated feature bagging. The randomized measurements kernel uses r=30 measurement settings with s=9000 shots per setting, while feature bagging reduces dimension from d to r′ = 2 + ⌈√d/2⌉ using random rotations from Gram-Schmidt on uniform [-1,1].

## Key Results
- VS-RFB-RM achieves highest average precision (0.25-0.35) while reducing training time by up to 90% and testing time by up to 80% compared to quantum inversion test methods
- The approach successfully mitigates exponential time complexity of randomized measurements kernels, making them suitable for high-dimensional datasets
- Variable subsampling ensembles demonstrate notable improvements in computational efficiency and reduction in variance compared to single-model approaches

## Why This Works (Mechanism)

### Mechanism 1
Randomized Measurements (RM) kernels reduce measurement redundancy by sampling in random bases and aggregating via classical post-processing. By replacing exact fidelity calculation with statistical estimates from multiple random basis rotations, RM kernels avoid the O(n²) complexity of inversion tests while preserving discriminative power. The core assumption is that random basis sampling approximates fidelity well enough for OC-SVM decision boundaries to remain stable. Evidence shows RM kernels enable faster kernel measurement but could only approximate Radial Basis Function (RBF) kernels. If variance from sampling overwhelms signal in high-dimensional data or low-shot regimes, model performance degrades sharply.

### Mechanism 2
Variable Subsampling ensembles trade off between bias and variance by training on differently sized subsets, implicitly sampling the data-size hyperparameter space. Each ensemble component uses a distinct subsample size ni, leading to varying support vector counts and decision boundaries; combined scores reduce variance while maintaining accuracy. The core assumption is that anomaly detection performance benefits from implicit sampling over data-size hyperparameter space without explicit grid search. Evidence shows variable subsampling approaches demonstrate notable improvements in computational efficiency and a reduction in variance. If subsampling size range is too narrow or c too small, ensemble cannot capture diversity needed to reduce variance.

### Mechanism 3
Rotated Feature Bagging projects high-dimensional data onto lower-dimensional random orthogonal axes, reducing qubit count while preserving correlations. Multiplying each subsample Di by a random rotation matrix Ei reduces dimension from d to r′ = 2 + ⌈√d/2⌉, allowing RM kernels to run on fewer qubits without losing separability. The core assumption is that real data correlations allow effective low-dimensional representation; random rotations sufficiently span the relevant subspace. Evidence shows that with original subsample data matrix Di of size ni × d and random projection matrix Ei of size d × r′, the resulting sample data matrix D′i = Di · Ei has size ni × r′. If data is inherently high-rank or noise-dominated, random projections collapse discriminative structure, hurting precision.

## Foundational Learning

- **Concept**: Fidelity measurement via swap vs inversion tests
  - Why needed here: Different kernel evaluation methods have distinct time and resource trade-offs; choosing impacts scalability.
  - Quick check question: Which kernel evaluation method requires twice the circuit width but works for mixed states?

- **Concept**: Ensemble score aggregation (max vs average)
  - Why needed here: Aggregation choice affects bias-variance trade-off; critical for ensemble stability.
  - Quick check question: What aggregation reduces bias but increases variance in OC-SVM ensembles?

- **Concept**: Randomized Measurements error mitigation
  - Why needed here: Mitigates statistical error in fidelity estimates; essential for stable performance at scale.
  - Quick check question: What correction formula uses purity estimates to stabilize RM kernel values?

## Architecture Onboarding

- **Component map**: Data preprocessing → PCA + scaling (RBF) or angle scaling (IT) or dual scaling (RM) → Ensemble builder → sample ni ∈ [50,100], c = ⌊n/100⌋ → Kernel evaluator → IT (1000 shots) or RM (r=30, s=9000 shots) → Model trainer → OC-SVM per subsample → Scorer → aggregate via max or average → Projection (RFB) → Di · Ei, Ei from Gram-Schmidt on uniform [-1,1]
- **Critical path**: Preprocessing → Ensemble component loop → Kernel eval → OC-SVM train → Aggregate scores
- **Design tradeoffs**: More components (higher c) → lower variance but O(c·n²) training cost; Larger ni range → better bias-variance but slower per-component; Unmitigated RM → higher precision, higher variance; mitigated → lower variance, lower precision
- **Failure signatures**: Exponential blow-up in training time → RM without RFB on high d; High variance in predictions → insufficient ensemble size or unmitigated RM; Poor precision → over-aggressive subsampling or bad rotation projections
- **First 3 experiments**: 1) Single OC-SVM with IT on synthetic 2D, measure precision/recall; 2) Single OC-SVM with unmitigated RM on same data, compare variance; 3) VS-IT ensemble with c=3, ni∈[50,100], compare training time vs single

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of the VS-RFB-RM method scale when using a higher number of ensemble components and larger maximum subsample sizes?
  - Basis in paper: The paper suggests that increasing the number of ensemble components and maximum subsample sizes could further improve the results of the VS-RFB-RM method.
  - Why unresolved: The experiments conducted in the paper used a fixed number of ensemble components and maximum subsample sizes, which may have limited the full potential of the VS-RFB-RM method.
  - What evidence would resolve it: Conducting experiments with varying numbers of ensemble components and maximum subsample sizes, and comparing the performance (average precision, variance, training/testing times) of the VS-RFB-RM method under these different configurations.

- **Open Question 2**: How does the performance of the VS-RFB-RM method compare to other kernel approximation methods, such as DISC and Block Basis Factorization, on high-dimensional datasets?
  - Basis in paper: The paper mentions that the VS-RFB-RM method successfully mitigates the exponential time complexity of the Randomized Measurements kernel, making it suitable for high-dimensional datasets. However, it does not compare the performance of VS-RFB-RM to other kernel approximation methods on such datasets.
  - Why unresolved: The paper does not provide a direct comparison between the VS-RFB-RM method and other kernel approximation methods, such as DISC and Block Basis Factorization, on high-dimensional datasets.
  - What evidence would resolve it: Conducting experiments on high-dimensional datasets using the VS-RFB-RM method and other kernel approximation methods (DISC and Block Basis Factorization), and comparing their performance (average precision, variance, training/testing times) on these datasets.

- **Open Question 3**: How does the performance of the VS-RFB-RM method change when using alternative feature maps, including learnable ones, instead of the IQP-like feature map?
  - Basis in paper: The paper suggests that future research could explore the use of alternative feature maps, including learnable ones, with the proposed methods.
  - Why unresolved: The experiments conducted in the paper used the IQP-like feature map, and the performance of the VS-RFB-RM method with alternative feature maps is unknown.
  - What evidence would resolve it: Conducting experiments using the VS-RFB-RM method with different feature maps, including learnable ones, and comparing their performance (average precision, variance, training/testing times) to the results obtained with the IQP-like feature map.

## Limitations
- High variance in performance across datasets raises Medium confidence in claims of general robustness
- Methodology relies on specific hyperparameter choices (c = ⌊n/100⌋, ni ∈ [50, 100], r′ = 2 + ⌈√d/2⌉) that may not transfer optimally to all domains
- Empirical scaling beyond tested ranges (n up to 1500, d up to 31) remains unverified

## Confidence
- Claims of linear complexity: High
- Claims of reduced training/testing time: High
- Claims of maintained/improved precision: Medium
- Claims of reduced variance via ensembles: Medium

## Next Checks
1. Systematically vary c and ni ranges to verify claimed variance reduction is not dataset-specific.
2. Test on higher-dimensional datasets (d > 100) to validate RFB scalability and information preservation.
3. Conduct ablation studies isolating the impact of each technique (RM, VS, RFB) on both precision and variance.