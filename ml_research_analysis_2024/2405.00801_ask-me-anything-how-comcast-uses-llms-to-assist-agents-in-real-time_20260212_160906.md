---
ver: rpa2
title: '"Ask Me Anything": How Comcast Uses LLMs to Assist Agents in Real Time'
arxiv_id: '2405.00801'
source_url: https://arxiv.org/abs/2405.00801
tags:
- agents
- answer
- customer
- retrieval
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Comcast developed "Ask Me Anything" (AMA), an AI-assisted feature
  for customer service agents that leverages large language models (LLMs) and retrieval-augmented
  generation (RAG) to provide real-time answers to agent queries. By indexing internal
  knowledge sources, retrieving relevant text snippets, and generating contextually
  accurate responses with citations, AMA reduces the need for agents to context-switch
  between tools.
---

# "Ask Me Anything": How Comcast Uses LLMs to Assist Agents in Real Time

## Quick Facts
- arXiv ID: 2405.00801
- Source URL: https://arxiv.org/abs/2405.00801
- Reference count: 28
- Comcast developed AMA to reduce agent context-switching and improve customer service efficiency using RAG and LLMs.

## Executive Summary
Comcast developed "Ask Me Anything" (AMA), an AI-assisted feature for customer service agents that leverages large language models (LLMs) and retrieval-augmented generation (RAG) to provide real-time answers to agent queries. By indexing internal knowledge sources, retrieving relevant text snippets, and generating contextually accurate responses with citations, AMA reduces the need for agents to context-switch between tools. In internal experiments, agents using AMA spent approximately 10% fewer seconds per conversation containing a search, translating to significant annual savings. Agents also provided positive feedback nearly 80% of the time, demonstrating the system's usefulness. An A/B test confirmed improvements in answer quality and positive feedback rates, validating AMA's effectiveness in enhancing customer service efficiency.

## Method Summary
AMA uses a Haystack-based pipeline to preprocess and chunk internal knowledge articles, embed them with OpenAI's ada-002, and store them in a vector database. When agents query, the system retrieves relevant chunks using dense retrieval, optionally re-ranks them with a fine-tuned MPNet model trained on synthetic data, and generates answers using gpt-3.5-turbo with explicit citation extraction. The system enforces citation-based answers and evaluates quality using GPT-4 as a judge. Experiments show improved retrieval and answer quality, with positive feedback from agents in production.

## Key Results
- Agents using AMA spent ~10% fewer seconds per conversation containing a search.
- Agents provided positive feedback nearly 80% of the time.
- A/B testing confirmed improvements in answer quality and positive feedback rates.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG with dense embeddings reduces context switching by delivering precise answers directly in the agent's workflow.
- Mechanism: The system indexes internal knowledge as embedded text chunks, retrieves the most relevant ones using OpenAI's ada-002 embeddings, and feeds them into an LLM to generate concise, cited answers. This eliminates the need for agents to leave their conversation interface to search manually.
- Core assumption: Dense embeddings (ada-002) capture semantic relevance better than sparse models like BM25 for proprietary, domain-specific queries.
- Evidence anchors:
  - [abstract] "AMA allows agents to ask questions to a large language model (LLM) on demand... the LLM provides accurate responses in real-time, reducing the amount of context switching the agent needs."
  - [section] "Mirroring prior work [23], we found that BM25 remains a strong baseline, outperforming DPR in R@3 and MRR... We observe MPNet-base (v1), OpenAI's ada-002, and MPNet-base (v2) to perform similarly."
  - [corpus] Weak—no directly comparable systems cited, but the methodology aligns with general RAG patterns in the literature.
- Break condition: If the retriever fails to surface relevant chunks, the LLM will produce poor or no answers, increasing agent frustration and reverting to manual search.

### Mechanism 2
- Claim: Finetuned reranking using synthetic data improves retrieval precision, which directly improves answer quality.
- Mechanism: GPT-4 generates synthetic questions from each text snippet; the system retrieves top-20 results and ranks them using BGE-reranker-large. A distilled MPNet model is then trained via RankNet to reorder results, prioritizing the source snippet and top semantically relevant ones.
- Core assumption: Synthetic question generation plus relevance ranking can approximate real user queries and improve downstream answer accuracy.
- Evidence anchors:
  - [section] "We found that reranking results using models finetuned on synthetic data improved the retrieval step... The final dataset after constructing the necessary pairs for RankNet consisted of over 10 million examples."
  - [section] "We observe that using reranked documents, LLM is able to achieve a higher answer quality... The improvement can also be explained by the increased Citation Match Rate and Recall@3 from the reranked documents."
  - [corpus] No explicit comparable reranking work cited, but the synthetic-data approach aligns with common IR augmentation techniques.
- Break condition: If synthetic data generation or reranking model fails, retrieval quality drops, causing LLM to cite irrelevant or incorrect sources.

### Mechanism 3
- Claim: LLM-as-a-judge evaluation and citation enforcement create measurable feedback loops that improve both system quality and agent trust.
- Mechanism: GPT-4 evaluates generated answers against ground-truth annotations, providing an "Answer Quality" score. The system prompts the LLM to cite sources explicitly, and only returns answers with citations, ensuring traceability and reducing hallucinations.
- Core assumption: LLM-based evaluation can reliably assess answer correctness and citation relevance for internal customer service knowledge.
- Evidence anchors:
  - [section] "To evaluate the system's responses, we follow the LLM-as-a-judge methodology [25], in addition to metrics around retrieval quality... GPT-4 was used to evaluate the overall quality of each document to the question."
  - [section] "As a final step in our prompt, we ask the LLM to answer the given question using the search results... If no citations were found, then the system would not return the answer."
  - [corpus] No comparable LLM-as-judge studies cited, but this is a known evaluation approach in recent IR literature.
- Break condition: If LLM-as-a-judge fails to discriminate answer quality, the system may falsely validate incorrect answers, eroding agent trust.

## Foundational Learning

- Concept: Dense vs. sparse retrieval models (e.g., DPR vs. BM25)
  - Why needed here: Choosing the right retriever directly impacts retrieval recall and downstream answer quality; the paper compares both to justify using dense embeddings.
  - Quick check question: What metric did the authors use to compare retriever performance, and which model won for their use case?
- Concept: Synthetic data generation for training rerankers
  - Why needed here: The reranker is trained on millions of synthetic question-passage pairs; understanding this pipeline is key to reproducing or improving it.
  - Quick check question: How does the synthetic data pipeline ensure that the source snippet appears in the top-20 retrieved results?
- Concept: LLM-as-a-judge evaluation methodology
  - Why needed here: The system relies on GPT-4 to score answer quality and citation match; understanding this is crucial for interpreting offline evaluation results.
  - Quick check question: What is the scoring range used by GPT-4 in the answer quality evaluation, and how is "Citation Match Rate" computed?

## Architecture Onboarding

- Component map:
  - Document preprocessor → Haystack chunker → Vector DB (ADA embeddings) → Retriever (ADA) → Reranker (MPNet finetuned) → Reader LLM (gpt-3.5-turbo) → Answer + Citations
- Critical path: Document chunking → Embedding storage → Query embedding → Top-K retrieval → Reranking → LLM generation → Citation validation
- Design tradeoffs:
  - Embedding choice: ADA offers high performance and low cost vs. training a custom model; tradeoff is vendor lock-in and dependency on OpenAI API.
  - Chunk size: Larger chunks improve context but risk truncation; the paper uses `max_chars_check=3000` after experiments.
  - Reranker vs. pure retriever: Reranker adds latency but improves relevance; acceptable given QA accuracy gains.
- Failure signatures:
  - Low recall@3: retriever embeddings not capturing semantics → check embedding model or chunk size.
  - High "No Answer Rate": LLM not finding citations → check reranker performance or prompt formatting.
  - Low citation match: reranker misordering or prompt not enforcing citations → inspect synthetic data generation or citation rail prompt.
- First 3 experiments:
  1. Vary `max_chars_check` in Haystack preprocessor and measure answer quality vs. MRR trade-off.
  2. Compare BM25 vs. ada-002 embeddings on a held-out set of real agent queries.
  3. Test reranker impact by running A/B on production traffic, measuring no-answer rate and positive feedback rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal chunking strategy for balancing retrieval quality and computational efficiency in RAG systems?
- Basis in paper: [explicit] The paper discusses various chunking parameters (split_length, split_overlap, max_chars_check) and their impact on metrics like answer quality and MRR, but doesn't identify an optimal configuration.
- Why unresolved: The experiments show trade-offs between different parameters, with no clear winner across all metrics. The choice seems to depend on specific use cases and resource constraints.
- What evidence would resolve it: Systematic ablation studies across diverse document types and query distributions, with comprehensive evaluation of both retrieval quality and computational costs.

### Open Question 2
- Question: How does the performance of RAG systems degrade with increasing document repository size, and what scaling strategies are effective?
- Basis in paper: [inferred] While the paper demonstrates successful deployment at scale, it doesn't address performance degradation or scaling challenges as the knowledge base grows.
- Why unresolved: Large-scale deployment experiences may vary significantly depending on infrastructure, query patterns, and document characteristics. The paper focuses on initial implementation rather than long-term scaling.
- What evidence would resolve it: Longitudinal studies tracking performance metrics over time as the knowledge base expands, testing different indexing strategies and hardware configurations.

### Open Question 3
- Question: What are the key factors that determine when agents prefer RAG-based assistance over traditional search tools?
- Basis in paper: [explicit] The paper mentions agents using AMA for two-thirds of typed queries, but doesn't deeply analyze the factors influencing this preference.
- Why unresolved: The paper provides usage statistics but lacks detailed qualitative analysis of agent decision-making processes and the specific scenarios where each tool excels.
- What evidence would resolve it: Mixed-methods studies combining quantitative usage data with qualitative interviews and observations of agent behavior in different task contexts.

## Limitations
- The study relies heavily on internal Comcast data and GPT-4 evaluation, which may not generalize to other customer service contexts or domains with different knowledge structures.
- The reranker was trained on synthetic data rather than real agent queries, introducing potential distribution shift that could impact performance in production.
- Citation enforcement, while improving traceability, may constrain answer quality if the retriever fails to surface relevant sources.

## Confidence
- High confidence: The reduction in handle time and positive agent feedback are directly measurable from A/B test data and agent surveys.
- Medium confidence: The retrieval and reranking mechanisms are well-documented and validated against standard IR metrics, but the synthetic training approach for the reranker introduces some uncertainty.
- Low confidence: The LLM-as-a-judge evaluation methodology, while commonly used, lacks direct human-annotated ground truth for answer correctness in the internal evaluation.

## Next Checks
1. Test the system with a held-out set of real agent queries to measure performance degradation compared to synthetic training data.
2. Conduct a controlled experiment varying chunk sizes and retriever models to identify optimal configurations for different query types.
3. Implement human evaluation of a random sample of answers to validate GPT-4's scoring and identify potential systematic biases in the evaluation process.