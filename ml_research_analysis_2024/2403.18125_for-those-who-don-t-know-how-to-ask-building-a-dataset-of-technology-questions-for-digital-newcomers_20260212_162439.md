---
ver: rpa2
title: 'For those who don''t know (how) to ask: Building a dataset of technology questions
  for digital newcomers'
arxiv_id: '2403.18125'
source_url: https://arxiv.org/abs/2403.18125
tags:
- digital
- questions
- will
- dataset
- tutoring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes building a dataset of technology questions from
  digital newcomers to improve LLM chatbot tutoring systems. It outlines collecting
  data from online forums, a decade of one-on-one tutoring sessions, and new recorded
  tutoring interactions.
---

# For those who don't know (how) to ask: Building a dataset of technology questions for digital newcomers

## Quick Facts
- arXiv ID: 2403.18125
- Source URL: https://arxiv.org/abs/2403.18125
- Reference count: 3
- Proposes building dataset of technology questions from digital newcomers to improve LLM tutoring systems

## Executive Summary
This paper addresses the challenge of building effective LLM-based tutoring systems for digital newcomers by creating a specialized dataset of technology questions. The authors identify that current LLMs struggle with unclear or nonstandard questions typical of learners with limited digital literacy, and existing public datasets lack sufficient examples of how these learners frame their questions. The proposed solution involves collecting data from online forums, a decade of one-on-one tutoring sessions, and new recorded tutoring interactions to better understand the question patterns of digital newcomers and develop strategies for improving chatbot responses.

## Method Summary
The research methodology involves three primary data collection approaches: mining online forums for technology questions from digital newcomers, analyzing a decade of recorded one-on-one tutoring sessions, and conducting new recorded tutoring interactions. The dataset will be used to study how unclear or nonstandard questions affect LLM outputs and to develop strategies for detecting poor questions, classifying learner competency levels, and tailoring responses using pedagogically motivated rhetorical techniques from personal tutoring. The authors plan to make the dataset publicly available to support further research into educational chatbots and digital literacy.

## Key Results
- Dataset construction will combine online forum data, tutoring session records, and new recorded interactions
- Focus on studying how unclear questions from digital newcomers affect LLM outputs
- Goal to develop competency classification systems and pedagogically motivated response strategies
- Dataset will be made publicly available for educational chatbot research

## Why This Works (Mechanism)
The approach works by addressing a fundamental gap in LLM training data - the lack of representation for how digital newcomers actually ask questions. By creating a specialized dataset that captures the linguistic patterns and question framing of learners with limited digital literacy, the research aims to improve chatbot understanding and response generation for this underserved population. The methodology leverages both existing data sources and new data collection to build a comprehensive picture of digital newcomer question patterns.

## Foundational Learning
- Digital literacy concepts: Understanding the spectrum of digital competence and what constitutes a "digital newcomer" is essential for properly framing the research problem and interpreting the data.
- Question quality assessment: Developing methods to evaluate question clarity and standardness is crucial for the dataset's utility in improving LLM responses.
- Pedagogical techniques in tutoring: Knowledge of effective tutoring strategies helps inform how responses should be tailored for different competency levels.
- Competency classification frameworks: Creating reliable methods to categorize learner skill levels enables more personalized and effective chatbot interactions.

## Architecture Onboarding
- Component map: Online forums -> Question extraction -> Tutoring session analysis -> New data collection -> Dataset assembly -> LLM training/evaluation
- Critical path: Data collection and preprocessing -> Question quality analysis -> Competency classification development -> Response strategy formulation
- Design tradeoffs: Balancing breadth of question types with depth of analysis, choosing between automated and manual data processing methods, determining appropriate level of detail for competency classification
- Failure signatures: Inadequate question representation in training data leading to poor chatbot performance, misclassification of competency levels resulting in inappropriate responses, overly complex classification systems that are difficult to implement
- First experiments: 1) Pilot data collection to assess question prevalence across forums, 2) Initial question quality analysis on small dataset sample, 3) Basic competency classification testing on subset of tutoring session data

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions beyond the general research objectives.

## Limitations
- Sampling bias toward more motivated learners who seek help publicly or engage in tutoring
- Uncertainty about capturing questions from learners who disengage entirely
- Effectiveness of pedagogically motivated techniques in LLM responses remains speculative
- Assumption that current LLM training data adequately represents digital newcomer linguistic patterns

## Confidence
- Dataset construction approach: Medium
- Identified problem of digital literacy gaps: High
- Impact of unclear questions on LLM outputs: High
- Competency classification framework generalizability: Medium

## Next Checks
1. Conduct pilot data collection across multiple online forums to validate question prevalence and characteristics
2. Implement small-scale study comparing LLM responses to clear versus unclear questions from digital newcomers
3. Develop and test proposed competency classification framework on collected data subset to evaluate reliability across learner populations