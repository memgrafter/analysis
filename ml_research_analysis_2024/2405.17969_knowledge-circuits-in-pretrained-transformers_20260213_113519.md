---
ver: rpa2
title: Knowledge Circuits in Pretrained Transformers
arxiv_id: '2405.17969'
source_url: https://arxiv.org/abs/2405.17969
tags:
- knowledge
- language
- circuit
- attention
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces knowledge circuits, a novel perspective for
  analyzing how transformers store and express knowledge. Unlike prior work focusing
  on isolated components like MLPs or attention heads, the authors identify critical
  subgraphs in the model's computation graph responsible for specific factual knowledge.
---

# Knowledge Circuits in Pretrained Transformers

## Quick Facts
- arXiv ID: 2405.17969
- Source URL: https://arxiv.org/abs/2405.17969
- Reference count: 40
- Primary result: Introduces knowledge circuits that explain how transformers store and express factual knowledge through collaborative attention head and MLP interactions

## Executive Summary
This paper introduces knowledge circuits, a novel framework for analyzing how transformers store and express knowledge. Unlike prior work focusing on isolated components, the authors identify critical subgraphs in the model's computation graph responsible for specific factual knowledge. Using GPT2 and TinyLLAMA, they discover circuits where attention heads and MLPs collaboratively encode knowledge, with mover heads transferring information to the final token position and relation heads capturing contextual relationships. The circuits can independently reproduce model performance with ~70% accuracy while using <10% of the original nodes.

The work demonstrates that ROME editing affects circuits primarily at edited layers while FT-M directly injects knowledge into specific components. Knowledge circuits also explain model behaviors like hallucinations (failed information transfer) and in-context learning (emergence of new attention heads). This provides new insights into transformer knowledge mechanisms and suggests improved approaches for knowledge editing.

## Method Summary
The authors use Automatic Circuit Discovery toolkit with zero ablation and MatchNLL loss to systematically identify critical subgraphs in transformer computation graphs. They construct knowledge circuits by abating edges and nodes, selecting components whose removal significantly impacts knowledge expression. The method projects edge importance through layer normalization and unembedding matrices to measure each component's contribution to final predictions. Circuit completeness is evaluated by testing standalone circuit performance on Hit@10 metric compared to original model performance.

## Key Results
- Knowledge circuits can independently reproduce model performance with ~70% accuracy while using <10% of the original nodes
- ROME editing affects circuits primarily at edited layers, while FT-M directly injects knowledge into specific components
- Hallucinations occur when circuits fail to correctly transfer knowledge to final token in earlier layers
- Knowledge circuits explain in-context learning through emergence of new attention heads

## Why This Works (Mechanism)

### Mechanism 1
Knowledge circuits act as collaborative subgraphs where attention heads and MLPs work together to encode and express specific factual knowledge. Information flows through mover heads that extract relevant subject data, relation heads that capture contextual relationships, and MLPs that aggregate and prioritize target entities. The circuit enables independent reproduction of model performance using <10% of original nodes. Core assumption: Knowledge storage requires coordinated component interaction rather than isolated neuron activation.

### Mechanism 2
ROME editing affects circuits primarily at edited layers while FT-M directly injects knowledge into specific components. ROME adds edited information at subject position, which mover heads then transport through subsequent layers. FT-M writes knowledge directly into edited components, dominating downstream predictions. Core assumption: Editing effectiveness depends on how information propagates through circuit components.

### Mechanism 3
Hallucinations occur when circuits fail to correctly transfer knowledge to final token in earlier layers. When hallucination occurs, the circuit lacks effective mover head or mover head selects incorrect information, preventing proper knowledge aggregation and prioritization. Core assumption: Hallucinations result from circuit-level failures rather than individual component malfunctions.

## Foundational Learning

- Concept: Computation graph representation of transformers
  - Why needed here: Understanding how attention heads, MLPs, and embeddings interact as nodes in a directed acyclic graph is essential for identifying knowledge circuits.
  - Quick check question: What components make up the nodes in a transformer's computation graph, and how do they connect?

- Concept: Causal mediation analysis
  - Why needed here: The method systematically ablates edges and nodes to determine their importance in knowledge expression, enabling circuit discovery.
  - Quick check question: How does zero ablation help identify critical edges in a computation graph?

- Concept: Residual stream mechanics
  - Why needed here: Knowledge aggregation and prioritization occur through residual connections, with information flowing from earlier to later layers.
  - Quick check question: How do residual connections allow information from multiple layers to combine in transformer architectures?

## Architecture Onboarding

- Component map: Input embedding node (I) → Attention heads (Al,j) - mover heads, relation heads, mixture heads → MLP layers (Ml) - key-value memory storage → Output node (O) - final prediction

- Critical path: Input → Attention heads (mover and relation) → MLPs (aggregation) → Output
  The mover heads extract subject information, relation heads capture contextual relationships, and MLPs prioritize target entities.

- Design tradeoffs: Coarse vs fine-grained analysis
  - Coarse: Circuit-level analysis (used in this work) - faster but may miss neuron-level interactions
  - Fine: Neuron-level analysis - more detailed but computationally expensive
  - Tradeoff: Speed and interpretability vs completeness and precision

- Failure signatures:
  - Circuit performance drops when ablating critical edges/heads
  - Hallucinations indicate mover head failures or incorrect information selection
  - Multi-hop reasoning failures suggest integrated relation circuit problems
  - Editing effectiveness depends on information propagation through circuit

- First 3 experiments:
  1. Construct knowledge circuit for a simple factual relation using ablation analysis
  2. Test circuit independence by evaluating Hit@10 on isolated circuit vs full model
  3. Apply ROME editing and observe how mover heads transport edited information through circuit layers

## Open Questions the Paper Calls Out

### Open Question 1
What are the specific mechanisms by which mover heads and relation heads are activated and how do they collaborate to encode knowledge in transformers? The paper mentions that mover heads transfer information to the final token position and relation heads capture contextual relationships, but the exact activation mechanisms are not fully explained.

### Open Question 2
How do knowledge circuits contribute to the generalization ability of language models, and can they be used to improve model performance on unseen tasks? The paper suggests that knowledge circuits can independently reproduce model performance, but it does not explicitly explore their role in generalization or their potential for improving performance on unseen tasks.

### Open Question 3
How can knowledge circuits be leveraged to develop more effective knowledge editing techniques that address issues such as poor generalization and side effects? The paper discusses the impact of current knowledge editing methods on knowledge circuits and highlights the limitations of these methods, suggesting that circuits could guide improved editing approaches.

## Limitations
- Circuit discovery relies on threshold τ selection, which is only provided as a range without specifying which value was used for each experiment
- Study focuses primarily on GPT2 and TinyLLAMA models, limiting generalizability to other architectures
- Knowledge editing experiments lack detailed implementation specifications beyond the general approach

## Confidence
- **High Confidence**: Core circuit discovery methodology and identification of mover heads, relation heads, and MLPs as collaborative components
- **Medium Confidence**: Explanations for hallucinations and in-context learning through circuit mechanisms
- **Low Confidence**: Generalizability of circuit patterns across different knowledge types and model scales

## Next Checks
1. **Threshold Sensitivity Analysis**: Systematically test circuit discovery across the full range of τ values (0.02, 0.01, 0.005) to determine how threshold selection affects circuit size, composition, and performance.

2. **Cross-Model Generalization Test**: Apply the circuit discovery methodology to diverse transformer architectures (BERT, OPT, LLaMA) and knowledge domains to validate whether mover head and relation head patterns consistently emerge.

3. **Multi-Hop Reasoning Circuit Analysis**: Extend the circuit framework to analyze multi-hop reasoning tasks by identifying circuits that span multiple inference steps. Test whether integrated relation circuits consistently outperform independent relation circuits across various reasoning chains.