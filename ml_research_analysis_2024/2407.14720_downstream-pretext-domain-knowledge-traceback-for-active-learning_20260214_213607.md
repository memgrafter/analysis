---
ver: rpa2
title: Downstream-Pretext Domain Knowledge Traceback for Active Learning
arxiv_id: '2407.14720'
source_url: https://arxiv.org/abs/2407.14720
tags:
- data
- samples
- dokt
- labeled
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DOKT, a method that combines pre-training
  guidance with downstream domain knowledge to improve active learning (AL) sampling.
  DOKT uses a traceback diversity indicator to explore data relationships in low-level
  and high-level feature spaces, and a domain-based uncertainty estimator to apply
  perceptual perturbations and estimate uncertainty.
---

# Downstream-Pretext Domain Knowledge Traceback for Active Learning

## Quick Facts
- arXiv ID: 2407.14720
- Source URL: https://arxiv.org/abs/2407.14720
- Authors: Beichen Zhang; Liang Li; Zheng-Jun Zha; Jiebo Luo; Qingming Huang
- Reference count: 40
- One-line primary result: DOKT combines pre-training guidance with downstream domain knowledge to improve active learning sampling, outperforming state-of-the-art methods across ten datasets.

## Executive Summary
This paper introduces DOKT, a method that combines pre-training guidance with downstream domain knowledge to improve active learning (AL) sampling. DOKT uses a traceback diversity indicator to explore data relationships in low-level and high-level feature spaces, and a domain-based uncertainty estimator to apply perceptual perturbations and estimate uncertainty. Experiments on ten datasets, including image classification, semantic segmentation, and image captioning, show that DOKT outperforms state-of-the-art AL methods, requiring fewer labeled samples to achieve competitive performance. For example, on CIFAR-10, DOKT achieves 90.35% accuracy with only 20% of labeled data, while the highest accuracy on fully labeled data is 93.5%. The method generalizes well to various downstream tasks and reduces annotation requirements significantly.

## Method Summary
DOKT leverages a pre-trained vision transformer as a pretext encoder and fine-tunes a downstream encoder with labeled data using intra-class mixing. It constructs two feature spaces (pretext and downstream) and traces the adaptive neighbors of unlabeled data from the downstream space back to the pretext space to estimate diversity. The domain-based uncertainty estimator mixes unlabeled samples with similar labeled samples in the pretext space, creates augmented versions with masked patches, and measures divergence in predicted probabilities to quantify uncertainty. DOKT first filters candidates by traceback diversity (2M most diverse) and then selects the most uncertain among them based on domain uncertainty, ensuring both coverage and informativeness.

## Key Results
- On CIFAR-10, DOKT achieves 90.35% accuracy with only 20% of labeled data, compared to 93.5% on fully labeled data
- Requires only 2% initial labeled data for classification tasks and 0.5% for semantic segmentation
- Outperforms state-of-the-art AL methods across ten datasets including image classification, semantic segmentation, and image captioning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The traceback diversity indicator bridges the gap between low-level pretext features and high-level downstream domain knowledge.
- Mechanism: It constructs two feature spaces (pretext and downstream) and traces the adaptive neighbors of unlabeled data from the downstream space back to the pretext space, thereby unifying data relationships across both levels.
- Core assumption: Neighbors in the pretext space can be meaningfully traced to the downstream space via shared domain knowledge, enabling comprehensive diversity estimation.
- Evidence anchors:
  - [abstract]: "The diversity indicator constructs two feature spaces based on the pre-training pretext model and the downstream knowledge from annotation... to explore the interaction of samples."
  - [section]: "The indicator constructs the pretext space with the low-level representation... and the downstream space with the high-level representation... We can analyze the relationship of unlabeled samples and calculate the diversity."
  - [corpus]: Weak evidence. Related papers focus on pretext tasks but do not directly address traceback between two feature spaces for diversity estimation.
- Break condition: If the pretext and downstream spaces are too misaligned (e.g., completely different feature distributions), neighbor tracing will fail and diversity estimation will be unreliable.

### Mechanism 2
- Claim: The domain-based uncertainty estimator leverages perceptual perturbations via domain mixing to estimate uncertainty near the decision boundary.
- Mechanism: It mixes unlabeled samples with similar labeled samples in the pretext space, creates augmented versions with masked patches, and measures divergence in predicted probabilities to quantify uncertainty.
- Core assumption: Perturbing unlabeled samples with similar labeled neighbors in the pretext space introduces domain uncertainty that correlates with model uncertainty near the decision boundary.
- Evidence anchors:
  - [abstract]: "domain mixing is designed to enforce perceptual perturbing to unlabeled samples with similar visual patches in the pretext space. Then the divergence of perturbed samples is measured to estimate the domain uncertainty."
  - [section]: "we design a perturbation augmentation process to mix unlabeled samples with similar labeled data... the estimator calculates the divergence for these two probabilities... a larger D(xU) value means that the perceptual perturbation process... makes the prediction unsteady and more uncertain."
  - [corpus]: Weak evidence. Related papers discuss uncertainty estimation but not via perceptual perturbations in a pretext space for active learning.
- Break condition: If the pretext space does not preserve perceptual similarity, the mixed samples will not be meaningful perturbations, and the divergence measure will not reflect true uncertainty.

### Mechanism 3
- Claim: The combination of traceback diversity and domain uncertainty enables selection of samples that are both diverse and near the decision boundary.
- Mechanism: DOKT first filters candidates by traceback diversity (2M most diverse) and then selects the most uncertain among them based on domain uncertainty, ensuring both coverage and informativeness.
- Core assumption: Diversity and uncertainty are complementary selection criteria that together identify the most informative samples for active learning.
- Evidence anchors:
  - [abstract]: "DOKT selects the most diverse and important samples based on these two modules."
  - [section]: "DOKT analyzes the data uncertainty with perceptual perturbing and domain divergence. By combining the uncertainty score and domain divergence, DOKT can estimate the domain uncertainty of unlabeled samples... DOKT can select instructive samples near the decision boundary to boost modal training."
  - [corpus]: No direct evidence. Related papers focus on either diversity or uncertainty, not their combination in this specific way.
- Break condition: If either diversity or uncertainty estimation fails, the combined selection will be suboptimal, potentially missing key samples or selecting uninformative ones.

## Foundational Learning

- Concept: Self-supervised pre-training and pretext tasks
  - Why needed here: DOKT relies on a pre-trained pretext encoder to provide high-quality low-level feature representations that capture visual perception without domain knowledge.
  - Quick check question: What is the difference between a pretext task and a downstream task, and why is this distinction important for DOKT?

- Concept: Active learning sampling strategies (diversity vs uncertainty)
  - Why needed here: DOKT combines both diversity-based and uncertainty-based approaches to select the most informative samples, requiring understanding of when each strategy is effective.
  - Quick check question: How do diversity-based and uncertainty-based active learning methods differ in their selection criteria, and what are the trade-offs?

- Concept: Feature space alignment and neighbor tracing
  - Why needed here: The traceback mechanism depends on being able to map neighbors between the pretext and downstream feature spaces, requiring understanding of how features from different spaces relate.
  - Quick check question: What challenges arise when trying to trace neighbors between two different feature spaces, and how does DOKT address them?

## Architecture Onboarding

- Component map:
  Pretext Encoder (Θp) -> Downstream Encoder (Θd) -> Traceback Diversity Indicator -> Domain-based Uncertainty Estimator -> Predictor (f)

- Critical path:
  1. Pre-train pretext encoder on large dataset with self-supervised task
  2. Fine-tune downstream encoder with intra-class mixing on labeled data
  3. For each unlabeled sample, compute traceback diversity by tracing neighbors between spaces
  4. Compute domain uncertainty via perceptual perturbation and probability divergence
  5. Select samples combining both criteria (diversity first, then uncertainty)

- Design tradeoffs:
  - Using a pre-trained model vs training from scratch: Pre-training provides better features but may introduce domain shift
  - Two-stage selection (diversity then uncertainty) vs joint optimization: Simpler implementation but may miss some informative samples
  - Intra-class mixing for domain knowledge vs using all labeled data: Better handling of annotation inadequacy but may introduce bias

- Failure signatures:
  - Poor performance on datasets with very different distributions from pre-training data
  - Ineffective sampling when labeled data is extremely limited (fewer than 2% of total)
  - Degradation when pretext and downstream feature spaces are poorly aligned

- First 3 experiments:
  1. Run DOKT on CIFAR-10 with varying amounts of initial labeled data (2%, 5%, 10%) to test sensitivity to annotation adequacy
  2. Compare DOKT's selection with random sampling and state-of-the-art methods on a fine-grained dataset to validate effectiveness
  3. Perform ablation study removing the traceback module to quantify its contribution to performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DOKT's performance scale when applied to very large datasets with billions of samples, and what computational bottlenecks arise?
- Basis in paper: [inferred] The paper demonstrates DOKT's effectiveness on datasets like ImageNet and iNaturalist, but these are still smaller than emerging billion-scale datasets. The authors acknowledge that DOKT is designed to leverage pre-trained models but may struggle with large model parameters.
- Why unresolved: The paper focuses on demonstrating effectiveness on datasets up to ImageNet scale (~1.2M images). Scaling to billion-scale datasets introduces challenges like distributed computing, memory management, and maintaining performance with extreme label imbalance.
- What evidence would resolve it: Systematic evaluation of DOKT on billion-scale datasets, analysis of computational complexity and memory requirements, and demonstration of efficient distributed implementations.

### Open Question 2
- Question: Can DOKT's traceback diversity mechanism be extended to handle multi-modal data (e.g., text, audio, and image) in a unified framework?
- Basis in paper: [inferred] The paper shows DOKT works well on image captioning (a multi-modal task), but the traceback mechanism is specifically designed for visual features. The authors suggest potential for cross-modal applications but don't provide a concrete extension.
- Why unresolved: The current traceback mechanism relies on visual feature spaces (pretext and downstream). Extending this to handle text embeddings, audio spectrograms, or other modalities requires a method to unify these diverse feature representations.
- What evidence would resolve it: A multi-modal version of DOKT that integrates diverse feature spaces (e.g., CLIP-like embeddings) and demonstrates improved performance on cross-modal AL tasks.

### Open Question 3
- Question: How does DOKT handle concept drift in streaming data scenarios where the underlying data distribution changes over time?
- Basis in paper: [inferred] The paper focuses on pool-based AL with static datasets. While DOKT shows robustness to label imbalance, it doesn't address the challenge of evolving data distributions common in real-world streaming applications.
- Why unresolved: Active learning typically assumes a static dataset. In streaming scenarios, the model needs to adapt to new concepts while maintaining performance on previously seen data, which may require modifications to DOKT's uncertainty estimation and diversity tracking.
- What evidence would resolve it: Experiments showing DOKT's performance on datasets with simulated concept drift, and analysis of how the traceback and uncertainty mechanisms adapt to changing distributions.

## Limitations

- The traceback mechanism's effectiveness critically depends on the alignment between pretext and downstream feature spaces, which is not empirically validated in the paper
- The method assumes that neighbors in the pretext space can be meaningfully traced to the downstream space, but this alignment may fail when the two spaces have significantly different distributions
- The paper focuses on demonstrating effectiveness on datasets up to ImageNet scale, but scaling to billion-scale datasets introduces computational challenges

## Confidence

- High confidence in the overall experimental results showing DOKT outperforms state-of-the-art AL methods across multiple datasets and tasks
- Medium confidence in the mechanism claims due to limited theoretical justification and lack of ablation studies isolating the traceback diversity indicator's contribution
- Low confidence in the generalization claims without testing on datasets substantially different from pre-training data

## Next Checks

1. Conduct ablation studies removing the traceback diversity indicator to quantify its contribution versus using domain uncertainty alone
2. Test DOKT on a dataset with significantly different visual characteristics from ImageNet (the likely pre-training source) to evaluate domain shift robustness
3. Perform a detailed analysis of pretext-downstream feature space alignment, including measuring neighborhood consistency between the two spaces