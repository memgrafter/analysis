---
ver: rpa2
title: A Survey of Few-Shot Learning for Biomedical Time Series
arxiv_id: '2405.02485'
source_url: https://arxiv.org/abs/2405.02485
tags:
- learning
- few-shot
- data
- shot
- support
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides a comprehensive review of few-shot learning
  methods for biomedical time series applications, addressing the challenge of training
  data-hungry deep learning models when labeled data is scarce due to privacy concerns,
  rare diseases, and annotation costs. The paper categorizes few-shot learning methods
  into five types: data-based (using GANs and data augmentation), model-based (adversarial
  networks, SVMs, and custom frameworks), metric-based (Siamese networks, matching
  networks, and prototypical networks), optimization-based (transfer learning and
  MAML), and hybrid methods.'
---

# A Survey of Few-Shot Learning for Biomedical Time Series

## Quick Facts
- arXiv ID: 2405.02485
- Source URL: https://arxiv.org/abs/2405.02485
- Authors: Chenqi Li; Timothy Denison; Tingting Zhu
- Reference count: 40
- Primary result: Comprehensive survey of few-shot learning methods for biomedical time series applications

## Executive Summary
This survey comprehensively reviews few-shot learning methods for biomedical time series applications, addressing the critical challenge of training deep learning models when labeled data is scarce. The paper categorizes few-shot learning methods into five types: data-based (GANs and data augmentation), model-based (adversarial networks, SVMs, custom frameworks), metric-based (Siamese networks, matching networks, prototypical networks), optimization-based (transfer learning and MAML), and hybrid methods. The survey highlights clinical benefits including improved long-term monitoring, early disease detection, and personalized healthcare delivery, while identifying key challenges such as lack of standardized benchmarks and evaluation metrics.

## Method Summary
The survey categorizes few-shot learning methods into five main approaches: data-based methods using GANs and data augmentation to generate synthetic data; model-based methods employing adversarial networks, SVMs, and custom frameworks; metric-based methods utilizing Siamese networks, matching networks, and prototypical networks to learn similarity comparisons; optimization-based methods including transfer learning and MAML that optimize for quick adaptation; and hybrid methods combining these approaches. The methods are evaluated based on their effectiveness in various biomedical time series tasks including sleep staging, arrhythmia detection, seizure detection, emotion recognition, and human activity recognition across different modalities like EEG, ECG, EMG, PPG, IMU, and EHR.

## Key Results
- Few-shot learning methods can achieve high performance on biomedical time series tasks with as few as 5-20 support samples
- Cross-subject and cross-session knowledge transfer presents significant challenges but shows promise for generalization
- Multimodal fusion and large language models represent promising future directions for improving few-shot learning performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot learning overcomes the data scarcity barrier in biomedical time series by leveraging knowledge transfer from related tasks or domains.
- Mechanism: The method learns a generalizable representation space during pre-training on abundant data from source tasks, then adapts this representation to new target tasks using only a few labeled samples.
- Core assumption: The source and target tasks share meaningful underlying patterns or representations that can be transferred.
- Evidence anchors:
  - [abstract] "An emerging approach to overcome the scarcity of labeled data is to augment AI methods with human-like capabilities to leverage past experiences to learn new tasks with limited examples, called few-shot learning."
  - [section] "Few-shot learning problems can be characterized based on the type of knowledge transfer a model is expected to bridge between the pre-training problem and the N-way-K-shot problem."
- Break condition: If the source and target tasks are too dissimilar, the transferred knowledge becomes irrelevant or even harmful, leading to negative transfer.

### Mechanism 2
- Claim: Metric-based few-shot learning methods learn to compare samples effectively by embedding them into a representation space where similar samples cluster together.
- Mechanism: An embedding network projects both support and query samples into a shared representation space. Similarity is then computed using distance metrics between the query sample and either all support samples or class prototypes.
- Core assumption: The embedding network can learn a representation space where samples from the same class are closer together than samples from different classes.
- Evidence anchors:
  - [section] "Metric-based methods focus on learning the similarity or distance between data points through an embedding network."
- Break condition: If the embedding network fails to learn meaningful representations, the similarity comparisons become meaningless.

### Mechanism 3
- Claim: Optimization-based few-shot learning methods guide model convergence to parameter spaces that can be quickly adjusted by fine-tuning with a few samples.
- Mechanism: Meta-learning algorithms like MAML explicitly optimize the model initialization such that a small number of gradient steps on a new task leads to good performance.
- Core assumption: There exists a set of initial parameters that can be quickly adapted to perform well on a wide range of tasks.
- Evidence anchors:
  - [section] "Optimization-based few-shot learning methods focus on developing innovative ways to train models... Optimization-based methods focus on designing the optimization objective and training procedure to guide the model to converge to weight spaces that generalize well to new tasks when fine-tuned with few-shot samples."
- Break condition: If the tasks are too diverse or the inner-loop optimization becomes unstable, the meta-learning process fails to find a useful initialization.

## Foundational Learning

- Concept: Knowledge Transfer in Machine Learning
  - Why needed here: Few-shot learning fundamentally relies on transferring knowledge from source tasks or domains to target tasks with limited data.
  - Quick check question: What are the different types of knowledge transfer mentioned in the paper (e.g., cross-session, cross-subject, cross-dataset, cross-class, cross-task, no pre-training)?

- Concept: Representation Learning and Embedding Spaces
  - Why needed here: Metric-based few-shot learning methods depend on learning meaningful representations where similar samples are close together in the embedding space.
  - Quick check question: How do prototypical networks construct class prototypes, and how does this differ from matching networks?

- Concept: Meta-Learning and Optimization
  - Why needed here: Optimization-based methods like MAML use meta-learning to find initial model parameters that can be quickly adapted to new tasks.
  - Quick check question: What is the difference between the inner loop and outer loop in MAML, and what is the purpose of each?

## Architecture Onboarding

- Component map: Data-based (GANs, augmentation) -> Model-based (adversarial networks, SVMs) -> Metric-based (Siamese, matching, prototypical) -> Optimization-based (transfer learning, MAML) -> Hybrid (combinations)

- Critical path: 1) Pre-training on source data (if using optimization-based or hybrid methods) 2) Few-shot adaptation to target task using support set 3) Inference on query set

- Design tradeoffs:
  - Data-based vs. model-based: Data augmentation can increase dataset size but may not capture true data distribution; custom models can be tailored but may be harder to design and train
  - Metric-based vs. optimization-based: Metric-based methods are more flexible but rely heavily on embedding quality; optimization-based methods can be more efficient but require extensive pre-training
  - Hybrid methods: Can combine strengths but increase complexity and potential for overfitting

- Failure signatures:
  - Poor performance on query set despite good performance on support set: Indicates overfitting to the support set
  - No improvement over baseline methods: Suggests the few-shot learning method is not effectively leveraging the limited data
  - High variance in performance across different few-shot tasks: May indicate sensitivity to the specific samples chosen for the support set

- First 3 experiments:
  1. Implement a simple prototypical network for a binary classification task on a publicly available biomedical time series dataset (e.g., ECG arrhythmia detection)
  2. Compare the performance of a prototypical network with a standard supervised learning approach on the same dataset under varying levels of data scarcity
  3. Experiment with different embedding network architectures (e.g., 1D CNN, LSTM) and distance metrics (e.g., Euclidean, cosine) to understand their impact on few-shot learning performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of support samples required for transfer learning to be effective in few-shot biomedical time series applications, and how does this vary across different tasks and modalities?
- Basis in paper: [explicit] The paper states that "With around 5 or fewer support samples, transfer learning fails, but with around 20 support samples, performance surpasses 99% accuracy and the marginal benefit of additional samples diminishes" for myocardial infarction classification.
- Why unresolved: The optimal number of support samples is likely task- and modality-specific, and the paper only provides one example.
- What evidence would resolve it: A large-scale empirical study comparing few-shot learning performance across various biomedical time series tasks and modalities as a function of the number of support samples.

### Open Question 2
- Question: How should the choice of pre-training dataset be made to maximize knowledge transfer and generalization to the target few-shot task in biomedical time series applications?
- Basis in paper: [explicit] The paper mentions that "there is limited research on how the choice of pre-training datasets impacts knowledge transfer and generalization to the target few-shot task" and suggests pre-training on datasets similar to the problem or application at hand.
- Why unresolved: The optimal strategy for selecting pre-training datasets is unclear, and the paper only provides general guidelines.
- What evidence would resolve it: A comprehensive study comparing few-shot learning performance using different pre-training strategies across various biomedical time series tasks.

### Open Question 3
- Question: How can few-shot learning methods be effectively applied to multimodal biomedical time series data to leverage complementary information sources and improve performance?
- Basis in paper: [explicit] The paper states that "Research on how to effectively fuse multi-modal information and how to learn with unaligned data pave the way for more robust few-shot learning systems that take advantage of complementary information sources" and mentions the potential of combining different time series sensors or data modalities.
- Why unresolved: The optimal approach for multimodal few-shot learning in biomedical time series is unclear, and the paper only provides general insights.
- What evidence would resolve it: A comprehensive study comparing the performance of unimodal and multimodal few-shot learning methods across various biomedical time series tasks and modalities, using different fusion strategies and alignment techniques.

## Limitations
- The survey lacks standardized benchmarks for objective comparison of few-shot learning methods across different biomedical time series applications
- The boundaries between method categories are often blurry, particularly for hybrid approaches
- Clinical validation of claimed benefits is limited to examples rather than systematic real-world testing

## Confidence
- Effectiveness of different few-shot learning methods: Medium - field is still evolving with limited standardized benchmarks
- Categorization of methods: Medium - boundaries between categories can be blurry
- Clinical benefits claims: Medium - supported by examples but lack systematic validation
- Future directions with large language models: Low - nascent research area with potential adaptation challenges

## Next Checks
1. Implement a standardized evaluation framework across multiple biomedical time series datasets to compare few-shot learning methods objectively
2. Conduct ablation studies to quantify the contribution of each component in hybrid few-shot learning methods
3. Develop domain adaptation techniques specifically for cross-subject and cross-session biomedical time series tasks to improve generalization