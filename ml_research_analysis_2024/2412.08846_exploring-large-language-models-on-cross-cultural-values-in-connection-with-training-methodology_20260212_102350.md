---
ver: rpa2
title: Exploring Large Language Models on Cross-Cultural Values in Connection with
  Training Methodology
arxiv_id: '2412.08846'
source_url: https://arxiv.org/abs/2412.08846
tags:
- llms
- values
- cultural
- humans
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how large language models judge cultural values
  across countries and explores how training methodology affects these judgments.
  Using the World Value Survey dataset, the authors assess model alignment with human
  responses across 55 countries and 12 categories of cultural values.
---

# Exploring Large Language Models on Cross-Cultural Values in Connection with Training Methodology

## Quick Facts
- arXiv ID: 2412.08846
- Source URL: https://arxiv.org/abs/2412.08846
- Reference count: 5
- Primary result: LLMs exhibit Western cultural bias and show improved cross-cultural understanding with multilingual training and larger model sizes

## Executive Summary
This paper investigates how large language models (LLMs) judge cultural values across countries and how training methodology affects these judgments. Using the World Value Survey dataset across 55 countries and 12 cultural value categories, the authors find that LLMs perform better on socio-cultural norms than on social systems and progress, while exhibiting a clear Western cultural bias. The study demonstrates that multilingual training, larger model sizes, and alignment techniques all contribute to improved cross-cultural understanding, with synthetic data from larger models showing promise for transferring cultural knowledge to smaller models.

## Method Summary
The authors use the World Value Survey (WVS) dataset containing 209 questions across 12 categories from 55 countries. They convert WVS questions to multiple-choice format and generate predictions using various open-source LLMs (phi-1.5, phi-2, phi-3, Llama-2, Llama-3, Yi). The methodology involves calculating mean scores for both LLM predictions and human responses, then computing Pearson's correlation coefficient between these scores to measure alignment with human cultural judgments. The study compares performance across different model sizes, training corpora, and alignment techniques.

## Key Results
- LLMs show better performance on socio-cultural norms than on social systems and progress
- Models exhibit Western cultural bias, performing better on Western cultural contexts
- Multilingual training and larger model sizes significantly improve cross-cultural understanding
- Synthetic data generated by larger models can effectively transfer cultural knowledge to smaller models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training LLMs on multilingual corpora improves cross-cultural understanding beyond just language proficiency.
- Mechanism: Multilingual training exposes models to diverse cultural contexts and value systems, allowing them to learn cultural patterns that transfer across languages rather than just learning to translate.
- Core assumption: Cultural knowledge and language patterns are sufficiently intertwined that exposure to multiple languages inherently provides exposure to multiple cultural perspectives.
- Evidence anchors:
  - [abstract] "training on the multilingual corpus" improves performance on non-Western cultures
  - [section] "incorporating a large Chinese corpus can markedly enhance performance in non-Western cultural contexts"
  - [corpus] Weak evidence - the paper shows correlation but doesn't prove the mechanism of knowledge transfer across languages
- Break condition: If multilingual training data is merely translated from English sources without authentic cultural content, or if cultural concepts don't transfer well across languages due to fundamental differences in worldview.

### Mechanism 2
- Claim: Larger model size enables better cultural understanding due to increased capacity to represent nuanced cultural distinctions.
- Mechanism: Larger models have more parameters and representational capacity to encode subtle cultural differences and maintain separate representations for different cultural contexts without interference.
- Core assumption: Cultural understanding requires fine-grained distinctions that scale with model capacity, similar to how reasoning capabilities improve with size.
- Evidence anchors:
  - [abstract] "increasing model size helps a better understanding of social values"
  - [section] "larger models exhibit a higher correlation with human ratings" and "Llama-2-13B-Chat and Llama-2-70B-Chat achieve increasingly higher correlation with model size"
  - [corpus] Moderate evidence - the paper compares different model sizes but doesn't isolate model size from other factors like training data quality
- Break condition: If cultural understanding saturates at smaller sizes or if training data quality is the limiting factor rather than model capacity.

### Mechanism 3
- Claim: Synthetic data generated by larger models can effectively transfer cultural knowledge to smaller models.
- Mechanism: Larger models generate culturally diverse synthetic examples that distill their cross-cultural understanding into training data for smaller models, effectively compressing and transferring this knowledge.
- Core assumption: Synthetic data created by culturally-aware larger models contains more culturally nuanced and diverse examples than typical web data, and smaller models can effectively learn from this distilled knowledge.
- Evidence anchors:
  - [abstract] "Synthetic data can distill cultural knowledge of larger language models to smaller models"
  - [section] "phi-3-3.8B trained on A) Synthetic data achieves higher correlation than Llama-3-8B trained on C) Multilingual data"
  - [corpus] Weak evidence - the paper speculates about synthetic data but doesn't provide details on what it contains or how it's generated
- Break condition: If synthetic data generation doesn't capture authentic cultural diversity, or if smaller models cannot effectively learn from synthetic examples compared to real human-generated data.

## Foundational Learning

- Concept: Pearson correlation coefficient
  - Why needed here: The paper uses Pearson correlation to measure similarity between LLM judgments and human responses across countries
  - Quick check question: If two sets of scores have perfect positive linear relationship, what would their Pearson correlation be?

- Concept: Cross-cultural value systems
  - Why needed here: Understanding how different cultures prioritize values (like individualism vs collectivism) is essential to interpreting why LLMs show Western bias
  - Quick check question: What are the two main dimensions in Hofstede's cultural dimensions theory that would be most relevant to analyzing value survey responses?

- Concept: Pre-training vs fine-tuning distinction
  - Why needed here: The paper examines how different training methodologies (pre-training on multilingual data vs fine-tuning with alignment) affect cultural understanding
  - Quick check question: What is the key difference between how knowledge is encoded during pre-training versus alignment fine-tuning?

## Architecture Onboarding

- Component map: WVS dataset → model inference → score calculation → correlation computation → comparative analysis
- Critical path: WVS dataset → model inference → score calculation → correlation computation → comparative analysis
- Design tradeoffs: Using mean scores with regular distribution vs more sophisticated scoring methods that might capture non-linear relationships between answers
- Failure signatures: Low correlations could indicate insufficient training data diversity, model capacity issues, or problems with the scoring methodology itself
- First 3 experiments:
  1. Run a smaller subset of WVS questions (5-10) across multiple models to verify the scoring and correlation computation works correctly
  2. Compare a model's performance on Western vs non-Western questions to validate the Western bias observation
  3. Test synthetic data generation by having a large model create 100 examples for a specific question type, then evaluate if smaller models improve on those questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the specific choice of training data (e.g., web crawl vs. curated datasets) impact the cultural biases observed in LLMs?
- Basis in paper: [inferred] The paper mentions that LLMs are trained on massive web data, but doesn't delve into the specific types of data or their impact on cultural biases.
- Why unresolved: The paper doesn't analyze the relationship between the composition of training data and the observed cultural biases. It only mentions that multilingual training can improve cross-cultural understanding.
- What evidence would resolve it: A study comparing the performance of LLMs trained on different types of data (e.g., web crawl, curated datasets, multilingual corpora) on cross-cultural tasks.

### Open Question 2
- Question: Can alignment techniques be specifically designed to mitigate cultural biases in LLMs, rather than just improving overall performance?
- Basis in paper: [explicit] The paper mentions that alignment improves LLM performance on cultural tasks, but doesn't explore whether it can be tailored to address specific cultural biases.
- Why unresolved: The paper doesn't investigate whether current alignment techniques are effective in reducing cultural biases or if new methods are needed.
- What evidence would resolve it: Experiments comparing the effectiveness of different alignment techniques in reducing cultural biases in LLMs.

### Open Question 3
- Question: How do different model architectures (e.g., transformer vs. other architectures) affect the ability of LLMs to understand and represent cultural nuances?
- Basis in paper: [inferred] The paper focuses on transformer-based LLMs but doesn't compare their performance to other architectures in cross-cultural tasks.
- Why unresolved: The paper doesn't explore whether the transformer architecture is optimal for capturing cultural nuances or if other architectures might be more effective.
- What evidence would resolve it: A comparison of the performance of different model architectures on cross-cultural tasks.

## Limitations
- Limited analysis of synthetic data generation process and characteristics
- Correlation vs. causation ambiguity in observed improvements
- Western bias assessment doesn't account for question interpretation differences

## Confidence

**High Confidence**: The core finding that larger models generally perform better on cross-cultural understanding tasks is well-supported by the comparative analysis across multiple model sizes.

**Medium Confidence**: The claim that multilingual training improves cross-cultural understanding beyond language proficiency is supported by observed performance differences, but the mechanism remains speculative.

**Low Confidence**: The assertion that synthetic data effectively distills cultural knowledge from larger models lacks sufficient empirical support.

## Next Checks
1. Conduct a detailed analysis of the synthetic data used to train phi-3-3.8B, including qualitative examination of generated examples and diversity metrics across cultures.
2. Design experiments testing culturally ambiguous questions that could have different valid interpretations across cultures to distinguish genuine cultural understanding from pattern matching.
3. Perform controlled experiments varying one factor at a time (model size, training data diversity, alignment technique) while holding others constant to establish causal relationships.