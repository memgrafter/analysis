---
ver: rpa2
title: 'Mind the Uncertainty in Human Disagreement: Evaluating Discrepancies between
  Model Predictions and Human Responses in VQA'
arxiv_id: '2410.02773'
source_url: https://arxiv.org/abs/2410.02773
tags:
- human
- uncertainty
- calibration
- beit3
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study examines how well state-of-the-art vision-language
  models align with human confidence distributions when human uncertainty exists.
  Using VQA 2.0, where ten annotators provided answers with confidence levels, the
  authors categorize samples into low, medium, and high human uncertainty in disagreement
  (HUD) levels and evaluate models using accuracy plus three human-correlated metrics:
  Total Variation Distance, Kullback-Leibler divergence, and Human Entropy Calibration
  Error.'
---

# Mind the Uncertainty in Human Disagreement: Evaluating Discrepancies between Model Predictions and Human Responses in VQA

## Quick Facts
- **arXiv ID**: 2410.02773
- **Source URL**: https://arxiv.org/abs/2410.02773
- **Reference count**: 11
- **Primary result**: State-of-the-art VQA models struggle to align with human confidence distributions when annotators disagree, and accuracy-oriented calibration can harm this alignment.

## Executive Summary
This study examines how well vision-language models align with human confidence distributions when human uncertainty exists in VQA. Using VQA 2.0 data where ten annotators provided answers with confidence levels, the authors categorize samples into low, medium, and high human uncertainty in disagreement (HUD) levels. They find that even the best VQA model (BEiT3) struggles to capture the multi-label distribution inherent in diverse human responses. The commonly used accuracy-oriented calibration technique adversely affects BEiT3's ability to capture HUD, while calibrating towards human distributions significantly improves alignment. The study demonstrates that traditional VQA accuracy metrics are insufficient and advocates for future research focusing on aligning models with human uncertainty and disagreement.

## Method Summary
The authors fine-tune BEiT3 and LXMERT models on VQA 2.0 training data, then categorize validation samples into low, medium, and high HUD levels based on human annotator disagreement patterns. They evaluate model performance using VQA-Accuracy plus three human-correlated metrics: Total Variation Distance, Kullback-Leibler divergence, and Human Entropy Calibration Error. Temperature scaling is applied as a calibration technique, testing both accuracy-oriented calibration (VQA-Accuracy) and human-distribution-oriented calibration. The study compares model alignment with human confidence distributions across different HUD levels to understand how human uncertainty impacts model predictions.

## Key Results
- BEiT3 and LXMERT show varying alignment with human confidence distributions across low, medium, and high HUD levels
- Accuracy-oriented calibration (VQA-Accuracy) harms BEiT3's ability to capture human uncertainty in disagreement
- Calibrating toward human distributions rather than accuracy significantly improves alignment metrics (TVD, KL, EntCE)
- Traditional VQA accuracy metrics are insufficient for evaluating model performance when human uncertainty exists

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Higher human uncertainty in disagreement (HUD) leads to more diverse human responses, making the single best answer prediction task harder but improving model alignment with human confidence distributions.
- **Mechanism**: When HUD is high, annotators disagree more, producing multiple plausible answers with similar confidence scores. This smoother distribution forces models to distribute probability mass across several labels rather than focusing on one, improving alignment with human uncertainty patterns.
- **Core assumption**: Human disagreement reflects genuine uncertainty about correct answers rather than just label noise.
- **Evidence anchors**: Abstract observation that models struggle with multi-annotator responses exhibiting human uncertainty; section noting smoother confidence distributions in high HUD samples help models pay attention to multiple labels.

### Mechanism 2
- **Claim**: Temperature Scaling calibrated toward accuracy (VQA-Accuracy) can harm model alignment with human distributions, especially for strong models like BEiT3.
- **Mechanism**: Traditional temperature scaling optimizes the gap between predicted confidence and actual accuracy, which works well for accuracy but may distort the model's probability distribution shape, making it less aligned with human confidence patterns.
- **Core assumption**: VQA-Accuracy and human confidence alignment are not perfectly correlated objectives.
- **Evidence anchors**: Abstract observation that accuracy-oriented calibration adversely affects BEiT3's HUD capture; section showing calibrating toward VQA-Accuracy causes probability distortions for specific labels.

### Mechanism 3
- **Claim**: Calibrating models toward human distributions (rather than accuracy) improves alignment metrics like TVD, KL divergence, and EntCE.
- **Mechanism**: By optimizing temperature scaling to minimize divergence from human confidence distributions, models learn to predict probabilities that better match human uncertainty patterns rather than just maximizing accuracy.
- **Core assumption**: Human confidence distributions contain meaningful information about answer uncertainty that should be preserved in model predictions.
- **Evidence anchors**: Abstract demonstration of benefits from calibrating models towards human distributions; section showing both models reach lower scores on alignment metrics when calibrated toward human distributions.

## Foundational Learning

- **Concept: Human uncertainty quantification**
  - Why needed here: The study converts natural language confidence labels ("yes", "maybe", "no") into numerical scores (1.0, 0.5, 0.01) to enable mathematical comparison between human and model distributions.
  - Quick check question: How would you convert a three-level confidence scale into numerical values that preserve relative ordering while avoiding zero probabilities?

- **Concept: Distribution divergence metrics**
  - Why needed here: The study uses TVD, KL divergence, and EntCE to measure how well model predictions align with human confidence distributions, going beyond simple accuracy metrics.
  - Quick check question: What's the key difference between TVD and KL divergence when comparing two probability distributions?

- **Concept: Temperature scaling calibration**
  - Why needed here: The study applies temperature scaling as a post-processing technique to adjust model confidence, testing both accuracy-oriented and human-distribution-oriented calibration.
  - Quick check question: How does temperature scaling modify the softmax output probabilities, and what happens when the temperature parameter approaches infinity?

## Architecture Onboarding

- **Component map**: VQA model inference -> HUD score computation -> calibration temperature optimization -> evaluation metrics calculation
- **Critical path**: Model prediction → HUD score calculation → calibration temperature optimization → evaluation metrics computation → analysis of alignment vs accuracy tradeoff
- **Design tradeoffs**: Using accuracy-oriented calibration maximizes VQA-Accuracy but may harm human distribution alignment; human-oriented calibration improves alignment but may reduce accuracy; simpler metrics like accuracy are easier to compute but miss important uncertainty information.
- **Failure signatures**: If TVD or KL divergence increase after calibration when they should decrease, the temperature parameter is set incorrectly; if accuracy drops significantly without corresponding alignment improvement, the calibration target is misaligned.
- **First 3 experiments**:
  1. Run both models on low, medium, and high HUD sets without calibration and compute all metrics to establish baseline performance.
  2. Apply temperature scaling calibrated toward VQA-Accuracy and measure changes in both accuracy and alignment metrics.
  3. Apply temperature scaling calibrated toward human distributions and compare results against both the baseline and accuracy-oriented calibration.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does human uncertainty in disagreement (HUD) impact the reliability of model confidence across different VQA tasks and datasets beyond VQA 2.0?
- **Basis in paper**: [explicit] The paper states that human uncertainty in disagreement is understudied and focuses on VQA 2.0.
- **Why unresolved**: The study is limited to one dataset and does not explore the generalizability of findings across other VQA datasets or tasks.
- **What evidence would resolve it**: Experiments testing model confidence and calibration across multiple VQA datasets and tasks would clarify the generalizability of HUD impacts.

### Open Question 2
- **Question**: What alternative calibration techniques could be more effective than Temperature Scaling for aligning model confidence with human distributions under HUD?
- **Basis in paper**: [explicit] The paper suggests that future studies should consider other calibration techniques to directly target human uncertainty in disagreement.
- **Why unresolved**: The study only uses Temperature Scaling and does not explore other calibration methods.
- **What evidence would resolve it**: Comparative studies of various calibration techniques (e.g., Dirichlet calibration, Platt scaling) on model alignment with human distributions would identify more effective methods.

### Open Question 3
- **Question**: How does the quantification of human confidence scores (e.g., 'yes', 'no', 'maybe') influence the evaluation of model performance and alignment with human uncertainty?
- **Basis in paper**: [explicit] The paper assigns specific numerical values to confidence labels and notes that exploring advanced quantification strategies is a limitation.
- **Why unresolved**: The study uses a standard quantification method but acknowledges the potential for alternative approaches.
- **What evidence would resolve it**: Experiments testing different quantification strategies for human confidence scores and their impact on model evaluation would clarify the influence of this quantification.

## Limitations
- The study's findings are based on a single dataset (VQA 2.0) with a specific three-level confidence annotation scheme, limiting generalizability.
- The conversion of human confidence labels to numerical scores is a design choice that could affect results and sensitivity to alternative mappings.
- Results are demonstrated only on BEiT3 and LXMERT models, and the observed calibration effects may not generalize to other VQA architectures.

## Confidence

**High Confidence**: The observation that accuracy-oriented calibration can harm human distribution alignment for strong models like BEiT3 is well-supported by empirical results and multiple evaluation metrics.

**Medium Confidence**: The claim that higher human uncertainty leads to better model alignment with human distributions is supported but could benefit from additional ablation studies across different model families.

**Medium Confidence**: The assertion that traditional VQA accuracy metrics are insufficient is reasonable given the results, but the specific thresholds for "insufficient" could vary by application context.

## Next Checks

1. **Cross-dataset validation**: Test the calibration techniques on other VQA datasets (e.g., GQA, Visual7W) to verify the generalizability of the HUD-based categorization and calibration findings.

2. **Alternative confidence mappings**: Experiment with different numerical mappings for the three-level confidence scale to assess the sensitivity of the calibration results to this design choice.

3. **Multi-model generalization**: Apply the calibration methodology to additional VQA architectures (e.g., CLIP-based models, SigLIP) to determine if the observed tradeoffs between accuracy and human alignment are model-agnostic or specific to BEiT3/LXMERT.