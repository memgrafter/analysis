---
ver: rpa2
title: 'Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human Annotation:
  A Case Study Using Schedule-of-Event Table Detection'
arxiv_id: '2405.06093'
source_url: https://arxiv.org/abs/2405.06093
tags:
- table
- fine-tuning
- data
- labels
- tables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using LLM-generated labels to fine-tune models
  for detecting Schedule-of-Event (SoE) tables in clinical trial protocols. Instead
  of relying on expensive expert annotations, the authors generate labels using Gemini-Pro
  1.0 and filter them based on agreement between JSON and text representations of
  tables.
---

# Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event Table Detection

## Quick Facts
- arXiv ID: 2405.06093
- Source URL: https://arxiv.org/abs/2405.06093
- Reference count: 37
- Primary result: LLM-generated labels with consensus filtering achieve near-expert performance on SoE table detection

## Executive Summary
This paper presents a method for fine-tuning large language models using selectively filtered LLM-generated labels instead of human annotations. The authors apply this approach to Schedule-of-Event (SoE) table detection in clinical trial protocols, using Gemini-Pro 1.0 to generate labels and filtering them based on agreement between JSON and text representations. The resulting PaLM-2 model achieves 85.9% precision, 89% F1-score, and 95.7% accuracy, approaching the performance of models fine-tuned with human annotations while avoiding the high costs of expert labeling.

## Method Summary
The approach uses Gemini-Pro 1.0 to generate labels for Schedule-of-Event table detection in clinical trial protocols. Labels are filtered through a consensus mechanism that compares JSON and text representations of tables, selecting only those where the LLM agrees on the table structure across both modalities. The filtered labels are then used to fine-tune the PaLM-2 model. This selective fine-tuning approach leverages the LLM's ability to generate labels at scale while using the dual representation agreement as a quality filter to reduce noise in the training data.

## Key Results
- PaLM-2 fine-tuned on consensus-filtered Gemini labels achieves 85.9% precision, 89% F1-score, and 95.7% accuracy
- Outperforms base PaLM-2 model and Gemini-Pro 1.0 on SoE table detection task
- Approaches performance level of models fine-tuned with human annotations
- Demonstrates cost-effective alternative to expert labeling for specialized domains

## Why This Works (Mechanism)
The method works by exploiting the complementary strengths of different LLM representations. When an LLM processes the same table in both structured (JSON) and unstructured (text) formats, agreement between these representations serves as a proxy for label quality. Tables where the LLM consistently identifies the same structure across both modalities are likely to be correctly labeled, while discrepancies may indicate ambiguity or error. This consensus filtering reduces the noise in LLM-generated labels, allowing the fine-tuning process to learn more accurate patterns without requiring expensive human verification.

## Foundational Learning
- **LLM label generation capabilities**: Understanding how modern LLMs can be prompted to extract structured information from unstructured text is essential for generating training labels at scale. Quick check: Test different prompting strategies on sample tables to assess consistency.
- **Multi-modal representation agreement**: The concept of using agreement between different data representations as a quality signal requires understanding how information is encoded differently in JSON vs. text formats. Quick check: Compare agreement rates across different table types.
- **Selective fine-tuning**: The technique of filtering training data based on quality metrics before fine-tuning differs from traditional approaches that use all available data. Quick check: Experiment with different filtering thresholds to find optimal balance.
- **Domain-specific table detection**: Recognizing that specialized domains like clinical trial protocols have unique table structures that may require domain-specific fine-tuning approaches. Quick check: Evaluate model performance on tables from different medical specialties.
- **Cost-benefit analysis of annotation methods**: Understanding the trade-offs between human annotation quality and LLM generation scalability is crucial for evaluating the approach's practical value. Quick check: Calculate cost per correctly labeled instance across methods.

## Architecture Onboarding

Component map: Clinical trial protocols -> Gemini-Pro 1.0 (label generation) -> Consensus filter (JSON+text agreement) -> PaLM-2 (fine-tuning) -> SoE table detection

Critical path: The most important sequence is label generation → consensus filtering → fine-tuning → evaluation. The quality of generated labels directly impacts the fine-tuned model's performance, making the consensus filter a critical bottleneck.

Design tradeoffs: The main tradeoff is between label quantity and quality. More permissive filtering increases training data but introduces noise, while stricter filtering improves quality but reduces dataset size. The JSON+text consensus approach represents a middle ground that leverages the LLM's self-consistency as a quality signal.

Failure signatures: Poor performance may indicate: (1) low agreement rates suggesting the LLM struggles with the domain, (2) overly strict filtering resulting in insufficient training data, or (3) the task being too specialized for the LLM to generate reliable labels. Monitoring agreement rates and training set size helps diagnose issues.

First experiments:
1. Test the consensus filtering mechanism on a small sample of expert-annotated tables to validate agreement as a quality proxy
2. Perform sensitivity analysis on filtering thresholds to determine optimal precision-recall tradeoff
3. Compare model performance using different proportions of filtered vs. unfiltered LLM-generated labels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the dual representation filtering mechanism (JSON + text consensus) generalize to other table classification tasks beyond SoE detection?
- Basis in paper: The authors state "While we use JSON and text based consensus approach as a proxy for selecting high quality training labels for our specific table classification task, any multi-modal data representation can serve a similar purpose in filtering out potentially noisy labels."
- Why unresolved: The paper only demonstrates effectiveness on SoE table classification, a highly specialized task. The authors acknowledge this limitation but don't empirically test the approach on other table types or domains.
- What evidence would resolve it: Testing the same consensus-based filtering approach on different table classification tasks (e.g., financial tables, experimental data tables) across multiple domains would show if the method generalizes or is specific to SoE tables.

### Open Question 2
- Question: What is the upper bound performance achievable with LLM-generated labels compared to expert-annotated labels for specialized tasks?
- Basis in paper: The authors note that their fine-tuned PaLM-2 with consensus-filtered Gemini labels "approaches the performance level of models fine-tuned with human annotations" but doesn't completely bridge the gap. They also acknowledge they couldn't directly compare to a model fine-tuned on expert annotations due to cost constraints.
- Why unresolved: Without a direct comparison between models fine-tuned on LLM-generated vs. expert-annotated data on the same task, we don't know how close LLM-generated labels can get to expert quality, or what the performance ceiling is.
- What evidence would resolve it: Training and evaluating multiple models fine-tuned on increasing proportions of expert vs. LLM-generated labels (e.g., 0%, 25%, 50%, 75%, 100% expert) on the same task would show the performance trajectory and potential convergence point.

### Open Question 3
- Question: How sensitive is the fine-tuning performance to the quality of the label-generating LLM?
- Basis in paper: The authors use Gemini-Pro 1.0 as the label generator and note it outperforms the base PaLM-2 model. They also mention that "If LLMs perform poorly across the board on a specific task, automated generation of labels may not be feasible even with powerful models like gemini-pro."
- Why unresolved: The study only tests one label-generating LLM (Gemini-Pro 1.0) and doesn't explore how performance changes with less capable models or models with different biases.
- What evidence would resolve it: Repeating the fine-tuning experiments with different LLMs of varying capabilities (e.g., GPT-3.5, Claude, smaller open-source models) as label generators would show how sensitive the approach is to the quality and characteristics of the label-generating model.

## Limitations
- Only tested on one specialized domain (clinical trial protocols) with one specific table type (SoE tables)
- Limited to single LLM (Gemini-Pro 1.0) and target model (PaLM-2) combination
- No direct comparison with models fine-tuned on human-annotated data due to cost constraints
- Selection mechanism based on JSON-text agreement lacks theoretical grounding and may not generalize
- Doesn't address potential biases in LLM-generated labels or performance degradation on more complex table structures

## Confidence
- **High confidence**: The methodological approach of using LLM-generated labels with agreement-based filtering is clearly described and implemented.
- **Medium confidence**: The reported performance metrics (85.9% precision, 89% F1-score) are internally consistent within the study's framework.
- **Low confidence**: Claims about scalability and cost-effectiveness relative to human annotation, as these weren't empirically validated beyond the specific case study.

## Next Checks
1. Conduct ablation studies testing different agreement thresholds and filtering criteria to determine optimal selection parameters across diverse table types.
2. Perform direct side-by-side comparisons between models fine-tuned on LLM-labeled data versus human-labeled data using identical evaluation protocols and datasets.
3. Test the approach with multiple LLM generators (GPT-4, Claude, etc.) and target models to assess generalizability of the selective fine-tuning methodology.