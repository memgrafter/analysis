---
ver: rpa2
title: Exploring the Potential of Synthetic Data to Replace Real Data
arxiv_id: '2408.14559'
source_url: https://arxiv.org/abs/2408.14559
tags:
- data
- synthetic
- training
- real
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores how synthetic data can replace real data in
  training computer vision models, particularly in cross-domain settings. The authors
  introduce two new metrics, train2test distance and APt2t, to evaluate how well synthetic
  data represents test instances.
---

# Exploring the Potential of Synthetic Data to Replace Real Data

## Quick Facts
- arXiv ID: 2408.14559
- Source URL: https://arxiv.org/abs/2408.14559
- Authors: Hyungtae Lee; Yan Zhang; Heesung Kwon; Shuvra S. Bhattacharrya
- Reference count: 0
- Primary result: Synthetic data most effectively improves training performance for medium-confidence detections in cross-domain UAV-view human detection

## Executive Summary
This paper investigates how synthetic data can replace real data in training computer vision models, particularly for cross-domain scenarios. The authors introduce two new metrics, train2test distance and APt2t, to evaluate how well synthetic data represents test instances. Through experiments with UAV-view human detection across multiple datasets, they find that synthetic data's impact varies significantly depending on the number of cross-domain real images used and the specific test set. The study reveals that synthetic data is most effective for medium-confidence detections and that its ability to replace real data differs across test sets due to varying potential false positives.

## Method Summary
The study uses Progressive Transformation Learning (PTL) with 5 iterations to progressively add synthetic data transformed to look real via syn2real transformation. VisDrone serves as the source domain for training while Okutama-Action, ICG, HERIDAL, and SARD serve as target domains for testing. The synthetic dataset is Archangel-Synthetic, generated for human detection with diverse poses and perspectives. The authors evaluate using standard detection metrics (AP and AP[.5:.95]) and introduce new metrics (train2test distance using Mahalanobis distance, and APt2t based on train2test distance) to analyze representation capability across confidence levels.

## Key Results
- Synthetic data has the greatest impact on medium-confidence detections
- Synthetic data's ability to replace real data improves as more real training images are used
- The impact of synthetic data on replacing real data varies across test sets due to different potential false positives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data most effectively improves training performance when it fills the representation gap for medium-confidence detections.
- Mechanism: Medium-confidence detections are cases where the model's learned features are neither perfectly aligned with training examples (high-confidence) nor completely misaligned (low-confidence). Synthetic data expands the feature space coverage, allowing these borderline cases to be better represented, thus improving precision without overfitting.
- Core assumption: The train2test distance metric accurately reflects feature similarity in the penultimate layer, and the model outputs follow a sigmoid distribution.
- Evidence anchors:
  - [abstract] "synthetic data has the greatest impact on medium-confidence detections"
  - [section] "For the above-medium-confidence detections, AP t2t increases as more real images are used in training only when synthetic data is also used in training."
  - [corpus] Weak evidence; no direct neighbor papers discuss confidence-level-specific impacts of synthetic data.
- Break condition: If the feature space does not follow a Gaussian distribution or the model does not use sigmoid activation, the train2test distance loses validity.

### Mechanism 2
- Claim: The impact of synthetic data on replacing real data varies across test sets due to differences in potential false positives.
- Mechanism: Different test sets contain different background patterns or distractors that can be mistaken for the object of interest. Synthetic data, while improving representation of true positives consistently, differentially affects the ability to reject false positives depending on the specific distractor distribution in each test set.
- Evidence anchors:
  - [abstract] "synthetic data's ability to replace real data varies across test sets due to different potential false positives"
  - [section] "the effect of reducing the train2test distances of FP by using the synthetic data depends on the test set"
  - [corpus] No direct evidence in neighbor papers; this appears to be a novel finding of the paper.
- Break condition: If false positive patterns are identical across all test sets, the variation in synthetic data impact would disappear.

### Mechanism 3
- Claim: The ability of synthetic data to replace real data improves as more real training images are used.
- Mechanism: Real images provide the foundational distribution that synthetic data augments. With more real images, the training set better captures the true data distribution, allowing synthetic data to effectively fill gaps rather than distort the learned representation. This synergistic effect is strongest for medium-confidence detections.
- Evidence anchors:
  - [abstract] "synthetic data's impact on training performance varies depending on the number of cross-domain real images"
  - [section] "The use of synthetic data affects medium-confidence detections greater than other detections" and "The ability of synthetic data to replace real data improves as more real training images are used."
  - [corpus] Weak evidence; neighbor papers discuss scaling behavior but not the specific synergy between real and synthetic data quantities.
- Break condition: If synthetic data introduces domain shift that cannot be corrected by additional real images, the beneficial scaling effect would break down.

## Foundational Learning

- Concept: Train2test distance as a metric for representation capability
  - Why needed here: The paper uses this metric to quantify how well the training set represents test instances, which is central to understanding synthetic data's impact.
  - Quick check question: What mathematical formulation does the train2test distance use to measure the gap between training and test instances?

- Concept: Average precision based on train2test distance (APt2t)
  - Why needed here: APt2t is introduced as a novel metric to analyze how representation ability varies with detection confidence levels, enabling the paper's key insights about medium-confidence detections.
  - Quick check question: How does APt2t differ from conventional average precision in its calculation and interpretation?

- Concept: Progressive Transformation Learning (PTL) methodology
  - Why needed here: PTL is the specific method used to incorporate synthetic data into training, and understanding its iterative nature is crucial for interpreting the experimental results.
  - Quick check question: What is the core principle behind PTL's iterative addition of synthetic data subsets?

## Architecture Onboarding

- Component map:
  - Data pipeline: Real image loaders (VisDrone, Okutama-Action, ICG, HERIDAL, SARD) → Synthetic image generator (Archangel-Synthetic) → PTL data selection module
  - Model: Base detector (likely RetinaNet or similar sigmoid-based architecture) → Feature extractor → Detection head
  - Evaluation: Detection metrics (AP, AP[.5:.95]) → Train2test distance calculator → APt2t metric calculator
  - Training loop: PTL iterations → Synthetic data transformation → Model training → Performance evaluation

- Critical path:
  1. Load real training images from source domain
  2. Generate or load synthetic images
  3. Apply PTL to select and transform synthetic subset
  4. Train model on combined dataset
  5. Evaluate on target domain test set
  6. Calculate train2test distance and APt2t metrics
  7. Analyze impact based on confidence levels

- Design tradeoffs:
  - Synthetic data realism vs. diversity: More realistic synthetic data may cover less diverse scenarios
  - Number of PTL iterations: More iterations increase computational cost but may improve synthetic data selection
  - Detection score thresholds: Different thresholds reveal different aspects of model performance but require separate analyses

- Failure signatures:
  - If APt2t shows no improvement with synthetic data, the synthetic data may not be properly aligned with the feature space
  - If train2test distance distributions for TP and FP overlap significantly, the model cannot distinguish true from false positives
  - If detection performance plateaus early, the synthetic data may not be providing new information beyond what real data offers

- First 3 experiments:
  1. Run PTL with only 1 iteration to establish baseline performance and verify the data pipeline works
  2. Compare train2test distance distributions for TP and FP with and without synthetic data on a single test set
  3. Measure APt2t at different detection score thresholds (0.01, 0.1, 0.5) to confirm the medium-confidence effect before scaling up experiments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of synthetic images to include in training when using PTL (Progressive Transformation Learning) with varying numbers of cross-domain real images?
- Basis in paper: [inferred] The paper discusses how the impact of synthetic data varies depending on the number of cross-domain real images, but doesn't provide a specific optimal ratio or number of synthetic images.
- Why unresolved: The paper focuses on analyzing the impact of synthetic data rather than optimizing the exact ratio of synthetic to real images in training.
- What evidence would resolve it: Experiments varying the proportion of synthetic images in the training set while keeping the number of cross-domain real images constant, and measuring the resulting detection accuracy.

### Open Question 2
- Question: How does the train2test distance metric correlate with downstream task performance metrics like AP and AP[.5:.95] across different datasets and detection tasks?
- Basis in paper: [explicit] The paper introduces train2test distance and APt2t metrics to evaluate representation capability, but doesn't directly correlate these with standard detection performance metrics.
- Why unresolved: The relationship between these representation metrics and actual task performance needs further empirical validation.
- What evidence would resolve it: Correlation analysis between train2test distance, APt2t, and standard detection metrics (AP, AP[.5:.95]) across multiple datasets and tasks.

### Open Question 3
- Question: What specific characteristics of false positives in different test sets make synthetic data more or less effective at reducing them?
- Basis in paper: [explicit] The paper concludes that the impact of synthetic data varies across test sets primarily due to different impacts on false positives, but doesn't specify which characteristics of false positives are most influential.
- Why unresolved: The paper identifies that false positives are key but doesn't analyze their specific characteristics (e.g., background complexity, object size, occlusion patterns).
- What evidence would resolve it: Detailed analysis of false positive types in each test set and their relationship to synthetic data effectiveness, potentially using clustering or other feature analysis methods.

### Open Question 4
- Question: How does the effectiveness of synthetic data differ across detection tasks beyond human detection, such as vehicle detection or animal detection?
- Basis in paper: [inferred] The paper focuses exclusively on UAV-view human detection, but the synthetic data generation and PTL methodology could potentially be applied to other detection tasks.
- Why unresolved: The paper doesn't explore synthetic data effectiveness on other object categories or detection tasks.
- What evidence would resolve it: Replicating the experiments with different object categories while keeping the synthetic data generation and training methodology constant.

### Open Question 5
- Question: What is the long-term generalization performance of models trained with synthetic data compared to those trained with real data, particularly as new test sets are introduced?
- Basis in paper: [inferred] The paper evaluates performance on specific test sets but doesn't examine long-term generalization or performance on future unseen data distributions.
- Why unresolved: The paper focuses on immediate performance on existing test sets rather than long-term model robustness.
- What evidence would resolve it: Long-term studies tracking model performance across multiple test set updates and distribution shifts over time.

## Limitations

- The specific model architecture used in the experiments is not mentioned, which could affect reproducibility
- Details on the syn2real transformation process are not provided, which is critical for synthetic data effectiveness
- The study focuses exclusively on UAV-view human detection, limiting generalizability to other domains

## Confidence

- Medium: Synthetic data's differential impact across confidence levels (mechanism 1)
- Medium: Synthetic data impact variation across test sets (mechanism 2)
- Low: Synthetic data's ability to replace real data improves with more real images (mechanism 3)

## Next Checks

1. Validate train2test distance metric assumptions by testing feature space distribution characteristics across different model architectures
2. Replicate experiments with different synthetic data generation approaches to isolate the impact of syn2real transformation quality
3. Test the medium-confidence effect across additional detection tasks beyond UAV-view human detection to assess generalizability