---
ver: rpa2
title: Continual Deep Reinforcement Learning to Prevent Catastrophic Forgetting in
  Jamming Mitigation
arxiv_id: '2410.10521'
source_url: https://arxiv.org/abs/2410.10521
tags:
- learning
- jamming
- scenario
- reward
- jammer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in deep reinforcement
  learning (DRL) for anti-jamming wireless systems. The proposed method uses PackNet-based
  parameter isolation to preserve previously learned jammer patterns while adapting
  to new ones, enabling continual learning without performance degradation.
---

# Continual Deep Reinforcement Learning to Prevent Catastrophic Forgetting in Jamming Mitigation

## Quick Facts
- arXiv ID: 2410.10521
- Source URL: https://arxiv.org/abs/2410.10521
- Reference count: 28
- Proposed PackNet-based parameter isolation prevents catastrophic forgetting in anti-jamming DRL, achieving mean reward of 0.829 vs 0.080 for standard DRL in retention tests.

## Executive Summary
This paper addresses catastrophic forgetting in deep reinforcement learning (DRL) for anti-jamming wireless systems. The proposed method uses PackNet-based parameter isolation to preserve previously learned jammer patterns while adapting to new ones, enabling continual learning without performance degradation. In Scenario 3A, PackNet achieved a mean reward of 0.829 versus 0.080 for standard DRL, demonstrating successful retention of prior knowledge. In Scenario 3B, PackNet converged 20-40 epochs faster than baseline methods while achieving superior anti-jamming performance (mean reward 1.405 vs 1.365). The approach enables DRL agents to learn new jamming patterns efficiently while maintaining effectiveness against previously encountered threats, resulting in more robust and adaptive anti-jamming systems.

## Method Summary
The method employs PackNet-based parameter isolation within DRL agents (DQN and SAC) to prevent catastrophic forgetting during sequential learning of different jamming patterns. The approach freezes parameters learned for earlier environments while training new ones, allowing the agent to retain performance on previously encountered jammer patterns. The reward function uses normalized and scaled spectral efficiency based on SINR and MCS. Three jamming scenarios are implemented: Env 1 (linear sweeping), Env 2 (non-uniform power allocation), and Env 3 (combined patterns), with evaluation focusing on retention and convergence speed.

## Key Results
- PackNet achieved mean reward of 0.829 versus 0.080 for standard DRL in Scenario 3A, demonstrating successful retention of previously learned jammer patterns
- In Scenario 3B, PackNet converged 20-40 epochs faster than baseline methods while achieving superior anti-jamming performance (mean reward 1.405 vs 1.365)
- The approach enables DRL agents to learn new jamming patterns efficiently while maintaining effectiveness against previously encountered threats

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PackNet-based parameter isolation prevents catastrophic forgetting by freezing parameters learned for earlier tasks while training new ones.
- Mechanism: During sequential learning of Env 1 → Env 2 → Env 3, PackNet maintains a separate set of weights for each environment. When the agent returns to a previously learned task, the frozen parameters ensure the performance is preserved without relearning.
- Core assumption: The key weights for each task can be isolated and frozen without affecting the adaptability to new tasks.
- Evidence anchors:
  - [abstract] "By employing PackNet in the context of DRL for anti-jamming, we aim to develop a robust, adaptive, and continuously learning system capable of mitigating the effects of a wide range of jamming attacks without forgetting previously learned strategies."
  - [section] "PackNet introduces a structured approach to neural network parameter management, allowing the network to retain knowledge of old tasks while learning new ones efficiently."
  - [corpus] Weak evidence in corpus - no direct mention of PackNet in related papers, though continual learning is a common theme.
- Break condition: If the parameter isolation is not precise, the frozen weights might interfere with new task learning or the new weights might not adequately cover the new task space.

### Mechanism 2
- Claim: The reward function based on normalized and scaled spectral efficiency provides a physics-informed signal that guides the agent toward efficient anti-jamming strategies.
- Mechanism: By scaling the reward according to spectral efficiency, the agent is incentivized to not only avoid collisions but also optimize the modulation coding scheme (MCS) based on SINR, leading to better resource utilization.
- Core assumption: Spectral efficiency is a meaningful proxy for the agent's performance in anti-jamming, capturing both collision avoidance and transmission quality.
- Evidence anchors:
  - [abstract] "Reward engineering [18] is increasingly becoming an important part of the DRL framework. For the anti-jamming framework considered in this paper, we assign the scaled and normalized spectral efficiency as the reward function."
  - [section] "As reinforcement learning environments typically use positive rewards to encourage behavior and negative ones for the negative feedback, we apply standard normalization the spectral efficiency of the data to obtain the reward function."
  - [corpus] Weak evidence - corpus does not discuss reward engineering in anti-jamming contexts.
- Break condition: If the reward scaling does not accurately reflect the real-world performance or if the agent exploits loopholes in the reward definition.

### Mechanism 3
- Claim: Faster convergence in Scenario 3B (20-40 epochs) is achieved because PackNet leverages prior knowledge efficiently through parameter isolation and pruning.
- Mechanism: By retaining and refining the parameters from previous environments, PackNet reduces the exploration space for the new task, allowing the agent to converge more quickly than methods that start from scratch or only fine-tune without isolation.
- Core assumption: The overlap between tasks is sufficient that retaining parameters provides a meaningful head start, but not so much that it causes interference.
- Evidence anchors:
  - [abstract] "In Scenario 3B, PackNet converged 20-40 epochs faster than baseline methods while achieving superior anti-jamming performance."
  - [section] "PackNet method approaches this point 20-40 epochs faster than both methods."
  - [corpus] No direct evidence in corpus - related papers do not discuss convergence speed in continual learning anti-jamming.
- Break condition: If the tasks are too dissimilar, the retained parameters may mislead the agent, slowing down or preventing effective learning.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper's central problem is that standard DRL agents lose performance on old jammer patterns when learning new ones, which is unacceptable in anti-jamming where past threats may reappear.
  - Quick check question: If an agent learns to avoid jammer A, then learns to avoid jammer B, what happens to its ability to avoid jammer A without intervention?

- Concept: Parameter isolation and pruning in neural networks
  - Why needed here: PackNet's core mechanism is to isolate and freeze parameters for each task, allowing the network to retain old knowledge while adapting to new tasks. Understanding this is essential to grasp how the method prevents forgetting.
  - Quick check question: In PackNet, after learning task 1, what happens to the weights associated with task 1 when the network starts learning task 2?

- Concept: Reward engineering and shaping in reinforcement learning
  - Why needed here: The reward function is designed to reflect both collision avoidance and spectral efficiency, guiding the agent toward realistic anti-jamming strategies. Misunderstanding the reward could lead to incorrect expectations of agent behavior.
  - Quick check question: Why does the reward function use normalized spectral efficiency instead of a simple +1/-1 collision signal?

## Architecture Onboarding

- Component map:
  - Environment simulators (Env 1, Env 2, Env 3) with different jammer patterns
  - DRL agent (DQN or SAC) with a fully connected feedforward network (6 layers, 256 neurons each, ReLU activations)
  - PackNet continual learning module (parameter isolation, pruning, retraining)
  - Reward engine (normalized and scaled spectral efficiency based on SINR and MCS)
  - Evaluation harness (ns-3 gym environment, multiple seeds, statistical analysis)

- Critical path:
  1. Initialize agent and environment
  2. Train on Env 1 (baseline)
  3. Apply PackNet: freeze Env 1 parameters, prune, retrain on Env 2
  4. Repeat for Env 3
  5. Evaluate retention on previous environments
  6. Measure convergence speed and mean reward

- Design tradeoffs:
  - Pruning ratio (50% vs 75%): Higher pruning risks more performance drop but may yield better isolation
  - Number of retraining steps after pruning: Too few may leave the agent underprepared for the new task
  - Reward scaling: Must balance between collision avoidance and spectral efficiency optimization

- Failure signatures:
  - Mean reward collapses on previous environments after learning new ones (catastrophic forgetting not prevented)
  - Convergence is slower than baseline (parameter isolation interfering with new learning)
  - Reward plateaus at suboptimal levels (reward engineering not aligned with true objectives)

- First 3 experiments:
  1. Run Scenario 1 (Env 1 only) with DQN and SAC to establish baseline performance and confirm predictable jammer pattern is learned.
  2. Run Scenario 2 with No pretraining, Pretrained, and PackNet agents to observe initial forgetting and PackNet's mitigation effect.
  3. Run Scenario 3A (Env 2 → Env 1) to explicitly test retention of old knowledge after learning a new task, comparing all three agent types.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the PackNet approach scale when learning from more than three sequential jamming patterns in dynamic wireless environments?
- Basis in paper: [explicit] The paper evaluates PackNet's performance on three scenarios but only sequential learning of two new patterns (Env 2 and Env 3 after initial Env 1)
- Why unresolved: The evaluation only covers three environments, leaving uncertainty about performance degradation or memory capacity limits with more sequential tasks
- What evidence would resolve it: Testing PackNet with five or more sequential jamming pattern environments while tracking mean reward retention and convergence speed

### Open Question 2
- Question: What is the minimum pruning ratio that still maintains acceptable performance without catastrophic forgetting in the anti-jamming application?
- Basis in paper: [explicit] The paper mentions pruning ratios of 50% or 75% but doesn't explore the sensitivity of performance to different pruning levels
- Why unresolved: The optimal pruning ratio for balancing parameter efficiency and performance retention is not investigated
- What evidence would resolve it: Systematic evaluation of PackNet performance across a range of pruning ratios (25%, 50%, 75%, 90%) while measuring catastrophic forgetting resistance

### Open Question 3
- Question: How does the proposed approach handle zero-day jamming patterns that differ significantly from previously encountered patterns?
- Basis in paper: [inferred] The paper demonstrates effectiveness against known patterns but doesn't address adaptation to completely novel jamming strategies
- Why unresolved: Real-world jammers may employ entirely new tactics that weren't part of the training sequence
- What evidence would resolve it: Testing PackNet against a completely novel jamming pattern (e.g., frequency hopping across all channels) after training on the three presented scenarios, measuring adaptation speed and performance retention

### Open Question 4
- Question: What is the computational overhead of PackNet compared to standard DRL methods in real-time anti-jamming applications?
- Basis in paper: [explicit] The paper mentions faster convergence for PackNet but doesn't quantify the computational cost or real-time feasibility
- Why unresolved: Parameter isolation and pruning operations may introduce latency that affects real-time anti-jamming performance
- What evidence would resolve it: Measuring inference time, memory usage, and power consumption of PackNet versus standard DRL methods during continuous operation in a simulated real-time environment

## Limitations
- Lack of empirical validation for PackNet implementation details, creating uncertainty about whether reported performance improvements are directly attributable to PackNet
- Restricted scope of evaluation environments using only three synthetic jamming scenarios in simulation
- Missing comparison to simpler continual learning approaches (e.g., elastic weight consolidation, rehearsal methods)

## Confidence
- **High Confidence**: The existence of catastrophic forgetting in standard DRL approaches for anti-jamming (validated by Scenario 3A results showing Pretrained agent dropping from mean reward 0.829 to 0.080)
- **Medium Confidence**: The PackNet method's effectiveness in preventing catastrophic forgetting (supported by numerical results but lacking implementation details)
- **Medium Confidence**: The faster convergence claim in Scenario 3B (20-40 epochs improvement) - results are presented but methodology for measuring convergence is not fully specified
- **Low Confidence**: The reward engineering approach's optimality for anti-jamming tasks (based on normalized spectral efficiency) - this is theoretically sound but lacks comparison to alternative reward structures

## Next Checks
1. **Implementation Verification**: Replicate the PackNet parameter isolation and pruning procedure with the exact specifications used in the paper to verify whether the 20-40 epoch convergence improvement is reproducible.
2. **Generalization Testing**: Evaluate the trained agents on additional jamming scenarios not seen during training to assess whether PackNet's benefits extend beyond the three environments studied.
3. **Ablation Study**: Conduct experiments comparing PackNet with simpler continual learning approaches (e.g., elastic weight consolidation, rehearsal methods) using identical anti-jamming environments to isolate the specific contribution of parameter isolation.