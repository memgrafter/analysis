---
ver: rpa2
title: Are LLMs classical or nonmonotonic reasoners? Lessons from generics
arxiv_id: '2406.06590'
source_url: https://arxiv.org/abs/2406.06590
tags:
- reasoning
- generics
- llms
- exception
- instantiation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) exhibit nonmonotonic reasoning patterns
  on generics (e.g., "Birds fly") when exceptions are introduced (e.g., "Penguins
  don't fly"), mirroring human reasoning. However, LLMs fail to maintain stable beliefs
  when presented with supporting examples (e.g., "Owls fly") or unrelated information
  (e.g., "Lions have manes").
---

# Are LLMs classical or nonmonotonic reasoners? Lessons from generics

## Quick Facts
- arXiv ID: 2406.06590
- Source URL: https://arxiv.org/abs/2406.06590
- Authors: Alina Leidinger; Robert van Rooij; Ekaterina Shutova
- Reference count: 24
- Large language models (LLMs) exhibit nonmonotonic reasoning patterns on generics (e.g., "Birds fly") when exceptions are introduced (e.g., "Penguins don't fly"), mirroring human reasoning. However, LLMs fail to maintain stable beliefs when presented with supporting examples (e.g., "Owls fly") or unrelated information (e.g., "Lions have manes"). Across seven state-of-the-art LLMs tested on two datasets, agreement with generics dropped significantly with all types of exemplars, with some models' agreement falling below 10% in certain conditions. Only OpenHermes-2.5-Mistral-7B showed relatively consistent reasoning across tasks. The study highlights challenges in attributing human-like reasoning to LLMs and assessing their reasoning capabilities, emphasizing the need for evaluation metrics that include logical consistency and robustness measures.

## Executive Summary
This study investigates whether large language models (LLMs) reason about generics in a human-like, nonmonotonic way or through classical logic. The researchers tested seven state-of-the-art LLMs on their ability to reason about generic statements (e.g., "Birds fly") when presented with different types of examples: exceptions (e.g., "Penguins don't fly"), supporting examples (e.g., "Owls fly"), and unrelated information (e.g., "Lions have manes"). The results reveal that while LLMs can exhibit nonmonotonic reasoning similar to humans when encountering exceptions, they struggle to maintain stable beliefs when presented with supporting examples or irrelevant information. This inconsistency across tasks raises important questions about how we assess and interpret LLM reasoning capabilities.

## Method Summary
The researchers tested seven state-of-the-art LLMs using two datasets containing generic statements. Each LLM was presented with generic statements in various conditions: in isolation, with exceptions, with supporting examples, and with unrelated information. The experiments used different prompt templates and instructions to test the models' reasoning. Responses were converted to agree/disagree labels through pattern matching, and statistical tests compared agreement rates across conditions. The study employed majority voting across different instructions to mitigate prompt sensitivity issues.

## Key Results
- LLMs show human-like nonmonotonic reasoning when exceptions are introduced to generics, with agreement dropping significantly in this condition.
- LLMs fail to maintain stable beliefs when presented with supporting examples or unrelated information, with agreement dropping to below 10% for some models in certain conditions.
- OpenHermes-2.5-Mistral-7B showed more consistent reasoning across tasks compared to other models tested.
- Across all seven LLMs tested, agreement with generics decreased significantly when any type of exemplar was introduced, not just exceptions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs show human-like nonmonotonic reasoning when exceptions are introduced to generics.
- Mechanism: The model's internal representations allow it to maintain a defeasible inference ("Birds fly") despite encountering a known exception ("Penguins don't fly"). This mirrors human reasoning where generics are treated as default rules rather than universal truths.
- Core assumption: The LLM's training data included sufficient examples of exceptions to generics, allowing it to learn the nonmonotonic reasoning pattern.
- Evidence anchors:
  - [abstract] "While LLMs exhibit reasoning patterns in accordance with human nonmonotonic reasoning abilities..."
  - [section 5.1] "Since humans maintain their beliefs about truth conditions of generics...we examine whether challenging LLMs with an exception decreases their agreement to generics significantly."
  - [corpus] Weak - corpus doesn't provide direct evidence for this mechanism
- Break condition: If the model's training data lacked sufficient examples of exceptions to generics, or if the prompt format didn't trigger the nonmonotonic reasoning pattern.

### Mechanism 2
- Claim: LLMs fail to maintain stable beliefs when presented with supporting examples or unrelated information.
- Mechanism: The model's response to a generic is influenced by the immediate context (the exemplar) rather than maintaining a stable belief about the generic itself. This suggests the model treats each prompt as a standalone question rather than maintaining consistent reasoning across related prompts.
- Core assumption: The model's context window and attention mechanism prioritize the most recent information over maintaining consistency with prior responses.
- Evidence anchors:
  - [abstract] "they fail to maintain stable beliefs on truth conditions of generics at the addition of supporting examples..."
  - [section 5.2] "Similarly, most LLMs are not able to disregard irrelevant random exemplars..."
  - [corpus] Weak - corpus doesn't provide direct evidence for this mechanism
- Break condition: If the model was specifically fine-tuned for consistency across related prompts, or if a different prompt format (e.g., chain-of-thought) encouraged more stable reasoning.

### Mechanism 3
- Claim: OpenHermes-2.5-Mistral-7B shows more consistent reasoning across tasks due to additional code training data.
- Mechanism: Training on code data improves the model's ability to follow logical rules and maintain consistency, which transfers to natural language reasoning tasks.
- Core assumption: The skills learned from code training (e.g., following strict syntax, maintaining logical consistency) are transferable to reasoning about natural language generics.
- Evidence anchors:
  - [section 5.2] "Notably, OpenHermes is the only model which has been trained on additional code data which has been shown to also help reasoning in natural language..."
  - [corpus] Weak - corpus doesn't provide direct evidence for this mechanism
- Break condition: If the code training data was not diverse enough to improve general reasoning, or if the model's architecture limited the transfer of these skills to natural language tasks.

## Foundational Learning

- Concept: Nonmonotonic reasoning
  - Why needed here: The paper tests whether LLMs can reason about generics in a way that's consistent with human nonmonotonic reasoning, where a hypothesis can be overturned by new information.
  - Quick check question: What's the difference between monotonic and nonmonotonic reasoning, and why is the latter important for human cognition?

- Concept: Generics and exceptions
  - Why needed here: The experiments use generics (e.g., "Birds fly") and test how LLMs respond when exceptions (e.g., "Penguins don't fly") are introduced.
  - Quick check question: How do humans typically reason about generics when they know exceptions exist, and how does this differ from classical logic?

- Concept: Prompt engineering
  - Why needed here: The paper uses different prompt templates and instructions to test LLM reasoning, and the results vary based on the prompt format.
  - Quick check question: How can small changes in prompt wording affect an LLM's response, and what strategies can be used to encourage consistent reasoning?

## Architecture Onboarding

- Component map: Encoder -> Attention mechanism -> Decoder
- Critical path: 1) Prompt formatting and instruction selection, 2) LLM inference with temperature=0 for reproducibility, 3) Pattern matching to convert LLM responses to agree/disagree labels, 4) Statistical testing to compare agreement rates across different prompt conditions
- Design tradeoffs: The paper uses a simple yes/no response format to enable consistent comparison across models, but this may limit the LLM's ability to express nuanced reasoning. The use of majority voting across different instructions helps mitigate the impact of prompt sensitivity, but may not capture all aspects of the model's reasoning.
- Failure signatures: If an LLM shows low agreement rates with generics even in the absence of exemplars, this suggests the model has difficulty with the concept of generics. If an LLM shows inconsistent responses to the same generic across different prompt conditions, this suggests the model is sensitive to prompt formatting.
- First 3 experiments:
  1. Test the LLM's agreement with a generic in isolation to establish a baseline.
  2. Test the LLM's agreement with the same generic when an exception is introduced to see if it shows nonmonotonic reasoning.
  3. Test the LLM's agreement with the generic when a supporting example is introduced to see if it maintains a stable belief.

## Open Questions the Paper Calls Out
None

## Limitations
- The study demonstrates significant prompt sensitivity in LLM responses, which affects the reliability of reasoning assessments and may reflect artifacts of prompting rather than genuine reasoning differences.
- Each LLM is tested on only 10-20 generic statements across two datasets, which may not capture the full range of reasoning capabilities or limitations.
- The binary agree/dislike classification may oversimplify nuanced reasoning that LLMs could potentially exhibit.

## Confidence
- High Confidence: The finding that LLMs fail to maintain stable beliefs when presented with supporting examples or unrelated information.
- Medium Confidence: The claim that LLMs exhibit human-like nonmonotonic reasoning when exceptions are introduced.
- Medium Confidence: The observation that OpenHermes-2.5-Mistral-7B shows more consistent reasoning due to code training.

## Next Checks
1. Test each LLM on a larger, more diverse set of 50-100 generic statements spanning multiple semantic domains to assess whether the observed patterns generalize beyond the current limited sample.
2. Systematically vary prompt wording, temperature settings, and response formats to quantify the extent to which observed reasoning patterns are artifacts of prompting rather than genuine reasoning capabilities.
3. Design a new evaluation framework that measures not just agreement rates but also logical consistency across related prompts, including tracking whether a model's responses to related generics form a coherent reasoning pattern rather than treating each as an independent question.