---
ver: rpa2
title: Exploring the Effect of Explanation Content and Format on User Comprehension
  and Trust in Healthcare
arxiv_id: '2408.17401'
source_url: https://arxiv.org/abs/2408.17401
tags:
- explanations
- explanation
- were
- risk
- trust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined how explanation content and format affect user
  comprehension and trust in AI-driven healthcare tools, specifically the QCancer
  algorithm for cancer risk prediction. Researchers compared two feature attribution
  methods (SHAP and Occlusion-1) across two formats (charts and text) using surveys
  with general public and medical students.
---

# Exploring the Effect of Explanation Content and Format on User Comprehension and Trust in Healthcare

## Quick Facts
- arXiv ID: 2408.17401
- Source URL: https://arxiv.org/abs/2408.17401
- Reference count: 40
- One-line primary result: Text-based explanations significantly outperform chart-based explanations for both comprehension and trust, while the choice of feature attribution method (SHAP vs. Occlusion-1) has less impact when format is controlled

## Executive Summary
This study investigates how explanation content and format affect user comprehension and trust in AI-driven healthcare tools, specifically the QCancer algorithm for cancer risk prediction. Researchers compared two feature attribution methods (SHAP and Occlusion-1) across two formats (charts and text) using surveys with general public and medical students. The findings reveal that explanation format plays a more critical role than content complexity in fostering user understanding and trust. Text-based explanations consistently showed higher comprehension and trust compared to chart-based explanations, with this effect persisting across different user expertise levels. Interestingly, the advantage of Occlusion-1 over SHAP was primarily driven by format preferences rather than inherent differences in the explanation content itself.

## Method Summary
The study employed two user studies with different samples: one from the general population and one from medical students. Researchers created twenty fictitious vignettes adapted from clinical cases describing patients with symptoms suggestive of gastro-oesophageal cancer. The QCancer algorithm generated risk predictions for these cases, which were then explained using either SHAP or Occlusion-1 feature attribution methods in both chart and text formats. Participants were randomly assigned to view different combinations of these explanations and completed surveys measuring subjective comprehension, trust, and objective comprehension through definition recognition tasks. The analysis used generalized linear and linear mixed-effects regression models to assess the impact of explanation format and content on user outcomes.

## Key Results
- Text-based Occlusion-1 explanations consistently showed higher comprehension and trust compared to chart-based SHAP explanations
- The advantage of Occlusion-1 over SHAP was primarily driven by format preferences rather than inherent content differences
- Medical students demonstrated higher objective comprehension scores compared to the general public, particularly for Occlusion-1 explanations
- The effect of explanation format on comprehension and trust was consistent across both user groups

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Explanation format (text vs. chart) has a stronger impact on comprehension and trust than the choice of feature attribution method (SHAP vs. Occlusion-1).
- **Mechanism**: Text-based explanations are easier for users to interpret because they provide a direct, narrative-like description of how features influence predictions, whereas charts require additional cognitive effort to decode visual encodings.
- **Core assumption**: Users process textual information more efficiently than visual encodings when the explanations involve simple counterfactual modifications.
- **Evidence anchors**:
  - [abstract]: "Text-based Occlusion-1 explanations consistently showed higher comprehension and trust compared to chart-based SHAP explanations."
  - [section]: "Our experiments showed higher subjective comprehension and trust for Occlusion-1 over SHAP explanations based on content. However, when controlling for format, only OT outperformed SC, suggesting this trend is driven by preferences for text."
  - [corpus]: Weak evidence; no direct corpus support for this mechanism.
- **Break condition**: If users have high graph literacy or the explanation content is too complex for text to capture, the advantage of text format may diminish.

### Mechanism 2
- **Claim**: Simpler feature attribution methods (Occlusion-1) are perceived as more comprehensible and trustworthy than complex methods (SHAP), but this effect is mediated by format rather than intrinsic to the method.
- **Mechanism**: Occlusion-1 explanations are inherently simpler because they involve a single, intuitive counterfactual change, making them easier to describe in text. However, when both methods are presented as charts, the simplicity advantage disappears.
- **Core assumption**: The perceived simplicity of Occlusion-1 is primarily due to its amenability to textual description rather than its mathematical simplicity.
- **Evidence anchors**:
  - [abstract]: "While Occlusion-1 explanations were generally preferred, the advantage was primarily driven by the text format rather than the content itself."
  - [section]: "Differently from existing studies, we control for the format of explanations (chart or text) to examine their effect on this paradigm."
  - [corpus]: Weak evidence; no direct corpus support for this mechanism.
- **Break condition**: If users are familiar with game-theoretic concepts underlying SHAP, they may find SHAP explanations more trustworthy despite format.

### Mechanism 3
- **Claim**: User expertise (general public vs. medical students) moderates the effect of explanation format on comprehension and trust.
- **Mechanism**: Medical students, due to their domain knowledge, may be better equipped to interpret charts and may not rely as heavily on textual explanations as the general public.
- **Core assumption**: Domain expertise influences the preferred modality for consuming explanations.
- **Evidence anchors**:
  - [abstract]: "We undertake this study, in collaboration with healthcare practitioners, by deploying different, well-known FA explanation methods, SHAP [26] and Occlusion-1 [2] (two of the most cited works in this area), for the QCancer algorithm in an online system, providing a useful tool for future research. We undertook two user studies, each with a different group of participants, one sampled from the general population and one from students in medical training, to examine the effect of different stakeholders' expertise and explanation goals [10], which are important factors in the evaluation of explanations [19, 13]."
  - [section]: "We assessed the RQs in two different samples: one from the general population where a limited number of participants had medical experience, representing stakeholders in the form of patients (Study 1), and an expert sample where participants had some medical training and experience, representing stakeholders in the form of healthcare practitioners (Study 2)."
  - [corpus]: Weak evidence; no direct corpus support for this mechanism.
- **Break condition**: If users have similar levels of domain expertise, the moderating effect of expertise may not be observed.

## Foundational Learning

- **Concept**: Feature Attribution Methods
  - Why needed here: Understanding the difference between SHAP and Occlusion-1 is crucial for interpreting the study's findings about explanation content.
  - Quick check question: What is the key difference between how SHAP and Occlusion-1 compute feature importance?

- **Concept**: Explanation Format
  - Why needed here: The study compares text and chart formats, so understanding the implications of each format is essential.
  - Quick check question: How might the cognitive load of interpreting charts differ from that of reading text explanations?

- **Concept**: User Comprehension and Trust Metrics
  - Why needed here: The study measures both subjective comprehension and trust, as well as an objective comprehension measure, so understanding these metrics is important.
  - Quick check question: What is the difference between subjective and objective measures of comprehension?

## Architecture Onboarding

- **Component map**:
  - Vignette Generator -> QCancer Algorithm -> Explanation Engine (SHAP/Text, SHAP/Chart, Occlusion-1/Text, Occlusion-1/Chart) -> Survey Platform -> Data Analysis Pipeline

- **Critical path**:
  1. Generate vignettes with patient data.
  2. Compute QCancer predictions.
  3. Generate explanations using SHAP and Occlusion-1.
  4. Present vignettes, predictions, and explanations to users via survey.
  5. Collect user responses on comprehension and trust.
  6. Analyze data to determine the impact of explanation format and content.

- **Design tradeoffs**:
  - Text vs. Chart: Text explanations are easier to generate and may be more comprehensible, but charts can convey complex relationships more efficiently.
  - SHAP vs. Occlusion-1: SHAP is more mathematically complex but may be more accurate, while Occlusion-1 is simpler but may be less precise.
  - Vignette complexity: More complex vignettes may better represent real-world scenarios but may also increase cognitive load for users.

- **Failure signatures**:
  - Low response rates on free-text questions indicate users may not have engaged deeply with the explanations.
  - Floor effects in objective comprehension measures suggest the task may have been too difficult.
  - Contradictory results between studies indicate potential issues with sample size or participant expertise.

- **First 3 experiments**:
  1. Compare comprehension and trust ratings between text and chart formats for a single feature attribution method.
  2. Compare comprehension and trust ratings between SHAP and Occlusion-1 for a single format.
  3. Test the effect of user expertise on the preferred explanation format and method.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of explanation format (chart vs. text) on comprehension and trust for different levels of user expertise (lay vs. medical professionals)?
- Basis in paper: Explicit - The study found that text-based explanations were generally preferred over chart-based ones, but the effect was not consistent across all user groups.
- Why unresolved: The study only compared two formats (chart and text) for Occlusion-1 explanations, not for SHAP. It also focused on medical students and the general public, not experienced healthcare practitioners.
- What evidence would resolve it: A study comparing chart and text formats for both SHAP and Occlusion-1 explanations across different levels of user expertise, including experienced healthcare practitioners.

### Open Question 2
- Question: How do different XAI methods (beyond SHAP and Occlusion-1) affect user comprehension and trust in healthcare AI systems?
- Basis in paper: Explicit - The authors state that they restricted their focus to SHAP and Occlusion-1 due to their prominence, but acknowledge that other methods exist.
- Why unresolved: The study only examined two specific XAI methods, leaving the impact of other methods unknown.
- What evidence would resolve it: A study comparing multiple XAI methods (e.g., LIME, counterfactual explanations) across different healthcare applications and user groups.

### Open Question 3
- Question: What is the relationship between self-reported understanding and actual understanding of AI explanations in healthcare?
- Basis in paper: Explicit - The study used both self-reported measures and an objective comprehension test, finding that performance on the objective test was only reliably above chance level for Occlusion-1 explanations in the medical student sample.
- Why unresolved: The study found a potential dissociation between self-reported and actual understanding, but the reasons for this and its implications are not fully explored.
- What evidence would resolve it: A study investigating the factors that contribute to this potential dissociation, such as cognitive biases, prior knowledge, and the complexity of the explanations.

## Limitations

- The use of fictitious vignettes may not fully capture the complexity of real clinical decision-making contexts
- Relatively small sample sizes (particularly in Study 2 with only 23 participants) limit statistical power and generalizability
- The study only examined one specific healthcare AI tool (QCancer) and two feature attribution methods, limiting generalizability to other AI systems

## Confidence

**High confidence**: The finding that explanation format (text vs. chart) has a stronger impact on comprehension and trust than feature attribution method content. This is supported by multiple statistical analyses showing consistent patterns across both studies.

**Medium confidence**: The mechanism that text explanations are easier to process due to their narrative structure. While this aligns with cognitive load theory, the study did not directly measure cognitive processing differences between formats.

**Low confidence**: The generalizability of findings to other healthcare AI tools and clinical contexts, given the study's focus on a single algorithm and use of fictitious vignettes rather than real clinical scenarios.

## Next Checks

1. Replicate the study with real clinical cases and measure actual clinical decision outcomes rather than self-reported comprehension and trust scores.

2. Test the findings with different types of healthcare AI tools (e.g., diagnostic imaging, treatment recommendation systems) to assess generalizability across AI applications.

3. Conduct eye-tracking or think-aloud studies to directly measure cognitive processing differences between text and chart explanations and validate the proposed mechanism.