---
ver: rpa2
title: 'OAC: Output-adaptive Calibration for Accurate Post-training Quantization'
arxiv_id: '2405.15025'
source_url: https://arxiv.org/abs/2405.15025
tags:
- quantization
- hessian
- calibration
- spqr
- output-adaptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Output-adaptive Calibration (OAC), a novel\
  \ method for accurate post-training quantization of large language models. Unlike\
  \ previous approaches that use layer-wise \u21132 loss, OAC directly minimizes the\
  \ distortion of the model output by incorporating the output cross-entropy loss\
  \ into the calibration process."
---

# OAC: Output-adaptive Calibration for Accurate Post-training Quantization

## Quick Facts
- arXiv ID: 2405.15025
- Source URL: https://arxiv.org/abs/2405.15025
- Reference count: 38
- Primary result: OAC significantly outperforms state-of-the-art methods like SpQR and BiLLM in extreme low-precision quantization (2-bit and binary), achieving superior accuracy on language modeling and reasoning tasks across various model sizes.

## Executive Summary
This paper introduces Output-adaptive Calibration (OAC), a novel method for accurate post-training quantization of large language models. Unlike previous approaches that use layer-wise ℓ2 loss, OAC directly minimizes the distortion of the model output by incorporating the output cross-entropy loss into the calibration process. The key innovation is the use of output-adaptive Hessian approximation, which captures inter-element correlations while maintaining computational efficiency through cross-layer and cross-row independence assumptions and aggregation of row-wise Hessians. Experimental results demonstrate that OAC significantly outperforms state-of-the-art methods like SpQR and BiLLM in extreme low-precision quantization (2-bit and binary), achieving superior accuracy on language modeling and reasoning tasks across various model sizes.

## Method Summary
OAC is a post-training quantization method that calibrates model weights by minimizing the distortion of the output cross-entropy loss rather than using layer-wise ℓ2 loss. The method computes an output-adaptive Hessian matrix for each weight matrix, which captures the sensitivity of the model's output to changes in weights. To maintain computational efficiency, OAC uses cross-layer and cross-row independence assumptions, allowing row-wise Hessian computation and aggregation. The aggregated Hessian is then used to update weights and detect salient outliers during calibration. OAC integrates with existing calibration methods like SpQR and BiLLM, providing more accurate weight updates for extreme low-precision quantization scenarios.

## Key Results
- OAC outperforms state-of-the-art methods (SpQR, BiLLM) in 2-bit and binary quantization across multiple model families (OPT, LLaMa, LLaMa 2)
- Achieves superior perplexity on language modeling tasks (C4, WikiText2) and reasoning scores on multiple datasets
- Particularly effective when average bit width or model size is smaller, demonstrating the importance of output-adaptive calibration in low-precision scenarios
- Shows consistent improvements across various model sizes from OPT-125M to LLaMa-70B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Output-adaptive calibration improves extreme low-precision quantization by directly minimizing the distortion of the model's output cross-entropy loss rather than using layer-wise ℓ2 loss.
- Mechanism: OAC computes the output-adaptive Hessian matrix for each weight matrix, which captures the sensitivity of the model's output to changes in weights, then uses this Hessian to update weights and detect salient outliers during calibration.
- Core assumption: The cross-entropy loss distortion is a better objective for preserving model accuracy than layer-wise ℓ2 loss during extreme quantization.
- Evidence anchors: [abstract] "Unlike previous approaches that use layer-wise ℓ2 loss, OAC directly minimizes the distortion of the model output by incorporating the output cross-entropy loss into the calibration process."

### Mechanism 2
- Claim: The computational feasibility of OAC is achieved through cross-layer independence and cross-row independence assumptions.
- Mechanism: OAC approximates the full output-adaptive Hessian by assuming each layer's Hessian is independent of other layers, and within each layer, each row's Hessian is independent of other rows. This allows row-wise Hessian computation and aggregation to significantly reduce memory requirements.
- Core assumption: Linear layers in transformers can be treated as independent blocks, and each row of a weight matrix contributes independently to the output.
- Evidence anchors: [section 4.1] "Following the layer-wise quantization and calibration recipe of our proposed method, we assume that the linear layers are independent"

### Mechanism 3
- Claim: Aggregation of row-wise Hessians further reduces computational complexity while maintaining approximation accuracy.
- Mechanism: Instead of computing and inverting each row-wise Hessian separately, OAC aggregates all row-wise Hessians into a single matrix bHOAC = Σj HOACj, then uses this aggregated Hessian for the optimization problem.
- Core assumption: Aggregating row-wise Hessians provides a reasonable upper bound for the optimization problem and significantly reduces memory footprint.
- Evidence anchors: [section 4.3] "We propose replacing HOACj with bHOAC = Σdrow j=1 HOACj to mitigate the memory complexity"

## Foundational Learning

- Concept: Second-order optimization and Hessian matrix computation
  - Why needed here: OAC fundamentally relies on computing and using Hessian matrices to understand how small weight perturbations affect the model's output loss, which requires understanding of second-order derivatives and optimization theory.
  - Quick check question: What is the relationship between the Fisher Information Matrix and the expected Hessian of the cross-entropy loss?

- Concept: Post-training quantization (PTQ) techniques and calibration procedures
  - Why needed here: OAC is a PTQ method that requires understanding of how quantization affects model accuracy and how calibration can mitigate these effects through weight updates and outlier detection.
  - Quick check question: How does layer-wise calibration differ from output-adaptive calibration in terms of the loss function being minimized?

- Concept: Transformer architecture and linear layer operations
  - Why needed here: OAC operates on the linear layers within transformer blocks, so understanding how these layers contribute to the overall model output is crucial for implementing the cross-row independence assumption correctly.
  - Quick check question: In a transformer's attention mechanism, how do the query, key, and value weight matrices relate to the linear layers that OAC calibrates?

## Architecture Onboarding

- Component map:
  Input pipeline (calibration dataset) -> Model loading (pre-trained LLM) -> Hessian computation module -> Calibration module -> Quantization module -> Evaluation pipeline

- Critical path:
  1. Load model and calibration data
  2. For each transformer block: compute gradients, approximate Hessian, calibrate weights
  3. Apply group quantization with outlier isolation
  4. Evaluate quantized model performance

- Design tradeoffs:
  - Accuracy vs. computational cost: Output-adaptive Hessian is more accurate but requires more gradient computations than response-agnostic methods
  - Memory vs. precision: FP16 gradient computations reduce memory by ~30% but may introduce numerical instability
  - Granularity vs. efficiency: Group quantization (size 32-128) balances compression ratio with accuracy

- Failure signatures:
  - NaN or inf values in Hessian computation (indicates numerical instability)
  - Significant perplexity degradation after quantization (>2x increase)
  - Calibration not converging (weights oscillating or diverging)

- First 3 experiments:
  1. Apply OAC to a small transformer (e.g., OPT-125M) with 3-bit quantization and verify perplexity improvement over baseline PTQ methods
  2. Test FP16 vs FP32 gradient computation on LLaMa-7B to measure memory savings and accuracy impact
  3. Compare different group sizes (32, 64, 128) for 2-bit quantization to find optimal balance between compression and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of OAC compare when using different approximation techniques for the output-adaptive Hessian, such as different aggregation methods or alternative independence assumptions?
- Basis in paper: [inferred] The paper mentions using cross-layer and cross-row independence assumptions and aggregation of row-wise Hessians, but does not explore alternative approximation techniques.
- Why unresolved: The paper only presents one specific method for approximating the output-adaptive Hessian, leaving the question of whether other approximation techniques could yield better or comparable results.
- What evidence would resolve it: Experiments comparing OAC's performance using various approximation techniques for the output-adaptive Hessian, such as different aggregation methods or alternative independence assumptions, would provide insights into the optimal approach.

### Open Question 2
- Question: What is the impact of using different loss functions in the OAC framework, such as KL divergence or other divergence measures, on the quantization accuracy?
- Basis in paper: [inferred] The paper focuses on using the cross-entropy loss to measure the quantization error, but does not explore the use of alternative loss functions.
- Why unresolved: The choice of loss function can significantly impact the optimization process and the final quantization accuracy, but the paper does not investigate the effect of using different loss functions in the OAC framework.
- What evidence would resolve it: Experiments comparing the performance of OAC when using different loss functions, such as KL divergence or other divergence measures, would provide insights into the impact of the choice of loss function on quantization accuracy.

### Open Question 3
- Question: How does the performance of OAC scale with increasing model size and complexity, particularly for extremely large language models with hundreds of billions of parameters?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of OAC on a range of model sizes, but does not explore its performance on extremely large language models.
- Why unresolved: The computational complexity of OAC increases with model size, and it is unclear how well the method would perform on extremely large language models with hundreds of billions of parameters.
- What evidence would resolve it: Experiments applying OAC to extremely large language models with hundreds of billions of parameters and comparing its performance to other quantization methods would provide insights into the scalability of the approach.

## Limitations

- The effectiveness of OAC relies heavily on the validity of cross-layer and cross-row independence assumptions in the Hessian approximation, which may break down in certain scenarios
- The computational overhead of output-adaptive calibration, though reduced through approximation techniques, remains substantial compared to simpler PTQ methods
- The method's performance in multi-stage quantization scenarios or with non-standard transformer architectures is unexplored

## Confidence

- **High Confidence**: The core mechanism of using output cross-entropy loss for calibration rather than layer-wise ℓ2 loss is well-supported by experimental results and theoretically sound.
- **Medium Confidence**: The computational efficiency claims through cross-layer and cross-row independence assumptions are reasonable but lack extensive ablation studies.
- **Low Confidence**: The long-term stability and generalization of OAC across diverse LLM architectures and tasks beyond those tested remains uncertain.

## Next Checks

1. **Ablation Study on Independence Assumptions**: Implement OAC with varying degrees of approximation (full Hessian, layer-wise independence only, full independence) on a medium-sized model to quantify the accuracy-cost tradeoff of each approximation level.

2. **Scaling Analysis Across Model Sizes**: Test OAC on models spanning three orders of magnitude (e.g., 125M to 70B parameters) with identical calibration datasets to identify potential scaling issues or regime changes in calibration effectiveness.

3. **Robustness to Calibration Data Quality**: Systematically degrade the calibration dataset (reduce sequence count, use out-of-domain data, add noise) to determine the minimum data quality requirements for OAC to maintain its accuracy advantage over baseline PTQ methods.