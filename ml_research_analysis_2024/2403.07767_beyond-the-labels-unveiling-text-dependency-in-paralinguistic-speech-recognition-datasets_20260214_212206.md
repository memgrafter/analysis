---
ver: rpa2
title: 'Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech Recognition
  Datasets'
arxiv_id: '2403.07767'
source_url: https://arxiv.org/abs/2403.07767
tags:
- datasets
- paralinguistic
- speech
- clse
- lexical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study reveals that commonly used paralinguistic speech recognition
  datasets like CLSE and IEMOCAP exhibit significant lexical overlap between labels
  (e.g., cognitive load, emotion) and spoken sentences. The authors show that machine
  learning models, including classical UBM-iVector, LSTM, and large pre-trained models
  like HuBERT, may inadvertently learn text-dependent features rather than the intended
  paralinguistic traits.
---

# Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech Recognition Datasets

## Quick Facts
- arXiv ID: 2403.07767
- Source URL: https://arxiv.org/abs/2403.07767
- Reference count: 19
- Primary result: Commonly used paralinguistic speech recognition datasets exhibit significant lexical overlap between labels and spoken sentences, leading models to learn text-dependent features rather than intended paralinguistic traits.

## Executive Summary
This study reveals that popular paralinguistic speech recognition datasets like CLSE and IEMOCAP contain substantial lexical overlap between labels (e.g., cognitive load, emotion) and spoken sentences. Through systematic experiments, the authors demonstrate that machine learning models—including classical UBM-iVector, LSTM, and large pre-trained models like HuBERT—may inadvertently learn text-dependent features rather than the intended paralinguistic characteristics. When lexical redundancy is minimized through shuffling, classification performance drops to chance levels, exposing a fundamental issue in dataset design and evaluation methodology for paralinguistic recognition tasks.

## Method Summary
The researchers conducted experiments on CLSE-Span (cognitive load recognition) and IEMOCAP (emotion recognition) datasets using multiple approaches: UBM-iVector system, LSTM-based models, and HuBERT-based systems for speech features, alongside Whisper ASR and LaBSE embeddings for text-based analysis. They employed hierarchical clustering on sentence embeddings to identify unique sentences and implemented cluster-based classification. The key experimental manipulation involved comparing performance on original dataset splits versus shuffled versions where sentences were randomly reassigned within speakers, allowing them to isolate the impact of lexical overlap on classification accuracy.

## Key Results
- HuBERT performance on CLSE drops from 0.74 to 0.52 UAR when lexical context is removed through shuffling
- Text-based emotion recognition on IEMOCAP achieves 0.61 UAR, nearly matching the 0.68 UAR of speech-based HuBERT models
- Cluster-based classification on original CLSE achieves 0.79 UAR but drops to 0.33 UAR on shuffled data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large pre-trained models like HuBERT inadvertently capture lexical features rather than paralinguistic traits.
- Mechanism: The models are trained on large text-rich corpora, so their representations encode word-level patterns that correlate with labels in paralinguistic datasets.
- Core assumption: Lexical redundancy in the dataset creates a shortcut for classification, bypassing the need to model prosody or physiological cues.
- Evidence anchors:
  - [abstract] states that HuBERT might "inadvertently focus on lexical characteristics rather than the intended paralinguistic features."
  - [section] shows HuBERT performance drops from 0.74 to 0.52 UAR when lexical context is removed in CLSE.
  - [corpus] evidence is weak: no related papers discuss this specific lexical shortcut mechanism.
- Break condition: If the model's embeddings are disentangled from lexical content via adversarial training, the performance gap between original and shuffled splits would shrink.

### Mechanism 2
- Claim: Emotion recognition can be performed nearly as well using only lexical features extracted from ASR transcripts.
- Mechanism: Semantic and syntactic cues in the transcribed text carry enough emotional signal to match acoustic-based models.
- Core assumption: The lexical content of utterances in IEMOCAP and CLSE is sufficiently informative to predict emotion or cognitive load without prosodic information.
- Evidence anchors:
  - [abstract] reports that emotion recognition based on text features performs "nearly as well as state-of-the-art speech-based models."
  - [section] shows text-based experiments on IEMOCAP yield 0.61 UAR, close to the 0.68 UAR of HuBERT.
  - [corpus] contains one relevant paper on "On the Contribution of Lexical Features to Speech Emotion Recognition," supporting this mechanism.
- Break condition: If the dataset is restructured so that lexical and paralinguistic signals are orthogonal, the text-only performance would drop significantly.

### Mechanism 3
- Claim: Cluster-based classification achieves high accuracy when lexical overlap exists between labels and sentences.
- Mechanism: Similar sentences cluster together, and if those clusters align with label distributions, classification can be done without modeling paralinguistic cues.
- Core assumption: The dataset's utterance clusters have strong label homogeneity, making them a proxy for classification.
- Evidence anchors:
  - [section] describes using agglomerative clustering on sentence embeddings, achieving 0.79 UAR on original CLSE and dropping to 0.33 on shuffled data.
  - [corpus] evidence is weak: no direct citation of this clustering-based mechanism.
- Break condition: If cluster labels are randomly reassigned, performance should revert to chance, revealing the dependence on lexical structure.

## Foundational Learning

- Concept: Lexical vs. paralinguistic feature distinction
  - Why needed here: The study hinges on separating what is said from how it is said.
  - Quick check question: Can you define a feature that depends only on word choice and not on tone or pitch?

- Concept: Dataset leakage through text-label correlation
  - Why needed here: Understanding how label predictability from text undermines model validity.
  - Quick check question: What would happen to model evaluation if two speakers always say the same sentence under the same label?

- Concept: Model generalization under controlled feature removal
  - Why needed here: The shuffling experiments test whether performance relies on lexical shortcuts.
  - Quick check question: If you randomize sentences within speakers, does the model still perform well?

## Architecture Onboarding

- Component map:
  - ASR module (Whisper) → Text embeddings (LaBSE) → Clustering or classifier
  - Speech feature extractor (HuBERT) → Classifier (Attentive correlation pooling)
  - Traditional pipeline (UBM-iVector + SVM) → Classifier

- Critical path:
  ASR → Embedding → Clustering/Classification → Evaluation (UAR)
  Speech → Feature extraction → Classification → Evaluation

- Design tradeoffs:
  - Using ASR adds transcription noise but enables text-based baselines.
  - Pre-trained models like HuBERT offer strong performance but may conflate lexical and paralinguistic features.
  - Clustering provides interpretable groupings but may not generalize across speakers.

- Failure signatures:
  - High performance on original splits but chance-level on shuffled splits.
  - Small performance gap between text-only and speech-based models.
  - Over-reliance on speaker-specific lexical patterns.

- First 3 experiments:
  1. Replicate the original UAR evaluation on shuffled CLSE to confirm chance-level drop.
  2. Compare HuBERT embeddings against LaBSE embeddings for clustering quality.
  3. Train a text-only logistic regression on IEMOCAP and compare to speech-based baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design paralinguistic speech recognition datasets that minimize lexical overlap with labels to ensure models focus on paralinguistic traits rather than text-dependent features?
- Basis in paper: [explicit] The paper calls for the development of methods for reducing text-dependency in existing datasets and suggests using datasets explicitly designed to minimize lexical overlap.
- Why unresolved: The paper identifies the problem but does not propose specific methods or frameworks for creating such datasets.
- What evidence would resolve it: Evidence would include successful implementation of dataset creation methods that demonstrably reduce lexical overlap, validated through experiments showing improved focus on paralinguistic features.

### Open Question 2
- Question: What evaluation methodologies can be developed to effectively decouple textual and non-textual features in paralinguistic recognition systems?
- Basis in paper: [explicit] The paper suggests the need for a more careful approach to evaluating paralinguistic recognition systems, with an explicit focus on decoupling textual and non-textual features.
- Why unresolved: The paper highlights the necessity but does not provide specific methodologies or metrics for such decoupling.
- What evidence would resolve it: Evidence would include the development and validation of evaluation frameworks that can accurately measure the contribution of textual versus paralinguistic features in recognition tasks.

### Open Question 3
- Question: How do large pre-trained models like HuBERT affect the integrity of paralinguistic trait recognition, and can their design be adapted to mitigate lexical overlap issues?
- Basis in paper: [explicit] The paper notes that reliance on ASR-focused pre-trained models like HuBERT risks conflating lexical and paralinguistic features, exacerbating lexical overlap issues.
- Why unresolved: The paper raises concerns about these models but does not explore potential adaptations or alternative designs to address these issues.
- What evidence would resolve it: Evidence would include experiments comparing modified versions of these models with reduced lexical dependency against standard models, demonstrating improved paralinguistic trait recognition.

## Limitations

- The analysis is constrained by the limited scope of examined datasets (CLSE and IEMOCAP) and the absence of more diverse paralinguistic tasks such as speaker state or disease detection.
- The shuffling methodology may not fully eliminate all forms of text-dependent shortcuts, leaving some uncertainty about the completeness of the control.
- The study does not explore the impact of different ASR systems on transcription quality, which could affect the reliability of text-based feature extraction.

## Confidence

- Medium-High: Claims about lexical overlap and its impact on classification performance
- Medium: Generalizability of findings to other paralinguistic datasets and tasks
- Medium: Effectiveness of shuffling as a control for text-dependency

## Next Checks

1. Replicate the study on additional paralinguistic datasets (e.g., SAVEE, RAVDESS) to assess generalizability.
2. Evaluate the impact of ASR system choice on text-dependency by comparing Whisper with other transcription models.
3. Conduct ablation studies to isolate the contribution of specific lexical features (e.g., keywords, syntactic structures) to classification performance.