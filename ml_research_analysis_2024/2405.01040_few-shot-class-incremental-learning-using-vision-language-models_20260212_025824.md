---
ver: rpa2
title: Few Shot Class Incremental Learning using Vision-Language models
arxiv_id: '2405.01040'
source_url: https://arxiv.org/abs/2405.01040
tags:
- learning
- classes
- class
- training
- incremental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel framework for Few-Shot Class Incremental
  Learning (FSCIL) that leverages language regularizer and subspace regularizer to
  seamlessly integrate new classes with limited data while preserving base class performance.
  The key idea is to reduce the semantic gap between visual and textual (class labels)
  modalities in base model training by introducing a cross-domain graph Laplacian
  regularizer.
---

# Few Shot Class Incremental Learning using Vision-Language models
## Quick Facts
- arXiv ID: 2405.01040
- Source URL: https://arxiv.org/abs/2405.01040
- Reference count: 5
- Achieved 7% improvement on CIFAR-FS and 1% on Mini-ImageNet and Tiered-ImageNet benchmarks

## Executive Summary
This paper proposes a novel framework for Few-Shot Class Incremental Learning (FSCIL) that addresses the challenge of integrating new classes with limited data while preserving performance on base classes. The approach leverages vision-language models by introducing a cross-domain graph Laplacian regularizer to reduce the semantic gap between visual and textual modalities during base model training. The framework has been validated across multiple FSCIL benchmarks in both single-session and multi-session settings.

## Method Summary
The proposed framework introduces two key regularizers: a language regularizer and a subspace regularizer. The core innovation is the cross-domain graph Laplacian regularizer, which aligns visual and textual representations during base model training. This regularizer works by constructing a graph that connects visual and textual embeddings of class labels, enabling better semantic alignment across modalities. The framework is designed to work seamlessly with vision-language models and has been tested in both single-session and multi-session FSCIL settings.

## Key Results
- Achieved state-of-the-art performance on FSCIL benchmarks
- 7% improvement over existing methods on CIFAR-FS dataset
- 1% improvement on Mini-ImageNet and Tiered-ImageNet datasets

## Why This Works (Mechanism)
The approach works by addressing the fundamental challenge in FSCIL where new classes with limited data must be integrated without catastrophic forgetting of base classes. The cross-domain graph Laplacian regularizer effectively bridges the semantic gap between visual and textual modalities by enforcing consistency in how visual features and text embeddings represent class semantics. This alignment ensures that when new classes are introduced, their representations can be more easily integrated into the existing knowledge space.

## Foundational Learning
- Graph Laplacian regularization - Why needed: Enforces smoothness in learned representations across connected nodes in a graph; Quick check: Verify that the regularization term actually reduces the Laplacian matrix eigenvalues during training.
- Vision-language model alignment - Why needed: Ensures consistent semantic understanding across visual and textual modalities; Quick check: Test whether cosine similarity between aligned visual and textual embeddings improves compared to baseline.
- Incremental learning stability - Why needed: Prevents catastrophic forgetting when adding new classes; Quick check: Monitor base class accuracy degradation when new classes are introduced.

## Architecture Onboarding
- Component map: Vision encoder -> Graph Laplacian layer -> Text encoder -> Regularization module -> Classifier
- Critical path: The cross-domain graph Laplacian regularization layer is the critical innovation that connects visual and textual representations.
- Design tradeoffs: The approach trades increased computational complexity during training for better semantic alignment and incremental learning performance.
- Failure signatures: Poor performance on new classes may indicate inadequate semantic alignment, while base class degradation suggests insufficient regularization strength.
- First experiments: 1) Baseline performance without any regularizers, 2) Performance with only language regularizer, 3) Performance with only subspace regularizer.

## Open Questions the Paper Calls Out
None

## Limitations
- Large performance variance across datasets (7% on CIFAR-FS vs 1% on other datasets) suggests potential overfitting or dataset-specific optimizations
- Limited examination of how label noise or semantic ambiguity affects the graph Laplacian regularizer's effectiveness
- Absence of comprehensive ablation studies to isolate individual component contributions

## Confidence
- Major claim (SOTA performance): Medium - Results presented systematically but lack statistical significance testing and show high variance across datasets
- Methodological novelty: High - Cross-domain graph Laplacian regularizer represents sound conceptual extension, though practical advantages need more rigorous validation
- Generalization capability: Low - Limited discussion of performance under different base-to-new class ratios or varying shot counts suggests potential brittleness

## Next Checks
1. Conduct ablation studies on all three datasets to quantify individual and combined effects of language and subspace regularizers, including statistical significance testing across multiple runs
2. Test framework's robustness to label noise by systematically corrupting a subset of class labels and measuring performance degradation
3. Evaluate performance across different base-to-new class size ratios and shot counts to assess scalability and identify potential failure modes in extreme scenarios