---
ver: rpa2
title: 'Does RoBERTa Perform Better than BERT in Continual Learning: An Attention
  Sink Perspective'
arxiv_id: '2410.05648'
source_url: https://arxiv.org/abs/2410.05648
tags:
- attention
- tokens
- sink
- learning
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies attention sinks\u2014tokens receiving high\
  \ attention with low deviation\u2014as a cause of over-smoothing and task interference\
  \ in continual learning. Pre-trained models like RoBERTa often exhibit this behavior,\
  \ which reduces their performance in learning sequential tasks."
---

# Does RoBERTa Perform Better than BERT in Continual Learning: An Attention Sink Perspective

## Quick Facts
- arXiv ID: 2410.05648
- Source URL: https://arxiv.org/abs/2410.05648
- Authors: Xueying Bai; Yifan Sun; Niranjan Balasubramanian
- Reference count: 13
- Primary result: Pre-scaling mechanism improves continual learning performance by encouraging diverse attention on non-sink tokens

## Executive Summary
This paper identifies attention sinks—tokens receiving high attention with low deviation—as a cause of over-smoothing and task interference in continual learning. Pre-trained models like RoBERTa often exhibit this behavior, which reduces their performance in learning sequential tasks. To address this, the authors propose a pre-scaling mechanism that encourages diverse attention by first training a scaling layer to allocate attention based on task relevance, then fine-tuning the entire model. Experiments show that pre-scaling improves performance on continual learning benchmarks without requiring experience replay or parameter storage, with RoBERTa outperforming BERT after pre-scaling.

## Method Summary
The proposed method uses a two-stage pre-scaling mechanism to mitigate attention sink effects in continual learning. First, a scaling layer is trained to allocate attention scores on tokens based on their contributions to the task while keeping the encoder frozen. This probing stage learns task-specific attention patterns. Then, the entire model is fine-tuned with standard optimization. The approach is evaluated on text classification tasks using BERT-base and RoBERTa-base models, comparing against baselines including fine-tuning, experience replay, and other continual learning methods across multiple benchmarks (Yahoo Split, DB Split, News Series).

## Key Results
- Pre-scaling mechanism improves average accuracy and reduces forgetting compared to standard fine-tuning
- RoBERTa shows higher over-smoothing tendencies than BERT, corresponding with lower attention deviations on sink tokens
- Pre-scaling helps RoBERTa outperform BERT in continual learning after addressing attention sink issues
- The method achieves performance gains without requiring experience replay or parameter storage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention sinks—tokens receiving high attention with low deviation—cause over-smoothing and task interference in continual learning.
- Mechanism: High attention on common tokens like [SEP] creates uniform representations across tokens, reducing model capacity to distinguish task-specific features.
- Core assumption: Pre-trained models allocate similar high attention to sink tokens across layers, which persists during fine-tuning.
- Evidence anchors:
  - [abstract] This paper identifies attention sinks—tokens receiving high attention with low deviation—as a cause of over-smoothing and task interference in continual learning.
  - [section] We show that over-smoothing is related to small sink attention deviations in pre-trained models.
  - [corpus] The corpus contains related work on transformer models and BERT, but no direct evidence about attention sinks causing over-smoothing.

### Mechanism 2
- Claim: Pre-scaling improves continual learning by encouraging diverse attention on non-sink tokens.
- Mechanism: A two-stage training process first learns a scaling layer to allocate attention based on task relevance, then fine-tunes the entire model.
- Core assumption: Task-specific attention on non-sink tokens can be learned from pre-trained representations.
- Evidence anchors:
  - [abstract] To address this, the authors propose a pre-scaling mechanism that encourages diverse attention by first training a scaling layer to allocate attention based on task relevance, then fine-tuning the entire model.
  - [section] We design a scaling layer to allocate attention scores on tokens based on their contributions to the task.
  - [corpus] Weak evidence - corpus mentions BERT and RoBERTa but doesn't specifically address pre-scaling mechanisms.

### Mechanism 3
- Claim: Pre-scaling helps RoBERTa outperform BERT in continual learning by reducing feature distortion.
- Mechanism: RoBERTa's higher tendency toward over-smoothing makes it more vulnerable to feature distortion, which pre-scaling mitigates.
- Core assumption: RoBERTa suffers more from over-smoothing than BERT, as evidenced by lower sink attention deviations.
- Evidence anchors:
  - [abstract] Experiments show that pre-scaling improves performance on continual learning benchmarks without requiring experience replay or parameter storage, with RoBERTa outperforming BERT after pre-scaling.
  - [section] Comparing BERT and RoBERTa, we observe that pre-trained RoBERTa suffers more from over-smoothing (i.e., high representation similarity), corresponding with low attention deviations on sink tokens.
  - [corpus] No direct evidence in corpus about RoBERTa's over-smoothing tendencies compared to BERT.

## Foundational Learning

- Concept: Attention mechanisms in transformers
  - Why needed here: Understanding how self-attention works is crucial for grasping why attention sinks form and how they affect model performance.
  - Quick check question: What is the role of the attention matrix in transformer models, and how does it determine token representations?

- Concept: Continual learning and catastrophic forgetting
  - Why needed here: The paper's main contribution is improving continual learning performance, so understanding the problem of forgetting previous tasks is essential.
  - Quick check question: How does catastrophic forgetting occur in neural networks, and what are common strategies to mitigate it?

- Concept: Over-smoothing in deep networks
  - Why needed here: The paper connects attention sinks to over-smoothing, so understanding this phenomenon is key to grasping the proposed solution.
  - Quick check question: What is over-smoothing in the context of neural networks, and how does it affect model performance?

## Architecture Onboarding

- Component map:
  Input layer -> Token embeddings -> Encoder (BERT/RoBERTa) -> Scaling layer -> Classifier -> Output layer

- Critical path:
  1. Input tokens are embedded
  2. Encoder processes embeddings through self-attention layers
  3. Scaling layer adjusts attention based on task relevance
  4. Classifier uses adjusted representations for prediction
  5. Loss is computed and gradients flow back through all components

- Design tradeoffs:
  - Two-stage training vs. end-to-end fine-tuning: Two-stage allows learning task-specific attention patterns before full fine-tuning, but may be slower.
  - Attention diversity vs. task relevance: Encouraging diverse attention may sometimes reduce focus on most relevant tokens.
  - Complexity vs. performance: Adding a scaling layer increases model complexity but improves continual learning performance.

- Failure signatures:
  - No improvement over baseline: Scaling layer not effectively learning task-specific patterns
  - Decreased performance: Over-regularization or disruption of pre-trained features
  - High variance across runs: Instability in learning attention patterns

- First 3 experiments:
  1. Ablation study: Compare pre-scaling with and without the scaling layer to isolate its effect
  2. Attention visualization: Examine attention patterns before and after pre-scaling on sample tasks
  3. Cross-task interference test: Measure interference between unrelated tasks with and without pre-scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different pre-training objectives (e.g., BERT's masked language modeling vs. RoBERTa's optimized pre-training) affect attention sink behavior and continual learning performance?
- Basis in paper: [explicit] The paper notes that RoBERTa's pre-training on more diverse data leads to better downstream performance, but it also mentions that RoBERTa does not always outperform BERT in continual learning, suggesting that pre-training objectives may influence attention sink behavior.
- Why unresolved: The paper does not explore the impact of different pre-training objectives on attention sinks and continual learning performance, focusing instead on the scaling mechanism.
- What evidence would resolve it: Experiments comparing models with different pre-training objectives (e.g., BERT vs. RoBERTa) on continual learning tasks, analyzing attention sink behavior and performance differences.

### Open Question 2
- Question: Can the attention sink phenomenon be generalized to other transformer-based architectures beyond BERT and RoBERTa, such as GPT or T5?
- Basis in paper: [inferred] The paper discusses attention sinks in BERT and RoBERTa, which are transformer-based models, suggesting that similar phenomena might occur in other transformer architectures.
- Why unresolved: The study is limited to BERT and RoBERTa, and the authors do not explore whether attention sinks are a general characteristic of transformer models.
- What evidence would resolve it: Analysis of attention sink behavior in other transformer architectures (e.g., GPT, T5) using similar metrics and tasks as in the study.

### Open Question 3
- Question: How does the pre-scaling mechanism affect the interpretability of model predictions, particularly in terms of understanding the role of sink tokens in task performance?
- Basis in paper: [explicit] The paper introduces a pre-scaling mechanism to encourage diverse attention on non-sink tokens, but it does not discuss the interpretability of model predictions after applying this mechanism.
- Why unresolved: The focus is on improving continual learning performance, without addressing how the scaling affects the interpretability of the model's decision-making process.
- What evidence would resolve it: Interpretability analysis comparing model predictions before and after applying the pre-scaling mechanism, focusing on the role of sink tokens in task performance.

## Limitations

- The mechanism's effectiveness on non-text classification tasks remains untested
- The causal relationship between attention deviation and over-smoothing needs more rigorous validation
- The comparative analysis of BERT vs. RoBERTa over-smoothing is based on limited experimental evidence

## Confidence

**High Confidence Claims**:
- Attention sinks exist as a measurable phenomenon in pre-trained transformers
- Pre-scaling mechanism improves continual learning performance on tested benchmarks
- Two-stage training (scaling layer first, then fine-tuning) is more effective than direct fine-tuning for mitigating attention sink effects

**Medium Confidence Claims**:
- Attention sinks are the primary cause of over-smoothing and task interference
- RoBERTa suffers more from over-smoothing than BERT
- The proposed method consistently outperforms experience replay approaches

**Low Confidence Claims**:
- The mechanism generalizes to non-text classification tasks
- Attention sinks are the dominant factor in catastrophic forgetting
- The scaling layer's learned parameters are interpretable as task-specific attention patterns

## Next Checks

1. **Cross-domain validation**: Test the pre-scaling mechanism on vision transformers and multimodal models to verify if attention sinks exist and can be mitigated similarly in non-text domains.

2. **Ablation on attention components**: Systematically disable different attention heads or layers to determine whether attention sinks in specific layers are more critical for over-smoothing than others.

3. **Long-sequence behavior analysis**: Evaluate how attention sinks evolve in very long sequences (>512 tokens) where sink tokens might constitute a larger proportion of the sequence, potentially amplifying their impact.