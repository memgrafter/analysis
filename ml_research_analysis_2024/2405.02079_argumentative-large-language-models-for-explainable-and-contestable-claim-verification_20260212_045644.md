---
ver: rpa2
title: Argumentative Large Language Models for Explainable and Contestable Claim Verification
arxiv_id: '2405.02079'
source_url: https://arxiv.org/abs/2405.02079
tags:
- claim
- argument
- chatgpt
- opro
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes argumentative LLMs (ArgLLMs) to address the
  limitations of standard LLMs in providing explainable and contestable outputs. The
  method constructs quantitative bipolar argumentation frameworks (QBAFs) from LLM-generated
  arguments and intrinsic strengths, then applies gradual semantics to derive decisions.
---

# Argumentative Large Language Models for Explainable and Contestable Claim Verification

## Quick Facts
- arXiv ID: 2405.02079
- Source URL: https://arxiv.org/abs/2405.02079
- Reference count: 40
- Primary result: ArgLLMs achieve accuracy comparable to baselines while providing faithful explanations and contestable outputs through QBAF construction

## Executive Summary
This paper introduces argumentative large language models (ArgLLMs) that address the opacity of standard LLMs in claim verification tasks. The approach constructs quantitative bipolar argumentation frameworks (QBAFs) from LLM-generated arguments and intrinsic strengths, then applies gradual semantics to derive decisions. ArgLLMs demonstrate accuracy parity with existing methods while offering verifiable explanations and the ability to contest outputs through QBAF modifications. The framework is formally validated to satisfy contestability properties under DF-QuAD semantics.

## Method Summary
ArgLLMs operate by generating arguments through LLMs for claim verification, then constructing QBAFs that capture argument interactions and strengths. The gradual semantics framework processes these QBAFs to produce explainable decisions. The method integrates chain-of-thought reasoning, direct questioning, and confidence estimation within a unified argumentation structure. Multiple LLM models are evaluated across different semantic variants to ensure robustness and effectiveness.

## Key Results
- ArgLLMs achieve accuracy comparable to chain-of-thought, direct questioning, and estimated confidence baselines
- The approach provides faithful explanations through QBAF construction and modification
- Formal analysis proves contestability properties under DF-QuAD semantics
- Seven models tested including both open-source and proprietary LLMs
- Ablation studies confirm robustness across semantic variants

## Why This Works (Mechanism)
The method works by translating LLM reasoning into structured argumentation frameworks that capture both the arguments and their interactions. QBAFs provide a mathematical foundation for representing argumentative relationships with quantitative weights. Gradual semantics then process these frameworks to derive decisions while maintaining explainability. The contestability properties ensure that users can verify and adjust outputs by modifying the underlying QBAF structure.

## Foundational Learning
- **Quantitative Bipolar Argumentation Frameworks (QBAFs)**: Mathematical structures representing arguments and their relationships with weights. Needed to capture both argumentative content and interaction strengths. Quick check: Verify that QBAF construction properly encodes argument relationships from LLM outputs.
- **Gradual Semantics**: Formal methods for deriving conclusions from argumentation frameworks with quantitative information. Required to process weighted arguments systematically. Quick check: Confirm gradual semantics produce consistent results across different argument sets.
- **DF-QuAD Semantics**: Specific gradual semantics variant providing contestability properties. Essential for ensuring verifiable and adjustable outputs. Quick check: Validate contestability properties hold under various QBAF modifications.
- **Argumentation Theory**: Theoretical foundation for representing reasoning as structured argument interactions. Needed to bridge LLM outputs with formal verification methods. Quick check: Ensure argument generation aligns with argumentation theory principles.
- **Intrinsic Strength Quantification**: Methods for measuring argument quality and confidence. Required to weight arguments appropriately in QBAFs. Quick check: Verify intrinsic strength measurements correlate with actual argument quality.

## Architecture Onboarding

**Component Map**: LLM -> Argument Generation -> QBAF Construction -> Gradual Semantics -> Decision/Explanation

**Critical Path**: The core workflow processes LLM outputs through QBAF construction, then applies gradual semantics to derive final decisions with explanations.

**Design Tradeoffs**: Accuracy parity with baselines versus enhanced explainability and contestability. Reliance on LLM quality versus formal verification properties. Computational overhead of QBAF processing versus benefits of structured reasoning.

**Failure Signatures**: Poor initial argument generation propagating through QBAF construction, scalability issues with large argument sets, semantic processing inconsistencies with complex argument interactions.

**First Experiments**: 
1. Test QBAF construction with adversarial arguments to assess robustness
2. Measure computational overhead and quality degradation with argument sets of 50, 100, and 200 elements
3. Conduct user studies comparing ArgLLM explanations to standard LLM outputs for comprehension and trust

## Open Questions the Paper Calls Out
None

## Limitations
- Primary contribution is explainability and contestability rather than improved accuracy
- Quality of LLM-generated arguments directly impacts QBAF construction and downstream decisions
- Gradual semantics may face scalability challenges with larger argument sets
- Evaluation scope across datasets is somewhat limited

## Confidence

**High confidence**: Formal contestability properties under DF-QuAD semantics are mathematically rigorous and well-established

**Medium confidence**: Empirical accuracy parity with baselines is convincing but evaluation scope could be broader

**Medium confidence**: Explanation generation through QBAF modification is theoretically sound, but real-world effectiveness depends on LLM argument quality

## Next Checks

1. Test framework robustness by subjecting it to adversarial or deliberately misleading input arguments
2. Evaluate scalability by measuring computational overhead and decision quality with increasing argument set sizes
3. Conduct user studies to verify that generated explanations improve human understanding and trust compared to standard LLM outputs