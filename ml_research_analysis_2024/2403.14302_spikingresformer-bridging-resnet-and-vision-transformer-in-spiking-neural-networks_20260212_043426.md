---
ver: rpa2
title: 'SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural
  Networks'
arxiv_id: '2403.14302'
source_url: https://arxiv.org/abs/2403.14302
tags:
- spiking
- neural
- dssa
- scaling
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SpikingResformer, a spiking neural network
  architecture that bridges the gap between ResNet and Vision Transformer designs
  for improved performance and energy efficiency. The authors propose a novel spiking
  self-attention mechanism called Dual Spike Self-Attention (DSSA), which is fully
  spike-driven and compatible with SNNs.
---

# SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks

## Quick Facts
- arXiv ID: 2403.14302
- Source URL: https://arxiv.org/abs/2403.14302
- Authors: Xinyu Shi; Zecheng Hao; Zhaofei Yu
- Reference count: 40
- One-line primary result: SpikingResformer achieves 79.40% top-1 accuracy on ImageNet with 4 time-steps

## Executive Summary
SpikingResformer introduces a novel architecture that bridges ResNet and Vision Transformer designs in spiking neural networks. The key innovation is Dual Spike Self-Attention (DSSA), a spike-driven self-attention mechanism that eliminates floating-point operations through logical AND-based matrix multiplication. This approach combines the multi-stage architecture of ResNet with transformer-like attention mechanisms, resulting in improved accuracy and energy efficiency with fewer parameters than competing methods.

## Method Summary
SpikingResformer employs a ResNet-inspired multi-stage architecture combined with a novel Dual Spike Self-Attention (DSSA) module. The architecture features a 7x7 convolutional stem followed by four stages of residual blocks with downsampling, where DSSA modules replace traditional attention mechanisms. DSSA uses Dual Spike Transformation to perform attention calculations entirely in the spiking domain through logical AND operations, avoiding floating-point matrix multiplication and softmax. Scaling factors based on average firing rates ensure proper gradient flow and convergence. The network is trained using surrogate gradient descent with 4 time-steps for inference.

## Key Results
- Achieves 79.40% top-1 accuracy on ImageNet with only 4 time-steps
- Outperforms other spiking Vision Transformer counterparts in both accuracy and energy efficiency
- Demonstrates superior transfer learning performance on CIFAR10 and CIFAR100 compared to direct training methods
- Shows competitive results on neuromorphic datasets (CIFAR10-DVS and DVSGesture) despite falling behind direct training methods

## Why This Works (Mechanism)

### Mechanism 1: Dual Spike Transformation
DSSA achieves self-attention in SNNs without floating-point operations by using Dual Spike Transformation (DST). DST performs spike-based matrix multiplication using logical AND operations between spike inputs, eliminating the need for floating-point multiplication, division, and exponentiation. This makes it compatible with the spike-driven nature of SNNs. The logical AND operation between spike inputs effectively captures attention relationships while maintaining computational efficiency.

### Mechanism 2: Scaling Factors for Multi-Scale Feature Maps
Scaling factors in DSSA are crucial for handling multi-scale feature maps and ensuring convergence. These factors are designed based on the mean and variance of DST outputs, which depend on the average firing rates of the input and attention map. This scaling ensures proper gradient flow and prevents vanishing gradients. The firing rates of the input and attention map are accurately estimated and used to derive appropriate scaling factors for different feature map scales.

### Mechanism 3: ResNet-Transformer Hybrid Architecture
Combining ResNet-based multi-stage architecture with DSSA improves both performance and energy efficiency. The multi-stage architecture extracts local features at different scales, while DSSA provides global attention. This combination leverages the strengths of both approaches, leading to better accuracy and efficiency. The multi-stage architecture effectively complements the global attention provided by DSSA without introducing significant computational overhead.

## Foundational Learning

- **Spiking Neural Networks (SNNs)**: Fundamental for understanding the spike-driven computation paradigm and the challenges addressed by SpikingResformer. *Quick check*: What are the key differences between SNNs and traditional Artificial Neural Networks (ANNs)?

- **Vision Transformers**: Provides the foundation for understanding self-attention mechanisms that DSSA aims to implement in the spiking domain. *Quick check*: How does the self-attention mechanism in Vision Transformers work?

- **Dual Spike Transformation (DST)**: Essential for grasping the core innovation that enables spike-based attention without floating-point operations. *Quick check*: How does DST perform spike-based matrix multiplication using logical AND operations?

## Architecture Onboarding

- **Component map**: Input -> Stem (7x7 convolution + 3x3 max pooling) -> Multi-stage backbone (Residual blocks + Downsample layers) -> Multi-Head Dual Spike Self-Attention (MHDSSA) module -> Group-Wise Spiking Feed-Forward Network (GWSFFN) -> Classifier (Global average pooling + Classification layer)

- **Critical path**: Input -> Stem -> Multi-stage backbone (MHDSSA + GWSFFN) -> Classifier

- **Design tradeoffs**: Accuracy vs. Efficiency (DSSA improves accuracy but may increase computational overhead); Local vs. Global features (multi-stage architecture extracts local features, while DSSA provides global attention)

- **Failure signatures**: Poor convergence during training (may indicate issues with scaling factors or DSSA-SNN compatibility); Low accuracy (could be due to ineffective feature extraction or attention mechanism)

- **First 3 experiments**: 1) Evaluate SpikingResformer performance on ImageNet classification; 2) Perform ablation studies on key components (multi-stage architecture, group-wise convolution, DSSA); 3) Evaluate transfer learning ability on static and neuromorphic datasets

## Open Questions the Paper Calls Out

1. How does DSSA compare to other spiking self-attention mechanisms (SSA, SDSA) in scalability and performance on multi-scale feature maps? While DSSA shows effectiveness on SpikingResformer, direct comparisons on the same architecture and dataset would provide more insight.

2. How would SpikingResformer perform on neuromorphic datasets with input sizes adapted to the dataset? The paper suggests current underperformance on DVSGesture may be due to input size mismatch.

3. What is the impact of different scaling factors in DSSA on convergence and performance? The paper demonstrates effectiveness of proposed scaling factors but doesn't explore alternatives.

## Limitations

- The paper doesn't provide detailed comparisons of DSSA with other spiking self-attention mechanisms on the same architecture and dataset
- Limited exploration of how different scaling factors in DSSA affect convergence and performance
- No comprehensive comparison of SpikingResformer's transfer learning performance across different datasets and with other state-of-the-art methods

## Confidence

- **High confidence**: General architecture and approach (ResNet-based multi-stage design with DSSA)
- **Medium confidence**: Specific mechanisms of DSSA (Dual Spike Transformation and scaling factors)
- **Low confidence**: Exact implementation details of DSSA and event-stream preprocessing

## Next Checks

1. Implement and validate the Dual Spike Transformation (DST) mechanism using logical AND operations for spike-based matrix multiplication
2. Verify the scaling factors in DSSA by comparing the estimated firing rates with actual firing rates during training
3. Test the transfer learning ability of SpikingResformer on both static (CIFAR) and neuromorphic (CIFAR10-DVS, DVSGesture) datasets