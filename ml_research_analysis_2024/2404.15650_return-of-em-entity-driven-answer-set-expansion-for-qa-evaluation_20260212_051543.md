---
ver: rpa2
title: 'Return of EM: Entity-driven Answer Set Expansion for QA Evaluation'
arxiv_id: '2404.15650'
source_url: https://arxiv.org/abs/2404.15650
tags:
- gold
- answers
- question
- answer
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of evaluating question answering
  (QA) models in the era of large language models (LLMs), which often generate answers
  with diverse surface forms. Traditional evaluation metrics like exact match (EM)
  and F1 score are less effective for this task, and while LLM-based evaluators are
  more reliable, they suffer from high costs, environmental impact, and limited interpretability.
---

# Return of EM: Entity-driven Answer Set Expansion for QA Evaluation

## Quick Facts
- arXiv ID: 2404.15650
- Source URL: https://arxiv.org/abs/2404.15650
- Authors: Dongryeol Lee; Minwoo Lee; Kyungmin Min; Joonsuk Park; Kyomin Jung
- Reference count: 15
- The paper proposes using soft EM with entity-driven answer set expansion to evaluate QA models, achieving reliability comparable to LLM-based evaluators while reducing costs and improving interpretability.

## Executive Summary
This paper addresses the challenge of evaluating question answering (QA) models in the era of large language models (LLMs), which often generate answers with diverse surface forms. Traditional evaluation metrics like exact match (EM) and F1 score are less effective for this task, and while LLM-based evaluators are more reliable, they suffer from high costs, environmental impact, and limited interpretability. To address these issues, the authors propose using soft EM with entity-driven answer set expansion. Their method expands the gold answer set to include diverse surface forms based on the entity type, leveraging the parametric knowledge of LLMs to guide the expansion. Experimental results on two widely-used QA datasets (Natural Questions and TriviaQA) show that their method outperforms traditional lexical matching metrics and achieves reliability comparable to LLM-based evaluators, while offering the benefits of high interpretability and reduced environmental footprint.

## Method Summary
The method expands the gold answer set to include diverse surface forms based on entity type using soft EM evaluation. First, answers are categorized by entity type using Spacy's NER. Then, InstructGPT's few-shot in-context learning is used to expand the gold answer set for each entity type with 8 illustrative examples per type. Finally, model predictions are evaluated using soft EM with the expanded answer set, checking if predictions contain any expanded gold answer. This approach requires only one-time expansion (3,020 inference calls total) compared to ongoing LLM evaluation costs (3,020 inference calls per model).

## Key Results
- Soft EM with entity-driven expansion outperforms traditional lexical matching metrics on QA evaluation
- Achieves reliability comparable to LLM-based evaluators on PERSON, GPE, and ORG entities
- Reduces inference costs by requiring only one-time expansion instead of ongoing LLM evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entity-driven answer set expansion improves evaluation reliability by capturing diverse surface forms that lexical matching metrics miss.
- Mechanism: The method leverages the parametric knowledge of LLMs to expand gold answer sets with plausible variations based on entity type. For example, "Joe Biden" expands to "Joseph Biden" or "Joseph Robinette Biden Jr." for PERSON entities.
- Core assumption: Surface forms of answers follow particular patterns depending on the entity type, and these patterns can be reliably captured by LLM few-shot prompting.
- Evidence anchors:
  - [abstract] "Our approach expands the gold answer set to include diverse surface forms, based on the observation that the surface forms often follow particular patterns depending on the entity type."
  - [section] "Our method is rooted in that surface forms of an answer can be diverse, but they often follow particular patterns depending on the entity type."
  - [corpus] Weak evidence - no direct citations about entity-driven expansion in related works.
- Break condition: If entity types don't follow predictable patterns or if LLM's parametric knowledge is insufficient for a given entity type, the expansion may fail or introduce errors.

### Mechanism 2
- Claim: Soft EM with expanded answer sets reduces inference costs and environmental impact while maintaining interpretability.
- Mechanism: Instead of using LLMs as evaluators (requiring 3,020 inference calls per model), the method uses expanded answer sets with soft EM, requiring only one-time expansion (3,020 inference calls total).
- Core assumption: Soft EM can reliably evaluate answers when provided with comprehensive answer sets, without needing real-time LLM evaluation.
- Evidence anchors:
  - [abstract] "The use of soft EM significantly reduces the inference cost and environmental footprint, while making it explicit why a given candidate answer is correct or wrong."
  - [section] "In contrast, our metric does not require inference calls at the time of evaluation, but only when expanding the answer set initially."
  - [corpus] Weak evidence - no direct citations about cost-benefit tradeoffs in evaluation methods.
- Break condition: If expanded answer sets miss important variations or if soft EM's leniency allows incorrect answers, reliability will suffer.

### Mechanism 3
- Claim: Entity-specific few-shot prompts guide effective answer set expansion that traditional methods miss.
- Mechanism: The method uses 8 illustrative examples per entity type as few-shot prompts to instruct the LLM on how to expand answers appropriately for each type.
- Core assumption: Few-shot prompting with entity-specific examples can reliably guide LLMs to produce appropriate answer variations.
- Evidence anchors:
  - [section] "We choose eight illustrative examples per entity type from our training data, each accompanied by a manually expanded answer set that aligns with the format diversity of that entity type."
  - [section] "The effectiveness of our method can vary depending on the entity's popularity... as it relies on InstructGPT's background knowledge for each entity."
  - [corpus] Weak evidence - no direct citations about few-shot prompting effectiveness for answer expansion.
- Break condition: If few-shot examples don't capture the full range of variations or if LLM fails to generalize from examples, expansion quality will suffer.

## Foundational Learning

- Concept: Named Entity Recognition (NER) categorization
  - Why needed here: The method categorizes answers into entity types (PERSON, GPE, ORG, etc.) to guide appropriate expansion strategies for each type.
  - Quick check question: What are the main entity types used in this approach and how do they influence answer expansion?

- Concept: Soft vs Hard Exact Match evaluation
  - Why needed here: Soft EM marks answers correct if they contain any gold answer, while Hard EM requires exact match. This distinction is crucial for understanding the evaluation method's leniency.
  - Quick check question: How does Soft EM differ from Hard EM in handling answer variations?

- Concept: Few-shot in-context learning with LLMs
  - Why needed here: The method uses few-shot examples to guide LLMs in expanding answer sets appropriately for each entity type.
  - Quick check question: How does the method use few-shot examples to guide answer set expansion for different entity types?

## Architecture Onboarding

- Component map:
  Input -> NER system -> LLM expansion module -> Soft EM evaluator -> Output

- Critical path:
  1. Categorize original answers by entity type using NER
  2. Generate few-shot examples for each entity type
  3. Prompt LLM to expand gold answer sets using entity-specific examples
  4. Apply Soft EM to check if predictions contain any expanded gold answer
  5. Return evaluation scores with clear justification

- Design tradeoffs:
  - Cost vs reliability: One-time expansion cost vs ongoing LLM evaluation costs
  - Generality vs specificity: Broad expansion may include incorrect variations
  - Interpretability vs accuracy: Clear justifications vs potential black-box decisions

- Failure signatures:
  - Low accuracy on numeric entities (may indicate expansion misses variations)
  - Inconsistent results across entity types (may indicate uneven few-shot quality)
  - High false positives (may indicate overly lenient Soft EM)

- First 3 experiments:
  1. Test expansion quality: Run few-shot prompts on sample answers and verify output variations match expected patterns
  2. Validate NER accuracy: Check if entity categorization correctly identifies all answer types
  3. Compare evaluation scores: Run both original and expanded evaluation on same predictions to measure improvement

## Open Questions the Paper Calls Out
None

## Limitations

- Entity Type Coverage and Quality: The method relies heavily on accurate entity categorization and effective expansion for 14 entity types, with limited discussion of performance on less common types like WORK_OF_ART, EVENT, or LANGUAGE.
- Few-shot Prompt Reliability: Uncertainty about how representative the 8 illustrative examples per entity type are across different domains and answer styles.
- Expansion Accuracy vs. Over-expansion: Risk of over-expansion introducing semantically incorrect variations without detailed analysis of false positives.

## Confidence

**High Confidence Claims**:
- Entity-driven expansion captures diverse surface forms better than lexical matching metrics
- The method significantly reduces inference costs compared to LLM-based evaluators
- Soft EM with expanded answer sets achieves reliability comparable to LLM-based evaluators on PERSON, GPE, and ORG entities

**Medium Confidence Claims**:
- Environmental impact reduction is substantial (no specific metrics provided)
- The method generalizes well across different QA datasets and model architectures
- Interpretability improvements are meaningful for practical deployment

**Low Confidence Claims**:
- Equal effectiveness across all 14 entity types
- No degradation in precision due to over-expansion
- Performance consistency across different LLM backbones beyond InstructGPT

## Next Checks

1. **Entity Type Performance Audit**: Conduct detailed error analysis on all 14 entity types, specifically examining numeric entities (MONEY, PERCENT, QUANTITY, CARDINAL) and rare entity types (WORK_OF_ART, EVENT, LANGUAGE) to quantify performance variation and identify problematic expansion patterns.

2. **False Positive Analysis**: Systematically measure the rate of false positives introduced by over-expansion by comparing expanded answer sets against a gold standard of semantically equivalent variations, and evaluate how Soft EM's leniency affects precision-recall tradeoffs.

3. **Cross-LLM Generalization Test**: Validate the approach using different LLM backbones (Claude, LLaMA, Gemini) to assess whether the entity-driven expansion patterns generalize beyond InstructGPT's parametric knowledge and few-shot capabilities.