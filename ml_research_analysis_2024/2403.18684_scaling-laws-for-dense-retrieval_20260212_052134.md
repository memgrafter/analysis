---
ver: rpa2
title: Scaling Laws For Dense Retrieval
arxiv_id: '2403.18684'
source_url: https://arxiv.org/abs/2403.18684
tags:
- retrieval
- scaling
- dense
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the scaling laws for dense retrieval models.
  It proposes to use contrastive entropy as a continuous evaluation metric for dense
  retrieval, addressing the limitations of discrete ranking metrics.
---

# Scaling Laws For Dense Retrieval

## Quick Facts
- arXiv ID: 2403.18684
- Source URL: https://arxiv.org/abs/2403.18684
- Reference count: 40
- Dense retrieval performance scales predictably with model size and annotated data following power-law relationships

## Executive Summary
This paper investigates scaling laws for dense retrieval models, proposing contrastive entropy as a continuous evaluation metric. The authors conduct extensive experiments with dense retrieval models of varying sizes trained on different amounts of annotated data. Results demonstrate that dense retrieval performance follows precise power-law scaling with respect to model size and number of annotations. The study also examines scaling with data augmentation methods and applies the scaling law to optimize resource allocation under budget constraints.

## Method Summary
The authors fine-tune dense retrieval models using contrastive ranking loss with random negative sampling. They evaluate models using contrastive entropy as a continuous metric, correlating strongly with standard ranking metrics. Scaling laws are fitted using power-law functions for both model size and data size. The joint effect is modeled to derive optimal resource allocation strategies under budget constraints. Experiments use MS MARCO Passage Ranking (English) and T2Ranking (Chinese) datasets with various BERT and ERNIE checkpoints.

## Key Results
- Dense retrieval performance follows precise power-law scaling with model size and annotation count
- Contrastive entropy provides a continuous evaluation metric that strongly correlates with standard ranking metrics
- Joint scaling laws enable optimal resource allocation strategies under budget constraints
- Scaling behavior remains consistent across different annotation methods (human vs. query generation)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dense retrieval performance scales predictably with model size and annotated data size following a power-law
- Mechanism: Increasing model parameters or annotated data reduces contrastive entropy in a predictable power-law manner
- Core assumption: The relationship between model/data size and performance is smooth and continuous
- Evidence anchors:
  - [abstract]: "Results indicate that, under our settings, the performance of dense retrieval models follows a precise power-law scaling"
  - [section]: "the contrastive entropy follows a power-law scaling relative to the number of annotated query-passage pairs"
  - [corpus]: Weak evidence; no direct citations for scaling laws in dense retrieval

### Mechanism 2
- Claim: Contrastive entropy (CE) is an effective continuous metric for evaluating dense retrieval performance
- Mechanism: CE measures the likelihood of retrieving relevant documents from randomly sampled candidates
- Core assumption: CE can effectively capture overall retrieval capability
- Evidence anchors:
  - [abstract]: "We propose to use contrastive log-likelihood as the evaluation metric"
  - [section]: "Figure 2 shows the contrastive entropy and ranking metrics...strong and positive correlation"
  - [corpus]: No direct evidence for CE as evaluation metric

### Mechanism 3
- Claim: Joint effect of model size and data size can be modeled using a single function for resource allocation optimization
- Mechanism: Combining scaling laws for model size and data size into a single function derives optimal resource allocation
- Core assumption: Cost factors for annotation, training, and inference are accurately estimated
- Evidence anchors:
  - [abstract]: "the joint effect of model and data sizes can be nicely fitted and predicted with a single function"
  - [section]: "We attempt to estimate the comprehensive cost associated with the lifecycle of dense retrieval models"
  - [corpus]: Weak evidence for resource allocation optimization

## Foundational Learning

- Concept: Power-law scaling relationships
  - Why needed here: Understanding power-law scaling is crucial for interpreting the relationship between model size, data size, and performance
  - Quick check question: If a dense retrieval model's performance scales with model size as ð¿(ð‘) = ð´/ð‘^ð›¼ + ð›¿ð‘, what happens to performance as ð‘ approaches infinity?

- Concept: Contrastive learning and loss functions
  - Why needed here: Familiarity with contrastive learning is essential for understanding how the contrastive entropy metric is derived
  - Quick check question: How does the contrastive ranking loss function used in dense retrieval training relate to the contrastive entropy metric?

- Concept: Resource allocation and optimization
  - Why needed here: Knowledge of resource allocation is necessary for applying scaling laws to find optimal strategies
  - Quick check question: If annotation cost is significantly higher than training cost, how would this affect optimal resource allocation?

## Architecture Onboarding

- Component map: Text encoder (Transformer-based) -> Projection layer -> Scoring function -> Training data -> Evaluation metric
- Critical path: 1. Initialize dense retrieval model with pre-trained checkpoints 2. Attach projection layer 3. Fine-tune on annotated data using contrastive loss 4. Evaluate using contrastive entropy 5. Analyze scaling laws
- Design tradeoffs:
  - Model size vs. data size: Larger models may be more data-efficient but require more annotated data
  - Training strategy: Random negative sampling vs. hard negative mining
  - Evaluation metric: CE provides smooth metric but may not directly correlate with ranking metrics
- Failure signatures:
  - Poor performance scaling may indicate issues with scaling law assumptions
  - Weak correlation between CE and ranking metrics suggests CE doesn't capture retrieval capability
  - Suboptimal resource allocation may result in underutilization or overfitting
- First 3 experiments:
  1. Fine-tune models of varying sizes on fixed annotated data and evaluate using CE
  2. Fine-tune fixed-size model on different data amounts and analyze scaling
  3. Compare human-annotated vs. query-generated data impact on scaling laws

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does power-law scaling hold across different domains and datasets beyond MSMARCO and T2Ranking?
- Basis in paper: [explicit] Evaluations conducted within in-domain datasets; more extensive evaluation across varied domains could be beneficial
- Why unresolved: Study focuses on English and Chinese web search datasets; scaling laws might differ for biomedical, legal, or scientific literature retrieval
- What evidence would resolve it: Experiments on diverse datasets from multiple domains comparing scaling parameters across domains

### Open Question 2
- Question: How do sophisticated training techniques affect observed scaling laws?
- Basis in paper: [explicit] Basic random negative sampling used; more sophisticated techniques could influence scaling behaviors
- Why unresolved: Simple training methods used to isolate model/data size effects; advanced techniques might change learning dynamics
- What evidence would resolve it: Replicating scaling law experiments using various advanced training techniques

### Open Question 3
- Question: What is optimal model size and annotation strategy considering training and inference costs?
- Basis in paper: [explicit] Analysis of budget allocation under different cost constraints
- Why unresolved: Analysis uses estimated cost factors and simplified assumptions; real-world costs may vary significantly
- What evidence would resolve it: Empirical studies measuring actual costs and performance trade-offs in real-world systems

## Limitations
- Scaling law analysis limited to models up to 70B parameters and datasets up to 1M annotations
- Quality of pseudo-annotations may not be directly comparable to human annotations
- Generalizability to other languages, domains, or retrieval tasks not explicitly addressed

## Confidence

- High confidence: Strong correlation between contrastive entropy and standard ranking metrics
- Medium confidence: Optimal resource allocation strategy relies on accurate cost estimates
- Low confidence: Generalizability of scaling laws to other languages, domains, or tasks

## Next Checks

1. Extend scaling law analysis to models beyond 70B parameters and datasets exceeding 1M annotations
2. Compare scaling laws using pseudo-annotations from different query generation methods vs. human annotations
3. Apply scaling laws and resource allocation to other retrieval tasks, languages, or domains