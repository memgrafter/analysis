---
ver: rpa2
title: 'FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions'
arxiv_id: '2403.15246'
source_url: https://arxiv.org/abs/2403.15246
tags:
- instructions
- instruction
- retrieval
- arxiv
- trec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces FollowIR, a dataset and evaluation framework
  for measuring instruction-following in retrieval models. FollowIR repurposes professional
  TREC annotator narratives into a benchmark with altered instructions to test whether
  models adapt document relevance correctly.
---

# FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions

## Quick Facts
- **arXiv ID**: 2403.15246
- **Source URL**: https://arxiv.org/abs/2403.15246
- **Reference count**: 20
- **Key outcome**: A 7B parameter FollowIR-7B model trained on synthetic instruction data shows significant improvement in instruction-following retrieval, with current models failing except for large (>3B) or instruction-tuned models.

## Executive Summary
FollowIR introduces a novel benchmark and evaluation framework for measuring instruction-following capabilities in information retrieval models. By repurposing professional TREC annotator narratives with altered instructions, the authors create a challenging test that requires models to adapt document relevance based on instruction semantics rather than keyword matching. The work demonstrates that existing IR models struggle with instruction-following except for large (>3B) or instruction-tuned models, and presents FollowIR-7B, a new model that shows marked improvement on both standard retrieval metrics and instruction-following evaluation.

## Method Summary
The paper introduces the FollowIR benchmark, which repurposes TREC collections by altering instructions and re-annotating relevance judgments. A novel pairwise metric (p-MRR) quantifies rank changes when instructions change, providing a direct measure of instruction-following ability. The authors fine-tune a Mistral-7B-Instruct model on synthetic instruction-document pairs to create FollowIR-7B, demonstrating that instruction-following can be learned through appropriate training data. Evaluation is conducted using three TREC collections (Robust 2004, Common Core 2017, News 2021) with both standard IR metrics and the new p-MRR metric.

## Key Results
- Existing IR models fail at instruction-following except for large (>3B) or instruction-tuned models
- FollowIR-7B shows significant improvement on both standard retrieval metrics and p-MRR
- Models consistently use instructions as keyword sources rather than semantic relevance definitions, particularly struggling with longer instructions and negations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-following ability in retrieval models improves with scale and instruction tuning, not just retrieval-specific training.
- Mechanism: Larger models (>3B parameters) and instruction-tuned models develop better semantic understanding and context processing, enabling them to use instructions as relevance definitions rather than keyword matching.
- Core assumption: Model capacity and instruction-tuning enable genuine instruction-following rather than superficial keyword extraction.
- Evidence anchors:
  - [abstract] "Results show existing IR models fail at instruction-following except large (>3B) or instruction-tuned models."
  - [section 4.2] "The only models that show positive results at following instructions are either IR models with over 3B parameters or those that have been explicitly trained to follow instructions."
  - [corpus] Weak - no direct citation to model scaling literature, though references to FLAN, T0 suggest this is established in broader LM community.
- Break condition: If instruction-following requires retrieval-specific training data, this mechanism fails. If smaller models can achieve similar results with different architectures, scale dependency is questioned.

### Mechanism 2
- Claim: Current retrieval models use instructions as keyword sources rather than semantic relevance definitions.
- Mechanism: Models extract keywords from instructions and use them for traditional keyword matching, ignoring the broader semantic content and negations in instructions.
- Core assumption: Models lack the capability to process long-form instructions semantically and instead default to familiar keyword-based retrieval patterns.
- Evidence anchors:
  - [section 4.3] "models use instructions for keyword matching and are unused to longer instructions that may contain slightly less relevant words."
  - [section 4.3] "we see a consistent trend where models that did poorly on longer instructions perform better on keywords and shorter instructions than with the full instruction."
  - [corpus] Weak - no direct analysis of model attention patterns or internal representations showing keyword extraction behavior.
- Break condition: If models can be trained to properly use instructions semantically, this mechanism is invalidated. If keyword matching is actually optimal for some retrieval scenarios, the problem framing may be incorrect.

### Mechanism 3
- Claim: Fine-tuning on synthetic instruction-document pairs can teach retrieval models to follow instructions effectively.
- Mechanism: Generating synthetic relevant/irrelevant documents for instruction-query pairs and filtering with a strong instruction-following model creates effective training data that teaches semantic instruction understanding.
- Core assumption: Synthetic data, when properly filtered, can capture the semantic relationships needed for instruction-following without requiring massive human annotation.
- Evidence anchors:
  - [section 5] "Our new model, FOLLOW IR-7B, shows improvement on both standard retrieval metrics as well as in instruction following."
  - [section 5] "Our results show marked improvement on FOLLOW IR for both standard retrieval metrics and for p-MRR, indicating a starting point for future progress on instruction following."
  - [corpus] Weak - no ablation study on synthetic vs real data, no analysis of filtering effectiveness.
- Break condition: If synthetic data introduces harmful biases or fails to generalize to real instructions, this mechanism fails. If human-annotated data is required for robust instruction-following, synthetic approaches are insufficient.

## Foundational Learning

- Concept: Semantic similarity estimation between text spans
  - Why needed here: Retrieval models fundamentally estimate similarity between queries and documents to rank results
  - Quick check question: How does a bi-encoder architecture differ from a cross-encoder for computing query-document similarity?

- Concept: Instruction tuning and few-shot learning capabilities
  - Why needed here: Instruction-following requires models to generalize from training instructions to novel instruction formats
  - Quick check question: What distinguishes instruction-tuned models from standard fine-tuned models in terms of task generalization?

- Concept: Reranking vs first-stage retrieval tradeoffs
  - Why needed here: The paper uses reranking due to evaluation requirements, but real systems need efficient first-stage retrieval
  - Quick check question: What are the latency and computational tradeoffs between reranking approaches and dense retrieval for production systems?

## Architecture Onboarding

- Component map: Query encoder → Document encoder → Similarity scorer → Ranking layer → (Optional) Reranker
- Critical path: Instruction parsing → Semantic understanding → Document relevance scoring → Rank optimization
- Design tradeoffs: Model size vs inference cost, synthetic data quality vs annotation cost, reranking vs retrieval efficiency
- Failure signatures: Negative p-MRR scores indicate keyword matching rather than instruction-following, poor performance on negations suggests shallow processing
- First 3 experiments:
  1. Compare p-MRR scores for base model vs instruction-tuned model on FOLLOW IR benchmark
  2. Test model performance using instruction keywords vs full instruction text
  3. Evaluate synthetic training data quality by testing on held-out real instructions

## Open Questions the Paper Calls Out

1. **Generalization of Instruction Following**
   - **Question:** Can instruction-following models trained on one domain (e.g., TREC collections) effectively generalize to other domains with different data distributions and instruction styles?
   - **Basis in paper:** [inferred] The paper shows that FOLLOW IR-7B improves performance on FOLLOW IR, but it's unclear if this generalizes to other datasets like InstructIR.
   - **Why unresolved:** The paper doesn't test the model's performance on diverse datasets beyond FOLLOW IR and InstructIR.
   - **What evidence would resolve it:** Evaluating FOLLOW IR-7B on a broader range of datasets with varying instruction styles and data distributions would clarify its generalization capabilities.

2. **Impact of Instruction Complexity on Model Performance**
   - **Question:** How does the complexity of instructions (e.g., length, use of negation, background information) affect the performance of instruction-following models?
   - **Basis in paper:** [explicit] The paper analyzes why models fail to follow instructions, mentioning that they struggle with long instructions and those containing negation.
   - **Why unresolved:** The paper doesn't systematically vary instruction complexity to measure its impact on model performance.
   - **What evidence would resolve it:** Conducting experiments with instructions of varying complexity and analyzing model performance would reveal the relationship between instruction complexity and model effectiveness.

3. **Role of Instruction Tuning in Improving Retrieval Models**
   - **Question:** Is instruction tuning the most effective method for improving the instruction-following capabilities of retrieval models, or are there alternative approaches that could yield better results?
   - **Basis in paper:** [explicit] The paper demonstrates that fine-tuning on synthetic instruction data improves the instruction-following ability of FOLLOW IR-7B.
   - **Why unresolved:** The paper doesn't compare instruction tuning with other potential methods for enhancing instruction following in retrieval models.
   - **What evidence would resolve it:** Comparing the performance of instruction-tuned models with those trained using alternative approaches (e.g., adversarial training, meta-learning) would determine the most effective method.

4. **Evaluation Metrics for Instruction Following**
   - **Question:** Are the current evaluation metrics (e.g., p-MRR, standard IR metrics) sufficient for comprehensively assessing the instruction-following capabilities of retrieval models, or are there other metrics that should be considered?
   - **Basis in paper:** [explicit] The paper introduces p-MRR as a novel pairwise evaluation metric for measuring instruction following.
   - **Why unresolved:** The paper doesn't explore other potential evaluation metrics or discuss the limitations of p-MRR.
   - **What evidence would resolve it:** Developing and evaluating alternative metrics for instruction following, and comparing their effectiveness with p-MRR, would provide a more comprehensive understanding of model performance.

5. **Ethical Considerations in Instruction-Following Models**
   - **Question:** What are the potential ethical implications of developing and deploying instruction-following retrieval models, and how can these be addressed?
   - **Basis in paper:** [inferred] The paper doesn't explicitly discuss ethical considerations, but the development of instruction-following models raises concerns about bias, fairness, and the potential for misuse.
   - **Why unresolved:** The paper focuses on the technical aspects of instruction following and doesn't address the broader societal impact of these models.
   - **What evidence would resolve it:** Conducting a thorough ethical analysis of instruction-following models, including potential biases and risks, and proposing mitigation strategies would address this question.

## Limitations

- The synthetic data generation process lacks detailed validation and may introduce domain-specific biases
- p-MRR measures relative rank changes rather than absolute instruction-following quality, making it unclear if improvements reflect genuine understanding
- Evaluation focuses on reranking rather than first-stage retrieval, limiting applicability to production systems

## Confidence

**High Confidence**: The finding that large models (>3B parameters) and instruction-tuned models outperform smaller IR-specific models on instruction-following tasks is well-supported by the experimental results. The FollowIR-7B model showing improved performance on both standard metrics and p-MRR is directly demonstrated.

**Medium Confidence**: The claim that models use instructions as keyword sources rather than semantic relevance definitions is plausible but under-supported. While the p-MRR metric suggests this behavior, direct analysis of model attention patterns or internal representations is absent.

**Low Confidence**: The synthetic data approach's effectiveness is claimed but not rigorously validated. Without ablation studies comparing synthetic vs human-annotated data, or analysis of synthetic data quality, the mechanism's reliability remains uncertain.

## Next Checks

1. **Synthetic Data Quality Analysis**: Analyze the synthetic document generation process by comparing model performance on synthetic vs held-out human-annotated instruction-document pairs to validate synthetic data effectiveness.

2. **Attention Pattern Investigation**: Examine model attention weights when processing instructions to determine if models are extracting keywords or processing full semantic content, providing direct evidence for or against the keyword-matching hypothesis.

3. **First-Stage Retrieval Evaluation**: Test the FollowIR-7B model in a dense retrieval setting rather than reranking to assess real-world applicability and computational tradeoffs between instruction-following and retrieval efficiency.