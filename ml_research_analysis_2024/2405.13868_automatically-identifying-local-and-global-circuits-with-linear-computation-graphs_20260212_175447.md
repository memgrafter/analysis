---
ver: rpa2
title: Automatically Identifying Local and Global Circuits with Linear Computation
  Graphs
arxiv_id: '2405.13868'
source_url: https://arxiv.org/abs/2405.13868
tags:
- features
- circuits
- feature
- linear
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a circuit discovery pipeline using Sparse
  Autoencoders (SAEs) and Transcoders to make transformer computation graphs linear
  for fine-grained analysis. By inserting these modules into the model, the computation
  graph with respect to OV and MLP circuits becomes strictly linear, enabling causal
  effect computation without linear approximation.
---

# Automatically Identifying Local and Global Circuits with Linear Computation Graphs

## Quick Facts
- arXiv ID: 2405.13868
- Source URL: https://arxiv.org/abs/2405.13868
- Authors: Xuyang Ge; Fukang Zhu; Wentao Shu; Junxuan Wang; Zhengfu He; Xipeng Qiu
- Reference count: 29
- One-line primary result: Automated, scalable circuit discovery using Sparse Autoencoders and Transcoders to make transformer computation graphs linear for fine-grained analysis without linear approximation.

## Executive Summary
This paper introduces a circuit discovery pipeline that automatically identifies local and global circuits in transformer language models by leveraging Sparse Autoencoders (SAEs) and Transcoders to create strictly linear computation graphs. The authors insert these modules into GPT-2 Small to enable exact causal effect computation without linear approximation, allowing fine-grained analysis of attention (OV) and MLP circuits. By applying Hierarchical Attribution, they isolate interpretable subgraphs related to specific outputs in an automatic and scalable manner. Their approach successfully analyzes three circuit types (bracket matching, induction, and Indirect Object Identification) in GPT-2 Small, revealing new detailed structures within existing discoveries.

## Method Summary
The method involves training Sparse Autoencoders on all transformer activations to decompose them into interpretable features, then developing Transcoders to handle MLP non-linearity by creating invariant edge weights through sparsity constraints. A linear computation graph is constructed for each input by combining QK circuits with the linearized OV and MLP circuits from SAEs and Transcoders. Hierarchical Attribution is applied during the backward pass to isolate relevant subgraphs by detaching nodes below a threshold. The pipeline enables automated circuit discovery by identifying SAE features that causally contribute to desired outputs, requiring only one forward and one backward pass through the model.

## Key Results
- The pipeline successfully isolates interpretable circuits for bracket matching, induction, and Indirect Object Identification in GPT-2 Small
- Hierarchical Attribution consistently outperforms standard attribution methods in logit recovery on IOI examples
- The method achieves exact causal effect computation without linear approximation, unlike previous approaches
- Finer-grained analysis reveals new detailed structures underlying existing circuit discoveries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transcoders enable strict linearity in MLP computation graphs without approximation.
- Mechanism: By decoupling input and output and constraining sparsity, Transcoders create invariant edge weights between upstream and downstream features, avoiding the non-linear activation function problem in MLPs.
- Core assumption: Sparse features have interpretable basis that makes edge weights invariant across inputs.
- Evidence anchors:
  - [abstract] "With these two modules inserted into the model, the model's computation graph with respect to OV and MLP circuits becomes strictly linear."
  - [section 2.2] "The key difference is that by constraining the sparsity, Transcoders neurons (which are just features) have an interpretable basis."
  - [corpus] Weak evidence - no direct corpus citations about Transcoder linearity, though related work on SAE circuits exists.
- Break condition: If sparsity constraint fails to create interpretable basis, or if feature superposition remains too dense to maintain invariant edges.

### Mechanism 2
- Claim: Hierarchical Attribution outperforms standard attribution by detaching nodes during backward pass.
- Mechanism: During gradient computation, nodes with attribution scores below threshold have their gradients zeroed immediately, preventing their influence on upstream attribution scores.
- Core assumption: Early node detachment preserves more accurate attribution paths by preventing low-value nodes from affecting predecessors.
- Evidence anchors:
  - [section 3] "Hierarchical Attribution detaches nodes on backward pass instead of after backward, as shown in Figure 2(a)"
  - [section 3] "Experiments are conducted on 20 IOI samples... Results in Figure 2(b) show that Hierarchical Attribution consistently outperforms standard attribution."
  - [corpus] Moderate evidence - attribution methods are common in interpretability literature but specific hierarchical approach is novel.
- Break condition: If threshold selection is poor, leading to premature detachment of important nodes or retention of irrelevant ones.

### Mechanism 3
- Claim: Linear computation graphs enable exact causal effect computation without intervention.
- Mechanism: The strict linearity of OV + Transcoder circuits means that downstream activation is a linear combination of upstream feature activations, allowing causal attribution through simple gradient computation.
- Core assumption: The circuit remains linear for a single input, even if it's non-linear across different inputs.
- Evidence anchors:
  - [abstract] "Our methods do not require linear approximation to compute the causal effect of each node."
  - [section 2.3] "This two-step paradigm gives us a simplified and feature-based version of attention functionality and allows a fine-grained analysis through attention in a non-approximated manner."
  - [corpus] Moderate evidence - linear approximation is standard in circuit analysis, but exact computation without approximation is novel.
- Break condition: If attention patterns or MLP transformations introduce non-linearities that break the single-input linearity assumption.

## Foundational Learning

- Concept: Sparse Autoencoders (SAEs)
  - Why needed here: SAEs decompose hidden activations into interpretable features, providing the primitive units for circuit analysis.
  - Quick check question: What are the three loss components in SAE training and what does each optimize for?

- Concept: Linear Computation Graphs
  - Why needed here: The circuit discovery pipeline relies on strict linearity to compute causal effects without approximation.
  - Quick check question: Why can't standard MLPs be used directly for circuit analysis while Transcoders can?

- Concept: Attribution Methods
  - Why needed here: Hierarchical Attribution is the key technique for isolating relevant subgraphs from the linear computation graph.
  - Quick check question: How does Hierarchical Attribution differ from standard gradient-based attribution methods?

## Architecture Onboarding

- Component map: Input → GPT-2 Small → SAEs on all residual stream activations → Transcoders on MLP outputs → Linear computation graph (QK + OV circuits) → Hierarchical Attribution → Subgraph isolation

- Critical path:
  1. Train SAEs on GPT-2 Small activations
  2. Train Transcoders on MLP pre/post activations
  3. Construct linear computation graph for input
  4. Apply Hierarchical Attribution with threshold
  5. Analyze isolated subgraph for circuit patterns

- Design tradeoffs:
  - SAE feature count vs reconstruction quality: More features improve interpretability but increase computational cost
  - Transcoder sparsity vs accuracy: Higher sparsity improves linearity but may reduce reconstruction fidelity
  - Attribution threshold vs circuit completeness: Lower thresholds capture more features but may include noise

- Failure signatures:
  - Poor reconstruction CE loss (>5% increase) indicates SAE/Transcoder training issues
  - Disconnected attribution subgraphs suggest threshold is too high
  - Non-interpretable features indicate feature pruning was too aggressive

- First 3 experiments:
  1. Run SAE training on single attention layer output, verify variance explained >80% and L0 loss <50
  2. Construct linear graph for simple bracket-matching input, verify exact linearity of OV circuits
  3. Apply Hierarchical Attribution with varying thresholds on IOI example, measure logit recovery vs sparsity tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we extend the circuit discovery pipeline to handle non-linear transformations beyond ReLU, such as LayerNorm and Softmax?
- Basis in paper: [inferred] The paper mentions that non-linear transformations are ubiquitous in neural networks and that the current pipeline relies on linear approximations for MLP layers. However, it does not explicitly address how to handle other non-linear transformations like LayerNorm and Softmax.
- Why unresolved: The paper focuses on linear computation graphs and does not provide a clear strategy for dealing with other non-linear functions that are commonly used in transformer models.
- What evidence would resolve it: Developing a method to decompose non-linear transformations into linear components or finding a way to approximate their effects within the linear framework would resolve this question.

### Open Question 2
- Question: How can we improve the interpretability of Sparse Autoencoders (SAEs) to reduce the number of uninterpretable features?
- Basis in paper: [explicit] The paper mentions that not all SAE features are interpretable and that they adopt methods to improve interpretability, but it does not provide a detailed explanation of these methods.
- Why unresolved: The paper acknowledges the issue of uninterpretable features but does not offer a comprehensive solution or methodology for improving the interpretability of SAEs.
- What evidence would resolve it: Developing a systematic approach to filter out uninterpretable features or enhancing the training process to produce more interpretable features would resolve this question.

### Open Question 3
- Question: How can we generalize the circuit discovery pipeline to handle more abstract behaviors beyond specific input sequences?
- Basis in paper: [inferred] The paper focuses on analyzing specific input sequences and their corresponding circuits, but it does not address how to generalize the findings to more abstract or diverse behaviors.
- Why unresolved: The current pipeline is tailored to specific inputs and may not be directly applicable to more general or abstract behaviors, limiting its broader applicability.
- What evidence would resolve it: Extending the pipeline to handle a wider range of inputs or developing a method to identify common patterns across different inputs would resolve this question.

## Limitations

- The sparsity constraints enabling Transcoder linearity are not empirically validated beyond GPT-2 Small, raising questions about scalability to larger models.
- Hierarchical Attribution threshold selection lacks systematic evaluation of sensitivity to different values or input types.
- The method's performance on more complex, distributed circuits beyond the three studied examples is unknown.

## Confidence

**High Confidence (Level 1):** The core claim that Transcoders enable strict linearity in MLP computation graphs is well-supported by the mathematical formulation and ablation studies. The implementation details for SAE training and the basic Hierarchical Attribution algorithm are clearly specified.

**Medium Confidence (Level 2):** The claim that Hierarchical Attribution outperforms standard attribution methods is supported by the reported experiments, though the evaluation is limited to 20 IOI samples and one comparison metric (logit recovery). The effectiveness of the overall pipeline for automatic circuit discovery is demonstrated but not comprehensively validated across diverse circuit types.

**Low Confidence (Level 3):** The scalability claims to larger models and more complex circuits are largely speculative, with no empirical evidence beyond GPT-2 Small. The assumption that sparsity constraints create invariant edge weights in Transcoders is theoretically motivated but not rigorously proven across different sparsity regimes.

## Next Checks

1. **Scale Validation Test:** Apply the pipeline to GPT-2 Medium or Large and measure whether the same level of linearity is maintained in the computation graphs. Track reconstruction CE loss, variance explained, and attribution accuracy as model size increases.

2. **Threshold Sensitivity Analysis:** Systematically vary the Hierarchical Attribution threshold τ across multiple orders of magnitude (e.g., 0.01, 0.1, 1.0, 10.0) on the same IOI examples and plot the tradeoff between circuit completeness (leaf node attribution sum) and noise (number of nodes retained).

3. **Circuit Complexity Stress Test:** Apply the method to a more distributed circuit type, such as the copy mechanism or multi-hop reasoning, and evaluate whether the discovered subgraphs can be meaningfully interpreted and whether they capture the full causal pathway from input to output.