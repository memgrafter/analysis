---
ver: rpa2
title: 'Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization
  in RLHF'
arxiv_id: '2401.16335'
source_url: https://arxiv.org/abs/2401.16335
tags:
- reward
- learning
- arxiv
- policy
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two key issues in RLHF reward learning: reward
  overfitting (test loss increasing after one epoch) and reward overoptimization (true
  reward decreasing with continued policy training). The authors identify the root
  cause as the inadequacy of cross-entropy loss for long-tailed preference datasets.'
---

# Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF

## Quick Facts
- **arXiv ID**: 2401.16335
- **Source URL**: https://arxiv.org/abs/2401.16335
- **Reference count**: 40
- **Key outcome**: IDS mitigates reward overfitting and overoptimization by updating both model and data labels in each epoch, replacing hard labels with soft labels that reflect model uncertainty.

## Executive Summary
This paper addresses two critical issues in RLHF reward learning: reward overfitting (where test loss increases after one epoch) and reward overoptimization (where true reward decreases with continued policy training). The authors identify the root cause as the inadequacy of cross-entropy loss for long-tailed preference datasets. They propose Iterative Data Smoothing (IDS), an algorithm that updates both model parameters and data labels in each training epoch, replacing hard labels with soft labels predicted by the model. This approach implicitly penalizes infrequent comparisons while accurately estimating frequently compared pairs. Theoretical analysis in the tabular bandit setting shows IDS learns ground truth for sufficiently compared pairs while ignoring infrequent comparisons. Empirical results demonstrate IDS outperforms traditional MLE and pessimistic MLE in both bandit and neural network settings, maintaining decreasing loss across epochs and achieving better reward-KL tradeoffs during policy learning.

## Method Summary
IDS is an iterative algorithm that alternates between updating model parameters using current labels and updating labels using current model predictions. The label update is a weighted combination of previous labels and model-predicted probabilities, with the weight determined by hyperparameter β. The model update uses standard gradient descent with step size α. This creates a smoothing effect where infrequently compared pairs maintain soft labels near 0.5, while frequently compared pairs converge to their true labels. The method is applied to both tabular bandit problems and neural network reward models trained on preference datasets like HH and TLDR.

## Key Results
- IDS prevents reward overfitting by maintaining decreasing test loss across training epochs, unlike MLE which shows increasing loss after the first epoch
- IDS mitigates reward overoptimization, achieving better reward-KL tradeoff curves during policy learning compared to MLE and pessimistic MLE
- Theoretical analysis shows IDS learns ground truth for sufficiently compared pairs while keeping rewards for infrequent comparisons close to initialization
- Empirical results on HH and TLDR datasets demonstrate IDS consistently outperforms traditional MLE and pessimistic MLE baselines

## Why This Works (Mechanism)

### Mechanism 1
IDS mitigates reward overfitting by replacing hard labels with soft labels that reflect model uncertainty, implicitly penalizing infrequent comparisons. During each training epoch, IDS updates both model parameters and data labels. The labels are updated as a weighted combination of previous labels and model-predicted probabilities. For infrequently compared pairs, the model-predicted probabilities remain near 0.5, effectively downweighting their contribution to training. Core assumption: The soft label update process converges to stable values that accurately reflect model confidence in predictions. Evidence: The abstract states "replacing hard labels with soft labels predicted by the model" and section 3.2.2 notes "For seldom-seen samples, the model will make minimal adjustments to the reward."

### Mechanism 2
IDS mitigates reward overoptimization by maintaining balanced reward estimates for infrequently observed arms. By keeping reward estimates for rarely compared arms close to initialization values (through minimal label updates), IDS prevents these arms from being incorrectly selected as optimal during policy learning. Core assumption: The initialization of rewards (typically zero) is reasonable and doesn't bias the final estimates. Evidence: The abstract mentions "replacing hard labels with soft labels predicted by the model" and section 3.2.2 states "For seldom-seen samples, the model will make minimal adjustments to the reward."

### Mechanism 3
IDS enables effective learning from imbalanced datasets by learning ground truth for frequently compared pairs while ignoring infrequent comparisons. The two-scale analysis shows that for pairs with many comparisons, the effective step size for reward updates is large enough to converge to ground truth, while for infrequent pairs, the effective step size is small enough that the reward estimates remain close to initialization. Core assumption: The number of observations can be partitioned into "frequent" and "infrequent" categories based on the relationship between α, β, and sample counts. Evidence: Section 3.2.2 states "When there are sufficient observations...the difference of reward will be close to the ground truth" and "When the number of observations is not large...the difference d(t) is updated less frequently."

## Foundational Learning

- **Concept: Cross-entropy loss and maximum likelihood estimation**
  - Why needed here: The paper identifies the inadequacy of cross-entropy loss for long-tailed preference datasets as the root cause of both overfitting and overoptimization
  - Quick check question: Why does maximizing cross-entropy likelihood lead to infinite rewards for infrequently compared arms?

- **Concept: Knowledge distillation and soft labels**
  - Why needed here: IDS leverages the insight from knowledge distillation that soft labels can be more effective than hard labels for training
  - Quick check question: How does using soft labels instead of hard labels help prevent overfitting to noisy comparisons?

- **Concept: Reinforcement learning with human feedback (RLHF) pipeline**
  - Why needed here: Understanding the two-phase RLHF process (reward learning then policy learning) is crucial for understanding how overfitting and overoptimization manifest
  - Quick check question: What are the two main phases of RLHF and how do they relate to the overfitting and overoptimization problems?

## Architecture Onboarding

- **Component map**: Preference dataset -> IDS algorithm -> Smoothed reward model -> Policy learner -> Optimized policy
- **Critical path**: 1. Initialize model and labels 2. While not converged: update model parameters using current labels, update labels using current model predictions 3. Return final model
- **Design tradeoffs**: Step size α (model update rate) vs β (label update rate): Need to balance how quickly the model learns vs how quickly labels are smoothed. Label initialization: Starting with hard labels vs soft labels. Convergence criteria: When to stop the iterative process.
- **Failure signatures**: Overfitting persists: Check if label update rate β is too low. Reward overoptimization persists: Check if model update rate α is too high. Slow convergence: Check if both step sizes are appropriate for dataset size.
- **First 3 experiments**: 1. Replicate the 3-armed bandit experiment from Figure 1 to verify IDS prevents infinite reward estimates. 2. Test IDS on a small neural network with known ground truth to verify it learns the correct reward function. 3. Compare policy performance (true reward vs KL divergence) between IDS and MLE on a simple RLHF task.

## Open Questions the Paper Calls Out

### Open Question 1
How does the Iterative Data Smoothing (IDS) algorithm perform when applied to datasets with more than three arms or with non-uniform comparison probabilities? The paper discusses performance in both bandit and neural network settings but focuses primarily on a simplified three-armed bandit problem and a neural network with three different model sizes. Conducting experiments with datasets containing more than three arms and varying comparison probabilities would provide insights into the scalability and robustness of the IDS algorithm.

### Open Question 2
What are the theoretical guarantees of IDS in terms of convergence rate and error bounds for non-asymptotic regimes? The paper provides theoretical analysis in the tabular bandit case, showing that IDS learns the ground truth for sufficiently compared pairs while ignoring infrequent comparisons. However, the convergence rate and error bounds are not explicitly discussed. Developing a comprehensive theoretical framework that includes convergence rate analysis and error bounds for IDS in non-asymptotic regimes would provide a clearer understanding of its performance guarantees.

### Open Question 3
How does the choice of hyperparameters (α and β) in IDS affect its performance, and are there optimal strategies for selecting these parameters? The paper mentions that the performance of IDS can be sensitive to the choice of hyperparameters α and β, but it does not provide a detailed analysis of how these parameters affect the algorithm's performance or strategies for selecting them. Conducting a sensitivity analysis of IDS performance with respect to different values of α and β, and developing strategies for optimal parameter selection based on the dataset characteristics, would help in understanding the algorithm's behavior and improving its practical applicability.

## Limitations

- The theoretical analysis relies on a specific hard instance construction in the bandit setting, but the exact mechanism for generating this instance is not fully specified
- Empirical validation is limited to preference ranking tasks, with untested performance on other types of human feedback like instruction following or safety alignment
- The paper doesn't provide systematic analysis of hyperparameter sensitivity, with different regimes appearing optimal for bandit versus neural network settings

## Confidence

- **High confidence**: IDS prevents reward overfitting (test loss not increasing after first epoch) - well-supported by both theoretical analysis and empirical results
- **Medium confidence**: IDS mitigates reward overoptimization - supported by empirical results but relies on reasonable initialization assumptions
- **Medium confidence**: Theoretical analysis provides insight but assumes clear partitioning between frequent and infrequent comparisons

## Next Checks

1. **Reproduce the hard instance construction**: Implement the specific comparison distribution µ used in the bandit experiments to verify the two-scale theoretical analysis. Test IDS on this instance and verify that rewards for infrequent arms remain bounded while rewards for frequent arms converge to ground truth.

2. **Cross-dataset generalization test**: Apply IDS to a different type of human feedback dataset (such as instruction following or safety alignment) and evaluate both overfitting metrics (test loss) and overoptimization metrics (KL-reward tradeoff) to verify the method's general applicability beyond preference ranking.

3. **Hyperparameter sensitivity analysis**: Systematically vary the step sizes α and β across orders of magnitude for both bandit and neural network experiments. Plot the performance curves to identify optimal regimes and test whether the theoretical analysis predicts the observed behavior patterns.