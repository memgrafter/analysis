---
ver: rpa2
title: 'Raidar: geneRative AI Detection viA Rewriting'
arxiv_id: '2401.12970'
source_url: https://arxiv.org/abs/2401.12970
tags:
- text
- detection
- arxiv
- generated
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Raidar, a method for detecting machine-generated
  text by leveraging large language models (LLMs) to rewrite input text and analyzing
  the editing distance of the output. The core idea is that LLMs tend to make fewer
  modifications when rewriting their own generated text compared to human-written
  text, due to perceiving the former as high-quality.
---

# Raidar: geneRative AI Detection viA Rewriting

## Quick Facts
- arXiv ID: 2401.12970
- Source URL: https://arxiv.org/abs/2401.12970
- Authors: Chengzhi Mao; Carl Vondrick; Hao Wang; Junfeng Yang
- Reference count: 40
- One-line primary result: Detects machine-generated text by analyzing rewriting behavior differences, achieving up to 29 F1 points improvement over existing methods.

## Executive Summary
Raidar is a novel method for detecting machine-generated text that leverages large language models (LLMs) to rewrite input text and analyzes the editing distance between the original and rewritten versions. The core insight is that LLMs tend to make fewer modifications when rewriting their own generated text compared to human-written text, perceiving the former as higher quality. This property allows Raidar to use editing distance as a robust feature for detection, significantly improving F1 scores over existing methods while remaining effective against text generated from different LLM sources.

## Method Summary
Raidar prompts LLMs to rewrite input text and calculates the editing distance (Levenshtein or bag-of-words) between the original and rewritten versions. These distances serve as features for binary classification to determine whether text is AI-generated or human-written. The method is designed to be robust to black-box generation models by operating solely on symbolic word representations without requiring access to high-dimensional features or probability outputs.

## Key Results
- Achieves up to 29 F1 points improvement over existing methods like Ghostbuster
- Effective at detecting text generated from multiple different LLM sources
- Remains robust even when generation models are aware of the detection mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs perceive AI-generated text as high-quality, leading to fewer rewriting modifications compared to human-written text.
- Mechanism: When prompted to rewrite, LLMs treat text generated by similar models as already high-quality and make fewer edits. Human-written text is perceived as less optimal, prompting more modifications.
- Core assumption: The LLM's rewriting behavior is influenced by its internal perception of text quality, which differs between AI-generated and human-written content.
- Evidence anchors:
  - [abstract]: "LLMs are more likely to modify human-written text than AI-generated text when tasked with rewriting. This tendency arises because LLMs often perceive AI-generated text as high-quality, leading to fewer modifications."
  - [section 3.1]: "Our key hypothesis is that text from auto-regressive generative models retains a consistent structure, which another such model will likely to also have a low loss and treat it as high quality."
  - [corpus]: Weak; neighboring papers discuss AI detection but do not anchor the quality-perception mechanism directly.
- Break condition: If the LLM is trained on a dataset where AI-generated text is common and not perceived as high-quality, or if adversarial prompts explicitly instruct the LLM to make significant changes to AI-generated text.

### Mechanism 2
- Claim: The editing distance between original and rewritten text serves as a robust feature for detecting AI-generated content.
- Mechanism: By calculating the Levenshtein distance or bag-of-words edit distance between the input text and its rewritten version, we obtain a numerical feature that distinguishes AI-generated text (lower distance) from human-written text (higher distance).
- Core assumption: The rewriting process by LLMs produces consistent and measurable changes that correlate with the source of the text.
- Evidence anchors:
  - [section 3.2]: "We treat the output of LLM as symbolic representations that encode information about the data... Our algorithm operates totally on the discrete, symbolic representations from the LLM."
  - [section 4.3]: "We compare our results on three datasets... Our method Raidar outperforms the Ghostbuster method by up to 29 points."
  - [corpus]: Weak; related work discusses detection features but not specifically the symbolic rewriting distance approach.
- Break condition: If the rewriting LLM is significantly different from the generation LLM, or if the input text is already optimized to minimize rewriting changes.

### Mechanism 3
- Claim: The method is robust to text generated from different LLM sources due to the symbolic nature of the rewriting process.
- Mechanism: Since Raidar operates on word symbols without requiring access to high-dimensional features or probability outputs, it can detect text from various LLMs by using a single rewriting LLM.
- Core assumption: The symbolic rewriting distance is a generalizable feature that transcends the specific architecture or training data of the generation LLM.
- Evidence anchors:
  - [section 4.4]: "Using the same GPT-3.5-Turbo rewriting model, we present F1 detection scores for detecting text from five generation models across three diverse tasks."
  - [section 4.3]: "Operating solely on word symbols without high-dimensional features, our method is compatible with black box LLMs."
  - [corpus]: Weak; neighboring papers do not provide evidence for cross-LLM robustness.
- Break condition: If the generation LLMs use fundamentally different text structures that the rewriting LLM cannot process effectively.

## Foundational Learning

- Concept: Levenshtein Distance
  - Why needed here: It quantifies the minimum number of single-character edits required to change one string into another, providing a measurable feature for detection.
  - Quick check question: What is the Levenshtein distance between "kitten" and "sitting"?

- Concept: Bag-of-Words Edit Distance
  - Why needed here: It captures the change in the composition of words between the original and rewritten text, offering a semantic-agnostic feature.
  - Quick check question: How would you compute the bag-of-words edit distance between two sentences?

- Concept: Binary Classification with Logistic Regression
  - Why needed here: It uses the calculated editing distances as features to classify text as either AI-generated or human-written.
  - Quick check question: What is the role of the logistic function in binary classification?

## Architecture Onboarding

- Component map: Input Text → Rewriting LLM (e.g., GPT-3.5-Turbo) → Rewritten Text → Edit Distance Calculation → Classifier (Logistic Regression or XGBoost) → Detection Output
- Critical path: Text rewriting and edit distance calculation are the core steps that directly impact detection accuracy.
- Design tradeoffs:
  - Using a single rewriting LLM simplifies the process but may limit detection of text from very different models.
  - Symbolic rewriting distance is robust but may be less sensitive to nuanced semantic differences.
- Failure signatures:
  - Low detection accuracy may indicate that the rewriting LLM is not effectively distinguishing between AI-generated and human-written text.
  - High false positives could suggest that the edit distance threshold is too low.
- First 3 experiments:
  1. Test the rewriting LLM on a small set of known AI-generated and human-written text to observe the edit distances.
  2. Vary the rewriting prompts to see how they affect the edit distances and detection performance.
  3. Evaluate the classifier's performance using different combinations of edit distance features (Levenshtein vs. Bag-of-Words).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Raidar vary with different prompting strategies beyond the three manually created prompts tested in the paper?
- Basis in paper: [explicit] The paper mentions that they manually created prompts to access the invariance property and notes that automatic ways to generate prompts could be explored in future work.
- Why unresolved: The paper only tests three specific prompts and does not explore a wide range of prompting strategies or optimize the prompts for better performance.
- What evidence would resolve it: Systematic testing of a diverse set of prompts, including those generated through optimization techniques, and comparison of their detection performance.

### Open Question 2
- Question: How does Raidar's performance scale with the length of the input text beyond the range tested in the paper?
- Basis in paper: [explicit] The paper shows detection performance trends with increasing input length for some datasets but does not explore extremely short or very long texts.
- Why unresolved: The paper only tests input lengths up to a certain point and does not investigate the method's effectiveness on very short texts (e.g., single sentences) or extremely long documents.
- What evidence would resolve it: Extensive testing of Raidar's performance on texts of varying lengths, from very short to extremely long, to determine the method's effectiveness across a wide range of input sizes.

### Open Question 3
- Question: How does Raidar perform when detecting text generated by future, more advanced language models that may be trained to minimize the editing distance when rewritten?
- Basis in paper: [explicit] The paper acknowledges that future language models may be trained to minimize the editing distance and suggests that this could be a limitation of the method.
- Why unresolved: The paper only tests Raidar on current language models and does not explore its performance on hypothetical future models designed to evade detection.
- What evidence would resolve it: Testing Raidar on a diverse set of current and future language models, including those specifically designed to minimize editing distance, to assess the method's robustness against evolving text generation techniques.

## Limitations

- The method's effectiveness relies on LLMs perceiving AI-generated text as high-quality, which may not hold across all model architectures or training paradigms.
- The paper lacks specificity on prompt engineering, which is critical for reproducibility - particularly which prompt combinations yielded optimal results.
- Evaluation focuses primarily on F1 scores without extensive analysis of false positive/false negative patterns that would reveal failure modes in practical deployment.

## Confidence

- **High Confidence**: The empirical results showing improved detection performance over baselines (up to 29 F1 points) are well-supported by the experimental data presented.
- **Medium Confidence**: The claim that Raidar is robust to black-box generation models is supported but limited to five generation models tested; generalization to arbitrary models remains partially unproven.
- **Medium Confidence**: The symbolic nature of the rewriting distance as a generalizable feature is theoretically sound but requires more extensive cross-model validation.

## Next Checks

1. **Prompt Sensitivity Analysis**: Systematically test multiple rewriting prompts (beyond those mentioned) on a small validation set to determine which prompt formulations maximize the edit distance differential between human and AI-generated text.

2. **Cross-Architecture Robustness**: Evaluate Raidar's performance on generation models with fundamentally different architectures (e.g., non-transformer models, models with different tokenization schemes) to test the limits of the symbolic rewriting distance approach.

3. **Adversarial Robustness Test**: Generate adversarial examples where human-written text is deliberately "polished" to appear AI-generated, and vice versa, to measure Raidar's resilience to intentional obfuscation attempts.