---
ver: rpa2
title: 'BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language'
arxiv_id: '2412.08329'
source_url: https://arxiv.org/abs/2412.08329
tags:
- retrieval
- beir
- benchmark
- datasets
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BEIR-NL extends the BEIR benchmark to Dutch by automatically translating
  English datasets using Gemini-1.5-flash. Translation quality was validated by native
  Dutch speakers, revealing 98% semantic accuracy.
---

# BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language

## Quick Facts
- arXiv ID: 2412.08329
- Source URL: https://arxiv.org/abs/2412.08329
- Reference count: 14
- Primary result: Machine-translated BEIR benchmark shows dense retrieval-trained models outperform BM25 in Dutch zero-shot IR evaluation

## Executive Summary
BEIR-NL extends the BEIR benchmark to Dutch by automatically translating English datasets using Gemini-1.5-flash. Translation quality was validated by native Dutch speakers, revealing 98% semantic accuracy. Evaluation on 14 datasets with multilingual dense and lexical models (including BM25) showed larger retrieval-trained models outperform BM25, though BM25 remains competitive for smaller models. Combining BM25 with reranking models achieves performance on par with the best dense ranking models. Back-translation experiments indicated translation impacts retrieval performance, suggesting limitations of machine-translated benchmarks. BEIR-NL is publicly available on Hugging Face and provides a foundation for Dutch IR model development, though native Dutch datasets are still needed for full linguistic representation.

## Method Summary
The authors automatically translated BEIR datasets from English to Dutch using Gemini-1.5-flash, validated translation quality through native Dutch speaker review achieving 98% semantic accuracy. They evaluated 14 Dutch datasets using BM25, dense ranking models (e5-multilingual-large-instruct, gte-multilingual-base, jina-embeddings-v3, bge-m3), and reranking models (bge-reranker-v2-m3, jina-reranker-v2-base-multilingual, gte-multilingual-reranker) with nDCG@10 and Recall@100 metrics. Back-translation experiments measured translation impact on retrieval performance. The benchmark is publicly available on Hugging Face for community use.

## Key Results
- Dense retrieval-trained models (e5-multilingual-large-instruct, gte-multilingual-base, jina-embeddings-v3, bge-m3) outperform BM25 on BEIR-NL
- BM25 combined with reranking models achieves performance comparable to the best dense ranking models
- Back-translation experiments show translation impacts retrieval performance, with BM25 dropping 1.9 points and dense models dropping 2.6 points in NDCG@10
- Translation quality validation showed 98% semantic accuracy by native Dutch speakers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machine translation with Gemini-1.5-flash enables zero-shot IR benchmarking across languages.
- Mechanism: The model automatically translates BEIR datasets from English to Dutch using prompts that specify input type and domain context, creating a parallel corpus for evaluation.
- Core assumption: Machine translation quality is sufficient to preserve semantic relationships necessary for IR evaluation.
- Evidence anchors:
  - [abstract] Translation quality was validated by native Dutch speakers, revealing 98% semantic accuracy.
  - [section] "Using Gemini-1.5-flash which offers a good balance of speed, cost, and translation quality."
  - [corpus] Corpus contains 14 BEIR datasets translated to Dutch, with translation validation showing 98% semantic accuracy.
- Break condition: If translation introduces significant semantic drift or lexical mismatches between queries and relevant passages, retrieval performance will degrade.

### Mechanism 2
- Claim: Dense retrieval models trained for retrieval outperform both BM25 and older sentence embedding models in zero-shot settings.
- Mechanism: Retrieval-trained models like multilingual-e5-large-instruct, gte-multilingual-base, jina-embeddings-v3, and bge-m3 generate contextualized embeddings optimized for semantic similarity, which captures relationships beyond keyword matching.
- Core assumption: The retrieval task benefits more from models explicitly trained on retrieval objectives than from general-purpose embeddings.
- Evidence anchors:
  - [abstract] "Our experiments show that BM25 remains a competitive baseline, and is only outperformed by the larger dense models trained for retrieval."
  - [section] "We observe a sizeable gap between the older 'sentence embedding' models, and the new generation of trained-for-retrieval models."
  - [corpus] Table 3 shows multilingual-e5-large-instruct achieving highest Recall@100 on half of the datasets, with older models performing significantly worse.
- Break condition: If the retrieval-trained models are exposed to BEIR data during training (in-domain contamination), performance comparisons become invalid.

### Mechanism 3
- Claim: Combining BM25 with reranking models achieves performance comparable to the best dense retrieval models.
- Mechanism: BM25 provides fast, high-recall initial retrieval, while reranking models apply cross-encoder attention to the top-100 results for improved precision, balancing speed and accuracy.
- Core assumption: The combination leverages BM25's lexical strengths and reranking models' semantic understanding without requiring full cross-encoding of all document-query pairs.
- Evidence anchors:
  - [abstract] "When combined with reranking models, BM25 achieves performance on par with the best dense ranking models."
  - [section] "As demonstrated, this approach can often offer a competitive edge over the best ranking models."
  - [corpus] Table 3 shows BM25 + reranker combinations (bge-reranker, jina-reranker, gte-reranker) achieving NDCG@10 scores competitive with or exceeding top dense models.
- Break condition: If reranking models cannot effectively distinguish relevance among the BM25 top-100 results, the combination approach loses its advantage.

## Foundational Learning

- Concept: Semantic similarity vs. lexical matching in information retrieval
  - Why needed here: Understanding why dense models outperform BM25 requires grasping how semantic embeddings capture meaning beyond exact keyword matches.
  - Quick check question: What happens to retrieval performance when queries and relevant documents use different vocabulary to express the same concept?

- Concept: Zero-shot evaluation methodology
  - Why needed here: The benchmark relies on evaluating models without task-specific fine-tuning, which affects how we interpret performance differences.
  - Quick check question: Why might models that were exposed to BEIR data during training show inflated performance in zero-shot evaluations?

- Concept: Translation impact on benchmark validity
  - Why needed here: Understanding how translation quality affects retrieval performance is crucial for interpreting BEIR-NL results.
  - Quick check question: How might independent translation of queries and documents create lexical mismatches that affect BM25 performance?

## Architecture Onboarding

- Component map: Gemini-1.5-flash (translation) → BEIR-NL datasets → IR models (BM25, dense ranking, reranking) → evaluation metrics (nDCG@10, Recall@100) → comparison with original BEIR
- Critical path: Translation quality → model performance → benchmark validity → research insights
- Design tradeoffs: Automated translation enables rapid benchmark creation but may introduce semantic drift; combining BM25 with reranking balances speed and accuracy but adds computational overhead
- Failure signatures: Performance drops when comparing translated vs. original datasets indicate translation issues; inflated scores suggest in-domain contamination; inconsistent results across datasets suggest model limitations
- First 3 experiments:
  1. Evaluate BM25 on translated vs. back-translated datasets to measure translation impact
  2. Compare retrieval-trained models against older sentence embedding models to validate the importance of retrieval-specific training
  3. Test BM25 + reranking combinations against pure dense models to assess the effectiveness of hybrid approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much does translation quality affect retrieval performance in BEIR-NL compared to native Dutch datasets?
- Basis in paper: [explicit] The authors observed a performance drop when back-translating Dutch datasets to English, with BM25 dropping 1.9 points and dense models dropping 2.6 points in NDCG@10. They note this provides a proxy for translation impact but don't quantify the full difference from native data.
- Why unresolved: The study only measured back-translation impact, not the gap between translated and native Dutch data. Native Dutch datasets would be needed for direct comparison.
- What evidence would resolve it: Creation and evaluation of BEIR-NL using native Dutch datasets for the same domains, followed by head-to-head comparison with the translated version.

### Open Question 2
- Question: What is the optimal combination strategy between BM25 and reranking models for Dutch IR tasks?
- Basis in paper: [explicit] The authors found that BM25 combined with reranking models achieved performance on par with the best dense ranking models, but didn't explore different combination strategies or thresholds.
- Why unresolved: The paper only tested reranking on the top-100 BM25 results. Different cutoffs, weighting schemes, or cascade approaches could yield better performance.
- What evidence would resolve it: Systematic experiments varying the number of BM25 results passed to rerankers, testing different fusion methods (Rerank-then-BM25, weighted sum, etc.), and optimizing parameters for each dataset.

### Open Question 3
- Question: How does the performance of Dutch IR models degrade on BEIR-NL datasets compared to their original BEIR counterparts?
- Basis in paper: [inferred] The authors observed that BM25 performance on BEIR-NL lags behind BEIR by 6-7 points in NDCG@10 and Recall@100, and dense models also show performance differences. However, they didn't provide a comprehensive analysis of degradation patterns.
- Why unresolved: While the paper noted performance gaps, it didn't analyze which types of queries or domains suffer most from translation, or whether certain retrieval methods are more robust to translation effects.
- What evidence would resolve it: Detailed error analysis categorizing performance drops by query type (lexical vs. semantic), domain (biomedical vs. general), and retrieval method characteristics.

## Limitations
- Translation quality limitations despite 98% semantic accuracy validation may not fully capture native linguistic patterns
- Absence of native Dutch datasets prevents definitive comparison between translated and native performance
- Limited analysis of which query types or domains are most affected by translation artifacts

## Confidence

| Claim | Confidence |
|-------|------------|
| Dense retrieval-trained models outperform older sentence embedding models | High |
| BM25 + reranking combination effectiveness | Medium |
| Overall benchmark validity | Medium |
| Model superiority comparisons controlling for parameters | Low |

## Next Checks
1. Conduct controlled experiments comparing native Dutch queries with their machine-translated counterparts on the same document collections to quantify translation impact on retrieval performance across all model types.
2. Perform contamination analysis to verify none of the evaluated models were exposed to BEIR data during training, ensuring zero-shot evaluation validity remains intact.
3. Test model performance on the original English BEIR datasets to establish baseline performance and determine whether observed improvements in Dutch are due to language-specific factors or general model capabilities.