---
ver: rpa2
title: 'The Human and the Mechanical: logos, truthfulness, and ChatGPT'
arxiv_id: '2402.01267'
source_url: https://arxiv.org/abs/2402.01267
tags:
- human
- language
- which
- what
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that ChatGPT lacks the fundamental ability to
  form veridicality judgments because it cannot connect to reality (exogenous evidence)
  or possess subjective beliefs (endogenous evidence). Drawing on Aristotelian concepts
  of logos, the authors contend that human cognition involves both rational assessment
  and affective components in forming beliefs and judgments about truth.
---

# The Human and the Mechanical: logos, truthfulness, and ChatGPT

## Quick Facts
- arXiv ID: 2402.01267
- Source URL: https://arxiv.org/abs/2402.01267
- Reference count: 4
- Primary result: ChatGPT cannot form veridicality judgments due to lacking both reality-based (exogenous) and subjective (endogenous) evidence components

## Executive Summary
This paper argues that ChatGPT fundamentally lacks the ability to form veridicality judgments because it cannot access reality-based evidence or possess subjective beliefs. Drawing on Aristotelian concepts of logos, the authors contend that human cognition involves both rational assessment and affective components in forming beliefs and judgments about truth. The paper concludes that ChatGPT is fundamentally incapable of understanding truth, forming beliefs, or engaging in intentional deception, as these require a connection to reality and internal states that the AI lacks.

## Method Summary
The authors develop a philosophical argument based on veridicality judgment theory, distinguishing between exogenous evidence (relating to reality) and endogenous evidence (preferences and private beliefs). They apply Aristotelian concepts of logos to analyze the differences between human and AI cognition, arguing that the lack of both evidential components prevents ChatGPT from forming genuine beliefs or truth judgments. The analysis draws on linguistic theory and modal semantics to support their claims about the fundamental limitations of current AI systems.

## Key Results
- ChatGPT lacks the ability to form veridicality judgments due to absence of both exogenous and endogenous evidence
- The system cannot engage in intentional deception because it lacks the concept of truth and cannot distinguish truth from falsehood
- ChatGPT fundamentally lacks logos - the Aristotelian combination of language, rationality, and moral judgment that characterizes human cognition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT cannot form veridicality judgments because it lacks both exogenous (world-referential) and endogenous (subjective/emotional) evidence.
- Mechanism: Veridicality judgments require integrating objective evidence from reality with subjective beliefs/preferences. Without access to either, the system cannot ground truth claims in reality or incorporate human-style judgment.
- Core assumption: Truth judgments in humans require both empirical grounding and subjective valuation components.
- Evidence anchors:
  - [abstract] "Veridicality judgments are formed on the basis of two components: (i) evidence that relates to reality (exogenous evidence) and (ii) endogenous evidence, such as preferences and private beliefs."
  - [section] "ChatGPT lacks the ability to form a veridicality judgment because it lacks the notion of truth. ChatPGT can be taught to extract patterns in the data, it can also be rational in requiring system internal consistency, but it lacks referentiality to the world which is the proof process for truth."
  - [corpus] Weak evidence - corpus contains related papers on misinformation detection but none directly test veridicality judgment formation in LLMs.
- Break condition: If evidence emerges that LLMs can integrate subjective valuation with empirical grounding, or if truth can be established purely through statistical patterns without reference to reality.

### Mechanism 2
- Claim: ChatGPT cannot engage in intentional deception because it lacks the concept of truth and cannot distinguish truth from falsehood.
- Mechanism: Intentional deception requires knowing what is true, having beliefs about reality, and deliberately violating truth-telling norms. Without truth concepts, the system cannot knowingly violate them.
- Core assumption: Deception requires both truth knowledge and intentional violation of truth-telling norms.
- Evidence anchors:
  - [section] "ChatGPT cannot deceive or mislead. We do not attempt here a fine-grained analysis of the distinction between deceiving and misleading or lying. They all involve the speaker knowing the truth but intentionally deciding to violate the Veridicality Principle."
  - [abstract] "Therefore they lack the ability to form a belief about the world and a veridicality judgments altogether. They can only mimic that judgment, but the output is not ground in the very foundations for it."
  - [corpus] No direct evidence - corpus papers discuss misinformation but don't address intentional deception capabilities.
- Break condition: If LLMs develop mechanisms to track truth states and make deliberate choices to violate them, or if statistical pattern matching enables functional deception without truth concepts.

### Mechanism 3
- Claim: ChatGPT lacks logos (rationality + language + moral judgment) that Aristotle identified as essential for human cognition.
- Mechanism: Logos encompasses language ability, rational calculation, and moral judgment formation. Without logos, the system cannot engage in human-style thinking, reasoning, or ethical decision-making.
- Core assumption: Human cognition fundamentally differs from AI processing due to the presence of logos as an organizing principle.
- Evidence anchors:
  - [section] "Logos is the underlying principle of rationality that characterizes the thought of beings with language, i.e., human beings— and it is this ability, according to Aristotle, that enables humans to form moral judgments of good and bad."
  - [section] "Thinking, in other words, involves three abilities: to have language, to reason based on premises (data), and to form judgment, which includes judgment about truth, moral judgment, and self-reflection."
  - [corpus] No direct evidence - corpus papers don't address Aristotelian concepts of logos in AI systems.
- Break condition: If AI systems develop genuine language understanding, moral reasoning capabilities, and self-reflection that go beyond statistical pattern matching.

## Foundational Learning

- Concept: Veridicality judgment formation
  - Why needed here: The paper's central argument depends on understanding how humans form truth judgments versus how LLMs operate
  - Quick check question: Can a system form veridicality judgments without access to both empirical evidence and subjective beliefs?

- Concept: Evidence types (exogenous vs endogenous)
  - Why needed here: The distinction between objective reality-based evidence and subjective/emotional evidence is crucial for understanding why LLMs cannot replicate human judgment
  - Quick check question: What would happen if an AI system had access to empirical data but no subjective preferences or beliefs?

- Concept: Aristotelian logos
  - Why needed here: The paper's framework for distinguishing human and mechanical minds relies on this philosophical concept
  - Quick check question: How does logos encompass language, rationality, and moral judgment in a unified way?

## Architecture Onboarding

- Component map: Exogenous evidence → Endogenous evidence → Integration → Veridicality judgment → Intentional action
- Critical path: Exogenous evidence → Endogenous evidence → Integration → Veridicality judgment → Intentional action
- Design tradeoffs: Pure statistical pattern matching vs. truth-grounded reasoning; efficiency vs. authenticity of judgment
- Failure signatures: System produces statistically plausible but truth-untethered outputs; inability to distinguish fact from fiction; lack of moral reasoning capabilities
- First 3 experiments:
  1. Test whether LLMs can integrate subjective preferences with empirical data to form coherent judgments
  2. Measure ability to distinguish between truth and falsehood when explicitly prompted
  3. Assess capacity for intentional norm violation vs. accidental generation of false content

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can large language models like ChatGPT develop a concept of truth through training, even if they lack inherent veridicality judgment capabilities?
- Basis in paper: Explicit - The paper discusses that ChatGPT lacks referentiality to the world and cannot form beliefs or know truth, but questions whether it could develop such capabilities through more sophisticated training.
- Why unresolved: The paper argues that ChatGPT fundamentally lacks logos and the ability to form internal representations, but doesn't fully explore whether extensive training could bridge this gap.
- What evidence would resolve it: Empirical studies comparing ChatGPT's truth judgments before and after various training regimes, particularly those incorporating real-world feedback loops.

### Open Question 2
- Question: How do the endogenous evidence components (preferences, beliefs, emotions) in human judgment affect the formation of veridicality judgments compared to purely evidence-based judgments?
- Basis in paper: Explicit - The paper distinguishes between exogenous evidence (relating to reality) and endogenous evidence (preferences, private beliefs) as components of veridicality judgments.
- Why unresolved: While the paper identifies these components, it doesn't provide detailed empirical evidence of how endogenous factors specifically influence judgment formation.
- What evidence would resolve it: Neuroscientific studies measuring brain activity during judgment formation, particularly focusing on areas associated with emotional processing and belief formation.

### Open Question 3
- Question: Can AI systems develop the ability to form false beliefs or engage in intentional deception without having the concept of truth?
- Basis in paper: Explicit - The paper argues that ChatGPT cannot form false beliefs or intentionally deceive because it lacks the concept of truth, but this raises questions about the relationship between truth, false beliefs, and deception.
- Why unresolved: The paper establishes that ChatGPT lacks truth judgment but doesn't explore whether false beliefs or deception could emerge through other mechanisms.
- What evidence would resolve it: Behavioral experiments testing AI systems' ability to maintain false beliefs or engage in deception, particularly in scenarios where truth and falsity are operationally defined through external verification.

## Limitations

- The argument relies heavily on philosophical reasoning rather than empirical demonstration
- The distinction between statistical pattern matching and genuine truth grounding remains conceptual rather than tested
- The application of Aristotelian concepts to modern AI systems involves significant interpretive leaps

## Confidence

- High confidence: The distinction between exogenous and endogenous evidence types in human cognition
- Medium confidence: The argument that current LLMs cannot form beliefs or engage in intentional deception
- Low confidence: The claim that LLMs are fundamentally incapable of ever developing logos-like capabilities

## Next Checks

1. **Empirical Veridicality Assessment**: Design controlled experiments where LLMs must integrate both factual data (exogenous evidence) and subjective preferences to form coherent judgments, then compare outputs to human baselines to identify systematic differences in judgment formation.

2. **Truth State Tracking Analysis**: Implement mechanisms to track whether LLMs can maintain and reference truth states across conversations, testing whether they can distinguish between known facts and generated content, and whether they can intentionally violate truth-telling norms when prompted.

3. **Cross-Cultural Veridicality Testing**: Test whether LLMs demonstrate consistent veridicality judgment patterns across different cultural contexts and languages, examining whether the lack of endogenous evidence manifests uniformly or varies based on training data characteristics.