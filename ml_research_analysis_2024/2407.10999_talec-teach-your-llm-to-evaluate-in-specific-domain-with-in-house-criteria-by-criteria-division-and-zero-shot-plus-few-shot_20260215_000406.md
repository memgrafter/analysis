---
ver: rpa2
title: 'TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria
  by Criteria Division and Zero-shot Plus Few-shot'
arxiv_id: '2407.10999'
source_url: https://arxiv.org/abs/2407.10999
tags:
- answer
- evaluation
- criteria
- table
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TALEC addresses the challenge of evaluating large language models
  in specific business domains where custom criteria are needed beyond general standards.
  It introduces a model-based evaluation framework using in-context learning to teach
  judge models in-house evaluation criteria, combined with criteria division and an
  engineering approach to adjust and iterate shots.
---

# TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot

## Quick Facts
- arXiv ID: 2407.10999
- Source URL: https://arxiv.org/abs/2407.10999
- Reference count: 33
- Primary result: TALEC achieves over 80% correlation with human judgments using in-context learning for domain-specific evaluation

## Executive Summary
TALEC introduces a novel approach for evaluating large language models in specific business domains where custom criteria are needed beyond general standards. The framework addresses the challenge of teaching judge models in-house evaluation criteria through in-context learning combined with criteria division and an engineering approach to adjust and iterate shots. By breaking down evaluation into individual label-specific prompts and combining zero-shot with few-shot judgments, TALEC achieves over 80% correlation with human judgments, outperforming other automatic evaluation methods and even inter-human correlation in some tasks. The method demonstrates that fine-tuning can be replaced by in-context learning for evaluation purposes while introducing prompt engineering techniques to improve model understanding of complex criteria.

## Method Summary
TALEC uses in-context learning (ICL) to teach evaluation criteria to judge models without requiring parameter updates. The framework employs criteria division, splitting complex evaluation tasks into individual label-specific prompts to reduce prompt complexity and prevent context length overload. It combines zero-shot and few-shot judgments to minimize shot imitation effects on model reasoning, using an engineering approach to adjust shots and iterate for optimal performance. The method also introduces prompt engineering techniques including repeating descriptions before evaluation and standardizing positive/negative example formats. TALEC validates that in-context learning can match or exceed fine-tuned model performance for evaluation tasks, demonstrating that domain-specific evaluation criteria can be effectively taught through demonstration examples alone.

## Key Results
- Achieves over 80% correlation with human judgments across multiple tasks
- Outperforms other automatic evaluation methods in domain-specific criteria evaluation
- Demonstrates criteria division significantly improves evaluation accuracy compared to single-prompt approaches
- Shows in-context learning can replace fine-tuning for evaluation tasks in many cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Criteria division improves model evaluation accuracy by reducing prompt complexity and preventing context length overload
- Mechanism: Breaking down evaluation into individual label-specific prompts allows the model to focus on one criterion at a time, reducing interference from other criteria
- Core assumption: Large language models perform better when prompts are simplified and focused rather than complex and multi-dimensional
- Evidence anchors:
  - [section] "In the tasks of Sentiment Analysis, Search QA, and Title Generation, the evaluation results of the criteria division method significantly outperform those of non-division"
  - [section] "multi-turn with zero-shot approach can improve scores on all task"
- Break condition: When holistic judgment across multiple criteria simultaneously is required

### Mechanism 2
- Claim: Combining zero-shot with few-shot judgment reduces negative impact of shot imitation on model reasoning
- Mechanism: Making independent zero-shot and few-shot judgments then comparing them forces the model to evaluate based on its own reasoning
- Core assumption: Large language models sometimes imitate shot format patterns at the expense of actual reasoning
- Evidence anchors:
  - [section] "This imitation may make the model ignore some key information and drop its Chain of Thought (CoT) ability"
  - [section] "the model will make two completely independent judgments, one is zero-shot judge and another one is few-shot judge"
- Break condition: When shots are exceptionally high quality and directly relevant to the specific judgment task

### Mechanism 3
- Claim: In-context learning can effectively replace fine-tuning for evaluation tasks
- Mechanism: Carefully curated examples through shots and system prompts teach the model evaluation criteria without requiring parameter updates
- Core assumption: Large language models can learn task-specific evaluation criteria through demonstration examples alone
- Evidence anchors:
  - [section] "GPT-4 with in-context learning outperforms the method based on Qwen-72B-Chat across all tasks"
  - [section] "Qwen-72B-Chat integrated with in-context learning exhibits comparable performance to Qwen-72B-Chat combined with SFT"
- Break condition: When evaluation criteria require extensive domain-specific knowledge not present in the base model

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: TALEC relies on teaching evaluation criteria through demonstration examples rather than fine-tuning
  - Quick check question: How does ICL differ from traditional supervised fine-tuning in terms of model parameter updates?

- Concept: Chain of Thought (CoT) reasoning
  - Why needed here: Understanding how model reasoning can be disrupted by shot imitation is critical for TALEC's prompt engineering approach
  - Quick check question: What is the difference between a model that reasons through CoT versus one that simply imitates patterns in examples?

- Concept: Spearman correlation
  - Why needed here: TALEC uses Spearman correlation to measure alignment between model judgments and human evaluations
  - Quick check question: Why might Spearman correlation be preferred over Pearson correlation when evaluating model judgment quality?

## Architecture Onboarding

- Component map: Dataset → Shot engineering → Criteria division → Zero-shot + few-shot judgment → Correlation evaluation
- Critical path: Dataset → Shot engineering → Criteria division → Zero-shot + few-shot judgment → Correlation evaluation
- Design tradeoffs: TALEC trades computational efficiency for accuracy by making multiple independent judgments rather than single holistic ones
- Failure signatures: Model outputs that simply repeat shot patterns without addressing actual evaluation criteria, or cases where context length limits prevent proper shot injection
- First 3 experiments:
  1. Implement basic ICL with few-shot examples and measure correlation with human judgments
  2. Add criteria division and compare performance against single-prompt approach
  3. Implement zero-shot + few-shot combination and measure improvement in specific labels that showed weakness in step 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TALEC's performance scale when evaluating more complex, multi-dimensional criteria beyond the 10 labels used in this study?
- Basis in paper: [explicit] The paper states "too many shots may cause exceeding context length limit" and mentions criteria division as a solution
- Why unresolved: The experiments only tested 10 labels across 4 tasks, with no exploration of performance with 20+ labels or more nuanced criteria
- What evidence would resolve it: Systematic evaluation of TALEC with increasing numbers of labels (10, 20, 50+) and more granular evaluation dimensions

### Open Question 2
- Question: Can the zero-shot plus few-shot approach be optimized beyond the simple two-turn method described?
- Basis in paper: [explicit] "We find that the imitation to text format in shots may make the judge model ignore some key information"
- Why unresolved: The paper uses a basic concatenation approach with no exploration of alternative architectures or aggregation methods
- What evidence would resolve it: Comparative evaluation of different multi-turn architectures and shot selection strategies

### Open Question 3
- Question: How do different LLM base models affect TALEC's evaluation accuracy for domain-specific tasks?
- Basis in paper: [explicit] The paper compares GPT-4, Qwen-72B-Chat with ICL, and Qwen-72B-Chat with SFT, finding GPT-4 performs best
- Why unresolved: The study only examines one domain and three models, with no analysis of how model size or training data distribution affects performance
- What evidence would resolve it: Cross-domain evaluation with multiple model families of varying sizes

## Limitations

- Reliance on human-generated custom criteria and manually optimized shots makes faithful reproduction difficult
- Study focuses exclusively on Chinese-language automobile domain tasks, limiting generalizability to other domains or languages
- Correlation-based evaluation method may not capture all aspects of model quality that human evaluators would consider
- Performance claims comparing to inter-human correlation require careful interpretation given unknown inter-annotator agreement rates

## Confidence

**High Confidence**: Criteria division approach well-supported by experimental results across multiple tasks showing consistent improvements

**Medium Confidence**: In-context learning replacing fine-tuning claim partially supported, but comparison involves different model families which confounds results

**Low Confidence**: "Better than inter-human correlation" assertion requires careful interpretation given unknown inter-annotator agreement rates in evaluation dataset

## Next Checks

1. **Criterion Specificity Test**: Implement TALEC with same evaluation criteria but different domain (e.g., healthcare or finance) to verify generalization beyond automobile-specific tasks

2. **Shot Optimization Analysis**: Systematically vary number and quality of few-shot examples to determine optimal shot-to-context ratio and identify when shot injection degrades performance

3. **Human Baseline Comparison**: Measure inter-annotator agreement rates within evaluation dataset to establish whether TALEC's performance truly exceeds human reliability