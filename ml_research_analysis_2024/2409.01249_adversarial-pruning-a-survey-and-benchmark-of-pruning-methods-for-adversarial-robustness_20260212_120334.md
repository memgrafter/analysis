---
ver: rpa2
title: 'Adversarial Pruning: A Survey and Benchmark of Pruning Methods for Adversarial
  Robustness'
arxiv_id: '2409.01249'
source_url: https://arxiv.org/abs/2409.01249
tags:
- pruning
- adversarial
- methods
- robustness
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys and benchmarks adversarial pruning methods,
  which aim to compress neural networks while maintaining robustness against adversarial
  attacks. The authors propose a taxonomy that categorizes these methods based on
  when they prune (after, before, or during training) and how they prune (structured/unstructured,
  local/global, and the criterion used).
---

# Adversarial Pruning: A Survey and Benchmark of Pruning Methods for Adversarial Robustness

## Quick Facts
- **arXiv ID**: 2409.01249
- **Source URL**: https://arxiv.org/abs/2409.01249
- **Reference count**: 40
- **Primary result**: The paper surveys and benchmarks adversarial pruning methods, finding that layer-wise sparsity optimization consistently outperforms other approaches, and that simple pruning criteria combined with flexible strategies can match complex methods.

## Executive Summary
This paper provides the first comprehensive survey and benchmark of adversarial pruning methods, which aim to compress neural networks while maintaining robustness against adversarial attacks. The authors develop a novel taxonomy categorizing these methods based on when they prune (after, before, or during training) and how they prune (structured/unstructured, local/global, and the criterion used). To fairly compare these methods, they establish a benchmark with standardized experimental settings using reliable adversarial attacks like AutoAttack and HO-FMN. Their re-evaluation reveals that methods optimizing layer-wise sparsity consistently outperform others, and that simpler pruning criteria combined with flexible layer-wise strategies can match or surpass more complex approaches.

## Method Summary
The paper presents a two-dimensional taxonomy classifying adversarial pruning (AP) methods by their pipeline (A: after training, B: before training, D: during training) and specifics (structure: structured/unstructured, locality: local/global, criterion: various importance scores). They develop a benchmark framework with standardized datasets (CIFAR10, SVHN), architectures (ResNet18, VGG16), sparsity rates (90%, 95%, 99% unstructured; 50%, 75%, 90% structured), and evaluation using AutoAttack. The authors re-evaluate multiple AP methods, loading available implementations and running attacks to generate robustness curves using the HO-FMN attack. Their analysis compares methods across taxonomy dimensions to identify performance patterns and validate the framework's utility for future research.

## Key Results
- Methods optimizing layer-wise sparsity consistently outperform uniform sparsity strategies across all evaluated sparsity rates
- Simpler pruning criteria like lowest weight magnitude (LWM) combined with global, layer-wise strategies can match or surpass more complex adversarial pruning approaches
- The benchmark reveals that many existing AP methods overestimate robustness when evaluated with single attacks, highlighting the importance of using reliable attack ensembles like AutoAttack

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial pruning methods can reduce model size while preserving robustness by jointly optimizing sparsity and adversarial defense during training.
- Mechanism: The pruning pipeline is extended with robust objectives such as adversarial training, allowing the method to find a binary mask that minimizes loss on both clean and adversarial examples while satisfying sparsity constraints.
- Core assumption: The model's decision boundary complexity can be maintained despite reduced parameters if pruning is guided by adversarial robustness metrics.
- Evidence anchors:
  - [abstract] "Recent work has proposed neural network pruning techniques to reduce the size of a network while preserving robustness against adversarial examples"
  - [section] "the goal of a defense is to create a robust model against adversarial attacks. The go-to technique, in this case, is represented by Adversarial Training (AT)"
  - [corpus] Weak evidence - corpus papers focus on different aspects of robustness and pruning but don't directly address joint optimization
- Break condition: If adversarial training becomes too computationally expensive or if the sparsity constraint severely compromises the model's ability to maintain a complex decision boundary.

### Mechanism 2
- Claim: Optimizing layer-wise sparsity consistently outperforms uniform sparsity across layers for adversarial pruning.
- Mechanism: Methods that allow different sparsity rates per layer (global pruning with layer-wise optimization) can better preserve robustness by retaining more critical parameters in layers that contribute most to adversarial robustness.
- Core assumption: Different layers contribute unequally to adversarial robustness, and optimal sparsity should reflect this heterogeneity.
- Evidence anchors:
  - [section] "We suppose their constant advantage stems from optimizing the layer-wise sparsity. In fact, both FlyingBird and HARP besides allowing different sparsities within each layer, additionally find an optimal strategy"
  - [section] "Interestingly, through our results in Table 8, we show that LWM has a significant gain from using the G strategy of LAMP"
  - [corpus] Moderate evidence - related work on adaptive sparsity exists but focused on clean accuracy rather than robustness
- Break condition: If layer-wise optimization becomes too complex or if the computational overhead outweighs the robustness benefits.

### Mechanism 3
- Claim: Simpler pruning criteria combined with flexible layer-wise strategies can match or surpass more complex approaches.
- Mechanism: Naive criteria like lowest weight magnitude (LWM) when combined with global pruning and layer-wise sparsity optimization can achieve comparable robustness to methods using complex importance scores.
- Core assumption: The complexity of pruning criteria adds diminishing returns compared to the flexibility gained from global, layer-wise strategies.
- Evidence anchors:
  - [section] "Interestingly, through our results in Table 8, we show that LWM has a significant gain from using the G strategy of LAMP, to the extent that some of the more complex and time-requiring APs retain lower robustness"
  - [section] "It shows that complex AP methods can often be surpassed by simple and low complexity solutions increasing the 'flexibility' of the chosen weights to be pruned"
  - [corpus] Moderate evidence - simpler pruning methods are common in literature but rarely compared against complex adversarial methods
- Break condition: If adversarial examples exploit specific parameter patterns that only complex criteria can identify.

## Foundational Learning

- Concept: Adversarial training and its formulation as a min-max optimization problem
  - Why needed here: Understanding adversarial training is crucial since most adversarial pruning methods build upon it to maintain robustness during compression
  - Quick check question: What is the difference between the inner and outer optimization problems in adversarial training?

- Concept: Pruning structure (structured vs unstructured) and its impact on model efficiency
  - Why needed here: The taxonomy distinguishes methods based on whether they remove single weights or entire structures, which affects both implementation complexity and robustness preservation
  - Quick check question: Why might unstructured pruning allow higher performance than structured pruning at the same sparsity level?

- Concept: Locality in pruning (local vs global) and its relationship to layer-wise optimization
  - Why needed here: The locality dimension determines whether sparsity is applied uniformly across layers or globally, which is key to understanding why some methods outperform others
  - Quick check question: How does global pruning with layer-wise optimization differ from simple local pruning in terms of parameter retention?

## Architecture Onboarding

- Component map:
  - Taxonomy classification system (pipeline and specifics dimensions)
  - Benchmark evaluation framework (datasets, models, sparsity rates, attacks)
  - Re-evaluation pipeline (loading checkpoints, running attacks, generating curves)
  - Result analysis dashboard (comparing methods across taxonomy dimensions)

- Critical path:
  1. Survey and classify AP methods using the two-dimensional taxonomy
  2. Implement standardized benchmark with AutoAttack and HO-FMN
  3. Load available implementations and run re-evaluation
  4. Analyze results to identify patterns across taxonomy dimensions
  5. Publish findings and maintain benchmark for community contributions

- Design tradeoffs:
  - Structured vs unstructured pruning: Structured is more efficient but unstructured allows better performance preservation
  - Local vs global pruning: Local is simpler but global with layer-wise optimization provides better robustness
  - Complex vs simple criteria: Complex criteria may provide marginal benefits over simple ones when combined with global strategies

- Failure signatures:
  - Overestimated robustness due to inadequate adversarial evaluations
  - Poor performance when high sparsity rates compromise decision boundary complexity
  - Implementation issues with specific datasets or architectures

- First 3 experiments:
  1. Replicate benchmark evaluation on a small subset of AP methods to verify setup correctness
  2. Compare LWM with complex criteria (LIS) under identical global pruning conditions
  3. Test layer-wise optimization on a single method across different sparsity configurations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does optimizing layer-wise sparsity consistently outperform uniform sparsity across all sparsity rates and architectures for adversarial pruning?
- Basis in paper: [explicit] The paper states that methods optimizing layer-wise sparsity (e.g., FlyingBird, HARP) consistently outperform others across sparsity rates on CIFAR10, and that flexibility in sparsity allocation enhances robustness.
- Why unresolved: While the paper demonstrates consistent performance gains on CIFAR10 for unstructured pruning, the analysis is limited to two architectures (ResNet18, VGG16) and does not explore deeper networks, larger-scale datasets (e.g., ImageNet), or structured pruning comprehensively across all sparsity levels.
- What evidence would resolve it: Systematic experiments across diverse architectures (e.g., ResNet50, WideResNet), multiple datasets (e.g., ImageNet, SVHN), and both structured/unstructured pruning at all sparsity rates, comparing uniform vs optimized layer-wise strategies.

### Open Question 2
- Question: Can simple pruning criteria like LWM combined with flexible layer-wise strategies match or surpass complex adversarial pruning methods in terms of adversarial robustness?
- Basis in paper: [explicit] The paper shows that combining LWM with Layer-wise Adaptive Magnitude Pruning (LAMP) can match or surpass more complex methods, suggesting that flexibility in weight selection may be more important than criterion complexity.
- Why unresolved: The comparison is limited to a specific combination (LWM + LAMP) and focuses on CIFAR10 with unstructured pruning. It does not test other simple criteria or combinations, nor does it evaluate across different datasets, architectures, or structured pruning.
- What evidence would resolve it: Broad empirical studies comparing simple criteria (e.g., LWM, filter L1 norm) with and without flexible layer-wise strategies against complex adversarial criteria across multiple datasets, architectures, and sparsity rates.

### Open Question 3
- Question: How does adversarial pruning affect the decision boundary complexity of neural networks compared to standard pruning, and what are the implications for robustness?
- Basis in paper: [explicit] The paper illustrates that pruning robust models is more challenging due to the need to retain a complex decision boundary, and shows that robustness decreases more gracefully when increasing perturbation budgets.
- Why unresolved: The paper provides qualitative insights into boundary complexity but lacks quantitative analysis of how pruning affects boundary geometry (e.g., curvature, margin). The relationship between pruning-induced boundary simplification and robustness degradation is not fully characterized.
- What evidence would resolve it: Quantitative analysis of decision boundary properties (e.g., curvature, margin) before and after pruning, correlated with robustness metrics, and experiments isolating the effect of boundary complexity from other factors.

## Limitations

- The benchmark primarily focuses on CIFAR10 and SVHN datasets with ResNet18/VGG16 architectures, limiting generalizability to larger-scale vision tasks
- Some adversarial pruning methods may have been evaluated under different threat models in their original papers, making direct comparisons challenging
- The study does not explore the impact of adversarial pruning on decision boundary geometry or provide quantitative analysis of boundary complexity changes

## Confidence

The paper's confidence in its findings is **High** for the general taxonomy framework and benchmark setup, as these provide a systematic way to classify and compare adversarial pruning methods. However, confidence is **Medium** for specific claims about method superiority due to potential implementation differences and hyperparameter sensitivity across evaluated approaches. The finding that layer-wise optimization consistently outperforms other strategies is well-supported, but the claim that simple criteria can match complex ones when combined with global strategies needs further validation across more diverse architectures and datasets.

## Next Checks

1. Test the benchmark on additional datasets (ImageNet, larger-scale vision datasets) and architectures (Vision Transformers, EfficientNets) to verify generalizability of findings
2. Conduct ablation studies on the specific components of layer-wise optimization (e.g., testing global pruning with naive uniform sparsity vs. global with adaptive layer-wise rates)
3. Perform sensitivity analysis on hyperparameters across methods to determine if observed performance differences persist under consistent optimization settings