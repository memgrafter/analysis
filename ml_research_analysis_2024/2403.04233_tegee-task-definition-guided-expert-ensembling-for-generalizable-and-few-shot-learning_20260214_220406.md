---
ver: rpa2
title: 'TEGEE: Task dEfinition Guided Expert Ensembling for Generalizable and Few-shot
  Learning'
arxiv_id: '2403.04233'
source_url: https://arxiv.org/abs/2403.04233
tags:
- task
- definition
- expert
- learning
- deep-icl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces TEGEE (Task Definition Guided Expert Ensembling),
  a method that explicitly extracts task definitions and generates responses based
  on specific tasks using a dual 3B model approach. One model focuses on task definition
  extraction while the other handles learning from demonstrations.
---

# TEGEE: Task dEfinition Guided Expert Ensembling for Generalizable and Few-shot Learning

## Quick Facts
- arXiv ID: 2403.04233
- Source URL: https://arxiv.org/abs/2403.04233
- Authors: Xingwei Qu; Yiming Liang; Yucheng Wang; Tianyu Zheng; Tommy Yue; Xingyuan Bu; Lei Ma; Stephen W. Huang; Jiajun Zhang; Yinan Shi; Chenghua Lin; Jie Fu; Ge Zhang
- Reference count: 40
- Primary result: Dual 3B model approach with task definition extraction performs comparably to LLaMA2-13B and significantly outperforms LLaMA2-7B and Falcon7B across 117 tasks

## Executive Summary
TEGEE introduces a novel approach to few-shot learning by explicitly extracting task definitions before learning. The method employs a dual 3B model architecture where one model extracts task definitions from demonstrations while the other handles learning from those definitions. This separation allows the system to first understand what needs to be done before attempting to do it, overcoming limitations of traditional in-context learning such as fixed input lengths and limited learning capabilities. The framework supports unlimited demonstrations and enhances continual learning through an expert pool that grows dynamically.

## Method Summary
TEGEE uses a dual 3B model architecture where one model (T5-3B) extracts task definitions from demonstrations, and the other learns from these definitions using LoRA adapters. The system builds a dynamic expert pool where each task has its own LoRA adapter with an associated task definition. During inference, the Task Definition Guided Retriever computes similarity between new task definitions and stored ones using SentenceBERT embeddings, retrieving the top 3 most relevant experts. These experts are combined through weighted averaging to initialize the final model for inference. The framework supports continual learning by adding new task definitions and LoRA adapters to the expert pool when they are sufficiently distinct from existing entries.

## Key Results
- TEGEE performs comparably to LLaMA2-13B model across 117 tasks from Super Natrual Instruction (SuperNI) test dataset
- TEGEE significantly outperforms baseline models like LLaMA2-7B and Falcon7B
- Ablation studies show 44.71% performance drop when task definition extraction is removed
- Performance improves with more demonstrations, demonstrating effective few-shot learning capabilities
- The framework supports unlimited demonstrations and enhances continual learning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task definition extraction is more vital than task processing for effective few-shot learning
- Mechanism: The dual-model approach separates task understanding from task execution, allowing the system to first extract clear definitions before attempting learning
- Core assumption: Task definitions can be reliably extracted from demonstrations and serve as effective guides for learning
- Evidence anchors:
  - [abstract] "Empirical evaluations show that TEGEE performs comparably to the larger LLaMA2-13B model"
  - [section 2.1.1] "Pan et al. (2023) with characterizing two ways through task recognition (TR) and task learning (TL)"
  - [corpus] Weak evidence - no direct papers on task definition extraction superiority found
- Break condition: If task definitions cannot be reliably extracted from demonstrations

### Mechanism 2
- Claim: Expert ensembling with LoRA adapters enables efficient few-shot learning with unlimited demonstrations
- Mechanism: Dynamic expert pool with task-specific LoRA adapters, where relevant experts are retrieved and combined through weighted averaging
- Core assumption: LoRA adapters can effectively capture task-specific knowledge and their weighted combination produces better results
- Evidence anchors:
  - [abstract] "Our framework employs a dual 3B model approach"
  - [section 3.4] "Model Soup (Wortsman et al., 2022) is an ensemble technique that enhances performance by merging multiple models"
  - [corpus] No direct evidence found for this specific ensemble mechanism
- Break condition: If retrieved experts are too dissimilar or LoRA adapters don't capture task knowledge effectively

### Mechanism 3
- Claim: Continual learning through expert pool expansion improves performance over time
- Mechanism: System adds new task definitions and LoRA adapters to the expert pool when sufficiently distinct from existing entries
- Core assumption: New task definitions can be meaningfully compared to existing ones and added when distinct enough
- Evidence anchors:
  - [section 3.5] "To ensure diversity and uniqueness in expert pool E, new task definitions Tconcluded are compared for similarity with existing entries T in E"
  - [abstract] "The framework supports unlimited demonstrations and enhances continual learning capabilities"
  - [corpus] No direct evidence found for continual learning through expert pools
- Break condition: If similarity threshold is too loose (redundancy) or too strict (missed learning)

## Foundational Learning

- Concept: Task Recognition vs Task Learning distinction
  - Why needed here: Understanding that extracting task definitions (TR) is different from learning task-specific patterns (TL) is crucial for appreciating why the dual-model approach works
  - Quick check question: Can you explain the difference between understanding what a task requires and actually learning to perform it?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: The system relies on LoRA adapters for efficient task-specific adaptation without full model retraining
  - Quick check question: How does LoRA differ from traditional fine-tuning in terms of parameter efficiency?

- Concept: Sentence embedding similarity
  - Why needed here: Task definition retrieval depends on computing cosine similarity between task definition embeddings
  - Quick check question: What metric does the system use to determine which experts are most relevant to a given task?

## Architecture Onboarding

- Component map: Task Definition Extractor (3B model) -> Task Definition Guided Retriever -> Expert Ensembler -> Final Model Inference
- Critical path: Task Definition Extractor → Task Definition Guided Retriever → Expert Ensembler → Final Model Inference
- Design tradeoffs:
  - Using 3B models instead of larger ones for efficiency vs potential performance loss
  - Dynamic expert pool growth vs storage/computation costs
  - Similarity threshold tuning for pool expansion vs redundancy
- Failure signatures:
  - Low similarity scores between retrieved experts and task definitions (retrieval failure)
  - Performance degradation as pool grows without proper pruning (expert redundancy)
  - Inconsistent task definition extraction leading to wrong expert selection
- First 3 experiments:
  1. Verify task definition extraction works by testing the extractor on known demonstrations and checking if extracted definitions match human annotations
  2. Test expert retrieval by creating a small expert pool and verifying that similar task definitions retrieve the correct experts
  3. Validate ensembling by comparing single expert performance vs weighted combination performance on a known task

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important questions emerge from the methodology:

### Open Question 1
- Question: What is the optimal number of task definitions to retrieve from the expert pool for achieving the best performance in TEGEE?
- Basis in paper: [explicit] The paper states that the top 3 experts are retrieved based on similarity to the task definition, but does not explore the impact of retrieving a different number of experts
- Why unresolved: The paper only uses the top 3 experts for retrieval and does not investigate how the number of retrieved experts affects the performance
- What evidence would resolve it: Conducting experiments with varying numbers of retrieved experts (e.g., top 1, top 5, top 10) and comparing the performance of TEGEE with each setting

### Open Question 2
- Question: How does the quality of task definitions generated by the task definition extractor impact the overall performance of TEGEE?
- Basis in paper: [explicit] The paper mentions that the task definition extractor is trained using the Galactica 3B model and Alpaca-GPT-4 data, but does not provide a detailed analysis of the quality of the generated task definitions
- Why unresolved: The paper does not assess the quality of the task definitions generated by the extractor or investigate how variations in quality affect the performance of TEGEE
- What evidence would resolve it: Conducting a comprehensive evaluation of the task definition extractor, including human assessment of the generated definitions and correlation analysis between definition quality and TEGEE performance

### Open Question 3
- Question: Can TEGEE be effectively applied to tasks outside the SuperNI dataset, and how does its performance generalize to other domains?
- Basis in paper: [explicit] The paper evaluates TEGEE on the SuperNI dataset, but does not explore its performance on tasks from other domains or datasets
- Why unresolved: The paper focuses solely on the SuperNI dataset and does not investigate whether TEGEE can generalize to tasks beyond this specific domain
- What evidence would resolve it: Evaluating TEGEE on a diverse set of tasks from various domains, such as different NLP benchmarks or real-world applications, and comparing its performance to other state-of-the-art methods

### Open Question 4
- Question: How does the performance of TEGEE scale with the size of the expert pool, and is there an optimal size for the expert pool?
- Basis in paper: [explicit] The paper mentions that the expert pool is dynamically augmented with input demonstrations, but does not investigate the impact of the expert pool size on performance
- Why unresolved: The paper does not explore how the size of the expert pool affects the performance of TEGEE or whether there is an optimal size for the pool
- What evidence would resolve it: Conducting experiments with expert pools of varying sizes and measuring the performance of TEGEE with each size

### Open Question 5
- Question: How does TEGEE compare to other few-shot learning methods in terms of sample efficiency and performance?
- Basis in paper: [explicit] The paper compares TEGEE to LLaMA2-13B and other baseline models, but does not provide a comprehensive comparison with other few-shot learning methods
- Why unresolved: The paper focuses on comparing TEGEE to specific baseline models but does not evaluate its performance against a broader range of few-shot learning methods
- What evidence would resolve it: Conducting a thorough comparison of TEGEE with other prominent few-shot learning methods, such as meta-learning approaches, metric-based methods, and optimization-based methods, on a common set of benchmarks

## Limitations
- The superiority claim of task definition extraction over task processing lacks direct empirical validation against alternatives
- The 44.71% ablation performance drop is reported but the experimental setup for this comparison is not detailed
- The continual learning mechanism relies on similarity thresholds that are not empirically justified
- The evaluation methodology doesn't clearly establish whether the comparison to LLaMA2-13B is fair given different model sizes

## Confidence
- **High confidence**: The dual-model architecture design and expert ensembling mechanism are well-described and technically sound
- **Medium confidence**: The performance claims against baselines are supported by results but lack detailed statistical analysis
- **Low confidence**: The superiority claim of task definition extraction over task processing lacks direct comparative evidence

## Next Checks
1. Conduct a controlled experiment comparing task definition extraction vs direct task processing on the same model architecture to validate the 44.71% performance difference claim
2. Test the continual learning mechanism with varying similarity thresholds to determine optimal settings and verify the diversity claims
3. Perform ablation studies on the expert ensembling weights to validate whether weighted averaging truly outperforms simple averaging or top-k selection