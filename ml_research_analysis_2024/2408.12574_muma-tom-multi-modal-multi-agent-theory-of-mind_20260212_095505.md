---
ver: rpa2
title: 'MuMA-ToM: Multi-modal Multi-Agent Theory of Mind'
arxiv_id: '2408.12574'
source_url: https://arxiv.org/abs/2408.12574
tags:
- goal
- social
- agent
- belief
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MuMA-ToM introduces the first multi-modal multi-agent Theory of
  Mind benchmark, featuring 225 interactive scenarios with 900 questions that evaluate
  belief inference, social goal inference, and belief of others' goals in realistic
  household settings. The benchmark combines video and text inputs, validated by human
  experiments showing near-perfect accuracy (93.5%) versus state-of-the-art models
  that struggle, with the best achieving only 56.4% accuracy.
---

# MuMA-ToM: Multi-modal Multi-Agent Theory of Mind

## Quick Facts
- arXiv ID: 2408.12574
- Source URL: https://arxiv.org/abs/2408.12574
- Authors: Haojun Shi; Suyu Ye; Xinyu Fang; Chuanyang Jin; Leyla Isik; Yen-Ling Kuo; Tianmin Shu
- Reference count: 40
- Primary result: Introduces first multi-modal multi-agent ToM benchmark; proposes LIMP achieving 76.6% accuracy vs human 93.5% and best baseline 56.4%

## Executive Summary
MuMA-ToM introduces the first comprehensive benchmark for evaluating multi-agent Theory of Mind capabilities in realistic household settings. The benchmark features 225 interactive scenarios with 900 questions that assess belief inference, social goal inference, and belief of others' goals using both video and text inputs. While humans achieve near-perfect performance (93.5%), state-of-the-art models struggle significantly. To address this gap, the authors propose LIMP (Language model-based Inverse Multi-agent Planning), which leverages language models for inverse planning to infer mental states. LIMP achieves 76.6% accuracy, significantly outperforming baselines by using natural language representations and multi-agent planning without requiring hand-crafted symbols.

## Method Summary
LIMP addresses the multi-agent ToM challenge by using inverse planning with language models rather than direct mental state inference. The method takes multi-modal inputs (video and text) and processes them through a Visual Language Model (VLM) for action/utterance extraction, followed by a Large Language Model (LLM) for hypothesis generation and inverse planning. LIMP represents all information using natural language, eliminating the need for domain-specific finetuning. The inverse planning approach evaluates the likelihood of observed behaviors given hypothetical mental states, leveraging the LLM's strength in forward generation rather than direct ToM reasoning.

## Key Results
- Human performance on MuMA-ToM: 93.5% accuracy
- Best baseline model performance: 56.4% accuracy
- LIMP performance: 76.6% accuracy, significantly outperforming all baselines
- LIMP handles adversarial behaviors and noisy visual perception better than existing methods
- Natural language representations eliminate need for hand-crafted symbols while maintaining strong performance

## Why This Works (Mechanism)

### Mechanism 1: Multi-modal Information Fusion
- Claim: Multi-modal inputs with complementary information improve mental state inference
- Mechanism: LIMP fuses video and text inputs to fill missing visual information with contextual text clues
- Core assumption: Visual perception failures can be compensated by textual context
- Evidence anchors: Abstract states LIMP "significantly outperforms baselines... by leveraging multi-agent planning and natural language representations"; section notes VLMs struggle with object inference that humans handle with context

### Mechanism 2: Inverse Multi-Agent Planning
- Claim: Inverse multi-agent planning with language models outperforms direct ToM reasoning
- Mechanism: LIMP uses GPT-4o to estimate action/utterance likelihood given mental state hypotheses
- Core assumption: LLMs excel at forward generation of behavior given mental states
- Evidence anchors: Abstract mentions leveraging "multi-agent planning"; section states "While LLMs struggle with direct ToM reasoning, they excel at the forward generation of multi-modal behavior given mental states"

### Mechanism 3: Natural Language Representations
- Claim: Natural language representations generalize better than hand-crafted symbols
- Mechanism: LIMP represents all information using natural language, eliminating domain-specific finetuning
- Core assumption: Pretrained language models can handle natural language representations
- Evidence anchors: Abstract mentions "eliminating the need for hand-crafted symbols"; section states "LIMP uses natural language to represent states, actions, and utterances, eliminating the need for finetuning"

## Foundational Learning

- Concept: Multi-modal information fusion
  - Why needed here: To combine complementary information from video and text inputs for more accurate mental state inference
  - Quick check question: How would you handle a scenario where video shows an agent opening a cabinet but text doesn't mention the object inside?

- Concept: Inverse planning in multi-agent settings
  - Why needed here: To infer agents' mental states by evaluating the likelihood of their actions/utterances given hypothetical mental states
  - Quick check question: What's the difference between forward planning and inverse planning in the context of Theory of Mind?

- Concept: Interactive Partially Observable Markov Decision Processes (I-POMDP)
  - Why needed here: To model the recursive mental reasoning between multiple agents with partial observability
  - Quick check question: How does I-POMDP extend POMDP to handle multiple agents with interdependent mental states?

## Architecture Onboarding

- Component map: VLM (Gemini 1.5 Pro) -> Text Parsing -> Multi-modal Fusion -> Hypothesis Parsing -> Inverse Planning (GPT-4o) -> Answer Selection
- Critical path: VLM → Text Parsing → Multi-modal Fusion → Hypothesis Parsing → Inverse Planning → Answer Selection
- Design tradeoffs:
  - Natural language vs symbolic representations: Generalizability vs efficiency
  - Complete vs partial observability: Accuracy vs computational complexity
  - Direct vs inverse reasoning: Human-like reasoning vs computational tractability
- Failure signatures:
  - VLM hallucination errors → Incorrect action recognition → Wrong initial state
  - LLM generation failures → Invalid forward plans → Poor likelihood estimates
  - Hypothesis parsing errors → Wrong mental state assumptions → Incorrect inferences
- First 3 experiments:
  1. Test LIMP on a simplified version of MuMA-ToM with ground truth actions to isolate inverse planning performance
  2. Compare LIMP with and without multi-modal fusion to quantify the benefit of complementary information
  3. Evaluate LIMP with different LLMs (e.g., Llama 3.1 8B) to test scalability and generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LIMP generalize to real-world household environments beyond the synthetic VirtualHome scenarios?
- Basis in paper: The paper states "What are the limitations of our benchmark? The scenarios in our benchmark are currently limited to interactions between two agents in household settings..." and "In future work, we intend to incorporate more complex real-world scenarios beyond household environments..."
- Why unresolved: The current benchmark uses procedurally generated synthetic environments, and there is no evaluation on real-world video data.
- What evidence would resolve it: Testing LIMP on real-world household video datasets and comparing its performance against current baselines would demonstrate its generalizability.

### Open Question 2
- Question: How does LIMP perform with more than two agents and more complex social goals beyond helping, hindering, and acting independently?
- Basis in paper: The paper states "What are the limitations of LIMP? Hallucinations created by the VLM can cause significant errors in LIMP..." and "What are the limitations of our benchmark? The scenarios in our benchmark are currently limited to interactions between two agents in household settings..."
- Why unresolved: The current implementation only handles two-agent scenarios with three social goals.
- What evidence would resolve it: Evaluating LIMP on multi-agent scenarios with more complex social dynamics would demonstrate its scalability.

### Open Question 3
- Question: What is the computational efficiency of LIMP compared to direct LMM approaches, especially for longer interactions?
- Basis in paper: The paper mentions "LIMP also does not perform recursive reasoning for more than two levels" and "additionally, LIMP does not explicitly infer an agent's belief of another agent's belief... which can become costly for longer events."
- Why unresolved: While LIMP outperforms LMMs in accuracy, the paper doesn't provide runtime comparisons or analysis of how performance scales with interaction length.
- What evidence would resolve it: Benchmarking LIMP's inference time against direct LMM approaches across interactions of varying lengths would quantify the computational trade-off.

## Limitations

- LIMP's performance (76.6%) still falls significantly short of human-level performance (93.5%), indicating fundamental gaps in machine social reasoning
- The benchmark is limited to household settings with predefined interaction patterns, which may not generalize to more diverse social situations
- LIMP relies on language models that may hallucinate or generate invalid plans, creating potential failure modes

## Confidence

**High Confidence Claims:**
- The MuMA-ToM benchmark represents the first comprehensive multi-modal multi-agent Theory of Mind evaluation
- LIMP outperforms existing baselines significantly (76.6% vs 56.4% for best baseline)
- Multi-modal fusion provides complementary information that improves inference accuracy

**Medium Confidence Claims:**
- Natural language representations eliminate the need for hand-crafted symbols without sacrificing performance
- Inverse planning with language models is superior to direct Theory of Mind reasoning for this task
- The benchmark effectively differentiates between human and machine capabilities in social reasoning

## Next Checks

1. **Cross-domain Generalization Test**: Evaluate LIMP on scenarios from different domains (e.g., workplace interactions, public spaces) to assess whether performance degrades when moving beyond household settings.

2. **Ablation Study on Multi-modal Fusion**: Systematically remove either visual or textual components and measure performance impact to quantify the exact contribution of each modality and identify potential redundancy.

3. **Adversarial Scenario Analysis**: Design scenarios specifically targeting LIMP's weaknesses identified in the paper (e.g., adversarial behaviors, noisy perception) and measure whether performance improvements are sustainable or brittle under stress conditions.