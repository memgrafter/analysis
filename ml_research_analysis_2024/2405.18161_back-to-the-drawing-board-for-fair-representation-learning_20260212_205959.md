---
ver: rpa2
title: Back to the Drawing Board for Fair Representation Learning
arxiv_id: '2405.18161'
source_url: https://arxiv.org/abs/2405.18161
tags:
- tasks
- task
- representations
- transfer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper highlights a critical flaw in current Fair Representation\
  \ Learning (FRL) evaluation: most methods are tested only on a single proxy task\
  \ used during training, which can lead to representations that collapse to trivial\
  \ forms and fail to transfer to unseen tasks. To address this, the authors propose\
  \ refocusing FRL evaluation on transfer tasks and introduce the TransFair benchmark\u2014\
  an extension of two popular fairness datasets (ACS and Heritage Health) with four\
  \ carefully calibrated transfer tasks each."
---

# Back to the Drawing Board for Fair Representation Learning

## Quick Facts
- arXiv ID: 2405.18161
- Source URL: https://arxiv.org/abs/2405.18161
- Reference count: 40
- Primary result: Current Fair Representation Learning methods overfit to single proxy tasks, necessitating evaluation on diverse transfer tasks

## Executive Summary
This paper identifies a fundamental flaw in Fair Representation Learning (FRL) evaluation: most methods are tested only on a single proxy task used during training, which incentivizes models to learn task-specific representations that fail to generalize to unseen tasks. The authors demonstrate that this evaluation protocol leads to representations that can collapse to trivial forms or even amplify bias compared to unfair baselines. To address this, they propose refocusing FRL evaluation on transfer tasks and introduce the TransFair benchmark—an extension of two popular fairness datasets with four carefully calibrated transfer tasks each that satisfy criteria for task diversity, appropriate difficulty, and fairness sensitivity.

## Method Summary
The paper introduces the TransFair benchmark, which extends two existing fairness datasets (ACS and Heritage Health) with four transfer tasks each, designed according to four criteria: task diversity, correlation variation with proxy task, appropriate difficulty levels, and fairness sensitivity. The benchmark evaluates FRL methods (FARE, sIPM-LFR, CVIB, FCRL, FNF) by training encoders with proxy task objectives plus task-agnostic loss components, then testing learned representations on transfer tasks using single-layer neural network classifiers. The evaluation measures both accuracy and demographic parity distance across tasks, with results visualized on fairness-accuracy Pareto fronts.

## Key Results
- State-of-the-art FRL methods show significant overfitting to proxy tasks, underperforming on weakly correlated transfer tasks
- Some FRL methods produce representations less fair than the unfair baseline on certain tasks
- Methods incorporating task-agnostic objectives (like reconstruction losses) yield more transferable representations
- The TransFair benchmark successfully reveals the true generalizability limitations of current FRL approaches

## Why This Works (Mechanism)

### Mechanism 1
Evaluating FRL methods on a single proxy task leads to representations that overfit to that task and fail to generalize across other tasks. The evaluation protocol only measures performance on the proxy task used during training, incentivizing the model to retain only task-specific information and discard broader task-agnostic features. This occurs because the original motivation of FRL is to provide transferable representations useful for multiple downstream tasks, not just one known task.

### Mechanism 2
Task-agnostic loss components (like reconstruction losses) improve transferability of learned representations across diverse downstream tasks. Task-agnostic losses encourage the model to retain general information about the input data, rather than only information useful for the specific proxy task, leading to richer representations. This works because reconstruction-based objectives capture task-agnostic information that benefits performance on unseen tasks.

### Mechanism 3
The TransFair benchmark provides a principled evaluation framework by including tasks with varying correlations to the proxy task, revealing the true generalizability of FRL methods. By evaluating representations on tasks with different correlation levels to the proxy task, the benchmark exposes overfitting and identifies methods that maintain performance across diverse tasks. This creates a realistic evaluation scenario for real-world FRL applications.

## Foundational Learning

- Concept: Fairness metrics (Demographic Parity, Equal Opportunity, Equalized Odds)
  - Why needed here: FRL methods need to be evaluated on their ability to reduce discrimination across different fairness metrics
  - Quick check question: What is the key difference between demographic parity and equalized odds in terms of what they measure?

- Concept: Transfer learning and generalization
  - Why needed here: The paper's core argument is that FRL methods should be evaluated on their ability to generalize to unseen tasks
  - Quick check question: Why is it problematic if an FRL method performs well on the proxy task but poorly on transfer tasks?

- Concept: Correlation metrics (Simple Matching Coefficient)
  - Why needed here: The TransFair benchmark uses correlation metrics to calibrate tasks with different relationships to the proxy task
  - Quick check question: What does an SMC of 50% indicate about the relationship between two tasks?

## Architecture Onboarding

- Component map: Data preprocessing -> Encoder training (with proxy task and task-agnostic losses) -> Representations -> Classifier training -> Evaluation across all tasks
- Critical path: Data → Encoder training (with losses) → Representations → Classifier training → Evaluation across all tasks
- Design tradeoffs:
  - Task-specific vs. task-agnostic losses: Balancing proxy task performance with transferability
  - Dataset selection: Choosing datasets with appropriate task diversity and difficulty
  - Evaluation protocol: Single vs. multiple tasks for comparison
- Failure signatures:
  - High performance on proxy task but poor performance on transfer tasks indicates overfitting
  - Representations worse than unfair baseline suggest harmful bias amplification
  - Inconsistent performance across transfer tasks suggests poor generalization
- First 3 experiments:
  1. Run FARE with reconstruction loss on ACS-Transfer and compare to standard FARE across all transfer tasks
  2. Evaluate sIPM-LFR with λr = 0 vs λr > 0 on Heritage-Health-Transfer to measure impact of task-agnostic loss
  3. Compare FRL methods on highly correlated vs. uncorrelated transfer tasks to quantify overfitting

## Open Questions the Paper Calls Out

### Open Question 1
How do FRL methods perform when evaluated on more complex downstream models beyond single-layer neural networks? The authors note that their evaluation of representations on downstream tasks is currently limited to single-layer neural networks, suggesting potential interactions between FRL methods and downstream model architectures remain unexplored.

### Open Question 2
What is the optimal balance between task-specific and task-agnostic loss components in FRL methods? The authors observe that methods with task-agnostic objectives yield more transferable representations, but also note that a combination of both task-specific and task-agnostic losses might be beneficial.

### Open Question 3
How generalizable are TransFair results to other types of data beyond tabular datasets? The authors acknowledge that their work focuses on a limited set of two tabular datasets, which may not fully represent the diversity of tasks FRL could be applied to.

## Limitations
- The benchmark only covers tabular data with specific characteristics, limiting conclusions about FRL performance on other data modalities
- The paper uses single-layer neural network classifiers, which may not fully capture representation quality
- The four selection criteria for benchmark construction may not capture all relevant aspects of transfer learning

## Confidence
- Central claim about overfitting to proxy tasks: High confidence
- Proposed TransFair benchmark: Medium confidence
- Mechanism explaining task-agnostic losses improve transferability: Low confidence

## Next Checks
1. Test whether the observed transfer performance degradation persists when using deeper neural network classifiers versus single-layer models, to validate the robustness of the overfitting findings.

2. Evaluate the same FRL methods on additional fairness datasets beyond TransFair to determine if the observed patterns generalize across different domains and task types.

3. Conduct ablation studies removing task-agnostic components from methods like FARE to quantify the specific contribution of reconstruction losses to transferability versus other architectural factors.