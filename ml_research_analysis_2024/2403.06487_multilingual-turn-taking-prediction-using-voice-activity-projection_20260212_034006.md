---
ver: rpa2
title: Multilingual Turn-taking Prediction Using Voice Activity Projection
arxiv_id: '2403.06487'
source_url: https://arxiv.org/abs/2403.06487
tags:
- turn-taking
- language
- multilingual
- english
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A multilingual voice activity projection (VAP) model for turn-taking
  prediction is proposed and evaluated on English, Mandarin, and Japanese dialogue
  datasets. The model uses cross-attention Transformers to predict future voice activities
  of two speakers in dyadic conversations.
---

# Multilingual Turn-taking Prediction Using Voice Activity Projection

## Quick Facts
- arXiv ID: 2403.06487
- Source URL: https://arxiv.org/abs/2403.06487
- Reference count: 18
- Primary result: Multilingual model trained on English, Mandarin, and Japanese outperforms monolingual models on cross-lingual transfer for turn-taking prediction

## Executive Summary
This paper introduces a multilingual voice activity projection (VAP) model for predicting turn-taking in dyadic conversations across English, Mandarin, and Japanese. The model uses cross-attention Transformers to capture the dynamic interplay between two speakers' audio streams, predicting future voice activity patterns. Key findings show that monolingual models trained on one language perform poorly when applied to others, while a multilingual model trained on all three languages achieves comparable performance to monolingual models across all languages. The model also learns to identify the input language implicitly. Sensitivity analysis reveals that pitch is more important for turn-taking prediction in Mandarin and Japanese than in English.

## Method Summary
The VAP model processes stereo audio through a frozen Contrastive Predictive Coding (CPC) encoder, applies self-attention Transformers to each speaker's audio separately, then uses cross-attention to fuse speaker representations. It predicts future voice activity patterns using a 256-dimensional output (all combinations of speaker voice activity over time bins) and includes a multitask learning component with voice activity detection (VAD). The model is trained on three languages - English (Switchboard), Mandarin (HKUST), and Japanese (Travel Agency Task Dialogues) - with evaluations comparing monolingual versus multilingual performance and different audio encoders.

## Key Results
- Monolingual models trained on one language perform poorly when applied to other languages
- Multilingual model trained on all three languages achieves comparable performance to monolingual models across all languages
- Model using CPC pre-trained on English performs slightly better than one using multilingual wav2vec 2.0 (MMS)

## Why This Works (Mechanism)

### Mechanism 1
Cross-attention Transformers enable dynamic modeling of speaker interactions by attending separately to each speaker's audio and then fusing their representations. The model first processes each speaker's audio through a self-attention Transformer, then cross-attends by using one speaker's output as query and the other's as key/value. This allows the model to capture turn-taking signals that emerge from the interplay between speakers, not just their individual acoustic patterns.

### Mechanism 2
Pre-training on English CPC works cross-lingually because the task-relevant temporal patterns in turn-taking are similar enough across languages. The CPC encoder compresses raw audio into a high-level latent representation trained on English, which captures temporal dynamics and speech patterns. This compressed representation is then used as input to the VAP model, which learns to predict voice activity across all three languages.

### Mechanism 3
Adding a language identification task as auxiliary loss does not degrade performance because the multilingual model implicitly learns to distinguish languages. During training, a linear layer predicts the language of the input audio, and its loss is added to the main VAP and VAD losses. This encourages the model to develop language-sensitive representations, which may help disambiguate language-specific turn-taking cues.

## Foundational Learning

- **Voice Activity Projection (VAP)**: Multi-class classification over discretized future time bins
  - Why needed here: The core output is a probability distribution over 256 possible joint voice activity patterns, representing all combinations of who speaks when in the next two seconds
  - Quick check question: How many binary bins are used to discretize the two-second future window, and why is this discretization necessary?

- **Cross-attention mechanism in Transformers**: Allows the model to directly model the interaction between two speakers' audio streams
  - Why needed here: It captures turn-taking cues that emerge from the interplay between speakers
  - Quick check question: In the cross-attention layer, which speaker's output is used as query, and which are used as key and value?

- **Contrastive Predictive Coding (CPC)**: Unsupervised pre-training that learns to predict future latent representations from past ones
  - Why needed here: CPC learns temporal dynamics suitable for modeling speech and turn-taking
  - Quick check question: What is the main difference between CPC and standard autoencoders in terms of the prediction target?

## Architecture Onboarding

- **Component map**: Raw stereo audio → CPC encoder (frozen) → self-attention Transformer per channel → cross-attention Transformer → VAP linear layer (256-dim output) + VAD linear layer (2-dim output) → loss aggregation
- **Critical path**: Raw stereo audio → CPC encoding → self-attention → cross-attention → VAP prediction (most important for turn-taking)
- **Design tradeoffs**: Frozen CPC encoder limits adaptability but speeds up training; cross-attention adds interaction modeling but increases compute; multi-task VAD+VAP stabilizes training but adds complexity
- **Failure signatures**: High VAD loss but low VAP loss may indicate speaker segmentation issues; poor cross-lingual performance may indicate encoder or interaction modeling issues
- **First 3 experiments**:
  1. Train monolingual VAP models on each language and evaluate cross-lingual transfer
  2. Add language identification auxiliary loss and compare performance
  3. Compare CPC vs MMS audio encoders on the multilingual model

## Open Questions the Paper Calls Out

- Can a multilingual VAP model be effectively applied to languages outside the three studied (English, Mandarin, Japanese)?
- How does the performance of the multilingual VAP model compare to monolingual models when trained on a smaller dataset?
- How does the multilingual VAP model handle languages with different turn-taking patterns, such as those with more overlapping speech or longer pauses?

## Limitations

- Limited evaluation to only three languages from different families, leaving generalizability to other languages uncertain
- Sensitivity analysis on pitch importance does not establish causal relationships or test alternative acoustic features
- Claim of implicit language identification learning is based on observation rather than controlled experiments

## Confidence

**High confidence**: Monolingual models perform poorly on cross-lingual transfer; multilingual training improves performance across all languages.

**Medium confidence**: Pitch is more important for Mandarin and Japanese than English turn-taking prediction; CPC slightly outperforms MMS.

**Low confidence**: Multilingual model learns language identification implicitly without explicit supervision.

## Next Checks

1. Conduct ablation study by removing the auxiliary language identification loss to determine if explicit supervision is necessary for maintaining cross-lingual performance.

2. Perform controlled experiments manipulating pitch, intensity, and other acoustic features across languages to establish causal relationships between acoustic cues and turn-taking patterns.

3. Evaluate the model on additional languages not seen during training (e.g., Spanish, German) to assess universality of learned turn-taking patterns and robustness of cross-lingual transfer.