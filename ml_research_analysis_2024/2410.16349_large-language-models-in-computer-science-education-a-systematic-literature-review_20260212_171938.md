---
ver: rpa2
title: 'Large Language Models in Computer Science Education: A Systematic Literature
  Review'
arxiv_id: '2410.16349'
source_url: https://arxiv.org/abs/2410.16349
tags:
- llms
- programming
- education
- language
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic literature review analyzes 125 papers to examine
  the impact of large language models (LLMs) in computer science education. The study
  focuses on five research questions addressing educational levels, CS sub-disciplines,
  research methodologies, programming languages, and commonly used LLMs.
---

# Large Language Models in Computer Science Education: A Systematic Literature Review

## Quick Facts
- arXiv ID: 2410.16349
- Source URL: https://arxiv.org/abs/2410.16349
- Reference count: 40
- Primary result: Systematic review of 125 papers reveals LLMs predominantly used in undergraduate programming courses, with Python as the main language and ChatGPT as the most studied model.

## Executive Summary
This systematic literature review examines the impact of large language models (LLMs) in computer science education through analysis of 125 peer-reviewed papers. The study reveals that most research focuses on undergraduate programming courses, with Python being the predominant programming language studied. ChatGPT (GPT-3.5) is the most widely used LLM, while case studies and action research are the dominant research methodologies. The review highlights that while students generally view LLMs positively, instructors express concerns about over-reliance and learning effectiveness. The study identifies a need for more research on advanced CS topics, diverse programming languages, and benchmarking of LLM performance in educational contexts.

## Method Summary
The study follows Kitchenham and Charters (2007) guidelines for systematic literature reviews, conducting searches across multiple databases (ACM Digital Library, IEEE Xplore, Scopus, ACL Anthology, ISI Web of Science, Springer Link, ScienceDirect, ArXiv) using a comprehensive query combining CS education terms with LLM terminology. Papers were screened using inclusion/exclusion criteria focusing on full papers (â‰¥4 pages), English language, code-focused LLM applications in computing education, and publication dates from 2019-June 2024. Data extraction focused on five research questions covering educational levels, CS sub-disciplines, research methodologies, programming languages, and commonly used LLMs.

## Key Results
- Most research targets undergraduate programming courses (83% of studies)
- Python is the predominant programming language studied (mentioned in 60% of papers)
- ChatGPT (GPT-3.5) is the most widely used LLM across reviewed papers
- Case studies and action research are the dominant research methodologies (61% combined)
- Students generally view LLMs positively, while instructors express concerns about over-reliance and learning effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs enhance CS education by automating feedback generation on programming assignments.
- Mechanism: LLMs process student code and natural language queries to produce contextual feedback, reducing instructor workload and providing rapid responses.
- Core assumption: The quality of LLM-generated feedback is sufficient to support learning outcomes.
- Evidence anchors:
  - [abstract] "LLMs are increasingly used in education, helping students write, debug, and understand code."
  - [section] "LLMs can also provide feedback to the student to improve their code [75, 89]."
  - [corpus] Weak evidence: Only one related corpus paper mentions AI feedback, and it is not directly about LLMs.
- Break condition: If feedback quality falls below threshold, students may reinforce misconceptions.

### Mechanism 2
- Claim: LLMs bridge the gap between natural language and programming language, enabling students to articulate problems more easily.
- Mechanism: Students use natural language prompts to describe programming tasks; LLMs translate these into code or explanations.
- Core assumption: Students can formulate clear prompts that LLMs can interpret accurately.
- Evidence anchors:
  - [abstract] "LLMs have extended their capabilities to coding tasks, bridging the gap between natural languages (NL) and programming languages (PL)."
  - [section] "Students expressed some frustration about crafting prompts that elicit the desired output [84]."
  - [corpus] No direct corpus evidence; this is an inference from the paper's findings.
- Break condition: If prompt formulation skills are lacking, LLM utility diminishes.

### Mechanism 3
- Claim: LLMs support personalized education by generating tailored examples and exercises.
- Mechanism: LLMs create diverse problem sets and explanations based on individual student needs.
- Core assumption: Generated content aligns with curriculum goals and student skill levels.
- Evidence anchors:
  - [abstract] "We analyze their effectiveness in enhancing the learning experience, supporting personalized education..."
  - [section] "LLMs can generate programming problems [101, 112], MCQs [109, 113]..."
  - [corpus] No corpus evidence; this is drawn from the paper's results.
- Break condition: If personalization leads to fragmented learning paths, coherence suffers.

## Foundational Learning

- Concept: Systematic Literature Review methodology
  - Why needed here: Ensures comprehensive, unbiased coverage of LLM research in CS education.
  - Quick check question: What are the three main phases of an SLR according to Kitchenham and Charters?

- Concept: Programming language prevalence in education
  - Why needed here: Explains why Python dominates LLM studies in CS courses.
  - Quick check question: Which programming language is mentioned most frequently in the reviewed papers?

- Concept: Educational level classification
  - Why needed here: Clarifies distribution of LLM studies across K-12, undergraduate, graduate, and professional contexts.
  - Quick check question: What percentage of papers focused on undergraduate-level CS education?

## Architecture Onboarding

- Component map:
  - Database queries (ACM Digital Library, IEEE Xplore, etc.) -> Title/keywords/abstract screening -> Full-text review -> Data extraction -> Analysis -> Reporting

- Critical path:
  1. Define RQs and search query
  2. Execute database searches
  3. Apply inclusion/exclusion criteria
  4. Extract and analyze data
  5. Report findings

- Design tradeoffs:
  - Breadth vs. depth: Broader search captures more studies but risks noise
  - Automation vs. manual review: Manual review ensures quality but is time-intensive
  - Language limitation: Restricting to English may exclude relevant non-English studies

- Failure signatures:
  - Incomplete coverage: Missing key studies due to narrow search terms
  - Bias: Overrepresentation of certain methodologies or educational levels
  - Data inconsistency: Inconsistent extraction leading to skewed results

- First 3 experiments:
  1. Run search query on one database and manually review 10 random results to validate inclusion criteria.
  2. Pilot data extraction on 5 papers to refine categories and ensure consistency.
  3. Cross-validate extracted data by having two reviewers independently extract from the same 10 papers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the long-term impact of LLM use on students' fundamental programming skills and problem-solving abilities?
- Basis in paper: [inferred] Instructors express concerns about over-reliance on tools and students' ability to solve programming tasks independently
- Why unresolved: Current studies focus on immediate outcomes and perceptions rather than longitudinal tracking of skill development over multiple semesters or years
- What evidence would resolve it: Longitudinal studies comparing skill development between students who extensively use LLMs versus those who don't, tracking performance across multiple courses and time periods

### Open Question 2
- Question: How can educational institutions effectively integrate LLMs into curricula while maintaining academic integrity and learning objectives?
- Basis in paper: [explicit] "educators are gradually adopting LLMs in their courses but that most CS curricula still need to be changed to accommodate recent advances in AI"
- Why unresolved: The paper identifies this as a challenge but doesn't provide specific implementation frameworks or best practices for curriculum redesign
- What evidence would resolve it: Empirical studies evaluating different curriculum integration models, their impact on learning outcomes, and effectiveness in preventing academic misconduct

### Open Question 3
- Question: What are the optimal prompting strategies and educational scaffolding techniques for maximizing LLM effectiveness in different CS educational contexts?
- Basis in paper: [inferred] Students express frustration about crafting prompts that elicit desired output, and studies show mixed results in task completion across different educational levels
- Why unresolved: Most studies focus on LLM capabilities rather than optimizing human-LLM interaction patterns for educational purposes
- What evidence would resolve it: Comparative studies testing different prompting frameworks, scaffolding techniques, and their impact on student learning outcomes across various CS sub-disciplines

## Limitations

- Search query may have missed papers using alternative terminology for LLMs or CS education
- Restriction to English-language papers potentially excludes relevant non-English research
- Dominance of case studies and action research methodologies limits ability to draw causal conclusions about LLM effectiveness

## Confidence

- High Confidence: Descriptive findings about paper distribution across educational levels, CS sub-disciplines, and commonly used LLMs
- Medium Confidence: Conclusions about student and instructor attitudes toward LLMs
- Low Confidence: Claims about LLM effectiveness in enhancing learning outcomes

## Next Checks

1. **Search Term Expansion**: Re-run the database searches using expanded terminology (e.g., "generative AI," "foundation models," "neural code generation") to identify potentially missed papers.

2. **Methodological Quality Assessment**: Develop and apply a standardized rubric to evaluate the methodological rigor of the reviewed studies, particularly examining the presence of control groups, pre/post testing, and statistical analysis.

3. **Cross-Discipline Comparison**: Conduct a parallel systematic review focusing on LLM applications in other STEM education domains (mathematics, physics) to identify whether the patterns observed in CS education are discipline-specific or represent broader trends in AI-assisted education.