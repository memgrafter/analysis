---
ver: rpa2
title: Evaluating and explaining training strategies for zero-shot cross-lingual news
  sentiment analysis
arxiv_id: '2409.20054'
source_url: https://arxiv.org/abs/2409.20054
tags:
- sentiment
- language
- cross-lingual
- learning
- slovenian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates zero-shot cross-lingual sentiment analysis
  for news, focusing on less-resourced languages. It introduces novel evaluation datasets
  for Macedonian, Serbian, Bosnian, Albanian, and Estonian, extending previous work
  on Slovenian-Croatian transfer.
---

# Evaluating and explaining training strategies for zero-shot cross-lingual news sentiment analysis
## Quick Facts
- arXiv ID: 2409.20054
- Source URL: https://arxiv.org/abs/2409.20054
- Reference count: 10
- Primary result: Novel datasets and evaluation of zero-shot cross-lingual sentiment analysis methods for Balkan and Baltic languages

## Executive Summary
This paper investigates zero-shot cross-lingual sentiment analysis for news in less-resourced languages, introducing evaluation datasets for Macedonian, Serbian, Bosnian, Albanian, and Estonian. The study compares machine translation, in-context learning with large language models, and novel intermediate training strategies including a proposed Part-Of-Article (POA) method that incorporates paragraph-level position information. Results show that in-context learning generally achieves the best performance across languages, while translation to English does not consistently improve results. The authors analyze transferability gaps using semantic content and structure similarity, finding that language similarity alone is insufficient for predicting transfer success.

## Method Summary
The study introduces novel evaluation datasets for five less-resourced languages (Macedonian, Serbian, Bosnian, Albanian, and Estonian) and compares several zero-shot cross-lingual sentiment analysis approaches. Methods evaluated include machine translation to English, in-context learning with the Mistral7B model, and intermediate training strategies. The novel POA approach incorporates paragraph-level position information during intermediate training. All methods are evaluated on the same datasets, with results compared across languages to assess relative performance and transferability.

## Key Results
- In-context learning with Mistral7B generally achieves the best performance across all languages
- Translation to English does not consistently improve sentiment analysis results
- The proposed POA approach is competitive with in-context learning while requiring lower computational overhead
- Language similarity alone is insufficient to predict cross-lingual transfer success

## Why This Works (Mechanism)
The effectiveness of in-context learning stems from the model's ability to leverage semantic understanding across languages through few-shot demonstrations, while the POA approach works by incorporating structural information about paragraph positions within articles during intermediate training. The failure of translation-based approaches likely results from information loss during translation and the additional noise introduced in the transfer pipeline. The analysis of transferability gaps reveals that semantic content and structural similarity between source and target languages provide better predictors of transfer success than language family membership alone.

## Foundational Learning
- **Zero-shot cross-lingual transfer**: Learning from one language and directly applying to another without target language examples; needed to reduce annotation costs for low-resource languages; check by verifying model hasn't seen target language during training
- **In-context learning**: Providing demonstrations within the prompt to guide model behavior; needed for parameter-efficient adaptation; check by examining prompt format and demonstration examples
- **Intermediate training**: Pre-training on related tasks or data before fine-tuning; needed to bridge domain and language gaps; check by verifying training pipeline includes intermediate stages
- **Part-Of-Article (POA) approach**: Incorporating structural position information (paragraph levels) into training; needed to capture document-level sentiment patterns; check by examining how position features are encoded
- **Cross-lingual evaluation**: Testing models across multiple language pairs; needed to assess generalizability; check by verifying evaluation covers diverse language families
- **Semantic similarity metrics**: Measuring content overlap between languages; needed to explain transfer performance; check by examining similarity computation methods

## Architecture Onboarding
- **Component map**: News articles -> Preprocessing -> Sentiment classification model -> Evaluation metrics
- **Critical path**: Data collection/annotation → Model training (various strategies) → Zero-shot evaluation across languages
- **Design tradeoffs**: Computational cost vs. performance (POA vs. in-context learning), translation quality vs. direct cross-lingual transfer
- **Failure signatures**: Poor performance when semantic similarity is low, degradation with distant language pairs, translation errors propagating to sentiment errors
- **First experiments**: 1) Compare in-context learning vs. POA on one language pair, 2) Test translation approach baseline, 3) Measure semantic similarity correlation with transfer performance

## Open Questions the Paper Calls Out
None

## Limitations
- Findings based on limited set of Balkan and Baltic languages, raising generalizability concerns to other language families
- Zero-shot setting doesn't account for domain adaptation challenges when transferring between news domains
- Manual annotation process may introduce inter-annotator variability affecting dataset quality

## Confidence
- Cross-lingual sentiment transfer performance (High): Direct experiments on same datasets provide strong evidence
- Language similarity as insufficient predictor (Medium): Methodologically sound but would benefit from more diverse language pairs
- POA approach competitiveness (Medium): Based on specific setup; computational overhead comparison needs more configurations

## Next Checks
1. Evaluate approaches on language pairs from different families (e.g., Romance, Slavic, Germanic) to test generalizability
2. Conduct domain adaptation experiments by training on news from one country and testing on another
3. Perform ablation studies on the POA method to isolate position information contribution