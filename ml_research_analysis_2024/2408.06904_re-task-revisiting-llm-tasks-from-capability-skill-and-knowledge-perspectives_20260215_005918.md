---
ver: rpa2
title: 'Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge Perspectives'
arxiv_id: '2408.06904'
source_url: https://arxiv.org/abs/2408.06904
tags:
- knowledge
- capability
- task
- item
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Re-TASK framework, a novel theoretical
  model that revisits LLM tasks from capability, skill, and knowledge perspectives.
  Drawing on insights from Bloom's Taxonomy and Knowledge Space Theory, Re-TASK provides
  a systematic methodology to understand, evaluate, and enhance LLMs for domain-specific
  tasks.
---

# Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge Perspectives

## Quick Facts
- arXiv ID: 2408.06904
- Source URL: https://arxiv.org/abs/2408.06904
- Reference count: 40
- Key outcome: Framework improves LLM performance on domain-specific tasks through capability-item-based knowledge injection and skill adaptation

## Executive Summary
Re-TASK introduces a novel theoretical framework that systematically revisits LLM tasks by decomposing them into capability items, each comprising specific knowledge and skill components. Drawing from Bloom's Taxonomy and Knowledge Space Theory, the framework addresses LLM failures in domain-specific tasks by identifying that such failures often stem from insufficient knowledge or inadequate skill adaptation. The approach employs targeted knowledge injection and skill adaptation strategies, demonstrating significant performance improvements across diverse domains, particularly in legal tasks where it achieved 45.00% improvement on Yi-1.5-9B and 24.50% on Llama3-Chinese-8B models.

## Method Summary
The Re-TASK framework operates through two primary approaches: prompting strategies and a two-stage fine-tuning pipeline. For prompting, it employs a Chain-of-Learning paradigm that incorporates capability item demonstrations into prompts to strengthen task-relevant capabilities. The fine-tuning pipeline prioritizes capability items before task-specific instructions, using LoRA-based adaptation. The framework identifies capability items associated with tasks, injects domain knowledge through methods like Retrieval-Augmented Generation, and adapts skills through carefully designed prompts or fine-tuning procedures.

## Key Results
- Achieved 45.00% improvement on Yi-1.5-9B and 24.50% on Llama3-Chinese-8B models for legal sentence prediction tasks
- Demonstrated effectiveness across diverse capability items in the Chinese criminal law domain
- Validated both prompting-based and fine-tuning-based approaches for enhancing LLM performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Failures in domain-specific tasks stem from insufficient domain knowledge or inadequate skill adaptation.
- **Mechanism**: The Re-TASK framework addresses these failures by identifying capability items, injecting domain knowledge, and adapting skills through targeted prompting or fine-tuning.
- **Core assumption**: Task performance depends on mastering multiple capability items, each involving specific knowledge and skills.
- **Evidence anchors**:
  - [abstract]: "The Re-TASK framework introduces a Chain-of-Learning (CoL) paradigm that highlights task dependencies on specific capability items, further broken down into their constituent knowledge and skill components."
  - [section 3.1]: "The successful completion of a task Tt requires mastering multiple capability items {C1, C2, ··· , Cc}."
- **Break condition**: If the identified capability items do not align with actual task requirements, the framework's effectiveness diminishes.

### Mechanism 2
- **Claim**: Enhancing capability items through prompting strategies improves LLM performance on domain-specific tasks.
- **Mechanism**: By incorporating capability item demonstrations into prompts, LLMs can better understand and apply domain-specific knowledge and skills.
- **Core assumption**: Capability item demonstrations in prompts serve as effective in-context examples for skill adaptation.
- **Evidence anchors**:
  - [abstract]: "To address CoT failures, we propose a Re-TASK prompting strategy, which strengthens task-relevant capabilities through targeted knowledge injection and skill adaptation."
  - [section 4.3]: "The SP + Cj strategy consistently outperforms the SP strategy across all tested capability items (Cj, j = 1, 2, 3, 4)."
- **Break condition**: If the LLM cannot effectively learn from in-context examples, the prompting strategy's impact is limited.

### Mechanism 3
- **Claim**: A capability-first fine-tuning pipeline enhances LLM performance more effectively than task-only fine-tuning.
- **Mechanism**: Prioritizing capability items in the fine-tuning process ensures foundational skills are developed before task-specific applications.
- **Core assumption**: Capability items are prerequisites for effective task performance, and their enhancement leads to better task outcomes.
- **Evidence anchors**:
  - [abstract]: "we propose structured strategies for enhancing LLMs through targeted knowledge injection and skill adaptation."
  - [section 4.4]: "our new pipeline demonstrates enhanced performance on the target task compared to traditional fine-tuning methods."
- **Break condition**: If the capability items are not well-aligned with the task requirements, the fine-tuning pipeline may not yield significant improvements.

## Foundational Learning

- **Concept**: Bloom's Taxonomy and Knowledge Space Theory
  - Why needed here: These theories provide the theoretical foundation for decomposing tasks into capability items, knowledge, and skills.
  - Quick check question: How do Bloom's Taxonomy and Knowledge Space Theory inform the identification of capability items in the Re-TASK framework?

- **Concept**: Chain-of-Thought (CoT) reasoning
  - Why needed here: CoT is a baseline method for complex reasoning, and Re-TASK builds upon it by adding capability-skill-knowledge decomposition.
  - Quick check question: How does Re-TASK's Chain-of-Learning (CoL) paradigm differ from traditional CoT reasoning?

- **Concept**: Prompt engineering and fine-tuning techniques
  - Why needed here: Re-TASK employs both prompting strategies and fine-tuning pipelines to enhance LLM performance.
  - Quick check question: What are the key differences between Re-TASK's prompting strategies and traditional prompt engineering approaches?

## Architecture Onboarding

- **Component map**: Task identification → Capability item decomposition → Knowledge injection → Skill adaptation → Performance evaluation
- **Critical path**: Identify capability items → Inject domain knowledge → Adapt skills → Evaluate task performance → Iterate as needed
- **Design tradeoffs**:
  - Prompting vs. fine-tuning: Prompting is more flexible and cost-effective, while fine-tuning can lead to more permanent improvements
  - Capability item granularity: More granular items may be more effective but require more effort to identify and implement
- **Failure signatures**:
  - Poor task performance despite capability item enhancement: May indicate misalignment between capability items and task requirements
  - Overfitting to capability items: Can occur if the fine-tuning process focuses too narrowly on specific items
- **First 3 experiments**:
  1. Test prompting strategies with a single capability item on a simple domain-specific task
  2. Implement the fine-tuning pipeline on a more complex task, comparing performance with traditional fine-tuning
  3. Scale up the model size and evaluate the framework's effectiveness across different scales

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can capability items be automatically identified for a given task without manual intervention?
- Basis in paper: [inferred] The paper mentions that identifying capability items is a complex undertaking and suggests that automating this process is a promising direction for future research.
- Why unresolved: The paper does not provide a concrete method for automatically identifying capability items. It only suggests the possibility of using pre-built domain knowledge bases or advanced LLMs.
- What evidence would resolve it: A concrete algorithm or method that can automatically identify capability items for a given task, validated through experiments on various domains.

### Open Question 2
- Question: How does the performance of the Re-TASK framework vary across different domains beyond the legal field?
- Basis in paper: [explicit] The paper acknowledges that experiments are currently limited to the legal domain due to the effort involved in manually constructing capability items. It suggests extending experiments to more public domains in the future.
- Why unresolved: The paper only presents experimental results for the legal domain, so it is unclear how the framework would perform in other domains.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of the Re-TASK framework in improving LLM performance on domain-specific tasks in various domains, such as medicine, finance, or education.

### Open Question 3
- Question: How does the Re-TASK framework address the issue of hallucinations in LLMs?
- Basis in paper: [explicit] The paper mentions that the framework acknowledges hallucinations as a result of knowledge deficiencies and inadequate skill adaptation. It recognizes the importance of diagnosing and mitigating hallucinations for future research.
- Why unresolved: The paper does not provide a concrete solution for addressing hallucinations within the Re-TASK framework. It only mentions the importance of this issue for future research.
- What evidence would resolve it: A detailed explanation of how the Re-TASK framework can be used to diagnose and mitigate hallucinations in LLMs, supported by experimental results showing a reduction in hallucination rates.

## Limitations
- Limited to single-domain evaluation (Chinese criminal law), raising questions about generalizability to other specialized domains
- Computational costs of the two-stage fine-tuning pipeline are not addressed, potentially limiting practical deployment
- No systematic evaluation of capability item selection sensitivity or alignment with actual task requirements

## Confidence
- **High Confidence**: Theoretical foundation from Bloom's Taxonomy and Knowledge Space Theory is well-established
- **Medium Confidence**: Experimental results are promising but limited to one domain and lack comparison with state-of-the-art methods
- **Low Confidence**: Scalability to larger models and complex domains remains untested, with no exploration of overfitting risks

## Next Checks
1. Cross-domain validation: Apply Re-TASK to at least three different domain-specific tasks (e.g., medical diagnosis, financial analysis, and legal reasoning) to assess generalizability beyond the criminal law domain
2. Computational efficiency analysis: Measure and compare the computational costs of Re-TASK's prompting strategies versus the two-stage fine-tuning pipeline across different model sizes to identify practical deployment thresholds
3. Capability item robustness test: Systematically vary the granularity and selection of capability items to determine how sensitive the framework's performance is to these choices, including cases where capability items are deliberately misaligned with task requirements