---
ver: rpa2
title: 'Moment&Cross: Next-Generation Real-Time Cross-Domain CTR Prediction for Live-Streaming
  Recommendation at Kuaishou'
arxiv_id: '2408.05709'
source_url: https://arxiv.org/abs/2408.05709
tags:
- live-streaming
- user
- users
- short-video
- moment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses two challenges in live-streaming recommendation:
  (1) recommending live-streams at the right moment based on real-time content, and
  (2) leveraging short-video interaction data to overcome data sparsity. The proposed
  Moment&Cross framework introduces a real-time 30s data-streaming engine with a first-only
  mask learning strategy to capture live-streaming highlight moments, and a cross-domain
  approach that transfers user interests from short-video history to live-streaming
  using contrastive learning and attention mechanisms.'
---

# Moment&Cross: Next-Generation Real-Time Cross-Domain CTR Prediction for Live-Streaming Recommendation at Kuaishou

## Quick Facts
- arXiv ID: 2408.05709
- Source URL: https://arxiv.org/abs/2408.05709
- Authors: Jiangxia Cao; Shen Wang; Yue Li; Shenghui Wang; Jian Tang; Shiyao Wang; Shuang Yang; Zhaojie Liu; Guorui Zhou
- Reference count: 40
- One-line primary result: Improves live-streaming CTR prediction with real-time 30s streaming and cross-domain short-video interest transfer, achieving +0.9% AUC offline and +1.63%/+0.64% CTR online.

## Executive Summary
The paper addresses two key challenges in live-streaming recommendation: recommending streams at the right moment based on real-time content and leveraging short-video interaction data to overcome data sparsity. The proposed Moment&Cross framework introduces a real-time 30-second data-streaming engine with first-only mask learning to capture live-streaming highlight moments, and a cross-domain approach that transfers user interests from short-video history to live-streaming using contrastive learning and attention mechanisms. The framework has been deployed at Kuaishou, serving 400 million users, and demonstrates significant improvements in both offline and online metrics.

## Method Summary
Moment&Cross combines real-time 30-second data-streaming with first-only mask learning (Moment) and cross-domain short-video interest transfer using contrastive learning and attention mechanisms (Cross). The real-time engine reports user interactions immediately and applies first-only masking to avoid overcounting repeated actions. The cross-domain module searches and compresses short-video history sequences, then aligns them with live-streaming embeddings via contrastive learning to infer live-streaming preferences from richer short-video interactions. The framework is trained with multi-task learning to predict multiple interaction types simultaneously.

## Key Results
- Offline experiments show improvements in AUC/GAUC metrics (+0.9% AUC)
- Online A/B tests demonstrate gains in click-through rate (+1.63%/+0.64%)
- Online A/B tests show increases in watch time (+4.13%/+1.85%)
- Deployed at Kuaishou, serving 400 million users

## Why This Works (Mechanism)

### Mechanism 1
Real-time 30-second data streaming captures rising CTR trends during live-streaming highlight moments. By reporting user interactions immediately and using first-only mask learning, the model receives fresh gradient signals when live-streaming content becomes engaging, allowing it to boost predictions for trending streams. The core assumption is that user positive feedback spikes during live-streaming highlight moments and is quickly observable.

### Mechanism 2
Cross-domain short-video interest transfer alleviates data sparsity in live-streaming recommendations. Short-video history sequences are searched and compressed, then aligned with live-streaming embeddings via contrastive learning. This allows the model to infer live-streaming preferences from richer short-video interactions. The core assumption is that user interests in short-videos correlate strongly with interests in similar live-streaming content.

### Mechanism 3
First-only mask learning strategy avoids overcounting repeated user actions and balances fast/slow feedback. Only the first positive interaction for each behavior type is learned; subsequent positives in the same session are masked, and negatives are reported only on exit. The core assumption is that early positive signals are most informative for CTR prediction; repeated actions add noise rather than signal.

## Foundational Learning

- Concept: Multi-task learning for CTR prediction.
  - Why needed here: Live-streaming recommendation requires predicting multiple interaction types (click, long-view, like, comment, gift) simultaneously to rank content effectively.
  - Quick check question: How does multi-task learning improve ranking compared to single-task CTR prediction?

- Concept: Contrastive learning for embedding alignment.
  - Why needed here: Aligning short-video and live-streaming embedding spaces enables transfer of user interests across domains, addressing data sparsity.
  - Quick check question: Why is contrastive learning preferred over direct concatenation of cross-domain features?

- Concept: Real-time data streaming and feedback delay handling.
  - Why needed here: Live-streaming content changes dynamically; delayed feedback can cause the model to miss trending moments or overfit stale signals.
  - Quick check question: What is the trade-off between window size and feedback freshness in live-streaming recommendation?

## Architecture Onboarding

- Component map: Real-time 30s data-streaming engine -> Multi-task CTR model (e.g., PLE/MMOE) with first-only mask learning -> Cross-domain interest extractor (GSUs + ESUs + contrastive loss) -> Ranking output
- Critical path: User interaction -> 30s data report -> Model update -> Ranking score calculation -> Content recommendation
- Design tradeoffs: Smaller report windows increase freshness but risk noisy labels; cross-domain transfer helps sparsity but may introduce domain mismatch
- Failure signatures: Sudden drops in AUC/GAUC may indicate misalignment in cross-domain embeddings; degraded click/watch time suggests feedback delay or mask learning issues
- First 3 experiments:
  1. Compare AUC/GAUC of 30s real-time vs. 5-min fixed-window streaming on a validation set
  2. Ablation study removing each GSU sequence to measure cross-domain contribution
  3. A/B test of first-only mask vs. full-label learning to quantify overfitting risk

## Open Questions the Paper Calls Out

### Open Question 1
How does the first-only mask learning strategy compare to other potential strategies for handling delayed feedback in live-streaming recommendation, such as weighted loss functions or temporal attention mechanisms? The paper introduces the first-only mask learning strategy but does not compare it to other potential strategies.

### Open Question 2
What are the potential limitations of using contrastive learning to align short-video and live-streaming embedding spaces, and how can these limitations be addressed? The paper introduces contrastive learning to align embedding spaces but does not discuss potential limitations or solutions.

### Open Question 3
How does the effectiveness of the Moment&Cross framework vary across different types of live-streaming content, such as talent shows, game plays, and educational content? The paper provides examples but does not discuss effectiveness across different content types comprehensively.

## Limitations
- Technical description of General Search Units (GSUs) and Exact Search Units (ESUs) lacks implementation details necessary for faithful reproduction
- The 30-second window choice appears arbitrary without ablation studies across different time windows
- Paper doesn't quantify how much short-video and live-streaming domains actually diverge in practice

## Confidence
- High confidence in the general framework and reported metric improvements (AUC/GAUC +0.9%, online CTR +1.63%/+0.64%)
- Medium confidence in the specific effectiveness of first-only mask learning strategy (no direct ablation shown)
- Medium confidence in cross-domain interest transfer mechanism (implementation details sparse)
- Low confidence in optimal parameter choices (window size, model architecture specifics)

## Next Checks
1. Conduct ablation studies comparing different real-time window sizes (15s, 30s, 60s, 300s) to identify optimal freshness vs. noise trade-off
2. Perform domain similarity analysis between short-video and live-streaming content to quantify cross-domain transfer potential
3. Test first-only mask learning against full-label learning on historical data to measure overfitting reduction and signal preservation