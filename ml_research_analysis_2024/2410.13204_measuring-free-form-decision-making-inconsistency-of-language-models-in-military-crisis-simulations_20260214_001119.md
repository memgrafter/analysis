---
ver: rpa2
title: Measuring Free-Form Decision-Making Inconsistency of Language Models in Military
  Crisis Simulations
arxiv_id: '2410.13204'
source_url: https://arxiv.org/abs/2410.13204
tags:
- inconsistency
- prompt
- military
- china
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors quantitatively measure inconsistency of free-form responses
  from language models in a high-stakes military crisis simulation using a metric
  based on BERTScore. They show that all five tested models exhibit semantic inconsistencies
  across varying levels of wargame escalation and anonymization, with inconsistencies
  often exceeding those from temperature sampling.
---

# Measuring Free-Form Decision-Making Inconsistency of Language Models in Military Crisis Simulations

## Quick Facts
- arXiv ID: 2410.13204
- Source URL: https://arxiv.org/abs/2410.13204
- Reference count: 38
- Key outcome: Five tested models exhibit semantic inconsistencies across wargame escalation levels and anonymization, with inconsistencies often exceeding those from temperature sampling

## Executive Summary
This paper quantifies inconsistency in free-form decision-making responses from language models in military crisis simulations using a BERTScore-based metric. The authors demonstrate that all five tested models (Claude 3.5 Sonnet, GPT-3.5 Turbo, GPT-4, GPT-4o, GPT-4o mini) produce semantically inconsistent responses even under controlled conditions like low temperature sampling. They find that prompt sensitivity-induced inconsistency can exceed temperature-induced inconsistency, revealing fundamental instability in how LMs process semantically equivalent inputs. The study raises concerns about relying on LM decisions for critical military or high-stakes decision-making contexts.

## Method Summary
The authors measure inconsistency by sampling 20 responses per simulation from five off-the-shelf LMs in a Taiwan Strait conflict scenario, then computing pairwise BERTScore-based similarity. Inconsistency is defined as 1 minus the rescaled F1 BERTScore between response pairs, with scores >0.25 indicating semantic variation. They test different temperature settings (T = 0.2, 0.4, 0.6, 0.8, 1.2) and prompt variations through lexical substitution, syntactic restructuring, and semantic shifts. The validation experiment applies these text ablations to question-answering outputs to verify the metric's sensitivity to semantic meaning versus surface-level variations.

## Key Results
- All five tested models exhibit semantic inconsistencies across varying wargame escalation levels
- Inconsistencies often exceed those produced by temperature sampling alone
- Prompt sensitivity variations can induce more inconsistency than temperature changes, even at T = 0.0

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERTScore-based inconsistency metric effectively captures semantic differences while ignoring surface-level lexical and syntactic variations in free-form decision-making outputs.
- Mechanism: BERTScore computes token similarity using contextual embeddings, then aggregates into F1 score. The authors define inconsistency as 1 minus this score, meaning identical texts yield 0 inconsistency and semantically unrelated texts yield high inconsistency. The validation experiment applies four types of text ablations (lexical substitution, syntactic restructuring, addition of irrelevance, semantic shift) to question-answering outputs and measures their effect on the inconsistency score.
- Core assumption: BERTScore's contextual embeddings preserve semantic meaning while being robust to legitimate linguistic variation, and that rescaling and inversion preserves interpretability for inconsistency measurement.
- Evidence anchors:
  - [abstract] "We use a metric based on BERTScore to measure response inconsistency quantitatively. Leveraging the benefits of BERTScore, we show that the inconsistency metric is robust to linguistic variations that preserve semantic meaning..."
  - [section] "We find that lexical substitution and syntactic restructuring generate the least inconsistency. Thus, the inconsistency score is able to emphasize semantic meaning in texts, even if the lexical or syntactic form of the sentence is changed."
- Break condition: If BERTScore fails to capture semantic meaning in longer, more complex decision-making texts, or if the rescaling introduces non-linear distortion in the inconsistency measurement.

### Mechanism 2
- Claim: Temperature sampling induces measurable inconsistency in LM responses, with higher temperatures producing greater inconsistency due to increased stochasticity in token selection.
- Mechanism: The authors sample 20 responses at different temperature settings (T = 0.2, 0.4, 0.6, 0.8, 1.2) using the same prompt, then compute pairwise inconsistency scores. Lower temperatures constrain the sampling distribution, producing more deterministic outputs with less variation.
- Core assumption: Temperature directly controls the randomness of sampling from the next-token distribution, and this randomness translates to semantic variation in the generated text.
- Evidence anchors:
  - [abstract] "...even when adjusting the wargame setting, anonymizing involved conflict countries, or adjusting the sampling temperature parameter T."
  - [section] "We show that inconsistency decreases with temperature, as expected. Notably, even with a low temperature of 0.2, we still observe levels of inconsistency surpassing what we would expect between semantically similar texts for all studied models."
- Break condition: If the models implement temperature differently than standard sampling distributions, or if semantic content is primarily determined by prompt structure rather than sampling randomness.

### Mechanism 3
- Claim: Prompt sensitivity-induced inconsistency (from semantically equivalent prompt variations) can exceed temperature-induced inconsistency, revealing fundamental instability in LM decision-making.
- Mechanism: The authors create two levels of prompt ablation: Level One preserves semantic meaning through synonym replacement and structural changes, while Level Two changes more meaningful aspects like conflict scenarios and decision roles. They set T = 0.0 to eliminate temperature effects and measure inconsistency from prompt variations alone.
- Core assumption: LMs exhibit sensitivity to prompt phrasing beyond what temperature sampling explains, indicating underlying instability in how they process semantically equivalent inputs.
- Evidence anchors:
  - [abstract] "We find that inconsistency due to semantically equivalent prompt variations can exceed response inconsistency from temperature sampling for most studied models across different levels of ablations."
  - [section] "We observe that inconsistency as a result of any T ≥ 0.2 exceeds inconsistency due to semantic preserving prompt ablations."
- Break condition: If the prompt variations inadvertently introduce semantic differences beyond what the authors intended, or if the T = 0.0 setting still contains residual stochasticity.

## Foundational Learning

- Concept: Semantic similarity evaluation using contextual embeddings
  - Why needed here: The core innovation relies on measuring whether BERTScore can capture semantic equivalence in free-form decision-making text, which is more complex than standard QA or translation tasks.
  - Quick check question: Why would lexical substitution and syntactic restructuring produce lower inconsistency scores than semantic shift in the validation experiment?

- Concept: Temperature parameter in autoregressive generation
  - Why needed here: Understanding how temperature affects sampling distributions is crucial for interpreting why lower temperatures reduce inconsistency and whether this reflects genuine decision stability.
  - Quick check question: If temperature controls randomness, why would any temperature level still produce inconsistency beyond what's expected from semantically similar texts?

- Concept: Military crisis simulation structure and escalation dynamics
  - Why needed here: The experimental design uses a Taiwan Strait conflict scenario with escalating incidents. Understanding how escalation affects decision-making consistency is central to the study's implications.
  - Quick check question: Why would the "status quo" and "revisionist" continuations produce different inconsistency levels across models?

## Architecture Onboarding

- Component map: Data generation pipeline → BERTScore inconsistency computation → Statistical analysis → Qualitative validation
- Critical path: The validation experiment (Section 4) is critical because it establishes whether the inconsistency metric is appropriate for the task. Without this validation, the main results would be questionable.
- Design tradeoffs: Using BERTScore trades interpretability for robustness to linguistic variation. The authors chose F1 over precision/recall to balance both, but this may obscure cases where models are consistently wrong rather than inconsistent.
- Failure signatures: High inconsistency scores that don't correlate with qualitative differences in responses, or inconsistency that varies dramatically across prompt ablations that should be semantically equivalent.
- First 3 experiments:
  1. Replicate the validation experiment with different embedding models (e.g., RoBERTa, GPT embeddings) to test robustness of the inconsistency metric.
  2. Test inconsistency at intermediate temperature values (e.g., T = 0.3, 0.5, 0.7) to better understand the temperature-inconsistency relationship.
  3. Apply the inconsistency metric to a different high-stakes domain (e.g., medical diagnosis recommendations) to test generalizability beyond military scenarios.

## Open Questions the Paper Calls Out

- Question: How does the level of wargame escalation (status quo vs. revisionist continuations) quantitatively affect the inconsistency of LM responses across different model families?
- Basis in paper: [explicit] The authors observe that the revisionist continuation results in lower inconsistency for some models, but do not provide a rigorous statistical analysis or explore the underlying mechanisms.
- Why unresolved: The paper reports differences but lacks a detailed quantitative analysis of escalation's effect on inconsistency, such as effect sizes or model-specific patterns.
- What evidence would resolve it: A comprehensive statistical analysis (e.g., ANOVA or regression) comparing inconsistency scores across escalation levels and models, along with qualitative insights into why escalation impacts inconsistency differently.

## Limitations
- The metric validation was conducted on simple question-answering tasks rather than the complex free-form decision-making texts in the main study
- Related papers focus on behavioral analysis rather than semantic evaluation metrics, providing weak supporting evidence
- Prompt ablation methodology is not fully detailed, making independent verification difficult

## Confidence

**High Confidence**: The temperature sampling mechanism and its relationship to inconsistency - this follows standard LM sampling theory and the experimental design is straightforward.

**Medium Confidence**: The prompt sensitivity findings - while theoretically sound, the prompt ablation methodology is not fully detailed in the paper, making independent verification difficult.

**Medium Confidence**: The core finding that all five models exhibit significant inconsistency across wargame escalation - the experimental design is sound but the metric validation creates some uncertainty.

## Next Checks

1. **Metric Robustness Test**: Replicate the validation experiment using different embedding models (RoBERTa, GPT embeddings) and compare consistency scores across the same lexical substitution, syntactic restructuring, and semantic shift conditions to verify that BERTScore is not uniquely sensitive to DeBERTa's characteristics.

2. **Temperature-Inconsistency Mapping**: Conduct experiments at intermediate temperature values (T = 0.3, 0.5, 0.7) rather than the coarse-grained levels used, and plot the precise relationship between temperature and inconsistency to determine if the relationship is linear or follows a different pattern.

3. **Cross-Domain Generalization**: Apply the inconsistency metric to decision-making outputs from a different high-stakes domain (such as medical diagnosis recommendations or financial risk assessment) using the same five models to test whether the observed inconsistency patterns are specific to military scenarios or represent a general LM limitation.