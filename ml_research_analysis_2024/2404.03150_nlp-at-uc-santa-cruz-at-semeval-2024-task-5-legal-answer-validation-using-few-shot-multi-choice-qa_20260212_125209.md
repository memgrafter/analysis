---
ver: rpa2
title: 'NLP at UC Santa Cruz at SemEval-2024 Task 5: Legal Answer Validation using
  Few-Shot Multi-Choice QA'
arxiv_id: '2404.03150'
source_url: https://arxiv.org/abs/2404.03150
tags:
- legal
- answer
- dataset
- task
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses legal answer validation in civil procedure
  using NLP models. The authors compare two approaches: fine-tuning BERT models and
  few-shot prompting with GPT models.'
---

# NLP at UC Santa Cruz at SemEval-2024 Task 5: Legal Answer Validation using Few-Shot Multi-Choice QA

## Quick Facts
- arXiv ID: 2404.03150
- Source URL: https://arxiv.org/abs/2404.03150
- Reference count: 2
- Best performance: 7th place out of 20 teams in SemEval-2024 Task 5

## Executive Summary
This paper tackles legal answer validation in civil procedure using NLP models. The authors explore two approaches: fine-tuning BERT-based models and few-shot prompting with GPT models. They discover that reformulating the task as a multiple-choice question answering problem significantly improves performance. Their best submission combines a BERT-based model with a rule-based algorithm, achieving 7th place in the competition. The work demonstrates the effectiveness of multi-choice QA few-shot prompting on GPT-4, which achieved an F1 score of 71.70 and accuracy of 80.61 on the test dataset.

## Method Summary
The authors employ two main approaches for legal answer validation. First, they fine-tune BERT-based models (LegalBERT and BERT) on a binary classification task for 500 epochs. Second, they use few-shot prompting with GPT-3.5 and GPT-4, reformulating the problem as multi-choice QA. The few-shot approach uses system instructions and examples to guide the model. They also implement a rule-based post-processing algorithm that leverages patterns in the training data. The dataset consists of 666 training, 84 validation, and 98 test examples from "The Glannon Guide to Civil Procedure."

## Key Results
- GPT-4 few-shot prompting with multi-choice QA format achieved F1 score of 71.70 and accuracy of 80.61
- Reformulating binary classification as multi-choice QA significantly improved model performance
- BERT models were limited by 512-token context length, preventing inclusion of full explanations
- Rule-based post-processing algorithm improved performance by exploiting dataset patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reformulating the task as a multi-choice QA problem significantly improves performance.
- **Mechanism:** The multi-choice format allows the model to consider all answer candidates together, leveraging contextual understanding and relative ranking rather than independent binary classification.
- **Core assumption:** The model can effectively compare and rank answer candidates when presented together in a single prompt.
- **Evidence anchors:** Abstract states multi-choice reformulation "remarkably improves the performance," and section 4.4 discusses shifting from binary to multi-choice classification for nuanced assessment.

### Mechanism 2
- **Claim:** GPT-4's larger context window enables better handling of complex legal texts.
- **Mechanism:** GPT-4 can process longer inputs including questions, answers, and explanations together, providing more comprehensive context for reasoning.
- **Core assumption:** Legal reasoning requires sufficient context from explanations and related text that cannot be captured in shorter input sequences.
- **Evidence anchors:** Abstract mentions "benefits of newer generation models like GPT-4 in handling complex legal texts," and section 4.3 discusses context length limitations.

### Mechanism 3
- **Claim:** Rule-based post-processing improves performance by leveraging dataset patterns.
- **Mechanism:** The rule-based algorithm identifies cases where all training answers are incorrect and forces the prediction to be correct.
- **Core assumption:** The test set follows the same pattern as training data where questions with all-incorrect answers have correct answers in the test set.
- **Evidence anchors:** Section 4.5 describes the strategy for presuming correct answers when all training answers are labeled as 0, with performance metrics shown in Table 5.

## Foundational Learning

- **Concept:** Binary vs multi-choice classification tradeoffs
  - Why needed here: Understanding when to use each format is critical for task formulation and model performance
  - Quick check question: What is the main advantage of multi-choice classification over binary classification for this legal QA task?
  - Answer: Multi-choice classification allows the model to compare all answer candidates together, reducing class imbalance issues and enabling better relative ranking

- **Concept:** Context length limitations in transformer models
  - Why needed here: BERT's 512-token limit vs GPT-4's longer context affects what information can be included in prompts
  - Quick check question: Why did the authors switch from BERT to GPT-4 for their best-performing model?
  - Answer: GPT-4's larger context window allowed including explanations and analysis features that BERT's 512-token limit prevented

- **Concept:** Few-shot prompting methodology
  - Why needed here: The authors achieved their best results using few-shot prompting rather than fine-tuning
  - Quick check question: What are the key components of the few-shot prompting approach used in this paper?
  - Answer: System instruction, two examples (one correct, one incorrect), and the multi-choice formatted dataset as input queries

## Architecture Onboarding

- **Component map:** Input preprocessing → Format conversion (binary→multi-choice) → Few-shot prompting (GPT-4) → Rule-based post-processing → Output predictions
- **Critical path:** Multi-choice QA format conversion → GPT-4 few-shot prompting → Rule-based algorithm application
- **Design tradeoffs:** 
  - Few-shot prompting vs fine-tuning: Few-shot avoids expensive retraining but may be less robust to distribution shifts
  - Binary vs multi-choice format: Multi-choice improves performance but requires more complex prompt engineering
  - Rule-based post-processing: Improves metrics but may not generalize to different datasets
- **Failure signatures:**
  - GPT-4 predictions showing low confidence across all options suggests prompt engineering issues
  - Rule-based algorithm degrading performance indicates test set distribution differs from training
  - Significant performance drop on binary format suggests over-reliance on multi-choice structure
- **First 3 experiments:**
  1. Compare BERT fine-tuning performance with and without including explanation text (testing context length limits)
  2. Test GPT-4 few-shot prompting with binary format vs multi-choice format on validation set
  3. Apply rule-based algorithm to GPT-4 multi-choice predictions and measure performance improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the inclusion of specific legal precedents or statutes, rather than general explanations, impact the performance of legal question-answering models?
- Basis in paper: [explicit] The paper suggests exploring the integration of specific laws or precedents as a form of analysis to enhance the capabilities of LLMs in legal reasoning tasks.
- Why unresolved: The current dataset uses general explanations to validate answers, which may not fully challenge the model's understanding of legal principles. The impact of using specific legal references remains untested.
- What evidence would resolve it: Conducting experiments where models are trained and tested with datasets that include specific legal precedents or statutes, and comparing their performance to models trained on general explanations.

### Open Question 2
- Question: What is the optimal balance between dataset size and diversity to prevent overfitting in legal question-answering tasks?
- Basis in paper: [inferred] The paper discusses the challenge of dataset imbalance and the tendency of models to overfit on the majority label (0) in binary classification tasks.
- Why unresolved: While the paper attempts to address overfitting by changing the dataset format and using few-shot prompting, the optimal balance between dataset size and diversity for preventing overfitting is not established.
- What evidence would resolve it: Systematic experiments varying the size and diversity of the dataset, measuring model performance and overfitting tendencies, to identify the optimal balance.

### Open Question 3
- Question: How does the performance of legal question-answering models differ when trained on data from multiple legal textbooks versus a single source?
- Basis in paper: [explicit] The paper suggests that future datasets should diversify sources to challenge models more rigorously and prevent exploitation of predictable structures.
- Why unresolved: The current dataset is sourced from a single textbook, which may limit the model's exposure to varied legal reasoning and argumentation styles.
- What evidence would resolve it: Comparing model performance on datasets sourced from multiple legal textbooks versus a single source, assessing the impact on generalization and robustness.

## Limitations

- Dataset-specific rule-based algorithm may not generalize to other legal datasets
- Comparison between BERT and GPT models conflates task reformulation with model capability differences
- Lack of ablation studies to isolate impact of individual components (multi-choice format, few-shot examples, rule-based algorithm)

## Confidence

- **High Confidence:** GPT-4 few-shot prompting with multi-choice QA format superiority (7th place vs 20 teams, F1 71.70, accuracy 80.61)
- **Medium Confidence:** Multi-choice format improving performance through better relative ranking and reduced class imbalance
- **Low Confidence:** Generalizability of rule-based post-processing algorithm beyond this specific dataset

## Next Checks

1. **Ablation study on task formulation:** Test GPT-4 with few-shot prompting using both binary and multi-choice formats on the same validation set to isolate the impact of task reformulation from model differences.

2. **Dataset generalization test:** Apply the rule-based post-processing algorithm to a different legal QA dataset (e.g., SFLA or similar) to evaluate whether the approach generalizes beyond the specific patterns in the SemEval-2024 dataset.

3. **Context length impact analysis:** Systematically test BERT models with truncated vs. full context inputs on the validation set to quantify how much of the performance gap between BERT and GPT-4 is attributable to context length limitations versus other factors.