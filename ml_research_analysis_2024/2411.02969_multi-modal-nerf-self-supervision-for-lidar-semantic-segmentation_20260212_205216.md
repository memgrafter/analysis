---
ver: rpa2
title: Multi-modal NeRF Self-Supervision for LiDAR Semantic Segmentation
arxiv_id: '2411.02969'
source_url: https://arxiv.org/abs/2411.02969
tags:
- lidar
- semantic
- unlabeled
- segmentation
- pixel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of LiDAR semantic segmentation,
  a critical task for autonomous driving perception, by leveraging unlabeled data
  through semi-supervised learning. The authors propose a novel approach that uses
  a Neural Radiance Field (NeRF) self-supervision technique to bridge the gap between
  LiDAR and camera modalities, addressing the challenge of parallax effects that arise
  from sensor position shifts.
---

# Multi-modal NeRF Self-Supervision for LiDAR Semantic Segmentation

## Quick Facts
- arXiv ID: 2411.02969
- Source URL: https://arxiv.org/abs/2411.02969
- Reference count: 40
- One-line primary result: Semi-supervised LiDAR semantic segmentation using NeRF self-supervision achieves 55.9% mIoU on 1% labeled nuScenes split, outperforming supervised-only baseline by 3.9%.

## Executive Summary
This paper addresses the challenge of LiDAR semantic segmentation in autonomous driving by leveraging unlabeled data through a semi-supervised learning approach. The authors propose a novel NeRF-based self-supervision technique that bridges the gap between 3D LiDAR features and 2D camera views while being resilient to parallax effects. By combining voxelized LiDAR features with camera-derived pseudo-labels generated through ray casting and Segment-Anything Model masks, the method significantly improves segmentation performance, particularly when labeled data is scarce.

## Method Summary
The approach uses a semi-supervised learning setup with Cylinder3D as the baseline 3D LiDAR segmentation architecture. For labeled data, the model trains with both a voxel 3D segmentation head and a NeRF MLP head. For unlabeled data, rays are cast from camera viewpoints into the voxelized LiDAR features, and the NeRF head renders pixel semantics through volumetric rendering. These pixel predictions are combined with SAM-generated generic masks to create pseudo-labels, which supervise the model through a self-supervised 2D loss. During inference, only the LiDAR-only architecture is used. The method is evaluated on nuScenes, SemanticKITTI, and ScribbleKITTI datasets with 1% and 10% labeled splits.

## Key Results
- Achieves 55.9% mIoU on 1% labeled nuScenes split, outperforming supervised-only baseline by 3.9%
- Shows greatest improvements on nuScenes where multi-view cameras cover full LiDAR angular range
- Significant performance gains observed across all three benchmarks (nuScenes, SemanticKITTI, ScribbleKITTI)
- Ablation studies confirm effectiveness of NeRF self-supervision and SAM mask refinement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NeRF self-supervision enables pixel-level semantic rendering that is resilient to parallax compared to direct LiDAR point projection.
- Mechanism: Instead of projecting 3D LiDAR points into 2D camera images (which is affected by parallax due to sensor position shifts), the method casts rays from the camera viewpoint into the 3D voxelized LiDAR features and samples semantic logits and densities along the ray. This volumetric rendering approach reasons about occupancy and semantics continuously along the ray rather than at discrete points, making it less sensitive to small position mismatches between sensors.
- Core assumption: The voxelized LiDAR features contain sufficient information to predict semantic logits and densities when queried along camera-view rays.
- Evidence anchors:
  - [abstract] "To address this, we include in our Semi-Supervised Learning (SSL) model a self-supervision technique to train on the unlabeled data which is inspired by the training mechanism of Neural Radiance Fields (NeRFs)"
  - [section] "The NeRF self-supervision, including ray casting, NeRF MLP head, and volumetric rendering is a crucial component of our system, as it enables effectively bridging the 3D LiDAR data and the 2D distilled knowledge from images"
  - [corpus] Weak evidence - no direct citations in neighbors discussing NeRF-based LiDAR semantic segmentation

### Mechanism 2
- Claim: SAM-generated generic masks combined with confident pixel predictions from rendered semantics produce high-quality pseudo-labels for supervision.
- Mechanism: The confidence sampler uses entropy of rendered pixel semantics to identify confident predictions (low entropy), especially at object interiors. These are merged with SAM's generic masks to produce segment-wise pseudo-labels. Segments with high confidence (low entropy) are kept for supervision.
- Core assumption: Entropy of rendered pixel predictions correlates with prediction reliability, with low entropy indicating confident interior predictions.
- Evidence anchors:
  - [section] "Fig. 3 shows how the entropy H (uncertainty) of the rendered pixel semantics ˆyp concentrates at the object boundaries. Our confidence sampler benefits from this effect by leveraging confident pixel predictions at the interiors of each segment to supervise the unreliable predictions at the borders"
  - [section] "We only keep segment pseudo-labels ˆCs with entropies lower than a threshold Hth"
  - [corpus] Weak evidence - no direct citations in neighbors discussing entropy-based confidence sampling for LiDAR pseudo-labels

### Mechanism 3
- Claim: Semi-supervised learning with multi-modal pseudo-labels improves model generalization, especially when labeled data is scarce.
- Mechanism: The model is trained on both labeled LiDAR scans (with 3D supervision) and unlabeled LiDAR scans (with 2D pseudo-labels from camera-SAM fusion). The voxel features learned during this semi-supervised process are more general and transfer better to new scenes.
- Core assumption: The voxel features learned from both labeled and unlabeled data capture more generalizable representations than those learned from labeled data alone.
- Evidence anchors:
  - [abstract] "We propose a Semi-Supervised Learning setup to leverage unlabeled LiDAR pointclouds alongside distilled knowledge from the camera images"
  - [section] "At inference time, we keep only the baseline LiDAR-only architecture... The weights of the 3D U-Net, which were trained with both labeled and unlabeled data in a semi-supervised fashion, provide powerful and general voxel features that boost the model's performance"
  - [corpus] Weak evidence - neighbors discuss multi-modal fusion but not specifically semi-supervised LiDAR semantic segmentation with NeRF self-supervision

## Foundational Learning

- Concept: Neural Radiance Fields (NeRF)
  - Why needed here: NeRF provides the framework for volumetric rendering along rays, which is essential for bridging the 3D LiDAR features with 2D camera views without suffering from parallax.
  - Quick check question: How does NeRF's volumetric rendering differ from simple point projection, and why is this difference important for multi-modal fusion?

- Concept: Semi-supervised learning with pseudo-labels
  - Why needed here: The method relies on generating pseudo-labels from unlabeled data to augment the limited labeled training set, which is crucial when labeled LiDAR data is expensive to obtain.
  - Quick check question: What are the key differences between self-training and consistency regularization in semi-supervised learning, and which approach does this method use?

- Concept: Entropy as a confidence measure
  - Why needed here: Entropy is used to filter pseudo-labels by measuring prediction uncertainty, ensuring only reliable labels are used for training.
  - Quick check question: Why does entropy tend to be higher at object boundaries and lower at object interiors in semantic segmentation?

## Architecture Onboarding

- Component map: LiDAR voxel features -> Ray casting -> NeRF head -> Volumetric rendering -> Pseudo-labels -> Semi-supervised loss
- Critical path: LiDAR voxel features → Ray casting → NeRF MLP head → Volumetric rendering → Pseudo-labels → Semi-supervised loss
- Design tradeoffs:
  - Tradeoff between ray sampling density (M) and computational cost
  - Tradeoff between entropy threshold and pseudo-label quality vs quantity
  - Tradeoff between number of SAM masks and pseudo-label confidence
- Failure signatures:
  - Low mIoU improvement on validation set - check pseudo-label quality and entropy threshold
  - Memory errors during training - reduce batch size or ray sampling density
  - Slow training - check ray sampling efficiency and voxel resolution
- First 3 experiments:
  1. Validate NeRF self-supervision: Train with labeled data only, then add NeRF self-supervision and measure pseudo-label quality and entropy distribution
  2. Test confidence sampling: Vary entropy threshold and measure pseudo-label quantity and downstream mIoU
  3. Compare to perspective projection: Implement perspective projection baseline and compare pseudo-label quality and final mIoU

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of the NeRF self-supervision approach scale with the number of labeled samples in the dataset? Does the performance improvement plateau at a certain point, or does it continue to grow as more labeled data is added?
  - Basis in paper: The authors mention that the model delivers the highest improvements with lower amounts of labeled data and suggest that the bigger classes already perform well with more labeled data, so the mIoU can only get significantly upgraded by improving the IoU on classes with thin objects.
  - Why unresolved: The paper does not provide experimental results with varying amounts of labeled data beyond the 1% and 10% splits, and does not investigate the relationship between the number of labeled samples and the performance improvement.
  - What evidence would resolve it: Conducting experiments with different percentages of labeled data, such as 5%, 20%, 50%, and 100%, and analyzing the performance improvement at each level would provide insights into how the performance scales with the number of labeled samples.

- **Open Question 2**: How does the NeRF self-supervision approach perform on datasets with different LiDAR sensor configurations, such as varying numbers of channels, ranges, or point densities?
  - Basis in paper: The authors mention that the performance improvement is greatest in nuScenes, where the multi-view cameras cover the full LiDAR angular range. However, they also note that the model produces competitive upgrades even on SemanticKITTI and ScribbleKITTI, where the single frontal camera covers only a small fraction of the LiDAR's angular range.
  - Why unresolved: The paper does not provide experimental results on datasets with different LiDAR sensor configurations, and it is unclear how the performance would be affected by variations in the number of channels, ranges, or point densities.
  - What evidence would resolve it: Conducting experiments on datasets with different LiDAR sensor configurations, such as datasets with 16, 32, or 64 channels, varying ranges (e.g., 50m, 100m, 200m), and different point densities, would provide insights into how the performance is affected by these variations.

- **Open Question 3**: How does the NeRF self-supervision approach compare to other semi-supervised learning methods that use different techniques for bridging the gap between 2D and 3D data, such as multi-view consistency or depth estimation?
  - Basis in paper: The authors mention that the parallax effect, which occurs when the position of the LiDAR and camera at their respective capture times differ, is a challenge in distilling knowledge from 2D foundation models into a 3D perception model. They also note that some works address this issue by exploiting the multi-view nature of video recordings at consecutive frames or by overlapping multiple-view cameras to select projections that are consistent from multiple views.
  - Why unresolved: The paper does not provide a direct comparison with other semi-supervised learning methods that use different techniques for bridging the gap between 2D and 3D data, such as multi-view consistency or depth estimation.
  - What evidence would resolve it: Conducting experiments that compare the performance of the NeRF self-supervision approach with other semi-supervised learning methods that use different techniques for bridging the gap between 2D and 3D data, such as multi-view consistency or depth estimation, would provide insights into the relative strengths and weaknesses of each approach.

## Limitations

- The paper lacks ablation studies comparing NeRF-based volumetric rendering against simpler perspective projection methods to validate the claimed resilience to parallax effects.
- The method's performance depends on the quality of SAM-generated masks, which is not thoroughly evaluated across different dataset domains or with varying SAM parameters.
- While ablation studies show individual component effectiveness, the interactions between NeRF self-supervision, entropy-based confidence sampling, and SAM mask fusion are not fully explored.

## Confidence

- Mechanism 1 (NeRF self-supervision): Medium - supported by conceptual reasoning but lacks direct comparative evidence
- Mechanism 2 (Entropy confidence sampling): Medium - based on observed entropy distributions but not rigorously validated
- Mechanism 3 (Semi-supervised improvement): High - supported by quantitative results across multiple benchmarks

## Next Checks

1. Implement a perspective projection baseline and conduct head-to-head comparison of pseudo-label quality and final mIoU metrics to validate the NeRF self-supervision advantage
2. Perform cross-dataset generalization tests by training on one dataset (e.g., nuScenes) and evaluating on another (e.g., SemanticKITTI) to assess the transferability of voxel features learned through semi-supervised learning
3. Conduct ablation studies varying the entropy threshold and SAM mask parameters to identify optimal configurations and quantify their impact on pseudo-label quality and final performance