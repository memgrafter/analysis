---
ver: rpa2
title: 'InAttention: Linear Context Scaling for Transformers'
arxiv_id: '2410.07063'
source_url: https://arxiv.org/abs/2410.07063
tags:
- context
- attention
- length
- inattention
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InAttention, a modification to transformer
  models that enables linear scaling of compute and memory with respect to context
  length during inference. The key innovation is having tokens attend only to initial
  states rather than other tokens in their own layer, which dramatically reduces VRAM
  usage - enabling queries up to 32,768 tokens to run on consumer GPUs with 16GB VRAM
  instead of requiring enterprise hardware like NVIDIA H100s.
---

# InAttention: Linear Context Scaling for Transformers

## Quick Facts
- arXiv ID: 2410.07063
- Source URL: https://arxiv.org/abs/2410.07063
- Reference count: 18
- Primary result: Enables linear scaling of transformer compute and memory with context length during inference by having tokens attend only to initial states

## Executive Summary
InAttention introduces a novel modification to transformer models that dramatically reduces VRAM usage during inference by replacing standard self-attention with a mechanism where tokens attend only to initial states rather than other tokens in their layer. This architectural change enables linear scaling of compute and memory with context length, allowing queries up to 32,768 tokens to run on consumer GPUs with 16GB VRAM instead of requiring enterprise hardware. While this modification causes a small capability degradation, the paper demonstrates that this can be offset by running larger models and that models can be efficiently fine-tuned to extend their context length capability.

## Method Summary
The InAttention modification replaces standard self-attention where Q = WQ X, K = WK X, and V = WV X with a version where tokens attend only to initial states: Q = WQ X, K = WK Y, and V = WV Y, where Y represents the initial states from the input embedding. The architecture adds layer normalization to initial states and modifies the attention computation to use these as keys and values while maintaining queries from hidden states. Models are pretrained at reasonable context lengths (1024 tokens) then fine-tuned on longer sequences to extend capability. Training uses AdamW optimizer with cosine annealing schedule on the C4 dataset across 16 A100 GPUs.

## Key Results
- Enables linear scaling of VRAM usage during inference instead of quadratic scaling
- Allows context lengths up to 32,768 tokens on consumer GPUs with 16GB VRAM
- Shows capability degradation can be offset by larger model sizes
- Demonstrates cheap fine-tuning can extend context length capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tokens attending only to initial states reduces the attention matrix from T×T to T×1 during inference
- Mechanism: InAttention replaces self-attention where each token attends all previous tokens with a mechanism where each token attends only the initial token embeddings, reducing the attention computation from O(T²) to O(T)
- Core assumption: Initial token embeddings contain sufficient contextual information for subsequent token predictions
- Evidence anchors:
  - [abstract]: "replacing self-attention with InAttention, which scales linearly with context length during inference by having tokens attend only to initial states"
  - [section]: "InAttention is very similar but instead Q = WQX; K = WK Y and V = WV Y where Y represents the initial-states of the model, which come straight from the Input Embedding"
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism

### Mechanism 2
- Claim: Eliminates need to cache intermediate hidden states during generation
- Mechanism: Since tokens only attend initial states, there's no dependency chain between hidden states, allowing each layer to compute its output independently without needing to store previous layer activations
- Core assumption: The residual connections and layer normalization are sufficient to propagate information without intermediate state dependencies
- Evidence anchors:
  - [abstract]: "After the initial next-token prediction, it is common practice to cache the hidden states at each layer so they do not need to be recomputed. This prevents us from needing to spin up the attention matrix again but it still represents substantial overhead. With InAttention, since we only attend initial states, we do not need to do this – freeing up VRAM even downstream of the initial surge"
  - [section]: "Work and features a residual tower 'pays forward' to help predict future tokens is, by its nature, speculative and immediately depreciates in value when the actual next token is revealed"
  - [corpus]: Weak - no direct corpus evidence found for this specific caching elimination mechanism

### Mechanism 3
- Claim: Fine-tuning on longer contexts can recover most capability lost from InAttention
- Mechanism: Models pretrained at shorter contexts can be efficiently fine-tuned on longer sequences to learn how to effectively use extended context without the quadratic cost
- Core assumption: The model architecture can adapt to use longer contexts even when trained on shorter ones
- Evidence anchors:
  - [abstract]: "We corroborate that models can be cheaply fine-tuned to extend their context length, improving performance on long sequences without high training costs"
  - [section]: "Using a NazX420 model pretrained at context length 128 as a starting point we fine-tune on a single additional file of C4, file 421, tokenized at a context length of 1024"
  - [corpus]: Weak - only 1 paper in corpus mentions context length fine-tuning directly

## Foundational Learning

- Concept: Self-attention mechanism in transformers
  - Why needed here: Understanding the O(T²) complexity and how attention works is fundamental to grasping why InAttention reduces this to O(T)
  - Quick check question: What is the computational complexity of standard self-attention with respect to sequence length?

- Concept: Residual connections and layer normalization
  - Why needed here: These architectural components are crucial for understanding how information flows through the transformer and why InAttention can eliminate intermediate state dependencies
  - Quick check question: How do residual connections and layer normalization interact in a transformer layer?

- Concept: Positional encoding and its role in transformers
  - Why needed here: Understanding how positional information is encoded helps explain why attending only to initial states might still preserve necessary sequential information
  - Quick check question: What are the two main approaches for encoding positional information in transformers?

## Architecture Onboarding

- Component map:
  - Input Embedding → Initial states → LayerNorm → Attention keys/values → Each transformer layer (queries from hidden states) → Final output

- Critical path:
  - Initial states → LayerNorm → Attention keys/values → Each transformer layer (queries from hidden states) → Final output

- Design tradeoffs:
  - Memory efficiency vs. model capability: InAttention dramatically reduces VRAM usage but causes small capability degradation
  - Training vs. inference optimization: InAttention optimizes inference but doesn't help with training quadratic scaling
  - Context length flexibility: Models need fine-tuning to effectively use longer contexts than they were trained on

- Failure signatures:
  - Excessive capability degradation on longer sequences
  - Inability to recover capability through fine-tuning
  - Poor performance on tasks requiring rich inter-token relationships

- First 3 experiments:
  1. Compare VRAM usage and inference time for standard attention vs. InAttention on varying context lengths (32, 256, 1024, 4096 tokens)
  2. Measure capability degradation by comparing evaluation loss on standard attention vs. InAttention models at their training context length
  3. Test fine-tuning effectiveness by pretraining at short context (128 tokens), fine-tuning at longer context (1024 tokens), and measuring performance at various lengths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between the number of parameters in InAttention models versus standard attention models for achieving equivalent performance on long-context tasks?
- Basis in paper: [explicit] The paper notes that InAttention causes capability degradation but suggests this can be offset by running larger models, with Figure 9 comparing loss versus VRAM footprint for different model sizes
- Why unresolved: The paper demonstrates that capability loss can be compensated by larger models but doesn't provide specific scaling relationships or determine the optimal parameter trade-off
- What evidence would resolve it: Controlled experiments varying model sizes for both InAttention and standard attention models while measuring performance on standardized long-context benchmarks

### Open Question 2
- Question: How does InAttention's performance on tasks requiring long-range dependencies compare to sparse attention mechanisms when both are optimized for equivalent VRAM usage?
- Basis in paper: [inferred] The paper focuses on VRAM efficiency gains but doesn't compare InAttention to sparse attention methods that also aim to reduce computational costs for long contexts
- Why unresolved: The paper presents InAttention as superior for VRAM efficiency but lacks direct comparisons to alternative approaches that also address the quadratic scaling problem
- What evidence would resolve it: Head-to-head benchmarks comparing InAttention, sliding window attention, Longformer-style patterns, and other sparse methods on tasks requiring retrieval of information across long distances

### Open Question 3
- Question: What architectural modifications to InAttention would enable linear scaling during training rather than just inference?
- Basis in paper: [explicit] The paper explicitly states that "InAttention provides a significant improvement in inference efficiency, but it does not reduce the computational burden of training transformers with very long contexts"
- Why unresolved: The authors identify this as a limitation but propose only finetuning as a partial workaround without exploring architectural solutions for training efficiency
- What evidence would resolve it: Development and validation of training-time modifications to InAttention that maintain its inference benefits while achieving linear scaling during the training phase

## Limitations

- Limited evaluation scope: The paper relies heavily on training loss as the primary metric without comprehensive evaluation on established downstream benchmarks
- Single dataset focus: Results are based primarily on C4 dataset with limited cross-domain validation
- Training efficiency gap: While inference scales linearly, training still suffers from quadratic complexity, limiting the approach's applicability to very long-context pretraining

## Confidence

**High Confidence**: The linear scaling claim for inference VRAM usage. This is directly verifiable through implementation and measurement, and the mechanism is straightforward - by attending only to initial states, the attention matrix becomes T×1 instead of T×T, which is mathematically guaranteed to reduce memory from O(T²) to O(T).

**Medium Confidence**: The capability degradation claim. While the paper provides training loss comparisons, these are measured on the same dataset used for training (C4), which introduces potential bias. The actual impact on real-world performance remains uncertain without broader evaluation.

**Low Confidence**: The fine-tuning effectiveness claim. Based on a single fine-tuning experiment with limited scope, it's unclear whether this approach generalizes to other datasets, model sizes, or domain-specific applications.

## Next Checks

1. **Benchmark Validation**: Evaluate InAttention models on established language understanding benchmarks (GLUE, SuperGLUE, LAMBADA) and compare performance against standard transformers. This would validate whether the training loss improvements translate to meaningful task performance.

2. **Scaling Analysis**: Conduct systematic experiments measuring both VRAM usage and inference latency across a wider range of context lengths (32 to 32,768 tokens) and model sizes (125M to 8B parameters) to verify the claimed linear scaling holds across different scales.

3. **Cross-Dataset Generalization**: Test the fine-tuning approach on multiple datasets beyond C4, including specialized domains like code (CodeXGLUE), scientific text (ArXiv), and multilingual data to assess whether context length extension through fine-tuning generalizes beyond the single experiment reported.