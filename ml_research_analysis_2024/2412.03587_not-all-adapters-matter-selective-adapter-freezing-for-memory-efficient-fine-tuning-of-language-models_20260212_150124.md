---
ver: rpa2
title: 'Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient Fine-Tuning
  of Language Models'
arxiv_id: '2412.03587'
source_url: https://arxiv.org/abs/2412.03587
tags:
- safe
- lora
- adapters
- e-04
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Selective Adapter Freezing (SAFE) addresses the problem of high
  memory usage and computational cost in adapter-tuning of large language models,
  where not all adapters contribute equally to task performance. SAFE works by adaptively
  freezing less important adapters early in training based on feature representation
  similarity (CKA) to reduce unnecessary resource usage while maintaining or improving
  accuracy.
---

# Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient Fine-Tuning of Language Models

## Quick Facts
- arXiv ID: 2412.03587
- Source URL: https://arxiv.org/abs/2412.03587
- Reference count: 23
- Primary result: SAFE reduces memory usage by 42.85% and computation by 34.59% while maintaining accuracy

## Executive Summary
Selective Adapter Freezing (SAFE) addresses the problem of high memory usage and computational cost in adapter-tuning of large language models, where not all adapters contribute equally to task performance. SAFE works by adaptively freezing less important adapters early in training based on feature representation similarity (CKA) to reduce unnecessary resource usage while maintaining or improving accuracy. Experiments show SAFE reduces memory usage by up to 42.85%, computation by up to 34.59%, and training time by up to 11.82% compared to baseline LoRA while achieving comparable or better task performance. The method also induces a regularization effect that leads to flatter loss landscapes and better generalization.

## Method Summary
SAFE introduces a two-stage approach to adapter-tuning that identifies and freezes less important adapters early in training. During the warm-up stage, SAFE calculates Centered Kernel Alignment (CKA) similarity scores between adapter and original layers to assess adapter importance. In the freezing stage, SAFE gradually freezes adapters following a cubic schedule, starting with those showing low importance scores. The method uses AdamW optimizer with weight decay=0 and LoRA adapters (r=4, alpha=16, dropout=0.1). By freezing adapters that contribute less to accuracy improvement, SAFE reduces activation memory and computation without sacrificing performance, while also inducing a regularization effect that creates flatter loss landscapes.

## Key Results
- Reduces memory usage by up to 42.85% compared to baseline LoRA
- Reduces computation by up to 34.59% while maintaining comparable accuracy
- Achieves up to 11.82% reduction in training time across multiple model sizes and tasks
- Demonstrates better generalization through flatter loss landscapes

## Why This Works (Mechanism)

### Mechanism 1
Freezing adapters that contribute less to accuracy improvement early in training reduces resource usage without sacrificing performance. SAFE identifies adapters with lower importance scores (measured via CKA similarity) and gradually freezes them during training, reducing activation memory and computation. Core assumption: Adapters in earlier layers learn more general features and can be frozen earlier without significant accuracy loss. Evidence: Memory and computation reductions of 42.85% and 34.59% while maintaining accuracy; CKA similarity shows early stability in several adapters. Break condition: If CKA similarity doesn't stabilize early in training, SAFE cannot identify freeze candidates reliably.

### Mechanism 2
SAFE induces a regularization effect that leads to flatter loss landscapes and better generalization. By constraining the number of trainable parameters through selective freezing, SAFE creates a regularization effect similar to weight decay, resulting in flatter loss surfaces. Core assumption: Flatter loss landscapes correlate with better generalization performance. Evidence: Demonstrated flatter loss landscapes; improved generalization on benchmark tasks. Break condition: If regularization effect doesn't manifest in flatter loss landscapes, generalization benefits may not materialize.

### Mechanism 3
Gradual freezing with a cubic schedule enables rapid exploration early while ensuring stable convergence later. SAFE increases the freezing threshold following a cubic schedule, freezing more adapters initially and stabilizing later to reach the target threshold. Core assumption: Non-linear freezing schedules balance exploration and exploitation better than linear schedules. Evidence: Cubic schedule enables rapid early exploration while ensuring stable convergence. Break condition: If cubic schedule doesn't provide better trade-off than linear schedule, performance may degrade.

## Foundational Learning

- Concept: Centered Kernel Alignment (CKA) for measuring feature representation similarity
  - Why needed here: SAFE uses CKA to identify less important adapters by measuring how much their feature representations change during training
  - Quick check question: What does a high CKA value indicate about the similarity between two sets of feature representations?

- Concept: Loss landscape flatness and its relationship to generalization
  - Why needed here: SAFE's regularization effect is validated by showing it creates flatter loss landscapes, which correlate with better generalization
  - Quick check question: Why do flatter loss landscapes generally lead to better generalization performance?

- Concept: Parameter-efficient fine-tuning (PEFT) methods and their memory characteristics
  - Why needed here: Understanding how adapters work and why they don't reduce activation memory is crucial for appreciating SAFE's contribution
  - Quick check question: Why doesn't adapter-tuning significantly reduce memory usage despite reducing the number of trainable parameters?

## Architecture Onboarding

- Component map: CKA calculation -> Importance score computation -> Adapter identification -> Cubic schedule freezing -> Training continuation
- Critical path: Calculate CKA similarity → Compute importance scores → Identify freezing candidates → Apply cubic schedule to gradually freeze adapters → Continue training with reduced parameter set
- Design tradeoffs: SAFE trades early adapter freezing (which could risk losing useful representations) for significant resource savings. The cubic schedule balances exploration vs. exploitation.
- Failure signatures: Accuracy degradation when important adapters are frozen too early; minimal resource savings if importance scores don't differentiate adapters effectively.
- First 3 experiments:
  1. Implement CKA similarity calculation and verify it correctly identifies adapters with stable feature representations
  2. Add importance score computation and test on a simple dataset to validate freezing candidates are correctly identified
  3. Implement cubic schedule freezing and verify gradual reduction in trainable parameters while maintaining accuracy

## Open Questions the Paper Calls Out
1. How does the performance of SAFE vary when combined with other memory-efficient training techniques like gradient checkpointing, low-precision training, or weight sharding? (Basis: Suggested as future work in limitations)
2. What is the optimal strategy for determining the warm-up epoch threshold (tw) and final freezing epoch threshold (tf) across different model architectures and tasks? (Basis: Empirically determined without systematic method)
3. How does SAFE's performance scale with extremely large language models (beyond 27B parameters) and what are the diminishing returns of adapter freezing at different model scales? (Basis: Only tested up to 27B parameters despite motivation discussing 65B+ models)

## Limitations
- Cubic schedule parameters (τT, tw, tf) may require task-specific tuning without systematic selection method
- Performance on domain-specific or low-resource tasks remains unexplored beyond standard benchmarks
- Computational overhead from CKA calculations during warm-up phase could be significant for very large models

## Confidence
- **High confidence**: Claims about memory usage reduction (42.85%) and computation savings (34.59%) - these are directly measurable and supported by experimental results.
- **Medium confidence**: Claims about accuracy maintenance/comparison - while results show comparable performance, the margin of difference is relatively small and could vary with different hyperparameter choices.
- **Low confidence**: Claims about regularization effects and loss landscape flattening - demonstrated but not thoroughly validated across different optimization landscapes or compared to other regularization methods.

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary τT, tw, and tf parameters across different tasks to establish guidelines for parameter selection and identify if task-specific tuning is truly necessary.
2. **Cross-domain generalization test**: Apply SAFE to specialized domains (medical imaging, legal documents, low-resource languages) to verify if the method generalizes beyond standard benchmarks and maintains resource efficiency.
3. **Overhead cost measurement**: Quantify the computational overhead of CKA calculations during warm-up phase and compare total training time (including overhead) against baseline LoRA to ensure net time savings are maintained across different model scales.