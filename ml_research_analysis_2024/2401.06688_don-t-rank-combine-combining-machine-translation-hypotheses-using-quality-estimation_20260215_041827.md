---
ver: rpa2
title: Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality
  Estimation
arxiv_id: '2401.06688'
source_url: https://arxiv.org/abs/2401.06688
tags:
- translation
- qe-fusion
- candidates
- beam
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QE-fusion, a method that synthesizes translations
  using a quality estimation (QE) metric. It leverages a pool of candidates sampled
  from a model, combining spans from different candidates using a QE metric such as
  CometKiwi.
---

# Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation

## Quick Facts
- **arXiv ID:** 2401.06688
- **Source URL:** https://arxiv.org/abs/2401.06688
- **Reference count:** 29
- **Primary result:** QE-fusion consistently outperforms reranking methods and matches oracle performance across varying candidate pool sizes (5-200) for both LLMs and NMT models

## Executive Summary
This paper introduces QE-fusion, a novel method for improving machine translation quality by synthesizing translations from a pool of candidate hypotheses. Unlike traditional reranking approaches that select the best candidate, QE-fusion combines complementary spans from multiple candidates using quality estimation metrics. The method demonstrates consistent improvements across five language pairs and multiple model types, with particularly strong results for large language models due to their ability to generate diverse outputs.

## Method Summary
QE-fusion generates multiple translation candidates using nucleus sampling for LLMs and epsilon sampling for NMT models, then identifies divergent spans between candidates using edit distance. The algorithm iteratively replaces spans in a base hypothesis with alternatives from other candidates, selecting replacements that maximize a QE metric score (COMET-KIWI or similar). This process continues until no further improvements can be made, producing a novel translation that integrates the best parts of multiple candidates. The method is compared against beam search, MBR decoding, and QE-reranking baselines.

## Key Results
- QE-fusion generates novel translations in over half of cases and consistently outperforms reranking approaches across all candidate pool sizes (5-200)
- The method matches oracle performance for small pools (5-25 candidates) and scales linearly with candidate count
- LLMs benefit more from QE-fusion than NMT models due to higher candidate diversity from general-domain pretraining
- Improvements are demonstrated across five language pairs using multiple model types including PolyLM, XGLM, Llama2, ALMA, Mistral, and NLLB

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QE-fusion synthesizes improved translations by combining complementary spans from diverse candidates
- Mechanism: The algorithm identifies divergent spans among candidate translations and iteratively replaces base spans with alternatives that maximize a QE metric score, constructing a new translation that integrates the best parts of multiple candidates
- Core assumption: Complementary errors exist across candidates, and a QE metric can effectively identify and rank superior spans
- Evidence anchors:
  - [abstract]: "QE-fusion leverages the complementary nature of a pool of translation candidates generated by an LLM... combining fragments from them into an improved output."
  - [section]: "While reranking methods such as MBR decoding and QE-reranking effectively improve the performance of MT systems, they are bound by the quality of the candidates in the pool. To address this limitation, QE-fusion leverages the complementary nature of the candidates to merge them into a novel and refined output."

### Mechanism 2
- Claim: LLMs benefit more from QE-fusion than NMT models due to higher candidate diversity
- Mechanism: LLMs' general-domain pretraining enables generation of diverse translations, creating more divergent spans for QE-fusion to leverage. NMT models, being task-specific, produce more homogeneous outputs
- Core assumption: Diversity in candidate pool directly correlates with QE-fusion's improvement potential
- Evidence anchors:
  - [abstract]: "Notably, QE-fusion exhibits larger improvements for LLMs due to their ability to generate diverse outputs."
  - [section]: "Increasing the temperature leads to an expected rise in diversity. The rise is higher for XGLM-2.9B than for NLLB-1.3B, illustrating the fact that LLMs generate more diverse outputs, likely due to their general-domain language pretraining compared to the task-specific training of NMT models."

### Mechanism 3
- Claim: QE-fusion maintains effectiveness across varying candidate pool sizes (5-200)
- Mechanism: The algorithm's linear scaling with candidate count allows it to extract improvements even from small pools, while larger pools provide diminishing returns due to increased redundancy
- Core assumption: Even small candidate pools contain sufficient complementary spans for meaningful fusion
- Evidence anchors:
  - [abstract]: "We demonstrate that our approach generates novel translations in over half of the cases and consistently outperforms other methods across varying numbers of candidates (5–200)."
  - [section]: "QE-fusion consistently outperforms reranking approaches across all sizes of candidate pools. Moreover, QE-fusion even matches the performance of the oracle method (COMET oracle) for pool sizes of 5, 10 and 25 candidates."

## Foundational Learning

- Concept: Quality Estimation (QE) metrics
  - Why needed here: QE metrics provide reference-free quality assessment to guide span selection without requiring ground truth translations
  - Quick check question: How does COMET-KIWI differ from reference-based metrics like COMET-22 in terms of input requirements?

- Concept: Hypothesis space exploration in decoding
  - Why needed here: Understanding how beam search, sampling, and reranking navigate translation space is crucial for appreciating QE-fusion's novel combination approach
  - Quick check question: What is the key limitation of Minimum Bayes Risk decoding that QE-fusion addresses?

- Concept: Span divergence detection using edit distance
  - Why needed here: The algorithm relies on identifying differing spans between candidates to enable targeted combination
  - Quick check question: How does the algorithm group divergent spans to form alternative hypotheses?

## Architecture Onboarding

- Component map: Candidate generator -> Span divergence detector -> QE metric scorer -> Fusion engine -> Output selector
- Critical path:
  1. Generate candidate pool via sampling
  2. Compute divergent spans between base hypothesis and alternatives
  3. Iteratively replace spans to maximize QE score
  4. Select highest-scoring fused output
- Design tradeoffs:
  - Sampling diversity vs. computational cost (higher diversity requires more candidates)
  - QE metric quality vs. inference speed (neural metrics are slower but more accurate)
  - Fusion depth vs. runtime (more iterations improve quality but increase latency)
- Failure signatures:
  - Poor diversity: All candidates share same errors, no complementary spans exist
  - QE metric failure: Metric cannot distinguish quality differences in span-level comparisons
  - Computational bottleneck: Quadratic scaling in MBR vs. linear in QE-fusion
- First 3 experiments:
  1. Compare QE-fusion vs. QE-reranking on small candidate pool (5 candidates) to validate effectiveness with minimal resources
  2. Test temperature scaling impact on LLM candidate diversity and QE-fusion performance
  3. Measure runtime scaling vs. candidate pool size to confirm linear complexity assumption

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal temperature range for balancing diversity and quality across different language pairs and model sizes?
- Basis in paper: [explicit] Section 6.3 discusses the impact of temperature on diversity and quality, finding optimal performance in the [0.4, 0.6] interval for en→de
- Why unresolved: The study only tested en→de; other language pairs may have different optimal temperature ranges due to varying linguistic complexities and model performance characteristics
- What evidence would resolve it: Systematic experiments varying temperature across multiple language pairs and model sizes, measuring both diversity metrics and translation quality scores

### Open Question 2
- Question: How does QE-fusion perform compared to task-specific models when sufficient fine-tuning data is available?
- Basis in paper: [explicit] Section 5.1 mentions ALMA-7B as a bridge between general LLMs and task-specific MT models, but does not directly compare QE-fusion to fully fine-tuned MT models
- Why unresolved: The paper focuses on zero-shot and few-shot scenarios, leaving the comparison with fully fine-tuned task-specific models unexplored
- What evidence would resolve it: Direct comparison of QE-fusion applied to LLMs vs. fully fine-tuned MT models on the same translation tasks, using the same evaluation metrics

### Open Question 3
- Question: What is the impact of candidate pool diversity on QE-fusion's performance for languages with different script systems (e.g., Cyrillic, logographic)?
- Basis in paper: [explicit] Section 6.3 notes that LLMs generate more diverse outputs than NMT models, but does not specifically analyze diversity impacts across different script systems
- Why unresolved: The paper tested multiple language pairs with different scripts but did not analyze whether script complexity affects the relationship between diversity and performance
- What evidence would resolve it: Comparative analysis of QE-fusion performance across languages with different script systems (Latin, Cyrillic, logographic), measuring both diversity metrics and translation quality

## Limitations

- Sampling Sensitivity: QE-fusion's effectiveness critically depends on candidate pool diversity, which varies with sampling strategy and temperature settings
- QE Metric Dependence: The approach's success hinges on the quality of QE metrics like COMET-KIWI for span-level evaluation
- Computational Overhead: Despite linear scaling, the method requires multiple inference passes for candidate generation, span detection, iterative scoring, and final selection

## Confidence

- **High Confidence:** The core mechanism of combining complementary spans from diverse candidates to improve translation quality is well-supported by the experimental results across multiple model types and language pairs
- **Medium Confidence:** The claim that LLMs benefit more than NMT models due to higher candidate diversity is supported but could be more rigorously quantified
- **Medium Confidence:** The assertion that QE-fusion generates novel translations in over half of cases is based on the experimental data, but the novelty metric definition could be more explicit

## Next Checks

1. **Sampling Parameter Sensitivity Analysis:** Systematically vary nucleus sampling p-value and temperature parameters across different language pairs to quantify their impact on candidate diversity and QE-fusion performance

2. **QE Metric Ablation Study:** Compare QE-fusion performance using different QE metrics (COMET-KIWI vs COMET-22 vs reference-based metrics) to isolate the impact of QE metric quality on final translation improvements

3. **Scaling Boundary Testing:** Evaluate QE-fusion performance at extreme candidate pool sizes (e.g., 3 candidates for minimal resources, 500+ candidates for maximum diversity) to identify the sweet spot where diversity benefits plateau