---
ver: rpa2
title: Conjuring Semantic Similarity
arxiv_id: '2410.16431'
source_url: https://arxiv.org/abs/2410.16431
tags:
- semantic
- diffusion
- similarity
- arxiv
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to measuring semantic similarity
  between textual expressions by leveraging the imagery they evoke rather than relying
  on linguistic or vector-based representations. The method computes the Jensen-Shannon
  divergence between the stochastic differential equations governing text-conditioned
  diffusion models, enabling direct computation via Monte-Carlo sampling.
---

# Conjuring Semantic Similarity

## Quick Facts
- arXiv ID: 2410.16431
- Source URL: https://arxiv.org/abs/2410.16431
- Reference count: 14
- This paper introduces a novel approach to measuring semantic similarity between textual expressions by leveraging the imagery they evoke rather than relying on linguistic or vector-based representations.

## Executive Summary
This paper presents a novel approach to measuring semantic similarity between textual expressions by computing the Jensen-Shannon divergence between the stochastic differential equations governing text-conditioned diffusion models. The method leverages Monte Carlo sampling to approximate the distance between image distributions evoked by different text prompts, offering an interpretable visual grounding for semantic similarity judgments. Experiments on standard semantic textual similarity benchmarks demonstrate that this visually-grounded approach achieves Spearman correlations of up to 65.4% with human annotations, rivaling large language models while providing fine-grained analysis of semantic relationships and diffusion model failure modes.

## Method Summary
The method computes semantic similarity by treating text prompts as generators of image distributions through diffusion models. It calculates the Jensen-Shannon divergence between the reverse-time diffusion SDEs using Monte Carlo sampling across multiple timesteps. The approach requires a pre-trained text-conditioned diffusion model (Stable Diffusion v1.4), uniform sampling across T timesteps (T=10), and k Monte Carlo iterations (k=1-5). The final similarity score is derived from the aggregated score function differences between the two prompts across the sampling process.

## Key Results
- Achieved Spearman correlations up to 65.4% with human-annotated semantic similarity scores
- Outperformed traditional static embedding methods on STS and SICK-R benchmarks
- Demonstrated interpretable visual explanations of semantic differences through generated image pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic similarity can be computed by measuring the distance between image distributions evoked by text prompts
- Mechanism: The method treats each text prompt as generating a probability distribution over images through the diffusion model. By computing the Jensen-Shannon divergence between these distributions using their underlying SDEs, we get a semantic distance measure
- Core assumption: The semantic meaning of text can be fully captured by the distribution of images it generates
- Evidence anchors:
  - [abstract] "we characterize the semantic similarity between two textual expressions simply as the distance between image distributions they induce, or 'conjure.'"
  - [section] "our method is the first to enable quantifying the alignment of semantic representations learnt by diffusion models compared to that of humans"
  - [corpus] Weak evidence - corpus neighbors focus on NLP similarity methods but don't directly support this visual grounding claim
- Break condition: When text prompts generate similar images but have different semantic meanings, or when text has meaning that cannot be visually represented

### Mechanism 2
- Claim: The Jensen-Shannon divergence between SDEs can be computed via Monte Carlo sampling of diffusion model predictions
- Mechanism: The JS divergence between two SDEs can be expressed as an expectation over noisy images, which can be approximated by sampling from the diffusion model at various timesteps and comparing score function predictions
- Core assumption: The score function predictions from the diffusion model at different timesteps contain sufficient information to compute semantic distance
- Evidence anchors:
  - [section] "we propose to leverage the Jensen Shannon Divergence between the stochastic differential equations (SDEs) that govern the flow of the diffusion model, which we will show to be computable using a Monte-Carlo sampling approach."
  - [section] "dours(y1, y2) = Et∼unif([0,T]),x∼ 1 2 pt(x|y1)+ 1 2 pt(x|y2) g(t)2∥sθ(x, t|y1) − sθ(x, t|y2)∥2 2"
  - [corpus] No direct evidence in corpus - corpus focuses on NLP similarity methods
- Break condition: When the diffusion model's score function predictions are unreliable or when Monte Carlo sampling requires too many iterations for convergence

### Mechanism 3
- Claim: Semantic alignment with human annotations can be measured by correlating computed distances with human-annotated similarity scores
- Mechanism: By computing semantic distances between pairs of sentences using the visual method and comparing these distances to human-annotated similarity scores from standard benchmarks (STS datasets), we can measure alignment with human semantic understanding
- Core assumption: Human annotations of semantic similarity represent a valid ground truth that can be used to evaluate the visual method
- Evidence anchors:
  - [section] "we use the Semantic Textual Similarity (STS) datasets... we then use our method to compute the image-grounded similarity score, and measure their resulting Spearman Correlation with the annotations."
  - [section] "our experiments in Table 1 show that our visually-grounded similarity scores exhibit significant degrees of correlation with that annotated by humans"
  - [corpus] Weak evidence - corpus neighbors don't discuss evaluation methods
- Break condition: When human annotations are inconsistent or when the visual method captures semantic aspects that humans don't prioritize

## Foundational Learning

- Concept: Diffusion models and score-based generative modeling
  - Why needed here: The method relies on understanding how diffusion models generate images from text and how their score functions can be used to compare semantic similarity
  - Quick check question: How does a diffusion model transform noise into an image conditioned on text, and what role does the score function play in this process?

- Concept: Stochastic differential equations (SDEs) and their properties
  - Why needed here: The method uses the SDEs governing diffusion models to compute semantic distances via JS divergence
  - Quick check question: What conditions ensure the existence and uniqueness of solutions to the SDEs in diffusion models, and why is Novikov's Condition important for the Girsanov theorem application?

- Concept: Information theory and divergence measures
  - Why needed here: The method uses Jensen-Shannon divergence to measure distance between probability distributions
  - Quick check question: How does JS divergence differ from KL divergence, and why is it symmetric and bounded, making it suitable for measuring semantic similarity?

## Architecture Onboarding

- Component map: Text prompts → Text encoder → Diffusion model (U-Net) → Score function predictions at multiple timesteps → Distance computation → Semantic similarity score
- Critical path: Text encoding → Diffusion sampling → Score function comparison → Distance aggregation → Final similarity score
- Design tradeoffs: Using Monte Carlo sampling trades computational efficiency for approximation accuracy; choosing uniform timestep distribution simplifies implementation but may not be optimal
- Failure signatures: Low correlation with human annotations, high variance across different prompt pairs, computational infeasibility with many Monte Carlo steps
- First 3 experiments:
  1. Test semantic similarity between semantically related word pairs (e.g., "dog" vs "puppy") and verify they have higher similarity scores than unrelated pairs
  2. Validate the method on a small subset of STS-B dataset and compute Spearman correlation with human annotations
  3. Perform ablation study on number of Monte Carlo steps (k) to find the minimum number needed for stable results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do semantic similarity scores change when using different text encoders (beyond CLIP) in diffusion models?
- Basis in paper: [explicit] The paper mentions that "modern diffusion models use a pre-trained text-encoder to pre-process textual prompts" and that "representation structures obtained from the diffusion model outputs would be bottle-necked by those learnt by the text encoder." It also suggests that "this limitation can be mitigated by the development of better encoders, such as those based on LLMs."
- Why unresolved: The paper only uses Stable Diffusion v1.4 with CLIP-based text encoding, leaving open whether other encoders (e.g., LLM-based encoders) would yield different semantic alignment results.
- What evidence would resolve it: Comparative experiments measuring semantic similarity scores using the same diffusion architecture but different text encoders (e.g., CLIP vs. LLM-based encoders) on the same benchmarks (STS and SICK-R datasets).

### Open Question 2
- Question: How does the choice of distance metric beyond Jensen-Shannon divergence affect semantic similarity measurements in diffusion models?
- Basis in paper: [explicit] The paper states "our approach can be used to derive many possible variants based on the metric used to compare images, which we leave for future exploration."
- Why unresolved: The paper only explores Jensen-Shannon divergence between SDEs, but other metrics (e.g., Wasserstein distance, maximum mean discrepancy) might capture different aspects of semantic similarity.
- What evidence would resolve it: Systematic comparison of semantic similarity scores and human alignment across multiple distance metrics using the same diffusion model and evaluation benchmarks.

### Open Question 3
- Question: How does semantic alignment vary across different types of textual expressions (e.g., abstract concepts, mathematical terms, concrete nouns)?
- Basis in paper: [explicit] The paper discusses limitations, noting that "imageries might indeed not be sufficient to fully capture the meaning of certain expressions, such as mathematical abstractions (like 'imaginary numbers') and metaphysical concepts (like 'conscience')."
- Why unresolved: The evaluation only covers concrete nouns and action verbs from standard semantic similarity datasets, not testing the method's performance on truly abstract or mathematical concepts.
- What evidence would resolve it: Targeted experiments measuring semantic similarity scores for abstract concepts, mathematical terms, and metaphysical concepts, comparing them against human judgments or other evaluation methods.

## Limitations
- The core assumption that visual imagery can fully capture semantic meaning is both the paper's innovation and its primary vulnerability
- Computational complexity of Monte Carlo sampling limits scalability to large-scale semantic similarity tasks
- Dependence on diffusion model quality means the method inherits any biases or limitations in the underlying generative model

## Confidence
- **High Confidence**: The mathematical framework for computing JS divergence between SDEs is well-established; the correlation with human annotations on standard STS benchmarks is empirically validated
- **Medium Confidence**: The claim that this method rivals large language models in semantic similarity tasks is supported by results but requires careful interpretation of what "rivalry" means across different evaluation contexts
- **Low Confidence**: The broader claims about interpretability and fine-grained analysis of diffusion model failure modes are demonstrated but not systematically validated across diverse failure scenarios

## Next Checks
1. Test the method on semantic relationships that lack strong visual grounding (e.g., abstract concepts, metaphors) to identify the boundary of visual-grounded semantics
2. Conduct controlled experiments comparing results when using different diffusion model architectures and training datasets to assess method robustness
3. Perform ablation studies varying the number of timesteps and Monte Carlo samples to establish the sensitivity of correlation scores to these hyperparameters