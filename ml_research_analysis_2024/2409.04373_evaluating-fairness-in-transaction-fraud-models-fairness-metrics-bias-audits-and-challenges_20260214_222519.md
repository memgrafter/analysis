---
ver: rpa2
title: 'Evaluating Fairness in Transaction Fraud Models: Fairness Metrics, Bias Audits,
  and Challenges'
arxiv_id: '2409.04373'
source_url: https://arxiv.org/abs/2409.04373
tags:
- fairness
- fraud
- parity
- bias
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first algorithmic bias audit in transaction
  fraud detection, addressing the gap in fairness evaluation for this domain. The
  authors evaluate LightGBM models using public synthetic datasets (Sparkov and IBMCard),
  focusing on group fairness metrics that account for fraud data's imbalanced nature
  and the trade-off between fraud protection and service quality.
---

# Evaluating Fairness in Transaction Fraud Models: Fairness Metrics, Bias Audits, and Challenges

## Quick Facts
- arXiv ID: 2409.04373
- Source URL: https://arxiv.org/abs/2409.04373
- Reference count: 40
- Primary result: First algorithmic bias audit in transaction fraud detection reveals significant bias in normalized fairness metrics and demonstrates limitations of fairness through unawareness approach

## Executive Summary
This study presents the first algorithmic bias audit in transaction fraud detection, addressing the gap in fairness evaluation for this domain. The authors evaluate LightGBM models using public synthetic datasets (Sparkov and IBMCard), focusing on group fairness metrics that account for fraud data's imbalanced nature and the trade-off between fraud protection and service quality. Their analysis reveals that certain fairness metrics expose significant bias only after normalization, highlighting the impact of class imbalance. The study also discusses socio-technical fairness challenges in transaction fraud models, emphasizing the need for nuanced approaches that balance protection and service quality.

## Method Summary
The study evaluates LightGBM classifiers on synthetic Sparkov and IBMCard datasets using standard ERM and fairness through unawareness approaches. Models are trained with cross-entropy objective, AUC validation, and hyperparameter tuning with Optuna. Group fairness metrics (recall parity, precision parity, FPR parity, ROC AUC parity, PR AUC parity, VDR parity) are computed both raw and normalized by maximum value. Analysis includes both global FP-ratio thresholds and group-wise thresholds to detect bias masking. The fairness through unawareness approach removes gender attribute to test bias mitigation.

## Key Results
- Normalized fairness metrics are essential to reveal bias in imbalanced fraud detection data
- Significant bias persists after removing gender attribute due to correlated proxy features
- Group-wise fairness metrics reveal bias missed by global metrics due to threshold masking
- VDR parity metrics highlight the importance of transaction value in fairness evaluation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Normalized fairness metrics are necessary to reveal bias in imbalanced fraud detection data.
- **Mechanism:** In imbalanced datasets, the majority class dominates absolute metric values, masking group disparities. Normalization by the maximum value within each metric (e.g., FPR parity) scales the difference to [0,1], making small absolute differences meaningful relative to the range of possible values.
- **Core assumption:** The class imbalance is severe enough that raw metric differences are too small to be statistically meaningful.
- **Evidence anchors:**
  - [abstract]: "Certain fairness metrics expose significant bias only after normalization, highlighting the impact of class imbalance."
  - [section 3.4]: "This normalization is essential for identifying bias due to the class imbalance inherent in the transaction fraud detection problem."
  - [corpus]: Weak - no direct corpus evidence provided.
- **Break condition:** If the dataset becomes balanced, raw differences may become sufficient without normalization.

### Mechanism 2
- **Claim:** Proxy features can maintain bias even when sensitive attributes are removed (fairness through unawareness).
- **Mechanism:** Sensitive attributes like gender are often correlated with other features (e.g., transaction patterns, merchant types). When gender is removed from training, the model learns these correlated proxies, preserving the original bias in predictions.
- **Core assumption:** Correlated proxies exist and are informative for the fraud detection task.
- **Evidence anchors:**
  - [abstract]: "The fairness through unawareness approach, which involved removing sensitive attributes such as gender, does not improve bias mitigation within these datasets, likely due to the presence of correlated proxies."
  - [section 4.2]: "Significant bias was not observed... even after normalization... The Fairness Through Unawareness approach did not help mitigate bias, indicating the presence of correlated proxies for gender."
  - [corpus]: Weak - no direct corpus evidence provided.
- **Break condition:** If all proxy features are removed or neutralized, or if the correlation is weak.

### Mechanism 3
- **Claim:** Group-wise fairness metrics (TPR/VDR parity at fixed FP-ratio) reveal bias missed by global metrics.
- **Mechanism:** Global metrics use a single threshold for all groups, potentially masking disparities. Group-wise metrics set thresholds per group to achieve the same false positive rate, exposing differences in fraud detection sensitivity (TPR) or value detection (VDR) between groups.
- **Core assumption:** Different demographic groups have different score distributions for fraud/non-fraud.
- **Evidence anchors:**
  - [abstract]: "Significant bias in normalized TPR and VDR parity metrics was not observed at a global FP ratio of 5.0... but was evident at a group-wise FP ratio of 5.0."
  - [section 3.1.2]: "TPR Parity @ FP-ratio assesses the modelâ€™s ability to correctly identify fraudulent transactions for a given FP-ratio."
  - [corpus]: Weak - no direct corpus evidence provided.
- **Break condition:** If score distributions are identical across groups, group-wise metrics yield same results as global.

## Foundational Learning

- **Concept:** Class imbalance and its impact on metric interpretation.
  - Why needed here: Fraud datasets have ~5% fraud rate; standard metrics like accuracy are misleading, and bias detection requires normalization.
  - Quick check question: If a model has 95% accuracy on a 5% fraud dataset, is it performing well?
- **Concept:** Difference between global and group-wise evaluation thresholds.
  - Why needed here: Global thresholds can mask disparities; group-wise ensure equal false positive rates, revealing detection sensitivity differences.
  - Quick check question: Why might a model have the same global TPR but different group-wise TPR at fixed FP-ratio?
- **Concept:** Value Detection Rate (VDR) and its role in fairness.
  - Why needed here: Transaction value matters; missing high-value fraud for one group has greater impact than missing many low-value frauds.
  - Quick check question: How does VDR differ from TPR, and why is it important in fraud detection?

## Architecture Onboarding

- **Component map:** Data ingestion -> Feature engineering (behavioral + demographic) -> Train LightGBM -> Prediction scoring -> Fairness evaluation (metrics + normalization) -> Reporting
- **Critical path:** Feature engineering -> Model training -> Transaction-level fairness evaluation (TPR/VDR parity at global/group-wise FP-ratio) -> VDR parity analysis -> Reporting bias findings
- **Design tradeoffs:**
  - Normalization vs raw metrics: Normalization reveals bias but may over-emphasize small differences
  - Global vs group-wise thresholds: Global simpler but may mask disparities; group-wise fairer but requires separate thresholds
  - Transaction-level vs cardholder-level: Transaction-level granular but may miss cumulative impact; cardholder-level holistic but harder to implement
- **Failure signatures:**
  - No bias detected with global metrics but bias found with group-wise -> threshold masking
  - Bias persists after removing sensitive attribute -> proxy features
  - High NPV parity but low TPR parity -> model biased toward non-fraud prediction accuracy
- **First 3 experiments:**
  1. Train LightGBM on Sparkov with gender included; compute raw and normalized TPR/VDR parity at global FP-ratio=5%
  2. Repeat with gender removed; compare bias levels to test fairness through unawareness
  3. Compute group-wise TPR/VDR parity at FP-ratio=5% for each gender; compare to global results to reveal threshold masking

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do fairness metrics for transaction fraud detection models change when evaluated at the cardholder level versus the transaction level?
- Basis in paper: explicit
- Why unresolved: The paper discusses the importance of cardholder-level evaluation but does not provide empirical results comparing cardholder-level versus transaction-level fairness metrics.
- What evidence would resolve it: Empirical comparison of fairness metrics computed at both transaction and cardholder levels using real-world transaction data.

### Open Question 2
- Question: What specific correlated proxies for gender exist in the transaction fraud datasets that prevent fairness through unawareness from mitigating bias?
- Basis in paper: explicit
- Why unresolved: The paper observes that removing gender from the model does not reduce bias but does not identify or analyze the specific proxy features responsible.
- What evidence would resolve it: Feature importance analysis or correlation studies identifying which features act as proxies for gender in the datasets.

### Open Question 3
- Question: How do different weighting schemes for transaction-level fairness evaluations affect the detection of systemic bias across cardholders?
- Basis in paper: inferred
- Why unresolved: The paper mentions the need for weighted aggregate of transaction-level evaluations but does not explore different weighting schemes or their impact on fairness detection.
- What evidence would resolve it: Comparative analysis of fairness metrics under different transaction weighting schemes (e.g., by transaction value, frequency, or time) to determine optimal approaches for detecting systemic bias.

## Limitations
- Reliance on synthetic datasets raises questions about external validity in real-world scenarios
- Analysis focuses primarily on gender as sensitive attribute, potentially overlooking other demographic factors
- Effectiveness of fairness through unawareness remains uncertain given persistence of proxy feature bias

## Confidence

- **High Confidence:** The core finding that class imbalance necessitates normalized fairness metrics is well-supported by the mathematical framework and empirical results
- **Medium Confidence:** The proxy feature mechanism is logically sound but lacks direct empirical evidence beyond the observation that bias persisted after removing gender
- **Low Confidence:** Claims about VDR parity's importance in real-world deployment are theoretically justified but not empirically validated with actual fraud outcomes

## Next Checks

1. **Real-world validation:** Test the normalized metric approach on actual transaction fraud data from multiple financial institutions to assess external validity beyond synthetic datasets
2. **Proxy feature identification:** Conduct feature importance analysis to identify specific proxy features that maintain bias when gender is removed, and test whether removing these correlated features eliminates bias
3. **Multi-attribute fairness:** Extend the analysis to include race, age, and income as sensitive attributes to determine if the proxy feature problem generalizes beyond gender