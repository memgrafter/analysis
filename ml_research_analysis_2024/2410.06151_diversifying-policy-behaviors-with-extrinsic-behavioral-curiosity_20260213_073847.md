---
ver: rpa2
title: Diversifying Policy Behaviors with Extrinsic Behavioral Curiosity
arxiv_id: '2410.06151'
source_url: https://arxiv.org/abs/2410.06151
tags:
- policy
- reward
- learning
- archive
- gail
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Quality Diversity Inverse Reinforcement Learning
  (QD-IRL), a framework that combines quality-diversity optimization with inverse
  reinforcement learning to learn diverse behaviors from limited demonstrations. The
  key innovation is Extrinsic Behavioral Curiosity (EBC), which adds curiosity rewards
  when agents explore novel behaviors in the policy archive.
---

# Diversifying Policy Behaviors with Extrinsic Behavioral Curiosity

## Quick Facts
- arXiv ID: 2410.06151
- Source URL: https://arxiv.org/abs/2410.06151
- Authors: Zhenglin Wan; Xingrui Yu; David Mark Bossens; Yueming Lyu; Qing Guo; Flint Xiaofeng Fan; Yew Soon Ong; Ivor Tsang
- Reference count: 40
- Primary result: EBC improves QD-IRL performance by up to 185%, 42%, and 150% on GAIL, VAIL, and DiffAIL respectively

## Executive Summary
This paper introduces Quality Diversity Inverse Reinforcement Learning (QD-IRL) with Extrinsic Behavioral Curiosity (EBC), a framework that learns diverse behaviors from limited demonstrations. The key innovation is adding curiosity rewards when agents explore novel behaviors in the policy archive, addressing behavioral overfitting in gradient-based policy optimization. EBC enhances three different IRL methods (GAIL, VAIL, DiffAIL) and improves QD-RL performance by encouraging exploration of diverse behavioral patterns. Experiments show EBC significantly outperforms standard QD-IRL across multiple robot locomotion tasks while maintaining or exceeding expert performance.

## Method Summary
EBC adds a curiosity reward bonus when policies generate behaviors in previously unexplored regions of the behavior archive. The framework integrates with existing IRL methods (GAIL, VAIL, DiffAIL) by computing an indicator function that rewards exploration of empty archive cells. This reward is combined with the learned imitation reward and used with PPGA (Policy Performance Guided Archive) for policy optimization. The method is evaluated across MuJoCo environments using leg contact measures to define behavior space, with performance measured through QD-Score, coverage, and adaptation to new environments.

## Key Results
- EBC improves QD-IRL performance by up to 185%, 42%, and 150% on GAIL, VAIL, and DiffAIL respectively
- EBC-enhanced QD-IRL achieves 4-5x higher QD-Scores than standard QD-IRL across all tested environments
- Few-shot adaptation from Walker2d to HalfCheetah shows EBC-enabled agents outperform baseline QD-IRL in most behavioral dimensions
- EBC significantly improves gradient-arborescence-based QD-RL algorithms, demonstrating its generic applicability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EBC prevents behavioral overfitting in gradient-based policy optimization by adding curiosity rewards for exploring novel behavioral patterns.
- Mechanism: When a policy generates behaviors in previously unexplored cells of the behavior archive, EBC adds a reward bonus to each timestep of that episode. This shifts the gradient optimization towards unexplored regions while maintaining high performance in known regions.
- Core assumption: Gradient-based policy optimization tends to converge to high-reward regions and ignore behavioral diversity without external exploration incentives.
- Evidence anchors:
  - [abstract]: "EBC addresses the challenge of behavioral overfitting in gradient-based policy optimization by encouraging exploration of diverse behavioral patterns."
  - [section 3.1]: "Lemma 3.1 states that using the indicator function I(m ∈ Ae) on the empty area of the current archive (denoted as Ae) as the reward function in the standard PPO objective steadily increases the probability that the policy generates new behavior measures."

### Mechanism 2
- Claim: EBC provides a generic framework that can enhance any IRL method for QD tasks.
- Mechanism: EBC is implemented as a modular reward bonus component that can be added to existing IRL methods (GAIL, VAIL, DiffAIL) without modifying their core learning algorithms. The curiosity reward is computed based on archive occupancy and added to the learned imitation reward.
- Core assumption: The archive-based curiosity mechanism is orthogonal to the specific IRL algorithm used for learning the reward function.
- Evidence anchors:
  - [abstract]: "Our framework can potentially enhance any IRL application that requires learning diverse policies"
  - [section 3.3]: "We implement EBC on three different IRL methods, namely GAIL, VAIL, DiffAIL. These IRL methods will learn a reward function from expert demonstration, and the QD-RL algorithm PPGA (Batra et al., 2023) is then applied on this learned reward to train the policy archive."

### Mechanism 3
- Claim: EBC improves QD-RL performance by encouraging exploration beyond what true rewards provide.
- Mechanism: Even with access to true rewards, PPGA with EBC discovers more diverse and high-performing policies by using the curiosity bonus to explore empty regions of the behavior space that might contain superior solutions.
- Core assumption: The true reward function alone may not provide sufficient exploration incentives to discover the full diversity of high-performing behaviors.
- Evidence anchors:
  - [section 4.1]: "Figure 4 shows that the QD performance significantly improves after the EBC reward bonus is applied. The increase in coverage is particularly high, namely from 60% to near 100%"
  - [abstract]: "Furthermore, we demonstrate that EBC is applicable to Gradient-Arborescence-based Quality Diversity Reinforcement Learning (QD-RL) algorithms, where it substantially improves performance"

## Foundational Learning

- Concept: Quality Diversity (QD) optimization
  - Why needed here: QD algorithms find multiple high-performing solutions that are diverse in behavior space, which is essential for learning varied locomotion strategies.
  - Quick check question: What is the difference between traditional RL's goal of finding a single optimal policy and QD's goal of finding a diverse set of high-performing policies?

- Concept: Inverse Reinforcement Learning (IRL)
  - Why needed here: IRL infers reward functions from expert demonstrations, allowing agents to learn behaviors without manually designed reward functions.
  - Quick check question: How does IRL differ from behavior cloning in terms of what it learns from demonstrations?

- Concept: Policy gradient methods and their limitations
  - Why needed here: Understanding why gradient-based methods tend to converge to local optima and ignore diversity is crucial for grasping EBC's value.
  - Quick check question: Why do standard policy gradient methods struggle with maintaining behavioral diversity in QD settings?

## Architecture Onboarding

- Component map:
  - IRL module (GAIL/VAIL/DiffAIL) -> EBC module -> PPGA/QD-RL module -> Archive -> Measure functions

- Critical path:
  1. Sample episodes using current search policy
  2. Compute imitation reward using IRL module
  3. Compute EBC reward based on archive occupancy
  4. Combine rewards and use VPPO to approximate gradients
  5. Update archive with new solutions
  6. Update search distribution and policy

- Design tradeoffs:
  - Exploration vs exploitation balance controlled by EBC reward weight
  - Archive resolution vs computational cost (grid size)
  - Measure dimensionality vs ability to capture meaningful behavioral differences
  - Number of demonstrations vs diversity of learned behaviors

- Failure signatures:
  - Archive coverage plateaus at low percentage → insufficient exploration or poor measure functions
  - QD-Score improvement stalls → EBC reward weight may be too low or archive learning rate too high
  - Performance worse than expert → IRL module may not be learning meaningful reward functions

- First 3 experiments:
  1. Test EBC on a simple QD-RL task (e.g., HalfCheetah with feet contact measure) to verify it improves coverage from baseline PPGA
  2. Compare EBC-enhanced vs non-enhanced versions of a single IRL method (e.g., GAIL-EBC vs GAIL) on Walker2d
  3. Test sensitivity to EBC reward weight parameter q on Humanoid to find optimal value for new environments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EBC perform on non-MDP tasks or non-policy-gradient optimization methods?
- Basis in paper: [inferred] The authors note in Appendix K that EBC's effectiveness relies on MDP properties and policy gradient methods.
- Why unresolved: The paper focuses on QD-IRL and QD-RL settings using PPGA with policy gradients, leaving other optimization paradigms unexplored.
- What evidence would resolve it: Empirical studies applying EBC to non-MDP tasks or optimization methods like evolutionary strategies without policy gradients would demonstrate its broader applicability or limitations.

### Open Question 2
- Question: What is the optimal strategy for propagating the episode-based reward bonus across timesteps?
- Basis in paper: [explicit] The authors acknowledge in Appendix L that their current approach of uniformly distributing the EBC bonus is naive.
- Why unresolved: The paper uses a simple uniform distribution of the curiosity reward but suggests this may not be the most effective approach.
- What evidence would resolve it: Comparative experiments testing different propagation strategies (e.g., time-dependent weighting, attention-based allocation) would identify optimal credit assignment methods.

### Open Question 3
- Question: How does EBC perform with real-world human motion capture data compared to synthetic MuJoCo demonstrations?
- Basis in paper: [explicit] The authors mention in Appendix L that real-world datasets represent a compelling future application domain.
- Why unresolved: The current experiments use synthetic demonstrations generated from a policy archive, which may not reflect real-world complexities.
- What evidence would resolve it: Experiments applying EBC to imitation learning from human motion capture data would reveal performance differences and practical challenges in real-world scenarios.

## Limitations
- EBC's effectiveness depends heavily on having well-designed behavior measure functions that may not generalize to all robotic tasks
- The method requires careful tuning of the EBC reward weight parameter q, with no comprehensive sensitivity analysis provided
- Archive-based curiosity mechanism relies on appropriate behavior space discretization, but the paper doesn't extensively explore how different grid resolutions affect performance

## Confidence
- **High Confidence**: The core mechanism of EBC adding curiosity rewards for exploring novel behaviors is well-established and the empirical results showing improved QD-Score and coverage are robust across multiple baselines (GAIL, VAIL, DiffAIL)
- **Medium Confidence**: The claim that EBC works as a generic enhancement for any IRL method is supported by experiments with three different IRL algorithms, but broader validation across more diverse IRL methods would strengthen this claim
- **Medium Confidence**: The few-shot adaptation results are promising but tested on only one specific adaptation scenario (Walker2d → HalfCheetah), limiting generalizability

## Next Checks
1. Systematically vary the archive grid resolution (e.g., 5×5, 10×10, 20×20) across all environments to determine the optimal discretization and test the claim that EBC works across different archive configurations

2. Replace the current leg contact measure function with alternative behavioral measures (e.g., velocity profiles, joint angle statistics) to validate that EBC's improvements are not specific to the chosen measure function

3. Test EBC-enhanced QD-IRL on non-locomotion tasks (e.g., manipulation or navigation) to verify that the method generalizes beyond the MuJoCo quadruped environments used in the current experiments