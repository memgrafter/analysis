---
ver: rpa2
title: Self-supervised contrastive learning performs non-linear system identification
arxiv_id: '2410.14673'
source_url: https://arxiv.org/abs/2410.14673
tags:
- dynamics
- learning
- linear
- latent
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes dynamics contrastive learning (DCL) as a framework
  for non-linear system identification. The core idea is to combine self-supervised
  contrastive learning with an explicit dynamics model to recover latent dynamics
  and state spaces from observational data.
---

# Self-supervised contrastive learning performs non-linear system identification

## Quick Facts
- arXiv ID: 2410.14673
- Source URL: https://arxiv.org/abs/2410.14673
- Reference count: 40
- Key outcome: DCL achieves up to 99.5% R² for latent space recovery and 99.9% dynamics R² for switching linear systems

## Executive Summary
This paper introduces dynamics contrastive learning (DCL), a framework that combines self-supervised contrastive learning with explicit dynamics models to identify latent dynamics from observational data. The authors show that DCL can recover both latent spaces and dynamics up to affine transformations under certain conditions, providing theoretical guarantees for linear and switching linear systems. Empirical results demonstrate strong performance across linear, switching linear, and non-linear dynamics, outperforming standard contrastive learning baselines on synthetic datasets.

## Method Summary
The DCL framework uses an encoder to map observations to latent space, a dynamics model to predict future latents, and a similarity function to compare predictions with actual next-step latents. The InfoNCE loss is minimized to learn these components, with a correction term for non-uniform marginal distributions. The framework generalizes existing methods like CPC and wav2vec while offering improved interpretability and theoretical grounding for system identification tasks.

## Key Results
- DCL achieves 99.5% R² for latent space recovery on linear dynamical systems
- 99.9% dynamics R² for switching linear systems with low noise
- Outperforms standard contrastive learning baselines across all synthetic datasets tested
- Successfully identifies latents for non-linear Lorenz system despite lacking theoretical guarantees

## Why This Works (Mechanism)

### Mechanism 1
Contrastive learning with a dynamics model can identify latent dynamics up to an affine transformation when the similarity function matches the form of system noise. The InfoNCE loss is minimized when the model learns to map observations to latents via an encoder, predict latents forward in time using a dynamics model, and compare these predictions to actual next-step latents. The negative squared Euclidean norm as similarity function encodes a normal distribution assumption for system noise, which matches the data-generating process.

### Mechanism 2
The composition of mixing function g and model encoder h recovers the ground-truth latents up to a bijective affine transform Lx + b. The proof shows that under the contrastive loss minimization, the Jacobian of the composition h ◦ g must be constant, implying an affine relationship. The matrix L is constrained to be a composition of an orthogonal transform and Σ−1/2 ε.

### Mechanism 3
The estimated dynamics ˆf identify the true dynamics f up to the relation ˆf(x) = Lf(L−1(x − b)) + b. By comparing the contrastive loss conditions for both the ground truth dynamics and the estimated dynamics, the proof derives that the estimated dynamics must satisfy this specific relation to the true dynamics, accounting for the affine transformation in the latent space.

## Foundational Learning

- **Concept:** InfoNCE loss and contrastive learning framework
  - **Why needed here:** The entire framework is built on minimizing the InfoNCE loss to learn meaningful representations that capture temporal structure in the data.
  - **Quick check question:** Can you explain how the InfoNCE loss encourages the model to distinguish between positive (adjacent time steps) and negative (distant time steps) samples?

- **Concept:** System identification and dynamical systems theory
  - **Why needed here:** Understanding the problem of inferring latent dynamics f and observation function g from observational data is crucial for appreciating the contributions of this work.
  - **Quick check question:** What is the difference between linear and non-linear dynamical systems, and why is identifying non-linear systems more challenging?

- **Concept:** Kernel density estimation (KDE) and non-uniform marginal distributions
  - **Why needed here:** The theoretical guarantees require correcting for non-uniform marginal distributions in the data, which can be achieved using KDE.
  - **Quick check question:** How does KDE correct for non-uniform marginal distributions, and why is this correction necessary for the theoretical guarantees to hold?

## Architecture Onboarding

- **Component map:** Encoder h → Dynamics model ˆf → Similarity function ϕ → InfoNCE loss → Parameter updates
- **Critical path:** Encoder h → Dynamics model ˆf → Similarity function ϕ → InfoNCE loss → Parameter updates
- **Design tradeoffs:**
  - Euclidean vs. hyperspherical embeddings: Euclidean space is more practical for dynamical systems but hyperspherical embeddings are common in other contrastive learning contexts.
  - Including vs. excluding the correction term α: Theoretically required but empirically often not necessary for the considered datasets.
  - Identity vs. explicit dynamics model: Identity dynamics cannot identify non-trivial dynamics, while explicit models (LDS, ∇-SLDS) enable identification.
- **Failure signatures:**
  - Poor R² scores for latent space recovery indicate issues with the encoder or similarity function.
  - High LDS error or low dynR² scores indicate issues with the dynamics model.
  - Instability during training may indicate problems with the loss function or hyperparameters.
- **First 3 experiments:**
  1. Linear dynamical system (LDS) with low noise: Verify that DCL with LDS dynamics can recover latents and dynamics with high accuracy.
  2. Switching linear dynamical system (SLDS) with low noise: Verify that DCL with ∇-SLDS dynamics can recover latents, dynamics, and mode sequence with high accuracy.
  3. Non-linear dynamical system (Lorenz) with low noise: Verify that DCL with ∇-SLDS dynamics can recover latents with high accuracy, even if dynamics identification is challenging.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important questions emerge from the discussion:

1. How does DCL perform on real-world datasets with complex non-linear dynamics and high-dimensional observations?
2. What is the theoretical justification for DCL's performance on non-linear chaotic systems like the Lorenz attractor?
3. How does the choice of dynamics model parameterization affect DCL's ability to identify complex non-linear dynamics?

## Limitations
- Theoretical guarantees are limited to Gaussian system noise and injective mixing functions
- Performance on high-noise systems or non-injective observations remains untested
- Framework assumes access to high-quality time-series data without missing values or irregular sampling

## Confidence
- **Theoretical guarantees (High):** The proofs for linear and switching linear systems are mathematically rigorous, though they rely on idealized assumptions about noise distributions and mixing function properties.
- **Empirical results (Medium):** While R² scores are impressive on synthetic datasets, validation on real-world systems with known ground truth is limited to one example (EM sensor data).
- **Generalizability (Low):** The framework's performance on highly non-linear systems with complex noise structures (e.g., heavy-tailed distributions) is not demonstrated.

## Next Checks
1. Apply DCL to a physical system (e.g., mechanical oscillator or chemical reactor) with known dynamics to validate performance beyond synthetic data.
2. Systematically vary noise levels and distribution types (e.g., Laplace, t-distribution) to assess robustness of theoretical guarantees.
3. Compare DCL against state-of-the-art system identification methods (e.g., Koopman operator-based approaches) on identical benchmark datasets.