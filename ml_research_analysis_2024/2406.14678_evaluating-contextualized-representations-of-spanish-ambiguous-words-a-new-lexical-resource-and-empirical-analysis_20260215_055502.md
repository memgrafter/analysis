---
ver: rpa2
title: 'Evaluating Contextualized Representations of (Spanish) Ambiguous Words: A
  New Lexical Resource and Empirical Analysis'
arxiv_id: '2406.14678'
source_url: https://arxiv.org/abs/2406.14678
tags:
- sense
- human
- relatedness
- spanish
- judgments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces SAW-C, a novel dataset of human relatedness
  judgments for Spanish ambiguous words in context, and evaluates several Spanish
  BERT-based language models' ability to capture these judgments. The research finds
  that while contextualized embeddings from models like BETO correlate with human
  judgments, they fall short of inter-annotator agreement and systematically underestimate
  relatedness for same-sense pairs while overestimating it for different-sense pairs.
---

# Evaluating Contextualized Representations of (Spanish) Ambiguous Words: A New Lexical Resource and Empirical Analysis

## Quick Facts
- arXiv ID: 2406.14678
- Source URL: https://arxiv.org/abs/2406.14678
- Reference count: 33
- Spanish BERT models capture human relatedness judgments for ambiguous words but systematically underestimate same-sense relatedness and overestimate different-sense relatedness

## Executive Summary
This study introduces SAW-C, a novel dataset of human relatedness judgments for Spanish ambiguous words in context, and evaluates several Spanish BERT-based language models' ability to capture these judgments. The research finds that while contextualized embeddings from models like BETO correlate with human judgments, they fall short of inter-annotator agreement and systematically underestimate relatedness for same-sense pairs while overestimating it for different-sense pairs. Interestingly, model size does not correlate with performance in Spanish, unlike in English. The study also identifies stereotyped trajectories of disambiguation across model architectures, with different families showing distinct patterns of performance improvement across layers.

## Method Summary
The study creates SAW-C, a dataset of 812 sentence pairs containing 102 target ambiguous Spanish nouns, each embedded in minimal-pair sentences. 131 native Spanish speakers provided relatedness judgments (10+ per pair), establishing human inter-annotator agreement. The researchers then extracted contextualized embeddings from various Spanish BERT-based models (BETO, ALBERT, RoBERTa-BNE, XLM-RoBERTa) at each layer, calculated cosine distances between sentence pairs, and correlated these distances with human judgments. Mixed-effects models analyzed the data while accounting for participant and item variance.

## Key Results
- BETO contextualized embeddings correlate with human relatedness judgments (R² = 0.33) but fall short of inter-annotator agreement
- Spanish BERT models systematically underestimate relatedness for same-sense pairs and overestimate it for different-sense pairs
- Model size does not correlate with performance in Spanish (unlike English), with BETO-cased performing best despite not being the largest model
- Different model families show distinct disambiguation trajectories: ALBERT shows "rise and fall" while BERT/RoBERTa show "rise and plateau"

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextualized embeddings from Spanish BERT-based models capture some variance in human relatedness judgments for ambiguous words, but systematically underestimate relatedness for same-sense pairs and overestimate it for different-sense pairs.
- Mechanism: The models encode multiple word meanings in the initial (static) embedding, which must be "teased apart" in context, but uncued meanings persist as attractors in subsequent layers, leading to systematic errors in disambiguation.
- Core assumption: Spanish BERT models, like their English counterparts, entangle multiple meanings in initial embeddings and struggle to fully disambiguate them in context.
- Evidence anchors:
  - [abstract]: "they fall short of inter-annotator agreement and systematically underestimate relatedness for same-sense pairs while overestimating it for different-sense pairs."
  - [section 6]: "The systematic underestimation and overestimation errors observed with respect to SAME vs. DIFFERENT sense contexts... One explanation for this is that the initial (static) embedding for an ambiguous wordform might entangle all of its multiple meanings, which must then be 'teased apart' in context—but which might nevertheless persist as 'attractors' in subsequent layers."
- Break condition: If models were trained with explicit sense disambiguation objectives or used sense embeddings from the start, this mechanism might not apply.

### Mechanism 2
- Claim: Model size does not correlate with performance in Spanish BERT-based models, unlike in English.
- Mechanism: The lack of correlation suggests that architectural features, training data characteristics, or language-specific properties (like morphological richness or ambiguity patterns) may be more important than sheer parameter count for Spanish language understanding.
- Core assumption: The relationship between model scale and performance observed in English does not generalize to Spanish due to fundamental differences in language structure or modeling requirements.
- Evidence anchors:
  - [abstract]: "Interestingly, model size does not correlate with performance in Spanish, unlike in English."
  - [section 5.2]: "We found virtually no evidence that a model's size was correlated with its ability to predict human relatedness judgments... The best-performing model (BETO-cased) was not the largest tested, and larger models... performed just as poorly as models with many fewer parameters."
- Break condition: If future studies with more controlled comparisons find a clear scaling relationship in Spanish, this mechanism would need revision.

### Mechanism 3
- Claim: Different model families (ALBERT, BERT, RoBERTa) show distinct trajectories of performance improvement across layers when disambiguating ambiguous words.
- Mechanism: Architectural differences in how these models process information through their layers lead to qualitatively different patterns of disambiguation performance, with some showing rise-and-plateau trajectories and others rise-and-fall patterns.
- Core assumption: The architectural design choices (like parameter sharing in ALBERT vs. separate parameters in BERT/RoBERTa) create different computational pathways that affect how semantic information is refined through layers.
- Evidence anchors:
  - [abstract]: "model size does not correlate with performance in Spanish, unlike in English. The study also identifies stereotyped trajectories of disambiguation across model architectures, with different families showing distinct patterns of performance improvement across layers."
  - [section 5.3]: "Models varied in their number of layers... we identified two qualitatively distinct 'classes' of trajectory: a rise and plateau trajectory... and a rise and fall trajectory... the ALBERT family of models shows a rise and fall trajectory, while the BERT and RoBERTa family of models shows a rise and plateau trajectory."
- Break condition: If systematic ablation studies show that these trajectory differences disappear when controlling for other architectural variables, this mechanism would need revision.

## Foundational Learning

- Cosine similarity and distance
  - Why needed here: The study uses cosine distance between contextualized embeddings to measure semantic relatedness and sense boundaries.
  - Quick check question: If two word vectors have a cosine similarity of 0.8, what is their cosine distance?

- Linear mixed-effects models
  - Why needed here: The analysis uses mixed-effects models to account for participant-level and item-level variance when analyzing relatedness judgments.
  - Quick check question: In a mixed-effects model with random intercepts for participants and words, what does the random intercept for words capture?

- Correlation and regression analysis
  - Why needed here: The study correlates model-derived cosine distances with human relatedness judgments and uses regression to identify which layers best predict human judgments.
  - Quick check question: If R² = 0.33 between model predictions and human judgments, what percentage of variance in human judgments is explained by the model?

## Architecture Onboarding

- Component map:
  Input layer -> Transformer layers (12 for BETO) -> Output layer -> Analysis pipeline

- Critical path:
  1. Tokenize Spanish sentences according to model-specific tokenization
  2. Extract contextualized embeddings for target ambiguous words from each layer
  3. Calculate pairwise cosine distances between sentence pairs
  4. Correlate distances with human relatedness judgments
  5. Identify optimal layers for disambiguation

- Design tradeoffs:
  - Tokenization differences across models affect comparability of results
  - Using minimal pairs provides control but may not reflect naturalistic language use
  - Correlation with human judgments is informative but doesn't establish causation

- Failure signatures:
  - If cosine distances show no correlation with human judgments across all layers
  - If model performance plateaus too early (suggesting insufficient capacity for disambiguation)
  - If different tokenizations produce wildly different results across models

- First 3 experiments:
  1. Replicate BETO analysis on a single ambiguous word across all layers to verify layer-wise trajectory
  2. Compare tokenization schemes by running the same analysis with different tokenizers on identical inputs
  3. Test a simplified model (fewer layers) to establish baseline performance and identify minimum requirements for disambiguation

## Open Questions the Paper Calls Out

- Does the systematic underestimation of same-sense relatedness and overestimation of different-sense relatedness observed in Spanish BERT models generalize to other languages and model architectures?
- What specific architectural or training features enable GPT-4 Turbo to achieve near-human-level relatedness judgments while still showing systematic biases in sense boundary sensitivity?
- Why do different model families (ALBERT vs BERT/RoBERTa) show distinct performance trajectories across layers when processing ambiguous words in context?

## Limitations

- Tokenization effects: Different models use different tokenization schemes, which may affect the comparability of results across models.
- Spanish-specific findings: The observation that model size doesn't correlate with performance in Spanish BERT models may be specific to the SAW-C dataset or Spanish language characteristics.
- Minimal pairs vs. naturalistic language: The use of minimal sentence pairs provides experimental control but may not reflect how disambiguation occurs in natural language use.

## Confidence

- High confidence: The systematic errors in disambiguation (underestimating same-sense pairs, overestimating different-sense pairs) are well-supported by the data and analysis.
- Medium confidence: The mechanism explaining why models struggle with disambiguation is plausible but not definitively proven.
- Medium confidence: The finding that model size doesn't correlate with Spanish performance is interesting but based on a limited set of models.

## Next Checks

1. **Cross-linguistic validation**: Replicate the SAW-C methodology with an English dataset to test whether the size-performance correlation observed in English holds when using the same experimental design and analysis pipeline.

2. **Ablation study on tokenization**: Conduct a controlled experiment where the same Spanish sentences are processed through different tokenization schemes (e.g., word-level, subword-level, character-level) and compare how tokenization choices affect disambiguation performance across models.

3. **Temporal analysis of disambiguation**: Track the evolution of sense representations through model layers for both correctly and incorrectly disambiguated pairs to identify specific points where models fail.