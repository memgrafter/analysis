---
ver: rpa2
title: Explaining Decisions of Agents in Mixed-Motive Games
arxiv_id: '2407.15255'
source_url: https://arxiv.org/abs/2407.15255
tags:
- agents
- explanation
- agent
- each
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the challenge of explaining AI agents' decisions\
  \ in mixed-motive games where cooperation and competition coexist. The authors propose\
  \ a framework with three conceptual levels\u2014strategic, situational, and diplomatic\u2014\
  and design explanation methods accordingly."
---

# Explaining Decisions of Agents in Mixed-Motive Games

## Quick Facts
- arXiv ID: 2407.15255
- Source URL: https://arxiv.org/abs/2407.15255
- Reference count: 25
- Key outcome: Framework with three explanation levels (strategic, situational, diplomatic) improves human understanding of AI agents' decisions in mixed-motive games across three environments and agent types.

## Executive Summary
This work addresses the challenge of explaining AI agents' decisions in mixed-motive games where cooperation and competition coexist. The authors propose a framework with three conceptual levels—strategic, situational, and diplomatic—and design explanation methods accordingly. These include Strategy-Based Utility Explanations (SBUE), Shared Interests Correlation Analysis (SICA), and probable actions-based explanations. They apply these methods across three diverse environments (no-press Diplomacy, Communicate Out of Prison game, and Risk) with different agent types (neural networks, LLM-based, and heuristic policies). User studies in Diplomacy and COP show that explanations significantly improve human understanding of strategic decisions, inter-agent relationships, and diplomatic influences, with combined explanations performing best.

## Method Summary
The framework introduces three explanation methods: Strategy-Based Utility Explanations (SBUE) that present expected utility vectors for all agents conditioned on actions; Shared Interests Correlation Analysis (SICA) that quantifies alignment of interests between agent pairs using Pearson correlation coefficients; and probable actions-based explanations that show the most likely actions of other agents given an explained action. These methods are applied through Monte Carlo simulation of game outcomes, with explanations presented to users through visualization interfaces. The methods are tested across three game environments with different agent types including neural networks, LLM-based agents, and heuristic policies, with user studies measuring improvements in strategic understanding.

## Key Results
- SBUE and SICA significantly improve user understanding of strategic decisions and inter-agent relationships
- Combined explanations perform better than individual methods
- Methods generalize across different game settings (Diplomacy, COP, Risk) and agent types (neural networks, LLM-based, heuristic policies)
- SICA and SBUE are particularly effective in improving user understanding

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Strategy-Based Utility Explanations (SBUE) improve strategic understanding by presenting the expected utility vector for all agents conditioned on the explained action.
- **Mechanism:** SBUE simulates outcomes under the explained action and computes mean utilities across agents, allowing users to compare how an action affects themselves and others strategically.
- **Core assumption:** The value function V(s) and reward function R(s,s') accurately reflect the game dynamics and agent incentives.
- **Evidence anchors:**
  - [abstract] "Strategy-Based Utility Explanations (SBUE) [...] explaining how an action influences the game outcomes for all agents"
  - [section 4.2] "The expected utility vector of action ai ∈ A is a vector of size p, in which the element with index j corresponds to the expected utility of action ai for agent j"
  - [corpus] No direct corpus evidence for SBUE specifically; the claim is supported by internal algorithmic design
- **Break condition:** If V(s) is inaccurate or if agents' policies π are unknown or poorly modeled, the utility estimates will mislead rather than inform.

### Mechanism 2
- **Claim:** Shared Interests Correlation Analysis (SICA) helps users understand situational and diplomatic levels by quantifying alignment of interests between agent pairs.
- **Mechanism:** SICA computes Pearson correlation coefficients between agents' utilities over simulated outcomes, revealing tendencies toward cooperation or competition.
- **Core assumption:** Utility correlations reflect underlying strategic alignment and can be interpreted as "friendliness" or "hostility."
- **Evidence anchors:**
  - [abstract] "Shared Interests Correlation Analysis (SICA), and probable actions-based explanations"
  - [section 4.4] "The SICA value of agents i, j is the Pearson correlation coefficient of their obtained utilities"
  - [corpus] No direct corpus evidence for SICA; the method is introduced as novel in the paper
- **Break condition:** If agents' actions have complex, nonlinear impacts on each other, correlation may be weak or misleading; also fails if utilities are not comparable across agents.

### Mechanism 3
- **Claim:** Probable actions-based explanations improve strategic understanding by showing the most likely actions of other agents given the explained action.
- **Mechanism:** By simulating the game under the explained action and extracting the most frequent actions of other agents, users see concrete examples of likely outcomes.
- **Core assumption:** Greedy decoding (temperature τ=0) yields actions representative of the agents' true policy distributions.
- **Evidence anchors:**
  - [abstract] "probable actions-based explanations"
  - [section 4.3] "We extract the most commonly used action of each agent accordingly"
  - [corpus] No direct corpus evidence; limitation acknowledged in the paper (temperature τ=0 can mislead)
- **Break condition:** If the agents are LLM-based and use stochastic policies, greedy decoding may produce actions that are not truly probable, leading to incorrect explanations.

## Foundational Learning

- **Concept:** Multi-agent game theory (mixed-motive games, utility interdependence)
  - **Why needed here:** The paper's methods rely on understanding how agents' utilities are interrelated in settings where cooperation and competition coexist.
  - **Quick check question:** In a mixed-motive game with three agents, if agent A cooperates with B against C, how does this affect A's and B's utilities relative to C's?

- **Concept:** Reinforcement learning value functions and policy simulation
  - **Why needed here:** SBUE and SICA both require simulating agent behavior and estimating expected utilities or correlations over simulated outcomes.
  - **Quick check question:** If you simulate an agent's policy K times from state s and average the resulting utilities, what are you estimating?

- **Concept:** Statistical correlation and its interpretation in strategic contexts
  - **Why needed here:** SICA uses Pearson correlation to quantify "shared interests" between agents; interpreting these values correctly is crucial.
  - **Quick check question:** If two agents have a Pearson correlation of 0.9 in their utilities across simulations, what does that suggest about their strategic relationship?

## Architecture Onboarding

- **Component map:** Environment wrappers (Diplomacy, COP, Risk) -> Agent policy interfaces (neural net, LLM, heuristic) -> Simulation engine (Monte Carlo rollout with constraints) -> Explanation modules (SBUE, SICA, probable actions) -> Visualization/UI for user studies
- **Critical path:** User selects action -> System simulates K outcomes -> Compute utility vectors (SBUE) or correlations (SICA) -> Display results -> User evaluates understanding
- **Design tradeoffs:**
  - Sampling depth vs. runtime (SBUE/SICA need many samples for convergence)
  - Greedy decoding vs. probabilistic decoding for LLM actions
  - Correlation interpretation vs. causal inference
- **Failure signatures:**
  - High variance in utility estimates -> Increase K or adjust discount factor
  - Low MAP@K for SICA vs human annotations -> Re-examine value function or policy modeling
  - User confusion on explanation format -> Simplify visualization or combine methods
- **First 3 experiments:**
  1. Run SBUE with K=100 samples on a mid-game Diplomacy state; compare RMSE vs. ground truth.
  2. Compute SICA on the same state; verify convergence by increasing K and checking cosine similarity.
  3. Apply both methods to COP with LLM agents; test whether explanations influence agent decision-making.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the effect of increasing the depth parameter d in the SICA and SBUE estimation algorithms on the accuracy and computational cost in different game states?
- Basis in paper: [explicit] The paper discusses convergence of SICA and SBUE in Diplomacy but does not explore the effect of varying depth parameter d.
- Why unresolved: The paper only mentions that depth > 1 is useful for heuristic value functions in sequential games like Risk, but does not provide experimental results for varying depth in different game states.
- What evidence would resolve it: Conducting experiments with varying depth parameters in different game states and measuring both accuracy (e.g., correlation with human annotations) and computational cost would provide evidence for the optimal depth.

### Open Question 2
- Question: How do the proposed explanation methods perform when applied to agents with heterogeneous policies in more complex mixed-motive games with explicit communication?
- Basis in paper: [inferred] The paper mentions that the methods were applied to three different environments with different agent types, but does not explore complex games with explicit communication beyond the COP game.
- Why unresolved: The paper focuses on games with implicit communication (Diplomacy, Risk) and one game with explicit communication (COP), but does not explore more complex games with explicit communication.
- What evidence would resolve it: Applying the methods to more complex mixed-motive games with explicit communication and heterogeneous agent policies, and evaluating their performance through user studies or other metrics, would provide evidence for their effectiveness in such scenarios.

### Open Question 3
- Question: What are the limitations of the probable actions-based explanations when applied to LLM agents, and how can these limitations be addressed?
- Basis in paper: [explicit] The paper discusses a limitation of the probable actions-based explanations when applied to LLM agents, stating that using a temperature τ = 0 can lead to misleading explanations.
- Why unresolved: The paper identifies a limitation but does not provide a solution or explore alternative approaches to address it.
- What evidence would resolve it: Investigating alternative approaches to generate probable actions for LLM agents, such as using higher temperature values or different decoding strategies, and evaluating their effectiveness through experiments, would provide evidence for addressing this limitation.

## Limitations

- User study methodology relies on small participant pools (8-24 users) and subjective self-reported understanding rather than objective performance measures
- Technical constraints include Monte Carlo simulation convergence issues and the use of greedy decoding for LLM agents which can produce misleading explanations
- SICA's assumption that Pearson correlation captures strategic alignment may not hold in complex nonlinear strategic environments

## Confidence

**High Confidence:** The technical framework of three explanation levels (strategic, situational, diplomatic) is well-defined and the methods (SBUE, SICA, probable actions) are algorithmically sound given their assumptions. The user study methodology is appropriately designed and executed.

**Medium Confidence:** The claim that combined explanations perform best and that SICA/SBUE are particularly effective is supported by user study data, but the small sample sizes and limited game variety constrain generalizability. The assertion that methods generalize across agent types is demonstrated but not thoroughly validated across diverse settings.

**Low Confidence:** The paper's claims about the mechanisms underlying user understanding improvements are speculative. The user studies measure self-reported understanding rather than actual behavioral changes or improved decision-making. The paper does not address potential cognitive biases in how users interpret the explanations.

## Next Checks

1. **Convergence Analysis:** Systematically vary the sampling depth K in SBUE and SICA methods across different game states and measure convergence rates using RMSE and MAP@K metrics. Determine minimum K values required for stable explanations and analyze how this scales with game complexity.

2. **Behavioral Validation Study:** Design a controlled experiment where users make actual strategic decisions with and without explanations, measuring objective performance metrics (win rates, utility achieved) rather than subjective understanding. Compare decision quality before and after explanation exposure.

3. **Cross-Game Generalization Test:** Apply the explanation framework to at least two additional mixed-motive games beyond the three studied (e.g., a public goods game and a bargaining scenario). Evaluate whether the same explanation methods remain effective and identify any game-specific adaptations needed.