---
ver: rpa2
title: 'KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large
  Language Models'
arxiv_id: '2407.14239'
source_url: https://arxiv.org/abs/2407.14239
tags:
- driving
- agents
- agent
- module
- scenarios
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The KoMA framework introduces a knowledge-driven multi-agent approach
  to autonomous driving using large language models (LLMs). It addresses the limitations
  of single-agent systems by enabling multiple LLM-driven agents to collaborate, share
  experiences, and reason collectively in complex driving scenarios.
---

# KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large Language Models

## Quick Facts
- arXiv ID: 2407.14239
- Source URL: https://arxiv.org/abs/2407.14239
- Authors: Kemou Jiang; Xuan Cai; Zhiyong Cui; Aoyong Li; Yilong Ren; Haiyang Yu; Hao Yang; Daocheng Fu; Licheng Wen; Pinlong Cai
- Reference count: 40
- Key outcome: KoMA achieves 70% success rate in ramp merging after 40 training rounds, outperforming MARL's 65% after 40,000 rounds

## Executive Summary
KoMA introduces a knowledge-driven multi-agent framework for autonomous driving that leverages large language models (LLMs) to address the limitations of single-agent systems. The framework enables multiple LLM-driven agents to collaborate, share experiences, and reason collectively in complex driving scenarios. By incorporating modules for multi-agent interaction, multi-step planning, shared memory, and ranking-based reflection, KoMA demonstrates superior performance in ramp merging tasks and strong generalization capabilities across different driving scenarios.

## Method Summary
The KoMA framework employs multiple LLM agents to handle complex driving scenarios through a knowledge-driven approach. It uses GPT-4 for reasoning and Chroma vector database for memory storage. The framework operates in a highway simulation environment where agents process text descriptions of scenarios, infer intentions of surrounding vehicles, plan actions through goal-plan-action methodology, and share successful experiences. Training involves 40 rounds where agents learn from corrected decisions and accumulate collective intelligence in the shared memory.

## Key Results
- Achieved 70% success rate in ramp merging tasks after 40 training rounds
- Demonstrated generalization to scenarios with different lane configurations and roundabouts without additional training
- Outperformed traditional MARL methods (65% success after 40,000 rounds)

## Why This Works (Mechanism)

### Mechanism 1: Intention Inference Through Behavior Analysis
LLM agents infer intentions of surrounding vehicles by analyzing their behavior history and current state from textual descriptions, mimicking human-like reasoning. This works because LLMs can effectively reason about vehicle intentions from textual descriptions and behavior patterns. Break condition: If LLM cannot accurately interpret vehicle behavior patterns from textual descriptions or if patterns are too complex.

### Mechanism 2: Multi-Step Planning for Coherent Decision-Making
The framework breaks down complex driving tasks into goal-plan-action layers, ensuring consistent long-term decisions across multiple frames. This works because the three-tiered reasoning process can effectively handle complex, time-varying scenarios. Break condition: If scenarios change too rapidly for plans to remain relevant or if LLM cannot break down complex tasks effectively.

### Mechanism 3: Shared Memory for Collective Intelligence
Multiple agents share experiences through a vector database, enhancing learning efficiency and decision-making quality. This works because sharing experiences among agents can significantly improve learning compared to individual learning. Break condition: If shared experiences are not relevant to current scenarios or memory becomes too large to efficiently search.

## Foundational Learning

- **Large Language Models (LLMs)**: Core reasoning engine for autonomous driving agents
  - Why needed here: Form the core reasoning engine for the autonomous driving agents in KoMA
  - Quick check question: What are the key differences between GPT-3.5 and GPT-4 that might affect their performance in the KoMA framework?

- **Multi-agent systems**: Multiple LLM agents working together to solve complex driving tasks
  - Why needed here: KoMA relies on multiple LLM agents working together to solve complex driving tasks
  - Quick check question: How does KoMA's approach to multi-agent interaction differ from traditional multi-agent reinforcement learning methods?

- **Vector databases**: Store and retrieve similar driving experiences efficiently
  - Why needed here: The shared memory module uses a vector database to store and retrieve similar driving experiences
  - Quick check question: What are the advantages of using a vector database over a traditional relational database for storing driving experiences?

## Architecture Onboarding

- **Component map**: Environment module → Multi-agent interaction module → Multi-step planning module → Action decision → Environment execution → Ranking-based reflection → Shared memory update
- **Critical path**: Scenario description → Multi-agent interaction → Multi-step planning → Action decision → Environment execution → Ranking-based reflection → Shared memory update
- **Design tradeoffs**: Using LLMs provides human-like reasoning but may be computationally expensive; shared memory enables faster learning but requires careful management of experience relevance; multi-step planning ensures coherent decisions but may struggle with rapidly changing scenarios
- **Failure signatures**: Poor intention inference leading to unsafe decisions; plans becoming irrelevant due to rapid scenario changes; shared memory retrieval returning irrelevant or outdated experiences
- **First 3 experiments**:
  1. Test multi-agent interaction module in simple scenario with one LLM agent and one rule-based agent to verify intention inference capabilities
  2. Evaluate multi-step planning module in ramp merging scenario with varying complexity to assess task handling
  3. Measure impact of shared memory module by comparing learning efficiency with and without access to shared experiences

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of KoMA change with different sizes and architectures of LLM agents, and what is the optimal configuration for balancing computational cost and decision-making accuracy? The paper mentions testing with various LLM models but does not extensively compare their performance or analyze the impact of model size and architecture.

### Open Question 2
How does KoMA handle edge cases and rare driving scenarios that are not well-represented in the training data, and what strategies are employed to ensure robustness in such situations? The paper focuses on general scenarios performance and does not address handling of edge cases or rare events.

### Open Question 3
How does the shared memory module scale with an increasing number of agents, and what are the potential bottlenecks or limitations in terms of memory capacity and retrieval efficiency? While the paper discusses shared memory benefits, it does not provide details on scalability or potential limitations as agent numbers increase.

## Limitations

- Performance relies heavily on LLM quality and may struggle with highly dynamic scenarios requiring rapid decision-making
- Shared memory assumes past successful experiences remain relevant to new scenarios, which may not hold in highly variable environments
- Computational expense of using LLMs for real-time autonomous driving applications

## Confidence

- **High Confidence**: Framework architectural design and core components are well-defined and technically sound, supported by experimental results
- **Medium Confidence**: Effectiveness of LLM-driven agents and shared memory benefits are supported by experimental results, but generalizability to diverse real-world scenarios may be limited
- **Low Confidence**: Insufficient details on exact prompt templates, reasoning chains, and threshold values used in framework

## Next Checks

1. **Intention Inference Validation**: Test multi-agent interaction module in controlled environment with varying scenario complexity to assess ability to accurately infer vehicle intentions from textual descriptions

2. **Shared Memory Relevance**: Evaluate effectiveness of shared memory module by measuring relevance and usefulness of retrieved experiences in different driving scenarios, and assess impact of memory size on retrieval efficiency

3. **Real-World Generalization**: Validate framework performance in diverse real-world driving scenarios including different road types, weather conditions, and traffic densities to assess generalizability and robustness