---
ver: rpa2
title: Optimal Projections for Classification with Naive Bayes
arxiv_id: '2409.05635'
source_url: https://arxiv.org/abs/2409.05635
tags:
- data
- class
- which
- bayes
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Optimal Projections for Naive Bayes (OPNB),
  a projection pursuit approach to enhance Naive Bayes classification by finding an
  optimal linear basis for density factorisation. The method maximises a multinomial
  likelihood using Naive Bayes factorisation on projected data, allowing dimension
  reduction and improved decision boundaries.
---

# Optimal Projections for Classification with Naive Bayes

## Quick Facts
- arXiv ID: 2409.05635
- Source URL: https://arxiv.org/abs/2409.05635
- Authors: David P. Hofmeyr; Francois Kamper; Michail C. Melonas
- Reference count: 7
- Outperforms all probabilistic baselines and is highly competitive with SVMs on 162 benchmark datasets

## Executive Summary
This paper introduces Optimal Projections for Naive Bayes (OPNB), a projection pursuit approach that enhances Naive Bayes classification by finding an optimal linear basis for density factorisation. The method maximises a multinomial likelihood using Naive Bayes factorisation on projected data, enabling dimension reduction and improved decision boundaries. Experiments on 162 benchmark datasets show OPNB substantially outperforms other probabilistic models and is highly competitive with SVMs, achieving the lowest standardised error overall.

## Method Summary
OPNB is a projection pursuit method that seeks an optimal linear projection matrix V such that the multinomial likelihood on the projected data, using Naive Bayes factorisation in the projected space, is maximised. The optimisation objective balances class-conditional independence and inter-class separation by combining two terms: within-class entropy minimisation and between-class mixture penalisation. The method uses univariate kernel density estimation on projected dimensions, allowing efficient computation while maintaining flexibility for mixed continuous-categorical data. Implementation involves preprocessing data with scaling and one-hot encoding, iteratively updating V via gradient ascent with column-wise normalisation, and classifying using posterior probabilities from estimated densities.

## Key Results
- OPNB achieves the lowest standardised error overall, outperforming SVM on ~40% of datasets
- OPNB outperforms each other method in at least 60% of cases
- Particularly excels with complex class boundaries and mixed continuous-categorical data
- Performance can degrade with highly imbalanced classes or excessive dimensionality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OPNB improves Naive Bayes by rotating the basis to a direction where class-conditional densities factorise better, improving decision boundaries.
- Mechanism: Projection pursuit searches for a linear projection matrix V such that the multinomial likelihood on the projected data, using Naive Bayes factorisation in the projected space, is maximised.
- Core assumption: There exists a linear transformation of the data such that the Naive Bayes assumption holds more closely, leading to improved classification accuracy.
- Evidence anchors: Abstract states OPNB finds optimal linear projection for classification; section explains seeking optimal basis for factorisation rather than standard coordinate basis.

### Mechanism 2
- Claim: The optimisation objective balances class-conditional independence and inter-class separation by combining two terms: within-class entropy minimisation and between-class mixture penalisation.
- Mechanism: First term encourages low dependence among projected features within each class, while second term penalises projections where all classes overlap.
- Core assumption: Combination of within-class independence and mixture separation leads to projections that both fit each class well and distinguish between classes.
- Evidence anchors: Section describes objective as weighted sum of ICA objectives for each class plus mixture log-likelihood term.

### Mechanism 3
- Claim: Using univariate kernel density estimation on projected dimensions allows efficient computation while maintaining flexibility for mixed continuous-categorical data.
- Mechanism: After projection, each dimension is treated as univariate, enabling fast kernel density estimation while preserving computational advantages.
- Core assumption: Univariate density estimation in projected space is computationally tractable and sufficiently accurate for classification.
- Evidence anchors: Section explains factorisation allows efficient univariate kernel density estimation; references fast kernel smoothing algorithm.

## Foundational Learning

- Concept: Naive Bayes classification and its factorisation assumption
  - Why needed here: OPNB builds directly on Naive Bayes by seeking a better basis for the factorisation
  - Quick check question: What is the key assumption of Naive Bayes regarding the class-conditional densities?

- Concept: Projection pursuit and optimisation of projection matrices
  - Why needed here: OPNB is fundamentally a projection pursuit method; optimisation of V is central to the algorithm
  - Quick check question: How does the optimisation of the projection matrix differ from standard linear discriminant analysis?

- Concept: Kernel density estimation and its computational implications
  - Why needed here: OPNB uses kernel density estimation on projected data
  - Quick check question: Why is univariate KDE computationally more efficient than multivariate KDE in this context?

## Architecture Onboarding

- Component map: Data preprocessing -> Projection optimisation -> Density estimation -> Classification -> Hyperparameter tuning

- Critical path:
  1. Preprocess data (scaling, encoding, subsampling)
  2. Initialise projection matrix V using LDA discriminant vectors
  3. Optimise V by maximising multinomial likelihood with Naive Bayes factorisation
  4. Estimate univariate densities on projected data
  5. Classify new points using posterior probabilities

- Design tradeoffs:
  - Projection dimension (p′): Higher p′ increases flexibility but also computational cost and risk of overfitting
  - Bandwidth selection: Too small → high variance; too large → oversmoothing and loss of discriminative power
  - Projection initialisation: Good initialisation (e.g., LDA) can speed convergence; poor initialisation may lead to local optima
  - Mixed data handling: One-hot encoding + univariate KDE is simple but may not capture all dependencies

- Failure signatures:
  - High variance in projections or density estimates (often due to small sample size or high p′)
  - Poor separation of classes in projection plots (may indicate local optima or inappropriate bandwidth)
  - Degraded performance with highly imbalanced classes
  - Excessive computation time for very high-dimensional data

- First 3 experiments:
  1. Run OPNB on a simple 2D dataset with rotated class-conditional densities and visualise the projection and decision boundaries
  2. Compare OPNB with standard Naive Bayes and LDA on a benchmark dataset, focusing on classification accuracy and projection plots
  3. Vary the projection dimension p′ and bandwidth, observing the impact on accuracy and computation time

## Open Questions the Paper Calls Out
None explicitly stated in the provided material.

## Limitations
- Performance can degrade with highly imbalanced classes or excessive dimensionality
- Does not fully specify implementation details of fast kernel smoothing algorithm
- Performance on extremely high-dimensional data (p > 300) not explored

## Confidence

- High: Core mathematical formulation and experimental methodology are clearly specified and reproducible
- Medium: Mechanism by which OPNB improves classification is theoretically sound but relies on assumptions about useful linear projections
- Low: Specific impact on highly imbalanced datasets is not fully quantified

## Next Checks
1. Implement OPNB on a synthetic dataset with known optimal projections to verify that the method recovers the expected basis
2. Conduct a sensitivity analysis on the projection dimension p' and bandwidth multiplier γ to quantify their impact on performance
3. Test OPNB on a highly imbalanced dataset to assess the extent of performance degradation and explore potential mitigation strategies