---
ver: rpa2
title: 'COSMOS: Cross-Modality Self-Distillation for Vision Language Pre-training'
arxiv_id: '2412.01814'
source_url: https://arxiv.org/abs/2412.01814
tags:
- cosmos
- image
- text
- clip
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: COSMOS addresses feature suppression in vision-language models
  by introducing a cross-modality self-distillation framework. The method employs
  a novel text-cropping strategy and cross-attention module to simultaneously enhance
  both image and text encoder representations.
---

# COSMOS: Cross-Modality Self-Distillation for Vision Language Pre-training

## Quick Facts
- arXiv ID: 2412.01814
- Source URL: https://arxiv.org/abs/2412.01814
- Authors: Sanghwan Kim; Rui Xiao; Mariana-Iuliana Georgescu; Stephan Alaniz; Zeynep Akata
- Reference count: 40
- Primary result: Achieves SOTA zero-shot image-text retrieval (92.9% R@1 on Flickr30K) and classification (57.6% ImageNet accuracy) with only 30M image-text pairs

## Executive Summary
COSMOS introduces a cross-modality self-distillation framework that addresses feature suppression in vision-language models by learning both local and global representations across image and text modalities. The method employs a novel text-cropping strategy and cross-attention module to simultaneously enhance both image and text encoder representations. By training on only 30 million image-text pairs—compared to billions for CLIP-based models—COSMOS achieves state-of-the-art performance on zero-shot image-text retrieval, classification, and semantic segmentation tasks.

## Method Summary
COSMOS builds upon CLIP's contrastive learning framework by introducing a student-teacher architecture where the student learns from both global and local views of images and texts, while the teacher receives only global views. The key innovations include a text-cropping strategy that creates local and global text representations, and a cross-attention module that enables bidirectional cross-modal grounding. The model is trained with a combined loss function that includes both the standard contrastive loss and a cross-modality self-distillation loss, where the student must predict the teacher's global representations from its own local views.

## Key Results
- Achieves 92.9% R@1 on zero-shot image-text retrieval for Flickr30K
- Obtains 57.6% top-1 accuracy on zero-shot ImageNet classification
- Reaches 20.0% mIoU average on zero-shot semantic segmentation across multiple datasets
- Outperforms CLIP-based models trained on billions of samples while using only 30M image-text pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modality self-distillation addresses feature suppression by forcing the model to learn both local and global representations across image and text modalities.
- Mechanism: The student model receives both global and local views of images and texts, while the teacher model receives only global views. The student must predict the teacher's richer global representations from its own local views, creating a distillation signal that encourages attention to details beyond the dominant objects.
- Core assumption: Global representations from the teacher contain information that local student views cannot easily infer without attending to additional contextual details.
- Evidence anchors:
  - [abstract] "COSMOS addresses feature suppression in vision-language models by introducing a cross-modality self-distillation framework"
  - [section 3.1] "we create global and local views of images and texts (i.e., multi-modal augmentations), which are essential for self-distillation in VLMs"
  - [corpus] Weak - no direct evidence about cross-modality distillation effectiveness in this corpus
- Break condition: If the teacher's global representations do not contain substantially more information than the student's local views, or if the cross-attention module fails to extract meaningful cross-modal information.

### Mechanism 2
- Claim: The text-cropping strategy enables local-to-global text representation learning, improving both image and text encoder representations simultaneously.
- Mechanism: By randomly sampling 1-5 sentences for global text crops and single sentences for local text crops, the model learns to match detailed local text descriptions with corresponding image regions while maintaining global context. This forces the text encoder to develop both fine-grained and holistic representations.
- Core assumption: Random sampling of text segments creates meaningful local and global correspondences that the model can learn from.
- Evidence anchors:
  - [section 3.1] "We randomly sample from the long synthetic caption datasets to construct sentences of varying length"
  - [section 3.3] "LCOSMOS is obtained between global crops given to the teacher and all crops given to the student"
  - [corpus] Weak - corpus doesn't directly address text-cropping strategy effectiveness
- Break condition: If the random text sampling doesn't create coherent local/global correspondences, or if the model fails to learn meaningful alignments between cropped text and image regions.

### Mechanism 3
- Claim: The cross-attention module enables comprehensive cross-modal representation learning by conditioning each modality on the other.
- Mechanism: The cross-attention module uses image tokens as keys/values and text tokens as queries (and vice versa) to create cross-modal embeddings. This allows the model to attend to relevant information in one modality based on the context from the other modality, improving grounding between visual and textual features.
- Core assumption: Cross-attention between modalities can effectively capture meaningful correspondences between image regions and text descriptions.
- Evidence anchors:
  - [section 3.2] "our cross-attention module focuses on different objects from the entire image, which are highlighted in both images and captions based on the attention weights"
  - [section 3.2] "CT_θ receives the [cls] token as a query and text tokens (txt-tok) as keys and values"
  - [corpus] Weak - corpus doesn't provide specific evidence about cross-attention module effectiveness
- Break condition: If the cross-attention mechanism fails to extract meaningful cross-modal correspondences, or if the attention weights don't align with semantically relevant regions.

## Foundational Learning

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: COSMOS builds upon CLIP's contrastive learning framework but extends it with self-distillation. Understanding InfoNCE loss and symmetric contrastive objectives is essential for grasping how COSMOS maintains alignment between image and text representations while adding the distillation component.
  - Quick check question: How does symmetric InfoNCE loss differ from standard InfoNCE, and why is it used in COSMOS?

- Concept: Self-distillation and teacher-student frameworks
  - Why needed here: COSMOS uses a teacher-student architecture where the student learns from the teacher through cross-modality distillation. Understanding how EMA updates work and how distillation losses function is crucial for implementing and debugging the model.
  - Quick check question: What is the role of the exponential moving average (EMA) in updating teacher parameters, and how does it differ from standard back-propagation?

- Concept: Cross-attention mechanisms in transformers
  - Why needed here: The cross-attention module is a core component that enables cross-modal learning. Understanding how cross-attention works in transformer architectures, including query/key/value interactions, is essential for implementing and modifying this component.
  - Quick check question: How does the cross-attention mechanism in COSMOS differ from standard self-attention, and what information does it help extract?

## Architecture Onboarding

- Component map:
  Vision encoder (ViT-B/16) -> Cross-attention module (CI_θ) -> Student model -> Teacher model (EMA-updated)
  Text encoder -> Cross-attention module (CT_θ) -> Student model -> Teacher model (EMA-updated)
  Augmentation pipeline -> Global/local crops for both modalities

- Critical path:
  1. Input image-text pair → augmentation (global/local crops)
  2. Student processes all crops → extract embeddings
  3. Cross-attention module generates cross-modal embeddings
  4. Teacher processes global crops only → extract target embeddings
  5. Compute CLIP loss (student only) + COSMOS loss (student vs teacher)
  6. Backpropagate through student only
  7. Update teacher via EMA

- Design tradeoffs:
  - Using only global crops for teacher vs including local crops - chosen to maintain teacher's global perspective
  - Independent text/image cropping vs aligned cropping - chosen to allow more diverse learning signals
  - Separate cross-attention modules for image→text and text→image - provides bidirectional cross-modal grounding

- Failure signatures:
  - Poor retrieval/classification performance despite good training loss - suggests feature suppression not properly addressed
  - Teacher and student outputs becoming too similar - indicates insufficient learning signal from distillation
  - Cross-attention weights not focusing on semantically relevant regions - suggests attention mechanism not working properly

- First 3 experiments:
  1. Compare baseline CLIP vs COSMOS with only image self-distillation (no text cropping, no cross-attention) to verify self-distillation benefits
  2. Test different numbers of local crops (0, 2, 4, 6) to find optimal trade-off between performance and computational cost
  3. Validate cross-attention effectiveness by visualizing attention maps and checking if they align with semantically relevant regions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the text-cropping strategy's performance vary when applied to non-synthetic caption datasets with different characteristics (e.g., shorter captions, different writing styles)?
- Basis in paper: [explicit] The paper mentions using long synthetic captions generated by MLLMs for text cropping, but doesn't explore performance on other caption types.
- Why unresolved: The paper only evaluates COSMOS using synthetic captions from DreamLIP [91], limiting generalizability to other caption sources.
- What evidence would resolve it: Comparative experiments training COSMOS on various caption datasets (Flickr captions, human-written descriptions) while keeping other components constant would reveal the strategy's robustness across caption types.

### Open Question 2
- Question: What is the precise mechanism by which the cross-attention module alleviates feature suppression compared to local-to-global alignment methods like SILC?
- Basis in paper: [explicit] The paper states that COSMOS alleviates feature suppression through "cross-modality embeddings and local-to-global matching" but doesn't provide detailed ablation studies isolating the cross-attention contribution.
- Why unresolved: While the paper shows COSMOS outperforms SILC, it doesn't separate the effects of the cross-attention module from the overall distillation framework.
- What evidence would resolve it: Ablation studies removing the cross-attention module while maintaining other COSMOS components, or comparing COSMOS with and without cross-attention on feature suppression metrics, would clarify its specific contribution.

### Open Question 3
- Question: How does the cross-attention module's attention distribution change during training, and what does this reveal about the model's learning dynamics?
- Basis in paper: [explicit] The paper visualizes attention maps at inference time but doesn't track their evolution during training.
- Why unresolved: Static attention visualizations provide limited insight into how the model develops its cross-modal understanding over time.
- What evidence would resolve it: Longitudinal analysis tracking attention weight distributions across training epochs, potentially revealing stages where the model transitions from local to global focus or vice versa, would illuminate the learning process.

## Limitations

- The cross-attention modules lack detailed architectural specifications and ablation studies demonstrating their individual contribution to performance gains.
- The text-cropping strategy relies on random sampling without justification for why this approach creates meaningful local/global correspondences.
- Claims about addressing feature suppression are primarily supported by downstream task performance rather than direct analysis of feature representations.
- Comparison against models trained on vastly different dataset sizes (billion-scale vs. 30 million) makes it difficult to isolate architectural innovations from scale effects.

## Confidence

- High confidence in experimental methodology and reported benchmark results, as the paper provides clear training procedures, hyperparameter settings, and evaluation protocols.
- Medium confidence in feature suppression claims, as evidence is primarily indirect through downstream task performance rather than direct feature analysis.
- Medium confidence in effectiveness of individual components (text-cropping, cross-attention) due to limited ablation studies.
- Low confidence in scalability claims, as comparison is made against models trained on vastly different dataset sizes without controlled experiments varying only architectural components.

## Next Checks

1. Conduct ablation studies isolating the contribution of the text-cropping strategy by comparing against random sentence sampling without the local/global distinction, and against aligned versus independent cropping approaches.

2. Perform feature analysis to directly measure feature suppression by examining the activation distributions across different object categories and comparing the attention patterns in baseline CLIP versus COSMOS models.

3. Replicate the results using the same dataset size (30 million samples) for both COSMOS and baseline CLIP to isolate the architectural improvements from scale effects, and test the approach on smaller datasets to assess data efficiency.