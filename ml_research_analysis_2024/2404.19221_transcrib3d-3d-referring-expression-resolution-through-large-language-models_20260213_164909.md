---
ver: rpa2
title: 'Transcrib3D: 3D Referring Expression Resolution through Large Language Models'
arxiv_id: '2404.19221'
source_url: https://arxiv.org/abs/2404.19221
tags:
- transcrib3d
- language
- referring
- reasoning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Transcrib3D introduces a method for resolving 3D referring expressions
  using large language models by converting detected 3D object information into text,
  filtering relevant objects, and applying iterative code generation and reasoning
  via an LLM. This approach bypasses the need for multi-modal joint representations
  and leverages existing 3D detection and LLM reasoning advances.
---

# Transcrib3D: 3D Referring Expression Resolution through Large Language Models

## Quick Facts
- arXiv ID: 2404.19221
- Source URL: https://arxiv.org/abs/2404.19221
- Reference count: 40
- Achieves state-of-the-art accuracy on ReferIt3D benchmarks by converting 3D object detections into text and using LLM reasoning

## Executive Summary
Transcrib3D introduces a novel approach for resolving 3D referring expressions using large language models by converting detected 3D object information into text, filtering relevant objects, and applying iterative code generation and reasoning via an LLM. This approach bypasses the need for multi-modal joint representations and leverages existing 3D detection and LLM reasoning advances. The method achieves state-of-the-art accuracy on ReferIt3D benchmarks, significantly outperforming prior multi-modal baselines, and demonstrates successful real-world deployment on robot pick-and-place tasks.

## Method Summary
Transcrib3D resolves 3D referring expressions by first converting 3D object detection results (category, location, size, color) into a textual scene description. The method filters out irrelevant objects based on the referring expression, then uses an LLM with iterative code generation and reasoning to identify the referred object. A self-correction fine-tuning strategy trains smaller models using the LLM's own reasoned corrections, achieving performance close to large models. The approach avoids the need for multi-modal joint representations by using text as the unifying medium between 3D scene data and LLM reasoning.

## Key Results
- Achieves state-of-the-art accuracy on ReferIt3D benchmarks, outperforming prior multi-modal baselines
- Reaches top accuracy on ScanRefer both with detected and ground-truth bounding boxes
- Demonstrates successful execution of complex pick-and-place tasks in real robot experiments
- Self-correction fine-tuning enables smaller models to achieve performance close to large models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transcrib3D sidesteps the need for multi-modal joint representations by converting 3D scene data into text.
- Mechanism: 3D object detection results (category, location, size, color) are transcribed into a textual scene description, which is then processed by an LLM using standard text reasoning techniques.
- Core assumption: An LLM can perform accurate spatial and semantic reasoning when given a well-structured text representation of the 3D scene.
- Break condition: If the LLM cannot accurately interpret spatial relations or semantic constraints from the text representation, or if the text transcription omits critical 3D information needed for reasoning.

### Mechanism 2
- Claim: Iterative code generation and reasoning via LLM improves the model's ability to perform complex compositional spatial reasoning.
- Mechanism: When the LLM encounters a reasoning task requiring arithmetic or complex spatial calculations, it generates Python code, executes it locally, and feeds the results back into the LLM for further reasoning. This process iterates until the LLM believes the reasoning is complete.
- Core assumption: LLMs can generate correct Python code for spatial reasoning tasks and can effectively use the output of that code to refine their reasoning.
- Break condition: If the LLM generates incorrect code, or if the code execution results are not properly interpreted by the LLM for further reasoning.

### Mechanism 3
- Claim: Self-correction fine-tuning allows smaller models to achieve performance close to large models by learning from their own mistakes.
- Mechanism: Incorrect predictions from the initial zero-shot inference are used to prompt the LLM to reflect on its mistakes and generate corrected reasoning. This corrected data is then used to fine-tune a smaller model.
- Core assumption: The LLM can accurately identify its own mistakes when prompted with the correct answer and can generate effective reasoning for the correct answer.
- Break condition: If the LLM cannot accurately identify its mistakes or generate effective corrected reasoning, or if the fine-tuning process does not improve the performance of the smaller model.

## Foundational Learning

- Concept: 3D Object Detection
  - Why needed here: Transcrib3D relies on accurate 3D object detection to generate the initial text representation of the scene.
  - Quick check question: What are the key outputs of a 3D object detector that are used by Transcrib3D? (Answer: Object category, center location, spatial extent (bounding box), and color.)

- Concept: Spatial Reasoning
  - Why needed here: Transcrib3D uses LLMs to perform complex spatial reasoning tasks, such as understanding relative positions, distances, and orientations of objects in the 3D scene.
  - Quick check question: What is an example of a spatial reasoning task that Transcrib3D might need to perform? (Answer: Determining which object is "between" two other objects.)

- Concept: Text-based Reasoning with LLMs
  - Why needed here: Transcrib3D uses LLMs to process the text representation of the 3D scene and perform the final reasoning to identify the referred object.
  - Quick check question: What is the key advantage of using text as the unifying medium for 3D referring expression resolution? (Answer: It avoids the need to learn shared representations connecting multi-modal inputs, which would require massive amounts of annotated 3D data.)

## Architecture Onboarding

- Component map: 3D Object Detector -> Transcriber -> Object Filter -> LLM with Code Interpreter -> Referred Object
- Critical path: 3D Object Detector -> Transcriber -> Object Filter -> LLM with Code Interpreter -> Referred Object
- Design tradeoffs:
  - Using text as the unifying medium simplifies the model architecture but relies on the LLM's ability to accurately interpret spatial relations from text.
  - Iterative code generation and reasoning improves the LLM's ability to perform complex spatial reasoning but adds computational overhead.
  - Self-correction fine-tuning allows smaller models to achieve high performance but requires a large initial dataset of zero-shot predictions.
- Failure signatures:
  - Incorrect 3D object detection results lead to incorrect text representation of the scene.
  - LLM fails to accurately interpret spatial relations or semantic constraints from the text representation.
  - LLM generates incorrect code or fails to properly interpret the output of the code for further reasoning.
  - Fine-tuning process does not improve the performance of the smaller model.
- First 3 experiments:
  1. Evaluate the accuracy of the 3D object detector on a held-out test set of 3D scenes.
  2. Test the accuracy of the Transcriber in converting 3D object detection results into a textual scene description.
  3. Evaluate the performance of the LLM with Code Interpreter on a set of 3D referring expressions with known ground truth answers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the scene transcript be made more object-centric to handle finer-grained details like shapes and open/closed states?
- Basis in paper: The paper mentions that the current scene transcript is object-centric and discusses limitations in handling scenarios requiring finer object details, such as "cylinder shaped trash can" or "open door with nothing blocking it".
- Why unresolved: The paper acknowledges this limitation but does not propose specific solutions for incorporating more detailed object properties into the scene transcript.
- What evidence would resolve it: Demonstrations of improved performance on benchmarks or real-world tasks when the scene transcript includes more detailed object properties like shapes, open/closed states, or material properties.

### Open Question 2
- Question: What are the effects of using more advanced 3D detection methods on the overall performance of Transcrib3D?
- Basis in paper: The paper notes that even state-of-the-art 3D detection methods yield suboptimal results and suggests room for improvement in 3D detection.
- Why unresolved: While the paper identifies the potential impact of 3D detection quality, it does not experiment with or compare different 3D detection methods to quantify their effects on Transcrib3D's performance.
- What evidence would resolve it: Comparative studies showing performance improvements on benchmarks when using advanced 3D detection methods, or ablation studies isolating the contribution of 3D detection accuracy to overall performance.

### Open Question 3
- Question: How can the feature selection strategy be made more adaptive to improve the quality of the scene transcript?
- Basis in paper: The paper mentions that the current method involves manual specification of desired information to be extracted from 3D detections and suggests that an adaptive feature selection strategy could potentially yield better results.
- Why unresolved: The paper proposes the idea of adaptive feature selection but does not implement or evaluate such a strategy.
- What evidence would resolve it: Experimental results demonstrating improved performance on benchmarks or real-world tasks when using an adaptive feature selection strategy compared to the current manual approach.

## Limitations

- Performance fundamentally bounded by quality of 3D object detections, with gap between ground-truth and detected results
- Self-correction fine-tuning effectiveness depends on LLM's ability to accurately identify and correct its own mistakes
- Method may struggle with fine-grained object details like shapes, open/closed states, and material properties

## Confidence

- High Confidence: The core mechanism of using text as a unifying medium for 3D referring expression resolution is well-supported by the results
- Medium Confidence: The effectiveness of iterative code generation and reasoning is supported by reported performance improvements
- Medium Confidence: The self-correction fine-tuning strategy shows promise in achieving performance close to large models with smaller ones

## Next Checks

1. Systematically evaluate Transcrib3D's performance as a function of 3D object detection accuracy, using controlled experiments with varying detection quality on the same scenes
2. Conduct a detailed analysis of cases where the iterative code generation and reasoning approach fails, identifying specific types of spatial reasoning tasks that remain challenging for the LLM
3. Test the self-correction fine-tuning approach on a held-out set of referring expressions that differ significantly in structure or complexity from the training set to assess generalization capabilities