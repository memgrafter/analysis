---
ver: rpa2
title: 'Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse
  Autoencoders'
arxiv_id: '2410.20526'
source_url: https://arxiv.org/abs/2410.20526
tags:
- features
- saes
- training
- sparse
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Llama Scope, a comprehensive suite of 256
  sparse autoencoders (SAEs) trained on each layer and sublayer of the Llama-3.1-8B-Base
  model, with 32K and 128K features. The authors modify the state-of-the-art Top-K
  SAEs by incorporating decoder column norms into sparsity computation and applying
  JumpReLU post-processing, achieving significantly better sparsity-fidelity trade-offs
  compared to vanilla SAEs.
---

# Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders

## Quick Facts
- arXiv ID: 2410.20526
- Source URL: https://arxiv.org/abs/2410.20526
- Reference count: 40
- Key outcome: Comprehensive suite of 256 sparse autoencoders (SAEs) trained on each layer and sublayer of Llama-3.1-8B-Base model, with 32K and 128K features, achieving improved sparsity-fidelity trade-offs through modified Top-K SAEs

## Executive Summary
This paper introduces Llama Scope, a comprehensive suite of 256 sparse autoencoders trained across all layers and sublayers of the Llama-3.1-8B-Base model. The authors modify state-of-the-art Top-K SAEs by incorporating decoder column norms into sparsity computation and applying JumpReLU post-processing, achieving significantly better sparsity-fidelity trade-offs compared to vanilla SAEs. The resulting Llama Scope SAEs maintain or improve reconstruction quality while reducing L0 sparsity from around 150 to 50 across all positions and layers.

The suite demonstrates strong generalization to longer contexts (up to 8192 tokens) and instruction-finetuned models with minimal performance degradation. Interpretability analysis reveals that approximately 10% of features are non-interpretable while the remaining features capture coherent semantic concepts. The geometry analysis shows that wider SAEs discover new features rather than merely composing existing ones. The entire suite, along with training and visualization tools, is publicly available to advance mechanistic interpretability research.

## Method Summary
The authors trained 256 sparse autoencoders on each layer and sublayer of the Llama-3.1-8B-Base model, using two different feature widths (32K and 128K). They modified the state-of-the-art Top-K SAE architecture by incorporating decoder column norms into the sparsity computation and applying JumpReLU post-processing. This modification achieved significantly better sparsity-fidelity trade-offs compared to vanilla SAEs. The training utilized a comprehensive dataset and incorporated regularization techniques to ensure stable feature extraction across all layers. The resulting SAEs were evaluated for reconstruction quality, sparsity levels, interpretability, and generalization to different model variants and context lengths.

## Key Results
- Achieved sparsity reduction from ~150 to ~50 L0 while maintaining or improving reconstruction quality across all positions and layers
- Demonstrated strong generalization to longer contexts (up to 8192 tokens) and instruction-finetuned models with minimal performance degradation
- Manual and automated interpretability analysis showed ~90% of features capture coherent semantic concepts while ~10% remain non-interpretable
- Geometry analysis revealed that wider SAEs discover new features rather than merely composing existing ones, with Top-K SAEs learning similar features to vanilla SAEs

## Why This Works (Mechanism)
The modified Top-K SAE architecture works by incorporating decoder column norms into the sparsity computation, which allows the model to prioritize features that have meaningful reconstruction capacity. The JumpReLU post-processing further enhances feature sparsity by creating a sharp transition in the activation function. These modifications enable the SAEs to maintain reconstruction quality while achieving significantly higher sparsity levels. The comprehensive training across all layers and sublayers ensures that the extracted features capture the full range of computational mechanisms within the model, from low-level patterns to high-level semantic concepts.

## Foundational Learning

**Sparse Autoencoders (SAEs)**: Neural networks trained to compress and reconstruct input data with sparse activations. Why needed: SAEs are the primary tool for feature extraction in mechanistic interpretability, allowing researchers to identify and analyze individual computational units within large language models. Quick check: Verify that SAE reconstruction loss remains low while activation sparsity is high.

**Top-K SAEs**: A variant of SAEs that selects the top-k most active features per token. Why needed: Top-K SAEs provide better control over sparsity levels and have shown superior performance in feature extraction compared to traditional L1 regularization. Quick check: Confirm that Top-K SAEs achieve desired sparsity targets while maintaining reconstruction quality.

**JumpReLU Activation**: An activation function that creates a sharp transition between active and inactive states. Why needed: JumpReLU enhances feature sparsity by making it easier for the model to distinguish between active and inactive features, improving the interpretability of extracted features. Quick check: Verify that JumpReLU produces more binary-like activation patterns compared to standard ReLU.

**Decoder Column Norms**: The L2 norm of each column in the decoder weight matrix. Why needed: Incorporating decoder column norms into sparsity computation ensures that only features with meaningful reconstruction capacity are activated, improving the quality of extracted features. Quick check: Confirm that features with higher decoder column norms correspond to more interpretable and impactful activations.

## Architecture Onboarding

**Component Map**: Dataset -> SAE Training (256 models) -> Feature Extraction -> Interpretability Analysis -> Geometry Analysis

**Critical Path**: Training data preparation -> SAE training across all layers/sublayers -> Reconstruction quality validation -> Interpretability analysis -> Generalization testing

**Design Tradeoffs**: The choice between 32K and 128K feature widths represents a tradeoff between computational efficiency and feature diversity. Wider SAEs (128K) discover more unique features but require more computational resources, while narrower SAEs (32K) are more efficient but may miss some complex features.

**Failure Signatures**: Poor reconstruction quality indicates inadequate SAE training or inappropriate hyperparameters. Non-interpretable features may suggest degenerate representations or insufficient training data. Generalization failures could indicate overfitting to specific model variants or context lengths.

**First Experiments**: 1) Validate reconstruction quality across all SAEs by comparing input-output similarity metrics, 2) Perform manual inspection of feature activations to assess interpretability rates, 3) Test generalization by applying SAEs to models with different fine-tuning or context lengths.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, focusing instead on presenting their comprehensive feature extraction methodology and results. The primary contribution is the Llama Scope suite itself, with the authors making all trained models and analysis tools publicly available for the research community to explore further questions.

## Limitations

- Interpretability analysis relies heavily on manual inspection, which may not capture the full complexity of feature behavior and could miss spurious correlations
- Claims about feature interpretability and generalization to longer contexts are reasonably supported but rely on manual analysis and limited downstream task evaluation
- The geometry analysis conclusions about feature diversity in wider SAEs are supported by decoder column similarity metrics but could benefit from additional validation

## Confidence

- **High Confidence**: Technical claims about sparsity improvements (reduction from ~150 to ~50 L0) and reconstruction quality maintenance are well-supported by quantitative metrics
- **Medium Confidence**: Claims about feature interpretability and generalization to longer contexts are reasonably supported but rely on manual analysis
- **Medium Confidence**: Geometry analysis conclusions about feature diversity are supported by decoder column similarity metrics but interpretation could be further validated

## Next Checks

1. Conduct a systematic ablation study varying the number of training examples and comparing feature stability across random seeds to better understand sensitivity to training conditions

2. Perform rigorous automated interpretability validation using causal interventions to verify that identified interpretable features actually influence model behavior in predictable ways

3. Evaluate practical utility of extracted features by testing whether they can be used for model editing, capability improvement, or detecting model failures across diverse tasks