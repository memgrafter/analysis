---
ver: rpa2
title: Improving Pinterest Search Relevance Using Large Language Models
arxiv_id: '2410.17152'
source_url: https://arxiv.org/abs/2410.17152
tags:
- relevance
- search
- data
- pinterest
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Pinterest improved search relevance by integrating Large Language
  Models (LLMs) into their ranking system. They fine-tuned cross-encoder LLMs to predict
  Pin relevance across five levels, using enriched text features like Pin titles,
  descriptions, synthetic image captions, link data, user-curated boards, and high-engagement
  queries.
---

# Improving Pinterest Search Relevance Using Large Language Models

## Quick Facts
- arXiv ID: 2410.17152
- Source URL: https://arxiv.org/abs/2410.17152
- Reference count: 31
- Primary result: 19.7% accuracy gain in offline experiments; +2.18% nDCG@20 and +0.7% to +2.0% fulfillment rate improvements in online A/B tests

## Executive Summary
Pinterest improved search relevance by integrating Large Language Models into their ranking system, fine-tuning cross-encoder LLMs to predict Pin relevance across five levels. The approach used enriched text features including Pin titles, descriptions, synthetic image captions, link data, user-curated boards, and high-engagement queries. A semi-supervised knowledge distillation approach scaled training data efficiently, enabling multilingual and cross-domain generalization despite training on primarily English data. The system achieved significant offline accuracy gains and positive online metrics across global markets.

## Method Summary
The system employs a cross-encoder LLM architecture that jointly encodes query and Pin text to capture semantic interactions, trained on five-scale relevance labels. To scale training data efficiently, a semi-supervised approach distills labels from the teacher LLM onto a large dataset of user interactions, which trains a lightweight student model with real-time features. Text representations are enriched with multimodal features including synthetic image captions, link titles, user-curated board titles, and high-engagement query tokens. The student model serves in production with a feed-forward network architecture that combines embeddings and interaction features for low-latency inference.

## Key Results
- 19.7% accuracy improvement in offline experiments compared to baselines
- +2.18% nDCG@20 improvement in online A/B tests globally
- +0.7% to +2.0% improvements in search fulfillment rates across different markets

## Why This Works (Mechanism)

### Mechanism 1
The cross-encoder LLM architecture captures query-document interaction better than bi-encoders by jointly encoding query and Pin text, preserving semantic dependencies that independent encoding cannot capture. This joint encoding allows direct modeling of interactions between queries and documents. The core assumption is that joint encoding preserves interaction patterns that independent encoding cannot capture. Evidence includes the paper's statement that cross-encoders "can better capture the interaction between queries and documents" and accuracy improvements shown in experiments. Break condition: If input text becomes too long or noisy, joint encoding may overwhelm the model and degrade interaction capture.

### Mechanism 2
Knowledge distillation from LLM teacher to student model enables scalable, low-latency deployment while retaining performance by generating soft labels for large datasets of user interactions, which train a lightweight student model with real-time features. The core assumption is that soft labels from high-capacity teachers carry richer information than hard labels, enabling effective transfer to smaller models. Evidence includes the distillation of "LLM-based model into real-time servable model architectures" and training on 30M rows of teacher-labeled data. Break condition: If teacher model predictions are too noisy or inconsistent, distillation quality degrades and student performance suffers.

### Mechanism 3
Enriching Pin text representations with multimodal and metadata features improves relevance prediction robustness by adding synthetic image captions, link titles, user-curated board titles, and high-engagement query tokens as complementary signals the LLM can leverage for better semantic understanding. The core assumption is that diverse text signals capture different aspects of Pin relevance, and the LLM can integrate them effectively. Evidence includes consistent accuracy gains shown in benchmark tables when adding text features sequentially. Break condition: If text features are low quality or irrelevant, they may introduce noise and hurt model performance.

## Foundational Learning

- Concept: Cross-encoder vs bi-encoder architectures
  - Why needed here: Understanding why Pinterest chose cross-encoder over bi-encoder for this task
  - Quick check question: What is the key architectural difference between cross-encoder and bi-encoder models, and why does it matter for relevance modeling?

- Concept: Knowledge distillation and semi-supervised learning
  - Why needed here: To grasp how Pinterest scaled training data and enabled multilingual generalization
  - Quick check question: How does knowledge distillation enable a small student model to benefit from a large teacher model's predictions on unlabeled data?

- Concept: Multimodal feature integration
  - Why needed here: To understand how Pinterest leverages image captions, link data, and user-curated content for relevance
  - Quick check question: Why might synthetic image captions and link metadata improve Pin relevance prediction compared to using only Pin titles and descriptions?

## Architecture Onboarding

- Component map: Query + Pin metadata → Teacher LLM (cross-encoder) → Soft labels → Student model (feed-forward) → Embeddings + BM25 + interaction features → 5-scale relevance score → Combined with engagement predictions for ranking

- Critical path: Input: Query + Pin metadata → Feature extraction: Embeddings, BM25 scores, interaction features → Model inference: Student model outputs 5-scale relevance score → Integration: Relevance score combined with engagement predictions for ranking

- Design tradeoffs: Cross-encoder vs bi-encoder (better accuracy but higher latency), LLM size (larger models improve accuracy but increase training/inference cost), feature richness (more text features improve robustness but require more preprocessing)

- Failure signatures: High latency in serving (likely student model too complex or features not optimized), poor accuracy on unseen languages (teacher model may not generalize well or distillation data insufficient), degradation with noisy text (text enrichment features may introduce irrelevant signals)

- First 3 experiments: 1) Compare cross-encoder vs bi-encoder baseline on small labeled dataset to confirm interaction capture benefit, 2) Test teacher model accuracy with different text feature combinations to find optimal enrichment set, 3) Validate distillation effectiveness by training student on varying amounts of teacher-labeled data and measuring accuracy vs human labels

## Open Questions the Paper Calls Out

### Open Question 1
How do LLM-based relevance models perform in languages and cultural contexts not represented in the training data? While the paper shows improvements across various countries, it does not provide detailed performance metrics for specific non-English languages or explore limitations of this generalization.

### Open Question 2
What is the optimal balance between model complexity and serving latency for real-time search relevance systems? The paper mentions cross-encoders have high computational costs but does not explore trade-offs in depth or analyze impact on user experience.

### Open Question 3
How can the quality and diversity of training data be further improved to enhance relevance model performance? While the paper demonstrates benefits of scaling up training data, it does not explore advanced data augmentation techniques, active learning strategies, or methods to ensure diversity in training data.

## Limitations
- Offline evaluation uses only accuracy metrics without human relevance judgments or ranking-specific measures like nDCG
- Online experiment results lack statistical significance testing details and long-term stability analysis
- Multilingual generalization claim based on zero-shot application rather than rigorous cross-lingual evaluation

## Confidence
- **High confidence**: Architectural choice of cross-encoder LLMs for query-document interaction capture is well-supported by literature and accuracy improvements
- **Medium confidence**: Effectiveness of text feature enrichment for robustness demonstrated through ablation studies, but specific contribution of each feature type isn't isolated
- **Low confidence**: Online experiment results show improvements but without statistical significance testing, confidence intervals, or long-term performance monitoring

## Next Checks
1. Conduct human relevance judgment studies to validate that 19.7% accuracy improvement translates to meaningful improvements in search quality using standardized relevance scales and multiple annotators

2. Perform cross-lingual evaluation by testing model on human-labeled query-Pin pairs in non-English languages to verify multilingual generalization capability, measuring both accuracy and ranking quality metrics

3. Analyze stability and bias of teacher model's automated labeling by examining label distributions across different query types, Pin categories, and user segments, measuring how label noise affects student model performance