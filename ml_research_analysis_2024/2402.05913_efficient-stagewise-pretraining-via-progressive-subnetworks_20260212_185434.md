---
ver: rpa2
title: Efficient Stagewise Pretraining via Progressive Subnetworks
arxiv_id: '2402.05913'
source_url: https://arxiv.org/abs/2402.05913
tags:
- training
- raptr
- layer
- layers
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a principled stagewise training framework
  called progressive subnetwork training, which trains random subnetworks within a
  model and progressively increases their size across stages. The authors propose
  Random Part Training (RaPTr), a simple instantiation that randomly selects subnetworks
  (e.g., depth-wise or width-wise) at each step, increasing their size over time.
---

# Efficient Stagewise Pretraining via Progressive Subnetworks

## Quick Facts
- arXiv ID: 2402.05913
- Source URL: https://arxiv.org/abs/2402.05913
- Reference count: 40
- This paper introduces a principled stagewise training framework called progressive subnetwork training, which trains random subnetworks within a model and progressively increases their size across stages.

## Executive Summary
This paper introduces a principled stagewise training framework called progressive subnetwork training, which trains random subnetworks within a model and progressively increases their size across stages. The authors propose Random Part Training (RaPTr), a simple instantiation that randomly selects subnetworks (e.g., depth-wise or width-wise) at each step, increasing their size over time. Theoretical analysis and experiments on polynomial data demonstrate that RaPTr effectively learns higher-order components of functions, unlike prior dropping strategies like progressive layer dropping (PLD). Comprehensive experiments on BERT and UL2 show that RaPTr significantly speeds up training (up to 33% fewer FLOPs) while achieving competitive or better downstream performance, including a 1.5% improvement on QA tasks and SuperGLUE. The method also provides a theoretical basis for stability across stage transitions, leveraging modern architecture components like residual connections and layer norms. RaPTr's flexibility and robustness to schedules make it a promising approach for efficient pretraining of large models.

## Method Summary
RaPTr implements progressive subnetwork training by selecting and training only a random subnetwork of the model at each step, progressively increasing the subnetwork size across stages. At each step, the method samples which layers to include in the forward and backward passes, scales the outputs of bypassed layers by a factor (typically square root), and updates all model parameters using gradients from the selected subnetwork. The subnetwork size grows across stages, allowing the model to first learn simpler patterns before moving to more complex ones. The method leverages residual connections and layer normalization to ensure stability during stage transitions. RaPTr can be applied to different axes (depth, width, attention heads) and offers flexibility in scheduling the progression of subnetwork sizes.

## Key Results
- RaPTr achieves up to 33% fewer FLOPs compared to standard training while maintaining or improving downstream performance
- On BERT-Base, RaPTr achieves 1.5% improvement on QA tasks and SuperGLUE compared to baseline
- Theoretical analysis shows RaPTr learns higher-order polynomial components more effectively than progressive layer dropping (PLD)
- RaPTr is robust to different scheduling strategies and provides smooth loss evolution across stage transitions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Progressive subnetworks impose a "simple-to-complex" inductive bias by training increasingly larger subnetworks across stages, aligning with the empirical phenomenon that SGD learns functions of increasing complexity over time.
- Mechanism: In each stage, only a random subnetwork (e.g., a subset of layers or width) is trained and backpropagated through. The size of this subnetwork is progressively increased across stages, so early stages focus on simpler correlations while later stages capture more complex ones.
- Core assumption: Learning in early stages does not interfere destructively with learning in later stages; instead, it builds a foundation that eases learning of higher-order components.
- Evidence anchors:
  - [abstract] "we show that this approach not only generalizes prior works like layer dropping but also fixes their key issues"
  - [section] "Furthermore, (P2) is motivated by the theoretically and empirically studied phenomenon that gradient-based training learns functions of increasing complexity over time (Kalimeris et al., 2019; Abbe et al., 2022)"
  - [corpus] weak: no direct neighbor paper provides empirical support for "simple-to-complex" in progressive subnetworks
- Break condition: If subnetwork size jumps too aggressively between stages, early-stage learning may be lost or destabilized.

### Mechanism 2
- Claim: RaPTr preserves or improves loss across stage transitions without explicit loss-preservation techniques.
- Mechanism: By keeping all parameters in the model active (even if some are bypassed in the forward pass), the full network is always part of the optimization landscape. The bypass uses residual connections, so bypassed layers still contribute to gradients, ensuring smooth loss evolution.
- Core assumption: Residual connections and layer norms ensure that bypassing a layer does not cause catastrophic loss spikes.
- Evidence anchors:
  - [abstract] "we establish the first theoretical basis for stagewise training based on dropping of layers that studies the behavior at stage transitions"
  - [section] "We establish the first theoretical basis for stagewise training based on dropping of layers that studies the behavior at stage transitions"
  - [corpus] weak: no neighbor paper discusses loss preservation in stagewise dropping
- Break condition: If architecture lacks residual connections or layer norms, bypassed layers may cause large loss jumps.

### Mechanism 3
- Claim: Subnetwork selection via random paths reduces FLOPs without harming final model quality.
- Mechanism: At each step, only the selected subnetwork's forward and backward passes are computed. Since the subnetwork size grows progressively, the total FLOPs over training can be much lower than full-model training, yet all parameters are updated eventually.
- Core assumption: Random selection is sufficient to expose all parameters to gradients over the course of training; the schedule ensures all parameters are eventually fully utilized.
- Evidence anchors:
  - [abstract] "RaPTr can significantly speed up training of standard benchmarks like BERT and UL2, up to 33% compared to standard training"
  - [section] "RaPTr can significantly speed up training of standard benchmarks like BERT and UL2, up to 33% compared to standard training"
  - [corpus] moderate: TrimLLM discusses progressive layer dropping for domain adaptation, providing indirect evidence for FLOPs savings
- Break condition: If the random schedule never selects certain parameters with sufficient frequency, their updates may be insufficient.

## Foundational Learning

- Concept: **Orthogonal basis polynomials under uniform Boolean distribution**
  - Why needed here: The theoretical analysis uses polynomial data to show that RaPTr learns higher-degree terms effectively, while PLD fails. This relies on the orthogonality of basis polynomials for clean component error analysis.
  - Quick check question: If you have a polynomial of degree k in d Boolean variables, how many basis polynomials are there of degree exactly ℓ ≤ k?

- Concept: **Residual network stability under random subnetwork training**
  - Why needed here: Theoretical bounds on loss gaps between stages rely on bounding the stability Ψℓ of the network output when a layer is dropped. This requires understanding how norms of intermediate activations and layer perturbations behave.
  - Quick check question: In a residual network with layer normalization, if you drop layer ℓ, by how much can the output norm change in the worst case, relative to the full model output norm?

- Concept: **Progressive layer dropping vs progressive layer stacking**
  - Why needed here: The paper contrasts RaPTr (dropping fewer layers as training progresses) with PLD (dropping more layers). Understanding why dropping more later is harmful requires knowledge of how expressive capacity changes with depth.
  - Quick check question: In a 20-layer residual network, if you drop 10 layers in the first half of training and 15 in the second half, how does the model's ability to fit high-degree polynomial terms change?

## Architecture Onboarding

- Component map: Main model backbone (e.g., Transformer) with L layers -> Random subnetwork selector (ζ1:L Bernoulli samples) -> Scaling function hsqrt that rescales bypassed layer outputs -> Optimizer that updates all parameters each step -> Training schedule (stages, subnetwork sizes, learning rates)

- Critical path:
  1. Sample ζ1:L to decide which layers to include
  2. Compute forward pass through selected subnetwork, scaling outputs
  3. Compute loss and backpropagate through selected subnetwork only
  4. Apply optimizer update to all parameters
  5. Repeat until stage end, then increase subnetwork size

- Design tradeoffs:
  - Random vs deterministic subnetwork selection: random ensures all parameters get gradients but may reduce stability; deterministic may miss parameters
  - Subnetwork size growth rate: aggressive growth risks instability; slow growth reduces FLOPs savings
  - Fixed vs learnable scaling: fixed (sqrt) is simpler and works at initialization; learnable may adapt better to later stages

- Failure signatures:
  - Training loss spikes at stage transitions → subnetwork size jump too large or missing residual connections
  - Validation loss plateaus early → subnetwork size too small or too slow growth
  - Some parameters never updated → random selection too sparse or epochs too few

- First 3 experiments:
  1. Implement a 6-8-10-12 schedule on BERT-Base and verify that loss evolves smoothly across stages.
  2. Compare RaPTr with PLD using the same average FLOPs; check if RaPTr recovers higher-degree polynomial terms better.
  3. Test different scaling functions (none, linear, sqrt) on a small transformer; measure impact on training stability.

## Open Questions the Paper Calls Out
- Question: How does RaPTr's performance scale with model size beyond the tested 1.6B parameter UL2 model?
- Question: What is the theoretical explanation for RaPTr's observed improvement in downstream task performance despite matching baseline perplexity?
- Question: How sensitive is RaPTr to the specific schedule of increasing subnetwork size, and what principles should guide schedule selection?
- Question: How does RaPTr perform when applied to other axes beyond depth, such as width or attention heads, and what are the trade-offs between different dimensions?

## Limitations
- Theoretical analysis relies heavily on polynomial data, which may not fully capture the complexity of real-world language modeling tasks
- Analysis of learning dynamics and function approximation is limited to a controlled setting
- Claims about random subnetwork selection being universally optimal lack extensive empirical validation across diverse settings

## Confidence
- **High confidence**: The FLOPs reduction claims (up to 33% for BERT, 20% for UL2) and downstream performance improvements are well-supported by experiments.
- **Medium confidence**: The theoretical framework for stage transitions and loss preservation is sound but relies on idealized assumptions (polynomial data, orthogonal basis functions).
- **Low confidence**: The claim that random subnetwork selection is universally optimal for all architectures and tasks lacks extensive empirical validation across diverse settings.

## Next Checks
1. **Architecture Generalization**: Test RaPTr on architectures without residual connections or layer normalization to assess the necessity of these components for stability.
2. **Real-World Data Complexity**: Evaluate RaPTr's ability to learn complex patterns on real-world datasets beyond polynomial data, such as image classification or speech recognition.
3. **Schedule Robustness**: Investigate the impact of different subnetwork size growth schedules (e.g., exponential vs. linear) on training stability and final performance across multiple tasks.