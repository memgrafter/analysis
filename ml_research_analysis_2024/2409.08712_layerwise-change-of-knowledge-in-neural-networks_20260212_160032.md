---
ver: rpa2
title: Layerwise Change of Knowledge in Neural Networks
arxiv_id: '2409.08712'
source_url: https://arxiv.org/abs/2409.08712
tags:
- interactions
- layers
- layer
- interaction
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to quantify and track how deep neural
  networks extract new knowledge and forget redundant features through layers during
  forward propagation. The authors extend the definition of interactions to intermediate
  layers by training linear classifiers on layer features and extracting AND/OR interactions
  from these classifiers.
---

# Layerwise Change of Knowledge in Neural Networks

## Quick Facts
- arXiv ID: 2409.08712
- Source URL: https://arxiv.org/abs/2409.08712
- Authors: Xu Cheng; Lei Cheng; Zhaoran Peng; Yang Xu; Tian Han; Quanshi Zhang
- Reference count: 40
- Key outcome: Introduces method to quantify and track how DNNs extract new knowledge and forget redundant features through layers during forward propagation

## Executive Summary
This paper addresses the challenge of understanding how knowledge emerges, evolves, and transforms through layers in deep neural networks. The authors develop a method to track AND/OR interaction primitives across layers by training linear classifiers on intermediate features and extracting symbolic patterns. They demonstrate that low-order interactions are stronger and more generalizable than high-order ones, that early and middle layers learn target interactions at the cost of encoding redundant ones which later layers remove, and that low-order interactions are more stable to input noise. The work provides new insights into DNN learning behavior and feature representation capacity through a symbolic interaction-based lens.

## Method Summary
The method extracts knowledge change through layers by first annotating semantic parts in input samples, then extracting intermediate-layer features from pre-trained DNNs, training linear classifiers on these features to compute scalar metrics for interaction extraction, extracting AND/OR interactions via masking and recombination, and tracking layerwise changes using overlap, forget, new metrics. The approach relies on the sparsity property of interactions (only a few are salient) and the universal-matching property (all outputs can be explained by these interactions). By computing interactions from classifiers trained on intermediate features, the method enables fair comparison across layers and quantifies how knowledge emerges and transforms during forward propagation.

## Key Results
- Low-order interactions are stronger and more generalizable across models than high-order ones
- Early and middle layers learn target interactions at the cost of encoding redundant ones, which later layers remove
- Low-order interactions are more stable to input noise than high-order interactions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The sparsity of interactions allows the model to represent knowledge as a small set of symbolic primitive patterns, making layerwise tracking computationally feasible.
- Mechanism: The DNN learns to encode only a few salient AND/OR interactions per sample, with negligible effects from most other interactions. This sparsity ensures that intermediate-layer interactions can be extracted and tracked without exponential blowup.
- Core assumption: Most DNNs satisfy the conditions for interaction sparsity (e.g., smooth inferences on masked samples).
- Evidence anchors:
  - [abstract] "the sparsity property and universal-matching property mathematically guarantee the faithfulness of interaction-based explanation"
  - [section] "Only about 21.8 AND/OR salient interactions in each MNIST image and about 45.6 AND/OR salient interactions in each CIFAR-10 image"
  - [corpus] No direct corpus support; based on Ren et al. (2024) and experimental verification.
- Break condition: If the DNN does not satisfy smoothness conditions or encodes many non-sparse interactions, the tracking becomes intractable.

### Mechanism 2
- Claim: Using a linear classifier trained on intermediate-layer features enables fair comparison of interactions across layers.
- Mechanism: The linear classifier's output v^(l)(x) serves as a scalar metric that reflects the task-relevant information in the intermediate feature. This allows interaction extraction via masking and ensures alignment between layers.
- Core assumption: The linear classifier trained on fixed DNN features can faithfully represent the task-relevant signals for interaction extraction.
- Evidence anchors:
  - [section] "We propose to train a linear classifier p^(l)(y|x) = softmax/sigmoid((w^(l))^T f^(l)(x)+b^(l)) based on the cross-entropy loss"
  - [section] "This new function v^(l)(x) enables a fair comparison between interactions extracted from different layers"
  - [corpus] Weak; the paper assumes this design without external validation.
- Break condition: If the linear classifier fails to capture task-relevant information or overfits, the interaction extraction becomes unreliable.

### Mechanism 3
- Claim: Low-order interactions generalize better across models and are more stable to input noise than high-order interactions.
- Mechanism: The complexity of an interaction (order) inversely correlates with its generalization and stability. Low-order interactions involve fewer variables, making them more robust to perturbations and consistent across different DNNs trained on the same task.
- Core assumption: The order of an interaction directly reflects its complexity and generalization capacity.
- Evidence anchors:
  - [abstract] "low-order interactions are stronger and more generalizable across models than high-order ones"
  - [section] "Fig. 4 reports the average IoU value of AND interactions extracted from two DNNs... We discovered low-order interactions extracted from different DNNs usually exhibited higher IoU values"
  - [section] "Fig. 5 shows that the relative stability decreased along with the order m, which indicated that low-order interactions were more stable to inevitable noises in data"
  - [corpus] No direct corpus support; based on experimental results in the paper.
- Break condition: If the relationship between interaction order and generalization/stability does not hold for certain tasks or architectures.

## Foundational Learning

- Concept: Interaction primitives (AND/OR interactions)
  - Why needed here: The paper defines knowledge in DNNs as interaction primitives, which are symbolic patterns extracted via masking and recombination. Understanding this concept is essential to grasp how the paper tracks knowledge through layers.
  - Quick check question: Can you explain how an AND interaction is computed using masked samples and why it represents a co-appearance relationship?

- Concept: Sparsity and universal-matching properties
  - Why needed here: These properties justify using interactions as primitive inference patterns. Sparsity ensures only a few interactions are salient, while universal-matching guarantees all network outputs can be explained by these interactions.
  - Quick check question: What is the difference between the sparsity property and the universal-matching property of interactions?

- Concept: Layerwise forward propagation and feature representation
  - Why needed here: The paper tracks how knowledge changes through layers during forward propagation. Understanding how features evolve and how they can be compared across layers is crucial.
  - Quick check question: Why is it challenging to directly compare features across layers, and how does the paper address this challenge?

## Architecture Onboarding

- Component map: Input annotations -> DNN -> Intermediate layers -> Linear classifier -> Interaction extractor -> Metrics
- Critical path: 1. Annotate semantic parts in input samples 2. Extract intermediate-layer features from pre-trained DNN 3. Train linear classifier on intermediate features 4. Compute v^(l)(x) for interaction extraction 5. Extract AND/OR interactions via masking and recombination 6. Track layerwise changes using overlap, forget, new metrics 7. Analyze generalization and stability across models and noise
- Design tradeoffs:
  - Computational cost vs. granularity: Annotating more semantic parts increases interaction extraction cost exponentially
  - Linear classifier complexity vs. faithfulness: Simpler classifiers are easier to train but may miss task-relevant signals
  - Interaction order vs. generalization: Lower-order interactions are more generalizable but may miss complex patterns
- Failure signatures:
  - No sparse interactions found: DNN may not satisfy sparsity conditions
  - Linear classifier performs poorly: Intermediate features may not capture task-relevant information
  - High-order interactions dominate: Model may encode complex patterns that are unstable or non-generalizable
- First 3 experiments:
  1. Verify sparsity of interactions in a simple DNN (e.g., MLP-7) on MNIST
  2. Extract intermediate-layer interactions and track layerwise changes
  3. Compare generalization and stability of low-order vs. high-order interactions across models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can interaction primitives be used to represent the complex learning dynamics of a DNN beyond simple layerwise tracking?
- Basis in paper: [inferred] The paper demonstrates tracking interactions through layers but acknowledges this is a simplified view of learning dynamics.
- Why unresolved: The paper only shows layerwise change but doesn't explore how interactions could capture temporal dynamics, curriculum learning effects, or meta-learning behaviors.
- What evidence would resolve it: Demonstrating interaction-based explanations of phenomena like catastrophic forgetting, transfer learning effectiveness, or curriculum learning optimization.

### Open Question 2
- Question: What methods can identify and boost reliable/generalizable interaction primitives in practical applications?
- Basis in paper: [explicit] "identifying and boosting the reliable/generalizable interaction primitives" listed as future challenge.
- Why unresolved: The paper shows low-order interactions are more generalizable but doesn't provide concrete methods to actively select, enhance, or engineer such interactions.
- What evidence would resolve it: Developing algorithms that prioritize learning specific interaction orders or patterns during training to improve generalization.

### Open Question 3
- Question: How can DNN inference logic (interaction primitives) be aligned with human cognitive processes?
- Basis in paper: [explicit] "aligning a DNN's detailed inference logic (interaction primitives) with human cognition" listed as future challenge.
- Why unresolved: While interactions provide symbolic representations, there's no framework connecting these to how humans process information or make decisions.
- What evidence would resolve it: Creating mapping between interaction patterns and psychological models of decision-making or cognitive processing stages.

## Limitations

- The method relies on linear classifiers trained on intermediate features, which may not capture all task-relevant information or complex non-linear relationships
- The sparsity assumption for interactions may not hold uniformly across all architectures and tasks, limiting the generalizability of the approach
- The paper focuses on symbolic AND/OR interactions but may miss other forms of knowledge representation or reasoning patterns in DNNs

## Confidence

- **High confidence**: The experimental findings regarding layerwise knowledge change (emergence, forgetting, new interactions) are well-supported by quantitative metrics and visualizations across multiple datasets and architectures.
- **Medium confidence**: The claim that low-order interactions are more generalizable and stable than high-order ones is supported by experiments but relies on specific definitions of interaction order and stability that may not generalize to all contexts.
- **Medium confidence**: The mechanism by which linear classifiers enable fair comparison across layers is logically sound but depends on empirical validation that the classifiers capture task-relevant features consistently.

## Next Checks

1. Test the interaction extraction method on architectures known to violate smoothness conditions (e.g., Transformers with attention mechanisms) to validate the break condition for Mechanism 1.
2. Compare interaction patterns extracted via linear classifiers against those obtained from directly analyzing layer activations to verify the faithfulness assumption in Mechanism 2.
3. Conduct ablation studies varying interaction order thresholds to determine if the observed generalization patterns hold across different complexity levels and task domains.