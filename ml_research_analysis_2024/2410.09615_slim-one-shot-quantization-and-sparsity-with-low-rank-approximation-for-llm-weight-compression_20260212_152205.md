---
ver: rpa2
title: 'SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for LLM
  Weight Compression'
arxiv_id: '2410.09615'
source_url: https://arxiv.org/abs/2410.09615
tags:
- quantization
- low-rank
- sparsity
- accuracy
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SLIM addresses the challenge of compressing large language models
  (LLMs) while maintaining accuracy and efficiency. The core idea is to integrate
  hardware-friendly quantization, sparsity, and low-rank approximation into a unified
  process.
---

# SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for LLM Weight Compression

## Quick Facts
- arXiv ID: 2410.09615
- Source URL: https://arxiv.org/abs/2410.09615
- Reference count: 40
- One-line primary result: Improves model accuracy by up to 5.66% on LLaMA-2-7B with 2:4 sparsity and 4-bit weight quantization, outperforming prior methods.

## Executive Summary
SLiM introduces a unified framework for compressing large language models by integrating hardware-friendly quantization, semi-structured sparsity, and low-rank approximation. The method reformulates quantization as a probabilistic optimization problem, applies Wanda pruning to quantized weights, and compensates errors with a novel saliency-based low-rank approximation (SLIM-LoRA). SLIM achieves significant accuracy improvements over prior methods while enabling substantial speedups and memory reduction on GPUs.

## Method Summary
SLiM compresses LLMs through a one-shot pipeline: first, SLIM-Quant uses a probabilistic approach with histogram-based numerical integration to find optimal scaling factors for uniform quantization, reducing quantization error. Second, Wanda applies semi-structured sparsity to the quantized weights. Third, SLIM-LoRA computes low-rank adapters from the aggregated quantization and sparsity errors using an invertible and additive saliency function, compensating for accuracy loss. Optionally, parameter-efficient fine-tuning can further improve accuracy by fine-tuning only the low-rank adapters.

## Key Results
- Improves model accuracy by up to 5.66% on LLaMA-2-7B with 2:4 sparsity and 4-bit weight quantization
- Achieves up to 4.3× and 3.8× layer-wise speedup on NVIDIA RTX3060 and A100 GPUs, respectively
- Reduces end-to-end memory by up to 0.23× compared to dense models

## Why This Works (Mechanism)

### Mechanism 1
SLIM-Quant reformulates uniform quantization into a convex optimization problem, reducing quantization error while retaining computational efficiency. Instead of minimizing MSE directly (a non-convex problem), SLIM-Quant treats quantization probabilistically and optimizes the expected error over the weight distribution using numerical integration over a histogram. The core assumption is that the weight distribution can be approximated well enough by a histogram for error estimation, and the quantization process is invertible.

### Mechanism 2
SLIM-LoRA uses a saliency function that is both invertible and additive to compute low-rank adapters without retraining. By defining saliency as the product of input and weight magnitudes, the method can derive the low-rank adapters from the singular value decomposition of the error saliency matrix. The core assumption is that the saliency function satisfies invertibility and additivity, and the error between compressed and original weights can be captured sufficiently by the top singular vectors.

### Mechanism 3
Combining sparsity and quantization with low-rank compensation improves accuracy compared to either alone, and the combined overhead is still much lower than dense. SLIM applies Wanda pruning after SLIM-Quant quantization, then compensates both errors with low-rank adapters. Quantizing the adapters themselves keeps overhead minimal. The core assumption is that the aggregation of quantization and sparsity errors can be modeled as additive noise, and low-rank approximation can sufficiently correct it.

## Foundational Learning

- **Concept: Probabilistic reformulation of deterministic optimization problems**
  - Why needed here: Enables turning the intractable MSE minimization in quantization into a convex problem solvable via numerical integration
  - Quick check question: Why is it beneficial to convert a non-convex optimization into a convex one in the context of quantization?

- **Concept: Singular Value Decomposition (SVD) for low-rank approximation**
  - Why needed here: Provides the mathematical foundation for computing the low-rank adapters from the error saliency matrix
  - Quick check question: How does SVD enable finding a low-rank matrix that best approximates a given matrix in the Frobenius norm?

- **Concept: Histogram-based probability density estimation**
  - Why needed here: Allows SLIM-Quant to approximate the weight distribution without assuming a specific parametric form
  - Quick check question: What is the trade-off between the number of histogram bins and the accuracy of the PDF approximation?

## Architecture Onboarding

- **Component map**: Calibration data → compute weight histogram → estimate scaling factor (SLIM-Quant) → Apply quantization → compute error matrix → Apply Wanda pruning → compute sparsity error → Aggregate errors → compute saliency → apply SVD → derive adapters (SLIM-LoRA) → Optionally, fine-tune adapters on PEFT dataset → Sparse Marlin + vLLM for runtime inference

- **Critical path**: 1) Calibration data → compute weight histogram → estimate scaling factor (SLIM-Quant) 2) Apply quantization → compute error matrix 3) Apply Wanda pruning → compute sparsity error 4) Aggregate errors → compute saliency → apply SVD → derive adapters (SLIM-LoRA) 5) Optionally, fine-tune adapters on PEFT dataset

- **Design tradeoffs**: Uniform vs. group quantization (simpler implementation and kernel support vs. potentially lower error); Adapter rank (higher rank → better accuracy but more overhead; SLIM uses 10% of hidden dim as default); Activation-aware scaling (improves accuracy but adds irregular memory access patterns)

- **Failure signatures**: Accuracy drop after quantization → scaling factor poorly estimated (histogram bins too few); Accuracy drop after sparsity → saliency metric not capturing important weights; Memory blowup → low-rank adapters not quantized or rank too high; Slow inference → group quantization not supported by kernel; using dense kernels instead

- **First 3 experiments**: 1) Run SLIM-Quant on a single dense layer and verify that the quantization error is lower than AbsMax 2) Apply SLIM-LoRA to a layer quantized with SLIM-Quant and check that accuracy recovers close to dense 3) Combine SLIM-Quant + Wanda + SLIM-LoRA on a small model and compare perplexity vs dense baseline

## Open Questions the Paper Calls Out

### Open Question 1
How does the computational overhead of SLIM-Quant scale with increasing model size and hidden dimensions? The paper states that SLIM-Quant uses a histogram-based approach to reduce overhead by sharing error computations between elements in the same histogram bin, but lacks empirical or theoretical analysis of how the computational cost changes as model dimensions increase.

### Open Question 2
Can SLIM-Quant be extended to support non-uniform distributions beyond those tested (Gaussian, Laplace, Pareto, q-Gaussian, Weibull)? The paper tested several standard distributions and found none matched the observed distributions accurately, motivating the data-driven histogram approach, but does not explore whether SLIM-Quant could be adapted for other types of distributions.

### Open Question 3
What is the impact of using different low-rank adapter ranks on the trade-off between model accuracy and computational/memory overhead in SLIM-LoRA? The paper states that a rank of 10% of the model's hidden dimension is used by default and mentions that increasing the rank reduces approximation error but also increases overhead, but lacks a comprehensive analysis of this trade-off.

## Limitations

- The paper does not provide sensitivity analysis to the number of histogram bins or integration resolution used in SLIM-Quant, which could affect numerical integration accuracy
- The claimed end-to-end memory reduction (up to 0.23×) is difficult to verify without access to full implementation details, as the paper doesn't specify whether this includes activation memory, optimizer states, or only model weights
- The claimed layer-wise speedups depend on specialized CUDA kernels for sparse quantized operations, and the paper does not disclose whether these kernels are publicly available

## Confidence

- **High Confidence**: The core idea of integrating quantization, sparsity, and low-rank approximation into a unified pipeline is well-founded, with clear pseudocode and ablation studies
- **Medium Confidence**: The accuracy improvements (up to 5.66% on LLaMA-2-7B) are significant but depend on specific hyperparameter choices, and the paper reports results for a fixed set of configurations without exploring the full hyperparameter space
- **Low Confidence**: The claimed end-to-end memory reduction is difficult to verify without access to full implementation details and clarification on what memory components are included

## Next Checks

1. **Histogram Sensitivity Analysis**: Reproduce SLIM-Quant with varying numbers of histogram bins (e.g., 50, 100, 200) and measure the impact on quantization error and final model accuracy to reveal whether the numerical integration is robust to discretization choices

2. **Cross-Model Saliency Evaluation**: Apply SLIM-LoRA to a different LLM architecture (e.g., GPT-2 or BLOOM) and compare the accuracy recovery to the reported results on LLaMA-2 and OPT to test the generalizability of the saliency function

3. **Kernel Independence Benchmark**: Measure the layer-wise speedup using standard PyTorch sparse and quantized operations (without custom kernels) to determine how much of the claimed speedup depends on specialized implementations