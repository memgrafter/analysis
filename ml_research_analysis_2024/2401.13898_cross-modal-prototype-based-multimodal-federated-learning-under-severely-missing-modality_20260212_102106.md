---
ver: rpa2
title: Cross-Modal Prototype based Multimodal Federated Learning under Severely Missing
  Modality
arxiv_id: '2401.13898'
source_url: https://arxiv.org/abs/2401.13898
tags:
- missing
- data
- modalities
- modality
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MFCPL, a novel multimodal federated learning
  framework that addresses data heterogeneity and severely missing modalities in decentralized
  training. The approach leverages complete prototypes to guide local training at
  both modality-specific and modality-shared representation levels, providing diverse
  modality knowledge to clients with missing data.
---

# Cross-Modal Prototype based Multimodal Federated Learning under Severely Missing Modality

## Quick Facts
- **arXiv ID**: 2401.13898
- **Source URL**: https://arxiv.org/abs/2401.13898
- **Reference count**: 8
- **Primary result**: MFCPL achieves up to 7.69%, 5.48%, and 5.24% performance improvements over state-of-the-art methods on UCI-HAR, Hateful Memes, and MELD datasets respectively when 80% of modality data is missing

## Executive Summary
This paper introduces MFCPL, a novel multimodal federated learning framework that addresses data heterogeneity and severely missing modalities in decentralized training. The approach leverages complete prototypes to guide local training at both modality-specific and modality-shared representation levels, providing diverse modality knowledge to clients with missing data. The framework consists of three key components: Cross-Modal Prototypes Regularization (CMPR), Cross-Modal Prototypes Contrastive (CMPC), and Cross-Modal Alignment (CMA). These components work together to bridge the gap between local and global models, encourage modality-specific representations to align with complete prototypes, and mitigate noise from missing modalities. Extensive experiments on three multimodal datasets demonstrate MFCPL's effectiveness, achieving significant improvements in performance compared to state-of-the-art methods while maintaining high accuracy even as missing modality rates increase.

## Method Summary
MFCPL is a multimodal federated learning framework designed to handle severely missing modalities through complete prototype aggregation and cross-modal guidance. The framework processes data through modality encoders and a self-attention fusion layer at each client, generating local prototypes that are aggregated at the server to form complete prototypes. These complete prototypes guide local training through three mechanisms: CMPR aligns modality-shared representations with complete prototypes, CMPC uses contrastive learning to align modality-specific representations, and CMA applies regularization to encourage coherence between available modalities. The approach enables clients with missing modalities to learn from complete prototypes while maintaining semantic consistency across different modality combinations.

## Key Results
- MFCPL achieves 7.69%, 5.48%, and 5.24% performance improvements over state-of-the-art methods on UCI-HAR, Hateful Memes, and MELD datasets respectively at 80% missing modality rates
- The framework maintains strong performance even with 80% missing modalities, demonstrating robustness to severe data heterogeneity
- Extensive ablation studies validate the effectiveness of each component, with all three (CMPR, CMPC, CMA) contributing significantly to overall performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Complete prototypes capture diverse modality knowledge across all client types, enabling robust training even with missing modalities
- Mechanism: The framework aggregates local prototypes from different client types into complete prototypes, which are then used to guide local training at both modality-specific and modality-shared representation levels
- Core assumption: Aggregating prototypes from diverse client types creates a representative complete prototype that captures cross-modal relationships and semantic consistency across classes
- Evidence anchors: [abstract] "Our MFCPL leverages the complete prototypes to provide diverse modality knowledge in modality-shared level with the cross-modal regularization and modality-specific level with cross-modal contrastive mechanism"
- Break condition: If the aggregation process fails to capture meaningful cross-modal relationships, or if the complete prototypes become dominated by one modality type due to data distribution imbalance

### Mechanism 2
- Claim: Cross-Modal Prototypes Contrastive (CMPC) aligns modality-specific representations with complete prototypes, enabling clients with missing modalities to learn from available modalities while maintaining semantic consistency
- Mechanism: CMPC uses complete prototypes as reference points in a contrastive learning framework, projecting modality-specific features into a shared space and encouraging them to be close to complete prototypes of the same class while being distant from different classes
- Core assumption: The complete prototype serves as an effective anchor point that captures the semantic meaning of each class across all modalities
- Evidence anchors: [abstract] "Our MFCPL leverages the complete prototypes to provide diverse modality knowledge in modality-shared level with the cross-modal regularization and modality-specific level with cross-modal contrastive mechanism"
- Break condition: If the projection head fails to align features meaningfully, or if the temperature parameter τ is poorly tuned

### Mechanism 3
- Claim: Cross-Modal Alignment (CMA) reduces the negative impact of zero-filling missing modalities by encouraging coherence between projected representations of available modalities
- Mechanism: CMA applies ℓ2 distance regularization between projected modality-specific representations of all available modality pairs, ensuring representations from different modalities remain aligned in the shared embedding space
- Core assumption: Alignment in a shared space enables coherence across modality-specific representations, effectively mitigating the impact of missing modalities
- Evidence anchors: [abstract] "Additionally, our approach introduces the cross-modal alignment to provide regularization for modality-specific features, thereby enhancing the overall performance, particularly in scenarios involving severely missing modalities"
- Break condition: If the alignment loss becomes too strong, forcing representations to be too similar and lose modality-specific discriminative information

## Foundational Learning

- **Federated Learning**: Why needed here - The entire framework operates in a decentralized setting where multiple clients with private multimodal data collaborate to train a global model without sharing raw data. Quick check: What is the main difference between traditional centralized learning and federated learning in terms of data handling? Answer: In federated learning, data remains on client devices and only model updates or intermediate representations are shared, preserving privacy.

- **Prototype Learning**: Why needed here - Prototypes serve as compact class representatives that reduce noise and outliers by averaging feature vectors within a class, providing stable and robust representations for federated training. Quick check: How are prototypes typically computed in few-shot learning scenarios? Answer: Prototypes are computed as the mean feature vector of all support examples belonging to the same class in the embedding space.

- **Contrastive Learning**: Why needed here - Contrastive learning is used to align modality-specific representations with complete prototypes, encouraging representations of the same class to be close while pushing apart representations of different classes. Quick check: What is the main objective of contrastive learning in representation learning? Answer: To learn representations where similar samples are close together in the embedding space while dissimilar samples are far apart.

## Architecture Onboarding

- **Component map**: Client-side: Modality encoders (fm) -> Self-attention fusion layer -> Classifier (g) -> Two projection heads (g1, g2). Server-side: Complete prototype aggregation module. Key components: CMPR (modality-shared alignment), CMPC (modality-specific contrastive learning), CMA (modality alignment)

- **Critical path**: 1. Local training: Each client processes data through modality encoders → self-attention fusion → classifier. 2. Prototype generation: Local prototypes are computed and projected using g1. 3. Server aggregation: Complete prototypes are created by averaging local prototypes. 4. Global guidance: Complete prototypes are sent back to clients for CMPR and CMPC. 5. Alignment: CMA operates locally on available modalities.

- **Design tradeoffs**: Complete vs unimodal prototypes (complete captures cross-modal knowledge but requires more computation), projection head dimension (smaller dimensions reduce computation but may limit expressiveness), temperature parameter (lower temperatures create stronger contrastive signals).

- **Failure signatures**: Poor performance with high missing rates (may indicate ineffective complete prototypes or broken contrastive learning), degraded performance with more modalities (could suggest over-alignment in CMA), instability during training (may indicate temperature τ is poorly tuned).

- **First 3 experiments**: 1. Verify complete prototype computation: Check that prototypes are correctly aggregated from different client types and that g1 projection is working. 2. Test CMPC effectiveness: Validate that modality-specific representations are being properly aligned with complete prototypes using contrastive loss. 3. Evaluate CMA impact: Measure the effect of modality alignment on performance, particularly comparing different loss functions (L2 vs L1 vs smooth L1).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MFCPL change when the missing modality rates are non-uniform across different clients rather than uniformly distributed?
- Basis in paper: [inferred] The paper uses uniform missing rates but mentions "real-world scenarios may not always allow the utilization of all available modalities"
- Why unresolved: The paper only evaluates uniform missing rates across all clients, limiting understanding of performance under more realistic heterogeneous missing patterns
- What evidence would resolve it: Experiments comparing MFCPL performance with varying missing rates across different client groups

### Open Question 2
- Question: What is the theoretical convergence guarantee of MFCPL under severely missing modality conditions, and how does it compare to convergence rates of other federated learning methods?
- Basis in paper: [inferred] While the paper provides extensive empirical results, it does not provide theoretical analysis of convergence properties under missing modalities
- Why unresolved: The paper focuses on empirical evaluation without formal convergence analysis
- What evidence would resolve it: Mathematical proofs or convergence bounds showing the rate at which MFCPL approaches the optimal solution

### Open Question 3
- Question: How does MFCPL scale with increasing numbers of modalities beyond the three modalities tested, particularly in terms of computational efficiency and alignment loss performance?
- Basis in paper: [explicit] The paper mentions scalability claims but only evaluates with 2-3 modalities in experiments
- Why unresolved: The scalability claims are not empirically validated with more than three modalities
- What evidence would resolve it: Experiments testing MFCPL with 4+ modalities, measuring computation time and alignment loss performance

## Limitations
- The framework's performance heavily depends on the quality of complete prototype aggregation across heterogeneous client types, which may fail with highly imbalanced data distributions
- The zero-filling strategy for missing modalities could introduce significant noise that the CMA component may not fully mitigate in extremely severe missing scenarios
- Limited evaluation on only three datasets with specific modality combinations may not generalize to all multimodal federated learning scenarios

## Confidence

- **High confidence**: The mechanism of using complete prototypes to provide cross-modal knowledge to clients with missing modalities
- **Medium confidence**: The effectiveness of the contrastive learning approach for aligning modality-specific representations
- **Low confidence**: The scalability of the framework to scenarios with more than three modalities or highly dynamic client populations

## Next Checks

1. **Prototype Quality Validation**: Measure the semantic similarity between complete prototypes and local prototypes across different client types to verify that cross-modal knowledge is being properly captured and transferred

2. **Missing Rate Robustness Test**: Systematically evaluate performance degradation as missing rates increase beyond 80% to identify the breaking point of the framework

3. **Modality-Specific Contribution Analysis**: Conduct ablation studies to quantify the individual contributions of CMPR, CMPC, and CMA components under different missing modality scenarios