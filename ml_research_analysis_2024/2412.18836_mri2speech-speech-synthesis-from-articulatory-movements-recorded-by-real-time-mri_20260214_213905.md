---
ver: rpa2
title: 'MRI2Speech: Speech Synthesis from Articulatory Movements Recorded by Real-time
  MRI'
arxiv_id: '2412.18836'
source_url: https://arxiv.org/abs/2412.18836
tags:
- speech
- text
- rtmri
- video
- usc-timit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of synthesizing speech from real-time
  MRI (rtMRI) articulatory movements, which is challenging due to noise in the ground-truth
  speech. The core method involves fine-tuning a multi-modal self-supervised AV-HuBERT
  model to predict text from rtMRI videos, using a flow-based duration predictor for
  speaker-specific alignment, and a speech decoder to synthesize speech.
---

# MRI2Speech: Speech Synthesis from Articulatory Movements Recorded by Real-time MRI

## Quick Facts
- **arXiv ID**: 2412.18836
- **Source URL**: https://arxiv.org/abs/2412.18836
- **Reference count**: 35
- **Primary result**: Achieves 15.18% WER on USC-TIMIT MRI corpus for speech synthesis from rtMRI

## Executive Summary
This paper addresses the challenge of synthesizing intelligible speech from noisy real-time MRI (rtMRI) articulatory movements. The core innovation is bypassing ground-truth mel-spectrogram loss (which entangles speech content with MRI noise) by instead fine-tuning a multi-modal AV-HuBERT model to predict text directly from rtMRI videos. The method uses a flow-based duration predictor trained on noisy audio-text pairs to align predicted text with acoustic space, enabling speaker-specific synthesis. The approach achieves state-of-the-art performance with 15.18% WER on USC-TIMIT MRI and demonstrates effective generalization to unseen speakers across different rtMRI databases.

## Method Summary
The MRI2Speech approach fine-tunes a pre-trained AV-HuBERT model on (rtMRI video, text) pairs using CTC loss to predict text from silent articulatory videos. A VITS-based stochastic duration predictor is trained on noisy audio-text pairs from both USC-TIMIT MRI and ASD1 datasets to learn speaker-specific phoneme durations. During synthesis, the predicted text is encoded, durations are estimated, and a flow-based normalizing flow maps latent representations to acoustic space, which a HiFi-GAN decoder converts to speech. For novel target voices, a single-speaker VITS is trained on clean LJSpeech data while preserving alignment from source speakers.

## Key Results
- Achieves 15.18% WER on USC-TIMIT MRI corpus, significantly outperforming prior state-of-the-art
- Demonstrates effective generalization to unseen speakers across different rtMRI databases (USC-TIMIT and ASD1)
- Shows internal articulators contribute more to accurate text prediction than lip movements alone (25.33% WER decrease when masking lips)
- Maintains intelligibility despite noisy ground-truth speech by avoiding direct mel-spectrogram loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing ground-truth mel-spectrogram loss with text prediction reduces noise entanglement and improves intelligibility.
- Mechanism: AV-HuBERT is fine-tuned to predict text from rtMRI videos, bypassing the noisy audio and its associated spectrogram artifacts. The CTC loss trains the model to map visual articulatory features directly to phoneme sequences.
- Core assumption: The multi-modal AV-HuBERT model can learn robust articulatory-to-text mappings without relying on acoustic supervision.
- Evidence anchors:
  - [abstract] "Applying loss directly over ground truth mel-spectrograms entangles speech content with MRI noise, resulting in poor intelligibility."
  - [section II-A] "We fine-tune the entire AV-HuBERT model to perform visual speech recognition using Connectionist Temporal Classification (CTC) loss."
  - [corpus] Weak; no directly comparable paper found, so this remains an assumption.
- Break condition: If the visual articulatory features are insufficiently discriminative, or if the AV-HuBERT pretraining does not transfer well to rtMRI data, text prediction performance will degrade, leading to poor speech synthesis.

### Mechanism 2
- Claim: A flow-based duration predictor aligns predicted text with acoustic space, enabling speaker-specific synthesis.
- Mechanism: VITS is trained on noisy audio-text pairs to learn a monotonic alignment between phonemes and audio frames. The duration predictor, once trained, estimates phoneme durations for any speaker, allowing expansion of the prior latents to match the target speaker's temporal structure.
- Core assumption: The alignment learned from noisy audio generalizes to clean synthesis, and the duration predictor captures speaker-specific timing.
- Evidence anchors:
  - [section II-B] "We follow the VITS training paradigm to train a stochastic duration predictor (SDP) for each speaker in both the USC-TIMIT MRI and ASD1 datasets."
  - [section II-C] "Using the inverse flow and the LJSpeech speech decoder, we can synthesize speech in the LJSpeech's speaker voice while preserving the alignment learned from the source speaker."
  - [corpus] Weak; no direct evidence in corpus for this flow-based alignment method, so it remains an assumption.
- Break condition: If the noisy audio contains significant timing distortions, or if speaker characteristics are too variable, the predicted durations may not align properly, resulting in incoherent speech.

### Mechanism 3
- Claim: Masking lip regions in input rtMRI tests the model's reliance on internal articulators versus lip movements.
- Mechanism: By cropping or masking the lip region and retraining the model, the paper demonstrates that internal articulators (velum, tongue root, larynx) contribute more to accurate text prediction than lips alone, countering the potential bias from AV-HuBERT's lip-reading pretraining.
- Core assumption: AV-HuBERT's pretraining on lip videos introduces a bias, but the model can adapt to internal articulatory cues when lips are unavailable.
- Evidence anchors:
  - [section IV-A] "We first trained the model using only the lip movements by cropping and inputting just the lip region from the rtMRI video. Next, we masked the lip region and trained the model using only the other visible articulatory movements..."
  - [section IV-A] "Inference using only internal articulators (with the lip region masked) results in a 25.33% decrease in WER compared to inference using only the cropped lip region as MRI input."
  - [corpus] Weak; no similar experimental setup found in corpus, so this remains an assumption.
- Break condition: If the rtMRI resolution or noise obscures internal articulators, or if the model cannot learn internal articulator features, performance will drop when lips are masked.

## Foundational Learning

- Concept: Multi-modal self-supervised learning (AV-HuBERT)
  - Why needed here: Enables learning robust articulatory-to-text mappings without acoustic supervision, crucial for noisy MRI data.
  - Quick check question: What is the role of the audio encoder in AV-HuBERT during fine-tuning for text prediction from rtMRI?

- Concept: Flow-based normalizing flows for duration alignment
  - Why needed here: Maps latent text representations to acoustic space while preserving speaker-specific timing learned from noisy audio.
  - Quick check question: How does the normalizing flow interact with the duration predictor to generate aligned speech?

- Concept: Conditional variational autoencoders (VITS)
  - Why needed here: Learns a probabilistic mapping between text and speech, enabling synthesis in novel voices while preserving alignments.
  - Quick check question: What is the difference between the prior and posterior encoders in VITS?

## Architecture Onboarding

- Component map:
  - Input: rtMRI video frames (96x96 pixels, 25 fps)
  - AV-HuBERT fine-tuned for text prediction (CTC loss)
  - VITS-based duration predictor (trained on noisy audio-text pairs)
  - Flow-based normalizing flow (maps latents to acoustic space)
  - HiFi-GAN speech decoder (generates speech from aligned latents)

- Critical path:
  1. rtMRI → AV-HuBERT → predicted text
  2. Text → VITS prior encoder → latents
  3. Latents + duration predictor → expanded latents
  4. Expanded latents → flow → acoustic latents
  5. Acoustic latents → HiFi-GAN decoder → speech

- Design tradeoffs:
  - Using text instead of mel-spectrograms avoids noise but loses fine-grained acoustic detail.
  - Training duration predictor on noisy audio risks propagating noise artifacts.
  - Fine-tuning AV-HuBERT on rtMRI requires careful handling of its audio encoder (replaced with zeros).

- Failure signatures:
  - High WER on predicted text indicates poor visual articulatory feature learning.
  - Distorted or robotic speech indicates misalignment or flow issues.
  - Failure to generalize to unseen speakers suggests overfitting to speaker-specific patterns.

- First 3 experiments:
  1. Validate AV-HuBERT fine-tuning on rtMRI → text prediction (check CER/WER on validation set).
  2. Train VITS duration predictor on noisy audio → text pairs → test duration estimation accuracy.
  3. End-to-end synthesis test: input rtMRI → predicted text → duration prediction → speech synthesis → measure WER on synthesized speech.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the MRI2Speech model be effectively adapted to handle real-time MRI data from languages other than English and French?
- Basis in paper: [explicit] The paper mentions the model's effectiveness across different languages and datasets but does not explore its application to additional languages.
- Why unresolved: The study primarily focuses on English (USC-TIMIT MRI) and French (ASD1) datasets, leaving the model's generalizability to other languages untested.
- What evidence would resolve it: Testing the model on real-time MRI data from speakers of other languages and comparing performance metrics such as WER and CER to those achieved with English and French datasets.

### Open Question 2
- Question: How does the MRI2Speech model perform when applied to real-time MRI data from speakers with speech disorders or atypical vocal tract configurations?
- Basis in paper: [inferred] The paper discusses the potential application of the model in assisting individuals with speech disorders but does not provide experimental results or performance data for such cases.
- Why unresolved: The current evaluation is limited to healthy speakers, and the model's robustness and accuracy in handling atypical articulatory movements remain unknown.
- What evidence would resolve it: Conducting experiments with real-time MRI data from speakers with various speech disorders and analyzing the model's performance in terms of intelligibility and synthesis quality.

### Open Question 3
- Question: Can the MRI2Speech model be extended to incorporate emotional content into the synthesized speech, enhancing expressiveness and naturalness?
- Basis in paper: [explicit] The paper suggests future work on embedding emotive features from articulators into the synthesized speech, indicating that this aspect has not yet been explored.
- Why unresolved: The current model focuses on text prediction and speech synthesis without considering the emotional context, which is a crucial aspect of natural speech.
- What evidence would resolve it: Modifying the model to include emotional annotations in the training data and evaluating the synthesized speech for emotional expressiveness using subjective listening tests or objective metrics like emotional classification accuracy.

## Limitations

- The flow-based duration alignment mechanism lacks direct empirical validation with comparable works in the corpus
- Specific implementation details of the normalizing flow and exact noise characteristics in ground-truth audio are underspecified
- The paper does not disclose specific noise suppression parameters applied to the USC-TIMIT MRI audio

## Confidence

- **High**: The core pipeline (AV-HuBERT fine-tuning → duration prediction → speech synthesis) is well-specified and the reported WER on USC-TIMIT is verifiable with the provided datasets.
- **Medium**: The generalization claims to unseen speakers across different MRI databases are supported by results but depend on the quality of the noisy audio alignment.
- **Low**: The specific implementation details of the normalizing flow and the exact noise characteristics in the ground-truth audio are underspecified.

## Next Checks

1. **Duration Predictor Generalization**: Train the VITS duration predictor on noisy audio from one speaker, then test phoneme duration estimation accuracy on held-out speakers to verify speaker-independent generalization.

2. **Flow Alignment Robustness**: Synthesize speech using clean audio-text pairs (without MRI) to verify the normalizing flow and duration predictor can properly align text to acoustic space in the absence of visual input.

3. **Ablation of Audio Encoder**: During AV-HuBERT fine-tuning, systematically vary the audio encoder replacement (zero-padding vs. learned reconstruction) to quantify its impact on text prediction accuracy from rtMRI.