---
ver: rpa2
title: Can Graph Learning Improve Planning in LLM-based Agents?
arxiv_id: '2405.19119'
source_url: https://arxiv.org/abs/2405.19119
tags:
- task
- planning
- graph
- llms
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates task planning in language agents using
  graph learning methods. The authors formulate task planning as a decision-making
  problem on task graphs, where nodes represent tasks and edges represent dependencies.
---

# Can Graph Learning Improve Planning in LLM-based Agents?

## Quick Facts
- arXiv ID: 2405.19119
- Source URL: https://arxiv.org/abs/2405.19119
- Reference count: 40
- Primary result: GNN-based methods significantly outperform LLM-only approaches for task planning on multiple datasets

## Executive Summary
This paper investigates task planning in language agents using graph learning methods. The authors formulate task planning as a decision-making problem on task graphs, where nodes represent tasks and edges represent dependencies. They demonstrate that LLMs struggle with task graph understanding due to attention bias and auto-regressive loss issues. To address this, they propose integrating graph neural networks (GNNs) with LLMs, introducing both training-free (SGC) and training-based (GraphSAGE) approaches for task retrieval. Extensive experiments across multiple datasets and LLMs show that GNN-based methods significantly outperform existing solutions, with performance gains increasing for larger task graphs.

## Method Summary
The method involves a two-stage approach: first, an LLM decomposes user requests into task steps, then a GNN retrieves appropriate tasks from a task graph based on these steps. The paper introduces training-free methods like SGC (Simple Graph Convolution) using pre-trained language model embeddings, as well as training-based methods like GraphSAGE with Bayesian Personalized Ranking loss. The approach leverages GNNs' ability to directly process graph structures, avoiding the hallucination issues that LLMs face when flattening graphs into sequences. Experiments use datasets including TaskBench, RestBench, and UltraTool, evaluating performance using Node F1, Link F1, and Accuracy metrics.

## Key Results
- GNN-based methods significantly outperform LLM-only approaches across multiple datasets and task graph sizes
- Performance gains increase with larger task graphs, particularly on the UltraTool benchmark with 260 nodes
- Training-free GNNs (SGC) provide competitive performance without requiring model training
- The proposed method achieves better accuracy while requiring less computation time compared to baseline approaches like GraphSearch

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GNNs avoid the hallucination issues that LLMs face when processing task graphs
- Mechanism: GNNs operate directly on the graph structure using message passing, while LLMs must flatten graphs into sequences which leads to misinterpretation
- Core assumption: The task graph structure is well-defined and available for GNN processing
- Evidence anchors:
  - [abstract] "the biases of attention and auto-regressive loss impede LLMs' ability to effectively navigate decision-making on graphs, which is adeptly addressed by graph neural networks (GNNs)"
  - [section 3.2] "LLMs exhibit a certain hallucination ratio, and there is a strong correlation between the hallucination ratio and planning performance"

### Mechanism 2
- Claim: GNNs can effectively solve graph decision-making problems through dynamic programming simulation
- Mechanism: Transformers can simulate DP algorithms on edge lists, but GNNs leverage the graph structure directly for more efficient computation
- Core assumption: The task planning problem can be formulated as a dynamic programming problem on graphs
- Evidence anchors:
  - [section 3.3] "by taking edge lists as the input, a constant-width Transformer can solve graph decision-making problems by simulating dynamic programming algorithms on edge lists"
  - [section 3.3] "GNNs can strictly operate on the task graph, thereby avoiding hallucinations"

### Mechanism 3
- Claim: Training-free GNNs (like SGC) provide effective task retrieval without requiring model training
- Mechanism: SGC uses pre-trained language models to embed task descriptions and performs simple feature aggregation for task selection
- Core assumption: Pre-trained language models can provide adequate embeddings for task descriptions
- Evidence anchors:
  - [section 4.2] "SGC [65] employs a training-free SGC for task retrieval based on decomposed task steps"
  - [section 5.2] "SGC consistently improves performance, underscoring the effectiveness of the proposed method"

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: GNNs form the core mechanism for processing task graphs and avoiding LLM limitations
  - Quick check question: Can you explain how message passing works in a GNN and why it's effective for graph-structured data?

- Concept: Dynamic Programming on Graphs
  - Why needed here: Task planning is formulated as a DP problem on task graphs
  - Quick check question: Can you describe how dynamic programming can be applied to graph problems like shortest path or task planning?

- Concept: Graph Attention Networks (GATs)
  - Why needed here: GATs are one of the GNN variants used for task retrieval with learnable attention mechanisms
  - Quick check question: How does GAT differ from standard GNNs in terms of how it aggregates information from neighbors?

## Architecture Onboarding

- Component map: LLM (task decomposition) -> GNN (task retrieval) -> LLM (parameter filling)
- Critical path: 1. LLM decomposes user request into task steps 2. GNN retrieves appropriate tasks from task graph based on steps 3. LLM fills in task parameters for final execution
- Design tradeoffs: Training-free (SGC) vs training-based (GraphSAGE/GAT) GNNs, Single LLM vs multiple specialized LLMs, Simple greedy selection vs beam search for task retrieval
- Failure signatures: Poor task decomposition by LLM leading to irrelevant GNN retrieval, Inadequate task graph structure causing GNN confusion, Mismatch between task descriptions and actual functionality
- First 3 experiments: 1. Compare LLM direct inference vs SGC on a small task planning dataset 2. Test different GNN variants (SGC, GraphSAGE, GAT) on the same dataset 3. Evaluate training-based vs training-free approaches on a medium-sized dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GNN-based task planning methods scale with the size of the task graph?
- Basis in paper: [explicit] The paper demonstrates that performance gains from GNN-based methods increase with larger task graph sizes, particularly in experiments with the UltraTool benchmark featuring 260 nodes.
- Why unresolved: While the paper shows improved performance with larger graphs, it doesn't provide a detailed analysis of the scaling behavior or identify specific thresholds where performance plateaus or degrades.
- What evidence would resolve it: Conducting experiments with varying task graph sizes (e.g., 50, 100, 200, 500 nodes) and analyzing the performance trends, computational costs, and any observed bottlenecks or limitations.

### Open Question 2
- Question: Can the proposed GNN-based approach be extended to handle dynamic task graphs where tasks and dependencies change over time?
- Basis in paper: [inferred] The paper mentions that training-free GNN methods are necessary when tasks are continuously changing or new tasks are emerging, but doesn't explore how the approach adapts to dynamic graph structures.
- Why unresolved: The current implementation assumes a static task graph, and it's unclear how well the method would perform in scenarios where the graph structure evolves based on user interactions or system updates.
- What evidence would resolve it: Implementing and evaluating the GNN-based method on a dynamic task graph dataset where tasks and dependencies are added, removed, or modified over time, and comparing the performance to static graph approaches.

### Open Question 3
- Question: How does the choice of GNN architecture impact the performance of task planning, and are there specific architectural features that are particularly beneficial?
- Basis in paper: [explicit] The paper compares various GNN architectures (SGC, GCN, GAT, GraphSAGE, GIN, Graph Transformer) and finds that while all improve performance, there are no significant differences between them.
- Why unresolved: The paper doesn't provide a detailed analysis of why different GNN architectures perform similarly or identify specific architectural features that contribute most to the task planning performance.
- What evidence would resolve it: Conducting ablation studies on GNN components (e.g., message passing, aggregation functions, attention mechanisms) and analyzing their individual contributions to task planning performance, potentially leading to insights for designing more effective GNN architectures for this specific application.

## Limitations
- The effectiveness of GNN-based methods depends on having well-structured task graphs, which may require significant manual curation
- Performance improvements may be dataset-specific and may not generalize to all task planning scenarios
- The comparison with baseline methods lacks detailed implementation specifications, making it difficult to isolate the contribution of GNN architecture

## Confidence
- **High Confidence**: The observation that LLMs struggle with task graph understanding due to attention and auto-regressive biases is well-supported by empirical evidence across multiple datasets
- **Medium Confidence**: The claim that GNNs consistently outperform LLMs in task retrieval is supported by experiments, but the magnitude of improvement may depend on specific dataset characteristics and implementation details
- **Medium Confidence**: The assertion that training-free GNNs provide competitive performance without requiring model training is plausible but needs validation across diverse task domains

## Next Checks
1. **Ablation Study on Task Graph Structure**: Systematically vary the complexity and size of task graphs to determine the threshold at which GNN advantages become significant, and identify when LLM-based approaches become inadequate

2. **Cross-Domain Generalization Test**: Evaluate the proposed method on task graphs from different domains (e.g., technical workflows, creative tasks, medical procedures) to assess whether the observed improvements generalize beyond the tested datasets

3. **Implementation Parity Validation**: Re-implement the GraphSearch baseline with identical hyperparameter settings and data preprocessing to isolate the contribution of GNN architecture from implementation-specific advantages