---
ver: rpa2
title: Multimodal Neurodegenerative Disease Subtyping Explained by ChatGPT
arxiv_id: '2402.00137'
source_url: https://arxiv.org/abs/2402.00137
tags:
- disease
- data
- each
- clinical
- imaging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multimodal framework for Alzheimer's disease
  (AD) subtyping using early-stage indicators like imaging, genetics, and clinical
  assessments. The key contribution is a tri-modal co-attention (Tri-COAT) mechanism
  that explicitly learns cross-modal feature associations.
---

# Multimodal Neurodegenerative Disease Subtyping Explained by ChatGPT

## Quick Facts
- **arXiv ID:** 2402.00137
- **Source URL:** https://arxiv.org/abs/2402.00137
- **Reference count:** 37
- **Primary result:** Tri-COAT achieves AUROC of 0.734 ± 0.076 on ADNI dataset for multimodal AD subtyping

## Executive Summary
This paper presents a multimodal framework for Alzheimer's disease subtyping using a tri-modal co-attention (Tri-COAT) mechanism that learns cross-modal feature associations from imaging, genetics, and clinical assessments. The model outperforms single-modality baselines and provides interpretable results through ChatGPT analysis, identifying biologically meaningful cross-modal relationships such as between CD2AP gene, temporal gyrus, and clinical symptoms. The framework demonstrates how multimodal fusion can improve AD subtyping accuracy while maintaining clinical interpretability.

## Method Summary
The Tri-COAT framework uses transformer encoders for each modality (imaging, genetics, clinical) with biologically meaningful tokenization - ROIs for imaging and SNPs with additional attributes for genetics. A tri-modal co-attention mechanism modulates the clinical branch with imaging and genetic information, followed by MLP classification. The model is trained on ADNI data using 10-fold cross-validation and evaluated on AUROC metrics, with interpretability provided by ChatGPT analysis of feature attributions.

## Key Results
- Tri-COAT achieves AUROC of 0.734 ± 0.076 on ADNI dataset
- Outperforms single-modality baselines by learning cross-modal feature associations
- Identifies biologically relevant cross-modal relationships supported by clinical literature
- ChatGPT provides accessible interpretations of model findings for clinical settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tri-COAT outperforms single-modality baselines by learning cross-modal feature associations that single modalities cannot capture.
- Mechanism: The tri-modal co-attention mechanism uses the clinical branch as the "subject" and modulates it with the imaging and genetic branches as "adjectives and adverbs."
- Core assumption: Clinical assessments are most directly related to disease phenotype, while imaging and genetics provide complementary information.
- Evidence anchors: [abstract] "Our proposed model outperforms baseline models and provides insight into key cross-modal feature associations supported by known biological mechanisms."

### Mechanism 2
- Claim: Tokenization into biologically meaningful units improves pattern learning.
- Mechanism: Representing ROIs as tokens with four imaging-derived traits and including additional SNP attributes provides more context for learning.
- Core assumption: Biologically meaningful units and additional attributes provide more relevant information than raw data.
- Evidence anchors: [section] "The imaging tokenization allows to build an initial representation for each ROI rather than each trait."

### Mechanism 3
- Claim: ChatGPT interpretation provides clear, accessible explanations for clinical settings.
- Mechanism: Building prompts based on model predictions and feature attributions allows ChatGPT to explain biological relevance.
- Core assumption: ChatGPT can provide accurate explanations based on model outputs.
- Evidence anchors: [abstract] "Similarly, we build prompts and use large language models, such as ChatGPT, to interpret the findings of our model."

## Foundational Learning

- **Transformer encoders and multi-head attention** - Why needed: Used in single-modality encoders and tri-modal co-attention mechanism to learn representations and cross-modal associations. Quick check: What is the purpose of the multi-head attention mechanism in the transformer encoder?

- **Multimodal fusion strategies (early, intermediate, late)** - Why needed: The paper discusses different fusion strategies and why intermediate fusion is most suitable for AD subtyping. Quick check: What are the main differences between early, intermediate, and late fusion strategies in multimodal deep learning?

- **Feature attribution methods (integrated gradients)** - Why needed: Used to determine most salient features for each patient, which are then used to build prompts for ChatGPT. Quick check: What is the purpose of using integrated gradients for feature attribution in this context?

## Architecture Onboarding

- **Component map:** Imaging encoder → Genetics encoder → Clinical encoder → Tri-modal co-attention → MLP → Classification output

- **Critical path:** Input features → Single-modality encoders → Tri-modal co-attention mechanism → MLP → Final classification output

- **Design tradeoffs:** Tri-modal co-attention enables cross-modal feature learning but increases model complexity compared to single-modality approaches

- **Failure signatures:** Poor performance on specific modalities may indicate tokenization/encoding issues; ineffective co-attention suggests cross-modal associations are not relevant

- **First 3 experiments:**
  1. Train and evaluate model on each modality independently to establish baseline performance
  2. Train and evaluate model with tri-modal co-attention to assess impact of cross-modal learning
  3. Train and evaluate model with different tokenization approaches for each modality

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the generalizability of learned cross-modal feature associations be tested on other heterogeneous neurodegenerative diseases, such as Parkinson's disease?
- **Open Question 2:** How does performance change when using different fusion strategies (early, late, intermediate) on ADNI dataset?
- **Open Question 3:** Can interpretability be further improved by using more advanced LLMs or incorporating additional domain knowledge?

## Limitations

- Generalizability to other neurodegenerative diseases beyond Alzheimer's disease remains untested
- Biological interpretability depends on accuracy and comprehensiveness of LLM training data
- Optimal number of subtypes determined using silhouette score may not capture true disease heterogeneity

## Confidence

- **High Confidence:** Technical implementation of tri-modal co-attention mechanism and superiority over single-modality baselines
- **Medium Confidence:** Biological interpretability provided by ChatGPT (depends on LLM response quality)
- **Medium Confidence:** Clinical relevance of identified subtypes (requires further validation)

## Next Checks

1. Validate model performance on independent Alzheimer's disease cohorts from different institutions
2. Conduct prospective studies to determine whether identified subtypes have different progression trajectories and treatment responses
3. Perform ablation studies to quantify individual contribution of each modality to overall model performance