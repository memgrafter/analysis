---
ver: rpa2
title: On the Effects of Fine-tuning Language Models for Text-Based Reinforcement
  Learning
arxiv_id: '2404.10174'
source_url: https://arxiv.org/abs/2404.10174
tags:
- games
- semantic
- agents
- score
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fine-tuning language models to reinforcement learning rewards leads
  to semantic degeneration, causing a loss of semantic understanding and reduced performance
  in text-based games. Using fixed pre-trained language models instead improves training
  efficiency and robustness to out-of-distribution vocabulary and text perturbations.
---

# On the Effects of Fine-tuning Language Models for Text-Based Reinforcement Learning

## Quick Facts
- arXiv ID: 2404.10174
- Source URL: https://arxiv.org/abs/2404.10174
- Reference count: 22
- Fine-tuning language models to reinforcement learning rewards leads to semantic degeneration, causing a loss of semantic understanding and reduced performance in text-based games.

## Executive Summary
This paper investigates the effects of fine-tuning language models (LMs) for text-based reinforcement learning (TBRL) and identifies a phenomenon called "semantic degeneration." When LMs are fine-tuned using RL rewards, they lose the rich semantic associations learned during pretraining, leading to reduced performance and generalization in text-based games. The authors demonstrate that using fixed pre-trained LMs instead of fine-tuned ones improves training efficiency, robustness to out-of-distribution vocabulary, and resilience to text perturbations like paraphrasing and lexical substitution.

## Method Summary
The authors compare TBRL agents using fixed pre-trained LMs versus fine-tuned LMs across multiple text-based game environments (TextWorld Commonsense and Jericho). They implement a DRRN-like architecture with actor-critic policy learning and experience replay, using three encoder types: Hash (no semantics), Word Embedding (GloVe + GRU), and Transformers (Albert, RoBERTa). Agents are trained on synthetic household cleaning games and classic text adventure games, then evaluated on original games, paraphrased versions, and lexical substitution variants to test robustness to language variations.

## Key Results
- Fixed pre-trained LMs converge faster and achieve higher performance than fine-tuned LMs in text-based reinforcement learning tasks
- Fine-tuned models show reduced robustness to language perturbations (paraphrasing, lexical substitution) compared to fixed models
- Agents with fixed LMs demonstrate better generalization to out-of-distribution vocabulary and text variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning language models on reinforcement learning rewards causes the model to lose semantic associations learned during pretraining, leading to "semantic degeneration."
- Mechanism: When the language model is fine-tuned using RL rewards, the optimization process prioritizes reward-related semantic relationships at the expense of general semantic understanding. This shifts the contextual word vectors such that words that co-occur in rewarded states become more similar, even if they are semantically unrelated in general language use.
- Core assumption: The fine-tuning process with sparse RL rewards creates a distributional shift in word representations that overfits to game-specific semantics.
- Evidence anchors:
  - [abstract] "we describe the occurrence of semantic degeneration as a consequence of inappropriate fine-tuning of language models in text-based reinforcement learning"
  - [section] "We refer to this process as semantic degeneration, because it leads to loss of relevant semantic information, in the LM, that would be crucial to produce generalizable representation"
  - [corpus] Weak evidence - no direct corpus support found for semantic degeneration mechanism

### Mechanism 2
- Claim: Fixed pre-trained language models provide more efficient training and better generalization compared to fine-tuned models in text-based reinforcement learning.
- Mechanism: Pre-trained language models encode rich semantic information that accelerates training by providing meaningful initial representations. Fixed models maintain these representations throughout training, allowing the RL network to learn effectively from stable semantic encodings. This stability enables better generalization to out-of-distribution vocabulary and text perturbations.
- Core assumption: The semantic representations learned during large-scale pretraining contain generalizable information that remains useful for downstream RL tasks when kept fixed.
- Evidence anchors:
  - [abstract] "rich semantic understanding leads to efficient training of text-based RL agents"
  - [section] "The rich semantic information of LMs accelerates training: The results from these experiments show that even an agent without semantic information can properly learn to play the games. However, an agent leveraging the semantic representations from language models are able to: (1) converge more quickly, in training, to a stable score than hash and simple"
  - [corpus] Weak evidence - no direct corpus support found for efficiency claims

### Mechanism 3
- Claim: Agents with fine-tuned language models show reduced robustness to language perturbations (paraphrasing, lexical substitution) compared to agents with fixed models.
- Mechanism: Fine-tuned models adapt their semantic representations to the specific vocabulary and phrasing of the training games. When the observation text is modified through paraphrasing or synonym replacement, these adapted representations no longer align well with the new surface forms, leading to performance degradation. Fixed models maintain general semantic relationships that can handle such variations.
- Core assumption: The semantic relationships encoded in fixed pre-trained models are robust to surface-level variations in language expression.
- Evidence anchors:
  - [abstract] "we describe the shift in the semantic representation of words in the LM, as well as how it affects the performance of the agent in tasks that are semantically similar to the training games"
  - [section] "The fine-tuned agent exhibits a decline in performance while playing the paraphrased and lexical substitution games. This is explained by the fact that the LM has been adjusted to the semantics of the original game, thus, tokens are no longer distributed according to semantic similarity"
  - [corpus] Weak evidence - no direct corpus support found for perturbation robustness claims

## Foundational Learning

- Concept: Semantic representation in language models
  - Why needed here: Understanding how language models encode meaning is crucial for grasping why fine-tuning affects RL performance and generalization
  - Quick check question: How do contextual word embeddings in transformers differ from static embeddings like GloVe in capturing semantic relationships?

- Concept: Reinforcement learning with text-based observations
  - Why needed here: The paper's core argument relies on understanding how language models interface with RL agents in text-based games
  - Quick check question: What are the key challenges in applying RL to text-based games compared to traditional state-based environments?

- Concept: Language model pretraining objectives
  - Why needed here: The paper assumes knowledge of how LMs are pretrained (e.g., masked token prediction) and how this shapes their semantic representations
  - Quick check question: How does the masked language modeling objective used in BERT-style pretraining influence the semantic representations learned by the model?

## Architecture Onboarding

- Component map:
  Text encoder (language model: fixed vs. fine-tuned) -> State-action encoder (GRU-based Q-value predictor) -> Action scorer (linear layer for action probability distribution) -> RL network (actor-critic with experience replay)

- Critical path:
  1. Observation and action texts are encoded using the language model
  2. Encoded representations are processed by the GRU state-action encoder
  3. Q-values are computed and used to derive action probabilities
  4. Action is selected and environment response is received
  5. Reward is used to update RL network parameters (and potentially LM parameters if fine-tuning)

- Design tradeoffs:
  - Fixed LM vs. Fine-tuned LM: Stability and generalization vs. task-specific adaptation
  - Choice of LM architecture (Albert vs. RoBERTa vs. others): Model capacity vs. efficiency
  - Training regime (fixed episodes vs. fixed steps): Consistency across environments vs. flexibility

- Failure signatures:
  - Fine-tuned models showing inconsistent performance across episodes due to changing semantic representations
  - Fixed models failing to capture game-specific semantics, leading to suboptimal policies
  - Both approaches underperforming if the LM architecture is poorly suited to the game's linguistic style

- First 3 experiments:
  1. Train identical RL agents with fixed vs. fine-tuned versions of the same LM on a simple text-based game, comparing convergence speed and final performance
  2. Evaluate both approaches on paraphrased and synonym-replaced versions of the training game to test robustness to language variations
  3. Visualize semantic shift in word representations before and after fine-tuning using dimensionality reduction techniques like t-SNE

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do multilingual LMs perform as semantic representations for text-based games beyond English?
- Basis in paper: [explicit] The authors suggest studying multilingual LMs as semantic representations for text-based games beyond English as a future direction.
- Why unresolved: The paper focuses on English text-based games and popular English LMs. The performance of multilingual LMs on non-English text-based games is not explored.
- What evidence would resolve it: Experiments comparing the performance of multilingual LMs vs. English LMs on non-English text-based games would provide insights into the effectiveness of multilingual LMs as semantic representations.

### Open Question 2
- Question: What are the specific biases and limitations of LM representations that carry over to game playing?
- Basis in paper: [explicit] The authors mention that some limitations of LM representations, such as inability to distinguish between related concepts or certain biases, might carry over to game playing.
- Why unresolved: The paper does not investigate the specific biases and limitations of LM representations in the context of game playing.
- What evidence would resolve it: A detailed analysis of the biases and limitations of LM representations when applied to text-based games would help identify potential issues and areas for improvement.

### Open Question 3
- Question: How does semantic degeneration affect the ability of agents to generalize to games with significantly different vocabulary and semantics?
- Basis in paper: [explicit] The authors discuss semantic degeneration leading to decreased performance in games with paraphrased or lexically substituted observations.
- Why unresolved: The experiments focus on games with slight modifications to vocabulary and semantics. The impact of semantic degeneration on agents' ability to generalize to games with significantly different vocabulary and semantics is not explored.
- What evidence would resolve it: Experiments evaluating agents trained on games with specific vocabulary and semantics on games with significantly different vocabulary and semantics would provide insights into the extent of semantic degeneration's impact on generalization.

## Limitations

- The paper lacks direct empirical evidence for semantic degeneration through corpus analysis of word embeddings
- Experimental results are based on relatively simple synthetic games with constrained vocabulary
- The findings may be specific to the game environments and reward structures used rather than representing a fundamental limitation

## Confidence

**High Confidence:** The experimental results showing that fixed pre-trained language models outperform fine-tuned models in text-based reinforcement learning tasks.

**Medium Confidence:** The interpretation that performance differences are due to "semantic degeneration" in fine-tuned models.

**Low Confidence:** The broader implications for real-world applications and the claim that these findings represent a fundamental limitation of fine-tuning language models for RL.

## Next Checks

1. **Embedding Space Analysis:** Perform direct analysis of word embeddings before and after fine-tuning using semantic similarity benchmarks (e.g., word analogy tasks, semantic textual similarity). Visualize embedding shifts using t-SNE or UMAP and quantify changes in semantic neighborhoods to provide empirical evidence for the semantic degeneration mechanism.

2. **Cross-Domain Transfer Evaluation:** Test the fixed vs. fine-tuned models on a diverse set of text-based tasks beyond games, including question answering, dialogue systems, and instruction following. This would validate whether the observed effects generalize to broader language understanding tasks and identify potential break conditions.

3. **Regularization Ablation Study:** Implement and compare different regularization techniques during fine-tuning (e.g., KL divergence from original embeddings, adversarial training to preserve general semantics) to determine whether semantic degeneration can be mitigated while maintaining task performance. This would test the core assumption that fine-tuning necessarily causes semantic degradation.