---
ver: rpa2
title: Efficient Sequential Decision Making with Large Language Models
arxiv_id: '2406.12125'
source_url: https://arxiv.org/abs/2406.12125
tags:
- algorithm
- llms
- decision
- making
- contextual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new approach to efficiently incorporate large
  language models (LLMs) into sequential decision making using online model selection
  algorithms. The method adaptively balances LLM-powered policies and standard contextual
  bandit algorithms, leveraging the knowledge in LLMs initially and transitioning
  to standard algorithms for long-term adaptation.
---

# Efficient Sequential Decision Making with Large Language Models

## Quick Facts
- arXiv ID: 2406.12125
- Source URL: https://arxiv.org/abs/2406.12125
- Reference count: 8
- Achieves >6x performance gain over baselines while calling LLMs in only 1.5% of time steps

## Executive Summary
This paper proposes a novel approach to integrate large language models (LLMs) into sequential decision making using online model selection algorithms. The framework adaptively balances LLM-powered policies with standard contextual bandit algorithms, leveraging LLM knowledge initially while transitioning to standard algorithms for long-term adaptation. The method avoids expensive model retraining and requires minimal LLM calls, achieving significant performance improvements on benchmark datasets while maintaining computational efficiency.

## Method Summary
The framework combines LLM-powered policies with standard contextual bandit algorithms through online model selection. LLMs are converted to decision-making agents using Algorithm 1, which uses embedding models and similarity measures to select actions. Algorithm 2 orchestrates the combination of these policies with standard bandits like SpannerGreedy, using sampling strategies that control the transition from LLM to bandit algorithms. The sampling probability can follow polynomial/exponential decay or be updated via log-barrier online mirror descent (Algorithm 3) based on importance-weighted loss estimates. The approach is tested on two textual contextual bandit datasets with up to 13,330 actions.

## Key Results
- Achieves more than 6x performance gain over baselines on AmazonCat-13K and OneShotWikiLinks-311 datasets
- LLM calls limited to only 1.5% of total time steps while maintaining high performance
- Framework works effectively with various pretrained LLMs, including smaller models with 80 million parameters
- Statistical efficiency significantly outperforms both traditional decision making algorithms and vanilla LLM agents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early LLM-guided exploration provides high-quality warm-start data that accelerates standard contextual bandit learning
- Mechanism: The online model selection framework initially samples from LLM-powered policies to select informative contexts, then gradually transitions to standard contextual bandit algorithms as they accumulate enough data to adapt to the environment
- Core assumption: LLM outputs contain general knowledge that aligns with high-reward actions in the target environment
- Evidence anchors:
  - [abstract] "Our framework leverages online model selection algorithms to adaptively balance the performance of LLM-powered policies and standard decision making algorithms. Initially, the framework relies more on LLM-powered policies to achieve good initial results."
  - [section 3.1.2] "At the beginning stage, we leverage LLMs to select more informative data to warm start the learning process, and help contextual bandit algorithms learn better."
  - [corpus] Weak evidence - no direct experimental comparison of warm-start effects in corpus
- Break condition: If LLM outputs consistently select suboptimal actions, the warm-start benefit disappears and the framework performs worse than standard contextual bandit algorithms

### Mechanism 2
- Claim: Adaptive sampling probability allows the system to transition from knowledge exploitation to learning adaptation
- Mechanism: The framework uses decaying probability functions (polynomial or exponential) or learning-based updates (log-barrier OMD) to shift sampling weight from LLM policies to contextual bandit algorithms as the environment becomes better understood
- Core assumption: Standard contextual bandit algorithms can achieve better long-term performance than LLM policies once they adapt to the specific environment
- Evidence anchors:
  - [abstract] "As standard decision making algorithms begin to adapt to the environments, it gradually shifts towards these algorithms."
  - [section 3.2.1] "They follow the basic idea of putting more probability on LLM-powered policies at the beginning and gradually transiting probability to standard contextual bandit algorithms."
  - [corpus] Weak evidence - no direct experimental comparison of different probability decay schedules in corpus
- Break condition: If the transition timing is too aggressive or too slow, the system fails to capitalize on either the initial LLM knowledge or the long-term bandit adaptation

### Mechanism 3
- Claim: The importance-weighted loss updates in Algorithm 3 enable regret-minimizing model selection
- Mechanism: Algorithm 3 constructs unbiased loss estimates for each base algorithm and updates sampling probabilities via log-barrier online mirror descent to minimize cumulative regret
- Core assumption: The importance-weighted loss estimates are accurate enough to guide the model selection process
- Evidence anchors:
  - [section 3.2.2] "Algorithm 3 first constructs the standard importance-weighted unbiased loss estimator for all base algorithms (line 1), and then follow log-barrier online mirror descent to update the sampling distribution with respect to the losses (line 3)."
  - [abstract] "Statistically, our approach significantly outperforms both traditional decision making algorithms and vanilla LLM agents."
  - [corpus] Weak evidence - no direct analysis of importance-weighted loss accuracy in corpus
- Break condition: If importance-weighted loss estimates are too noisy, the model selection process becomes unstable and performance degrades

## Foundational Learning

- Concept: Contextual bandits and exploration-exploitation tradeoff
  - Why needed here: The framework balances between exploiting LLM knowledge and exploring with standard bandit algorithms
  - Quick check question: What is the key difference between contextual bandits and multi-armed bandits?

- Concept: Online model selection and expert tracking
  - Why needed here: The framework uses online model selection algorithms to balance multiple base algorithms
  - Quick check question: How does log-barrier OMD differ from Hedge algorithm in model selection?

- Concept: Importance-weighted loss estimation
  - Why needed here: Algorithm 3 uses importance-weighted losses to update sampling probabilities
  - Quick check question: Why do we need importance weighting when estimating losses for non-selected algorithms?

## Architecture Onboarding

- Component map:
  - LLM-powered policies -> Online model selection -> Standard contextual bandit algorithms -> Sampling strategy

- Critical path:
  1. Convert LLMs to policies using Algorithm 1
  2. Initialize sampling probabilities
  3. At each timestep: sample algorithm, get context, take action, observe loss
  4. Update bandit algorithms and sampling strategy
  5. Repeat until horizon

- Design tradeoffs:
  - LLM size vs computational cost: Larger models provide better initial performance but are more expensive to call
  - Transition speed: Faster transitions save LLM calls but may hurt initial performance
  - Exploration vs exploitation: More exploration helps bandit adaptation but wastes LLM calls

- Failure signatures:
  - LLM calls never decrease: Transition probability update is broken or environment is too complex
  - Bandit performance never improves: LLM-selected data is poor quality or bandit algorithm is incompatible
  - Overall performance worse than baselines: Transition timing is incorrect or importance-weighted losses are noisy

- First 3 experiments:
  1. Run Algorithm 2 with a single LLM policy and SpannerGreedy on a small dataset to verify basic functionality
  2. Compare polynomial vs exponential decay schedules on the same dataset to find optimal transition speed
  3. Test with multiple LLM policies (different sizes) to verify the framework can balance multiple knowledge sources

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Algorithm 2 scale with increasing action space size, particularly in scenarios where the action space becomes exponentially larger than the context space?
- Basis in paper: [inferred] The paper demonstrates Algorithm 2's effectiveness on datasets with up to 13,330 actions (AmazonCat-13K), but does not explore performance in much larger action spaces.
- Why unresolved: The paper focuses on datasets with relatively moderate action space sizes. The scalability of Algorithm 2 to extremely large action spaces, where the number of actions could be orders of magnitude larger than the context space, remains untested.
- What evidence would resolve it: Conducting experiments on datasets with exponentially larger action spaces (e.g., 100,000+ actions) and comparing Algorithm 2's performance to baselines would provide insights into its scalability limitations and potential modifications needed for very large action spaces.

### Open Question 2
- Question: Can the online model selection framework be extended to handle environments with non-stationary or evolving contexts, where the underlying data distribution changes over time?
- Basis in paper: [inferred] The paper focuses on stationary contextual bandit settings. The adaptability of Algorithm 2 to non-stationary environments, where the context distribution or optimal policy changes, is not explored.
- Why unresolved: The paper assumes a stationary environment where the optimal policy remains constant. Real-world applications often involve dynamic environments where the underlying data distribution changes, requiring algorithms to adapt continuously.
- What evidence would resolve it: Designing and testing variants of Algorithm 2 that incorporate mechanisms for detecting and adapting to non-stationarity (e.g., sliding window approaches, change-point detection) on datasets with known distribution shifts would assess its robustness in dynamic environments.

### Open Question 3
- Question: What is the impact of prompt engineering and in-context learning on the performance of LLM-powered policies within Algorithm 2, and how sensitive is the overall performance to the quality of prompts?
- Basis in paper: [explicit] The paper mentions that "Additional instructions or prior interaction history can also be incorporated into the prompt design" but does not systematically explore the impact of different prompt strategies on Algorithm 2's performance.
- Why unresolved: The paper uses relatively simple prompt designs for the LLM-powered policies. The sensitivity of Algorithm 2's performance to the quality and sophistication of prompts, and the potential benefits of advanced prompt engineering techniques, remain unexplored.
- What evidence would resolve it: Conducting experiments that systematically vary the prompt design (e.g., using different prompt templates, incorporating interaction history, or using few-shot learning) and measuring the impact on Algorithm 2's performance would quantify the importance of prompt engineering in this framework.

## Limitations

- Performance claims rely heavily on the assumption that LLM knowledge aligns with environment rewards, but this alignment is not directly validated
- Importance-weighted loss estimation mechanism lacks empirical validation for accuracy in this specific application
- No systematic analysis of warm-start effects or comparison with different transition schedules
- Limited testing of framework robustness across different domains and LLM qualities

## Confidence

**High Confidence**: The core framework architecture combining LLM-powered policies with standard contextual bandit algorithms is technically sound and the experimental methodology is rigorous. The reported performance gains on benchmark datasets are credible given the controlled experimental setup.

**Medium Confidence**: The statistical efficiency claims (6x performance gain) are well-supported by experiments, but the computational efficiency claims depend on specific LLM implementation details not fully specified in the paper. The importance-weighted loss estimation approach appears theoretically valid but lacks direct empirical validation.

**Low Confidence**: The generalizability of results across different domains and the robustness of the framework to different LLM qualities and sizes have not been thoroughly tested. The paper doesn't address failure modes where LLM knowledge might be misleading or contradictory to the true environment structure.

## Next Checks

1. **Warm-start validation**: Conduct ablation studies comparing the full framework against variants that either skip the LLM warm-start phase or use randomly initialized policies to isolate the warm-start benefit.

2. **Importance-weighted loss accuracy**: Measure the variance and bias of importance-weighted loss estimates across different algorithms and contexts to validate Algorithm 3's stability and effectiveness.

3. **Cross-domain generalization**: Test the framework on datasets from different domains (e.g., recommendation systems, clinical decision making) to evaluate robustness when LLM knowledge may not align well with environment structure.