---
ver: rpa2
title: Spectral Greedy Coresets for Graph Neural Networks
arxiv_id: '2405.17404'
source_url: https://arxiv.org/abs/2405.17404
tags:
- graph
- coreset
- sggc
- node
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces spectral greedy graph coresets (SGGC), a
  novel method for efficient GNN training on large graphs by selecting informative
  ego-graphs based on their spectral embeddings. The method avoids GNN's interdependent
  nodes issue by selecting ego-graphs (neighborhood subgraphs) independently in the
  graph-spectral domain.
---

# Spectral Greedy Coresets for Graph Neural Networks

## Quick Facts
- arXiv ID: 2405.17404
- Source URL: https://arxiv.org/abs/2405.17404
- Authors: Mucong Ding, Yinhan He, Jundong Li, Furong Huang
- Reference count: 9
- Primary result: SGGC outperforms other coreset methods on ten datasets, scales to millions of nodes, and is significantly faster than graph condensation

## Executive Summary
This paper introduces Spectral Greedy Graph Coresets (SGGC), a novel method for efficient Graph Neural Network (GNN) training on large graphs by selecting informative ego-graphs based on their spectral embeddings. The method addresses the challenge of interdependent node selection in traditional coresets by treating ego-graphs as independent samples in the graph-spectral domain. SGGC operates in two stages: first selecting widely spread ego-graphs using greedy geodesic iterative ascent (GIGA), then refining selection to diversify topologies using submodular maximization. Extensive experiments demonstrate SGGC's superior performance across ten datasets, generalizability to different GNN architectures, and scalability to graphs with millions of nodes.

## Method Summary
SGGC addresses the inefficiency of training GNNs on large graphs by selecting a small subset of ego-graphs that best represent the full graph. The method leverages the smoothness of spectral embeddings to avoid evaluating all possible ego-graphs, using a two-stage greedy selection process. First, GIGA selects widely spread ego-graphs that approximate the average spectral embedding across the graph. Then, submodular maximization refines this selection to ensure topological diversity, improving the approximation of node classification loss. The selected ego-graphs are compressed using PCA, and a weighted GNN is trained on this compressed coreset for inference on the original graph.

## Key Results
- SGGC outperforms other coreset methods by a wide margin across ten datasets
- The method generalizes well across GNN architectures (GCN, GraphSAGE, SGC)
- SGGC is significantly faster than graph condensation while maintaining comparable performance
- Scales to graphs with millions of nodes and works on both high- and low-homophily graphs
- Achieves test accuracy comparable to or better than state-of-the-art approaches like graph condensation

## Why This Works (Mechanism)

### Mechanism 1
Selecting ego-graphs independently in the spectral domain avoids the interdependence problem of node-wise selection. By focusing on ego-graphs centered at nodes, each subgraph is treated as an independent sample, sidestepping the combinatorial complexity of selecting nodes and edges jointly. This works because GNNs applied to large graphs are "local functions" where the output for a node depends only on its ego-graph (neighborhood within depth L).

### Mechanism 2
Spectral embeddings of ego-graphs vary smoothly across the graph, enabling effective coresets without evaluating all embeddings. The high-frequency components of spectral embeddings are small, so nodes with similar spectral embeddings can be excluded without significant loss. This relies on the assumption that real-world graphs are often self-similar, so spectral embeddings of large-enough ego-graphs are close to full-graph embeddings.

### Mechanism 3
Two-stage greedy selection (coarse then refined) approximates both the spectral embedding and the node classification loss. First, widely spread ego-graphs approximate the average spectral embedding (using GIGA). Second, refined selection using submodular maximization ensures topological diversity to better approximate the classification loss. This works because the transformation from spectral to spatial domain is linear and depends on graph topology, so distinctive topologies lead to more distinctive spatial embeddings.

## Foundational Learning

- **Graph spectral domain and Laplacian eigendecomposition**: Understanding how to transform graph data into spectral representations is fundamental to the method's approach. Quick check: What is the relationship between the eigenvalues of the graph Laplacian and the smoothness of eigenvectors?

- **Message passing in GNNs and receptive fields**: The method relies on GNNs being local functions where node outputs depend only on their ego-graphs. Quick check: For a 3-layer GNN with nearest-neighbor message passing, what is the size of a node's receptive field?

- **Submodular optimization and greedy algorithms**: The refined selection stage uses submodular maximization to ensure topological diversity. Quick check: What property of submodular functions makes greedy algorithms effective for maximization?

## Architecture Onboarding

- **Component map**: Input graph (A, X, y) -> Compute diffusion matrix P and eigendecomposition -> Stage 1: GIGA coarse selection -> Stage 2: Submodular maximization refinement -> Compress ego-graph features via PCA -> Train GNN on coreset graph

- **Critical path**: 1) Compute diffusion matrix P = 1/2 In + 1/2 D−1A; 2) Eigendecomposition of normalized Laplacian L = In − D−1/2AD−1/2; 3) Stage 1: GIGA to select widely spread ego-graph centers; 4) Stage 2: Submodular maximization to refine selection; 5) Compress selected ego-graph features via PCA; 6) Train GNN on coreset graph

- **Design tradeoffs**: Ego-graph size vs. compression efficiency (larger ego-graphs provide better approximation but require more storage); Slack parameter κ vs. algorithm behavior (higher κ emphasizes geodesic alignment, lower κ emphasizes linear classification); Max budget vs. selection speed (higher budget accelerates selection but may reduce quality)

- **Failure signatures**: Poor performance on low-homophily graphs (indicates smoothness assumption is violated); Degraded results with very large ego-graphs (suggests diminishing returns or over-smoothing); Memory issues with dense graphs (ego-graph union may become too large despite compression)

- **First 3 experiments**: 1) Compare node-wise selection vs. diffusion ego-graph selection on Cora dataset; 2) Ablation study: SCGIGA alone vs. CRAIG-Linear alone vs. combined SGGC on Citeseer; 3) Test generalizability across GNN architectures (GCN, GraphSAGE, SGC) on Pubmed

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of SGGC compare to graph condensation when the number of training nodes is very large (e.g., millions)? The paper states that SGGC scales to graphs with millions of nodes and is significantly faster than graph condensation while maintaining comparable performance, but does not provide experimental results for graphs with millions of training nodes.

### Open Question 2
How does the choice of ego-graph size affect the performance of SGGC on different types of graphs (e.g., citation networks, social networks)? The paper mentions that the diffusion ego-graph size is an important hyperparameter that needs to be determined for each dataset, but does not provide a systematic study of how ego-graph size affects performance on different types of graphs.

### Open Question 3
Can SGGC be extended to handle graphs with dynamic node and edge additions or deletions? The paper focuses on static graphs, but the concept of selecting informative ego-graphs could potentially be applied to dynamic graphs. The paper does not discuss the application of SGGC to dynamic graphs.

## Limitations
- Performance may degrade on graphs with very large diameter (> 10) where the local GNN assumption may not hold
- The smoothness assumption of spectral embeddings may break down on graphs with extreme heterophily (correlation coefficient < 0.1)
- Limited theoretical guarantees beyond Lipschitz continuity of spectral embeddings, which may not directly translate to improved classification performance

## Confidence

- **High confidence**: SGGC's superior performance compared to existing coreset methods across multiple datasets
- **Medium confidence**: The theoretical claims about spectral smoothness and local GNN behavior
- **Medium confidence**: The generalizability to different GNN architectures, though this is well-supported by experiments

## Next Checks

1. Test SGGC on graphs with high diameter (> 10) to verify the local GNN assumption holds
2. Compare performance with other coreset methods on extreme heterophily graphs (correlation coefficient < 0.1)
3. Evaluate the method with attention-based GNNs to check if the local assumption breaks down