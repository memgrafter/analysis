---
ver: rpa2
title: Robust Decision Aggregation with Adversarial Experts
arxiv_id: '2403.08222'
source_url: https://arxiv.org/abs/2403.08222
tags:
- aggregator
- experts
- loss
- optimal
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes robust information aggregation in the presence
  of adversarial experts. The authors consider a binary decision aggregation problem
  where some experts are truthful while others can report arbitrarily.
---

# Robust Decision Aggregation with Adversarial Experts

## Quick Facts
- arXiv ID: 2403.08222
- Source URL: https://arxiv.org/abs/2403.08222
- Reference count: 40
- Key outcome: Proves truncated mean aggregator is optimal for binary decision aggregation with up to k adversaries, with regret depending only on adversary ratio

## Executive Summary
This paper analyzes robust information aggregation when some experts are adversarial while others are truthful. The authors prove that under certain conditions, the optimal aggregator discards the k lowest and highest reported values and averages the remaining ones. They show this truncated mean approach minimizes regret for both L1 and L2 loss functions when the adversary ratio is below critical thresholds. The regret depends only on the proportion of adversaries, not their total number, and the results extend to multi-state settings.

## Method Summary
The paper uses a minimax framework to find optimal aggregators that minimize worst-case regret against adversarial strategies. For binary state aggregation, the authors derive piecewise linear optimal aggregators for both L1 and L2 loss functions. The method involves analyzing information structures where expert signals are correlated with world states, then determining how an aggregator should combine reports when some experts can report arbitrarily. The theoretical analysis proves that truncated mean aggregators are optimal under specific conditions on the adversary ratio and expert distributions.

## Key Results
- Truncated mean aggregator is optimal for L1 loss when adversary ratio γ < min( aµ−(1−µ)b µ+aµ−(1−µ)b , a 1+a , 1−b 2−b )
- For L2 loss, optimal aggregators are piecewise linear functions with breakpoints at k and n-k
- Regret depends only on adversary ratio γ, not total number of experts n
- Negative results show limitations for general adversarial aggregation problems under broader information structures

## Why This Works (Mechanism)

### Mechanism 1: Truncated Mean Robustness Under Adversarial Pressure
When adversarial experts report extreme values, removing the k lowest and k highest reports and averaging the remainder minimizes regret. The adversarial experts' optimal strategy is to push reports to the extremes (0 or n) to distort the aggregator's output. By discarding these extremes, the aggregator isolates the truthful experts' signals. Core assumption: Adversarial experts are omniscient and collude, and the truthful experts are marginally symmetric. Break condition: If the ratio of adversaries γ exceeds min( aµ−(1−µ)b µ+aµ−(1−µ)b , a 1+a , 1−b 2−b ), the truncated mean may no longer be optimal.

### Mechanism 2: Regret Independence from Total Expert Count
The regret of the optimal aggregator depends only on the ratio of adversaries γ, not the total number of experts n. The adversarial influence is scaled by their proportion in the group. Adding more truthful experts dilutes the impact of the same proportion of adversaries. Core assumption: The marginal distributions of truthful experts remain constant regardless of n. Break condition: If the truthful experts' distributions change with n, or if the adversary-to-truthful ratio crosses critical thresholds.

### Mechanism 3: Piecewise Linear Optimality Under L2 Loss
For L2 loss, the optimal aggregator is piecewise linear with breakpoints at k and n-k. L2 loss penalizes deviations quadratically, encouraging more conservative estimates near the middle. The piecewise structure allows the aggregator to sharply respond to extreme report patterns while maintaining smoothness in intermediate regions. Core assumption: The loss function is quadratic and symmetric, and adversaries can manipulate reports but not the underlying signal distributions. Break condition: If the adversary-to-truthful ratio γ is too high (exceeds min( a 1+a , 1−b 2−b )), the piecewise structure may break down.

## Foundational Learning

- Concept: Information Structures and Joint Distributions
  - Why needed here: The aggregator must reason about how expert signals map to world states without knowing the exact mapping. Understanding information structures is key to designing robust aggregators.
  - Quick check question: If an expert reports "high" with probability 0.8 when the true state is high and 0.1 when it's low, what is the expected report count given a state? (Answer: a = 0.8, b = 0.1)

- Concept: Regret Minimization in Adversarial Settings
  - Why needed here: The goal is to minimize worst-case regret compared to an omniscient benchmark. This frames the problem as a robust optimization.
  - Quick check question: If the aggregator outputs 0.6 when the true state is 1, and the benchmark outputs 1, what is the L1 regret? (Answer: 0.4)

- Concept: Marginal Symmetry of Experts
  - Why needed here: Symmetry ensures that the aggregator can treat all truthful experts identically, simplifying the design and analysis.
  - Quick check question: If all experts share the same prior µ = 0.5 and conditional probabilities a = 0.8, b = 0.1, are they marginally symmetric? (Answer: Yes)

## Architecture Onboarding

- Component map: Input reports -> Count "H" signals -> Check adversary ratio γ -> Apply piecewise formula -> Output probability estimate
- Critical path: 1. Parse input reports into count of "H" signals 2. Check adversary ratio γ = k/n against thresholds 3. Apply appropriate piecewise formula (L1 vs L2 loss) 4. Return probability estimate
- Design tradeoffs:
  - L1 loss: Simpler, encourages decisions, uses truncated mean
  - L2 loss: More conservative, quadratic penalty, piecewise linear
  - Fixed k: Optimal but requires prior knowledge
  - Unknown k: Use upper bound K, revert to K-ignorance strategy
- Failure signatures:
  - γ too high → regret spikes, aggregator becomes uninformative
  - Incorrect a,b,µ estimates → suboptimal aggregator
  - Non-symmetric experts → theoretical guarantees break
- First 3 experiments:
  1. Simulate binary aggregation with k=2, n=10, a=0.8, b=0.1, µ=0.5; verify truncated mean matches theoretical output
  2. Vary γ from 0 to 0.4; plot regret and confirm independence from n
  3. Compare L1 vs L2 aggregator outputs for same inputs; confirm L2 is more conservative

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact regret value for the L2 loss non-adversarial setting as a function of the number of experts n?
- Basis in paper: The paper proves R(Θn) = c - O(1/n) but does not provide the exact closed-form expression for c.
- Why unresolved: The paper states the regret is not independent of n for L2 loss, but does not derive the exact functional form.
- What evidence would resolve it: A closed-form expression for the optimal aggregator under L2 loss without adversaries, which could then be used to calculate the exact regret.

### Open Question 2
- Question: How does the truncated mean aggregator perform under more general information structures beyond binary states and reports?
- Basis in paper: The paper provides negative results for general adversarial aggregation problems but does not explore the performance of truncated mean under these broader settings.
- Why unresolved: The paper focuses on binary decision aggregation and does not extend the analysis to multi-state or non-binary report spaces.
- What evidence would resolve it: Numerical experiments or theoretical analysis of truncated mean performance under general information structures with multi-state worlds or non-binary reports.

### Open Question 3
- Question: What happens to the regret when the ratio of adversaries γ is exactly 1/2?
- Basis in paper: The paper states that when γ ≈ 1/2, the regret reaches the maximum (1/2 for L1 loss, 1/4 for L2 loss), but does not analyze the exact case γ = 1/2.
- Why unresolved: The paper's theorems require γ < 1/2 for the optimal aggregators to satisfy certain constraints, leaving the boundary case unexplored.
- What evidence would resolve it: A theoretical analysis of the adversarial information aggregation problem specifically at the threshold γ = 1/2, determining whether the regret continues to increase or plateaus at the stated maximum values.

## Limitations
- Results rely on assumption of marginally symmetric truthful experts
- Requires known bounds on number of adversaries
- Numerical evaluation limited to single ensemble learning task on CIFAR-10
- Theoretical guarantees break down when adversary ratio exceeds critical thresholds

## Confidence
- High: The truncated mean aggregator is optimal under L1 loss when adversary ratio is below critical thresholds (supported by Theorems 4.2 and 4.6)
- Medium: The piecewise linear aggregator for L2 loss is optimal (supported by Theorem 4.11, but less intuitive than L1 case)
- Medium: Regret independence from total expert count (theoretical result with limited empirical validation)

## Next Checks
1. Test the truncated mean aggregator on non-symmetric expert distributions to quantify performance degradation
2. Evaluate regret scaling empirically by varying both adversary ratio and total expert count across multiple problem domains
3. Compare the proposed aggregators against alternative robust methods (e.g., median-based approaches) in high-adversary scenarios where γ exceeds theoretical thresholds