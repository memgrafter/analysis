---
ver: rpa2
title: Towards Explainability in Legal Outcome Prediction Models
arxiv_id: '2403.16852'
source_url: https://arxiv.org/abs/2403.16852
tags:
- precedent
- legal
- case
- cases
- correlation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the lack of explainability in current legal
  outcome prediction models, which are based on neural networks and cannot provide
  clear reasoning for their decisions. The authors propose a novel method to identify
  the precedent cases relied upon by these models, enabling comparison with human
  judges' reasoning.
---

# Towards Explainability in Legal Outcome Prediction Models

## Quick Facts
- arXiv ID: 2403.16852
- Source URL: https://arxiv.org/abs/2403.16852
- Authors: Josef Valvoda; Ryan Cotterell
- Reference count: 40
- Primary result: Models achieve F1 scores of 0.64-0.68 but show only 0.18 Spearman's ρ correlation with human judges' use of precedent

## Executive Summary
This paper addresses the critical challenge of explainability in legal outcome prediction models, which currently lack transparency in their decision-making processes. The authors propose a novel approach to identify which precedent cases influence model predictions, enabling comparison with human judicial reasoning. They develop a comprehensive taxonomy of legal precedent and employ influence functions to measure the impact of training cases on predictions. Experiments on the European Court of Human Rights (ECtHR) dataset reveal that while models achieve reasonable performance metrics, their reasoning patterns differ significantly from human judges, particularly in distinguishing and negative precedent cases.

## Method Summary
The authors introduce a method to make legal prediction models more interpretable by identifying which precedent cases influence their decisions. They develop a taxonomy categorizing precedent into applied/distinguished and positive/negative types, then use influence functions to measure the effect of each training case on model predictions. The approach involves training neural network models on legal case data and analyzing which training examples most strongly influence specific predictions. This allows for comparison between model reasoning and human judges' documented reasoning patterns, particularly focusing on how models use precedent cases to justify their decisions.

## Key Results
- Models achieve F1 scores of 0.64-0.68 on ECtHR dataset
- Spearman's ρ correlation between model and human precedent usage is only 0.18
- Models struggle particularly with distinguished and negative precedent cases
- The highest correlation achieved was 0.18, indicating significant divergence from human reasoning

## Why This Works (Mechanism)
The approach works by leveraging influence functions, a technique from robust statistics that measures how much each training example affects a model's predictions. By analyzing these influence scores, researchers can identify which precedent cases the model relies on most heavily when making decisions. This creates a window into the model's reasoning process, allowing comparison with documented human judicial reasoning. The taxonomy of precedent usage provides a structured framework for categorizing how both models and humans reference past cases, enabling systematic comparison of reasoning patterns.

## Foundational Learning

1. **Influence Functions** - Statistical tools measuring how much each training example affects model predictions; needed for identifying which precedent cases drive model decisions; quick check: can compute influence scores for all training examples on a given prediction

2. **Legal Precedent Taxonomy** - Classification system distinguishing applied vs. distinguished and positive vs. negative precedent; needed for systematic comparison between model and human reasoning; quick check: can categorize any precedent reference into one of four types

3. **ECtHR Dataset Structure** - European Court of Human Rights case data with documented judicial reasoning; needed as benchmark for comparing model vs. human precedent usage; quick check: dataset contains cases with clear precedent citations and outcomes

## Architecture Onboarding

Component Map: Legal Cases -> Neural Network Model -> Predictions -> Influence Analysis -> Precedent Identification

Critical Path: Training data → Model training → Prediction generation → Influence function computation → Precedent mapping → Comparison with human reasoning

Design Tradeoffs: Neural network complexity vs. interpretability, dataset size vs. computational feasibility, granularity of precedent taxonomy vs. practical applicability

Failure Signatures: Low influence function scores across all training examples, inability to distinguish between different types of precedent, poor correlation with human reasoning patterns

First Experiments:
1. Compute influence scores for a small subset of predictions to validate the methodology
2. Manually verify precedent identification for several cases to ensure accuracy
3. Compare model predictions with and without influence analysis to assess added value

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions beyond the general need for more interpretable legal AI systems and the challenge of bridging the gap between model and human reasoning patterns.

## Limitations

- Limited sample size for comparing model decisions with human judges' reasoning
- Potential oversimplification of legal precedent categorization into four types
- Does not account for temporal evolution of legal reasoning or variations across legal systems
- The 0.18 Spearman's ρ correlation suggests significant limitations in current approach

## Confidence

- Model performance metrics (F1 0.64-0.68): High confidence
- Influence function methodology: High confidence
- Comparison with human reasoning: Medium confidence
- Generalization to other legal systems: Low confidence

## Next Checks

1. Replicate influence function analysis on additional legal datasets from different jurisdictions to test generalizability
2. Conduct larger-scale comparison between model decisions and diverse panel of human judges across multiple cases
3. Test alternative legal precedent taxonomies and their impact on model explainability metrics