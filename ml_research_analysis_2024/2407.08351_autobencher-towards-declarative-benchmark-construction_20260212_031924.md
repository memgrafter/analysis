---
ver: rpa2
title: 'AutoBencher: Towards Declarative Benchmark Construction'
arxiv_id: '2407.08351'
source_url: https://arxiv.org/abs/2407.08351
tags:
- dataset
- autobencher
- datasets
- safety
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AutoBencher, a declarative framework for
  automatic benchmark construction. The core idea is to optimize dataset descriptions
  based on explicit desiderata (difficulty, novelty, salience) using language models
  augmented with privileged information.
---

# AutoBencher: Towards Declarative Benchmark Construction

## Quick Facts
- **arXiv ID**: 2407.08351
- **Source URL**: https://arxiv.org/abs/2407.08351
- **Reference count**: 40
- **Primary result**: AutoBencher creates datasets that elicit 22% more model errors than existing benchmarks, with 27% more novel performance patterns

## Executive Summary
AutoBencher introduces a declarative framework for automatic benchmark construction that optimizes dataset descriptions based on explicit desiderata (difficulty, novelty, salience) using language models augmented with privileged information. The core innovation is an iterative LM-based search that proposes and refines dataset descriptions, generating topic-specific questions and answers to evaluate existing models. By providing privileged information (e.g., Wikipedia articles) only to the evaluator LM but not candidate models, AutoBencher creates information asymmetry that exposes specific model weaknesses. Experiments show AutoBencher outperforms existing benchmarks across multiple dimensions, identifying specific failure modes like Gemini-Pro on Permian Extinction questions and GPT-4o vulnerabilities to harmful cryptocurrency prompts.

## Method Summary
AutoBencher operates through an iterative LM-based search framework that proposes dataset descriptions, generates questions/answers using privileged information, evaluates candidate models, and selects descriptions maximizing the objective function. The evaluator LM (GPT-4) proposes K dataset descriptions and generates corresponding questions using privileged information sources (Wikipedia, Python libraries, translation systems). Candidate models are evaluated on these generated datasets, and their accuracies are tracked in a trajectory that informs future proposals. The framework quantifies desiderata like novelty (rank correlation reduction), difficulty (model error rate), and salience (topic importance) to guide the search. After N iterations, the best-performing dataset description is selected for large-scale generation and human verification.

## Key Results
- AutoBencher datasets elicit 22% more model errors than existing benchmarks
- Generated datasets show 27% more novel performance patterns compared to prior work
- Safety evaluation achieves 20% higher attack success rates than existing safety datasets

## Why This Works (Mechanism)

### Mechanism 1
Iterative refinement with privileged information improves dataset quality by creating information asymmetry between evaluator and candidate models. The evaluator LM generates questions using privileged information (e.g., Wikipedia articles) that candidate models don't have access to, creating harder questions that expose model weaknesses. Core assumption: Information asymmetry between evaluator and candidate models creates more challenging evaluation scenarios. Evidence: Abstract mentions iterative proposal and refinement of dataset descriptions optimized for declared desiderata; section 4.1 describes augmenting LMevaluator with privileged information to improve difficulty and correctness. Break condition: If candidate models gain access to the same privileged information as evaluator, the information asymmetry breaks down.

### Mechanism 2
Adaptive search with trajectory tracking discovers difficult topics by learning from past accuracy patterns. AutoBencher maintains a trajectory of (description, accuracy) pairs from previous iterations and uses this to condition future topic proposals, focusing on areas where candidate models perform poorly. Core assumption: Past model performance patterns on proposed topics can guide discovery of future difficult topics. Evidence: Section 4.2 describes keeping track of trajectory H as sequence of (description, accuracy) pairs and using it to inform subsequent proposals. Break condition: If the candidate model pool changes significantly or if initial trajectory doesn't capture meaningful difficulty patterns.

### Mechanism 3
Declarative optimization converts qualitative benchmark desiderata into quantitative metrics that can be optimized. The framework translates abstract concepts like "difficulty" and "novelty" into concrete mathematical formulas that can be directly optimized through dataset generation. Core assumption: Qualitative benchmark properties can be accurately captured through quantitative metrics. Evidence: Section 3 formalizes NOVELTY as function of dataset, prior datasets, and models; describes using regression to quantify variance explainable by existing datasets. Break condition: If quantitative metrics don't accurately capture intended qualitative properties or if optimizing metrics leads to gaming rather than genuine improvement.

## Foundational Learning

- **Optimization problem formulation with constraints**: Needed to convert benchmark desiderata into mathematical optimization problem that can be solved algorithmically. Quick check: How would you formulate "find a dataset that's both difficult and novel" as a constrained optimization problem?

- **Information asymmetry in evaluation**: Understanding how providing privileged information to evaluator but not candidates creates harder evaluation scenarios. Quick check: Why does giving evaluator access to Wikipedia articles but not candidates make generated questions harder?

- **Regression analysis for novelty measurement**: The novelty metric uses regression to determine how much new information a dataset reveals about model performance. Quick check: How does regressing current model accuracies against previous accuracies help measure novelty?

## Architecture Onboarding

- **Component map**: LMevaluator (GPT-4) -> Generate descriptions and questions with privileged information -> Evaluate candidates (GPT-3.5, Claude-3, Mistral) -> Update trajectory H -> Objective calculator (novelty, difficulty, salience metrics) -> Select best description

- **Critical path**: Propose description → Generate dataset with privileged info → Evaluate candidates → Update trajectory → Select best dataset

- **Design tradeoffs**:
  - Temperature vs determinism: Lower temperature gives consistent results but may miss diverse topics
  - Privileged info vs generalization: More privileged info creates harder questions but may not generalize to real-world scenarios
  - Trajectory length vs exploration: Longer trajectories provide better guidance but may get stuck in local optima

- **Failure signatures**:
  - High accuracy across all candidate models → Questions too easy or privileged info leaking to candidates
  - No improvement over iterations → Trajectory not capturing meaningful patterns or exploration too limited
  - Extremely low salience scores → Description generation failing to focus on important topics

- **First 3 experiments**:
  1. Run with temperature=0 to establish baseline deterministic behavior and verify basic functionality
  2. Run with temperature=1 to test diversity of discovered topics and compare metric distributions
  3. Run without privileged information to measure its impact on difficulty scores (expect significant degradation)

## Open Questions the Paper Calls Out

- How robust are AutoBencher's findings to the choice of evaluator LM? Would using Claude-3.5-sonnet or Llama-3.1-405B as evaluator consistently identify same model weaknesses as GPT-4-turbo?
- Can AutoBencher be extended to discover novel capabilities rather than just weaknesses, and how would this change optimization objectives?
- How does AutoBencher perform on low-resource languages where machine translation quality is poor?

## Limitations
- Framework's reliance on GPT-4 as evaluator introduces single point of failure
- Iterative search may converge to local optima rather than globally optimal datasets
- Quantification of qualitative desiderata (novelty, salience) may not fully capture nuanced requirements of real-world benchmarking

## Confidence

- **High**: The core iterative refinement mechanism with privileged information is well-supported by experimental results showing 22% more model errors and 27% more novel performance patterns
- **Medium**: The adaptive search trajectory approach is theoretically sound but lacks extensive validation across diverse domains
- **Low**: The declarative optimization framework assumes quantitative metrics perfectly capture qualitative desiderata, which may not hold in practice

## Next Checks

1. **Cross-domain validation**: Test AutoBencher on a new domain (e.g., healthcare or legal reasoning) to verify privileged information approach generalizes beyond history and programming

2. **Model independence analysis**: Evaluate whether datasets generated for one model family (e.g., GPT-4) remain challenging for different architectures (e.g., Claude, Gemini) to test for overfitting

3. **Human evaluation study**: Conduct blinded human assessments of generated datasets to validate that AutoBencher's novelty and salience metrics align with human judgments of benchmark quality and importance