---
ver: rpa2
title: Inference Optimization of Foundation Models on AI Accelerators
arxiv_id: '2407.09111'
source_url: https://arxiv.org/abs/2407.09111
tags:
- arxiv
- inference
- memory
- large
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive overview of techniques for
  optimizing inference of large language models (LLMs) on AI accelerators. The key
  outcomes include: System optimization techniques like KV caching, FlashAttention,
  continuous batching, and PagedAttention that reduce redundant computation and memory
  fragmentation while maintaining prediction accuracy.'
---

# Inference Optimization of Foundation Models on AI Accelerators

## Quick Facts
- arXiv ID: 2407.09111
- Source URL: https://arxiv.org/abs/2407.09111
- Reference count: 40
- Primary result: Comprehensive survey of inference optimization techniques for LLMs and foundation models on AI accelerators

## Executive Summary
This paper provides a systematic overview of techniques for optimizing inference of large language models (LLMs) and other foundation models on AI accelerators. The authors categorize optimization approaches into system-level techniques (KV caching, FlashAttention, continuous batching, PagedAttention), architectural improvements (multi-query attention, mixture of experts, sliding window transformers), model compression methods (quantization, pruning, distillation), and fast decoding strategies (speculative decoding). The survey emphasizes that these optimizations are crucial not only for Transformers but also for other foundation models like Stable Diffusion and State Space Models. While significant progress has been made, challenges remain for handling extremely long sequences and context lengths across different hardware accelerators.

## Method Summary
The paper synthesizes existing research on LLM inference optimization without presenting new experimental results. It draws from 40 references to provide a comprehensive survey of techniques including KV caching, FlashAttention, PagedAttention, speculative decoding, and various compression methods. The methodology involves categorizing optimization approaches by their level of abstraction (system, architectural, model-level) and explaining their mechanisms and trade-offs. The authors provide theoretical analysis of each technique's benefits and limitations, though empirical validation is primarily drawn from referenced works rather than original experiments.

## Key Results
- System optimization techniques like KV caching, FlashAttention, continuous batching, and PagedAttention can reduce redundant computation and memory fragmentation while maintaining prediction accuracy
- Architectural improvements including multi-query and grouped-query attention, mixture of experts, and sliding window transformers enhance inference efficiency
- Model compression methods such as quantization, pruning, and distillation trade off some accuracy for improved memory efficiency and inference speed
- Fast decoding strategies like speculative decoding leverage draft tokens to accelerate inference while provably maintaining the same output distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KV caching reduces redundant attention computation by storing and reusing past key-value pairs during autoregressive generation.
- Mechanism: During decoding, each new token requires computing attention scores over all previous tokens. KV caching stores the K and V matrices from earlier steps, so only Q needs to be recomputed for the new token, avoiding quadratic recomputation.
- Core assumption: Attention computation is memory-bound and reusing K,V pairs preserves exact semantics.
- Evidence anchors:
  - [abstract] "System optimization techniques like KV caching... that reduce redundant computation and memory fragmentation while maintaining prediction accuracy."
  - [section] "KV- cache [90] stores and reuses these past key-value pairs, eliminating the need of recalculation for every new token."
  - [corpus] No direct evidence in neighbors; corpus score moderate but no citations.
- Break condition: Cache eviction or block-level partitioning without maintaining correct causal dependencies breaks output semantics.

### Mechanism 2
- Claim: FlashAttention reduces memory accesses by processing attention in blocks that fit in fast SRAM, avoiding materializing the full attention matrix.
- Mechanism: Reformulates attention as block-wise matrix multiplications; each block of K,V is loaded to SRAM, processed with a block of Q, and written back, minimizing HBM traffic.
- Core assumption: Block size can be tuned to fit device memory hierarchy without degrading numerical stability.
- Evidence anchors:
  - [abstract] "System optimization techniques... like... FlashAttention... that reduce redundant computation and memory fragmentation."
  - [section] "FlashAttention [23, 24] was introduced to address these challenges, which reformulates the attention computation as a sequence of matrix multiplications and applies block-sparse decomposition."
  - [corpus] No neighbor papers directly cover FlashAttention; evidence weak.
- Break condition: Block size too large for available SRAM causes spilling and performance collapse.

### Mechanism 3
- Claim: Speculative decoding increases throughput by generating draft tokens with a smaller model and verifying them in bulk with the target model.
- Mechanism: Draft model generates multiple tokens; target model verifies them in one forward pass using rejection sampling; accepted drafts skip sequential decoding steps.
- Core assumption: Draft model is sufficiently aligned to target so that verification cost is low relative to speed gain.
- Evidence anchors:
  - [abstract] "Fast decoding strategies like speculative decoding that leverage draft tokens to accelerate inference while provably maintaining the same output distribution."
  - [section] "Speculative decoding (SD) [18, 63] exploits the fact that multiple draft tokens can be verified in a single forward pass of the target model."
  - [corpus] No direct neighbor evidence; corpus score moderate.
- Break condition: Poor alignment between draft and target increases rejection rate, erasing throughput gains.

## Foundational Learning

- Concept: Transformer attention mechanism (query-key-value scoring)
  - Why needed here: All optimization techniques ultimately reduce attention computation cost or memory footprint.
  - Quick check question: What is the shape of the attention score matrix for a sequence of length L with H heads and head dimension d?
- Concept: Memory hierarchy in AI accelerators (HBM, SRAM, cache)
  - Why needed here: FlashAttention and PagedAttention rely on fitting data into fast memory levels.
  - Quick check question: If SRAM is 16 MB and block size is 128, how many blocks of K,V can be held simultaneously?
- Concept: Quantization and precision trade-offs
  - Why needed here: KV cache quantization and model compression rely on low-precision representations.
  - Quick check question: What is the maximum speedup bound when moving from FP32 to INT8 for weights only?

## Architecture Onboarding

- Component map: Prompt preprocessing -> KV cache lookup/initialization -> Block-wise attention computation (FlashAttention) -> KV cache update/store (PagedAttention) -> Optional speculative draft -> verify -> accept/reject -> Output generation
- Critical path:
  1. Prompt encoding → KV cache lookup/initialization
  2. Block-wise attention computation (FlashAttention)
  3. KV cache update/store (PagedAttention)
  4. Optional speculative draft → verify → accept/reject
  5. Output generation
- Design tradeoffs:
  - Batch size vs. latency: Larger batches improve throughput but increase per-request latency.
  - Block size vs. memory fit: Larger blocks reduce memory traffic but may not fit SRAM.
  - Quantization bits vs. accuracy: Lower bits save memory but may degrade quality.
  - Draft model size vs. acceptance rate: Larger drafts improve alignment but cost more compute.
- Failure signatures:
  - Stalled attention kernels → block size too large for SRAM
  - Degraded output quality → cache eviction policy or quantization error
  - Low speculative decoding throughput → draft/target misalignment or high rejection rate
  - Out-of-memory errors → KV cache or model weights exceed device capacity
- First 3 experiments:
  1. Benchmark baseline attention kernel with synthetic data; measure HBM bandwidth usage.
  2. Implement FlashAttention block size sweep; profile SRAM utilization vs. throughput.
  3. Set up KV cache with different quantization levels; measure memory savings and accuracy drop.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal block size for FlashAttention on different hardware accelerators to maximize computational efficiency while minimizing data movement?
- Basis in paper: [explicit] The paper discusses how FlashAttention reduces memory footprint by processing attention in smaller blocks and mentions that the block size is selected based on the memory hierarchy and capacity of the device, but does not provide specific optimal values for different hardware.
- Why unresolved: The optimal block size depends on the specific memory hierarchy and capacity of each hardware accelerator, which can vary significantly between different types of accelerators (e.g., GPUs, TPUs, Trainium).
- What evidence would resolve it: Empirical studies comparing the performance of FlashAttention with different block sizes on various hardware accelerators would provide concrete data to determine optimal values for each type of device.

### Open Question 2
- Question: How can the memory fragmentation issues in PagedAttention be effectively addressed when distributing KV cache blocks across multiple accelerators with imbalanced block counts?
- Basis in paper: [explicit] The paper mentions that the best practice for handling PagedAttention in distributed settings is yet to be explored, especially when the number of KV cache blocks are imbalanced among accelerators.
- Why unresolved: Current solutions either shard along the sequence dimension or replicate the KV heads for each accelerator, but neither approach fully addresses the issue of memory fragmentation when block counts are imbalanced.
- What evidence would resolve it: Development and evaluation of new strategies for distributing KV cache blocks that minimize memory fragmentation across accelerators, potentially through dynamic block allocation or more sophisticated sharding techniques.

### Open Question 3
- Question: What is the most effective strategy for aligning draft and target models in speculative decoding to maximize acceptance rates and overall speedup?
- Basis in paper: [explicit] The paper discusses that the rejection rate in speculative decoding is equal to the Total Variation divergence between target and draft models' token probabilities, and mentions that knowledge distillation from the target to draft model can improve alignment, but does not provide a definitive answer on the best approach.
- Why unresolved: While knowledge distillation is suggested as a method to align models, the paper does not compare different distillation objectives (e.g., KL-div vs TV-div) or explore other potential alignment strategies.
- What evidence would resolve it: Comparative studies evaluating the performance of different alignment strategies, including various distillation objectives and alternative methods, to determine which approach yields the highest acceptance rates and speedups in speculative decoding.

## Limitations
- The paper lacks empirical validation data - most claims are theoretical or based on referenced works without direct experimental evidence
- Effectiveness of speculative decoding depends heavily on draft-target alignment but no specific alignment thresholds or failure rates are provided
- Memory hierarchy optimizations assume specific hardware configurations that may not generalize across different accelerator types

## Confidence
- High confidence: Foundational concepts of KV caching, attention mechanisms, and basic quantization principles
- Medium confidence: Architectural improvements like multi-query attention and mixture of experts have documented benefits
- Low confidence: Speculative decoding's claimed throughput improvements are highly dependent on draft model quality, which isn't quantified

## Next Checks
1. Implement FlashAttention with varying block sizes on different accelerators (GPU, TPU, CPU) and measure actual SRAM utilization vs theoretical predictions to identify breaking points
2. Train draft models with varying sizes and architectures, measure acceptance rates across different generation tasks, and determine minimum draft quality required for positive throughput gains
3. Create synthetic long-sequence workloads with varying KV cache access patterns, measure fragmentation levels over time, and validate whether PagedAttention's proposed solutions actually prevent performance degradation in practice