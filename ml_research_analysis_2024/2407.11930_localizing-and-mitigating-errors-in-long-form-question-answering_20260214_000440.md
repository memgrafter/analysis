---
ver: rpa2
title: Localizing and Mitigating Errors in Long-form Question Answering
arxiv_id: '2407.11930'
source_url: https://arxiv.org/abs/2407.11930
tags:
- answer
- answers
- feedback
- error
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces HaluQuestQA, the first hallucination dataset
  with localized error annotations for human-written and model-generated long-form
  question answering (LFQA) answers. The dataset contains 698 QA pairs with 1.8k span-level
  error annotations across five error types (question misconception, factuality, relevance,
  completeness, references) by domain experts, along with preference judgments.
---

# Localizing and Mitigating Errors in Long-form Question Answering

## Quick Facts
- arXiv ID: 2407.11930
- Source URL: https://arxiv.org/abs/2407.11930
- Reference count: 40
- One-line primary result: Introduces HaluQuestQA, the first dataset with localized error annotations for LFQA, and demonstrates error-informed refinement improves answer quality

## Executive Summary
This paper addresses the critical issue of errors in long-form question answering (LFQA) by introducing HaluQuestQA, the first dataset with span-level error annotations across five error types. The authors develop an error feedback model that predicts and localizes errors with detailed explanations, then use this feedback to refine answers through a prompt-based approach. Human evaluation shows refined answers are preferred 84% of the time over baseline answers, demonstrating significant improvements in answer quality and comprehensiveness.

## Method Summary
The method consists of three main components: first, the HaluQuestQA dataset is created with 698 QA pairs containing 1.8k span-level error annotations across five error types (question misconception, factuality, relevance, completeness, references). Second, an error feedback model is trained to predict error spans with incomplete information and provide explanations, using a consistency scoring mechanism to ensure reliable feedback. Third, an error-informed refinement approach uses signals from the feedback model to prompt a refinement model to generate improved answers. The entire pipeline is evaluated using both automatic metrics and human judgment.

## Key Results
- HaluQuestQA contains 698 QA pairs with 1.8k span-level error annotations across five error types
- Refined answers reduce errors and increase comprehensiveness, with human evaluation showing 84% preference over baseline answers
- Analysis reveals LFQA answers lack comprehensiveness and provide unhelpful references, with reference hallucinations reducing by 50% using consistency scoring

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Error-informed refinement improves LFQA answer quality by using fine-grained feedback to target specific errors.
- Mechanism: The feedback model identifies incomplete sentences and provides detailed explanations, which the refinement model uses to generate more comprehensive answers.
- Core assumption: Fine-grained error localization is more effective than coarse-grained feedback for improving answer quality.
- Evidence anchors:
  - [abstract] "We train an automatic feedback model on this dataset that predicts error spans with incomplete information and provides associated explanations. Finally, we propose a prompt-based approach, Error-informed refinement, that uses signals from the learned feedback model to refine generated answers, which we show reduces errors and improves answer quality across multiple models."
  - [section 5.2] "While directly prompting the refinement model to generate answers (ZERO-SHOT) or improve answers without detailed feedback (IMPROVE) performs better than the baseline, using more targeted feedback, such as asking the model to complete the answer (GENERIC), consistently leads to higher-quality LFQA answers."
  - [corpus] Weak - only 25 related papers found with low citation counts and average neighbor FMR of 0.592.

### Mechanism 2
- Claim: The consistency score (SR_C) ensures that the feedback model's predictions are reliable and consistent across multiple samples.
- Mechanism: During inference, the feedback model samples 20 responses and calculates a consistency score based on tag consistency and reason consistency. Only the highest scoring output is used as feedback.
- Core assumption: Consistent feedback across multiple samples indicates higher reliability and reduces hallucination.
- Evidence anchors:
  - [section 4.1] "To combat this, we opt for a sampling-based approach (Malon and Zhu, 2024) to provide more consistent feedback. The intuition is that trustworthy details and references should appear in many other generated samples."
  - [section 4.1] "After sampling, reference hallucinations reduce by 50% (from 20% to ~10% of the test set)."
  - [corpus] Weak - no direct evidence in related papers about sampling-based consistency scoring.

### Mechanism 3
- Claim: Human evaluation shows that refined answers are more comprehensive and preferred over baseline answers.
- Mechanism: Annotators compare baseline and refined answers for comprehensiveness and overall quality, finding that refined answers are more complete and preferred in most cases.
- Core assumption: Human judgment is a reliable measure of answer quality and comprehensiveness.
- Evidence anchors:
  - [abstract] "Human evaluation shows refined answers are preferred 84% of the time over baseline answers."
  - [section 5.3] "We observe that refined answers are considered more comprehensive in ~60% of cases and preferred overall in ~84% of comparisons on average across all evaluated datasets."
  - [corpus] Weak - no direct evidence in related papers about human evaluation of LFQA refinement approaches.

## Foundational Learning

- Concept: Error localization and categorization
  - Why needed here: LFQA answers are prone to various errors (factuality, relevance, completeness, etc.) that need to be identified and categorized for effective refinement.
  - Quick check question: Can you explain the difference between error localization and error categorization in the context of LFQA?

- Concept: Fine-grained vs. coarse-grained feedback
  - Why needed here: The paper compares the effectiveness of detailed error feedback with generic improvement prompts to determine which approach leads to better answer quality.
  - Quick check question: Why might fine-grained feedback be more effective than coarse-grained feedback for improving LFQA answers?

- Concept: Consistency scoring in language models
  - Why needed here: The feedback model uses a consistency score to ensure that its predictions are reliable and not hallucinated.
  - Quick check question: How does the consistency score (SR_C) help ensure the reliability of the feedback model's predictions?

## Architecture Onboarding

- Component map:
  - Error Feedback Model -> Consistency Scoring -> Refinement Model -> Refined Answers

- Critical path:
  1. Input question-answer pair to error feedback model
  2. Generate 20 samples from feedback model
  3. Calculate consistency scores (tag and reason)
  4. Select highest scoring sample as feedback
  5. Input question, original answer, and feedback to refinement model
  6. Output refined answer

- Design tradeoffs:
  - Sampling vs. deterministic generation: Sampling provides more consistent feedback but increases inference time.
  - Fine-grained vs. coarse-grained feedback: Fine-grained feedback is more effective but requires more complex error detection.
  - Human evaluation vs. automatic metrics: Human evaluation provides more reliable quality assessment but is more expensive and time-consuming.

- Failure signatures:
  - Low consistency scores (SR_C < 0.80) indicate unreliable feedback.
  - High error rates in refined answers suggest ineffective error detection or refinement.
  - Human evaluators preferring baseline answers indicates the refinement approach is not effective.

- First 3 experiments:
  1. Test the error feedback model's accuracy on sentence-level error detection using the weighted accuracy metric.
  2. Compare the effectiveness of fine-grained vs. coarse-grained feedback on answer quality using TIGERScore and human evaluation.
  3. Evaluate the impact of the consistency score threshold on the reliability of the feedback and the quality of refined answers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of fine-grained feedback compare to coarse-grained feedback across different types of errors (e.g., factual vs. completeness errors)?
- Basis in paper: [explicit] The paper shows fine-grained feedback outperforms coarse-grained feedback on error reduction and F1 scores, but only tests completeness errors in the feedback model.
- Why unresolved: The feedback model was trained specifically on completeness errors, so the comparison between feedback types for other error types (factuality, relevance, etc.) remains untested.
- What evidence would resolve it: Training and evaluating the feedback model on multiple error types, then comparing refinement results using fine-grained versus coarse-grained feedback for each error category.

### Open Question 2
- Question: Would incorporating question misconception detection into the feedback model improve the quality of refined answers?
- Basis in paper: [explicit] The paper identifies question misconception as a significant issue in the dataset but does not incorporate it into the feedback model or refinement approach.
- Why unresolved: The feedback model only addresses completeness errors, leaving potential improvements from handling question misconceptions unexplored.
- What evidence would resolve it: Extending the feedback model to detect and address question misconceptions, then measuring the impact on answer quality and error reduction.

### Open Question 3
- Question: How does the consistency scoring mechanism affect the reliability of the feedback model across different domains and question complexities?
- Basis in paper: [explicit] The paper uses a consistency scoring mechanism (tag and reason consistency) to select reliable feedback, but only reports average consistency scores without domain-specific analysis.
- Why unresolved: The paper does not analyze how consistency varies by domain complexity or question type, which could impact feedback reliability.
- What evidence would resolve it: Analyzing consistency scores across different domains and question complexities to identify patterns in feedback reliability and potential improvements to the scoring mechanism.

## Limitations
- The effectiveness relies heavily on the quality of human annotations in the HaluQuestQA dataset, which may not fully capture real-world LFQA diversity.
- The consistency scoring mechanism may not generalize well to domains outside those covered in the dataset.
- The paper does not address potential biases in the refinement process from training data or prompt templates.

## Confidence

- **High Confidence**: The error localization and categorization framework is well-supported by the HaluQuestQA dataset and demonstrates clear improvements in answer quality.
- **Medium Confidence**: The consistency scoring mechanism's effectiveness is supported by internal evaluations but lacks external validation across diverse domains.
- **Medium Confidence**: Human evaluation results showing 84% preference for refined answers are compelling but may be influenced by specific annotation protocols and evaluator biases.

## Next Checks
1. **Cross-Domain Generalization**: Evaluate the error feedback model's performance on LFQA datasets from domains not represented in HaluQuestQA (e.g., medical, legal) to assess robustness.
2. **Ablation Study on Consistency Threshold**: Systematically vary the consistency score threshold (0.70, 0.80, 0.90) to determine its impact on feedback quality and refinement effectiveness.
3. **Bias Analysis**: Conduct a bias audit of the refinement process by comparing error correction patterns across different question types and demographic groups to ensure equitable performance.