---
ver: rpa2
title: Steering Large Language Models to Evaluate and Amplify Creativity
arxiv_id: '2412.06060'
source_url: https://arxiv.org/abs/2412.06060
tags:
- creative
- creativity
- town
- arxiv
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a method to steer large language models toward
  increased creativity using mechanistic interpretability techniques. The authors
  extract a "creativity direction" in the activation space by contrasting internal
  states when the model generates creative versus uncreative text.
---

# Steering Large Language Models to Evaluate and Amplify Creativity

## Quick Facts
- arXiv ID: 2412.06060
- Source URL: https://arxiv.org/abs/2412.06060
- Authors: Matthew Lyle Olson; Neale Ratzlaff; Musashi Hinck; Shao-yen Tseng; Vasudev Lal
- Reference count: 6
- Key outcome: A method to steer LLMs toward increased creativity using mechanistic interpretability, with scoring that aligns strongly with human judgment

## Executive Summary
This work introduces a mechanistic approach to controlling and measuring creativity in large language models. The authors extract a "creativity direction" in the activation space by contrasting internal states when the model generates creative versus uncreative text. They demonstrate that this direction can be used both to induce more creative generations and to accurately score the creativity of outputs. The scoring method achieves strong alignment with human judgment, outperforming naive prompting baselines, while the steering approach significantly enhances creative writing diversity.

## Method Summary
The method involves three main components working together to control and measure creativity in LLMs. First, a creativity direction is extracted by computing the normalized difference between average activation vectors when the model generates creative versus uncreative text. Second, this direction is used to score creativity by measuring cosine similarity between activation vectors and the creativity direction, averaged over all generated tokens. Third, the direction is applied at inference time to steer the model toward more creative output by adding the creativity vector (scaled by λ=3) to intermediate activations at layer 8. The approach is validated through human pairwise comparisons showing over 70% agreement when a large frontier model identifies more creative generations.

## Key Results
- Creativity scoring via activation cosine similarity achieves strong alignment with human judgment, outperforming naive prompting baselines
- Applying the creativity direction to intermediate activations at layer 8 significantly enhances creative output diversity
- Large frontier models using this method achieve over 70% agreement with human annotators in pairwise creativity comparisons
- The mechanistic approach enables both evaluation and induction of creativity without requiring fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear directions in activation space can represent and manipulate specific concepts like creativity
- Mechanism: The authors extract a "creativity direction" by computing the normalized difference between average activation vectors when the model generates creative versus uncreative text. This direction captures the latent concept of creativity within the model's internal representations
- Core assumption: Creative and uncreative text generations produce distinguishable patterns in the activation space that can be linearly separated
- Evidence anchors:
  - [abstract] "We take a mechanistic approach that extracts differences in the internal states of an LLM when prompted to respond 'boringly' or 'creatively'"
  - [section] "Specific directions corresponding to social bias, refusal, harmlessness, or humor have been identified by prior work [Ratzlaff et al., 2024, Gao et al., 2024, Templeton et al., 2024]"
  - [corpus] Weak - the corpus contains related work on LLM creativity but doesn't directly confirm the linear separability assumption
- Break condition: If creative and uncreative text do not produce linearly separable activation patterns, or if the concept is represented non-linearly in the activation space

### Mechanism 2
- Claim: Adding the creativity direction to intermediate activations amplifies creative output
- Mechanism: By adding the computed creativity direction (scaled by λ=3) to the model's intermediate activations at layer 8, the model is steered toward generating more creative text
- Core assumption: The model's generation process is sensitive to additive interventions in the activation space, and the creativity direction is causal for creative output
- Evidence anchors:
  - [abstract] "We also show these internal state differences can be applied to enhance the creativity of generated text at inference time"
  - [section] "We can steer the LLM to induce increased creativity of generated text by adding the creativity attribute to the intermediate activations at the target layer"
  - [corpus] Weak - related work on activation steering exists but doesn't specifically validate this mechanism for creativity
- Break condition: If the model's generation process is not sensitive to additive activation interventions, or if the creativity direction is not causal for creative output

### Mechanism 3
- Claim: Cosine similarity between activations and the creativity direction provides an accurate creativity score
- Mechanism: The creativity score is computed as the cosine similarity between the normalized activation vector and the creativity direction, averaged over all generated tokens
- Core assumption: Higher similarity between activations and the creativity direction correlates with more creative output as judged by humans
- Evidence anchors:
  - [abstract] "We take a mechanistic approach that extracts differences in the internal states of an LLM...to provide a robust measure of creativity that corresponds strongly with human judgment"
  - [section] "We construct an estimator to score the creativity of generated output leverages the creative steering directions. We find this estimator aligns closely with human judgment, unlike simple prompting baselines"
  - [corpus] Weak - while related work exists on LLM evaluation, direct evidence for this specific scoring method is limited
- Break condition: If the cosine similarity measure does not correlate with human judgments of creativity, or if the relationship is not monotonic

## Foundational Learning

- Concept: Activation steering in LLMs
  - Why needed here: The entire method relies on identifying and manipulating linear directions in activation space to control model behavior
  - Quick check question: How do activation steering methods differ from fine-tuning approaches to modifying model behavior?

- Concept: Contrastive dataset construction
  - Why needed here: The method requires paired creative and uncreative prompts to isolate the creativity direction, requiring careful dataset curation
  - Quick check question: Why is it important that the only difference between contrastive pairs is the presence or absence of the target concept?

- Concept: Cosine similarity as a similarity metric
  - Why needed here: The creativity scoring method uses cosine similarity between activation vectors, which requires understanding this metric
  - Quick check question: What properties of cosine similarity make it suitable for comparing activation vectors in high-dimensional space?

## Architecture Onboarding

- Component map: Creativity direction extraction -> Creativity scoring -> Creativity induction
- Critical path: The most critical path is the extraction of the creativity direction, as this determines the quality of both the scoring and induction methods. This involves collecting contrastive data, computing layer activations, and finding the direction.
- Design tradeoffs: The choice of layer (layer 8 in this case) represents a tradeoff between representation quality and computational efficiency. Earlier layers may capture more fundamental features while later layers may be too task-specific.
- Failure signatures: Poor creativity direction extraction will result in both ineffective scoring and induction. The model may fail to generate meaningful text when intervened on too early in the network, or show little effect when intervened on too late.
- First 3 experiments:
  1. Generate creative and uncreative text using contrastive prompts to verify the dataset quality
  2. Compute creativity directions at different layers and visualize their effects on sample generations
  3. Test the scoring method by comparing its judgments against human annotations on a small sample set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of the creativity steering direction transfer across different model architectures or sizes beyond Llama3-8B and Llama3-70B?
- Basis in paper: [inferred] The paper uses Llama3-8B for creativity extraction and Llama3-70B for frontier model evaluation, but doesn't explore cross-model transfer of the creativity direction.
- Why unresolved: The study focuses on a single architecture family (Llama3) and doesn't test whether the creativity direction learned from one model can be applied to others.
- What evidence would resolve it: Testing the same creativity direction on models like GPT-4, Claude, or other Llama variants, measuring both generation quality and scoring accuracy across models.

### Open Question 2
- Question: How does the creativity steering approach compare to fine-tuning methods that explicitly optimize for creative text generation?
- Basis in paper: [inferred] The paper presents a mechanistic approach but doesn't benchmark against traditional fine-tuning or RLHF methods for enhancing creativity.
- Why unresolved: The authors demonstrate effectiveness but don't provide a direct comparison to established optimization-based approaches for improving creative output.
- What evidence would resolve it: Head-to-head comparisons measuring creativity scores, generation diversity, and human preference between steered models and fine-tuned counterparts.

### Open Question 3
- Question: What are the long-term effects of repeatedly applying the creativity intervention during extended text generation tasks?
- Basis in paper: [explicit] The authors mention "inference time" applications but don't explore iterative or prolonged use of the steering mechanism.
- Why unresolved: The paper demonstrates single-pass creativity induction but doesn't investigate whether cumulative effects enhance or degrade creativity over longer sequences.
- What evidence would resolve it: Experiments tracking creativity scores and generation quality across multiple iterations or longer generation contexts, measuring both enhancement and potential degradation effects.

### Open Question 4
- Question: Can the creativity direction be decomposed into sub-directions representing different aspects of creativity (e.g., originality, emotional depth, narrative complexity)?
- Basis in paper: [inferred] The authors treat creativity as a monolithic concept but don't explore its potential substructure within the activation space.
- Why unresolved: The single creativity vector approach doesn't reveal whether creativity is a unified concept or composed of multiple independent dimensions in the model's representation.
- What evidence would resolve it: Factor analysis of creativity directions, testing whether independent steering of sub-components produces more nuanced creative control or reveals separable aspects of creative expression.

## Limitations
- The method assumes creativity can be captured by a single linear direction in activation space, which may oversimplify the complex, multi-dimensional nature of creative expression
- The effectiveness of the creativity direction may be sensitive to the specific model architecture and layer chosen for intervention
- The evaluation framework relies on pairwise comparisons rather than absolute creativity ratings, which may not fully capture the richness of creative expression

## Confidence

- **High Confidence**: The mechanistic approach of extracting directional differences in activation space is well-established in prior work on activation steering. The implementation details for computing cosine similarity and applying additive interventions are technically sound.
- **Medium Confidence**: The claim that the creativity direction can induce more creative generations is supported by experimental results, but the subjective nature of creativity evaluation introduces uncertainty. The pairwise comparison setup provides stronger evidence than absolute scoring.
- **Low Confidence**: The claim that a single linear direction can capture the full concept of creativity is the most speculative. Creativity encompasses novelty, appropriateness, and value - dimensions that may not align perfectly with the extracted direction.

## Next Checks

1. **Ablation study on layer selection**: Systematically test creativity direction extraction and application at multiple layers (not just layer 8) to determine the optimal intervention point and assess architectural sensitivity.

2. **Dimensionality reduction validation**: Apply techniques like PCA or t-SNE to visualize whether creative and uncreative activations actually form distinct clusters in the activation space, confirming the linear separability assumption.

3. **Cross-dataset generalization test**: Evaluate the extracted creativity direction on entirely different creative writing tasks (e.g., poetry, dialogue, or code) to assess whether it generalizes beyond the Fan et al. (2018) dataset used for direction extraction.