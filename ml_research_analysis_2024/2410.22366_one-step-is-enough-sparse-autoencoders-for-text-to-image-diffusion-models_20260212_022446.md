---
ver: rpa2
title: 'One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion Models'
arxiv_id: '2410.22366'
source_url: https://arxiv.org/abs/2410.22366
tags:
- features
- feature
- source
- images
- sdxl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of sparse autoencoders (SAEs) to analyze
  and manipulate text-to-image diffusion models, focusing on SDXL Turbo. SAEs, previously
  used for large language models, are trained to decompose intermediate representations
  into interpretable features.
---

# One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion Models

## Quick Facts
- arXiv ID: 2410.22366
- Source URL: https://arxiv.org/abs/2410.22366
- Reference count: 40
- Key outcome: Sparse autoencoders can decompose text-to-image diffusion models into interpretable features that generalize across denoising steps and models

## Executive Summary
This paper explores the use of sparse autoencoders (SAEs) to analyze and manipulate text-to-image diffusion models, focusing on SDXL Turbo. SAEs, previously used for large language models, are trained to decompose intermediate representations into interpretable features. The authors train SAEs on the updates performed by transformer blocks within SDXL Turbo's denoising U-Net and demonstrate that these features generalize across different denoising steps and even to other models like SDXL and Flux-schnell. They show that SAE features are interpretable, causally influence the generation process, and reveal specialization among transformer blocks. To quantify the impact of SAE features, they introduce RIEBench, a benchmark for representation-based image editing, which measures features' sensitivity, specificity, and causality. Results show that SAE features enable fine-grained visual feature transfer across denoising processes with significantly fewer features compared to neuron baselines.

## Method Summary
The authors apply sparse autoencoders to text-to-image diffusion models by training them on intermediate representations within the denoising U-Net. They focus on SDXL Turbo and train SAEs to decompose the updates performed by transformer blocks into interpretable features. The trained SAEs are then evaluated for their ability to generalize across different denoising steps and to transfer to other diffusion models. A new benchmark called RIEBench is introduced to measure the sensitivity, specificity, and causality of representation-based editing methods. The paper demonstrates that SAE features can be used for fine-grained visual feature transfer with fewer features than neuron-level baselines.

## Key Results
- SAE features trained on SDXL Turbo generalize across different denoising steps and transfer to other models like SDXL and Flux-schnell
- SAE features are interpretable and causally influence the generation process
- RIEBench benchmark shows SAE features enable fine-grained visual feature transfer with significantly fewer features than neuron baselines

## Why This Works (Mechanism)
SAE features work by decomposing complex intermediate representations in diffusion models into sparse, interpretable components. The sparsity constraint forces the autoencoder to capture the most salient features of the representations, making them more interpretable and potentially more transferable across different contexts. By training on the updates performed by transformer blocks rather than the raw representations, the SAEs capture the dynamic changes that occur during denoising, which may explain their ability to generalize across different denoising steps.

## Foundational Learning

**Sparse Autoencoders**: Neural networks trained to reconstruct their input with a sparsity constraint on the hidden layers. Needed to decompose complex representations into interpretable features. Quick check: Verify that SAEs produce sparse activations (most neurons inactive) during reconstruction.

**Diffusion Models**: Generative models that denoise random noise into coherent images through a series of iterative steps. Needed as the target for SAE analysis. Quick check: Confirm the model follows the standard U-Net architecture with transformer blocks.

**RIEBench**: A benchmark for measuring representation-based editing methods' sensitivity, specificity, and causality. Needed to quantify the impact of SAE features on image editing. Quick check: Ensure the benchmark measures all three aspects (sensitivity, specificity, causality) systematically.

## Architecture Onboarding

**Component Map**: Text prompt -> Text encoder -> Cross-attention -> Transformer blocks (U-Net) -> SAE training -> Interpretable features -> Feature manipulation -> Image generation

**Critical Path**: The transformer blocks within the denoising U-Net are the critical path, as SAEs are trained on their intermediate representations to extract interpretable features.

**Design Tradeoffs**: The paper trades computational efficiency for interpretability by using sparse autoencoders instead of analyzing individual neurons. This approach requires training additional models but provides more interpretable features that generalize better.

**Failure Signatures**: SAE features may fail to generalize if the target diffusion model has significantly different architecture or training objectives. Features might also lose interpretability if the sparsity constraint is not properly tuned.

**First Experiments**:
1. Train SAEs on SDXL Turbo transformer blocks and visualize the learned features for interpretability
2. Test SAE feature generalization across different denoising steps within SDXL Turbo
3. Transfer SAE features to SDXL and Flux-schnell models and evaluate performance

## Open Questions the Paper Calls Out

None

## Limitations

- Generalizability of SAE features across different model architectures and training paradigms remains uncertain
- The causal influence of SAE features on generation quality and diversity is not fully characterized
- Scalability and computational efficiency of SAE training and feature manipulation in production settings is underexplored

## Confidence

- **High confidence**: The interpretability and cross-step generalization of SAE features within SDXL Turbo
- **Medium confidence**: The transferability of SAE features across different diffusion models (SDXL and Flux-schnell)
- **Low confidence**: The practical utility of SAE features for real-world image editing tasks

## Next Checks

1. Cross-architecture validation: Train SAEs on intermediate representations of diverse diffusion models (e.g., Stable Diffusion v1.5, Imagen, DALL-E) and evaluate feature transferability across architectures with different conditioning mechanisms and training objectives.

2. Quality impact assessment: Systematically evaluate how manipulating SAE features affects output quality metrics (FID, CLIP scores) and perceptual quality through human preference studies, comparing against baseline editing methods.

3. Computational overhead analysis: Quantify the training time, memory requirements, and inference latency of SAE-based feature manipulation versus neuron-level baselines, including profiling for different hardware configurations and batch sizes.