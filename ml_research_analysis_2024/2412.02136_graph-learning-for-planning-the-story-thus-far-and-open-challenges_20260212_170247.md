---
ver: rpa2
title: 'Graph Learning for Planning: The Story Thus Far and Open Challenges'
arxiv_id: '2412.02136'
source_url: https://arxiv.org/abs/2412.02136
tags:
- planning
- learning
- graph
- tasks
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Graph learning is a natural fit for planning due to its ability
  to exploit relational structures in planning domains and handle arbitrary numbers
  of objects. This paper studies the use of graph learning for planning by examining
  the theoretical and empirical effects of graph representations, graph learning architectures,
  and optimization formulations for learning.
---

# Graph Learning for Planning: The Story Thus Far and Open Challenges

## Quick Facts
- **arXiv ID**: 2412.02136
- **Source URL**: https://arxiv.org/abs/2412.02136
- **Reference count**: 21
- **Primary result**: Classical ML approaches (specifically WL graph kernels) significantly outperform deep learning methods for symbolic planning tasks

## Executive Summary
This paper examines the use of graph learning for planning by analyzing graph representations, architectures, and optimization formulations. The authors propose the GOOSE framework that learns from small planning tasks to scale up to larger ones. Through theoretical analysis and empirical evaluation, they demonstrate that classical machine learning approaches like WL graph kernels outperform GNNs for planning tasks. The paper identifies five major open challenges in Learning for Planning and provides a comprehensive taxonomy of graph representations and learning approaches.

## Method Summary
The method involves converting planning tasks into graph representations (SLG, ILG, PLOI), applying graph learning models (WL kernels or GNNs), and optimizing for either cost-to-go regression or ranking objectives. The GOOSE framework learns from small training tasks and applies the learned heuristics to larger test tasks. The approach leverages classical ML techniques that can capture the same theoretical expressiveness as MPNNs while being computationally more efficient. Training involves extracting graph features, applying optimization objectives, and using the resulting heuristics in GBFS planning.

## Key Results
- Classical ML approaches (WL kernels) consistently outperform deep learning methods across various metrics and often by several orders of magnitude
- Graph representations effectively capture relational structures in planning domains, enabling learning from small to large tasks
- Ranking-based optimization formulations for heuristic functions outperform cost-to-go estimation approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Graph representations of planning tasks exploit relational structures and allow arbitrary numbers of objects, making them well-suited for planning.
- **Mechanism**: By converting planning tasks into graph structures, the relational dependencies between objects and actions can be captured as nodes and edges. This enables graph learning models to leverage these relationships for improved planning performance.
- **Core assumption**: The relational structure of planning tasks can be effectively captured in graph representations without losing essential information needed for planning.
- **Evidence anchors**: [abstract] "Graph learning is naturally well suited for use in planning due to its ability to exploit relational structures exhibited in planning domains and to take as input planning instances with arbitrary number of objects."
- **Break condition**: If the graph representation loses critical information about the planning domain that cannot be recovered through learning, or if the relational structure cannot be effectively captured in the graph format.

### Mechanism 2
- **Claim**: Classical machine learning approaches (specifically graph kernels like WL) outperform deep learning methods for planning tasks.
- **Mechanism**: Classical ML methods like WL graph kernels can capture the same theoretical expressiveness as MPNNs but with significantly less computational overhead, making them more efficient for planning tasks.
- **Core assumption**: The computational efficiency gained from classical ML approaches translates to better planning performance, especially given the time-sensitive nature of planning.
- **Evidence anchors**: [section] "However, it has been shown very recently that classical ML approaches such as linear graph kernels significantly outperform GNN approaches for planning [CTT24b], over various metrics and often by several orders of magnitude."
- **Break condition**: If the domain characteristics require the expressiveness of deep learning models that cannot be captured by classical approaches, or if the planning tasks become too complex for classical methods to handle effectively.

### Mechanism 3
- **Claim**: Learning ranking functions for heuristic search is more effective than learning cost-to-go estimates.
- **Mechanism**: By framing heuristic functions as ranking states rather than estimating their cost-to-go, the optimization aligns better with the actual goal of heuristic search (ranking states for expansion) and allows for a larger hypothesis space.
- **Core assumption**: The ranking of states is a more natural representation for heuristic search than absolute cost estimates, and this framing leads to better optimization outcomes.
- **Evidence anchors**: [section] "Garrett et al. [GKL16] proposed to frame heuristic functions not as learning cost-to-go estimates but as ranking states for expansion in GBFS."
- **Break condition**: If the ranking optimization becomes too computationally expensive or if the ranking space is too complex to learn effectively, making cost-to-go estimates a more practical choice.

## Foundational Learning

- **Concept: Graph Neural Networks and their expressive limits**
  - Why needed here: Understanding the theoretical foundations of GNNs helps explain why certain approaches (like WL kernels) can match their performance while being more efficient.
  - Quick check question: What theoretical result bounds the expressive power of MPNNs, and how does this relate to WL graph kernels?

- **Concept: Classical vs. Deep Learning tradeoffs**
  - Why needed here: The paper makes a strong case for classical ML approaches over deep learning for planning, so understanding these tradeoffs is crucial.
  - Quick check question: What are the key computational differences between classical ML approaches like WL kernels and deep learning approaches like GNNs?

- **Concept: Heuristic search fundamentals**
  - Why needed here: The paper focuses on learning heuristics for search algorithms, so understanding how heuristic search works is essential.
  - Quick check question: How do heuristic functions guide search algorithms like GBFS, and what properties make a heuristic function "good"?

## Architecture Onboarding

- **Component map**: Planning tasks -> Graph representations (SLG, ILG, PLOI) -> Graph learning model (WL kernel or GNN) -> Optimization (regression or ranking) -> Heuristic functions -> GBFS planning

- **Critical path**:
  1. Convert planning tasks to graph representations
  2. Extract features using graph learning model
  3. Train on optimization objective (regression or ranking)
  4. Use learned heuristics in planning search

- **Design tradeoffs**:
  - Graph representation choice: Grounded vs. lifted graphs with different expressivity levels
  - Learning model choice: Classical ML (WL kernel) vs. deep learning (GNN)
  - Optimization objective: Cost-to-go regression vs. ranking optimization
  - Tradeoff between model expressiveness and computational efficiency

- **Failure signatures**:
  - Poor planning performance despite good training metrics
  - Excessive training or inference time compared to baseline planners
  - Inability to generalize to larger planning tasks
  - Convergence issues during training

- **First 3 experiments**:
  1. **Baseline comparison**: Implement and compare WL kernel and GNN approaches on a small planning domain to verify the performance difference
  2. **Representation impact**: Test different graph representations (SLG vs. ILG) on the same domain to measure the effect of expressivity
  3. **Optimization comparison**: Compare cost-to-go regression vs. ranking optimization on the same graph representation and learning model

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can we develop graph learning models that achieve PSPACE-completeness for propositional planning, given that current MPNNs are limited by two-variable counting logics?
- **Basis in paper**: [explicit] The paper explicitly states that "Propositional planning is PSPACE-complete in general" and that "tractable graph learning architectures alone have yet to be able to achieve expressivity for the basic P-time complexity time which is not directly achievable with finite-variable counting logics."
- **Why unresolved**: Current graph learning architectures like MPNNs are theoretically bounded by two-variable counting logics, which cannot express PSPACE-complete problems. Developing models with higher expressivity while maintaining tractability remains an open challenge.
- **What evidence would resolve it**: A graph learning model that can solve a wide range of PSPACE-complete planning benchmarks, demonstrating both high expressivity and practical performance.

### Open Question 2
- **Question**: What theoretical framework can be developed to understand generalization in Learning for Planning, given that testing tasks are arbitrarily large and drawn from a different distribution than training tasks?
- **Basis in paper**: [explicit] The paper states that "L4P is inherently an out-of-distribution task as testing tasks are arbitrarily large and hence are drawn from a different distribution from bounded-size training tasks" and that "exploiting common generalisation theory tools...for bounding generalisation theory is not straightforward."
- **Why unresolved**: Traditional generalization theory tools assume similar training and testing distributions, which doesn't apply to L4P. The unique characteristics of planning domains, such as rich relational structures, require new theoretical approaches.
- **What evidence would resolve it**: A generalization bound for L4P that accounts for the arbitrary size of testing tasks and the unique characteristics of planning domains, validated through empirical experiments.

### Open Question 3
- **Question**: What is the optimal optimization criterion for Learning for Planning, given that there is no single criterion that is best for all planning domains?
- **Basis in paper**: [explicit] The paper states that "there is no clear consensus on the best optimisation criteria for learning for planning as this may depend on the domain, learning architecture and also training data" and that "theoretical and empirical results on optimisation criteria that are well suited for specific planning domain characteristics are still unknown."
- **Why unresolved**: The choice of optimization criterion depends on various factors, and there is no universal best approach. Different domains may require different criteria, such as learning cost-to-go estimates, ranking functions, policies, or subgoals.
- **What evidence would resolve it**: A comprehensive study comparing different optimization criteria across various planning domains, identifying the strengths and weaknesses of each approach and providing guidelines for selecting the most appropriate criterion.

## Limitations

- The computational efficiency gains of WL kernels over GNNs, while theoretically sound, need broader empirical validation across diverse planning domains
- The assumption that ranking functions are inherently better suited for heuristic search than cost-to-go estimates requires further investigation, particularly in domains where precise cost information is crucial
- The framework's performance on extremely large planning tasks (beyond current benchmarks) remains to be tested

## Confidence

- **High confidence**: The theoretical foundation linking graph representations to planning expressiveness, and the general framework of GOOSE for learning from small to large tasks
- **Medium confidence**: The superiority of classical ML approaches over deep learning, based on recent empirical results but requiring broader validation
- **Medium confidence**: The effectiveness of ranking optimization over cost-to-go regression, though this appears to be domain-dependent

## Next Checks

1. **Cross-domain validation**: Test WL kernel and GNN approaches across at least 5 diverse planning domains beyond IPC23LT to verify generalizability of performance differences
2. **Scaling analysis**: Systematically vary problem sizes to quantify how performance differences between classical and deep learning approaches evolve with problem complexity
3. **Ablation study**: Isolate the contribution of ranking optimization by comparing against cost-to-go estimates on identical graph representations and learning architectures