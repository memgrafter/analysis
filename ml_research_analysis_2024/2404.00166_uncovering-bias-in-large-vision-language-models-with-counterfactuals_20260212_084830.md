---
ver: rpa2
title: Uncovering Bias in Large Vision-Language Models with Counterfactuals
arxiv_id: '2404.00166'
source_url: https://arxiv.org/abs/2404.00166
tags:
- llava-1
- bakllava-v1
- instructblip-vicuna-7b
- scores
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates social biases in large vision-language
  models (LVLMs) by generating text responses to open-ended prompts while varying
  input images depicting different intersectional social attributes (race, gender,
  physical characteristics). Using the SocialCounterfactuals dataset of 171k synthetic
  images, the authors systematically evaluate five LVLMs across multiple prompts and
  produce over 12 million generations.
---

# Uncovering Bias in Large Vision-Language Models with Counterfactuals

## Quick Facts
- **arXiv ID**: 2404.00166
- **Source URL**: https://arxiv.org/abs/2404.00166
- **Reference count**: 40
- **Primary result**: Study reveals significant toxicity and competency bias differences across social groups in LVLMs using counterfactual image analysis

## Executive Summary
This study investigates social biases in large vision-language models (LVLMs) by systematically evaluating how these models respond to images depicting different intersectional social attributes. Using the SocialCounterfactuals dataset of 171k synthetic images, the authors test five LVLMs across multiple prompts, generating over 12 million responses. The analysis combines automated toxicity scoring via Perspective API with lexical analysis of competency-related words, revealing significant differences in how LVLMs describe individuals across racial and gender attributes. Results show concerning patterns of bias, with some models showing notably higher toxicity scores for certain social groups.

## Method Summary
The study employs a counterfactual image analysis approach, using the SocialCounterfactuals dataset containing synthetic images of people across various occupations with systematically varied social attributes (race, gender, physical characteristics). Five LVLMs are evaluated across six open-ended prompts, with text generations analyzed using Perspective API toxicity classifiers and lexical analysis based on the Stereotype Content Model framework. The methodology isolates social attribute effects by holding non-social image details constant across counterfactual sets, enabling attribution of output differences specifically to social characteristics.

## Key Results
- InstructBLIP shows highest toxicity scores across social groups, with Flirtation scores of 0.94 for White males
- BakLLaV A demonstrates highest flirtation scores at 0.68 for Black males
- Significant differences in competence word frequency across race-gender groups, with higher competence counts for White individuals compared to Black individuals
- GPT-4 Vision refuses to answer prompts for images depicting certain social groups, potentially introducing bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual image variation isolates social attribute effects on LVLM outputs
- Mechanism: By holding all visual details constant except targeted social attributes, the study creates controlled comparisons where output differences can be attributed to those attributes rather than confounding image factors
- Core assumption: SocialCounterfactuals dataset images are sufficiently similar in non-social attributes to enable valid attribution
- Evidence anchors: [abstract] "Our use of counterfactual images allows us to isolate the influence of social attributes depicted in images on text generated by LVLMs because other image details (e.g., image background) are held constant"

### Mechanism 2
- Claim: Perspective API toxicity scoring reveals systematic bias patterns
- Mechanism: By applying standardized toxicity classifiers to LVLM outputs across social groups, the study quantifies bias as measurable differences in toxicity scores between groups
- Core assumption: Perspective API provides reliable, consistent toxicity measurements across different social contexts and content types
- Evidence anchors: [section] "For each model and counterfactual set, we calculate the maximum of the Perspective API's toxicity scores across model generations for the images in the set depicting different intersectional social attributes"

### Mechanism 3
- Claim: Lexical analysis of competence words reveals social bias beyond toxicity
- Mechanism: By counting competence-related words using the Stereotype Content Model framework, the study identifies how LVLMs differentially describe social groups' capabilities
- Core assumption: Word frequency differences reflect underlying model bias rather than random variation
- Evidence anchors: [section] "From the field of social psychology, Fiske [3] presents the widely-accepted Stereotype Content Model, which proposes that social stereotypes can be mapped to two primary dimensions of warmth and competence"

## Foundational Learning

- **Concept: Counterfactual analysis methodology**
  - Why needed here: Enables isolation of specific variable effects by holding all other factors constant
  - Quick check question: What makes counterfactual images different from simply using diverse real-world images for bias testing?

- **Concept: Stereotype Content Model dimensions**
  - Why needed here: Provides theoretical framework for understanding how social biases manifest in language
  - Quick check question: According to the Stereotype Content Model, what are the two primary dimensions used to map social stereotypes?

- **Concept: Toxicity classifier operation**
  - Why needed here: Understanding how automated toxicity detection works is crucial for interpreting study results
  - Quick check question: How does Perspective API's scoring system work to quantify different types of toxic content?

## Architecture Onboarding

- **Component map**: SocialCounterfactuals dataset (171k synthetic images) → 5 LVLMs generating text responses to 6 prompts → Perspective API toxicity classifiers + lexical analysis → Comparative analysis across social groups

- **Critical path**: Image → LVLM generation → toxicity scoring → lexical analysis → statistical comparison

- **Design tradeoffs**: Large-scale generation (12M+ responses) vs. API cost constraints for GPT-4 Vision; synthetic data vs. real-world representativeness

- **Failure signatures**: High variance in toxicity scores within social groups suggests confounding factors; systematic refusal patterns may indicate guardrail bias

- **First 3 experiments**:
  1. Generate responses for a single counterfactual set across all 5 LVLMs and 6 prompts to validate setup
  2. Compare toxicity scores for images with identical non-social attributes but varying race
  3. Analyze competence word frequency differences between two specific race-gender groups across occupations

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic nature of SocialCounterfactuals images may not capture full complexity of real-world visual contexts
- Automated toxicity classifiers may have limitations in handling context and cultural nuances
- Results based on occupation-specific prompts may not generalize to other contexts where LVLMs are deployed

## Confidence

- **High confidence**: Counterfactual methodology can isolate social attribute effects when implemented correctly
- **Medium confidence**: Significant toxicity and competence bias differences exist across social groups in LVLMs
- **Medium confidence**: InstructBLIP shows highest toxicity scores and BakLLaV A shows highest flirtation scores across social groups

## Next Checks
1. Conduct manual toxicity and bias assessments by human raters on a subset of LVLM outputs to validate whether Perspective API scores align with human judgment across different social groups.

2. Test the same methodology using real-world photographs (rather than synthetic counterfactuals) to verify whether observed bias patterns persist when using naturally-occurring images.

3. Systematically vary the open-ended prompts to test whether bias patterns remain consistent across different types of queries and task descriptions.