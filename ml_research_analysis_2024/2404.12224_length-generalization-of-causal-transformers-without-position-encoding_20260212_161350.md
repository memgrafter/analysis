---
ver: rpa2
title: Length Generalization of Causal Transformers without Position Encoding
arxiv_id: '2404.12224'
source_url: https://arxiv.org/abs/2404.12224
tags:
- nope
- length
- layer
- attention
- scale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies length generalization of Transformers without
  explicit position encodings (NoPE). The authors find that while NoPE can generalize
  to longer sequences than models with explicit position encodings, it still has limited
  context length and eventually fails when extended beyond a certain range.
---

# Length Generalization of Causal Transformers without Position Encoding

## Quick Facts
- arXiv ID: 2404.12224
- Source URL: https://arxiv.org/abs/2404.12224
- Authors: Jie Wang; Tao Ji; Yuanbin Wu; Hang Yan; Tao Gui; Qi Zhang; Xuanjing Huang; Xiaoling Wang
- Reference count: 40
- Key outcome: Head-based attention scaling enables NoPE models to generalize to 18K context length without fine-tuning

## Executive Summary
This paper addresses length generalization in causal Transformers without explicit position encodings (NoPE). The authors identify that NoPE models fail during sequence extension due to increasing attention entropy, which causes attention distributions to become more uniform. They propose a parameter-efficient method that learns individual temperature scaling factors for each attention head, substantially expanding NoPE's context size. Experiments show this approach achieves competitive performance with state-of-the-art length generalization algorithms for models with explicit position encodings.

## Method Summary
The method involves pre-training a NoPE model (1.1B parameters, 22 layers, 32 heads) on standard data, then analyzing attention entropy patterns during sequence extension. To address entropy increase, the authors implement two scaling approaches: UniformScale (single temperature scaling factor) and HeadScale (individual scaling factors per attention head). HeadScale uses parameter-efficient fine-tuning to optimize 704 scaling factors on a small subset (0.03%) of pre-training data. The scaling factors are constrained to prevent attention from becoming too uniform (λ(h) ≥ 1/√d).

## Key Results
- NoPE models can extend to longer sequences than models with explicit position encodings, but still have limited context length
- Head-based scaling with 704 learnable parameters substantially expands NoPE's context size to 18K tokens
- NoPE with head-based scaling achieves competitive performance with SOTA length generalization algorithms
- The entropy inflection point in attention distributions highly correlates with perplexity degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention scaling directly controls attention entropy, which determines length generalization capability
- Mechanism: The uniform attention scale parameter λ scales the softmax temperature in self-attention. Increasing λ makes attention distributions more peaked (lower entropy), counteracting the entropy increase that occurs during sequence extension.
- Core assumption: Attention entropy is the primary bottleneck in length generalization for NoPE models
- Evidence anchors: Attention pattern visualization shows entropy-perplexity correlation; NoPE failure linked to attention distribution distraction

### Mechanism 2
- Claim: Different attention heads have inherently different entropy patterns, requiring individualized scaling factors
- Mechanism: Each attention head converges to different entropy levels during training. Heads with lower entropy need larger scaling factors to maintain concentration during extension, while heads with higher entropy need smaller scaling factors.
- Core assumption: Attention heads are not homogeneous in their entropy behavior
- Evidence anchors: Entropy values span a broad spectrum across heads; correlation is layer-dependent with concentrated heads requiring larger scales

### Mechanism 3
- Claim: Parameter-efficient fine-tuning can effectively search for optimal head-based scaling factors without extensive retraining
- Mechanism: Treating head-based scaling factors as trainable parameters and optimizing them on a small subset of extended-length data finds values enabling successful length generalization while keeping tunable parameters extremely small (704 vs 1B total parameters).
- Core assumption: Optimal scaling factors can be found through efficient search rather than exhaustive grid search
- Evidence anchors: Only 704 delta parameters over 1B model parameters; 0.03% of pre-training data used for fine-tuning

## Foundational Learning

- Concept: Self-attention mechanism and attention entropy
  - Why needed here: Understanding how attention distributions change with sequence length is central to this work
  - Quick check question: What happens to attention entropy when a sequence extends beyond training length in NoPE models?

- Concept: Positional encoding and its role in transformers
  - Why needed here: The paper contrasts NoPE with models using explicit position encodings like RoPE
  - Quick check question: How does the absence of explicit position encodings affect a transformer's ability to track token positions?

- Concept: Parameter-efficient fine-tuning techniques
  - Why needed here: The head-based scaling method uses a PEFT approach to search for optimal hyperparameters
  - Quick check question: What are the advantages of tuning only 704 parameters versus full fine-tuning for 1B parameters?

## Architecture Onboarding

- Component map: Base NoPE model (1.1B parameters, 22 layers, 32 heads) -> Attention mechanism with temperature scaling parameter λ -> Head-based scaling approach with 704 individual λ(h) parameters -> Parameter-efficient fine-tuning module -> Evaluation framework for length generalization

- Critical path: Pre-train NoPE model without position encodings -> Analyze attention entropy patterns during sequence extension -> Implement uniform scaling baseline -> Develop head-based scaling with PEFT search -> Evaluate on language modeling and synthetic tasks -> Test on real-world long context benchmarks

- Design tradeoffs: Uniform scaling is simple but suboptimal due to head heterogeneity; Head-based scaling is more effective but requires careful initialization and constraint enforcement; PEFT approach minimizes compute but may miss optimal configurations requiring full fine-tuning

- Failure signatures: Uniform scaling causes performance degradation at both extended and original lengths; Head-based scaling without proper initialization produces unstable results; Missing focus constraint makes attention become distracted rather than concentrated

- First 3 experiments: Measure attention entropy across all heads at different sequence lengths to confirm entropy-perplexity correlation; Test uniform scaling with various λ values on extended sequences to find optimal single-scale values; Implement head-based scaling with different initialization strategies to compare stability and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural changes to NoPE models could further improve length generalization beyond the head-based attention scale method?
- Basis in paper: The paper mentions that current NoPE models underperform compared to state-of-the-art RoPE models, particularly in long sequence language modeling tasks and LongBench tasks
- Why unresolved: While the head-based attention scale method improves performance, the paper notes that NoPE still lags behind RoPE baselines, suggesting room for architectural improvements
- What evidence would resolve it: Comparative experiments testing various architectural modifications alongside the head-based scale method would reveal which changes most effectively enhance length generalization

### Open Question 2
- Question: What is the theoretical upper limit of length generalization for NoPE models, and how does it compare to models with explicit position encodings?
- Basis in paper: The paper demonstrates successful length generalization up to 18K tokens but does not explore the absolute maximum achievable length or provide a theoretical framework for understanding this limit
- Why unresolved: The paper focuses on empirical results up to 18K tokens without establishing a theoretical bound or comparing the ultimate generalization capacity to explicit encoding methods
- What evidence would resolve it: Theoretical analysis combined with systematic experiments testing NoPE models at increasingly extreme lengths would establish practical and theoretical limits of NoPE length generalization

### Open Question 3
- Question: How do different attention head patterns (concentrated vs. dispersed) emerge during NoPE training, and can these patterns be predicted or controlled during initialization?
- Basis in paper: The paper observes that different attention heads exhibit distinct entropy patterns, with some heads showing concentrated attention (entropy ≈ 1) and others dispersed patterns (entropy ≈ 10)
- Why unresolved: While the paper identifies these patterns, it does not investigate the underlying mechanisms that cause certain heads to develop concentrated versus dispersed attention distributions during training
- What evidence would resolve it: Detailed analysis of training dynamics including attention entropy evolution over training steps and correlation with initialization parameters would reveal mechanisms governing head pattern development

## Limitations

- The paper's findings rely on the assumption that attention entropy is the primary bottleneck for NoPE length generalization, though alternative explanations are not systematically ruled out
- The parameter-efficient fine-tuning approach searches a relatively constrained space of scaling factors, potentially missing optimal configurations requiring full fine-tuning
- Evaluation focuses heavily on perplexity and passkey retrieval accuracy, with limited qualitative analysis of what the model actually learns about position tracking

## Confidence

**High confidence**: The empirical observation that NoPE attention distributions become more uniform during sequence extension, leading to performance degradation. This is directly observable from attention entropy measurements and perplexity curves.

**Medium confidence**: The causal mechanism linking attention entropy to length generalization failure. While the correlation is strong, the paper does not definitively prove that controlling entropy is sufficient to address all NoPE extension challenges.

**Medium confidence**: The effectiveness of the head-based scaling approach. The method shows consistent improvements across multiple tasks, but the optimization process's sensitivity to initialization and hyperparameters is not fully characterized.

**Low confidence**: The claim that the proposed method achieves "competitive performance" with state-of-the-art algorithms for models with explicit position encodings. The comparison is limited to average scores without deeper analysis of when and why differences occur.

## Next Checks

1. **Ablation study on focus constraint**: Remove the λ(h) ≥ 1/√d constraint and compare performance to test whether the constraint is truly necessary or if the optimization process naturally finds appropriate scaling factors without it.

2. **Attention visualization at failure points**: For each task, identify the specific sequence lengths where performance degrades and visualize the corresponding attention patterns to determine whether attention entropy is the sole factor or if other attention dynamics contribute to failure.

3. **Comparison with alternative position tracking methods**: Implement a simple learned position embedding added to NoPE and compare its extension performance with HeadScale to test whether the entropy control approach is superior to adding explicit position information for NoPE models.