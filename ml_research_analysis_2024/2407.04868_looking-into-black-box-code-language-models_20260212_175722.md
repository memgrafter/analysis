---
ver: rpa2
title: Looking into Black Box Code Language Models
arxiv_id: '2407.04868'
source_url: https://arxiv.org/abs/2407.04868
tags:
- layers
- code
- layer
- keys
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work aims to understand the inner workings of code language
  models by examining their feed-forward layers. The authors investigate two state-of-the-art
  code LMs, Codegen-Mono and Polycoder, across three programming languages: Java,
  Go, and Python.'
---

# Looking into Black Box Code Language Models

## Quick Facts
- arXiv ID: 2407.04868
- Source URL: https://arxiv.org/abs/2407.04868
- Reference count: 40
- This work examines feed-forward layers in code language models, revealing how lower layers capture syntax while higher layers encode semantics, and demonstrates concept editability without performance loss.

## Executive Summary
This study investigates the inner workings of code language models by focusing on their feed-forward layers. The authors analyze two state-of-the-art models (Codegen-Mono and Polycoder) across Java, Go, and Python to understand how information is stored and processed. Their findings reveal a clear hierarchical organization where lower layers capture syntactic patterns like keywords and n-grams, while higher layers encode abstract concepts and semantics. The research also demonstrates that concepts can be precisely edited by masking specific keys without significantly impacting overall model performance, and shows that later layers are crucial for predicting subsequent code tokens while earlier layers excel at smaller contexts.

## Method Summary
The authors analyze two code language models (Codegen-Mono-2.7B and Polycoder-2.7B) using GitHub repositories with over 50 stars, processing 5,000 code files per language. They compute activation coefficients to identify top trigger examples for each key across layers, apply regex filtering to analyze stored patterns, and mask keys related to specific concepts to test editability. Layer agreement is measured by comparing each layer's predictions with the final output through value × output embedding matrix multiplication. The study examines how context size affects layer contributions and evaluates performance impacts of concept editing on both specific concepts and general tasks.

## Key Results
- Lower layers primarily capture syntactic patterns (keywords, n-grams) while higher layers encode abstract concepts and semantics
- Concepts of interest can be edited by masking related keys without significantly impacting overall model performance
- Initial layers serve as "thinking" layers with limited agreement to final output, while later layers are crucial for predicting subsequent code tokens
- Earlier layers accurately predict smaller contexts, but larger contexts require critical contributions from later layers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feed-forward layers function as key-value memory banks, storing syntactic patterns in lower layers and semantic concepts in higher layers.
- Mechanism: Each neuron/key acts as a detector for specific input patterns; activation coefficients determine which patterns are triggered, and value vectors encode the associated responses.
- Core assumption: The ReLU non-linearity and key-value matrix multiplication create a distributed representation where each key captures a distinct concept or pattern.
- Evidence anchors:
  - [abstract] "lower layers capture syntactic patterns while higher layers encode abstract concepts and semantics"
  - [section] "Feed-forward layers act as key-value memories of the model" and "transformers employ ReLU non-linearity and the function of FF layers can be expressed as: FF (x) = ReLU(x · K ⊤ ) · V"
  - [corpus] Weak; no direct citation, but the concept of key-value memories is well-established in transformer literature
- Break condition: If the ReLU non-linearity is replaced with a different activation, the key-value separation may collapse, or if the model is not trained on sufficient diverse data, the layers may not learn distinct syntactic vs semantic representations.

### Mechanism 2
- Claim: Concepts of interest can be precisely edited by masking keys related to that concept without significantly impacting general model performance.
- Mechanism: By identifying keys that frequently trigger on the concept of interest (using regex filtering), and setting their weights to zero, the model's knowledge of that concept is removed while leaving other knowledge intact.
- Core assumption: The knowledge is localized to specific keys, and masking these keys only affects the concept of interest, not general performance.
- Evidence anchors:
  - [abstract] "concepts of interest can be edited within feed-forward layers without compromising code LM performance"
  - [section] "we can mask them by zeroing out the weights of the key" and "there was no significant decrease in the model's performance in areas other than the concept of interest"
  - [corpus] Weak; the concept of localized knowledge is inferred from the results but not explicitly stated in the corpus
- Break condition: If the concept of interest is polysemantic (i.e., the same key is involved in multiple unrelated concepts), masking it may have unintended consequences on other unrelated concepts.

### Mechanism 3
- Claim: Layer agreement with the final output increases as we move deeper into the model, with initial layers serving as "thinking" layers and later layers crucial for predicting subsequent tokens.
- Mechanism: Each layer transforms the input representation; early layers focus on local pattern detection, while later layers integrate information to make predictions closer to the final output. Agreement is measured by comparing the top predicted token of each layer with the model's final output.
- Core assumption: The model's output is constructed hierarchically, with later layers building upon the representations formed by earlier layers.
- Evidence anchors:
  - [abstract] "initial layers serve as 'thinking' layers, while later layers are crucial for predicting subsequent code tokens"
  - [section] "initial layers exhibit limited agreement with the final layers" and "later layers, which possess more processed information, play a pivotal role in generating the final output"
  - [corpus] Weak; the concept of hierarchical information processing is well-established but not explicitly stated in the corpus
- Break condition: If the model architecture is significantly different (e.g., a transformer variant without feed-forward layers), the layer agreement pattern may not hold.

## Foundational Learning

- Concept: Feed-forward layers as key-value memories
  - Why needed here: Understanding this concept is crucial for interpreting how information is stored and retrieved in the model, which is the core of the paper's analysis.
  - Quick check question: How does the ReLU non-linearity in feed-forward layers contribute to their function as key-value memories?
- Concept: Layer-wise information processing and agreement
  - Why needed here: This concept explains how the model's output is constructed from the contributions of different layers, which is essential for understanding the paper's findings on layer agreement.
  - Quick check question: What does it mean for a layer to "agree" with the final output, and why does this agreement increase as we move deeper into the model?
- Concept: Concept localization and masking for editing
  - Why needed here: This concept is key to understanding how the paper demonstrates the feasibility of editing specific concepts without affecting general performance.
  - Quick check question: How does masking keys related to a concept of interest allow for precise editing of that concept?

## Architecture Onboarding

- Component map:
  Input sequence → Self-attention layers → Feed-forward (key-value) layers → Output embedding matrix → Final predictions
  Key components: Self-attention blocks, feed-forward layers (with K and V matrices), output embedding matrix, regex filtering for concept identification
- Critical path:
  Data collection (GitHub repos) → Model selection (Codegen-Mono, Polycoder) → Key activation coefficient computation → Trigger example analysis → Concept editing (masking) → Layer agreement analysis
- Design tradeoffs:
  Using regex filtering for concept identification is efficient but may miss some instances; masking keys is a simple form of editing but may not be the most sophisticated approach
  Focusing on feed-forward layers provides insights into information storage but ignores the role of attention layers, which are also important
- Failure signatures:
  If layer agreement does not increase as expected, it may indicate issues with the model's hierarchical information processing
  If masking keys related to a concept does not significantly impact performance on that concept, it may suggest the knowledge is not localized as assumed
- First 3 experiments:
  1. Analyze the top trigger examples for a few selected keys in different layers to understand the types of patterns they capture (syntactic vs semantic)
  2. Perform masking on keys related to a specific concept (e.g., numpy) and evaluate the impact on model performance for that concept and general performance
  3. Compute layer agreement with the final output for different context sizes to understand how context affects the model's predictions and layer contributions

## Open Questions the Paper Calls Out
None explicitly stated in the provided material.

## Limitations
- Analysis is limited to feed-forward layers only, not accounting for the full model architecture including self-attention mechanisms
- Regex filtering for concept identification may miss instances where concepts appear in different syntactic forms or contexts
- Concept localization assumption may not hold for polysemantic keys that participate in multiple unrelated concepts

## Confidence
- **High Confidence**: The hierarchical layer agreement findings showing that later layers have higher agreement with final outputs
- **Medium Confidence**: The key-value memory bank interpretation of feed-forward layers and the syntactic vs semantic separation between lower and higher layers
- **Medium Confidence**: The concept editing results showing that masking keys related to specific concepts does not significantly impact general performance

## Next Checks
1. **Cross-Architecture Validation**: Test the layer agreement patterns and concept localization findings on additional code language models (such as StarCoder or CodeT5) and non-code language models to determine if the observed patterns generalize across architectures and domains.

2. **Attention Layer Analysis**: Extend the analysis to include self-attention layers to understand how information flows between attention and feed-forward components, and whether attention layers show similar hierarchical patterns or serve different functional roles.

3. **Polysemantic Key Investigation**: Systematically identify and test polysemantic keys that participate in multiple concepts to quantify the unintended effects of masking and develop more sophisticated editing techniques that can handle concept overlap without collateral damage.