---
ver: rpa2
title: 'Towards Self-Contained Answers: Entity-Based Answer Rewriting in Conversational
  Search'
arxiv_id: '2403.01747'
source_url: https://arxiv.org/abs/2403.01747
tags:
- answer
- entities
- entity
- salient
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of ensuring user understanding\
  \ of answers in conversational information seeking (CIS) systems, where the limited\
  \ interface restricts users from easily exploring unfamiliar concepts. The authors\
  \ focus on identifying and leveraging salient entities\u2014those central to understanding\
  \ an answer\u2014to improve user comprehension through answer rewriting."
---

# Towards Self-Contained Answers: Entity-Based Answer Rewriting in Conversational Search

## Quick Facts
- arXiv ID: 2403.01747
- Source URL: https://arxiv.org/abs/2403.01747
- Reference count: 40
- Focuses on improving user comprehension of answers in conversational information seeking systems through entity-based rewriting strategies

## Executive Summary
This paper addresses the challenge of ensuring user understanding in conversational information seeking (CIS) systems where limited interfaces restrict exploration of unfamiliar concepts. The authors propose leveraging salient entities—those central to understanding answers—to improve comprehension through answer rewriting. They create a dataset of 360 annotated conversations revealing that most answers contain highly salient entities, and that entity salience in CIS differs from documents. Two rewriting strategies are proposed: inline definitions of salient entities and follow-up questions offering further exploration. A crowdsourcing study shows rewritten answers are preferred over originals, with inline definitions slightly favored.

## Method Summary
The authors first analyzed conversational answer corpora from a conference helpdesk domain to identify salient entities through manual annotation by three annotators. They developed two answer rewriting strategies: inline definitions that expand salient entities within answers, and follow-up questions that offer further exploration of these entities. Both strategies were implemented using large language models (GPT-4 for definitions, GPT-3.5-turbo for questions). The effectiveness of these strategies was evaluated through a crowdsourced study where participants rated original versus rewritten answers across 30 conversations, with 30 different annotators per conversation.

## Key Results
- Most answers in conversational search contain highly salient entities that are crucial for user understanding
- Inline definitions and follow-up questions both improve answer quality compared to original answers
- Inline definitions show slight but consistent preference over follow-up questions (preference rate of 1.14)
- LLM-generated follow-up questions had a 26% error rate with questions unrelated to conversation context

## Why This Works (Mechanism)
The approach works by addressing the comprehension gap that occurs when users encounter unfamiliar concepts in conversational search interfaces. By identifying salient entities that are central to understanding answers and providing immediate contextual support through inline definitions or exploration paths via follow-up questions, the system reduces cognitive load and improves the likelihood of user understanding. The mechanism leverages the fact that conversational search contexts have unique entity salience patterns distinct from traditional document retrieval, making standard entity linking approaches insufficient.

## Foundational Learning

**Entity Salience in Conversational Search**: Understanding which entities in answers are most critical for user comprehension - needed to identify what should be explained; quick check: analyze entity frequency and importance scores in answer corpora.

**Answer Rewriting Strategies**: Techniques for modifying answers to improve clarity without disrupting conversational flow - needed to implement practical solutions; quick check: test different rewriting approaches on sample answers.

**Crowdsourced Evaluation Methods**: Systematic approaches to gathering human judgments on answer quality - needed to validate effectiveness of rewriting; quick check: pilot test annotation interface with small sample.

## Architecture Onboarding

**Component Map**: Conversational Answer Corpus -> Entity Annotation -> Salience Scoring -> Rewriting Strategy Selection -> LLM Generation -> Crowdsourced Evaluation

**Critical Path**: Entity identification and salience scoring directly determines which rewriting strategy is applied and whether the result successfully improves comprehension.

**Design Tradeoffs**: Inline definitions provide immediate context but can disrupt conversational flow; follow-up questions maintain flow but require additional user effort and have higher error rates in generation.

**Failure Signatures**: LLM-generated follow-up questions unrelated to conversation context (26% error rate); over-emphasis on entities that don't impact user understanding; definitions that are too technical or too simplistic.

**First 3 Experiments**: 1) Manual annotation of entity salience in 360 conversations to establish baseline patterns, 2) Implementation and testing of inline definition generation on sample answers, 3) Crowdsourced comparison of original vs. rewritten answers across 30 conversations.

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Analysis limited to single domain (conference helpdesk) may not generalize to other conversational search contexts
- Manual annotation involved only three annotators, potentially introducing bias in salience judgments
- Evaluation through crowdsourced preferences rather than longitudinal comprehension studies
- LLM-generated follow-up questions showed high error rate (26%) with unrelated questions

## Confidence
- High confidence in core findings about entity salience patterns in conversational search
- Medium confidence in practical effectiveness of rewriting strategies due to limited evaluation scope
- Low confidence in generalizability to broader conversational search scenarios beyond conference helpdesk domain

## Next Checks
1. Conduct longitudinal user studies measuring actual comprehension improvements rather than just preference ratings
2. Test rewriting strategies across multiple diverse conversational search domains to assess generalizability
3. Develop and evaluate more robust quality control mechanisms for LLM-generated follow-up questions to address the high error rate observed