---
ver: rpa2
title: You can remove GPT2's LayerNorm by fine-tuning
arxiv_id: '2409.13710'
source_url: https://arxiv.org/abs/2409.13710
tags:
- layers
- chen
- fine-tuning
- https
- layernorm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that LayerNorm layers in GPT2-small can
  be removed by fine-tuning on 500M tokens, achieving near-identical performance (0.05
  cross-entropy loss difference) on OpenWebText, ThePile, and Hellaswag benchmarks.
  The method incrementally disables LayerNorm layers while fine-tuning, treating different
  LayerNorm variants separately and handling special cases for first tokens and end-of-text
  markers.
---

# You can remove GPT2's LayerNorm by fine-tuning

## Quick Facts
- arXiv ID: 2409.13710
- Source URL: https://arxiv.org/abs/2409.13710
- Authors: Stefan Heimersheim
- Reference count: 15
- Removes LayerNorm from GPT2-small while maintaining performance within 0.05 cross-entropy loss

## Executive Summary
This paper demonstrates that LayerNorm layers in GPT2-small can be completely removed by fine-tuning on just 500M tokens while maintaining near-identical performance on OpenWebText, ThePile, and Hellaswag benchmarks. The key innovation is a gradual LayerNorm removal approach where layers are disabled incrementally during fine-tuning, treating different LayerNorm variants separately and handling special cases for first tokens and end-of-text markers. This provides a simplified model for mechanistic interpretability research by removing non-linear normalization layers that hinder analysis of residual streams and circuit decomposition.

## Method Summary
The method incrementally disables LayerNorm layers while fine-tuning, treating different LayerNorm variants separately and handling special cases for first tokens and end-of-text markers. The approach involves computing average standard deviations from sample prompts, then gradually disabling LayerNorm layers one by one during fine-tuning on 500M tokens. Special handling is applied to EOT and BOS tokens that have larger standard deviations, using separate fixed values to prevent destabilization during normalization removal.

## Key Results
- Achieves near-identical performance (0.05 cross-entropy loss difference) on OpenWebText, ThePile, and Hellaswag benchmarks
- LayerNorm layers can be completely removed through incremental disabling during fine-tuning
- Special handling for EOT and BOS tokens maintains performance during normalization removal

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradual LayerNorm removal allows the model to compensate for normalization changes incrementally
- Mechanism: By disabling one LayerNorm layer at a time and fine-tuning between removals, the model can adjust its internal representations to maintain performance
- Core assumption: The model can adapt to normalization changes without catastrophic forgetting
- Evidence anchors:
  - [abstract] "fine-tuning on a small fraction of the training data"
  - [section] "a more gradual approach—freezing (parts of) the LN layers incrementally—yields a recoverable state"
  - [corpus] Weak - only mentions LayerNorm in neighbor titles, no direct mechanism discussion
- Break condition: Removing too many layers simultaneously or too quickly causes irrecoverable performance collapse

### Mechanism 2
- Claim: Treating LayerNorm variants separately improves stability during removal
- Mechanism: The ln1 layer before attention has different sensitivity than ln1 before values, so handling them separately prevents destabilization
- Core assumption: Different LayerNorm positions have distinct sensitivities to normalization changes
- Evidence anchors:
  - [section] "We noticed that the latter appeared to be more sensitive to freezing the LN scale, and disabling ln1v after ln1qk led to a more stable fine-tuning procedure"
  - [abstract] "treating different LayerNorm variants separately"
  - [corpus] Weak - no direct evidence in corpus about variant sensitivity
- Break condition: Treating all LayerNorm variants identically regardless of position leads to instability

### Mechanism 3
- Claim: Special handling for EOT and BOS tokens maintains performance during normalization removal
- Mechanism: These tokens have larger standard deviations, so using separate fixed values prevents destabilization
- Core assumption: EOT and BOS tokens have systematically different statistical properties requiring special treatment
- Evidence anchors:
  - [section] "In these situation the standard deviation tends to be much larger... so we use a second fixed σ̄0 value for these cases"
  - [abstract] "handling special cases for first tokens and end-of-text markers"
  - [corpus] Weak - corpus doesn't mention EOT/BOS handling
- Break condition: Removing special case handling too early in the fine-tuning process

## Foundational Learning

- Transformer architecture
  - Why needed here: Understanding residual streams, attention layers, and feed-forward layers is crucial for grasping how LayerNorm affects model behavior
  - Quick check question: What are the two main types of layers in each transformer block besides LayerNorm?

- Layer Normalization
  - Why needed here: LayerNorm's non-linear nature is the core obstacle being addressed
  - Quick check question: How does LayerNorm differ from Batch Normalization in terms of when it operates?

- Sparse dictionary learning
  - Why needed here: This interpretability technique benefits from LayerNorm removal as mentioned in the paper
  - Quick check question: Why would LayerNorm removal help sparse dictionary learning techniques?

## Architecture Onboarding

- Component map:
  - Input embedding → Residual stream → Transformer blocks (12 total) → LayerNorm layers (3 types per block) → Output unembedding
  - Special handling: First token and EOT tokens get separate normalization treatment

- Critical path:
  - Gradual LayerNorm removal schedule → Fine-tuning between removals → Performance monitoring → Special case handling

- Design tradeoffs:
  - Gradual vs. simultaneous removal: Gradual is more stable but slower
  - Special case handling: Improves stability but adds complexity
  - Learning rate schedule: Constant vs. variable affects final performance

- Failure signatures:
  - Loss spike to ~20 during LayerNorm removal
  - Cross-entropy loss reaching NaN
  - Performance degradation that doesn't recover after fine-tuning

- First 3 experiments:
  1. Remove all LayerNorm simultaneously without fine-tuning to observe catastrophic failure
  2. Implement gradual removal of ln2 layers only, keeping ln1 layers intact
  3. Test special case handling for first token but not EOT tokens to isolate their effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LayerNorm-free models scale with model size compared to models with LayerNorm?
- Basis in paper: [inferred] The paper demonstrates success with GPT2-small (117M parameters) but notes this is a limitation, stating "Training larger models is harder, so this fine-tuning procedure might be more difficult or more expensive for larger models."
- Why unresolved: The paper only tested on GPT2-small, leaving open questions about whether LayerNorm-free approaches would work for larger models or if LayerNorm becomes more critical for stability in larger architectures.
- What evidence would resolve it: Testing the LayerNorm-free fine-tuning approach on progressively larger models (GPT2-medium, GPT2-large, GPT-Neo, etc.) and comparing performance, stability, and computational requirements to their LayerNorm-enabled counterparts.

### Open Question 2
- Question: Do LayerNorm-free models learn fundamentally different computational mechanisms compared to LayerNorm-enabled models?
- Basis in paper: [explicit] The paper states "We don’t know whether mechanistic interpretability insights from the no-LN version of a model would transfer to the original model, i.e. whether the no-LN model is sufficiently faithful to the original model."
- Why unresolved: While the paper shows similar performance metrics, it's unclear whether the underlying computational mechanisms are identical or if the models use different pathways to achieve similar outputs.
- What evidence would resolve it: Detailed mechanistic interpretability studies comparing circuit structures, attention patterns, and feature representations between LayerNorm-free and LayerNorm-enabled models of the same architecture and performance level.

### Open Question 3
- Question: What is the precise role of LayerNorm in stabilizing training versus inference-time computation?
- Basis in paper: [explicit] The paper demonstrates that "at inference time, the LN layers do not play a crucial role in transformer models" and provides evidence that "LN can be removed after pre-training by fine-tuning on a small fraction of the training data."
- Why unresolved: The paper shows LayerNorm can be removed post-training but doesn't explain why it's essential during training or what specific instabilities it prevents during the pre-training phase.
- What evidence would resolve it: Comparative studies examining training dynamics with and without LayerNorm, including analysis of gradient stability, learning rates, and the emergence of catastrophic forgetting or mode collapse in LayerNorm-free training scenarios.

## Limitations
- Method relies heavily on gradual fine-tuning, with unclear boundaries for when the approach fails
- Special handling for first tokens and EOT tokens adds complexity that may not generalize to all architectures
- Experiments limited to GPT2-small and specific datasets, leaving scalability questions open

## Confidence
- High Confidence: The core claim that LayerNorm can be removed while maintaining near-identical performance (0.05 cross-entropy loss difference) on tested benchmarks
- Medium Confidence: The claim that different LayerNorm variants require separate handling, though underlying reasons for sensitivity differences are not fully explained
- Low Confidence: The generalizability of this approach to other transformer architectures beyond GPT2 and to tasks outside tested domains

## Next Checks
1. **Boundary Testing**: Systematically test the limits of the gradual removal approach by varying the number of tokens between LayerNorm removals (e.g., test with 50M, 100M, 200M, 500M tokens between removals) to establish the minimum fine-tuning duration required for stable LayerNorm removal.

2. **Architecture Transfer**: Apply the same LayerNorm removal technique to other transformer variants (e.g., BERT, RoBERTa, or OPT models) to test generalizability beyond GPT2 architecture, measuring performance degradation and recovery patterns.

3. **Special Case Necessity**: Conduct ablation studies to quantify the impact of special EOT/BOS handling by creating versions that remove this special treatment at different stages of the LayerNorm removal process, measuring the performance cost of delayed special case removal.