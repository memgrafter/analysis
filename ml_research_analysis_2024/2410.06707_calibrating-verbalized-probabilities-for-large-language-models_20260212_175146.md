---
ver: rpa2
title: Calibrating Verbalized Probabilities for Large Language Models
arxiv_id: '2410.06707'
source_url: https://arxiv.org/abs/2410.06707
tags:
- probability
- confidence
- llms
- calibration
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of calibrating confidence scores
  from black-box Large Language Models (LLMs) for classification tasks. While previous
  work focused on eliciting single confidence scores, this paper extends to generating
  full probability distributions over categorical labels.
---

# Calibrating Verbalized Probabilities for Large Language Models

## Quick Facts
- arXiv ID: 2410.06707
- Source URL: https://arxiv.org/abs/2410.06707
- Reference count: 40
- Primary result: The paper proposes the "invert softmax trick" to calibrate verbalized probabilities from LLMs, avoiding "re-softmaxing" issues and improving calibration metrics while maintaining accuracy.

## Executive Summary
This paper addresses the challenge of calibrating confidence scores from black-box Large Language Models for classification tasks, extending beyond single confidence scores to full probability distributions over categorical labels. The key innovation is the "invert softmax trick," which estimates logits from verbalized probabilities and applies temperature scaling to avoid the "re-softmaxing" distortion that occurs when scaling probabilities directly. Extensive experiments on three datasets (IMDB, Emotion, Amazon Massive) demonstrate improved calibration metrics (ECE, MCE) while maintaining high accuracy, with particular benefits when using two decimal places for probability values.

## Method Summary
The method involves prompting LLMs to generate probability distributions over categorical labels, then applying the invert softmax trick to estimate logits from these verbalized probabilities. Temperature scaling is applied to the estimated logits rather than the probabilities directly, avoiding the re-softmaxing issue. The approach uses validation sets to tune temperature parameters and evaluates performance using accuracy, NLL, ECE, MCE, and reliability diagrams. The method is particularly effective when using two decimal places for probability values, improving precision-recall control.

## Key Results
- The invert softmax trick significantly improves calibration metrics (ECE, MCE) compared to uncalibrated verbalized probabilities
- Using two decimal places for probability values provides better calibration and smoother precision-recall curves
- The method maintains high accuracy while improving calibration across three different datasets and multiple LLM models
- Label positional bias in prompts affects probability generation, with optimal ordering improving calibration performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Directly scaling verbalized probabilities with temperature scaling causes "re-softmaxing" that distorts the distribution and leads to unreliable calibration.
- Mechanism: When temperature scaling is applied directly to verbalized probabilities, the softmax function is effectively applied twice in sequence, creating a more uniform distribution than the original. This uniformity requires an opposite temperature adjustment than what would be needed for the original probability distribution.
- Core assumption: The verbalized probability distributions generated by LLMs behave like standard softmax probabilities from neural networks.
- Evidence anchors:
  - [abstract] "directly applying temperature scaling to these verbalized probabilities causes 're-softmaxing,' distorting the distribution and leading to unreliable calibration"
  - [section] "Let's first understand the effects of re-applying the softmax function (i.e., re-softmaxing) towards temperature scaling (TS)-based calibration"
  - [corpus] Weak evidence - no direct corpus citations addressing re-softmaxing mechanism

### Mechanism 2
- Claim: The invert softmax trick can estimate logits from verbalized probabilities, enabling proper temperature scaling.
- Mechanism: By inverting the softmax function using logarithmic transformation and normalization, approximate logits can be recovered from verbalized probabilities. These estimated logits can then be temperature scaled without causing re-softmaxing.
- Core assumption: The softmax function is approximately invertible when we know the output probabilities and can recover logits up to a constant offset.
- Evidence anchors:
  - [abstract] "propose using the invert softmax trick to approximate the 'logit' by inverting verbalized probabilities"
  - [section] "Equation (5) brings the additional advantage of re-normalizing the inverted probabilities (logits) before performing calibration"
  - [corpus] Weak evidence - no direct corpus citations about invert softmax trick implementation

### Mechanism 3
- Claim: Generating verbalized probabilities with more decimal precision (e.g., 2 decimal places) improves calibration quality and enables better precision-recall control.
- Mechanism: Higher precision in verbalized probabilities creates a more diverse distribution of probability scores, reducing the clustering effect where many samples share identical probabilities. This diversity enables finer-grained threshold tuning for precision-recall optimization.
- Core assumption: LLMs can generate and accurately represent probabilities with higher precision when explicitly instructed to do so.
- Evidence anchors:
  - [abstract] "particularly effective when using two decimal places for probability values, improving precision-recall control"
  - [section] "One of observed limitations of verbalised probability is that, without more specific instruction, LLMs generate less diversely distributed probability, mostly around the 11 different probability scores"
  - [corpus] Weak evidence - no direct corpus citations about decimal precision effects on calibration

## Foundational Learning

- Concept: Temperature scaling and its role in calibration
  - Why needed here: Understanding how temperature scaling works on logits versus probabilities is fundamental to recognizing why direct application to verbalized probabilities fails
  - Quick check question: Why does temperature scaling require logits as input rather than probabilities?

- Concept: Softmax function properties and invertibility
  - Why needed here: The invert softmax trick relies on understanding that softmax is many-to-one and can be partially inverted with additional constraints
  - Quick check question: Why can't we perfectly recover logits from probabilities using the inverse softmax?

- Concept: Calibration metrics (ECE, MCE, NLL)
  - Why needed here: Evaluating calibration performance requires understanding these metrics and their interpretations
  - Quick check question: What does a high ECE score indicate about a model's calibration?

## Architecture Onboarding

- Component map:
  - Prompt template generation → LLM API call → Probability parsing → Invert softmax transformation → Temperature scaling → Calibration evaluation

- Critical path:
  1. Generate probability distributions using well-crafted prompts
  2. Parse and validate the generated probabilities
  3. Apply invert softmax to estimate logits
  4. Perform temperature scaling on estimated logits
  5. Evaluate calibration performance using ECE, MCE, and NLL

- Design tradeoffs:
  - Prompt complexity vs. LLM compliance: More detailed prompts may improve probability generation but risk LLM refusal
  - Precision vs. stability: Higher decimal precision in probabilities improves calibration but may introduce more variance
  - Temperature tuning range: Need to find optimal temperature values that work across different datasets and LLMs

- Failure signatures:
  - Low success rate in probability generation (probabilities not summing to 1)
  - High variance in sum of probabilities across samples
  - Calibration metrics not improving despite temperature scaling
  - Optimal temperature values outside expected ranges (τ << 1 or τ >> 1)

- First 3 experiments:
  1. Test basic probability generation on a simple binary classification dataset (e.g., IMDB) with different prompt templates
  2. Apply temperature scaling directly to verbalized probabilities and observe re-softmaxing effects on calibration metrics
  3. Implement and test the invert softmax trick on the same dataset, comparing calibration performance with direct scaling approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the invert softmax trick compare to other methods for estimating logits from probabilities, such as using a fixed offset (e.g., c=1) or alternative approximations?
- Basis in paper: [explicit] The paper proposes the invert softmax trick as a method to estimate logits from verbalized probabilities and mentions that the choice of the constant c doesn't affect calibration metrics.
- Why unresolved: The paper only tests c=0 and c=1, but does not explore other potential methods or constants for estimating logits, leaving uncertainty about whether the invert softmax trick is the optimal approach.
- What evidence would resolve it: A systematic comparison of different logit estimation methods (e.g., fixed offsets, learned constants, or alternative approximations) on multiple datasets, measuring their impact on calibration metrics like ECE and MCE.

### Open Question 2
- Question: How does the performance of the invert softmax trick vary with the granularity of the verbalized probabilities (e.g., 1 decimal vs. 2 decimals) across different datasets and LLM models?
- Basis in paper: [explicit] The paper shows that using 2 decimal places for probabilities improves calibration metrics like MCE and provides smoother precision-recall curves, but this is only tested on one dataset (IMDB) with one model (Claude-v3).
- Why unresolved: The study is limited in scope, and it is unclear whether the benefits of increased granularity generalize to other datasets, models, or tasks.
- What evidence would resolve it: A comprehensive ablation study across multiple datasets, LLM models, and tasks, comparing the calibration performance of probabilities with different levels of granularity (e.g., 1, 2, or more decimal places).

### Open Question 3
- Question: How does the label order in the prompt affect the quality of the generated probability distributions, and can this bias be mitigated?
- Basis in paper: [explicit] The paper mentions a label positional bias in the prompt, where the order of labels might influence the LLM's output, and conducts a limited ablation study on the Emotion dataset with Claude-v3.
- Why unresolved: The study only tests a few variations of label order and one model, leaving uncertainty about the extent and generalizability of this bias.
- What evidence would resolve it: A thorough investigation of the label order bias across multiple datasets, LLM models, and tasks, testing various mitigation strategies (e.g., randomizing label order, using a consistent order, or omitting explicit label lists).

## Limitations

- The invert softmax trick introduces approximation error that may offset calibration benefits
- Method performance depends on LLM compliance with generating valid probability distributions
- Computational overhead of multiple API calls during calibration may limit practical applications
- Effectiveness on truly open-ended tasks beyond classification remains unexplored

## Confidence

### High Confidence Claims
- The re-softmaxing problem when applying temperature scaling directly to verbalized probabilities is well-established theoretically
- The invert softmax trick provides a mathematically sound approach to estimate logits from probabilities
- Higher precision probability generation (2 decimal places) improves calibration performance

### Medium Confidence Claims
- The method's effectiveness generalizes across different LLMs (Claude-v2, Claude-v3, Mixtral)
- The calibration improvements are statistically significant and robust across different datasets
- The computational overhead is acceptable for practical applications

### Low Confidence Claims
- The invert softmax approximation error is negligible in practice
- The method's effectiveness on tasks beyond the three tested classification datasets
- The impact of label ordering bias across different LLMs and prompt styles

## Next Checks

1. **Invert Softmax Accuracy Validation**: Implement a controlled experiment where logits are generated from known probability distributions, then apply the invert softmax trick and compare recovered logits to original values. Measure approximation error across different probability distributions to quantify the method's reliability.

2. **Cross-LLM Robustness Test**: Test the calibration method on at least two additional LLMs not included in the original study (e.g., GPT-4, Llama models) across the same three datasets. This will validate whether the approach generalizes beyond the specific models used in the paper.

3. **Real-World Application Benchmark**: Apply the calibrated verbalized probabilities to a downstream task requiring precision-recall optimization, such as medical diagnosis or fraud detection. Compare the actual task performance using calibrated versus uncalibrated probabilities to validate the practical utility of the calibration improvements.