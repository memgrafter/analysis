---
ver: rpa2
title: 'MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark'
arxiv_id: '2412.15194'
source_url: https://arxiv.org/abs/2412.15194
tags:
- question
- arxiv
- test
- choices
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMLU-CF, a contamination-free benchmark designed
  to fairly evaluate large language models (LLMs) by addressing both unintentional
  and malicious data leakage. To avoid unintentional leakage, the authors source questions
  from over 200 billion documents across 3,000+ domains, apply three decontamination
  rules (rephrasing questions, shuffling choices, and replacing choices with "None
  of the other choices"), and sample questions with moderate difficulty.
---

# MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark

## Quick Facts
- arXiv ID: 2412.15194
- Source URL: https://arxiv.org/abs/2412.15194
- Reference count: 18
- GPT-4o achieves 73.4% on test set vs 88.0% on original MMLU

## Executive Summary
This paper introduces MMLU-CF, a contamination-free benchmark designed to fairly evaluate large language models (LLMs) by addressing both unintentional and malicious data leakage. The authors source questions from over 200 billion documents across 3,000+ domains and apply three decontamination rules (rephrasing questions, shuffling choices, and replacing choices with "None of the other choices"). To prevent malicious leakage, the benchmark is split into a closed-source test set and an open-source validation set. Evaluations on 40+ models show GPT-4o achieves a 5-shot score of 73.4% on the test set, significantly lower than its 88.0% on the original MMLU, demonstrating the benchmark's rigor and effectiveness.

## Method Summary
The benchmark construction involves three main components: source diversity, decontamination rules, and dataset splitting. Questions are sourced from an extensive corpus of over 200 billion documents spanning 3,000+ domains to maximize diversity and minimize contamination risk. Three decontamination rules are applied: questions are rephrased to avoid direct copying, answer choices are shuffled to prevent positional bias, and one choice is replaced with "None of the other choices" to eliminate recognizable patterns. The benchmark is split into a closed-source test set for final evaluation and an open-source validation set that allows transparency and ongoing verification of the test set's integrity. Model evaluations include both few-shot and zero-shot settings across 40+ models ranging from GPT-4o to various open-source alternatives.

## Key Results
- GPT-4o achieves 73.4% on MMLU-CF test set (5-shot) vs 88.0% on original MMLU
- Significant performance gap demonstrates effectiveness of contamination prevention
- Validation set enables transparency while protecting test set integrity
- Benchmark evaluates across 40+ models including both closed and open-source systems

## Why This Works (Mechanism)
The benchmark addresses contamination through systematic prevention of both unintentional and malicious data leakage. Unintentional leakage is prevented by sourcing questions from an unprecedentedly large and diverse corpus (200B+ documents across 3,000+ domains), making it statistically unlikely that any particular question appeared in pretraining data. The three decontamination rules break patterns that models might have learned: rephrasing questions prevents direct memorization, shuffling choices eliminates positional biases, and replacing choices with "None of the other choices" removes recognizable answer patterns. Malicious leakage is prevented through the closed-source test set/open-source validation set split, which allows community verification while maintaining evaluation integrity.

## Foundational Learning

**Data Contamination in LLM Evaluation** - Understanding how pretraining data can overlap with evaluation benchmarks, leading to inflated performance metrics that don't reflect true model capabilities. Needed to identify why existing benchmarks like MMLU may overestimate model performance. Quick check: Compare model performance on original vs decontaminated versions of benchmark questions.

**Decontamination Methodologies** - Techniques for modifying evaluation data to prevent models from recognizing questions while maintaining semantic meaning and difficulty. Needed to create fair evaluation conditions that test genuine understanding rather than memorization. Quick check: Measure performance drop when applying decontamination rules to questions models previously answered correctly.

**Dataset Splitting Strategies** - Approaches for partitioning data between public validation and private test sets to enable transparency while preventing data leakage. Needed to balance community trust with evaluation integrity. Quick check: Verify that validation set performance correlates with test set performance across multiple model families.

**Question Difficulty Calibration** - Methods for sampling questions at appropriate difficulty levels to create meaningful differentiation between model capabilities. Needed to ensure benchmark provides useful signal about relative model strengths. Quick check: Analyze score distributions across model families to confirm appropriate difficulty calibration.

## Architecture Onboarding

**Component Map**: Data Sourcing -> Decontamination Rules -> Question Selection -> Dataset Split -> Model Evaluation

**Critical Path**: The most critical sequence is Data Sourcing -> Decontamination Rules -> Question Selection, as these three steps directly determine whether the benchmark achieves its contamination-free goal. The quality and diversity of sourced questions, combined with effective decontamination, establishes the foundation for fair evaluation.

**Design Tradeoffs**: The authors chose extensive data sourcing (200B+ documents) over more aggressive decontamination, prioritizing question quality and natural language preservation. They opted for a closed-source test set despite community preferences for full transparency, choosing evaluation integrity over complete openness. The "None of the other choices" replacement trades some naturalness for stronger contamination prevention.

**Failure Signatures**: If decontamination rules are ineffective, models will show minimal performance difference between MMLU-CF and MMLU. If the validation set leaks information about the test set, performance on validation will become a perfect predictor of test performance. If question difficulty is mis-calibrated, score distributions will be either too compressed (all models score similarly) or too dispersed (no meaningful differentiation).

**First Experiments**:
1. Run GPT-4o on both MMLU and MMLU-CF with identical prompts to establish baseline contamination effect
2. Apply individual decontamination rules to MMLU questions and measure performance impact
3. Evaluate a diverse set of open-source models to establish baseline performance distribution

## Open Questions the Paper Calls Out
None

## Limitations
- Decontamination rules lack systematic validation, relying primarily on empirical observations
- Performance differences between benchmarks could stem from factors beyond contamination (difficulty calibration, prompt sensitivity)
- No quantitative analysis of overlap between source documents and pretraining corpora
- Moderate difficulty sampling criterion is vaguely defined, potentially introducing bias

## Confidence
High confidence: The technical implementation of the benchmark split into closed-source test set and open-source validation set follows standard practices in ML evaluation and is methodologically sound.

Medium confidence: The general approach of using large-scale web data with decontamination rules is reasonable for creating contamination-free benchmarks, though the specific effectiveness of individual rules remains uncertain.

Low confidence: The claim that MMLU-CF provides a definitive measure of LLM capabilities free from contamination, given the lack of systematic validation of the decontamination methods and the multiple confounding factors that could affect performance differences between benchmarks.

## Next Checks
1. Conduct a systematic ablation study testing each decontamination rule independently to measure their individual effectiveness at preventing contamination while maintaining question integrity and difficulty calibration.

2. Perform cross-validation experiments comparing model performance on MMLU-CF versus MMLU using identical prompt engineering and temperature settings to isolate whether performance differences are due to contamination versus benchmark design factors.

3. Analyze the overlap between MMLU-CF's source documents and commonly used pretraining corpora using embedding similarity measures or n-gram overlap statistics to quantify the actual contamination risk reduction achieved by the large-scale sourcing approach.