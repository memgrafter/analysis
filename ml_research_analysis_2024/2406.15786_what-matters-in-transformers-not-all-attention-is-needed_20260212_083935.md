---
ver: rpa2
title: What Matters in Transformers? Not All Attention is Needed
arxiv_id: '2406.15786'
source_url: https://arxiv.org/abs/2406.15786
tags:
- uni00000013
- attention
- uni00000018
- drop
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates redundancy in transformer-based large language
  models, focusing on Blocks, MLP, and Attention layers. Using a similarity-based
  metric, the authors find that Attention layers exhibit significant redundancy and
  can be pruned without degrading performance.
---

# What Matters in Transformers? Not All Attention is Needed

## Quick Facts
- arXiv ID: 2406.15786
- Source URL: https://arxiv.org/abs/2406.15786
- Reference count: 40
- Primary result: Attention layers in transformer models exhibit significant redundancy and can be pruned to achieve substantial speedups (up to 48.4%) with minimal performance degradation (2.4%)

## Executive Summary
This paper investigates redundancy in transformer-based large language models, focusing on Blocks, MLP, and Attention layers. Using a similarity-based metric, the authors find that Attention layers exhibit significant redundancy and can be pruned without degrading performance. For instance, Llama-2-70B achieved a 48.4% speedup with only a 2.4% performance drop by pruning half of its attention layers. The study also reveals that attention layer redundancy is consistent across training stages. Additionally, the authors propose "Joint Layer Drop," which combines Attention and MLP layer pruning, allowing for more aggressive layer removal while maintaining high performance. For example, when dropping 31 layers (Attention + MLP), Llama-2-13B retained 90% of its performance on the MMLU task. These findings provide valuable insights for designing more efficient transformer architectures.

## Method Summary
The authors use a similarity-based metric to measure layer importance, computing the cosine similarity between layer inputs and outputs. They then apply one-shot pruning to remove layers with the lowest importance scores. The methodology includes three specific pruning strategies: Block Drop (removing entire blocks), MLP Drop (removing MLP layers with associated LayerNorm), and Attention Drop (removing Attention layers with associated LayerNorm). They also propose Joint Layer Drop, which combines Attention and MLP pruning based on concatenated importance scores. The approach is validated across multiple model families including Llama-2-70B, Llama-2-13B, Mistral-7B, Llama-3-8B, Llama-3-70B, and MAP-Neo-7B using calibration datasets like C4, CodeAlpaca-20k, MathInstruct, and LIMA.

## Key Results
- Attention layers exhibit significantly higher redundancy than MLP layers, with deeper attention layers showing the highest redundancy
- Llama-2-70B achieved 48.4% speedup with only 2.4% performance drop by pruning 50% of attention layers
- Attention layer redundancy is consistent across training stages, suggesting it's an inherent architectural property
- Joint Layer Drop allows for more aggressive pruning, with Llama-2-13B retaining 90% performance on MMLU after dropping 31 layers (Attention + MLP)
- Pruning attention layers proportionally reduces KV-cache memory usage, providing significant memory savings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention layers are redundant because their outputs are highly similar to their inputs
- Mechanism: The similarity-based metric measures cosine similarity between layer input and output. High similarity indicates low transformation, implying the layer's function is redundant
- Core assumption: Cosine similarity between input and output is a reliable proxy for layer importance
- Evidence anchors:
  - [abstract]: "we found that a large portion of these layers exhibit excessively high similarity and can be pruned without degrading performance"
  - [section 3.1]: "The underlying hypothesis is that redundant modules produce outputs that are similar to their inputs, implying minimal transformation"
  - [corpus]: Weak - related work focuses on MLP understanding but not cosine-based redundancy metrics
- Break condition: If attention layers perform critical computation that requires significant transformation of inputs (e.g., complex multi-hop reasoning), the similarity metric would fail to identify redundancy

### Mechanism 2
- Claim: Deeper layers exhibit higher redundancy than shallower layers
- Mechanism: Layer importance scores systematically decrease with depth, making deeper attention layers more likely to be pruned
- Core assumption: Layer depth correlates with redundancy due to information saturation or diminishing returns
- Evidence anchors:
  - [section 6.1]: "deeper layers (excluding the last ones) often exhibit excessively low importance across Block, MLP, and Attention modules"
  - [section 3.3]: Visualization shows dropping order starts with deeper layers
  - [corpus]: Weak - related work on layer redundancy exists but doesn't specifically analyze depth correlation
- Break condition: If deeper layers contain unique, non-redundant information critical for task performance, systematic depth-based pruning would fail

### Mechanism 3
- Claim: Attention layer redundancy is consistent across training stages
- Mechanism: Importance scores remain stable throughout training, indicating redundancy is an inherent architectural property rather than a training artifact
- Core assumption: If redundancy were training-dependent, importance scores would change significantly during training
- Evidence anchors:
  - [abstract]: "attention layer redundancy is inherent and consistent across training stages"
  - [section 6.2]: "we observed that attention layer redundancy is inherent and consistent across training stages"
  - [corpus]: Weak - related work doesn't examine temporal stability of layer importance
- Break condition: If training dynamics or curriculum learning affect attention layer importance differently over time, the consistency assumption would break

## Foundational Learning

- Concept: Cosine similarity as a metric for measuring transformation
  - Why needed here: The entire pruning methodology relies on measuring how much a layer transforms its input
  - Quick check question: If a layer's output has cosine similarity of 0.95 with its input, what does this imply about the layer's importance?

- Concept: Residual connections in transformer architectures
  - Why needed here: Understanding how residual connections affect the interpretation of layer outputs is critical for proper importance scoring
  - Quick check question: How does treating a LayerNorm+Attention pair as a unit (rather than just the attention) affect the importance score calculation?

- Concept: KV-cache memory structure in autoregressive generation
  - Why needed here: The paper claims significant memory savings from pruning attention layers, which directly impacts KV-cache size
  - Quick check question: Why does removing attention layers proportionally reduce KV-cache memory usage?

## Architecture Onboarding

- Component map: Transformer block → LayerNorm → Attention (with residual) → LayerNorm → MLP (with residual) → output. Each block is independent except for sequential data flow.
- Critical path: Input → Embedding → Sequential blocks → LM Head → Output. Attention and MLP layers are both on the critical path but serve different functions.
- Design tradeoffs: Pruning attention layers saves memory and computation but may lose long-range dependency modeling; pruning MLP layers saves parameters but may lose channel mixing capabilities.
- Failure signatures: Performance degradation when pruning essential attention layers for tasks requiring complex reasoning; memory usage doesn't decrease as expected if pruning is incorrect.
- First 3 experiments:
  1. Compute importance scores for all attention layers on a calibration dataset and verify they follow expected depth-based patterns
  2. Prune 10% of attention layers with lowest scores and measure performance degradation on a validation set
  3. Compare memory usage and inference speed before and after pruning to verify claimed efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the redundancy pattern of attention layers vary across different model sizes and architectures beyond those tested in this study?
- Basis in paper: [explicit] The paper mentions testing on Llama-2-70B, Llama-3, and smaller models like Llama-2-13B and Mistral-7B, but suggests testing across a broader range of models in the limitations section.
- Why unresolved: The study primarily focused on a limited set of transformer-based models, leaving uncertainty about whether the observed redundancy patterns hold for other architectures like vision transformers or vision-language models.
- What evidence would resolve it: Conducting experiments on a diverse set of models, including vision transformers and vision-language models, to compare redundancy patterns and validate the consistency of attention layer redundancy across different architectures.

### Open Question 2
- Question: Can retraining pruned models with attention layer removal lead to performance recovery or even improvement beyond the baseline?
- Basis in paper: [inferred] The paper discusses post-training pruning without retraining and suggests that retraining could potentially recover or improve performance in the limitations section.
- Why unresolved: The study did not explore the effects of retraining pruned models, leaving uncertainty about whether performance can be enhanced through this process.
- What evidence would resolve it: Performing retraining experiments on pruned models to assess whether performance can be recovered or improved compared to the original baseline models.

### Open Question 3
- Question: How does the efficiency of attention layer pruning scale with increasing sequence lengths and batch sizes in practical deployment scenarios?
- Basis in paper: [explicit] The paper discusses memory savings and speed improvements with attention layer pruning but focuses on specific scenarios with fixed sequence lengths and batch sizes.
- Why unresolved: The study does not address how the benefits of pruning scale with larger sequence lengths and batch sizes, which are common in real-world applications.
- What evidence would resolve it: Evaluating the impact of attention layer pruning on memory usage and inference speed across a range of sequence lengths and batch sizes to determine scalability in practical deployment.

### Open Question 4
- Question: What alternative mechanisms could replace attention layers to address the bottleneck in training large models, and how would they compare in terms of performance and efficiency?
- Basis in paper: [explicit] The paper suggests exploring alternative mechanisms to attention layers due to their consistent redundancy throughout training, as mentioned in the insights section.
- Why unresolved: The study does not investigate specific alternative mechanisms or their comparative performance and efficiency.
- What evidence would resolve it: Implementing and testing alternative mechanisms to attention layers in transformer models to evaluate their performance and efficiency relative to traditional attention-based architectures.

## Limitations

- The similarity-based metric may not capture all forms of layer importance, particularly for attention layers that perform complex transformations not reflected in simple cosine similarity
- Results are primarily validated on decoder-only architectures (LLaMA family), limiting generalizability to encoder-decoder or encoder-only models
- The study focuses on medium-to-large scale models, leaving uncertainty about how these findings apply to smaller models or different architectural choices

## Confidence

- **High confidence**: Empirical findings on specific model families (LLaMA-2, Mistral) regarding attention layer redundancy and pruning effectiveness
- **Medium confidence**: Claims about attention layer redundancy being inherent and consistent across training stages
- **Medium confidence**: The Joint Layer Drop methodology and its claimed advantages over individual pruning approaches

## Next Checks

1. **Ablation on similarity metric**: Systematically compare the proposed cosine-based importance scores against alternative metrics (e.g., gradient-based importance, activation-based measures) to validate that cosine similarity is the most appropriate proxy for redundancy
2. **Cross-architecture validation**: Apply the pruning methodology to encoder-decoder models (e.g., BERT-based architectures) and smaller models (< 1B parameters) to test the generalizability of attention layer redundancy claims
3. **Fine-tuning impact analysis**: Evaluate whether pruned models can recover performance through fine-tuning and whether this affects the observed redundancy patterns, particularly for tasks requiring complex reasoning where attention layers may be more critical