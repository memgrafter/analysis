---
ver: rpa2
title: 'sDPO: Don''t Use Your Data All at Once'
arxiv_id: '2403.19270'
source_url: https://arxiv.org/abs/2403.19270
tags:
- sdpo
- reference
- arxiv
- data
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces stepwise direct preference optimization (sDPO),
  an extension of DPO for aligning LLMs. The key idea is to divide the available preference
  datasets and use them incrementally, rather than all at once.
---

# sDPO: Don't Use Your Data All at Once

## Quick Facts
- arXiv ID: 2403.19270
- Source URL: https://arxiv.org/abs/2403.19270
- Reference count: 8
- Primary result: sDPO achieves 74.31 H4 score on SOLAR 10.7B + SFT, surpassing larger models and improving generation capabilities

## Executive Summary
This paper introduces stepwise direct preference optimization (sDPO), an extension of DPO for aligning LLMs. The key innovation is dividing preference datasets into chunks and using them incrementally with progressively more aligned reference models, rather than all at once. This approach results in better overall performance, with experiments showing sDPO outperforms traditional DPO and achieves state-of-the-art results on alignment benchmarks. The method also demonstrates improvements in generation capabilities, as evidenced by higher scores on EQ Bench and MT Bench.

## Method Summary
sDPO extends DPO by dividing preference datasets into T chunks and performing DPO training T times. The reference model for each step is initialized as the aligned model from the previous step, creating a curriculum where reference models become increasingly aligned. The target model is initialized as the previous step's aligned model rather than the base SFT model. Data is partitioned using an easy-to-hard strategy based on reward accuracy, ensuring earlier chunks contain easier samples. This stepwise approach maintains the theoretical benefits of DPO while ensuring the final model has seen all data.

## Key Results
- sDPO achieves 74.31 H4 score on SOLAR 10.7B + SFT, surpassing larger models
- Significant improvements on EQ Bench and MT Bench demonstrate enhanced generation capabilities
- Easy-to-hard partitioning outperforms random partitioning (Table 3 results)
- sDPO shows more stable loss curves compared to standard DPO

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using progressively more aligned reference models in sDPO leads to better final alignment performance.
- Mechanism: sDPO divides preference data into chunks and uses the aligned model from the previous step as the reference for the current step, creating a curriculum where reference models become increasingly aligned.
- Core assumption: The reference model acts as a lower bound for alignment quality in DPO, and a more aligned reference model provides a stricter but more effective optimization target.
- Evidence anchors: [abstract], [section], [corpus]

### Mechanism 2
- Claim: Easy-to-hard data partitioning improves sDPO performance by creating a curriculum learning effect.
- Mechanism: The preference data is partitioned such that earlier chunks contain easier samples (higher reward accuracy with initial model), allowing the model to first learn simpler alignment tasks before progressing to harder ones.
- Core assumption: Learning alignment preferences follows a curriculum where easier examples should be learned before harder ones, similar to curriculum learning in supervised learning.
- Evidence anchors: [section], [section], [corpus]

### Mechanism 3
- Claim: Initializing the target model as the previous step's aligned model (rather than the SFT base) provides stability and ensures full data utilization.
- Mechanism: By initializing the target model as Mt-1 instead of the base SFT model S, each step uses the same setup as traditional DPO, maintaining theoretical benefits while ensuring the final model has seen all data.
- Core assumption: The DPO theoretical framework works best when the target and reference models are similar in their alignment state, and initializing from the base model at each step would be suboptimal.
- Evidence anchors: [section], [section], [corpus]

## Foundational Learning

- Concept: Preference optimization and DPO
  - Why needed here: sDPO is an extension of DPO, so understanding the base DPO mechanism is essential
  - Quick check question: In DPO, how is the loss function structured to encourage the model to prefer chosen responses over rejected ones?

- Concept: Curriculum learning
  - Why needed here: sDPO implicitly uses curriculum learning by progressing from easier to harder alignment tasks
  - Quick check question: What is the core idea behind curriculum learning and why might it help in training complex models?

- Concept: Reference model role in preference optimization
  - Why needed here: The paper emphasizes that the reference model acts as a lower bound and using more aligned references improves results
  - Quick check question: In the context of DPO, what role does the reference model play in determining the optimization direction?

## Architecture Onboarding

- Component map:
  SFT base model -> Preference datasets (partitioned) -> Reference model (updated each step) -> Target model (initialized from previous aligned model) -> Evaluation framework

- Critical path:
  1. Load SFT base model and preference datasets
  2. Partition datasets using easy-to-hard strategy
  3. For each chunk:
     - Set reference model = previous step's aligned model
     - Set target model = reference model (for initialization)
     - Train target model on current chunk using DPO loss
  4. Evaluate final model

- Design tradeoffs:
  - Number of steps vs. data utilization: More steps mean more gradual alignment but also more training overhead
  - Partitioning strategy: Easy-to-hard vs. random vs. other criteria
  - Model initialization: Starting from base vs. previous aligned model

- Failure signatures:
  - High variance in loss curves across steps (indicates instability)
  - Degraded performance on earlier tasks after later steps (indicates catastrophic forgetting)
  - Minimal improvement between steps (indicates the partitioning isn't effective)

- First 3 experiments:
  1. Run sDPO with 2 steps on a small dataset and compare to standard DPO - verify the H4 score improvement
  2. Test different partitioning strategies (easy-to-hard vs. random) with the same number of steps - confirm the curriculum effect
  3. Compare target model initialization strategies (from base vs. from previous) - verify the stability benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does sDPO's performance scale with the number of partitioned datasets (T)? Is there an optimal T for balancing performance gains and computational cost?
- Basis in paper: [inferred] The paper mentions partitioning preference datasets into T chunks and performing DPO training T times, but does not explore the impact of varying T.
- Why unresolved: The paper only demonstrates sDPO with two partitioned datasets (D1 and D2), leaving the effect of different T values unexplored.
- What evidence would resolve it: Experiments comparing sDPO performance with varying numbers of partitioned datasets (e.g., T=2, 3, 4, 5) while keeping the total dataset size constant would show how performance scales with T.

### Open Question 2
- Question: How does sDPO perform when applied to different base model architectures beyond the Llama-2 family (e.g., GPT-2, OPT, or BLOOM)?
- Basis in paper: [explicit] The paper notes that results likely transfer to other decoder-only transformer models but focuses on SOLAR 10.7B based on Llama-2 architecture.
- Why unresolved: The paper primarily uses SOLAR 10.7B for experiments, limiting generalizability to other architectures.
- What evidence would resolve it: Applying sDPO to various base model architectures and comparing performance against DPO baselines would demonstrate cross-architecture effectiveness.

### Open Question 3
- Question: Can sDPO be effectively combined with iterative data generation methods (like those proposed by Yuan et al., 2024) to create a more powerful alignment pipeline?
- Basis in paper: [explicit] The paper mentions that its approach is complementary to iterative data generation methods and suggests this as an exciting future direction.
- Why unresolved: The paper does not explore combining sDPO with data generation methods, leaving potential synergies untested.
- What evidence would resolve it: Experiments applying sDPO to preference datasets generated through iterative methods, and comparing against sDPO on static datasets, would reveal performance differences.

## Limitations

- Experimental validation relies heavily on synthetic benchmarks rather than comprehensive real-world deployment studies
- Lacks ablation studies on different partitioning strategies beyond easy-to-hard versus random
- Doesn't explore how the method scales to different model sizes or domain-specific preference data
- Claims about stability improvements are primarily based on visual inspection of loss curves rather than quantitative stability metrics

## Confidence

- **High confidence**: The core sDPO methodology (progressive reference model updating) is well-defined and the experimental setup is reproducible with the provided information
- **Medium confidence**: The empirical improvements over standard DPO are supported by benchmark results, though the magnitude of improvement may vary with different datasets and model architectures
- **Medium confidence**: The theoretical justification for using aligned reference models is sound, but the specific mechanisms (curriculum learning effects, initialization stability) could benefit from more rigorous analysis

## Next Checks

1. **Ablation on partitioning strategies**: Test sDPO with multiple partitioning criteria (difficulty-based, diversity-based, random) across different preference datasets to isolate the contribution of the easy-to-hard approach versus the progressive reference model mechanism

2. **Intermediate model analysis**: Measure and compare the alignment quality of each intermediate model (M1, M2, etc.) using independent evaluation metrics to verify that the progressive improvement hypothesis holds at each step

3. **Cross-dataset generalization**: Apply sDPO to a preference dataset from a different domain (e.g., technical Q&A or creative writing) to test whether the method's benefits generalize beyond the conversational preference data used in the main experiments