---
ver: rpa2
title: 'TCMBench: A Comprehensive Benchmark for Evaluating Large Language Models in
  Traditional Chinese Medicine'
arxiv_id: '2406.01126'
source_url: https://arxiv.org/abs/2406.01126
tags:
- uni00000013
- uni00000011
- uni00000044
- llms
- uni00000037
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TCMBench, a comprehensive benchmark for evaluating
  large language models (LLMs) in Traditional Chinese Medicine (TCM). It addresses
  the gap in standardized evaluation benchmarks for LLMs in the TCM domain, which
  has a profound history and vast influence.
---

# TCMBench: A Comprehensive Benchmark for Evaluating Large Language Models in Traditional Chinese Medicine

## Quick Facts
- arXiv ID: 2406.01126
- Source URL: https://arxiv.org/abs/2406.01126
- Reference count: 30
- Primary result: Current LLMs show unsatisfactory performance on TCM tasks, with significant room for improvement

## Executive Summary
This paper introduces TCMBench, a comprehensive benchmark for evaluating large language models (LLMs) in Traditional Chinese Medicine (TCM). The work addresses the critical gap in standardized evaluation benchmarks for LLMs in TCM, a domain with profound historical significance and vast influence. The authors construct a large-scale TCM evaluation dataset (TCM-ED) sourced from the TCM Licensing Exam and develop a domain-specific evaluation metric (TCMScore) that considers both TCM semantics and knowledge consistency. The benchmark reveals that current LLMs perform poorly on TCM tasks, highlighting the need for improved domain-specific capabilities.

## Method Summary
The core methodology involves constructing TCM-ED, a dataset of 5,473 questions from the TCM Licensing Exam, and developing TCMScore, a domain-specific evaluation metric. The metric combines TCM terminology matching with semantic consistency evaluation using a fine-tuned TCM-Deberta model. LLMs are evaluated using zero-shot and few-shot prompting approaches across different question types. The evaluation pipeline includes both accuracy metrics and expression quality metrics (Rouge, BertScore, BartScore, SARI), with TCMScore providing a comprehensive assessment of semantic and knowledge consistency in TCM domain responses.

## Key Results
- Current LLMs demonstrate unsatisfactory performance on TCMBench, indicating significant room for improvement
- Domain-specific fine-tuning (e.g., ZhongJing-TCM) improves accuracy but may degrade general reasoning capabilities
- TCMScore effectively addresses limitations of traditional text generation metrics by incorporating TCM terminology and semantic consistency
- Fine-tuning with TCM knowledge can enhance LLMs' performance, but may negatively impact fundamental LLM abilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TCMScore effectively evaluates semantic and knowledge consistency in TCM domain where traditional metrics fail
- Mechanism: Combines TCM terminology matching (Term F1 Score) with semantic inference (TCM-Deberta) to weight semantic consistency by knowledge relevance
- Core assumption: TCM terminology presence correlates with knowledge correctness and semantic evaluation should be weighted by terminology matching
- Evidence anchors:
  - [abstract] "We introduce an automatic metric called TCMScore to evaluate the consistency of TCM semantics and knowledge. It combines the matching of TCM terms and the semantic consistency between the generated and standard analyses."
  - [section] "The core idea of the F1* is to comprehensively consider the redundancy(i.e., precision), matching degree (i.e., recall), and term diversity of TCM terminologies."
- Break condition: If TCM terminology database is incomplete or LLMs generate correct answers without using expected TCM terminology, the weighting mechanism would incorrectly penalize valid responses

### Mechanism 2
- Claim: TCMBench reveals LLMs' significant room for improvement in TCM domain through comprehensive evaluation
- Mechanism: Uses actual TCM licensing exam questions across all question types and branches, evaluating both accuracy and expression quality with domain-specific metrics
- Core assumption: TCM licensing exam questions comprehensively represent required TCM knowledge and skills for clinical practice
- Evidence anchors:
  - [abstract] "The unsatisfactory performance of LLMs on this benchmark underscores their significant room for improvement in TCM."
  - [section] "It comprises 5,473 question-answer(Q&A) pairs, with 1,300 data pairs with standard analysis, ensuring the reliability of the data quality."
- Break condition: If the licensing exam questions don't represent the full scope of practical TCM knowledge or if the evaluation metrics don't capture clinically relevant performance differences

### Mechanism 3
- Claim: Domain-specific fine-tuning improves TCM performance but may degrade general reasoning capabilities
- Mechanism: Models like ZhongJing-TCM fine-tuned on TCM data show improved accuracy but worse expression quality metrics compared to general models
- Core assumption: Fine-tuning on domain-specific data trades off general reasoning capabilities for domain-specific knowledge
- Evidence anchors:
  - [abstract] "Introducing domain knowledge can enhance LLMs' performance. However, for in-domain models like ZhongJing-TCM, the quality of generated analysis text has decreased, and we hypothesize that their fine-tuning process affects the basic LLM capabilities."
  - [section] "From the express quality and human evaluation, fine-tuning LLMs with domain knowledge in TCM weakens their fundamental abilities in logical reasoning, knowledge analysis, and semantic expression."
- Break condition: If the observed degradation is due to factors other than fine-tuning (e.g., model architecture limitations or evaluation bias), or if improved domain performance justifies the trade-off

## Foundational Learning

- Concept: Domain-specific terminology and semantic representation
  - Why needed here: TCM has unique terminology (e.g., "external attack of wind cold") that differs from Western medicine, requiring specialized handling for accurate evaluation
  - Quick check question: Why can't standard semantic similarity metrics like BERTScore effectively evaluate TCM responses?

- Concept: Natural Language Inference (NLI) for evaluation metrics
  - Why needed here: TCM semantic consistency requires understanding entailment relationships between generated and standard analyses, which NLI models can provide
  - Quick check question: What makes the TCM-Deberta model more suitable for TCM semantic evaluation than general NLI models?

- Concept: Question type differentiation and prompt engineering
  - Why needed here: TCMBench includes different question types (A1/A2, A3, B1) requiring specific prompt templates and evaluation approaches
  - Quick check question: How does the prompt structure differ between zero-shot and few-shot evaluation for A3 type questions?

## Architecture Onboarding

- Component map: TCM-ED dataset (5,473 questions) -> TMNLI dataset (29,497 NLI pairs) -> TCM-Deberta model -> TCMScore metric -> Evaluation pipeline with multiple metrics
- Critical path: Data collection → Dataset construction → Metric development → Model evaluation → Human validation
- Design tradeoffs: Domain-specific metrics vs. general metrics, terminology matching vs. semantic similarity, accuracy vs. expression quality evaluation
- Failure signatures: Poor performance on specific branches, inconsistency between accuracy and expression metrics, degradation in general reasoning capabilities
- First 3 experiments:
  1. Evaluate a general LLM (e.g., GPT-4) on all question types to establish baseline performance
  2. Compare general vs. domain-specific models (e.g., ChatGPT vs. ZhongJing-TCM) on both accuracy and TCMScore
  3. Test the impact of prompt engineering (CoT vs. no CoT) on model performance across different question types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the hallucination phenomenon in LLMs' generated content be effectively identified and quantified when evaluating TCM knowledge?
- Basis in paper: [explicit] The paper mentions that "experiments reveal that some LLMs produce wrong information (i.e., hallucination phenomenon) when generating content" and states this will be a focus of future research
- Why unresolved: While the paper acknowledges the issue of hallucination, it does not provide concrete methods for identifying or quantifying this phenomenon in the context of TCM knowledge generation
- What evidence would resolve it: Development and validation of metrics or methods that can reliably detect and measure the extent of hallucination in LLMs' TCM-related outputs, with clear benchmarks for acceptable vs. problematic levels

### Open Question 2
- Question: How can LLMs be improved to better handle the unique clinical logic of TCM, particularly syndrome differentiation and treatment?
- Basis in paper: [explicit] The paper states that future work will focus on "accurately evaluating if LLMs can follow the unique clinical logic of TCM, which is syndrome differentiation and treatment."
- Why unresolved: The paper acknowledges the importance of syndrome differentiation and treatment in TCM but does not provide specific approaches for evaluating or improving LLMs' ability to apply this unique clinical logic
- What evidence would resolve it: Development of evaluation metrics and fine-tuning techniques that specifically target syndrome differentiation and treatment capabilities in LLMs, with demonstrated improvements in clinical scenario tasks

### Open Question 3
- Question: How does the incorporation of domain-specific knowledge during pre-training compare to fine-tuning in terms of improving LLMs' performance in specialized domains like TCM?
- Basis in paper: [inferred] The paper discusses how general LLMs with hundreds of billions of parameters show potential for better application in TCM, and how incorporating professional TCM knowledge during pre-training can significantly improve model performance
- Why unresolved: While the paper suggests that pre-training with domain-specific knowledge is beneficial, it does not directly compare the effectiveness of pre-training versus fine-tuning approaches for domain adaptation
- What evidence would resolve it: Controlled experiments comparing LLMs trained with domain-specific knowledge from the start versus those fine-tuned on domain data, with comprehensive evaluations of their performance in TCM tasks

## Limitations

- Potential bias in evaluation framework due to reliance on TCM licensing exam questions that may not fully capture practical clinical knowledge
- TCMScore metric depends on a comprehensive TCM terminology database that may be incomplete or contain errors, potentially penalizing valid responses
- Observed degradation in general reasoning capabilities for domain-specific fine-tuned models needs further investigation to establish causation

## Confidence

- **High Confidence**: The construction of TCMBench as a benchmark using actual TCM licensing exam questions is methodologically sound and well-documented
- **Medium Confidence**: The effectiveness of TCMScore as a domain-specific evaluation metric is plausible but requires independent validation
- **Medium Confidence**: The observation that domain fine-tuning may degrade general reasoning capabilities is based on observed correlations but requires controlled experiments to establish causation

## Next Checks

1. **Independent TCMScore Validation**: Replicate the TCMScore metric evaluation using a completely independent TCM terminology database and TMNLI dataset to verify that the metric performance is not dependent on the specific datasets used in the original study

2. **Clinical Relevance Assessment**: Conduct a study with practicing TCM physicians to evaluate whether the TCMBench questions and the TCMScore metric actually correlate with clinically relevant knowledge and decision-making capabilities in real-world TCM practice

3. **Fine-tuning Architecture Comparison**: Perform controlled experiments comparing different fine-tuning architectures (adapter-based vs. full fine-tuning) and data selection strategies to determine if the observed degradation in general reasoning is inherent to domain adaptation or can be mitigated through architectural choices