---
ver: rpa2
title: 'The Ink Splotch Effect: A Case Study on ChatGPT as a Co-Creative Game Designer'
arxiv_id: '2403.02454'
source_url: https://arxiv.org/abs/2403.02454
tags:
- game
- games
- player
- chatgpt
- would
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores the creative potential of large language models\
  \ (LLMs) like ChatGPT in game design by comparing games designed with human intuition\
  \ to those guided by AI suggestions. Three genres\u2014arcade space shooter, platformer,\
  \ and turn-based roguelike\u2014were prototyped, each with three versions: a minimalist\
  \ base, a human-designed game, and a ChatGPT-guided game."
---

# The Ink Splotch Effect: A Case Study on ChatGPT as a Co-Creative Game Designer

## Quick Facts
- arXiv ID: 2403.02454
- Source URL: https://arxiv.org/abs/2403.02454
- Reference count: 26
- Key outcome: Human-designed games outperformed ChatGPT-guided games in overall preference, game feel, and thematic cohesion, though both outperformed minimalist base games.

## Executive Summary
This paper investigates the creative potential of large language models like ChatGPT in game design by comparing human-designed games to those guided by AI suggestions. Three game genres (arcade space shooter, platformer, and turn-based roguelike) were prototyped in three versions: minimalist base, human-designed, and ChatGPT-guided. Through user studies with 45 participants, human-designed games consistently ranked higher in overall preference, game feel, and thematic cohesion, while both outperformed the base games. The study reveals that while ChatGPT excels at generating abstract ideas and can broaden a designer's ideation range, it struggles with context-specific code implementation and creating cohesive gameplay experiences. The findings suggest AI functions best as a co-creative tool rather than a replacement for human designers.

## Method Summary
The study employed a comparative experimental design where three game genres were developed in three versions each: a minimalist base game, a human-designed version, and a ChatGPT-guided version. Human designers first created the minimalist versions, then either enhanced them based on their own ideas or implemented ChatGPT's suggestions directly without refinement. User studies were conducted through a website where participants played the games blindly and ranked them across six criteria. The study included 45 participants with varying gaming and design experience, and data was collected via surveys to evaluate differences in design approaches.

## Key Results
- Human-designed games consistently outperformed ChatGPT-guided games in overall preference, game feel, and thematic cohesion across all three genres
- Both human-designed and ChatGPT-guided games significantly outperformed the minimalist base games
- ChatGPT excelled at generating abstract ideas and broadening ideation range but struggled with context-specific code implementation and creating cohesive gameplay

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human designers can better integrate abstract ideas into cohesive gameplay than ChatGPT.
- Mechanism: Humans evaluate and refine ideas through iterative playtesting, while ChatGPT provides disconnected suggestions that require manual integration.
- Core assumption: The designer's ability to assess and refine based on player experience is critical for creating cohesive gameplay.
- Evidence anchors:
  - [abstract] "ChatGPT excelled at generating abstract ideas but struggled with context-specific code and complex mechanics, often producing less polished gameplay."
  - [section] "The process began with searching for a basic color shifting script, but transformed into something else entirely which better suited the designer's goals."
  - [corpus] Found 25 related papers; average neighbor FMR=0.483; however, none directly address human-AI creative integration in game design.
- Break condition: If AI can be trained to understand and apply context-specific game design principles and player experience feedback, this advantage may diminish.

### Mechanism 2
- Claim: ChatGPT's suggestions broaden the designer's ideation range but lack thematic coherence.
- Mechanism: ChatGPT provides diverse ideas without creative biases, while humans tend to follow established patterns once committed to an idea.
- Core assumption: Exposure to diverse ideas can help designers overcome creative fixation.
- Evidence anchors:
  - [section] "The human designer acknowledged they had not considered non-offensive options for abilities, and nearly instantly began to absorb these suggestions into their 'elemental' theme for the game."
  - [section] "In some cases, those instincts made their way into the game design without the prompter realizing."
  - [corpus] Related papers on co-creative storytelling and AI-driven game level editors suggest broader applications of AI in creative processes.
- Break condition: If the designer is already adept at generating diverse ideas, ChatGPT's broadening effect may be less significant.

### Mechanism 3
- Claim: ChatGPT's code implementation struggles with context and complex mechanics.
- Mechanism: ChatGPT can suggest relevant code snippets but often fails to account for the specific context of the game engine and mechanics.
- Core assumption: Understanding the nuances of the game engine and mechanics is crucial for successful code implementation.
- Evidence anchors:
  - [abstract] "ChatGPT excelled at generating abstract ideas but struggled with context-specific code and complex mechanics, often producing less polished gameplay."
  - [section] "ChatGPT performed best when it was prompted for abstract, high level game feature suggestions... Conversely, ChatGPT struggled with context, particularly when suggesting code snippets to implement into the game."
  - [corpus] No direct evidence in corpus regarding code implementation struggles, but related papers on AI-driven game level editors suggest challenges in computational creativity.
- Break condition: If ChatGPT's training data is enhanced with more context-specific examples and it's fine-tuned for specific game engines, this struggle may be reduced.

## Foundational Learning

- Concept: Iterative game design and playtesting
  - Why needed here: Humans use iterative playtesting to refine gameplay, which ChatGPT lacks.
  - Quick check question: What is the purpose of iterative playtesting in game design?

- Concept: Thematic cohesion in game design
  - Why needed here: Humans create cohesive themes by integrating abilities and visual effects, which ChatGPT struggles with.
  - Quick check question: How does thematic cohesion enhance player experience in games?

- Concept: Context-specific code implementation
  - Why needed here: ChatGPT struggles with implementing code that accounts for the specific context of the game engine and mechanics.
  - Quick check question: Why is understanding the game engine context crucial for successful code implementation?

## Architecture Onboarding

- Component map: User study website (GitHub Pages) -> Game prototypes (Unity) -> Survey form (Google Forms) -> Data analysis
- Critical path: Game design -> User study -> Data collection -> Analysis -> Conclusion
- Design tradeoffs: Simplicity of game prototypes vs. complexity of mechanics; anonymity of games vs. detailed feedback
- Failure signatures: Inconsistent game rankings; low user engagement; technical issues with game prototypes
- First 3 experiments:
  1. Recreate the base games and have a designer implement features based on their own ideas.
  2. Have a designer use ChatGPT for ideation but implement features themselves.
  3. Have a designer use ChatGPT for both ideation and code implementation, documenting the process and challenges.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform in generating code for game engines with fewer online resources compared to Unity?
- Basis in paper: [inferred] from discussion of Unity's extensive online resources and the challenges faced with Unity's coordinate system.
- Why unresolved: The paper focuses on Unity and does not test other game engines.
- What evidence would resolve it: Comparative studies of LLM-generated code in engines like Godot, CryEngine, or proprietary engines with limited online documentation.

### Open Question 2
- Question: Can LLMs effectively contribute to level design, encounter design, and other diverse gameplay features beyond basic mechanics?
- Basis in paper: [inferred] from the authors' suggestion for future work to test LLMs on more diverse game design tasks.
- Why unresolved: The study primarily focuses on basic game mechanics and does not explore LLMs' potential in more complex design aspects.
- What evidence would resolve it: Experiments where LLMs are tasked with designing levels, encounters, and other gameplay elements in various game genres.

### Open Question 3
- Question: How does the use of LLMs in game design affect the creative process and final products when used by experienced developers?
- Basis in paper: [explicit] from the discussion of future work involving experienced developers using LLMs for creative inspiration.
- Why unresolved: The study uses novice designers and does not explore the impact on experienced professionals.
- What evidence would resolve it: Studies where experienced game developers use LLMs in their design process, with feedback on the usefulness and analysis of the resulting games.

## Limitations
- The study's conclusions are based on a small sample of 45 participants with self-reported expertise levels, which may not represent the broader gaming population.
- ChatGPT-guided games were implemented "as suggested" without designer refinement, potentially exaggerating AI's limitations rather than reflecting practical usage.
- The study only tested three game genres and six specific criteria, leaving open questions about AI's effectiveness in other genres or evaluation dimensions.

## Confidence
- High Confidence: The finding that human-designed games outperformed ChatGPT-guided games in overall preference, game feel, and thematic cohesion is supported by direct experimental evidence and multiple participant rankings.
- Medium Confidence: The conclusion that ChatGPT excels at generating abstract ideas but struggles with context-specific implementation is based on observable patterns in the design process, though the extent of this limitation may vary with different prompting strategies.
- Low Confidence: The claim that AI cannot replace human designers' creative vision is extrapolated from this single case study and may not hold across different design contexts or more advanced AI systems.

## Next Checks
1. **Prompt Engineering Study**: Systematically test different prompting strategies with ChatGPT to determine if improved prompting can reduce the context-specific implementation issues observed in this study.

2. **Iterative Design Comparison**: Compare outcomes when designers use ChatGPT for ideation only versus ideation plus implementation, measuring the impact of designer refinement on final game quality.

3. **Genre Expansion Validation**: Replicate the study across additional game genres (e.g., puzzle, simulation, narrative games) to determine if the observed patterns hold across different design spaces.