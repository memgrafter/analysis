---
ver: rpa2
title: Boosting In-Context Learning in LLMs Through the Lens of Classical Supervised
  Learning
arxiv_id: '2505.23783'
source_url: https://arxiv.org/abs/2505.23783
tags:
- calibration
- context
- methods
- learning
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of systematic biases in in-context
  learning (ICL) predictions of large language models (LLMs), which lead to unstable
  and inaccurate classifications. The core contribution is Supervised Calibration
  (SC), a loss-minimization framework that learns per-class affine transformations
  of LLM logits in the logit space.
---

# Boosting In-Context Learning in LLMs Through the Lens of Classical Supervised Learning

## Quick Facts
- arXiv ID: 2505.23783
- Source URL: https://arxiv.org/abs/2505.23783
- Reference count: 40
- Primary result: SC achieves up to 22.6% improvement in Macro-F1 over base LLMs

## Executive Summary
This paper addresses systematic biases in large language model in-context learning predictions through Supervised Calibration (SC), a novel loss-minimization framework that learns per-class affine transformations of LLM logits. Unlike prior calibration methods limited to shifting decision boundaries, SC can alter or reverse boundary orientation by optimizing both scaling and bias terms. The framework employs a leave-subset-out strategy to generate surrogate training data without external data and integrates context-invariance and directional trust-region regularizations to enhance stability and control calibration degree.

## Method Summary
Supervised Calibration (SC) treats calibration as a supervised learning problem where LLM logits serve as features and true labels as targets. The method learns optimal bias and scaling parameters for each class via logistic regression on surrogate data generated through a leave-subset-out strategy. SC incorporates two key regularizations: context-invariance to mitigate ICL's sensitivity to context composition and ordering, and directional trust-region to control calibration degree by constraining parameter updates to align with base LLM predictions. The framework uses ensembling across multiple context sizes for final predictions.

## Key Results
- SC achieves state-of-the-art performance, improving Macro-F1 by up to 22.6% (8-shot on Qwen2-7B-Instruct)
- Outperforms baseline calibration methods by up to 13.4% absolute
- Doubled accuracy on SST-5 (24% to 44%) by reorienting decision boundaries for closely related classes
- Validated across nine datasets and three LLMs with 4-shot, 8-shot, and 16-shot settings

## Why This Works (Mechanism)

### Mechanism 1
SC improves ICL performance by learning per-class affine transformations in logit space, enabling both shifting and reorienting the decision boundary. It treats calibration as supervised learning where LLM logits are features and true labels are targets, learning optimal bias and scaling parameters for each class via logistic regression on surrogate data. The relationship between true logits L* and LLM logits m is approximated by an affine transformation Lc(x; θk c) = wk c mc(x;Ck) + bk c.

### Mechanism 2
SC's context-invariance regularization mitigates ICL's sensitivity to context composition and ordering by adding symmetric cross-entropy penalty between predictions made using different sub-contexts of the same size. This encourages consistent outputs regardless of context variations, addressing the instability issue inherent in ICL.

### Mechanism 3
SC's directional trust-region regularization controls calibration degree by constraining parameter updates to align with base LLM predictions. It adds a constraint that average cosine similarity between learned parameters θi c and identity direction [0,1] must exceed threshold τ, controlling how much the calibration can deviate from base LLM.

## Foundational Learning

- **Logistic Regression**
  - Why needed here: SC uses logistic regression on LLM logits to learn optimal calibration parameters. Understanding this helps grasp how SC optimizes both bias and scaling terms.
  - Quick check question: In multi-class logistic regression, what do the learned weights represent in terms of decision boundaries?

- **Kullback-Leibler (KL) Divergence**
  - Why needed here: The calibration objective minimizes KL divergence between true posterior and calibrated predictions. This connects to information theory foundations.
  - Quick check question: What does minimizing KL divergence between P* and f(PLLM) accomplish in the calibration context?

- **Cross-Entropy Loss**
  - Why needed here: Both the main objective and context-invariance regularizer use cross-entropy to measure distributional differences. Understanding this is crucial for the regularization terms.
  - Quick check question: How does symmetric cross-entropy differ from standard cross-entropy in the context-invariance regularizer?

## Architecture Onboarding

- **Component map**: LLM inference layer → Logit extraction → Surrogate data generator → Affine-logit model trainer → Context-invariance regularizer → Directional trust-region constraint → Ensemble aggregator
- **Critical path**: LLM logits → Surrogate data generation → Parameter optimization → Regularized training → Ensembled prediction
- **Design tradeoffs**: Expressiveness vs. complexity (affine model more expressive but requires more parameters); Variance vs. bias (regularization reduces variance but may increase approximation error); Computational cost vs. performance (ensembling improves performance but increases inference time)
- **Failure signatures**: Calibration parameters explode or become NaN (trust-region constraint too loose); Performance worse than base LLM (regularization too strong or surrogate data generation flawed); High variance across seeds (insufficient regularization or poor context sampling)
- **First 3 experiments**: 1) Implement SC without any regularization (λinv=0, τ=0) and verify it matches or exceeds simple bias-only methods; 2) Add context-invariance regularization and measure stability across different context permutations; 3) Implement trust-region constraint and test on datasets where base LLM is known to be severely misaligned

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of reference class (currently class 0) affect the performance and calibration of Supervised Calibration (SC)? The paper does not explore the impact of choosing different reference classes on the model's performance or calibration quality.

### Open Question 2
What is the theoretical justification for the specific form of the context-invariance regularizer used in SC, and are there alternative formulations that might be more effective? The paper describes the regularizer's purpose but does not provide theoretical analysis of why this specific formulation is optimal.

### Open Question 3
How does SC perform on regression tasks, and what modifications would be needed to extend the framework beyond classification? The paper mentions that extending SC principles to regression tasks presents an interesting direction for future research.

## Limitations

- Evaluation framework primarily compares against baseline calibration methods rather than newer ICL-specific techniques like Demonstration Selection or Prompt Compression
- Leave-subset-out strategy for generating surrogate training data may not fully capture the distribution of contexts the LLM encounters during inference
- Regularization hyperparameters are not extensively tuned across datasets, raising questions about optimal performance

## Confidence

**High Confidence**: The core technical contribution of learning per-class affine transformations in logit space is well-defined and theoretically sound. The claim that this enables both shifting and reorienting decision boundaries is strongly supported by experimental results.

**Medium Confidence**: The effectiveness of context-invariance and directional trust-region regularizations is reasonably supported, though more ablation studies would help isolate their individual contributions.

**Low Confidence**: The claim that improvements are primarily due to boundary reorientation rather than simple shifting is partially supported but could benefit from more detailed analysis and visualizations.

## Next Checks

1. **Ablation Study on Regularization**: Run experiments with SC without any regularization, then with only context-invariance, then with only trust-region constraints to quantify individual contributions.

2. **Boundary Analysis Visualization**: Generate visualizations of decision boundaries before and after calibration for datasets showing dramatic improvements to demonstrate actual reorientation versus shifting.

3. **Robustness Across Seed Variance**: Run additional seeds (e.g., 10-20) for top-performing and bottom-performing combinations to reveal whether SC's benefits are robust or sensitive to random initialization.