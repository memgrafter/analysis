---
ver: rpa2
title: Spectral Self-supervised Feature Selection
arxiv_id: '2407.09061'
source_url: https://arxiv.org/abs/2407.09061
tags:
- feature
- features
- selection
- eigenvectors
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a spectral self-supervised feature selection
  method that addresses the challenge of selecting meaningful features in unsupervised
  settings. The core idea is to compute robust pseudo-labels from graph Laplacian
  eigenvectors and use these as targets for supervised feature selection models.
---

# Spectral Self-supervised Feature Selection

## Quick Facts
- arXiv ID: 2407.09061
- Source URL: https://arxiv.org/abs/2407.09061
- Authors: Daniel Segal; Ofir Lindenbaum; Ariel Jaffe
- Reference count: 18
- Key outcome: Proposes spectral self-supervised feature selection using Laplacian eigenvectors as pseudo-labels, demonstrating superior performance on biological datasets and challenging scenarios

## Executive Summary
This paper introduces a novel spectral self-supervised feature selection method that leverages graph Laplacian eigenvectors to generate pseudo-labels for training supervised feature selection models in unsupervised settings. The approach addresses the challenge of selecting meaningful features without labeled data by computing robust pseudo-labels through eigenvector processing and selecting informative features based on model stability. The method shows particular effectiveness in handling outliers and complex substructures, outperforming existing unsupervised feature selection methods on real-world datasets, especially in biological domains.

## Method Summary
The method constructs a similarity graph from data, computes its Laplacian eigenvectors, and generates pseudo-labels by binarizing these eigenvectors using k-medoids clustering. Rather than using all leading eigenvectors, it selects the most informative subset based on feature score stability across multiple surrogate models trained on resampled data. This stability criterion filters out eigenvectors corrupted by noise or outliers. The final feature selection is performed by training a supervised model to predict the selected pseudo-labels and ranking features by their importance scores.

## Key Results
- Outperforms existing unsupervised feature selection methods on real-world datasets, particularly biological datasets
- Demonstrates robustness to outliers and complex substructures through stability-based eigenvector selection
- Shows improved clustering performance compared to baseline methods when using selected features
- Ablation studies confirm the importance of both binarization and stability-based eigenvector selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Identifies features most relevant to latent cluster structure by leveraging graph Laplacian eigenvectors as pseudo-labels
- Mechanism: Graph Laplacian eigenvectors capture the intrinsic manifold structure of the data. By selecting the most informative eigenvectors based on model stability, the method ensures that the pseudo-labels reflect the primary data substructures rather than noise or outliers
- Core assumption: The data lies on a low-dimensional manifold where the Laplacian eigenvectors approximate the eigenfunctions of the Laplace-Beltrami operator
- Evidence anchors:
  - [abstract] "Our method's core involves computing robust pseudo-labels by applying simple processing steps to the graph Laplacian's eigenvectors"
  - [section] "The subset of eigenvectors used for computing pseudo-labels is chosen based on a model stability criterion"
- Break condition: If the data does not lie on a manifold structure or if the manifold assumption is severely violated, the Laplacian eigenvectors may not capture meaningful substructures, rendering the pseudo-labels ineffective

### Mechanism 2
- Claim: Binarizing eigenvectors and using classification models improves feature selection accuracy compared to regression-based approaches
- Mechanism: Binarization converts continuous eigenvector values into discrete pseudo-labels using k-medoids, making the task a classification problem. Classification models are more robust to the non-linear relationships between features and cluster structures, leading to more discriminative feature selection
- Core assumption: The cluster structure in the data can be effectively represented by binary pseudo-labels derived from eigenvectors
- Evidence anchors:
  - [abstract] "We then measure the importance of each feature by training a surrogate model to predict the pseudo-labels from the observations"
  - [section] "The feature selection is thus based on a classification rather than a regression task, which is more aligned with selecting features for clustering"
- Break condition: If the cluster boundaries are highly overlapping or if the data distribution is not well-separated, binarization may lead to loss of information, and the classification model may not perform well

### Mechanism 3
- Claim: The stability-based eigenvector selection improves robustness by filtering out eigenvectors corrupted by noise or outliers
- Mechanism: The method trains multiple surrogate models on resampled subsets of the data and measures the variance of feature scores. Eigenvectors with lower variance in feature scores are considered more stable and informative, reducing the impact of noise and outliers on the feature selection process
- Core assumption: Informative eigenvectors should yield consistent feature importance scores across different data subsets
- Evidence anchors:
  - [abstract] "The subset of eigenvectors used for computing pseudo-labels is chosen based on a model stability criterion"
  - [section] "We keep, as pseudo-labels, the k binarized eigenvectors with the lowest sum of variance"
- Break condition: If the data is highly imbalanced or if there are very few informative features, the variance measure may not effectively distinguish between informative and uninformative eigenvectors

## Foundational Learning

- Concept: Graph Laplacian and spectral clustering
  - Why needed here: The method relies on constructing a graph representation of the data and using the Laplacian eigenvectors to capture the intrinsic structure for feature selection
  - Quick check question: What is the relationship between the graph Laplacian and the manifold structure of the data?

- Concept: Manifold learning and eigenfunctions
  - Why needed here: The theoretical analysis assumes that the data lies on a manifold, and the Laplacian eigenvectors approximate the eigenfunctions of the Laplace-Beltrami operator, which is crucial for understanding the convergence and properties of the method
  - Quick check question: How do the eigenfunctions of the Laplace-Beltrami operator relate to the eigenvectors of the graph Laplacian in the context of manifold learning?

- Concept: Model stability and variance estimation
  - Why needed here: The method uses stability-based selection of eigenvectors by measuring the variance of feature scores across multiple models trained on resampled data subsets
  - Quick check question: How does the variance of feature scores across multiple models relate to the stability and informativeness of the eigenvectors?

## Architecture Onboarding

- Component map: Pairwise distances -> Similarity graph -> Laplacian matrix -> Eigenvectors -> Binarization -> Stability selection -> Final model -> Feature ranking
- Critical path:
  1. Construct the similarity graph and compute the Laplacian matrix
  2. Compute the leading eigenvectors of the Laplacian
  3. Generate pseudo-labels by binarizing the eigenvectors
  4. Select informative eigenvectors based on model stability
  5. Train a final surrogate model and rank features
- Design tradeoffs:
  - Computational complexity: The method involves computing pairwise distances and eigendecomposition, which can be expensive for large datasets
  - Choice of surrogate models: The performance of the method depends on the choice of surrogate models for eigenvector selection and feature scoring
  - Number of eigenvectors: Selecting too few or too many eigenvectors can impact the quality of pseudo-labels and feature selection
- Failure signatures:
  - Poor clustering performance: If the selected features do not improve clustering accuracy, it may indicate issues with graph construction, eigenvector selection, or surrogate model choice
  - High variance in feature scores: If the variance in feature scores is consistently high across all eigenvectors, it may suggest that the data does not have a clear manifold structure or that the surrogate models are not appropriate
- First 3 experiments:
  1. Verify graph construction and Laplacian computation on a simple dataset (e.g., two Gaussian blobs)
  2. Test eigenvector selection and pseudo-label generation on a dataset with known cluster structure
  3. Evaluate feature selection performance on a benchmark dataset with ground truth labels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SSFS change when using different surrogate models for eigenvector selection and feature selection, beyond the logistic regression and XGBoost models tested?
- Basis in paper: [explicit] The paper states that SSFS is compatible with any supervised model capable of providing feature importance scores, and that the choice of model for the feature selection step can differ from that used in the eigenvector selection step
- Why unresolved: The paper only tested SSFS with logistic regression for eigenvector selection and XGBoost for feature selection. The impact of using other models, such as random forests or neural networks, on SSFS performance is unknown
- What evidence would resolve it: Experiments comparing the performance of SSFS using various surrogate models for both eigenvector selection and feature selection on multiple datasets

### Open Question 2
- Question: How does the choice of the number of eigenvectors to select (k) affect the performance of SSFS, and is there an optimal way to determine this parameter?
- Basis in paper: [explicit] The paper sets k to the number of distinct classes in the dataset and selects eigenvectors from a total of d = 2k eigenvectors. It also mentions that the number of considered eigenvectors should increase with the number of classes and noise level in the data
- Why unresolved: The paper does not explore the impact of varying k on SSFS performance or provide a method for determining the optimal value of k for a given dataset
- What evidence would resolve it: Experiments varying k and analyzing its impact on SSFS performance, along with a proposed method for selecting the optimal k value

### Open Question 3
- Question: How does the performance of SSFS compare to other unsupervised feature selection methods when the number of samples (n) is small or the dimensionality (p) is very high?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of SSFS on real-world datasets but does not specifically address scenarios with limited samples or very high dimensionality
- Why unresolved: The computational complexity of SSFS scales with n and p, and the performance of the method may degrade in scenarios with limited data or high dimensionality due to increased noise or overfitting
- What evidence would resolve it: Experiments evaluating the performance of SSFS and other methods on datasets with varying n and p, particularly focusing on scenarios with limited samples or very high dimensionality

## Limitations

- Theoretical assumptions about manifold structure may not hold for all real-world datasets
- Computational complexity scales poorly with dataset size due to pairwise distance computation and eigendecomposition
- Performance depends on hyperparameter choices (k in k-medoids, number of eigenvectors, kernel bandwidth) that are not thoroughly explored

## Confidence

- **Mechanism 1 (Graph Laplacian eigenvectors as pseudo-labels)**: Medium - While the theoretical foundation is standard, the empirical validation of pseudo-label quality is limited to indirect measures through feature selection performance
- **Mechanism 2 (Binarization + classification)**: Medium - The claim is reasonable but lacks direct comparison with regression-based alternatives on the same datasets
- **Mechanism 3 (Stability-based eigenvector selection)**: High - This component is well-justified through both theoretical reasoning and ablation studies showing its importance

## Next Checks

1. **Direct pseudo-label quality assessment**: Measure the agreement between pseudo-labels and ground truth cluster assignments (where available) or use silhouette scores to evaluate pseudo-label quality independently of feature selection performance

2. **Hyperparameter sensitivity analysis**: Systematically vary k in k-medoids, number of eigenvectors, and kernel bandwidth across a grid to quantify their impact on feature selection quality and computational cost

3. **Scalability testing**: Evaluate the method's performance and runtime on datasets with >10,000 samples and >1,000 features to assess practical limitations for large-scale applications