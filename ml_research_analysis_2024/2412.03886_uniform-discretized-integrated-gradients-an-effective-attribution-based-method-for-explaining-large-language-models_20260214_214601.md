---
ver: rpa2
title: 'Uniform Discretized Integrated Gradients: An effective attribution based method
  for explaining large language models'
arxiv_id: '2412.03886'
source_url: https://arxiv.org/abs/2412.03886
tags:
- word
- udig
- attribution
- have
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel technique for attributing feature importance
  scores for large language models. Unlike existing methods like Integrated Gradients
  (IG) and Discretized Integrated Gradients (DIG), the authors argue that for discrete
  feature spaces like word embeddings, a non-linear interpolation path between the
  input and a baseline word is preferable.
---

# Uniform Discretized Integrated Gradients: An effective attribution based method for explaining large language models

## Quick Facts
- arXiv ID: 2412.03886
- Source URL: https://arxiv.org/abs/2412.03886
- Authors: Swarnava Sinha Roy; Ayan Kundu
- Reference count: 14
- Primary result: UDIG consistently outperforms IG and DIG in attribution quality metrics across sentiment classification and QA tasks

## Executive Summary
This paper proposes Uniform Discretized Integrated Gradients (UDIG), a novel attribution method for explaining large language models. Unlike existing methods like Integrated Gradients (IG) and Discretized Integrated Gradients (DIG), UDIG uses a non-linear interpolation path with uniformly distributed intermediate points that are both equidistant and lie close to actual words in the embedding space. This addresses limitations of DIG where interpolated points can be non-uniform and violate monotonicity constraints, leading to suboptimal attribution scores. The authors evaluate UDIG on two NLP tasks using three datasets and three automated metrics, demonstrating consistent improvements over IG and DIG.

## Method Summary
UDIG generates equidistant points along the straight line between baseline and target word embeddings, then selects nearest actual words in the vocabulary. The method uses the MASK token as baseline to represent a neutral starting point, and enforces monotonicity after nearest-word selection to ensure valid Riemann integration. The attribution scores are computed via gradient-based calculations at each selected word embedding, with results aggregated using Riemann sum approximation. The approach is designed to satisfy Integrated Gradients axioms while producing more accurate and reliable feature attributions for discrete feature spaces like word embeddings.

## Key Results
- UDIG consistently outperforms IG and DIG in Log Odds, Comprehensiveness, and Sufficiency metrics across SST2, IMDB, Rotten Tomatoes, and SQuAD datasets
- UDIG achieves lower Delta % error compared to DIG, indicating better adherence to the completeness axiom and more accurate integral approximation
- Despite being computationally more expensive, UDIG achieves superior performance with fewer interpolation steps than IG and DIG

## Why This Works (Mechanism)

### Mechanism 1
Uniform discretization of the interpolation path yields more accurate integral approximation than non-uniform discretization. UDIG samples equidistant points along the straight line between baseline and target word embeddings, then selects nearest actual words. This bounded and uniform sampling allows consistent application of Riemann summation. Core assumption: The embedding space is approximately locally linear between a word and its nearest neighbors. Evidence: Abstract states UDIG ensures intermediate points are uniformly distributed and close to actual words. Section 2.2 describes fixing points along the linear path in a uniform manner.

### Mechanism 2
Using the MASK token as baseline yields better attribution scores than PAD. The MASK embedding is trained to represent missing or masked words during pre-training, carrying minimal semantic bias. This satisfies the "near zero" baseline condition required for completeness. Core assumption: The MASK token embedding is effectively neutral with respect to the model's prediction function. Evidence: Section 2.3 explains MASK token satisfies neutrality criterion better than PAD. Table 1 shows model output at baseline drops to near zero for longer sequences when using MASK vs PAD.

### Mechanism 3
Enforcing monotonicity after nearest-word selection ensures the path satisfies completeness axiom. After selecting a neighbor word, any non-monotonic dimensions are perturbed to enforce monotonicity with respect to the baseline and target. This guarantees that each dimension's contribution can be integrated via Riemann sum. Core assumption: Monotonicity is sufficient for accurate attribution. Evidence: Section 2.2 defines monotonicity requirement for Riemann summation. Table 2 shows UDIG achieves lower Delta % error than DIG, supporting monotonicity enforcement improves completeness.

## Foundational Learning

- Concept: Riemann sum approximation of integrals
  - Why needed here: UDIG approximates the path integral of gradients via summation over discrete steps; understanding Riemann sums is essential to see why uniformity and monotonicity matter
  - Quick check question: Why does the completeness axiom require that the sum of attributions equals the difference in model outputs between input and baseline?

- Concept: Word embedding geometry and nearest-neighbor search
  - Why needed here: UDIG relies on selecting actual words near interpolated points; understanding how embeddings cluster and how nearest-neighbor search works is key to grasping the method's non-linearity
  - Quick check question: What could go wrong if the nearest-word search consistently picks outliers far from the interpolation path?

- Concept: Integrated Gradients axioms (Sensitivity, Implementation Invariance, Completeness)
  - Why needed here: UDIG is built to satisfy these axioms; knowing them explains why the method works and how it differs from other attribution techniques
  - Quick check question: How does using a zero embedding baseline violate the completeness axiom compared to using MASK?

## Architecture Onboarding

- Component map: Embedding space navigator -> Nearest-word selector -> Monotonicity enforcer -> Attribution calculator
- Critical path:
  1. Generate uniform points on line between baseline and target
  2. For each point, find nearest word in vocabulary
  3. Enforce monotonicity on each candidate word
  4. Compute gradient at each word embedding
  5. Apply Riemann sum to approximate integral
- Design tradeoffs:
  - Uniform sampling vs. adaptive sampling: Uniform is simpler but may miss important regions if embedding space is highly non-linear
  - Nearest-word selection vs. interpolation: Nearest-word keeps results interpretable but can introduce jumps; pure interpolation would be smoother but less grounded in actual words
  - Monotonicity enforcement vs. raw gradients: Enforcing monotonicity guarantees completeness but may distort gradients
- Failure signatures:
  - High Delta % error: indicates path is too non-linear or monotonicity enforcement is failing
  - Sudden attribution spikes: suggests nearest-word search is picking outliers
  - Degraded performance on non-BERT models: may mean baseline choice (MASK) is suboptimal
- First 3 experiments:
  1. Compare Delta % error of UDIG vs DIG on SST2 with f=1 and f=2; verify monotonicity enforcement improves completeness
  2. Run UDIG with PAD baseline vs MASK baseline on SST2; measure change in Log Odds and Sufficiency metrics
  3. Vary number of interpolation steps (10, 30, 60) for UDIG on Rotten Tomatoes; observe tradeoff between time complexity and metric improvement

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of baseline token affect attribution scores for language models without a MASK token (e.g., GPT models)? Basis: The authors recommend using MASK token as baseline when available, suggesting PAD or zero embedding as alternatives for models without MASK. Unresolved because the paper only demonstrates effectiveness on BERT-based models. Evidence needed: Experiments comparing attribution scores using PAD, zero embedding, and alternative baselines on GPT models across multiple tasks and datasets.

### Open Question 2
What is the optimal number of interpolation steps for UDIG that balances attribution quality and computational cost? Basis: The authors note UDIG is computationally more expensive but can achieve better performance with fewer steps. Unresolved because the paper doesn't systematically explore this tradeoff across different tasks and model sizes. Evidence needed: Empirical studies varying step counts for UDIG across multiple tasks, measuring attribution quality and computation time.

### Open Question 3
How does UDIG perform on multilingual and cross-lingual tasks compared to IG and DIG? Basis: The paper only evaluates UDIG on English datasets despite discussing applicability to any predictive language model. Unresolved because interpolation strategy and monotonicity constraints may behave differently with non-English vocabularies and linguistic structures. Evidence needed: Experiments applying UDIG to multilingual benchmarks and comparing attribution scores across languages.

### Open Question 4
Does UDIG's interpolation strategy introduce bias toward certain types of words or semantic relationships in the embedding space? Basis: The method searches for nearest neighbors and applies monotonicity constraints, which may systematically favor certain word relationships. Unresolved because the paper demonstrates superior performance but doesn't analyze whether attribution patterns reflect linguistic phenomena or artifacts. Evidence needed: Linguistic analysis of words receiving high attribution scores across different contexts.

## Limitations

- Methodological assumptions about embedding space linearity and nearest-word selection validity are not rigorously validated beyond empirical metric improvements
- Evaluation scope limited to BERT and DistilBERT models on sentiment classification and QA tasks, without testing on other architectures or domains
- Computational claims about fewer interpolation steps are not empirically validated with actual wall-clock time measurements

## Confidence

**UDIG attribution accuracy claims**: Medium confidence - Strong evidence of improved metrics but effect sizes vary and translation to interpretability not established

**Uniform discretization superiority claims**: Low confidence - Reasonable theoretical argument but lacks ablation studies to isolate uniform sampling effect

**Baseline selection claims**: High confidence - Well-supported by empirical evidence showing MASK yields near-zero model outputs

## Next Checks

1. Conduct ablation study comparing UDIG with uniform vs. non-uniform discretization while holding all other factors constant to isolate the specific contribution of uniform sampling

2. Implement UDIG for GPT-style models and encoder-decoder architectures to test whether MASK baseline remains optimal or if alternatives perform better

3. Perform statistical significance analysis using paired t-tests or bootstrap confidence intervals for performance improvements across all datasets and metrics