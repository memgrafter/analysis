---
ver: rpa2
title: 'Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large
  Language Models'
arxiv_id: '2406.12311'
source_url: https://arxiv.org/abs/2406.12311
tags:
- binarymos
- scaling
- binarization
- experts
- factors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mixture of Scales (BinaryMoS), a novel binarization
  technique for large language models that improves linguistic performance while maintaining
  compression efficiency. Unlike conventional binarization methods that use static
  scaling factors, BinaryMoS employs multiple scaling experts that are dynamically
  combined in a token-adaptive manner to generate contextual scaling factors for binary
  weights.
---

# Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models

## Quick Facts
- arXiv ID: 2406.12311
- Source URL: https://arxiv.org/abs/2406.12311
- Reference count: 39
- Primary result: BinaryMoS achieves 7.16 perplexity on WikiText2 for LLaMA-1-13B, outperforming OneBit (7.65) and matching GPTQ (7.15) while using 1-bit weights vs 2-bit

## Executive Summary
This paper introduces Mixture of Scales (BinaryMoS), a novel binarization technique for large language models that improves linguistic performance while maintaining compression efficiency. Unlike conventional binarization methods that use static scaling factors, BinaryMoS employs multiple scaling experts that are dynamically combined in a token-adaptive manner to generate contextual scaling factors for binary weights. This approach enhances representational capacity by allowing contextual adjustments to binary weight values while preserving the memory efficiency of binarization. Experiments show BinaryMoS consistently outperforms previous binarization techniques and even surpasses 2-bit quantization methods across various models and tasks.

## Method Summary
BinaryMoS replaces static scaling vectors with a mixture of scale experts that are dynamically combined per token. The router computes gating scores using input activations and a learned weight matrix, then linearly combines multiple scaling experts based on those scores to produce token-specific scaling factors. These factors adjust the binary weight values according to the context before matrix multiplication. The method uses knowledge distillation from full-precision teacher models during training, with four scaling experts per dimension (input and output) adding only about 0.2% parameter overhead while enabling generation of infinitely many token-adaptive scaling factors via linear combinations.

## Key Results
- BinaryMoS achieves 7.16 perplexity on WikiText2 and 7.16 on C4 for LLaMA-1-13B, compared to 7.65 and 9.56 for OneBit
- The method outperforms 2-bit quantization approaches (GPTQ, OmniQuant) in perplexity while maintaining 16× compression
- BinaryMoS shows particular effectiveness for smaller models like OPT-125M and OPT-1.3B, where it significantly reduces perplexity increases and improves zero-shot accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-adaptive scaling factors preserve contextual information that static binarization loses.
- Mechanism: BinaryMoS replaces static scaling vectors with a mixture of scale experts that are dynamically combined per token. The router computes gating scores using input activations and a learned weight matrix, then linearly combines multiple scaling experts based on those scores to produce token-specific scaling factors. These factors adjust the binary weight values according to the context before matrix multiplication.
- Core assumption: The context-dependent variation in gating scores across tokens reflects meaningful differences in optimal scaling factors.
- Evidence anchors:
  - [abstract] "dynamically merging these experts for each token to adaptively generate scaling factors"
  - [section] "Instead of selecting only a few experts, as done in the conventional MoE framework, BinaryMoS dynamically generates instructions on how to combine these scaling experts based on the context."
  - [corpus] Weak: No direct citation from neighbor papers; similarity is high but no corroborating mechanism.
- Break condition: If gating scores converge to near-uniform values across experts, token-adaptive scaling would collapse to static scaling, eliminating the benefit.

### Mechanism 2
- Claim: Linear combination of scaling experts increases representational capacity without significant memory overhead.
- Mechanism: BinaryMoS uses four scaling experts per dimension (input and output), each with dimension equal to the model's hidden size. The router weights and experts together add only about 0.2% of the original parameter count, while enabling generation of infinitely many token-adaptive scaling factors via linear combinations. This preserves the low-memory advantage of binarization while enhancing model expressiveness.
- Core assumption: Multiple experts can be linearly combined without introducing non-linear interactions that would require more parameters.
- Evidence anchors:
  - [section] "the number of these extra parameters constitutes only 0.2% of the original weight parameters"
  - [section] "the proposed BinaryMoS utilizes four scaling experts for each dimension to enhance accuracy while maintaining efficiency"
  - [corpus] Weak: Neighbor papers discuss binary quantization but not token-adaptive mixture approaches.
- Break condition: If the router fails to assign distinct scores to experts, or if combining more experts than 4 does not improve accuracy, the representational benefit disappears.

### Mechanism 3
- Claim: Mixture-of-scales approach outperforms 2-bit quantization while using only 1-bit weights.
- Mechanism: By using token-adaptive scaling, BinaryMoS can achieve perplexity and zero-shot accuracy comparable to or better than 2-bit quantization methods (GPTQ, OmniQuant) while maintaining the 16x compression of 1-bit binarization. The scaling experts capture information that would otherwise require higher bit precision in the weights.
- Core assumption: The information lost in weight binarization can be largely recovered through contextual scaling adjustments.
- Evidence anchors:
  - [abstract] "surpasses conventional binarization techniques in various natural language processing tasks and even outperforms 2-bit quantization methods"
  - [section] "BinaryMoS even outperforms these 2-bit quantization methods, despite its lower memory requirement during inference"
  - [corpus] Weak: No direct evidence from neighbor papers that mixture-of-scales specifically outperforms 2-bit quantization.
- Break condition: If the scaling experts fail to capture sufficient information to match 2-bit quantization performance, the approach loses its key advantage.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) routing
  - Why needed here: BinaryMoS applies the MoE idea not to layers but to scaling factors, using a router to dynamically combine scaling experts per token.
  - Quick check question: In MoE, what determines which experts are selected or how they are combined for a given input?

- Concept: Binarization error and scaling factors
  - Why needed here: Understanding how scaling factors minimize the L2 error between full-precision and binarized weights is critical to seeing why adaptive scaling helps.
  - Quick check question: In standard binarization, how is the scaling factor typically computed to minimize error?

- Concept: Knowledge distillation for quantization
  - Why needed here: BinaryMoS uses quantization-aware knowledge distillation to transfer knowledge from full-precision teacher models to binarized student models.
  - Quick check question: What are the two main loss components used in the knowledge distillation training of BinaryMoS?

## Architecture Onboarding

- Component map: Input activations -> Router (linear layer with softmax) -> Gating scores -> Combine scaling experts -> Token-adaptive Sin/Sout -> Binary matrix multiplication -> Output scaling -> Final output
- Critical path:
  1. Input activations → Router → Gating scores
  2. Gating scores → Combine scaling experts → Token-adaptive Sin/Sout
  3. Input scaling → Binary matrix multiplication → Output scaling → Final output
- Design tradeoffs:
  - Memory: 0.2% overhead vs. 16× compression
  - Accuracy: 4 experts optimal vs. more causing routing instability
  - Training: QAT approach requires 3 epochs on mixed dataset
- Failure signatures:
  - Uniform gating scores → No benefit over static scaling
  - High perplexity on C4 → Poor generalization from WikiText2-only training
  - Degraded latency → Inefficient kernel fusion for scaling experts
- First 3 experiments:
  1. Vary number of scaling experts (1, 2, 4, 8) on LLaMA-1-7B subset to find optimal balance
  2. Compare gating score distributions across tokens to verify token-adaptivity
  3. Measure latency overhead of BinaryMoS kernel vs. OneBit baseline on A100 GPU

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the token-adaptive scaling factors in BinaryMoS relate to the underlying semantic or syntactic properties of the tokens they are generated from?
- Basis in paper: [explicit] The paper shows that BinaryMoS generates diverse scaling factors for different tokens (Figure 3) and that these factors improve performance over static scaling.
- Why unresolved: While the paper demonstrates that the scaling factors vary across tokens and improve model accuracy, it does not investigate what linguistic properties (e.g., part-of-speech, syntactic role, semantic class) correlate with these scaling factor patterns.
- What evidence would resolve it: Analyzing correlations between token types and their corresponding scaling factors, or using interpretability techniques to understand how different linguistic properties are captured by the scaling experts.

### Open Question 2
- Question: Can the mixture-of-scales approach be effectively extended to multi-bit quantization schemes beyond binary weights?
- Basis in paper: [inferred] The paper mentions in the Discussion section that "this MoS approach holds promise for extension to multi-bit quantization" but explicitly states this was not explored in the current work.
- Why unresolved: The paper only demonstrates BinaryMoS on binary weights and suggests the approach could work for multi-bit quantization without empirical validation. The effectiveness of multiple scaling experts for higher precision quantization remains untested.
- What evidence would resolve it: Experiments applying BinaryMoS to 2-bit, 3-bit, or 4-bit quantization schemes and comparing performance against traditional multi-bit quantization methods.

### Open Question 3
- Question: What is the optimal architecture and training strategy for the router that generates scaling factor weights in BinaryMoS?
- Basis in paper: [explicit] The paper uses a simple linear layer followed by softmax for the router (Equation 3) and notes in the Discussion that "BinaryMoS does not fully leverage advanced training techniques established in the field of MoE."
- Why unresolved: The current router design is relatively simple and the paper acknowledges that more sophisticated MoE training techniques (like expert balancing) could potentially improve performance but were not investigated.
- What evidence would resolve it: Systematic comparison of different router architectures (deeper networks, attention-based routers) and training techniques (expert balancing, load balancing losses) on BinaryMoS performance.

### Open Question 4
- Question: How does BinaryMoS perform on generation tasks compared to full-precision models in terms of coherence, factual accuracy, and task completion?
- Basis in paper: [inferred] While the paper shows BinaryMoS outperforms other binarization methods on perplexity and zero-shot accuracy, it only provides one example of generation quality comparison in Appendix A.4.
- Why unresolved: The paper focuses primarily on perplexity and classification-style zero-shot tasks but doesn't comprehensively evaluate generation quality across diverse tasks like story completion, question answering, or instruction following.
- What evidence would resolve it: Comprehensive human and automated evaluation of generated text quality across multiple tasks, comparing BinaryMoS to both full-precision models and other quantization methods.

## Limitations
- Evaluation focuses primarily on perplexity and zero-shot reasoning tasks, leaving downstream fine-tuning performance and robustness to domain shifts unexplored
- Method's reliance on knowledge distillation assumes access to full-precision teacher models, which may not be feasible for all applications
- Paper does not address inference-time computational overhead beyond memory savings, particularly regarding the additional scaling operations and potential latency impacts on real-time applications

## Confidence

**High Confidence Claims:**
- BinaryMoS achieves lower perplexity than OneBit binarization on WikiText2 and C4 datasets
- The method outperforms 2-bit quantization approaches (GPTQ, OmniQuant) in perplexity while maintaining 16× compression
- BinaryMoS shows consistent improvements across different model sizes (125M to 13B parameters)
- The 0.2% parameter overhead from scaling experts is minimal compared to the representational benefits

**Medium Confidence Claims:**
- Token-adaptive scaling through mixture of experts is the primary driver of performance gains
- Four scaling experts represent the optimal balance between accuracy and stability
- The routing mechanism effectively captures contextual information for scaling factor generation

**Low Confidence Claims:**
- BinaryMoS will maintain its performance advantages on tasks beyond the evaluated benchmarks
- The method scales equally well to models significantly larger than 13B parameters
- No negative impact on inference latency despite additional scaling operations

## Next Checks

1. **Scaling Expert Dynamics Analysis**: Collect and analyze the distribution of gating scores across tokens to verify that the router produces meaningfully different scaling factor combinations for different contexts, and test how performance degrades when forcing uniform gating distributions.

2. **Cross-Domain Generalization**: Evaluate BinaryMoS on diverse NLP tasks beyond WikiText2/C4 and the specific reasoning benchmarks tested, including domain-specific datasets and tasks requiring fine-tuning, to assess generalization and robustness.

3. **Latency and Throughput Benchmarking**: Measure actual inference latency and throughput on various hardware platforms (CPU, GPU, edge devices) to quantify the real-world performance trade-offs of the additional scaling operations and expert routing.