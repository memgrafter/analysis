---
ver: rpa2
title: 'How ChatGPT Changed the Media''s Narratives on AI: A Semi-Automated Narrative
  Analysis Through Frame Semantics'
arxiv_id: '2408.06120'
source_url: https://arxiv.org/abs/2408.06120
tags:
- frame
- chatgpt
- more
- sentences
- frames
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study analyzes media narratives on AI before and after ChatGPT\u2019\
  s launch, using a dataset of 5,846 articles from major English-language publishers.\
  \ By applying frame semantics and LOME model annotation, it identifies a tenfold\
  \ increase in AI coverage post-launch."
---

# How ChatGPT Changed the Media's Narratives on AI: A Semi-Automated Narrative Analysis Through Frame Semantics

## Quick Facts
- arXiv ID: 2408.06120
- Source URL: https://arxiv.org/abs/2408.06120
- Reference count: 39
- Primary result: Tenfold increase in AI media coverage post-ChatGPT, with greater emphasis on risks and anthropomorphic portrayals

## Executive Summary
This study analyzes media narratives on AI before and after ChatGPT's November 2022 launch using a dataset of 5,846 articles from major English-language publishers. By applying frame semantics and LOME model annotation, researchers identified a tenfold increase in AI coverage post-launch, with significant shifts in framing. The analysis reveals heightened focus on expert and political leader commentary, greater emphasis on AI risks (danger-related frames increasing from 1.76% to 2.19%), and increased anthropomorphic portrayals, especially regarding ChatGPT's communication abilities. Manual thematic coding confirms a shift toward alarmist framing, with AI often portrayed as a source of danger.

## Method Summary
The study combines automated frame semantic annotation with manual qualitative coding to analyze narrative shifts. Researchers collected 5,846 news articles mentioning "AI" from 18 major English-language publishers over a 12-month period centered on ChatGPT's launch. Using the LOME model, they annotated 49,120 sentences with FrameNet semantic frames, then compared frame frequencies pre- and post-ChatGPT. Key frames analyzed included Statement, Leadership, Text, and danger-related frames. Manual coding was applied to subsets of 400 danger-related sentences and 390 anthropomorphism-related sentences to validate and contextualize the automated findings.

## Key Results
- Tenfold increase in AI media coverage following ChatGPT's launch
- Danger-related frames increased from 1.76% to 2.19% of AI-related sentences
- Greater emphasis on expert and political leader commentary in AI discourse
- Increased anthropomorphic portrayals of ChatGPT's communication abilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The combination of large-scale frame semantic annotation with targeted qualitative coding reveals shifts in media narratives that purely quantitative approaches would miss.
- Mechanism: FrameNet-based semantic role labeling automatically tags thousands of sentences, enabling quick filtering by semantic roles (e.g., "AI as Cognizer" vs. "AI as Speaker"). Manual annotation of these filtered subsets captures nuanced narrative changes, such as the shift from image-generation AI tropes to chatbot anthropomorphism.
- Core assumption: Semantic frames reliably map to narrative roles and can be automatically annotated at scale without significant noise.
- Evidence anchors:
  - [abstract] "by applying frame semantics and LOME model annotation, it identifies a tenfold increase in AI coverage post-launch."
  - [section 4.2] "We apply frame semantics, a descriptive framework that characterises lexical units in terms of semantic frames."
  - [corpus] Weak: No directly comparable papers found; corpus shows only narrative analysis but not the hybrid semantic + manual approach.
- Break condition: If the LOME model produces too many false positives, manual review would become prohibitively time-consuming, eroding the scalability advantage.

### Mechanism 2
- Claim: Filtering sentences with AI as a frame element (rather than all sentences mentioning AI) isolates the roles AI plays in discourse, enabling fine-grained tracking of thematic shifts.
- Mechanism: By matching AI to specific frame elements (e.g., "Cognizer," "Speaker"), the study distinguishes between AI as a tool, threat, or conversational partner, revealing the anthropomorphic shift tied to chatbots.
- Core assumption: Frame elements directly reflect the semantic function of the term in context, so frequency changes indicate real shifts in framing.
- Evidence anchors:
  - [section 4.3.2] "We target only sentences where AI is part of the frame elements that indicate potential anthropomorphism."
  - [section 4.3] "By filtering out the frames where the term AI is one of the frame elements, we can get a clearer idea of the roles it plays."
  - [corpus] Moderate: Papers on narrative classification exist, but none use frame elements to isolate entity roles at this scale.
- Break condition: If AI's role is consistently implied rather than explicit, the frame-element filter would miss key narrative uses, biasing the analysis.

### Mechanism 3
- Claim: The ten-fold increase in coverage post-ChatGPT is not just volume growth but also a qualitative shift toward more alarmist and expert-driven narratives.
- Mechanism: Statistical comparison of frame frequencies (e.g., increase in "Warning" frames, leadership mentions) combined with manual coding of danger-related sentences shows the shift from balanced to alarmist framing.
- Core assumption: Changes in the distribution of semantic frames correspond to real shifts in editorial stance and framing.
- Evidence anchors:
  - [abstract] "heightened focus on expert and political leader commentary... greater emphasis on AI risks, with danger-related frames increasing from 1.76% to 2.19%."
  - [section 4.1] "The total number of publications after the ChatGPT release is more than 6 times higher than in the six months before."
  - [corpus] Weak: No direct citation of prior work measuring frame shifts in media post-major AI event.
- Break condition: If the increased coverage is driven by fewer outlets with consistently alarmist tones, the broader qualitative shift might be overstated.

## Foundational Learning

- Concept: Frame semantics and FrameNet
  - Why needed here: The study uses semantic frames to map how AI is talked about, distinguishing between AI as a tool, threat, or agent. Understanding this theory is key to interpreting results.
  - Quick check question: What is the difference between a semantic frame and a frame element in FrameNet?

- Concept: Information extraction with LOME model
  - Why needed here: LOME automatically tags semantic frames in large text corpora, enabling scalable analysis. Knowing its capabilities and limits is crucial for evaluating the study's claims.
  - Quick check question: What type of model is LOME, and what is its primary task?

- Concept: Mixed-method qualitative + quantitative analysis
  - Why needed here: The study combines automatic frame annotation with manual thematic coding. Understanding this hybrid approach explains how nuanced shifts are detected at scale.
  - Quick check question: Why might purely quantitative sentiment analysis miss the type of narrative shift this study identifies?

## Architecture Onboarding

- Component map: Data collection layer -> NLP preprocessing -> Semantic annotation -> Statistical analysis -> Manual coding pipeline -> Integration layer

- Critical path: Sentence extraction → LOME annotation → Frame frequency analysis → Manual coding of key frames → Synthesis of results

- Design tradeoffs:
  - Automated annotation trades recall for speed; rare but important frames might be missed.
  - Manual coding is limited to subsets to maintain feasibility but risks missing broader patterns.
  - Using only anglophone open-access outlets biases coverage toward certain regions and publication types.

- Failure signatures:
  - Low agreement between manual annotators (kappa < 0.6) indicates ambiguous or subjective frame boundaries.
  - If frame frequency changes do not align with manual coding trends, annotation quality or sampling may be suspect.
  - Sudden jumps in frame counts could indicate LOME retraining or changes in corpus composition unrelated to narrative shifts.

- First 3 experiments:
  1. Run LOME on a held-out subset of articles and manually verify frame accuracy to estimate precision/recall.
  2. Replicate frame frequency comparison using a different NLP model (e.g., Stanza) to check robustness.
  3. Randomly sample sentences from the top-10 most frequent frames and code for sentiment to validate that frame changes correspond to narrative tone.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the anthropomorphism of AI in media narratives differ between text-based AI systems (like ChatGPT) and image-based AI systems (like DALL-E)?
- Basis in paper: [explicit] The paper discusses the different types of anthropomorphism observed in AI coverage, including task-based anthropomorphism related to communication abilities of chatbots and high anthropomorphism related to image generation AI.
- Why unresolved: The paper provides a qualitative analysis of anthropomorphism but does not directly compare the levels or types of anthropomorphism between text-based and image-based AI systems.
- What evidence would resolve it: A quantitative comparison of the frequency and types of anthropomorphic language used in media coverage of text-based vs. image-based AI systems would help determine if there are significant differences in how these technologies are portrayed.

### Open Question 2
- Question: Does the increased media coverage of AI risks and dangers post-ChatGPT correlate with actual changes in AI-related incidents or public concerns?
- Basis in paper: [explicit] The paper finds a significant increase in the relative frequency of danger-related frames and sentences where AI is portrayed as a source of risk post-ChatGPT.
- Why unresolved: The paper does not investigate whether this shift in media narratives is driven by actual changes in AI-related incidents or public concerns, or if it is primarily a result of increased media attention and sensationalism.
- What evidence would resolve it: Data on AI-related incidents, public surveys on AI concerns, and a comparison of the timing of these trends with the media coverage would help determine if the shift in narratives is driven by real-world events or media dynamics.

### Open Question 3
- Question: How do the sources of information (e.g., industry experts, academics, political leaders) cited in AI media coverage influence the framing of AI narratives?
- Basis in paper: [explicit] The paper finds an increase in the frequency of the "Leadership" frame post-ChatGPT, suggesting that leaders and experts play a bigger role in AI discourse.
- Why unresolved: The paper does not analyze the specific sources of information or how their perspectives might shape the framing of AI narratives, particularly regarding risks and benefits.
- What evidence would resolve it: A content analysis of the sources cited in AI media coverage, along with an examination of their affiliations and potential biases, would help determine how different sources influence the framing of AI narratives.

## Limitations
- The study relies entirely on LOME model outputs without validating precision or recall against human annotations
- Geographic and socioeconomic bias exists due to reliance on English-language, open-access articles from major publishers
- The modest absolute increase in danger-related frames (1.76% to 2.19%) raises questions about whether this represents a meaningful qualitative shift

## Confidence
- High confidence: The tenfold increase in AI coverage post-ChatGPT is well-supported by the raw publication counts
- Medium confidence: The shift toward more alarmist framing is supported by both quantitative frame frequency changes and manual thematic coding
- Low confidence: The specific claim about increased anthropomorphic portrayals of ChatGPT requires more validation due to limited manual coding sample size

## Next Checks
1. Manually annotate a random sample of 100 sentences to estimate LOME's precision and recall for key frames
2. Replicate frame frequency analysis using a different semantic role labeling model (such as Stanza)
3. Identify specific articles and publishers driving the increased danger-related framing to determine if the shift is distributed or concentrated