---
ver: rpa2
title: Human-in-the-Loop Feature Selection Using Interpretable Kolmogorov-Arnold Network-based
  Double Deep Q-Network
arxiv_id: '2411.03740'
source_url: https://arxiv.org/abs/2411.03740
tags:
- feature
- selection
- feedback
- mnist
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a human-in-the-loop (HITL) feature selection
  framework that integrates a Kolmogorov-Arnold Network (KAN) with a Double Deep Q-Network
  (DDQN) for dynamic, per-instance feature selection. The method uses simulated feedback
  and stochastic sampling (Beta distribution) to iteratively refine feature subsets,
  improving model interpretability and performance.
---

# Human-in-the-Loop Feature Selection Using Interpretable Kolmogorov-Arnold Network-based Double Deep Q-Network

## Quick Facts
- arXiv ID: 2411.03740
- Source URL: https://arxiv.org/abs/2411.03740
- Authors: Md Abrar Jahin; M. F. Mridha; Nilanjan Dey; Md. Jakir Hossen
- Reference count: 11
- Key outcome: KAN-DDQN achieves higher test accuracies (93% MNIST, 83% FashionMNIST) than MLP-DDQN while using 4× fewer neurons and improves accuracy by up to 35 percentage points through feature selection.

## Executive Summary
This paper proposes a human-in-the-loop (HITL) feature selection framework that integrates a Kolmogorov-Arnold Network (KAN) with a Double Deep Q-Network (DDQN) for dynamic, per-instance feature selection. The method uses simulated feedback and stochastic sampling (Beta distribution) to iteratively refine feature subsets, improving model interpretability and performance. Experiments on MNIST and FashionMNIST datasets show that KAN-DDQN achieves higher test accuracies (93% and 83%) than conventional MLP-DDQN, while using 4 times fewer neurons. Feature selection improves accuracy by up to 35 percentage points compared to models without it. The approach also scales well to CIFAR-10/CIFAR-100, with notable F1-score gains and reduced calibration error. Pruning and visualization enhance transparency, making the model suitable for real-time, adaptive decision-making with minimal human oversight.

## Method Summary
The proposed method combines KAN-based DDQN with HITL feedback for dynamic feature selection. The Feature Selection Network (FSNet) processes input images through convolutional layers, fully connected layers, and probabilistic sampling to generate feature masks. Simulated feedback is created using Gaussian heatmaps centered on the most salient pixel, which are aligned with the model's output probabilities via MSE loss. The KAN replaces traditional weight parameters with learnable B-splines, enabling parameter-efficient learning. The model is trained using a combination of Q-learning loss and feedback alignment loss, with pruning and visualization enhancing interpretability. The framework is evaluated on MNIST, FashionMNIST, and CIFAR datasets, demonstrating superior performance compared to MLP-DDQN baselines.

## Key Results
- KAN-DDQN achieves 93% test accuracy on MNIST and 83% on FashionMNIST, outperforming MLP-DDQN
- Model uses 4× fewer neurons than MLP-DDQN while maintaining higher accuracy
- Feature selection improves accuracy by up to 35 percentage points compared to models without it
- On CIFAR-10/CIFAR-100, KAN-DDQN shows F1-score improvements and reduced calibration error

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simulated feedback via Gaussian heatmaps and stochastic sampling iteratively refines feature subsets on a per-instance basis, improving model interpretability and performance.
- Mechanism: The model generates a Gaussian heatmap centered on the most salient pixel (maximum intensity) in each image, creating a spatial feedback signal. This signal is normalized and aligned with the feature selection network's output probabilities. Stochastic sampling (Bernoulli, Beta, etc.) then probabilistically selects features based on these relevance scores, allowing dynamic, instance-specific feature subsets.
- Core assumption: Simulated feedback approximates human feature relevance judgments and can guide the model toward more interpretable and accurate predictions.
- Evidence anchors:
  - [abstract]: "Our novel approach leverages simulated human feedback and stochastic distribution-based sampling, specifically Beta, to iteratively refine feature subsets per data instance..."
  - [section]: "To simulate supervisory guidance, we designed a feedback mechanism that highlights the most salient regions within each image via a Gaussian heatmap."
  - [corpus]: Weak evidence. Corpus neighbors focus on feature selection but do not address simulated feedback or HITL integration.
- Break condition: If simulated feedback does not correlate with actual human judgments, the model may select irrelevant features, degrading interpretability and performance.

### Mechanism 2
- Claim: KAN-based DDQN achieves higher accuracy with fewer parameters than MLP-based DDQN by using learnable spline functions instead of fixed activations.
- Mechanism: KAN replaces traditional weight parameters with learnable one-dimensional B-spline functions, allowing the model to approximate complex relationships more efficiently. This reduces the number of neurons needed in the hidden layer (e.g., 8 vs. 32) while maintaining or improving accuracy.
- Core assumption: B-splines can capture complex feature interactions more efficiently than fixed activation functions, enabling parameter-efficient learning.
- Evidence anchors:
  - [abstract]: "The KAN-based model provided high interpretability via symbolic representation while using 4 times fewer neurons in the hidden layer than MLPs did."
  - [section]: "KANs implement learnable activation functions along edges, contrasting with MLPs, which implement fixed activation functions at nodes."
  - [corpus]: Weak evidence. While corpus neighbors discuss feature selection and KANs, none directly compare KAN vs. MLP parameter efficiency in reinforcement learning.
- Break condition: If the dataset requires complex interactions that B-splines cannot efficiently model, the KAN may underperform or require more neurons to compensate.

### Mechanism 3
- Claim: Feature selection improves accuracy by up to 35 percentage points compared to models without it by reducing noise and focusing on relevant features.
- Mechanism: The feature selection network (FSNet) processes input through convolutional layers, fully connected layers, and probabilistic sampling to generate feature masks. These masks are applied to the feature vector, retaining only the most relevant features for classification. The model minimizes a combined loss of Q-learning error and feedback alignment, encouraging selective feature retention.
- Core assumption: Removing irrelevant or noisy features reduces overfitting and improves generalization, especially in high-dimensional spaces.
- Evidence anchors:
  - [abstract]: "Feature selection improves accuracy by up to 35 percentage points compared to models without it."
  - [section]: "This loss function penalizes discrepancies between the model’s probability vector ˆq and the feedback f. Minimizing Cf encourages FSNet to produce relevance scores that align with human intuition, resulting in a more interpretable feature selection."
  - [corpus]: Weak evidence. Corpus neighbors focus on feature selection methods but do not quantify accuracy improvements in the context of reinforcement learning.
- Break condition: If the feature selection network over-prunes or selects incorrect features, the model may lose critical information, reducing accuracy.

## Foundational Learning

- Concept: Kolmogorov-Arnold Networks (KANs) and their spline-based parameterization.
  - Why needed here: KANs provide a parameter-efficient alternative to MLPs, enabling the model to achieve high accuracy with fewer neurons while maintaining interpretability through symbolic representation.
  - Quick check question: How does the KAN's use of learnable B-splines differ from traditional MLP activation functions, and why is this advantageous for feature selection?

- Concept: Double Deep Q-Network (DDQN) and temporal-difference learning.
  - Why needed here: DDQN stabilizes Q-learning by using two networks (Q-network and target network) to reduce overestimation bias, enabling reliable feature selection through reinforcement learning.
  - Quick check question: How does the DDQN's use of a target network improve the stability of Q-learning compared to standard DQN?

- Concept: Reinforcement learning with human-in-the-loop (HITL) feedback.
  - Why needed here: HITL feedback allows the model to iteratively refine its feature selection policy based on simulated or real human guidance, improving interpretability and alignment with domain knowledge.
  - Quick check question: How does the integration of HITL feedback via MSE loss influence the feature selection policy in the DDQN framework?

## Architecture Onboarding

- Component map: Input image -> Convolutional feature extraction -> FSNet -> Feature mask -> Q-network (KAN/MLP) -> Action selection -> Reward computation -> Replay buffer update -> Q-network update

- Critical path:
  1. Input image → Convolutional feature extraction → Flattened feature vector
  2. FSNet processes features → Generates probability vector → Stochastic sampling → Feature mask
  3. Masked features → Q-network → Action selection → Reward computation → Replay buffer update
  4. Q-network update via TD target + feedback cost → Target network sync

- Design tradeoffs:
  - KAN vs. MLP: KAN offers parameter efficiency and interpretability but may require careful tuning of spline grids and widths. MLP is simpler but less efficient.
  - Distribution choice: Beta distribution yields best performance, but other distributions (e.g., uniform) may be viable depending on the dataset.
  - Feedback mechanism: Simulated feedback is scalable but may not fully capture human judgment; real feedback could improve alignment but is costly.

- Failure signatures:
  - Low accuracy with feature selection: Incorrect feature masks due to poor feedback alignment or distribution choice
  - High training time: Overly complex KAN configurations (e.g., large grid size or width)
  - Unstable learning: Insufficient replay buffer size or poor ϵ-greedy policy tuning

- First 3 experiments:
  1. Compare KAN-DDQN vs. MLP-DDQN on MNIST with feature selection enabled, using Beta distribution
  2. Test different stochastic distributions (Bernoulli, Gaussian, Uniform) on FashionMNIST to identify optimal feature selection performance
  3. Evaluate KAN-DDQN with and without feature selection on CIFAR-10 to assess scalability and accuracy gains

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas warrant further investigation based on the presented work, including scalability to high-dimensional datasets, optimization of feedback generation, and comparison with other feature selection methods.

## Limitations
- Simulated feedback mechanism's correlation with human judgment is unverified
- 35-percentage-point accuracy improvement lacks direct comparison to feature selection baselines
- KAN parameter efficiency claim relies on indirect evidence without ablation studies on grid size or spline width
- Confidence in scalability claims is medium as performance metrics are reported without architectural details for larger images

## Confidence
- Simulated feedback effectiveness: Low confidence - depends on unverified assumption that Gaussian heatmaps approximate human relevance judgments
- KAN parameter efficiency: Medium confidence - based on indirect evidence without ablation studies on grid size or spline order
- Scalability to CIFAR-10/CIFAR-100: Medium confidence - performance metrics reported without architectural details for larger images

## Next Checks
1. Validate simulated feedback by comparing feature selection performance using real human annotations vs. Gaussian heatmaps on a subset of MNIST/FashionMNIST
2. Conduct ablation studies on KAN architecture by varying grid size (2-5) and spline order (1-4) to quantify parameter efficiency trade-offs
3. Test KAN-DDQN on a high-dimensional dataset (e.g., CIFAR-100 at full resolution) to evaluate scalability and identify potential architectural bottlenecks