---
ver: rpa2
title: Evaluating Cultural Adaptability of a Large Language Model via Simulation of
  Synthetic Personas
arxiv_id: '2408.06929'
source_url: https://arxiv.org/abs/2408.06929
tags:
- language
- arxiv
- human
- preprint
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how well GPT-3.5 can simulate human cultural
  and national traits using synthetic personas. It uses the model to generate responses
  to a psychological experiment on persuasion, comparing results with real participants
  from 15 countries.
---

# Evaluating Cultural Adaptability of a Large Language Model via Simulation of Synthetic Personas

## Quick Facts
- arXiv ID: 2408.06929
- Source URL: https://arxiv.org/abs/2408.06929
- Reference count: 19
- Primary result: Explicitly stating a person's country of residence improves GPT-3.5's alignment with their responses

## Executive Summary
This study evaluates how well GPT-3.5 can simulate human cultural and national traits using synthetic personas. It uses the model to generate responses to a psychological experiment on persuasion, comparing results with real participants from 15 countries. The findings show that explicitly stating a person's country of residence improves GPT-3.5's alignment with their responses. However, prompting in a participant's native language reduces overall alignment, with Greek and Hebrew showing the largest negative effects. Overall, nationality information enhances cultural adaptability, while native language cues can impair the model's effectiveness. This suggests careful consideration is needed when incorporating language-based personalization in multicultural settings.

## Method Summary
The study creates 7,286 synthetic personas from 15 countries based on demographic and deprivation data from a psychological experiment. These personas are used to generate questionnaire responses via GPT-3.5 under different prompting conditions: varying nationality information (masked vs unmasked) and language settings (monolingual English vs native languages vs shuffled languages). The LLM responses are then compared to real human responses using regression analysis and sign agreement scoring across country-specific coefficients and framing/deprivation coefficients.

## Key Results
- Specifying a person's country of residence significantly improves GPT-3.5's alignment with their responses (+19% sign agreement for country-specific coefficients)
- Native language prompting introduces shifts that significantly reduce overall alignment compared to monolingual English prompting
- Greek and Hebrew showed the largest negative effects from native language prompting, while monolingual and shuffled-language approaches performed similarly well

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit nationality information improves LLM simulation fidelity
- Mechanism: GPT-3.5 leverages in-context information about country of residence to adjust response patterns, aligning more closely with culturally specific behavioral trends observed in human data
- Core assumption: The model's pretraining corpus contains sufficient cultural cues tied to country names to influence response generation
- Evidence anchors:
  - [abstract] "specifying a person's country of residence improves GPT-3.5's alignment with their responses"
  - [section 5.1] "Sign agreement rates are significantly greater than chance only in the unmasked condition"
  - [corpus] Weak: no direct citation of country-level cultural embedding mechanisms; only inferred from performance improvement
- Break condition: If cultural traits are not tied to explicit country names in pretraining data, or if nationality cues conflict with other contextual signals, alignment gains disappear

### Mechanism 2
- Claim: Native language prompting reduces simulation fidelity compared to monolingual English prompting
- Mechanism: Shifting prompting language alters the model's internal stylistic and semantic priors, introducing variance that misaligns with the target population's cultural response patterns
- Core assumption: The model's cross-lingual representations are not perfectly aligned with the cultural dimensions embedded in English responses
- Evidence anchors:
  - [abstract] "using native language prompting introduces shifts that significantly reduce overall alignment"
  - [section 5.3] "For country-specific coefficients, native language and country-shuffled prompting schemes perform much less well than mono-lingual prompting"
  - [corpus] Missing: no explicit theoretical model explaining why cross-lingual shifts degrade cultural alignment
- Break condition: If prompting language changes reinforce cultural stereotypes or if the model's multilingual embeddings are perfectly aligned across languages

### Mechanism 3
- Claim: Monolingual prompting across all participants yields stable country-specific bias alignment
- Mechanism: Uniform prompting language removes confounding stylistic shifts, allowing nationality cues to drive culturally consistent response patterns
- Core assumption: Consistency in language modality isolates nationality effects without introducing additional linguistic variance
- Evidence anchors:
  - [section 5.3] "approaches that were constant across nationalities, i.e. monolingual in experiment 2 and full-shuffled in experiment 3, performed similarly well"
  - [abstract] "while direct nationality information enhances the model's cultural adaptability, native language cues do not reliably improve simulation fidelity"
  - [corpus] Weak: no empirical comparison of stylistic variance introduced by multilingual prompts
- Break condition: If language uniformity masks important cultural nuances that are only accessible through native-language expression

## Foundational Learning

- Concept: Cultural framing in psychological experiments
  - Why needed here: The study relies on understanding how anti-elite vs anti-immigrant framing shifts persuasion and mobilization differently across cultures
  - Quick check question: What is the difference between persuasion and mobilization scores in this experimental design?

- Concept: Linear regression with interaction terms
  - Why needed here: Sign agreement analysis depends on extracting and comparing regression coefficients across human and LLM datasets
  - Quick check question: Why are interaction terms (E×I, D×E, etc.) included in the model?

- Concept: Cross-lingual model behavior
  - Why needed here: Explaining why Greek and Hebrew perform worse requires understanding how lexical distance and script differences affect model output
  - Quick check question: What role does alphabet script play in cross-lingual LLM performance?

## Architecture Onboarding

- Component map: Persona synthesis module -> Prompt generation engine -> LLM interface -> Evaluation pipeline
- Critical path:
  1. Synthesize personas from original dataset
  2. Generate prompts per experiment condition
  3. Collect LLM responses
  4. Run regression and compute sign agreement
  5. Compare results across conditions
- Design tradeoffs:
  - Using English-only prompts simplifies alignment but may miss language-specific cultural cues
  - Adding nationality info increases alignment but risks reinforcing stereotypes
  - Prompt translation must preserve semantic meaning without introducing bias
- Failure signatures:
  - Low sign agreement across all coefficients indicates fundamental misalignment
  - High variability in Greek/Hebrew results suggests language-specific instability
  - Shuffled-language results similar to monolingual indicates language itself is not the driver
- First 3 experiments:
  1. Masked vs unmasked nationality in English prompts
  2. Monolingual prompting across 12 languages
  3. Native-language vs shuffled-language prompting per participant

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would GPT-4 and other state-of-the-art LLMs perform in this cultural adaptability experiment compared to GPT-3.5?
- Basis in paper: [explicit] The paper notes that GPT-3.5 was used for the study, but suggests future work could extend to a broader range of models
- Why unresolved: This study only evaluated GPT-3.5. Performance may vary significantly with newer models that have different training data and capabilities
- What evidence would resolve it: Repeating the experiments with GPT-4, Claude, Gemini, etc., and comparing their cultural alignment scores across the same prompts and tasks

### Open Question 2
- Question: To what extent do these findings generalize to non-Western cultures and languages?
- Basis in paper: [explicit] The authors acknowledge that their study predominantly focused on European nationals and recommends future research expand to a broader range of cultures
- Why unresolved: The experiment was limited to 15 countries, mostly European, and cannot speak to the model's performance with truly diverse cultural contexts
- What evidence would resolve it: Conducting the same experiments with participants from Asia, Africa, Latin America, and indigenous cultures, and analyzing if the observed patterns (e.g., native language prompting reducing alignment) still hold

### Open Question 3
- Question: What is the mechanism by which native language prompting reduces cultural alignment in some languages?
- Basis in paper: [inferred] The paper observes this effect but does not deeply explore why Greek and Hebrew, in particular, showed the largest negative impacts
- Why unresolved: The study measured the effect but didn't investigate underlying causes—whether it's related to training data distribution, script differences, or other linguistic features
- What evidence would resolve it: Linguistic and training data analysis to correlate model performance with factors like script type, corpus size, or cultural representation in pretraining data

## Limitations
- The study lacks theoretical grounding for why cross-lingual prompting reduces alignment, particularly for Greek and Hebrew
- Synthetic personas may not fully capture the diversity of cultural responses in real populations
- The focus on GPT-3.5 limits generalizability to other LLMs with different training corpora or architectures

## Confidence

- High confidence: Explicit nationality information improves alignment in English prompts (supported by significant statistical differences)
- Medium confidence: Monolingual prompting yields stable country-specific bias alignment (consistent results across experiments but limited theoretical explanation)
- Low confidence: Native language prompting consistently reduces fidelity (empirical observation without clear mechanism)

## Next Checks

1. Test whether the nationality alignment effect persists when using country names in non-English languages, or if the effect is specific to English-language cultural embeddings
2. Conduct ablation studies varying the specificity of nationality cues (e.g., country name vs region vs cultural descriptor) to isolate what aspects of nationality information drive alignment improvements
3. Evaluate model performance on a held-out dataset of real participants (not synthetic personas) to verify that simulation fidelity translates to actual human response prediction