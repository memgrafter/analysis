---
ver: rpa2
title: 'DiversityMedQA: Assessing Demographic Biases in Medical Diagnosis using Large
  Language Models'
arxiv_id: '2409.01497'
source_url: https://arxiv.org/abs/2409.01497
tags:
- original
- gender
- ethnicity
- medical
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces DiversityMedQA, a novel benchmark to evaluate
  demographic biases in large language models (LLMs) used for medical diagnosis. By
  perturbing questions from the MedQA dataset to include gender and ethnicity variations,
  the authors assess how model performance changes across different patient demographics.
---

# DiversityMedQA: Assessing Demographic Biases in Medical Diagnosis using Large Language Models

## Quick Facts
- arXiv ID: 2409.01497
- Source URL: https://arxiv.org/abs/2409.01497
- Reference count: 34
- Primary result: Novel benchmark reveals demographic bias in medical LLMs, with newer GPT models showing better mitigation than Llama3-8B

## Executive Summary
This study introduces DiversityMedQA, a novel benchmark to evaluate demographic biases in large language models (LLMs) used for medical diagnosis. By perturbing questions from the MedQA dataset to include gender and ethnicity variations, the authors assess how model performance changes across different patient demographics. Their findings reveal notable discrepancies in model accuracy when tested against these demographic variations, particularly highlighting significant bias in Llama3-8B. In contrast, newer GPT models (GPT-4.0 Turbo and GPT-4o) demonstrated superior performance and better alignment with mitigating surface-level biases. The study provides a valuable resource for systematically evaluating and mitigating demographic bias in LLM-based medical diagnoses.

## Method Summary
The authors created DiversityMedQA by perturbing MedQA questions with demographic variations using GPT-4 to generate realistic clinical scenarios. They filtered out clinically dependent questions using GPT-4 ratings, ensuring that correct answers remained invariant to demographic changes. Each model was prompted five times per question using few-shot chain-of-thought prompting, with answers extracted via regex. Accuracy was calculated using both single-answer and majority vote (Maj@5) metrics, with Z-tests used to assess bias significance between original and perturbed questions.

## Key Results
- Llama3-8B showed significant bias across gender and ethnicity perturbations, with accuracy drops of 6-7% compared to GPT-4 models
- GPT-4.0 Turbo and GPT-4o demonstrated better mitigation of surface-level biases with smaller performance gaps across demographics
- Filtering process removed approximately 40-50% of questions that were clinically dependent on gender/ethnicity before analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perturbing medical questions with demographic variations exposes model bias by creating controlled counterfactual scenarios.
- Mechanism: By systematically replacing gender or ethnicity descriptors in clinical vignettes and observing changes in model accuracy, bias is revealed through performance discrepancies between original and perturbed inputs.
- Core assumption: The clinical content and correct answers remain unchanged when demographic descriptors are swapped, so any accuracy change reflects model bias rather than domain shift.
- Evidence anchors:
  - [abstract] "By perturbing questions from the MedQA dataset, which comprises medical board exam questions, we created a benchmark that captures the nuanced differences in medical diagnosis across varying patient profiles."
  - [section 3.1] "To simulate realistic clinical reasoning, we used the few-shot chain-of-thought (COT) prompting technique with GPT-4 to simulate a realistic clinical reasoning process. [4, 18] Specifically, we prompted GPT-4 to assess the impact of gender and ethnicity changes on clinical outcomes."
  - [corpus] Weak. No direct corpus evidence that perturbation alone reliably isolates bias; the assumption relies on careful filtering described in section 3.1.
- Break condition: If demographic descriptors influence clinical facts or answer correctness, then accuracy differences would conflate true domain variation with bias.

### Mechanism 2
- Claim: Filtering out clinically dependent questions ensures that differences in model accuracy arise solely from demographic bias, not from changes in medical content.
- Mechanism: GPT-4 is used to rate whether gender or ethnicity changes affect the clinical outcome; questions rated as dependent are excluded, leaving only those where the correct answer is invariant to demographic change.
- Core assumption: GPT-4 can reliably distinguish clinically dependent from independent questions, and this filtering is accurate enough to isolate bias effects.
- Evidence anchors:
  - [section 3.1] "In order to focus our analysis on scenarios where demographic changes could potentially bias clinical outcomes, we filtered out questions rated ‘1’ for both gender and ethnicity modifications. By excluding questions clinically dependent on gender/ethnicity, we ensured that correct answers remained the same when the questions were perturbed, yielding differences in answers caused only by model bias."
  - [section 3.1] Table 1 shows the number of questions kept versus filtered for each demographic type.
  - [corpus] No direct corpus evidence; relies on internal validation via GPT-4 filtering logic.
- Break condition: If GPT-4 misclassifies clinically dependent questions as independent, then bias estimates will be confounded by genuine clinical variation.

### Mechanism 3
- Claim: Using multiple model runs and majority voting (Maj@5) stabilizes accuracy estimates and reduces noise from stochastic generation.
- Mechanism: Each question is presented to each model five times, and answers are extracted via regex; majority vote is used to determine final prediction accuracy, improving robustness over single-shot accuracy.
- Core assumption: Model predictions are sufficiently variable across runs that majority voting captures a more reliable signal than any single prediction.
- Evidence anchors:
  - [section 3.2] "we ran these through GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o, Llama3-8B, and Gemini-1.5 flash models. We prompted all the models in a way to have its final output line contain "The answer is __." where __ is the model’s answer choice. We ran each model on each question 5 times, and stored the 5 completions."
  - [section 3.3] "we compared the resulting accuracies between models, examining the first index accuracies, which tests accuracies for a single prediction and max vote (Maj@5) accuracies, which checks for the majority vote answer after 5 predictions."
  - [corpus] No direct corpus evidence that majority voting improves accuracy in this specific setup; the approach is standard in medical QA literature but not proven here.
- Break condition: If model outputs are highly consistent across runs, majority voting provides no benefit and adds computational cost.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: Simulates clinical reasoning to elicit model explanations and improve answer consistency in medical QA tasks.
  - Quick check question: How does few-shot COT prompting differ from direct answer prompting in eliciting model reasoning?
- Concept: Demographic bias measurement in AI
  - Why needed here: Provides the conceptual framework for detecting disparities in model performance across protected demographic groups.
  - Quick check question: Why is it important to distinguish between bias and legitimate clinical variation when measuring demographic disparities?
- Concept: Z-test for statistical significance
  - Why needed here: Determines whether observed differences in model accuracy between original and perturbed questions are statistically significant.
  - Quick check question: What is the null hypothesis when comparing original vs. perturbed question accuracies?

## Architecture Onboarding

- Component map: MedQA dataset -> Perturbation generation (GPT-4) -> Filtering (GPT-4) -> Benchmark dataset -> Model inference (5× per question) -> Answer extraction via regex -> Accuracy calculation -> Z-test analysis
- Critical path:
  1. Perturb questions with demographic changes.
  2. Filter out clinically dependent questions.
  3. Run each model 5 times per question.
  4. Extract answers via regex.
  5. Compute accuracy metrics and perform Z-tests.
- Design tradeoffs:
  - Perturbation vs. clinical validity: More aggressive perturbations may better reveal bias but risk altering clinical facts.
  - Filtering rigor vs. dataset size: Stricter filtering ensures validity but reduces the number of evaluable questions.
  - Single-answer vs. Maj@5 accuracy: Single-answer is faster but noisier; Maj@5 is more stable but computationally heavier.
- Failure signatures:
  - No significant Z-scores despite qualitative bias in generations → Filtering may be too strict or bias too subtle.
  - Low overall accuracy → Possible domain mismatch or model inadequacy for medical QA.
  - Regex extraction failures → Inconsistent model output formatting.
- First 3 experiments:
  1. Run a small subset of questions without filtering to see if accuracy differences appear before bias control.
  2. Compare single-answer vs. Maj@5 accuracy on the same data to quantify noise reduction.
  3. Perturb a question known to be clinically dependent and confirm it is filtered out by the GPT-4 rating step.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of medical LLMs change when evaluated on larger, more diverse datasets beyond the 1040 gender and 1068 ethnicity questions used in this study?
- Basis in paper: [explicit] The authors note that due to the extensiveness of the MedQA dataset, they were only able to fully prompt 1040 questions for gender perturbation and 1068 for ethnicity perturbation.
- Why unresolved: The current study's limited sample size may not capture the full extent of demographic biases present in medical LLMs, and larger datasets could reveal additional patterns or discrepancies.
- What evidence would resolve it: Testing the LLMs on a significantly larger and more diverse set of medical questions, ideally with expert validation of each perturbation's clinical accuracy.

### Open Question 2
- Question: What specific linguistic or contextual features in medical questions trigger demographic biases in LLMs, particularly in models like Llama3-8B?
- Basis in paper: [inferred] The qualitative analysis revealed that certain words related to emotions, actions of distress, and stereotypical body parts triggered differences in how models answered questions across demographics.
- Why unresolved: While the paper identifies that these triggers exist, it does not systematically analyze which specific linguistic features or contextual cues are most problematic.
- What evidence would resolve it: A detailed linguistic analysis of question features that correlate with biased responses, followed by controlled experiments testing the impact of specific modifications.

### Open Question 3
- Question: How effective are current bias mitigation strategies when applied to medical LLMs, and what novel approaches might be needed to address demographic biases in healthcare applications?
- Basis in paper: [explicit] The authors note that while GPT-4.0 Turbo and GPT-4o showed better alignment with mitigating surface-level biases, substantial bias was still detected in Llama3-8B.
- Why unresolved: The paper does not explore or evaluate specific bias mitigation techniques, leaving open the question of how to effectively address the identified biases.
- What evidence would resolve it: Systematic testing of various bias mitigation strategies (e.g., data augmentation, adversarial training, post-processing) on the models, measuring their impact on demographic bias while maintaining diagnostic accuracy.

## Limitations

- Core assumption validity: The study's central claim rests on the assumption that demographic descriptor changes do not alter the correct clinical answer, which relies on GPT-4 filtering without external validation.
- Model representation: Evaluation includes only five models, which may not represent the full landscape of medical LLMs or capture how model size or training data composition influences bias patterns.
- Statistical robustness: The study doesn't address multiple comparisons correction given the numerous pairwise tests across models and demographic dimensions, potentially inflating false positive rates.

## Confidence

**High confidence:** The methodology for creating perturbed questions and the basic framework for bias assessment are sound and well-documented. The finding that newer GPT models show less bias than Llama3-8B is supported by the data.

**Medium confidence:** The claim that filtering ensures clinical invariance is plausible but not externally validated. The magnitude of bias differences between models is reasonably well-established, though the exact values depend on the filtering quality.

**Low confidence:** Claims about the relative performance of specific models in bias mitigation would benefit from larger-scale validation across diverse medical domains and patient scenarios.

## Next Checks

1. **External validation of filtering accuracy:** Test a subset of filtered questions with human clinical experts to verify that demographic changes genuinely don't affect correct answers. This would validate the core assumption underlying the entire benchmark.

2. **Cross-dataset generalization:** Apply the same perturbation and filtering methodology to a different medical QA dataset (e.g., USMLE-like questions or clinical vignettes from medical literature) to assess whether bias patterns hold across different question styles and medical domains.

3. **Bias-variance tradeoff analysis:** Systematically vary the perturbation intensity (e.g., adding more demographic context vs. minimal changes) and measure how this affects both bias detection and clinical validity, helping to optimize the perturbation-filtering balance.