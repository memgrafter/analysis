---
ver: rpa2
title: Audio-Guided Fusion Techniques for Multimodal Emotion Analysis
arxiv_id: '2409.05007'
source_url: https://arxiv.org/abs/2409.05007
tags:
- feature
- emotion
- fusion
- multimodal
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses semi-supervised multimodal emotion recognition
  (MER) in the MER-SEMI 2024 track, focusing on improving feature extraction and fusion
  from audio, video, and text modalities with limited labeled data. The authors propose
  an Audio-Guided Transformer (AGT) fusion mechanism that uses audio features from
  Hubert-large to guide the integration of visual and textual features, enhancing
  complementary information capture.
---

# Audio-Guided Fusion Techniques for Multimodal Emotion Analysis

## Quick Facts
- arXiv ID: 2409.05007
- Source URL: https://arxiv.org/abs/2409.05007
- Reference count: 40
- Primary result: Third place in MER-SEMI 2024 with 89.83% F1-score

## Executive Summary
This paper tackles semi-supervised multimodal emotion recognition in the MER-SEMI 2024 track by addressing the challenge of limited labeled data and significant label distribution imbalance. The authors propose a comprehensive framework that combines fine-tuned feature extractors, an Audio-Guided Transformer fusion mechanism, and pseudo-labeling with a prior-knowledge-based voting system. Their approach effectively leverages both labeled and unlabeled data to achieve state-of-the-art performance, securing third place in the competition with an 89.83% F1-score.

## Method Summary
The method consists of three main stages: First, the authors fine-tune CLIP-vit-large and Baichuan-13B on labeled data to improve modality-specific emotion extraction and reduce background noise interference. Second, they implement an Audio-Guided Transformer (AGT) fusion mechanism that uses Hubert-large audio features to guide the integration of visual and textual features through Context-Based Transformer modules and Adaptive Multimodal Fusion. Third, they apply iterative self-supervised learning by generating high-confidence pseudo-labels (p > 0.9) from multiple models and combining them using a prior-knowledge-based voting mechanism that addresses the label distribution imbalance between training and test sets.

## Key Results
- Achieved 89.83% F1-score on MER-SEMI 2024 dataset
- Secured third place in the competition
- Successfully handled significant label distribution imbalance (worried, sad classes underrepresented in training)

## Why This Works (Mechanism)

### Mechanism 1: Fine-tuning for Noise Reduction
Fine-tuning CLIP-vit-large and Baichuan-13B on labeled data aligns visual and textual features with emotional labels, filtering out non-emotional information learned during pretraining. This preserves original emotional content while reducing background interference. Break condition: mislabeled training data could propagate errors during fine-tuning.

### Mechanism 2: Audio-Guided Transformer Fusion
AGT uses Hubert-large audio features to guide visual and textual fusion through Context-Based Transformer modules, then applies Adaptive Multimodal Fusion to compute similarity and suppress cross-modal interference. This leverages audio's reliability for emotion cues to improve fusion robustness. Break condition: noisy or corrupted audio samples could introduce errors into the guided fusion process.

### Mechanism 3: Pseudo-Labeling with Voting
High-confidence pseudo-labels (p > 0.9) from multiple models are generated and combined using a voting mechanism that weights Hubert-large predictions more heavily for imbalanced labels (worried, sad) while using majority voting for others. This leverages unlabeled data while correcting for label distribution mismatch. Break condition: overly conservative threshold limits training data; overly lenient threshold introduces noise.

## Foundational Learning

- **Semi-supervised learning**: Needed because MER-SEMI provides limited labeled data and large unlabeled dataset. Quick check: What's the main difference between supervised and semi-supervised learning in terms of data usage?
- **Multimodal feature fusion**: Required to integrate complementary information from audio, video, and text modalities while balancing contributions and reducing interference. Quick check: What's the difference between early fusion and late fusion in multimodal systems?
- **Imbalanced data handling**: Critical due to significant label distribution differences between training and test sets, particularly for 'sad' and 'worried' classes. Quick check: Why is class imbalance problematic for model generalization, and how can weighted voting help?

## Architecture Onboarding

- **Component map**: Fine-tuned CLIP-vit-large → Fine-tuned Baichuan-13B → Hubert-large → Context-Based Transformer (CBT) → Adaptive Multimodal Fusion (AMF) → Voting mechanism
- **Critical path**: Labeled data → fine-tune feature extractors → AGT fusion → supervised training → pseudo-label generation → self-supervised training → voting-based inference
- **Design tradeoffs**: Audio-guided fusion vs. equal modality fusion (robustness vs. error propagation risk); high-confidence pseudo-label threshold (noise reduction vs. data limitation); weighted voting (imbalance correction vs. complexity)
- **Failure signatures**: Low pseudo-label confidence (poor generalization); voting mechanism degrades performance (incorrect modality reliability assumptions); fine-tuned extractors underperform (overfitting or misalignment)
- **First 3 experiments**: 1) Ablation study: AGT vs. baseline fusion with same fine-tuned extractors. 2) Pseudo-label impact: with vs. without pseudo-labels. 3) Voting mechanism test: weighted vs. simple majority voting.

## Open Questions the Paper Calls Out

### Open Question 1
How does AGT fusion performance scale with different audio feature extractors beyond Hubert-large? The paper doesn't explore alternatives like Wav2Vec2 or Speech2Vec, which would require comparative experiments with these extractors.

### Open Question 2
What's the impact of varying the confidence threshold (currently 0.9) for pseudo-label selection? The optimal threshold isn't investigated, requiring experiments with different thresholds (e.g., 0.8, 0.95) to optimize the noise-reduction vs. data-utilization tradeoff.

### Open Question 3
How does the voting mechanism perform on balanced datasets? Since it's designed specifically for imbalanced distributions, its effectiveness on balanced data remains untested and would require performance comparisons across both scenarios.

## Limitations

- The effectiveness of audio-guided fusion assumes audio is consistently the most reliable modality, which may not hold for all samples
- High confidence threshold (p > 0.9) for pseudo-labeling may severely limit the amount of unlabeled data incorporated
- The voting mechanism relies on prior knowledge about label distributions that may not generalize to other datasets

## Confidence

- **High Confidence**: General framework of fine-tuning multimodal feature extractors and using fusion mechanisms is well-established
- **Medium Confidence**: Specific AGT architecture and superiority over baseline fusion methods require further validation
- **Medium Confidence**: Effectiveness of pseudo-labeling with high confidence thresholds and voting mechanism needs empirical verification

## Next Checks

1. **Ablation Study**: Compare complete system against variants without AGT fusion, without pseudo-labeling, and without voting mechanism to quantify individual component contributions.

2. **Pseudo-Label Quality Analysis**: Evaluate pseudo-label confidence distribution and impact of varying threshold (0.8 vs. 0.9) on model performance and training data volume.

3. **Modality Reliability Testing**: Analyze voting mechanism performance when audio is intentionally corrupted or removed to validate audio's assumed reliability for emotion recognition.