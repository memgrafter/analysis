---
ver: rpa2
title: Similarity-Dissimilarity Loss for Multi-label Supervised Contrastive Learning
arxiv_id: '2410.13439'
source_url: https://arxiv.org/abs/2410.13439
tags:
- loss
- multi-label
- contrastive
- positive
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel Similarity-Dissimilarity Loss for multi-label
  supervised contrastive learning. It addresses the challenge of identifying positive
  samples in multi-label scenarios by systematically formulating multi-label relations
  and introducing a re-weighting mechanism that dynamically adjusts sample contributions
  based on similarity and dissimilarity factors.
---

# Similarity-Dissimilarity Loss for Multi-label Supervised Contrastive Learning

## Quick Facts
- arXiv ID: 2410.13439
- Source URL: https://arxiv.org/abs/2410.13439
- Reference count: 40
- Primary result: Achieves state-of-the-art performance on MIMIC-III-Full and consistently improves multi-label classification across image, text, and medical domains

## Executive Summary
This paper introduces the Similarity-Dissimilarity Loss for multi-label supervised contrastive learning, addressing the challenge of identifying positive samples in multi-label scenarios. The proposed method systematically formulates multi-label relations using set-theoretic operations and introduces a re-weighting mechanism that dynamically adjusts sample contributions based on similarity and dissimilarity factors. The loss generalizes existing supervised contrastive loss functions and unifies single-label and multi-label settings. Extensive experiments across image, text, and medical datasets demonstrate consistent improvements over strong baselines, with the method achieving state-of-the-art performance on MIMIC-III-Full. The approach is particularly effective in handling long-tailed distributions and complex multi-label interactions.

## Method Summary
The Similarity-Dissimilarity Loss extends supervised contrastive learning to multi-label scenarios by introducing a re-weighting mechanism based on label set cardinality. For each anchor-positive pair, the method computes similarity (label overlap) and dissimilarity (label difference) factors using set-theoretic relations. These factors are multiplied to create a weighting coefficient that adjusts the contribution of each positive pair to the contrastive loss. The approach generalizes to single-label cases and provides a unified framework for both settings. The method is evaluated across three modalities - images (MS-COCO, PASCAL, NUS-WIDE), text (AAPD, MIMIC-III, MIMIC-IV), and medical coding - demonstrating consistent performance improvements.

## Key Results
- Achieves state-of-the-art performance on MIMIC-III-Full with significant improvements in macro-F1 and AUC metrics
- Consistently outperforms strong baselines across image (MS-COCO), text (AAPD), and medical (MIMIC-III, MIMIC-IV) datasets
- Demonstrates particular effectiveness in handling long-tailed distributions, with substantial gains on datasets with imbalanced label frequencies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The similarity-dissimilarity weighting factor adjusts positive sample contributions based on multi-label relations.
- Mechanism: The product $K_s^{i,p} \cdot K_d^{i,p}$ acts as a dynamic re-weighting coefficient for each positive pair, with values bounded in [0,1]. When $K_s^{i,p}$ is high (many shared labels) and $K_d^{i,p}$ is close to 1 (few extra labels), the pair contributes more to the loss.
- Core assumption: Label overlap and label difference can be quantified by set cardinality and meaningfully map to semantic alignment.
- Evidence anchors:
  - [abstract] "dynamically re-weights samples based on similarity and dissimilarity factors"
  - [section] "we introduce the concepts of similarity and dissimilarity based on set-theoretic relations"
- Break condition: If the cardinality-based weighting fails to reflect semantic relevance (e.g., in cases of label noise or hierarchical dependencies), the effectiveness of re-weighting diminishes.

### Mechanism 2
- Claim: The proposed loss generalizes supervised contrastive loss to both single-label and multi-label settings.
- Mechanism: When $| \tilde{y}_i | = | \tilde{y}_s^p |$ and $| \tilde{y}_d^p | = 0$, the similarity-dissimilarity factor equals 1, reducing the loss to the standard supervised contrastive form.
- Core assumption: Single-label cases can be treated as a special case of multi-label with singleton label sets.
- Evidence anchors:
  - [abstract] "offers a unified form and paradigm for both single-label and multi-label supervised contrastive loss"
  - [section] "the Similarity-Dissimilarity Loss reduces to Equation 3, when the following conditions are simultaneously satisfied"
- Break condition: If label sets are not cleanly representable as sets (e.g., ordinal labels), the reduction may not hold.

### Mechanism 3
- Claim: The re-weighting mechanism improves performance on long-tailed multi-label distributions.
- Mechanism: By assigning higher weights to positives with more label overlap and fewer extraneous labels, the method mitigates the dominance of frequent labels and preserves minority class signals.
- Core assumption: Long-tailed distributions benefit from emphasizing high-quality positive pairs over sheer quantity.
- Evidence anchors:
  - [abstract] "particularly effective in handling long-tailed distributions"
  - [section] "Macro-F1 assigns equal importance to each class regardless of its frequency"
- Break condition: If the dataset lacks sufficient diversity in multi-label relations or the tail labels have minimal semantic overlap, the weighting may not yield gains.

## Foundational Learning

- Concept: Multi-label relations (R1-R5) defined by set operations (intersection, subset, equality).
  - Why needed here: To formalize how positive samples are selected and weighted in multi-label contrastive learning.
  - Quick check question: What is the cardinality of the intersection $S \cap T$ when $S \subseteq T$?

- Concept: Contrastive loss formulation with temperature scaling and positive/negative sample separation.
  - Why needed here: The similarity-dissimilarity loss builds on the standard supervised contrastive loss structure.
  - Quick check question: In the InfoNCE loss, what role does the temperature parameter $\tau$ play?

- Concept: Set cardinality as a proxy for semantic similarity/dissimilarity.
  - Why needed here: The weighting factors are computed directly from label set cardinalities.
  - Quick check question: If two label vectors have 3 overlapping labels out of 5 total in the anchor, what is the similarity factor $K_s$?

## Architecture Onboarding

- Component map: Encoder -> Projection head -> Similarity-dissimilarity re-weighting module -> Loss aggregator

- Critical path:
  1. Forward pass through encoder → projection head → get embeddings
  2. Compute pairwise similarities between anchor and all samples
  3. For each positive pair, calculate $K_s$ and $K_d$ based on label sets
  4. Apply re-weighting in numerator of contrastive loss
  5. Backpropagate through projection head and encoder

- Design tradeoffs:
  - Using cardinality vs. learned weighting functions: simpler, faster, but less expressive
  - Applying re-weighting only to positives vs. both positives and negatives: maintains compatibility with standard contrastive objectives
  - Temperature $\tau$ fixed vs. adaptive: stability vs. potential for finer control

- Failure signatures:
  - Loss plateaus early: likely issue with positive set selection or weighting
  - Model overfits to frequent labels: insufficient penalty on dissimilar positives
  - Slow convergence: overly aggressive weighting or temperature setting

- First 3 experiments:
  1. Verify the re-weighting factor behaves as expected on synthetic multi-label data with known relations
  2. Compare macro-F1 improvement on a long-tailed multi-label dataset (e.g., MIMIC-III-Full)
  3. Test sensitivity to temperature $\tau$ and projection head architecture on image classification (MS-COCO)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Similarity-Dissimilarity Loss maintain its advantages in extremely sparse multi-label datasets where the average label cardinality is close to one?
- Basis in paper: [inferred] The paper shows limited gains on PASCAL, where the average label cardinality is approximately 1.5, and suggests the method degenerates to single-label scenarios under such conditions.
- Why unresolved: The paper only provides evidence from PASCAL; broader empirical validation across multiple sparse datasets is lacking.
- What evidence would resolve it: Systematic experiments on multiple multi-label datasets with varying label cardinalities, especially those with average cardinality near one, to compare performance against existing methods.

### Open Question 2
- Question: Can the re-weighting mechanism be extended to capture hierarchical or intra-label relationships, or is it strictly limited to inter-label relations?
- Basis in paper: [explicit] The paper explicitly states it focuses exclusively on inter-label relations and does not address hierarchical or intra-label relations.
- Why unresolved: The theoretical framework and experiments do not explore hierarchical or nested label structures, which are common in domains like medical coding.
- What evidence would resolve it: Experiments incorporating hierarchical label taxonomies or datasets with explicit label hierarchies to assess performance gains from extending the re-weighting to such structures.

### Open Question 3
- Question: How does the Similarity-Dissimilarity Loss scale with extremely large label spaces, such as in full ICD-10 coding, where label sparsity and computational demands are severe?
- Basis in paper: [inferred] The paper notes challenges in full ICD-10 settings but does not analyze scalability limits of the re-weighting mechanism itself.
- Why unresolved: While the paper tests large label spaces, it does not report computational complexity analysis or performance trends as label space size increases.
- What evidence would resolve it: Detailed scalability studies measuring runtime, memory usage, and performance degradation as label space size grows, ideally with datasets spanning orders of magnitude in label space cardinality.

### Open Question 4
- Question: Is the proposed re-weighting factor optimal, or could alternative formulations of similarity and dissimilarity (e.g., learned weighting functions) yield better performance?
- Basis in paper: [explicit] The paper proposes a specific formulation (Jaccard-based similarity and inverse label difference dissimilarity) but acknowledges it is one possible design.
- Why unresolved: The paper does not compare against alternative weighting schemes or learned weighting functions.
- What evidence would resolve it: Ablation studies or direct comparisons with alternative re-weighting functions (e.g., learned neural weighting, exponential decay) on the same datasets to measure relative performance.

### Open Question 5
- Question: Can the Similarity-Dissimilarity Loss framework be adapted to unsupervised or self-supervised contrastive learning scenarios, where label information is unavailable?
- Basis in paper: [inferred] The paper extends supervised contrastive loss and is framed around label-based weighting; no discussion of adaptation to label-free scenarios.
- Why unresolved: The re-weighting relies entirely on label overlap; unsupervised scenarios lack this signal.
- What evidence would resolve it: Proof-of-concept experiments showing whether and how the weighting scheme can be approximated using proxy signals (e.g., clustering, pseudo-labels) in unsupervised settings.

## Limitations
- Limited effectiveness on sparse multi-label datasets where label cardinality is close to one
- Dependence on rich multi-label structure means reduced performance on predominantly single-label samples
- Set-theoretic approach may miss hierarchical or conditional label dependencies

## Confidence

**High Confidence**: Claims about the loss function's mathematical formulation and its ability to reduce to standard supervised contrastive loss in single-label cases are well-supported by the equations and theoretical analysis.

**Medium Confidence**: The assertion that the method significantly improves performance across all evaluated datasets is supported by experimental results, but the magnitude of improvement varies substantially by dataset characteristics and evaluation metrics.

**Low Confidence**: The claim about particular effectiveness on long-tailed distributions is primarily supported by results on MIMIC-III-Full. Additional validation on diverse long-tailed multi-label datasets would strengthen this claim.

## Next Checks

1. **Dataset Diversity Test**: Evaluate the method on datasets with varying label densities and distribution characteristics (e.g., NUS-WIDE vs. PASCAL-VOC) to quantify the relationship between label structure richness and performance gains.

2. **Ablation on Weighting Factors**: Systematically disable the similarity and dissimilarity re-weighting separately to measure their individual contributions to performance improvements, particularly on long-tailed datasets.

3. **Hierarchical Label Structure**: Test the method on datasets with known label hierarchies (e.g., WordNet-based relationships) to assess whether the set-theoretic approach captures or misses these dependencies.