---
ver: rpa2
title: Causal Contrastive Learning for Counterfactual Regression Over Time
arxiv_id: '2406.00535'
source_url: https://arxiv.org/abs/2406.00535
tags:
- causal
- representation
- learning
- treatment
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to counterfactual regression
  over time, emphasizing long-term predictions. The method, called Causal CPC, leverages
  Recurrent Neural Networks (RNNs) and Contrastive Predictive Coding (CPC) to capture
  long-term dependencies while maintaining computational efficiency.
---

# Causal Contrastive Learning for Counterfactual Regression Over Time

## Quick Facts
- arXiv ID: 2406.00535
- Source URL: https://arxiv.org/abs/2406.00535
- Authors: Mouad El Bouchattaoui; Myriam Tami; Benoit Lepetit; Paul-Henry Cournède
- Reference count: 40
- Key outcome: Novel approach using RNNs and CPC achieves state-of-the-art counterfactual estimation while maintaining computational efficiency

## Executive Summary
This paper introduces Causal CPC, a novel approach to counterfactual regression over time that addresses the challenge of long-term prediction in the presence of time-varying confounders. Unlike computationally expensive transformer-based methods, Causal CPC leverages Recurrent Neural Networks (RNNs) and Contrastive Predictive Coding (CPC) to efficiently capture long-term dependencies while maintaining strong performance. The method incorporates Information Maximization (InfoMax) to ensure invertible representations critical for causal identification assumptions. Experiments on both synthetic and real-world data demonstrate that Causal CPC outperforms existing methods in counterfactual estimation, particularly at large prediction horizons, while being more computationally efficient than transformer-based alternatives.

## Method Summary
Causal CPC employs a two-stage training process: first, an encoder is pretrained using CPC and InfoMax objectives to learn a representation that captures long-term dependencies and ensures invertibility; second, a decoder is trained with adversarial balancing and autoregressive outcome prediction. The encoder uses GRU-based networks to process historical data into context representations, while CPC distinguishes true future features from negatives across multiple horizons. InfoMax regularization maximizes mutual information between the process history and its representation to preserve confounding information. The decoder incorporates an adversarial game to achieve treatment-invariant representations, reducing selection bias. The method is evaluated on synthetic tumor growth simulations and semi-synthetic MIMIC-III data, measuring performance across multiple forecasting horizons using NRMSE and RMSE metrics.

## Key Results
- Achieves state-of-the-art counterfactual estimation performance on both synthetic and real-world datasets
- Outperforms transformer-based methods like Causal Transformer while maintaining higher computational efficiency
- Particularly effective at large prediction horizons where long-term dependencies are most critical
- InfoMax regularization ensures representation invertibility necessary for satisfying identification assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive Predictive Coding (CPC) captures long-term dependencies by learning to predict future local features across multiple horizons.
- Mechanism: CPC encodes past context and uses contrastive loss to distinguish true future features from negatives, creating a representation predictive of future states.
- Core assumption: Future features are conditionally dependent on past context given the learned representation.
- Evidence anchors:
  - [abstract] "Leveraging CPC, our method captures long-term dependencies in the presence of time-varying confounders."
  - [section 5.1] "We learn a neural network to distinguish, given context Ct, future local features Zt+1, ..., Zt+τ related to the same individual from those related to other individuals serving as negative representations."
- Break condition: If the conditional independence assumption fails, the contrastive objective no longer guarantees predictive representations.

### Mechanism 2
- Claim: InfoMax regularization ensures invertibility of the representation, retaining confounding information for unbiased counterfactual estimation.
- Mechanism: InfoMax maximizes mutual information between different views of the process history, implicitly enforcing that the full history can be reconstructed from the representation.
- Core assumption: An invertible representation preserves sufficient information about confounders to satisfy identification assumptions.
- Evidence anchors:
  - [abstract] "We employ the InfoMax principle, maximizing a lower bound of mutual information between sequence data and its representation."
  - [section 5.1] "To remedy this, we employ the InfoMax principle to maximize the MI between Ht and the context Ct."
- Break condition: If the encoder is not invertible, some confounding information may be lost, violating sequential ignorability.

### Mechanism 3
- Claim: Adversarial balancing via InfoMax achieves treatment-invariant representations, reducing selection bias.
- Mechanism: The model learns to minimize mutual information between the representation and treatment, creating a balanced representation across treatment arms.
- Core assumption: Minimizing I(Φt+1, Wt+1) yields a treatment-invariant representation that satisfies the balancing property.
- Evidence anchors:
  - [abstract] "We suggest minimizing an upper bound over MI between representation and treatment to make the representation non-predictive of the treatment."
  - [section 5.2] "To achieve this, we set up an adversarial game: one network learns a distribution over the next treatment given the representation, while a regularization term over the representation encourages it to be non-predictive of that treatment."
- Break condition: If the adversarial game does not reach equilibrium, the representation may remain predictive of treatment.

## Foundational Learning

- Concept: Contrastive Predictive Coding (CPC)
  - Why needed here: CPC provides a way to learn representations that capture long-term dependencies without requiring computationally expensive transformers.
  - Quick check question: How does CPC use contrastive loss to learn predictive representations?

- Concept: Information Maximization (InfoMax)
  - Why needed here: InfoMax ensures that the learned representation retains sufficient information about the input to satisfy identification assumptions for causal inference.
  - Quick check question: What is the relationship between maximizing mutual information and representation invertibility?

- Concept: Sequential Ignorability
  - Why needed here: This assumption underlies the identifiability of counterfactual outcomes in time-varying settings with time-dependent confounding.
  - Quick check question: What are the three key assumptions required for identifying counterfactual responses from observational data?

## Architecture Onboarding

- Component map: Encoder (GRU) -> CPC/InfoMax module -> Decoder (GRU) -> Factual outcome prediction
- Critical path: Encoder → CPC/InfoMax → Decoder → Factual outcome prediction
- Design tradeoffs:
  - Simplicity vs. expressiveness: Using GRU instead of transformers for efficiency
  - Computational cost vs. predictive power: Balancing CPC horizons and batch sizes
  - Invertibility vs. compression: Trade-off in representation capacity
- Failure signatures:
  - Poor counterfactual estimation at long horizons suggests insufficient CPC training
  - Biased estimates indicate failure of balancing regularization
  - Unstable training may indicate improper learning rate or architecture mismatch
- First 3 experiments:
  1. Train encoder with CPC only on synthetic data to verify long-term dependency capture
  2. Add InfoMax regularization and test representation invertibility on toy dataset
  3. Combine all components and evaluate counterfactual estimation on MIMIC-III data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Causal CPC compare to more recent transformer-based approaches when both are optimized for computational efficiency?
- Basis in paper: [explicit] The paper states that Causal CPC achieves state-of-the-art results while avoiding computationally expensive transformers and emphasizes efficiency.
- Why unresolved: The paper only compares to existing transformer-based models (Causal Transformer) without directly comparing to other recent efficient transformer variants or ablations that would isolate the efficiency gains.
- What evidence would resolve it: Direct comparison experiments between Causal CPC and efficient transformer variants (like efficient transformers, distilled transformers) on the same datasets measuring both accuracy and computational resources.

### Open Question 2
- Question: What is the theoretical limit of the mutual information bound between the process history and its representation when using the InfoMax regularization?
- Basis in paper: [inferred] The paper discusses InfoMax regularization and its relation to reconstruction quality, but doesn't provide theoretical bounds on the maximum achievable MI.
- Why unresolved: The paper focuses on practical implementation and empirical results rather than theoretical limits of the MI bounds.
- What evidence would resolve it: Theoretical analysis deriving the maximum achievable MI bound between Ht and (Ch_t, Cf_t) under the given constraints, possibly using information-theoretic techniques.

### Open Question 3
- Question: How does the model's performance degrade under different types and degrees of violations of sequential ignorability?
- Basis in paper: [explicit] The paper conducts a falsifiability test where some confounders are masked, but only tests one specific type of violation.
- Why unresolved: The paper only tests one specific violation scenario (masking confounders) rather than systematically varying the type and degree of ignorability violations.
- What evidence would resolve it: Experiments systematically varying different types of ignorability violations (e.g., different types of hidden confounders, varying degrees of confounding strength) and measuring performance degradation.

## Limitations

- Reliance on sequential ignorability assumptions may not hold in practice with complex time-varying confounding
- InfoMax regularization may be overly restrictive and could lead to information loss if encoder cannot perfectly invert input
- Adversarial balancing mechanism may not fully achieve treatment invariance in high-dimensional or imbalanced treatment scenarios
- Computational complexity of CPC still scales with sequence length and may become prohibitive for very long time series

## Confidence

- **High Confidence**: The core architectural design using RNNs with CPC and InfoMax is well-founded and theoretically justified. The computational efficiency gains over transformers are likely real and significant.
- **Medium Confidence**: The empirical results on synthetic and semi-synthetic datasets are promising but may not generalize to more complex real-world scenarios. The adversarial balancing mechanism's effectiveness requires further validation.
- **Low Confidence**: The assumption that InfoMax regularization alone is sufficient to ensure identification in all practical settings is not well-established. The long-term stability of the adversarial game and its impact on counterfactual estimation remains uncertain.

## Next Checks

1. **Ablation Study on CPC Horizons**: Systematically vary the number of CPC prediction horizons and evaluate the impact on counterfactual estimation accuracy and computational efficiency. This would validate whether the claimed benefits of CPC over transformers are robust to hyperparameter choices.

2. **Robustness to Confounding Violation**: Design experiments where the sequential ignorability assumption is deliberately violated (e.g., by introducing unobserved confounders) and measure the degradation in Causal CPC's performance compared to baseline methods. This would test the practical limitations of the approach.

3. **Real-World Deployment Test**: Apply Causal CPC to a real-world clinical dataset with known treatment effects (e.g., from a randomized controlled trial) to assess its performance in a setting where ground truth counterfactuals are available. This would provide the strongest validation of the method's practical utility.