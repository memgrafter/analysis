---
ver: rpa2
title: Multiple Realizability and the Rise of Deep Learning
arxiv_id: '2405.13231'
source_url: https://arxiv.org/abs/2405.13231
tags:
- multiple
- cognitive
- https
- realizability
- brain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper challenges the widely held view that multiple realizability
  entails that the study of the mind can and must be pursued independently of the
  study of its implementation in the brain or in artificial analogues. The authors
  argue that this inference does not hold, as one can accept the possibility of multiple
  realization while embracing neuroscience and contemporary deep neural networks (DNNs)
  as key sources of insight into cognition.
---

# Multiple Realizability and the Rise of Deep Learning

## Quick Facts
- arXiv ID: 2405.13231
- Source URL: https://arxiv.org/abs/2405.13231
- Reference count: 0
- Primary result: Multiple realizability does not entail methodological autonomy, allowing deep neural networks to inform cognitive science despite being implementation-level models

## Executive Summary
This paper challenges the widely held view that multiple realizability—the thesis that mental states can be realized in different physical systems—entails that cognitive science must be pursued independently of neuroscience and artificial implementations. The authors argue that this inference is false: one can accept multiple realizability while still embracing neuroscience and deep neural networks as key sources of insight into cognition. They demonstrate how two influential philosophical articles—Fodor and Pylyshyn (1988) and Bechtel and Mundale (1999)—rely on this mistaken inference from opposing positions. Severing this connection opens the door for contemporary deep neural networks to contribute to the formulation and evaluation of psychological hypotheses, even when interpreted as implementation-level models.

## Method Summary
The paper employs philosophical analysis and argumentation to examine the relationship between multiple realizability and methodological autonomy in cognitive science. It analyzes how the false inference from multiple realizability to methodological autonomy has been used in both connectionist critiques and identity theory arguments. The authors distinguish between ontological and methodological autonomy, showing that while multiple realizability may have ontological implications, it does not directly entail anything about the process of discovering psychological generalizations. They explore how deep neural networks, despite being implementation-level models, can reveal properties of cognitive constructs that were not explicitly built in, challenging the notion of "mere implementation."

## Key Results
- Multiple realizability does not entail methodological autonomy in cognitive science
- Deep neural networks can inform psychological hypotheses even when interpreted as implementation-level models
- The inference from multiple realizability to methodological autonomy undermines both connectionist critiques and identity theory arguments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple realizability does not entail methodological autonomy; the brain (or its artificial analogues) can still provide crucial evidence for psychological theories.
- Mechanism: The inference from multiple realizability to methodological autonomy assumes that because mental states can be realized in multiple substrates, knowledge of any particular substrate is irrelevant. This mechanism breaks by recognizing that we do not know the rules of "cognitive chess" in advance—we must reverse engineer them from implementation details.
- Core assumption: Multiple realizability implies that implementation-level facts are not essential to discovering psychological generalizations.
- Evidence anchors:
  - [abstract] "the paper challenges the widely held view that multiple realizability entails that the study of the mind can and must be pursued independently of the study of its implementation"
  - [section] "The fact that there are explanatory generalizations involving mental constructs does not directly entail anything about what the process of discovering them will look like"
- Break condition: When implementation-level facts fail to provide any clues to function, or when the implementation is so different that no informative mapping exists.

### Mechanism 2
- Claim: Deep neural networks can contribute to psychological hypothesis formation even if interpreted as implementation-level models.
- Mechanism: The paper distinguishes between "mere implementation" (strict, pre-specified mapping) and informative implementation (models that diverge from theoretical assumptions in illuminating ways). DNNs often fall into the latter category, revealing unexpected properties of cognitive constructs.
- Core assumption: Implementation-level models can reveal properties of higher-level theories that were not explicitly built in.
- Evidence anchors:
  - [abstract] "deep neural networks may play a crucial role in formulating and evaluating hypotheses about cognition, even if they are interpreted as implementation-level models"
  - [section] "Much ink has been spilled debating whether artificial neural networks are implementation-level models or offer competing cognitive or algorithmic accounts"
- Break condition: When DNN behavior is fully explained by lower-level mechanisms without revealing anything about the psychological constructs being modeled.

### Mechanism 3
- Claim: Severing the link between multiple realizability and methodological autonomy undermines both Fodor and Pylyshyn's and Bechtel and Mundale's arguments.
- Mechanism: Both papers rely on the false inference that multiple realizability implies methodological autonomy. Rejecting this inference neutralizes both the argument for insulating cognitive science from neural networks and the argument for reducing psychological states to brain states.
- Core assumption: The validity of both Fodor and Pylyshyn's and Bechtel and Mundale's central arguments depends on the truth of this inference.
- Evidence anchors:
  - [abstract] "one can accept the possibility of multiple realization while embracing neuroscience and contemporary DNNs as key sources of insight into cognition"
  - [section] "Fodor and Pylyshyn's discussion obscures the possibility of informative implementation by exploiting a certain mismatch between a strict conception of 'implementation' and the looser way that the notion gets employed in practice"
- Break condition: When either paper presents an argument for methodological autonomy that does not rely on multiple realizability.

## Foundational Learning

- Concept: Multiple realizability thesis
  - Why needed here: This is the central philosophical claim that the paper challenges and reinterprets; understanding it is essential to following the argument.
  - Quick check question: Can the same psychological state (e.g., pain) be realized in different physical systems (human brain, alien brain, silicon)?

- Concept: Methodological autonomy vs. ontological autonomy
  - Why needed here: The paper distinguishes these two senses of autonomy and argues only the ontological one follows from multiple realizability; confusing them leads to the flawed inference.
  - Quick check question: Does multiple realizability imply that neuroscientists cannot contribute to understanding the mind?

- Concept: Implementation-level vs. cognitive-level explanations
  - Why needed here: The paper argues that DNNs, even if implementation-level, can still inform cognitive-level theories; understanding this distinction is crucial to the argument.
  - Quick check question: Can studying how a neural network implements a cognitive function teach us something about that function itself?

## Architecture Onboarding

- Component map: Multiple realizability thesis -> False inference (multiple realizability → methodological autonomy) -> Analysis of Fodor and Pylyshyn's and Bechtel and Mundale's arguments -> Implications for DNNs and cognitive science methodology
- Critical path: Identify the false inference (multiple realizability → methodological autonomy) → Show how it's used in both opposing arguments → Demonstrate why it fails → Explore implications for DNNs and cognitive science.
- Design tradeoffs: Balancing depth of philosophical analysis with accessibility to cognitive scientists; deciding how much technical detail about DNNs to include versus focusing on conceptual arguments.
- Failure signatures: If the paper doesn't clearly articulate the distinction between methodological and ontological autonomy, or if it doesn't provide concrete examples of how DNNs can inform cognitive theories despite being implementation-level models.
- First 3 experiments:
  1. Analyze a specific DNN model (e.g., a language model) to identify how its implementation reveals properties of linguistic competence that challenge prior theories.
  2. Map the argumentative structure of Fodor and Pylyshyn's "mere implementation" objection and identify where the false inference enters.
  3. Examine Bechtel and Mundale's case studies of neuroscience informing cognitive theory to show that methodological autonomy is not warranted even without multiple realization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a principled method to distinguish between "mere implementation" and "informative implementation" in deep neural networks, particularly when the implemented functions are underspecified?
- Basis in paper: [explicit] The paper discusses the difficulty of distinguishing between mere implementation and informative implementation, noting that the psychological constructs implemented in DNNs are often underspecified, leaving room for informative divergence.
- Why unresolved: The paper highlights the challenge of defining clear criteria for what constitutes informative implementation versus mere implementation, as the line between the two can be blurry.
- What evidence would resolve it: Developing a framework that clearly delineates the criteria for informative implementation, perhaps through case studies or empirical testing, would help resolve this question.

### Open Question 2
- Question: Can the "bitter lesson" from deep learning, which suggests that scaling models and allowing them to find solutions is more effective than implementing human-designed theories, be generalized beyond engineering to the domain of scientific explanation?
- Basis in paper: [explicit] The paper discusses the "bitter lesson" and its potential implications for traditional top-down approaches to cognitive science, suggesting that allowing models to find solutions might be more effective than pre-specifying theories.
- Why unresolved: The paper acknowledges that while the bitter lesson has been effective in engineering, its applicability to scientific explanation is still uncertain and requires further investigation.
- What evidence would resolve it: Empirical studies comparing the effectiveness of scaled models versus human-designed theories in explaining cognitive phenomena would provide insights into the generalizability of the bitter lesson.

### Open Question 3
- Question: What are the philosophical implications of deep neural networks providing concrete realizations of the multiple realizability thesis, and how does this affect the debate between identity theory and functionalism?
- Basis in paper: [explicit] The paper discusses how DNNs provide plausible examples of multiple realizations of cognitive capacities, which challenges the identity theory and supports the multiple realizability thesis.
- Why unresolved: While the paper suggests that DNNs support multiple realizability, the full philosophical implications and how this affects ongoing debates are not fully explored.
- What evidence would resolve it: Further philosophical analysis and empirical studies on DNNs as realizations of cognitive capacities could clarify their implications for the identity theory versus functionalism debate.

## Limitations

- The argument depends on contested philosophical distinctions between methodological and ontological autonomy
- The analysis of DNNs as implementation-level models requires acceptance of specific interpretations of what constitutes "implementation" versus "cognitive explanation"
- The critique of Fodor and Pylyshyn's and Bechtel and Mundale's arguments relies on identifying a shared inferential structure that may not be universally recognized

## Confidence

- **High confidence**: The distinction between methodological and ontological autonomy is clearly articulated and well-supported
- **Medium confidence**: The claim that DNNs can inform psychological hypotheses despite being implementation-level models, as this depends on contested philosophical positions about the nature of implementation
- **Medium confidence**: The critique of Fodor and Pylyshyn's and Bechtel and Mundale's arguments, as it requires acceptance of the paper's interpretation of their inferential structure

## Next Checks

1. Examine a specific DNN architecture (e.g., a transformer model) to identify concrete examples of how its implementation-level properties reveal unexpected features of the cognitive constructs it models
2. Survey practicing cognitive scientists to assess how widely the inference from multiple realizability to methodological autonomy is actually employed in research practice
3. Analyze the behavior of a DNN trained on a cognitive task to determine whether its performance can be fully explained by lower-level mechanisms or whether it reveals properties of the higher-level psychological constructs being modeled