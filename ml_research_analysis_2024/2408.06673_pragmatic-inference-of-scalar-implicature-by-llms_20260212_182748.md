---
ver: rpa2
title: Pragmatic inference of scalar implicature by LLMs
arxiv_id: '2408.06673'
source_url: https://arxiv.org/abs/2408.06673
tags:
- implicature
- pragmatic
- scalar
- some
- processing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study examined how large language models (LLMs) process scalar\
  \ implicature\u2014the pragmatic interpretation of \"some\" as \"not all\"\u2014\
  under different contexts. Experiment 1 used cosine similarity to compare BERT and\
  \ GPT-2's interpretations of sentences with \"some\" against semantic (\"possibly\
  \ all\") and pragmatic (\"not all\") alternatives, finding both models preferred\
  \ pragmatic interpretations even without context."
---

# Pragmatic inference of scalar implicature by LLMs

## Quick Facts
- arXiv ID: 2408.06673
- Source URL: https://arxiv.org/abs/2408.06673
- Reference count: 0
- This study found BERT inherently encodes pragmatic implicature while GPT-2 relies on context-dependent processing

## Executive Summary
This study investigates how transformer-based language models process scalar implicature, specifically the pragmatic interpretation of "some" as "not all." Using BERT and GPT-2, the researchers compared semantic ("possibly all") versus pragmatic ("not all") interpretations through cosine similarity and measured processing difficulty via surprisal scores under different contextual conditions. The findings reveal that BERT inherently encodes pragmatic implicature as a default behavior, while GPT-2 shows greater processing difficulty when implicature is required but context is insufficient, suggesting fundamentally different approaches to pragmatic reasoning.

## Method Summary
The study employed two experiments using 198 sentences with "some + NP" structure extracted from the British National Corpus. Experiment 1 measured cosine similarity between sentence embeddings to compare semantic versus pragmatic interpretations. BERT used [CLS] token embeddings while GPT-2 used averaged token embeddings. Experiment 2 manipulated Question Under Discussion (QUD) as contextual cues and measured processing difficulty through surprisal scores derived from next sentence prediction probabilities for BERT and next token prediction for GPT-2. Statistical analysis compared processing patterns across upper-bound QUDs (containing "all") and lower-bound QUDs (containing "any").

## Key Results
- BERT consistently preferred pragmatic interpretations of "some" as "not all" regardless of context
- GPT-2 showed greater processing difficulty with upper-bound QUDs containing "all" compared to lower-bound QUDs with "any"
- BERT's processing remained consistent across QUD types while GPT-2's varied significantly

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERT inherently encodes the pragmatic interpretation of "some" as "not all" due to pre-training exposure to scalar implicature in natural language data
- Mechanism: Pre-training on large-scale corpora naturally captures pragmatic implicature patterns, embedding them as default interpretive behavior in the model's representations
- Core assumption: Natural language text inherently contains sufficient scalar implicature instances for models to learn the pragmatic default without explicit context
- Evidence anchors:
  - [abstract] "BERT inherently incorporates pragmatic implicature not all within the term some, adhering to Default model"
  - [section] "Jeretic et al. (2020) found that BERT learned scalar implicature... The natural language data employed in the pre-training inherently include pragmatic information"
  - [corpus] Weak - corpus only shows related papers, no direct evidence about pre-training data composition
- Break condition: If pre-training data lacks sufficient scalar implicature examples or contains strong cancellation patterns, the default pragmatic interpretation may not emerge

### Mechanism 2
- Claim: GPT-2 relies on contextual cues (like QUD) to derive pragmatic implicature, showing processing difficulty when implicature is required but context is insufficient
- Mechanism: GPT-2's autoregressive architecture and training objective make it more sensitive to explicit contextual information, requiring additional cognitive effort to infer implicatures when context demands it
- Core assumption: The autoregressive prediction task in GPT-2 makes it more context-dependent than BERT's bidirectional approach
- Evidence anchors:
  - [abstract] "GPT-2 seems to encounter processing difficulties in inferring pragmatic implicature within context, consistent with Context-driven model"
  - [section] "GPT-2 showed more processing difficulties in the upper-bound QUD... GPT-2 exhibited higher surprisal scores for the upper-bound QUD"
  - [corpus] Weak - corpus shows related work but no direct evidence about GPT-2's architectural context dependence
- Break condition: If GPT-2 is provided with rich contextual cues, the processing difficulty may decrease, suggesting the effect is context-dependent rather than inherent

### Mechanism 3
- Claim: Different transformer architectures (BERT vs GPT-2) lead to distinct pragmatic processing patterns due to their training objectives and data exposure
- Mechanism: BERT's next-sentence prediction pre-training captures sentence-level relationships and pragmatic patterns, while GPT-2's autoregressive token prediction makes it more sensitive to immediate context and explicit cues
- Core assumption: The architectural differences between BERT and GPT-2 fundamentally shape how they process pragmatic implicature
- Evidence anchors:
  - [section] "BERT is pre-trained using Next Sentence Prediction (NSP)... GPT-2 does not utilize methods like BERT's NSP"
  - [section] "BERT showed no significant difference in processing difficulties based on QUDs, whereas GPT-2 showed more processing difficulties in the upper-bound QUD"
  - [corpus] Weak - corpus mentions related papers but lacks direct evidence about architectural impact on pragmatic processing
- Break condition: If both models are fine-tuned on the same pragmatic inference task, the architectural differences may diminish, suggesting the effect is task-dependent

## Foundational Learning

- Concept: Scalar implicature
  - Why needed here: Understanding how "some" pragmatically implies "not all" is the core phenomenon being studied
  - Quick check question: What is the semantic meaning of "some" versus its typical pragmatic interpretation?

- Concept: Question Under Discussion (QUD)
  - Why needed here: QUD manipulation is the key contextual variable used to test whether models process implicature contextually
  - Quick check question: How does an upper-bound QUD (containing "all") differ from a lower-bound QUD (containing "any") in triggering implicature?

- Concept: Next Sentence Prediction (NSP) vs Autoregressive prediction
  - Why needed here: These are the core training objectives that differentiate BERT and GPT-2, explaining their different pragmatic processing patterns
  - Quick check question: How does BERT's bidirectional NSP training differ from GPT-2's unidirectional autoregressive approach?

## Architecture Onboarding

- Component map:
  Input tokenization -> Sentence embedding extraction -> Cosine similarity computation -> Next sentence/token prediction -> Surprisal transformation

- Critical path:
  1. Tokenize input sentences using model-specific tokenizer
  2. Extract sentence embeddings from final layer
  3. Compute cosine similarities between some-sentences and semantic/pragmatic interpretations
  4. Calculate next sentence/token prediction probabilities for QUD pairs
  5. Transform probabilities to surprisal scores
  6. Statistical analysis of differences across conditions

- Design tradeoffs:
  - Using cosine similarity provides computational efficiency but may underestimate semantic similarity for high-frequency words
  - Averaging token embeddings for GPT-2 is simpler than using [CLS] but may lose some sentence-level information
  - Linear transformation and sigmoid application of cosine scores makes interpretation easier but adds processing steps

- Failure signatures:
  - Similar cosine similarity scores across semantic and pragmatic interpretations suggest the model isn't distinguishing pragmatic meaning
  - No difference in surprisal scores across QUD types suggests the model isn't using context for pragmatic inference
  - High variance in similarity scores may indicate instability in embedding extraction or tokenization

- First 3 experiments:
  1. Replicate the baseline experiment with a larger dataset to test generalizability of the semantic vs pragmatic interpretation findings
  2. Test additional transformer models (e.g., RoBERTa, GPT-3) to see if the BERT/GPT-2 patterns hold across architectures
  3. Manipulate the position of "some" within sentences to test whether subject position is critical for implicature generation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do newer transformer-based models like BERT-large or GPT-3 exhibit different patterns in pragmatic inference of scalar implicature compared to BERT-base and GPT-2?
- Basis in paper: [explicit] The paper acknowledges using only early transformer models (BERT-base and GPT-2) as a limitation, noting that newer models with significantly higher performance might yield varying results
- Why unresolved: The study was limited to foundational models to better understand processing architectures, but the authors explicitly state that different models could yield varying results due to their diverse linguistic information incorporation
- What evidence would resolve it: Comparative experiments using multiple transformer models (BERT-large, RoBERTa, GPT-3, etc.) with identical experimental designs measuring cosine similarity and surprisal scores across different QUD conditions

### Open Question 2
- Question: Does the presence of multiple unspoken alternatives beyond the <some, all> scale strengthen or weaken the pragmatic inference capabilities of language models?
- Basis in paper: [explicit] The paper cites Hu et al. (2023) showing that BERT's pragmatic inference ability strengthens with more alternatives becoming available, depending on contextual predictability
- Why unresolved: While Hu et al. demonstrated this effect for BERT, the paper suggests this might lead to contrasting results if more alternatives were presented, but doesn't test this hypothesis directly
- What evidence would resolve it: Experiments manipulating the number and type of alternatives available in contextual cues, measuring changes in cosine similarity scores and surprisal patterns across different models and alternative set sizes

### Open Question 3
- Question: How do different metrics beyond cosine similarity and next sentence prediction affect the measurement of pragmatic inference in scalar implicature processing?
- Basis in paper: [explicit] The paper acknowledges in the limitations section that the metrics used (cosine similarity and next sentence prediction) may yield different results when other metrics are applied
- Why unresolved: The study employed these specific metrics for their computational efficiency and widespread use, but recognizes that metric diversity could lead to variations in results
- What evidence would resolve it: Comparative analysis using alternative metrics such as Euclidean distance, Manhattan distance, BERTScore, or human judgment ratings alongside the original metrics to assess consistency and validity of pragmatic inference measurements

## Limitations

- The corpus analysis revealed only 25 related papers with limited citation support, suggesting the research area may be underexplored or the study's specific combination of methods and claims may be novel
- Preprocessing criteria for filtering BNC sentences beyond "some + NP" subject requirement and single clause constraint are not fully specified, affecting reproducibility
- The study relies on cosine similarity and surprisal scores as proxies for pragmatic understanding, which may not capture the full complexity of implicature processing

## Confidence

- **High confidence**: BERT's inherent encoding of pragmatic implicature as demonstrated by consistent cosine similarity patterns across different contexts
- **Medium confidence**: GPT-2's context-dependent processing showing greater difficulty with upper-bound QUDs
- **Low confidence**: The claim that architectural differences (BERT's NSP vs GPT-2's autoregressive prediction) fundamentally determine pragmatic processing patterns

## Next Checks

1. Replicate the baseline experiment with a larger and more diverse dataset to test whether the semantic vs pragmatic interpretation patterns hold across different linguistic contexts and domains beyond the BNC corpus

2. Conduct ablation studies on GPT-2 by systematically varying contextual cues and measuring the resulting changes in processing difficulty, to determine whether the observed effects are genuinely context-dependent or could be explained by other factors like sentence complexity

3. Implement fine-tuning experiments where both models are trained on explicit scalar implicature tasks, then compare whether the architectural differences persist or whether task-specific training overrides the observed default/context-driven patterns