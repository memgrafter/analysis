---
ver: rpa2
title: 'DRoP: Distributionally Robust Data Pruning'
arxiv_id: '2404.05579'
source_url: https://arxiv.org/abs/2404.05579
tags:
- pruning
- data
- dataset
- learning
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the impact of data pruning algorithms on
  classification bias, revealing that existing methods often exacerbate performance
  disparities across classes while maintaining high average accuracy. The authors
  conduct a comprehensive empirical study showing that popular pruning algorithms
  like Dynamic Uncertainty, EL2N, GraNd, Forgetting, and CoreSet fail to improve worst-class
  performance compared to training on full datasets.
---

# DRoP: Distributionally Robust Data Pruning

## Quick Facts
- arXiv ID: 2404.05579
- Source URL: https://arxiv.org/abs/2404.05579
- Reference count: 40
- Key outcome: Existing pruning algorithms worsen worst-class performance; DRoP improves robustness with minimal overhead

## Executive Summary
This paper investigates how data pruning algorithms impact classification bias in deep learning. Through extensive empirical studies across multiple datasets (CIFAR-10/100, TinyImageNet, Waterbirds) and architectures (VGG, ResNet, Wide-ResNet), the authors demonstrate that popular pruning methods like Dynamic Uncertainty, EL2N, GraNd, Forgetting, and CoreSet consistently worsen worst-class performance compared to training on full datasets. Theoretical analysis on a mixture of Gaussians reveals that class-conditional independent random pruning with difficulty-aware class quotas can achieve optimal worst-class accuracy. Based on these insights, the authors propose DRoP (Distributionally Robust Pruning), a simple random pruning protocol with error-based class ratios that significantly reduces classification bias while offering enhanced data efficiency.

## Method Summary
DRoP operates through a two-stage process: first, a preliminary model is trained on the full dataset for a small fraction (10%) of total training time to compute class-wise error rates on a validation set. Second, DRoP computes pruning quotas for each class inversely proportional to their validation error rates, then performs random subsampling within each class according to these quotas. This approach requires no additional computational overhead beyond existing pruning methods, as it uses the same preliminary model and validation set already needed for computing pruning scores. The final model is trained on the pruned dataset using standard data augmentation techniques.

## Key Results
- DRoP consistently improves worst-class accuracy by 10% or more compared to full dataset training while maintaining average accuracy within 6% of baseline
- Random+DRoP outperforms all tested baselines (Dynamic Uncertainty, EL2N, GraNd, Forgetting, CoreSet) in robustness metrics across CIFAR-10, CIFAR-100, TinyImageNet, and Waterbirds datasets
- Existing pruning algorithms fail to improve worst-class performance, often exacerbating performance disparities across classes despite maintaining high average accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Random pruning with DRoP class quotas improves worst-class performance by correcting the imbalance in the distribution of errors across classes.
- **Mechanism:** DRoP sets class pruning fractions inversely proportional to class-specific validation error rates, so classes with higher error are retained more often. Random pruning within classes then avoids cherry-picking individual samples, which would otherwise reintroduce bias.
- **Core assumption:** Class-wise error rates are correlated with the underlying variance of the class distributions, which governs the optimal pruning ratio for worst-class robustness.
- **Evidence anchors:**
  - [abstract]: "target class proportions based on the corresponding class-wise error rates computed on a hold-out validation set after a preliminary training round on the full dataset."
  - [section]: "Based on these observations, we design a first 'robustness-aware' data pruning protocol, coined Distributionally Robust Pruning (DRoP). It selects target class proportions based on the corresponding class-wise error rates computed on a hold-out validation set after a preliminary training round on the full dataset."
- **Break condition:** If class error rates do not correlate with the actual difficulty (variance) of the class distribution, the DRoP quotas will not be optimal.

### Mechanism 2
- **Claim:** Random pruning within classes preserves the distributional structure needed for robust generalization, unlike sample-level scoring methods.
- **Mechanism:** Sample-level scoring methods (e.g., EL2N, GraNd) prune based on individual sample difficulty, but these scores are highly correlated across methods and often lead to pruning entire classes. Random pruning maintains a representative sample from each class.
- **Core assumption:** Class-conditional independence holds when samples are randomly retained within each class, which is violated by score-based pruning.
- **Evidence anchors:**
  - [abstract]: "random pruning within classes coupled with difficulty-aware class quotas has potential to improve worst-class performance."
  - [section]: "The class-conditional independence assumption we made above is crucial. While it is respected when subsampling randomly within each class, it clearly does not hold for existing, more sophisticated, data pruning algorithms."
- **Break condition:** If class distributions are heavily multimodal, random sampling may not preserve sufficient coverage of decision-relevant regions.

### Mechanism 3
- **Claim:** DRoP improves robustness without requiring additional computational overhead beyond existing pruning methods.
- **Mechanism:** DRoP uses the same preliminary model and validation set that existing pruning methods already require to compute sample scores, so it adds no extra training cost.
- **Core assumption:** The validation set is large enough to provide stable class-wise error estimates.
- **Evidence anchors:**
  - [abstract]: "Such a query model is still required by all existing data pruning algorithms to compute scores, so we introduce no additional resource overhead."
  - [section]: "We propose to select the pruning fraction of each class based on its validation performance given a preliminary model ψ trained on the whole dataset."
- **Break condition:** If the preliminary training is too short or the validation set too small, error estimates will be noisy and DRoP quotas will be suboptimal.

## Foundational Learning

- **Concept:** Class-wise error estimation from a preliminary model.
  - Why needed here: DRoP relies on knowing which classes are harder to classify to set pruning quotas.
  - Quick check question: If a preliminary model achieves 95% accuracy on class A and 60% on class B, which class should DRoP retain more samples from?

- **Concept:** Random subsampling preserving class proportions.
  - Why needed here: Random pruning within classes maintains representativeness and avoids bias introduced by score-based selection.
  - Quick check question: If a class has 1000 samples and we want to retain 30% using random pruning, how many samples will be kept?

- **Concept:** Worst-class vs average accuracy tradeoff.
  - Why needed here: DRoP is designed to improve worst-class accuracy, which often comes at a slight cost to average accuracy.
  - Quick check question: If a model has accuracies [90%, 85%, 60%, 55%] for four classes, what is the worst-class accuracy?

## Architecture Onboarding

- **Component map:** Data loader → preliminary model training → class error estimation → DRoP quota computation → random pruning → final model training
- **Critical path:**
  1. Train preliminary model on full dataset (10% of full epochs)
  2. Compute class-wise recall on validation set
  3. Apply DRoP algorithm to compute class densities
  4. Perform random pruning per class
  5. Train final model on pruned dataset
- **Design tradeoffs:**
  - Using class error rates instead of variance estimates trades theoretical optimality for practical feasibility
  - Random pruning is simpler but may lose some efficiency compared to score-based methods
  - Reserving a validation set for error estimation reduces training data but is necessary for DRoP
- **Failure signatures:**
  - If DRoP does not improve worst-class accuracy, the error estimates may be unreliable
  - If average accuracy drops sharply, the pruning ratio may be too aggressive or class quotas misestimated
  - If pruning removes too many samples from minority classes, ensure the quota computation handles saturation correctly
- **First 3 experiments:**
  1. Run DRoP with a small validation set (e.g., 5% of training data) to check sensitivity of class quotas
  2. Compare random pruning with DRoP quotas vs random pruning without quotas on CIFAR-10
  3. Verify that DRoP improves worst-class accuracy while maintaining average accuracy within 5% of full dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DRoP's effectiveness vary across different model architectures beyond those tested (VGG, ResNet, Wide-ResNet)?
- Basis in paper: [inferred] The paper tested DRoP on VGG, ResNet, and Wide-ResNet architectures, finding consistent improvements in robustness metrics across all tested models. The authors acknowledge that larger models may enjoy better distributional robustness, suggesting model architecture could be a factor.
- Why unresolved: The paper only evaluated a limited set of common architectures. Deep learning encompasses many architectural paradigms (Transformers, Vision Transformers, MLPs) that may respond differently to DRoP's pruning strategy.
- What evidence would resolve it: Comprehensive testing of DRoP across diverse architectural families, including self-attention based models, residual-free architectures, and architectures with different inductive biases, measuring both average and worst-class accuracy.

### Open Question 2
- Question: Can the theoretical insights from the Gaussian mixture model be extended to more complex, realistic data distributions?
- Basis in paper: [explicit] The paper provides theoretical analysis specifically for a mixture of two isotropic Gaussians with linear classifiers, demonstrating optimal pruning ratios based on class variances. The authors acknowledge this is a toy model and suggest future work could connect this theory to feature distributions in neural networks.
- Why unresolved: Real-world datasets exhibit complex, non-Gaussian distributions with multimodal class structures, correlations between features, and varying decision boundaries. The simple Gaussian mixture model may not capture these complexities.
- What evidence would resolve it: Theoretical analysis of worst-class optimal pruning ratios for more complex distributions (multimodal mixtures, non-isotropic Gaussians, heavy-tailed distributions) and empirical validation showing these theoretical predictions match observed performance on realistic datasets.

### Open Question 3
- Question: What is the relationship between DRoP's pruning strategy and other forms of robustness beyond classification bias (e.g., adversarial robustness, out-of-distribution detection)?
- Basis in paper: [inferred] The paper focuses exclusively on classification bias and group robustness, testing DRoP on datasets with spurious correlations (Waterbirds). The authors mention limitations regarding evaluation of worst-group accuracy and spurious correlations, suggesting these areas need deeper investigation.
- Why unresolved: DRoP optimizes for balanced class accuracy by pruning easier classes more aggressively. This strategy might have unintended consequences for other robustness dimensions that require different trade-offs between classes or samples.
- What evidence would resolve it: Systematic evaluation of DRoP-pruned models on adversarial robustness benchmarks, out-of-distribution detection tasks, and calibration metrics to determine whether improving class balance through pruning affects these other robustness measures positively or negatively.

## Limitations

- Theoretical analysis limited to Gaussian mixture model, which may not capture real-world complexity
- Random pruning within classes may not preserve sufficient coverage for highly multimodal distributions
- Performance depends on stability of class error estimates from preliminary training, which may be unreliable with small validation sets

## Confidence

- **High Confidence:** The empirical observation that existing pruning algorithms worsen worst-class performance is well-supported by extensive experiments across multiple datasets and architectures
- **Medium Confidence:** The theoretical insight that class-conditional independent random pruning with difficulty-aware quotas can improve worst-class performance is plausible but relies on assumptions about the relationship between error rates and distributional difficulty
- **Medium Confidence:** The claim that DRoP introduces no computational overhead beyond existing methods is reasonable but depends on the stability of class error estimates from preliminary training

## Next Checks

1. **Sensitivity Analysis:** Test DRoP's performance across varying validation set sizes (1-10% of training data) to quantify the robustness of class error estimates to validation data scarcity
2. **Multimodal Distribution Test:** Evaluate DRoP on datasets with known multimodal class structures (e.g., synthetic datasets or fine-grained classification tasks) to assess whether random pruning within classes adequately preserves coverage of all decision-relevant regions
3. **Error-Rate Correlation Study:** Systematically vary the difficulty of different classes (e.g., by adding class-specific noise or distortion) and measure how well DRoP's class quotas track the true distributional difficulty versus observed error rates