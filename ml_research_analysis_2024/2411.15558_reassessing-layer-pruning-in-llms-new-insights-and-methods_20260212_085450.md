---
ver: rpa2
title: 'Reassessing Layer Pruning in LLMs: New Insights and Methods'
arxiv_id: '2411.15558'
source_url: https://arxiv.org/abs/2411.15558
tags:
- pruning
- layer
- arxiv
- layers
- llama-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper systematically benchmarks layer pruning techniques for
  large language models, evaluating multiple layer selection metrics, fine-tuning
  methods, and pruning strategies across 10 datasets and thousands of GPU hours. Key
  findings include: reverse-order pruning (removing final layers) performs as well
  as complex metrics; partial-layer fine-tuning (updating only last few layers and
  lmhead) outperforms LoRA-based fine-tuning in accuracy and training time; and one-shot
  pruning matches iterative pruning while reducing computational costs.'
---

# Reassessing Layer Pruning in LLMs: New Insights and Methods

## Quick Facts
- arXiv ID: 2411.15558
- Source URL: https://arxiv.org/abs/2411.15558
- Reference count: 27
- The paper shows reverse-order pruning and partial-layer fine-tuning outperform complex metrics and LoRA respectively, while one-shot pruning matches iterative pruning in cost and performance.

## Executive Summary
This paper systematically benchmarks layer pruning techniques for large language models across 10 datasets and thousands of GPU hours. The authors evaluate multiple layer selection metrics, fine-tuning methods, and pruning strategies, discovering that simple reverse-order pruning (removing final layers) performs as well as complex metrics. They find that partial-layer fine-tuning (updating only last few layers and lm_head) outperforms LoRA-based fine-tuning in both accuracy and training time, and that one-shot pruning matches iterative pruning while reducing computational costs. Applying these insights, they create pruned Llama-3.1-8B-Instruct models that achieve performance comparable to or better than many popular 6-7B models while using up to 106× fewer training tokens.

## Method Summary
The paper conducts comprehensive experiments on layer pruning for LLMs, testing multiple layer selection metrics including reverse-order, Taylor, and L2-norm methods. They evaluate pruning rates from 12.5% to 37.5% across models including Llama-3.1-8B, Gemma2-2B, Vicuna-7B, and Qwen1.5-7B. The study compares three fine-tuning approaches (LoRA, partial-layer fine-tuning, and QLoRA) and two pruning strategies (one-shot vs iterative). Extensive benchmarking is performed across 10 datasets including Alpaca, Dolly, MMLU, CMMLU, and ARC-c, with thousands of GPU hours invested in validation. The authors release their pruned models Llama-3.1-6.3B-It-Alpaca and Llama-3.1-6.3B-It-Dolly on HuggingFace.

## Key Results
- Reverse-order pruning (removing final 25% of layers) consistently matches or exceeds performance of complex layer selection metrics across multiple datasets
- Partial-layer fine-tuning (updating only last few layers and lm_head) outperforms LoRA-based fine-tuning in both accuracy and training time
- One-shot pruning achieves equivalent performance to iterative pruning while reducing computational costs by up to 75%
- Pruned Llama-3.1-6.3B models achieve performance comparable to or better than many popular 6-7B models while using up to 106× fewer training tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reverse-order pruning is as effective as complex metrics because later layers capture more task-specific representations that can be discarded without harming generalization.
- Mechanism: In LLMs, initial layers capture general linguistic features while deeper layers encode specialized, task-specific transformations. Removing the last 25% of layers trims redundant specialization while preserving general knowledge.
- Core assumption: Later layers contribute less to cross-task generalization than earlier layers, making them more disposable.
- Evidence anchors:
  - [abstract] "simple approach, i.e., pruning the final 25% of layers followed by fine-tuning the lm head and the remaining last three layer, yields remarkably strong performance"
  - [section 4.1] "reverse-order metric delivers stable and superior results across various models under the 25% pruning rate, making it a reliable choice for pruning"
  - [corpus] Weak - corpus shows many papers explore complex metrics, but no direct evidence comparing reverse-order to sophisticated methods in terms of generalization preservation.
- Break condition: If task-specific fine-tuning requires deep layer representations, reverse-order pruning will degrade performance significantly.

### Mechanism 2
- Claim: Partial-layer fine-tuning outperforms LoRA because it adapts the pruned model's parameter distribution more effectively than low-rank updates.
- Mechanism: After pruning, the remaining layer weights are no longer optimized for the full architecture. Partial-layer fine-tuning allows these layers to recalibrate to the new depth, while LoRA applies low-rank adjustments that may not fully capture the structural shift.
- Core assumption: The pruned architecture creates a new parameter distribution that benefits from direct fine-tuning rather than external low-rank modifications.
- Evidence anchors:
  - [abstract] "partial-layer fine-tuning (updating only last few layers and lm_head) outperforms LoRA-based fine-tuning in accuracy and training time"
  - [section 4.2] "partial-layer fine-tuning is significantly better than LoRA, providing a viable new direction for fine-tuning models after pruning"
  - [corpus] Weak - corpus lacks direct comparisons of LoRA vs. partial-layer fine-tuning specifically in post-pruning scenarios.
- Break condition: If memory constraints prevent updating large parameter sets, partial-layer fine-tuning becomes infeasible.

### Mechanism 3
- Claim: One-shot pruning matches iterative pruning because the performance impact of removing individual layers is predictable and doesn't require gradual adjustment.
- Mechanism: Each layer's contribution to model performance is relatively independent, so removing multiple layers at once doesn't create compounding negative effects that iterative pruning would avoid.
- Core assumption: Layer importance is stable enough that removing multiple layers simultaneously doesn't cause catastrophic forgetting or performance collapse.
- Evidence anchors:
  - [abstract] "one-shot pruning matches iterative pruning while reducing computational costs"
  - [section 4.3] "iterative approach for LLMs may not provide the same benefits and can even lead to performance degradation"
  - [corpus] Weak - corpus doesn't provide evidence about layer independence or the predictability of multi-layer removal effects.
- Break condition: If layer interactions are highly nonlinear, one-shot pruning could remove synergistic layers that iterative pruning would preserve.

## Foundational Learning

- Concept: Layer importance metrics and their relationship to model generalization
  - Why needed here: Understanding why reverse-order works requires grasping how different layers contribute to generalization versus task-specific performance.
  - Quick check question: If early layers capture general features and later layers capture task-specific features, which would you expect to be more important for zero-shot generalization?

- Concept: Parameter-efficient fine-tuning vs. full fine-tuning tradeoffs
  - Why needed here: The paper's core finding is that traditional PEFT methods (LoRA) underperform compared to partial-layer fine-tuning, requiring understanding of when each approach is appropriate.
  - Quick check question: What's the key architectural difference between LoRA and partial-layer fine-tuning that might explain their different performance after pruning?

- Concept: Catastrophic forgetting and its relationship to iterative vs. one-shot pruning
  - Why needed here: The paper's finding that iterative pruning offers no benefit contradicts traditional understanding of gradual adaptation, requiring understanding of when catastrophic forgetting occurs.
  - Quick check question: Why might iterative pruning cause catastrophic forgetting in LLMs when it works well for traditional CNNs?

## Architecture Onboarding

- Component map: Layer selection metrics (reverse-order, Taylor, L2-norm) -> Pruning execution (one-shot vs iterative) -> Fine-tuning methods (LoRA, partial-layer, QLoRA) -> Evaluation on benchmark datasets
- Critical path: Select layers -> prune model -> fine-tune pruned model -> evaluate on benchmark datasets -> compare against baselines
- Design tradeoffs: Simple metrics (reverse-order) vs. complex metrics (computational efficiency vs. potential accuracy gains); LoRA vs. partial-layer fine-tuning (memory efficiency vs. performance); one-shot vs. iterative pruning (training cost vs. potential performance benefits)
- Failure signatures: Poor performance on specific datasets (especially MMLU, CMMLU, ARC-c) indicates sensitivity to layer removal; memory constraints during fine-tuning suggest LoRA may be necessary despite lower performance; inconsistent results across pruning runs suggest calibration sample sensitivity
- First 3 experiments:
  1. Implement reverse-order pruning on Llama-3.1-8B-Instruct, removing 8 layers, then fine-tune using partial-layer fine-tuning (lm_head + last 3 layers) on Alpaca-cleaned dataset
  2. Compare reverse-order pruning performance against Taylor metric on the same model and fine-tuning approach to validate simplicity advantage
  3. Test one-shot vs. iterative pruning (4-step) on Llama-3.1-8B-Instruct using reverse-order metric to confirm computational efficiency without performance loss

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of pruned models vary when using different fine-tuning datasets across multiple pruning rates?
- Basis in paper: [explicit] The paper notes that fine-tuning with different SFT datasets significantly impacts pruned model performance and suggests further exploration of suitable datasets for fine-tuning pruned models.
- Why unresolved: The study only tested two datasets (Alpaca-cleaned and Dolly-15k) at a single pruning rate (25%), leaving uncertainty about how dataset choice interacts with pruning severity.
- What evidence would resolve it: Systematic experiments comparing multiple SFT datasets (e.g., MMLU, Dolly-15k, OpenWebText) across varying pruning rates (e.g., 10%, 25%, 50%) to identify optimal dataset-pruning rate combinations.

### Open Question 2
- Question: Does iterative pruning become beneficial at lower pruning rates or with different fine-tuning strategies?
- Basis in paper: [explicit] The paper found iterative pruning offers no benefit at 25% pruning rate but acknowledges this may be specific to the tested conditions.
- Why unresolved: The study only tested iterative pruning at 25% rate with LoRA and partial-layer fine-tuning, not exploring whether benefits emerge at smaller pruning rates or with other fine-tuning methods.
- What evidence would resolve it: Experiments testing iterative pruning at various rates (e.g., 10%, 15%, 20%) with different fine-tuning methods (e.g., LoRA variants, full fine-tuning) to identify conditions where iterative approaches might excel.

### Open Question 3
- Question: How do layer pruning techniques perform on larger model families beyond Llama and Gemma architectures?
- Basis in paper: [explicit] The study focused on Llama-3.1-8B, Gemma2-2B, Vicuna-7B, and Qwen1.5-7B, leaving uncertainty about generalizability to other architectures.
- Why unresolved: The research only benchmarked four model families, potentially missing architectural factors that influence pruning effectiveness.
- What evidence would resolve it: Comparative pruning experiments across diverse architectures (e.g., Mistral, Falcon, Yi) to identify architectural characteristics that affect pruning outcomes.

## Limitations
- Findings primarily validated on Llama family architectures, limiting generalizability to other model families
- Evaluation focused on reasoning and general knowledge tasks, with limited coverage of specialized domains like code generation or multilingual understanding
- The 106× training token reduction claim is based on relative comparison to Alpaca's training process, which may not reflect typical fine-tuning costs

## Confidence

**High Confidence:**
- Reverse-order pruning consistently matches or exceeds performance of complex metrics across multiple datasets
- One-shot pruning achieves equivalent performance to iterative pruning while reducing computational costs
- Pruned Llama-3.1-6.3B models achieve competitive performance with popular 6-7B models despite 25% parameter reduction

**Medium Confidence:**
- Partial-layer fine-tuning consistently outperforms LoRA-based approaches in both accuracy and training time
- The performance degradation on MMLU, CMMLU, and ARC-c datasets represents a systematic limitation of layer pruning
- The claimed 106× reduction in training tokens is based on relative comparison to Alpaca's training process

**Low Confidence:**
- The mechanism explanation that later layers encode more task-specific representations that can be discarded without harming generalization
- The assertion that partial-layer fine-tuning adapts pruned architectures more effectively than LoRA in all scenarios
- The claim that layer independence explains why one-shot pruning matches iterative pruning across all model scales and tasks

## Next Checks

1. **Architecture Generalization Test**: Apply the reverse-order pruning and partial-layer fine-tuning methodology to non-Llama model families (e.g., Mistral, Gemma, or custom transformer variants) to verify the universality of the findings beyond the Llama architecture.

2. **Domain-Specific Performance Evaluation**: Evaluate the pruned models on specialized benchmarks including code generation (HumanEval), multilingual tasks (XNLI), and long-context understanding (PG-19 or similar) to identify domain-specific limitations not captured in the current reasoning-focused evaluation suite.

3. **Iterative vs. One-Shot Sensitivity Analysis**: Conduct a systematic study varying the number of layers removed (10%, 25%, 40%) to determine whether the equivalence between iterative and one-shot pruning holds across different pruning magnitudes, and identify the threshold where layer interactions become significant enough to favor iterative approaches.