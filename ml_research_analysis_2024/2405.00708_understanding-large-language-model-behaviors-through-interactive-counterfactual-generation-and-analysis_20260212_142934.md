---
ver: rpa2
title: Understanding Large Language Model Behaviors through Interactive Counterfactual
  Generation and Analysis
arxiv_id: '2405.00708'
source_url: https://arxiv.org/abs/2405.00708
tags:
- requires
- users
- segments
- counterfactuals
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces LLM Analyzer, an interactive visualization\
  \ system for understanding large language model (LLM) behaviors through counterfactual\
  \ analysis. The system addresses the limitations of existing static, word-level\
  \ explanations by enabling users to actively generate and explore counterfactuals\u2014\
  meaningful text variations\u2014to test hypotheses and iteratively refine their\
  \ mental model of the LLM's behavior."
---

# Understanding Large Language Model Behaviors through Interactive Counterfactual Generation and Analysis

## Quick Facts
- arXiv ID: 2405.00708
- Source URL: https://arxiv.org/abs/2405.00708
- Authors: Furui Cheng; Vilém Zouhar; Robin Shing Moon Chan; Daniel Fürst; Hendrik Strobelt; Mennatallah El-Assady
- Reference count: 40
- Primary result: Introduces LLM Analyzer, an interactive visualization system for understanding LLM behaviors through counterfactual analysis with >95% grammaticality across five datasets

## Executive Summary
This paper introduces LLM Analyzer, an interactive visualization system designed to help users understand large language model (LLM) behaviors through counterfactual analysis. Unlike traditional static explanations, the system enables active exploration by generating and analyzing text variations (counterfactuals) that probe how changes in input affect LLM outputs. The approach combines dependency-based text segmentation, counterfactual generation, KernelSHAP attribution, and interactive visualization to support users in answering explanatory questions about LLM decision-making processes.

The system addresses the fundamental challenge that explanation is inherently an interactive and iterative process, where users seek to ask follow-up questions, test new hypotheses, and progressively refine their understanding. By allowing users to generate counterfactuals at different granularities and explore their impact on model predictions, LLM Analyzer provides a more intuitive and effective way to develop mental models of LLM behavior compared to static, word-level explanations.

## Method Summary
The system processes text by first segmenting sentences into interpretable components using dependency parsing, then generating counterfactuals by removing or replacing these segments according to grammatical rules. These counterfactuals are used to compute feature attribution scores via KernelSHAP, which estimates how each text segment influences the model's prediction outcome. The results are visualized in a table-based interface that allows users to explore individual and joint segment influences, compare different counterfactual examples, and iteratively refine their understanding of the LLM's behavior through active interaction.

## Key Results
- Achieved >95% grammaticality rate across five diverse datasets (MedQA, BillSum, FiQA, TinyTextbooks, MultiNews)
- Generated approximately 46 counterfactuals per sentence with successful attribution computation in ~0.7 seconds per sentence on a laptop
- User study with eight participants and expert interviews demonstrated the system's usability and effectiveness
- Users appreciated the system's ability to answer why, what-if, how to be that, and how to still be this questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interactive counterfactual generation allows users to iteratively test hypotheses about LLM behavior, leading to better mental models
- Mechanism: Users generate text perturbations at different granularities, observe LLM outputs, and refine understanding through direct interaction rather than static explanations
- Core assumption: Human understanding improves through active exploration and hypothesis testing rather than passive receipt of static explanations
- Evidence anchors: [abstract]: "enables users to actively engage in the analysis process by exploring the counterfactuals-variations of a target input crafted to understand the model's prediction under a what-if scenario"; [section]: "explanation is inherently an interactive and iterative process: users often seek to ask follow-up questions, test new hypotheses about model behavior, and progressively refine their understanding"

### Mechanism 2
- Claim: Dependency-based text segmentation produces grammatically correct counterfactuals while maintaining semantic coherence
- Mechanism: The system uses dependency parsing to identify which words can be removed without breaking grammatical structure, grouping words into segments based on their removability and dependency relationships
- Core assumption: Grammatical correctness of counterfactuals is essential for meaningful interpretation and that dependency relationships accurately capture removability
- Evidence anchors: [section]: "Building on this idea, we propose a computational pipeline that segments the input sentence into interpretable components, forming a simplified representation of the sentence as a binary vector"; [section]: "Using the segments derived in the previous step, along with their correspondence relationships, we select meaningful counterfactuals from all possible combinations of these segments using three rules"

### Mechanism 3
- Claim: KernelSHAP attribution scores computed from counterfactual outcomes provide interpretable feature importance for understanding LLM behavior
- Mechanism: The system generates multiple counterfactuals, computes LLM outputs for each, and uses KernelSHAP to estimate how each text segment influences the prediction outcome
- Core assumption: Shapley values computed from perturbation outcomes accurately represent feature importance in the LLM's decision-making process
- Evidence anchors: [abstract]: "These counterfactuals are used to compute feature attribution scores, which are then integrated with concrete examples in a table-based visualization"; [section]: "Using the KernelSHAP aggregation [21]. The resulting attribution scores are visualized to support user exploration and understanding"

## Foundational Learning

- Concept: Dependency parsing and syntactic structure
  - Why needed here: Essential for identifying which words can be removed while maintaining grammatical correctness in counterfactual generation
  - Quick check question: Can you identify which words in a sentence depend on others for grammatical correctness?

- Concept: Shapley values and cooperative game theory
  - Why needed here: Forms the theoretical foundation for computing feature attributions from perturbation outcomes
  - Quick check question: Do you understand why Shapley values fairly distribute contributions among players (features) in a cooperative game?

- Concept: Counterfactual reasoning and causal inference
  - Why needed here: Provides the conceptual framework for understanding how changes in input lead to changes in output
  - Quick check question: Can you explain the difference between correlation and causation in the context of feature importance?

## Architecture Onboarding

- Component map: Input processing layer (Text segmentation algorithm) -> Counterfactual generation engine (Rules-based perturbation system) -> LLM interaction module (Query interface and response handler) -> Attribution computation (KernelSHAP implementation) -> Visualization layer (Table-based interface with interactive features) -> User interaction handlers (Experiment panel and configuration management)

- Critical path: User inputs prototype text and evaluator -> Text segmentation algorithm processes input -> Counterfactual generation creates perturbations -> LLM processes each counterfactual -> KernelSHAP computes attributions -> Results visualized in table interface -> User interacts with visualization to explore findings

- Design tradeoffs: Granularity vs. interpretability (finer segments provide more precise attribution but may overwhelm users); Computation time vs. attribution quality (more counterfactuals improve attribution accuracy but increase processing time); Static vs. dynamic explanations (interactive system provides flexibility but requires more user effort than automated explanations)

- Failure signatures: High grammaticality rate (>95%) but low diversity of counterfactuals suggests overly restrictive segmentation rules; Attribution scores show high variance across similar perturbations indicates insufficient sampling; Users report difficulty interpreting results suggests poor visualization design or complex attribution interpretation

- First 3 experiments: Test segmentation algorithm on sentences from different domains (medical, legal, news) to verify grammaticality and diversity claims; Compare KernelSHAP attribution results with ground truth importance scores on a synthetic dataset where true importance is known; Conduct user study with simple classification tasks to validate usability and effectiveness of the interactive workflow

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM Analyzer scale when analyzing longer, more complex text passages compared to the shorter sentences evaluated in the current study?
- Basis in paper: [inferred] The paper evaluates grammaticality and diversity on shorter sentences (average 13-18 words) across five datasets, but does not explore performance on longer, more complex passages that might be encountered in real-world applications
- Why unresolved: The current experiments focus on relatively short text segments, leaving uncertainty about whether the segmentation algorithm and counterfactual generation approach maintain efficiency and quality when applied to longer documents or more intricate text structures
- What evidence would resolve it: Systematic experiments testing the algorithm on progressively longer text passages (e.g., 50, 100, 500+ words) measuring grammaticality rates, generation diversity, and computational time would clarify scalability limitations

### Open Question 2
- Question: What is the optimal number of evaluators to define for a given analysis task, and how does evaluator complexity affect user comprehension and system performance?
- Basis in paper: [explicit] The paper mentions that "typically, the user only needs to define an evaluator" but also notes that "for more complex tasks, users can define multiple evaluators to handle different aspects of the generation"
- Why unresolved: The paper does not provide guidance on when multiple evaluators are necessary, how they interact with each other, or what trade-offs exist between evaluator simplicity and comprehensiveness in terms of both user cognitive load and computational efficiency
- What evidence would resolve it: Comparative studies testing different numbers and types of evaluators across various task complexities, measuring both user performance in answering explanatory questions and system response times, would identify optimal configurations

### Open Question 3
- Question: How do different dependency parsing algorithms affect the quality and interpretability of generated counterfactuals, and is there an optimal parsing approach for specific text domains?
- Basis in paper: [explicit] The paper states "We built our algorithm on existing dependency parsing algorithms and implementations" and mentions validating rules on the MedQA dataset, but does not compare alternative parsing approaches
- Why unresolved: The paper uses a single dependency parsing approach without exploring whether other parsers (e.g., different models or linguistic frameworks) might produce better segmentations for specific domains like legal text versus medical text
- What evidence would resolve it: Comparative experiments using multiple dependency parsing algorithms across diverse text domains, measuring counterfactual quality metrics (grammaticality, diversity, semantic preservation) would identify optimal parsing strategies for different application areas

### Open Question 4
- Question: How does the interactive counterfactual analysis approach compare to traditional static explanation methods in terms of user understanding and trust calibration when evaluating LLM behaviors?
- Basis in paper: [explicit] The paper positions its interactive approach against "static, word-level explanations" and claims benefits for hypothesis testing and mental model refinement, but does not directly compare user outcomes between interactive and static methods
- Why unresolved: While the paper demonstrates the system's usability and usefulness, it does not empirically measure whether interactive counterfactual analysis leads to better understanding or more accurate trust calibration compared to providing users with static feature attribution explanations alone
- What evidence would resolve it: Controlled user studies where participants complete identical analysis tasks using either the interactive LLM Analyzer system or traditional static explanations, with pre/post measures of understanding and trust calibration, would quantify the comparative benefits

### Open Question 5
- Question: What are the limitations of logic-based evaluators (e.g., ENTAIL) for open-ended text generation tasks, and how can the system better support evaluator refinement during exploratory analysis?
- Basis in paper: [explicit] The paper identifies that "for free-form text generation tasks, such as creative writing or complex reasoning, defining appropriate evaluators becomes much more challenging" and suggests logic-based evaluators as a solution while acknowledging this remains an "important and open research challenge"
- Why unresolved: The paper acknowledges the difficulty of defining evaluators for open-ended tasks but does not explore specific failure modes of logic-based evaluators or provide concrete mechanisms for supporting iterative evaluator refinement during analysis
- What evidence would resolve it: Systematic analysis of logic-based evaluator performance across different open-ended generation tasks, identifying specific failure patterns, combined with prototype implementations of evaluator refinement interfaces would clarify both limitations and potential solutions

## Limitations

- Corpus evidence quality is weak with only 8 relevant papers providing limited direct support for the core interactive counterfactual analysis mechanisms
- Exact segmentation rules for determining "optional" vs "unremovable" dependency relations are not fully specified and were iteratively refined, raising reproducibility concerns
- Specific evaluator operators and their integration with LLMs for logic-based operations are not detailed, potentially affecting system functionality

## Confidence

**High Confidence**: The system's usability and effectiveness demonstrated through user study and expert interviews, the achievement of >95% grammaticality rate across five datasets, and the generation of ~46 counterfactuals per sentence

**Medium Confidence**: The dependency-based segmentation approach and KernelSHAP attribution methodology are theoretically sound, but lack direct corpus validation for this specific application context

**Low Confidence**: The claims about interactive hypothesis testing improving mental models are primarily supported by theoretical reasoning rather than empirical evidence from the user study or corpus analysis

## Next Checks

1. **Cross-Domain Validation**: Test the segmentation algorithm on sentences from medical, legal, and news domains to verify that the grammaticality and diversity claims hold across different text types, not just the five datasets mentioned

2. **Attribution Accuracy Benchmark**: Create a synthetic dataset where ground truth feature importance is known (e.g., by controlling which words influence the output), then compare KernelSHAP attribution results against this ground truth to validate the attribution quality

3. **Interactive Workflow Effectiveness**: Conduct a controlled user study where participants solve classification tasks using LLM Analyzer versus a static explanation tool, measuring both task accuracy and time to completion to quantify the benefits of interactive exploration