---
ver: rpa2
title: Re-examining learning linear functions in context
arxiv_id: '2411.11465'
source_url: https://arxiv.org/abs/2411.11465
tags:
- values
- training
- linear
- learning
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates in-context learning (ICL) of linear functions
  in small transformer models trained from scratch. The study challenges the claim
  that transformers use algorithmic approaches like linear regression to learn linear
  functions, instead proposing that models memorize boundary values from their training
  data.
---

# Re-examining learning linear functions in context

## Quick Facts
- arXiv ID: 2411.11465
- Source URL: https://arxiv.org/abs/2411.11465
- Reference count: 40
- Key outcome: Transformers learn linear functions through boundary value memorization rather than algorithmic regression

## Executive Summary
This study challenges the prevailing understanding of in-context learning (ICL) in transformers by demonstrating that small models trained from scratch learn linear functions through boundary value memorization rather than algorithmic approaches like linear regression. The research shows that transformer performance on linear function tasks depends critically on whether training and test distributions match, with models exhibiting dramatic performance degradation when distributions shift. Attention layers are found to be both necessary and sufficient for ICL of linear functions, while more layers and heads improve performance. The authors propose a mathematical model explaining ICL as a projection-based method that relies on stored sequences and proximity reasoning rather than regression.

## Method Summary
The study investigates in-context learning of linear functions using synthetic data generated by f(x) = ax + b with coefficients and inputs sampled from various distributions (normal, uniform, bimodal). Transformer models with varying architectures (1-18 layers, 1-8 attention heads) are trained on the ICL objective using Adam optimizer (learning rate 1e-4, batch size 64) for 500k steps. Performance is evaluated using mean squared error and error rate across different training and test distributions, with particular focus on how models perform when test distributions are shifted from training distributions. Attention-only models are compared with full transformer models to assess the sufficiency of attention mechanisms.

## Key Results
- Models achieve near-zero error when training and test distributions match but performance degrades significantly with distribution shifts
- Attention layers are necessary and sufficient for ICL of linear functions; MLP-only models fail to learn
- Model performance depends on the proportion of training examples within the test interval rather than total number of points
- Boundary values (maximum and minimum outputs seen during training) limit generalization, forcing constant or random outputs outside these bounds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models use boundary value memorization rather than algorithmic linear regression
- Mechanism: During training, models observe maximal and minimal function values and store these as "boundary values" B− and B+. At inference, when predicting f(x), if the true value exceeds B+ or falls below B−, the model outputs approximately B+ or B− respectively, creating a bandpass filter effect.
- Core assumption: The model's behavior is determined by the range of values seen during training, not by understanding the underlying linear relationship.
- Evidence anchors:
  - [abstract] "models can achieve near-zero error when training and test distributions match, but performance degrades significantly when tested on shifted distributions"
  - [section 5] "boundary values B−, B+ are determined by maximal and minimal elements seen during training"
  - [corpus] Weak - neighboring papers focus on general ICL mechanisms rather than this specific memorization hypothesis
- Break condition: If training data is sufficiently diverse to include the full range of test values, or if the model architecture explicitly prevents boundary value formation.

### Mechanism 2
- Claim: Attention layers alone are sufficient for in-context learning of linear functions
- Mechanism: The model uses attention weights to create a projection π that maps inputs to outputs based on stored sequences. More attention heads and layers provide more memory capacity for storing and retrieving similar sequences.
- Core assumption: Attention mechanisms can implement the necessary projection operations without feedforward networks.
- Evidence anchors:
  - [section 4] "Attention layers are necessary and sufficient to ICL1 the class of linear functions. Models with only MLP were unable to ICL1"
  - [section 6] "we hypothesize that the model optimizes a projection π based on stored projections"
  - [corpus] Moderate - neighboring papers discuss attention mechanisms in ICL but not this specific sufficiency claim
- Break condition: If the task complexity exceeds what attention-based projections can handle, or if the sequence length becomes too large for attention memory.

### Mechanism 3
- Claim: Model performance depends on the proportion of training examples within the test interval
- Mechanism: Models trained on distributions with larger variance have better generalization ability but less precision within their training range. The key factor is the proportion of examples from training that fall within the test distribution, not the total number of points observed.
- Core assumption: The model's learned projections are optimized for the specific distribution characteristics of the training data.
- Evidence anchors:
  - [section 4] "Models have better performance over intervals that contain a larger proportion of examples in the training distribution"
  - [section 5] "a consequence of Observation 4 is that error rates depend on the distance of the target function's values from the boundary values"
  - [corpus] Weak - neighboring papers focus on general ICL performance rather than this specific distribution proportion effect
- Break condition: If the test distribution is shifted in a way that breaks the proportion relationship, or if the model architecture can generalize beyond distribution characteristics.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: This paper challenges the prevailing understanding of how transformers perform ICL, specifically for linear functions
  - Quick check question: Can you explain the difference between ICL1 (learning within distribution) and ICL2 (learning the abstract function form)?

- Concept: Transformer attention mechanisms
  - Why needed here: The paper demonstrates that attention layers alone are sufficient for ICL, making understanding attention mechanisms crucial
  - Quick check question: How does the attention mechanism create the "projection" that the authors hypothesize is being learned?

- Concept: Distribution shift effects
  - Why needed here: The paper shows that model performance dramatically degrades when test distributions differ from training distributions
  - Quick check question: What happens to model predictions when f(x) falls outside the boundary values established during training?

## Architecture Onboarding

- Component map: Embedding layer -> L layers (each with H attention heads) -> Output layer
- Critical path: Training → Boundary value formation → Projection learning via attention → Inference with boundary value constraints
- Design tradeoffs: More layers/heads improve performance but increase computational cost; wider training distributions improve generalization but reduce precision
- Failure signatures: Constant or random outputs outside boundary values, dramatic performance degradation with distribution shift, inability to generalize the linear function form
- First 3 experiments:
  1. Train a 1L1AH model on N(0,1) and test on N(0,1) to verify baseline ICL1 capability
  2. Train a 12L8AH model on U(-5,5) and test on N(0,σ) for increasing σ to observe boundary value effects
  3. Compare attention-only vs full transformer models on the same task to verify attention sufficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do larger transformer models maintain their ability to perform in-context learning when tested on shifted distributions, or does their performance still degrade significantly outside their training distribution?
- Basis in paper: [explicit] The paper shows that larger models trained on N(0, 10) had better generalization ability than models trained on N(0, 1), but still degraded when tested on shifted distributions.
- Why unresolved: The paper only tested models up to 18 layers with 8 attention heads. It's unclear if scaling to much larger models (e.g., GPT-3 or GPT-4 sized) would overcome the boundary value limitations.
- What evidence would resolve it: Testing much larger transformer models (hundreds of layers, thousands of attention heads) on the same linear function task with various distribution shifts would determine if scaling helps overcome the generalization limitations.

### Open Question 2
- Question: What is the precise mathematical mechanism by which transformers store and retrieve sequences to perform in-context learning, beyond the proposed projection-based hypothesis?
- Basis in paper: [explicit] The authors propose a projection-based hypothesis but acknowledge they haven't fully shown that transformers implement this algorithm. They suggest probing what models do at various layers.
- Why unresolved: The paper provides observational evidence for boundary values and sequence usage but doesn't provide a complete mechanistic explanation of how transformers actually compute their predictions.
- What evidence would resolve it: Layer-by-layer analysis of transformer attention patterns and weight matrices during inference, combined with ablation studies that selectively disable components, would reveal the precise computational mechanism.

### Open Question 3
- Question: Does the boundary value phenomenon observed in linear function learning generalize to other mathematical or linguistic tasks in transformers?
- Basis in paper: [inferred] The paper focuses specifically on linear functions, but the boundary value concept suggests a fundamental limitation in how transformers handle out-of-distribution examples.
- Why unresolved: The paper only examines one specific task domain. It's unclear whether similar memorization and boundary effects occur in tasks like arithmetic, logical reasoning, or language understanding.
- What evidence would resolve it: Testing transformers on various mathematical tasks (polynomial functions, logical operations) and linguistic tasks (text completion, reasoning) while analyzing their performance on out-of-distribution examples would determine if boundary values are a general phenomenon.

## Limitations

- Focus on synthetic linear functions may not generalize to more complex in-context learning tasks
- Boundary value hypothesis remains untested for non-linear relationships
- Analysis relies heavily on controlled synthetic data distributions, potentially missing real-world data complexities

## Confidence

**High Confidence Claims:**
- Models exhibit boundary value effects when training and test distributions mismatch
- Attention layers are necessary and sufficient for ICL1 of linear functions (based on empirical ablation)
- More layers/heads improve performance in a predictable manner

**Medium Confidence Claims:**
- The projection-based mathematical model accurately describes the mechanism
- Distribution proportion effects are the primary driver of generalization performance
- Boundary value memorization is the dominant mechanism over algorithmic approaches

**Low Confidence Claims:**
- Generalization to non-linear functions would follow similar boundary value patterns
- The observed effects would persist in larger models or with real-world data
- Attention sufficiency extends to more complex ICL tasks beyond linear functions

## Next Checks

1. **Non-linear Function Validation**: Test the boundary value hypothesis with quadratic and exponential functions to determine if the mechanism generalizes beyond linear relationships.

2. **Architecture Ablation Extension**: Systematically remove attention components (e.g., key/query transformations) to identify which specific attention mechanisms are essential versus sufficient for ICL1.

3. **Distribution Shift Robustness**: Design experiments with gradual distribution shifts rather than binary shifts to measure the smooth degradation of performance and better characterize the boundary value effect's strength.