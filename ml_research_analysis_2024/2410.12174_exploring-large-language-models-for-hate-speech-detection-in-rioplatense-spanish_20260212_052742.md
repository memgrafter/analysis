---
ver: rpa2
title: Exploring Large Language Models for Hate Speech Detection in Rioplatense Spanish
arxiv_id: '2410.12174'
source_url: https://arxiv.org/abs/2410.12174
tags:
- speech
- hate
- language
- spanish
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work evaluates the performance of large language models in
  detecting hate speech in Rioplatense Spanish, focusing on four categories: misogyny,
  homophobia/transphobia, racism/xenophobia, and classism. The study compares three
  LLMs (GPT-3.5, Mixtral, and Aya) against a fine-tuned BERT classifier using a corpus
  of annotated tweets and news comments.'
---

# Exploring Large Language Models for Hate Speech Detection in Rioplatense Spanish

## Quick Facts
- arXiv ID: 2410.12174
- Source URL: https://arxiv.org/abs/2410.12174
- Reference count: 15
- Large language models show higher recall but lower precision than fine-tuned BERT for detecting hate speech in Rioplatense Spanish

## Executive Summary
This study evaluates the effectiveness of large language models (LLMs) versus traditional machine learning approaches for detecting hate speech in Rioplatense Spanish. The research focuses on four categories of hate speech: misogyny, homophobia/transphobia, racism/xenophobia, and classism. Through comparative analysis of GPT-3.5, Mixtral, and Aya against a fine-tuned BERT classifier, the study reveals important tradeoffs between precision and recall in automated hate speech detection systems.

The findings demonstrate that while fine-tuned BERT classifiers maintain superior precision and overall F1-scores, LLMs exhibit higher recall rates, particularly for nuanced cases of hate speech. This suggests that LLMs may be better at capturing subtle forms of discriminatory language, though they tend to generate more false positives. The study highlights the potential of LLMs in understanding regional linguistic variations while emphasizing the need for careful evaluation of their performance characteristics in sensitive classification tasks.

## Method Summary
The study compares three large language models (GPT-3.5, Mixtral, and Aya) against a fine-tuned BERT classifier for detecting hate speech in Rioplatense Spanish. The evaluation uses a corpus of annotated tweets and news comments across four hate speech categories: misogyny, homophobia/transphobia, racism/xenophobia, and classism. The LLMs are evaluated using zero-shot prompting approaches, while BERT is fine-tuned on the same dataset. Performance metrics including precision, recall, and F1-scores are calculated for each model and hate speech category to identify strengths and weaknesses in their classification capabilities.

## Key Results
- BERT outperforms LLMs in precision and overall F1-score across all hate speech categories
- LLMs demonstrate higher recall rates, particularly for nuanced cases like homophobic/transphobic hate speech
- GPT-3.5 shows the best balance between precision and recall among the LLMs, outperforming BERT in detecting LGBTI-related hate speech
- All models exhibit higher false positive rates for homophobic/transphobic content compared to other hate speech categories

## Why This Works (Mechanism)
The mechanism behind LLM performance in hate speech detection appears to stem from their ability to capture contextual nuances and regional linguistic variations. LLMs leverage their pretraining on diverse text corpora to recognize subtle hate speech patterns that may not be explicitly represented in smaller, domain-specific training datasets. Their transformer architecture allows for attention-based modeling of long-range dependencies and context, enabling better detection of implicit or coded hate speech expressions that traditional classifiers might miss.

## Foundational Learning
- Hate speech detection fundamentals: Why needed - Establishes baseline understanding of classification challenges; Quick check - Can you explain the precision-recall tradeoff in this context?
- Rioplatense Spanish linguistic features: Why needed - Regional variations affect model performance; Quick check - Identify three key dialectal differences from standard Spanish
- Zero-shot vs. fine-tuned learning: Why needed - Critical for understanding model comparison methodology; Quick check - Can you articulate the fundamental difference in how these approaches learn?
- Transformer architecture basics: Why needed - Core to understanding both BERT and LLM capabilities; Quick check - Explain attention mechanism in one sentence

## Architecture Onboarding
- Component map: Raw text -> Tokenizer -> Embedding layer -> Transformer blocks -> Classification head -> Output labels
- Critical path: Input text → Prompt engineering (LLMs) or fine-tuning (BERT) → Feature extraction → Classification → Performance metrics
- Design tradeoffs: Zero-shot learning (LLMs) offers flexibility but lower precision vs. supervised learning (BERT) offers higher precision but requires labeled data
- Failure signatures: High false positives indicate over-sensitivity to context; Low recall suggests missing subtle hate speech patterns
- First experiments: 1) Test prompt sensitivity across multiple formulations, 2) Evaluate cross-domain performance on different text sources, 3) Conduct ablation studies removing specific hate speech categories

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single regional Spanish variant, limiting generalizability
- Small sample sizes per hate speech category (13-108 examples) reduce statistical reliability
- Fundamental methodological difference between zero-shot LLM evaluation and fine-tuned BERT creates apples-to-oranges comparison
- Limited qualitative analysis of misclassification patterns and false positive types

## Confidence
- Methodology: Medium - Valid approach but methodological inconsistency between model types
- Results: Medium - Statistically significant but limited sample sizes
- Generalizability: Low - Single dialect and text source restricts broader applicability
- Reproducibility: Medium - Clear methodology but dependent on specific prompt formulations

## Next Checks
1. Conduct cross-validation across multiple Spanish dialects and text sources to assess generalizability of findings
2. Perform systematic prompt engineering studies with LLMs to quantify performance variability and identify optimal prompting strategies
3. Implement human evaluation studies comparing LLM and BERT classifications to better understand the nature of false positives and negatives, particularly for nuanced cases like homophobic/transphobic speech