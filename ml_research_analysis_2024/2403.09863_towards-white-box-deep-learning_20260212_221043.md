---
ver: rpa2
title: Towards White Box Deep Learning
arxiv_id: '2403.09863'
source_url: https://arxiv.org/abs/2403.09863
tags:
- features
- semantic
- adversarial
- layer
- affine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces semantic features as a general architectural
  solution to make deep neural networks inherently interpretable and adversarially
  robust. The key idea is to make features locality-sensitive in the adequate semantic
  topology of the domain, introducing strong regularization.
---

# Towards White Box Deep Learning

## Quick Facts
- arXiv ID: 2403.09863
- Source URL: https://arxiv.org/abs/2403.09863
- Reference count: 40
- One-line primary result: Introduces semantic features as a general architectural solution for interpretable and adversarially robust deep neural networks, demonstrating strong results on MNIST digit classification

## Executive Summary
This paper introduces semantic features as a novel architectural approach to make deep neural networks inherently interpretable and adversarially robust. The key innovation is making features locality-sensitive in the domain's semantic topology, introducing strong regularization that constrains learning to semantically meaningful transformations. A lightweight white box neural network using semantic features is demonstrated on MNIST digit classification, achieving nearly human-level adversarial accuracy (92%) without adversarial training while maintaining 99.5% clean accuracy. The inherent interpretability enables architectural defenses against adversarial attacks, representing a promising direction for reconciling deep learning with feature engineering principles.

## Method Summary
The method introduces semantic features that are defined as tuples of base vectors and their semantically meaningful transformations. The framework includes several specialized layers: a Two Step Layer that performs pixel-wise thresholding to group intensities into abstract values (ON, OFF, MEH), a Convolutional Semantic Layer that matches rotated filters against pixel neighborhoods to detect "bright lines", an Affine Layer that matches localized shapes robust to small perturbations, and a Logical Layer that makes classification decisions based on the presence or absence of specific shapes. The model is trained using standard cross-entropy loss with Adam optimizer and Gaussian noise augmentation for 15 epochs on a binary MNIST subset (digits "3" vs "5").

## Key Results
- Achieved 99.5% clean test accuracy and 92% adversarial accuracy under AutoAttack on binary MNIST classification
- Demonstrated human-level interpretability with features that capture core characteristics of real-world objects
- Showed that white box nature enables architectural defenses against adversarial attacks without requiring adversarial training
- Established proof-of-concept that semantic features can reconcile deep learning with feature engineering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic features introduce strong regularization by constraining learning to semantically meaningful transformations
- Mechanism: By encoding domain-specific locality functions (like affine transformations), the model learns features capturing core object characteristics rather than spurious correlations
- Core assumption: Chosen locality function adequately captures semantic topology of the domain
- Evidence anchors:
  - [abstract]: "The main idea is to make features locality-sensitive in the adequate semantic topology of the domain, thus introducing a strong regularization."
  - [section]: "The framework of semantic features allows to do this in a pretty natural way."
  - [corpus]: Weak - novel contribution with no direct discussion of locality-sensitive features in corpus

### Mechanism 2
- Claim: White box nature enables architectural defenses against adversarial attacks
- Mechanism: Interpretable features allow identification and mitigation of vulnerabilities through architectural modifications
- Core assumption: Interpretability is sufficient to understand and address adversarial vulnerabilities
- Evidence anchors:
  - [abstract]: "The inherent interpretability allows designing a strong architectural defense against adversarial attacks."
  - [section]: "Its white box nature makes it easy to understand where this vulnerability comes from and to fix it."
  - [corpus]: Weak - novel approach not discussed in corpus on adversarial defenses

### Mechanism 3
- Claim: Semantic features enable controlled dimensionality reduction within layers
- Mechanism: Features defined as tuples of base vectors and transformations focus model on relevant data aspects, reducing effective dimensionality
- Core assumption: Semantic topology defined by locality function captures essential information
- Evidence anchors:
  - [abstract]: "The main idea is to make features locality-sensitive in the adequate semantic topology of the domain, thus introducing a strong regularization."
  - [section]: "The intention is to learn both f and P while defining L explicitly as an inductive bias for the given modality."
  - [corpus]: Weak - novel contribution not directly addressed in corpus

## Foundational Learning

- Concept: Affine transformations and their properties
  - Why needed here: Understanding how affine transformations preserve semantic identity is crucial for designing robust semantic features
  - Quick check question: What are the key properties of affine transformations that make them suitable for capturing semantic locality in image data?

- Concept: Local inhibition and XOR gates in neural networks
  - Why needed here: The matching mechanism involves local inhibition, which functions as a form of XOR gate over feature poses
  - Quick check question: How does local inhibition relate to the matching mechanism of semantic features and its role in capturing semantic locality?

- Concept: Inductive bias and its role in deep learning
  - Why needed here: Semantic features introduce strong inductive bias by explicitly encoding domain-specific knowledge
  - Quick check question: How does the inductive bias from semantic features differ from implicit bias in standard deep networks and what are the implications?

## Architecture Onboarding

- Component map: Input → Two Step Layer → Convolutional Semantic Layer → Affine Layer → Logical Layer → Output
- Critical path: The Two Step Layer is critical for handling pixel intensity variations and improving adversarial robustness
- Design tradeoffs: Simplicity vs. expressiveness (semantic features simplify architecture but may limit complex pattern capture), Interpretability vs. performance (white box nature improves interpretability but may slightly reduce performance)
- Failure signatures: Poor adversarial robustness (indicates inadequate locality function), Low interpretability (suggests ineffective feature capture), Degraded performance (may indicate too restrictive semantic topology)
- First 3 experiments:
  1. Train model on binary MNIST (digits "3" vs "5") and evaluate adversarial robustness under AutoAttack
  2. Visualize learned semantic features to assess interpretability and alignment with human understanding
  3. Perform ablation study by removing or modifying individual layers to understand their contribution to performance and robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do semantic features perform on larger, more complex datasets beyond MNIST, such as CIFAR-10 or ImageNet?
- Basis in paper: [inferred] Paper suggests preliminary CIFAR-10 results seem promising but require careful engineering of locality function
- Why unresolved: Paper focuses on MNIST as proof of concept with only brief mention of preliminary CIFAR-10 results
- What evidence would resolve it: Experiments on larger datasets like CIFAR-10 or ImageNet comparing semantic features to traditional deep learning methods

### Open Question 2
- Question: Can the locality function for semantic features be learned automatically rather than manually engineered?
- Basis in paper: [explicit] Paper states semantic features require locality engineering and manual engineering for harder datasets
- Why unresolved: Paper does not explore methods for automatically learning locality function
- What evidence would resolve it: Developing and testing automatic locality function learning methods (meta-learning, neural architecture search) on various datasets

### Open Question 3
- Question: How do semantic features compare to other approaches to improving adversarial robustness?
- Basis in paper: [inferred] Paper mentions existing robustness methods but does not provide direct comparison to semantic features
- Why unresolved: Paper focuses on introducing semantic features without detailed comparison to other methods
- What evidence would resolve it: Experiments comparing semantic features to adversarial training and certified robustness on same datasets

## Limitations
- Scalability concerns: Limited validation beyond MNIST binary classification to more complex multi-class or real-world problems
- Locality function engineering: Requires manual design of domain-specific locality functions, which may not generalize well
- Computational overhead: The structured approach may introduce additional computational costs compared to standard deep learning methods

## Confidence
- Adversarial robustness claims: Medium - strong empirical results on MNIST but limited validation on more complex datasets
- Interpretability claims: Medium - demonstrated interpretability on simple task but unclear if scales to complex problems
- General applicability claims: Low - proof-of-concept demonstrates concept but extensive validation on diverse tasks needed

## Next Checks
1. Evaluate semantic features approach on multi-class MNIST (10 digits) and Fashion-MNIST to assess scalability and generalization
2. Apply approach to real-world image classification dataset (e.g., CIFAR-10) to test effectiveness on complex visual patterns and higher-resolution images
3. Conduct systematic ablation study to quantify contribution of each semantic feature layer and assess impact of different locality functions on performance and interpretability