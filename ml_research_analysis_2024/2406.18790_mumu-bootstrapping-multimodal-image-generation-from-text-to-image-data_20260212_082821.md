---
ver: rpa2
title: 'MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data'
arxiv_id: '2406.18790'
source_url: https://arxiv.org/abs/2406.18790
tags:
- image
- mumu
- arxiv
- multimodal
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MUMU, a multimodal image generation model that
  uses interleaved text and image prompts (e.g., "a <picture of a man man and his
  <picture of a dog dog in an <picture of a cartoon animated style"). The authors
  bootstrap a multimodal training dataset by extracting semantically meaningful image
  crops from text-image data using open vocabulary object detection, then interleaving
  these crops with corresponding words in captions.
---

# MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data

## Quick Facts
- arXiv ID: 2406.18790
- Source URL: https://arxiv.org/abs/2406.18790
- Authors: William Berman; Alexander Peysakhovich
- Reference count: 40
- Key outcome: Multimodal image generation model trained on bootstrapped dataset using object detection crops from text-image data

## Executive Summary
This paper presents MUMU, a multimodal image generation model that enables composition of text and images through interleaved prompts (e.g., "a <picture of a man> man and his <picture of a dog> dog"). The authors bootstrap a multimodal training dataset by extracting semantically meaningful image crops from text-image data using open vocabulary object detection, then interleaving these crops with corresponding words in captions. MUMU is built by replacing the CLIP text encoder in SDXL with a modified Idefics2 vision-language model and adding a small adapter transformer. Trained on a single 8xH100 GPU node for 6 days, MUMU learns to compose inputs from different images into coherent outputs despite being trained only on crops from the same image. The model generalizes to tasks like style transfer and character consistency, demonstrating that multimodal models can serve as general-purpose controllers for image generation.

## Method Summary
The method involves bootstrapping a multimodal dataset by extracting image crops using open vocabulary object detection on text-image data captions, then interleaving these crops with corresponding words to create multimodal prompts. MUMU is constructed by replacing the CLIP encoder in SDXL with a modified Idefics2 vision-language model (removing the perceiver transformer and using more tokens per image), adding a small adapter transformer on top of Idefics2's hidden states, and training the entire system using LoRA adapters on 8xH100 GPUs for 6 days.

## Key Results
- MUMU successfully learns to compose inputs from different images into coherent outputs despite being trained only on crops from the same image
- The model generalizes to style transfer and character consistency tasks, demonstrating multimodal understanding
- Trained efficiently using LoRA adapters on a single 8xH100 GPU node for 6 days

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MUMU learns to harmonize conditioning images from different inputs despite being trained only on crops from the same image
- Mechanism: The model learns a generalized representation of visual concepts and their relationships through interleaved multimodal prompts during training, allowing it to compose disparate visual elements coherently at inference time
- Core assumption: The vision-language model encoder can learn to represent both semantic content and style information from image crops, and the diffusion decoder can learn to compose these representations into a coherent output
- Evidence anchors:
  - [abstract]: "Despite being only trained on crops from the same image, MUMU learns to compose inputs from different images into a coherent output"
  - [section]: "MUMU can directly place conditioning images into the generated image. Additionally, despite being only trained on crops from the same input image, MUMU can also harmonize conditioning images from different inputs into a coherent output"
  - [corpus]: Weak evidence - corpus does not contain direct studies of this specific generalization mechanism
- Break condition: If the vision-language model fails to capture sufficient semantic and style information from the image crops, or if the diffusion decoder cannot learn to compose these representations effectively

### Mechanism 2
- Claim: Replacing CLIP with a modified Idefics2 vision-language model improves multimodal understanding and generation quality
- Mechanism: The Idefics2 model, with its vision transformer and large vision-language transformer, can capture richer multimodal representations than CLIP, and the removal of the perceiver transformer allows for more tokens per image, improving detail preservation
- Core assumption: The Idefics2 model, when modified to use more tokens per image, can provide better multimodal representations than CLIP for the task of image generation
- Evidence anchors:
  - [abstract]: "MUMU is composed of a vision-language model encoder with a diffusion decoder"
  - [section]: "We find that removing the perceiver and using more tokens improves image quality with image quality saturating at approximately 1, 000 tokens per image"
  - [corpus]: Weak evidence - corpus does not contain direct comparisons of CLIP vs. Idefics2 for this specific task
- Break condition: If the Idefics2 model does not provide significantly better multimodal representations than CLIP, or if the increased number of tokens does not lead to improved detail preservation

### Mechanism 3
- Claim: The dataset bootstrapping method, using object detection to extract image crops corresponding to words in captions, enables effective multimodal training
- Mechanism: By extracting semantically meaningful image crops and interleaving them with corresponding words in the captions, the model learns to associate visual concepts with their textual descriptions, enabling it to understand and generate images from multimodal prompts
- Core assumption: The object detection model can accurately identify and extract image crops that correspond to the words in the captions, and the model can learn to associate these visual concepts with their textual descriptions
- Evidence anchors:
  - [abstract]: "We bootstrap a multimodal dataset by extracting semantically meaningful image crops corresponding to words in the image captions of synthetically generated and publicly available text-image data"
  - [section]: "We construct a multimodal training set bootstrapped from text-image data. We use open vocabulary object detection to extract image crops corresponding to words in the image captions"
  - [corpus]: Weak evidence - corpus does not contain direct studies of this specific dataset bootstrapping method
- Break condition: If the object detection model fails to accurately identify and extract relevant image crops, or if the model cannot learn to effectively associate visual concepts with their textual descriptions

## Foundational Learning

- Concept: Vision-Language Models (VLMs)
  - Why needed here: VLMs are the core component of MUMU, replacing the CLIP text encoder in SDXL. Understanding how VLMs work is crucial for understanding how MUMU processes multimodal inputs.
  - Quick check question: What are the main components of a typical VLM architecture, and how do they process interleaved text and image inputs?

- Concept: Diffusion Models
  - Why needed here: MUMU uses a diffusion decoder, which is based on the SDXL architecture. Understanding how diffusion models work is essential for understanding how MUMU generates images from the multimodal representations learned by the VLM.
  - Quick check question: How do diffusion models generate images iteratively, and what is the role of the denoising process in this generation?

- Concept: Multimodal Prompting
  - Why needed here: MUMU is designed to generate images from multimodal prompts, which interleave text and images. Understanding the concept of multimodal prompting is crucial for understanding how users can interact with MUMU and what types of inputs it can process.
  - Quick check question: What are the advantages of using multimodal prompts over text-only prompts for image generation, and how does MUMU handle these multimodal inputs?

## Architecture Onboarding

- Component map: Modified Idefics2 (vision transformer + large vision-language transformer) -> Adapter transformer -> SDXL UNet (diffusion decoder)

- Critical path:
  1. Image crops and text are interleaved and processed by the modified Idefics2
  2. The adapter transformer further processes the Idefics2 hidden states
  3. The processed hidden states are passed to the SDXL UNet via cross-attention
  4. The UNet generates the final image through iterative denoising

- Design tradeoffs:
  - Using a modified Idefics2 instead of CLIP allows for richer multimodal representations but may require more computational resources
  - Removing the perceiver transformer in Idefics2 increases the number of tokens per image, improving detail preservation but also increasing computational costs
  - Using LoRA adapters instead of full fine-tuning reduces computational requirements but may limit the model's ability to fully adapt to the multimodal task

- Failure signatures:
  - Poor detail preservation in generated images
  - Inability to harmonize disparate conditioning images
  - Artifacts or inconsistencies in the generated images
  - Failure to accurately represent the semantic content of the multimodal prompts

- First 3 experiments:
  1. Test the model's ability to generate images from simple multimodal prompts with a single image crop and corresponding text
  2. Evaluate the model's performance on more complex multimodal prompts with multiple image crops and text
  3. Assess the model's ability to harmonize conditioning images from different sources by providing it with image crops from separate images and observing the coherence of the generated output

## Open Questions the Paper Calls Out
None

## Limitations

- The paper relies primarily on qualitative evaluation through visual inspection rather than quantitative metrics, making it difficult to assess statistical significance of improvements
- The model's performance on real-world data (non-synthetic images) is not thoroughly evaluated, limiting understanding of generalization capabilities
- The computational efficiency claims are based on LoRA training but do not provide runtime comparisons with alternative approaches

## Confidence

- **High Confidence**: The technical feasibility of replacing CLIP with Idefics2 and the basic training methodology are well-established and clearly described
- **Medium Confidence**: The claim that MUMU learns to harmonize conditioning images from different sources is supported by qualitative examples but lacks rigorous quantitative validation
- **Low Confidence**: The assertion that the multimodal training approach is the primary driver of generalization capability, as opposed to other architectural factors

## Next Checks

1. **Quantitative Evaluation Protocol**: Implement automated metrics to measure style transfer accuracy, character consistency, and detail preservation across multiple prompts, comparing MUMU against CLIP-based baselines with statistical significance testing

2. **Ablation Study**: Train variants of the model with different components removed (e.g., the adapter transformer, LoRA adapters, or different vision-language encoders) to isolate which architectural choices contribute most to the observed generalization capability

3. **Cross-Domain Generalization Test**: Evaluate the model's performance on held-out domains not represented in the training data (e.g., medical imaging, satellite imagery) to assess whether the multimodal training approach provides benefits beyond stylistic variations within the same domain