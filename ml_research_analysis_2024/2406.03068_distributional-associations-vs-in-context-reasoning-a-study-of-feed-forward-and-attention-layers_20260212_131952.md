---
ver: rpa2
title: 'Distributional Associations vs In-Context Reasoning: A Study of Feed-forward
  and Attention Layers'
arxiv_id: '2406.03068'
source_url: https://arxiv.org/abs/2406.03068
tags:
- have
- noise
- attention
- training
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how transformer models learn to distinguish
  between distributional associations (e.g., bigrams) and in-context reasoning (e.g.,
  copying from context). Through synthetic experiments, the authors show that feed-forward
  layers tend to learn simple distributional associations like bigrams, while attention
  layers focus on in-context reasoning.
---

# Distributional Associations vs In-Context Reasoning: A Study of Feed-forward and Attention Layers

## Quick Facts
- arXiv ID: 2406.03068
- Source URL: https://arxiv.org/abs/2406.03068
- Authors: Lei Chen, Joan Bruna, Alberto Bietti
- Reference count: 40
- Primary result: Feed-forward layers learn distributional associations while attention layers focus on in-context reasoning; low-rank truncation of feed-forward layers improves reasoning performance

## Executive Summary
This paper investigates the distinct roles of feed-forward and attention layers in transformer models, showing that feed-forward layers tend to learn simple distributional patterns like bigrams, while attention layers focus on in-context reasoning capabilities. Through synthetic experiments and theoretical analysis, the authors demonstrate that gradient noise drives this separation, with feed-forward layers learning distributional associations first while attention layers take longer to develop reasoning capabilities. The work shows that low-rank truncation of feed-forward layers can improve reasoning performance on tasks like indirect object identification, factual recall, and few-shot Chain-of-Thought reasoning on GSM8K.

## Method Summary
The paper uses synthetic data generation with trigger tokens, correct tokens, and generic noise tokens to study how transformer models learn different types of information. A two-layer transformer architecture with both feed-forward and attention layers is trained using SGD optimization. The authors develop LASER (Layer-Selective Rank Reduction) for weight truncation, which applies low-rank approximation to feed-forward layers. Experiments are conducted on Pythia models for IOI and factual recall tasks, and on Phi-3, Llama-3.1-8B, and Llama-3.1-8B-Instruct models for GSM8K reasoning tasks. The theoretical analysis examines gradient noise levels and their differential effects on feed-forward versus attention layer learning.

## Key Results
- Feed-forward layers learn distributional associations (bigrams) while attention layers focus on in-context reasoning
- Low-rank truncation of feed-forward layers improves reasoning performance on IOI and factual recall tasks
- LASER improves few-shot Chain-of-Thought reasoning on GSM8K in 1-2 shot settings, though performance degrades in 8-shot settings
- Theoretical analysis identifies gradient noise as a key factor driving the separation between feed-forward and attention layer learning

## Why This Works (Mechanism)

### Mechanism 1
Feed-forward layers store distributional associations while attention layers focus on in-context reasoning. During training, noise in gradients causes feed-forward layers to learn simple distributional patterns (like bigrams) first, while attention layers take longer to develop in-context reasoning capabilities. The noise in the gradients drives a separation between what feed-forward and attention layers learn.

### Mechanism 2
Low-rank truncation of feed-forward layers improves reasoning by removing distributional associations. When feed-forward layers are truncated, the model loses its ability to rely on simple distributional patterns, forcing attention layers to focus more on in-context reasoning. The distributional associations are localized in low-rank subspaces of feed-forward layers.

### Mechanism 3
Value matrices in attention layers store both in-context and distributional information when feed-forward layers are absent. Without feed-forward layers, attention layers must handle both types of information, storing distributional associations in low-rank subspaces of the value matrix.

## Foundational Learning

- **Gradient noise and its effect on learning dynamics**: Understanding how gradient noise affects the separation between feed-forward and attention layers is central to the paper's theoretical contribution. Quick check: How does gradient noise differentially affect the learning of distributional associations versus in-context reasoning?

- **Low-rank matrix approximation and its impact on model performance**: The paper uses low-rank truncation as a tool to improve reasoning, so understanding this technique is crucial. Quick check: What happens to model performance when we truncate feed-forward layers to different rank levels?

- **Transformer architecture and the distinct roles of feed-forward and attention layers**: The paper's core claim is about the different roles these layers play, so understanding their architecture is essential. Quick check: How do feed-forward and attention layers process information differently in a transformer?

## Architecture Onboarding

- **Component map**: Input tokens → Embeddings → Feed-forward layers (distributional associations) → Attention layers (in-context reasoning) → Output logits
- **Critical path**: For distributional associations: Tokens → Feed-forward layers → Output; For in-context reasoning: Tokens → Attention layers → Output
- **Design tradeoffs**: More feed-forward parameters → Better distributional learning but potentially worse reasoning; More attention parameters → Better reasoning but potentially worse distributional learning; Low-rank truncation → Removes distributional associations but may improve reasoning
- **Failure signatures**: If feed-forward layers are too dominant, model may over-rely on distributional patterns; If attention layers are too dominant, model may struggle with simple distributional patterns; If truncation is too aggressive, model may lose essential information
- **First 3 experiments**: 1) Compare model performance on reasoning tasks with and without feed-forward layer truncation; 2) Analyze gradient noise levels during training for feed-forward vs attention layers; 3) Test how different rank levels of truncation affect reasoning performance

## Open Questions the Paper Calls Out

### Open Question 1
How do distributional associations and in-context reasoning interact in more complex reasoning tasks beyond the simple tasks studied? The paper shows separation in simple tasks but acknowledges this may not extend to complex interactions. Experiments on complex reasoning benchmarks showing how distributional associations interfere with or complement in-context reasoning in multi-step problems would resolve this.

### Open Question 2
What are the optimal architectural allocations between feed-forward and attention layers for balancing distributional knowledge storage versus reasoning capabilities? While the paper observes effects of truncation, it doesn't systematically explore optimal parameter allocation ratios. Systematic study varying MLP/attention parameter ratios across diverse tasks to identify optimal allocations would resolve this.

### Open Question 3
How robust are the findings about distributional vs in-context mechanisms to different data distributions and noise levels? The theoretical analysis assumes uniform distributions and specific noise parameters; experiments use limited data sources. Experiments with non-uniform distributions, varying noise levels, and diverse real-world datasets showing consistency of the distributional/in-context separation would resolve this.

### Open Question 4
Can selective fine-tuning strategies based on distributional/in-context separation improve reasoning performance more effectively than general fine-tuning? The paper only demonstrates weight truncation, not fine-tuning strategies. Comparative experiments showing fine-tuning specific layers based on distributional/in-context separation outperforms traditional fine-tuning on reasoning tasks would resolve this.

## Limitations
- Theoretical analysis relies on simplifying assumptions about gradient noise that may not fully capture real-world training dynamics
- Low-rank truncation mechanism for localizing distributional associations in subspaces remains somewhat speculative without more detailed empirical validation
- GSM8K results show improvement in few-shot settings but degradation in 8-shot settings, suggesting context-dependent effects

## Confidence

**Major Claims and Confidence Levels:**
- **High Confidence**: The empirical observation that feed-forward layer truncation improves reasoning performance on specific tasks (IOI, factual recall). Directly supported by experimental results with clear before/after comparisons.
- **Medium Confidence**: The theoretical framework explaining why feed-forward layers learn distributional associations while attention layers focus on in-context reasoning. Plausible mechanism but simplifying assumptions reduce confidence.
- **Medium Confidence**: The claim that LASER improves few-shot Chain-of-Thought reasoning on GSM8K. Results show improvement in 1-2 shot settings but degradation in 8-shot settings and limited model evaluation suggest context-dependent effects.

## Next Checks
1. **Gradient Noise Analysis**: Conduct controlled experiments varying noise levels during training to directly test the hypothesis that gradient noise drives the separation between feed-forward and attention layers. Measure how different noise distributions affect the learning of distributional associations versus in-context reasoning.

2. **Low-Rank Subspace Analysis**: Perform detailed singular value decomposition analysis on feed-forward layer weight matrices to empirically verify that distributional associations are indeed localized in low-rank subspaces. Track how these subspaces evolve during training and how truncation affects them.

3. **Cross-Architecture Validation**: Test the feed-forward vs attention separation hypothesis across different transformer architectures (decoder-only, encoder-only, hybrid) and model scales to determine if the observed patterns are architecture-specific or more general principles.