---
ver: rpa2
title: Chain-of-Thought Reasoning Without Prompting
arxiv_id: '2402.10200'
source_url: https://arxiv.org/abs/2402.10200
tags:
- reasoning
- decoding
- paths
- cot-decoding
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models struggle with reasoning when using standard
  greedy decoding, despite having inherent reasoning capabilities. The study found
  that CoT reasoning paths naturally exist within alternative top-k decoding paths
  but are obscured by greedy decoding.
---

# Chain-of-Thought Reasoning Without Prompting

## Quick Facts
- **arXiv ID**: 2402.10200
- **Source URL**: https://arxiv.org/abs/2402.10200
- **Reference count**: 40
- **Primary result**: LLMs possess inherent CoT reasoning capabilities that are obscured by greedy decoding, which can be revealed through confidence-based selection of alternative decoding paths

## Executive Summary
This study demonstrates that large language models inherently possess chain-of-thought reasoning capabilities that are not accessible through standard greedy decoding. The research shows that CoT reasoning paths exist within the alternative top-k decoding paths but are obscured by greedy decoding's selection of the highest probability token. By implementing a confidence-based selection method (CoT-decoding) that explores these alternative paths, the study achieves significant improvements in reasoning performance across multiple models and tasks. The approach reveals LLMs' intrinsic reasoning abilities without requiring explicit prompting or fine-tuning.

## Method Summary
The method explores CoT reasoning paths by modifying the decoding process to consider top-k alternative tokens rather than just the highest probability token. A confidence-based selection mechanism identifies which of these alternative paths contain valid reasoning chains. This approach is tested across multiple models (including PaLM-2 Large) and reasoning tasks (like GSM8K), comparing performance against standard greedy decoding. The technique reveals that LLMs can perform reasoning internally but require a different decoding strategy to access these capabilities.

## Key Results
- PaLM-2 Large achieved 63.2% accuracy on GSM8K with CoT-decoding versus 34.8% with greedy decoding
- The method reveals LLMs' intrinsic reasoning abilities without prompting or fine-tuning
- Benefits vary significantly with task complexity, showing minimal gains on simple tasks but substantial improvements on complex reasoning problems

## Why This Works (Mechanism)
The mechanism works because CoT reasoning paths are naturally embedded within the probability distributions of alternative tokens during decoding. Greedy decoding, which selects only the highest probability token at each step, systematically misses these reasoning paths. The confidence-based selection in CoT-decoding effectively filters and identifies the alternative paths that contain valid reasoning chains. This reveals that LLMs have the reasoning capability internally but require a decoding strategy that can navigate the probability space to access these paths.

## Foundational Learning

**Greedy Decoding**
- Why needed: Standard decoding method that selects highest probability token at each step
- Quick check: Compare token selection process with alternative methods

**Top-k Decoding**
- Why needed: Explores multiple probable token alternatives rather than single best choice
- Quick check: Understand how probability distributions are truncated at k-th position

**Chain-of-Thought Reasoning**
- Why needed: Sequential reasoning process that breaks down complex problems into intermediate steps
- Quick check: Identify characteristic patterns in reasoning chains

## Architecture Onboarding

**Component Map**
LLM model -> Top-k decoder -> Confidence scorer -> Output selector -> Final response

**Critical Path**
Token generation → Top-k candidate collection → Confidence evaluation → Path selection → Response construction

**Design Tradeoffs**
- Computational overhead vs. reasoning performance improvement
- Choice of k value balancing exploration and efficiency
- Confidence threshold selection affecting accuracy and false positives

**Failure Signatures**
- No improvement over greedy decoding on simple tasks
- Inconsistent reasoning path discovery across runs
- Performance degradation with excessive k values

**3 First Experiments**
1. Compare greedy decoding vs. top-5 CoT-decoding on GSM8K benchmark
2. Vary k values (3, 5, 10) to find optimal balance of performance and efficiency
3. Test across different model scales to assess scalability of the approach

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the findings raise several implicit questions about generalizability to real-world reasoning tasks, computational efficiency for deployment, and the consistency of reasoning path discovery across different model runs and domains.

## Limitations
- Effectiveness highly dependent on task complexity, with simple tasks showing minimal gains
- Computational overhead of generating and evaluating multiple decoding paths may limit practical deployment
- Does not establish generalizability across diverse real-world reasoning tasks beyond benchmark datasets

## Confidence

**High Confidence**: The core finding that CoT reasoning paths exist within alternative top-k decoding paths and can be identified through confidence-based selection is well-supported by empirical results across multiple models and tasks.

**Medium Confidence**: The claim that LLMs possess inherent reasoning capabilities obscured by greedy decoding is plausible but requires further validation across broader task domains.

**Low Confidence**: The assertion that CoT-decoding reveals LLMs' "intrinsic reasoning abilities" without prompting is overstated, as the method still requires a specific decoding algorithm rather than being a fundamental property of the models themselves.

## Next Checks

1. Test CoT-decoding on open-ended reasoning tasks (e.g., multi-step problem-solving in unfamiliar domains) to assess generalizability beyond benchmark datasets

2. Quantify the computational overhead and latency trade-offs for different k values to determine practical deployment thresholds

3. Conduct ablation studies comparing CoT-decoding against other decoding strategies (e.g., beam search, diverse decoding) to isolate what specifically drives performance improvements