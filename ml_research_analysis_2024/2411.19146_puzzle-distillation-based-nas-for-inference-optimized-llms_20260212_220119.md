---
ver: rpa2
title: 'Puzzle: Distillation-Based NAS for Inference-Optimized LLMs'
arxiv_id: '2411.19146'
source_url: https://arxiv.org/abs/2411.19146
tags:
- block
- parent
- accuracy
- puzzle
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Puzzle, a framework for optimizing large
  language models (LLMs) for efficient inference through neural architecture search
  (NAS). The key idea is to transform a trained LLM into a non-uniform architecture
  with adapted computation allocation, using blockwise local knowledge distillation
  (BLD) for parallel architecture exploration and mixed-integer programming for constraint
  optimization.
---

# Puzzle: Distillation-Based NAS for Inference-Optimized LLMs

## Quick Facts
- arXiv ID: 2411.19146
- Source URL: https://arxiv.org/abs/2411.19146
- Authors: Akhiad Bercovich, Tomer Ronen, Talor Abramovich, Nir Ailon, Nave Assaf, Mohammad Dabbah, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Netanel Haber, Ehud Karpas, Roi Koren, Itay Levy, Pavlo Molchanov, Shahar Mor, Zach Moshe, Najeeb Nabwani, Omri Puny, Ran Rubin, Itamar Schen, Ido Shahaf, Oren Tropp, Omer Ullman Argov, Ran Zilberstein, Ran El-Yaniv
- Reference count: 40
- One-line primary result: Nemotron-51B achieves 2.17× inference throughput speedup while retaining 98.4% of Llama-3.1-70B-Instruct accuracy

## Executive Summary
This paper introduces Puzzle, a novel framework for optimizing large language models (LLMs) for efficient inference through neural architecture search (NAS). The key innovation is transforming a trained LLM into a non-uniform architecture with adapted computation allocation, using blockwise local knowledge distillation for parallel architecture exploration and mixed-integer programming for constraint optimization. Applied to Llama-3.1-70B-Instruct, Puzzle produces Nemotron-51B which achieves 2.17× inference throughput speedup while retaining 98.4% of the parent model's accuracy across various benchmarks. The framework demonstrates that powerful LLMs can be optimized for efficient deployment with minimal quality loss using only 45B training tokens compared to 15T for the original model.

## Method Summary
Puzzle employs a three-stage framework: Blockwise Local Distillation (BLD) for parallel training of individual blocks, Mixed-Integer Programming (MIP) for searching optimal architectural configurations under hardware constraints, and Global Knowledge Distillation (GKD) for final model training. The approach constructs a block library where each block variant is trained independently to mimic its parent, enabling pipeline parallelism and faster convergence. MIP then searches through this library to find optimal architectures that maximize quality while meeting memory, latency, and throughput constraints. The final model is trained using GKD with a reduced token budget of 45B compared to 15T for the original training.

## Key Results
- Nemotron-51B achieves 2.17× inference throughput speedup compared to Llama-3.1-70B-Instruct
- Retains 98.4% of parent model accuracy across benchmarks (MMLU, MT-Bench, GSM8K, TruthfulQA, HellaSwag, Winogrande, ARC Challenge)
- Reduces training token requirement from 15T to 45B tokens
- Successfully optimized Llama-3.1-8B-Instruct for 3.39× higher throughput with 98.8% accuracy retention
- Created Nemotron-49B optimized for 128K context length

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Blockwise local distillation (BLD) enables parallel training of individual blocks, drastically reducing training cost while maintaining quality.
- **Mechanism:** Each block is trained independently to mimic its corresponding parent block using normalized MSE loss, allowing pipeline parallelism and faster convergence with higher learning rates.
- **Core assumption:** Each block's function can be effectively distilled from its parent without interference from other blocks, and quality can be measured locally before full assembly.
- **Evidence anchors:**
  - [abstract] "Our approach utilizes blockwise local knowledge distillation (BLD) for parallel architecture exploration"
  - [section] "We train each child block independently and in parallel to locally mimic its corresponding parent block, with only the parent activations being transferred between layers"
  - [corpus] Weak - no direct mention of BLD in related papers corpus
- **Break condition:** If blocks are highly interdependent or if local scoring doesn't correlate with global performance, the parallel approach fails.

### Mechanism 2
- **Claim:** Mixed-integer programming (MIP) efficiently searches the enormous architectural space to find optimal configurations under hardware constraints.
- **Mechanism:** Each layer is modeled as a group in a knapsack problem, with block variants as items having quality scores and resource costs. MIP finds the combination maximizing quality while meeting memory, latency, and throughput constraints.
- **Core assumption:** Block quality scores (from replace-1-block experiments) accurately predict full-model performance and resource requirements can be measured precisely on target hardware.
- **Evidence anchors:**
  - [abstract] "employs mixed-integer programming for precise constraint optimization"
  - [section] "Our approach is to decompose the crafting process to operate on individual blocks instead of complete child models"
  - [corpus] Weak - related papers don't discuss MIP-based NAS for LLMs
- **Break condition:** If scoring underestimates block interactions or hardware measurements don't generalize to deployment scenarios.

### Mechanism 3
- **Claim:** Decoupled blockwise distillation reduces training cost by separating attention and FFN subblock training while maintaining quality.
- **Mechanism:** Instead of training each full block variant [aj, fk], we train [aj, fparent] and [aparent, fk] separately with frozen parent subblocks, then combine them. This transforms multiplicative complexity into additive.
- **Core assumption:** Subblocks can be trained independently and composed without significant loss of fidelity, and the combined blocks retain the necessary representational capacity.
- **Evidence anchors:**
  - [abstract] "Our approach utilizes blockwise local knowledge distillation (BLD) for parallel architecture exploration"
  - [section] "training [aj, f(frozen)parent]i and [a(frozen)parent, fk]i separately to emulate [aparent, fparent]i while freezing the weights"
  - [corpus] Weak - no direct mention of decoupled BLD in related papers corpus
- **Break condition:** If subblock composition introduces significant performance degradation or if certain block combinations require coupled training.

## Foundational Learning

- **Concept:** Knowledge Distillation (KD)
  - **Why needed here:** Puzzle uses both blockwise local distillation (BLD) for block library creation and global knowledge distillation (GKD) for final model training. Understanding KD principles is essential for grasping how the student model learns from the parent.
  - **Quick check question:** What is the primary difference between blockwise local distillation and global knowledge distillation in the Puzzle framework?

- **Concept:** Mixed-Integer Programming (MIP)
  - **Why needed here:** MIP is the core optimization algorithm that searches through the architectural space. Understanding MIP formulation and solution methods is crucial for comprehending how Puzzle finds optimal architectures.
  - **Quick check question:** How does the MIP formulation ensure exactly one block variant is selected per layer while maximizing total quality?

- **Concept:** Neural Architecture Search (NAS)
  - **Why needed here:** Puzzle is a NAS framework specifically designed for LLMs. Understanding general NAS principles helps contextualize how Puzzle's approach differs from traditional CV-based methods.
  - **Quick check question:** Why is traditional NAS evaluation infeasible for LLMs, and how does Puzzle's approach address this challenge?

## Architecture Onboarding

- **Component map:**
  Parent Model (e.g., Llama-3.1-70B-Instruct) → Block Library Construction (BLD) → Resource Estimation → Block Scoring → MIP Optimization → Architecture Assembly → Global Knowledge Distillation (GKD) → Optimized Child Model

- **Critical path:**
  1. Build block library using BLD (0.25-1B tokens)
  2. Measure resource requirements on target hardware
  3. Score blocks using KL divergence or downstream accuracy
  4. Run MIP optimization with hardware constraints
  5. Perform GKD training (45B tokens in main experiment)
  6. Validate and deploy

- **Design tradeoffs:**
  - Training cost vs. model quality (fewer tokens in BLD/GKD reduces cost but may impact performance)
  - Search space breadth vs. computational feasibility (more block variants = better optimization but higher training cost)
  - Hardware specificity vs. generalization (architectures optimized for specific hardware may not transfer well)

- **Failure signatures:**
  - Poor accuracy retention despite optimization (suggests scoring metrics don't correlate with global performance)
  - Inability to meet hardware constraints (indicates resource estimation errors or overly aggressive optimization)
  - Training instability or divergence (may indicate issues with BLD formulation or learning rates)

- **First 3 experiments:**
  1. **Block library validation:** Run BLD on a small subset of layers with different token budgets (0.25B, 0.5B, 1B) and measure replace-1-block scores to understand training efficiency
  2. **Scoring metric comparison:** Score the same block library using LM loss, KL divergence, and downstream accuracy to validate which metric best predicts full-model performance
  3. **MIP solver testing:** Run MIP optimization with synthetic scores and resource requirements to verify constraint satisfaction and solution quality before full implementation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Puzzle framework perform when applied to multimodal models like vision-language models?
- Basis in paper: The authors mention that the framework could be extended to optimize models for multimodal tasks, including vision-language models.
- Why unresolved: The paper only applies Puzzle to text-based language models. Testing on multimodal models would reveal if the framework's effectiveness transfers across different modalities.
- What evidence would resolve it: Successful application of Puzzle to vision-language models with comparable accuracy preservation and computational efficiency gains as demonstrated in text models.

### Open Question 2
- Question: What is the theoretical upper bound on the accuracy preservation percentage when using Puzzle, particularly for extremely large models (e.g., 100B+ parameters)?
- Basis in paper: The authors note that Nemotron-51B retained 98.4% of its parent's accuracy, but this may not represent the maximum achievable preservation. They also mention that Puzzle was applied to Llama-3.1-405B-Instruct.
- Why unresolved: The paper does not explore the relationship between model size and accuracy preservation limits. Larger models may have different architectural characteristics affecting the optimization process.
- What evidence would resolve it: Systematic experiments applying Puzzle to increasingly large models with quantification of accuracy preservation rates and identification of diminishing returns or failure points.

### Open Question 3
- Question: How sensitive is the Puzzle framework to the choice of training data composition when the parent model's training data is completely unavailable?
- Basis in paper: The authors show that using Project Gutenberg (a limited-domain dataset) resulted in 92.7% and 95.5% of the performance obtained with the diverse Distillation Mix dataset.
- Why unresolved: While the paper demonstrates some robustness to dataset composition, it doesn't explore the limits of this robustness or identify which aspects of data diversity are most critical.
- What evidence would resolve it: Controlled experiments varying dataset composition systematically while keeping all other factors constant, identifying the minimum data diversity requirements for acceptable performance.

## Limitations

- Requires access to parent model weights and specialized hardware measurement infrastructure
- Block independence assumptions may not hold for all model architectures
- Hardware-specific optimization may limit portability across different platforms

## Confidence

- **High Confidence:** Inference throughput measurements (2.17× speedup) and accuracy retention metrics (98.4% on benchmarks) are well-documented with specific hardware configurations and reproducible results.
- **Medium Confidence:** The scalability claims (45B tokens vs 15T original training) are convincing but depend on assumptions about block independence that may not hold for all model architectures.
- **Low Confidence:** The generalizability of MIP-based optimization across different hardware platforms and the long-term stability of non-uniform architectures under varying workloads.

## Next Checks

1. **Cross-platform validation:** Test Nemotron-51B on different GPU architectures (e.g., A100, H200) to verify hardware optimization portability and identify any platform-specific performance degradation.

2. **Stress testing under variable conditions:** Evaluate the optimized model's performance under dynamic batch sizes, sequence lengths, and mixed-precision configurations to assess real-world deployment robustness.

3. **Block interaction analysis:** Systematically test architectures where highly-scored blocks are combined in various configurations to quantify the impact of block interdependence on the MIP optimization's effectiveness.