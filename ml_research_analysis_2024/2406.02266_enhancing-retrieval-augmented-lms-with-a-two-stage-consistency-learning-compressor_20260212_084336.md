---
ver: rpa2
title: Enhancing Retrieval-Augmented LMs with a Two-stage Consistency Learning Compressor
arxiv_id: '2406.02266'
source_url: https://arxiv.org/abs/2406.02266
tags:
- learning
- consistency
- language
- retrieved
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of integrating retrieval mechanisms
  with language models in retrieval-augmented language models (RALMs), particularly
  the difficulty of distinguishing pertinent from extraneous information in retrieved
  documents, which leads to inconsistencies and reduced precision in generated outputs.
  To tackle this, the authors propose C2LPRCom, a two-stage consistency learning framework
  that includes a fine-grained extractive compressor based on contrastive learning
  and a lightweight post-retrieval information compressor.
---

# Enhancing Retrieval-Augmented LMs with a Two-stage Consistency Learning Compressor

## Quick Facts
- arXiv ID: 2406.02266
- Source URL: https://arxiv.org/abs/2406.02266
- Reference count: 17
- This work proposes C2LPRCom, a two-stage consistency learning framework that improves retrieval-augmented language models by enhancing precision and inference efficiency through fine-grained extractive compression and semantic alignment.

## Executive Summary
This paper addresses the challenge of integrating retrieval mechanisms with language models in retrieval-augmented language models (RALMs), particularly the difficulty of distinguishing pertinent from extraneous information in retrieved documents. The authors propose C2LPRCom, a two-stage consistency learning framework that includes a fine-grained extractive compressor based on contrastive learning and a lightweight post-retrieval information compressor. Extensive experiments on three benchmark datasets (WikiText-103, Natural Questions, and HotpotQA) demonstrate significant improvements in precision and inference efficiency, outperforming state-of-the-art baselines.

## Method Summary
The C2LPRCom framework consists of two stages: (1) a contrastive selector that performs fine-grained sentence-level extraction using a three-way data construction method with granular positive, semi-positive, and negative sample pairs, and (2) a consistency learning module that employs teacher-student distillation to align outputs under perturbed inputs with unperturbed outputs. The method uses a teacher model (GPT-3.5-turbo) to generate high-quality summaries, which are then distilled into a student model (T5-large or LLaMA2) through supervised fine-tuning followed by consistency learning with L2 norm-based alignment loss.

## Key Results
- On Natural Questions dataset, the selector module achieves 5% compression rate while losing only 2 EM points compared to using complete documents
- Consistency learning module improves inference efficiency by 9% to 16% compared to RECOMP baseline
- Outperforms state-of-the-art baselines across all three benchmark datasets (WikiText-103, Natural Questions, and HotpotQA)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage consistency learning framework improves faithfulness and reduces noise impact in RALMs by using teacher-student distillation and perturbed input consistency.
- Mechanism: Stage 1 uses full supervision on distilled data from a teacher LM, selecting high-quality summary sentences via a contrastive selector. Stage 2 adds consistency loss to align outputs under perturbed inputs with the unperturbed outputs of upstream modules, enforcing semantic coherence.
- Core assumption: The teacher LM produces higher-quality summaries than the student, and perturbations preserve semantic meaning while forcing robustness.
- Evidence anchors:
  - [abstract] "By incorporating consistency learning, the aim is to generate summaries that maintain coherence and alignment with the intended semantic representations of a teacher model while improving faithfulness to the original retrieved documents."
  - [section] "We then calculate the similarity between the encoded CS(xi, Di) and the teacher model output S′, and select the top-5 from the selector output as the result se..."
  - [corpus] No direct empirical support in cited neighbors; weak evidence.
- Break condition: If the teacher LM's output is noisy or misaligned with ground truth, or if perturbations alter semantic content too much, the consistency loss can degrade performance.

### Mechanism 2
- Claim: The fine-grained contrastive selector improves sentence relevance ranking by using granular positive, semi-positive, and negative sample pairs.
- Mechanism: Sentences are encoded, and a three-way loss encourages close embedding for positives, moderate distance for semi-positives, and large distance for negatives. This enables nuanced discrimination between relevant and less relevant content.
- Core assumption: Sentence embeddings from the encoder capture semantic relevance to the query and can be meaningfully separated via contrastive learning.
- Evidence anchors:
  - [abstract] "introducing a three-way data construction method to enhance localization accuracy during extraction by leveraging granular positive, semi-positive, and negative sample pairs."
  - [section] "The loss used in model training takes the form of contrastive learning, similar to[4]... The training objective of the model aims to minimize Lc and maximize the distance between positive and negative samples, while semi-positive samples lie in between based on their weights."
  - [corpus] No cited contrastive retriever work in neighbors; weak evidence.
- Break condition: If the encoder cannot differentiate sentence relevance well, or if the weight assignment for semi-positives is inaccurate, contrastive separation may fail.

### Mechanism 3
- Claim: The two-stage approach (first supervised, then consistency learning) optimizes both fidelity to ground truth and robustness to input variations.
- Mechanism: In stage 1, the model learns from labeled data via cross-entropy loss. In stage 2, the same model is fine-tuned with an additional consistency loss that measures embedding distance between perturbed and unperturbed inputs, encouraging invariant outputs.
- Core assumption: Early supervised learning establishes a reasonable baseline, and consistency learning further improves generalization without overfitting.
- Evidence anchors:
  - [abstract] "During the initial stage, the model undergoes fine-tuning solely guided by the supervision loss. Subsequently, in the second stage, the model is trained under the joint supervision of both the supervised loss and the consistency loss..."
  - [section] "For the consistency learning loss, it primarily consists of two components... we obtain their embedded representations. We then separately calculate the L2 norm..."
  - [corpus] No explicit empirical comparison of single vs. two-stage learning in neighbors; weak evidence.
- Break condition: If the consistency loss outweighs supervised signals too early, the model may diverge from ground truth; if supervised loss dominates, robustness gains are minimal.

## Foundational Learning

- Concept: Contrastive learning in sentence retrieval
  - Why needed here: To rank sentences by relevance to a query, especially when documents contain mixed relevant and irrelevant content.
  - Quick check question: How does a three-way contrastive loss differ from binary positive/negative contrastive learning in terms of sample weighting and gradient flow?

- Concept: Teacher-student distillation with semantic consistency
  - Why needed here: To transfer knowledge from a stronger LM to a lighter one while ensuring generated summaries stay faithful to retrieved context.
  - Quick check question: What is the effect of using L2 norm vs. cosine similarity for consistency loss in embedding space?

- Concept: Perturbation-based data augmentation for semantic consistency
  - Why needed here: To make the model robust to minor input variations (synonym replacement, paraphrasing) without changing meaning.
  - Quick check question: Why is it important to apply the same augmentation strategy to upstream, midstream, and downstream components during consistency learning?

## Architecture Onboarding

- Component map: Input query X and retrieved documents D -> Contrastive retriever -> sentence selector (top-k) -> Teacher LM -> summary output S′ -> Student LM -> summary output S -> Consistency learning module -> alignment loss -> Downstream RAG -> final generation
- Critical path: 1. Query -> BM25 -> candidate sentences 2. Contrastive selector -> top sentences 3. Teacher LM + student LM -> summaries 4. Consistency learning -> aligned outputs 5. RAG generation -> final answer
- Design tradeoffs:
  - Selector accuracy vs. compression rate (longer summaries preserve more info but cost more tokens)
  - Teacher LM quality vs. latency (stronger teachers improve distillation but slow training)
  - Perturbation strength vs. semantic preservation (too much change breaks consistency, too little fails robustness)
- Failure signatures:
  - High perplexity but low EM/F1 -> selector too aggressive, losing key info
  - Slow inference with marginal gains -> consistency loss not well tuned
  - Large variance in output length -> distillation module unstable
- First 3 experiments:
  1. Compare selector-only vs. full two-stage pipeline on NQ dev set (EM, token count).
  2. Ablate consistency learning (stage 2 only) to measure robustness gain under perturbed inputs.
  3. Vary perturbation rate (10%, 30%, 50%) and observe effect on consistency loss and final performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between selector module compression rate and performance loss across different datasets and tasks?
- Basis in paper: [explicit] The paper mentions that on the NQ dataset, the selector module achieves a 5% compression rate while losing 2 EM points compared to appending the complete document, and on HotpotQA, it achieves an 11% compression rate while losing 2.4 EM points.
- Why unresolved: The paper does not explore the relationship between compression rate and performance loss across different datasets and tasks. It also does not investigate the optimal balance between these two factors.
- What evidence would resolve it: Conducting experiments on various datasets and tasks to determine the optimal compression rate that minimizes performance loss while maximizing efficiency gains.

### Open Question 2
- Question: How does the proposed method handle complex multi-hop reasoning tasks that require synthesizing information from multiple documents?
- Basis in paper: [inferred] The paper mentions that large language models struggle with synthesizing information from multiple documents in complex tasks like HotpotQA.
- Why unresolved: The paper does not provide a detailed analysis of how the proposed method performs on complex multi-hop reasoning tasks that require synthesizing information from multiple documents.
- What evidence would resolve it: Evaluating the proposed method on a variety of complex multi-hop reasoning tasks and comparing its performance to existing methods that specifically target this type of task.

### Open Question 3
- Question: How does the proposed method scale to larger datasets and more complex models?
- Basis in paper: [explicit] The paper mentions that the proposed method is evaluated on three benchmark datasets (WikiText-103, Natural Questions, and HotpotQA) and uses GPT-3.5-turbo and LLaMA2 (13B) as teacher and student models, respectively.
- Why unresolved: The paper does not investigate the scalability of the proposed method to larger datasets and more complex models. It also does not discuss the computational resources required for training and inference.
- What evidence would resolve it: Conducting experiments on larger datasets and using more complex models to evaluate the scalability of the proposed method. Additionally, providing a detailed analysis of the computational resources required for training and inference.

## Limitations

- The three-way data construction method for semi-positive samples lacks explicit detail on weight assignment and similarity thresholds, making exact replication challenging
- The computational overhead of the two-stage training process, particularly the consistency learning stage, is not thoroughly analyzed in terms of practical deployment considerations
- The relative contribution of each training stage is not fully isolated through ablation studies, making it unclear whether the two-stage approach provides additive benefits

## Confidence

- **High confidence**: The core mechanism of using teacher-student distillation with consistency learning for RALMs is well-established in the literature and the paper's implementation follows standard practices. The reported performance improvements on benchmark datasets are specific and measurable.
- **Medium confidence**: The effectiveness of the three-way contrastive learning approach for sentence selection is supported by the results but the exact implementation details are sparse, making independent verification difficult.
- **Medium confidence**: The claim that the two-stage approach outperforms single-stage alternatives is supported by results but lacks direct ablation comparisons with stage-1-only or stage-2-only training.

## Next Checks

1. **Ablation study of training stages**: Train and evaluate the model with only stage 1 (supervised learning) and only stage 2 (consistency learning) to quantify the marginal contribution of each component and verify that the two-stage approach provides additive benefits.

2. **Perturbation sensitivity analysis**: Systematically vary the perturbation rate (10%, 30%, 50%) and type (synonym replacement, insertion/deletion, paraphrasing) to measure the relationship between perturbation intensity and consistency loss effectiveness, identifying the optimal balance point.

3. **Computational overhead benchmarking**: Measure and compare the training time, inference latency, and memory requirements of the two-stage C2LPRCom framework against baseline RALM approaches across different hardware configurations to assess practical deployment feasibility.