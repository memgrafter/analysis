---
ver: rpa2
title: 'UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval
  and Generation'
arxiv_id: '2408.11305'
source_url: https://arxiv.org/abs/2408.11305
tags:
- image
- retrieval
- fashion
- generation
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UniFashion, a unified vision-language model
  that simultaneously tackles multimodal fashion retrieval and generation tasks. The
  core idea is to integrate a diffusion model and a large language model (LLM) with
  a querying transformer (Q-Former) to enable controllable and high-fidelity generation.
---

# UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation

## Quick Facts
- **arXiv ID**: 2408.11305
- **Source URL**: https://arxiv.org/abs/2408.11305
- **Reference count**: 35
- **Primary result**: UniFashion achieves state-of-the-art performance across fashion retrieval (87.55% recall@10 on FashionGen) and generation (FID 12.43 on VITON-HD) tasks using a unified vision-language architecture.

## Executive Summary
UniFashion presents a unified vision-language model that simultaneously tackles multimodal fashion retrieval and generation tasks. The model integrates a diffusion model and a large language model (LLM) with a querying transformer (Q-Former) to enable controllable and high-fidelity generation. Through a two-phase training strategy, UniFashion first learns cross-modal representations using image-text pairs, then fine-tunes on composed multimodal inputs. The approach demonstrates significant performance improvements over previous state-of-the-art methods across various fashion-related challenges, achieving 87.55% recall@10 on FashionGen cross-modal retrieval, 35.53 BLEU-4 on image captioning, and 67.93% average recall on Fashion-IQ composed image retrieval.

## Method Summary
UniFashion employs a modular architecture integrating a BLIP2 Q-Former, Vicuna-1.5 LLM, and Stable Diffusion v1.4 diffusion model through task-specific adapters. The model uses a two-phase training strategy: Phase 1 pre-trains all components on image-text pairs using contrastive learning, matching, and text generation objectives; Phase 2 fine-tunes only the Q-Former on composed multimodal inputs while freezing the LLM and diffusion modules. This approach enables the model to learn general cross-modal representations before specializing in composed tasks. The architecture supports both retrieval tasks (using similarity calculation) and generation tasks (using the diffusion model conditioned on Q-Former outputs) within a unified framework.

## Key Results
- Achieves 87.55% recall@10 on FashionGen cross-modal retrieval, significantly outperforming previous state-of-the-art methods
- Obtains 35.53 BLEU-4 score on image captioning tasks, demonstrating strong text generation capabilities
- Reaches 67.93% average recall on Fashion-IQ composed image retrieval, showing effectiveness in multimodal understanding
- Generates high-fidelity fashion images with FID score of 12.43 on VITON-HD dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-phase training strategy allows the model to first learn general cross-modal representations before specializing in composed multimodal tasks, leading to improved performance across all tasks.
- Mechanism: In Phase 1, UniFashion learns cross-modal representations using image-text pairs with frozen Q-Former and fine-tuned LLM and diffusion modules. This establishes strong foundational understanding of visual and textual relationships. In Phase 2, only the Q-Former is fine-tuned on composed multimodal inputs while freezing the LLM and diffusion modules, allowing it to specialize in combining reference images with modifying text.
- Core assumption: Cross-modal representations learned in Phase 1 transfer effectively to composed multimodal tasks in Phase 2.
- Evidence anchors:
  - [abstract]: "Our model significantly outperforms previous state-of-the-art models focused on single tasks across various fashion-related challenges"
  - [section 3.1]: "In the first stage, we perform multimodal representation learning on image-text pairs datasets. We freeze Q-Former and fine-tune the LLM and diffusion modules, ensuring they develop the capability to comprehend the multimodal representations provided by Q-Former."
- Break condition: If the cross-modal representations learned in Phase 1 are not transferable to composed tasks, or if the frozen LLM/diffusion modules cannot adequately process the refined Q-Former outputs.

### Mechanism 2
- Claim: The integration of LLM and diffusion model through task-specific adapters creates a unified representation space that enables both retrieval and generation tasks.
- Mechanism: Task-specific adapters (TSA) linearly project Q-Former outputs to match the dimensionality of LLM and diffusion model embeddings. This alignment allows the same learnable queries to condition both caption generation and image generation processes, creating a shared representation space that bridges embedding and generative tasks.
- Core assumption: The task-specific adapters can effectively map between Q-Former output space and both LLM and diffusion model embedding spaces without significant information loss.
- Evidence anchors:
  - [section 3.1.2]: "To synchronize the space of Q-Former with that of the LLM, we propose to use the image-grounded text generation (ITG) objective to drive the model to generate texts based on the input image by computing the auto-regressive loss"
  - [section 3.2.2]: "The loss function for the target image generation is formulated in a way that is similar to Eq. 7"
- Break condition: If the adapter mappings introduce significant distortion, or if the shared representation space cannot adequately capture task-specific nuances.

### Mechanism 3
- Claim: The instruction-tuning approach for LLMs allows the model to adapt to different caption styles across various fashion datasets, improving generation quality and task specificity.
- Mechanism: Different instruction templates are designed for different datasets (Fashion200K, FashionGen, Fashion-IQ-cap) to match their specific caption style requirements. This enables the LLM to generate appropriate captions for each dataset context while maintaining the unified architecture.
- Core assumption: LLMs can effectively adapt to different caption style requirements through instruction tuning without catastrophic forgetting of general capabilities.
- Evidence anchors:
  - [section 3.3]: "Due to the different styles of captions in different fashion datasets, we adopt different instructions to tune the LLM so that it can generate captions of different styles"
  - [table 7]: Shows specific instruction templates for different datasets
- Break condition: If instruction tuning causes the LLM to overfit to specific instructions, or if the learned style adaptation doesn't generalize to new fashion contexts.

## Foundational Learning

- **Concept: Cross-modal representation learning**
  - Why needed here: UniFashion needs to understand relationships between visual and textual information across diverse fashion tasks, from simple image-text pairs to complex composed inputs
  - Quick check question: Can you explain how contrastive learning objectives (ITC and ITM) help learn aligned image-text representations?

- **Concept: Diffusion model conditioning mechanisms**
  - Why needed here: The model uses latent diffusion models conditioned on both textual and multimodal inputs for image generation and editing tasks
  - Quick check question: How does cross-attention in diffusion models enable conditioning on external information like text or learned queries?

- **Concept: Multimodal encoder architectures (Q-Former)**
  - Why needed here: Q-Former serves as the bridge between visual inputs and both LLM and diffusion modules, requiring understanding of how learnable queries interact with different modalities
  - Quick check question: What is the purpose of unimodal self-attention masking in the Q-Former, and how does it differ from bidirectional masking?

## Architecture Onboarding

- **Component map**: Image/Text → Q-Former → TSA → LLM/Diffusion → Output
  - For retrieval: Q-Former → similarity calculation
  - For generation: Q-Former → TSA → LLM/Diffusion → image/caption

- **Critical path**: Image/Text → Q-Former → TSA → LLM/Diffusion → Output
  - For retrieval: Q-Former → similarity calculation
  - For generation: Q-Former → TSA → LLM/Diffusion → image/caption

- **Design tradeoffs**:
  - Freezing vs. fine-tuning modules between phases: Freezing enables specialization but may limit adaptability
  - Learnable queries vs. fixed embeddings: Learnable queries provide flexibility but increase parameter count
  - Unified vs. task-specific architectures: Unified approach enables synergy but increases complexity

- **Failure signatures**:
  - Poor retrieval performance: Q-Former not learning effective cross-modal representations
  - Low-quality image generation: TSA not properly aligning Q-Former outputs with diffusion model conditioning space
  - Caption style mismatch: LLM not properly adapting to dataset-specific instructions

- **First 3 experiments**:
  1. Phase 1 training: Train UniFashion on FashionGen with only retrieval and generation objectives, evaluate cross-modal retrieval performance
  2. Phase 2 fine-tuning: Take Phase 1 model and fine-tune on Fashion-IQ, evaluate CIR performance improvement
  3. Ablation study: Compare full model with variants lacking LLM, diffusion model, or both to quantify contribution of each component

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of UniFashion vary when using different Large Language Models (LLMs) as the language generation component?
- **Basis in paper**: [explicit] The paper mentions that they used LLaVA-1.5 as the LLM component in UniFashion, but also notes that due to the flexibility of the modular architectural design of BLIP-2, they could adapt the model to a broad spectrum of LLMs.
- **Why unresolved**: The paper does not provide a comparative analysis of UniFashion's performance when using different LLMs. This could be an important factor in determining the optimal LLM for this specific task.
- **What evidence would resolve it**: A study comparing the performance of UniFashion using different LLMs on the same tasks would provide insights into which LLM works best for this model.

### Open Question 2
- **Question**: What is the impact of the number of learnable queries (q) on the performance of UniFashion?
- **Basis in paper**: [explicit] The paper states that they employed 32 queries in q, with each query having a dimension of 768. However, it does not discuss the impact of varying this number on the model's performance.
- **Why unresolved**: The optimal number of learnable queries for UniFashion is not known. This could potentially affect the model's ability to capture multimodal information.
- **What evidence would resolve it**: An ablation study varying the number of learnable queries and measuring the impact on UniFashion's performance across different tasks would provide insights into the optimal number of queries.

### Open Question 3
- **Question**: How does UniFashion perform on tasks beyond those evaluated in the paper, such as fashion recommendation or style transfer?
- **Basis in paper**: [inferred] The paper demonstrates UniFashion's effectiveness on various fashion-related tasks, including cross-modal retrieval, composed image retrieval, and multimodal generation. This suggests that the model has the potential to be applied to other fashion tasks.
- **Why unresolved**: The paper does not evaluate UniFashion on tasks like fashion recommendation or style transfer, which are also important in the fashion domain.
- **What evidence would resolve it**: Evaluating UniFashion on additional fashion tasks, such as recommendation or style transfer, would demonstrate the model's versatility and potential applications in the fashion domain.

## Limitations

- The two-phase training strategy's effectiveness relies heavily on the transferability of cross-modal representations from Phase 1 to the more complex composed tasks in Phase 2, which may not generalize to all fashion contexts
- The computational cost of fine-tuning large pre-trained models (Vicuna-1.5 and Stable Diffusion) is substantial, potentially limiting accessibility for researchers with limited resources
- The performance gains could be partially attributed to the specific combination of pre-trained components rather than the unified architecture itself, making it unclear how much improvement comes from the architectural design versus component selection

## Confidence

- **High Confidence**: The core architectural design of integrating Q-Former with LLM and diffusion model through task-specific adapters is well-specified and experimentally validated across multiple tasks
- **Medium Confidence**: The effectiveness of the two-phase training strategy is supported by strong quantitative results, but the exact contribution of each phase requires further ablation studies
- **Medium Confidence**: The instruction-tuning approach for handling different caption styles shows promise, but its generalization to unseen fashion contexts needs validation

## Next Checks

1. **Ablation Study**: Compare UniFashion performance against variants where LLM or diffusion model components are removed or replaced with task-specific models to quantify the contribution of the unified architecture
2. **Cross-Domain Generalization**: Evaluate UniFashion on non-fashion multimodal datasets to assess whether the learned representations transfer effectively beyond the fashion domain
3. **Fine-tuning Dynamics Analysis**: Monitor the alignment between Q-Former outputs and LLM/diffusion model spaces throughout Phase 2 training to identify potential bottlenecks in information flow