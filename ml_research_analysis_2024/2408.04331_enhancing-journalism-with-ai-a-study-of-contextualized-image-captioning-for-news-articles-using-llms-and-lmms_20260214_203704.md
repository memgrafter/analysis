---
ver: rpa2
title: 'Enhancing Journalism with AI: A Study of Contextualized Image Captioning for
  News Articles using LLMs and LMMs'
arxiv_id: '2408.04331'
source_url: https://arxiv.org/abs/2408.04331
tags:
- image
- context
- captioning
- news
- captions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of large language models (LLMs) and
  large multimodal models (LMMs) to generate contextualized image captions for news
  articles. The researchers compare a two-stage pipeline combining image captioning
  with post-hoc contextualization to direct LMM approaches.
---

# Enhancing Journalism with AI: A Study of Contextualized Image Captioning for News Articles using LLMs and LMMs

## Quick Facts
- arXiv ID: 2408.04331
- Source URL: https://arxiv.org/abs/2408.04331
- Reference count: 9
- Key outcome: Two-stage pipelines outperform LMMs in image captioning, with focused context yielding better results than full articles

## Executive Summary
This study investigates the use of large language models (LLMs) and large multimodal models (LMMs) for generating contextualized image captions in news journalism. The researchers compare a two-stage pipeline approach, combining image captioning with post-hoc contextualization, against direct LMM approaches. Using the GoodNews dataset, the study evaluates nine model configurations including both open-source options like LLaVA and closed-source models like GPT-4v. The research demonstrates that the choice of contextualization model significantly impacts two-stage pipelines, while this effect is less pronounced in LMMs where smaller open-source models perform comparably to proprietary ones.

## Method Summary
The researchers developed a comprehensive evaluation framework comparing two distinct approaches to contextualized image captioning. The first approach uses a two-stage pipeline where an image captioning model generates initial descriptions that are then refined by a contextualization model using article content. The second approach employs direct LMMs that process both images and text simultaneously. The study tested various configurations across nine model combinations, evaluating their performance on the GoodNews dataset using automated metrics including CIDEr and SPICE scores. Different types of contextual information were provided to the models, ranging from full article content to focused elements like named entities.

## Key Results
- Two-stage pipelines show significant performance variation based on the contextualization model choice, while LMMs demonstrate more consistent performance
- Open-source models like LLaVA achieve comparable results to closed-source models like GPT-4v in LMM approaches
- Focused contextual information (such as named entities) consistently outperforms full article context in generating quality captions
- The study highlights limitations of fully automated approaches and emphasizes the need for interactive, human-in-the-loop strategies in journalism

## Why This Works (Mechanism)
The effectiveness of contextualized image captioning stems from the models' ability to understand and integrate multiple information sources. Two-stage pipelines leverage specialized image captioning models for visual understanding, then enhance these descriptions with contextual information from text. This separation allows for optimization of each component's strengths. LMMs, conversely, process visual and textual information simultaneously, enabling more direct integration of context into the captioning process. The improved performance with focused context suggests that models benefit from targeted, relevant information rather than potentially distracting full article content.

## Foundational Learning
- **Multimodal Learning**: Understanding how models process and integrate visual and textual information simultaneously - needed to design effective LMM architectures
- **Contextual Understanding**: Models must comprehend relationships between images and surrounding text - essential for generating relevant captions
- **Pipeline Design**: Two-stage approaches separate specialized tasks for optimization - important for balancing computational efficiency and output quality
- **Context Selection**: Focused contextual information outperforms broad content - critical for efficient model performance
- **Evaluation Metrics**: CIDEr and SPICE scores measure caption quality - necessary for objective performance assessment
- **Human-AI Collaboration**: Interactive approaches complement automated systems - vital for practical journalism applications

## Architecture Onboarding

Component Map:
Image Captioning Model -> Contextualization Model -> Final Caption Output
OR
LMM (Image + Text Input) -> Direct Caption Output

Critical Path:
For two-stage pipeline: Image input → Image Captioning → Context extraction → Contextualization → Final caption
For LMM approach: Image + Article text → Direct caption generation

Design Tradeoffs:
- Two-stage pipelines offer flexibility in model selection but add complexity
- LMMs provide simpler integration but may be computationally intensive
- Focused context improves efficiency but may miss broader article themes

Failure Signatures:
- Hallucinations in captions when context is insufficient
- Repetitive or generic descriptions without proper contextualization
- Inconsistent performance across different news domains

First Experiments:
1. Test baseline performance using standard image captioning without context
2. Compare focused versus full article context using the same base model
3. Evaluate open-source versus closed-source model performance on identical tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Exclusive focus on the GoodNews dataset may limit generalizability to diverse journalistic contexts
- Reliance on automated metrics rather than editorial quality assessment
- Limited evaluation of model performance in breaking news or rapidly evolving story scenarios

## Confidence
- **High confidence**: Relative performance of two-stage versus LMM approaches, focused context benefits
- **Medium confidence**: Open-source versus closed-source model performance comparison
- **Medium confidence**: Need for human-in-the-loop approaches based on metric evaluation

## Next Checks
1. Conduct user studies with professional journalists to validate automated metric findings against editorial quality standards and assess real-world usability
2. Test model performance across diverse news domains (sports, politics, business) and cultural contexts to evaluate generalizability
3. Implement and evaluate human-in-the-loop workflows to quantify the practical value of interactive approaches in newsroom settings