---
ver: rpa2
title: Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models
arxiv_id: '2410.11081'
source_url: https://arxiv.org/abs/2410.11081
tags:
- diffusion
- training
- step
- arxiv
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the training instability that has limited
  continuous-time consistency models (CMs) in diffusion-based generative modeling.
  The authors propose a unified theoretical framework called TrigFlow, which simplifies
  the parameterization of diffusion models and CMs by using trigonometric functions.
---

# Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models

## Quick Facts
- arXiv ID: 2410.11081
- Source URL: https://arxiv.org/abs/2410.11081
- Authors: Cheng Lu; Yang Song
- Reference count: 40
- One-line primary result: Proposed sCM achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64×64, and 1.88 on ImageNet 512×512 using two sampling steps

## Executive Summary
This paper addresses the training instability that has limited continuous-time consistency models (CMs) in diffusion-based generative modeling. The authors propose a unified theoretical framework called TrigFlow, which simplifies the parameterization of diffusion models and CMs by using trigonometric functions. They identify root causes of instability in continuous-time CM training, particularly in the time-derivative components, and introduce key improvements: identity time transformation, positional embeddings, adaptive double normalization, tangent normalization, and adaptive weighting. These enhancements enable stable training of large-scale continuous-time CMs up to 1.5 billion parameters on ImageNet 512×512. The proposed sCM achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64×64, and 1.88 on ImageNet 512×512 using two sampling steps, narrowing the gap with state-of-the-art diffusion models to within 10% while using less than 10% of the sampling compute.

## Method Summary
The authors propose a unified theoretical framework called TrigFlow that simplifies and stabilizes continuous-time consistency models through trigonometric parameterization. The method replaces complex EDM coefficients with simple cos(t) and sin(t) functions, introduces identity time transformation with positional embeddings, and implements adaptive double normalization to replace unstable adaptive group normalization. Key innovations include tangent normalization to control gradient variance and adaptive weighting that learns optimal time-dependent loss weighting. The framework enables stable training of large-scale CMs up to 1.5 billion parameters on ImageNet 512×512 while achieving state-of-the-art FID scores with minimal sampling steps.

## Key Results
- Achieved FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64×64, and 1.88 on ImageNet 512×512 using two sampling steps
- Narrowed the FID gap with teacher diffusion models to within 10% while using less than 10% of the sampling compute
- Demonstrated predictable scaling across datasets and model sizes, outperforming previous few-step sampling approaches at large scales
- Enabled stable training of continuous-time CMs up to 1.5 billion parameters on ImageNet 512×512

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The TrigFlow framework simplifies and stabilizes CM training by unifying EDM and flow matching with trigonometric parameterization
- Mechanism: Replaces complex EDM coefficients with simple cos(t) and sin(t) functions, eliminating numerical instability near t=π/2 and simplifying theoretical analysis
- Core assumption: The data distribution's standard deviation σd can be incorporated into the trigonometric parameterization without loss of generality
- Evidence anchors:
  - [abstract]: "propose a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs"
  - [section 3]: "We propose TrigFlow, a formulation of diffusion models that keep the EDM properties but satisfy cskip(t) = cos(t), cout(t) = −σd sin(t)"
  - [corpus]: Weak - corpus neighbors don't discuss this specific unification mechanism
- Break condition: If the data distribution has very different variance scales that cannot be captured by σd scaling

### Mechanism 2
- Claim: Identity time transformation and positional embeddings stabilize the time-derivative components that cause training instability
- Mechanism: Replaces EDM's log(σd tan t) transformation with cnoise(t)=t and uses small-scale positional embeddings to avoid exploding gradients
- Core assumption: The time derivative sin(t)∂tFθ− is the primary source of instability, not the PF-ODE itself
- Evidence anchors:
  - [section 4.1]: "we found that σdFθ−, the PF-ODE dxt/dt, and the noisy sample xt are all relatively stable. The only term left in the tangent function now is sin(t) dFθ−/dt"
  - [section 4.1]: "we propose to use cnoise(t) = t as the default time transformation"
  - [corpus]: Weak - corpus neighbors don't discuss this specific stabilization mechanism
- Break condition: If positional embeddings prove insufficient for very deep networks or extreme time ranges

### Mechanism 3
- Claim: Adaptive weighting and tangent normalization provide explicit control over training gradient variance
- Mechanism: Trains an adaptive weighting network alongside the CM and normalizes the tangent function dfθ−/dt to stabilize optimization
- Core assumption: Gradient variance is the primary cause of instability in continuous-time CM training
- Evidence anchors:
  - [section 4.2]: "we propose to train an adaptive weighting function alongside the CM, which not only eases the burden of hyperparameter tuning but also outperforms manually designed weighting functions"
  - [section 4.2]: "we propose to explicitly normalize the tangent function by replacing dfθ−/dt with dfθ−/dt/(∥dfθ−/dt∥ + c)"
  - [section 5.1]: "Training continuous-time CMs requires computing the tangent dfθ−/dt accurately"
- Break condition: If adaptive weighting overfits to specific time ranges or tangent normalization introduces too much smoothing

## Foundational Learning

- Concept: Probability Flow ODE (PF-ODE)
  - Why needed here: The entire CM framework is built around following the PF-ODE trajectory from noisy to clean samples
  - Quick check question: What is the relationship between the PF-ODE and the denoising process in diffusion models?

- Concept: Jacobian-Vector Product (JVP) computation
  - Why needed here: Computing dfθ−/dt requires JVP for efficient gradient calculation without storing intermediate activations
  - Quick check question: How does JVP rearrangement improve numerical precision in tangent computation?

- Concept: Adaptive normalization techniques
  - Why needed here: AdaGN layer causes instability in CM training, requiring adaptive double normalization as a replacement
  - Quick check question: What's the difference between standard normalization and adaptive double normalization in this context?

## Architecture Onboarding

- Component map: x0 -> cos(t)x0 + sin(t)z diffusion process -> Network Fθ -> xt' reference -> tangent dfθ−/dt computation -> tangent normalization -> adaptive weighting -> FID evaluation

- Critical path:
  1. Sample xt = cos(t)x0 + sin(t)z from data
  2. Compute reference xt′ via DDIM from pretrained diffusion model
  3. Calculate tangent dfθ−/dt using JVP
  4. Apply tangent normalization
  5. Compute adaptive weighting loss
  6. Update parameters with RAdam optimizer

- Design tradeoffs:
  - TrigFlow vs EDM: Simplicity vs potential minor performance differences
  - Identity transformation vs EDM's log transformation: Stability vs theoretical elegance
  - Adaptive weighting vs manual weighting: Automatic optimization vs potential overfitting
  - Tangent normalization vs clipping: Smooth gradients vs gradient capping

- Failure signatures:
  - NaN values in tangent computation: Likely numerical overflow in JVP
  - Exploding gradients: Missing tangent normalization or improper adaptive weighting
  - Mode collapse: Too aggressive tangent normalization or improper guidance scale
  - Slow convergence: Inadequate proposal distribution for t sampling

- First 3 experiments:
  1. Train CM on CIFAR-10 with default EDM parameters vs TrigFlow - compare training stability and FID
  2. Test different time transformations (identity vs log(σd tan t)) on small ImageNet - measure gradient variance
  3. Implement adaptive weighting on pretrained diffusion model - verify loss variance reduction across time steps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the fundamental limit of sample quality for continuous-time consistency models, and how does it compare to the theoretical optimum for diffusion models?
- Basis in paper: [explicit] The paper states "we narrowed the FID gap with the teacher model to within 10% using two-step generation" but doesn't establish what the absolute best possible performance is for consistency models.
- Why unresolved: The paper benchmarks against existing diffusion models but doesn't establish whether consistency models can theoretically achieve the same quality as diffusion models or if there's an inherent quality ceiling due to the one-step nature of the mapping.
- What evidence would resolve it: A theoretical analysis proving either (1) consistency models can match diffusion model quality given sufficient capacity, or (2) there's a fundamental information-theoretic limit preventing consistency models from achieving the same sample quality as diffusion models that use sequential sampling.

### Open Question 2
- Question: How do the proposed improvements scale to video generation tasks where temporal consistency across frames becomes critical?
- Basis in paper: [inferred] The paper mentions "ensuring the training stability of sCM requires several significant modifications of the network architecture" and notes limitations with certain architectures, but doesn't explore video generation where temporal coherence is essential.
- Why unresolved: Video generation introduces additional constraints around temporal consistency that aren't present in image generation, and the paper's architectural modifications may not generalize well to maintaining consistency across time steps.
- What evidence would resolve it: Experiments showing sCM performance on video generation tasks with quantitative metrics for temporal consistency (like temporal FID or other video-specific metrics), comparing against state-of-the-art video diffusion models.

### Open Question 3
- Question: What is the relationship between model capacity and sample quality for consistency models, and does it follow the same scaling laws as diffusion models?
- Basis in paper: [explicit] The paper states "sCD scales predictably for a given dataset, maintaining a consistent relative difference in FIDs across model sizes" but doesn't establish if the absolute scaling laws match those of diffusion models.
- Why unresolved: While the paper shows predictable relative scaling, it doesn't determine whether consistency models follow the same absolute compute-to-quality scaling laws as diffusion models, which would inform whether they're truly competitive at scale.
- What evidence would resolve it: A comprehensive scaling study comparing how FID decreases with model FLOPs for both consistency models and diffusion models on the same datasets, determining if the slopes of these relationships are equivalent or if consistency models have different scaling efficiency.

## Limitations
- The framework's robustness to different data distributions and model architectures remains untested beyond the specific experimental conditions
- The claim of "predictable scaling" is supported by limited scale experiments on ImageNet datasets but lacks theoretical justification for why scaling should be predictable
- The method's effectiveness for video generation tasks with temporal consistency requirements has not been explored

## Confidence
- High confidence: Core stability improvements (identity transformation, positional embeddings, tangent normalization) - these are empirically validated across all experiments
- Medium confidence: Adaptive weighting effectiveness - shown to work but could be overfitting to the specific training setup
- Medium confidence: Scaling predictions - supported by limited scale experiments but lacks theoretical justification for why scaling should be predictable

## Next Checks
1. Test the TrigFlow framework on non-ImageNet datasets (e.g., LSUN, CelebA) to verify the claimed predictable scaling behavior across different data distributions and visual domains

2. Implement ablation studies comparing adaptive weighting against manually tuned weighting functions across multiple random seeds to quantify the claimed "outperforms" relationship and assess potential overfitting

3. Evaluate the stability of the proposed method when using different backbone architectures (e.g., Vision Transformers, ResNets) to test architectural robustness beyond the DDPM++ framework used in experiments