---
ver: rpa2
title: 'DoPAMine: Domain-specific Pre-training Adaptation from seed-guided data Mining'
arxiv_id: '2410.00260'
source_url: https://arxiv.org/abs/2410.00260
tags:
- data
- domain
- industry
- language
- seed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents DoPAMine, a scalable framework for mining domain-specific
  training data from large corpora to improve large language model (LLM) performance
  on specialized tasks. The core idea is to use a large language model to generate
  diverse synthetic seed data for a target domain, which is then used to retrieve
  semantically similar real-world documents from a large corpus like Common Crawl.
---

# DoPAMine: Domain-specific Pre-training Adaptation from seed-guided data Mining

## Quick Facts
- arXiv ID: 2410.00260
- Source URL: https://arxiv.org/abs/2410.00260
- Reference count: 40
- Key outcome: DoPAMine improves domain-specific LLM performance by 4.9-6.7% on healthcare and finance tasks

## Executive Summary
DoPAMine is a scalable framework for mining domain-specific training data from large corpora to improve large language model performance on specialized tasks. The method uses LLM-generated synthetic seed data to retrieve semantically similar real-world documents from sources like Common Crawl, which are then used to train domain-specific classifiers and adapt LLMs through continued pre-training. The framework was evaluated by training 7B parameter models in healthcare and finance domains, showing consistent performance improvements over baseline models trained without domain-specific data.

## Method Summary
DoPAMine generates diverse synthetic seed data using an LLM with a carefully crafted prompt template that varies document types, personas, and intended audiences. This seed data is embedded and used to retrieve highly similar real-world documents from a large corpus via nearest neighbor search with a high similarity threshold. The retrieved documents are combined with seed data to train a multi-label text classifier, which then labels remaining unlabeled documents in the corpus. Finally, the domain-specific data is used for continued pre-training of LLMs, mixing it with general data to prevent catastrophic forgetting.

## Key Results
- Healthcare domain: 4.9% and 5.1% improvement in zero-shot and 5-shot settings respectively
- Finance domain: 2.9% and 6.7% improvement in zero-shot and 5-shot settings respectively
- Consistent improvements across both domains compared to baseline models trained without domain-specific data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated synthetic seed data captures diverse domain-relevant prompts that serve as effective semantic anchors for retrieving real-world documents.
- Mechanism: By systematically varying document types, personas, author demeanors, intended audiences, and generation lengths, the seed generation prompt produces a representative and diverse sample of the target domain. This diversity ensures that the retrieved documents span the semantic breadth of the domain, capturing both common and niche aspects.
- Core assumption: The parametric knowledge of the LLM accurately reflects the target domain's characteristics and can generate diverse, semantically coherent documents.
- Evidence anchors:
  - [abstract] "We propose a versatile prompt template that harnesses the parametric knowledge of a large language model (LLM) to generate diverse and representative seed data tailored to a specific industry domain."
  - [section] "We validated the diversity and realistic nature of the generated seed data by computing several lexical metrics and comparing them against real world web-crawled documents."
  - [corpus] "Average neighbor FMR=0.507, average citations=0.0" (weak corpus signal, not directly validating seed data quality)

### Mechanism 2
- Claim: Using a high similarity threshold (tsim=0.85) for nearest neighbor extraction ensures that the retrieved documents are highly relevant to the seed data and can be reliably labeled with the seed domain.
- Mechanism: By setting a high similarity threshold, the framework retrieves only the most semantically similar documents to each seed. This high relevance allows the system to confidently assign the domain label of the seed to the retrieved documents, creating a clean labeled dataset for classifier training.
- Core assumption: The vector embeddings capture the semantic similarity accurately, and the high threshold ensures that only truly relevant documents are retrieved.
- Evidence anchors:
  - [abstract] "We choose a high tsim to extract only the most semantically similar documents per seed document from the indexed vector database (V )."
  - [section] "For each synthetic seed, we mine k semantically similar documents, adhering to the following formulation: NNk(d) = d′ ∈ D : cos( ⃗d, ⃗d′) ≥ tsim"
  - [corpus] "Average neighbor FMR=0.507" (weak signal, not directly validating high threshold effectiveness)

### Mechanism 3
- Claim: The combination of high-quality seed data and relevant retrieved documents enables the training of a robust multi-label text classifier that can accurately predict domain labels for new documents.
- Mechanism: The seed data provides diverse domain examples, and the retrieved documents provide real-world instances labeled with high confidence. This combination creates a rich training set that covers various aspects of the domain, allowing the classifier to learn robust domain-specific features.
- Core assumption: The classifier can effectively learn from the combination of synthetic seed data and real retrieved documents, capturing the domain's nuances.
- Evidence anchors:
  - [abstract] "We then aggregate the retrieved documents for all target domains with the multi-labels to train a multi-label text classifier."
  - [section] "We then aggregate the retrieved documents for all target domains with the multi-labels to train a multi-label text classifier."
  - [corpus] "Average neighbor FMR=0.507" (weak signal, not directly validating classifier effectiveness)

## Foundational Learning

- Concept: Semantic embeddings and nearest neighbor search
  - Why needed here: The framework relies on converting text to vector embeddings and finding semantically similar documents through nearest neighbor search. Understanding how embeddings capture semantic meaning and how similarity metrics work is crucial for grasping the data mining process.
  - Quick check question: How does cosine similarity measure the semantic relatedness between two text documents represented as vectors?

- Concept: Continued pre-training (CPT) and catastrophic forgetting
  - Why needed here: The framework uses CPT to adapt pre-trained LLMs to specific domains. Understanding CPT and the risk of catastrophic forgetting (forgetting previously learned knowledge) is essential for appreciating the need for mixing domain-specific and general data during training.
  - Quick check question: Why is it important to include some general data from the original training distribution when doing domain-specific continued pre-training?

- Concept: Multi-label classification
  - Why needed here: Documents can belong to multiple domains, so the framework trains a multi-label classifier. Understanding how multi-label classification differs from single-label and how to handle multiple labels per instance is key to grasping the labeling process.
  - Quick check question: How does a multi-label classifier differ from a single-label classifier in terms of output and training data structure?

## Architecture Onboarding

- Component map:
  Seed Data Generator -> Document Embedding Pipeline -> Vector Database -> Nearest Neighbor Extractor -> Classifier Trainer -> Domain Adapter

- Critical path: Seed Data Generator → Document Embedding Pipeline → Vector Database → Nearest Neighbor Extractor → Classifier Trainer → Domain Adapter

- Design tradeoffs:
  - Seed data diversity vs. domain specificity: More diverse seed data captures more of the domain but may include less relevant aspects.
  - Similarity threshold vs. dataset size: Higher threshold ensures relevance but may limit the number of retrieved documents.
  - Classifier complexity vs. training efficiency: More complex classifiers may perform better but require more resources to train.

- Failure signatures:
  - Poor seed data quality: Classifier performance is low, and retrieved documents are not relevant to the target domain.
  - Ineffective embeddings: Nearest neighbor search does not retrieve semantically similar documents, leading to poor classifier training data.
  - High similarity threshold issues: Very few documents are retrieved, limiting the dataset size and potentially leading to overfitting.

- First 3 experiments:
  1. Seed Data Quality Check: Generate seed data for a target domain and compute lexical diversity metrics (e.g., type-token ratio, hapax legomena) to ensure they are similar to real domain documents.
  2. Embedding and Retrieval Test: Embed a small set of seed and real documents, perform nearest neighbor search, and manually inspect the retrieved documents to ensure semantic relevance.
  3. Classifier Training and Evaluation: Train a simple classifier (e.g., logistic regression) on a small labeled dataset and evaluate its performance on a held-out test set to ensure it can learn from the combined seed and real data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal mixing ratio between domain-specific data mined by DoPAMine and unlabeled Common Crawl data to maximize downstream task performance while avoiding catastrophic forgetting?
- Basis in paper: Inferred - The paper mentions conducting ablation experiments with a 25% curated in-domain data to 75% unlabeled Common Crawl data ratio, but notes that finding the optimal ratio is extraneous to the DoPAMine methodology.
- Why unresolved: The paper did not explore varying the mixing ratio to find the upper bound of curated in-domain data that does not lead to catastrophic forgetting and improves performance.
- What evidence would resolve it: Conducting ablation studies with different mixing ratios (e.g., 10%, 20%, 30%, 40%, 50%) and evaluating their impact on downstream task performance would provide insights into the optimal ratio.

### Open Question 2
- Question: How does the quality and performance of synthetic seed data generated by the latest state-of-the-art LLMs (e.g., Claude 3.5 Sonnet or Claude 3 Opus) compare to that of Claude 3 Sonnet used in the paper?
- Basis in paper: Inferred - The paper mentions that while Claude 3 Sonnet was used for synthetic data generation, employing the latest SOTA models could potentially enhance the quality of synthetic data and improve overall performance.
- Why unresolved: The paper used Claude 3 Sonnet as it was the most advanced accessible model at the time, but did not explore the impact of using more recent LLM versions.
- What evidence would resolve it: Generating synthetic seed data using the latest SOTA LLMs and comparing their quality (e.g., lexical metrics, semantic similarity to real data) and downstream task performance with the results obtained using Claude 3 Sonnet would provide insights into the potential benefits of using more advanced models.

### Open Question 3
- Question: Would incorporating a combination of synthetic and real data mined by DoPAMine into the domain adaptation training process improve performance compared to using only real data?
- Basis in paper: Inferred - The paper mentions that while real documents mined using DoPAMine were utilized for domain adaptation training, combining some synthetic data with the curated data might improve performance, as evidenced by recent works.
- Why unresolved: The paper did not explore the impact of combining synthetic and real data on the performance of the domain-adapted language models.
- What evidence would resolve it: Conducting ablation studies where the domain adaptation training data includes different proportions of synthetic and real data (e.g., 100% real, 75% real + 25% synthetic, 50% real + 50% synthetic) and evaluating their impact on downstream task performance would provide insights into the potential benefits of combining synthetic and real data.

## Limitations
- Limited evaluation to two specific domains (healthcare and finance) with two model sizes (7B parameters)
- High similarity threshold may limit dataset diversity and size, particularly for niche domains
- Reliance on LLM-generated synthetic data introduces uncertainty about domain representation accuracy across all possible domains

## Confidence
- High Confidence: The framework's core methodology of using LLM-generated seeds to mine domain-specific data is technically sound and well-established. The continued pre-training approach with mixed domain and general data is a proven technique for avoiding catastrophic forgetting.
- Medium Confidence: The reported performance improvements (4.9-6.7% on domain tasks) are based on specific experimental conditions and may vary significantly with different domains, corpora, or evaluation protocols. The choice of similarity threshold and its impact on dataset quality could be more thoroughly validated.
- Low Confidence: The long-term effectiveness of synthetic seed data quality across diverse domains remains uncertain without broader validation. The framework's scalability to very large corpora and its performance with different embedding models or vector database configurations have not been thoroughly tested.

## Next Checks
1. **Seed Data Quality Validation**: Conduct a systematic comparison of lexical diversity metrics (type-token ratio, hapax legomena, lexical density) between synthetic seed data and real domain documents across multiple domains to quantify how well the LLM captures domain characteristics.

2. **Threshold Sensitivity Analysis**: Systematically vary the similarity threshold (tsim) from 0.7 to 0.95 and measure its impact on dataset size, classifier performance, and final model effectiveness to identify optimal threshold ranges for different domain characteristics.

3. **Cross-Domain Generalization Test**: Apply DoPAMine to at least three additional diverse domains (e.g., legal, scientific research, and technical documentation) and evaluate whether the performance improvements observed in healthcare and finance generalize across domains with different vocabulary sizes, document structures, and semantic complexities.