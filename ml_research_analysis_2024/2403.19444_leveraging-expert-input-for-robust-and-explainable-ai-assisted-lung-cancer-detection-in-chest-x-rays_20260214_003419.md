---
ver: rpa2
title: Leveraging Expert Input for Robust and Explainable AI-Assisted Lung Cancer
  Detection in Chest X-rays
arxiv_id: '2403.19444'
source_url: https://arxiv.org/abs/2403.19444
tags:
- clinical
- chest
- concepts
- explanations
- concept
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the interpretability and robustness of deep
  learning models for lung cancer detection in chest X-rays. Post-hoc image-based
  methods (LIME, SHAP, Grad-CAM) failed to highlight clinically relevant regions and
  showed low agreement with each other and ground truth pathology locations.
---

# Leveraging Expert Input for Robust and Explainable AI-Assisted Lung Cancer Detection in Chest X-rays

## Quick Facts
- arXiv ID: 2403.19444
- Source URL: https://arxiv.org/abs/2403.19444
- Reference count: 40
- Primary result: Expert-driven concept bottleneck model (ClinicXAI) achieves 100% concept capture accuracy, F1 score 0.891, and superior adversarial robustness compared to standard InceptionV3

## Executive Summary
This study addresses critical limitations in explainable AI for lung cancer detection by evaluating post-hoc image-based methods (LIME, SHAP, Grad-CAM) and text-based approaches (CXR-LLaV A, XCBs), which demonstrated poor clinical relevance and low agreement with ground truth pathology locations. The authors developed ClinicXAI, an expert-driven concept bottleneck model incorporating radiologist-curated clinical concepts that bridges the gap between model predictions and clinical decision-making. ClinicXAI achieved 100% accuracy in capturing ground truth clinical concepts, maintained high classification performance (F1 score 0.891), and demonstrated significantly greater resilience to adversarial perturbations compared to standard InceptionV3. The findings underscore the importance of integrating domain expertise into AI systems for medical diagnostics.

## Method Summary
The study developed ClinicXAI using a concept bottleneck architecture where an InceptionV3-based model predicts clinical concepts from chest X-rays, and a Decision Tree uses these concepts to predict final diagnoses. Clinical concepts were defined through collaboration with a consultant radiologist and extracted from radiology reports using NLP techniques. The model was trained on the MIMIC-CXR dataset (19,478 balanced pairs) using 10-fold cross-validation and evaluated on VinDr-CXR for pathology location alignment. Adversarial robustness was tested using FGSM, PGD, and SimBA attacks, with comparisons to standard InceptionV3 and other XAI approaches including LIME, SHAP, Grad-CAM, CXR-LLaV A, and XCBs.

## Key Results
- Post-hoc image-based methods (LIME, SHAP, Grad-CAM) failed to highlight clinically relevant regions and showed low agreement with ground truth pathology locations
- Text-based approaches demonstrated limited clinical relevance, with CXR-LLaV A suffering from high false-negative rates and XCBs generating ambiguous concepts
- ClinicXAI achieved 100% accuracy in capturing ground truth clinical concepts, F1 score 0.891, and superior robustness to adversarial attacks compared to standard InceptionV3

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert-curated clinical concepts improve model interpretability and clinical relevance in lung cancer detection
- Mechanism: ClinicXAI replaces unsupervised concept learning with domain-expert defined clinical concepts, creating a bridge between model predictions and radiologist decision-making
- Core assumption: The expert-defined concepts accurately represent clinically meaningful diagnostic features
- Evidence anchors:
  - [abstract]: "To address these limitations, we collaborated with a radiologist to define diagnosis-specific clinical concepts and developed ClinicXAI, an expert-driven approach leveraging the concept bottleneck methodology."
  - [section]: "ClinicXAI, an expert-driven CBM approach that incorporates radiologist-curated clinical concepts"
  - [corpus]: Found related papers with FMR ~0.37-0.47, suggesting moderate contextual similarity but limited direct evidence for this specific mechanism

### Mechanism 2
- Claim: Concept bottleneck architecture improves robustness to adversarial attacks
- Mechanism: By decomposing the classification pipeline into concept prediction and label prediction stages, the model creates intermediate representations that are more stable and interpretable
- Core assumption: The concept layer captures essential diagnostic features that remain consistent under adversarial perturbations
- Evidence anchors:
  - [abstract]: "Our analysis demonstrates that ClinicXAI exhibits significantly greater resilience to adversarial perturbations"
  - [section]: "We find that ClinicXAI exhibits significantly greater robustness to adversarial perturbations compared to the standard InceptionV3 model"
  - [corpus]: No direct evidence in corpus neighbors for this specific robustness mechanism

### Mechanism 3
- Claim: NLP-based clinical concept extraction from radiology reports creates accurate ground truth for model training
- Mechanism: Natural language processing techniques automatically identify and classify clinical concepts from free-text radiology reports, creating a scalable labeling approach
- Core assumption: The NLP pipeline accurately captures all relevant clinical concepts without introducing significant false positives or negatives
- Evidence anchors:
  - [section]: "Using natural language processing (NLP) techniques, these concepts are automatically extracted from radiology reports during model training"
  - [section]: "We apply the concept extraction approach detailed in Section III-C to all 1948 radiological reports in the MIMIC-CXR test set"
  - [corpus]: Weak evidence - corpus contains no papers specifically addressing NLP-based concept extraction from radiology reports

## Foundational Learning

- Concept: Concept Bottleneck Models
  - Why needed here: This architecture enables interpretable intermediate representations that bridge raw image data and final classification decisions
  - Quick check question: What are the two stages in a concept bottleneck model, and why is this decomposition beneficial for interpretability?

- Concept: Adversarial Machine Learning
  - Why needed here: Understanding attack vectors and defense mechanisms is critical for developing robust medical AI systems
  - Quick check question: What's the key difference between white-box and black-box adversarial attacks, and why does this matter for medical AI deployment?

- Concept: Natural Language Processing for Medical Text
  - Why needed here: Extracting clinical concepts from radiology reports requires understanding medical terminology and negation patterns
  - Quick check question: How does the concept extraction pipeline handle negation (e.g., "no evidence of mass") in radiology reports?

## Architecture Onboarding

- Component map:
  Concept Prediction Model (InceptionV3-based) -> Label Prediction Model (Decision Tree) -> Final Diagnosis

- Critical path:
  1. Preprocess chest X-ray and extract clinical concepts from associated report
  2. Train concept prediction model on image-concept pairs
  3. Train label prediction model on concept-label pairs
  4. During inference, concept model processes image, label model processes concepts
  5. Return top clinical concepts as explanation along with diagnosis

- Design tradeoffs:
  - Accuracy vs. interpretability: Adding concept layer slightly reduces raw classification accuracy but significantly improves interpretability
  - Expert time vs. model performance: Requires one-time expert input for concept definition but enables automatic concept extraction thereafter
  - Complexity vs. robustness: Two-stage architecture adds complexity but provides better adversarial robustness

- Failure signatures:
  - High false positive rate in concept prediction: Likely indicates NLP extraction issues or concept definitions too broad
  - Concept predictions not matching label predictions: Suggests misalignment between concept model and label model training
  - Poor adversarial robustness: May indicate concept layer not capturing robust features or insufficient adversarial training

- First 3 experiments:
  1. Validate concept extraction accuracy by comparing NLP-extracted concepts against manually annotated subset of reports
  2. Test concept prediction model performance on held-out data to ensure concepts are being learned correctly
  3. Evaluate explanation quality by having radiologist review concept predictions for sample cases

## Open Questions the Paper Calls Out
- How does the performance of ClinicXAI compare to other expert-driven explainable AI approaches for different medical imaging modalities beyond chest X-rays?
- What is the impact of using different types of expert input (e.g., radiologists with varying experience levels, other medical specialists) on the performance and interpretability of ClinicXAI?
- How does the inclusion of temporal information (e.g., longitudinal patient data) affect the interpretability and robustness of ClinicXAI for lung cancer detection?

## Limitations
- The expert-curated concept definitions may not capture all clinically relevant features for lung cancer detection
- The study's reliance on a single radiologist for concept validation introduces potential bias
- The evaluation focuses on a single disease domain (lung cancer), limiting conclusions about broader applicability

## Confidence
- **High Confidence**: The comparison of post-hoc XAI methods (LIME, SHAP, Grad-CAM) showing poor clinical relevance and low inter-method agreement
- **Medium Confidence**: The claim that expert-curated concepts improve model interpretability
- **Medium Confidence**: The robustness improvements against adversarial attacks

## Next Checks
1. Conduct a formal inter-rater reliability study with multiple radiologists to validate the expert-curated clinical concepts
2. Test ClinicXAI architecture on chest X-rays from different thoracic pathologies to evaluate generalizability
3. Implement ClinicXAI in a clinical setting with actual diagnostic workflows to assess practical utility