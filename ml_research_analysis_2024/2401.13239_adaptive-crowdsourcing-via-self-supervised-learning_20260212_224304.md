---
ver: rpa2
title: Adaptive Crowdsourcing Via Self-Supervised Learning
arxiv_id: '2401.13239'
source_url: https://arxiv.org/abs/2401.13239
tags:
- estimates
- estimate
- crowdworkers
- crowdworker
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a new approach to crowdsourcing aggregation
  called "predict-each-worker" that addresses limitations of existing methods when
  dealing with complex relationships among crowdworker estimates. The key innovation
  is using self-supervised learning to predict each worker's estimate based on others'
  estimates, then computing aggregation weights that account for skill and independence.
---

# Adaptive Crowdsourcing Via Self-Supervised Learning

## Quick Facts
- arXiv ID: 2401.13239
- Source URL: https://arxiv.org/abs/2401.13239
- Authors: Anmol Kagrecha; Henrik Marklund; Benjamin Van Roy; Hong Jun Jeon; Richard Zeckhauser
- Reference count: 40
- Primary result: New approach using self-supervised learning to predict each worker's estimate based on others' estimates, then computing aggregation weights that account for skill and independence

## Executive Summary
This paper presents a new approach to crowdsourcing aggregation called "predict-each-worker" that addresses limitations of existing methods when dealing with complex relationships among crowdworker estimates. The key innovation is using self-supervised learning to predict each worker's estimate based on others' estimates, then computing aggregation weights that account for skill and independence. The method outperforms simple averaging and matches the performance of expectation maximization (EM) with large datasets. Theoretical analysis proves asymptotic optimality for Gaussian data-generating processes, and experiments show strong finite-data performance.

## Method Summary
The method predicts each crowdworker's estimate using self-supervised learning (SSL) based on past observations and estimates of other crowdworkers. For each crowdworker, an SSL model is trained to predict their estimate from others' estimates. Aggregation weights are then computed based on the expected error (skill) and independence (sensitivity to others' estimates). The approach can handle complex patterns using neural networks while maintaining computational tractability. The algorithm is designed for settings where outcomes are not observed, which is common in practical crowdsourcing applications.

## Key Results
- Predict-each-worker outperforms simple averaging for all tested configurations of crowdworkers and engagements
- With large datasets, the method matches the performance of clairvoyant and EM-based policies
- Theoretical proof of asymptotic optimality for Gaussian data-generating processes
- SSL models can capture complex patterns among crowdworker estimates without requiring ground truth outcomes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The predict-each-worker approach improves aggregation accuracy by learning complex patterns among crowdworker estimates through self-supervised learning
- Mechanism: Each crowdworker's estimate is predicted using others' estimates via neural networks, then aggregation weights are computed based on predictability (expected error) and independence (SSL gradient)
- Core assumption: Crowdworker estimates exhibit learnable patterns that can be captured by neural networks
- Evidence anchors:
  - [abstract] "leverages self-supervised learning to predict each worker's estimate based on others' estimates, then computing aggregation weights that account for skill and independence"
  - [section 3.1] "For each crowdworker, predict-each-worker produces an SSL model that predicts the crowdworker's estimate based on past observations and estimates of other crowdworkers"
  - [corpus] Weak evidence - related papers focus on crowdworker preferences and coordination rather than pattern learning

### Mechanism 2
- Claim: The aggregation weights are asymptotically optimal as the number of engagements per crowdworker grows
- Mechanism: The aggregation formula balances skill (predictability) against independence (sensitivity to others' estimates) to converge to optimal weights
- Core assumption: With sufficient data, linear SSL models converge to the true conditional expectations
- Evidence anchors:
  - [abstract] "prove that our algorithm is asymptotically optimal for a Gaussian data-generating process"
  - [section 3.2] "for each k, θ determines coefficients u(k)* ∈ ℜ^(K-1) and a noise variance ℓ(k)* such that Yt,k = (u(k)*)^T Yt,-k + ηt, with ηt|θ ~ N(0, ℓ(k)*)"
  - [section 3.3] "the aggregation weights ˆνt converge to ν* as the linear SSL model parameters converge to the true parameters"

### Mechanism 3
- Claim: The method outperforms simple averaging and matches EM performance with large datasets
- Mechanism: By accounting for skill variations and correlations among crowdworkers, the weighted aggregation produces more accurate estimates than uniform averaging
- Core assumption: Crowdworkers have varying skill levels and their estimates are correlated due to shared information sources
- Evidence anchors:
  - [abstract] "When skills vary across crowdworkers or their estimates correlate, the weighted sum offers a more accurate group estimate than the average"
  - [section 4.2] "to match the performance attained by averaging over one hundred crowdworkers, the clairvoyant policy requires about twenty crowdworkers, while only-skills-clairvoyant needs about seventy"
  - [section 4.3] "our algorithm outperforms averaging for all K and t" and "our algorithm performs equally well compared to the clairvoyant, as well as EM policies, for large t"

## Foundational Learning

- Concept: Self-supervised learning (SSL)
  - Why needed here: SSL enables learning patterns among crowdworker estimates without requiring ground truth outcomes
  - Quick check question: How does SSL differ from supervised learning in terms of data requirements?

- Concept: Exchangeability in probability theory
  - Why needed here: Exchangeability justifies treating crowdworker estimates as samples from a common distribution and enables the use of de Finetti's theorem
  - Quick check question: What is the relationship between exchangeability and the existence of a latent variable θ?

- Concept: Bayesian linear regression
  - Why needed here: BLR provides a principled way to estimate SSL model parameters with regularization and uncertainty quantification
  - Quick check question: How does BLR handle the trade-off between fitting data and avoiding overfitting through its prior distribution?

## Architecture Onboarding

- Component map: SSL models (K models, each predicting one crowdworker's estimate from others) -> Aggregation weight computation (using expected error and SSL gradient) -> Hyperparameter tuning (for SSL and aggregation) -> Data preprocessing (handling missing estimates, incorporating context/metadata)

- Critical path: SSL model training → SSL statistics computation → Aggregation weight calculation → Final estimate generation

- Design tradeoffs:
  - K separate SSL models vs. single model with K output heads (computational efficiency vs. potential overfitting)
  - Linear vs. neural network SSL models (simplicity and interpretability vs. flexibility for complex patterns)
  - Regularization strength (bias-variance tradeoff)

- Failure signatures:
  - Poor performance relative to averaging → SSL models failing to learn meaningful patterns
  - Performance degradation with more data → Overfitting in SSL models
  - Instability in aggregation weights → Incorrect hyperparameter settings or numerical issues

- First 3 experiments:
  1. Implement predict-each-worker with linear SSL models on synthetic Gaussian data to verify convergence to optimal weights
  2. Compare performance against averaging and EM on same synthetic data across different K and t values
  3. Extend to neural network SSL models and test on data with non-linear patterns among crowdworkers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical performance guarantee of the predict-each-worker algorithm when the data-generating process deviates significantly from the Gaussian assumption?
- Basis in paper: [explicit] The paper proves asymptotic optimality for Gaussian data-generating processes but does not extend this analysis to non-Gaussian settings
- Why unresolved: The authors acknowledge the need for future work on more complex data-generating processes beyond Gaussian assumptions
- What evidence would resolve it: Empirical studies or theoretical proofs showing convergence rates and error bounds for predict-each-worker under non-Gaussian distributions

### Open Question 2
- Question: How does the predict-each-worker algorithm perform when crowdworker estimates exhibit temporal dependencies or non-stationary patterns?
- Basis in paper: [inferred] The paper assumes exchangeability of crowdworker estimates across time periods, which implies stationarity
- Why unresolved: Real-world crowdsourcing applications often involve evolving crowdworker behaviors and information cascades that violate exchangeability
- What evidence would resolve it: Experiments on datasets with known temporal correlations or time-varying crowdworker characteristics

### Open Question 3
- Question: What is the optimal trade-off between the number of crowdworkers (K) and the number of engagements per crowdworker (t) for minimizing expected error?
- Basis in paper: [explicit] The authors compare different values of K and t but do not provide a systematic analysis of their relative importance
- Why unresolved: The paper shows that predict-each-worker outperforms averaging but does not quantify the marginal benefit of increasing K versus t
- What evidence would resolve it: A parametric study varying both K and t systematically to identify regions of diminishing returns

### Open Question 4
- Question: How sensitive is the predict-each-worker algorithm to the choice of hyperparameters (v, r, Λ, λℓ, u, ℓ) when applied to real-world datasets?
- Basis in paper: [explicit] The authors describe a hyperparameter tuning procedure but acknowledge this was done using knowledge of the clairvoyant distribution
- Why unresolved: The paper discusses how to tune hyperparameters without observing outcomes but does not validate this approach empirically
- What evidence would resolve it: A comprehensive sensitivity analysis showing performance variation across different hyperparameter settings on real datasets

### Open Question 5
- Question: Can the predict-each-worker framework be extended to handle non-continuous estimate types such as text, bounding boxes, or single-question wisdom-of-crowds scenarios?
- Basis in paper: [explicit] The authors identify this as an important direction for future work in the concluding remarks
- Why unresolved: The aggregation rule is currently limited to continuous estimates, and no extensions are proposed for other data types
- What evidence would resolve it: A demonstration of predict-each-worker applied to a non-continuous crowdsourcing task with quantitative performance metrics

## Limitations

- Theoretical guarantees rely heavily on the Gaussian data-generating process assumption, which may not hold in practical crowdsourcing scenarios
- Asymptotic optimality assumes an increasing number of engagements per crowdworker, but practical applications often have limited data per worker
- Paper does not provide extensive empirical validation on real-world datasets, with most experiments conducted on synthetic data

## Confidence

- High Confidence: The mechanism by which SSL enables pattern learning among crowdworker estimates is well-established and theoretically grounded
- Medium Confidence: The asymptotic optimality proof for Gaussian processes is mathematically sound, but its practical relevance depends on data availability
- Low Confidence: The claim of matching EM performance with large datasets lacks extensive empirical validation on real-world data

## Next Checks

1. **Real-world dataset validation**: Apply the method to a public crowdsourcing dataset (e.g., MTurk image annotation tasks) and compare performance against established baselines under realistic conditions

2. **Non-Gaussian data generation**: Test the algorithm's performance when the data-generating process deviates from Gaussian assumptions, varying the degree of non-normality and examining when optimality breaks down

3. **Sensitivity analysis**: Systematically vary the number of crowdworkers, number of observations per worker, and pattern complexity to identify performance thresholds and failure modes