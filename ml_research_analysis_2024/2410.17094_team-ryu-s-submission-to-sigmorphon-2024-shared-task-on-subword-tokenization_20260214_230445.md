---
ver: rpa2
title: Team Ryu's Submission to SIGMORPHON 2024 Shared Task on Subword Tokenization
arxiv_id: '2410.17094'
source_url: https://arxiv.org/abs/2410.17094
tags:
- words
- tokenizer
- word
- tokens
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores whether morphological segmentation methods
  can be used as part of subword tokenizers. Two approaches are adopted: a statistical
  segmentation method (Morfessor) and a transformer-based sequence-to-sequence model.'
---

# Team Ryu's Submission to SIGMORPHON 2024 Shared Task on Subword Tokenization

## Quick Facts
- arXiv ID: 2410.17094
- Source URL: https://arxiv.org/abs/2410.17094
- Reference count: 5
- Key outcome: Morphological segmentation methods can be as effective as commonly used subword tokenizers, and tokenizers with balanced frequency distributions tend to work better for language models.

## Executive Summary
This paper explores whether morphological segmentation methods can serve as effective subword tokenizers for language models. Two approaches are investigated: a statistical segmentation method (Morfessor) and a transformer-based sequence-to-sequence model. The findings show that morphological segmentation can achieve competitive performance with standard tokenizers while providing better linguistic alignment. The study also reveals that tokenizers with more balanced frequency distributions perform better, which can be achieved by keeping frequent words as unique tokens rather than decomposing them.

## Method Summary
The approach involves using morphological segmentation models to create token vocabularies for language model pretraining. Two segmentation methods are employed: Morfessor (a statistical model) and a neural transformer-based seq2seq model. These models are trained on SIGMORPHON morphological segmentation data and used to segment words from the BabyLM corpus. Three vocabulary construction strategies are tested: type-based (using corpus type frequencies), token-based (using corpus token frequencies), and a frequency-retained approach that keeps the most frequent words as unique tokens. The resulting vocabularies are used to pretrain ELC-BERT models, which are then fine-tuned on three downstream tasks: Word and Definition, Word and Morphology, and Word and Word classification.

## Key Results
- Morphological segmentation tokenizers can outperform or match commonly used tokenizers like WordPiece on morphologically-rich downstream tasks
- Tokenizers that keep frequent words as unique tokens achieve better average performance across tasks
- A balanced token frequency distribution, measured by higher entropy, correlates with better language model performance
- The neural seq2seq approach performs well at morpheme segmentation but generates non-existent morphs that harm tokenization quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Morphological segmentation methods can be as effective as commonly used subword tokenizers.
- Mechanism: Morphological segmentation aligns tokens with morpheme boundaries, potentially improving linguistic representation while maintaining similar performance on downstream tasks.
- Core assumption: The linguistic alignment of morphological segmentation provides comparable benefits to the compression and OOV handling of statistical methods like BPE.
- Evidence anchors:
  - [abstract] states "The prediction results show that morphological segmentation could be as effective as commonly used subword tokenizers."
  - [section] reports "In the morphological task, morphological segmentation based tokenizers commonly outperform currently used tokenizers, like Wordpiece in the ELC-BERT model."
  - [corpus] shows related work on morphological segmentation effectiveness but no direct comparative evidence in this specific setup.
- Break condition: If the morphological segmentation produces non-existent morphs or fails to handle OOV words effectively, the performance advantage would diminish.

### Mechanism 2
- Claim: Tokenizers with more balanced token frequency distributions tend to work better for language models.
- Mechanism: A balanced distribution prevents the model from over-representing very frequent tokens while still maintaining efficiency through unique representations of common words.
- Core assumption: The relationship between token frequency balance and model performance is causal rather than correlational.
- Evidence anchors:
  - [abstract] states "A tokenizer with a balanced token frequency distribution tends to work better."
  - [section] shows "The tokenizer which keeps the most frequent words as tokens has the highest entropy value" and correlates this with better performance.
  - [corpus] references Zouhar et al. (2023) who found correlations between token distribution and model performance, but this specific experiment design isn't in the corpus.
- Break condition: If the frequency distribution balance becomes too extreme (e.g., keeping too many words as unique tokens), the vocabulary size could become unmanageable and negate the benefits.

### Mechanism 3
- Claim: Retaining the most frequent words as unique tokens improves tokenizer performance.
- Mechanism: Keeping frequent words intact preserves their semantic integrity while allowing the tokenizer to still benefit from subword decomposition for less common terms.
- Core assumption: The semantic value of frequent words as complete units outweighs the vocabulary efficiency gained from always decomposing them.
- Evidence anchors:
  - [section] states "Among three morfessor based tokenizers, the one with the most frequent words retained as independent tokens performs better on average."
  - [section] shows this tokenizer has "more tokens that appear between 1000 and 10000 times" compared to others.
  - [corpus] references Wolleb et al. (2023) who concluded "the strength of BPE mainly comes from its ability to keep frequent words as unique tokens."
- Break condition: If the threshold for "frequent" is set too low, the vocabulary becomes too large; if too high, the benefits of unique token representation are lost.

## Foundational Learning

- Concept: Morphological segmentation and its distinction from statistical tokenization
  - Why needed here: The paper compares morphological segmentation methods (Morfessor, neural seq2seq) with statistical methods like BPE, requiring understanding of how they differ in approach.
  - Quick check question: What is the fundamental difference between morphological segmentation and Byte-Pair Encoding in terms of how they determine token boundaries?

- Concept: Token frequency distribution and Shannon entropy
  - Why needed here: The paper uses entropy to measure token distribution balance and correlates this with model performance.
  - Quick check question: How does Shannon entropy quantify the "balance" of a token frequency distribution?

- Concept: Language model pretraining and fine-tuning workflows
  - Why needed here: The experiments involve pretraining on BabyLM data and fine-tuning on three specific downstream tasks.
  - Quick check question: What are the key differences between pretraining and fine-tuning in the context of transformer-based language models?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Morphological segmentation models -> Vocabulary building -> Tokenizer variants -> Language model pretraining -> Downstream task fine-tuning

- Critical path:
  1. Preprocess BabyLM corpus
  2. Train morphological segmentation models on SIGMORPHON data
  3. Build token vocabularies using different strategies
  4. Pretrain language models using each tokenizer variant
  5. Fine-tune on downstream tasks
  6. Evaluate and compare performance

- Design tradeoffs:
  - Vocabulary size vs. token frequency balance: Larger vocabularies from keeping frequent words improve balance but increase model size
  - Morphological accuracy vs. computational efficiency: More complex segmentation models may capture morphology better but train slower
  - Generalization vs. specialization: Tokenizers optimized for morphology may perform differently on other tasks

- Failure signatures:
  - Poor downstream performance: May indicate suboptimal vocabulary distribution or inadequate morphological segmentation
  - Extremely large vocabularies: Suggests threshold for keeping frequent words is too low
  - Neural model hallucinations: Indicates seq2seq model generating non-existent morphs, potentially harming vocabulary quality

- First 3 experiments:
  1. Compare type-based vs token-based Morfessor tokenizers on a small subset to verify frequency incorporation effect
  2. Test different thresholds for keeping frequent words as unique tokens (e.g., 500, 1000, 1500) to find optimal balance point
  3. Evaluate neural vs statistical morphological segmentation on morpheme segmentation accuracy before full tokenization pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we determine the optimal point where tokenizers can handle OOV words while maintaining a balanced vocabulary distribution?
- Basis in paper: [explicit] The paper concludes with this as an open question, noting that keeping frequent words as unique tokens helps achieve balance but finding the optimal threshold remains unresolved.
- Why unresolved: The paper shows that different approaches to vocabulary construction (type-based, token-based, frequency-based) yield varying results, but doesn't provide a systematic method for finding the ideal balance between unique tokens and subword units.
- What evidence would resolve it: Systematic experiments varying the frequency threshold for keeping words as unique tokens across multiple languages and tasks, measuring both OOV handling and model performance.

### Open Question 2
- Question: What is the relative importance of frequency, compositionality, and OOV handling in determining the effectiveness of subword tokenization?
- Basis in paper: [explicit] The paper states that subword tokenization advantages are summarized into these three aspects, but "few people have explored which part of advantages plays the most important role."
- Why unresolved: While the paper compares different tokenization approaches, it doesn't systematically isolate and measure the contribution of each advantage to model performance.
- What evidence would resolve it: Controlled experiments with tokenizers designed to isolate each advantage (e.g., Huffman-based tokenizer that only preserves frequency) across multiple tasks and model architectures.

### Open Question 3
- Question: Why do neural-based morphological segmentation models perform poorly as tokenizers despite excelling at morpheme segmentation?
- Basis in paper: [explicit] The paper observes that "neural models perform perfectly in the morpheme segmentation task, they are not suitable for tokenization, due to the issue of neural model's hallucination."
- Why unresolved: The paper identifies hallucination as a problem but doesn't investigate whether this is the only factor or explore potential modifications to neural segmentation approaches that could make them more suitable for tokenization.
- What evidence would resolve it: Comparative analysis of hallucination rates versus vocabulary balance in neural vs. statistical segmentation models, and experiments testing whether constraining neural models to existing vocabulary reduces their effectiveness.

## Limitations

- The paper lacks direct comparison between morphological segmentation and standard tokenizers like BPE on identical tasks
- Evaluation is limited to only three downstream tasks, which may not provide comprehensive coverage of language modeling capabilities
- Neural seq2seq model's hallucination problem is acknowledged but not quantified or systematically evaluated
- Vocabulary sizes for different tokenizer variants are not reported, making trade-offs difficult to assess

## Confidence

**High Confidence:**
- Morphological segmentation methods can achieve competitive performance with statistical tokenizers on morphologically-rich downstream tasks
- Keeping frequent words as unique tokens improves token frequency distribution balance
- Token frequency distribution correlates with downstream task performance

**Medium Confidence:**
- The specific relationship between token frequency entropy and model performance (causal vs. correlational)
- The superiority of neural seq2seq over statistical methods for morphological segmentation
- The general applicability of these findings beyond the three tested downstream tasks

**Low Confidence:**
- Claims about morphological segmentation being "as effective as commonly used subword tokenizers" without direct comparative evidence
- The optimal threshold for keeping frequent words as unique tokens
- Generalizability to languages with different morphological complexity than English

## Next Checks

1. **Direct Tokenizer Comparison**: Implement and compare Morfessor-based tokenizers against standard BPE and WordPiece tokenizers on identical downstream tasks using the same pretraining setup. This would validate whether morphological segmentation provides genuine advantages beyond frequency balancing.

2. **Segmentation Accuracy Analysis**: Systematically evaluate the morphological segmentation accuracy of both statistical and neural approaches on held-out morphological data, measuring precision, recall, and hallucination rates. This would clarify whether neural models actually produce better linguistic alignments.

3. **Vocabulary Size Impact Study**: Test the tokenizer variants across a range of vocabulary sizes (e.g., 5k, 10k, 20k tokens) to quantify the trade-off between frequency balance and model efficiency. This would provide concrete guidance on practical implementation thresholds.