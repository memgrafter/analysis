---
ver: rpa2
title: Automated Data Curation for Robust Language Model Fine-Tuning
arxiv_id: '2403.12776'
source_url: https://arxiv.org/abs/2403.12776
tags:
- data
- dataset
- response
- fine-tuning
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLEAR, a data-centric pipeline for improving
  LLM fine-tuning by systematically curating training datasets. CLEAR automatically
  filters low-quality data and corrects confidently improvable examples using LLM-derived
  confidence estimates (via BSDetector), without requiring stronger external models.
---

# Automated Data Curation for Robust Language Model Fine-Tuning

## Quick Facts
- arXiv ID: 2403.12776
- Source URL: https://arxiv.org/abs/2403.12776
- Reference count: 21
- Key outcome: Introduces CLEAR, a data-centric pipeline using LLM-derived confidence estimates to filter and correct noisy instruction-tuning datasets, significantly improving fine-tuned model performance without requiring stronger external models.

## Executive Summary
This paper introduces CLEAR, a data-centric pipeline for improving LLM fine-tuning by systematically curating training datasets. CLEAR automatically filters low-quality data and corrects confidently improvable examples using LLM-derived confidence estimates (via BSDetector), without requiring stronger external models. The approach works with any LLM and fine-tuning algorithm. Experiments on noisy versions of SQuAD-N, Email-N, and DROP-N datasets show that CLEAR significantly improves both JSON formatting accuracy and response accuracy of fine-tuned models (e.g., Llama-2 and GPT-3.5), often outperforming zero/few-shot GPT-4 and reducing performance degradation seen when fine-tuning on noisy data directly.

## Method Summary
CLEAR operates in two stages: Auto-Filter and Auto-Correct. First, BSDetector estimates confidence scores for each (prompt, response) pair using consistency checks across multiple LLM-generated candidates. Examples below a median confidence threshold are filtered out. The remaining data is used to fine-tune the base LLM. In the Auto-Correct stage, the fine-tuned LLM generates candidate responses for all prompts; BSDetector compares these to original responses and replaces originals when confidently better (threshold > 0.8). The final curated dataset undergoes one more fine-tuning pass. The method requires no stronger external models and works with any LLM architecture.

## Key Results
- CLEAR significantly improves JSON formatting accuracy and response accuracy on noisy SQuAD-N, Email-N, and DROP-N datasets
- Outperforms zero/few-shot GPT-4 on several tasks despite using smaller base models
- Reduces performance degradation from 20% noisy data to near-baseline levels
- Works across different LLM architectures (GPT-3.5 and Llama-2-7b)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Confidence-based filtering removes noisy examples that degrade fine-tuning performance.
- Mechanism: BSDetector estimates confidence that each response is good using consistency checks across multiple generated candidates and self-reflection scores, then filters out low-confidence pairs before training.
- Core assumption: LLM-generated confidence estimates correlate with true data quality.
- Evidence anchors:
  - [abstract]: "CLEAR estimates which training data is low-quality and either filters or corrects it. Automatically identifying which data to filter or correct is done via LLM-derived confidence estimates, to ensure only confident modifications to the dataset."
  - [section]: "Given an instruction fine-tuning dataset of input-output pairs {(xi, yi)n i=1}, we use BSDetector with the base pretrained LLM (before it is fine-tuned) to estimate a confidence score ci for each pair (xi, yi). We then filter out data pairs with low confidence scores below a predefined threshold γ: F = {(xi, yi)|ci > γ }."
- Break condition: If confidence estimates become noisy or systematically biased, filtering may remove useful data or retain harmful data.

### Mechanism 2
- Claim: Auto-correction stage improves dataset quality by replacing low-quality responses with high-confidence LLM-generated alternatives.
- Mechanism: Fine-tuned LLM generates candidate responses for each prompt; a separate LLM (BSDetector) compares candidate vs. original response and replaces original if confidently better.
- Core assumption: Fine-tuned LLM generates better responses for its own domain than the original dataset.
- Evidence anchors:
  - [section]: "For examples where the confidence (as estimated by BSDetector) that y′ i is better than yi falls above a threshold η: we replace their target response with the LLM generated response and retain this pair in our curated dataset."
  - [section]: "The LLM we fine-tuned after the Auto-Filter stage is specialized to our domain and should be able to generate some reasonable responses."
- Break condition: If fine-tuned LLM is poorly trained or generates low-quality responses, auto-correction may degrade dataset quality.

### Mechanism 3
- Claim: Data-centric approach improves model performance without requiring stronger external models or algorithmic changes.
- Mechanism: Same LLM is used for both data curation and fine-tuning, enabling improvement through better data rather than stronger base models.
- Core assumption: Improving training data quality yields better fine-tuned models than relying solely on stronger base models.
- Evidence anchors:
  - [abstract]: "We don't assume access to a stronger LLM than the model being fine-tuned (e.g. relying on GPT-4 when fine-tuning GPT-3.5), to see whether CLEAR can meaningfully improve the capabilities of any LLM."
  - [section]: "Experiments reveal that CLEAR consistently improves the performance of fine-tuned models across many datasets and models (like GPT-3.5 and Llama2)."
- Break condition: If data curation introduces biases or if the LLM lacks capability to generate high-quality responses even for its own domain.

## Foundational Learning

- Concept: BSDetector confidence estimation using consistency and self-reflection
  - Why needed here: Provides reliable measure of response quality without requiring stronger external models
  - Quick check question: How does BSDetector estimate confidence for a given response?

- Concept: Data filtering vs. data correction trade-offs
  - Why needed here: Understanding when to remove data vs. when to correct it is crucial for effective curation
  - Quick check question: What criteria determine whether to filter or correct a low-quality example?

- Concept: Temperature sampling and chain-of-thought for generating diverse responses
  - Why needed here: Multiple candidate responses are required for consistency-based confidence estimation
  - Quick check question: Why generate multiple candidate responses instead of just one?

## Architecture Onboarding

- Component map:
  - Dataset -> BSDetector confidence scores -> Auto-Filter -> Fine-tuning -> Auto-Correct -> Final fine-tuning

- Critical path: Dataset → BSDetector confidence scores → Auto-Filter → Fine-tuning → Auto-Correct → Final fine-tuning

- Design tradeoffs:
  - Confidence threshold γ: Lower thresholds retain more data but include more noise; higher thresholds ensure quality but reduce dataset size
  - Confidence threshold η: Lower thresholds enable more corrections but risk introducing errors; higher thresholds ensure only confident improvements
  - Number of candidate responses: More candidates improve confidence estimation but increase computational cost

- Failure signatures:
  - Model performance degrades after filtering: Confidence estimation may be systematically wrong
  - Auto-correction introduces errors: Fine-tuned LLM may not be sufficiently capable for its domain
  - Performance plateaus: Dataset may be already near-optimal or curation thresholds too conservative

- First 3 experiments:
  1. Run BSDetector on entire dataset to visualize confidence score distribution and set initial γ threshold
  2. Apply Auto-Filter with current γ threshold and evaluate performance gain on small validation set
  3. Apply Auto-Correct to filtered dataset and compare performance against filtered-only baseline

## Open Questions the Paper Calls Out
No specific open questions were called out in the provided paper content.

## Limitations
- BSDetector implementation details are not fully specified, creating uncertainty about faithful reproduction
- Dataset availability issues, particularly with Email-N and specific noisy versions of SQuAD/DROP
- Auto-correction may degrade quality if fine-tuned LLM generates inferior responses

## Confidence

**High Confidence Claims:**
- Data filtering based on LLM confidence scores improves fine-tuning performance when the base dataset contains noise
- The two-stage approach (filter then correct) provides incremental improvements over single-stage approaches
- CLEAR works across different LLM architectures (GPT-3.5 and Llama-2)

**Medium Confidence Claims:**
- BSDetector provides reliable confidence estimates without requiring stronger external models
- The specific threshold values (median for filtering, 0.8 for correction) are optimal across datasets
- Performance gains are consistent across all three datasets (SQuAD-N, Email-N, DROP-N)

**Low Confidence Claims:**
- CLEAR's approach generalizes to all LLM fine-tuning tasks beyond the tested domains
- The computational overhead of confidence estimation is negligible compared to performance gains
- Auto-correction never introduces errors that outweigh its benefits

## Next Checks

1. **BSDetector Implementation Validation**: Replicate BSDetector's confidence estimation mechanism using the described components (diversity sampling, consistency checks, self-reflection) and validate that confidence scores correlate with human-judged data quality on a small subset of the datasets.

2. **Threshold Sensitivity Analysis**: Systematically vary the filtering threshold (γ) and correction threshold (η) across a wider range of values to determine if the chosen thresholds (median, 0.8) are optimal or if performance is robust to different threshold settings.

3. **Domain Generalization Test**: Apply CLEAR to a different domain (e.g., summarization or code generation) with controlled noise injection to verify that the approach generalizes beyond the reading comprehension and email categorization tasks tested in the paper.