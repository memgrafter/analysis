---
ver: rpa2
title: 'E-QGen: Educational Lecture Abstract-based Question Generation System'
arxiv_id: '2404.13547'
source_url: https://arxiv.org/abs/2404.13547
tags:
- questions
- question
- student
- generator
- e-qgen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: E-QGen is an LLM-based system that generates potential student
  questions from lecture abstracts to help teachers prepare lessons. It uses multitask
  learning with LoRA fine-tuning on a novel dataset of lecture transcripts paired
  with real student questions from YouTube comments.
---

# E-QGen: Educational Lecture Abstract-based Question Generation System

## Quick Facts
- arXiv ID: 2404.13547
- Source URL: https://arxiv.org/abs/2404.13547
- Reference count: 1
- E-QGen outperforms GPT-3.5 and GPT-4 in generating questions closer to student inquiries, achieving ROUGE-1 of 0.267, ROUGE-2 of 0.087, ROUGE-L of 0.216, and BERTScore of 0.864, with higher diversity than GPT-4.

## Executive Summary
E-QGen is an LLM-based system that generates potential student questions from lecture abstracts to help teachers prepare lessons. It uses multitask learning with LoRA fine-tuning on a novel dataset of lecture transcripts paired with real student questions from YouTube comments. The system produces three question types—actual, probable, and potential student questions—and includes a reference question generator for general conceptual queries. Evaluated on computer science courses, E-QGen demonstrates superior performance in generating questions that align with student inquiries compared to GPT-3.5 and GPT-4.

## Method Summary
E-QGen fine-tunes a Vicuna-7b-v1.5 model using LoRA adapters across three multitask subtasks (PG, PS, PP) with real student questions from YouTube comments. The system segments transcripts into paragraphs, aligns questions to relevant segments using cosine similarity and LLM classification, then generates three types of student questions plus reference questions via GPT-3.5. The multitask approach leverages limited gold data by incorporating silver and platinum pairs to improve generalization.

## Key Results
- Outperforms GPT-3.5 and GPT-4 in generating questions closer to student inquiries
- Achieves ROUGE-1 of 0.267, ROUGE-2 of 0.087, ROUGE-L of 0.216, and BERTScore of 0.864
- Generates questions with higher diversity than GPT-4
- Ablation shows performance degrades most when excluding pseudo-training data

## Why This Works (Mechanism)

### Mechanism 1
Multitask learning with LoRA fine-tuning improves question quality and diversity by leveraging auxiliary tasks (silver and platinum pairs) when gold data is limited. Fine-tuning on three subtasks using LoRA adapters allows the model to learn patterns from broader, noisier data while still specializing on high-quality gold pairs.

### Mechanism 2
Using real student questions from YouTube comments as training data yields questions more aligned with student inquiries than SQuAD-based generators. Real student questions capture diverse inquiry patterns, confusion points, and terminology usage that SQuAD questions miss, so training on them produces more authentic student-like questions.

### Mechanism 3
Multimodal data sources (transcripts + comments) enable alignment of questions to conceptual segments via semantic similarity and classification, improving contextual relevance. Segmentation of transcripts into paragraphs + cosine similarity/LLM classification aligns questions to relevant content, allowing fine-tuning on concept-specific pairs.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**: Why needed: Enables efficient fine-tuning of large LLMs without full-parameter updates, crucial for multitask training on limited data. Quick check: What does LoRA stand for, and how does it reduce the number of trainable parameters compared to full fine-tuning?

- **ROUGE and BERTScore metrics**: Why needed: ROUGE measures n-gram overlap with references; BERTScore uses contextual embeddings to assess semantic similarity—both evaluate question quality and diversity. Quick check: Why might BERTScore be preferred over ROUGE for evaluating question generation diversity?

- **Multitask learning**: Why needed: Combines learning from multiple related tasks (actual, probable, potential questions) to improve generalization when gold data is scarce. Quick check: How does multitask learning help when one task has much less labeled data than others?

## Architecture Onboarding

- **Component map**: Lecture Abstract → Educational Transcript Generator (LLM) → Student Question Generator (multitask LoRA fine-tuned LLM) → Reference Question Generator (GPT-3.5) → Output questions
- **Critical path**: Abstract input → Transcript generation → Question generation (actual + reference) → Output
- **Design tradeoffs**: Multitask fine-tuning vs. single-task specialization; use of LoRA vs. full fine-tuning; real student question data vs. synthetic/cleaned data
- **Failure signatures**: Low ROUGE/BERTScore on test set; high variance in generated questions; alignment mismatch between questions and transcript segments
- **First 3 experiments**:
  1. Validate transcript generation quality by comparing generated transcript against ground-truth transcript for a sample abstract.
  2. Test student question generator on a held-out paragraph from training set and measure ROUGE-1, ROUGE-2, BERTScore against gold questions.
  3. Compare ablation results by training with and without silver/platinum pairs and measure impact on test metrics.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but identifies several areas for future work including extending the system to other academic fields beyond computer science and improving question quality through additional training techniques.

## Limitations
- Reliance on YouTube comments introduces potential noise and platform-specific artifacts
- Several critical hyperparameters (LoRA rank, learning rate) are not specified
- Evaluation limited to computer science courses, restricting generalizability
- Metrics measure surface similarity but don't capture pedagogical effectiveness

## Confidence
- **High Confidence**: Multitask learning with LoRA fine-tuning is technically sound and well-established
- **Medium Confidence**: Real student questions from YouTube yield more authentic student-like questions than SQuAD
- **Low Confidence**: System significantly improves lesson preparation efficiency for teachers (not empirically validated)

## Next Checks
1. Evaluate E-QGen on lecture abstracts from non-computer science domains to test generalizability
2. Conduct a blind study where teachers rate pedagogical quality and usefulness of E-QGen vs GPT-4 questions
3. Systematically vary the proportion of gold vs silver/platinum pairs to quantify multitask learning benefits