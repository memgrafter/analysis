---
ver: rpa2
title: Decoupled Prompt-Adapter Tuning for Continual Activity Recognition
arxiv_id: '2407.14811'
source_url: https://arxiv.org/abs/2407.14811
tags:
- learning
- prompt
- recognition
- continual
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel framework, DPAT, to tackle the problem
  of continual action recognition, where a model needs to learn from evolving video
  data streams without forgetting previously acquired knowledge. The key idea is to
  combine adapters for capturing spatial-temporal information with learnable prompts
  for mitigating catastrophic forgetting, using a decoupled training strategy.
---

# Decoupled Prompt-Adapter Tuning for Continual Activity Recognition

## Quick Facts
- arXiv ID: 2407.14811
- Source URL: https://arxiv.org/abs/2407.14811
- Reference count: 12
- Key outcome: DPAT achieves state-of-the-art performance in continual action recognition, with 61.3% accuracy and 22.3% backward forgetting on Kinetics-400 using CLIP ViT-B/16 backbone.

## Executive Summary
This paper introduces DPAT (Decoupled Prompt-Adapter Tuning), a novel framework for continual action recognition that addresses catastrophic forgetting when learning from evolving video data streams. DPAT combines adapters for spatial-temporal information capture with learnable prompts for forgetting mitigation, using a two-stage decoupled training strategy. The approach achieves state-of-the-art performance across Kinetics-400, ActivityNet, and EPIC-Kitchens-100 benchmarks, outperforming existing rehearsal-free methods and PIVOT when using the same CLIP ViT-B/16 backbone.

## Method Summary
DPAT uses a frozen ViT-B/16 backbone pre-trained on ImageNet-21K, with task-agnostic prompts (gT, gS) for general feature encoding and task-specific prompts (eT, eS) for adaptation. The method employs spatial and temporal adapters with bottleneck ratio of 0.25, positioned in blocks 3-5 for task-specific prompts and blocks 1-2 for task-agnostic prompts. Training follows a two-stage process: Stage 1 uses prefix tuning with learning rate 1e-3, followed by Stage 2 adapter tuning with learning rate 3e-4, both using Adam optimizer with cosine decay for 50 epochs at batch size 64.

## Key Results
- Achieves 61.3% accuracy and 22.3% backward forgetting on Kinetics-400 with CLIP ViT-B/16 backbone
- Outperforms existing rehearsal-free methods and PIVOT on same backbone
- State-of-the-art performance across Kinetics-400, ActivityNet, and EPIC-Kitchens-100 benchmarks

## Why This Works (Mechanism)

### Mechanism 1
DPAT uses decoupled training to balance adaptation and generalization, reducing catastrophic forgetting. The two-stage approach first establishes stable generalization through prefix tuning, then specializes with adapter tuning for task-specific adaptation. This preserves learned knowledge while allowing efficient adaptation to new tasks. The core assumption is that adapter tuning without prior prefix tuning leads to instability, while prefix tuning alone is too slow to adapt.

### Mechanism 2
Task-agnostic and task-specific prompts capture both shared and unique temporal/spatial features. Task-agnostic prompts (gT, gS) are fixed across tasks to encode general features, while task-specific prompts (eT, eS) adapt to new tasks via key-query matching. This configuration leverages the inherent strengths of pre-trained image encoders while minimizing modifications. The approach assumes general features can be captured in fixed prompts while task-specific information requires separate adaptation.

### Mechanism 3
Enhanced key-query matching with softmax normalization improves task-specific prompt selection. The matching loss uses softmax over all task keys to ensure predictions are influenced by relative similarity to all tasks, not just the nearest task key. This addresses the limitation where models may overfit to nearest task keys without normalization, leading to poor generalization and forgetting.

## Foundational Learning

- **Continual learning and catastrophic forgetting**: DPAT aims to learn from evolving video data streams without forgetting previously acquired knowledge. Quick check: What is the primary challenge that continual learning frameworks like DPAT aim to address?

- **Vision Transformers (ViT) and pre-trained models**: DPAT leverages a frozen ViT-B/16 backbone pre-trained on ImageNet-21K to extract features. Quick check: Why does DPAT freeze the pre-trained ViT backbone instead of fine-tuning it?

- **Adapters and prompt tuning in pre-trained models**: DPAT uses adapters for spatial-temporal adaptation and learnable prompts for mitigating forgetting. Quick check: How do adapters and prompt tuning differ in their approach to adapting pre-trained models to new tasks?

## Architecture Onboarding

- **Component map**: Video clips (T frames, HÃ—W pixels, C channels) -> Frozen ViT-B/16 backbone -> Temporal adapter and prompt -> Spatial adapter and prompt -> Key-query matching (task-specific prompt selection) -> Classification head

- **Critical path**: Video clip processed by frozen ViT backbone to extract spatial-temporal features -> Temporal adapter and prompt process features for temporal dynamics -> Spatial adapter and prompt process features for spatial information -> Task-specific prompt selected via key-query matching -> Classification head predicts action class

- **Design tradeoffs**: Decoupled training vs. joint training balances adaptation and generalization but requires careful tuning; adapter bottleneck ratio affects model size vs. adaptation capacity; prompt length impacts information capture vs. computational cost

- **Failure signatures**: High backward forgetting indicates insufficient regularization or poor prompt design; low accuracy on new tasks suggests inadequate adapter tuning; slow convergence may indicate poorly tuned decoupled training or overly complex architecture

- **First 3 experiments**: 1) Ablation study removing temporal adapter to assess impact on accuracy and BWF; 2) Ablation study removing all adapters to evaluate necessity for task adaptation; 3) Ablation study removing task-agnostic prompts to assess their role in stabilizing across tasks

## Open Questions the Paper Calls Out

### Open Question 1
How would DPAT perform in an online learning scenario where task boundaries are not predefined? The current framework assumes class-incremental learning with clear task boundaries, which doesn't reflect real-world streaming data without explicit task divisions.

### Open Question 2
Would incorporating open-set recognition capability improve DPAT's performance? Current DPAT operates within closed-set classification and doesn't handle unknown classes that may appear in new tasks, limiting real-world applicability.

### Open Question 3
How does performance scale with larger pre-trained models or different backbone architectures? Current experiments are limited to specific model sizes and architectures, leaving scalability with more powerful backbones unexplored.

### Open Question 4
What is the impact of the decoupled training strategy compared to other training approaches? The paper only compares decoupled training to joint training, not exploring other potential strategies like interleaved training or curriculum learning.

## Limitations

- The approach relies on predefined task boundaries, limiting applicability to online learning scenarios
- Current implementation operates within closed-set classification, unable to handle unknown classes
- Performance scaling with larger models or alternative architectures remains unexplored

## Confidence

- **High confidence**: Empirical results showing DPAT outperforming baselines on Kinetics-400, ActivityNet, and EPIC-Kitchens-100
- **Medium confidence**: Effectiveness of decoupled training strategy and necessity of softmax-enhanced key-query matching
- **Low confidence**: Specific architectural choices and their sensitivity to hyperparameter tuning

## Next Checks

1. **Mechanism isolation test**: Run DPAT with only prefix tuning (Stage 1) and only adapter tuning (Stage 2) separately to quantify each stage's contribution and validate decoupled training superiority.

2. **Key-query matching ablation**: Implement version without softmax normalization in matching loss and compare forgetting rates to test if enhanced matching is truly necessary.

3. **Cross-dataset generalization test**: Train DPAT on Kinetics-400 and evaluate directly on ActivityNet without fine-tuning to assess prompt and adapter generalization across domains.