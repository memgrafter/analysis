---
ver: rpa2
title: Evaluating Large Language Models with Psychometrics
arxiv_id: '2406.17675'
source_url: https://arxiv.org/abs/2406.17675
tags:
- llms
- personality
- language
- table
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive psychometric benchmark for
  evaluating Large Language Models (LLMs) across five psychological dimensions: personality,
  values, emotion, theory of mind, and motivation. The authors develop 13 datasets
  spanning diverse scenarios and item types, then conduct rigorous validation through
  multiple reliability measures including internal consistency, parallel forms reliability,
  and inter-rater reliability.'
---

# Evaluating Large Language Models with Psychometrics

## Quick Facts
- arXiv ID: 2406.17675
- Source URL: https://arxiv.org/abs/2406.17675
- Authors: Yuan Li; Yue Huang; Hongyi Wang; Ying Cheng; Xiangliang Zhang; James Zou; Lichao Sun
- Reference count: 40
- One-line primary result: Comprehensive psychometric benchmark reveals LLMs show human-like consistency in some psychological aspects but notable discrepancies in others

## Executive Summary
This paper presents a comprehensive psychometric benchmark for evaluating Large Language Models (LLMs) across five psychological dimensions: personality, values, emotion, theory of mind, and motivation. The authors develop 13 datasets spanning diverse scenarios and item types, then conduct rigorous validation through multiple reliability measures including internal consistency, parallel forms reliability, and inter-rater reliability. Key findings include significant discrepancies between LLMs' self-reported traits and their responses in real-world scenarios, variability in responses to preference-based questions, and differing robustness to adversarial attacks.

The benchmark reveals that while some LLMs demonstrate human-like consistency in certain psychological aspects, others show notable inconsistencies, particularly in culturally-oriented and moral reasoning tasks. This work provides insights into reliable LLM evaluation and has applications in AI development, social sciences, and healthcare.

## Method Summary
The paper develops a psychometric framework with five psychological dimensions assessed through 13 datasets from standard tests, established datasets, and self-designed scenarios. The evaluation uses nine popular LLMs including ChatGPT, GPT-4, GLM4, Qwen-Turbo, Llama3-8b, Llama3-70b, Mistral-7b, Mixtral-8*7b, and Mixtral-8*22b. The assessment employs both self-reported questionnaires and open-ended scenarios with prompt templates at temperature 0.5. Results are validated using five reliability measures: internal consistency, parallel forms reliability, inter-rater reliability, option position robustness, and adversarial attack robustness. The LLM-as-a-judge approach uses GPT-4 and Llama3-70b as raters for open-ended items.

## Key Results
- LLMs exhibit significant discrepancies between self-reported personality scores and open-ended scenario responses
- Role-playing prompts effectively manipulate LLM personality trait scores across both self-reported and vignette tests
- Some preference-based tests designed for humans fail to elicit reliable responses from LLMs
- LLMs show varying robustness to adversarial attacks and position bias across different psychological dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Psychometric tests originally designed for humans can reliably assess LLM behaviors if modified for consistency validation.
- Mechanism: The framework extends human psychometric reliability checks (internal consistency, parallel forms, inter-rater agreement) to LLMs, then validates these tests by checking robustness to position bias and adversarial attacks.
- Core assumption: LLMs exhibit consistent behavioral patterns under similar conditions, analogous to human psychological constructs.
- Evidence anchors: [abstract] "Our work identifies five key psychological constructs...assessed through a suite of 13 datasets featuring diverse scenarios and item types." [section] "We conduct rigorous validation to ensure that the assessment results are reliable and interpretable [15]. Extending the reliability considerations in psychometrics, we focus on five forms of reliability: internal consistency, parallel forms reliability, inter-rater reliability, option position robustness, and adversarial attack robustness."
- Break condition: LLMs show inconsistent responses to similar scenarios, making psychological constructs unreliable.

### Mechanism 2
- Claim: LLMs exhibit discrepancies between self-reported traits and open-ended responses in real-world scenarios.
- Mechanism: Self-reported questionnaires (rating scales) may capture different aspects of LLM behavior than open-ended vignettes, revealing underlying generation patterns.
- Core assumption: LLMs do not have a unified internal representation aligning their responses across different question formats.
- Evidence anchors: [abstract] "Our findings also show that some preference-based tests, originally designed for humans, could not solicit reliable responses from LLMs." [section] "LLMs exhibit discrepancies in psychological tendencies when responding to closed-form versus open-ended questions...suggesting that LLMs lack an internal representation that aligns their self-reported answers with their responses to real-world questions."
- Break condition: LLMs consistently align their self-reported traits with open-ended responses across all scenarios.

### Mechanism 3
- Claim: Role-playing prompts can effectively manipulate LLM personality trait scores by directing behavioral generation patterns.
- Mechanism: Descriptive role-playing prompts (e.g., "You are an extraverted person...") significantly alter LLM responses in both self-reported and vignette tests, demonstrating controllable personality manipulation.
- Core assumption: LLMs can leverage their understanding of personality traits to generate responses with designated personalities when given appropriate prompts.
- Evidence anchors: [section] "We observe inconsistencies between self-reported personality scores and open-ended responses...Role-playing prompts...significantly influence scores on both tests." [section] "These results demonstrate the effectiveness of prompts in altering the behavioral patterns of LLMs."
- Break condition: Role-playing prompts fail to consistently influence LLM responses across different personality aspects.

## Foundational Learning

- Concept: Psychometric reliability
  - Why needed here: Ensures test results are consistent and interpretable across different evaluation scenarios
  - Quick check question: What are the five forms of reliability extended from psychometrics to LLMs?

- Concept: Internal consistency measurement
  - Why needed here: Quantifies whether LLMs exhibit consistent preferences when responding to questions examining the same psychological aspect
  - Quick check question: How is standard deviation (σ) used to measure internal consistency in BFI tests?

- Concept: Parallel forms reliability
  - Why needed here: Assesses whether two different yet equivalent versions of a test yield consistent results for LLMs
  - Quick check question: What does low parallel forms reliability imply about LLM response consistency?

## Architecture Onboarding

- Component map: Dataset curation -> Prompt design -> Model evaluation -> Reliability validation -> Results analysis
- Critical path: Design psychometrically valid tests -> Implement robust prompt templates -> Run evaluations across multiple models -> Validate results using reliability metrics -> Analyze discrepancies
- Design tradeoffs: Comprehensive psychological coverage vs. computational cost of running 13 datasets across 9 models; detailed reliability validation vs. evaluation speed
- Failure signatures: Inconsistent responses across similar scenarios; low inter-rater agreement; poor position bias robustness; adversarial attack susceptibility
- First 3 experiments:
  1. Run BFI test with naive prompts on GPT-4 and compare with human average scores
  2. Test LLM self-efficacy questionnaire parallel forms (can/cannot) on ChatGPT and calculate κ
  3. Evaluate false belief task with position bias variation on Llama3-70b and calculate MR score

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the psychological traits of LLMs change over time as they are updated or fine-tuned, and can these changes be reliably measured using the proposed psychometric framework?
- Basis in paper: [explicit] The paper mentions that proprietary LLMs like GPT-4 are periodically updated based on user feedback, but the details are often not disclosed publicly. It suggests that evaluating changes in LLM abilities through psychological dimensions is critical.
- Why unresolved: The paper does not provide longitudinal data or experiments tracking how LLM psychological traits evolve with updates. The framework focuses on static evaluation rather than temporal dynamics.
- What evidence would resolve it: Repeated evaluations of the same LLM models across multiple versions/time points using the psychometric benchmark, tracking changes in their psychological trait profiles and correlating these with known updates or fine-tuning events.

### Open Question 2
- Question: What are the underlying mechanisms that cause LLMs to exhibit different psychological traits in self-reported versus open-ended responses, and can these mechanisms be identified and controlled?
- Basis in paper: [explicit] The paper finds significant discrepancies between LLMs' self-reported personality scores and their responses in open-ended scenarios, noting that this pattern is also observed in humans. It suggests LLMs lack an internal representation that aligns these tendencies.
- Why unresolved: The paper observes the phenomenon but does not investigate the computational or architectural reasons behind it. It does not explore whether this stems from training data patterns, prompt sensitivity, or fundamental differences in how LLMs process different response formats.
- What evidence would resolve it: Detailed ablation studies comparing LLM responses across formats while controlling for various factors (temperature, prompt structure, training data exposure), potentially combined with interpretability techniques to examine internal representations during different types of responses.

### Open Question 3
- Question: Can the psychometric framework be extended to evaluate cross-cultural variations in LLM behavior across different languages and cultural contexts, and what would such an evaluation reveal about global AI deployment?
- Basis in paper: [explicit] The paper evaluates cultural orientation in LLMs using the GLOBE project questionnaire and finds substantial inconsistency in cultural traits exhibited by different models. It notes that cultural orientation can provide insights into improving model's ability to handle cross-cultural contexts effectively.
- Why unresolved: The current evaluation focuses on English-language LLMs and a limited set of cultural dimensions. The paper does not explore how these traits manifest across different languages or whether cultural calibration is possible.
- What evidence would resolve it: Multi-lingual evaluation of the same psychometric framework across LLMs trained on different language corpora, measuring consistency of cultural traits across languages and testing whether targeted cultural fine-tuning can produce more consistent cross-cultural behavior.

## Limitations

- Transferability of human psychometrics to LLMs remains uncertain as LLMs lack consciousness and genuine motivations
- Framework focuses on static evaluation without addressing how psychological traits evolve with model updates or fine-tuning
- Limited exploration of cross-cultural variations and multi-lingual behavior across different LLM training corpora

## Confidence

- High: The paper's comprehensive approach combining psychometric validation with multiple reliability measures
- Medium: The assumption that human psychometric constructs meaningfully capture LLM behavioral patterns
- Low: The transferability of human psychometric measures to non-conscious AI systems

## Next Checks

1. Replicate the BFI personality test on GPT-4 and compare results with human average scores
2. Calculate parallel forms reliability for the self-efficacy questionnaire on ChatGPT
3. Test adversarial attack robustness on the cultural orientation assessment across multiple LLM models