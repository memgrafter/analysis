---
ver: rpa2
title: 'No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions
  in 28 Languages'
arxiv_id: '2411.03769'
source_url: https://arxiv.org/abs/2411.03769
tags:
- language
- languages
- arxiv
- image
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ArtELingo-28 addresses the lack of multilingual affective vision-language
  benchmarks by introducing a dataset of 200,000 emotional captions across 28 languages
  for 2,000 WikiArt images. The core method idea is to collect emotion-labeled captions
  in diverse languages and evaluate cross-lingual transfer under three setups: Zero-Shot,
  Few-Shot, and One-vs-All Zero-Shot.'
---

# No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages

## Quick Facts
- arXiv ID: 2411.03769
- Source URL: https://arxiv.org/abs/2411.03769
- Reference count: 40
- ArtELingo-28 introduces a dataset of 200,000 emotional captions across 28 languages for 2,000 WikiArt images, addressing the lack of multilingual affective vision-language benchmarks.

## Executive Summary
ArtELingo-28 addresses the critical gap in multilingual affective vision-language understanding by creating a benchmark of 200,000 emotional captions across 28 languages for 2,000 WikiArt images. The dataset enables evaluation of cross-lingual transfer for affective captioning through three novel experimental setups: Zero-Shot, Few-Shot, and One-vs-All Zero-Shot. The work demonstrates that cross-lingual transfer is more successful for culturally related languages and that MiniGPT-4 achieves the best multilingual performance among tested vision-language models.

## Method Summary
The method involves collecting emotion-labeled captions in 28 languages through native speakers, then adapting vision-language models using a two-stage instruction tuning approach with BLOOMZ tokenizer. The three experimental setups evaluate cross-lingual transfer: Zero-Shot trains on high-resource languages and tests on 25 others, Few-Shot fine-tunes with small amounts of target language data, and One-vs-All Zero-Shot fine-tunes on one language and evaluates on all others. Models are evaluated using BLEU-4, METEOR, ROUGE, and CIDEr metrics for caption generation and accuracy/precision/recall/F1 for emotion prediction.

## Key Results
- MiniGPT-4 outperforms other models in multilingual caption generation with BLEU-4 scores ranging from 0.002 (Burmese) to 1.26 (Kinyarwanda) in Zero-Shot
- Cross-lingual transfer is more successful for culturally-related languages, with Urdu-Hindi-Tamil and Uzbek-Kyrgyz clustering in One-vs-All Zero-Shot
- The Few-Shot setup shows no significant improvement with more fine-tuning data, suggesting horizontal expansion (more languages) may be more beneficial than vertical expansion (more samples per language)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual affective captioning works better when models are trained on culturally related languages.
- Mechanism: The One-vs-All Zero-Shot experiment shows that fine-tuning on one language (e.g., Hindi) and evaluating on another (e.g., Urdu) yields higher scores when the languages share cultural context, indicating that the model learns cross-lingual patterns that generalize better within culturally related language groups.
- Core assumption: Cultural similarity enables better transfer of emotional and linguistic patterns across languages.
- Evidence anchors:
  - [abstract]: "We find that cross-lingual transfer is more successful for culturally-related languages."
  - [section]: "Interestingly, we see language groupings that reflect cultural connections irrespective of other factors such as writing systems."
  - [corpus]: Weak—no corpus paper directly validates this cultural transfer claim.
- Break condition: If cultural distance is large or emotional expressions are significantly different across cultures, transfer performance drops.

### Mechanism 2
- Claim: Instruction tuning with multilingual data improves cross-lingual zero-shot performance.
- Mechanism: By fine-tuning BLOOMZ with instructions in two languages for the same image, the model learns to align visual features with multilingual captions, improving generalization to unseen languages.
- Core assumption: Multilingual instruction tuning aligns language models with vision features across multiple languages.
- Evidence anchors:
  - [section]: "We utilize a two-stage training process... We follow MiniGPT-4 and use the following instructions... We found this setup to improve the model's alignment across different languages."
  - [abstract]: "Baseline results will be presented for three novel conditions: Zero-Shot, Few-Shot and One-vs-All Zero-Shot."
  - [corpus]: Weak—no corpus paper provides direct evidence for multilingual instruction tuning benefits.
- Break condition: If the instruction format or data quality is poor, the alignment benefit disappears.

### Mechanism 3
- Claim: Using native speakers for data collection yields better cross-lingual transfer than translated data.
- Mechanism: Native annotators provide culturally grounded captions that reflect authentic emotional expressions, which improves model performance when evaluated on unseen languages.
- Core assumption: Native speakers capture cultural nuances that are lost in translation.
- Evidence anchors:
  - [section]: "We observe that the multilingual setup is challenging for vision and language models, partly because of the massive vocabulary. We address this challenge by utilizing pretrained multilingual LLMs such as BLOOMZ."
  - [abstract]: "Traditionally, vision research focused on unambiguous class labels, whereas ArtELingo-28 emphasizes diversity of opinions over languages and cultures."
  - [corpus]: Weak—no corpus paper directly validates native vs translated data quality.
- Break condition: If annotators are not truly native or lack cultural context, the quality advantage disappears.

## Foundational Learning

- Concept: Cross-lingual transfer
  - Why needed here: Understanding how knowledge learned in one language can be applied to another is central to the One-vs-All Zero-Shot setup.
  - Quick check question: What factors determine whether a model trained in language A will perform well on language B?

- Concept: Multilingual tokenization challenges
  - Why needed here: The paper notes that multilingual tokenization affects metric comparability and model performance.
  - Quick check question: How does tokenization differ between languages like Burmese (character-level) and English (word-level), and why does this matter?

- Concept: Instruction tuning in multimodal models
  - Why needed here: The adaptation strategy relies on instruction tuning to align vision features with multilingual text.
  - Quick check question: What is the difference between standard fine-tuning and instruction tuning, and why is it useful here?

## Architecture Onboarding

- Component map:
  Vision encoder (CLIP-based or similar) → Q-former/linear projection → BLOOMZ language model → instruction tuning pipeline → XLM-RoBERTa for emotion classification tasks → Data collection interface with emotion selection and caption writing

- Critical path:
  1. Load pretrained vision encoder
  2. Fine-tune with instruction data (LAION + ArtELingo-28)
  3. Evaluate on Zero-Shot, Few-Shot, One-vs-All Zero-Shot setups
  4. Analyze cross-lingual transfer patterns

- Design tradeoffs:
  - Using BLOOMZ vs smaller multilingual models: larger vocab but better cross-lingual transfer
  - Native data collection vs translation: more authentic but harder to scale
  - Few-shot vs zero-shot: better performance but requires some target language data

- Failure signatures:
  - Poor performance on languages with different scripts (e.g., Burmese, Thai)
  - Metric scores not comparable across languages due to tokenization differences
  - Cross-lingual transfer fails when cultural distance is large

- First 3 experiments:
  1. Run Zero-Shot evaluation to establish baseline cross-lingual performance
  2. Run One-vs-All Zero-Shot to identify cultural transfer patterns
  3. Fine-tune on a small sample of target language data and re-evaluate Few-Shot performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of multilingual tokenization affect cross-lingual transfer performance in vision-language models?
- Basis in paper: [inferred] The paper mentions that Burmese and Thai languages have poor performance in zero-shot settings due to the inability of the Bloomz LLM to handle these languages without fine-tuning, and that multilingual tokenization is biased towards Latin script languages.
- Why unresolved: The paper identifies this as a limitation but does not provide concrete evidence or solutions to address the tokenization bias.
- What evidence would resolve it: Experiments comparing models with different tokenization strategies (e.g., character-level vs. subword-level) on the same languages, along with ablation studies on the impact of tokenization on performance metrics.

### Open Question 2
- Question: What is the impact of instruction fine-tuning strategies on the cross-lingual generalization of vision-language models?
- Basis in paper: [explicit] The paper describes using two-stage instruction fine-tuning with LAION datasets and ArtELingo data, but does not provide a detailed analysis of how different fine-tuning strategies affect cross-lingual performance.
- Why unresolved: The paper only briefly mentions the instruction fine-tuning process and its benefits but does not explore alternative strategies or provide a systematic comparison.
- What evidence would resolve it: Experiments comparing different instruction fine-tuning approaches (e.g., varying the amount of multilingual vs. monolingual data, different instruction formats) and their impact on cross-lingual transfer scores across languages.

### Open Question 3
- Question: How do cultural differences in emotional expression affect the performance of emotion prediction models across languages?
- Basis in paper: [explicit] The paper shows that emotion distributions vary across languages and that cross-lingual transfer is more successful for culturally-related languages, but does not quantify the relationship between cultural similarity and model performance.
- Why unresolved: The paper provides qualitative observations about cultural clusters but lacks a quantitative analysis of how cultural distance affects emotion prediction accuracy.
- What evidence would resolve it: Correlation analysis between cultural distance metrics (e.g., based on linguistic, geographical, or social factors) and emotion prediction performance, along with experiments controlling for cultural factors.

### Open Question 4
- Question: What is the optimal balance between expanding horizontally (adding more languages) vs. vertically (collecting more samples per language) for improving cross-lingual transfer in affective vision-language tasks?
- Basis in paper: [explicit] The paper mentions that the Few-Shot results show no significant improvement with more fine-tuning data, suggesting horizontal expansion might be more beneficial, but does not provide a systematic comparison.
- Why unresolved: The paper only tests a limited range of data ratios and does not explore the trade-off between language coverage and sample size in depth.
- What evidence would resolve it: Experiments varying both the number of languages and samples per language, with analysis of the marginal gains from each type of expansion on cross-lingual transfer metrics.

## Limitations
- Weak corpus support for key mechanisms, particularly the claims about cultural transfer and native speaker advantages
- Poor performance on languages with non-Latin scripts (Burmese, Thai) suggesting tokenizer bias not fully explored
- Does not address how fundamental differences in emotional expression across cultures might impact transfer performance

## Confidence
- **High confidence**: Dataset construction methodology and experimental setup are clearly specified and reproducible; MiniGPT-4 outperforming other models is well-supported by metrics
- **Medium confidence**: Claim about culturally related languages clustering in One-vs-All Zero-Shot performance is supported but lacks deeper analysis of cultural similarity
- **Low confidence**: Assertion that native speaker data yields better cross-lingual transfer than translated data is plausible but not empirically validated against translated alternatives

## Next Checks
1. **Cultural expression validation**: Conduct a controlled study comparing emotional caption distributions across culturally distant language pairs (e.g., Japanese vs Swahili) to verify whether the claimed cultural transfer patterns hold when emotional expressions differ significantly.

2. **Tokenizer bias analysis**: Systematically compare tokenization patterns and caption quality metrics across languages with different script types (Latin vs non-Latin) to identify whether tokenizer bias explains the poor performance on Burmese and Thai.

3. **Translation vs native comparison**: Create a parallel test set where the same captions are both natively written and professionally translated for the same language pairs, then compare model performance to validate whether native speaker data truly provides advantages over high-quality translation.