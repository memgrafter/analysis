---
ver: rpa2
title: 'True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments
  via Reinforcement Learning'
arxiv_id: '2401.14151'
source_url: https://arxiv.org/abs/2401.14151
tags:
- action
- walk
- agent
- tomato
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TWOSOME, a framework that uses reinforcement
  learning to align large language models (LLMs) with embodied environments for decision-making
  tasks. Unlike traditional RL methods that learn from scratch, TWOSOME leverages
  the knowledge of LLMs by generating valid action policies through joint probability
  calculations over valid actions.
---

# True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning

## Quick Facts
- arXiv ID: 2401.14151
- Source URL: https://arxiv.org/abs/2401.14151
- Reference count: 40
- Primary result: Introduces TWOSOME, a framework using reinforcement learning to align large language models with embodied environments, achieving significantly better sample efficiency and performance compared to conventional RL methods and prompt tuning approaches.

## Executive Summary
This paper introduces TWOSOME, a novel framework that leverages large language models (LLMs) for decision-making in embodied environments through reinforcement learning. Unlike traditional RL methods that learn from scratch, TWOSOME uses LLMs to generate valid action policies by querying joint probabilities over valid actions. The framework addresses the issue of longer actions having lower probabilities through two normalization techniques—token normalization and word normalization. Extensive experiments in Overcooked and VirtualHome environments demonstrate that TWOSOME achieves significantly better sample efficiency and performance compared to conventional RL methods and prompt tuning approaches, while also maintaining the LLM's original capabilities after fine-tuning.

## Method Summary
TWOSOME aligns LLMs with embodied environments by querying the loglikelihood scores of all available actions from a frozen LLM and computing their joint probabilities to form valid behavior policies. The framework uses two normalization techniques—token normalization and word normalization—to address the issue of longer actions having lower probabilities. A parameter-efficient training method is employed where both the actor and critic share a frozen LLM with low-rank adapters (LoRA) updated by proximal policy optimization (PPO). This approach allows TWOSOME to efficiently interact with and align with embodied environments without requiring prepared datasets or prior knowledge of the environments.

## Key Results
- TWOSOME achieves significantly better sample efficiency and performance compared to conventional RL methods and prompt tuning approaches in Overcooked and VirtualHome environments.
- The framework demonstrates superior generalization ability to unseen tasks and maintains the LLM's original capabilities after fine-tuning.
- Word normalization outperforms token normalization in addressing the length bias in action probability calculation, leading to more balanced and stable policies.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TWOSOME resolves LLM-environment misalignment by replacing direct action generation with joint probability queries over valid actions.
- Mechanism: Instead of letting LLMs directly generate actions, TWOSOME queries the loglikelihood scores of all available actions from LLMs and computes their joint probabilities to form valid behavior policies.
- Core assumption: LLMs' token-level probabilities reflect meaningful action preferences when properly normalized and can serve as valid behavior policies.
- Evidence anchors: [abstract]: "TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL"; [section 4.1]: "TWOSOME queries the scores of all available actions from LLMs. These scores are used to determine the probabilities of executing the actions"
- Break condition: If LLMs cannot reliably assign meaningful probabilities to valid actions, or if normalization fails to correct action length bias.

### Mechanism 2
- Claim: Word normalization addresses the length bias in action probability calculation, leading to more balanced and stable policies.
- Mechanism: TWOSOME normalizes token-level probabilities by dividing by the number of words rather than tokens, accounting for multi-token words and preventing longer actions from being systematically undervalued.
- Core assumption: Word-based normalization better reflects semantic meaning than token-based normalization in LLM-generated policies.
- Evidence anchors: [section 4.2]: "Instead of normalizing the probabilities of actions according to the number of tokens, it is more reasonable to regard the several tokens made up of one word as an integrated symbol"; [section 5.2]: "TWOSOME with word normalization succeeds in learning the optimal policy among all tasks, exhibiting great performance, and high sample efficiency"
- Break condition: If word boundaries don't align with semantic units in the target domain, or if normalization overcompensates and introduces new biases.

### Mechanism 3
- Claim: Parameter-efficient PPO finetuning with frozen LLM and LoRA adapters enables effective environment alignment without catastrophic forgetting.
- Mechanism: TWOSOME uses a shared frozen LLM with separate LoRA adapters for the actor and MLP layers for the critic, updated via PPO, achieving efficient finetuning while preserving original LLM capabilities.
- Core assumption: LoRA can effectively encode environment-specific alignments while keeping the base LLM frozen preserves general capabilities.
- Evidence anchors: [section 4.3]: "we design a novel architecture for efficient training, where both the actor and critic in RL methods share the same frozen LLaMA-7B model (Touvron et al., 2023), updated by parameter efficient finetuning methods, e.g., LoRA"; [section 5.5]: "there is no significant loss of the LLMs' original ability during online PPO fine-tuning"
- Break condition: If the LoRA parameters cannot capture sufficient environment-specific knowledge, or if PPO training destabilizes the frozen LLM outputs.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: TWOSOME frames the embodied decision-making task as an MDP to apply RL techniques like PPO
  - Quick check question: What are the key components of an MDP, and how does TWOSOME's environment (e.g., Overcooked) fit this framework?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: TWOSOME is conceptually similar to RLHF, using environmental rewards instead of human feedback to align LLMs
  - Quick check question: How does TWOSOME's use of environmental rewards differ from traditional RLHF, and what are the implications for training stability?

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: TWOSOME relies on LoRA and other PEFT methods to efficiently adapt LLMs for decision-making without full finetuning
  - Quick check question: What are the key advantages of LoRA over other PEFT methods like adapter tuning or prompt tuning in the context of TWOSOME?

## Architecture Onboarding

- Component map: Observation → Prompt generation → LLM action probability calculation → Action selection → Environment execution → Reward calculation → PPO update

- Critical path: Observation → Prompt generation → LLM action probability calculation → Action selection → Environment execution → Reward calculation → PPO update

- Design tradeoffs:
  - Shared frozen LLM vs. separate models: Saves memory but may limit expressivity
  - LoRA vs. full finetuning: More efficient but may limit adaptation capacity
  - Word normalization vs. token normalization: Better semantic alignment but requires word boundary detection

- Failure signatures:
  - Unstable training: May indicate issues with normalization, learning rate balance, or KL divergence clipping
  - Poor sample efficiency: Could suggest inadequate exploration or suboptimal prompt design
  - Catastrophic forgetting: Might indicate overly aggressive LoRA updates or insufficient regularization

- First 3 experiments:
  1. Compare TWOSOME with and without normalization on a simple task (e.g., Tomato Salad) to validate the normalization mechanism
  2. Test TWOSOME with different learning rate pairs (actor/critic) to find optimal training stability
  3. Evaluate TWOSOME's zero-shot performance on NLP benchmarks after finetuning to verify capability preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TWOSOME's performance scale with increasing action space complexity and partial observability in embodied environments?
- Basis in paper: [explicit] The paper mentions that TWOSOME performs well in Overcooked and VirtualHome, which have different levels of complexity and partial observability. It also states that vanilla PPO struggles with large action spaces and partial observability.
- Why unresolved: The paper only evaluates TWOSOME on a limited set of tasks and environments. It's unclear how the method would perform on more complex environments with larger action spaces and more severe partial observability.
- What evidence would resolve it: Experiments on a wider range of environments with varying levels of complexity, action space size, and partial observability. Comparisons with other RL methods on these tasks would also be valuable.

### Open Question 2
- Question: How does TWOSOME's performance compare to other methods that use LLMs for decision-making in embodied environments?
- Basis in paper: [explicit] The paper compares TWOSOME to SayCan, which uses LLMs for decision-making without fine-tuning. It also mentions GLAM, which uses RL to ground LLMs but focuses on simpler tasks.
- Why unresolved: The paper only compares TWOSOME to a few other methods. It's unclear how it would perform against other state-of-the-art approaches for using LLMs in embodied environments.
- What evidence would resolve it: Experiments comparing TWOSOME to a wider range of methods, including those that use LLMs in different ways (e.g., for planning, reasoning, or perception). Evaluations on the same tasks would be ideal for fair comparison.

### Open Question 3
- Question: How does TWOSOME's performance depend on the size and quality of the LLM used?
- Basis in paper: [explicit] The paper uses LLaMA-7B for all experiments. It mentions that other works rely on larger LLMs like GPT-4 and PaLM, which are more capable but harder to apply to smaller models.
- Why unresolved: The paper doesn't explore how TWOSOME's performance varies with different LLM sizes or qualities. It's unclear how much the method relies on the specific capabilities of LLaMA-7B.
- What evidence would resolve it: Experiments using different LLM sizes and qualities (e.g., LLaMA-13B, GPT-3.5, etc.) on the same tasks. Analyses of how TWOSOME's performance metrics (e.g., sample efficiency, success rate) change with different LLMs would be informative.

## Limitations
- Normalization Mechanism Uncertainty: While the paper claims word normalization outperforms token normalization, the underlying reason (semantic alignment vs. simple length correction) is not rigorously tested.
- Generalization Claims: The paper demonstrates good generalization to unseen tasks within the same environment but does not test cross-domain generalization or performance on truly novel environments.
- Capability Preservation: The claim that TWOSOME maintains LLM capabilities after fine-tuning is based on zero-shot NLP benchmarks, which may not capture all aspects of the LLM's original knowledge.

## Confidence
- High Confidence: The core framework design (frozen LLM + LoRA adapters + PPO) is well-established in prior work, and the implementation details are clearly specified.
- Medium Confidence: The normalization techniques and their impact on policy stability are supported by experimental results, but the theoretical justification could be stronger.
- Low Confidence: Claims about maintaining LLM capabilities post-fine-tuning require more comprehensive testing across diverse domains and tasks.

## Next Checks
1. **Ablation on Normalization**: Run TWOSOME without any normalization, with token normalization only, and with word normalization only on a simple task to isolate the specific contribution of each normalization method to performance gains.
2. **Cross-Environment Generalization**: Test TWOSOME on a completely different environment (e.g., ALFRED or a new gridworld) with different action spaces and dynamics to evaluate true generalization beyond the training domains.
3. **Capability Retention Testing**: Conduct a comprehensive evaluation of the fine-tuned LLM on a diverse set of NLP and domain-specific tasks to verify that no significant knowledge degradation occurs during the RL alignment process.