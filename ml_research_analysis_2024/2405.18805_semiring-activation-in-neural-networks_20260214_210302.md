---
ver: rpa2
title: Semiring Activation in Neural Networks
arxiv_id: '2405.18805'
source_url: https://arxiv.org/abs/2405.18805
tags:
- semiring
- networks
- neural
- activation
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces semiring-based trainable nonlinear operators
  for neural networks, expanding beyond traditional linear-nonlinear alternation.
  The authors propose using quasilinear operators derived from commutative semirings
  (logarithmic, tropical) as trainable activation functions.
---

# Semiring Activation in Neural Networks

## Quick Facts
- **arXiv ID:** 2405.18805
- **Source URL:** https://arxiv.org/abs/2405.18805
- **Authors:** Bart M. N. Smets; Peter D. Donker; Jim W. Portegies
- **Reference count:** 9
- **Primary result:** Semiring-based trainable nonlinear operators show competitive performance on small datasets but underperform on large architectures compared to traditional MLP blocks

## Executive Summary
This paper introduces semiring-based trainable nonlinear operators as an alternative to traditional activation functions in neural networks. The authors propose using quasilinear operators derived from commutative semirings (logarithmic, tropical) to replace standard nonlinear activation layers. Through theoretical development and experimental validation on small-scale datasets, they demonstrate that semiring activations are a viable alternative to ReLU, though not consistently superior. The work builds on their previous PDE-G-CNN research and provides insights into the relative importance of semiring structure versus other architectural features like equivariance and PDE constraints.

## Method Summary
The authors develop a novel approach to neural network design by replacing traditional linear-nonlinear alternation with semiring-based operators. They introduce quasilinear operators derived from commutative semirings, specifically logarithmic and tropical semirings, as trainable activation functions. A key contribution is the "fair tropical initialization" scheme designed to address gradient imbalance issues in tropical semirings. The framework allows for different levels of nonlinearity through the parameter α, where α=0 provides linear operation and α=1 provides full nonlinearity. The semiring structure enables training through backpropagation while maintaining mathematical properties that differ from standard neural network operations.

## Key Results
- Semiring activations show competitive performance on small fully connected networks (Iris, Heart Disease, Circles, Spheres, FashionMNIST) compared to ReLU baselines
- Some datasets show modest improvements with semiring activations over traditional ReLU functions
- When applied to ConvNeXt architecture for FashionMNIST, semiring activations significantly underperform compared to standard MLP-based blocks
- The results suggest that benefits observed in previous PDE-G-CNN work likely stem more from equivariance/PDE structure than semiring structure alone

## Why This Works (Mechanism)
Semiring activations work by replacing the traditional nonlinear activation function with quasilinear operators that maintain certain algebraic properties while introducing trainable nonlinearity. The semiring structure provides a mathematical framework where operations like addition and multiplication can be generalized (e.g., max-plus algebra in tropical semirings), allowing for different types of nonlinear transformations that are still differentiable and trainable. The quasilinear property ensures that these operators can be expressed as a linear transformation followed by a fixed nonlinear function, making them compatible with backpropagation. The fair tropical initialization specifically addresses the issue of gradient imbalance that occurs in tropical semirings, where gradients can become disproportionately large or small depending on the operation.

## Foundational Learning
- **Commutative semirings:** Algebraic structures with two operations (addition and multiplication) that are commutative and associative, with multiplication distributing over addition. Needed to provide the mathematical foundation for semiring activations; quick check: verify properties of max-plus and log semirings.
- **Tropical semiring:** A specific commutative semiring where addition is defined as taking the maximum and multiplication as addition. Required for understanding the tropical activation functions; quick check: confirm max-plus operations satisfy semiring axioms.
- **Quasilinear operators:** Operators that can be expressed as a linear transformation followed by a fixed nonlinear function. Essential for ensuring trainability through backpropagation; quick check: verify that quasilinear operators are differentiable.
- **Backpropagation through semirings:** The process of computing gradients in neural networks using semiring-based operations. Critical for practical implementation; quick check: ensure gradient flow is maintained through semiring operations.
- **Fair tropical initialization:** A specialized initialization scheme designed to balance gradients in tropical semiring operations. Necessary to prevent training instability; quick check: verify gradient magnitudes remain stable during training.

## Architecture Onboarding

**Component map:** Input -> Linear Layer -> Semiring Activation -> Linear Layer -> Output

**Critical path:** The forward pass through the network, where semiring activations replace traditional nonlinear functions between linear layers. The backward pass uses standard backpropagation with gradients computed through the quasilinear operators.

**Design tradeoffs:** Semiring activations offer mathematical elegance and potentially different inductive biases compared to traditional activations, but may introduce computational overhead and optimization challenges. The fair tropical initialization adds complexity but addresses critical gradient issues.

**Failure signatures:** Performance degradation on large-scale architectures, gradient explosion or vanishing with improper initialization, and inconsistent behavior across different datasets suggesting sensitivity to problem structure.

**3 first experiments:** 1) Compare semiring activation performance vs ReLU on Iris dataset with small fully connected network. 2) Test fair tropical initialization effectiveness by training with and without it on Circles dataset. 3) Evaluate semiring activations on FashionMNIST using standard MLP architecture.

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited to relatively small-scale datasets and architectures, raising scalability concerns
- Significant performance gap observed when applying semiring activations to ConvNeXt architectures versus traditional MLP blocks
- Fair tropical initialization scheme lacks comprehensive ablation studies to isolate its contribution
- Claim that semiring structure may be less critical than equivariance/PDE structure requires further validation

## Confidence
- **Medium confidence** in semiring activations as viable alternatives to ReLU for small-scale problems
- **Low confidence** in scalability of semiring activations to large architectures based on ConvNeXt results
- **Medium confidence** in theoretical framework connecting semirings to neural networks

## Next Checks
1. Test semiring activations on standard large-scale benchmarks (ImageNet, CIFAR-100) using both CNN and Transformer architectures to assess scalability
2. Conduct controlled ablation studies comparing semiring activations with and without fair tropical initialization across diverse initialization schemes
3. Evaluate semiring activations on architectures lacking equivariant/PDE structure to isolate contribution of semiring properties from other architectural features