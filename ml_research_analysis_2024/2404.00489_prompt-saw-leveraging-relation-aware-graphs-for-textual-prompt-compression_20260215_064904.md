---
ver: rpa2
title: 'Prompt-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression'
arxiv_id: '2404.00489'
source_url: https://arxiv.org/abs/2404.00489
tags:
- prompt
- compression
- information
- prompts
- compressed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models (LLMs) face performance limitations and increased
  inference costs due to exceedingly lengthy prompts. Existing prompt compression
  techniques, which operate at the token level, often result in compressed prompts
  that are hard to interpret and lead to a loss of semantic integrity.
---

# Prompt-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression

## Quick Facts
- **arXiv ID:** 2404.00489
- **Source URL:** https://arxiv.org/abs/2404.00489
- **Reference count:** 31
- **Primary result:** Improves EM score by up to 14.3% and 13.7% in task-aware and task-agnostic settings respectively

## Executive Summary
Large Language Models (LLMs) face significant challenges with lengthy prompts, including increased inference costs and degraded performance. Existing token-level compression techniques often sacrifice semantic integrity and produce hard-to-interpret results. PROMPT-SAW addresses these limitations by leveraging knowledge graphs to extract key information elements while preserving semantic relationships. The method operates in both task-aware and task-agnostic settings, achieving superior performance compared to state-of-the-art baselines while producing more interpretable compressed prompts.

## Method Summary
PROMPT-SAW transforms prompt text into a Knowledge Graph (KG) where entities are nodes and relations are edges, then extracts key information elements based on similarity with the question embedding (task-aware) or redundancy reduction (task-agnostic). For task-aware compression, it computes similarity between question embeddings and graph elements to retain only task-relevant information. For task-agnostic compression, it sequentially selects elements that are dissimilar to previously selected elements to remove redundancy. The method uses VICUNNA-7B for graph construction and employs binary search to find optimal similarity thresholds for achieving target compression ratios.

## Key Results
- Outperforms state-of-the-art baselines by up to 14.3% in task-aware settings
- Achieves compression ratios of 33.0% while maintaining performance
- Improves EM score by up to 13.7% in task-agnostic settings with 56.7% compression
- Produces significantly more readable and interpretable compressed prompts compared to previous methods

## Why This Works (Mechanism)

### Mechanism 1
Graph-based representation preserves semantic relationships better than token-level compression by transforming prompts into knowledge graphs where entities and relations capture logical connections. This approach maintains the semantic integrity that would otherwise be lost during token deletion. The method assumes that key information can be adequately represented as entities and relations in a graph structure. Evidence shows the approach works on GSM8K-AUG and NaturalQuestions benchmarks, though literature comparison of KG effectiveness in prompt compression is limited.

### Mechanism 2
Task-aware compression preserves task-specific information by aligning graph elements with question embeddings through similarity computation. The system ranks graph elements by their similarity to the question embedding and selects only the most relevant elements. This assumes the similarity metric correlates with actual task relevance. Experimental results show improvements of up to 13.7% on NaturalQuestions, though embedding-based task relevance ranking for prompt compression lacks literature comparison.

### Mechanism 3
Task-agnostic compression removes redundancy by selecting information elements that are dissimilar to previously selected elements. The system uses a similarity threshold determined by binary search to ensure only sufficiently different elements are selected. This assumes highly similar elements represent redundant information. Results show compression ratios of 31.8-33.0%, though the assumption about redundant generation by auxiliary models is empirically observed but not rigorously validated.

## Foundational Learning

- **Knowledge Graph construction from text**: Essential for transforming prompts into graph structures where entities and relations represent semantic content. Quick check: Can you explain how "John married Mary in 2020" would be represented as a knowledge graph triplet?

- **Similarity computation between embeddings**: Critical for both task-aware and task-agnostic compression decisions. Quick check: What is the cosine similarity between embeddings [0.2, 0.8, 0.1] and [0.3, 0.7, 0.2]?

- **Binary search algorithm**: Used to find optimal similarity thresholds for task-agnostic compression. Quick check: How does binary search find a value within a sorted range?

## Architecture Onboarding

- **Component map:** Input prompt → Graph Constructor (extracts entities/relations) → Encoder (produces embeddings) → Similarity Calculator → Selector (chooses elements) → Reconstructor (converts to text) → Compressed output

- **Critical path:** Graph Construction → Embedding Computation → Similarity Calculation → Element Selection → Text Reconstruction

- **Design tradeoffs:** Uses VICUNNA-7B for cost efficiency but may miss complex relationships; binary search ensures target compression but adds computation time; trades interpretability for compression quality

- **Failure signatures:** Poor compression ratios (graph construction issues); significant performance drop (loss of critical information); unreadable compressed prompts (reconstruction problems); excessive computation time (inefficient operations)

- **First 3 experiments:** 1) Test graph construction on simple prompt "Alice gave Bob a book" to verify KG extraction; 2) Test similarity calculation between question and graph elements on known task-aware example; 3) Test complete pipeline on GSM8K-AUG with 1-shot setting to measure EM score and compression ratio

## Open Questions the Paper Calls Out

### Open Question 1
How can knowledge graphs be effectively leveraged to improve prompt compression while maintaining semantic integrity and prompt utility? The paper demonstrates PROMPT-SAW's effectiveness but doesn't explore theoretical limits of information preservation during compression. Comparative experiments across various prompt lengths and complexity levels would establish these limits.

### Open Question 2
Can the PROMPT-SAW approach be generalized to other languages beyond English? The paper focuses on English prompts without discussing cross-language applicability or required adaptations. Testing on multiple languages would provide insights into generalizability.

### Open Question 3
How does the choice of auxiliary model for graph construction impact PROMPT-SAW performance? The paper uses VICUNNA-7B for cost reasons but doesn't explore how different models affect compressed prompt quality. Comparative studies with various auxiliary models would clarify this impact.

## Limitations

- Heavy reliance on automatic metrics without human judgment of prompt quality or semantic preservation
- Graph construction using VICUNNA-7B may miss complex relationships or introduce errors for abstract concepts
- Claims about improved interpretability lack empirical support from user studies or qualitative analysis
- No evaluation on domain-specific prompts containing technical jargon or nuanced relationships

## Confidence

**High confidence:** Task-aware compression mechanism is well-specified with robust experimental results on NaturalQuestions, showing 13.7% improvement over baselines.

**Medium confidence:** Task-agnostic compression mechanism is theoretically sound but empirical validation is weaker; binary search approach reasonable but lacks sensitivity analysis.

**Low confidence:** Claims about improved interpretability and readability lack empirical support; no user study or qualitative assessment validates these assertions.

## Next Checks

1. Conduct qualitative evaluation with human annotators rating interpretability and completeness of PROMPT-SAW compressed prompts versus token-level methods on 5-point scale.

2. Perform sensitivity analysis varying similarity threshold δ and compression ratio η* across multiple orders of magnitude to measure performance degradation and compression efficiency.

3. Apply PROMPT-SAW to domain-specific prompts (medical, legal, technical) to validate generalization beyond tested evaluation datasets and measure performance retention.