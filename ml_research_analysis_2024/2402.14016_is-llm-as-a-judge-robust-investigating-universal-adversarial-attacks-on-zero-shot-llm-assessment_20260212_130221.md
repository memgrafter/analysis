---
ver: rpa2
title: Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot
  LLM Assessment
arxiv_id: '2402.14016'
source_url: https://arxiv.org/abs/2402.14016
tags:
- attack
- assessment
- adversarial
- phrase
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the vulnerability of Large Language Models
  (LLMs) as zero-shot assessors to adversarial manipulation. It demonstrates that
  short universal adversarial phrases can deceive LLM assessment systems into predicting
  inflated scores.
---

# Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment

## Quick Facts
- arXiv ID: 2402.14016
- Source URL: https://arxiv.org/abs/2402.14016
- Reference count: 40
- Primary result: Short universal adversarial phrases can deceive LLM assessment systems into predicting inflated scores

## Executive Summary
This work investigates the vulnerability of Large Language Models (LLMs) as zero-shot assessors to adversarial manipulation. The study demonstrates that short universal adversarial phrases can deceive LLM assessment systems into predicting inflated scores, causing maximum scores to be predicted regardless of input text quality. The research proposes a practical surrogate attack method where a universal attack phrase is learned on a smaller model and then transferred to unknown judge-LLMs. Experiments show that comparative assessment is significantly more robust than absolute scoring against such attacks. The findings raise concerns about the reliability of LLM-as-a-judge methods and emphasize the need to address vulnerabilities before deployment in high-stakes real-world scenarios.

## Method Summary
The paper proposes a practical algorithm using greedy search to determine short universal attack phrases for LLM-as-a-judge systems. The attack learns on a smaller surrogate model (FlanT5-xl) and transfers to larger target models (Llama2-7B, Mistral-7B, ChatGPT). The method is evaluated on two benchmark datasets: SummEval and TopicalChat, using both comparative and absolute assessment settings. Attack effectiveness is measured by the average rank improvement of attacked text, with a rank of 1 indicating maximum vulnerability. The paper also investigates perplexity as a simple detection mechanism for adversarial examples.

## Key Results
- Universal adversarial phrases can cause judge-LLMs to predict maximum scores regardless of input text quality
- Comparative assessment is significantly more robust to adversarial attacks than absolute scoring
- Attack phrases learned on surrogate models successfully transfer to unknown judge-LLMs (Llama2-7B, Mistral-7B, ChatGPT)
- Perplexity-based detection shows promise but has limitations in identifying adversarial examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Universal adversarial phrases can deceive LLM assessment systems into predicting inflated scores regardless of input text quality.
- Mechanism: Concatenating a short, learned phrase to any candidate response exploits vulnerabilities in how LLMs score or compare text quality. The phrase causes the model to misattribute high quality even to poor text.
- Core assumption: The learned phrase has properties that the LLM's scoring or comparison logic interprets as indicators of high quality.
- Evidence anchors:
  - [abstract]: "demonstrates that short universal adversarial phrases can be concatenated to deceive judge LLMs to predict inflated scores"
  - [section 4.1]: Defines attack objective as maximizing rank improvement via phrase concatenation
  - [corpus]: Weak - no specific corpus evidence provided for mechanism
- Break condition: If the LLM's scoring logic is modified to ignore or downweight certain phrase patterns, or if comparative assessment inherently constrains the attack.

### Mechanism 2
- Claim: Surrogate model attacks transfer to unknown judge-LLMs, causing inflated scores.
- Mechanism: An attack phrase learned on a small surrogate model (e.g., FlanT5-3B) can be transferred to larger models (e.g., Llama2-7B, Mistral-7B, ChatGPT) due to similarities in training data or model architecture.
- Core assumption: Transferability is possible because models share common training data distributions or architectural biases.
- Evidence anchors:
  - [abstract]: "propose a practical surrogate attack where a universal attack phrase is learned on a smaller model and then transferred to unknown judge-LLMs"
  - [section 6.3]: Demonstrates transfer success on Llama2-7B, Mistral-7B, and GPT3.5
  - [corpus]: Weak - no corpus evidence provided for transferability mechanism
- Break condition: If target models are sufficiently different in architecture or training data, or if defenses are in place.

### Mechanism 3
- Claim: Absolute scoring is more vulnerable to adversarial attacks than comparative assessment.
- Mechanism: In absolute scoring, the model directly assigns a score to the text. An adversary can exploit this by finding a phrase that consistently pushes the score toward the maximum. In comparative assessment, the model compares two texts, and the attack phrase must work across both orderings, making it harder to find an effective universal phrase.
- Core assumption: The comparative setup inherently constrains the attack by requiring the phrase to work in both positions of a pairwise comparison.
- Evidence anchors:
  - [abstract]: "judge-LLMs are significantly more susceptible to these adversarial attacks when used for absolute scoring, as opposed to comparative assessment"
  - [section 6.2]: Shows absolute scoring is more vulnerable in experiments
  - [section B]: Provides analysis suggesting comparative assessment is more robust
- Break condition: If the comparative assessment is modified to use more than two texts or asymmetric evaluation.

## Foundational Learning

- Concept: Adversarial attacks in NLP
  - Why needed here: Understanding how adversarial examples are crafted and transferred is crucial for designing and defending against attacks on LLM assessment systems.
  - Quick check question: What is the difference between a white-box and black-box adversarial attack, and which type is more practical in this setting?

- Concept: Transferability of adversarial examples
  - Why needed here: The attack relies on transferring a learned phrase from a surrogate model to unknown judge-LLMs. Understanding the conditions for successful transfer is essential.
  - Quick check question: What factors influence the transferability of adversarial examples between models?

- Concept: Perplexity as a detection mechanism
  - Why needed here: Perplexity is used as a simple detection approach for adversarial examples. Understanding how perplexity works is important for evaluating its effectiveness.
  - Quick check question: How does perplexity measure the naturalness of a text, and why would adversarial examples have higher perplexity?

## Architecture Onboarding

- Component map: Surrogate Model (FlanT5-xl) -> Judge-LLMs (Llama2-7B, Mistral-7B, ChatGPT) -> Datasets (SummEval, TopicalChat) -> Attack Algorithm (Greedy Search) -> Detection Mechanism (Perplexity)

- Critical path:
  1. Learn universal attack phrase on surrogate model using greedy search
  2. Transfer attack phrase to target judge-LLMs
  3. Evaluate attack effectiveness by measuring average rank of attacked text
  4. (Optional) Apply perplexity detection to identify adversarial examples

- Design tradeoffs:
  - Attack effectiveness vs. transferability: More effective attacks on the surrogate model may not transfer as well to target models
  - Computational cost vs. robustness: Comparative assessment is more robust but requires more inferences than absolute scoring
  - Detection accuracy vs. false positives: Perplexity detection may have false positives for naturally high-perplexity text

- Failure signatures:
  - Attack fails to transfer: Attack phrase has no effect on target model
  - Attack causes maximum score for all texts: Phrase is too effective, reducing assessment utility
  - Detection mechanism has high false positive rate: Legitimate text is flagged as adversarial

- First 3 experiments:
  1. Implement the greedy search attack on the surrogate model and verify that the learned phrase improves the rank of attacked text.
  2. Transfer the learned attack phrase to a target judge-LLM and measure the impact on absolute scoring.
  3. Evaluate the effectiveness of perplexity detection on a dataset augmented with adversarial examples.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in a dedicated section.

## Limitations
- Attack success heavily depends on specific phrasing and prompt templates which are not fully standardized
- Transferability results show mixed success across different model architectures
- The paper does not address potential defense mechanisms beyond simple perplexity detection
- Implications for real-world deployment scenarios are not fully explored

## Confidence
- **High Confidence**: The core finding that LLM-as-a-judge systems are vulnerable to universal adversarial phrases is well-supported by experimental evidence across multiple models and datasets.
- **Medium Confidence**: The transferability of attacks between models shows promising results but varies significantly across different model pairs.
- **Low Confidence**: The paper's implications for real-world deployment scenarios are not fully explored.

## Next Checks
1. **Cross-domain transferability test**: Evaluate attack phrase transferability on LLM-as-a-judge systems for completely different tasks (e