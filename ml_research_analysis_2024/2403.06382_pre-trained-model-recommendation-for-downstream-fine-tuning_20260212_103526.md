---
ver: rpa2
title: Pre-Trained Model Recommendation for Downstream Fine-tuning
arxiv_id: '2403.06382'
source_url: https://arxiv.org/abs/2403.06382
tags:
- transfer
- features
- task
- time
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fennec, a model ranking framework that efficiently
  selects the best pre-trained model for a target task by learning latent transfer
  preferences in a collaborative filtering-style subspace, while also encoding model
  architecture via a novel archi2vec method. Experiments on two benchmarks (PARC and
  Fennec) show that Fennec achieves state-of-the-art performance with O(1) inference
  time and without requiring forward passes or labels.
---

# Pre-Trained Model Recommendation for Downstream Fine-tuning

## Quick Facts
- arXiv ID: 2403.06382
- Source URL: https://arxiv.org/abs/2403.06382
- Reference count: 40
- One-line primary result: Fennec achieves state-of-the-art pre-trained model ranking with O(1) inference time and 87.79% mean Pearson correlation on the Fennec benchmark.

## Executive Summary
This paper introduces Fennec, a model ranking framework that efficiently selects the best pre-trained model for a target task by learning latent transfer preferences in a collaborative filtering-style subspace. The approach combines a novel archi2vec method for encoding model architectures with a large proxy vision model (CLIP) to infer task representations without requiring labels. Experiments on two benchmarks show Fennec achieves state-of-the-art performance while drastically reducing computation time compared to existing methods.

## Method Summary
Fennec learns latent embeddings for models and tasks from historical performance data using Non-negative Matrix Factorization, then combines these with architecture-encoded meta-features via a linear regressor. The framework operates in three phases: (1) Transfer phase learns model and task embeddings from historical FDA-based performance scores, (2) Meta phase encodes model architectures using archi2vec and extracts statistical features, and (3) Merge phase infers new task embeddings via CLIP proxy and combines transfer and meta scores to produce final rankings. The method achieves O(1) inference time by avoiding forward passes or label requirements.

## Key Results
- Achieves 87.79% mean Pearson correlation on the larger Fennec benchmark, surpassing existing methods
- Demonstrates O(1) inference time by avoiding forward passes and label requirements
- Reduces computation time drastically compared to state-of-the-art methods while maintaining high accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fennec maps models and tasks into a shared transfer-related subspace where inner products estimate transferability.
- Mechanism: Uses Non-negative Matrix Factorization (NMF) to decompose a historical performance matrix into latent model and task embeddings; the dot product of these embeddings serves as the transfer score.
- Core assumption: Historical performance matrix entries capture transferable patterns that can be linearly combined via latent factors.
- Evidence anchors:
  - [abstract] "The key insight is to map all models and historical tasks into a transfer-related subspace, where the distance between model vectors and task vectors represents the magnitude of transferability."
  - [section 3.1] "We maintain a historical performance matrix...We use the Non-negative Matrix Factorization(NMF) to decompose the matrix...The objective function is: min ||P - MD⊤||²F + αm||M||²F + αd||D||²F."
  - [corpus] Weak: No direct evidence in neighbors; corpus not directly relevant.
- Break condition: If the historical performance matrix is sparse or contains noise, NMF may fail to capture meaningful transfer preferences.

### Mechanism 2
- Claim: The archi2vec method encodes complex neural network architectures into low-dimensional vectors that capture inductive bias.
- Mechanism: Builds a directed acyclic attributed graph representing the model's computational structure, then applies graph2vec to learn embeddings; these embeddings are clustered and used as architectural features.
- Core assumption: The runtime data flow and operation types of a model are sufficient to encode its inductive bias relevant to transferability.
- Evidence anchors:
  - [abstract] "We also investigate the impact of the inherent inductive bias of models on transfer results and propose a novel method called archi2vec to encode the intricate structures of models."
  - [section 3.2] "We represent the architectural structure of models using a directed acyclic attributed graph...We employ graph2vec...We utilize KMeans to cluster these embeddings."
  - [corpus] Weak: No direct evidence in neighbors; corpus not directly relevant.
- Break condition: If graph2vec fails to differentiate between architectures that have similar transfer performance, the meta-features will be uninformative.

### Mechanism 3
- Claim: A large proxy vision model (CLIP) can infer new task representations in the transfer space without requiring labels.
- Mechanism: Extracts CLIP features from a small subset of the target task, trains a regressor to map CLIP features to historical task embeddings, then uses this mapping to embed new tasks.
- Core assumption: Tasks with similar CLIP embeddings will have similar latent transfer preferences.
- Evidence anchors:
  - [abstract] "A large vision model, as a proxy, infers a new task's representation in the transfer space, thereby circumventing the computational burden of extensive forward passes and reliance on labels."
  - [section 3.3] "We employ one common proxy model to generate forward features for different tasks...We construct a regressor that connects the CLIP features gj with the learned embedding dj."
  - [corpus] Weak: No direct evidence in neighbors; corpus not directly relevant.
- Break condition: If the proxy model's feature space does not correlate with transferability, the regressor will produce poor task embeddings.

## Foundational Learning

- Concept: Non-negative Matrix Factorization (NMF)
  - Why needed here: Decomposes the historical performance matrix into non-negative latent factors, ensuring interpretability and compatibility with transfer scores.
  - Quick check question: What property of the performance matrix makes NMF a suitable choice for decomposition?
- Concept: Fisher Discriminant Analysis (FDA)
  - Why needed here: Provides a proxy for true transfer performance by projecting features into a discriminative subspace without full fine-tuning.
  - Quick check question: How does FDA approximate the separability that would be achieved by fine-tuning on the target task?
- Concept: Graph embedding via graph2vec
  - Why needed here: Encodes the structural and semantic properties of neural network architectures into continuous vectors that can be used as meta-features.
  - Quick check question: What graph-level property does graph2vec preserve that is relevant for comparing model architectures?

## Architecture Onboarding

- Component map: Historical FDA matrix -> NMF decomposition -> Model and task embeddings -> CLIP feature extraction -> Regressor prediction -> Transfer scores -> Linear combination with meta features -> Final ranking
- Critical path: For a new task -> extract CLIP features -> predict embedding via regressor -> compute transfer scores via dot products -> extract meta features -> combine via Linear Regression -> final ranking
- Design tradeoffs:
  - Offline vs online computation: heavy NMF and graph2vec done once; only CLIP inference and simple arithmetic at inference time
  - Accuracy vs speed: CLIP proxy avoids full forward passes but may lose fine-grained task information
  - Model diversity vs training cost: 105 models cover 60+ architectures but require extensive pre-training
- Failure signatures:
  - Poor Pearson correlation indicates either bad embedding alignment or irrelevant meta-features
  - High reconstruction error in NMF suggests insufficient historical data or noisy performance scores
  - Degraded performance when α is too high or too low suggests imbalance between meta and transfer contributions
- First 3 experiments:
  1. Run NMF on synthetic historical performance matrix to confirm convergence and embedding quality
  2. Validate archi2vec on a small set of known architectures to check that similar structures get similar vectors
  3. Test CLIP regressor mapping on held-out historical tasks to ensure accurate embedding prediction before full pipeline integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Fennec vary with the size and diversity of the historical performance matrix? The paper uses a fixed historical performance matrix, but it is unclear how the performance scales with larger or more diverse datasets.
- Basis in paper: [explicit] The paper mentions using a historical performance matrix but does not explore the impact of its size or diversity on performance.
- Why unresolved: The paper does not provide experiments or analysis on how varying the size or diversity of the historical performance matrix affects the performance of Fennec.
- What evidence would resolve it: Experiments showing the performance of Fennec with different sizes and diversities of the historical performance matrix would provide insights into its scalability and robustness.

### Open Question 2
- Question: What is the impact of using different proxy models for inferring task embeddings in the merge phase? The paper uses CLIP as a proxy model, but it is unclear how other models like Open_Clip or Dinov2 would perform.
- Basis in paper: [explicit] The paper compares CLIP with Open_Clip and Dinov2 but does not provide a comprehensive analysis of their impact on performance.
- Why unresolved: The paper only provides a brief comparison of different proxy models without exploring their impact on the overall performance of Fennec.
- What evidence would resolve it: Experiments comparing the performance of Fennec using different proxy models for inferring task embeddings would clarify the impact of the choice of proxy model.

### Open Question 3
- Question: How does the choice of the weight α in the merge phase affect the performance of Fennec? The paper uses a default value of 0.5 but does not explore the impact of different values.
- Basis in paper: [explicit] The paper mentions the use of α to tune the weight between transfer and meta scores but does not provide an in-depth analysis of its impact.
- Why unresolved: The paper does not provide experiments or analysis on how varying the value of α affects the performance of Fennec.
- What evidence would resolve it: Experiments showing the performance of Fennec with different values of α would provide insights into the optimal choice of this parameter.

## Limitations
- The paper does not specify the size or diversity of the historical performance matrix, which could significantly impact embedding quality
- The archi2vec method's ability to capture architectural inductive bias is demonstrated but not thoroughly validated against alternative encodings
- The CLIP-based proxy task embedding assumes visual features correlate with transferability, which may not generalize to non-visual tasks

## Confidence

- **High confidence**: The core NMF-based transfer subspace learning mechanism is well-established and the empirical results on both PARC and Fennec benchmarks are clearly reported with strong Pearson correlations
- **Medium confidence**: The archi2vec method for encoding model architectures is novel and shows promise, but the paper provides limited ablation studies on alternative architectural encodings or validation of the graph2vec embeddings' quality
- **Medium confidence**: The CLIP-based proxy task embedding is a reasonable approach to avoid label dependence, but the paper does not provide detailed analysis of when this proxy might fail or how sensitive the results are to the choice of proxy model

## Next Checks

1. **NMF Embedding Quality Validation**: Perform a thorough analysis of the historical performance matrix reconstruction error across different latent dimensions and evaluate how reconstruction quality correlates with downstream ranking performance

2. **Architectural Encoding Ablation**: Compare archi2vec embeddings against alternative architectural encodings (e.g., simple parameter-based features, flattened weight matrices) to quantify the actual contribution of the graph-based approach to ranking accuracy

3. **Proxy Model Sensitivity**: Test the framework's robustness to different proxy models by replacing CLIP with other large vision models or domain-specific proxies, and measure how much ranking performance degrades with each alternative