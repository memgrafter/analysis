---
ver: rpa2
title: Cross-View Referring Multi-Object Tracking
arxiv_id: '2412.17807'
source_url: https://arxiv.org/abs/2412.17807
tags:
- language
- tracking
- cross-view
- description
- multi-object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Cross-view Referring Multi-Object Tracking
  (CRMOT), a new task that extends Referring Multi-Object Tracking (RMOT) by incorporating
  cross-view data to address the problem of object invisibility in single-view scenarios.
  The authors construct a benchmark called CRTrack, which includes 13 scenes, 82K
  frames, 344 objects, and 221 language descriptions, derived from the CAMPUS and
  DIVOTrack datasets.
---

# Cross-View Referring Multi-Object Tracking

## Quick Facts
- arXiv ID: 2412.17807
- Source URL: https://arxiv.org/abs/2412.17807
- Authors: Sijia Chen; En Yu; Wenbing Tao
- Reference count: 14
- Key outcome: Introduces CRMOT task and CRTrack benchmark, proposes CRTracker achieving 54.88% CVRIDF1 and 35.97% CVRMA in in-domain evaluation

## Executive Summary
This paper introduces Cross-view Referring Multi-Object Tracking (CRMOT), a novel task that extends Referring Multi-Object Tracking (RMOT) by incorporating cross-view data to address object invisibility issues in single-view scenarios. The authors construct CRTrack, a benchmark derived from CAMPUS and DIVOTrack datasets, containing 13 scenes, 82K frames, 344 objects, and 221 language descriptions. They propose CRTracker, an end-to-end method that combines the multi-object tracking capabilities of CrossMOT with the multi-modal capabilities of APTM, along with a prediction module to enhance tracking accuracy. Extensive experiments demonstrate that CRTracker significantly outperforms existing methods, achieving 54.88% CVRIDF1 and 35.97% CVRMA in in-domain evaluation, and 12.52% CVRIDF1 and 2.32% CVRMA in cross-domain evaluation.

## Method Summary
The proposed method, CRTracker, is an end-to-end approach that integrates CrossMOT's accurate multi-object tracking capability with APTM's powerful multi-modal capability. The architecture consists of a detection head (CenterNet backbone), single-view Re-ID head, cross-view Re-ID head, full Re-ID head, APTM image encoder, APTM text encoder, APTM cross encoder, and a prediction module. The method operates by first detecting objects in synchronized video sequences from multiple views, extracting features, encoding language descriptions, fusing visual and textual features, aggregating scores, and applying prediction module filtering to output consistent trajectories. The model is trained for 20 epochs using Adam optimizer with learning rate 1e-4 and batch size 12.

## Key Results
- CRTracker achieves 54.88% CVRIDF1 and 35.97% CVRMA in in-domain evaluation on CRTrack benchmark
- Outperforms existing methods by significant margins in both in-domain and cross-domain evaluations
- Successfully tracks objects matching language descriptions while maintaining identity consistency across cross-views
- Demonstrates the effectiveness of combining cross-view data with language grounding for multi-object tracking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The fusion of multi-view appearance information resolves object invisibility issues in single-view tracking.
- Mechanism: Cross-view data provides redundant visual observations of the same object from different angles, ensuring that occluded or temporarily invisible objects in one view can still be tracked using their appearances in other synchronized views.
- Core assumption: Objects remain spatially consistent across views with large overlapping areas, and their visual features are stable enough for reliable matching.
- Evidence anchors:
  - [abstract]: "It introduces the cross-view to obtain the appearances of objects from multiple views, avoiding the problem of the invisible appearances of objects in RMOT task."
  - [section]: "In the single-view, some appearances of the objects are easily invisible, causing the network for the RMOT task to incorrectly match objects with the fine-grained language description."

### Mechanism 2
- Claim: The prediction module uses multi-view fusion scores to maintain identity consistency across cross-views.
- Mechanism: The module aggregates per-view confidence scores and hit/miss tracking statistics to filter and retain only high-confidence tracks that consistently match the language description across multiple synchronized views.
- Core assumption: Consistent detection across multiple views indicates a true positive match, while inconsistent detections across views are likely false positives.
- Evidence anchors:
  - [section]: "The design idea of the prediction module is to regard the frame-to-frame association results as the detection results, the fusion scores Sf as the confidences, and the prediction module plays a tracking role."
  - [section]: Algorithm 1 shows the score aggregation logic using average fusion scores and single-view fusion scores.

### Mechanism 3
- Claim: The combination of CrossMOT's tracking capability and APTM's multi-modal strength creates a unified end-to-end solution.
- Mechanism: CrossMOT provides accurate multi-object tracking across cross-views using appearance and motion features, while APTM adds language grounding through text-image matching, enabling the system to track objects that match specific language descriptions.
- Core assumption: The two components can be effectively merged without interfering with each other's learning objectives, and the language grounding can be performed in a post-detection pipeline.
- Evidence anchors:
  - [abstract]: "CRTracker combines the accurate multi-object tracking capability of CrossMOT (Hao et al. 2024) and the powerful multi-modal capability of APTM (Yang et al. 2023)."
  - [section]: "Specifically, CRTracker combines the accurate multi-object tracking capability of CrossMOT (Hao et al. 2024) and the powerful multi-modal capability of APTM (Yang et al. 2023)."

## Foundational Learning

- Concept: Cross-view multi-object tracking (CV-MOT)
  - Why needed here: CRMOT builds upon CV-MOT by adding language constraints, so understanding how objects are associated across views is fundamental.
  - Quick check question: How does CV-MOT typically handle the association of the same object across different camera views?

- Concept: Referring multi-object tracking (RMOT)
  - Why needed here: CRMOT extends RMOT by incorporating cross-view data, so understanding how language descriptions guide tracking is essential.
  - Quick check question: What are the main challenges in matching objects to fine-grained language descriptions in video sequences?

- Concept: Feature fusion and multi-modal learning
  - Why needed here: The model combines visual features from multiple views with language features, requiring understanding of how to effectively merge heterogeneous data types.
  - Quick check question: What are common strategies for fusing visual and textual features in multi-modal learning tasks?

## Architecture Onboarding

- Component map: Detection head (CenterNet) -> Single-view Re-ID head -> Cross-view Re-ID head -> Full Re-ID head -> APTM image encoder -> APTM text encoder -> APTM cross encoder -> Prediction module
- Critical path: Input synchronized video sequences -> Object detection -> Multi-view feature extraction -> Language description encoding -> Feature fusion -> Score aggregation -> Prediction module filtering -> Output trajectories
- Design tradeoffs:
  - End-to-end vs. two-stage: End-to-end allows joint optimization but increases complexity; two-stage separates detection from language grounding but may lose optimization synergy.
  - Feature fusion weight (α): Higher values give more importance to ground truth features but may reduce generalization; lower values rely more on learned features but may miss fine-grained details.
  - Score fusion weight (β): Controls the balance between text scores and attribute scores; incorrect weighting can lead to poor language matching or attribute consistency.
- Failure signatures:
  - High false positives: Likely due to poor language grounding or overly permissive score thresholds.
  - Identity switches: May indicate insufficient cross-view Re-ID capability or inconsistent feature extraction across views.
  - Low recall: Could result from overly strict filtering in the prediction module or poor detection in challenging views.
- First 3 experiments:
  1. Validate cross-view association: Test the CrossMOT component alone on synchronized views to ensure accurate ID consistency.
  2. Validate language grounding: Test the APTM component with pre-extracted object features to ensure accurate language matching.
  3. Validate end-to-end integration: Run the full CRTracker on a simple CRMOT scene to check if the combined system maintains both tracking accuracy and language matching.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the CRMOT performance change if we used a more advanced language model for generating language descriptions, beyond GPT-4o?
- Basis in paper: [inferred] The paper mentions using GPT-4o for generating language descriptions, but does not explore alternative language models or their impact on performance.
- Why unresolved: The paper does not compare the effectiveness of different language models for generating descriptions, leaving the potential for improvement with more advanced models unexplored.
- What evidence would resolve it: Conducting experiments using different language models (e.g., GPT-4, Claude) and comparing the resulting CRMOT performance metrics (CVRIDF1, CVRMA) would provide evidence.

### Open Question 2
- Question: What is the impact of varying the object density on CRMOT performance, and how does it compare to single-view MOT in high-density scenarios?
- Basis in paper: [explicit] The paper discusses object density as a factor in the CRTrack benchmark, but does not provide a detailed analysis of its impact on CRMOT performance or compare it to single-view MOT.
- Why unresolved: The paper does not provide a systematic study on how object density affects CRMOT performance, nor does it compare this to the performance of single-view MOT in similar conditions.
- What evidence would resolve it: Conducting experiments with varying object densities and comparing CRMOT performance to single-view MOT in high-density scenarios would provide insights.

### Open Question 3
- Question: How does the CRMOT task generalize to real-world scenarios with dynamic lighting conditions and occlusions not present in the CRTrack benchmark?
- Basis in paper: [inferred] The CRTrack benchmark is constructed from specific datasets (DIVOTrack and CAMPUS), which may not fully capture the complexity of real-world scenarios with dynamic lighting and occlusions.
- Why unresolved: The paper does not test the CRMOT task on real-world data or scenarios with dynamic lighting and occlusions, leaving the generalization capabilities uncertain.
- What evidence would resolve it: Evaluating CRMOT performance on real-world datasets with varying lighting conditions and occlusions would provide evidence of its generalization capabilities.

## Limitations

- Scalability concerns: The CRTrack benchmark contains only 13 scenes and 221 language descriptions, limiting generalizability to more diverse real-world scenarios.
- Cross-view synchronization assumption: The method assumes perfectly synchronized video sequences across views, which may not hold in practical deployments with different frame rates or synchronization delays.
- Feature fusion sensitivity: The method relies heavily on weighted fusion of visual and textual features with hyperparameters α and β, but the sensitivity of performance to these hyperparameters is not thoroughly explored.

## Confidence

- High confidence in the core mechanism that cross-view data provides redundant observations to resolve single-view invisibility issues.
- Medium confidence in the prediction module's effectiveness and the integration of CrossMOT and APTM components.
- Low confidence in the generalizability of the end-to-end integration to scenarios beyond the CRTrack benchmark.

## Next Checks

1. **Synchronization robustness test**: Evaluate CRTracker performance when cross-view videos have varying degrees of temporal misalignment (e.g., 0.5s, 1s, 2s desynchronization) to assess real-world applicability.

2. **Hyperparameter sensitivity analysis**: Systematically vary α (feature fusion weight) and β (score fusion weight) across a range of values to identify optimal settings and understand performance stability.

3. **Zero-shot cross-dataset evaluation**: Test CRTracker on CRMOT-like scenarios from entirely different datasets (e.g., nuScenes, Cityscapes) without fine-tuning to evaluate true generalization capability beyond the CRTrack benchmark.