---
ver: rpa2
title: Smaller Large Language Models Can Do Moral Self-Correction
arxiv_id: '2410.23496'
source_url: https://arxiv.org/abs/2410.23496
tags:
- llms
- arxiv
- self-correction
- language
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the capability of small language models
  (less than 22B parameters) to perform moral self-correction, particularly in addressing
  social stereotyping. The authors hypothesize that while larger models excel at following
  instructions and understanding abstract social norms, smaller models might struggle
  in these areas.
---

# Smaller Large Language Models Can Do Moral Self-Correction

## Quick Facts
- **arXiv ID:** 2410.23496
- **Source URL:** https://arxiv.org/abs/2410.23496
- **Reference count:** 20
- **Primary result:** Models with over 3.8B parameters can achieve good moral self-correction performance, highlighting the role of safety alignment fine-tuning.

## Executive Summary
This paper investigates whether smaller language models (under 22B parameters) can perform moral self-correction, particularly in addressing social stereotyping. The authors hypothesize that while larger models excel at following instructions and understanding abstract social norms, smaller models might struggle in these areas. Through empirical testing across model scales from 355M to 70B parameters on benchmarks like Winogender and BBQ, they find that models with over 3.8B parameters can achieve good moral self-correction performance, though all scales show poor self-correction performance when given unethical instructions.

## Method Summary
The authors conducted empirical validation by testing various language model scales (355M to 70B parameters) on moral self-correction tasks using benchmarks like Winogender and BBQ. They employed meticulous prompting strategies and tested the impact of Chain-of-Thought (CoT) explanations, instruction specificity, and safety alignment fine-tuning. The study compared performance across different model scales to identify the minimum parameter threshold for effective moral self-correction capabilities.

## Key Results
- Models with over 3.8B parameters can achieve good moral self-correction performance, highlighting the significant role of safety alignment fine-tuning
- Smaller models are weaker than larger-scale models in comprehending social norms and self-explanation through Chain-of-Thought, but all scales show poor self-correction performance given unethical instructions
- CoT explanations help the 70B model surpass self-correction performance, but smaller models (less than 70B) cannot generate informative explanations for morality-relevant questions
- Increasing instruction specificity benefits self-correction performance across all model scales

## Why This Works (Mechanism)
The mechanism behind moral self-correction in smaller language models appears to be heavily dependent on model scale, prompting strategy, and safety alignment fine-tuning. Larger models benefit from better comprehension of abstract social norms and can generate more informative Chain-of-Thought explanations, while smaller models require more specific instructions to achieve comparable performance. The safety alignment fine-tuning plays a crucial role in enabling even smaller models to perform moral self-correction tasks effectively.

## Foundational Learning
- **Social Norm Comprehension**: Understanding abstract social norms is essential for moral reasoning and self-correction
  - *Why needed*: Moral self-correction requires understanding what constitutes ethical behavior in social contexts
  - *Quick check*: Test models on varied social norm scenarios to assess comprehension depth

- **Chain-of-Thought Reasoning**: The ability to generate step-by-step explanations for moral decisions
  - *Why needed*: CoT helps models articulate their reasoning process and identify potential ethical issues
  - *Quick check*: Evaluate explanation quality across different model scales on morality tasks

- **Safety Alignment Fine-tuning**: Specialized training to align model behavior with ethical guidelines
  - *Why needed*: Enables models to recognize and correct potentially harmful outputs
  - *Quick check*: Compare performance between aligned and non-aligned models on moral tasks

## Architecture Onboarding
**Component Map**: Input Prompt -> Model Processing -> Chain-of-Thought Generation -> Self-Correction Mechanism -> Output

**Critical Path**: The critical path involves prompt processing, moral reasoning through CoT, and self-correction implementation, with safety alignment fine-tuning serving as a foundational component.

**Design Tradeoffs**: Larger models offer better social norm comprehension and CoT generation but require more computational resources, while smaller models need more specific instructions but are more efficient.

**Failure Signatures**: Models fail to self-correct when given unethical instructions, struggle with abstract social norm comprehension, and cannot generate informative CoT explanations for morality-relevant questions.

**First Experiments**:
1. Test model performance on progressively more nuanced ethical scenarios to identify capability boundaries
2. Conduct ablation studies on different prompting strategies to isolate performance factors
3. Compare aligned versus non-aligned models on the same moral reasoning tasks

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses on only two specific benchmark datasets (Winogender and BBQ), which may not represent the full range of moral reasoning scenarios
- Performance appears heavily dependent on prompt specificity and structure, raising questions about whether improvements reflect genuine capabilities or prompt sensitivity
- The empirical validation is constrained by specific model families and fine-tuning procedures tested, without exploring variations in approaches

## Confidence
- **High**: Models with over 3.8B parameters can achieve good moral self-correction performance
- **Medium**: CoT explanations benefit the 70B model but not smaller models
- **Low**: None of the models can reliably detect and refuse unethical instructions

## Next Checks
1. Test the same model scales on a broader range of moral reasoning benchmarks beyond Winogender and BBQ to assess generalizability of the findings
2. Conduct ablation studies on different prompting strategies and safety alignment fine-tuning approaches to isolate the factors contributing to moral self-correction performance
3. Evaluate model performance on progressively more nuanced ethical scenarios to identify the boundaries of current moral reasoning capabilities and determine whether the observed limitations are fundamental or can be overcome with better techniques