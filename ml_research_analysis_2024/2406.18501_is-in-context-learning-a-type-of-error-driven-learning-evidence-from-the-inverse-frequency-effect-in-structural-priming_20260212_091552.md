---
ver: rpa2
title: Is In-Context Learning a Type of Error-Driven Learning? Evidence from the Inverse
  Frequency Effect in Structural Priming
arxiv_id: '2406.18501'
source_url: https://arxiv.org/abs/2406.18501
tags:
- priming
- learning
- sentence
- prime
- structural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether in-context learning (ICL) in large\
  \ language models (LLMs) functions like error-driven learning. Using the inverse\
  \ frequency effect (IFE) from psycholinguistics\u2014where unexpected examples exert\
  \ a stronger influence\u2014the authors tested if LLMs exhibit the IFE during ICL."
---

# Is In-Context Learning a Type of Error-Driven Learning? Evidence from the Inverse Frequency Effect in Structural Priming
## Quick Facts
- arXiv ID: 2406.18501
- Source URL: https://arxiv.org/abs/2406.18501
- Reference count: 24
- One-line primary result: Large language models show inverse frequency effects in structural priming, with larger models displaying stronger IFE, suggesting ICL becomes more error-driven as models scale.

## Executive Summary
This paper investigates whether in-context learning (ICL) in large language models functions like error-driven learning by testing for the inverse frequency effect (IFE). The IFE, a well-established phenomenon in human language processing, shows that unexpected or rare examples exert stronger learning influences than frequent ones. The authors simulated structural priming using ditransitive sentences, varying whether the indirect object was a pronoun, and tested across several LLM sizes. Results showed that larger models displayed stronger IFE effects, especially in the pronoun condition, supporting the hypothesis that ICL implicitly computes an error signal during forward passes. This suggests ICL may share underlying mechanisms with human error-driven learning, becoming more human-like at scale.

## Method Summary
The authors tested for inverse frequency effects in LLMs by simulating structural priming with ditransitive sentences ("The mother gave the boy a toy" vs. "The mother gave him a toy"). They created stimuli where the first sentence established a syntactic structure, and the second sentence required continuation, varying whether the indirect object was a pronoun. They measured surprisal (negative log probability) of expected continuations across different model sizes. The key manipulation was whether the prime sentence contained a pronoun or full noun phrase, allowing them to test if less frequent structures (pronoun conditions) showed stronger priming effects. They compared surprisal differences across model sizes to detect inverse frequency patterns.

## Key Results
- Larger models showed stronger inverse frequency effects than smaller models in structural priming tasks
- The pronoun condition (less frequent structure) produced the strongest IFE effects, especially in larger models
- These results support the hypothesis that ICL becomes more error-driven as models scale, with larger models showing more human-like processing patterns

## Why This Works (Mechanism)
The inverse frequency effect emerges because unexpected or rare linguistic structures generate higher prediction errors, which drive stronger learning signals. In ICL, when a model encounters an unexpected structure, the mismatch between prediction and input creates a larger "error signal" that influences subsequent predictions more strongly. This mirrors error-driven learning in humans, where surprising events trigger more robust memory encoding and behavioral adaptation. The pronoun condition showed the strongest effects because pronoun usage in ditransitive structures is less frequent, creating larger prediction errors when encountered.

## Foundational Learning
1. **Inverse Frequency Effect** - A cognitive phenomenon where rare or unexpected stimuli exert stronger learning influences than frequent ones. Needed to establish the behavioral signature being tested. Quick check: Do humans show stronger learning from surprising vs. expected examples in controlled experiments?

2. **Structural Priming** - A psycholinguistic phenomenon where exposure to a syntactic structure increases the likelihood of using that structure subsequently. Needed as the experimental paradigm to test IFE in LLMs. Quick check: Does repeated exposure to a sentence structure bias subsequent sentence completions in both humans and models?

3. **Error-Driven Learning** - Learning mechanisms where prediction errors drive weight updates and behavioral change. Needed to frame ICL as potentially analogous to established cognitive mechanisms. Quick check: Do models that show IFE also show patterns consistent with explicit error backpropagation during training?

## Architecture Onboarding
**Component Map:** Input tokens -> Embedding layer -> Transformer blocks (attention + feed-forward) -> Output logits -> Probability distribution
**Critical Path:** Token embedding → Multi-head attention → Feed-forward network → Next token prediction
**Design Tradeoffs:** The study leverages existing transformer architectures without modification, trading experimental control for ecological validity in testing real-world ICL behavior
**Failure Signatures:** Absence of IFE would suggest ICL doesn't compute error signals; uniform effects across model sizes would contradict the scaling hypothesis
**First Experiments:** 1) Test IFE in non-linguistic domains (math, code) to assess generality 2) Compare IFE patterns between ICL and traditional fine-tuning 3) Measure attention pattern changes during IFE conditions

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- The study uses only one specific linguistic phenomenon (structural priming with ditransitive constructions), limiting generalizability to other domains or learning tasks
- The inverse frequency effect observed in human language processing may have distinct computational underpinnings compared to LLM mechanisms, despite surface similarities
- Results represent a narrow slice of possible ICL scenarios and could potentially stem from attention patterns or token frequency biases rather than explicit error computation

## Confidence
- **High confidence**: The empirical observation that larger models show stronger inverse frequency effects during structural priming tasks
- **Medium confidence**: The interpretation that this pattern reflects error-driven learning mechanisms analogous to human cognitive processing
- **Low confidence**: The claim that ICL fundamentally computes error signals in the same way as explicit supervised learning paradigms

## Next Checks
1. Replicate the inverse frequency effect across diverse linguistic structures and semantic domains to test generalizability beyond ditransitive constructions
2. Conduct ablation studies comparing IFE patterns in ICL versus traditional fine-tuning to isolate whether the effect truly reflects error-driven mechanisms or alternative computational processes
3. Test the hypothesis using different model architectures (RNNs, transformers with varying attention mechanisms) to determine if the IFE pattern is specific to the transformer architecture or represents a more general property of large-scale neural networks