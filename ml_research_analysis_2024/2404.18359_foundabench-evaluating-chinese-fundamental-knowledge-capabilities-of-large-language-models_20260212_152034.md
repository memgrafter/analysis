---
ver: rpa2
title: 'FoundaBench: Evaluating Chinese Fundamental Knowledge Capabilities of Large
  Language Models'
arxiv_id: '2404.18359'
source_url: https://arxiv.org/abs/2404.18359
tags:
- knowledge
- questions
- chinese
- common
- sense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FoundaBench is a benchmark designed to evaluate Chinese large language
  models on fundamental knowledge, covering common sense and K-12 educational subjects
  across 3354 multiple-choice questions. The benchmark is curated from diverse sources
  and validated using psychostatistical methods to ensure alignment with human knowledge
  levels.
---

# FoundaBench: Evaluating Chinese Fundamental Knowledge Capabilities of Large Language Models
## Quick Facts
- arXiv ID: 2404.18359
- Source URL: https://arxiv.org/abs/2404.18359
- Reference count: 8
- A benchmark of 3354 multiple-choice questions evaluating Chinese LLMs on fundamental knowledge across common sense and K-12 subjects

## Executive Summary
FoundaBench is a comprehensive benchmark designed to evaluate the fundamental knowledge capabilities of Chinese large language models across common sense and K-12 educational subjects. The benchmark consists of 3354 multiple-choice questions carefully curated from diverse sources and validated using psychostatistical methods to ensure alignment with human knowledge levels. Evaluations on 12 models using zero-shot and CircularEval methods reveal that models pre-trained on Chinese corpora significantly outperform English-oriented models, with InternLM-123B and GPT-4 achieving the highest scores. The results demonstrate that while models exhibit strong memory recall abilities, they struggle with reasoning tasks, particularly in correspondence and numerical reasoning.

## Method Summary
FoundaBench was constructed by curating questions from textbooks, exams, online forums, and educational resources, followed by careful filtering to ensure alignment with human knowledge levels. The benchmark employs psychostatistical validation methods to maintain quality and relevance. Evaluations were conducted using zero-shot prompting and the CircularEval method, which uses questions as both prompts and answers in a circular manner to test model consistency. Twelve models were assessed, including both Chinese-oriented and English-oriented models, with performance measured across different difficulty levels and subject domains.

## Key Results
- Chinese-oriented models significantly outperform English-oriented models on FoundaBench
- InternLM-123B and GPT-4 achieve the highest scores, with GPT-4 leading overall performance
- Models demonstrate strong memory recall but struggle with reasoning tasks, particularly correspondence and numerical reasoning
- Hard examples reveal significant challenges, with most models failing to answer correctly in these areas

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of fundamental knowledge domains and rigorous validation process. By incorporating questions from diverse sources including textbooks, exams, and online forums, FoundaBench captures a broad spectrum of knowledge requirements. The psychostatistical validation ensures questions are appropriately calibrated to human knowledge levels, making the benchmark a reliable measure of model capabilities. The use of multiple-choice format provides clear evaluation metrics while the CircularEval method offers an innovative approach to assess model consistency and reasoning depth.

## Foundational Learning
- **Psychometric validation**: Ensures questions are properly calibrated to human knowledge levels and maintain statistical reliability
- **Curriculum design**: Covers K-12 subjects systematically to test comprehensive knowledge acquisition
- **Question taxonomy**: Categorizes questions by difficulty and subject to enable detailed performance analysis
- **Evaluation methodology**: Implements both zero-shot and CircularEval approaches for robust assessment
- **Cross-lingual benchmarking**: Enables fair comparison between Chinese-oriented and English-oriented models
- **Performance metrics**: Develops comprehensive scoring systems to quantify model capabilities across domains

## Architecture Onboarding
**Component Map**: Question Curation -> Psychometric Validation -> Benchmark Assembly -> Model Evaluation -> Performance Analysis
**Critical Path**: Data Collection → Quality Filtering → Statistical Validation → Benchmark Construction → Model Testing → Results Analysis
**Design Tradeoffs**: Multiple-choice format provides clear metrics but may underestimate reasoning capabilities; zero-shot evaluation shows raw capabilities but misses potential gains from few-shot approaches
**Failure Signatures**: Models consistently fail on correspondence and numerical reasoning tasks, particularly in hard examples
**First Experiments**:
1. Run baseline evaluations on diverse model families to establish performance baselines
2. Implement CircularEval method to test model consistency and reasoning depth
3. Conduct difficulty analysis to identify specific challenge areas and reasoning gaps

## Open Questions the Paper Calls Out
None

## Limitations
- Multiple-choice format may not fully capture the depth of models' reasoning capabilities for complex problem-solving
- Psychostatistical validation based on human knowledge levels may not perfectly align with how models acquire and process information
- Focus on zero-shot performance may overlook the impact of few-shot or fine-tuning approaches

## Confidence
- **High confidence**: Benchmark construction methodology and validation processes are well-documented and methodologically sound
- **Medium confidence**: Comparative performance analysis between Chinese-oriented and English-oriented models is reasonable but may not account for all confounding factors
- **Medium confidence**: Identification of reasoning weaknesses, particularly in correspondence and numerical reasoning, is supported by data but may benefit from additional task-specific evaluations

## Next Checks
1. Conduct a follow-up study using open-ended questions to assess whether the multiple-choice format underestimates models' true reasoning capabilities
2. Implement few-shot evaluations alongside zero-shot testing to determine if performance gaps between Chinese and English-oriented models persist under different prompting strategies
3. Design targeted experiments focusing specifically on correspondence and numerical reasoning tasks to isolate the factors contributing to model failures in these areas