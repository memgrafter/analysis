---
ver: rpa2
title: CTC-based Non-autoregressive Textless Speech-to-Speech Translation
arxiv_id: '2406.07330'
source_url: https://arxiv.org/abs/2406.07330
tags:
- latexit
- sha1
- base64
- translation
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of slow decoding in direct speech-to-speech
  translation (S2ST) caused by long speech sequences. The authors investigate using
  Connectionist Temporal Classification (CTC)-based non-autoregressive (NAR) models
  for S2ST, building on previous successes in machine translation.
---

# CTC-based Non-autoregressive Textless Speech-to-Speech Translation

## Quick Facts
- arXiv ID: 2406.07330
- Source URL: https://arxiv.org/abs/2406.07330
- Authors: Qingkai Fang; Zhengrui Ma; Yan Zhou; Min Zhang; Yang Feng
- Reference count: 17
- Primary result: CTC-based NAR model achieves translation quality comparable to AR models with up to 26.81x decoding speedup

## Executive Summary
This paper addresses the challenge of slow decoding in direct speech-to-speech translation (S2ST) caused by long speech sequences. The authors investigate using Connectionist Temporal Classification (CTC)-based non-autoregressive (NAR) models for S2ST, building on previous successes in machine translation. They develop CTC-S2UT, which combines several techniques including pretraining, knowledge distillation, glancing training, and non-monotonic latent alignments. Experimental results on CVSS-C dataset show that CTC-S2UT achieves translation quality comparable to autoregressive models while maintaining up to 26.81x decoding speedup. The model is particularly effective for translating long speech segments, reaching up to 34.28x speedup.

## Method Summary
The CTC-S2UT model architecture consists of a speech encoder (subsampling module + LeConformer layers) and a NAR unit decoder (Ld Transformer decoder layers). The training pipeline involves encoder pretraining, knowledge distillation from autoregressive models, glancing training with random masking and alignment-based hints, and fine-tuning with non-monotonic latent alignments. The model uses discrete unit sequences as targets, obtained through K-means clustering of mHuBERT features. Two-stage training is employed: first with CTC loss, then fine-tuned with NMLA loss. The model is evaluated on CVSS-C dataset with French→English, English→French, and English→Spanish translation tasks.

## Key Results
- CTC-S2UT achieves translation quality comparable to autoregressive models with BLEU scores of 23.85-25.16
- The model maintains up to 26.81x decoding speedup compared to autoregressive baseline
- CTC-S2UT is particularly effective for long speech segments, reaching up to 34.28x speedup
- Knowledge distillation and glancing training significantly improve NAR model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CTC-based NAR models can achieve translation quality comparable to AR models in S2ST.
- Mechanism: By expanding the output space with a blank token and marginalizing over alignments, CTC reduces the complexity of modeling variable-length speech sequences, allowing NAR decoding without sacrificing accuracy.
- Core assumption: The expanded output space and proper alignment modeling sufficiently capture the dependencies between source and target speech units.
- Evidence anchors:
  - [abstract] "Experimental results demonstrate that by combining pretraining, knowledge distillation, and advanced NAR training techniques such as glancing training and non-monotonic latent alignments, CTC-based NAR models achieve translation quality comparable to the AR model, while preserving up to 26.81× decoding speedup."
  - [section 2.2] "Connectionist temporal classification (CTC; Graves et al., 2006) is a sequence modeling algorithm used to model variable-length mappings between sequences... it first generates a sequence A = (a1, ..., aT) based on the input sequence X, where T is a pre-determined length, and A is referred to as the alignment."

### Mechanism 2
- Claim: Knowledge distillation from AR models improves NAR model performance.
- Mechanism: AR models generate high-quality target unit sequences that NAR models can learn to mimic, reducing the multimodality problem inherent in NAR decoding.
- Core assumption: The AR model's output distribution is a better target for training than the ground truth, especially for NAR models that cannot rely on iterative refinement.
- Evidence anchors:
  - [section 3.2] "Knowledge Distillation We perform sequence-level knowledge distillation (KD; Kim and Rush, 2016) with the autoregressive S2UT model, which is a widely used approach in non-autoregressive translation to reduce data multimodality (Zhou et al., 2020; Shao et al., 2022)."
  - [section 4.3 table 3] "✓ ✓ ✓ × 23.85" shows KD improves performance over models without it.

### Mechanism 3
- Claim: Glancing training with non-monotonic latent alignments enables NAR models to handle long speech sequences efficiently.
- Mechanism: By randomly masking positions in the decoder input and replacing them with high-probability tokens from the alignment, the model learns to fill in missing information in parallel. Non-monotonic latent alignments further improve by maximizing F1 score of expected bigram matching.
- Core assumption: The combination of random masking and alignment-based hints allows the model to learn robust parallel generation without autoregressive dependencies.
- Evidence anchors:
  - [section 3.2] "Glancing training (GLAT; Qian et al., 2021) has been proven to significantly enhance the translation quality of NAR translation models. It first selects the alignment with the highest posterior probability: A∗ = arg maxA∈β−1(Y) P(A|X), and then randomly chooses some positions in the decoder input E to replace with the corresponding token embeddings from the alignment A∗."
  - [section 4.3 table 3] "✓ ✓ ✓ ✓ 25.16" shows NMLA further improves performance.

## Foundational Learning

- Concept: Connectionist Temporal Classification (CTC)
  - Why needed here: CTC provides the mathematical framework for modeling variable-length alignments between source speech and target unit sequences without requiring explicit alignment annotation.
  - Quick check question: How does CTC handle the alignment between variable-length input and output sequences?

- Concept: Non-autoregressive (NAR) sequence generation
  - Why needed here: NAR models generate output tokens in parallel, enabling the significant speedup claimed in the paper compared to autoregressive models.
  - Quick check question: What is the main challenge NAR models face compared to autoregressive models, and how does CTC help address it?

- Concept: Knowledge distillation in sequence modeling
  - Why needed here: Distillation from autoregressive models helps NAR models overcome the multimodality problem by providing a single, high-quality target distribution.
  - Quick check question: Why might knowledge distillation be particularly beneficial for NAR models in speech-to-speech translation?

## Architecture Onboarding

- Component map: Speech -> Encoder (subsampling + LeConformer) -> Decoder (NAR Transformer) -> Alignment (CTC) -> Units -> Vocoder -> Target speech
- Critical path: Speech → Encoder → Decoder → Alignment → Units → Vocoder → Target speech
- Design tradeoffs:
  - Upsample factor λ: Higher values give more decoder input but increase computational cost
  - Glancing ratio: Balances between parallel generation and guidance from alignments
  - Encoder pretraining: Essential for convergence but adds training complexity
- Failure signatures:
  - Low BLEU scores: May indicate poor alignment modeling or insufficient training
  - Slow decoding: Could mean the NAR model is not generating in parallel as expected
  - Mode collapse: Suggests the model is not capturing the diversity of possible translations
- First 3 experiments:
  1. Train CTC-S2UT with only CTC loss (no GLAT, no KD) to establish baseline performance
  2. Add knowledge distillation to compare improvement over baseline
  3. Add glancing training to measure impact on translation quality and decoding speed

## Open Questions the Paper Calls Out

- Open Question 1: How does the performance of CTC-S2UT scale with the number of discrete units (K) in the K-means clustering?
- Open Question 2: Can CTC-S2UT be effectively extended to other language pairs beyond the three tested (Fr→En, En→Fr, En→Es)?
- Open Question 3: How does the glancing ratio affect the trade-off between translation quality and decoding speed in CTC-S2UT?

## Limitations

- The evaluation relies on ASR-BLEU rather than direct reference comparisons, which may underestimate translation quality due to ASR errors.
- Experiments are conducted on a single multilingual dataset (CVSS-C), limiting generalizability to other domains or languages.
- While the paper reports impressive speedup metrics (up to 34.28x), it does not evaluate the quality-speed tradeoff curve or resource utilization during decoding.

## Confidence

- **High Confidence**: CTC-based NAR models can achieve comparable translation quality to AR models while providing significant decoding speedup.
- **Medium Confidence**: The effectiveness of knowledge distillation and glancing training techniques.
- **Low Confidence**: The claim that CTC-S2UT is particularly effective for long speech segments.

## Next Checks

1. Implement CTC-S2UT from the provided architectural specifications and train on CVSS-C data, comparing both BLEU scores and decoding speed against the reported values to verify the claimed 26.81x average speedup.

2. Conduct controlled experiments varying input speech duration (short, medium, long segments) to quantify how translation quality and decoding speedup scale with sequence length, validating the claim of 34.28x speedup for long segments.

3. Profile GPU/CPU usage and memory consumption during both training and inference phases of CTC-S2UT compared to AR baselines to provide a complete picture of the speed-quality-resource tradeoff.