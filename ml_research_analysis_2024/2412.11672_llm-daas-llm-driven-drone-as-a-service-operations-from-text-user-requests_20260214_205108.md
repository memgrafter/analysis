---
ver: rpa2
title: 'LLM-DaaS: LLM-driven Drone-as-a-Service Operations from Text User Requests'
arxiv_id: '2412.11672'
source_url: https://arxiv.org/abs/2412.11672
tags:
- daas
- drone
- delivery
- node
- structured
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents LLM-DaaS, a framework using large language models
  to transform unstructured text user requests into structured Drone-as-a-Service
  (DaaS) operation tasks. The core method involves fine-tuning multiple LLM models
  (Phi-3.5, LLaMA-3.2 7b, Gemma 2b, and Qwen-2.5) on a dataset of text user requests
  mapped to structured DaaS requests, enabling extraction of delivery time, source
  and destination locations, and package weight.
---

# LLM-DaaS: LLM-driven Drone-as-a-Service Operations from Text User Requests

## Quick Facts
- **arXiv ID**: 2412.11672
- **Source URL**: https://arxiv.org/abs/2412.11672
- **Reference count**: 20
- **Primary result**: Fine-tuned LLMs achieve G-Eval scores up to 0.9977 for transforming unstructured text requests into structured DaaS operations

## Executive Summary
This paper presents LLM-DaaS, a framework that leverages large language models to convert unstructured text user requests into structured Drone-as-a-Service (DaaS) operation tasks. The system extracts critical parameters including delivery time, source and destination locations, and package weight from natural language queries. By fine-tuning multiple LLM architectures (Phi-3.5, LLaMA-3.2 7b, Gemma 2b, and Qwen-2.5) on a specialized dataset, the framework achieves high accuracy in task interpretation and operational planning. The integration of real-time weather data enables route optimization, demonstrating the system's capability to handle dynamic operational conditions.

## Method Summary
The LLM-DaaS framework employs a fine-tuning approach where multiple LLM models are trained on a dataset mapping unstructured text user requests to structured DaaS operation parameters. The fine-tuning process focuses on extracting delivery time, source and destination locations, and package weight from natural language inputs. The system incorporates real-time weather data to optimize routing decisions. Four different LLM architectures were evaluated: Phi-3.5, LLaMA-3.2 7b, Gemma 2b, and Qwen-2.5. The framework's performance was measured using G-Eval scores, with the best-performing model (LLaMA 3.2) achieving a score of 0.9977 after fine-tuning.

## Key Results
- Fine-tuned models achieved G-Eval scores approaching 95%, with LLaMA 3.2 reaching 0.9977 and Qwen-2.5 achieving 0.9887
- The framework successfully extracts delivery parameters (time, locations, weight) from unstructured text with high accuracy
- Weather-aware routing capabilities demonstrated robustness in optimizing drone operations under uncertain conditions

## Why This Works (Mechanism)
The framework works by leveraging the natural language understanding capabilities of large language models, which have been trained on vast amounts of text data. When fine-tuned on domain-specific DaaS requests, these models learn to recognize patterns and extract structured information from unstructured text. The multi-task fine-tuning approach enables the models to handle various aspects of DaaS operations simultaneously, including spatial reasoning for location extraction and temporal understanding for delivery scheduling. The incorporation of weather data provides contextual awareness that enhances routing decisions, making the system more robust to real-world operational challenges.

## Foundational Learning

1. **Text-to-Structure Mapping** - Understanding how unstructured text requests map to structured operational parameters (why needed: core functionality; quick check: validate extraction accuracy on diverse request formats)

2. **Multi-Model Architecture Comparison** - Evaluating different LLM architectures for task-specific performance (why needed: optimize model selection; quick check: controlled experiments with identical training conditions)

3. **Weather-Aware Routing** - Integrating environmental data into operational decision-making (why needed: enhance reliability under varying conditions; quick check: measure performance impact with and without weather integration)

4. **Fine-Tuning Methodology** - Adapting pre-trained models to domain-specific tasks (why needed: achieve high accuracy on specialized tasks; quick check: evaluate generalization to out-of-domain requests)

5. **G-Eval Scoring** - Using automated evaluation metrics for task accuracy (why needed: objective performance measurement; quick check: correlation with human evaluation)

6. **Real-time Data Integration** - Incorporating dynamic information into decision-making (why needed: adapt to changing operational conditions; quick check: response time and accuracy under varying data update frequencies)

## Architecture Onboarding

**Component Map**: User Request -> Text Parser -> Parameter Extractor -> Weather Integrator -> Route Optimizer -> Delivery Planner

**Critical Path**: User Request → Parameter Extractor → Route Optimizer → Delivery Execution

**Design Tradeoffs**: The framework balances model size (computational efficiency) against accuracy, with smaller models like Qwen-2.5 showing competitive performance after fine-tuning. Weather integration adds complexity but improves operational reliability.

**Failure Signatures**: Model misinterpretation of ambiguous requests, weather data latency affecting routing decisions, parameter extraction failures for complex multi-item deliveries.

**3 First Experiments**:
1. Test model performance on out-of-domain user requests with linguistic variation
2. Compare delivery success rates with and without weather integration in simulated environments
3. Conduct controlled ablation studies comparing all four models with identical training protocols

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations

- **Generalization Uncertainty**: The 95% G-Eval scores may not translate to real-world performance with diverse linguistic patterns and ambiguous requests
- **Weather Integration Validation**: Weather-aware routing capabilities mentioned but lack quantitative validation in operational scenarios
- **Model Comparison Standardization**: Performance differences between models may be influenced by training methodology rather than architecture alone

## Confidence

**High Confidence Claims**:
- Technical framework architecture and fine-tuning methodology are well-documented
- Reported G-Eval scores appear reproducible based on described methodology

**Medium Confidence Claims**:
- Operational efficiency improvements lack sufficient real-world validation
- Practical impact of weather-aware routing not rigorously evaluated

**Low Confidence Claims**:
- None identified in major claims cluster

## Next Checks

1. **Out-of-Domain Request Testing**: Evaluate fine-tuned models on test sets containing user requests with significantly different linguistic patterns, ambiguous phrasing, and requests from different cultural contexts to assess true generalization capabilities.

2. **End-to-End Operational Simulation**: Conduct comprehensive simulations integrating weather data, drone flight dynamics, and delivery constraints to measure actual delivery success rates, time efficiency, and resource utilization compared to baseline non-LLM approaches.

3. **Cross-Model Ablation Study**: Implement controlled experiments where all four models are trained with identical hyperparameters, training data splits, and evaluation protocols to isolate the impact of model architecture versus training methodology on performance differences.