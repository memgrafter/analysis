---
ver: rpa2
title: Theoretical Proof that Auto-regressive Language Models Collapse when Real-world
  Data is a Finite Set
arxiv_id: '2412.14872'
source_url: https://arxiv.org/abs/2412.14872
tags:
- data
- corpus
- generation
- collapse
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first theoretical proof that auto-regressive
  language models (LMs) collapse when trained on recursively generated data in data-scarce
  domains where real-world data is a finite set. The authors prove that regardless
  of how small the proportion of generated data is in the training corpus, LM collapse
  is inevitable after sufficient generations if no new real-world data is added.
---

# Theoretical Proof that Auto-regressive Language Models Collapse when Real-world Data is a Finite Set

## Quick Facts
- **arXiv ID**: 2412.14872
- **Source URL**: https://arxiv.org/abs/2412.14872
- **Authors**: Lecheng Wang; Xianjie Shi; Ge Li; Jia Li; Xuanming Zhang; Yihong Dong; Wenpin Jiao; Hong Mei
- **Reference count**: 40
- **Primary result**: First theoretical proof that auto-regressive LMs collapse when trained on recursively generated data with finite real-world data

## Executive Summary
This paper provides the first theoretical proof that auto-regressive language models inevitably collapse when trained on recursively generated data in data-scarce domains where real-world data is finite. The authors prove that regardless of how small the proportion of generated data is in the training corpus, LM collapse is inevitable after sufficient generations if no new real-world data is added. This occurs because errors accumulate across generations, causing the LM's output distribution to diverge from the original data distribution. The theoretical proof applies to both Replace and Accumulate-Subsample paradigms. Experimental results with 33M-parameter LMs trained for 40 generations on recursively generated children's stories demonstrate this collapse, with model perplexity increasing as generations progress.

## Method Summary
The authors prove that language model collapse is inevitable when training on recursively generated data from a finite real-world dataset. They consider two training paradigms: Replace (training on previous generation's output only) and Accumulate-Subsample (gradually incorporating generated data). The theoretical framework shows that errors accumulate across generations, causing the model's output distribution to diverge from the original data distribution. The proof establishes that this divergence occurs regardless of the proportion of generated data in the training corpus. Experimental validation uses 33M-parameter LMs trained for 40 generations on recursively generated children's stories, demonstrating increasing perplexity as generations progress.

## Key Results
- Theoretical proof that LM collapse is inevitable with recursive training on finite real-world data
- Error accumulation causes output distribution to diverge from original data distribution
- Experimental validation shows increasing perplexity across 40 generations with 33M-parameter LMs
- Collapse occurs regardless of generated data proportion in training corpus

## Why This Works (Mechanism)
The collapse occurs because auto-regressive LMs trained on recursively generated data inherit and amplify errors from previous generations. Each generation introduces small deviations from the true data distribution, and these deviations compound over time. Without new real-world data to correct these errors, the model's learned distribution gradually drifts away from the original distribution, leading to collapse. The theoretical proof demonstrates that this process is mathematically inevitable given finite real-world data and recursive training.

## Foundational Learning
- **Recursive training dynamics**: Understanding how models trained on their own outputs behave over multiple generations - needed to grasp why errors accumulate
- **Distribution divergence**: Concept of how model output distributions drift from original data distributions - needed to understand collapse mechanism
- **Finite data constraints**: Recognition that real-world data is limited in many domains - needed to contextualize the problem
- **Error propagation theory**: Mathematical framework for how errors compound across generations - needed to follow the theoretical proof
- **Auto-regressive model behavior**: Understanding how sequential prediction models generate text - needed to grasp the collapse process
- **Training paradigm differences**: Replace vs Accumulate-Subsample approaches - needed to understand the scope of the proof

## Architecture Onboarding
- **Component map**: Real-world data → Initial LM training → Generation 1 → Training on generation 1 → Generation 2 → ... → Collapse point
- **Critical path**: The accumulation of errors across generations that leads to distribution divergence
- **Design tradeoffs**: Quality of synthetic data vs quantity, frequency of real-world data incorporation, model size considerations
- **Failure signatures**: Increasing perplexity, decreasing diversity, generation quality degradation over time
- **First experiments**: 1) Test collapse with different model sizes (33M vs 1B+ parameters), 2) Vary the proportion of real-world data in training corpus, 3) Test with different data types beyond children's stories

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical proof assumes specific error models that may not capture all real-world LM training dynamics
- Experimental validation uses relatively small models (33M parameters) and specific dataset type
- Definition of "collapse" could be more precisely specified
- Does not address potential mitigation strategies beyond quality control

## Confidence
- **High confidence**: The core theoretical claim that recursive training with finite real-world data leads to inevitable model collapse
- **Medium confidence**: Experimental results showing collapse patterns in tested conditions
- **Low confidence**: The claim about synthetic data quality being the primary factor for preventing collapse

## Next Checks
1. Replicate the theoretical proof with different error models and data generation assumptions to test robustness
2. Conduct experiments with larger-scale models (1B+ parameters) and diverse data types to validate generalizability
3. Test the quality versus quantity hypothesis by systematically varying both synthetic data quality metrics and inclusion ratios in training corpora