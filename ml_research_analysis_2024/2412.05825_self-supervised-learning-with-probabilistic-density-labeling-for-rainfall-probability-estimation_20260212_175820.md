---
ver: rpa2
title: Self-Supervised Learning with Probabilistic Density Labeling for Rainfall Probability
  Estimation
arxiv_id: '2412.05825'
source_url: https://arxiv.org/abs/2412.05825
tags:
- precipitation
- data
- labeling
- learning
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a self-supervised learning approach for post-processing
  NWP forecasts to estimate rainfall probability. The method uses masked modeling
  with deformable convolution layers to learn variable dependencies and reconstruct
  atmospheric physics variables.
---

# Self-Supervised Learning with Probabilistic Density Labeling for Rainfall Probability Estimation

## Quick Facts
- arXiv ID: 2412.05825
- Source URL: https://arxiv.org/abs/2412.05825
- Authors: Junha Lee; Sojung An; Sujeong You; Namik Cho
- Reference count: 40
- Primary result: SSLPDL improves mIoU by over 9.9% compared to six benchmark models

## Executive Summary
This paper proposes a self-supervised learning approach for post-processing NWP forecasts to estimate rainfall probability. The method uses masked modeling with deformable convolution layers to learn variable dependencies and reconstruct atmospheric physics variables. A key innovation is probabilistic density labeling to address class imbalance in extreme weather events. Experimental results on regional precipitation data show that SSLPDL improves mIoU by over 9.9% compared to six benchmark models and demonstrates competitive performance in extending forecast lead times.

## Method Summary
The method employs a two-stage process: first, self-supervised pre-training with masked modeling using deformable convolution layers to learn dependencies between atmospheric variables; second, transfer learning to precipitation segmentation with probabilistic density labeling. The approach addresses class imbalance in rainfall prediction by dynamically adjusting probability values across classes and uses deformable convolutions to handle spatially varying distribution shifts in NWP forecast errors. The system is trained on the RDAPS dataset with 16 variables and evaluated using mIoU, CSI, F1 score, Precision, and Recall metrics.

## Key Results
- SSLPDL improves mIoU by over 9.9% compared to six benchmark models
- Demonstrates competitive performance in extending forecast lead times
- Effectively addresses class imbalance in extreme weather events through probabilistic density labeling

## Why This Works (Mechanism)

### Mechanism 1
Deformable convolutions enable the model to learn spatiotemporal bias by adaptively aggregating features from neighboring pixels rather than fixed ones. Standard convolutions use fixed kernel weights across the entire image, which cannot adapt to the varying distribution shifts caused by accumulated errors in NWP forecasts. Deformable convolutions introduce learnable offsets for each sampling location, allowing the kernel to dynamically adjust to local spatial structures and better capture the non-stationary patterns in precipitation data.

### Mechanism 2
Probabilistic density labeling addresses class imbalance by dynamically adjusting probability values across different classes based on distributional assignment principles. Traditional one-hot labeling assigns probability 1 to a single class and 0 to all others, which can lead to model instability and overfitting on minority classes. The proposed probabilistic density labeling method applies label smoothing that is proximity-based to rainfall intensity thresholds, creating a more continuous probability distribution that better represents the inherent variability in rainfall intensity.

### Mechanism 3
Masked modeling with variable reconstruction enables the model to learn dependencies between atmospheric physics variables before being applied to the precipitation segmentation task. By masking random patches of the input and requiring the model to reconstruct the original variables, the model learns to understand the relationships and dependencies between different atmospheric variables. This pre-training process creates a more robust feature extractor that can then be transferred to the precipitation segmentation task.

## Foundational Learning

- **Self-supervised learning through masked modeling**: Why needed here - The paper uses masked modeling to learn variable dependencies from unlabeled atmospheric data before applying transfer learning to precipitation prediction. Quick check - How does masking random patches and requiring reconstruction help the model learn about variable dependencies?

- **Deformable convolutions**: Why needed here - Standard convolutions use fixed kernel weights that cannot adapt to spatially varying distribution shifts in NWP forecast errors. Quick check - What is the key difference between deformable convolutions and standard convolutions that allows for adaptive spatial aggregation?

- **Label smoothing and probabilistic labeling**: Why needed here - Traditional one-hot labeling creates hard boundaries that can lead to overfitting on minority classes in imbalanced precipitation datasets. Quick check - How does probabilistic density labeling differ from traditional label smoothing in addressing class imbalance?

## Architecture Onboarding

- **Component map**: Pre-training stage (masking → encoder with deformable convolutions → decoder) → Transfer learning stage (pre-trained encoder → UperNet decoder with probabilistic density labeling)
- **Critical path**: Masked modeling reconstruction → Pre-trained encoder feature extraction → Precipitation segmentation with probabilistic labeling
- **Design tradeoffs**: Deformable convolutions add computational overhead but provide better spatial adaptability; probabilistic labeling adds complexity but addresses class imbalance; masked modeling requires additional pre-training but improves downstream performance
- **Failure signatures**: Poor reconstruction in pre-training indicates issues with variable dependencies learning; low segmentation performance despite good reconstruction suggests transfer learning problems; class imbalance issues persist despite probabilistic labeling
- **First 3 experiments**:
  1. Verify deformable convolution implementation by comparing fixed vs deformable kernels on synthetic spatially varying data
  2. Test masking ratios during pre-training to find optimal balance between reconstruction difficulty and learning effectiveness
  3. Compare probabilistic density labeling with traditional label smoothing and one-hot labeling on a balanced subset of the data to isolate the labeling effect

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of SSLPDL vary with different masking ratios during pre-training and fine-tuning? The paper conducts ablation studies on masking ratios, showing that larger masking ratios in pre-training lead to better results, while smaller ratios in transfer learning yield better outcomes. However, a comprehensive analysis of how masking ratios affect performance across various datasets and scenarios is lacking.

### Open Question 2
How does the proposed probabilistic density labeling approach compare to other class imbalance techniques, such as data augmentation or loss reweighting? The paper introduces probabilistic density labeling as a method to address class imbalance, but does not compare it to other techniques. The effectiveness of probabilistic density labeling compared to other class imbalance methods remains unexplored.

### Open Question 3
How does the performance of SSLPDL scale with increasing forecast lead times beyond 30 hours? The paper demonstrates competitive performance in extending forecast lead times, but the evaluation is limited to 30 hours. The paper does not explore the performance of SSLPDL at longer forecast lead times, which is crucial for real-world applications.

## Limitations
- Limited to single regional dataset (RDAPS over Korea) without cross-regional validation
- Lack of ablation studies to isolate individual contributions of proposed components
- Specific baseline models and configurations not detailed, making performance claims difficult to assess

## Confidence
- **High Confidence**: The general framework of using self-supervised learning for pre-training followed by transfer learning is well-established in computer vision and has strong theoretical grounding for weather prediction tasks.
- **Medium Confidence**: The specific implementation details and performance claims are reasonably supported by the experimental results presented, though limited by the single dataset evaluation.
- **Low Confidence**: The relative contributions of each proposed component (deformable convolutions, probabilistic density labeling, masked modeling) to the overall performance improvement cannot be confidently assessed without ablation studies.

## Next Checks
1. **Ablation Study**: Conduct controlled experiments removing each proposed component (deformable convolutions, probabilistic density labeling, masked modeling) to quantify their individual contributions to the 9.9% mIoU improvement.
2. **Cross-Regional Validation**: Test the trained model on precipitation forecasting data from different geographical regions to assess generalization beyond the Korean RDAPS dataset and verify that the deformable convolution approach handles different spatial bias patterns.
3. **Parameter Sensitivity Analysis**: Systematically vary the masking ratio (particularly testing lower values than 90%), label smoothing parameters (α and β), and rainfall intensity thresholds to determine the robustness of the approach to hyperparameter choices.