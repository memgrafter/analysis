---
ver: rpa2
title: Interpretable Pre-Trained Transformers for Heart Time-Series Data
arxiv_id: '2407.20775'
source_url: https://arxiv.org/abs/2407.20775
tags:
- attention
- token
- context
- transformer
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PPG-PT and ECG-PT, two decoder-only transformer
  models pretrained on photoplethysmography and electrocardiography time series to
  predict the next token. Unlike prior work, these models use cross-entropy loss on
  discrete token values rather than continuous regression, yielding highly interpretable
  attention patterns.
---

# Interpretable Pre-Trained Transformers for Heart Time-Series Data

## Quick Facts
- arXiv ID: 2407.20775
- Source URL: https://arxiv.org/abs/2407.20775
- Authors: Harry J. Davies; James Monsen; Danilo P. Mandic
- Reference count: 30
- Primary result: Transformer models pretrained on PPG/ECG achieve 0.93-0.99 AUC for AF classification with fully interpretable attention patterns

## Executive Summary
This paper introduces PPG-PT and ECG-PT, decoder-only transformer models pretrained on photoplethysmography and electrocardiography time series for next-token prediction. Unlike prior work using continuous regression, these models use cross-entropy loss on discrete token values, yielding highly interpretable attention patterns. When fine-tuned for atrial fibrillation classification, the models achieve AUCs of 0.93 (PPG) and 0.99 (ECG) in leave-one-subject-out evaluation, with attention maps revealing clinically interpretable decision regions. The work demonstrates that transformer models can be both highly accurate and fully interpretable for cardiac time series, enabling safe deployment in clinical settings.

## Method Summary
The authors developed decoder-only transformer models pretrained on PPG and ECG signals using token-level next prediction with cross-entropy loss. The models tokenize continuous signals into discrete integer vocabularies (101 tokens for both PPG at 50Hz and ECG at 100Hz), embed them with positional information, and use masked self-attention to predict the next token. After pretraining, the models are fine-tuned for downstream tasks: atrial fibrillation classification using binary cross-entropy and beat detection using sigmoid outputs for each token position. The approach leverages the periodic structure of cardiac signals, where attention naturally clusters on physiologically relevant cycle positions.

## Key Results
- AF classification AUC: 0.93 (PPG-PT) and 0.99 (ECG-PT) in leave-one-subject-out evaluation
- Beat detection F1 score: 98% for PPG-PT with confidence scores correlating with signal quality
- Attention analysis shows models focus on physiologically relevant features (peaks, troughs, dicrotic notch, P/Q/R waves)
- Attention maps reveal interpretable decision regions for AF detection, showing increased focus on irregular beat timing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model focuses attention on physiologically relevant cycle positions because tokenisation preserves the discrete, periodic structure of PPG and ECG signals.
- Mechanism: By mapping continuous waveforms to a small, fixed vocabulary of integer tokens, each token becomes associated with a specific phase in the cardiac cycle. Attention can then naturally cluster tokens of the same value that occur at the same relative position (e.g., peaks, troughs, dicrotic notch) across different cycles.
- Core assumption: The periodic nature of cardiac signals allows meaningful clustering of token values by cycle phase after embedding and attention.
- Evidence anchors:
  - [abstract] "tokens with the same value, which occur at different distinct points in the electrocardiography (ECG) and photoplethysmography (PPG) cycle, form separate clusters in high dimensional space"
  - [section] "the PPG-PT and ECG-PT models look at similar points in previous cycles of PPG or ECG when predicting the next token"
- Break condition: If the signal contains strong non-periodic components or noise that corrupts the discrete token mapping, attention clustering by cycle phase would degrade.

### Mechanism 2
- Claim: Fine-tuning for atrial fibrillation detection is interpretable because the base model's attention is already physiologically aligned, so shifts in attention during fine-tuning directly reveal diagnostic features.
- Mechanism: The pre-trained model has attention heads tuned to peaks, troughs, dicrotic notch (PPG) and P/Q/R waves (ECG). When fine-tuned for AF classification, attention maps show increased focus on irregular timing of beats, making the model's reasoning visible to clinicians.
- Core assumption: The base model's physiologically relevant attention patterns persist and are repurposed during fine-tuning without being overwritten.
- Evidence anchors:
  - [abstract] "fine-tuned models for AF screening are also fully explainable, with attention shifting to regions in the context that are strongly indicative of atrial fibrillation"
  - [section] "attention shifts to regions in the input context that most indicate the presence of the arrhythmia"
- Break condition: If fine-tuning uses very large learning rates or many epochs, the base model's attention structure could be overwritten, reducing interpretability.

### Mechanism 3
- Claim: The model provides signal-quality-aware beat detection because its continuous output values encode confidence tied to local waveform quality.
- Mechanism: Instead of binary peak labels, the model outputs sigmoid values between 0 and 1 for every token. High confidence aligns with clean peaks; low confidence flags noisy or ambiguous regions. This dual output (location + confidence) is unique compared to rule-based detectors.
- Core assumption: The transformer's learned embeddings capture subtle waveform quality cues that correlate with prediction confidence.
- Evidence anchors:
  - [abstract] "uniquely providing a beat confidence level which acts as a signal quality estimator"
  - [section] "confidence estimates also aligned with signal quality, with lower confidence in areas of poor signal quality"
- Break condition: If the training data lacks sufficient examples of low-quality signals, confidence estimates may not generalize to real-world noisy conditions.

## Foundational Learning

- Concept: Transformer decoder-only architecture with masked self-attention
  - Why needed here: Allows the model to predict the next token in a time series by conditioning only on past context, mirroring the autoregressive training objective.
  - Quick check question: What prevents a token from attending to future tokens in this architecture?

- Concept: Tokenisation of periodic signals into discrete vocabularies
  - Why needed here: Enables the use of cross-entropy loss instead of regression, aligning the training objective with natural language transformer models and improving interpretability.
  - Quick check question: How does discretising a continuous PPG or ECG signal into integer tokens preserve physiologically relevant features?

- Concept: Attention heatmaps and cluster analysis in embedding space
  - Why needed here: Provides a concrete method to verify that the model's attention aligns with known cardiac features (peaks, troughs, dicrotic notch, etc.) and to explain predictions.
  - Quick check question: How would you verify that a specific attention head consistently focuses on R-peaks across different ECG segments?

## Architecture Onboarding

- Component map: Input signal -> Resample -> Scale to token range -> Tokenise -> Embed + add position -> Multi-head masked self-attention blocks (8 layers, 8 heads each) -> Feed-forward networks -> Final linear layer (vocab size) -> Softmax (for training) or sigmoid (for fine-tuning classification)
- Critical path: Input PPG/ECG signal -> Resample -> Scale to token range -> Tokenise -> Embed + add position -> Pass through transformer blocks -> Output logits -> (Train: cross-entropy, Fine-tune: sigmoid + binary cross-entropy)
- Design tradeoffs: Smaller embedding dimension (64) and limited context (500 tokens) trade off model capacity for faster training and interpretability; cross-entropy loss trades continuous prediction accuracy for discrete interpretability.
- Failure signatures: Attention maps show no clear clustering by cycle phase; fine-tuning collapses base model attention patterns; confidence scores do not correlate with known signal quality issues.
- First 3 experiments:
  1. Generate synthetic periodic PPG/ECG signals, tokenise, and verify that attention heads cluster on expected cycle positions.
  2. Fine-tune on a small AF dataset and visualize attention shifts between base and fine-tuned models to confirm interpretability.
  3. Run beat detection on a dataset with labeled signal quality, and plot confidence scores against ground-truth quality labels to validate the confidence estimator.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but highlights limitations around transferability to wearable PPG data and the need for further training when applying to different sensor types.

## Limitations

- The cross-entropy training objective assumes discretizing continuous cardiac signals into 101 integer tokens preserves physiologically meaningful structure, which lacks formal theoretical grounding.
- Reported signal-quality correlation for beat detection is based on qualitative alignment rather than quantitative correlation metrics, making the strength of this claim difficult to assess.
- The transformer architecture uses relatively modest dimensions (64 embedding, 500-token context) which may limit performance on longer or more complex cardiac recordings.

## Confidence

- **High Confidence**: The mechanistic claim that token discretization enables attention clustering by cycle phase is strongly supported by the abstract statement that "tokens with the same value...form separate clusters in high dimensional space" and the observation that models "look at similar points in previous cycles." The 98% F1 score for beat detection with confidence estimation is also well-supported by direct evidence.

- **Medium Confidence**: The claim about interpretability during AF fine-tuning is reasonably supported by the statement that "attention shifts to regions...strongly indicative of atrial fibrillation," but lacks quantitative metrics comparing attention maps between healthy and AF cases. The physiological alignment of attention patterns is supported by evidence but could benefit from more rigorous statistical validation.

- **Low Confidence**: The signal-quality estimation through confidence scores lacks quantitative validation. The claim that confidence "aligns with signal quality" is qualitative, and no correlation coefficients or error metrics are provided to substantiate this relationship.

## Next Checks

1. **Quantify Attention-Physiology Alignment**: For a subset of ECG/PPG segments, manually annotate key physiological landmarks (R-peaks, dicrotic notch, etc.) and compute the overlap between these positions and the attention clusters identified by the model. Report precision, recall, and F1 score for landmark detection via attention.

2. **Validate Signal-Quality Correlation**: On a dataset with labeled signal quality (e.g., SNR or artifact ratings), compute the correlation between the model's confidence scores and ground-truth quality metrics. Report Pearson correlation coefficients and assess whether confidence scores can reliably stratify signal quality into clinically meaningful categories.

3. **Stress-Test Interpretability Under Domain Shift**: Fine-tune the pretrained models on a small, clinically distinct dataset (e.g., a different population or recording device) and analyze whether attention patterns remain interpretable or degrade. Compare attention maps before and after fine-tuning to assess whether interpretability is preserved during adaptation.