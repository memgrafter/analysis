---
ver: rpa2
title: Multimodal Misinformation Detection using Large Vision-Language Models
arxiv_id: '2407.14321'
source_url: https://arxiv.org/abs/2407.14321
tags:
- evidence
- retrieval
- text
- multimodal
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LVLM4FV, a pipeline for multimodal misinformation
  detection that incorporates evidence retrieval into the process. The approach uses
  large language models (LLMs) and large vision-language models (LVLMs) for both evidence
  retrieval (LVLM4EV) and fact verification (LVLM4FV) in a zero-shot setting.
---

# Multimodal Misinformation Detection using Large Vision-Language Models

## Quick Facts
- arXiv ID: 2407.14321
- Source URL: https://arxiv.org/abs/2407.14321
- Authors: Sahar Tahmasebi; Eric Müller-Budack; Ralph Ewerth
- Reference count: 40
- Primary result: Zero-shot multimodal misinformation detection pipeline achieving mAP of 70.48% (text) and 75.83% (image) on MOCHEG, with improved generalization over supervised baselines

## Executive Summary
This paper presents LVLM4FV, a zero-shot pipeline for multimodal misinformation detection that combines evidence retrieval with fact verification using large language and vision-language models. The approach addresses the challenge of multimodal misinformation by leveraging generative models for re-ranking initial retrieval results and employing LLaVA for multimodal fact verification with majority voting. Experiments on MOCHEG and Factify datasets demonstrate that multimodal evidence verification outperforms text-only approaches, while the zero-shot nature enables better generalization compared to supervised methods.

## Method Summary
The pipeline uses a two-stage approach: first, evidence retrieval using initial retrievers (SBERT for text, CLIP for images) followed by LLM/LVLM re-ranking with GAIS-YN strategy; second, fact verification using LLaVA with prompt-based classification and majority voting. The system operates in zero-shot mode without fine-tuning, relying on carefully crafted prompts to elicit relevance judgments and classification labels. For multimodal verification, two-level prompting combines text and image evidence pairs, while text-only verification uses one-level prompting.

## Key Results
- LVLM4EV achieves mAP of 70.48% for text retrieval and 75.83% for image retrieval on MOCHEG
- LVLM4FV achieves micro F1-scores of 0.549 for retrieved multimodal evidence and 0.534 for gold multimodal evidence
- Multimodal fact verification outperforms text-only verification across all metrics
- Zero-shot approach shows improved generalization compared to supervised baselines on Factify dataset

## Why This Works (Mechanism)

### Mechanism 1
Generative models can accurately assess semantic relevance of text-image pairs to claims in zero-shot manner, enabling effective re-ranking of initial retrieval results. The GAIS-YN strategy extracts normalized probabilities for "Yes"/"No" tokens to create ranking scores, pushing more relevant items to the top.

### Mechanism 2
Multimodal pairs provide more discriminative information for fact verification than text alone, as LLaVA captures richer semantic relations between images and text through prompt-based classification with majority voting.

### Mechanism 3
Addressing incomplete ground truth through manual annotation improves system evaluation accuracy by providing more complete and accurate annotations than relying solely on expert-verified sources, though this may introduce subjective biases.

## Foundational Learning

- Concept: Prompt engineering for generative models
  - Why needed here: The entire re-ranking and verification pipeline depends on carefully crafted prompts to elicit accurate "Yes"/"No" or label outputs
  - Quick check question: What are the key differences between one-level and two-level prompting in this context?

- Concept: Zero-shot learning with large models
  - Why needed here: The approach avoids fine-tuning and relies on the model's pre-existing knowledge and instruction-tuning to perform tasks on unseen data
  - Quick check question: How does the GAIS-YN normalization strategy differ from GAIS-ALL, and why is it preferred here?

- Concept: Multimodal representation and alignment
  - Why needed here: The fact verification step requires the model to understand semantic relationships between text and images, which is a core capability of LVLMs like LLaVA
  - Quick check question: Why might LLaVA struggle with chart images despite strong performance on other tasks?

## Architecture Onboarding

- Component map: Claim → Initial Retriever (SBERT/CLIP) → Re-ranker (Mistral-7B/InstructBLIP) → Fact Verifier (LLaVA/Mistral-7B) → Supported/Refuted/NEI label
- Critical path: Claim → Initial retrieval → Re-ranking → Fact verification → Label
- Design tradeoffs: Zero-shot vs. fine-tuned (faster deployment, better generalization, but potentially lower accuracy); Re-ranking vs. re-retrieval (cheaper but limited by initial candidate quality)
- Failure signatures: Low precision in re-ranking (initial retriever too noisy, model misclassifies relevance); Poor multimodal verification (LLaVA struggles with certain image types); Evaluation bias (incomplete ground truth masks true performance)
- First 3 experiments: 1) Compare re-ranking strategies (IRS, GAIS-ALL, GAIS-YN, GAIS-YNO) on a held-out set; 2) Evaluate fact verification with text-only vs. multimodal evidence on gold labels; 3) Measure generalization by testing on Factify after training on MOCHEG

## Open Questions the Paper Calls Out

### Open Question 1
How can the interpretability of multimodal misinformation detection models be improved, particularly in understanding the specific contributions of different evidence components? The paper identifies interpretability as a key future direction but does not provide specific methods for achieving this.

### Open Question 2
How can the maximum length constraint of language models be addressed to improve multimodal misinformation detection, especially for lengthy evidence passages? The paper acknowledges this limitation with chart images but does not propose specific solutions.

### Open Question 3
How can large-scale, high-quality annotated datasets for multimodal evidence retrieval in misinformation detection be created to advance research in this field? The paper highlights the importance of such datasets but does not provide a concrete plan for their creation.

## Limitations

- Reliance on generative model outputs for re-ranking introduces uncertainty when models struggle with nuanced relevance judgments
- Performance on chart images remains challenging due to difficulties with text recognition and visual reasoning in complex graphics
- Manual annotation process for incomplete ground truth may introduce subjective biases affecting performance measurements

## Confidence

Medium-High confidence in core claims based on experimental results across two datasets. The zero-shot nature and generalization claims require further validation across diverse misinformation types.

## Next Checks

1. **Domain Transferability Test**: Evaluate the zero-shot pipeline on a third, unseen misinformation dataset to assess generalization beyond MOCHEG and Factify

2. **Ablation Study on Image Types**: Systematically test performance breakdown across different image categories (photographs, charts, memes, screenshots) to identify specific failure modes

3. **Annotation Reliability Assessment**: Conduct inter-annotator agreement studies on the union-annotated subset to quantify subjectivity and potential biases in the expanded ground truth