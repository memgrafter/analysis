---
ver: rpa2
title: Robustness to distribution shifts of compressed networks for edge devices
arxiv_id: '2401.12014'
source_url: https://arxiv.org/abs/2401.12014
tags:
- networks
- adversarial
- compressed
- domain
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study investigates the robustness of compressed neural networks\
  \ to distribution shifts, including domain shifts and adversarial perturbations.\
  \ Three compression techniques\u2014filter pruning, knowledge distillation, and\
  \ post-training quantization\u2014are evaluated on ResNets using the Office-31 dataset."
---

# Robustness to distribution shifts of compressed networks for edge devices

## Quick Facts
- arXiv ID: 2401.12014
- Source URL: https://arxiv.org/abs/2401.12014
- Reference count: 40
- One-line primary result: Post-training quantization outperforms pruning and distillation for robustness to distribution shifts in compressed models.

## Executive Summary
This study evaluates the robustness of compressed neural networks to distribution shifts, including domain shifts and adversarial perturbations. Three compression techniques—filter pruning, knowledge distillation, and post-training quantization—are tested on ResNet architectures using the Office-31 dataset. Results demonstrate that post-training quantization maintains superior accuracy under both domain shifts and adversarial attacks, even at high compression ratios, making it the recommended approach for robust compressed models on edge devices.

## Method Summary
The study fine-tunes ImageNet-pretrained ResNet models (18, 34, 50, 101, 152 layers) on Office-31 source domains, then applies three compression techniques: L1-norm filter pruning, knowledge distillation, and post-training 8-bit quantization. Compressed models are evaluated on target domains for domain shift robustness and against multiple adversarial attacks. The experimental pipeline includes training baselines, applying compression, and systematically evaluating performance across both types of distribution shifts.

## Key Results
- Post-training quantization maintains higher accuracy than pruned or distilled models under domain shifts, even at 25% compression ratio
- Larger base models produce less robust compressed networks compared to smaller ones when compressed to similar sizes
- Quantized models show greater robustness to gradient-based adversarial attacks due to gradient masking effects
- Pruned and distilled models degrade more severely in adversarial robustness as compression ratios increase

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-training quantization provides gradient masking that reduces vulnerability to gradient-based adversarial attacks.
- Mechanism: Quantization converts 32-bit floating-point weights and activations to 8-bit integers, altering the gradient landscape in a way that prevents gradient-based attacks from effectively finding adversarial perturbations.
- Core assumption: Gradient masking occurs because the quantization operation is non-differentiable or has discontinuous gradients that gradient-based attacks cannot exploit.
- Evidence anchors:
  - [abstract] "post-training quantization is a reliable method for achieving significant robustness to distribution shifts"
  - [section] "adversarial attacks can be specifically designed to overcome gradient masking"
  - [corpus] Weak corpus evidence - only 8 related papers found, none directly discussing gradient masking in quantized networks

### Mechanism 2
- Claim: Knowledge distillation transfers generalization ability from larger teacher models to smaller student models, improving robustness to domain shifts.
- Mechanism: The teacher model's soft target probabilities contain richer information about class boundaries and uncertainty, which the student model learns to mimic, resulting in better performance on unseen domains.
- Core assumption: The teacher model's generalization ability is preserved and effectively transferred through the distillation process, particularly for handling distribution shifts.
- Evidence anchors:
  - [abstract] "compact networks obtained by knowledge distillation are much more robust to distribution shifts than pruned networks"
  - [section] "the larger teacher model transfers its generalization ability to the student model"
  - [corpus] Weak corpus evidence - limited papers discussing knowledge distillation robustness specifically

### Mechanism 3
- Claim: Smaller base models pruned to specific sizes maintain better generalization to target domains compared to larger models pruned to the same size.
- Mechanism: Smaller models have fewer parameters and simpler representations that are less likely to overfit to source domain characteristics, making them more adaptable to distribution shifts.
- Core assumption: Model complexity and parameter count directly influence the model's ability to generalize across domain shifts, with simpler models being more robust.
- Evidence anchors:
  - [abstract] "larger networks are more vulnerable to losing robustness than smaller ones, even when they are compressed to a similar size"
  - [section] "it is more beneficial to prune a smaller base model rather than a larger one to a particular size since its corresponding compressed model has better generalization performance on unseen domains"
  - [corpus] Weak corpus evidence - limited discussion of model size effects on domain shift robustness

## Foundational Learning

- Concept: Domain adaptation and distribution shift
  - Why needed here: Understanding how models perform when training and testing data come from different distributions is crucial for evaluating robustness
  - Quick check question: What are the key differences between domain adaptation (with labeled target data) and domain generalization (without target labels)?

- Concept: Adversarial attack methodologies
  - Why needed here: Evaluating model robustness requires understanding different attack types (gradient-based vs. gradient-free) and their effectiveness
  - Quick check question: How do gradient-based attacks like FGSM and PGD differ from gradient-free attacks like Salt&Pepper in their approach to finding adversarial examples?

- Concept: Neural network quantization techniques
  - Why needed here: Post-training quantization is shown to be effective for robustness, requiring understanding of how quantization affects model behavior
  - Quick check question: What are the key differences between post-training quantization and quantization-aware training in terms of accuracy and robustness trade-offs?

## Architecture Onboarding

- Component map: ImageNet-pretrained ResNet models -> Fine-tuning on source domain -> Compression (pruning/distillation/quantization) -> Evaluation on target domains and adversarial attacks
- Critical path: Train baseline models → Apply compression → Evaluate on source domain → Evaluate on target domains → Evaluate against adversarial attacks
- Design tradeoffs: Compression rate vs. accuracy vs. robustness; model size vs. deployment constraints; quantization bit-width vs. performance
- Failure signatures: Accuracy drop on target domains indicates domain shift vulnerability; large accuracy drop under adversarial attacks indicates security vulnerability
- First 3 experiments:
  1. Fine-tune ResNet-18 on Amazon domain and evaluate on Webcam/DSLR to establish baseline domain shift performance
  2. Apply L1-norm filter pruning at 50% rate and compare target domain performance to unpruned baseline
  3. Apply post-training quantization to ResNet-18 and evaluate robustness to both domain shifts and adversarial attacks compared to pruned version

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does post-training quantization (PTSQ) affect the adversarial robustness of compressed models compared to their unquantized counterparts, particularly against non-gradient-based attacks like Salt&Pepper?
- Basis in paper: [explicit] The paper mentions that quantized models are more robust to gradient-based attacks due to gradient masking but are vulnerable to non-gradient-based attacks like Salt&Pepper. It also notes that quantization improves the accuracy of smaller models (ResNet-18 and ResNet-50) compared to their baselines under heavy Salt&Pepper attacks.
- Why unresolved: The paper does not provide a comprehensive comparison of the adversarial robustness of quantized models against non-gradient-based attacks, especially for deeper models like ResNet-152.
- What evidence would resolve it: A detailed evaluation of the adversarial robustness of quantized models against a wide range of non-gradient-based attacks, including Salt&Pepper, across different model architectures and depths.

### Open Question 2
- Question: How does the robustness of compressed models to domain shifts vary with the complexity of the target domain, and what factors contribute to this variation?
- Basis in paper: [inferred] The paper evaluates the robustness of compressed models to domain shifts using the Office-31 dataset, which consists of three domains with varying levels of complexity (Amazon, Webcam, and DSLR). However, it does not explicitly investigate how the complexity of the target domain affects the robustness of compressed models.
- Why unresolved: The paper does not provide a detailed analysis of the relationship between domain complexity and the robustness of compressed models to domain shifts.
- What evidence would resolve it: A systematic study evaluating the robustness of compressed models to domain shifts across domains with varying levels of complexity, along with an analysis of the factors contributing to the observed variations.

### Open Question 3
- Question: How does the compression rate of pruned and distilled models affect their robustness to adversarial attacks, and what are the underlying mechanisms behind this relationship?
- Basis in paper: [explicit] The paper observes that the average adversarial robustness of pruned models decreases as the pruning rate increases. It also notes that the adversarial robustness of ResNet-18, trained by distillation from teachers with different sizes, remains roughly the same against both light and heavy attacks.
- Why unresolved: The paper does not provide a detailed analysis of the mechanisms behind the observed relationship between compression rate and adversarial robustness for pruned and distilled models.
- What evidence would resolve it: A comprehensive study investigating the relationship between compression rate and adversarial robustness for pruned and distilled models, along with an analysis of the underlying mechanisms contributing to this relationship.

## Limitations

- Claims about gradient masking lack theoretical grounding and mechanistic explanation
- Office-31 dataset limitations may not fully represent real-world edge deployment scenarios
- No analysis of computational efficiency trade-offs on actual edge hardware
- Limited investigation of why different compression techniques yield differential robustness

## Confidence

- **High Confidence**: Claims about post-training quantization maintaining better accuracy under domain shifts compared to pruning and distillation are strongly supported by experimental results across multiple model sizes and compression ratios.
- **Medium Confidence**: The assertion that larger base models produce less robust compressed networks requires careful interpretation, as it depends on the specific compression technique and target size.
- **Medium Confidence**: Claims about gradient masking providing adversarial robustness for quantized models are empirically demonstrated but lack mechanistic explanation.

## Next Checks

1. Conduct ablation studies to isolate whether gradient masking or other factors (e.g., reduced precision affecting attack optimization) are responsible for quantized model robustness to adversarial attacks.

2. Test the same compression techniques on a more diverse dataset (e.g., DomainNet) to verify that Office-31 results generalize to broader distribution shifts.

3. Measure inference latency and memory usage on representative edge devices to validate the practical deployment advantages claimed for compressed models.