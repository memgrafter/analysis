---
ver: rpa2
title: Anisotropy Is Inherent to Self-Attention in Transformers
arxiv_id: '2401.12143'
source_url: https://arxiv.org/abs/2401.12143
tags:
- anisotropy
- figure
- representations
- drift
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors investigate anisotropy in Transformers across modalities,
  showing it is not solely caused by token-level language model objectives or linguistic
  properties. They demonstrate that anisotropy is observed in character-based NLP
  models, speech models, and most vision models.
---

# Anisotropy Is Inherent to Self-Attention in Transformers

## Quick Facts
- arXiv ID: 2401.12143
- Source URL: https://arxiv.org/abs/2401.12143
- Authors: Nathan Godey; Éric de la Clergerie; Benoît Sagot
- Reference count: 12
- The authors show that anisotropy is an inherent property of Transformers across modalities, arising from the self-attention mechanism rather than training objectives or data distributions.

## Executive Summary
This paper investigates the phenomenon of anisotropy in Transformer models across multiple modalities including language, speech, and vision. The authors demonstrate that anisotropy is not solely caused by language model objectives or linguistic properties, but is instead an inherent characteristic of the self-attention mechanism itself. Through systematic analysis of query and key representations during training, they show that anisotropy emerges from the model's optimization of sharp attention patterns, which requires representations to drift in parallel directions. The findings suggest that anisotropy is a fundamental architectural property rather than an artifact of specific training objectives or data distributions.

## Method Summary
The authors conduct a comprehensive analysis of anisotropy across multiple Transformer-based models in different modalities (language, speech, vision). They compute hidden representations for validation sets and calculate average cosine-similarity between uniformly sampled pairs of vectors to measure anisotropy. The study includes correlation analyses between cosine-similarity and norm of average representation using Spearman and Pearson tests. They analyze untrained Transformer block behavior with biased inputs and study query/key representation dynamics along training using intermediate checkpoints. The experiments cover character-level models (CharacterBERT, CANINE, MANTa-LM, ByT5), speech models (wav2Vec 2.0, HuBERT, Whisper), vision models (ViT, BEiT, MiT, DEiT), and convolution-based vision models (ResNet, EfficientNet, CvT, ConvNeXt, VAN).

## Key Results
- Anisotropy is observed across all modalities including character-based NLP models, speech models, and most vision models
- The drift of query and key representations in parallel directions during training drives anisotropy and enables sharper attention patterns
- Anisotropy emerges from the self-attention mechanism itself rather than from specific training objectives or data distributions
- Untrained Transformer layers exhibit a tendency towards anisotropy when biased inputs are provided

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Anisotropy is an inherent property of Transformers, not solely caused by language model objectives or data distributions.
- Mechanism: Anisotropy arises from the self-attention mechanism during training. As training progresses, query and key representations drift in parallel directions, increasing the magnitude of scalar products and enabling sharper attention patterns. This drift is driven by the model's optimization of attention sharpness, not by the specific task or data.
- Core assumption: Sharp attention patterns are beneficial for Transformer performance, and the self-attention mechanism naturally induces anisotropy when optimizing for sharpness.
- Evidence anchors:
  - [abstract]: "We show that anisotropy is observed in character-based NLP models, speech models, and most vision models... anisotropy is inherent to Transformers-based models, as it arises from the self-attention mechanism rather than the training objective or data distribution."
  - [section]: "We have established that manually pushing for drift-based anisotropy on untrained Transformers models leads to sharper (i.e. low-entropy) self-attention patterns. We also show in section 5 that along training, query and key distributions drift in parallel directions, which increases anisotropy in the inner representations of the Transformer layers, while allowing sharper attention patterns."
  - [corpus]: Weak evidence; corpus neighbors focus on anisotropy in other contexts but don't directly support this mechanism.
- Break condition: If attention patterns do not require sharp distributions, or if an alternative attention mechanism does not induce anisotropy.

### Mechanism 2
- Claim: The drift effect in Transformers is not the sole cause of anisotropy; anisotropy can occur without a strong correlation to the drift magnitude.
- Mechanism: Anisotropy emerges from the self-attention mechanism's tendency to sharpen attention distributions during training, which distorts the geometry of hidden representations. This distortion is not always directly correlated with the norm of the average representation (drift effect), as seen in some vision and speech models.
- Core assumption: The self-attention mechanism inherently distorts representation geometry when optimizing for attention sharpness, independent of the drift effect.
- Evidence anchors:
  - [abstract]: "Our observations suggest that anisotropy is actually inherent to Transformers-based models."
  - [section]: "Interestingly, we notice that the anisotropy affecting most CNN-based vision models is generally not correlated with the drift effect, contrary to Tranformers-based models in the same modality... This could partially be explained by the fact that the batch normalization (Ioffe and Szegedy, 2015) used in some of these models mitigates a posteriori the drift effect by removing the mean component of the representations."
  - [corpus]: Weak evidence; corpus neighbors discuss anisotropy but don't provide direct support for this mechanism.
- Break condition: If anisotropy can be fully explained by the drift effect, or if alternative mechanisms can account for anisotropy without relying on self-attention.

### Mechanism 3
- Claim: Untrained Transformer layers exhibit a tendency towards anisotropy, which is exacerbated by biased inputs that facilitate the emergence of sharp attention patterns.
- Mechanism: Even without training, adding a common bias term to input representations increases the variance of attention scores and leads to sharper attention distributions. This suggests that the Transformer architecture itself has an inherent tendency towards anisotropy, which is further amplified during training.
- Core assumption: The Transformer architecture, even in its untrained state, is predisposed to produce anisotropic representations when inputs are biased.
- Evidence anchors:
  - [abstract]: "We also study anisotropy in untrained Transformers layers."
  - [section]: "In this section, we focus on some intrinsic properties of the Transformer block in a modality-agnostic fashion, i.e. with minimal assumptions on the data distribution, and without training... We study the average norm of the input representations E(||xi + b||2) against the average norm of the output representations E(||T (xi + b)||2) in Figure 6b."
  - [corpus]: Weak evidence; corpus neighbors do not directly support this mechanism.
- Break condition: If untrained Transformer layers do not exhibit a tendency towards anisotropy, or if biased inputs do not lead to sharper attention patterns.

## Foundational Learning

- Concept: Cosine similarity and its relation to anisotropy
  - Why needed here: Understanding anisotropy requires grasping how cosine similarity measures the angular distance between vectors, and how high average cosine similarity indicates that representations are clustered in a narrow cone.
  - Quick check question: If the average cosine similarity between all pairs of hidden representations in a layer is 0.8, is this layer likely to be anisotropic? (Yes)

- Concept: Self-attention mechanism and its components (queries, keys, values)
  - Why needed here: Anisotropy in Transformers is shown to arise from the self-attention mechanism, specifically from the drift and alignment of query and key representations during training.
  - Quick check question: In the self-attention operation, what is the mathematical operation performed between queries and keys to compute attention scores? (Dot product)

- Concept: Sharp attention patterns and their relation to anisotropy
  - Why needed here: The paper argues that anisotropy emerges when the model optimizes for sharp attention patterns, which are distributions with low entropy where attention focuses on specific tokens.
  - Quick check question: If the self-attention softmax distributions have high maximum values and low minimum values, are they considered sharp? (Yes)

## Architecture Onboarding

- Component map:
  Input embeddings -> Transformer block -> Multi-head self-attention -> Query/key/value projections -> Attention scores -> Attention probabilities -> Value aggregation -> Feed-forward network -> Output representations

- Critical path:
  1. Input embeddings are passed through the Transformer block.
  2. In each self-attention head, queries and keys are computed from the input embeddings.
  3. Attention scores are calculated as the dot product of queries and keys.
  4. Attention scores are passed through a softmax to obtain attention probabilities.
  5. Values are aggregated based on attention probabilities to produce the output of the self-attention layer.
  6. The feed-forward network further transforms the output of the self-attention layer.
  7. This process is repeated for multiple layers, with each layer potentially contributing to the anisotropy of the final representations.

- Design tradeoffs:
  - Sharp attention patterns vs. isotropic representations: Optimizing for sharp attention patterns may lead to anisotropic representations, which could have implications for downstream tasks.
  - Model capacity vs. anisotropy: Larger models with more parameters may be more prone to anisotropy, as they have more degrees of freedom to optimize for sharp attention patterns.
  - Training objectives vs. anisotropy: Different training objectives (e.g., masked language modeling, next token prediction) may have varying effects on the emergence of anisotropy.

- Failure signatures:
  - High average cosine similarity between hidden representations across layers, indicating anisotropy.
  - Sharp attention patterns with low entropy distributions, suggesting that the model is optimizing for focused attention at the cost of representation isotropy.
  - Drift of query and key representations in parallel directions during training, leading to increased anisotropy.

- First 3 experiments:
  1. Measure the average cosine similarity between hidden representations across layers for a pre-trained Transformer model on a specific task (e.g., language modeling, image classification).
  2. Analyze the evolution of query and key representations during training by extracting intermediate checkpoints and computing their drift and alignment.
  3. Compare the anisotropy levels and attention sharpness between models trained with different objectives (e.g., masked language modeling vs. next token prediction) to understand the impact of training objectives on anisotropy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can sharp self-attention patterns emerge without anisotropy in keys and queries representations?
- Basis in paper: [inferred] The authors hypothesize that sharp self-attention patterns require anisotropy in keys and queries representations, but do not prove this theoretically.
- Why unresolved: The paper focuses on empirical observations and correlations rather than providing a theoretical proof for the necessity of anisotropy in achieving sharp attention patterns.
- What evidence would resolve it: A theoretical analysis or empirical study demonstrating that sharp self-attention patterns can or cannot be achieved without anisotropy in keys and queries representations.

### Open Question 2
- Question: How does anisotropy in Transformers-based models impact downstream task performance across different modalities?
- Basis in paper: [inferred] The authors discuss the potential harmfulness of anisotropy but do not provide a comprehensive analysis of its impact on downstream task performance across modalities.
- Why unresolved: The paper does not include experiments or analysis on how anisotropy affects the performance of Transformers-based models in various downstream tasks across different modalities.
- What evidence would resolve it: Empirical studies comparing the performance of anisotropic and isotropic Transformers-based models on a range of downstream tasks across different modalities.

### Open Question 3
- Question: What are the underlying causes of anisotropy in Transformers-based models, and can they be mitigated through architectural or training modifications?
- Basis in paper: [explicit] The authors conclude that anisotropy is inherent to Transformers-based models and appears in the self-attention mechanism when modeling sharp attention patterns. They suggest that revising the self-attention operation could help reduce anisotropy.
- Why unresolved: The paper does not provide a definitive answer on the root causes of anisotropy or propose specific architectural or training modifications to mitigate it.
- What evidence would resolve it: A comprehensive study identifying the key factors contributing to anisotropy in Transformers-based models and evaluating the effectiveness of proposed architectural or training modifications in reducing anisotropy while maintaining or improving model performance.

## Limitations

- The study relies on correlation analyses between anisotropy measures and training dynamics, which may not establish definitive causation.
- The focus on pre-trained models limits understanding of how anisotropy develops during the full training trajectory.
- The analysis of untrained Transformers provides suggestive evidence but does not establish whether anisotropy emerges solely from the attention mechanism or requires specific initialization schemes.

## Confidence

**High Confidence**: The observation that anisotropy is present across multiple modalities (language, speech, vision) and model architectures. The correlation analysis between cosine similarity measures and representation norms is straightforward and robust.

**Medium Confidence**: The claim that anisotropy arises from the self-attention mechanism rather than training objectives or data distributions. While the evidence is compelling, the analysis relies on indirect measures and does not provide direct causal evidence.

**Low Confidence**: The assertion that anisotropy is beneficial for model performance through enabling sharper attention patterns. The paper does not provide direct evidence that sharper attention patterns improve performance, nor does it demonstrate that anisotropy is necessary for achieving such sharpness.

## Next Checks

1. **Causal Intervention Experiment**: Design a controlled experiment where attention sharpness is decoupled from representation geometry. For instance, implement an alternative attention mechanism that produces sharp distributions without requiring query/key drift, then measure whether anisotropy still emerges.

2. **Performance Impact Analysis**: Conduct systematic ablation studies measuring the impact of anisotropy on downstream task performance. Compare models with artificially induced isotropy (e.g., through representation normalization) against standard models across multiple tasks to determine if anisotropy has measurable benefits or costs.

3. **Training Dynamics Study**: Implement continuous monitoring of representation geometry throughout the entire training process, not just at intermediate checkpoints. Use techniques like representation space visualization and trajectory analysis to track how query/key alignments develop and whether they correlate with specific training milestones or gradient patterns.