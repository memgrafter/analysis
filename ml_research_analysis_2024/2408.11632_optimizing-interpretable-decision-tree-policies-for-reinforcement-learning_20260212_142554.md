---
ver: rpa2
title: Optimizing Interpretable Decision Tree Policies for Reinforcement Learning
arxiv_id: '2408.11632'
source_url: https://arxiv.org/abs/2408.11632
tags:
- tree
- decision
- learning
- policy
- trees
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DTPO, a method for optimizing decision tree
  policies in reinforcement learning using policy gradients. DTPO addresses the challenge
  of optimizing non-differentiable decision trees by leveraging regression tree learning
  heuristics and incrementally improving tree predictions based on gradient information.
---

# Optimizing Interpretable Decision Tree Policies for Reinforcement Learning

## Quick Facts
- arXiv ID: 2408.11632
- Source URL: https://arxiv.org/abs/2408.11632
- Authors: DaniÃ«l Vos; Sicco Verwer
- Reference count: 24
- Key outcome: DTPO achieves interpretable policies with as few as 4-128 leaves depending on environment complexity, demonstrating that small decision trees can sometimes match or exceed neural network performance while remaining interpretable

## Executive Summary
This paper introduces DTPO, a method for optimizing decision tree policies in reinforcement learning using policy gradients. DTPO addresses the challenge of optimizing non-differentiable decision trees by leveraging regression tree learning heuristics and incrementally improving tree predictions based on gradient information. The algorithm iteratively updates a decision tree policy using batches of environment experience, employing Generalized Advantage Estimation for advantage estimates and a softmax-based loss function. Experiments on 17 control tasks and discrete MDPs show that DTPO performs competitively with VIPER, a state-of-the-art imitation learning method, and outperforms neural network policies in 4 environments.

## Method Summary
DTPO is a policy gradient method that directly optimizes decision tree policies rather than neural networks. The algorithm collects batches of environment experience (T=10,000 steps), computes GAE advantages, and uses regression tree learning heuristics to fit a new decision tree that predicts targets based on the gradient of the loss function with respect to current predictions. The new tree replaces the old one if it improves the loss. DTPO maintains both a decision tree policy and a value function neural network, updating the value function with Adam optimizer for 4 epochs on batches of 64 samples. The method is evaluated on 17 benchmark environments with maximum 16 leaf nodes per tree, comparing against VIPER, DQN, and PPO baselines.

## Key Results
- DTPO performs competitively with VIPER on 17 benchmark environments
- DTPO outperforms neural network policies in 4 of the 17 tested environments
- Small decision trees (4-128 leaves) can match or exceed neural network performance while remaining interpretable
- DTPO shows particular effectiveness in environments where Q-learning struggles, such as Navigation3D and TigerVsAntelope

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DTPO optimizes decision trees by iteratively updating tree targets using the gradient of a differentiable loss function.
- Mechanism: In each iteration, DTPO collects a batch of environment experience, estimates advantage values, and uses regression tree learning heuristics to fit a new decision tree that predicts targets based on the gradient of the loss function with respect to current predictions. The new tree replaces the old one if it improves the loss.
- Core assumption: A sufficiently large batch of experience can capture enough information about the current tree's behavior to allow meaningful updates without forgetting past learning.
- Evidence anchors:
  - [abstract] "DTPO addresses the challenge of optimizing non-differentiable decision trees by leveraging regression tree learning heuristics and incrementally improving tree predictions based on gradient information."
  - [section] "The main idea is to leverage existing regression tree learning heuristics and repurpose them to iteratively optimize regression trees for a differentiable loss function L on a sufficiently large batch of samples X."
  - [corpus] Weak evidence - corpus neighbors focus on tree-based RL but don't discuss gradient-based tree optimization directly.
- Break condition: If batch size is too small, the tree will "forget" previous behavior and fail to learn coherent policies.

### Mechanism 2
- Claim: Decision tree policies can perform competitively with neural network policies in certain environments.
- Mechanism: By limiting decision trees to 16 leaf nodes and optimizing them directly with policy gradients, DTPO achieves interpretable policies that match or exceed neural network performance on specific control tasks and discrete MDPs.
- Core assumption: Some environments have decision boundaries that can be captured by small decision trees without significant loss of performance.
- Evidence anchors:
  - [abstract] "Experiments on 17 control tasks and discrete MDPs show that DTPO performs competitively with VIPER... and outperforms neural network policies in 4 environments."
  - [section] "We find that we can get near-optimal performance on CartPole-v1 with 4 leaves, 8 leaves for FrozenLake4x4, 16 leaves for Pendulum-v1, and 64 leaves for CartPoleSwingup."
  - [corpus] Weak evidence - corpus neighbors discuss tree-based RL but don't provide direct comparisons with neural networks.
- Break condition: When environments require complex, non-linear decision boundaries that exceed the representational capacity of small decision trees.

### Mechanism 3
- Claim: DTPO's policy gradient approach is advantageous compared to Q-learning-based tree extraction methods in certain environments.
- Mechanism: Unlike VIPER which relies on a pre-trained Deep Q-Network teacher, DTPO directly optimizes decision trees using policy gradients, making it effective in environments where Q-learning struggles.
- Core assumption: Policy gradient methods can find good policies in environments where Q-learning fails due to, for example, function approximation errors or exploration difficulties.
- Evidence anchors:
  - [abstract] "DTPO performs competitively with VIPER, and in 4 of the 17 tested environments, one of the decision tree methods outperforms the neural network-based methods."
  - [section] "Since VIPER relies on the DQN to optimize its policy, the method fails in environments such as Navigation3D and TigerVsAntelope, where the DQN finds a bad policy."
  - [corpus] Weak evidence - corpus neighbors focus on tree-based RL but don't discuss Q-learning versus policy gradient tradeoffs.
- Break condition: When Q-learning is particularly well-suited to the environment structure, VIPER may outperform DTPO.

## Foundational Learning

- Concept: Reinforcement Learning and Policy Gradients
  - Why needed here: DTPO is fundamentally a policy gradient method that directly optimizes a decision tree policy rather than a neural network.
  - Quick check question: What is the key difference between policy gradient methods and Q-learning in terms of how they update policies?

- Concept: Decision Tree Learning Heuristics
  - Why needed here: DTPO repurposes established regression tree learning algorithms (like CART) to optimize decision trees for a differentiable loss function in RL settings.
  - Quick check question: How do standard decision tree learning algorithms like CART determine where to split nodes?

- Concept: Advantage Estimation (GAE)
  - Why needed here: DTPO uses Generalized Advantage Estimation to compute advantage values for each state-action pair, which guide the policy updates.
  - Quick check question: What is the purpose of advantage estimation in policy gradient methods, and how does GAE improve upon basic advantage estimation?

## Architecture Onboarding

- Component map:
  - Environment interaction module -> Advantage estimator -> Decision tree optimizer -> Value function approximator -> Determinization module

- Critical path:
  1. Collect T=10,000 steps of experience using current policy
  2. Compute GAE advantages and normalize
  3. Update decision tree policy by fitting a new tree with targets based on log probabilities and advantage gradients
  4. Update value function neural network parameters
  5. Every 10 iterations, evaluate deterministic version of policy

- Design tradeoffs:
  - Batch size vs. sample efficiency: Larger batches prevent forgetting but require more environment interactions
  - Tree size vs. interpretability: Smaller trees are more interpretable but may sacrifice performance
  - Single vs. multiple policy updates per batch: DTPO uses one update per batch to avoid overfitting

- Failure signatures:
  - Performance plateaus early: May indicate insufficient batch size or learning rate issues
  - High variance in results: Could suggest instability in advantage estimation or tree optimization
  - Decision trees become too large: Might indicate need for stronger regularization or smaller maximum leaf limits

- First 3 experiments:
  1. CartPole-v1 with max 4 leaves: Tests basic functionality and demonstrates potential for extreme simplification
  2. FrozenLake4x4 with max 8 leaves: Tests performance on discrete MDPs with sparse rewards
  3. Pendulum-v1 with max 16 leaves: Tests continuous control environment with moderate complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sample efficiency of DTPO compare to VIPER and PPO when controlling for tree size and computational resources?
- Basis in paper: [inferred] The paper mentions that DTPO uses 15 million samples compared to VIPER's 4.7 million additional samples, but doesn't directly compare sample efficiency across methods.
- Why unresolved: The paper doesn't provide a direct comparison of sample efficiency across the different methods while controlling for tree size and computational resources.
- What evidence would resolve it: A controlled experiment comparing the number of samples needed by each method to achieve comparable performance for decision trees of similar sizes, while also measuring computational resources used.

### Open Question 2
- Question: What is the impact of batch size on DTPO's performance and sample efficiency, and what is the optimal batch size for different environment complexities?
- Basis in paper: [explicit] The paper mentions experimenting with batch sizes between 5,000 and 100,000, finding good performance with 10,000 samples, and suggesting 50,000 for more complex environments.
- Why unresolved: The paper doesn't provide a systematic analysis of how batch size affects performance and sample efficiency across different environments.
- What evidence would resolve it: A detailed study varying batch sizes across a range of environment complexities, measuring both performance and sample efficiency to determine optimal batch sizes.

### Open Question 3
- Question: How does DTPO perform in environments with continuous action spaces, and what modifications are needed to adapt the algorithm?
- Basis in paper: [explicit] The paper mentions that "policy gradient techniques such as DTPO can be adapted for environments with continuous action spaces with relative ease."
- Why unresolved: The paper doesn't provide any experimental results or detailed discussion of DTPO's performance in continuous action space environments.
- What evidence would resolve it: Experiments applying DTPO to benchmark continuous control environments (e.g., MuJoCo tasks) and analyzing performance, required modifications, and any limitations encountered.

## Limitations
- DTPO's performance depends heavily on batch size selection, with small batches risking catastrophic forgetting of learned policies
- The method is currently limited to relatively small decision trees (up to 16 leaves in experiments), which may not capture complex decision boundaries
- DTPO requires significant computational resources for collecting large batches of experience and maintaining both decision tree and neural network components

## Confidence
- High confidence: The core mechanism of using regression tree learning heuristics with gradient-based updates is well-founded and experimentally validated
- Medium confidence: Claims about DTPO outperforming neural networks in specific environments are supported but limited to 4 out of 17 tested environments
- Medium confidence: Comparisons with VIPER are meaningful but constrained by the fact that both methods use different underlying optimization approaches

## Next Checks
1. Test DTPO with varying batch sizes (from 1,000 to 20,000 steps) to empirically determine the optimal balance between sample efficiency and policy stability
2. Evaluate DTPO on more complex continuous control environments (like MuJoCo tasks) to assess scalability beyond the current 17 benchmark tasks
3. Conduct ablation studies removing the value function neural network to quantify its contribution to overall policy optimization performance