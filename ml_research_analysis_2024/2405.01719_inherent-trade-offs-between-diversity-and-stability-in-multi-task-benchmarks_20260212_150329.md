---
ver: rpa2
title: Inherent Trade-Offs between Diversity and Stability in Multi-Task Benchmarks
arxiv_id: '2405.01719'
source_url: https://arxiv.org/abs/2405.01719
tags:
- benchmarks
- tasks
- benchmark
- sensitivity
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses social choice theory to analyze multi-task machine
  learning benchmarks, treating tasks as voters and models as candidates. It identifies
  a fundamental trade-off between diversity (task disagreement in rankings) and stability
  (sensitivity to irrelevant changes) in such benchmarks.
---

# Inherent Trade-Offs between Diversity and Stability in Multi-Task Benchmarks

## Quick Facts
- arXiv ID: 2405.01719
- Source URL: https://arxiv.org/abs/2405.01719
- Reference count: 37
- This paper uses social choice theory to analyze multi-task machine learning benchmarks, treating tasks as voters and models as candidates.

## Executive Summary
This paper reveals a fundamental trade-off in multi-task benchmark design between diversity (task disagreement in rankings) and stability (sensitivity to irrelevant changes). By framing benchmark aggregation as a voting system, the authors apply social choice theory to show that no benchmark can simultaneously maximize both diversity and stability. Through extensive experiments on seven cardinal and eleven ordinal benchmarks, they demonstrate that existing benchmarks cluster near a linear interpolation between random (most diverse, least stable) and constant (least diverse, most stable) baselines, with many popular benchmarks showing high instability to irrelevant changes.

## Method Summary
The authors introduce quantitative measures for diversity (Kendall's coefficient of concordance W) and sensitivity (impact of irrelevant task transformations on final rankings). They develop efficient approximation algorithms using gradient-based optimization to compute these measures despite computational hardness. The analysis treats tasks as voters and models as candidates in an electoral system analogy, applying social choice theory to benchmark aggregation. For cardinal benchmarks, sensitivity is measured by adding label noise; for ordinal benchmarks, by adding irrelevant models. The methods enable practical computation and comparison across diverse benchmark types.

## Key Results
- Existing benchmarks strike no better trade-off than a linear interpolation between constant and random benchmarks
- Many popular benchmarks are highly unstable to irrelevant changes, raising reliability concerns
- The diversity-sensitivity trade-off is demonstrated across seven cardinal and eleven ordinal benchmarks
- Efficient approximation algorithms enable practical computation of otherwise intractable measures

## Why This Works (Mechanism)

### Mechanism 1
Treating tasks as voters and models as candidates allows application of social choice theory to analyze multi-task benchmark design. By framing the benchmark aggregation problem as an election system, the mathematical structure of Arrow's impossibility theorem becomes directly applicable. The analogy maps tasks to voters, models to candidates, and the benchmark's aggregation function to a voting rule. This breaks down if tasks have fundamentally different evaluation criteria that cannot be meaningfully ordered or compared.

### Mechanism 2
The proposed diversity and sensitivity measures quantify the inherent trade-off between disagreement among tasks and robustness to irrelevant changes. Diversity is measured by Kendall's coefficient of concordance (W), capturing how much tasks disagree in their rankings. Sensitivity measures how much the final ranking changes when irrelevant transformations are applied. These create a 2D space where benchmarks must trade off between the two dimensions. This breaks down if benchmarks achieve both high diversity and high sensitivity, or if the measures fail to capture important aspects of benchmark quality.

### Mechanism 3
Efficient approximation algorithms enable practical computation of diversity and sensitivity despite their computational hardness. The paper develops gradient-based optimization procedures that relax the non-differentiable ranking distance into continuous objectives, allowing gradient descent to find approximately optimal perturbations. This breaks down if the relaxation significantly underestimates the true sensitivity, or if the optimization gets stuck in poor local optima.

## Foundational Learning

- **Concept:** Social Choice Theory and Arrow's Impossibility Theorem
  - Why needed here: Provides the theoretical framework that reveals fundamental limitations in how tasks can be aggregated into a single benchmark ranking
  - Quick check question: What are the three conditions in Arrow's theorem that cannot be simultaneously satisfied by any ordinal voting system?

- **Concept:** Kendall's τ coefficient and rank correlation
  - Why needed here: Used to measure both diversity (across tasks) and sensitivity (to perturbations) by quantifying rank disagreement
  - Quick check question: How does Kendall's τ coefficient differ from Spearman's rank correlation coefficient in measuring rank agreement?

- **Concept:** Cardinal vs Ordinal aggregation
  - Why needed here: Distinguishes two fundamentally different approaches to combining task results, each with different theoretical properties and practical implications
  - Quick check question: What is the key difference between how cardinal and ordinal benchmarks aggregate task results?

## Architecture Onboarding

- **Component map:** Data collection from multiple sources → Diversity computation module using Kendall's W → Sensitivity computation module with task-specific perturbation strategies → Visualization and analysis pipeline → Approximation algorithms for efficient computation

- **Critical path:** For a new benchmark evaluation: load benchmark data → compute diversity → compute sensitivity (using Algorithm 1 or 2) → generate visualization → interpret trade-off. The approximation algorithms are the computational bottleneck.

- **Design tradeoffs:** The paper chooses to use relaxed continuous objectives for sensitivity computation to make it tractable, accepting some approximation error. The choice of using top-20% models for ordinal sensitivity focuses on practical relevance but may underestimate the true sensitivity.

- **Failure signatures:** High sensitivity values indicate benchmarks that are unreliable to irrelevant changes. Very low diversity suggests benchmarks that don't adequately represent the task space. Benchmarks clustering near the linear interpolation between random and constant baselines suggest no improvement over trivial solutions.

- **First 3 experiments:**
  1. Run diversity and sensitivity computation on a small synthetic benchmark with known properties to verify the implementation.
  2. Compare the approximation algorithms' outputs against exact computation on a small benchmark to assess approximation quality.
  3. Apply the analysis to a new benchmark not in the paper to see if it exhibits similar trade-off behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum number of tasks needed to approximate the overall ranking in a benchmark, and how does this relate to diversity?
- Basis in paper: [explicit] The paper discusses diversity in multi-task benchmarks and its impact on the reliability of evaluation outcomes.
- Why unresolved: While the paper presents an experiment on the smallest rank change by re-calculating the average score based on a subset of tasks, it does not provide a clear answer to the minimum number of tasks needed to approximate the overall ranking.
- What evidence would resolve it: Further experiments that systematically vary the number of tasks in each benchmark and measure the ranking distance from the original ranking would provide evidence to determine the minimum number of tasks needed to approximate the overall ranking.

### Open Question 2
- Question: How does the inclusion of irrelevant models affect the sensitivity of ordinal benchmarks?
- Basis in paper: [explicit] The paper defines sensitivity in ordinal benchmarks as the largest ranking distance after adding a subset of extra candidate models into comparison.
- Why unresolved: While the paper provides a definition for sensitivity in ordinal benchmarks and presents experimental results, it does not explore the specific impact of including irrelevant models on the sensitivity of different ordinal benchmarks.
- What evidence would resolve it: Conducting experiments that systematically vary the number and characteristics of irrelevant models added to each ordinal benchmark and measure the resulting ranking distances would provide evidence to understand how the inclusion of irrelevant models affects the sensitivity of ordinal benchmarks.

### Open Question 3
- Question: How do cardinal and ordinal benchmarks compare in terms of their diversity and sensitivity trade-offs?
- Basis in paper: [explicit] The paper presents experimental results for both cardinal and ordinal benchmarks, showing a trade-off between diversity and sensitivity in both types of benchmarks.
- Why unresolved: While the paper provides experimental results for both cardinal and ordinal benchmarks, it does not directly compare the diversity and sensitivity trade-offs between the two types of benchmarks.
- What evidence would resolve it: Conducting experiments that directly compare the diversity and sensitivity trade-offs between cardinal and ordinal benchmarks, such as plotting the diversity and sensitivity measures for both types of benchmarks on the same graph, would provide evidence to understand how the two types of benchmarks compare in terms of their diversity and sensitivity trade-offs.

## Limitations

- The analysis assumes Kendall's W and sensitivity measures capture all relevant aspects of benchmark quality, potentially missing other important factors like task representativeness
- Approximation algorithms introduce uncertainty about whether calculated sensitivity values accurately reflect true benchmark properties
- The paper establishes the theoretical framework but does not provide concrete recommendations for benchmark designers

## Confidence

- **High confidence:** The theoretical framework connecting social choice theory to multi-task benchmarks is well-established and mathematically sound. The definitions of diversity (Kendall's W) and sensitivity measures are clearly specified and grounded in established statistical methods.

- **Medium confidence:** The empirical demonstration of the trade-off across multiple benchmarks is convincing, but the claim that existing benchmarks achieve "no better than a linear interpolation between random and constant benchmarks" may be sensitive to the specific approximation methods used.

- **Medium confidence:** The claim that many popular benchmarks are "highly unstable to irrelevant changes" is supported by the data, but the practical significance depends on how often irrelevant changes occur in practice.

## Next Checks

1. **Approximation accuracy validation:** Compare the sensitivity values computed by the approximation algorithms against exact computation (where feasible) on smaller benchmarks to quantify the approximation error and assess whether the observed trade-offs are robust to approximation quality.

2. **Practical significance analysis:** Design a simulation study where small, irrelevant perturbations are applied to real benchmark data, then measure how often these perturbations would change which model a practitioner would select as "best," providing concrete evidence about the practical impact of benchmark instability.

3. **Alternative quality metrics evaluation:** Compute additional benchmark quality metrics (such as task coverage, computational efficiency, or domain-specific evaluation criteria) for the same set of benchmarks to determine whether the diversity-sensitivity trade-off captures the full picture of benchmark quality or if other dimensions are equally important.