---
ver: rpa2
title: Referring Expression Generation in Visually Grounded Dialogue with Discourse-aware
  Comprehension Guiding
arxiv_id: '2409.05721'
source_url: https://arxiv.org/abs/2409.05721
tags:
- candidate
- image
- dialogue
- context
- referent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a two-stage approach to referring expression
  generation (REG) in visually grounded dialogue that aims to produce referring expressions
  (REs) that are both discriminative and discourse-appropriate. The method uses a
  fine-tuned vision-language model to autoregressively generate candidate REs based
  on dialogue context and visual content, then applies discourse-aware comprehension
  guiding to rerank candidates based on their contextual discriminatory power.
---

# Referring Expression Generation in Visually Grounded Dialogue with Discourse-aware Comprehension Guiding

## Quick Facts
- arXiv ID: 2409.05721
- Source URL: https://arxiv.org/abs/2409.05721
- Reference count: 32
- Primary result: Human evaluation shows reranked referring expressions achieve 78% text-image retrieval accuracy versus 74% for greedily generated REs

## Executive Summary
This paper proposes a two-stage approach to referring expression generation (REG) in visually grounded dialogue that produces referring expressions (REs) that are both discriminative and discourse-appropriate. The method uses a fine-tuned vision-language model to autoregressively generate candidate REs based on dialogue context and visual content, then applies discourse-aware comprehension guiding to rerank candidates based on their contextual discriminatory power. Human evaluation results demonstrate that the reranked REs achieve higher text-image retrieval accuracy compared to greedily generated REs while maintaining strong performance against ground truth mentions.

## Method Summary
The method employs a two-stage approach: first, IDEFICS (80B parameter variant) is fine-tuned using LoRA for multimodal causal language modeling to generate candidate REs conditioned on dialogue context and referent images. Second, a generate-and-rerank strategy is applied where candidate REs are inserted into the dialogue context and processed by a CRDG framework to generate referent descriptions. These descriptions are used to compute TIM (text→image matching) and ITM (image→text matching) scores via a discriminative VLM, which are then combined using a weighted linear formula to select the final RE.

## Key Results
- Reranked referring expressions achieve 78% text-image retrieval accuracy
- Greedily generated referring expressions achieve 74% text-image retrieval accuracy
- Reranked expressions achieve 88% accuracy against ground truth mentions
- The approach demonstrates improved discourse-appropriateness while maintaining discriminative power

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a generative VLM with LoRA enables context-aware RE generation in visually grounded dialogue. The model learns to condition autoregressive token generation on both dialogue context and visual input through gated cross-attention layers interleaved with frozen LLM layers. Core assumption: Visual tokens from a pretrained vision model can be effectively integrated into the language generation process to produce REs that are both discriminative and discourse-appropriate.

### Mechanism 2
Discourse-aware comprehension guiding improves RE discriminative power by reranking candidates based on both TIM and ITM scores. Candidate REs are inserted into the dialogue context and processed by a CRDG framework to generate referent descriptions, which are then used to compute TIM and ITM scores via a discriminative VLM. Core assumption: The CRDG framework can accurately generate referent descriptions that capture the discourse-dependent discriminatory power of candidate REs.

### Mechanism 3
Weighted reranking using TIM and ITM scores prevents selection of REs that are descriptive but not discriminative. A composite score is calculated as a weighted linear combination of the log softmax of TIM and ITM probabilities, with weights w = 2/3 for TIM and w = 1/3 for ITM. Core assumption: The relative difference between TIM and ITM scores provides meaningful information about an RE's appropriateness in context.

## Foundational Learning

- Multimodal Transformers: Why needed here - The system needs to process both text (dialogue context) and images (referent representations) simultaneously. Quick check: How do gated cross-attention layers enable a language model to incorporate visual information during token generation?
- Parameter-efficient fine-tuning (LoRA): Why needed here - Fine-tuning large VLMs is computationally expensive, and LoRA reduces the number of trainable parameters while maintaining performance. Quick check: What is the key insight behind LoRA that allows it to reduce trainable parameters without significant performance loss?
- Generate-and-rerank strategies: Why needed here - Generating multiple candidate REs and selecting the best one based on discriminative power is more effective than greedy decoding. Quick check: Why might a candidate RE that scores highest on text→image similarity not be the best choice for selection?

## Architecture Onboarding

- Component map: Dialogue context + referent image → IDEFICS generation → CRDG processing → InternVL scoring → reranking → selected RE
- Critical path: The system processes dialogue context and visual input through IDEFICS to generate candidates, then uses CRDG and InternVL to score and rerank them for final selection
- Design tradeoffs: Using a fine-tuned generative VLM vs. a rule-based system (more flexible but requires training data), reranking based on discourse-aware comprehension vs. direct generation (more accurate but computationally expensive), using a single discriminative VLM vs. ensemble (simpler but potentially less robust)
- Failure signatures: Low diversity in generated REs (insufficient beam search width or model collapse), poor TIM scores but good ITM scores (model generating REs that match other candidates but not target image), CRDG generates irrelevant referent descriptions (context window too small or CRDG not properly fine-tuned)
- First 3 experiments: 1) Test IDEFICS generation with varying beam widths (1, 3, 6, 10) to find optimal balance between diversity and quality, 2) Compare TIM-only reranking vs ITM-only reranking vs weighted combination to validate the reranking approach, 3) Test different context window sizes (3, 5, 7, 9 previous messages) to find optimal dialogue context for RE generation

## Open Questions the Paper Calls Out

1. How does the proposed two-stage REG approach perform when extended to handle multi-image referents instead of single-image referents? The authors focused on single-image referents for their experiments and note that extending to multi-image referents is a future direction.

2. What is the impact of incorporating spatial relations between objects/entities into the referring expressions generated by the model? The authors explicitly left this as future work and did not implement or evaluate spatial relations in their current model.

3. How would the performance of the REG model change if the visual context (all images in the scene) was included as input rather than conditioning only on the target image? The authors suggest this as future research but did not test this alternative approach.

## Limitations
- The AGOS dataset contains only 15 dialogues across 5 image categories, raising concerns about generalizability to diverse real-world dialogue scenarios
- The paper lacks direct ablation studies to isolate the contributions of individual mechanisms
- The use of IDEFICS-80B with LoRA for fine-tuning may limit practical applicability due to computational requirements

## Confidence
- High Confidence: The generate-and-rerank approach for producing contextually appropriate referring expressions is well-established in the literature
- Medium Confidence: The discourse-aware comprehension guiding mechanism shows promise through improved human evaluation metrics, but lacks direct empirical validation
- Low Confidence: The weighted reranking formula's effectiveness (2/3 TIM, 1/3 ITM) appears arbitrary without sensitivity analysis or justification

## Next Checks
1. Conduct an ablation study comparing text-image retrieval accuracy using greedy decoding only, generate-and-rerank without discourse-aware comprehension (TIM-only), and generate-and-rerank with discourse-aware comprehension (weighted TIM/ITM)
2. Test the fine-tuned model on a different referring expression dataset (e.g., RefCOCO, RefCOCO+) to assess generalization beyond the AGOS dataset
3. Systematically vary the TIM/ITM weights (e.g., 0.5/0.5, 0.7/0.3, 0.3/0.7) and evaluate their impact on text-image retrieval accuracy and human judgments of RE quality