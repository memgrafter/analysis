---
ver: rpa2
title: 'DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels'
arxiv_id: '2409.02465'
source_url: https://arxiv.org/abs/2409.02465
tags:
- reasoning
- context
- evidence
- long-context
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DetectiveQA is a benchmark for evaluating long-context reasoning
  using detective novels, featuring 1200 human-annotated questions in Chinese and
  English with an average context length of 118k tokens. Each question includes reference
  reasoning steps, with explicit and implicit evidence.
---

# DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels

## Quick Facts
- arXiv ID: 2409.02465
- Source URL: https://arxiv.org/abs/2409.02465
- Reference count: 22
- Primary result: 1200 human-annotated questions in Chinese and English with 118k average token length

## Executive Summary
DetectiveQA is a benchmark for evaluating long-context reasoning using detective novels, featuring 1200 human-annotated questions in Chinese and English with an average context length of 118k tokens. Each question includes reference reasoning steps, with explicit and implicit evidence. The dataset introduces a step-wise reasoning metric using LLM judgment to assess reasoning process quality. Experiments on mainstream LLMs reveal significant challenges in long-context reasoning, particularly in evidence retrieval and logical coherence.

## Method Summary
DetectiveQA uses detective novels averaging over 100k tokens to create a dataset of 1200 human-annotated questions. The evaluation framework employs multiple-choice accuracy combined with a step-wise reasoning metric that uses GPT-4 to judge the quality of reasoning processes. The benchmark tests models under three settings: full context with question, question only, and evidence with question, to isolate long-context processing, data contamination, and reasoning capabilities.

## Key Results
- Detective novels averaging 118k tokens provide challenging long-context scenarios
- LLaMA3.1 notably struggles with contexts exceeding 100k tokens
- Significant performance gaps exist between open-source and closed-source models in long-context reasoning
- Step-wise reasoning metric reveals reasoning process quality beyond simple accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The step-wise reasoning metric improves evaluation reliability by explicitly comparing LLM outputs against reference steps, rather than just checking final answers.
- **Mechanism:** By using GPT-4 to judge whether generated reasoning steps align with reference steps, the metric captures both evidence retrieval and logical coherence, offering a finer-grained assessment than accuracy alone.
- **Core assumption:** GPT-4 can reliably judge whether one chain of reasoning implicitly contains the logic of another, even without exact phrase overlap.
- **Evidence anchors:**
  - [abstract] "we introduce a step-wise reasoning metric, which enhances the evaluation of LLMs’ reasoning processes"
  - [section] "we introduce a step-wise reasoning metric, the average score across all questions, to reflect the LLM’s reasoning performance on DetectiveQA"
  - [corpus] Weak evidence - this mechanism is introduced in the paper but not yet validated by other studies.
- **Break condition:** If GPT-4 cannot accurately judge implicit reasoning containment, the metric would lose its discriminative power.

### Mechanism 2
- **Claim:** Context+Question setting tests true long-context reasoning, while Question-Only and Evidence+Question settings help isolate data contamination and reasoning capability.
- **Mechanism:** By varying what information is provided before the question (full context, just metadata, or only evidence), the benchmark disentangles the contributions of long-context processing, data contamination, and reasoning ability.
- **Core assumption:** If a model can answer without context, it likely memorized patterns from pre-training data.
- **Evidence anchors:**
  - [section] "We perform a question-wise comparison between the Question-Only and Question+Context settings"
  - [section] "we study the comparison between Question+Context and Evidence+Context settings"
  - [corpus] Weak evidence - this design is novel and not yet validated in broader literature.
- **Break condition:** If models consistently answer questions correctly without context, it would invalidate the benchmark's claim of testing long-context reasoning.

### Mechanism 3
- **Claim:** Long detective novels (100k+ tokens) provide realistic and challenging scenarios for evaluating long-context reasoning, more so than synthetic or short passages.
- **Mechanism:** The complexity of detective plots with multiple characters, events, and clues requires models to track information across long narratives, mimicking real-world reasoning tasks.
- **Core assumption:** Detective novels present sufficiently realistic and reasoning-intensive content to challenge LLMs.
- **Evidence anchors:**
  - [abstract] "We leverage detective novels, averaging over 100k tokens, to create a dataset containing 1200 human-annotated questions"
  - [section] "Detective novels are valuable for studying language models’ ability to handle long contexts due to their reasoning-heavy content"
  - [corpus] Moderate evidence - related works like NovelQA and DetectBench use novels but with different focuses.
- **Break condition:** If models can answer questions correctly without engaging with the full narrative context, the benchmark would fail to test long-context reasoning.

## Foundational Learning

- **Concept:** Long-context understanding in LLMs
  - **Why needed here:** DetectiveQA evaluates whether models can process and reason over texts exceeding 100k tokens, requiring understanding of how LLMs handle extended contexts.
  - **Quick check question:** Can you explain the difference between attention mechanisms for short vs. long contexts, and why models like LLaMA3.1 struggle with contexts over 100k tokens?

- **Concept:** Multi-hop reasoning
  - **Why needed here:** DetectiveQA questions require synthesizing evidence from multiple parts of the text, testing the model's ability to perform multi-step reasoning across long contexts.
  - **Quick check question:** How would you distinguish between simple fact retrieval and multi-hop reasoning in the context of answering questions about detective novels?

- **Concept:** Evaluation metrics for reasoning processes
  - **Why needed here:** The step-wise reasoning metric requires understanding how to evaluate the quality of reasoning chains, not just final answers.
  - **Quick check question:** What are the limitations of using accuracy alone to evaluate reasoning, and how might a step-wise metric address these limitations?

## Architecture Onboarding

- **Component map:** Novel collection → Annotation (with agent workflow) → Human refinement → Question generation → Reference step creation → Multiple-choice accuracy evaluation + Step-wise reasoning metric evaluation → Context setting variations → Performance analysis
- **Critical path:** Novel collection → Agent-assisted annotation → Human refinement → Metric validation → Model evaluation → Performance analysis
- **Design tradeoffs:**
  - Using agent workflow for annotation speeds up the process but may introduce subtle biases; human refinement mitigates this but adds cost
  - The step-wise reasoning metric is more informative but relies on GPT-4 judgment, which may not be perfectly reliable
  - Multiple context settings provide comprehensive analysis but increase evaluation complexity
- **Failure signatures:**
  - If step-wise reasoning scores are consistently low despite high accuracy, it indicates models are guessing correctly without proper reasoning
  - If Question-Only setting performs similarly to Question+Context, it suggests data contamination issues
  - If Llama3.1 performance drops significantly with contexts over 100k tokens, it reveals limitations in its long-context processing
- **First 3 experiments:**
  1. Validate the step-wise reasoning metric by comparing GPT-4 judgments with human annotations on a sample of 100 reasoning chains
  2. Test the win rate between Question-Only and Question+Context settings to quantify data contamination impact
  3. Analyze evidence retrieval performance across different context lengths using the multi-needle-in-a-haystack approach to identify long-context processing bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DetectiveQA compare when evaluated with models that have context lengths exceeding 200k tokens versus those limited to 128k tokens?
- Basis in paper: [inferred] The paper evaluates models like GPT-4-1106-preview-128k, Claude3-opus-20240229-200k, and LLaMA3.1-8B-Instruct-128k, with varying context lengths, and notes LLaMA3.1's struggles with contexts over 100k tokens.
- Why unresolved: The paper does not provide direct comparisons between models with context lengths above 200k and those limited to 128k, nor does it explore how models with larger context windows perform on DetectiveQA.
- What evidence would resolve it: Experiments comparing models with context lengths above 200k (e.g., Claude3-opus-200k) against those limited to 128k on DetectiveQA, measuring both answer accuracy and reasoning scores.

### Open Question 2
- Question: What specific factors contribute to the observed gap in reasoning performance between open-source models like LLaMA3.1 and closed-source models like Claude3 on DetectiveQA?
- Basis in paper: [explicit] The paper notes that LLaMA3.1 notably struggles with contexts exceeding 100k tokens and that open-source models lag behind closed-source models in long-context reasoning capabilities.
- Why unresolved: The paper does not provide a detailed analysis of the architectural or training differences that lead to this performance gap, nor does it explore whether fine-tuning or other techniques could bridge this gap.
- What evidence would resolve it: A detailed comparative analysis of the architectures, training datasets, and reasoning capabilities of LLaMA3.1 and Claude3, along with experiments testing whether fine-tuning LLaMA3.1 on DetectiveQA improves its performance.

### Open Question 3
- Question: How does the step-wise reasoning metric correlate with human judgments of reasoning quality across different types of reasoning tasks beyond narrative reasoning?
- Basis in paper: [explicit] The paper introduces a step-wise reasoning metric using LLM judgment to assess reasoning process quality and validates it by comparing GPT-4's judgments with human annotations on DetectiveQA.
- Why unresolved: The paper only validates the metric on DetectiveQA and does not explore its applicability or correlation with human judgments on other types of reasoning tasks (e.g., mathematical, logical, or scientific reasoning).
- What evidence would resolve it: Experiments applying the step-wise reasoning metric to other reasoning datasets (e.g., mathematical or logical reasoning tasks) and comparing the metric's judgments with human evaluations across these diverse tasks.

## Limitations
- Data contamination risk from using publicly available detective novels
- GPT-4 judgment reliability for step-wise reasoning metric without comprehensive human validation
- Evaluation settings don't reflect typical usage patterns where users might have access to both full context and supporting evidence

## Confidence
- High confidence: The benchmark's core design and methodology are sound
- Medium confidence: The step-wise reasoning metric's effectiveness is moderately supported but not fully validated
- Low confidence: Claims about the relative difficulty of different reasoning aspects are based on performance gaps but lack deeper analysis

## Next Checks
1. Human validation of step-wise metric: Have human annotators independently evaluate a random sample of 100 reasoning chains to assess the reliability and consistency of GPT-4 judgments compared to human judgments.
2. Cross-dataset generalization: Test the same models on DetectiveQA and related benchmarks (DetectBench, NovelQA) to determine whether performance patterns are consistent across different long-context reasoning datasets.
3. Scaling analysis: Evaluate model performance across varying context lengths (e.g., 50k, 100k, 150k tokens) to identify specific breakpoints where reasoning capabilities degrade and whether this degradation is linear or follows a different pattern.