---
ver: rpa2
title: 'OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety'
arxiv_id: '2403.12316'
source_url: https://arxiv.org/abs/2403.12316
tags:
- llms
- chinese
- evaluation
- openeval
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OpenEval, a comprehensive evaluation platform
  for Chinese LLMs across capability, alignment, and safety. It includes 53 tasks
  from 25 benchmark datasets covering NLP tasks, disciplinary knowledge, commonsense
  reasoning, mathematical reasoning, bias, offensiveness, illegalness, and safety.
---

# OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety

## Quick Facts
- arXiv ID: 2403.12316
- Source URL: https://arxiv.org/abs/2403.12316
- Reference count: 40
- Primary result: Comprehensive evaluation platform for Chinese LLMs across 53 tasks from 25 benchmark datasets, revealing proprietary models excel in disciplinary knowledge while open-source models perform better in alignment and safety

## Executive Summary
OpenEval is a comprehensive evaluation platform for Chinese LLMs that benchmarks across capability, alignment, and safety dimensions. The platform includes 53 tasks from 25 benchmark datasets covering NLP tasks, disciplinary knowledge, commonsense reasoning, mathematical reasoning, bias, offensiveness, illegalness, and safety risks. The authors evaluated 9 open-source (7B-72B parameters) and 5 proprietary Chinese LLMs, finding that proprietary models excel in disciplinary knowledge and mathematical reasoning while open-source models perform better in alignment and safety. Both types of models struggle with commonsense reasoning tasks.

## Method Summary
The evaluation platform uses zero-shot evaluation with task-specific prompts for each of the 53 tasks across 25 benchmark datasets. Models are evaluated using task-appropriate metrics including accuracy, BLEU, ROUGE, EM, F1, and specialized metrics for alignment and safety tasks. The platform implements a phased public evaluation strategy with dynamic benchmark updates to prevent data contamination, releasing new benchmarks only after evaluation cycles complete.

## Key Results
- Proprietary models (Qwen-72B-Chat, Yi-34B-Chat) outperform open-source models in disciplinary knowledge and mathematical reasoning
- Open-source models generally perform better in alignment and safety tasks compared to proprietary models
- Both proprietary and open-source models struggle with commonsense reasoning tasks across the evaluation suite
- Larger models like Qwen-72B-Chat do not always outperform smaller models like Yi-34B-Chat in NLP tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Phased public evaluation with continuous benchmark updates prevents data contamination.
- Mechanism: By releasing new benchmarks only after a public evaluation cycle and replacing older ones, the system ensures that LLMs cannot overfit to specific test sets, maintaining the integrity of future evaluations.
- Core assumption: Benchmark datasets are static enough that they cannot be reverse-engineered from evaluation results alone.
- Evidence anchors:
  - [abstract] "In addition to these benchmarks, we have implemented a phased public evaluation and benchmark update strategy to ensure that OpenEval is in line with the development of Chinese LLMs or even able to provide cutting-edge benchmark datasets to guide the development of Chinese LLMs."
  - [section 5.2] "Hence, we have introduced a dynamic evaluation strategy in OpenEval, allowing evaluations to be conducted periodically. We will continue to collect new benchmarks to replace the previous ones in OpenEval to prevent data contamination..."

### Mechanism 2
- Claim: Task-specific prompts and metrics enable consistent evaluation across diverse capabilities.
- Mechanism: Each benchmark task has a unique prompt that leverages LLMs' instruction-following ability, paired with metrics tailored to the task type (accuracy, BLEU, EM, F1, etc.), ensuring apples-to-apples comparisons.
- Core assumption: LLMs can reliably interpret and follow the provided prompts across different task types.
- Evidence anchors:
  - [section 3] "In the current version of OpenEval, we collect 25 datasets and further split them into 53 tasks... Users can also modify the prompts by themselves, as different LLMs use different prompts that are defined during their fine-tuning stage."
  - [section 4.1] Examples of task-specific prompts for reading comprehension, QA, text generation, etc.

### Mechanism 3
- Claim: Multi-dimensional evaluation (capability, alignment, safety) captures LLM performance beyond traditional NLP benchmarks.
- Mechanism: By including 4 capability sub-dimensions, 3 alignment sub-dimensions, and 6 safety sub-dimensions, the platform assesses LLMs comprehensively, identifying strengths and weaknesses that single-dimension benchmarks miss.
- Core assumption: The selected sub-dimensions are comprehensive enough to represent the key aspects of LLM evaluation.
- Evidence anchors:
  - [abstract] "OpenEval... benchmarks Chinese LLMs across capability, alignment and safety... For capability assessment, we include 12 benchmark datasets to evaluate Chinese LLMs from 4 sub-dimensions: NLP tasks, disciplinary knowledge, commonsense reasoning and mathematical reasoning."
  - [section 4] "Inspired by Guo et al. (2023), we design an evaluation taxonomy with three major dimensions for OpenEval, which are capability, alignment, and safety..."

## Foundational Learning

- Concept: Zero-shot evaluation setting
  - Why needed here: Ensures LLMs are tested on their general capabilities without task-specific training, making results more generalizable.
  - Quick check question: What's the difference between zero-shot and few-shot evaluation, and why might zero-shot be preferred for benchmarking LLMs?

- Concept: Instruction following in LLMs
  - Why needed here: The evaluation relies on LLMs correctly interpreting and executing task-specific prompts, which is a key capability being measured.
  - Quick check question: How does instruction following differ from traditional fine-tuning, and why is it important for evaluation consistency?

- Concept: Metric selection for different task types
  - Why needed here: Different tasks require different evaluation metrics (e.g., accuracy for multiple choice, BLEU for text generation), and choosing the right metric is crucial for meaningful results.
  - Quick check question: Why wouldn't you use accuracy as the sole metric for all tasks, and what are the advantages of task-specific metrics?

## Architecture Onboarding

- Component map: 25 datasets → 53 tasks → 300K questions → Task-specific prompts → LLM execution → Answer extraction → Metric calculation → Result aggregation → Leaderboard system → Benchmark update mechanism

- Critical path: Data ingestion → Prompt generation → LLM execution → Answer extraction → Metric calculation → Result aggregation → Leaderboard update

- Design tradeoffs:
  - Comprehensive evaluation vs. evaluation efficiency (selecting one benchmark per capability dimension)
  - Static benchmark datasets vs. dynamic updates (preventing data contamination)
  - Open-source accessibility vs. proprietary model evaluation (API-based vs. local evaluation modes)

- Failure signatures:
  - Poor answer extraction: LLMs generate unexpected output formats that don't match metric requirements
  - Inconsistent results: Different prompt versions lead to varying LLM performance
  - Data contamination: LLMs perform well on outdated benchmarks but poorly on new ones

- First 3 experiments:
  1. Run the same LLM on two different prompt versions of the same task to verify prompt sensitivity
  2. Compare zero-shot results with few-shot results on a subset of tasks to understand the impact of examples
  3. Test the answer extraction mechanism with intentionally malformed LLM outputs to verify robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific trade-offs between model size and performance across different capability dimensions in Chinese LLMs?
- Basis in paper: [explicit] The paper shows that larger models like Qwen-72B-Chat do not always outperform smaller models like Yi-34B-Chat in NLP tasks, and that earlier models like BELLE-7B-2M and MOSS-SFT-16B perform poorly in most dimensions except safety.
- Why unresolved: The paper only provides a snapshot of performance across model sizes, but does not deeply analyze the underlying reasons for these trade-offs or the relationship between model size and specific capabilities.
- What evidence would resolve it: A comprehensive study analyzing the relationship between model size and performance across all capability dimensions, including an investigation of the training data and fine-tuning strategies used for different model sizes.

### Open Question 2
- Question: How can the alignment of Chinese LLMs be improved, particularly in areas like bias and offensiveness detection?
- Basis in paper: [explicit] The paper shows that open-source models generally outperform proprietary models in alignment tasks, indicating that alignment is a significant area for improvement.
- Why unresolved: The paper does not delve into the specific techniques or data used for alignment training, nor does it provide insights into why open-source models perform better in this area.
- What evidence would resolve it: A detailed analysis of the alignment training methods used for different models, including an investigation of the quality and diversity of the alignment data, and an exploration of potential techniques for improving alignment performance.

### Open Question 3
- Question: What are the most effective strategies for mitigating the potential safety risks associated with advanced Chinese LLMs?
- Basis in paper: [explicit] The paper highlights the importance of safety evaluation and the need to address risks like power-seeking, myopia, and self-awareness in advanced LLMs.
- Why unresolved: The paper does not provide specific recommendations or techniques for mitigating these risks, nor does it explore the potential trade-offs between safety and other capabilities.
- What evidence would resolve it: A comprehensive study investigating the effectiveness of different safety mitigation strategies, including an analysis of their impact on model performance and an exploration of potential trade-offs between safety and other capabilities.

## Limitations

- The evaluation platform's reliance on zero-shot evaluation may not fully capture LLM capabilities compared to fine-tuned or few-shot approaches
- The focus on Chinese language models limits generalizability to other languages and cultural contexts
- The dynamic benchmark update strategy may introduce variability in evaluation results over time, complicating longitudinal comparisons

## Confidence

**High Confidence**: The platform's multi-dimensional evaluation framework and phased public evaluation strategy are well-supported by methodology and evidence.

**Medium Confidence**: The claim about proprietary models excelling in disciplinary knowledge while open-source models perform better in alignment is supported but may be influenced by specific model selection.

**Low Confidence**: The assertion that both model types struggle with commonsense reasoning is based on evaluation results but may be task-dependent.

## Next Checks

1. Conduct systematic prompt sensitivity analysis by running the same LLM on multiple prompt versions for identical tasks to quantify impact on evaluation consistency.

2. Evaluate a subset of benchmark tasks using English-language LLMs to assess cross-lingual generalizability and identify potential language-specific biases.

3. Implement controlled longitudinal study comparing LLM performance on identical benchmark tasks across multiple evaluation cycles to quantify impact of dynamic benchmark updates.