---
ver: rpa2
title: 'ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within
  Large Language Models'
arxiv_id: '2402.13516'
source_url: https://arxiv.org/abs/2402.13516
tags:
- sparsity
- activation
- arxiv
- prosparse
- regularization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ProSparse introduces a progressive sparsity regularization technique
  to enhance activation sparsity in LLMs without performance degradation. By substituting
  ReLU activation and gradually increasing L1 regularization along sine curves, ProSparse
  achieves state-of-the-art sparsity levels: 89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B,
  and 87.89% for MiniCPM-1B, while maintaining comparable performance to their original
  Swish-activated versions.'
---

# ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models

## Quick Facts
- arXiv ID: 2402.13516
- Source URL: https://arxiv.org/abs/2402.13516
- Reference count: 40
- ProSparse achieves state-of-the-art activation sparsity: 89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for MiniCPM-1B while maintaining comparable performance.

## Executive Summary
ProSparse introduces a progressive sparsity regularization technique to enhance activation sparsity in large language models without performance degradation. The method replaces non-ReLU activations with ReLU, applies multi-stage L1 regularization with sine curve scheduling, and uses FATReLU threshold shifting to achieve state-of-the-art sparsity levels. ProSparse demonstrates up to 4.52× inference speedup with PowerInfer and practical GPU acceleration, making it the most sparsely activated open-source LLaMA model to date.

## Method Summary
ProSparse operates in three steps: (1) replaces activation functions with ReLU to enable intrinsic sparsity, (2) applies progressive L1 regularization with factors increasing along multi-stage sine curves to enhance sparsity while avoiding performance degradation, and (3) shifts the ReLU threshold using FATReLU to prune less influential neurons. The method targets feed-forward networks in Transformer layers and achieves high sparsity through continual training on mixed datasets of language modeling and instruction tuning data.

## Key Results
- Achieves 89.32% activation sparsity for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for MiniCPM-1B
- Maintains comparable performance to original Swish-activated versions across multiple benchmarks
- Enables up to 4.52× inference speedup with PowerInfer and practical GPU acceleration with custom sparse operators
- Demonstrates superior sparsity-performance trade-off compared to existing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Progressive L1 regularization with smoothly increasing factors prevents radical activation distribution shifts and maintains model performance.
- Mechanism: By scheduling the L1 regularization factor along multi-stage sine curves with a warmup stage, the model gradually adapts to increasing sparsity constraints without sudden distribution changes.
- Core assumption: Smooth, incremental increases in regularization allow the model to adapt without performance degradation.
- Evidence anchors: [abstract] "This can enhance activation sparsity and mitigate performance degradation by avoiding radical shifts in activation distributions." [section 3.2] "Considering the potential performance degradation due to fixed regularization factors... we propose the progressive sparsity regularization, where the factor λ is carefully scheduled to gently increase in multiple stages." [corpus] Weak - related works discuss activation sparsity but don't specifically address progressive regularization strategies.
- Break condition: If the sine curve scheduling is too aggressive or stages are too short, the model cannot adapt and performance degrades.

### Mechanism 2
- Claim: FATReLU activation threshold shifting increases sparsity by pruning weakly-contributed neurons without significant performance loss.
- Mechanism: Shifting the ReLU threshold to a positive value t > 0 prunes neurons with low activation values, increasing sparsity while preserving model capacity.
- Core assumption: Neurons with activation values below threshold t contribute minimally to final outputs and can be pruned safely.
- Evidence anchors: [abstract] "The final step adopts FATReLU (Kurtz et al., 2020), shifting the ReLU activation threshold to a positive value. This prunes less influential neurons to improve sparsity." [section 3.2] "As long as t is properly chosen... FATReLU can increase sparsity with negligible losses (Zhang et al., 2024)." [corpus] Weak - corpus mentions FATReLU but lacks detailed performance analysis for this specific application.
- Break condition: If threshold t is set too high, significant performance degradation occurs as important neurons are pruned.

### Mechanism 3
- Claim: Activation function substitution from non-ReLU to ReLU enables intrinsic activation sparsity.
- Mechanism: Replacing activation functions like Swish or GELU with ReLU creates zero-valued outputs for negative inputs, enabling dynamic sparsity during inference.
- Core assumption: ReLU activation naturally creates zero elements in activation outputs, enabling sparsity without additional constraints.
- Evidence anchors: [abstract] "After substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization..." [section 3.2] "The first step is to replace the activation function with ReLU and then apply continual training." [corpus] Strong - corpus contains multiple papers discussing ReLU-based activation sparsity in LLMs.
- Break condition: If the model cannot adapt to ReLU activation during continual training, performance drops significantly.

## Foundational Learning

- Concept: Activation sparsity in neural networks
  - Why needed here: Understanding how and why activation sparsity occurs is fundamental to ProSparse's approach
  - Quick check question: What is the difference between static and dynamic sparsity in neural networks?

- Concept: L1 regularization and its effects on neural network training
  - Why needed here: Progressive L1 regularization is the core technique for enhancing sparsity in ProSparse
  - Quick check question: How does L1 regularization differ from L2 regularization in terms of weight distribution?

- Concept: Transformer architecture and feed-forward networks
  - Why needed here: ProSparse operates specifically on FFN layers within Transformer-based LLMs
  - Quick check question: What are the computational steps in a gated feed-forward network within a Transformer layer?

## Architecture Onboarding

- Component map: Activation function substitution (ReLU replacement) -> Progressive sparsity regularization (multi-stage L1 with sine curve scheduling) -> Activation threshold shifting (FATReLU implementation)

- Critical path: The key computational path is through the FFN layers where activation sparsity is created and enhanced. The regularization factor scheduling must be properly implemented to avoid performance degradation.

- Design tradeoffs: Higher sparsity vs. performance - ProSparse must balance achieving high activation sparsity while maintaining model accuracy. The progressive approach trades some training efficiency for better final performance.

- Failure signatures: Poor performance indicates either insufficient adaptation to ReLU activation, overly aggressive regularization scheduling, or inappropriate threshold selection in FATReLU.

- First 3 experiments:
  1. Implement basic activation function substitution and measure initial sparsity levels
  2. Test different regularization factor scheduling approaches (fixed vs. progressive)
  3. Evaluate various FATReLU threshold values for optimal sparsity-performance tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ProSparse compare to other ReLU-based LLMs beyond LLaMA2 and MiniCPM?
- Basis in paper: [explicit] The paper only evaluates ProSparse on LLaMA2-7B, LLaMA2-13B, and MiniCPM-1B models.
- Why unresolved: The paper does not test ProSparse on other popular LLMs like Falcon or OPT.
- What evidence would resolve it: Benchmarking ProSparse on a wider range of LLMs to see if the performance gains hold.

### Open Question 2
- Question: What is the optimal scheduling strategy for the regularization factor λ beyond the sine curve approach used in ProSparse?
- Basis in paper: [inferred] The paper uses a sine curve to increase λ progressively but does not explore other scheduling methods.
- Why unresolved: The paper does not compare the sine curve to other potential scheduling strategies like linear or exponential increase.
- What evidence would resolve it: Comparing the performance of ProSparse with different λ scheduling strategies on the same models.

### Open Question 3
- Question: How does the sparsity distribution across layers impact the overall inference efficiency of ProSparse models?
- Basis in paper: [explicit] The paper observes that lower layers are denser than higher layers but does not quantify the impact on inference speed.
- Why unresolved: The paper does not measure the actual inference speedup per layer or analyze the load balancing implications.
- What evidence would resolve it: Profiling the inference time per layer for ProSparse models and analyzing the impact of sparsity imbalance on overall speedup.

## Limitations

- Critical implementation details such as exact training step numbers, peak regularization factor values, sine curve parameters, and FATReLU threshold values are not specified.
- Practical GPU acceleration results using custom sparse operators are not extensively validated across different hardware configurations.
- The claimed state-of-the-art status requires broader comparison with other methods under identical conditions.

## Confidence

**High confidence**: The core claim that ReLU activation enables intrinsic activation sparsity in LLMs is well-supported by established literature and the paper's experimental results. The general framework of progressive sparsity regularization with multi-stage sine curve scheduling is theoretically sound and aligns with known techniques for preventing catastrophic forgetting during training.

**Medium confidence**: The specific sparsity percentages achieved (89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for MiniCPM-1B) are likely reproducible given proper implementation, but depend heavily on the unspecified scheduling parameters. The claim of "state-of-the-art" sparsity levels requires broader comparison with other methods under identical conditions.

**Low confidence**: The practical acceleration claims (up to 4.52× speedup with PowerInfer and GPU-specific optimizations) are based on limited validation and may not generalize across different hardware, batch sizes, or workload patterns. The real-world deployment benefits are not fully quantified.

## Next Checks

1. **Parameter Sensitivity Analysis**: Systematically vary the sine curve parameters (amplitude, frequency, stage durations) and FATReLU threshold values to determine their impact on the sparsity-performance trade-off. Document the sensitivity of final results to these hyperparameters.

2. **Cross-Hardware Acceleration Validation**: Test the custom sparse operator implementations across multiple GPU architectures (different NVIDIA and AMD GPUs) and batch sizes to verify the claimed inference speedups are consistent and significant across diverse deployment scenarios.

3. **Generalization to Other Model Families**: Apply ProSparse to additional LLM architectures beyond LLaMA and MiniCPM (such as Falcon, MPT, or BLOOM) to assess whether the technique generalizes or if the results are specific to particular model families or training regimes.