---
ver: rpa2
title: 'PingPong: A Benchmark for Role-Playing Language Models with User Emulation
  and Multi-Model Evaluation'
arxiv_id: '2409.06820'
source_url: https://arxiv.org/abs/2409.06820
tags:
- language
- character
- claude
- player
- gpt-4o
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a benchmark for evaluating the role-playing
  capabilities of language models. The approach uses language models to simulate users
  and judge the resulting dialogues.
---

# PingPong: A Benchmark for Role-Playing Language Models with User Emulation and Multi-Model Evaluation

## Quick Facts
- arXiv ID: 2409.06820
- Source URL: https://arxiv.org/abs/2409.06820
- Reference count: 40
- Key outcome: Multi-model evaluation correlates better with human judgments than single models for role-playing assessment

## Executive Summary
This paper introduces PingPong, a benchmark designed to evaluate the role-playing capabilities of language models through simulated conversations. The approach uses three language models working in concert: a player model that adopts character roles, an interrogator model that simulates user behavior, and a judge model ensemble that evaluates conversation quality. The benchmark was validated by comparing automated evaluations with human annotations, demonstrating strong correlations across multiple criteria including character consistency, entertainment value, and language fluency.

## Method Summary
The method involves a three-component framework where language models simulate role-playing scenarios. A player model adopts character roles based on character cards, an interrogator model generates user questions dynamically (not predefined), and a judge model ensemble evaluates the resulting conversations across three metrics. The evaluation uses a 5-point Likert scale and aggregates scores from multiple judge models. Models participate in 64 conversations each, with 8 characters and 8 situations per language, using specific decoding parameters for each component role.

## Key Results
- Multi-model evaluation averaging scores from different judge models correlates better with human judgments than single models
- Fine-tuning models for creative writing improves their role-playing abilities, with Gemma 2 Ataraxy 9B showing consistently high rankings
- Dynamic question generation prevents test data contamination by producing different questions for each evaluation run

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-model evaluation reduces individual model bias and correlates better with human judgments
- Mechanism: Averaging scores from multiple judge models smooths out idiosyncratic scoring patterns and provides more reliable evaluation
- Core assumption: Different judge models have complementary biases that cancel out when averaged
- Evidence anchors: Correlations above 0