---
ver: rpa2
title: 'Analyzing Large language models chatbots: An experimental approach using a
  probability test'
arxiv_id: '2407.12862'
source_url: https://arxiv.org/abs/2407.12862
tags:
- chatbots
- more
- mary
- tests
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study examined how two LLMs, ChatGPT and Gemini, handle probability
  questions designed to trigger conjunction fallacy. The Linda and Mary Problems were
  used to test whether chatbots prioritize semantic descriptions or probabilistic
  reasoning.
---

# Analyzing Large language models chatbots: An experimental approach using a probability test

## Quick Facts
- **arXiv ID**: 2407.12862
- **Source URL**: https://arxiv.org/abs/2407.12862
- **Reference count**: 33
- **Primary result**: Examined how ChatGPT and Gemini handle probability questions designed to trigger conjunction fallacy

## Executive Summary
This study investigates how two large language models, ChatGPT and Gemini, handle probability questions that are designed to trigger conjunction fallacy. The researchers employed classic cognitive science problems - the Linda and Mary Problems - to test whether the chatbots prioritize semantic descriptions or probabilistic reasoning when ranking the likelihood of events. The experimental approach used randomized answer options across four test variants to systematically analyze model behavior.

## Method Summary
The study conducted 544 interactions with ChatGPT and Gemini using two classic conjunction fallacy problems: the Linda Problem and the Mary Problem. Four test variants were employed (LPSV, MPSV, LPEV, MPEV) with randomized answer options to test model responses under different semantic conditions. The primary objective was to measure accuracy in ranking single events as more probable than conjunctions, which would indicate proper probabilistic reasoning versus falling prey to the conjunction fallacy.

## Key Results
The results section appears incomplete in the available abstract. Based on the methodology described, the study would have measured whether ChatGPT and Gemini correctly identified that a single event (P(A)) is always more probable than its conjunction with another event (P(A and B)). Proper probabilistic reasoning requires recognizing that conjunction fallacy occurs when respondents incorrectly rank the conjunction as more likely than one of its constituent events. The specific findings regarding whether the models committed this fallacy or demonstrated correct probabilistic reasoning are not available in the abstract provided.

## Why This Works (Mechanism)
The Linda and Mary Problems are well-established tools in cognitive psychology for identifying conjunction fallacy - the tendency to believe that specific conditions are more probable than general ones. These problems work by presenting detailed, vivid descriptions that create strong semantic associations, making the conjunction seem more plausible than the single event alone, despite being mathematically impossible. The effectiveness of these problems lies in their ability to test whether cognitive systems (human or artificial) can override intuitive but incorrect probabilistic judgments when presented with rich contextual information.

## Foundational Learning
1. **Conjunction Fallacy**: Why needed - Core concept being tested; Quick check - Can you explain why P(A and B) must always be ≤ P(A)?
2. **Probability Theory**: Why needed - Framework for evaluating model responses; Quick check - Can you calculate probabilities for simple events and their conjunctions?
3. **Cognitive Biases**: Why needed - Understanding why conjunction fallacy occurs; Quick check - Can you identify other common reasoning biases?
4. **LLM Behavior Analysis**: Why needed - Context for interpreting model responses; Quick check - Can you describe how models might prioritize different types of information?

## Architecture Onboarding

**Component Map**: Problem Prompt -> Model Response -> Answer Ranking -> Accuracy Assessment

**Critical Path**: Test Problem → LLM Processing → Response Generation → Probability Ranking → Statistical Analysis

**Design Tradeoffs**: Semantic richness vs. probabilistic accuracy; multiple variants vs. sample size per variant

**Failure Signatures**: Conjunction fallacy committed when models rank conjunction as more probable than single event; correct probabilistic reasoning when single event is ranked higher

**Three First Experiments**:
1. Run a single Linda Problem with both models and manually analyze response patterns
2. Test one model with all four variants to check for consistency across semantic conditions
3. Vary the order of answer options to test for position bias in responses

## Open Questions the Paper Calls Out
None

## Limitations
- Results section is incomplete in the abstract, preventing full evaluation of findings
- Sample size distribution across models and test variants is not specified
- Lack of explicit hypothesis statements makes it difficult to assess experimental design alignment

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Experimental approach using Linda/Mary Problems is methodologically sound | Medium |
| All claims about actual model performance and findings | Low |
| Use of randomized answer options and multiple test variants | Medium |

## Next Checks
1. Obtain and analyze the complete results section to determine whether ChatGPT and Gemini committed conjunction fallacy or successfully applied probabilistic reasoning
2. Verify the distribution of responses across the four test variants and both models to ensure adequate sample sizes for statistical comparisons
3. Examine the specific prompts used for each variant (LPSV, MPSV, MPEV, MPEV) to understand how semantic descriptions were manipulated and whether this aligns with conjunction fallacy research methodology