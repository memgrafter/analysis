---
ver: rpa2
title: 'Through the Theory of Mind''s Eye: Reading Minds with Multimodal Video Large
  Language Models'
arxiv_id: '2406.13763'
source_url: https://arxiv.org/abs/2406.13763
tags:
- social
- reasoning
- video
- frames
- frame
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether multimodal large language models (LLMs)
  can exhibit human-like theory-of-mind (ToM) reasoning in video-based tasks. While
  prior work has shown LLMs can reason about mental states from text, real-world social
  reasoning is inherently multimodal and dynamic.
---

# Through the Theory of Mind's Eye: Reading Minds with Multimodal Video Large Language Models

## Quick Facts
- **arXiv ID:** 2406.13763
- **Source URL:** https://arxiv.org/abs/2406.13763
- **Reference count:** 9
- **Primary result:** Video-ChatGPT finetuned on ToMLoc achieves 77.97% frame localization accuracy and 47.15% QA accuracy on Social-IQ 2.0

## Executive Summary
This paper investigates whether multimodal large language models can exhibit human-like theory-of-mind (ToM) reasoning in video-based tasks. While previous research has demonstrated LLMs' ability to reason about mental states from text, real-world social reasoning is inherently multimodal and dynamic. The authors introduce Video Theory of Mind (VToM), a pipeline that combines video and text to localize and reason about mental states in video frames. They propose the Theory of Mind Localization (ToMLoc) task and release a benchmark dataset for this purpose. Experiments show that finetuning Video-ChatGPT on ToMLoc significantly improves both frame localization and question-answering accuracy compared to baselines, demonstrating the potential of video-based approaches for achieving more human-like social intelligence in LLMs.

## Method Summary
The authors propose a pipeline called Video Theory of Mind (VToM) that leverages both video and text modalities to perform theory-of-mind reasoning. The core innovation is the Theory of Mind Localization (ToMLoc) task, which requires models to identify specific video frames containing crucial mental-state cues. The VToM pipeline processes video frames and accompanying text descriptions, localizing mental-state indicators before performing reasoning tasks. The approach was evaluated on Social-IQ 2.0, a benchmark dataset that was augmented with ToMLoc annotations. The model was finetuned on this augmented dataset, enabling it to better ground its inferences in visual evidence. The evaluation measured both frame localization accuracy and question-answering accuracy, with the multimodal approach showing significant improvements over text-only baselines.

## Key Results
- Video-ChatGPT finetuned on ToMLoc achieves 77.97% frame localization accuracy and 47.15% QA accuracy on Social-IQ 2.0
- Finetuning improves performance by localizing crucial mental-state cues in video frames
- The approach demonstrates better interpretability by revealing how the model grounds its inferences in visual evidence

## Why This Works (Mechanism)
The VToM approach works by bridging the gap between textual and visual modalities in theory-of-mind reasoning. By localizing mental-state cues in specific video frames, the model can ground its inferences in concrete visual evidence rather than relying solely on text descriptions. This multimodal grounding allows the model to capture dynamic, context-dependent mental states that are difficult to represent in text alone. The ToMLoc task specifically trains the model to identify which visual information is relevant for ToM reasoning, effectively teaching it to "look" for mental-state indicators in the same way humans do when watching social interactions unfold.

## Foundational Learning
- **Theory of Mind (ToM)**: The ability to attribute mental states to others and understand that others have beliefs, desires, and intentions different from one's own. Needed to define the target capability; quick check: can humans distinguish between their own and others' mental states?
- **Multimodal Learning**: Integration of information from multiple sources (text, video, audio) to form unified representations. Needed to handle the complexity of real-world social interactions; quick check: does the model effectively combine visual and textual cues?
- **Frame Localization**: Identifying specific moments in a video sequence that contain relevant information. Needed to ground mental-state reasoning in concrete visual evidence; quick check: are the localized frames truly containing crucial mental-state cues?
- **Fine-tuning**: Adapting a pre-trained model to a specific task using task-specific data. Needed to adapt Video-ChatGPT to the ToMLoc task; quick check: does fine-tuning improve performance on both localization and reasoning tasks?

## Architecture Onboarding

**Component Map:**
Video Input -> Frame Extractor -> Visual Encoder -> Fusion Module -> Text Encoder -> Reasoning Module -> Output

**Critical Path:**
Video frames are extracted and encoded, then fused with text embeddings. The combined representation is processed by the reasoning module to produce ToM inferences. The critical path for ToMLoc specifically involves the Visual Encoder and Fusion Module, as accurate frame localization depends on properly encoding visual features and integrating them with textual context.

**Design Tradeoffs:**
The authors chose to fine-tune an existing multimodal LLM (Video-ChatGPT) rather than training from scratch, trading computational efficiency for potential limitations in the base model's architecture. They prioritized frame localization as an intermediate task to improve interpretability, though this adds complexity compared to direct reasoning approaches. The decision to focus on Social-IQ 2.0 provides a controlled evaluation environment but may limit generalizability to more diverse real-world scenarios.

**Failure Signatures:**
- Poor frame localization accuracy indicates the model fails to identify relevant visual cues for ToM reasoning
- High localization accuracy but low QA accuracy suggests the model can find relevant frames but struggles to extract mental-state information
- Performance degradation on out-of-distribution videos indicates limited generalizability
- Failure to align visual and textual modalities in the fusion module results in incoherent reasoning

**Three First Experiments:**
1. Ablation study removing the visual modality to quantify the contribution of video information to ToM reasoning performance
2. Human evaluation of frame localization to establish baseline performance and validate the quality of localized frames
3. Cross-dataset evaluation using videos from different sources to assess model generalizability beyond Social-IQ 2.0

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is primarily focused on Social-IQ 2.0, which may not fully represent the complexity of real-world ToM scenarios
- The extent to which the 1,499 questions across 12 categories capture the full spectrum of human ToM reasoning remains unclear
- Lack of human performance baselines makes it difficult to assess the practical significance of the reported accuracy improvements
- The ToMLoc task assumes mental-state cues can be localized to specific frames, which may not align with continuous human processing of mental states

## Confidence
**High confidence:** The core methodology of combining video and text modalities for ToM reasoning is technically sound and well-implemented. The introduction of the ToMLoc task and benchmark dataset represents a meaningful contribution to the field.

**Medium confidence:** The claim that bridging textual and visual modalities is essential for capturing complex mental states is supported by the results but would benefit from broader validation across multiple datasets and real-world scenarios.

**Low confidence:** The assertion that this approach achieves "more human-like social intelligence" lacks sufficient comparative evidence against human performance and other established benchmarks in social cognition research.

## Next Checks
1. Conduct human performance baselines on the ToMLoc task to establish meaningful comparison metrics and better understand the gap between current model performance and human capabilities.

2. Expand evaluation to include diverse real-world video datasets beyond Social-IQ 2.0 to assess generalizability and robustness of the VToM approach across different contexts and scenarios.

3. Implement ablation studies to quantify the specific contribution of video information versus text alone in ToM reasoning tasks, helping to clarify the true value added by the multimodal approach.