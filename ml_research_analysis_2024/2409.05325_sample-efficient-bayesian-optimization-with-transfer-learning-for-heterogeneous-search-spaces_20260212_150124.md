---
ver: rpa2
title: Sample-Efficient Bayesian Optimization with Transfer Learning for Heterogeneous
  Search Spaces
arxiv_id: '2409.05325'
source_url: https://arxiv.org/abs/2409.05325
tags:
- search
- learning
- optimization
- parameters
- spaces
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses Bayesian optimization (BO) with transfer
  learning when source and target tasks have different search spaces (heterogeneous
  spaces). The authors propose two methods: a conditional kernel-based approach that
  builds a tree-structured representation of common parameters across tasks and uses
  this to define task correlations, and a learned imputation approach that treats
  missing parameters as additional hyperparameters to be inferred.'
---

# Sample-Efficient Bayesian Optimization with Transfer Learning for Heterogeneous Search Spaces

## Quick Facts
- arXiv ID: 2409.05325
- Source URL: https://arxiv.org/abs/2409.05325
- Reference count: 28
- Primary result: Proposed methods enable BO transfer learning across heterogeneous search spaces, outperforming vanilla BO and random search

## Executive Summary
This paper addresses Bayesian optimization with transfer learning when source and target tasks have different search spaces. The authors propose two methods: a conditional kernel approach that builds a tree-structured representation of common parameters across tasks, and a learned imputation approach that treats missing parameters as additional hyperparameters. Both methods are implemented in BoTorch and evaluated on synthetic and real-world benchmark problems. The experiments show that both proposed methods outperform vanilla BO and random search, with the conditional kernel approach performing particularly well with limited source data, while learned imputation excels when more source data is available.

## Method Summary
The paper presents two approaches for transfer learning in Bayesian optimization when source and target tasks have different search spaces. The conditional kernel method leverages a Gaussian process model with a conditional kernel that compares inputs from different tasks based on their matching parameters in a dependency tree representing all search spaces. The learned imputation approach treats missing parameters (from the union of search spaces) as additional GP hyperparameters that can be inferred jointly with other GP hyperparameters or set to fixed values. Both methods naturally extend to standard transfer learning when search spaces are identical.

## Key Results
- Both conditional kernel and learned imputation methods outperform vanilla BO and random search
- Conditional kernel approach shows superior performance with limited source data (30 trials per source task)
- Learned imputation approach excels when more source data is available (100 trials per source task)
- Methods successfully handle real-world HPO-B benchmark problems with different search spaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditional kernel enables information transfer between tasks by leveraging common parameters
- Mechanism: The conditional kernel defines similarity between two inputs from different tasks by summing base kernels only over parameters common to both tasks, creating a tree-structured representation
- Core assumption: Parameters common to multiple tasks capture the underlying task correlation structure
- Break condition: If tasks have no common parameters, the conditional kernel provides no transfer capability

### Mechanism 2
- Claim: Treating missing parameters as hyperparameters allows the model to learn appropriate values through joint optimization
- Mechanism: Missing parameters from the union of search spaces are treated as additional GP hyperparameters that can be learned jointly or set to fixed values
- Core assumption: Missing parameters represent fixed but unknown values that can be inferred from data
- Break condition: If the search space difference is too large or the number of missing parameters is excessive

### Mechanism 3
- Claim: Both methods naturally extend to standard transfer learning when search spaces are identical
- Mechanism: When all tasks share the same search space, the conditional kernel becomes equivalent to standard kernels and learned imputation has no missing parameters
- Core assumption: The proposed methods are designed as generalizations of standard transfer learning approaches
- Break condition: If implementation doesn't properly handle the homogeneous case

## Foundational Learning

- Concept: Gaussian Process (GP) surrogate modeling
  - Why needed here: Both proposed methods rely on GP models as the probabilistic surrogate for black-box function optimization
  - Quick check question: What are the key components of a GP model and how does it handle uncertainty in function predictions?

- Concept: Bayesian Optimization (BO) framework
  - Why needed here: The paper applies transfer learning specifically within the BO context, where expensive function evaluations are optimized using acquisition functions
  - Quick check question: How does BO balance exploration and exploitation when selecting the next evaluation point?

- Concept: Multi-task Gaussian Process (MTGP) modeling
  - Why needed here: The proposed methods extend MTGP to handle heterogeneous search spaces by modifying how inputs from different tasks are compared
  - Quick check question: How do standard MTGP models handle task correlations and what changes when search spaces differ?

## Architecture Onboarding

- Component map: Search space parsing -> Tree-structured representation (conditional kernel) OR Parameter imputation module (learned imputation) -> Base kernel library -> Task correlation matrix -> GP model training -> Acquisition function optimization -> Evaluation loop

- Critical path: Search space parsing → Kernel construction → GP model training → Acquisition optimization → Evaluation loop

- Design tradeoffs:
  - Conditional kernel: No additional hyperparameters but requires careful tree construction
  - Learned imputation: More flexible but introduces computational overhead and risk of overfitting
  - Tree depth: Deeper trees capture more structure but increase computational complexity

- Failure signatures:
  - Poor performance when tasks share few common parameters
  - Slow convergence when learned imputation has too many missing parameters
  - Numerical instability in GP training with highly heterogeneous spaces

- First 3 experiments:
  1. Implement conditional kernel on synthetic example with known parameter overlap
  2. Test learned imputation on problem with small number of missing parameters
  3. Compare both methods on benchmark with moderate heterogeneity (e.g., Hartmann6 variant)

## Open Questions the Paper Calls Out

- How does the performance of conditional kernel-based methods scale with the number of source tasks and their heterogeneity?
- What is the theoretical regret bound for the conditional kernel approach in the heterogeneous transfer learning setting?
- How do the proposed methods perform when source tasks have partial but non-empty overlap in their search spaces?

## Limitations

- Performance degrades when tasks share minimal common parameters or when heterogeneity is extreme
- Experimental validation limited to synthetic benchmarks and two real-world HPO-B problems
- Learned imputation approach may overfit with insufficient data or excessive missing parameters

## Confidence

- High confidence: Fundamental mechanisms of both methods are theoretically sound
- Medium confidence: Experimental results showing superiority over baseline methods
- Low confidence: Performance guarantees in cases of extreme heterogeneity or very limited source data

## Next Checks

1. Test both methods on a real-world industrial optimization problem with known parameter overlap and measure wall-clock performance
2. Systematically vary the degree of search space heterogeneity to identify the breaking point for each method
3. Implement cross-validation on the source tasks to evaluate robustness of learned imputation hyperparameters