---
ver: rpa2
title: Improving Adversarial Training using Vulnerability-Aware Perturbation Budget
arxiv_id: '2403.04070'
source_url: https://arxiv.org/abs/2403.04070
tags:
- adversarial
- perturbation
- examples
- natural
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of improving adversarial training
  by challenging the use of uniform perturbation radii for crafting adversarial examples.
  It proposes two vulnerability-aware reweighting functions, MWPB and SDWPB, which
  assign varying perturbation radii to individual adversarial samples based on the
  vulnerability of their corresponding natural examples.
---

# Improving Adversarial Training using Vulnerability-Aware Perturbation Budget

## Quick Facts
- arXiv ID: 2403.04070
- Source URL: https://arxiv.org/abs/2403.04070
- Reference count: 40
- Primary result: Proposed vulnerability-aware perturbation budget methods improve adversarial training robustness by 2-3% across multiple attack scenarios

## Executive Summary
This paper addresses a fundamental limitation in adversarial training: the use of uniform perturbation radii when crafting adversarial examples. The authors propose two vulnerability-aware reweighting functions (MWPB and SDWPB) that assign varying perturbation budgets to individual samples based on their estimated vulnerability. By allocating larger perturbation radii to naturally robust samples and smaller radii to vulnerable ones, the methods consistently improve the performance of popular adversarial training methods across various datasets and attack types.

## Method Summary
The core contribution involves estimating sample vulnerability using either logit margins or a modified standard deviation measure of output logits, then using these estimates to assign sample-specific perturbation radii during adversarial example generation. The MWPB method uses maximum logit margin as the vulnerability metric, while SDWPB uses a modified standard deviation of logits. These vulnerability estimates dynamically adjust the perturbation budget for each sample during training, allowing more robust samples to be pushed further into adversarial regions while protecting vulnerable samples from excessive perturbation.

## Key Results
- MWPB-AT improved standard AT by +3.06% against PGD-20, +2.41% against CW, and +2.24% against Autoattack on CIFAR-10 with WRN-34-10
- Methods showed consistent improvements across CIFAR-10 and CIFAR-100 datasets
- Performance gains were observed against both white-box attacks (PGD, CW, Autoattack) and black-box attacks (Square, SPSA)

## Why This Works (Mechanism)
The method works by recognizing that uniform perturbation budgets are suboptimal because samples have varying inherent vulnerabilities. By tailoring the perturbation budget to each sample's vulnerability, the training process can more effectively explore the decision boundary. Robust samples can withstand larger perturbations, allowing them to be pushed further into adversarial regions where the model learns more robust features. Vulnerable samples receive smaller perturbations to prevent catastrophic degradation of their already fragile classification.

## Foundational Learning
- Adversarial training fundamentals: Understanding how adversarial examples are generated and used in training is crucial for grasping the vulnerability-aware approach
- Logit analysis: The methods rely on analyzing output logits to estimate sample vulnerability, requiring understanding of how logits relate to confidence and decision boundaries
- Perturbation budget concepts: Familiarity with L-infinity norm constraints and how they define the adversarial example generation space

## Architecture Onboarding
**Component map:** Natural examples -> Vulnerability estimation (MWPB/SDWPB) -> Sample-specific perturbation radius -> Adversarial example generation -> Model training

**Critical path:** Vulnerability estimation occurs during each training iteration and directly influences the perturbation budget used to generate adversarial examples for that batch.

**Design tradeoffs:** The main tradeoff is between computational overhead (estimating vulnerability metrics adds computation) and improved robustness. The methods also trade off between protecting vulnerable samples and fully exploring robust samples' decision boundaries.

**Failure signatures:** If vulnerability estimation is inaccurate, samples may receive inappropriate perturbation budgets, potentially harming training. Overly conservative budgets may limit robustness gains, while overly aggressive budgets may degrade vulnerable samples.

**First experiments:** 1) Compare vulnerability estimation accuracy against ground truth robustness measures, 2) Analyze computational overhead across different model sizes, 3) Test performance on a non-image dataset to assess domain generalization

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Computational overhead from vulnerability estimation may limit scalability to larger models or datasets
- Effectiveness against adaptive attacks specifically designed to exploit vulnerability-aware budgets remains untested
- Focus on image classification benchmarks raises questions about generalization to other domains or tasks

## Confidence
- Improvement claims over standard AT: **High**
- Vulnerability estimation methodology: **Medium**
- Scalability and computational efficiency: **Low**

## Next Checks
1. Benchmark computational overhead and training time compared to standard adversarial training across different model architectures
2. Test vulnerability-aware budgets against adaptive white-box attacks specifically designed to exploit the vulnerability estimation mechanism
3. Evaluate performance on non-image datasets (text, graphs, or speech) to assess domain generalization