---
ver: rpa2
title: Adapting Multilingual LLMs to Low-Resource Languages using Continued Pre-training
  and Synthetic Corpus
arxiv_id: '2410.14815'
source_url: https://arxiv.org/abs/2410.14815
tags:
- hindi
- english
- language
- arxiv
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting multilingual large
  language models to perform well in low-resource languages, specifically Hindi. The
  authors propose a continued pre-training approach using a mix of real and synthetic
  corpora to enhance model performance.
---

# Adapting Multilingual LLMs to Low-Resource Languages using Continued Pre-training and Synthetic Corpus

## Quick Facts
- arXiv ID: 2410.14815
- Source URL: https://arxiv.org/abs/2410.14815
- Authors: Raviraj Joshi; Kanishk Singla; Anusha Kamath; Raunak Kalani; Rakesh Paul; Utkarsh Vaidya; Sanjay Singh Chauhan; Niranjan Wartikar; Eileen Long
- Reference count: 7
- Key outcome: Nemotron-Mini-Hindi 4B achieves state-of-the-art Hindi performance while maintaining English competitiveness through continued pre-training on mixed real/synthetic corpora

## Executive Summary
This paper addresses the challenge of adapting multilingual LLMs to perform well in low-resource languages, specifically Hindi. The authors propose a continued pre-training approach using a mix of real and synthetic corpora to enhance model performance. They introduce Nemotron-Mini-Hindi 4B, a bilingual model supporting both Hindi and English, which is trained on 400B tokens with an equal mix of Hindi and English data. The model achieves state-of-the-art results on Hindi benchmarks while remaining competitive on English tasks. Additionally, the approach improves the model's factual accuracy and reduces hallucinations. An ablation study demonstrates that Hindi pre-training is crucial for strong performance on Hindi benchmarks, and cross-lingual transfer enhances both Hindi and English capabilities.

## Method Summary
The authors adapt Nemotron-Mini-4B (4.19B parameters) to Hindi through continued pre-training on a 400B token corpus with equal Hindi and English representation. The Hindi data includes 60B synthetic tokens from translation, 40B real tokens, and 120B transliterated tokens, while English data comes from the Nemotron-15B pre-training corpus. The model undergoes SFT using 200K English examples followed by DPO using 200K English and 60K synthetic Hindi examples. Training uses Megatron-LM and Nemo Aligner frameworks on NVIDIA A100 GPUs.

## Key Results
- Achieved state-of-the-art performance on Hindi benchmarks (IndicXTREME, IndicNLG, IndicQuest)
- Maintained competitive performance on English benchmarks (MMLU, Hella Swag, BoolQ, ARC)
- Demonstrated improved factual accuracy and reduced hallucinations compared to baseline models
- Ablation study showed Hindi pre-training is essential for strong Hindi benchmark performance

## Why This Works (Mechanism)

### Mechanism 1
Synthetic corpus generation via translation and transliteration improves low-resource language performance. High-quality English corpora are translated into Hindi and transliterated into Roman script, expanding token availability for continued pre-training. Core assumption: machine-translated data can be filtered for quality and used to effectively supplement scarce real target-language data. Break condition: if synthetic data quality cannot be sufficiently filtered, or if transliteration introduces significant noise, the benefits of expanded corpus size may not translate into improved model performance.

### Mechanism 2
Equal mixing of Hindi and English tokens during continued pre-training prevents catastrophic forgetting and enhances cross-lingual transfer. The model is trained on a balanced 400B token corpus (200B Hindi, 200B English), with higher sampling weight for real data to maintain English capabilities while improving Hindi. Core assumption: the base model's English knowledge is sufficiently robust to persist when supplemented with extensive target-language pre-training. Break condition: if the base English representation is not robust enough, or if Hindi data dominates the mix, English performance may degrade despite intentions to maintain it.

### Mechanism 3
Hindi pre-training is essential for strong Hindi benchmark performance; Hindi alignment alone is insufficient. Pre-training the model on Hindi data before alignment allows it to develop robust Hindi language representations, which alignment (SFT + DPO) then refines for instruction following. Core assumption: pre-training establishes foundational language understanding that alignment alone cannot provide for low-resource languages. Break condition: if the base model already has sufficient Hindi knowledge, or if alignment data is of extremely high quality, pre-training may offer diminishing returns.

## Foundational Learning

- Concept: Continued pre-training on target language data
  - Why needed here: The base multilingual model has limited exposure to Hindi (20B out of 8T tokens), so further pre-training is required to improve Hindi fluency and accuracy.
  - Quick check question: If the model only underwent SFT/DPO without continued pre-training, would Hindi performance match the results shown in the paper?

- Concept: Cross-lingual transfer in language models
  - Why needed here: Improvements in Hindi performance also benefit English factuality, indicating the model is learning transferable linguistic and factual knowledge.
  - Quick check question: How does the model's factual accuracy on English benchmarks change after Hindi pre-training, compared to a model only aligned in English?

- Concept: Synthetic data generation and quality filtering
  - Why needed here: Limited real Hindi data necessitates the creation of synthetic data, but noisy translations can harm model quality if not filtered.
  - Quick check question: What criteria and methods are used to filter out low-quality machine-translated data before including it in the training corpus?

## Architecture Onboarding

- Component map: Nemotron-Mini-4B (4.19B parameters) -> Continued pre-training (400B tokens) -> SFT (200K English examples) -> DPO (200K English + 60K Hindi examples) -> Evaluation
- Critical path: Real + synthetic Hindi + English data → Continued pre-training (400B tokens) → SFT (200k examples, 1 epoch) → DPO (260k examples, 1 epoch) → Evaluation
- Design tradeoffs: Using synthetic data expands coverage but risks introducing noise; balanced token mixing prevents forgetting but may slow Hindi-specific gains; skipping tokenizer extension leverages existing large vocabulary but may miss Hindi-specific subwords.
- Failure signatures: Hindi performance lags despite pre-training (likely synthetic data noise); English performance drops (catastrophic forgetting); alignment does not improve Hindi beyond pre-training (insufficient or poor-quality alignment data).
- First 3 experiments:
  1. Run continued pre-training with only real Hindi data to measure synthetic data contribution.
  2. Vary the Hindi/English token ratio (e.g., 70/30, 50/50, 30/70) to find optimal balance.
  3. Apply Hindi SFT+DPO to the base model (without pre-training) to isolate the effect of pre-training versus alignment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal mix ratio of real versus synthetic Hindi data for continued pre-training to maximize performance while minimizing hallucinations?
- Basis in paper: [explicit] The authors mention that during batch sampling, "greater weight is given to real data compared to synthetic data," but do not specify the optimal ratio or provide detailed ablation studies on this aspect.
- Why unresolved: The paper focuses on the overall effectiveness of using synthetic data but doesn't explore different ratios or their impact on model performance and hallucination rates in depth.
- What evidence would resolve it: Conducting systematic experiments varying the proportion of real to synthetic Hindi data in the pre-training corpus and measuring performance metrics and hallucination rates for each ratio.

### Open Question 2
- Question: How does the effectiveness of continued pre-training with synthetic data compare to other adaptation methods like adapter-based approaches for low-resource languages?
- Basis in paper: [inferred] The paper focuses on continued pre-training but mentions related work on adapter-based approaches (Gurgurov et al., 2024) without directly comparing their effectiveness.
- Why unresolved: While the paper demonstrates success with their approach, it doesn't benchmark against alternative methods that might offer different trade-offs in terms of computational efficiency or performance.
- What evidence would resolve it: Comparative studies implementing adapter-based approaches for Hindi adaptation and evaluating their performance against the continued pre-training method across the same benchmarks.

### Open Question 3
- Question: Can the cross-lingual transfer benefits observed from Hindi pre-training be generalized to other low-resource language pairs, and what factors influence the strength of this transfer?
- Basis in paper: [explicit] The authors observe that "Hindi pretraining leads to substantial improvements not only in Hindi understanding but also in factual accuracy and English performance, demonstrating effective cross-lingual transfer."
- Why unresolved: The paper only studies Hindi-English transfer and doesn't investigate whether this phenomenon extends to other language pairs or what characteristics of the language pair affect transfer strength.
- What evidence would resolve it: Experiments applying the same methodology to other low-resource language pairs (e.g., Tamil-English, Bengali-English) and analyzing factors like linguistic similarity, script differences, and corpus size that influence cross-lingual transfer effectiveness.

## Limitations

- The quality filtering criteria for synthetic data are not fully specified, with only general mention of perplexity-based filtering without exact thresholds or validation procedures.
- The model's Hindi performance heavily depends on the quality of translated and transliterated data, but no detailed analysis of synthetic data quality is provided.
- Limited Hindi alignment data (60K examples) compared to English (200K) raises questions about whether Hindi improvements are truly from pre-training rather than data quantity effects.

## Confidence

- **High Confidence**: The core methodology of continued pre-training with mixed real/synthetic corpora is sound and follows established practices in the field. The architectural choices (4.19B parameters, Megatron-LM training) are well-specified and reproducible.
- **Medium Confidence**: The claim that Hindi pre-training is essential for strong Hindi performance is supported by ablation studies, but the evidence could be stronger with more comprehensive comparisons. The catastrophic forgetting prevention mechanism is theoretically sound but not empirically validated.
- **Low Confidence**: The synthetic data quality and its actual contribution to performance improvements are not rigorously validated. The specific filtering criteria and quality assessment methods remain unclear, making it difficult to assess the reliability of the 60B synthetic tokens.

## Next Checks

1. **Synthetic Data Quality Audit**: Conduct a comprehensive analysis of the synthetic Hindi corpus quality by comparing perplexity scores of machine-translated vs human-translated samples, and measuring the impact of different filtering thresholds on final model performance.

2. **Pre-training vs Alignment Isolation**: Train a control model with only Hindi alignment (SFT+DPO) but no continued pre-training, using the same 60K Hindi examples, to definitively establish whether the pre-training is genuinely necessary.

3. **Token Ratio Sensitivity Analysis**: Systematically vary the Hindi/English token ratio (0%, 30%, 50%, 70%, 100% Hindi) during continued pre-training to identify the optimal balance point where Hindi performance peaks while English capabilities are maintained.