---
ver: rpa2
title: 'DreamStory: Open-Domain Story Visualization by LLM-Guided Multi-Subject Consistent
  Diffusion'
arxiv_id: '2407.12899'
source_url: https://arxiv.org/abs/2407.12899
tags:
- subject
- subjects
- image
- story
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DreamStory, an automatic open-domain story
  visualization framework that leverages LLMs and a novel multi-subject consistent
  diffusion model. The core method involves using LLMs to generate descriptive prompts
  for subjects and scenes, creating subject portraits as multimodal anchors, and employing
  a Multi-Subject consistent Diffusion model (MSD) with Masked Mutual Self-Attention
  (MMSA) and Masked Mutual Cross-Attention (MMCA) modules to maintain subject consistency.
---

# DreamStory: Open-Domain Story Visualization by LLM-Guided Multi-Subject Consistent Diffusion

## Quick Facts
- **arXiv ID**: 2407.12899
- **Source URL**: https://arxiv.org/abs/2407.12899
- **Reference count**: 40
- **Primary result**: DreamStory achieves strong multi-subject consistency in open-domain story visualization through LLM-guided prompt generation and a novel MSD model with MMSA and MMCA modules.

## Executive Summary
DreamStory introduces a novel approach to open-domain story visualization that addresses the challenge of maintaining multi-subject consistency across generated images. The framework leverages LLMs to generate descriptive prompts and employs a Multi-Subject consistent Diffusion model (MSD) with Masked Mutual Self-Attention (MMSA) and Masked Mutual Cross-Attention (MMCA) modules. The system demonstrates strong performance in both subjective and objective evaluations, including a newly constructed DS-500 benchmark.

## Method Summary
DreamStory employs LLMs to generate descriptive prompts for subjects and scenes, creating subject portraits as multimodal anchors. The core innovation is the Multi-Subject consistent Diffusion model (MSD) with MMSA and MMCA modules that maintain appearance and semantic consistency while preventing subject blending through masking mechanisms. The approach is training-free, relying on the capabilities of existing models while introducing novel attention mechanisms to handle multi-subject consistency challenges.

## Key Results
- DreamStory achieves strong subject consistency across story scenes using LLM-guided prompt generation
- The MMSA module ensures detailed appearance consistency through masked self-attention
- The MMCA module captures subject attributes from text to ensure semantic consistency
- DreamStory outperforms baselines on both subjective (aesthetic) and objective (DreamSim, D&C-DS) metrics

## Why This Works (Mechanism)

### Mechanism 1
MMSA ensures appearance consistency by restricting each subject's tokens to reference only their corresponding subject in reference images. The module constructs subject masks for each reference image, concatenates them with target image features, and applies masks to logits during self-attention computation, nullifying attention scores between different subjects.

### Mechanism 2
MMCA captures subject attributes from text to ensure semantic consistency by allowing each subject in the target image to query its corresponding reference text embedding. A subject mask ensures that only the subject area of the target image attends to the corresponding reference text, injecting rich attribute information.

### Mechanism 3
LLM-based prompt generation and rewriting improve prompt quality for the diffusion model. The LLM generates detailed prompts, annotates subjects in each scene, and rewrites scene prompts to replace names with descriptive attributes, ensuring the diffusion model can understand subjects and generate consistent images.

## Foundational Learning

- **Concept**: Diffusion models
  - Why needed here: DreamStory is built upon diffusion models for image generation based on text prompts
  - Quick check question: What is the core idea behind diffusion models, and how do they generate images?

- **Concept**: Attention mechanisms
  - Why needed here: MMSA and MMCA are based on attention mechanisms that ensure subject consistency
  - Quick check question: How do self-attention and cross-attention mechanisms work in diffusion models?

- **Concept**: Large Language Models (LLMs)
  - Why needed here: LLMs generate prompts for subjects and scenes that guide the diffusion model
  - Quick check question: What are the key capabilities of LLMs, and how can they generate effective prompts for diffusion models?

## Architecture Onboarding

- **Component map**: LLM (story director) -> T2I model -> Multi-Subject consistent Diffusion model (MSD)
- **Critical path**: LLM -> T2I model -> MSD
- **Design tradeoffs**: LLM prompt generation adds complexity but improves quality; MMSA/MMCA add complexity but ensure consistency; training-free approach avoids training costs but relies on existing models
- **Failure signatures**: Inconsistent subjects (masks inaccurate or LLM fails), incorrect attributes (insufficient text information), poor image quality (diffusion model struggles)
- **First 3 experiments**:
  1. Test accuracy of subject masks generated by GroundingSAM
  2. Evaluate effectiveness of LLM rewriting by comparing images with/without rewriting
  3. Assess impact of MMSA and MMCA modules by comparing images with/without these modules

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of DreamStory's prompt generation and rewriting strategy compare when using different LLM architectures (e.g., ChatGPT, LLaMA, Yi) for stories with varying levels of complexity and subject diversity?

### Open Question 2
What is the optimal balance between self-attention and cross-attention in the MSD model for maintaining subject consistency while preserving background semantics in multi-subject story visualization?

### Open Question 3
How does DreamStory's performance scale with increasing numbers of subjects (beyond 3) in terms of subject consistency, aesthetic quality, and computational efficiency?

## Limitations
- The approach relies on accurate subject masks from segmentation models, which may fail with complex scenes
- LLM-based evaluation metrics may not fully capture human perception of subject consistency
- Performance with more than 3 subjects has not been extensively tested

## Confidence

- **High confidence**: MMSA and MMCA modules effectively ensure subject and semantic consistency through masking mechanisms
- **Medium confidence**: LLM-guided prompt generation and rewriting improves image quality and consistency
- **Medium confidence**: Training-free approach is effective for open-domain story visualization
- **Low confidence**: LLM-based evaluation metrics fully capture subject consistency quality

## Next Checks

1. Conduct a user study to evaluate the accuracy of subject masks generated by GroundingSAM across diverse story scenarios, particularly for complex scenes with overlapping subjects.

2. Test the framework's performance using different LLMs (e.g., GPT-3.5, LLaMA2) to assess the robustness of the prompt generation and rewriting process.

3. Evaluate subject consistency across extended story sequences (e.g., 20+ frames) to assess whether the current approach maintains consistency over longer narratives.