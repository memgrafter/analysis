---
ver: rpa2
title: 'Prometheus 2: An Open Source Language Model Specialized in Evaluating Other
  Language Models'
arxiv_id: '2405.01535'
source_url: https://arxiv.org/abs/2405.01535
tags:
- evaluation
- evaluator
- ranking
- assessment
- pairwise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Prometheus 2, a set of open-source language
  models specialized in evaluating other language models. The key innovation is a
  novel weight merging approach that combines models trained on direct assessment
  and pairwise ranking formats, enabling a unified evaluator that excels in both schemes.
---

# Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models

## Quick Facts
- arXiv ID: 2405.01535
- Source URL: https://arxiv.org/abs/2405.01535
- Reference count: 38
- Primary result: Prometheus 2 achieves highest correlation with human and proprietary LM judges among open evaluator LMs, with Pearson correlations surpassing baselines by 0.2 units

## Executive Summary
Prometheus 2 introduces a novel approach to language model evaluation by creating open-source evaluator models that outperform existing baselines in both direct assessment and pairwise ranking formats. The key innovation is a weight merging technique that combines models trained on different evaluation paradigms, enabling a unified evaluator that maintains strong performance across both schemes. The resulting models achieve state-of-the-art correlation with human judgments while being fully open-source and accessible to the research community.

## Method Summary
The approach involves fine-tuning base models (Mistral-7B and Mixtral-8x7B) separately on direct assessment and pairwise ranking datasets, then merging their weights using linear or DARE-based methods. The weight merging allows the model to inherit strengths from both evaluation paradigms while avoiding the negative transfer that occurs in joint training. The method is validated on eight benchmarks, showing significant improvements over both jointly trained models and single-format models, with optimal performance achieved at different merging weights for different evaluation tasks.

## Key Results
- Pearson correlations surpassing baselines by 0.2 units across four direct assessment benchmarks
- Performance gap with GPT-4 reduced by half on pairwise ranking benchmarks
- Optimal merging weight α=0.5 for direct assessment and α=0.3 for pairwise ranking
- Reference-based evaluation shows significantly better performance than reference-free evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weight merging of models trained on different evaluation formats creates a unified evaluator that outperforms both single-format and jointly trained models.
- Mechanism: Linear combination of parameters from separately trained models preserves task-specific expertise while enabling cross-format evaluation capabilities.
- Core assumption: Direct assessment and pairwise ranking are complementary tasks that can be effectively combined through parameter averaging.
- Evidence anchors: Weight merging yields an evaluator that outperforms jointly trained or single-format models; weight-merged models show positive task transfer.

### Mechanism 2
- Claim: Training on pairwise ranking data improves direct assessment performance more than the reverse.
- Mechanism: Pairwise ranking forces the model to learn relative quality distinctions, which transfers well to absolute scoring because understanding relative differences is foundational to absolute quality assessment.
- Core assumption: The ability to compare two responses and determine which is better provides stronger signal for understanding quality gradients than learning absolute scores in isolation.
- Evidence anchors: Performance is optimal when α=0.3 for pairwise ranking and α=0.5 for direct assessment, implying asymmetric transfer benefits.

### Mechanism 3
- Claim: Including reference answers as input significantly improves evaluation performance compared to reference-free evaluation.
- Mechanism: Reference answers provide grounding for the evaluator, allowing it to assess how well responses address the specific question rather than evaluating based on general quality metrics alone.
- Core assumption: Without a reference, the model lacks context about what constitutes a correct or complete answer to the instruction.
- Evidence anchors: Correlation with humans diminishes when the reference answer is discarded; reference inclusion is crucial for effective evaluations.

## Foundational Learning

- Concept: Linear weight merging in neural networks
  - Why needed here: The core innovation relies on combining model parameters from different training regimes through weighted averaging
  - Quick check question: What happens to the dimensionality and parameter count when you merge two models of the same architecture using linear merging?

- Concept: Correlation metrics (Pearson, Spearman, Kendall-Tau)
  - Why needed here: The evaluation of evaluator models requires understanding how to measure agreement between model scores and human judgments
  - Quick check question: When would Spearman correlation be more appropriate than Pearson correlation for evaluating model outputs?

- Concept: Task transfer and negative transfer in multi-task learning
  - Why needed here: Understanding why joint training fails while weight merging succeeds requires knowledge of how different tasks interact during training
  - Quick check question: What conditions typically lead to negative transfer when training a single model on multiple tasks?

## Architecture Onboarding

- Component map: Base models (Mistral-7B, Mixtral-8x7B) → Task-specific fine-tuning → Weight merging → Prometheus 2 models → Multiple benchmarks (Vicuna Bench, MT Bench, FLASK, Feedback Bench, HHH Alignment, MT Bench Human Judgment, Auto-J Eval, Preference Bench)

- Critical path: 1. Fine-tune base model on direct assessment data, 2. Fine-tune base model on pairwise ranking data, 3. Merge weights using optimal merging strategy (DARE-Linear), 4. Evaluate on all benchmarks, 5. Iterate with different α values for optimal performance

- Design tradeoffs: Joint training vs weight merging (joint training causes negative transfer, weight merging preserves task-specific expertise), Model size vs performance (larger base models achieve better consistency but require more resources), Reference-based vs reference-free (reference-based performs better but requires reference answers)

- Failure signatures: Low correlation with human judgments across all benchmarks, Inconsistent scoring across multiple inference runs (high variance), Poor transitivity in pairwise ranking (A > B, B > C, but C > A), Degraded performance when switching between evaluation formats

- First 3 experiments: 1. Train Mistral-7B on direct assessment only, evaluate on Vicuna Bench and MT Bench, 2. Train Mistral-7B on pairwise ranking only, evaluate on HHH Alignment and MT Bench Human Judgment, 3. Merge the two models using linear merging with α=0.5, evaluate on all benchmarks to verify improvement over single-format models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Prometheus 2 models vary across different merging methods, and what are the theoretical justifications for the observed differences?
- Basis in paper: The paper discusses various merging methods and their impact on performance, noting that fundamentally explaining why weight merging works well remains challenging.
- Why unresolved: The paper mentions optimal merging methods might differ and future work could explore whether findings hold true, but lacks comprehensive theoretical analysis.
- What evidence would resolve it: A comprehensive theoretical analysis of each merging method, comparing their mathematical properties and how they affect the evaluator LM's performance in different evaluation formats.

### Open Question 2
- Question: To what extent can evaluator LMs trained in one evaluation format generalize to handle evaluations in a new format when weight merging is employed?
- Basis in paper: The paper mentions that open-source LMs cannot produce good evaluation results without training, and conversely, if trained in one or two formats, they lose flexibility to conduct different evaluations.
- Why unresolved: The paper does not provide experimental results on training evaluator LMs in one format and then merging them with models trained in another format to see if the resulting model can handle the new format.
- What evidence would resolve it: Experimental results showing the performance of evaluator LMs trained in one format and then merged with models trained in another format, tested on the new format.

### Open Question 3
- Question: How does the inclusion of a reference answer impact the evaluation performance of evaluator LMs in both direct assessment and pairwise ranking formats?
- Basis in paper: The paper mentions that including a reference answer is crucial for conducting effective evaluations with LMs, as shown by the degradation in performance when the reference answer is discarded.
- Why unresolved: The paper does not provide a detailed analysis of how the reference answer impacts performance in both evaluation formats or explore the mechanisms by which the reference answer improves evaluation.
- What evidence would resolve it: A detailed study comparing the performance of evaluator LMs with and without reference answers in both direct assessment and pairwise ranking formats, along with an analysis of how the reference answer influences the evaluation process.

## Limitations
- Weight merging introduces limitations in optimal merging weight determination and assumes linear superposition of parameter spaces
- Reference-based evaluation requires access to ground truth answers, limiting applicability to tasks with definitive correct responses
- Current evaluation focuses primarily on English language tasks, with multilingual capabilities mentioned for future work but not yet validated

## Confidence

- **High Confidence**: The weight merging mechanism and its superiority over joint training
- **Medium Confidence**: The bidirectional task transfer improvements
- **Medium Confidence**: The necessity of reference answers for optimal performance

## Next Checks

1. **Cross-linguistic generalization test**: Evaluate Prometheus 2 on multilingual benchmarks to verify the claimed future multilingual capabilities and identify any language-specific biases in the evaluation criteria or model performance.

2. **Reference-free evaluation robustness**: Systematically test the model's performance across different types of open-ended tasks where reference answers are unavailable, measuring how performance degrades and whether fine-tuning on reference-free data can mitigate this limitation.

3. **Ablation study on merging weights**: Conduct a more granular analysis of the α parameter space, testing intermediate values between 0.3 and 0.5, and across different base model architectures to establish whether the observed optimal values are robust or task-specific.