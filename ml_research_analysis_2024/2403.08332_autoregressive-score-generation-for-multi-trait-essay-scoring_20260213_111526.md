---
ver: rpa2
title: Autoregressive Score Generation for Multi-trait Essay Scoring
arxiv_id: '2403.08332'
source_url: https://arxiv.org/abs/2403.08332
tags:
- trait
- scoring
- traits
- essay
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an autoregressive multi-trait scoring method
  (ArTS) for automated essay scoring by leveraging pre-trained T5 models. Unlike existing
  approaches that replicate separate trait-specific models, ArTS generates all trait
  scores as a sequence, allowing subsequent trait predictions to benefit from prior
  scores via causal self-attention.
---

# Autoregressive Score Generation for Multi-trait Essay Scoring

## Quick Facts
- arXiv ID: 2403.08332
- Source URL: https://arxiv.org/abs/2403.08332
- Authors: Heejin Do; Yunsu Kim; Gary Geunbae Lee
- Reference count: 8
- Primary result: ArTS achieves over 5% average improvement in QWK scores compared to multi-task learning baseline

## Executive Summary
This paper introduces ArTS (Autoregressive Trait Scoring), a novel approach for automated essay scoring that treats multi-trait scoring as a text generation task using pre-trained T5 models. Unlike traditional methods that require separate models for each trait, ArTS generates all trait scores as a sequence, allowing later predictions to condition on earlier ones through causal self-attention. The method achieves significant improvements in scoring accuracy while using a single model for all traits and prompts, demonstrating both performance gains and training efficiency.

## Method Summary
The approach reformulates AES as a text generation task by adding a prompt prefix ("score the essay of the prompt N:") to each essay and generating a text sequence containing predicted trait scores. A pre-trained T5 encoder-decoder model is fine-tuned to produce these score sequences autoregressively, with causal self-attention allowing later trait predictions to attend to earlier ones. During inference, predicted scores are extracted from the generated text and evaluated using Quadratic Weighted Kappa (QWK) across five-fold cross-validation.

## Key Results
- ArTS achieves over 5% average improvement in QWK scores across both prompts and traits compared to multi-task learning baseline
- Single integrated model outperforms multiple trait-specific models in both accuracy and training efficiency
- Significant improvements observed on low-resource traits, demonstrating better generalization
- Performance gains are consistent across different T5 model sizes (base, small, large)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditioning later trait predictions on earlier ones improves scoring accuracy through causal self-attention.
- Mechanism: The autoregressive decoder in T5 allows subsequent trait score generation to attend to previously generated trait scores, capturing inter-trait dependencies.
- Core assumption: Trait scores are not independent; earlier traits provide useful context for predicting later ones.
- Evidence anchors:
  - [abstract] "During decoding, the subsequent trait prediction can benefit by conditioning on the preceding trait scores."
  - [section] "The causal self-attention of the transformer decoder enables subsequent trait-scoring tasks to attend to prior predicted trait scores"
  - [corpus] Weak - no explicit mention of inter-trait dependencies in related works.
- Break condition: If traits are truly independent or if the ordering is arbitrary, the benefit disappears.

### Mechanism 2
- Claim: Redefining AES as text generation rather than regression/classification improves performance.
- Mechanism: T5's pre-training on text-to-text tasks (including semantic textual similarity) makes it well-suited to generate structured score sequences.
- Core assumption: The model's text generation capabilities transfer effectively to structured score prediction.
- Evidence anchors:
  - [abstract] "we redefine AES as a score-generation task, allowing a single model to predict multiple scores."
  - [section] "T5 has achieved competitive performance in numerous natural-language processing tasks by handling various tasks using a text-to-text approach."
  - [corpus] Weak - no related work explicitly discusses this text-to-text shift for AES.
- Break condition: If the text generation objective doesn't align with scoring quality metrics.

### Mechanism 3
- Claim: Providing prompt context improves scoring accuracy.
- Mechanism: Adding "score the essay of the prompt N:" as a prefix gives the model clear task context and prompt-specific score ranges.
- Core assumption: The model benefits from explicit prompt identification for accurate score range mapping.
- Evidence anchors:
  - [abstract] "We hypothesize that providing the prompt number, N, allows more accurate guidance."
  - [section] "We hypothesize that providing the prompt number, N, allows more accurate guidance."
  - [corpus] Weak - no related work discusses prompt number guidance in detail.
- Break condition: If the model can infer prompt context from the essay content alone.

## Foundational Learning

- Concept: Quadratic Weighted Kappa (QWK)
  - Why needed here: QWK is the evaluation metric used in the paper to measure agreement between model predictions and human raters, accounting for distance between scores.
  - Quick check question: Why might QWK be preferred over simple accuracy for essay scoring tasks?

- Concept: Causal self-attention
  - Why needed here: This mechanism allows the autoregressive decoder to condition later predictions on earlier ones, which is central to the proposed approach.
  - Quick check question: How does causal self-attention differ from bidirectional attention in transformer models?

- Concept: Text-to-text pre-training
  - Why needed here: T5's pre-training on diverse text-to-text tasks (including semantic similarity) is leveraged for the score generation task.
  - Quick check question: What advantages might a text-to-text pre-trained model have over encoder-only models for structured prediction tasks?

## Architecture Onboarding

- Component map: T5 encoder-decoder model with prompt prefix input → sequential trait score generation output → post-processing to extract numeric scores
- Critical path: Input essay → prompt prefix → T5 encoding → autoregressive decoding → text output → trait score extraction → QWK evaluation
- Design tradeoffs: Single integrated model vs. multiple trait-specific models; autoregressive generation vs. parallel prediction; text generation vs. regression/classification
- Failure signatures: Out-of-range predictions, inconsistent trait ordering, poor performance on low-resource traits
- First 3 experiments:
  1. Compare ArTS with baseline MTL-BiLSTM on Overall score only to establish baseline performance
  2. Test different trait prediction orders to verify the hypothesized ordering effect
  3. Evaluate the impact of prompt number guidance by training with and without prompt prefix

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ArTS compare to BERT-based models when trained on the same dataset and evaluated on the same traits?
- Basis in paper: [explicit] The paper mentions that BERT-based models have not been used for multi-trait scoring due to resource inefficiency, and that ArTS achieves comparable results to BERT-based models on overall scoring while being more efficient.
- Why unresolved: The paper does not provide direct comparisons between ArTS and BERT-based models on multi-trait scoring tasks.
- What evidence would resolve it: A direct comparison of ArTS and BERT-based models on multi-trait scoring tasks, using the same dataset and evaluation metrics.

### Open Question 2
- Question: What is the impact of different trait prediction orders on the performance of ArTS?
- Basis in paper: [explicit] The paper mentions that the current trait prediction order is set from rare to frequent traits, but suggests that more effective ordering strategies could be explored.
- Why unresolved: The paper does not provide a comprehensive analysis of different trait prediction orders and their impact on performance.
- What evidence would resolve it: An empirical study comparing the performance of ArTS with different trait prediction orders, using the same dataset and evaluation metrics.

### Open Question 3
- Question: How does the performance of ArTS change when using different pre-trained language models, such as GPT-based models?
- Basis in paper: [explicit] The paper mentions that the current study only uses T5-based models and suggests that exploring other pre-trained models could shed more light on future AES.
- Why unresolved: The paper does not provide any experiments using other pre-trained language models, such as GPT-based models.
- What evidence would resolve it: An empirical study comparing the performance of ArTS with different pre-trained language models, using the same dataset and evaluation metrics.

## Limitations

- The paper lacks explicit validation of whether trait scores exhibit meaningful inter-dependencies that justify the autoregressive approach
- Score extraction process from generated text is underspecified, potentially impacting reproducibility
- No direct comparison with state-of-the-art large language models (LLMs) or specialized AES systems that have emerged recently
- Evaluation is limited to specific datasets without broader generalization testing across diverse essay types

## Confidence

**High Confidence**: The core methodology of reformulating AES as a text generation task using T5 is well-established and technically sound. The reported QWK improvements over the MTL baseline are plausible given T5's strong performance on text-to-text tasks.

**Medium Confidence**: The autoregressive conditioning mechanism likely provides benefits, but the magnitude of improvement (5% average) may be dataset-dependent. The assumption that prompt number guidance improves accuracy is reasonable but not rigorously tested through ablation.

**Low Confidence**: Claims about training efficiency benefits are weakly supported without explicit comparisons of training time or computational resources. The assertion that this approach generalizes well to low-resource traits lacks empirical validation.

## Next Checks

1. **Ablation on Trait Ordering**: Systematically test different trait prediction orders (random, reverse, domain-knowledge-based) to verify that the ordering effect is not arbitrary and that causal self-attention provides measurable benefits.

2. **Direct LLM Comparison**: Evaluate ArTS against zero-shot or few-shot prompting of large language models (e.g., GPT-4, Claude) on the same datasets to establish whether the autoregressive approach offers advantages beyond simply using a powerful pre-trained model.

3. **Score Extraction Robustness**: Implement and test multiple strategies for extracting numeric scores from generated text (exact match, regex patterns, confidence thresholding) to determine how sensitive the results are to this critical post-processing step.