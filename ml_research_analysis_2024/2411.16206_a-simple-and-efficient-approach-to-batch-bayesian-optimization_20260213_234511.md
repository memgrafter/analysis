---
ver: rpa2
title: A Simple and Efficient Approach to Batch Bayesian Optimization
arxiv_id: '2411.16206'
source_url: https://arxiv.org/abs/2411.16206
tags:
- essi
- batch
- optimization
- function
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a simple and efficient approach to batch Bayesian
  optimization for large-scale parallel evaluation. The key idea is to select multiple
  query points from different axis-aligned subspaces using the expected subspace improvement
  (ESSI) criterion, rather than from the original high-dimensional space.
---

# A Simple and Efficient Approach to Batch Bayesian Optimization

## Quick Facts
- arXiv ID: 2411.16206
- Source URL: https://arxiv.org/abs/2411.16206
- Reference count: 40
- Key outcome: ESSI achieves better performance than sequential BO and seven batch methods, especially with large batch sizes

## Executive Summary
This paper introduces the Expected SubSpace Improvement (ESSI) criterion for batch Bayesian optimization, which selects multiple query points from different axis-aligned subspaces rather than from the full high-dimensional space. By reducing the dimensionality of acquisition function optimization from d×q to approximately d/2 on average, ESSI significantly lowers computational costs while maintaining or improving solution quality. The method demonstrates superior performance across 58 benchmark problems compared to sequential optimization and seven state-of-the-art batch approaches.

## Method Summary
The ESSI approach draws q axis-aligned subspaces randomly from the d-dimensional space and optimizes a separate acquisition function in each subspace. For each subspace, a single point is selected that maximizes the improvement criterion, resulting in a batch of q points. The acquisition functions are optimized in parallel using a genetic algorithm, reducing total optimization time. The method uses Gaussian process regression with a constant mean function and squared exponential kernel, initialized with Latin hypercube sampling. Unlike existing batch methods that optimize a single q-dimensional acquisition function, ESSI optimizes q separate 1D to dD acquisition functions in parallel.

## Key Results
- ESSI outperforms sequential Bayesian optimization and seven batch methods on 58 CEC 2017 benchmark problems
- Computational time for acquisition optimization scales favorably with batch size due to parallel optimization
- ESSI maintains effectiveness with large batch sizes (up to 128) while other methods degrade
- The approach shows good scalability with problem dimension, reducing acquisition optimization dimensionality from d×q to approximately d/2 on average

## Why This Works (Mechanism)

### Mechanism 1
The ESSI approach reduces acquisition function dimensionality from d×q to approximately d/2 on average. By selecting q points from axis-aligned subspaces rather than the full d-dimensional space, each acquisition optimization only needs to search over s variables where s is drawn uniformly from 1 to d. This significantly reduces the computational complexity of finding optimal acquisition points.

### Mechanism 2
Parallel acquisition optimization across subspaces provides computational speedup beyond just parallel evaluation. The q acquisition optimizations are independent and can be distributed across q CPU cores, reducing total acquisition optimization time from sequential to parallel processing. This parallelization scales with batch size, providing increasing benefits as q grows.

### Mechanism 3
Random subspace selection avoids hyper-parameter tuning while maintaining performance. Instead of setting s (subspace dimension) explicitly, drawing s uniformly from 1 to d provides a distribution of subspace sizes that balances exploration and exploitation. This simplicity comes without sacrificing performance compared to more complex selection strategies.

## Foundational Learning

- **Gaussian Process Regression**: ESSI relies on GP mean and variance predictions to compute improvement in subspaces. Quick check: What happens to GP variance at observed points versus unexplored regions?
- **Expected Improvement (EI) acquisition function**: ESSI is a generalization of EI to subspaces; understanding EI is prerequisite to understanding ESSI. Quick check: How does the EI formula combine GP mean and standard deviation?
- **Axis-aligned subspace decomposition**: The core idea is to partition the d-dimensional space into subspaces where optimization is easier. Quick check: How many total axis-aligned subspaces exist in a d-dimensional space?

## Architecture Onboarding

- **Component map**: GP Model Training → Subspace Selection → Parallel ESSI Optimization → Batch Evaluation → Update Loop
- **Critical path**: GP training → ESSI optimization → evaluation → update
- **Design tradeoffs**: Random vs. structured subspace selection (simplicity vs. potential performance); Parallel vs. sequential acquisition optimization (speed vs. resource usage); Single vs. multiple GP models (consistency vs. diversity)
- **Failure signatures**: Poor convergence (random subspaces missing optimal regions); High computational cost (insufficient parallelization or large d); Unstable results (GP model uncertainty dominates subspace predictions)
- **First 3 experiments**: 1) Run ESSI with q=2 on a 10D Rosenbrock problem, compare convergence to sequential EI; 2) Measure acquisition optimization time for q=4,8,16 on a 30D problem; 3) Test different s distributions (uniform vs. truncated) on a benchmark function

## Open Questions the Paper Calls Out

### Open Question 1
How can the ESSI approach be extended to handle expensive constrained optimization problems? The authors conclude by stating that extending this idea to expensive constrained optimization might be interesting for future research.

### Open Question 2
What is the optimal strategy for subspace selection beyond random sampling? The authors note that random subspace selection is straightforward but acknowledge that more sophisticated strategies could potentially improve performance.

### Open Question 3
How does the ESSI approach scale with extremely high-dimensional problems (d > 100)? While the authors demonstrate effectiveness for d = 10 and d = 30, they don't test the method on problems with very high dimensions.

## Limitations

- Performance depends on random subspace selection, which may miss optimal regions if they lie in rarely selected subspaces
- The method's reliance on axis-aligned subspaces may underperform on problems with anisotropic correlations
- Computational benefits assume ideal parallelization conditions without accounting for coordination overhead

## Confidence

- **ESSI efficiency gains**: Medium - Theoretical reductions in dimensionality are clear, but empirical validation is limited to benchmark functions
- **Random subspace selection effectiveness**: Low - Performance is demonstrated but lacks theoretical justification for why uniform sampling preserves solution quality
- **Scalability claims**: Medium - Acquisition optimization time scales favorably, but parallelization benefits assume ideal conditions

## Next Checks

1. Test ESSI on problems with known low-dimensional structure versus high-dimensional irrelevant dimensions to verify the method's ability to identify informative subspaces
2. Implement and compare against optimized versions of the seven batch methods using identical GP training procedures to isolate ESSI's contribution
3. Measure actual wall-clock time including GP hyperparameter optimization and parallel coordination overhead on multi-core systems