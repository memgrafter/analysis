---
ver: rpa2
title: Specialized curricula for training vision-language models in retinal image
  analysis
arxiv_id: '2407.08410'
source_url: https://arxiv.org/abs/2407.08410
tags:
- image
- retinal
- training
- images
- reports
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Foundation vision-language models (VLMs) such as ChatGPT-4o and
  Med-Flamingo underperform on specialist medical tasks despite strong general capabilities.
  To address this, the authors deconstructed clinical problems into essential capabilities
  and created a curriculum-based training approach.
---

# Specialized curricula for training vision-language models in retinal image analysis

## Quick Facts
- arXiv ID: 2407.08410
- Source URL: https://arxiv.org/abs/2407.08410
- Reference count: 40
- Foundation VLMs underperform on specialist medical tasks despite strong general capabilities

## Executive Summary
Foundation vision-language models like ChatGPT-4o and Med-Flamingo underperform on specialist medical tasks despite strong general capabilities. To address this, the authors deconstructed clinical problems into essential capabilities and created a curriculum-based training approach. They developed RetinaVLM, a generative VLM for retinal OCT image analysis, trained on two specialist curricula: one based on tabular biomarker data and another on detailed textual reports by ophthalmologists. RetinaVLM-Specialist significantly outperformed baseline models in AMD disease staging (F1: 0.63 vs. 0.33) and patient referral (F1: 0.67 vs. 0.50), approaching the performance of junior ophthalmologists.

## Method Summary
The authors developed RetinaVLM, a generative vision-language model for retinal OCT image analysis, by combining a Resnet50 vision encoder with Llama 3 LLM using an adapter layer. The model was trained sequentially on a two-part curriculum: first on tabular biomarker reports with automated question-answer pair generation, then on detailed textual reports by ophthalmologists with advanced question-answer pair generation. The approach aimed to selectively train VLMs in the essential capabilities required for AMD diagnosis and management, addressing the gap between general VLM performance and real-world clinical utility.

## Key Results
- RetinaVLM-Specialist outperformed baseline models in AMD disease staging (F1: 0.63 vs. 0.33) and patient referral (F1: 0.67 vs. 0.50)
- Senior ophthalmologists rated RetinaVLM's reports substantially more accurate than ChatGPT-4o (64.3% vs. 14.3%)
- RetinaVLM achieved F1 scores of 0.81-0.88 for biomarker detection, approaching junior ophthalmologist performance
- The model generalized to external datasets but showed performance degradation with different OCT manufacturers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Specialized curricula improve VLM performance by training on task-specific capabilities rather than general knowledge.
- Mechanism: Deconstructing clinical problems into essential capabilities and training VLMs selectively on these skills bridges the gap between general VLM performance and real-world clinical utility.
- Core assumption: General VLMs lack the nuanced knowledge necessary for effective application in specialized clinical contexts, even if they perform well on standardized exams.
- Evidence anchors:
  - [abstract] "We propose to deconstruct clinical problems into sets of mandatory capabilities required for their resolution and selectively train VLMs in these skills."
  - [section] "We postulate that the poor performance of ChatGPT-4o and medical foundation models alike stems from their lack of detailed knowledge related retinal OCT and AMD."

### Mechanism 2
- Claim: Instruction finetuning with progressively refined datasets improves VLM performance on specialist tasks.
- Mechanism: Using a two-part curriculum that starts with tabular biomarker data and progresses to detailed textual reports by ophthalmologists allows VLMs to learn both basic and advanced clinical reasoning skills.
- Core assumption: VLMs can effectively learn from structured tabular data before progressing to more complex textual reports, building knowledge incrementally.
- Evidence anchors:
  - [section] "The first part of the curriculum, named Introduction to retina, primarily covers the appearance of the retina and AMD biomarkers in OCT images."
  - [section] "The second part of the curriculum, named Advanced retinal specialism, builds on top of the first part to link imaging biomarkers to AMD stage and the recommended course of treatment."

### Mechanism 3
- Claim: Training VLMs on specialized datasets maintains their ability to handle diverse textual queries while improving domain-specific performance.
- Mechanism: By keeping the pretrained LLM and vision encoder frozen during training and only updating the adapter, RetinaVLM preserves its original language and reasoning capabilities while specializing in retinal OCT image analysis.
- Core assumption: Freezing the pretrained components prevents catastrophic forgetting of general capabilities while allowing the adapter to learn domain-specific mappings.
- Evidence anchors:
  - [section] "Crucially, when training RetinaVLM we keep both the vision encoder and LLM frozen. That is, they are not updated during the entire training process."
  - [section] "This enabled RetinaVLM to interpret a variety of textual instructions that were never seen during training."

## Foundational Learning

- Concept: Contrastive learning for self-supervised feature extraction from retinal OCT images
  - Why needed here: The vision encoder needs to learn meaningful representations of retinal OCT images without relying on labeled data, which is scarce in specialized medical domains.
  - Quick check question: What self-supervised learning method was used to train the vision encoder before it was frozen in the RetinaVLM architecture?

- Concept: Curriculum learning and progressive task complexity
  - Why needed here: VLMs need to learn basic clinical concepts before advancing to complex reasoning tasks, mirroring how medical specialists are trained.
  - Quick check question: How does the two-part curriculum structure (Introduction to retina â†’ Advanced retinal specialism) reflect the progression of clinical training?

- Concept: Chain-of-thought reasoning in medical diagnosis
  - Why needed here: Medical diagnosis requires sequential reasoning from observations to conclusions, which VLMs need to learn through instruction finetuning.
  - Quick check question: What instruction template was used to prompt VLMs to first describe OCT images in detail before making disease stage predictions?

## Architecture Onboarding

- Component map:
  - Ophthalmic vision encoder (frozen Resnet50 with self-supervised pretraining)
  - Generative LLM (frozen Llama3 8B-Instruct)
  - Image-to-language adapter (trainable linear layer mapping vision embeddings to LLM input space)
  - Tokenizer and embedding layer (from pretrained LLM)

- Critical path:
  1. Process OCT image through vision encoder to extract spatial embeddings
  2. Pass embeddings through adapter to project to LLM embedding space
  3. Replace image tokens in LLM input with adapted vision embeddings
  4. Generate response through frozen LLM using causal language modeling

- Design tradeoffs:
  - Freezing pretrained components preserves general capabilities but may limit domain adaptation
  - Using an adapter instead of full fine-tuning reduces parameter updates and computational cost
  - Progressive curriculum design improves performance but requires expert-annotated data

- Failure signatures:
  - Poor performance on domain-specific tasks despite good general capabilities
  - Hallucinations or fabricated information in generated reports
  - Inability to follow complex multi-step instructions

- First 3 experiments:
  1. Test basic image-to-text mapping with simple yes/no questions about visible biomarkers
  2. Evaluate disease staging accuracy on a small validation set with known ground truth
  3. Compare generated reports against expert annotations for correctness and completeness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would RetinaVLM perform on OCT images from different manufacturers and imaging protocols beyond Topcon?
- Basis in paper: Explicit - The paper tested RetinaVLM on Heidelberg Spectralis and Moorfields Eye Hospital images but noted performance degradation
- Why unresolved: The paper only qualitatively tested on a small number of images from different manufacturers. A systematic evaluation across multiple scanner types and protocols would be needed
- What evidence would resolve it: Large-scale testing of RetinaVLM on diverse OCT datasets from multiple manufacturers, with quantitative performance metrics

### Open Question 2
- Question: What is the optimal balance between tabular and free-text report training data for medical VLM specialization?
- Basis in paper: Explicit - The paper used two curriculum parts with different data types and found cumulative benefits, but didn't systematically compare different ratios
- Why unresolved: The paper used a fixed ratio (part 1: 408,545 QA pairs from tabular reports, part 2: 71,165 QA pairs from free-text reports) without exploring alternatives
- What evidence would resolve it: Controlled experiments varying the ratio of tabular to free-text training data and measuring impact on model performance

### Open Question 3
- Question: How does catastrophic forgetting manifest in RetinaVLM and what mitigation strategies are most effective?
- Basis in paper: Explicit - The paper tested on MedQA and found some performance degradation, particularly in the more specialized RetinaVLM-Specialist
- Why unresolved: The paper only tested one specific benchmark and didn't explore mitigation strategies beyond freezing foundation model weights
- What evidence would resolve it: Systematic evaluation of catastrophic forgetting across multiple medical and general knowledge benchmarks, plus testing of various mitigation strategies

## Limitations
- Small sample size of the external validation dataset (95 patients) may not represent clinical diversity
- Reliance on retrospective data from a single institution raises generalizability concerns
- Automated generation of question-answer pairs introduces potential biases and errors

## Confidence

**High Confidence**: The superiority of RetinaVLM over baseline models in AMD disease staging and patient referral tasks is well-supported by quantitative metrics (F1 scores) and qualitative assessments from senior ophthalmologists.

**Medium Confidence**: The claim that specialized curricula are essential for developing clinically useful VLMs is supported by the observed performance gap between foundation models and RetinaVLM, though this could also be attributed to other factors like model architecture or training methodology.

**Low Confidence**: The assertion that RetinaVLM approaches the performance of junior ophthalmologists is based on indirect comparisons and single-blind reader studies, which may not fully capture the nuances of clinical decision-making.

## Next Checks

1. **External Validation**: Test RetinaVLM on a multi-center dataset with diverse patient populations to assess generalizability and robustness across different clinical settings.

2. **Longitudinal Performance**: Evaluate the model's performance over time on patients with progressive AMD to ensure consistent accuracy in disease staging and referral recommendations.

3. **Clinical Integration Study**: Conduct a prospective study where RetinaVLM assists real clinicians in making AMD management decisions, measuring both accuracy and impact on clinical workflow efficiency.