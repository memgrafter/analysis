---
ver: rpa2
title: Grayscale Image Colorization with GAN and CycleGAN in Different Image Domain
arxiv_id: '2401.11425'
source_url: https://arxiv.org/abs/2401.11425
tags:
- image
- cyclegan
- images
- colorization
- color
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates GAN-based grayscale image colorization methods
  across three image domains: bedroom scenes, human faces, and comic images. The authors
  reproduce a baseline wGAN model, experiment with a GAN variant, and propose a conditional
  CycleGAN approach that incorporates Y-channel grayscale images as conditioning information.'
---

# Grayscale Image Colorization with GAN and CycleGAN in Different Image Domain

## Quick Facts
- arXiv ID: 2401.11425
- Source URL: https://arxiv.org/abs/2401.11425
- Authors: Chen Liang; Yunchen Sheng; Yichen Mo
- Reference count: 12
- Primary result: Conditional CycleGAN outperforms baseline GAN methods on human faces and comics, achieving more consistent colorization but struggling with diverse colorization and generalization to unseen characters.

## Executive Summary
This paper evaluates GAN-based grayscale image colorization methods across three image domains: bedroom scenes, human faces, and comic images. The authors reproduce a baseline wGAN model, experiment with a GAN variant, and propose a conditional CycleGAN approach that incorporates Y-channel grayscale images as conditioning information. Experiments show the CycleGAN outperforms the baseline on human faces and comics, achieving more consistent and plausible colorization. However, it struggles with diverse colorization and can inadvertently learn structural features (e.g., comic frames) instead of colors. The model successfully maintains consistent character hair colors when present in training data but fails for unseen characters. Overall, conditional CycleGAN shows improved performance over GAN variants, particularly in face and comic coloring tasks, though limitations remain in generalization and color diversity.

## Method Summary
The paper implements three approaches for grayscale image colorization: a baseline wGAN-based conditional GAN, a traditional GAN variant, and a proposed conditional CycleGAN. The methods operate on images converted to YUV color space, where the Y-channel represents luminance (grayscale) and UV channels represent chrominance (color). The baseline and GAN variant use standard architectures with different loss formulations, while the CycleGAN approach employs two separate generators for Y-to-UV and UV-to-Y transformations with cycle consistency loss. The models are trained on three datasets: LSUN bedroom, Labeled Face in the Wild, and a comic dataset, with evaluations focusing on visual quality and consistency of colorization results.

## Key Results
- Conditional CycleGAN outperforms baseline wGAN and GAN variants on human face and comic image colorization tasks
- The model successfully maintains consistent character hair colors when those colors appear in training data
- CycleGAN struggles with diverse colorization and can inadvertently learn structural features (comic frames) instead of colors
- The model fails to colorize characters not seen in training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditional CycleGAN outperforms baseline GAN methods by preserving structural information through conditioning on Y-channel grayscale.
- Mechanism: By conditioning the generator and discriminator on the Y-channel grayscale image, the model focuses learning on color mapping rather than learning both structure and color. This separation reduces the complexity of the mapping function and prevents the model from inadvertently learning structural features.
- Core assumption: The Y-channel contains sufficient structural information that can be preserved and concatenated with generated color channels to produce realistic images.
- Evidence anchors:
  - [abstract] "The model successfully maintains consistent character hair colors when present in training data"
  - [section 4.3] "By using YUV, we can just predict UV channels and then concatenate with the grayscale channel to give a full color image"
  - [corpus] Weak evidence - no direct corpus citations support this specific mechanism
- Break condition: If the Y-channel loses critical structural information during preprocessing, or if the color distribution is too complex for the UV prediction, the conditioning benefit disappears.

### Mechanism 2
- Claim: Cycle consistency loss improves colorization quality by constraining the generator to map specific inputs to specific outputs.
- Mechanism: The cycle consistency loss ensures that when an image is transformed from grayscale to color and back to grayscale, the resulting grayscale image matches the original. This creates a stronger mapping constraint than standard GAN losses alone.
- Core assumption: The cyclic transformation should preserve the original image content when mapped back through the inverse function.
- Evidence anchors:
  - [section 4.3] "the losses of cycleGAN are the typical GAN loss combined with the cycle loss"
  - [section 5] "During training, we find that the loss and output image of conditional cycleGAN is more stable compared to original cycleGAN"
  - [corpus] No direct corpus support for this specific claim
- Break condition: If the cycle consistency constraint is too strict relative to the colorization task, it may force the model to prioritize structural reconstruction over color plausibility.

### Mechanism 3
- Claim: Separate generators for forward and backward transformations (GenG->C and GenC->G) enable better specialization than single-generator approaches.
- Mechanism: By using distinct generators for Y-to-UV and UV-to-Y transformations, each network can specialize in its specific mapping task rather than trying to learn a bidirectional transformation.
- Core assumption: Separate networks can learn more effective mappings than a single network attempting both directions.
- Evidence anchors:
  - [section 4.3] "Our coloring CycleGAN consists of two generators: GenG->C and GenC->G"
  - [section 5] "For each input sample, one color image and one grayscale image is picked, and both are split into YUV channels"
  - [corpus] No direct corpus citations support this specific architectural claim
- Break condition: If the separate generators become too specialized and lose the ability to generalize across domains, or if the training becomes unstable due to increased complexity.

## Foundational Learning

- Concept: Generative Adversarial Networks (GANs)
  - Why needed here: The paper builds upon GAN architecture as the foundation for both baseline and CycleGAN approaches
  - Quick check question: What are the two main components of a GAN and what are their respective objectives?

- Concept: Image color spaces (YUV vs RGB)
  - Why needed here: The paper uses YUV color space to separate luminance (Y) from chrominance (UV) for conditioning
  - Quick check question: How does separating luminance from chrominance in YUV help with grayscale image colorization?

- Concept: Cycle consistency in image-to-image translation
  - Why needed here: Cycle consistency is the key innovation that distinguishes CycleGAN from standard GAN approaches
  - Quick check question: What mathematical property does cycle consistency enforce between forward and backward transformations?

## Architecture Onboarding

- Component map: Y-channel grayscale input → GenG->C → UV channels → DisC → adversarial loss, plus UV-channel input → GenC->G → Y-channel output → DisG → adversarial loss, plus cycle consistency between original and reconstructed images

- Critical path: Y-channel input → GenG->C → UV output → DisC → adversarial loss, plus UV-channel input → GenC->G → Y output → DisG → adversarial loss, plus cycle consistency between original and reconstructed images

- Design tradeoffs: Conditioning on Y-channel simplifies the colorization task but requires careful preprocessing to preserve structural information; separate generators increase model complexity but enable better specialization

- Failure signatures: Color bleeding or inconsistent colorization indicates poor conditioning; structural artifacts suggest the model learned image structure instead of color; training instability suggests improper loss weighting or learning rate issues

- First 3 experiments:
  1. Train baseline wGAN on LSUN bedroom dataset to reproduce reported poor results with color blocks
  2. Train conditional CycleGAN on Labeled Face dataset and compare loss stability and output quality against baseline
  3. Test CycleGAN on comic dataset with character-specific color consistency evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed CycleGAN-based model generalize better to unseen domains (e.g., natural landscapes, animals) compared to bedroom and face datasets?
- Basis in paper: [inferred] The paper tests on three domains (bedroom, faces, comics) but does not explore broader or more diverse image types. It suggests the model performs well on faces and comics but struggles with diverse colorization.
- Why unresolved: The study is limited to structured, domain-specific datasets with relatively consistent patterns (e.g., human faces, comic frames). Generalization to unstructured, highly variable domains remains untested.
- What evidence would resolve it: Experiments applying the model to diverse, open-domain datasets like ImageNet or COCO, with quantitative metrics comparing color plausibility and diversity across categories.

### Open Question 2
- Question: What is the impact of conditioning the CycleGAN on grayscale (Y) channels versus using full grayscale images with learned luminance mapping?
- Basis in paper: [explicit] The authors use Y-channel grayscale as conditioning and note this simplifies the task to predicting only UV channels. They do not compare this to alternative conditioning strategies.
- Why unresolved: While the approach is intuitive, the paper does not explore whether alternative conditioning methods (e.g., concatenating full grayscale or learning a mapping from grayscale to YUV) could yield better or more diverse colorization.
- What evidence would resolve it: Ablation studies comparing colorization quality and diversity when conditioning on different grayscale representations or luminance mappings.

### Open Question 3
- Question: How does the CycleGAN model perform on grayscale images with ambiguous or unseen color contexts (e.g., novel characters, objects, or lighting conditions)?
- Basis in paper: [explicit] The authors note the model fails to colorize characters not seen in training and struggles with diverse colorization, implying limited generalization to novel contexts.
- Why unresolved: The experiments focus on datasets with limited variability (e.g., recurring comic characters, standardized face datasets). The model’s behavior on truly novel inputs is not explored.
- What evidence would resolve it: Testing on datasets with high intra-class variability or synthetic datasets introducing novel objects/characters, measuring color accuracy and plausibility.

## Limitations
- The paper lacks detailed architectural specifications for generators and discriminators, making exact reproduction difficult
- Evaluation relies heavily on visual assessment rather than quantitative metrics, limiting objective comparison
- The model shows limited generalization to unseen characters and struggles with diverse colorization scenarios
- The model can inadvertently learn structural features instead of color information, as observed in comic coloring

## Confidence

- **High Confidence**: The observation that conditional CycleGAN outperforms baseline GAN methods on human faces and comic images is well-supported by the experimental results presented.
- **Medium Confidence**: The claim that Y-channel conditioning improves colorization quality by separating structural learning from color learning is plausible but lacks direct empirical validation.
- **Low Confidence**: The assertion that separate generators for forward and backward transformations enable better specialization is stated but not thoroughly tested against alternative architectures.

## Next Checks

1. **Architectural Reproducibility**: Implement the exact generator and discriminator architectures used in the experiments, including layer configurations and activation functions, to verify that the reported results can be reproduced.

2. **Quantitative Evaluation**: Develop and apply objective metrics (such as color histogram similarity, structural similarity index, or user preference studies) to complement the visual assessment and enable more rigorous comparison between methods.

3. **Generalization Testing**: Create a systematic evaluation protocol for testing the model's ability to generalize to unseen characters in comic images, including metrics for color consistency across character appearances and detection of structural feature learning artifacts.