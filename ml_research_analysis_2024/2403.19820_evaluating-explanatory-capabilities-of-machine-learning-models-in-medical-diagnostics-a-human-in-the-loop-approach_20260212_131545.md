---
ver: rpa2
title: 'Evaluating Explanatory Capabilities of Machine Learning Models in Medical
  Diagnostics: A Human-in-the-Loop Approach'
arxiv_id: '2403.19820'
source_url: https://arxiv.org/abs/2403.19820
tags:
- features
- cancer
- feature
- different
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the explainability of machine learning
  models in medical diagnostics, specifically for pancreatic cancer treatment decisions.
  The study employs a Human-in-the-Loop approach, incorporating medical expert knowledge
  and guidelines to establish feature importance.
---

# Evaluating Explanatory Capabilities of Machine Learning Models in Medical Diagnostics: A Human-in-the-Loop Approach

## Quick Facts
- arXiv ID: 2403.19820
- Source URL: https://arxiv.org/abs/2403.19820
- Reference count: 40
- Primary result: Minimum feature set yields best model performance; Weighted Jaccard Similarity enables quantitative comparison of XAI method consistency

## Executive Summary
This paper investigates the explainability of machine learning models in pancreatic cancer treatment decisions using a Human-in-the-Loop approach. The study evaluates three models (Decision Tree, Random Forest, XGBoost) across three feature sets (recommended, maximum, minimum) using both model-specific and model-agnostic explainability techniques. Results demonstrate that the minimum feature set achieves superior accuracy, and that explainability methods show consistent patterns when compared using Weighted Jaccard Similarity coefficients. The XGBoost model particularly aligns well with expert and guideline-based feature importance rankings.

## Method Summary
The study uses the TCGA-PAAD dataset (181 cases, 27 features) to train and evaluate three machine learning models: Decision Tree, Random Forest, and XGBoost. Three feature sets are defined: recommended (14 features based on expert ratings and guidelines), maximum (all 27 features), and minimum (5 key staging features). Models are evaluated using accuracy metrics (Precision, Recall, F1-score) and explainability methods including MDI, MDA, SHAP, and LIME. Feature importance rankings from these methods are compared using Weighted Jaccard Similarity coefficients to assess consistency. Expert medical knowledge and guidelines inform feature selection and serve as ground truth for importance rankings.

## Key Results
- Minimum feature set (TNM staging variables) achieved the best accuracy across all models
- Decision Tree and XGBoost models showed similar accuracy performance
- XGBoost model considered more features overall, aligning better with expert and guideline-based importance
- Explainability methods demonstrated consistent feature importance rankings when measured by Weighted Jaccard Similarity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using similarity measures like Weighted Jaccard coefficients allows quantitative comparison of explainability method consistency across models.
- Mechanism: The method converts ranked feature importance lists from different XAI methods into weighted sets, then computes similarity coefficients to assess agreement. Higher similarity indicates consistent explanatory patterns across methods.
- Core assumption: Feature importance rankings from different XAI methods can be meaningfully compared using weighted set similarity metrics.
- Evidence anchors:
  - [abstract]: "we propose the use of similarity measures such as the Weighted Jaccard Similarity coefficient"
  - [section]: "the Weighted Jaccard Similarity index assumes that a weighted set exists that associates a real weight to each element in it"
  - [corpus]: Weak evidence - no direct corpus support found for this specific similarity approach.
- Break condition: If different XAI methods produce fundamentally incompatible importance rankings (e.g., completely different top features), similarity measures may not provide meaningful comparisons.

### Mechanism 2
- Claim: Including medical experts and guidelines in feature selection improves model interpretability by aligning features with domain knowledge.
- Mechanism: Expert ratings and guideline analysis identify features most relevant to clinical decision-making, reducing noise from less relevant features while maintaining medical validity.
- Core assumption: Medical experts and guidelines can accurately identify which features matter most for clinical decisions.
- Evidence anchors:
  - [abstract]: "We use Human-in-the-Loop related techniques and medical guidelines as a source of domain knowledge"
  - [section]: "we asked a panel of medical doctors to rate the relevance regarding the prescription of chemotherapy treatment"
  - [corpus]: Weak evidence - corpus contains related medical ML papers but no specific validation of expert-guided feature selection.
- Break condition: If expert opinions or guidelines contain biases or outdated information that doesn't reflect current best practices.

### Mechanism 3
- Claim: Using minimum feature sets can improve model accuracy compared to maximum feature sets in medical datasets.
- Mechanism: Reducing features to essential clinical indicators (TNM staging, age) removes noise and multicollinearity while preserving critical diagnostic information.
- Core assumption: TNM staging variables contain sufficient information to capture disease severity and treatment decisions.
- Evidence anchors:
  - [section]: "the minimum set of features was the one that obtained better results in terms of accuracy"
  - [section]: "features like pathologic T...pathologic N...pathologic M...are summarizing codes of the cancer status"
  - [corpus]: Weak evidence - corpus contains segmentation and classification papers but no direct comparison of feature set sizes.
- Break condition: If additional clinical features contain independent predictive information not captured by staging alone.

## Foundational Learning

- Concept: Weighted Jaccard Similarity coefficient
  - Why needed here: To quantitatively compare explainability method consistency across different ML models and feature sets
  - Quick check question: If two feature sets have intersection size 2 and union size 4, what is their Jaccard similarity?

- Concept: Feature importance methods (MDI, MDA, SHAP, LIME)
  - Why needed here: To evaluate and compare different explainability approaches for medical ML models
  - Quick check question: Which feature importance method measures accuracy decrease when features are permuted?

- Concept: TNM staging system
  - Why needed here: Forms the basis for minimum feature set and clinical interpretation of model decisions
  - Quick check question: What do the letters T, N, and M represent in cancer staging?

## Architecture Onboarding

- Component map:
  - Data preprocessing → Feature engineering (expert + guideline input) → Model training (DT, RF, XGB) → Explainability evaluation (MDI, MDA, SHAP, LIME) → Similarity analysis → Performance + interpretability comparison

- Critical path: Feature selection → Model training → Explainability evaluation → Similarity comparison
  - This path determines the final model recommendation based on both accuracy and interpretability

- Design tradeoffs:
  - Expert involvement: Higher interpretability confidence vs. increased development complexity and potential bias
  - Feature set size: More features may capture more information vs. increased model complexity and reduced interpretability
  - XAI methods: Model-specific methods are faster but less generalizable vs. model-agnostic methods that work across models

- Failure signatures:
  - Low similarity between XAI methods on same model suggests inconsistent explanations
  - Expert guidelines diverging from XAI results indicates potential model behavior issues
  - Accuracy drops with minimum feature set suggest missing critical information

- First 3 experiments:
  1. Train DT, RF, XGB models on recommended feature set and compute accuracy + feature importance with all XAI methods
  2. Calculate Weighted Jaccard similarity between all XAI method pairs for each model
  3. Compare feature importance rankings from XAI methods against expert ratings and guideline-derived importance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can explainability metrics be standardized to evaluate model interpretability consistently across different domains?
- Basis in paper: [inferred] The authors propose using similarity measures like the Weighted Jaccard Similarity coefficient to compare feature importance rankings across models and methods, but acknowledge that this is a step towards standardization rather than a complete solution.
- Why unresolved: While the paper introduces a promising approach to evaluate explainability, it does not provide a comprehensive framework for standardizing explainability metrics across diverse applications and domains.
- What evidence would resolve it: Development and validation of a universal set of explainability metrics that can be applied consistently across various machine learning models and domains, with empirical studies demonstrating their effectiveness and generalizability.

### Open Question 2
- Question: What is the optimal balance between model accuracy and interpretability in critical domains like healthcare?
- Basis in paper: [explicit] The authors discuss the trade-off between model interpretability and performance, noting that more interpretable models tend to perform less well than less interpretable models. They also emphasize the importance of interpretability in medical diagnostics.
- Why unresolved: The paper does not provide a definitive answer on how to balance accuracy and interpretability, especially in high-stakes domains where both are crucial.
- What evidence would resolve it: Empirical studies comparing the performance and interpretability of various models in critical domains, along with guidelines for determining the optimal balance based on specific use cases and risk factors.

### Open Question 3
- Question: How can Human-in-the-Loop approaches be effectively integrated into the development and evaluation of explainable AI systems?
- Basis in paper: [explicit] The authors incorporate medical expert knowledge and guidelines to establish feature importance and evaluate explainability, highlighting the importance of HITL techniques in complex domains like healthcare.
- Why unresolved: While the paper demonstrates the value of HITL approaches, it does not provide a comprehensive framework for integrating these techniques into the broader development and evaluation process of explainable AI systems.
- What evidence would resolve it: Development of best practices and guidelines for incorporating HITL approaches at various stages of AI system development, along with empirical studies demonstrating their impact on model performance and interpretability.

## Limitations
- Small sample size (181 cases) may limit generalizability of findings
- Reliance on single expert panel could introduce bias in feature selection and importance rankings
- Novel Weighted Jaccard approach lacks validation against established medical ML benchmarks

## Confidence
- Model performance comparisons: High
- Expert-guided feature selection: Medium
- XAI method consistency: Medium
- Weighted Jaccard application: Low

## Next Checks
1. Replicate results using k-fold cross-validation with the 181-sample dataset to assess stability of findings
2. Conduct blinded expert review comparing model-derived feature importance to independent clinical guidelines
3. Test the Weighted Jaccard approach on synthetic datasets with known feature importance to validate its sensitivity and specificity