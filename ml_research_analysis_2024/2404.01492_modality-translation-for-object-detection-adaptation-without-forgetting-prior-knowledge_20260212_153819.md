---
ver: rpa2
title: Modality Translation for Object Detection Adaptation Without Forgetting Prior
  Knowledge
arxiv_id: '2404.01492'
source_url: https://arxiv.org/abs/2404.01492
tags:
- modtr
- detector
- modality
- detection
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting object detection
  models trained on RGB images to new modalities like infrared (IR) with significant
  distribution shift, while avoiding catastrophic forgetting of the original RGB knowledge.
  The authors propose Modality Translation (ModTr), a method that uses a small transformation
  network to translate IR inputs into an RGB-like space that the pre-trained RGB detector
  can process without any changes or fine-tuning to its parameters.
---

# Modality Translation for Object Detection Adaptation Without Forgetting Prior Knowledge

## Quick Facts
- arXiv ID: 2404.01492
- Source URL: https://arxiv.org/abs/2404.01492
- Reference count: 40
- Primary result: ModTr achieves over 29 AP improvement for FCOS and RetinaNet on LLVIP and over 11 AP on FLIR compared to standard fine-tuning

## Executive Summary
This paper addresses the challenge of adapting object detection models trained on RGB images to new modalities like infrared (IR) with significant distribution shift, while avoiding catastrophic forgetting of the original RGB knowledge. The authors propose Modality Translation (ModTr), a method that uses a small transformation network to translate IR inputs into an RGB-like space that the pre-trained RGB detector can process without any changes or fine-tuning to its parameters. ModTr is trained directly to minimize the detection loss, allowing the original detector to retain its knowledge for other tasks. Experiments on two datasets (LLVIP and FLIR) show that ModTr achieves competitive or better performance compared to standard fine-tuning and other image translation methods.

## Method Summary
ModTr adapts object detection models to new modalities by using a small transformation network (U-Net) to translate input images into an RGB-like representation. The key innovation is training this translator directly to minimize the detection loss rather than image reconstruction loss, while keeping the original detector's weights frozen. The translated images are fused with the original input using element-wise multiplication, allowing the network to learn residual transformations. This approach enables a single pre-trained detector to handle multiple modalities through different translators, reducing memory costs compared to maintaining multiple specialized detectors.

## Key Results
- ModTr achieves over 29 AP improvement for FCOS and RetinaNet on LLVIP dataset compared to standard fine-tuning
- ModTr improves detection performance by over 11 AP on FLIR dataset compared to fine-tuning approaches
- The element-wise product fusion strategy (ModTr⊙) yields the best results among tested fusion methods
- ModTr successfully preserves the original RGB detector's performance while adapting to IR inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ModTr avoids catastrophic forgetting by freezing the pre-trained detector weights and only training a small transformation network.
- Mechanism: The transformation network (U-Net) learns to map IR inputs into an RGB-like representation that the frozen detector can process. Since the detector's parameters remain unchanged, its knowledge from RGB training is preserved.
- Core assumption: The transformation network can learn to represent IR data in a way that the RGB detector's learned features remain valid.
- Evidence anchors:
  - [abstract] "ModTr adapts the IR input image with a small transformation network trained to directly minimize the detection loss. The original RGB model can then work on the translated inputs without any further changes or fine-tuning to its parameters."
  - [section 3b] "the weight vector θ remains constant. This constraint is consistent with the premise of this study, where a pre-trained detector is solely available on the server side and remains unaltered."
- Break condition: If the distribution shift between IR and RGB is too large for the transformation network to bridge effectively, or if the detector requires fine-tuning to handle even translated IR data.

### Mechanism 2
- Claim: ModTr achieves competitive performance by training the translation network directly for the detection task rather than just image reconstruction quality.
- Mechanism: The translation network is optimized using the detection loss (Ldet) from the detector, forcing it to produce translations that are useful for object detection rather than just visually similar to RGB images.
- Core assumption: Optimizing for detection performance produces better translations for object detection than optimizing for image reconstruction metrics.
- Evidence anchors:
  - [abstract] "Unlike other image-to-image translation approaches, we drive the process using the aforementioned detection cost... the underlying optimization problem is formulated as ϑ∗ = arg min Ldet(ϑ)"
  - [section 3b] "we drive the process using the aforementioned detection cost (Equation (1)). Thus, the underlying optimization problem is formulated as ϑ∗ = arg min Ldet(ϑ)"
- Break condition: If the detection loss is not a good proxy for generating useful translations, or if the detector's loss function is not differentiable through the translation network.

### Mechanism 3
- Claim: The element-wise product fusion strategy (ModTr⊙) helps the translation network learn residual transformations by acting as a gating mechanism.
- Mechanism: The translation network output acts as a weight map that scales the input IR image. When the translation network outputs values near 1, the input is preserved; when near 0, features are suppressed. This residual structure makes learning easier.
- Core assumption: The translation network only needs to learn small modifications to the input to make it suitable for detection.
- Evidence anchors:
  - [section 3c] "we utilize a residual learning strategy in which the function hd_ϑ focuses on capturing the small variations in the input that are necessary to solve the task... the Hadamard product-based fusion serves as a gating mechanism to filter or highlight information from the input image."
  - [section 3c] "the output of the translation network acts as a weight map for the input, and they are fused using pixel-wise multiplication, ⊙."
- Break condition: If the IR-to-RGB transformation requires large, non-residual changes rather than small adjustments, the residual learning strategy may be insufficient.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why freezing detector weights is critical to preserving RGB knowledge
  - Quick check question: What happens to a neural network's performance on previous tasks when it is fine-tuned on a new task?

- Concept: Image-to-image translation fundamentals
  - Why needed here: Understanding how U-Net architectures can transform between image modalities
  - Quick check question: How does a typical encoder-decoder architecture for image translation work?

- Concept: Object detection loss functions
  - Why needed here: Understanding how detection loss can be used to train the translation network
  - Quick check question: What are the main components of an object detection loss function?

## Architecture Onboarding

- Component map:
  Pre-trained RGB detector (fixed weights) -> ModTr translation network (U-Net with Sigmoid output) -> Fusion layer (element-wise product) -> Detector -> Detection loss -> Update translation network weights

- Critical path:
  Input IR → Translation network → Element-wise product with IR → Detector → Detection loss → Update translation network weights

- Design tradeoffs:
  - Small translation network reduces computational overhead but may limit translation capacity
  - Freezing detector weights preserves knowledge but prevents detector adaptation
  - Using detection loss for translation may be slower to converge than reconstruction losses

- Failure signatures:
  - Poor detection performance suggests the translation network isn't learning effective mappings
  - Translation network producing outputs outside [0,1] range indicates Sigmoid issues
  - High variance in training suggests learning rate or optimization problems

- First 3 experiments:
  1. Verify translation network can learn to produce RGB-like outputs on a simple dataset (MNIST digits to RGB digits)
  2. Test detection performance with random translation network weights to establish baseline
  3. Validate that freezing detector weights preserves original RGB detection performance on test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ModTr be effectively applied to modalities beyond IR, such as depth images or thermal images with different spectral ranges, and what are the limits of its effectiveness?
- Basis in paper: [explicit] The authors mention the potential to integrate modalities beyond IR, including Canny edges extracted from IR images, and note that ModTr significantly enhances performance on edges but does not surpass full fine-tuning.
- Why unresolved: The paper only provides preliminary experiments with Canny edges and does not explore other potential modalities like depth images or thermal images with different spectral characteristics.
- What evidence would resolve it: Empirical results showing ModTr's performance on a variety of modalities, including depth, thermal, and other spectral ranges, compared to fine-tuning and other adaptation methods.

### Open Question 2
- Question: How does the choice of fusion strategy (e.g., element-wise product, addition, or attention-based) affect ModTr's performance across different object detection architectures and datasets?
- Basis in paper: [explicit] The authors explore different fusion strategies (ModTr+, ModTr⊕, and ModTr⊙) and report that the element-wise product (ModTr⊙) yields the best results, but they do not provide a detailed analysis of why or how these strategies perform differently.
- Why unresolved: The paper presents results for different fusion strategies but does not offer a comprehensive analysis of the underlying reasons for their performance differences or their impact on various detection architectures and datasets.
- What evidence would resolve it: A systematic study comparing the performance of different fusion strategies across multiple detection architectures and datasets, including visualizations of the translated images and attention maps.

### Open Question 3
- Question: Can ModTr be combined with generative models to further enhance the information content of the translated modality, potentially improving zero-shot object detection performance?
- Basis in paper: [inferred] The authors suggest that replacing the deterministic translator module within ModTr with a generative model could enrich modality information by generating missing data, potentially improving zero-shot detector performance.
- Why unresolved: This is a proposed direction for future research, but the paper does not provide any experimental results or theoretical analysis of how generative models could be integrated with ModTr.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of combining ModTr with generative models for zero-shot object detection, along with a theoretical analysis of how the generative model enhances the translated modality's information content.

## Limitations

- Performance gains are demonstrated primarily on two datasets (LLVIP and FLIR), limiting generalizability claims
- The paper lacks ablation studies on U-Net architecture depth and the impact of different fusion strategies on translation quality
- Translation quality assessment (visual fidelity, PSNR/SSIM metrics) is absent, making it difficult to evaluate whether the translated images are truly RGB-like beyond their utility for detection

## Confidence

- **High Confidence:** The core claim that freezing detector weights prevents catastrophic forgetting is well-supported by the mechanism description and consistent with established deep learning principles.
- **Medium Confidence:** The claim of competitive performance improvements (29+ AP on LLVIP, 11+ AP on FLIR) is supported by experimental results but lacks comparisons to stronger baselines or state-of-the-art methods in cross-spectral detection.
- **Low Confidence:** The assertion that ModTr can handle "multiple modalities" is only demonstrated for IR-to-RGB, and the flexibility claims are extrapolated from a single modality experiment.

## Next Checks

1. Evaluate ModTr on a third dataset with different domain characteristics (e.g., synthetic-to-real or day-to-night) to test generalization beyond IR-to-RGB translation.
2. Conduct ablation studies comparing the three fusion strategies (ModTr+, ModTr⊕, ModTr⊙) to identify which provides the best trade-off between translation quality and detection performance.
3. Measure translation quality using standard image metrics (PSNR, SSIM) alongside detection AP to quantify how "RGB-like" the translated images actually are.