---
ver: rpa2
title: Trajectory Forecasting through Low-Rank Adaptation of Discrete Latent Codes
arxiv_id: '2405.20743'
source_url: https://arxiv.org/abs/2405.20743
tags:
- codebook
- trajectory
- latent
- prediction
- discrete
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel trajectory forecasting method that
  leverages Vector Quantized Variational Autoencoders (VQ-VAEs) with an instance-based
  codebook conditioned on low-rank updates. The approach addresses posterior collapse
  issues common in variational methods by using discrete latent spaces and adapting
  codebook entries based on contextual information from observed trajectories.
---

# Trajectory Forecasting through Low-Rank Adaptation of Discrete Latent Codes

## Quick Facts
- arXiv ID: 2405.20743
- Source URL: https://arxiv.org/abs/2405.20743
- Reference count: 40
- Authors: Riccardo Benaglia; Angelo Porrello; Pietro Buzzega; Simone Calderara; Rita Cucchiara
- Primary result: State-of-the-art performance on Stanford Drone, NBA, and NFL trajectory forecasting benchmarks using VQ-VAEs with low-rank adaptation

## Executive Summary
This paper proposes a novel trajectory forecasting method that leverages Vector Quantized Variational Autoencoders (VQ-VAEs) with instance-based codebook conditioning through low-rank updates. The approach addresses posterior collapse issues common in variational methods by using discrete latent spaces and adapting codebook entries based on contextual information from observed trajectories. The model employs a two-stage framework: first learning the codebook with instance-level dynamics through low-rank updates, then training a diffusion-based model to generate predictions. The method demonstrates state-of-the-art performance on three established benchmarks (Stanford Drone, NBA, and NFL datasets), achieving significant improvements in Average and Final Displacement Errors compared to existing approaches.

## Method Summary
The method employs a two-stage framework using Vector Quantized Variational Autoencoders (VQ-VAEs) with low-rank adaptation of discrete latent codes. In the first stage, a VQ-VAE encoder learns a codebook where each codeword is dynamically adjusted using contextual information from past trajectories via low-rank updates. This instance-based adaptation is implemented as a matrix product of two low-rank matrices Bctx and A, where Bctx is generated from learnable tokens B and the context hctx. The discrete latent codes are then used by a diffusion-based prior model in the second stage to generate diverse trajectory predictions. The model uses a social-temporal transformer architecture for encoding trajectories and employs k-means clustering for refining sampled trajectories during evaluation.

## Key Results
- Achieves state-of-the-art performance on Stanford Drone Dataset with significant improvements in ADE20 and FDE20 metrics
- Demonstrates superior results on NBA and NFL sports datasets compared to existing trajectory forecasting approaches
- Low-rank adaptation strategy effectively balances per-instance customization with cross-instance concept learning
- The discrete latent space approach mitigates posterior collapse issues common in continuous variational methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instance-based codebook adaptation through low-rank updates improves trajectory reconstruction accuracy.
- Mechanism: The codebook is dynamically adjusted using contextual information from past trajectories via low-rank updates, allowing per-instance customization while maintaining cross-instance concept learning. This adaptation is implemented as a matrix product of two low-rank matrices Bctx and A, where Bctx is generated from learnable tokens B and the context hctx.
- Core assumption: Historical trajectory information is relevant for future trajectory reconstruction and can be effectively encoded through low-rank modifications to the codebook.
- Evidence anchors:
  - [abstract]: "instance-level dynamics are injected into the codebook through low-rank updates, which restrict the customization of the codebook to a lower dimension space"
  - [section 4.2]: "we propose to dynamically adjust its values based on the context of each example, leading to an instance-based codebook"
  - [corpus]: No direct corpus evidence found. Weak signal.
- Break condition: If the historical information becomes irrelevant or the low-rank constraint is too restrictive to capture necessary variations, the adaptation will degrade performance.

### Mechanism 2
- Claim: Discrete latent space with VQ-VAE mitigates posterior collapse compared to continuous variational methods.
- Mechanism: By replacing the continuous latent space with a discrete set of codewords and using a learnable categorical prior instead of Gaussian, the model avoids the tendency for latent variables to collapse to the prior. The deterministic quantization process ensures the posterior distribution remains informative.
- Core assumption: Discrete latent spaces are inherently more stable than continuous ones for trajectory data.
- Evidence anchors:
  - [abstract]: "utilize a discrete latent space to tackle the issue of posterior collapse"
  - [section 3]: "VQ-VAEs [39] extend VAEs by employing discrete latent variables and Vector Quantization (VQ)"
  - [corpus]: No direct corpus evidence found. Weak signal.
- Break condition: If the discrete space becomes too coarse to capture necessary trajectory variations, the model will lose expressiveness and accuracy.

### Mechanism 3
- Claim: Non-autoregressive diffusion-based prior generation avoids error accumulation and unidirectional bias.
- Mechanism: The diffusion model generates the sequence of indices in parallel rather than sequentially, eliminating the propagation of errors from earlier time steps to later ones. This order-free generation captures multiple plausible trends without the bias toward earlier information.
- Core assumption: Future trajectory prediction benefits from considering all time steps simultaneously rather than sequentially.
- Evidence anchors:
  - [abstract]: "avoid the error accumulation and the unidirectional bias problem, typical of auto-regressive methods"
  - [section 4.3]: "we make use of a discrete diffusion model for the generation of the sequence of indices"
  - [corpus]: No direct corpus evidence found. Weak signal.
- Break condition: If the diffusion process requires too many steps or the noise schedule is poorly tuned, generation quality will degrade significantly.

## Foundational Learning

- Concept: Vector Quantization (VQ)
  - Why needed here: VQ enables the conversion of continuous trajectory embeddings into discrete latent codes, which are more stable for learning and generation.
  - Quick check question: What is the difference between VQ-VAE's discrete latent space and traditional VAE's continuous latent space?

- Concept: Low-rank matrix decomposition
  - Why needed here: Low-rank updates to the codebook allow efficient instance-level customization while maintaining computational tractability and preventing overfitting.
  - Quick check question: How does a low-rank matrix product BctxA compare to a full-rank matrix in terms of parameter efficiency?

- Concept: Diffusion probabilistic models
  - Why needed here: Diffusion models provide a principled way to learn the prior distribution over discrete latent codes without relying on autoregressive generation, avoiding error accumulation.
  - Quick check question: What is the key difference between denoising diffusion models and autoregressive models in terms of generation order?

## Architecture Onboarding

- Component map:
  - Context encoder (Ectx) -> VQ-VAE encoder (E) -> Codebook with low-rank adaptation -> VQ-VAE decoder (G) -> Diffusion prior (pdiff)

- Critical path:
  1. Encode context (Ectx)
  2. Generate low-rank codebook adaptation (fÎ¾)
  3. Encode future trajectory with adapted codebook (E)
  4. Sample from diffusion prior
  5. Decode sampled latent codes (G)

- Design tradeoffs:
  - Discrete vs continuous latent space: Discrete provides stability but may lose expressiveness
  - Low-rank vs full-rank adaptation: Low-rank is computationally efficient but may be too restrictive
  - k-means refinement vs direct sampling: Refinement improves quality but adds computational overhead

- Failure signatures:
  - Codebook utilization imbalance: Some codebook entries are never used
  - Posterior collapse: Latent codes become uninformative and reconstruction quality drops
  - Error accumulation in diffusion: Generated trajectories diverge from ground truth

- First 3 experiments:
  1. Ablation study: Remove low-rank adaptation and compare ADE/FDE
  2. Sensitivity analysis: Vary rank r and measure impact on reconstruction vs generation performance
  3. Baseline comparison: Replace diffusion prior with autoregressive model and measure error accumulation

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the text provided.

## Limitations
- The empirical evaluation is limited to three specific datasets (SDD, NBA, NFL) which may not represent complex, unstructured real-world scenarios
- Lack of ablation studies comparing against standard VAE approaches without discretization
- Missing sensitivity analysis for the low-rank adaptation rank parameter across different datasets
- k-means refinement step introduces additional complexity that may obscure true method performance

## Confidence
- **High Confidence**: The discrete latent space approach effectively addresses posterior collapse - the VQ-VAE framework with learnable categorical prior is well-established in literature and the mechanism is theoretically sound.
- **Medium Confidence**: Low-rank adaptation provides meaningful instance-level customization - while the approach is plausible, the paper lacks ablation studies showing performance degradation when removing the adaptation mechanism.
- **Low Confidence**: The two-stage training procedure is optimal - the paper does not explore alternative training strategies or justify why this specific sequence is superior to joint training approaches.

## Next Checks
1. **Cross-dataset generalization test**: Train the model on one dataset (e.g., SDD) and evaluate zero-shot performance on another (e.g., NBA) to assess how well the low-rank adaptation and discrete codebook generalize across different trajectory domains.

2. **Posterior collapse stress test**: Systematically vary the number of codewords in the codebook (e.g., 8, 16, 32) and measure codebook utilization statistics and reconstruction quality to verify that the discrete approach maintains informative latent codes across different capacity settings.

3. **Rank sensitivity analysis**: Conduct a comprehensive ablation study varying the low-rank parameter r across multiple values (e.g., 2, 4, 8, 16) on all three datasets, measuring both reconstruction accuracy and generation diversity to identify optimal rank settings and determine when the constraint becomes too restrictive.