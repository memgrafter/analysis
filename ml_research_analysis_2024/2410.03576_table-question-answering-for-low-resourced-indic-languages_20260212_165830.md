---
ver: rpa2
title: Table Question Answering for Low-resourced Indic Languages
arxiv_id: '2410.03576'
source_url: https://arxiv.org/abs/2410.03576
tags:
- table
- bengali
- language
- tableqa
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first large-scale datasets and models
  for low-resource table question answering (tableQA) in Indic languages. The authors
  propose an automatic data generation pipeline to create synthetic tableQA datasets
  for Bengali and Hindi, languages lacking existing tableQA resources.
---

# Table Question Answering for Low-resourced Indic Languages

## Quick Facts
- arXiv ID: 2410.03576
- Source URL: https://arxiv.org/abs/2410.03576
- Reference count: 40
- Key outcome: First large-scale tableQA datasets and models for low-resource Indic languages, with models achieving up to 60% exact match accuracy on synthetic test sets

## Executive Summary
This paper addresses the critical gap in table question answering (tableQA) resources for low-resource Indic languages by introducing BanglaTabQA and HindiTabQA datasets. The authors develop an automatic data generation pipeline that extracts Wikipedia tables, generates code-mixed SQL queries, translates them to natural language questions, and extracts answer tables. Models trained on these datasets, particularly BnTQA-mBart and HiTQA-mBart, significantly outperform state-of-the-art LLMs like GPT-4 on tableQA tasks. The approach also demonstrates strong zero-shot cross-lingual transfer capabilities and effective handling of mathematical reasoning tasks, providing a foundation for advancing tableQA research in low-resource languages.

## Method Summary
The authors propose an automatic data generation pipeline for low-resource tableQA. The method involves extracting tables from Wikipedia, generating code-mixed SQL queries using SQUALL templates, translating these queries to natural language questions via a trained SQL2NQ model, and extracting answer tables by executing SQL queries. Automatic quality control using sentence similarity ensures data reliability. The generated datasets are used to train multilingual encoder-decoder models like mbart-50-large and m2m100_418M. The approach is validated through evaluation on manually annotated test sets for Bengali (165 samples) and Hindi (121 samples), measuring table exact match accuracy, row/column/cell F1 scores, and cross-lingual transfer capabilities.

## Key Results
- BnTQA-mBart and HiTQA-mBart models outperform GPT-4 on table exact match accuracy (up to 60%) and column EM F1
- Models demonstrate strong zero-shot cross-lingual transfer from BanglaTabQA to Hindi tableQA
- Automatic synthetic data generation successfully closes the low-resource gap without manual annotation
- Models show effective handling of mathematical reasoning tasks within tableQA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automatic synthetic data generation closes the low-resource tableQA gap by replacing costly manual annotation
- Mechanism: The pipeline extracts Wikipedia tables, generates code-mixed SQL queries, converts them to natural language questions, and extracts answer tables automatically
- Core assumption: SQL execution yields correct answer tables that align with generated questions
- Evidence anchors: [abstract] Automatic large-scale tableQA data generation for low-resource languages; [section] method for automatically generating low-resource tableQA data
- Break condition: SQL execution errors, translation mismatches, or low-quality question generation break alignment

### Mechanism 2
- Claim: Fine-tuning multilingual models on synthetic datasets yields superior performance compared to few-shot LLMs
- Mechanism: Models learn target language and table structure through training, while few-shot LLMs rely on in-context learning
- Core assumption: Synthetic dataset distribution matches reasoning patterns needed for tableQA
- Evidence anchors: [abstract] Models trained on large-scale datasets outperform state-of-the-art LLMs; [section] BnTQA-mBart outperforms GPT-4 on EM and column F1
- Break condition: Distribution shift between synthetic queries and real user questions, or model overfitting to synthetic patterns

### Mechanism 3
- Claim: Zero-shot cross-lingual transfer works because table reasoning skills generalize across languages
- Mechanism: Model trained on BanglaTabQA can answer Hindi tableQA questions without Hindi-specific training
- Core assumption: Table structure and reasoning patterns are language-agnostic
- Evidence anchors: [abstract] Study trained models on zero-shot cross-lingual transfer; [section] BanglaTabQA models perform table reasoning in Hindi
- Break condition: Large script differences or cultural fact mismatches break transfer

## Foundational Learning

- Concept: Sequence-to-sequence modeling for tableQA
  - Why needed here: TableQA requires generating structured answers (tables) from semi-structured inputs
  - Quick check question: What is the difference between extractive and abstractive tableQA, and why does this method use abstractive generation?

- Concept: Code-mixed SQL generation and translation
  - Why needed here: Generating valid SQL queries over real tables ensures coverage of diverse table operations
  - Quick check question: How does the pipeline convert code-mixed SQL to monolingual queries before generating natural language questions?

- Concept: Sentence similarity for quality control
  - Why needed here: Ensures generated questions match corresponding SQL queries semantically, filtering low-quality synthetic data
  - Quick check question: Why was LaBse inadequate for this task, and how did the authors fix it?

## Architecture Onboarding

- Component map: Wikipedia table extractor → SQL template engine → code-mixed SQL generator → SQL2NQ model → answer table extractor → quality control → dataset split → model training → evaluation
- Critical path: Table extraction → SQL generation → question generation → answer extraction → quality filtering. Any failure in these steps breaks the pipeline.
- Design tradeoffs: Synthetic data vs. manual annotation (speed/cost vs. realism), multilingual vs. monolingual models (coverage vs. performance), automatic vs. manual quality control (scalability vs. accuracy)
- Failure signatures: Low similarity scores in quality control, SQL execution errors, model overfitting to synthetic patterns, poor cross-lingual transfer
- First 3 experiments:
  1. Run pipeline on small Wikipedia dump and verify SQL execution yields correct answers
  2. Fine-tune mbart-50-large on subset of synthetic data and evaluate on test set
  3. Test zero-shot transfer by evaluating BanglaTabQA model on Hindi test set with and without post-processing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the BanglaTabQA model performance compare to other low-resource language models on similar tasks like text-based question answering?
- Basis in paper: [inferred] The paper discusses challenges of low-resource tableQA and mentions that existing neural models have poor alignment in low-resource languages
- Why unresolved: Paper does not directly compare BanglaTabQA model's performance to other low-resource language models on text-based QA tasks
- What evidence would resolve it: Comparative study evaluating BanglaTabQA model against other low-resource language models on text-based question answering tasks

### Open Question 2
- Question: What are the limitations of using SQUALL templates for generating SQL queries, and how might these limitations affect the diversity and complexity of generated questions?
- Basis in paper: [explicit] SQUALL templates do not support multi-table operations or complex queries
- Why unresolved: Paper does not explore impact of these limitations on quality and diversity of generated questions
- What evidence would resolve it: Analysis of generated questions' diversity and complexity, comparing results from SQUALL templates to more advanced SQL generation methods

### Open Question 3
- Question: How does the automatic quality control process handle ambiguous or poorly formed natural language questions generated during dataset creation?
- Basis in paper: [explicit] Quality control evaluates generated questions using sentence similarity model and discards questions with low similarity scores
- Why unresolved: Paper does not provide details on how quality control handles ambiguous or poorly formed questions that may still pass similarity threshold
- What evidence would resolve it: Detailed analysis of quality control process's performance in filtering out ambiguous or poorly formed questions, possibly through manual inspection or additional evaluation metrics

## Limitations
- Entire evaluation pipeline relies on synthetic data generation without independent verification
- Quality control mechanism using sentence similarity is innovative but unproven
- Cross-lingual transfer results limited to authors' own Hindi test set
- Comparison with GPT-4 lacks broader benchmarking and exploration of performance reasons

## Confidence
- High Confidence: Automatic data generation pipeline is technically sound and represents legitimate approach to low-resource scenarios
- Medium Confidence: Claim that synthetic data can close low-resource gap is plausible but needs real-world validation
- Low Confidence: Assertion that models "significantly outperform" state-of-the-art LLMs like GPT-4 is overstated without broader benchmarking

## Next Checks
1. **Independent Test Set Evaluation**: Deploy trained models on held-out set of manually annotated questions from actual users in Bengali and Hindi communities, rather than relying solely on authors' synthetic test sets
2. **External Cross-Lingual Transfer Test**: Evaluate Bangla-trained models on independently collected Hindi tableQA datasets from different sources to verify genuine cross-lingual transfer
3. **Real-World Deployment Pilot**: Run small-scale pilot where models answer real user-submitted table questions through simple interface, measuring accuracy and user satisfaction