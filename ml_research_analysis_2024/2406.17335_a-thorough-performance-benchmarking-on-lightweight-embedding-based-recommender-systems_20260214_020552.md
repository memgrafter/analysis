---
ver: rpa2
title: A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender
  Systems
arxiv_id: '2406.17335'
source_url: https://arxiv.org/abs/2406.17335
tags:
- embedding
- performance
- recommendation
- methods
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper benchmarks various lightweight embedding-based recommender
  systems (LERSs) through extensive experiments on two recommendation tasks: collaborative
  filtering and content-based recommendation. The study compares seven different LERSs
  methods and proposes a simple magnitude-based pruning baseline (MagPrune) that achieves
  competitive performance.'
---

# A Thorough Performance Benchmarking on Lightweight Embedding-based Recommender Systems

## Quick Facts
- arXiv ID: 2406.17335
- Source URL: https://arxiv.org/abs/2406.17335
- Reference count: 40
- Primary result: Benchmarks seven lightweight embedding-based recommender systems, introducing MagPrune baseline that performs competitively across compression rates

## Executive Summary
This paper presents a comprehensive benchmarking study of lightweight embedding-based recommender systems (LERSs) across two core recommendation tasks: collaborative filtering and content-based recommendation. The study evaluates seven different LERS methods and introduces a simple magnitude-based pruning baseline called MagPrune. Through extensive experiments, the authors identify distinct performance patterns across different compression rates and tasks, revealing that simple methods excel at low sparsity rates while complex methods perform better at high sparsity rates. The benchmarking also includes efficiency evaluations on both GPU workstations and Raspberry Pi devices, providing insights into method selection for different deployment scenarios.

## Method Summary
The study benchmarks seven lightweight embedding-based recommender systems across collaborative filtering and content-based recommendation tasks. The methods include compositional approaches (TALL, NetTailor, Network Pruning) and pruning-based approaches (MagPrune, Sparse Variational Dropout, Soft Threshold Pruning). Experiments are conducted on MovieLens-100K/1M datasets for collaborative filtering and Amazon datasets for content-based recommendation. The study introduces MagPrune as a simple magnitude-based pruning baseline and evaluates all methods across multiple compression rates (10-95% sparsity). Performance metrics include NDCG@K and Recall@K for ranking effectiveness, along with efficiency measurements on both GPU workstations and Raspberry Pi devices.

## Key Results
- MagPrune, a simple magnitude-based pruning baseline, achieves competitive performance across compression rates
- Distinct performance patterns emerge: simple methods perform well at low sparsity rates while complex methods excel at high sparsity rates
- Compositional methods show better training efficiency while pruning methods are more suitable for edge deployment

## Why This Works (Mechanism)
The effectiveness of MagPrune stems from its ability to identify and remove less important weights based on magnitude, preserving the most influential connections for recommendation tasks. The performance patterns observed suggest that different methods have inherent trade-offs - simple methods maintain core functionality at lower compression rates, while complex methods with more sophisticated pruning strategies can better preserve recommendation quality at higher compression rates by maintaining more nuanced weight relationships.

## Foundational Learning

**Embedding-based recommender systems**: Why needed - Form core of modern recommendation pipelines by representing users/items as vectors for similarity computation. Quick check - Verify embedding dimensionality and initialization methods used in baseline implementations.

**Magnitude-based pruning**: Why needed - Provides computationally efficient model compression by removing weights below threshold. Quick check - Confirm threshold selection criteria and its impact on recommendation quality.

**Compression rate evaluation**: Why needed - Critical for understanding trade-offs between model size and performance. Quick check - Validate sparsity percentages and corresponding performance metrics across different methods.

## Architecture Onboarding

**Component map**: Dataset preprocessing -> Model training -> Pruning/compression -> Inference evaluation -> Efficiency benchmarking

**Critical path**: Data loading → Model initialization → Training (with or without pruning) → Inference computation → Metric calculation → Efficiency measurement

**Design tradeoffs**: Simplicity vs. performance (MagPrune vs. complex methods), training efficiency vs. inference efficiency, general applicability vs. task-specific optimization

**Failure signatures**: Performance degradation at extreme compression rates, inconsistency across different datasets, hardware-specific efficiency bottlenecks

**First experiments**: 1) Baseline performance comparison without compression, 2) MagPrune effectiveness across varying sparsity levels, 3) Cross-dataset generalization of performance patterns

## Open Questions the Paper Calls Out
The paper identifies several important open questions: How well do these LERS methods generalize to sequential recommendation tasks and multi-domain scenarios? What is the impact of different pruning strategies on recommendation diversity and novelty metrics? How do efficiency measurements vary across different edge device configurations and real-world deployment scenarios?

## Limitations
- Limited to two recommendation tasks, may not generalize to sequential or multi-domain recommendations
- Evaluation focuses primarily on dense retrieval and rank correlation metrics, potentially overlooking user diversity and novelty
- Edge deployment performance claims may vary significantly with different hardware configurations

## Confidence
High - Comparative analysis of LERS methods under controlled conditions
Medium - Efficiency claims on Raspberry Pi devices due to hardware variability
Low-Medium - Generalizability of MagPrune baseline across different dataset characteristics

## Next Checks
1. Replicate benchmarking across additional recommendation tasks (sequential, multi-domain) to test generalizability
2. Conduct ablation studies on MagPrune to identify key effectiveness components and dataset transferability
3. Evaluate robustness of efficiency claims across wider range of edge devices with varying computational capabilities