---
ver: rpa2
title: 'CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs'
arxiv_id: '2404.01343'
source_url: https://arxiv.org/abs/2404.01343
tags:
- customer
- llms
- service
- classifier
- apis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes CHOPS, an LLM-based customer service framework
  that integrates with existing user profile systems and databases. It uses a classifier-executor-verifier
  architecture to accurately answer user queries or perform system operations while
  avoiding harmful actions.
---

# CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs

## Quick Facts
- arXiv ID: 2404.01343
- Source URL: https://arxiv.org/abs/2404.01343
- Reference count: 10
- Key outcome: CHOPS achieves 98% accuracy on system operations and 99% on file-based questions using a classifier-executor-verifier architecture with mixed LLM sizes

## Executive Summary
CHOPS is an LLM-based customer service framework that integrates with existing user profile systems and databases to handle customer queries and perform system operations while avoiding harmful actions. The system uses a three-stage architecture where a classifier determines query type, an executor generates responses or API calls, and a verifier validates the output with iterative feedback. By strategically using smaller LLMs for classification and verification while reserving larger models for complex execution tasks, CHOPS achieves high accuracy while reducing inference costs compared to using large models throughout.

## Method Summary
The CHOPS framework implements a classifier-executor-verifier architecture that processes customer queries through specialized stages. The classifier determines whether queries require system APIs or guide files, the executor performs the appropriate operations, and the verifier validates outputs with feedback loops for improvement. The system is trained on the CPHOS-dataset containing real customer service interactions from a physics olympiad organization, using prompt-tuning on LLMs (gpt-4 and gpt-3.5-turbo). The approach combines smaller LLMs for simpler tasks with larger models for complex operations to optimize cost-performance tradeoffs.

## Key Results
- Achieves 98% accuracy on system operations and 99% on file-based questions
- Reduces inference cost by using smaller LLMs for classifier and verifier roles
- Outperforms baselines using naive large LLM approaches while maintaining better cost efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The classifier-executor-verifier architecture improves accuracy by decomposing the task into specialized stages
- Mechanism: The classifier determines whether a query needs system APIs or guide files, allowing the executor to focus on the relevant domain. The verifier then validates the executor's response and provides feedback for re-execution if needed
- Core assumption: Each LLM agent can effectively handle its specialized role when given appropriate context and prompts
- Evidence anchors:
  - [abstract]: "leveraging the combination of small and large LLMs together to provide satisfying performance while having decent inference cost"
  - [section]: "The Classifier is given the UserTexts, the System API descriptions and several relevant (and short) chunks from the guiding files"
  - [corpus]: Weak - related papers discuss RAG and API usage but don't specifically address this three-stage decomposition approach
- Break condition: If the classifier cannot accurately determine the query type, the executor may receive irrelevant context, leading to poor performance

### Mechanism 2
- Claim: Using smaller LLMs for classifier and verifier while reserving larger LLMs for executor achieves high accuracy at lower cost
- Mechanism: The classifier and verifier tasks are simpler and can be handled by smaller, cheaper models, while the executor handles the more complex task of generating accurate responses or API calls
- Core assumption: Simpler tasks can be effectively handled by smaller models without significant accuracy loss
- Evidence anchors:
  - [abstract]: "by using weaker LLMs in our architecture, we can achieve significantly better performance compared to naively using stronger LLMs while saving cost"
  - [section]: "by substituting the Executor backbone with gpt-4, we can reach accuracy of above 98% on both accuracy metrics, hugely surpassing the plain gpt-4 baseline while even cost less than the gpt-4 baseline"
  - [corpus]: Weak - related papers don't specifically discuss cost-performance tradeoffs of using different model sizes for different roles
- Break condition: If the smaller models are too weak to handle their tasks effectively, the overall accuracy will suffer

### Mechanism 3
- Claim: The verifier's feedback loop improves accuracy by catching and correcting errors
- Mechanism: The verifier checks the executor's output and, if invalid, provides a reason for failure. This information is then used by the classifier and executor in subsequent iterations to improve their output
- Core assumption: The verifier can accurately identify errors and provide useful feedback for correction
- Evidence anchors:
  - [abstract]: "employing a verifier agent to assess and ensure the validity of commands executed by another LLM"
  - [section]: "If invalid, then the whole process will be redone, while the Classifier and Executor can see the invalid reason provided by the verifier as a reference"
  - [corpus]: Weak - related papers don't specifically discuss this iterative verification approach
- Break condition: If the verifier is too lenient or too strict, it may either fail to catch errors or reject valid outputs, reducing overall accuracy

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: To efficiently retrieve relevant information from guide files without overwhelming the executor with unnecessary context
  - Quick check question: How does RAG improve the performance of LLMs in knowledge-intensive tasks?

- Concept: API wrapping and repository pattern
  - Why needed here: To provide a safe and controlled interface for LLMs to interact with the database, preventing harmful operations
  - Quick check question: Why is it better to wrap database operations in APIs rather than allowing LLMs to generate SQL commands directly?

- Concept: Multimodal input classification
  - Why needed here: To determine whether a query requires system APIs, guide files, or both, optimizing the use of computational resources
  - Quick check question: How does classifying the input type improve the efficiency of the system?

## Architecture Onboarding

- Component map:
  - Classifier -> Executor -> Verifier
  - RAG system (integrated with guide files)
  - API wrapper (for database operations)

- Critical path:
  1. User query is classified by the classifier
  2. Relevant context is provided to the executor
  3. Executor generates a response or API call
  4. Verifier validates the output
  5. If invalid, feedback is provided and the process repeats

- Design tradeoffs:
  - Using smaller models for classifier and verifier reduces cost but may impact accuracy if the models are too weak
  - The number of iterations in the verification loop affects both accuracy and latency
  - The granularity of input classification impacts the efficiency of context provision to the executor

- Failure signatures:
  - High rate of invalid outputs from the executor indicates issues with context provision or model capability
  - Classifier consistently misclassifying queries suggests problems with the classification prompt or model
  - Verifier failing to catch errors or rejecting valid outputs points to issues with the verification prompt or model

- First 3 experiments:
  1. Test the classifier's accuracy in determining query types with a diverse set of queries
  2. Evaluate the executor's performance with and without the classifier's context provision
  3. Measure the impact of the verifier's feedback loop on overall system accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the CHOPS architecture handle ambiguous user queries that could require both system API calls and guidance file retrieval?
- Basis in paper: [explicit] The paper discusses the Classifier-Executor-Verifier architecture but does not detail how it handles ambiguous queries requiring both types of information
- Why unresolved: The paper focuses on classifying queries into distinct categories but doesn't address the scenario where a query might span multiple categories, which is common in customer service
- What evidence would resolve it: A detailed case study or simulation showing how the architecture handles ambiguous queries, including the decision-making process and any fallback mechanisms

### Open Question 2
- Question: What is the impact of using different LLMs for the Classifier, Executor, and Verifier on the overall performance and cost-effectiveness of the CHOPS framework?
- Basis in paper: [explicit] The paper mentions using different LLMs for different roles but does not provide a comprehensive analysis of how these choices affect performance and cost
- Why unresolved: While the paper shows that using gpt-3.5-turbo for Classifier and Verifier with gpt-4 for Executor is effective, it does not explore the full range of possible LLM combinations or their impact on performance metrics
- What evidence would resolve it: An experiment varying the LLMs used in each role, measuring the impact on accuracy, cost, and efficiency, and providing a cost-benefit analysis for different combinations

### Open Question 3
- Question: How scalable is the CHOPS framework when applied to larger, more complex customer service scenarios with thousands of APIs and extensive guiding documents?
- Basis in paper: [inferred] The paper focuses on a specific dataset with a limited number of APIs and documents, implying that scalability might be an issue when applied to more complex scenarios
- Why unresolved: The current experiments are based on a controlled dataset, and there is no evidence or discussion on how the framework would perform in a real-world setting with significantly more complexity
- What evidence would resolve it: A scalability study testing the framework on a larger dataset with thousands of APIs and extensive documents, measuring performance degradation, accuracy, and cost implications

## Limitations

- Evaluation relies on a single dataset from a physics olympiad organization, limiting generalizability to other customer service domains
- The CPHOS-dataset appears relatively small without confidence intervals or statistical significance testing
- The paper does not thoroughly explore the impact of varying verification iterations on accuracy-latency tradeoffs

## Confidence

High confidence: The core architecture design (classifier-executor-verifier) is sound and the reported accuracy numbers are plausible given the approach

Medium confidence: The claimed cost reduction benefits depend heavily on specific implementation details and domain characteristics

Low confidence: The generalizability of results to other customer service domains remains uncertain due to limited evaluation dataset

## Next Checks

1. Test CHOPS on customer service datasets from different domains (e-commerce, technical support, healthcare) to assess generalizability and identify domain-specific limitations

2. Conduct systematic testing to identify edge cases and failure modes, including adversarial queries and complex multi-step requests, documenting types of errors made by each component

3. Perform detailed ablation studies varying model sizes for each component, number of verification iterations, and API operation complexity to establish optimal configurations for different performance and cost requirements