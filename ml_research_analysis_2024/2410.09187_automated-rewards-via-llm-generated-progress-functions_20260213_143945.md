---
ver: rpa2
title: Automated Rewards via LLM-Generated Progress Functions
arxiv_id: '2410.09187'
source_url: https://arxiv.org/abs/2410.09187
tags:
- progress
- task
- reward
- function
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProgressCounts, a method that uses LLMs to
  generate task progress functions and count-based intrinsic rewards to improve sample
  efficiency in reinforcement learning. The approach involves generating progress
  functions that estimate task progress from environment states, discretizing these
  states, and using count-based intrinsic rewards for training.
---

# Automated Rewards via LLM-Generated Progress Functions

## Quick Facts
- arXiv ID: 2410.09187
- Source URL: https://arxiv.org/abs/2410.09187
- Reference count: 40
- Primary result: Achieves 4% higher performance than Eureka on Bi-DexHands benchmark while requiring 20x fewer training samples

## Executive Summary
This paper introduces ProgressCounts, a method that uses LLMs to generate task progress functions and count-based intrinsic rewards to improve sample efficiency in reinforcement learning. The approach involves generating progress functions that estimate task progress from environment states, discretizing these states, and using count-based intrinsic rewards for training. ProgressCounts achieves state-of-the-art performance on the Bi-DexHands benchmark, outperforming the prior method Eureka by 4% while requiring 20 times fewer training samples.

## Method Summary
ProgressCounts automates reward engineering by generating task-specific progress functions using LLMs, then converting these into count-based intrinsic rewards. The method takes task descriptions, environment state descriptions, and helper function libraries as inputs to the LLM, which outputs code that maps environment states to scalar progress measures for one or more subtasks. These progress function outputs are discretized into bins and used to compute state visitation counts, which generate intrinsic rewards proportional to 1/sqrt(visit count). The policy is trained using PPO with combined extrinsic and intrinsic rewards, achieving significantly greater sample efficiency compared to approaches that use LLMs directly for estimating reward weighting and scaling.

## Key Results
- Achieves 4% higher performance than Eureka on Bi-DexHands benchmark
- Requires 20 times fewer training samples than Eureka
- Outperforms human-designed dense reward functions on multiple tasks

## Why This Works (Mechanism)

### Mechanism 1
LLMs generate progress functions that coarsely estimate task progress by identifying key features relevant to the task. The LLM takes task description, environment state description, and helper function library as inputs, then outputs code that maps environment states to scalar progress measures for one or more subtasks.

### Mechanism 2
Count-based intrinsic rewards derived from discretized progress function outputs provide effective learning signals. Progress function outputs are treated as simplified state representations, discretized into bins, and state visitation counts are used to generate intrinsic rewards proportional to 1/sqrt(visit count).

### Mechanism 3
Using progress functions with count-based rewards is more sample-efficient than using LLMs to generate dense reward functions directly. The constrained approach of generating progress functions and applying count-based rewards requires fewer iterations than searching through many complex reward functions.

## Foundational Learning

- **Reinforcement Learning with sparse rewards**: Needed to address the challenge of learning from sparse rewards by introducing dense reward signals through progress estimation. Quick check: Why are sparse rewards problematic for reinforcement learning algorithms like PPO?
- **Count-based exploration and intrinsic motivation**: Needed to use state visitation counts to generate intrinsic rewards that encourage exploration in under-visited states. Quick check: How does the formula 1/sqrt(visit count) encourage exploration of less-visited states?
- **Large Language Models for code generation**: Needed to leverage LLMs to generate code for progress functions based on task descriptions and environment information. Quick check: What are the key inputs provided to the LLM to generate effective progress function code?

## Architecture Onboarding

- **Component map**: Task description → LLM progress function generation → Progress function application → State discretization → Count tracking → Intrinsic reward computation → RL training
- **Critical path**: Task description → LLM progress function generation → Progress function application → State discretization → Count tracking → Intrinsic reward computation → RL training
- **Design tradeoffs**: Accuracy vs. simplicity in progress function generation; Granularity of discretization vs. computational tractability; Intrinsic reward weight vs. extrinsic reward signal balance
- **Failure signatures**: Poor policy performance despite successful training; Progress function outputs that don't correlate with actual task progress; Discretization that creates too few or too many bins; Count-based rewards that don't provide meaningful exploration signals
- **First 3 experiments**:
  1. Test LLM-generated progress function on a simple environment to verify it outputs reasonable progress measures
  2. Verify discretization of progress function outputs creates meaningful state bins
  3. Test count-based intrinsic rewards in isolation on a simple exploration task

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal granularity for discretizing progress variables in ProgressCounts? The paper mentions that subtasks are discretized with finer granularity to encourage more exploration closer to the goal, but does not provide specific guidelines for determining optimal granularity.

### Open Question 2
How does ProgressCounts perform on tasks with more than two subtasks? The paper primarily discusses tasks with one or two subtasks in the Bi-DexHands benchmark and does not explicitly address tasks with more complex multi-stage structures.

### Open Question 3
How sensitive is ProgressCounts to the choice of feature engineering library? The paper mentions that the environment feature engineering library helps the LLM generate effective progress features, but does not provide a detailed analysis of its impact on performance.

## Limitations
- Relies heavily on LLM's ability to generate meaningful progress functions with limited prompt engineering details
- 20x sample efficiency improvement claim is based on a single benchmark (Bi-DexHands)
- Discretization of progress function outputs introduces approximation that could lose important state information

## Confidence
- **High confidence**: Fundamental mechanism of using LLM-generated progress functions with count-based rewards
- **Medium confidence**: 20x sample efficiency claim due to limited benchmarking
- **Medium confidence**: Generalizability across different task domains

## Next Checks
1. Test ProgressCounts on additional benchmark suites beyond Bi-DexHands to verify generalizability
2. Conduct ablation studies to isolate the contribution of LLM-generated progress functions versus count-based rewards
3. Analyze the sensitivity of performance to different discretization granularities and intrinsic reward coefficients