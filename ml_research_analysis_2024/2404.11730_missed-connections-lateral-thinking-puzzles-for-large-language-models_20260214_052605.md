---
ver: rpa2
title: 'Missed Connections: Lateral Thinking Puzzles for Large Language Models'
arxiv_id: '2404.11730'
source_url: https://arxiv.org/abs/2404.11730
tags:
- word
- words
- puzzle
- group
- connections
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of large language models (LLMs) and
  sentence embeddings to solve the Connections puzzle, a word association game published
  by the New York Times. The authors implement automated solvers for both the standard
  version and a more challenging variant where all four groups must be guessed simultaneously.
---

# Missed Connections: Lateral Thinking Puzzles for Large Language Models

## Quick Facts
- arXiv ID: 2404.11730
- Source URL: https://arxiv.org/abs/2404.11730
- Authors: Graham Todd; Tim Merino; Sam Earle; Julian Togelius
- Reference count: 31
- Key outcome: GPT-4 achieves 29.2% success rate on Connections puzzles, significantly outperforming other models

## Executive Summary
This paper explores the use of large language models (LLMs) and sentence embeddings to solve the Connections puzzle, a word association game published by the New York Times. The authors implement automated solvers for both the standard version and a more challenging variant where all four groups must be guessed simultaneously. Their experiments show that GPT-4 achieves a 29.2% success rate on the standard puzzle, significantly outperforming other models like GPT-3.5 (6.43%) and various sentence embedding baselines (11.6% best). Chain-of-thought prompting notably improves GPT-4's performance to 38.93%. The study reveals that while LLMs can solve many puzzles, they struggle with non-semantic properties and abstract features of words, suggesting limitations in current models' understanding of nuanced linguistic associations.

## Method Summary
The paper presents automated solvers for the Connections puzzle using both sentence embeddings and large language models. The authors collect 250 Connections puzzles from June 2023 to February 2024, then implement multiple approaches including BERT, RoBERTa, MPNet, and MiniLM sentence embeddings, as well as GPT-3.5 Turbo and GPT-4 Turbo. They test both standard (finding groups sequentially) and challenge (finding all four groups simultaneously) variants, using iterative and all-in-one solving strategies. The evaluation measures success rates and breaks down performance by category difficulty.

## Key Results
- GPT-4 achieves 29.2% success rate on standard Connections puzzles, outperforming GPT-3.5 (6.43%) and sentence embeddings (11.6% best)
- Chain-of-thought prompting improves GPT-4's performance to 38.93%
- Sentence embedding approach solves puzzles with 417 of 500 allowed guesses on average
- Models struggle significantly with non-semantic properties and abstract word features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's superior performance stems from better semantic encoding of word relationships in its latent representations.
- Mechanism: The model's training on vast text corpora allows it to capture nuanced word associations and abstract relationships that other models miss.
- Core assumption: Larger model size and training data directly translate to better semantic understanding of word groupings.
- Evidence anchors:
  - [abstract] "GPT-4 achieves a 29.2% success rate on the standard puzzle, significantly outperforming other models like GPT-3.5 (6.43%)"
  - [section VI-A] "GPT-4- TURBO , has a 'knowledge cut-off date' of April 2023, the point at which its training data was collected"
  - [corpus] FMR score of 0.6719 for COLUMBUS paper suggests strong semantic reasoning capabilities

### Mechanism 2
- Claim: Chain-of-thought prompting improves performance by forcing explicit reasoning about word relationships.
- Mechanism: By requiring the model to explain its reasoning before providing an answer, it engages in deeper processing of the word associations.
- Core assumption: Language models can engage in multi-step reasoning when prompted to do so.
- Evidence anchors:
  - [abstract] "Chain-of-thought prompting notably improves GPT-4's performance to 38.93%"
  - [section VI-B] "Chain-of-thought is a prompting technique that instructs the model to work through a problem in multiple steps"
  - [corpus] Average neighbor FMR of 0.487 indicates the task requires complex reasoning

### Mechanism 3
- Claim: Sentence embeddings capture semantic relationships but require extensive search to find correct groupings.
- Mechanism: The embeddings encode word meanings and relationships, but the Connections puzzle requires finding specific combinations among millions of possibilities.
- Core assumption: High-dimensional vector representations can encode the semantic relationships needed for word grouping.
- Evidence anchors:
  - [section VI-A] "the straightforward sentence embedding approach solves every one of the 150 puzzles within 417 of the allowed 500 incorrect guesses"
  - [section VI-A] "There are over 2.5 million possible four-group guesses for a given Connections puzzle"
  - [corpus] FMR scores ranging from 0.4731 to 0.6719 suggest varying degrees of semantic capture

## Foundational Learning

- Concept: Cosine similarity in high-dimensional spaces
  - Why needed here: Used to measure semantic similarity between word embeddings when clustering words into groups
  - Quick check question: If two word embeddings have a cosine similarity of 0.8, are they more or less semantically related than embeddings with a similarity of 0.3?

- Concept: Chain-of-thought reasoning
  - Why needed here: Improves model performance by forcing explicit step-by-step reasoning about word relationships
  - Quick check question: How does requiring a model to explain its reasoning before answering potentially improve its final answer?

- Concept: Large language model prompting techniques
  - Why needed here: Different prompting strategies (iterative vs all-in-one, with/without CoT) significantly affect performance
  - Quick check question: What is the difference between an iterative solving approach and an all-in-one approach for this puzzle task?

## Architecture Onboarding

- Component map: Data collection pipeline -> Puzzle solver interface -> Embedding models (BERT, RoBERTa, MPNet, MiniLM) -> LLM API wrappers (GPT-3.5 Turbo, GPT-4 Turbo) -> Evaluation framework
- Critical path: 1. Fetch puzzle from archive 2. Process puzzle through solver 3. Parse and validate model response 4. Submit guess to puzzle interface 5. Record outcome and continue until solved or out of guesses
- Design tradeoffs:
  - Iterative solving allows for progressive refinement but may lead to rabbit holes
  - All-in-one solving is more challenging but requires less interaction
  - Sentence embeddings are deterministic but require extensive search
  - LLMs are more flexible but can produce invalid outputs
- Failure signatures:
  - Consistently poor performance on non-semantic categories
  - Getting stuck in rabbit holes after nearly correct guesses
  - Producing malformed outputs that don't follow instructions
  - Performance degradation when word order is randomized
- First 3 experiments:
  1. Compare baseline embedding model performance with varying numbers of allowed guesses
  2. Test GPT-4 with and without chain-of-thought prompting on the same puzzle set
  3. Evaluate both standard and challenge variants using the best-performing approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would a training set of Connections puzzles affect LLM performance?
- Basis in paper: [explicit] The paper mentions that using some puzzles as training data remains an interesting question for future work.
- Why unresolved: The authors chose not to split their dataset into training and evaluation sets, focusing instead on using the puzzles solely for evaluation. This decision leaves open the question of whether fine-tuning on Connections data would significantly improve model performance.
- What evidence would resolve it: Conducting experiments with models trained on a subset of Connections puzzles and comparing their performance to the current results would provide insight into the potential benefits of training data.

### Open Question 2
- Question: What is the impact of word ordering on LLM performance in Connections puzzles?
- Basis in paper: [explicit] The authors note a discrepancy in performance when comparing their results to a prior study, attributing it to differences in word ordering presentation.
- Why unresolved: While the authors replicated the prior study's results by sorting words, they did not systematically explore how different word orderings affect LLM performance or whether models can adapt to randomized word orders.
- What evidence would resolve it: Systematically testing LLM performance with various word orderings (random, sorted by category, sorted alphabetically) would clarify the importance of word presentation in puzzle-solving.

### Open Question 3
- Question: How do LLM and human solving strategies differ in Connections puzzles?
- Basis in paper: [explicit] The authors express interest in comparing LLM and human player behaviors, noting that category difficulty aligns with LLM success rates but questioning whether they solve puzzles in the same order.
- Why unresolved: The paper does not provide any direct comparison between LLM and human solving patterns, leaving open questions about strategy differences and cognitive processes.
- What evidence would resolve it: Conducting user studies where human players solve Connections puzzles while being observed, and comparing their strategies and success rates to LLM approaches, would provide insights into differences in solving methods.

### Open Question 4
- Question: What role do non-semantic properties play in LLM puzzle-solving, and how can models be improved to recognize them?
- Basis in paper: [inferred] The authors note that LLMs struggle with categories involving non-semantic properties, abstract features, or contextual usage, suggesting limitations in current models' understanding of nuanced linguistic associations.
- Why unresolved: While the paper identifies this as a limitation, it does not explore specific techniques or model architectures that might better capture these non-semantic associations.
- What evidence would resolve it: Developing and testing models that incorporate additional knowledge bases (like WordNet) or use multi-modal approaches to capture non-semantic properties would demonstrate whether such enhancements improve puzzle-solving performance.

### Open Question 5
- Question: Can LLMs generate novel Connections puzzles that are both solvable and challenging?
- Basis in paper: [explicit] The authors express interest in using LLMs to generate novel linguistic puzzles, noting that this task is harder than solving them as it requires creativity and design sense in addition to semantic understanding.
- Why unresolved: The paper focuses on solving existing puzzles and does not attempt to generate new ones, leaving the feasibility and quality of LLM-generated puzzles unexplored.
- What evidence would resolve it: Implementing an LLM-based Connections puzzle generator and evaluating the generated puzzles for solvability, difficulty balance, and creativity would determine whether LLMs can effectively create challenging new puzzles.

## Limitations

- The paper doesn't fully address the challenge of non-semantic word properties that language models cannot infer from text alone
- Performance metrics are based on a relatively small sample of 250 puzzles, which may not capture the full difficulty distribution
- The sentence embedding approach requires extensive search (up to 417 of 500 guesses) rather than intelligent reasoning

## Confidence

**High Confidence**: The relative performance ranking of different models (GPT-4 > GPT-3.5 > sentence embeddings) is well-supported by the experimental data. The mechanism of chain-of-thought prompting improving performance through explicit reasoning is also strongly evidenced.

**Medium Confidence**: The claim that GPT-4's superior performance stems from better semantic encoding of word relationships is plausible but not definitively proven. Alternative explanations, such as better pattern matching or memorization of common word groupings from training data, are not ruled out.

**Low Confidence**: The assertion that LLMs struggle specifically with non-semantic properties is based on qualitative observations rather than systematic testing. The paper mentions this limitation but doesn't provide comprehensive evidence of which specific types of non-semantic properties cause failures.

## Next Checks

1. **Systematic testing of non-semantic properties**: Design a controlled experiment testing model performance on puzzles that specifically target visual, phonetic, or other non-semantic word properties to quantify this limitation.

2. **Cross-dataset generalization**: Evaluate the best-performing models on Connections puzzles from different time periods or sources to test whether performance is consistent or overfits to the specific puzzle set used in the study.

3. **Embedding optimization analysis**: Compare the performance of general-purpose sentence embeddings against embeddings specifically trained or fine-tuned for word grouping tasks to determine if the search efficiency can be improved.