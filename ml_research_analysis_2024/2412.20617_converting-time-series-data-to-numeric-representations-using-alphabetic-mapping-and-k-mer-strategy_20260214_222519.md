---
ver: rpa2
title: Converting Time Series Data to Numeric Representations Using Alphabetic Mapping
  and k-mer strategy
arxiv_id: '2412.20617'
source_url: https://arxiv.org/abs/2412.20617
tags:
- time
- series
- data
- sequence
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method to transform time series data
  into biological sequence-type representations using alphabetic mapping and k-mer
  strategies. The approach involves dividing the range of time series values into
  26 intervals corresponding to the 26 letters of the English alphabet, then mapping
  each value to a specific character based on its range.
---

# Converting Time Series Data to Numeric Representations Using Alphabetic Mapping and k-mer strategy

## Quick Facts
- arXiv ID: 2412.20617
- Source URL: https://arxiv.org/abs/2412.20617
- Authors: Sarwan Ali; Tamkanat E Ali; Imdad Ullah Khan; Murray Patterson
- Reference count: 25
- Primary result: Novel method converting time series to biological sequence-type representations using alphabetic mapping and k-mer strategies

## Executive Summary
This paper presents a novel approach for transforming time series data into biological sequence-type representations using alphabetic mapping and k-mer strategies. The method divides the range of time series values into 26 intervals corresponding to the 26 letters of the English alphabet, then maps each value to a specific character based on its range. These character sequences are analyzed using k-mer techniques from bioinformatics to create numeric embeddings for classification tasks. The approach was evaluated on a human activity recognition dataset with smartphone sensor data, demonstrating consistent improvements over baseline approaches across multiple classification targets.

## Method Summary
The method transforms time series data into symbolic sequences by dividing the continuous value range into 26 equal intervals, each mapped to an English alphabet letter. The time series values are converted to character sequences based on which interval they fall into, preserving temporal ordering and relative magnitudes. These sequences are then analyzed using k-mer techniques, which segment the sequences into overlapping substrings of length k to create a feature space. The k-mer counts form numeric embeddings that can be used with standard classification algorithms. The approach was evaluated on a smartphone sensor dataset for age, gender, application, and hand usage classification, consistently outperforming traditional feature engineering and deep learning baselines.

## Key Results
- Achieved 0.953 accuracy and 0.967 ROC-AUC for age prediction using logistic regression, improving over baseline by 2.8% and 1.9% respectively
- Consistently outperformed baseline approaches across all evaluation metrics except training runtime
- Demonstrated superior classification performance while being more accessible and resource-efficient than deep learning approaches
- Validated on human activity recognition dataset with 112 samples from 29 users using smartphone sensors

## Why This Works (Mechanism)

### Mechanism 1
Alphabetic mapping preserves relative magnitudes while creating symbolic sequences by dividing the continuous value range into 26 equal intervals and mapping each to a letter, creating symbolic representations where order and relative spacing are preserved through character sequence.

### Mechanism 2
k-mer analysis captures local temporal patterns that are effective for classification by segmenting character sequences into overlapping substrings of length k, creating a feature space where local temporal patterns become distinguishable features for classification algorithms.

### Mechanism 3
Sequence-based representation is more effective than traditional statistical features for certain classification tasks because the symbolic sequence representation combined with k-mer features captures sequential dependencies and patterns that traditional statistical feature engineering might miss.

## Foundational Learning

- **Time series discretization and symbolic representation**: Why needed - The method fundamentally transforms continuous time series into symbolic sequences, requiring understanding of how discretization affects information preservation. Quick check - What happens to the information content when you map continuous values to discrete symbols? How does the number of intervals affect this?

- **k-mer analysis in bioinformatics**: Why needed - The k-mer strategy is borrowed from bioinformatics, where it's used for sequence analysis. Understanding its application in that domain helps grasp its use here. Quick check - How do k-mer frequencies capture sequence characteristics? What does k represent and how does it affect the granularity of pattern detection?

- **Sequence classification techniques**: Why needed - The method applies sequence classification algorithms to the transformed data, requiring understanding of how these algorithms work with symbolic sequences. Quick check - How do sequence classification algorithms differ from traditional time series classification? What types of patterns are they particularly good at detecting?

## Architecture Onboarding

- **Component map**: Data preprocessing (flattening, range computation) → Alphabetic mapping (Algorithm 3) → k-mer computation (Algorithm 4) → Feature vector creation (spectrum of k-mer counts) → Classification
- **Critical path**: The alphabetic mapping and k-mer computation are the core transformation steps that must be correct for the method to work
- **Design tradeoffs**: Equal-interval mapping vs. adaptive binning; k-mer size selection; choice of classification algorithm
- **Failure signatures**: Poor classification performance could indicate inadequate range boundaries, inappropriate k-mer size, or loss of critical information during discretization
- **First 3 experiments**:
  1. Verify that alphabetic mapping preserves relative ordering by checking if sorted sequences remain sorted
  2. Test different k-mer sizes (2, 3, 4) on a small dataset to observe impact on classification performance
  3. Compare classification results using the proposed method versus using the raw statistical features from the baseline method

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the proposed method scale with larger alphabets beyond 26 letters for finer-grained time series representation? The paper uses 26 ranges corresponding to English alphabet letters but doesn't explore performance with different alphabet sizes.

### Open Question 2
What is the impact of k-mer size selection on classification performance across different types of time series data? The paper uses a fixed k-mer size of 3 without exploring the sensitivity to this parameter.

### Open Question 3
How does the proposed method handle irregular or unevenly sampled time series data? The method assumes regularly sampled time series but doesn't address irregular sampling scenarios.

### Open Question 4
What is the theoretical justification for why alphabetic mapping preserves discriminative information in time series data? The paper demonstrates empirical success but lacks formal analysis of information preservation through the mapping process.

## Limitations
- Limited generalizability beyond the specific human activity recognition dataset with only 112 samples from 29 users
- Equal-interval mapping assumes uniform information distribution, which may not hold for skewed or multimodal distributions
- Sensitivity to parameter choices (particularly k-mer size) is not thoroughly explored with k=3 chosen without systematic validation

## Confidence
- **High Confidence (90%+)**: The core algorithmic framework for converting time series to alphabetic sequences and extracting k-mer features is mathematically sound and reproducible
- **Medium Confidence (70-89%)**: Classification performance improvements over baseline methods are well-documented for the specific dataset used, but magnitude may vary across different datasets
- **Low Confidence (below 70%)**: Claims about superiority in handling limited labeled data and accessibility advantages over deep learning require additional validation on diverse datasets

## Next Checks
1. **Dataset Diversity Test**: Apply the method to multiple time series classification datasets from different domains (medical, financial, industrial) to assess cross-domain performance consistency
2. **Parameter Sensitivity Analysis**: Conduct systematic experiments varying the number of alphabetic intervals and k-mer sizes across a range of values to determine optimal configurations
3. **Ablation Study**: Perform controlled experiments comparing the full method against variations to quantify each component's contribution to overall performance