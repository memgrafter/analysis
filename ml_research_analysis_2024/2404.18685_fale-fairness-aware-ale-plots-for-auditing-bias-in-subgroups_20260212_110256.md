---
ver: rpa2
title: 'FALE: Fairness-Aware ALE Plots for Auditing Bias in Subgroups'
arxiv_id: '2404.18685'
source_url: https://arxiv.org/abs/2404.18685
tags:
- fairness
- subgroups
- fale
- plots
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FALE (Fairness-aware Accumulated Local Effects)
  plots, an extension of ALE plots, for auditing bias in subgroups within machine
  learning models. The key innovation is adapting ALE plots to measure the change
  in fairness for affected populations corresponding to different values of a feature,
  rather than measuring the average model prediction.
---

# FALE: Fairness-Aware ALE Plots for Auditing Bias in Subgroups

## Quick Facts
- arXiv ID: 2404.18685
- Source URL: https://arxiv.org/abs/2404.18685
- Reference count: 21
- Proposes FALE plots to identify bias in subgroups using visual explanations

## Executive Summary
This paper introduces FALE (Fairness-aware Accumulated Local Effects) plots, an extension of ALE plots designed to audit bias in subgroups within machine learning models. The method adapts ALE plots to measure changes in fairness metrics rather than model predictions, allowing users to visualize how different feature values affect fairness disparities between protected and non-protected groups. Demonstrated on the Adult dataset using XGBoost with sex as the sensitive attribute, FALE plots show how statistical parity fairness varies across subgroups defined by attributes like age, education level, and hours worked per week. The approach provides a user-friendly, visual first-stage tool for quickly identifying potential bias issues in specific population subgroups.

## Method Summary
FALE extends ALE plots by replacing the model prediction difference with a fairness measure difference in the local effect calculation. The method partitions the domain of a feature into bins and calculates the change in a chosen fairness metric (such as statistical parity difference) between protected and non-protected groups within each bin. This generates a plot showing how the measured fairness changes across different values of the examined feature, enabling users to identify subgroups where bias is increased or decreased compared to the average population.

## Key Results
- Successfully extends ALE plots to measure fairness impacts instead of model predictions
- Visualizes how fairness disparities vary across different feature values and subgroups
- Demonstrated effectiveness on the Adult dataset with XGBoost classifier and sex as sensitive attribute
- Shows increased or decreased unfairness for subgroups defined by age, education level, and hours per week
- Provides intuitive, user-friendly visualization for first-stage bias identification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FALE adapts ALE plots to measure fairness instead of model prediction
- Mechanism: By replacing the model prediction difference with a fairness measure difference in the local effect calculation, FALE quantifies how much a feature value changes the fairness disparity between protected and non-protected groups
- Core assumption: The fairness measure u(G0, G1) is differentiable and meaningful for small changes in group composition
- Evidence anchors:
  - [abstract] "extend the ALE plots explainability method, proposing FALE (Fairness aware Accumulated Local Effects) plots, a method for measuring the change in fairness for an affected population corresponding to different values of a feature (attribute)"
  - [section] "we extend them in order to calculate, instead of the average model prediction, the measured fairness with respect to a selected statistical fairness definition [19] and a sensitive attribute"
- Break condition: If the fairness measure is not sensitive to local changes or if the bin partitioning is too coarse to capture meaningful subgroup differences

### Mechanism 2
- Claim: FALE enables user-friendly identification of biased subgroups
- Mechanism: By visualizing the change in measured fairness across feature values, users can quickly identify which subgroups experience increased or decreased bias compared to the average population
- Core assumption: Users can interpret the FALE plot correctly and understand the relationship between feature values, subgroup populations, and fairness impact
- Evidence anchors:
  - [abstract] "FALE plots provide a user-friendly, comprehensible way to visualize fairness impacts across subgroups, serving as an efficient first-stage tool for bias identification"
  - [section] "Our method can serve as a first-step auditing tool to help users quickly point their focus on specific subgroups to investigate fairness issues"
- Break condition: If users misinterpret the plot or if the visualization does not clearly show the relationship between feature values and fairness impact

### Mechanism 3
- Claim: FALE complements existing subgroup fairness auditing methods
- Mechanism: By providing a visual, intuitive representation of fairness impacts, FALE fills a gap in the literature where other methods focus on computational efficiency but lack user-friendly presentation
- Core assumption: Visual explainability methods are more accessible to end users than purely computational methods
- Evidence anchors:
  - [abstract] "A common issue with all aforementioned works is that they hardly focus on presenting their findings in a user intuitive and explainable way"
  - [section] "In this paper, we propose a method that addresses this issue"
- Break condition: If users do not find the visual representation helpful or if the method does not provide additional insights beyond existing computational methods

## Foundational Learning

- Concept: ALE (Accumulated Local Effects) plots
  - Why needed here: FALE builds upon ALE plots, so understanding how ALE plots work is crucial for understanding FALE
  - Quick check question: How do ALE plots measure the local effect of a feature on model predictions?

- Concept: Statistical fairness definitions
  - Why needed here: FALE uses statistical fairness definitions to measure fairness, so understanding these definitions is necessary
  - Quick check question: What is the difference between statistical parity and other fairness definitions like equalized odds?

- Concept: Subgroup fairness
  - Why needed here: FALE is designed to identify bias in subgroups, so understanding the concept of subgroup fairness is essential
  - Quick check question: How does subgroup fairness differ from individual fairness or group fairness?

## Architecture Onboarding

- Component map:
  - Input: ML model, dataset, sensitive attribute, fairness definition, feature to examine
  - Processing: Calculate FALE estimates for each feature value bin
  - Output: FALE plot showing fairness impact across feature values

- Critical path:
  1. Select model, dataset, sensitive attribute, and fairness definition
  2. Calculate fairness measure for each subgroup defined by feature values
  3. Generate FALE plot
  4. Interpret plot to identify biased subgroups

- Design tradeoffs:
  - Granularity of feature bins: finer bins provide more detailed information but may be harder to interpret
  - Choice of fairness definition: different definitions may highlight different aspects of bias
  - Visualization style: different plot styles may be more or less intuitive for different users

- Failure signatures:
  - Plot does not show clear patterns: may indicate insufficient data or inappropriate choice of feature or fairness definition
  - Extreme values in plot: may indicate issues with the fairness measure or data quality

- First 3 experiments:
  1. Apply FALE to a simple synthetic dataset with known bias patterns to verify correct identification
  2. Compare FALE results across different fairness definitions on a real dataset
  3. Test FALE with different bin sizes to understand the impact of granularity on interpretability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is FALE at detecting intersectional fairness issues in real-world datasets compared to existing subgroup fairness auditing methods?
- Basis in paper: [explicit] The authors state that "in real world settings which involve much larger feature sets, as well as feature value ranges, this task becomes infeasible" for exhaustive subgroup investigation, suggesting FALE as an alternative approach.
- Why unresolved: The paper only demonstrates FALE on the Adult dataset and does not compare its effectiveness to other methods on real-world data with many features.
- What evidence would resolve it: Empirical comparison of FALE's detection rate and false positive rate against other subgroup fairness auditing methods on multiple real-world datasets with known intersectional bias.

### Open Question 2
- Question: What is the optimal bin partitioning strategy for FALE to balance computational efficiency and detection accuracy of subgroup fairness issues?
- Basis in paper: [inferred] The paper mentions that "The domain of the feature Xi is partitioned into n bins" but does not discuss how to choose n or the bin boundaries.
- Why unresolved: The authors provide the mathematical framework but leave the practical implementation details open.
- What evidence would resolve it: Experimental results showing detection accuracy and computational time for different bin partitioning strategies (fixed number of bins, adaptive bins based on data distribution, etc.) on datasets of varying sizes.

### Open Question 3
- Question: How does FALE handle non-monotonic relationships between features and fairness metrics?
- Basis in paper: [explicit] The authors state that "FALE values are estimated by the following procedure assuming a dataset" but the estimation method appears to assume monotonic relationships between feature values and fairness.
- Why unresolved: The paper does not discuss or demonstrate how FALE performs when fairness metrics vary non-monotonically with feature values.
- What evidence would resolve it: Analysis showing FALE's ability to correctly identify fairness impacts in scenarios with non-monotonic relationships, potentially using synthetic datasets with known non-monotonic patterns.

## Limitations
- Limited empirical validation with only one dataset (Adult) and one fairness definition (statistical parity)
- Claims about user-friendliness and comprehensibility lack empirical support through user studies
- Does not address how FALE handles non-monotonic relationships between features and fairness metrics

## Confidence
- **High Confidence:** The mathematical extension of ALE plots to incorporate fairness measures is sound and well-defined.
- **Medium Confidence:** The demonstration on the Adult dataset shows the method works as intended, but the results are limited to one dataset and one fairness definition.
- **Low Confidence:** Claims about user-friendliness and comprehensibility lack empirical validation through user studies or comparisons with alternative methods.

## Next Checks
1. Conduct a user study comparing FALE plots with alternative bias auditing methods to measure actual user comprehension and effectiveness in identifying biased subgroups.
2. Apply FALE across multiple fairness definitions (e.g., equalized odds, predictive parity) on the same dataset to assess consistency and identify cases where different definitions reveal different bias patterns.
3. Test FALE on datasets with known bias patterns where ground truth subgroup fairness issues are established, to validate that the method correctly identifies these issues.