---
ver: rpa2
title: 'CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding
  and Reasoning'
arxiv_id: '2401.14011'
source_url: https://arxiv.org/abs/2401.14011
tags:
- questions
- question
- arxiv
- cmmu
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CMMU, a novel Chinese benchmark for evaluating
  multi-modal large language models (MLLMs) on multi-type questions covering primary
  to high school subjects. CMMU contains 3,603 questions across 7 subjects in multiple-choice,
  multiple-response, and fill-in-the-blank formats, posing a more comprehensive challenge
  than existing English benchmarks focused on multiple-choice questions only.
---

# CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning

## Quick Facts
- arXiv ID: 2401.14011
- Source URL: https://arxiv.org/abs/2401.14011
- Authors: Zheqi He; Xinya Wu; Pengfei Zhou; Richeng Xuan; Guang Liu; Xi Yang; Qiannan Zhu; Hua Huang
- Reference count: 7
- Primary result: Top models achieve only ~30% accuracy on CMMU, highlighting significant challenges for MLLMs on Chinese educational content

## Executive Summary
This paper introduces CMMU, a novel Chinese benchmark for evaluating multi-modal large language models (MLLMs) on multi-type questions covering primary to high school subjects. CMMU contains 3,603 questions across 7 subjects in multiple-choice, multiple-response, and fill-in-the-blank formats, posing a more comprehensive challenge than existing English benchmarks focused on multiple-choice questions only. The authors propose ShiftCheck, a strategy for rigorously evaluating multiple-choice questions by cyclically shifting answer positions to reduce position bias and ensure correctness beyond guessing. Evaluations on 11 models including GPT-4V, Gemini-Pro, and open-source MLLMs show that CMMU poses significant challenges, with top models achieving only around 30% accuracy. The benchmark provides detailed error analysis and demonstrates the need for further research to improve MLLMs' domain-specific knowledge and reasoning capabilities.

## Method Summary
CMMU is a Chinese multi-modal benchmark containing 3,603 questions across 7 subjects (math, biology, physics, chemistry, geography, politics, history) in three formats: multiple-choice, multiple-response, and fill-in-the-blank. Questions are presented in JSON format with LaTeX formulas and images, split into validation (1,800) and test (1,803) sets. The evaluation uses zero-shot prompting with specific templates for each question type. For multiple-choice questions, the ShiftCheck strategy is employed, which cyclically shifts answer positions to mitigate position bias. GPT-4 is used to validate fill-in-the-blank answers. Temperature settings are 0 for MCQ/MRQ and 0.2 for FBQ, with max_new_token limits of 10 for MCQ/MRQ and 128 for FBQ.

## Key Results
- Top models (GPT-4V, Gemini-Pro) achieve only ~30% accuracy on CMMU test set
- Significant subject bias observed: models perform better on politics/history (knowledge-based) than physics/math/biology (computation/reasoning-based)
- Position bias reduction via ShiftCheck reveals genuine knowledge gaps in models
- Multiple-response and fill-in-the-blank questions prove more challenging than multiple-choice questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ShiftCheck reduces position bias by cyclically shifting answer options
- Mechanism: The approach rotates answer positions for each multiple-choice question, ensuring the correct answer appears in every position equally. This prevents models from learning positional heuristics rather than actual knowledge.
- Core assumption: MLLMs develop position biases when consistently exposed to static answer orderings in training data
- Evidence anchors:
  - [abstract]: "To mitigate the position bias in LLM and ensure genuine correctness rather than guessing, inspired by CircularEval, we adopt a ShiftCheck approach for multiple-choice questions"
  - [section 4.1]: "We consider the model to have sufficient knowledge to answer the question Q only if all of Ai are correct, in which case the accuracy score of Q is 1, otherwise it is 0"
  - [corpus]: Weak - neighboring papers focus on different bias types, not position bias in MCQs
- Break condition: If models can still learn position patterns despite rotation, or if the number of options is small relative to dataset size

### Mechanism 2
- Claim: Multi-type question formats better assess comprehensive MLLM capabilities
- Mechanism: By including multiple-choice, multiple-response, and fill-in-the-blank questions, CMMU evaluates different cognitive dimensions (recognition, selection, generation) that single-format benchmarks miss.
- Core assumption: Different question types stress different model capabilities beyond what MCQs alone can reveal
- Evidence anchors:
  - [abstract]: "CMMU offers a wider variety of question types, including multiple-choice, multiple-response, and fill-in-the-blank questions, bringing greater challenges to MLLMs"
  - [section 1]: "Previous datasets only have multiple-choice questions, while CMMU offers a wider variety of question types"
  - [corpus]: Moderate - K12Vista and VisScience also use multiple formats, but CMMU is more comprehensive
- Break condition: If models perform similarly across all types despite different formats, suggesting format doesn't add discriminative power

### Mechanism 3
- Claim: Chinese language domain adds evaluation specificity not captured by English benchmarks
- Mechanism: By focusing on Chinese-language educational content, CMMU tests MLLMs' ability to handle non-English domain knowledge with cultural and linguistic specificity.
- Core assumption: English benchmarks don't adequately capture the performance of models on Chinese educational content
- Evidence anchors:
  - [abstract]: "Current multi-modal benchmarks for domain-specific knowledge concentrate on multiple-choice questions and are predominantly available in English"
  - [section 1]: "These two datasets only contain English questions, while some datasets, such as M3Exam, turn attention to the multilingual setting"
  - [corpus]: Strong - TCC-Bench and Edu-Values explicitly validate this assumption for Chinese cultural content
- Break condition: If models achieve similar performance on Chinese and English versions of equivalent content

## Foundational Learning

- Concept: Circular evaluation methodology
  - Why needed here: Understanding how rotating answer positions tests genuine knowledge vs. positional bias
  - Quick check question: If a question has 4 options and a model gets 3 out of 4 shifted versions correct, what is its accuracy score under ShiftCheck?

- Concept: Multi-modal question understanding
  - Why needed here: MLLMs must integrate visual and textual information to answer educational questions correctly
  - Quick check question: What are the three question types in CMMU and how do they differ in the cognitive skills they test?

- Concept: Position bias quantification
  - Why needed here: The BiasRate metric measures how consistently a model favors certain answer positions
  - Quick check question: How is the BiasRate calculated and what does a higher value indicate about model behavior?

## Architecture Onboarding

- Component map: JSON-formatted questions with LaTeX formulas, images, and answer explanations → Question categorization → ShiftCheck evaluation for MCQs → Direct checking for MRQs → GPT-4 validation for FB questions → Result aggregation
- Critical path: Data ingestion → LaTeX conversion → JSON formatting → Question categorization → ShiftCheck evaluation → Bias analysis → Result aggregation
- Design tradeoffs: Trade between comprehensive evaluation (multiple formats) and complexity of implementation (ShiftCheck requires k evaluations per MCQ)
- Failure signatures: High BiasRate indicates positional preference; low accuracy on MRQs suggests inability to select multiple correct answers; poor FBQ performance reveals generation limitations
- First 3 experiments:
  1. Run a small subset of MCQs through ShiftCheck to verify circular rotation implementation
  2. Compare model performance on MRQs vs MCQs to quantify the added difficulty of multi-answer selection
  3. Test the GPT-4 validation prompt on FBQs with known variations to calibrate semantic matching thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the ShiftCheck strategy be optimized to reduce computational cost while maintaining its effectiveness in detecting position bias across different model architectures?
- Basis in paper: [explicit] The paper introduces the ShiftCheck strategy for evaluating multiple-choice questions by cyclically shifting answer positions to detect position bias, but does not explore optimization techniques for reducing computational overhead.
- Why unresolved: The current implementation requires evaluating each question multiple times (once for each shifted position), which could be computationally expensive, especially for large-scale benchmarks. The trade-off between thoroughness and efficiency needs investigation.
- What evidence would resolve it: Comparative studies showing accuracy of bias detection with reduced shift numbers, alternative sampling methods for position analysis, or adaptive strategies that adjust shift depth based on model confidence scores.

### Open Question 2
- Question: What are the key factors contributing to the observed subject-specific performance differences in MLLMs, and how can these insights inform targeted model improvements?
- Basis in paper: [explicit] The paper notes significant subject bias, with models performing better on politics/history (knowledge-based) versus physics/math/biology (computation/reasoning-based), but does not analyze the underlying causes.
- Why unresolved: Understanding whether the performance gap stems from training data distribution, reasoning architecture limitations, or domain-specific knowledge representation would enable more effective model development strategies.
- What evidence would resolve it: Detailed error analysis across subjects, ablation studies on reasoning vs. knowledge components, or training data augmentation experiments focused on underrepresented subjects.

### Open Question 3
- Question: How can the evaluation framework for fill-in-the-blank questions be improved to better handle semantic equivalence and partial correctness in responses?
- Basis in paper: [inferred] The paper mentions using GPT-4 to judge FBQ answers with binary scoring, but does not address challenges with semantic equivalence or partial credit for incomplete but partially correct answers.
- Why unresolved: The current binary evaluation may be too rigid for FBQ questions where multiple valid expressions exist or where answers contain both correct and incorrect components. A more nuanced evaluation metric is needed.
- What evidence would resolve it: Development and validation of multi-level scoring rubrics, comparison of binary versus graded evaluation accuracy, or implementation of semantic similarity measures for answer comparison.

## Limitations

- The benchmark's Chinese language specificity may limit its applicability to non-Chinese educational contexts
- The paper doesn't provide detailed error analysis breakdowns by subject matter or question difficulty
- No ablation studies are presented to quantify the marginal value added by each question type

## Confidence

- **High confidence**: The fundamental design of CMMU as a comprehensive Chinese multi-modal benchmark with diverse question types and subjects
- **Medium confidence**: The claim that CMMU poses significantly greater challenges than existing English benchmarks
- **Low confidence**: The generalizability of ShiftCheck's effectiveness across different MLLM architectures

## Next Checks

1. Replicate the ShiftCheck evaluation on a subset of MCQs with a different MLLM to verify consistent position bias reduction across model architectures
2. Compare CMMU performance with equivalent English benchmarks using cross-lingual question pairs to quantify the added difficulty from language differences
3. Conduct controlled experiments varying question difficulty within each subject to identify whether performance correlates with educational level as expected