---
ver: rpa2
title: Joint Diffusion models in Continual Learning
arxiv_id: '2411.08224'
source_url: https://arxiv.org/abs/2411.08224
tags:
- learning
- diffusion
- generative
- joint
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces JDCL, a continual learning method that mitigates
  catastrophic forgetting by integrating a classifier with a diffusion-based generative
  model into a single joint parametrization. Unlike traditional generative replay,
  which trains separate generative and discriminative models, JDCL jointly optimizes
  both objectives, enabling stable adaptation to new tasks while preserving knowledge
  of previous ones.
---

# Joint Diffusion models in Continual Learning

## Quick Facts
- arXiv ID: 2411.08224
- Source URL: https://arxiv.org/abs/2411.08224
- Authors: Paweł Skierś; Kamil Deja
- Reference count: 40
- One-line primary result: JDCL achieves state-of-the-art performance in continual learning by integrating classifier with diffusion-based generative model into single joint parametrization

## Executive Summary
This paper introduces JDCL, a continual learning method that addresses catastrophic forgetting by integrating a classifier with a diffusion-based generative model into a single joint parametrization. Unlike traditional generative replay that trains separate generative and discriminative models, JDCL jointly optimizes both objectives, enabling stable adaptation to new tasks while preserving knowledge of previous ones. The method employs a two-stage training strategy (local-to-global) and knowledge distillation to further reduce forgetting. Evaluated on CIFAR-10, CIFAR-100, and ImageNet-100 under class-incremental learning, JDCL achieves state-of-the-art performance, surpassing other generative replay techniques and approaching a theoretical upper bound defined by infinite buffer replay.

## Method Summary
JDCL is a continual learning method that mitigates catastrophic forgetting by integrating a classifier with a diffusion-based generative model into a single joint parametrization. The method uses a two-stage training strategy: first, a local model is trained on new task data, then a global model is distilled from both local and previous global models using synthetic rehearsal data. Knowledge distillation is applied to both classification and diffusion losses to preserve task-specific adaptations while integrating new knowledge. The joint model enables stable adaptation to new tasks without catastrophic forgetting, as demonstrated by experimental results on CIFAR-10, CIFAR-100, and ImageNet-100.

## Key Results
- JDCL achieves 83.69% average accuracy on CIFAR-10 with 5 tasks, compared to 64.47% for the previous best method (GUIDE)
- JDCL outperforms buffer-based replay methods when adapted to semi-supervised continual learning
- The method learns more stable and meaningful representations with lower forgetting and better generative quality (measured by FID) than standard diffusion models trained incrementally

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint modeling of generative and discriminative objectives stabilizes classifier training by keeping rehearsal data in-distribution.
- Mechanism: The classifier receives logits from the same UNet that generates rehearsal samples, so the feature distribution for synthetic and real data matches closely.
- Core assumption: The pooled latent vector from the UNet is sufficiently discriminative for classification while being stable across tasks.
- Evidence anchors:
  - [abstract] "Such shared parametrization, combined with the knowledge distillation technique allows for stable adaptation to new tasks without catastrophic forgetting."
  - [section] Fig. 4 shows logit distributions for real vs. synthetic data aligning for joint model but diverging for separate classifier.
  - [corpus] GUIDE paper also uses guidance-based sampling but does not report joint logit alignment; likely weaker in-distribution preservation.

### Mechanism 2
- Claim: Knowledge distillation from a frozen local model to a global model preserves task-specific adaptations while integrating new knowledge.
- Mechanism: After local fine-tuning on new task, the model is distilled into the global model using both classification and diffusion KD losses, which regularizes updates toward previously learned behavior.
- Core assumption: The local model retains task-relevant knowledge that can be transferred via distillation without catastrophic forgetting.
- Evidence anchors:
  - [abstract] "such shared parametrization, combined with the knowledge distillation technique, allows for stable adaptation to new tasks without catastrophic forgetting."
  - [section] LKD definitions in Equations 8–10 and ablation in Tab. 3 showing drop in performance when KD is removed.
  - [corpus] RTF and DGR+Distill also use distillation but with separate models; joint KD likely more effective due to tighter coupling.

### Mechanism 3
- Claim: Two-stage local-to-global training balances plasticity and stability by first adapting locally then consolidating globally.
- Mechanism: The local model is trained only on new task data (high plasticity), then a global model is distilled from both local and previous global models using synthetic rehearsal data (high stability).
- Core assumption: Separating adaptation (local) from consolidation (global) prevents interference between new and old tasks.
- Evidence anchors:
  - [abstract] "two-stage training allows for high-quality modeling in CL scenarios."
  - [section] Fig. 5 (right) shows higher stability on first task with two-stage training; Tab. 3 shows large drop without it.
  - [corpus] Some CL works use episodic memory or replay buffers; two-stage local/global is less common but aligns with neuroscience-inspired consolidation models.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper's motivation is to mitigate abrupt loss of performance on previous tasks when learning new ones.
  - Quick check question: What happens to a CNN's accuracy on task A after fine-tuning on task B without any regularization?
- Concept: Diffusion models and denoising score matching
  - Why needed here: The generative component is a denoising diffusion probabilistic model (DDPM) that provides rehearsal samples.
  - Quick check question: In a DDPM, what is the role of the noise schedule α_t during training?
- Concept: Knowledge distillation in continual learning
  - Why needed here: Distillation loss aligns new model predictions with frozen old model predictions to reduce forgetting.
  - Quick check question: How does classification KD differ from diffusion KD in terms of target signals?

## Architecture Onboarding

- Component map: UNet (denoising diffusion backbone) -> pooled latent vector z -> classifier head -> loss sum (classification + diffusion + KD)
- Critical path: Generate rehearsal samples -> compute joint loss -> update shared parameters -> apply KD -> produce global model
- Design tradeoffs: Joint model reduces parameters and data drift but may limit expressivity of each component separately
- Failure signatures: Diverging logit distributions (Fig. 4), large forgetting spikes, unstable training curves
- First 3 experiments:
  1. Train joint diffusion on single task; verify classifier logits match synthetic data logits
  2. Add KD loss; confirm stability when continuing training on synthetic data
  3. Implement two-stage training; measure forgetting on first task after learning second task

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the limitations and potential future work discussed, some open questions include:
- How does JDCL perform on larger or more complex datasets beyond CIFAR-10, CIFAR-100, and ImageNet-100?
- What is the impact of different data augmentation strategies on JDCL's performance?
- How does JDCL handle class imbalance in the data?
- What is the impact of the two-stage training strategy compared to other training strategies?

## Limitations

- The paper's experiments are limited to CIFAR-10, CIFAR-100, and ImageNet-100, and do not explore performance on larger or more complex datasets
- The choice of data augmentation strategies is not explored, and their impact on JDCL's performance is unclear
- The paper does not address class imbalance in the data, which is a common issue in real-world applications
- The effectiveness of the two-stage training strategy is not fully evaluated, and its advantages over other strategies are not clear

## Confidence

- High: The core mechanism of joint diffusion models with knowledge distillation for mitigating catastrophic forgetting is well-supported by the experimental results
- Medium: The two-stage training strategy's effectiveness in balancing plasticity and stability is demonstrated, but the optimal hyperparameters for this strategy may vary depending on the dataset and task sequence
- Low: The paper's claim of significantly reduced computational cost compared to separate models is not fully substantiated, and the actual computational overhead of the joint diffusion model remains unclear

## Next Checks

1. Implement a minimal version of the joint diffusion model and train it on a single task to verify the classifier logits match the synthetic data logits
2. Add the knowledge distillation loss and confirm stability when continuing training on synthetic data
3. Implement the two-stage training pipeline and measure forgetting on the first task after learning the second task