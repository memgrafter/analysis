---
ver: rpa2
title: 'HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts'
arxiv_id: '2402.12656'
source_url: https://arxiv.org/abs/2402.12656
tags:
- experts
- expert
- knowledge
- information
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel Mixture of Experts (MoE) architecture
  called HyperMoE to address the trade-off between sparsity and expert knowledge availability
  in MoE models. HyperMoE integrates hypernetworks with MoE to transfer knowledge
  among experts while maintaining sparsity.
---

# HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts

## Quick Facts
- arXiv ID: 2402.12656
- Source URL: https://arxiv.org/abs/2402.12656
- Reference count: 30
- Key outcome: HyperMoE achieves +0.48% and +0.84% improvements on GLUE and SuperGLUE benchmarks respectively compared to standard MoE, while maintaining training and inference speeds within 15% and 10% of the baseline.

## Executive Summary
This paper introduces HyperMoE, a novel Mixture of Experts (MoE) architecture that addresses the trade-off between sparsity and expert knowledge availability. The key innovation is using a shared hypernetwork to generate conditional modules called HyperExperts based on unselected expert information, providing supplementary knowledge to selected experts while maintaining sparsity. Experiments on 20 NLP datasets demonstrate that HyperMoE outperforms baseline MoE methods across diverse tasks including sequence classification, question answering, summarization, and text generation, with minimal impact on computational efficiency.

## Method Summary
HyperMoE integrates hypernetworks with MoE to transfer knowledge among experts while maintaining sparsity. The method uses a shared hypernetwork across all transformer layers to generate HyperExperts conditioned on selection embeddings derived from unselected expert information. Selection embeddings encode which unselected expert knowledge is needed for the current input, and the hypernetwork generates supplementary experts that combine with selected expert outputs. This approach captures cross-expert knowledge that unselected experts possess but selected experts lack, improving overall model performance while preserving the efficiency benefits of sparse expert selection.

## Key Results
- HyperMoE achieves +0.48% improvement on GLUE benchmark compared to standard MoE
- HyperMoE achieves +0.84% improvement on SuperGLUE benchmark compared to standard MoE
- Maintains training speed within 15% and inference speed within 10% of baseline MoE methods
- Outperforms baseline MoE methods across 20 NLP datasets spanning diverse tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The shared hypernetwork captures cross-expert knowledge that unselected experts possess but selected experts lack
- Mechanism: The hypernetwork generates HyperExperts conditioned on selection embeddings derived from unselected expert information, providing supplementary knowledge to selected experts
- Core assumption: Unselected experts contain complementary knowledge relevant to the current input that can improve selected expert performance
- Evidence anchors:
  - [abstract]: "Specific modules generated based on the information of unselected experts serve as supplementary information"
  - [section 3.2]: "HyperMoE, which captures the information from every expert by leveraging expert-shared hypernetwork"
  - [corpus]: Weak - no direct corpus evidence for this specific knowledge transfer mechanism
- Break condition: If unselected experts' knowledge is not complementary or relevant to the current input context

### Mechanism 2
- Claim: Selection embeddings effectively encode which unselected expert knowledge is needed for the current input
- Mechanism: Selection embeddings are computed by aggregating unselected expert embeddings weighted by their gating scores, creating a representation of needed knowledge
- Core assumption: The gating mechanism's binary selection pattern contains information about which unselected experts would be beneficial
- Evidence anchors:
  - [section 3.2]: "We define the selection embedding to encode the information of experts not selected for each token"
  - [section 4.4.4]: "The unselected expert embeddings are more informative than selected expert embeddings"
  - [corpus]: Weak - limited corpus discussion of selection embedding effectiveness
- Break condition: If gating scores don't correlate with actual knowledge complementarity between experts

### Mechanism 3
- Claim: Cross-layer hypernetwork sharing enables information flow among MoE layers, improving parameter efficiency
- Mechanism: A single hypernetwork is shared across all transformer layers, allowing layer-specific knowledge transfer through shared parameters
- Core assumption: Information captured by the hypernetwork in earlier layers is useful for generating better HyperExperts in later layers
- Evidence anchors:
  - [section 3.2]: "To better share information across different layers and improve parameter efficiency, we share the hypernetwork among all layers"
  - [section 4.4.5]: "HyperMoEtrain 65.69 65.95 75.38" showing training efficiency
  - [corpus]: Weak - no direct corpus evidence for cross-layer sharing benefits
- Break condition: If layer-specific information becomes too specialized for sharing, or if parameter sharing causes interference

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: HyperMoE builds directly on MoE by adding hypernetwork-generated supplementary experts
  - Quick check question: In standard MoE, how many experts are typically selected per token, and why is this a limitation?

- Concept: Hypernetworks and conditional generation
  - Why needed here: The core innovation uses hypernetworks to generate expert-specific parameters based on unselected expert information
  - Quick check question: How does a hypernetwork differ from standard neural network parameter generation?

- Concept: Knowledge transfer in multi-task learning
  - Why needed here: The paper draws inspiration from multi-task learning's knowledge transfer to enable expert-to-expert transfer
  - Quick check question: What is the key difference between task-to-task knowledge transfer and expert-to-expert knowledge transfer?

## Architecture Onboarding

- Component map:
  Gate network (Noisy Top-K) → Expert layers → HyperExpert generator → Final output
  Key components: Selection embeddings (from unselected experts), shared hypernetwork, HyperExperts (generated experts), bottleneck structure

- Critical path:
  1. Input token processed by gate network
  2. Gate network selects top-1 expert
  3. Selection embeddings computed from unselected expert embeddings
  4. Hypernetwork generates HyperExpert conditioned on selection embeddings and layer embeddings
  5. Selected expert output + HyperExpert output combined

- Design tradeoffs:
  - Sparsity vs knowledge availability: Maintaining top-1 routing while adding knowledge from unselected experts
  - Parameter efficiency vs expressiveness: Shared hypernetwork reduces parameters but may limit specialization
  - Bottleneck dimension: Smaller bottlenecks save parameters but may limit knowledge transfer capacity

- Failure signatures:
  - Performance worse than baseline MoE: Indicates negative knowledge transfer or ineffective conditioning
  - Minimal training/inference speed reduction: Suggests hypernetwork computation is too simple to provide benefit
  - Large parameter increase: Indicates inefficient hypernetwork design or inappropriate bottleneck sizing

- First 3 experiments:
  1. Replace selection embeddings with random vectors to test if gating information is crucial
  2. Vary bottleneck dimension (b) to find optimal trade-off between parameter efficiency and performance
  3. Compare shared vs per-layer hypernetworks to quantify cross-layer sharing benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal bottleneck dimension b for HyperExpert to balance parameter efficiency and performance?
- Basis in paper: [explicit] The paper mentions using a bottleneck structure for conditional experts with b ≪ din/out, but does not specify optimal values
- Why unresolved: The paper states that the bottleneck dimension b is adopted for parameter efficiency but doesn't provide specific guidelines or experimental results showing how different values affect performance
- What evidence would resolve it: Systematic experiments varying the bottleneck dimension b and measuring both parameter efficiency and downstream task performance

### Open Question 2
- Question: How does the performance of HyperMoE scale with increasing model size beyond the tested 16 billion parameters?
- Basis in paper: [explicit] The paper tests on DeepSeek-MoE 16B but mentions future work on larger models
- Why unresolved: The paper only provides results up to 16 billion parameters and states that utilizing the method on larger models will be future work
- What evidence would resolve it: Training and evaluation results on models with 100B+ parameters using the same methodology

### Open Question 3
- Question: What is the relationship between the number of selected experts and the effectiveness of knowledge transfer in HyperMoE?
- Basis in paper: [explicit] The paper mentions that increasing selected experts can enhance performance but focuses on maintaining sparsity with top-1 routing
- Why unresolved: The paper primarily uses top-1 routing and doesn't systematically explore how varying the number of selected experts affects knowledge transfer effectiveness
- What evidence would resolve it: Experiments comparing performance across different k values in top-k routing while measuring both task performance and knowledge transfer metrics

### Open Question 4
- Question: How does the proposed method perform in multilingual settings compared to dense models and other MoE variants?
- Basis in paper: [explicit] The paper mentions multilingual tasks briefly but focuses primarily on English datasets
- Why unresolved: The paper only evaluates on English datasets and doesn't explore cross-lingual transfer or multilingual model performance
- What evidence would resolve it: Comprehensive evaluation on multilingual benchmarks like XTREME or mGLUE with comparisons to both dense and sparse models

### Open Question 5
- Question: What is the computational overhead of using depthwise separable convolutions for expert embeddings compared to alternative compression methods?
- Basis in paper: [inferred] The paper mentions using depthwise separable convolutions for expert embeddings but doesn't compare with other methods
- Why unresolved: The paper presents one compression method without comparing to alternatives like PCA, random projections, or other neural network compression techniques
- What evidence would resolve it: Systematic comparison of different expert embedding compression methods measuring both computational efficiency and downstream performance

## Limitations

- The core claims about knowledge transfer between experts rely heavily on theoretical mechanisms with limited empirical validation through ablation studies
- The computational efficiency claims (within 15% training and 10% inference slowdowns) are stated but the actual impact on real-world deployment scenarios with different hardware configurations is not explored
- The paper focuses on 8-expert configurations, leaving uncertainty about how the approach scales to larger expert counts common in modern MoE systems

## Confidence

- **High Confidence**: The experimental methodology is sound, with proper baselines and comprehensive evaluation across 20 datasets spanning multiple NLP tasks
- **Medium Confidence**: The performance improvements (+0.48% on GLUE, +0.84% on SuperGLUE) are statistically significant across the evaluated datasets
- **Low Confidence**: The theoretical claims about why knowledge transfer between experts works are not strongly supported by direct evidence

## Next Checks

1. **Ablation Study on Selection Embeddings**: Replace selection embeddings with random vectors of the same dimension and retrain to quantify how much performance gain comes specifically from using gating-based selection information versus any learned conditioning signal

2. **Cross-Expert Knowledge Analysis**: For a subset of inputs, analyze the similarity between selected expert outputs and HyperExpert outputs to empirically verify whether unselected expert knowledge provides genuinely complementary information that improves predictions

3. **Scaling Analysis**: Evaluate HyperMoE with varying numbers of experts (4, 16, 32) to determine whether the knowledge transfer benefits scale with model capacity and whether the shared hypernetwork approach remains effective as the expert count increases