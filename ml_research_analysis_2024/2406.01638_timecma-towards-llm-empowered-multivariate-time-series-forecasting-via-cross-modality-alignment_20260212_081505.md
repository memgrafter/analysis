---
ver: rpa2
title: 'TimeCMA: Towards LLM-Empowered Multivariate Time Series Forecasting via Cross-Modality
  Alignment'
arxiv_id: '2406.01638'
source_url: https://arxiv.org/abs/2406.01638
tags:
- time
- series
- embeddings
- prompt
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TimeCMA, a large language model (LLM)-empowered
  framework for multivariate time series forecasting (MTSF) that addresses data entanglement
  issues in existing prompt-based approaches. The key idea is to leverage dual-modality
  encoding with a time series encoder and an LLM-empowered prompt encoder, followed
  by cross-modality alignment via channel-wise similarity retrieval to extract disentangled
  yet robust embeddings.
---

# TimeCMA: Towards LLM-Empowered Multivariate Time Series Forecasting via Cross-Modality Alignment

## Quick Facts
- arXiv ID: 2406.01638
- Source URL: https://arxiv.org/abs/2406.01638
- Authors: Chenxi Liu, Qianxiong Xu, Hao Miao, Sun Yang, Lingzheng Zhang, Cheng Long, Ziyue Li, Rui Zhao
- Reference count: 12
- Primary result: LLM-empowered framework for multivariate time series forecasting using cross-modality alignment to address data entanglement

## Executive Summary
This paper introduces TimeCMA, a framework for multivariate time series forecasting that leverages large language models (LLMs) through a dual-modality encoding approach. The method addresses the data entanglement problem in prompt-based LLM embeddings by combining time series encoder outputs with LLM prompt embeddings through cross-modality alignment. The framework demonstrates significant improvements over state-of-the-art baselines, achieving up to 13.9% improvement in MSE and 12.6% in MAE across eight real-world datasets.

## Method Summary
TimeCMA employs a dual-modality encoding strategy where historical time series data is processed through both a time series encoder (using inverted embedding) and an LLM prompt encoder. The time series encoder creates weak but disentangled embeddings, while the LLM prompt encoder generates robust but potentially entangled embeddings. These are then aligned through channel-wise similarity retrieval, where time series components are extracted from the prompt embeddings based on similarity metrics and combined with the original time series embeddings. To improve computational efficiency, only the last token embeddings from the LLM are stored and used downstream, avoiding repetitive processing during training.

## Key Results
- Outperforms state-of-the-art baselines on eight real-world datasets
- Achieves up to 13.9% improvement in MSE compared to existing prompt-based LLMs
- Achieves up to 12.6% improvement in MAE compared to existing prompt-based LLMs
- Demonstrates computational efficiency through last-token storage optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-modality encoding + cross-modality alignment mitigates data entanglement in prompt-based LLM embeddings.
- Mechanism: Creates separate weak (disentangled) and robust (entangled) embeddings, then uses channel-wise similarity retrieval to extract time series components from prompt embeddings and combine them with original time series embeddings.
- Core assumption: Time series components in prompt embeddings have stronger correlations with original time series embeddings than textual components do.
- Evidence anchors: Abstract and section discussing cross-modality alignment retrieving "the best of two worlds" from prompt embeddings.
- Break condition: If similarity assumption fails, alignment won't effectively retrieve disentangled time series information.

### Mechanism 2
- Claim: Storing only the last token embedding reduces computational cost and speeds up inference.
- Mechanism: LLM is instructed to encapsulate temporal information in the last token, which is stored offline to avoid repetitive processing during training epochs.
- Core assumption: Last token contains most comprehensive temporal knowledge due to masked self-attention dynamics.
- Evidence anchors: Section on prompt design and storage optimization using frozen LLM.
- Break condition: If last token doesn't capture key temporal information, using only it will degrade forecasting accuracy.

### Mechanism 3
- Claim: Combining inverted embedding with multivariate attention improves performance on datasets with many variables.
- Mechanism: Inverted embedding treats each variable's entire time series as a single token, combined with multivariate attention in the forecasting decoder to map variable inter-dependencies.
- Core assumption: Treating entire variable time series as a token preserves inter-variable correlations better than treating each timestep as a token.
- Evidence anchors: Section on time series encoding branch and inverted embedding design.
- Break condition: If variable sequence length is too long, treating it as a single token may exceed model capacity or lose local temporal patterns.

## Foundational Learning

- Concept: Masked self-attention in LLMs
  - Why needed here: Understanding why last token contains most comprehensive knowledge justifies storage optimization.
  - Quick check question: In a prompt with 10 tokens, which token's representation is influenced by all previous tokens due to masked self-attention?

- Concept: Cross-modal similarity retrieval
  - Why needed here: Alignment module relies on measuring similarity between embeddings from different modalities to retrieve relevant information.
  - Quick check question: If two embeddings have cosine similarity 0.9 in one channel and 0.3 in another, which channel is more likely to contain shared modality-specific features?

- Concept: Inverted embedding for time series
  - Why needed here: This design choice affects how temporal and inter-variable dependencies are modeled before alignment.
  - Quick check question: What is the shape of the embedding matrix when each variable's full time series is treated as one token?

## Architecture Onboarding

- Component map: Input → Time Series Encoder (inverted embedding + Pre-LN Transformer) → Weak TS embeddings → Cross-modality alignment → Aligned TS embeddings → Multivariate Transformer decoder → Forecast
- Critical path: Time series encoding → Cross-modality alignment → Multivariate decoding
- Design tradeoffs:
  - Using full prompt vs. last token: accuracy vs. speed
  - Channel-wise similarity retrieval vs. concatenation: disentanglement vs. simplicity
  - Frozen LLM vs. fine-tuning: inference speed vs. adaptation
- Failure signatures:
  - Degraded accuracy after alignment → similarity retrieval failing
  - High memory usage → last token optimization not effective
  - Slow inference → prompt encoder bottleneck
- First 3 experiments:
  1. Ablation: Replace cross-modality alignment with concatenation; measure MSE/MAE change.
  2. Efficiency test: Compare inference speed with full prompt vs. last token only.
  3. Embedding analysis: Visualize attention from last token to confirm it focuses on time series values.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the cross-modality alignment mechanism perform when scaling to time series with significantly more variables or longer temporal sequences?
- Basis in paper: Explicit discussion of inverted embedding treating entire variable time series as token and challenges with computational costs for multivariate data.
- Why unresolved: Experiments focus on eight specific datasets without systematically varying number of variables or sequence lengths.
- What evidence would resolve it: Systematic experiments varying number of variables and sequence lengths, measuring performance degradation and computational costs.

### Open Question 2
- Question: Can the cross-modality alignment approach be effectively adapted for non-stationary time series where temporal patterns change over time?
- Basis in paper: Inferred from focus on alignment without addressing changing temporal dynamics or concept drift.
- Why unresolved: Methodology assumes relatively stable temporal patterns without testing on non-stationary data.
- What evidence would resolve it: Experiments on datasets with known temporal drift, measuring whether alignment maintains performance as patterns evolve.

### Open Question 3
- Question: What is the optimal balance between contributions of time series encoder and LLM encoder branches for different types of time series patterns?
- Basis in paper: Explicit discussion of dual-modality encoding design without systematic exploration of weighting contributions.
- Why unresolved: Paper presents fixed architecture without exploring adaptive weighting between encoding branches.
- What evidence would resolve it: Comparative studies testing different weighting schemes across various time series patterns.

## Limitations
- The cross-modality alignment mechanism's effectiveness depends on unproven assumption about channel-wise similarity retrieval between time series and prompt embeddings.
- Inverted embedding approach may struggle with very long sequences or lose important local temporal patterns crucial for accurate forecasting.
- Storage optimization using only last token embeddings assumes successful prompt design that may not generalize across different datasets or time series types.

## Confidence

- Medium confidence in dual-modality encoding and cross-modality alignment claims: Theoretically sound but core similarity assumption lacks direct validation and may not generalize.
- Medium confidence in computational efficiency claims: Last-token storage optimization is straightforward but effectiveness depends entirely on unproven assumption about last token content.
- Low confidence in overall superiority claims: Substantial reported improvements but lack of ablation studies and limited dataset diversity make it difficult to determine which specific mechanisms drive improvements.

## Next Checks

1. **Cross-modality alignment ablation**: Remove the cross-modality alignment module and replace it with simple concatenation of the two embeddings. Compare forecasting accuracy (MSE/MAE) to determine whether the complex alignment mechanism actually contributes to performance gains or if improvements come from having dual embeddings regardless of alignment.

2. **Last token effectiveness validation**: Design prompts where temporal information is explicitly distributed across multiple tokens rather than concentrated in the last token. Compare forecasting accuracy and inference speed between using only the last token versus using all tokens to empirically test whether storage optimization compromises accuracy.

3. **Channel similarity analysis**: For each dataset, compute and visualize the cosine similarities between corresponding channels of time series embeddings and prompt embeddings. Quantify what percentage of channels show higher similarity for time series components versus textual components to empirically validate the core assumption underlying cross-modality alignment mechanism.