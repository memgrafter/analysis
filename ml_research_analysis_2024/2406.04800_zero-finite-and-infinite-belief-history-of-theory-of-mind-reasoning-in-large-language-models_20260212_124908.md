---
ver: rpa2
title: Zero, Finite, and Infinite Belief History of Theory of Mind Reasoning in Large
  Language Models
arxiv_id: '2406.04800'
source_url: https://arxiv.org/abs/2406.04800
tags:
- item
- belief
- position
- history
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for evaluating Theory of
  Mind (ToM) reasoning in Large Language Models (LLMs) through the concept of Zero,
  Finite, and Infinite Belief History. The authors propose a new multi-round text-based
  game called "Pick the Right Stuff" as a benchmark to assess LLMs' ability to reason
  about others' beliefs under these conditions.
---

# Zero, Finite, and Infinite Belief History of Theory of Mind Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2406.04800
- Source URL: https://arxiv.org/abs/2406.04800
- Reference count: 29
- This paper introduces a novel framework for evaluating Theory of Mind (ToM) reasoning in Large Language Models (LLMs) through the concept of Zero, Finite, and Infinite Belief History.

## Executive Summary
This paper introduces a novel framework for evaluating Theory of Mind (ToM) reasoning in Large Language Models (LLMs) through the concept of Zero, Finite, and Infinite Belief History. The authors propose a new multi-round text-based game called "Pick the Right Stuff" as a benchmark to assess LLMs' ability to reason about others' beliefs under these conditions. They evaluate six LLMs, finding that models perform better under Zero Belief History than Finite Belief History. Surprisingly, two smaller parameter models outperform larger ones, suggesting that increasing model size may not always enhance ToM capabilities. This work provides a new direction for ToM evaluation and benchmark development.

## Method Summary
The authors develop a multi-round text-based game called "Pick the Right Stuff" to evaluate Theory of Mind reasoning in LLMs under Zero, Finite, and Infinite Belief History conditions. The game involves users placing items in lockers, with items being randomly shuffled, and LLMs must predict users' beliefs about item locations based on their observations. The experiment uses six LLMs with varying parameter sizes (three large, three small) and evaluates their performance across 60 game turns with 5 users per model. The study compares model performance under Zero Belief History (where belief updates are explicitly signaled) and Finite Belief History (requiring reasoning from prior states).

## Key Results
- LLMs perform better under Zero Belief History than Finite Belief History conditions
- Two smaller parameter models (gemma:7b-instruct and mistral:7b-instruct) outperform all larger models in ToM tasks
- Increasing model size may not always enhance ToM capabilities
- The "Pick the Right Stuff" game successfully distinguishes between different belief history reasoning abilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs perform better under Zero Belief History because they can directly search for contextual information rather than needing to reason through prior states.
- Mechanism: When belief updates are explicitly signaled by events (like entering Room 2), the model can use the latest context directly without inference chains.
- Core assumption: The model's context window contains sufficient explicit cues about belief states when users observe the monitor.
- Evidence anchors:
  - [abstract]: "LLMs perform better under Zero Belief History than Finite Belief History" and can "identify the latest beliefs of others without needing to reason from the available or known belief history"
  - [section]: "the LLM only needs to find associated information provided in the context which is whether the users enter the Room 2 to view the monitor"
  - [corpus]: Weak - corpus doesn't contain specific studies comparing Zero vs Finite Belief History performance
- Break condition: If belief updates are not explicitly marked in the context or require inference about what the user saw, the Zero Belief History advantage disappears.

### Mechanism 2
- Claim: Finite Belief History requires mathematical or logical reasoning about prior states, which is harder for LLMs than direct pattern matching.
- Mechanism: When users view snapshots of previous monitor states, the LLM must track temporal sequences and infer current beliefs from historical observations.
- Core assumption: The model must maintain and reason about belief histories to correctly infer current user beliefs from indirect evidence.
- Evidence anchors:
  - [abstract]: "under the condition of Finite Belief History, in addition to searching and finding available and associated information from the context, due to the information is not sufficient to enable it to directly identify the latest beliefs of others, it needs and has to utilize a belief history and reason with it"
  - [section]: "the LLM must utilize and reason with a finite belief history to identify the latest beliefs of the users"
  - [corpus]: Missing - no corpus evidence about mathematical reasoning requirements in ToM tasks
- Break condition: If the temporal reasoning required is simple enough to be handled by direct pattern matching rather than explicit reasoning, the performance gap narrows.

### Mechanism 3
- Claim: Smaller parameter models can outperform larger ones on ToM tasks because parameter count doesn't directly correlate with ToM reasoning ability.
- Mechanism: ToM reasoning may depend more on specific architectural features or training data composition than raw parameter count.
- Core assumption: The relationship between model size and ToM capability is non-linear and may plateau or even reverse at certain scales.
- Evidence anchors:
  - [abstract]: "two of the models with small parameter sizes outperform all the evaluated models with large parameter sizes" and "increasing model size may not always enhance ToM capabilities"
  - [section]: "gemma:7b-instruct emerged as the best-performing, achieving an average score of 43.00" and "mistral:7b-instruct also shows better performance over all the other LLMs with large parameter sizes"
  - [corpus]: Missing - corpus doesn't contain studies specifically addressing parameter size vs ToM performance
- Break condition: If larger models are fine-tuned specifically on ToM reasoning tasks or have architectural advantages for this type of reasoning, the size advantage may reappear.

## Foundational Learning

- Concept: Dynamic epistemic logic for belief state tracking
  - Why needed here: The benchmark relies on tracking how beliefs change as agents observe different states, which is fundamentally about modeling knowledge updates
  - Quick check question: Can you explain how a belief state changes when an agent observes a monitor showing item positions?

- Concept: Temporal reasoning and belief inference from indirect evidence
  - Why needed here: Finite Belief History requires inferring current beliefs from past observations, which is a core challenge in the benchmark
  - Quick check question: If a user last saw the monitor 3 states ago, how would you determine their current belief about item positions?

- Concept: Theory of Mind evaluation methodologies and benchmark design
  - Why needed here: The paper introduces a new framework and benchmark, requiring understanding of how ToM is typically evaluated in LLMs
  - Quick check question: What distinguishes a good ToM benchmark from one that can be solved through pattern matching?

## Architecture Onboarding

- Component map: Game engine (state management, random shuffling) -> LLM interface (prompt engineering, temperature control) -> Scoring system (evaluation metrics) -> Data collection (result aggregation)
- Critical path: User places items → Locker shuffles → LLM predicts beliefs → System swaps items → Score updates → Next turn
- Design tradeoffs: Simplicity vs comprehensiveness in game design, randomization vs reproducibility, parameter tuning vs generalization
- Failure signatures: LLM consistently mispredicts beliefs despite correct context, performance degradation with more users, inability to handle belief updates correctly
- First 3 experiments:
  1. Test Zero Belief History with 2 users and 3 items to verify basic functionality
  2. Test Finite Belief History with 2 users and 3 items, comparing performance to Zero Belief History
  3. Test model size comparison with 3 users and 4 items to verify the smaller model advantage observation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design a robust benchmark for evaluating Infinite Belief History in LLMs?
- Basis in paper: [explicit] The paper introduces the concept of Infinite Belief History but leaves its implementation for future work, stating "we leave the implementation of Infinite Belief History for future work."
- Why unresolved: The paper acknowledges the need for a benchmark to evaluate Infinite Belief History but does not provide one. The authors mention that future work can "work along it to develop various benchmarks to evaluate ToM reasoning ability under the conditions of Zero, Finite, and Infinite Belief History."
- What evidence would resolve it: A well-designed benchmark that can effectively measure an LLM's ability to reason with infinite belief history, along with experimental results showing the LLM's performance on this benchmark.

### Open Question 2
- Question: Can increasing model parameter size effectively enhance LLMs' capabilities in Theory of Mind reasoning?
- Basis in paper: [explicit] The paper finds that two smaller parameter models (gemma:7b-instruct and mistral:7b-instruct) outperform all larger models in ToM tasks, suggesting that increasing model parameter size may not always enhance ToM capabilities.
- Why unresolved: While the paper provides evidence that smaller models can outperform larger ones in ToM tasks, it does not conclusively determine whether increasing parameter size is generally ineffective for ToM reasoning. The authors note that "there may be differences in the training data and architectures leveraged by these models."
- What evidence would resolve it: A comprehensive study comparing the ToM performance of LLMs with varying parameter sizes, controlling for other factors such as training data and architecture.

### Open Question 3
- Question: How do LLMs perform on Theory of Mind tasks when incorporating different types of belief history (e.g., social rules, cultural background)?
- Basis in paper: [inferred] The paper mentions that the game does not cover all possibilities and variants under the conditions of Zero and Finite Belief History, such as "replacing mathematical reasoning in the case of Finite Belief History with social rules or cultural background."
- Why unresolved: The current benchmark focuses on mathematical reasoning for Finite Belief History but does not explore other types of belief history. The authors expect future work to develop various benchmarks to evaluate ToM reasoning ability with different kinds of belief history.
- What evidence would resolve it: Experimental results comparing LLM performance on ToM tasks when using different types of belief history (e.g., mathematical reasoning, social rules, cultural background) in the Finite Belief History condition.

## Limitations

- Benchmark Scope and Generalization: The results may not generalize to other ToM scenarios involving more complex social reasoning, emotional states, or natural language contexts beyond the controlled game environment.
- Corpus Evidence Gaps: Several key claims lack supporting evidence from the corpus, particularly regarding the relationship between model size and ToM performance.
- Implementation Details: Critical implementation details remain unspecified, including exact game mechanics, belief history tracking algorithms, and evaluation procedures.

## Confidence

**High Confidence**: The experimental observation that models perform better under Zero Belief History than Finite Belief History is directly supported by the reported results.

**Medium Confidence**: The claim that smaller parameter models can outperform larger ones is supported by the specific experimental results, but lacks broader corpus evidence about model size vs. ToM performance relationships.

**Low Confidence**: The assertion that this work provides a "new direction for ToM evaluation and benchmark development" is somewhat speculative given the limited scope of the benchmark.

## Next Checks

1. **Generalization Test**: Evaluate the same models on additional ToM benchmarks (such as the classic Sally-Anne test or other established ToM datasets) to determine whether the Zero Belief History advantage and smaller model performance patterns hold across different task types and contexts.

2. **Ablation Study on Belief History Tracking**: Systematically vary the explicitness of belief update markers in the context to determine the threshold at which Zero Belief History performance advantage disappears.

3. **Parameter Size Scaling Analysis**: Conduct a more comprehensive study varying model sizes across a broader range (e.g., 1B to 70B parameters) to determine whether the observed smaller model advantage represents a consistent pattern or an artifact of the specific models tested.