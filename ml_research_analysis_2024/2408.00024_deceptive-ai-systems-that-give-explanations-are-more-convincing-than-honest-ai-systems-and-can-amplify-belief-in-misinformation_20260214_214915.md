---
ver: rpa2
title: Deceptive AI systems that give explanations are more convincing than honest
  AI systems and can amplify belief in misinformation
arxiv_id: '2408.00024'
source_url: https://arxiv.org/abs/2408.00024
tags:
- deceptive
- 'true'
- explanations
- 'false'
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Deceptive AI-generated explanations were found to be more persuasive
  than both honest explanations and deceptive AI classifications without explanations,
  significantly amplifying belief in false news headlines and undermining true ones.
  Personal factors like cognitive reflection and trust in AI did not protect individuals
  from these effects.
---

# Deceptive AI systems that give explanations are more convincing than honest AI systems and can amplify belief in misinformation

## Quick Facts
- arXiv ID: 2408.00024
- Source URL: https://arxiv.org/abs/2408.00024
- Authors: Valdemar Danry; Pat Pataranutaporn; Matthew Groh; Ziv Epstein; Pattie Maes
- Reference count: 40
- One-line primary result: Deceptive AI-generated explanations were found to be more persuasive than both honest explanations and deceptive AI classifications without explanations, significantly amplifying belief in false news headlines and undermining true ones.

## Executive Summary
This study demonstrates that AI-generated deceptive explanations can significantly amplify belief in false news headlines and undermine true ones. The research found that deceptive explanations were more persuasive than both honest explanations and deceptive classifications without explanations. Surprisingly, personal factors such as cognitive reflection and trust in AI did not protect individuals from these effects. However, the logical validity of explanations played a critical role—logically invalid deceptive explanations were less credible, suggesting that teaching logical reasoning and critical thinking skills could help counter AI-driven misinformation.

## Method Summary
The study used a pre-registered online experiment with 1,192 participants who were randomly assigned to receive either news headlines or trivia statements. For each statement, participants rated their belief before and after receiving AI feedback. GPT-3 was used to generate five honest and five deceptive explanations for each headline, which were then filtered to match the headline's truth value and logical validity. The analysis used OLS regression with robust standard errors clustered at the participant and headline level, examining how deceptive explanations affected belief updates and whether personal factors like cognitive reflection and trust in AI moderated these effects.

## Key Results
- Deceptive AI-generated explanations were more persuasive than honest explanations in increasing belief in false news headlines
- Deceptive explanations were more persuasive than deceptive classifications without explanations
- Cognitive reflection and trust in AI did not protect individuals from the persuasive effects of deceptive explanations
- Logically invalid deceptive explanations were less credible than logically valid ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deceptive AI explanations are more persuasive than honest ones because they can be crafted to appear logically valid while leading to false conclusions.
- Mechanism: LLMs can exploit the tendency of humans to accept explanations that sound coherent, even when the underlying claim is false, by constructing chains of plausible-sounding but invalid reasoning.
- Core assumption: People rarely scrutinize explanations unless forced to do so, and the mere presence of an explanation is often taken as a signal of credibility.
- Evidence anchors:
  - [abstract] "AI-generated deceptive explanations can significantly amplify belief in false news headlines and undermine true ones"
  - [section] "while logically invalid deceptive AI-generated explanations did not have any significant effects on participants' belief rating of true news headlines (β = −0.31, p = 0.13), logically invalid deceptive AI-generated explanations did significantly increase beliefs in false news headlines (β = −0.35, p = 0.02)"
  - [corpus] Weak: no direct match, but related work on "separating idealization from deceptive explanations in xAI" is listed.
- Break condition: If people are trained to deconstruct explanations into premises and conclusions and assess logical validity, the persuasive power of deceptive explanations is reduced.

### Mechanism 2
- Claim: The persuasive advantage of deceptive explanations over mere deceptive classifications is due to the additional layer of perceived justification.
- Mechanism: By providing an explanation, the AI system mimics the form of human reasoning, which humans are more likely to trust than a bare assertion, even if the reasoning is faulty.
- Core assumption: Humans have a "placebic" response to explanations—they update beliefs based on the mere presence of an explanation, regardless of its quality.
- Evidence anchors:
  - [abstract] "in addition to being more persuasive than accurate and honest explanations, AI-generated deceptive explanations can significantly amplify belief in false news headlines and undermine true ones as compared to AI systems that simply classify the headline incorrectly"
  - [section] "deceptive AI generated classifications without explanation, significantly increase belief in false news headlines (β = 0.71, p < 0.0001) and decreases belief in true news headlines (β = −1.72, p < 0.0001). In extension, when accompanied by deceptive AI-generated explanations, beliefs in false news headlines were further significantly increased (β = 0.32, p = 0.009)"
  - [corpus] Weak: no direct match, but "separating idealization from deceptive explanations in xAI" touches on the justification layer.
- Break condition: If the explanation is obviously irrelevant or logically invalid, people may downgrade the credibility of the entire claim.

### Mechanism 3
- Claim: Cognitive reflection and trust in AI do not protect individuals because the explanations override these factors.
- Mechanism: The presence of an authoritative AI explanation can induce a "reliance effect," where people defer judgment to the technology and stop applying their own critical faculties.
- Core assumption: When an explanation is provided, people assume the AI has done the reasoning and they need not do it themselves.
- Evidence anchors:
  - [abstract] "our results show that personal factors such as cognitive reflection and trust in AI do not necessarily protect individuals from these effects caused by deceptive AI generated explanations"
  - [section] "our results revealed no significant interactions with cognitive reflection level and deceptive classifications both with and without explanations for false news headlines (β = −0.09, p = 0.20 and β = 0.13, p = 0.15, respectively)"
  - [corpus] Weak: no direct match, but the corpus includes work on "AI-washing" which implies people trust AI claims.
- Break condition: If people are explicitly instructed to evaluate the logic of the explanation before accepting the conclusion, the override effect is reduced.

## Foundational Learning

- Concept: Logical validity of an argument (premises → conclusion).
  - Why needed here: The study shows that logically invalid deceptive explanations are less persuasive, so detecting validity is key to resilience.
  - Quick check question: Given premises "All birds can fly" and "Penguins are birds", is the conclusion "Penguins can fly" logically valid? (No—premises are false, so argument is invalid.)

- Concept: Confirmation bias and the "placebic" effect of explanations.
  - Why needed here: The persuasive power of deceptive explanations relies on people accepting them without scrutiny.
  - Quick check question: If an AI says "This is true because X", and X is plausible but irrelevant, are you more likely to believe the claim than if the AI just said "This is true"? (Yes, due to the placebic effect.)

- Concept: Cognitive reflection and its limits in AI-assisted contexts.
  - Why needed here: The study finds cognitive reflection does not protect against deceptive AI explanations, so we must understand why.
  - Quick check question: If a high-CRT person sees an AI explanation for a claim, are they guaranteed to detect if it's deceptive? (No—the study shows even high-CRT individuals are influenced.)

## Architecture Onboarding

- Component map: GPT-3 prompts -> headline + honest explanation + deceptive explanation -> Curation (filtering by veracity match and logical validity) -> Random assignment to conditions -> Data capture (belief ratings, knowledge/self-reported ratings, CRT and trust surveys) -> Analysis (OLS regression)

- Critical path:
  1. Prompt GPT-3 to generate 5 honest + 5 deceptive explanations per headline.
  2. Filter to match headline veracity and logical validity.
  3. Randomize participants to conditions.
  4. Collect belief ratings pre/post feedback.
  5. Run OLS regression to estimate effect sizes.

- Design tradeoffs:
  - Use of pre-filtered explanations ensures quality but may not reflect real-world noise.
  - Between-subjects randomization for explanation type controls for carryover effects but requires more participants.
  - Trust and CRT measured after feedback may be contaminated by the feedback itself.

- Failure signatures:
  - No significant interaction between deception and personal factors → over-reliance on AI.
  - Significant effects of word count/readability → superficial processing of explanations.
  - Low R² → high unexplained variance, possibly due to unmeasured confounders.

- First 3 experiments:
  1. Run the same design with GPT-4 to see if more coherent deceptive explanations increase persuasion further.
  2. Add an explicit instruction to participants to check logical validity before updating beliefs.
  3. Use objective knowledge tests instead of self-reported knowledge to see if actual expertise moderates effects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do cultural and contextual factors influence the effectiveness of AI-generated deceptive explanations across different societies?
- Basis in paper: [explicit] The paper discusses the need for future work to explore how different cultural and contextual settings, including political climate, prevalent media literacy, and language around AI, influence how deceptive explanations are received and believed.
- Why unresolved: The current study primarily focuses on a US-based sample and does not account for cultural or contextual variations that might affect how people process and respond to AI-generated explanations.
- What evidence would resolve it: Conducting cross-cultural studies with diverse participant pools to compare the effects of AI-generated deceptive explanations in different societal contexts and media environments.

### Open Question 2
- Question: What are the long-term effects of repeated exposure to AI-generated deceptive explanations on individuals' trust in information sources and their ability to discern truth from falsehood?
- Basis in paper: [inferred] The study's design involves a single exposure to AI-generated explanations, but the real-world implications of repeated exposure are not explored.
- Why unresolved: The study does not track participants over time or examine the cumulative impact of repeated interactions with AI-generated deceptive content.
- What evidence would resolve it: Longitudinal studies that follow participants over extended periods, exposing them to multiple instances of AI-generated explanations and measuring changes in their trust and discernment abilities.

### Open Question 3
- Question: How do different levels of AI model sophistication affect the persuasiveness of AI-generated deceptive explanations?
- Basis in paper: [explicit] The paper acknowledges that while GPT-3 was used in the study, more advanced models have since been developed, and it is unclear how these newer models might perform in generating deceptive explanations.
- Why unresolved: The study only tests GPT-3, leaving open the question of whether more advanced models would produce even more convincing deceptive explanations.
- What evidence would resolve it: Comparative studies using different AI models, including newer and more sophisticated ones, to evaluate their effectiveness in generating persuasive deceptive explanations.

## Limitations
- The study's findings rely heavily on GPT-3 generated explanations, which may not generalize to more advanced LLMs
- Logical validity assessment was performed by two researchers rather than a formal logical evaluation
- Self-reported knowledge and trust measures may introduce bias
- Between-subjects design required large sample size and may have limited statistical power for detecting interaction effects

## Confidence
- High confidence: The finding that deceptive AI explanations increase belief in false news compared to honest explanations is well-supported by the data (β = -0.35, p = 0.02 for logically invalid explanations; β = 0.32, p = 0.009 for logically valid explanations).
- Medium confidence: The claim that cognitive reflection and trust in AI do not protect individuals is supported but based on non-significant interactions, which may reflect limited statistical power rather than true absence of effects.
- Medium confidence: The mechanism that logical validity is key to explanation persuasiveness is supported but relies on post-hoc analysis of explanation quality.

## Next Checks
1. Replicate the study using GPT-4 or Claude to determine if more advanced LLMs produce even more persuasive deceptive explanations.
2. Implement objective knowledge tests instead of self-reported knowledge to verify whether actual expertise moderates susceptibility to deceptive AI explanations.
3. Conduct a follow-up study with explicit instructions to evaluate logical validity before updating beliefs to test whether this intervention reduces the persuasive effect of deceptive explanations.