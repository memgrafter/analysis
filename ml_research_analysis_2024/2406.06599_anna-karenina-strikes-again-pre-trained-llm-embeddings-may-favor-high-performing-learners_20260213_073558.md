---
ver: rpa2
title: 'Anna Karenina Strikes Again: Pre-Trained LLM Embeddings May Favor High-Performing
  Learners'
arxiv_id: '2406.06599'
source_url: https://arxiv.org/abs/2406.06599
tags:
- responses
- clusters
- embeddings
- clustering
- kmeans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of pre-trained LLM embeddings for
  unsupervised clustering of student responses into pedagogically meaningful profiles.
  The authors compare theory-driven Knowledge Profiles (KPs) derived from expert analysis
  of student responses to open-ended biology questions with clusters discovered using
  KMeans and HDBSCAN algorithms on LLM embeddings.
---

# Anna Karenina Strikes Again: Pre-Trained LLM Embeddings May Favor High-Performing Learners

## Quick Facts
- arXiv ID: 2406.06599
- Source URL: https://arxiv.org/abs/2406.06599
- Authors: Abigail Gurin Schleifer; Beata Beigman Klebanov; Moriah Ariely; Giora Alexandron
- Reference count: 19
- Primary result: Pre-trained LLM embeddings consistently fail to cluster student responses by pedagogically meaningful error profiles, retrieving only the correct responses profile.

## Executive Summary
This study investigates whether pre-trained LLM embeddings can discover pedagogically meaningful clusters in student responses to open-ended biology questions. The authors compare theory-driven Knowledge Profiles (KPs) derived from expert analysis with data-driven clusters using KMeans and HDBSCAN algorithms. Results show poor alignment between theory-driven and data-driven clusters, with only the correct responses profile being consistently retrievable. The authors identify an "Anna Karenina principle" where incorrect responses are more similar to correct responses than to each other in the embedding space, hindering the discovery of error-based profiles that would enable targeted formative feedback.

## Method Summary
The study embeds 669 student responses to two biology questions using AlephBERT (Hebrew BERT), then applies KMeans and HDBSCAN clustering algorithms to discover emergent clusters. These clusters are compared against expert-derived Knowledge Profiles using Adjusted Rand Index and F1 scores. The researchers analyze pairwise cosine similarities within and between KPs, conduct statistical tests (Kruskal-Wallis, Dunn's post-hoc, Spearman correlation), and examine the geometry of the embedding space to understand why certain profiles are retrievable while others are not.

## Key Results
- KMeans and HDBSCAN consistently recover the correct responses profile but fail to retrieve error-based profiles
- Incorrect responses are more similar to correct responses than to each other in the embedding space
- The lower the knowledge level of the profile, the less similar its members are in the embedding space
- Only the correct responses profile is consistently retrievable across both clustering algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained embeddings encode a global "correctness" bias that dominates local error-type distinctions.
- Mechanism: The model's semantic space is structured around "correct" vs. "incorrect" responses, with the former forming a dense cluster. Within the "incorrect" region, responses are arranged by severity, but their internal distances are smaller than their distances to the "correct" cluster. This creates a dominance hierarchy where correct responses act as a gravitational center.
- Core assumption: The model's pretraining objective and corpus lead it to encode correctness as the primary axis of semantic similarity.
- Evidence anchors: [abstract] "incorrect responses are more similar to correct responses than to each other in the embedding space"; [section] "the embeddings of KPi responses tend to be more similar to the embeddings of KP1 than to embeddings of their own KP"
- Break condition: If embeddings are retrained or fine-tuned on domain-specific correctness labels, or if the task shifts to classification rather than clustering, the bias may diminish.

### Mechanism 2
- Claim: Low-quality responses are scattered in the embedding space, reducing clusterability.
- Mechanism: As response quality degrades, embeddings become increasingly dissimilar from one another, spreading across the space. This high dispersion means density-based clustering (HDBSCAN) cannot find compact regions, and centroid-based clustering (KMeans) cannot find meaningful centers.
- Core assumption: The semantic representation of errors is not uniform, and the model does not have a structured way to represent different types of mistakes.
- Evidence anchors: [abstract] "incorrect responses are more similar to correct responses than to each other"; [section] "the lower the knowledge level of the profile, the less similar to each other its members are in the embeddings space"
- Break condition: If embeddings are regularized or projected into a lower-dimensional space that collapses variance, or if clustering is performed on a different representation (e.g., rubric-based vectors), the scatter may be reduced.

### Mechanism 3
- Claim: Clustering algorithms are biased toward convex, dense regions, which favors correct responses.
- Mechanism: KMeans assumes convex clusters and will snap to the densest region, which is the correct responses. HDBSCAN requires sufficient density to form a core sample, which again is more likely for correct responses. Both algorithms thus preferentially recover KP1.
- Core assumption: The chosen clustering algorithms have inherent biases that interact with the embedding geometry to favor certain profiles.
- Evidence anchors: [section] "KMeans clusters are convex and all samples are assigned to a cluster"; [section] "HDBSCAN can find clusters with varied densities and clusters may have non-convex shapes"
- Break condition: If non-convex clustering algorithms are used (e.g., DBSCAN with different parameters, or graph-based clustering), or if the embedding space is transformed (e.g., via UMAP or t-SNE), the bias may be reduced.

## Foundational Learning

- Concept: Anna Karenina Principle in embeddings
  - Why needed here: To understand why correct responses dominate the clustering space and why errors are harder to distinguish.
  - Quick check question: In the context of student responses, what does it mean for the "Anna Karenina Principle" to apply to embeddings?

- Concept: Clusterability and density in high-dimensional spaces
  - Why needed here: To understand why KMeans and HDBSCAN fail to recover error-based profiles, and how the geometry of the embedding space affects this.
  - Quick check question: Why do density-based clustering algorithms like HDBSCAN require sufficient density to form clusters?

- Concept: Embedding bias from pretraining
  - Why needed here: To understand how the model's pretraining objective and corpus influence its representation of correctness and errors.
  - Quick check question: How might a model's pretraining corpus influence its representation of "correct" vs. "incorrect" responses?

## Architecture Onboarding

- Component map: Student responses -> AlephBERT embeddings -> KMeans/HDBSCAN clustering -> ARI/F1 evaluation -> Similarity analysis
- Critical path: 1) Embed student responses using AlephBERT; 2) Apply clustering algorithms to embeddings; 3) Evaluate clusters against expert Knowledge Profiles; 4) Analyze embedding geometry to understand bias
- Design tradeoffs: Using pre-trained embeddings avoids manual labeling but may introduce domain bias; KMeans is simple but assumes convex clusters; HDBSCAN can find non-convex clusters but requires sufficient density; Expert Knowledge Profiles provide gold standard but are costly to create
- Failure signatures: Low Adjusted Rand Index between clusters and Knowledge Profiles; High F1 score for correct responses but low F1 for error-based profiles; High dispersion of low-quality response embeddings; Correct responses forming a dense cluster that dominates the space
- First 3 experiments: 1) Visualize the embedding space using t-SNE or UMAP to see the distribution of correct and incorrect responses; 2) Try different clustering algorithms (e.g., DBSCAN, Spectral Clustering) to see if they recover error-based profiles better; 3) Fine-tune AlephBERT on a small labeled dataset of correct and incorrect responses to see if the bias is reduced

## Open Questions the Paper Calls Out

- Would using a larger, more advanced Hebrew LLM (e.g., with more parameters than AlephBERT) reduce or eliminate the "Anna Karenina principle" effect observed in this study?
- Does the "Anna Karenina principle" manifest similarly in other low-resource languages, or is it specific to Hebrew?
- Would fine-tuning the pre-trained LLM on domain-specific educational data (e.g., biology explanations) reduce the "Anna Karenina principle" effect?

## Limitations

- The study's findings are based on a relatively small dataset (669 responses across two questions) and a single pre-trained embedding model (AlephBERT)
- The analysis assumes that the expert Knowledge Profiles represent the "ground truth" structure of student understanding, but this rubric-based approach may not capture all pedagogically meaningful distinctions
- The study only examines unsupervised clustering methods, leaving open the question of whether supervised approaches might better leverage pre-trained embeddings for educational applications

## Confidence

- **High Confidence**: The empirical observation that KMeans and HDBSCAN consistently recover the correct responses profile while failing to retrieve error-based profiles
- **Medium Confidence**: The theoretical mechanism explaining why pre-trained embeddings encode a "correctness bias" that dominates clustering
- **Low Confidence**: The generalizability of the Anna Karenina principle to other domains, embedding models, or clustering tasks

## Next Checks

1. **Cross-model validation**: Repeat the clustering analysis using different pre-trained embeddings (e.g., multilingual BERT, domain-specific biomedical embeddings) to test whether the correctness bias is model-specific or a general property of pre-trained LLMs

2. **Supervised learning baseline**: Train a supervised classifier to predict Knowledge Profiles from embeddings and compare its performance to unsupervised clustering

3. **Dimensionality reduction analysis**: Apply t-SNE or UMAP to visualize the embedding space and examine whether the observed pattern persists after dimensionality reduction