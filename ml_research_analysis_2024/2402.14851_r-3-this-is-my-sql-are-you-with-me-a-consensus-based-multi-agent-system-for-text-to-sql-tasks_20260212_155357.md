---
ver: rpa2
title: '$R^3$: "This is My SQL, Are You With Me?" A Consensus-Based Multi-Agent System
  for Text-to-SQL Tasks'
arxiv_id: '2402.14851'
source_url: https://arxiv.org/abs/2402.14851
tags:
- select
- gold
- text-to-sql
- pred
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: R3 introduces a consensus-based multi-agent system for Text-to-SQL
  tasks, inspired by peer-review mechanisms. It employs one SQL-writer agent and multiple
  reviewer agents with distinct specializations to collaboratively generate and refine
  SQL queries.
---

# $R^3$: "This is My SQL, Are You With Me?" A Consensus-Based Multi-Agent System for Text-to-SQL Tasks

## Quick Facts
- **arXiv ID**: 2402.14851
- **Source URL**: https://arxiv.org/abs/2402.14851
- **Reference count**: 18
- **Primary result**: 88.1%, 89.9%, and 61.8% execution accuracy on Spider and Bird benchmarks

## Executive Summary
R3 introduces a consensus-based multi-agent system for Text-to-SQL tasks, inspired by peer-review mechanisms. It employs one SQL-writer agent and multiple reviewer agents with distinct specializations to collaboratively generate and refine SQL queries. The system uses execution results to guide iterative improvements through review, negotiation, and revision phases until consensus is reached. On Spider and Bird benchmarks, R3 achieves 88.1%, 89.9%, and 61.8% execution accuracy respectively, outperforming existing single LLM and multi-agent Text-to-SQL systems by 1.3% to 8.1%.

## Method Summary
R3 implements a consensus-based multi-agent system for Text-to-SQL tasks using one SQL-writer agent and multiple reviewer agents with distinct specializations. The system follows an iterative process where the SQL-writer generates queries, reviewers critique them based on execution results, and the writer refines the SQL through negotiation until consensus is reached. The approach incorporates memory sequences to maintain context across iterations and uses execution results as concrete evidence for refinement. The system was tested on Spider and Bird benchmarks, demonstrating effectiveness with both proprietary and open-source LLMs while using succinct prompts.

## Key Results
- Achieves 88.1% execution accuracy on Spider development set
- Achieves 89.9% execution accuracy on Spider test set
- Achieves 61.8% execution accuracy on Bird test set
- Outperforms existing systems by 1.3% to 8.1% on execution accuracy
- Surpasses chain-of-thought prompting by over 20% for Llama-3-8B

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Reviewer agents improve SQL accuracy through domain-specialized critique
- **Mechanism**: The system generates multiple reviewer agents with distinct professions (e.g., "Senior Database Engineer", "Data Analyst") who analyze SQL queries and execution results from different perspectives, providing targeted feedback for refinement
- **Core assumption**: Diverse expert perspectives lead to more comprehensive SQL evaluation than single-agent approaches
- **Evidence anchors**: System uses execution results to guide iterative improvements through review, negotiation, and revision phases
- **Break condition**: When reviewer agents provide conflicting feedback that cannot be reconciled through negotiation

### Mechanism 2
- **Claim**: Execution result feedback creates closed-loop refinement
- **Mechanism**: After SQL generation, the system executes the query and uses the results as evidence for reviewers to analyze, creating a feedback loop where execution outcomes directly inform the next revision cycle
- **Core assumption**: SQL execution results provide concrete evidence that helps identify and correct logical errors
- **Evidence anchors**: Employs execution results to guide iterative improvements
- **Break condition**: When execution results are ambiguous or non-deterministic

### Mechanism 3
- **Claim**: Memory sequence maintains context across iterative refinements
- **Mechanism**: The system maintains a dialogue history that accumulates all SQL versions, reviewer comments, and execution results, truncated when context limits are reached but preserving the evolution of the query
- **Core assumption**: Maintaining conversation history helps agents understand the refinement trajectory and avoid repeating mistakes
- **Evidence anchors**: First Text-to-SQL system to equip agents with memory sequences to enhance SQL generation
- **Break condition**: When context window is exceeded and critical information is truncated

## Foundational Learning

- **Concept**: Chain-of-Thought reasoning
  - **Why needed here**: Provides structured reasoning path for SQL generation before query construction
  - **Quick check question**: What Python code would you generate to find the maximum salary in a table?

- **Concept**: Program-of-Thoughts prompting
  - **Why needed here**: Enables step-by-step computation before SQL generation, handling complex aggregations
  - **Quick check question**: How would you break down "find average salary by department" into computational steps?

- **Concept**: Schema linking and understanding
  - **Why needed here**: Essential for mapping natural language to correct database tables and columns
  - **Quick check question**: Given a schema with "employees(name, department_id)" and "departments(id, name)", how would you identify the correct join path?

## Architecture Onboarding

- **Component map**: Natural Language Question + Schema → SQL-Writer → Execution Engine → Reviewers → Negotiation → Consensus Checker → Output
- **Critical path**: Question + Schema → SQL-Writer → Execution → Reviewers → Negotiation → Consensus → Output
- **Design tradeoffs**:
  - Multiple reviewers vs. single reviewer: More perspectives but higher computational cost
  - Memory length vs. context limits: Longer history preserves more context but risks truncation
  - Execution frequency vs. efficiency: More executions provide better feedback but increase latency
- **Failure signatures**:
  - Infinite loops in negotiation phase
  - Reviewers providing contradictory feedback without resolution
  - Memory truncation losing critical context
  - Execution failures due to malformed SQL
- **First 3 experiments**:
  1. Run R3 on a single simple Spider example with 1 reviewer to verify basic loop functionality
  2. Test memory truncation by running on a complex example that exceeds context window
  3. Compare performance with 1 vs 3 reviewers on a validation set to measure consensus benefit

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal number of reviewer agents for the R3 system?
- **Basis in paper**: The paper mentions that the performance gap between 1R-Lp and 3R-Lp demonstrates that the number of reviewers is a worthwhile topic of research, but this work does not delve into this much
- **Why unresolved**: The paper only tests with one and three reviewer agents in the ablation study
- **What evidence would resolve it**: Conducting experiments with varying numbers of reviewer agents (e.g., 2, 4, 5) and comparing their performance across different benchmarks and LLM sizes would help determine the optimal number

### Open Question 2
- **Question**: How does the R3 system handle ambiguous user questions in real-world applications?
- **Basis in paper**: The error analysis section mentions that ambiguous requests may be common in real-world applications as lay users may not be familiar with the database schema
- **Why unresolved**: The paper identifies ambiguity as an issue but does not propose solutions for handling ambiguous questions interactively
- **What evidence would resolve it**: Implementing and testing an interactive version of R3 that can ask clarifying questions to users when ambiguity is detected would demonstrate how well the system can handle real-world ambiguity

### Open Question 3
- **Question**: How does the R3 system's performance compare to human experts in SQL generation?
- **Basis in paper**: The paper compares R3's performance to existing LLM-based systems but does not mention comparisons to human performance
- **Why unresolved**: There is no benchmark or comparison of R3's performance against human-generated SQL queries
- **What evidence would resolve it**: Conducting a study where human experts generate SQL queries for the same questions used in the benchmarks and comparing their performance to R3 would provide insight into how well the system matches human-level SQL generation

## Limitations
- Performance relies heavily on execution accuracy metrics without addressing cases where execution succeeds but returns incorrect results
- System's reliance on multiple LLM calls for each query introduces significant computational overhead that is not quantified
- Paper does not address how the system handles databases with complex relationships or ambiguous schema designs

## Confidence
- **High confidence**: The consensus-based multi-agent architecture is clearly described and the iterative refinement mechanism is well-articulated
- **Medium confidence**: Performance claims on Spider and Bird benchmarks, though execution accuracy alone may not capture semantic correctness
- **Low confidence**: Claims about effectiveness with open-source LLMs versus proprietary models, as specific model configurations and prompts are not fully disclosed

## Next Checks
1. **Semantic validation test**: Run R3 on Spider examples where execution succeeds but the query returns semantically incorrect results to verify the system can catch logical errors beyond syntax
2. **Computational overhead measurement**: Profile the number of LLM calls and total processing time per query to quantify the efficiency tradeoff of the multi-agent approach
3. **Schema ambiguity stress test**: Evaluate R3 on databases with deliberately ambiguous schema designs to assess how well the reviewer agents handle uncertain mappings