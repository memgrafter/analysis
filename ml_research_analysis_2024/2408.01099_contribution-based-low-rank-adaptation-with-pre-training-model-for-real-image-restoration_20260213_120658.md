---
ver: rpa2
title: Contribution-based Low-Rank Adaptation with Pre-training Model for Real Image
  Restoration
arxiv_id: '2408.01099'
source_url: https://arxiv.org/abs/2408.01099
tags:
- colora
- tasks
- prod
- data
- full
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient fine-tuning for
  image restoration tasks by proposing a novel contribution-based low-rank adaptation
  (CoLoRA) method with a pre-training strategy using random order degradations (PROD).
  Unlike prior methods that tune all network parameters, CoLoRA leverages LoRA and
  a contribution-based approach to adaptively determine layer-by-layer capacity for
  each task, significantly reducing the number of tuned parameters (approximately
  7% compared to full tuning) while maintaining comparable performance.
---

# Contribution-based Low-Rank Adaptation with Pre-training Model for Real Image Restoration

## Quick Facts
- arXiv ID: 2408.01099
- Source URL: https://arxiv.org/abs/2408.01099
- Authors: Donwon Park; Hayeon Kim; Se Young Chun
- Reference count: 40
- One-line primary result: CoLoRA achieves comparable performance to full fine-tuning using only ~7% of parameters

## Executive Summary
This paper addresses the challenge of efficient fine-tuning for image restoration tasks by proposing a novel contribution-based low-rank adaptation (CoLoRA) method with a pre-training strategy using random order degradations (PROD). Unlike prior methods that tune all network parameters, CoLoRA leverages LoRA and a contribution-based approach to adaptively determine layer-by-layer capacity for each task, significantly reducing the number of tuned parameters (approximately 7% compared to full tuning) while maintaining comparable performance. PROD extends the capability of pre-trained models by generating low-quality training images using synthetic degradations and their random combinations, improving both performance and robustness.

## Method Summary
The method combines PROD for pre-training with synthetic degradation combinations and CoLoRA for efficient fine-tuning. PROD applies random combinations of synthetic degradations during pre-training, creating ~137K degradation representations. CoLoRA uses LoRA-style low-rank adaptation with layer-specific rank values determined by contribution scores computed via FAIG (Filter Attribution based on Integral Gradient). The approach achieves parameter efficiency by tuning only small adapters (2-2.9M parameters vs 26-29M full) while maintaining performance comparable to full fine-tuning across various image restoration tasks.

## Key Results
- CoLoRA with PROD achieved similar performance on NAFNet and Restormer by updating only 7% and 11% of total network parameters respectively
- The method demonstrates superior performance across various image restoration tasks on both synthetic and real-world datasets
- PROD generates ~137K degradation representations versus ~4K in prior methods, improving generalization

## Why This Works (Mechanism)

### Mechanism 1
Contribution-based layer capacity allocation improves parameter efficiency without sacrificing performance. The method uses FAIG scores to quantify each layer's contribution to the new task, allocating more learnable parameters to high-contribution layers (encoder/decoder) and fewer to low-contribution layers (middle layers). This assumes FAIG scores accurately reflect layer importance and that encoder/decoder parts are more task-specific than middle layers.

### Mechanism 2
PROD improves robustness and generalization for real-world degradation by applying random combinations of synthetic degradations during pre-training. This creates a broader pre-training distribution and bridges synthetic pre-training with real-world fine-tuning, assuming real-world degradations are combinations of simpler synthetic degradations.

### Mechanism 3
CoLoRA achieves comparable performance to full fine-tuning with small learnable parameters (7%) by using LoRA-style low-rank decomposition with layer-specific rank values determined by contribution scores. This creates small adapters that can be merged with frozen weights without inference overhead, assuming low-rank decomposition can capture necessary task adaptation within the constrained parameter budget.

## Foundational Learning

- Concept: Low-rank matrix decomposition (matrix factorization)
  - Why needed here: LoRA and CoLoRA rely on decomposing weight updates into low-rank matrices to achieve parameter efficiency
  - Quick check question: If you have a weight matrix W of size d×k, and you decompose it as BA where B is d×r and A is r×k with r << min(d,k), what's the total number of parameters needed versus the original?

- Concept: Gradient-based attribution methods (Integrated Gradients)
  - Why needed here: FAIG scores used to determine layer contributions are based on Integrated Gradients, which measures feature importance
  - Quick check question: If you have a baseline model and a fine-tuned model, and you compute the gradient of the loss with respect to interpolated models, what does this tell you about which parameters are most important for the new task?

- Concept: Pre-training paradigms and domain adaptation
  - Why needed here: PROD extends pre-training concepts from NLP/CV to low-level vision with synthetic degradation combinations
  - Quick check question: What's the key difference between pre-training on clean images versus pre-training on synthetically degraded images for image restoration tasks?

## Architecture Onboarding

- Component map: Pre-trained backbone (NAFNet or Restormer) -> PROD module (synthetic degradation generator) -> CoLoRA adapter (layer-specific LoRA matrices with contribution-based scaling) -> FAIG computation module (for contribution scoring) -> Training pipeline (pre-training → fine-tuning with CoLoRA)

- Critical path: 1. Pre-train backbone with PROD synthetic degradations, 2. Compute FAIG scores on a small validation set, 3. Configure CoLoRA layer capacities based on FAIG, 4. Fine-tune with CoLoRA adapters only, 5. Merge adapters with backbone for inference

- Design tradeoffs: FAIG computation vs. random allocation (FAIG is more principled but adds pre-processing overhead), PROD complexity vs. performance (more degradation combinations improve generalization but increase pre-training time), CoLoRA parameter budget vs. task difficulty (harder tasks may need larger r values)

- Failure signatures: Low FAIG variance across layers (contribution-based allocation provides no benefit over uniform allocation), PROD pre-training hurts rather than helps (synthetic degradations don't match real distribution), CoLoRA underfits (low-rank approximation insufficient for task adaptation)

- First 3 experiments: 1. FAIG ablation: Compare CoLoRA with FAIG-based allocation vs. uniform allocation on a single task, 2. PROD ablation: Compare PROD pre-training vs. single-degradation pre-training on a single task, 3. Rank sensitivity: Sweep r values for different layers to find optimal contribution scaling

## Open Questions the Paper Calls Out

### Open Question 1
How does the contribution-based allocation of learnable parameters in CoLoRA compare to alternative strategies, such as optimizing rank per layer or using adaptive rank selection during training? The paper does not explore or compare this approach to other possible strategies for determining the rank per layer, and experimental results comparing CoLoRA to alternative methods would help evaluate its effectiveness.

### Open Question 2
How does the proposed PROD method for pre-training with random order degradations compare to other data augmentation techniques or pre-training strategies for low-level vision tasks? The paper does not provide any experimental comparison of PROD to other data augmentation techniques or pre-training strategies specifically designed for low-level vision tasks.

### Open Question 3
How does the proposed CoLoRA method perform on other low-level vision tasks beyond image restoration, such as image super-resolution, denoising, or deblurring? The paper focuses on image restoration tasks but does not explore its performance on other low-level vision tasks, and experimental results on these tasks would help determine its generalizability.

## Limitations
- Unknown implementation details for synthetic degradation functions used in PROD method
- Unknown exact values of scaling factors α and β used in CoLoRA method for different network architectures
- Poor performance possible due to improper selection of synthetic degradation functions during pre-training

## Confidence

**High Confidence**: The parameter efficiency claims (7% of parameters) and general LoRA adaptation framework are well-established in the literature

**Medium Confidence**: The PROD pre-training strategy and FAIG-based contribution scoring are novel contributions with reasonable theoretical foundations but limited empirical validation across diverse tasks

**Medium Confidence**: The claim of comparable performance to full fine-tuning with 7% of parameters is supported by experiments but requires verification across more diverse real-world scenarios

## Next Checks

1. **FAIG Ablation Study**: Implement CoLoRA with uniform layer capacity allocation versus FAIG-based allocation to quantify the actual performance benefit of the contribution-based approach

2. **PROD Generalization Test**: Evaluate PROD pre-training on real-world degradation types not represented in the synthetic combinations to test true robustness claims

3. **Memory and Inference Overhead**: Measure actual memory usage and inference latency when merging CoLoRA adapters with frozen weights versus full fine-tuning