---
ver: rpa2
title: 'Decision Making under the Exponential Family: Distributionally Robust Optimisation
  with Bayesian Ambiguity Sets'
arxiv_id: '2411.16829'
source_url: https://arxiv.org/abs/2411.16829
tags:
- posterior
- ambiguity
- bayesian
- distribution
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Distributionally Robust Optimization with\
  \ Bayesian Ambiguity Sets (DRO-BAS), a framework that incorporates posterior beliefs\
  \ into distributionally robust optimization to mitigate model and data uncertainty.\
  \ The method defines two types of ambiguity sets\u2014one based on posterior predictive\
  \ distributions and another on posterior expectations\u2014that lead to tractable\
  \ single-stage stochastic programs via strong duality results."
---

# Decision Making under the Exponential Family: Distributionally Robust Optimisation with Bayesian Ambiguity Sets

## Quick Facts
- arXiv ID: 2411.16829
- Source URL: https://arxiv.org/abs/2411.16829
- Authors: Charita Dellaporta; Patrick O'Hara; Theodoros Damoulas
- Reference count: 40
- This paper introduces DRO-BAS, a framework that incorporates posterior beliefs into distributionally robust optimization to mitigate model and data uncertainty.

## Executive Summary
This paper introduces Distributionally Robust Optimization with Bayesian Ambiguity Sets (DRO-BAS), a framework that incorporates posterior beliefs into distributionally robust optimization to mitigate model and data uncertainty. The method defines two types of ambiguity sets—one based on posterior predictive distributions and another on posterior expectations—that lead to tractable single-stage stochastic programs via strong duality results. The authors prove that for conjugate exponential family models, the worst-case risk can be computed exactly, and the optimal tolerance level can be derived in closed form.

## Method Summary
The authors propose two DRO-BAS formulations: DRO-BASPE based on posterior expectations and DRO-BASPP based on posterior predictive distributions. Both methods define ambiguity sets using KL divergence from posterior-informed distributions, leading to dual formulations that reduce to single-stage stochastic programs. For exponential family models with conjugate priors, the expected KL divergence decomposes into tractable terms, enabling closed-form expressions for the worst-case risk. The methods are evaluated on Newsvendor and Portfolio problems, showing improved computational efficiency and robustness compared to existing Bayesian DRO approaches.

## Key Results
- DRO-BAS achieves strong duality for exponential family models via a posterior-adjusted KL ambiguity set
- DRO-BASPE offers closed-form dual and faster solve times for linear objectives with Gaussian likelihood
- Experiments show DRO-BAS dominates existing Bayesian DRO methods in out-of-sample mean-variance tradeoffs while achieving faster solve times

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DRO-BASPE achieves strong duality for exponential family models via a posterior-adjusted KL ambiguity set.
- Mechanism: The posterior expectation of the KL divergence decomposes into the KL divergence from the posterior mean distribution plus a constant term G(⌢τ, ⌢ν). This allows the dual formulation to be reduced to a single-stage stochastic program.
- Core assumption: The model belongs to the exponential family and the posterior is conjugate; the moment generating function of fx(ξ) under the posterior mean distribution is finite.
- Evidence anchors:
  - [abstract]: "For DRO-BAS(PE) this covers all conjugate exponential family members while for DRO-BAS(PP) this is shown under conditions on the predictive's moment generating function."
  - [section 3.3]: Derives closed-form expressions for the expected KL divergence and shows strong duality under the stated assumptions.
  - [corpus]: Weak. No direct corpus paper focuses on this specific duality decomposition.
- Break condition: If the model is not in the exponential family, or if the moment generating function assumption is violated (e.g., heavy-tailed posteriors like Student-t).

### Mechanism 2
- Claim: DRO-BASPP reduces to a tractable single-stage stochastic program when the posterior predictive has a closed-form distribution.
- Mechanism: The KL divergence from the posterior predictive distribution yields a dual formulation involving the log moment generating function of the objective under the posterior predictive.
- Core assumption: The posterior predictive distribution can be computed in closed form and its moment generating function is finite for the objective.
- Evidence anchors:
  - [abstract]: "When the chosen discrepancy is the KL divergence and the posterior predictive can be obtained in closed-form, DRO-BAS with the posterior predictive attains a dual formulation which is an efficient single-stage stochastic program."
  - [section 3.1]: Provides the dual formulation under the assumption that the moment generating function exists.
  - [corpus]: Weak. No corpus paper provides a direct anchor for this mechanism.
- Break condition: If the posterior predictive is not available in closed form, or if the moment generating function is infinite (e.g., Student-t posterior predictive).

### Mechanism 3
- Claim: DRO-BAS dominates BDRO in out-of-sample mean-variance tradeoffs when the number of SAA samples is small.
- Mechanism: DRO-BAS requires sampling only from the posterior or posterior predictive, while BDRO requires nested sampling from both posterior and likelihood. This reduces variance in the SAA estimate and improves robustness.
- Core assumption: The objective function is convex and the dual can be solved efficiently with fewer samples.
- Evidence anchors:
  - [abstract]: "Experiments on the Newsvendor and Portfolio problems show that DRO-BAS dominates existing Bayesian DRO methods in the Pareto sense for out-of-sample mean-variance tradeoffs while achieving faster solve times..."
  - [section 4.1]: Demonstrates that DRO-BAS forms a Pareto front for out-of-sample mean-variance tradeoffs in the Newsvendor problem with fewer samples.
  - [section 4.2]: Shows that DRO-BASPE has comparable or better cumulative returns than BDRO on the Portfolio problem.
- Break condition: If the objective is non-convex or the problem structure prevents efficient SAA with fewer samples.

## Foundational Learning

- Concept: Exponential family distributions and conjugate priors
  - Why needed here: The duality results and closed-form expressions rely on the exponential family structure and the existence of conjugate priors.
  - Quick check question: What is the form of the log-partition function for a Gaussian distribution, and how does it relate to the natural parameters?

- Concept: KL divergence and its convex conjugate
  - Why needed here: The ambiguity sets are defined using KL divergence, and the dual formulations use the convex conjugate of the KL divergence.
  - Quick check question: What is the convex conjugate of the KL divergence, and under what conditions does it equal the log moment generating function?

- Concept: Sample Average Approximation (SAA) and its variance
  - Why needed here: Both DRO-BAS and BDRO use SAA to approximate expectations, but DRO-BAS requires fewer samples due to its single-stage structure.
  - Quick check question: How does the variance of an SAA estimate scale with the number of samples, and why does nested sampling in BDRO increase variance?

## Architecture Onboarding

- Component map: Data D -> Model PΘ with prior π(θ) -> Posterior Π(θ|D) -> DRO-BASPE: Compute G(⌢τ, ⌢ν) and ˆη, solve dual with expectation over p(ξ|ˆη) OR DRO-BASPP: Solve dual with expectation over Pn -> Output: Decision x

- Critical path:
  1. Fit model to data and obtain posterior
  2. For DRO-BASPE: Calculate G(⌢τ, ⌢ν) and ˆη using Lemma 1
  3. For both methods: Set up and solve the dual program using SAA
  4. Return the optimal decision

- Design tradeoffs:
  - DRO-BASPE: Requires exponential family and conjugate prior, but offers closed-form dual and faster solve times for linear objectives with Gaussian likelihood
  - DRO-BASPP: More general model applicability, but may require more samples and has no closed-form dual for exponential family cases
  - BDRO: More general than DRO-BASPE, but requires nested sampling and has higher variance in SAA estimates

- Failure signatures:
  - DRO-BASPE: Unbounded problem if ϵ < ϵmin(n), or numerical instability if moment generating function assumption is violated
  - DRO-BASPP: High variance in SAA estimates if posterior predictive is not available in closed form
  - Both: Poor performance if model is severely misspecified

- First 3 experiments:
  1. Verify the duality results for a simple exponential family model (e.g., Gaussian with known variance) by comparing the primal and dual objective values
  2. Compare the out-of-sample performance of DRO-BASPE and BDRO on a small Newsvendor problem with few observations
  3. Test the computational efficiency of DRO-BASPE vs BDRO on a Portfolio problem with linear objective and Gaussian likelihood

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does DRO-BASPE outperform DRO-BASPP in high-dimensional settings?
- Basis in paper: [explicit] The paper discusses computational differences between the two formulations, noting that DRO-BASPP samples from the posterior predictive without any closed-form expression for the mean or covariance, while DRO-BASPE leverages properties of the exponential family to obtain more efficient solutions.
- Why unresolved: The paper empirically demonstrates that DRO-BASPE is more suitable for the Portfolio problem, but does not provide theoretical analysis identifying conditions favoring one method over the other.
- What evidence would resolve it: A theoretical analysis comparing the growth of ambiguity set volumes as a function of tolerance level ϵ for both formulations in high-dimensional settings.

### Open Question 2
- Question: How does the performance of DRO-BAS compare to existing Bayesian DRO methods under model misspecification?
- Basis in paper: [explicit] The paper includes experiments with a truncated normal distribution as the true DGP but uses a normal model, showing that both DRO-BAS formulations perform at least as well as BDRO in the Pareto sense.
- Why unresolved: While the paper shows DRO-BAS is robust to mild model misspecification, it does not explore the limits of this robustness or compare performance across different degrees of misspecification.
- What evidence would resolve it: A systematic study varying the degree of model misspecification and comparing DRO-BAS performance against other robust optimization methods.

### Open Question 3
- Question: Can DRO-BAS be extended to handle time series or sequence data beyond i.i.d. observations?
- Basis in paper: [inferred] The paper notes that the current approach is limited to i.i.d. data and exponential-family models, leaving open questions about its applicability to more complex data types.
- Why unresolved: The paper focuses on the i.i.d. case and does not explore extensions to dependent data structures.
- What evidence would resolve it: Development and testing of DRO-BAS variants that incorporate time series or sequential dependencies in the data-generating process.

## Limitations
- The strong duality results require exponential family models with conjugate priors, limiting applicability to non-exponential family cases
- Both methods require finite moment generating functions, which may be violated for heavy-tailed posteriors or objectives
- The exact sample complexity bounds and convergence rates for SAA are not explicitly characterized

## Confidence
- **High confidence**: The dual formulations and closed-form expressions for exponential family models are mathematically rigorous and supported by strong duality theory
- **Medium confidence**: The computational efficiency claims are supported by experimental evidence but may depend on problem-specific factors like linear vs. nonlinear objectives
- **Medium confidence**: The out-of-sample performance improvements over BDRO are demonstrated empirically but may not generalize to all problem structures

## Next Checks
1. **Verify moment generating function conditions**: Test DRO-BASPP on a case where the posterior predictive has infinite moment generating function (e.g., Student-t posterior) to confirm the failure mode
2. **Characterize sample complexity**: Compare the convergence rates of SAA for DRO-BAS vs BDRO on a simple problem with known ground truth to quantify the variance reduction
3. **Test non-exponential family models**: Apply DRO-BAS to a non-exponential family model (e.g., logistic regression with Gaussian prior) to assess the robustness of the duality results