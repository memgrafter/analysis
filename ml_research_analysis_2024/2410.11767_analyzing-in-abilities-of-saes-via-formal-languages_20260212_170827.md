---
ver: rpa2
title: Analyzing (In)Abilities of SAEs via Formal Languages
arxiv_id: '2410.11767'
source_url: https://arxiv.org/abs/2410.11767
tags:
- features
- saes
- causal
- value
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the ability of sparse autoencoders (SAEs) to
  learn interpretable and causally relevant features from transformer representations
  of formal languages. The authors train SAEs on transformer hidden states for Dyck-2,
  Expr, and English PCFGs under various hyperparameter settings.
---

# Analyzing (In)Abilities of SAEs via Formal Languages

## Quick Facts
- **arXiv ID**: 2410.11767
- **Source URL**: https://arxiv.org/abs/2410.11767
- **Authors**: Abhinav Menon; Manish Shrivastava; David Krueger; Ekdeep Singh Lubana
- **Reference count**: 40
- **Primary result**: SAEs can identify interpretable features correlating with generative factors in formal languages, but these features often lack causal impact on model outputs.

## Executive Summary
This paper investigates the ability of sparse autoencoders (SAEs) to learn interpretable and causally relevant features from transformer representations of formal languages. The authors train SAEs on transformer hidden states for Dyck-2, Expr, and English PCFGs under various hyperparameter settings. They find that SAEs can identify features correlating with key generative factors like bracket depth, expression counters, and parts-of-speech, but these features often lack causal impact on model outputs. The performance is highly sensitive to hyperparameters and regularization methods. To address this, the authors propose a causal regularization term that exploits token-level correlations as weak supervision, which leads to features with predictable causal effects on model behavior. This work highlights the importance of causality in SAE training and demonstrates that synthetic formal languages can be valuable testbeds for studying SAE interpretability.

## Method Summary
The authors train transformers on formal languages (Dyck-2, Expr, and English PCFGs) and then apply sparse autoencoders to the transformer hidden states to identify interpretable features. They experiment with different SAE architectures (varying hidden sizes, normalization methods, and regularization types like L1 and top-k) and evaluate feature interpretability through correlation analysis and causal relevance through intervention protocols. The key innovation is a causal regularization term that encourages the SAE decoder to write to the MLP's input subspace rather than the attention's input subspace, using interpolation-based weak supervision from token-level correlations.

## Key Results
- SAEs can identify interpretable features that correlate with generative factors like bracket depth in Dyck-2 and parts-of-speech in English
- Identified features often lack causal impact on model outputs despite showing strong correlations
- The proposed causal regularization method successfully promotes features with predictable causal effects on model behavior
- SAE performance is highly sensitive to hyperparameters, regularization methods, and normalization strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse autoencoders (SAEs) can identify interpretable features that correlate with generative factors in formal language models.
- Mechanism: SAEs learn a sparse latent representation that reconstructs the transformer hidden states. By enforcing sparsity (via L1 or top-k regularization), the autoencoder is encouraged to activate only on specific patterns in the input that correspond to meaningful generative factors like depth in Dyck-2 or parts of speech in English.
- Core assumption: The transformer's hidden states contain disentangled representations of the generative factors, and sparsity regularization will isolate these factors into individual latent dimensions.
- Evidence anchors:
  - [abstract] "finding interpretable latents often emerge in the features learned by our SAEs"
  - [section] "we find features representing fundamental aspects of the corresponding grammars"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: If the transformer representations are highly entangled or if sparsity regularization fails to isolate individual factors, SAEs will not identify interpretable features.

### Mechanism 2
- Claim: Causal regularization using token-level correlations as weak supervision promotes the learning of causally relevant features.
- Mechanism: The proposed causal loss term encourages the SAE decoder to write to the MLP's input subspace rather than the attention's input subspace. This is achieved by defining a loss that measures the difference between interpolated model outputs and the output of interpolated latents. By interpolating between latents of related tokens, the model is incentivized to learn features that have predictable causal effects on the model's output.
- Core assumption: The MLP module is more linear than the attention module, and features that affect the MLP will have more predictable causal effects.
- Evidence anchors:
  - [section] "it effectively incentivizes the function g to be distributive over convex combinations (i.e., over addition and scalar multiplication)"
  - [section] "the more D writes to the MLP's input subspace, the 'more linear' it is"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: If the MLP is not significantly more linear than the attention module, or if the interpolation technique fails to capture meaningful correlations between tokens, the causal regularization will not promote the learning of causally relevant features.

### Mechanism 3
- Claim: The performance of SAEs is highly sensitive to hyperparameters and regularization methods.
- Mechanism: The choice of regularization method (L1 vs. top-k), normalization strategy, and other hyperparameters significantly affects the ability of SAEs to identify interpretable and causally relevant features. L1 regularization may fail to find interpretable features, while top-k regularization may perform better. Normalization can have widely varying effects on reconstruction abilities.
- Core assumption: The SAE training process is sensitive to the specific hyperparameters and regularization methods used.
- Evidence anchors:
  - [abstract] "performance turns out to be highly sensitive to inductive biases of the training pipeline"
  - [section] "no clear trend is visible across languages, regularization methods, or normalization settings"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: If the SAE training process is not sensitive to hyperparameters and regularization methods, or if there is a clear trend across different settings, the claim of high sensitivity will be invalid.

## Foundational Learning

- Concept: Sparse Autoencoders (SAEs)
  - Why needed here: SAEs are the core method used to identify interpretable features in transformer representations. Understanding their architecture and training process is essential for interpreting the results.
  - Quick check question: What is the primary difference between SAEs and standard autoencoders?
- Concept: Formal Languages
  - Why needed here: The study uses formal languages (Dyck-2, Expr, and English PCFGs) as a testbed for evaluating SAEs. Understanding the properties of these languages and their generative factors is crucial for interpreting the identified features.
  - Quick check question: What is the generative factor in Dyck-2 that SAEs are expected to identify?
- Concept: Causality and Interventions
  - Why needed here: The study investigates the causal relevance of the identified features. Understanding the concept of interventions and their effects on model behavior is essential for interpreting the results.
  - Quick check question: How are interventions used to assess the causal relevance of SAE features?

## Architecture Onboarding

- Component map: Transformer model -> SAE (encoder and decoder with sparsity regularization) -> Causal regularization term -> Intervention protocol
- Critical path:
  1. Train transformer on formal language
  2. Train SAE on transformer hidden states with sparsity regularization
  3. Identify interpretable features in SAE latent space
  4. Assess the causal relevance of identified features using interventions
  5. Apply causal regularization and repeat steps 2-4
- Design tradeoffs:
  - Sparsity regularization vs. reconstruction accuracy: Higher sparsity may lead to more interpretable features but lower reconstruction accuracy
  - Causal regularization vs. disentanglement: Causal regularization may promote causally relevant features but may also introduce biases
- Failure signatures:
  - Low reconstruction accuracy: SAEs fail to learn meaningful representations of transformer hidden states
  - Lack of interpretable features: SAEs fail to identify features that correlate with generative factors
  - Lack of causal effects: Identified features do not have predictable effects on model behavior
- First 3 experiments:
  1. Train SAE with L1 regularization on Dyck-2 transformer hidden states and assess reconstruction accuracy and interpretability
  2. Train SAE with top-k regularization on Expr transformer hidden states and assess reconstruction accuracy and interpretability
  3. Apply causal regularization to English PCFG SAE and assess the causal effects of identified features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed causal regularization method be adapted to identify features that affect the attention module rather than just the MLP?
- Basis in paper: [explicit] The authors note that their causal loss method prefers features affecting the MLP over attention, and hypothesize this explains why no interpretable features were found in Dyck-2 and Expr models.
- Why unresolved: The current causal regularization method is designed to promote linearity in the function g (which includes MLP), but attention modules are highly nonlinear due to softmax operations. Alternative regularization approaches may be needed to identify attention-relevant features.
- What evidence would resolve it: Demonstrating a modified causal regularization method that successfully identifies interpretable features in Dyck-2 and Expr models would show the approach can be adapted for attention-relevant features.

### Open Question 2
- Question: Does the sensitivity to hyperparameters observed in synthetic formal languages persist when applying SAEs to natural language data?
- Basis in paper: [inferred] The authors observe high sensitivity to hyperparameters across regularization methods, normalization strategies, and other settings in their synthetic testbed, noting this aligns with Locatello et al. (2019)'s findings.
- Why unresolved: While the authors validate their setup using synthetic data, they acknowledge significant complications can arise when moving to natural language settings, particularly regarding full-distribution variance versus task-specific variance.
- What evidence would resolve it: Systematic testing of SAE performance across various hyperparameter settings on natural language datasets would reveal whether the same sensitivity patterns observed in formal languages extend to real-world applications.

### Open Question 3
- Question: Can the trade-off between feature interpretability and model capability preservation be overcome?
- Basis in paper: [explicit] The authors note that interventions on SAE-identified features often damage model output quality in terms of coherence, creating a control-capabilities tradeoff, and reference recent work observing this limitation.
- Why unresolved: While the authors propose methods to incentivize causally relevant features, they acknowledge this limitation and suggest overcoming it may require approaches beyond simple regularization.
- What evidence would resolve it: Demonstrating SAE interventions that achieve both interpretable feature identification and maintained model performance on downstream tasks would resolve whether this fundamental tradeoff can be overcome.

## Limitations
- The effectiveness of the causal regularization method relies heavily on the linearity assumption of the MLP module versus the attention module, which lacks direct empirical validation
- The study is limited to synthetic formal languages, and it's unclear whether the observed hyperparameter sensitivity and interpretability patterns extend to natural language data
- The intervention protocol may not capture all forms of causal relevance, as identified features often damage model output quality when intervened upon

## Confidence

**High confidence**: Claims about SAEs identifying correlated features - supported by explicit correlation measurements and manual feature inspection across multiple formal languages. **Medium confidence**: Claims about hyperparameter sensitivity - the paper demonstrates varying performance but lacks systematic hyperparameter sweeps to establish definitive sensitivity patterns. **Low confidence**: Claims about causal regularization effectiveness - relies heavily on theoretical arguments without sufficient empirical validation of the linearity assumptions or the interpolation technique's reliability.

## Next Checks

1. **Ablation study on MLP vs attention linearity**: Modify the SAE architecture to force latents to only affect either MLP or attention module, then measure which produces more causally predictable features.
2. **Synthetic feature intervention benchmark**: Create controlled synthetic features with known causal effects, train SAEs on representations containing these features, and measure recovery accuracy under different regularization schemes.
3. **Cross-grammar feature transferability test**: Train SAEs on one formal language (e.g., Dyck-2), then test if the learned features transfer to related languages (e.g., Dyck-3) and maintain their causal properties.