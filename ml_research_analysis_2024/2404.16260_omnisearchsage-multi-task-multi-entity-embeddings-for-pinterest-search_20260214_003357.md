---
ver: rpa2
title: 'OmniSearchSage: Multi-Task Multi-Entity Embeddings for Pinterest Search'
arxiv_id: '2404.16260'
source_url: https://arxiv.org/abs/2404.16260
tags:
- query
- search
- embeddings
- pinterest
- product
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OmniSearchSage introduces a unified multi-task multi-entity embedding
  system for Pinterest search that jointly learns query, pin, and product representations.
  The system enriches entity representations using diverse text sources including
  synthetic GenAI captions, historical engagement queries, and user-curated board
  titles.
---

# OmniSearchSage: Multi-Task Multi-Entity Embeddings for Pinterest Search

## Quick Facts
- arXiv ID: 2404.16260
- Source URL: https://arxiv.org/abs/2404.16260
- Reference count: 40
- Primary result: Unified multi-task multi-entity embeddings achieve >8% relevance, >7% engagement, and >5% ads CTR improvements in production Pinterest search

## Executive Summary
OmniSearchSage introduces a unified multi-task multi-entity embedding system for Pinterest search that jointly learns query, pin, and product representations. The system enriches entity representations using diverse text sources including synthetic GenAI captions, historical engagement queries, and user-curated board titles. It employs a multilingual DistilBERT query encoder, a unified pin/product encoder, and compatibility encoders to maintain alignment with existing embeddings. Multi-task learning with sampled softmax loss enables efficient training across query-query, query-pin, and query-product retrieval tasks. The system serves 300k requests per second with 3ms median latency and achieves significant improvements: >8% relevance, >7% engagement, and >5% ads CTR in production.

## Method Summary
OmniSearchSage learns multi-task multi-entity embeddings through joint training of query, pin, and product representations using sampled softmax loss. The system employs a multilingual DistilBERT query encoder producing 256-dimensional embeddings, a unified pin/product encoder with hash embeddings and multiple tokenizers, and compatibility encoders to maintain alignment with existing PinSage and ItemSage embeddings. Text enrichment uses synthetic GenAI captions, board titles, and engaged queries. The model is trained on 1.5B query-pin, 136M query-product, and 195M query-query pairs, serving embeddings with GPU-based inference and 30-day TTL caching.

## Key Results
- +4.1% search fulfillment rate and +3% relevance improvements in organic search A/B tests
- +5.27% gCTR gains in ads search A/B tests
- 20% improvement in engagement datasets and 30% improvement in relevance metrics from text enrichment

## Why This Works (Mechanism)

### Mechanism 1
Jointly learning query, pin, and product embeddings in a single unified model achieves performance comparable to or better than task-specific embeddings. Multi-task learning allows shared representation space across different entity types and tasks, reducing model redundancy while enabling information transfer between tasks. The semantic relationships between queries, pins, and products are sufficiently aligned to benefit from joint training. Ablation studies show slight degradation on pin tasks but neutral to positive results on product and query tasks from multi-task learning.

### Mechanism 2
Enriching pin and product representations with diverse text sources significantly improves embedding quality. Expanding the text context associated with each entity (using synthetic captions, board titles, and engaged queries) provides richer semantic information that improves the model's understanding of content. Additional text sources provide complementary and non-redundant information about the entities. Adding Title, Description, and Synthetic GenAI Captions to the baseline model shows 20% improvement in engagement datasets and 30% improvement in relevance metrics.

### Mechanism 3
Compatibility encoders enable seamless integration with existing pin and product embeddings without degradation in performance. Dedicated encoders learn to map the unified embeddings to the spaces of pre-existing PinSage and ItemSage embeddings, maintaining backward compatibility. The semantic relationships captured by the new unified embeddings are compatible with those captured by the existing embeddings. Incorporating compatibility encoders shows almost no noticeable degradation in learned encoder metrics, achieving seamless compatibility.

## Foundational Learning

- **Multi-task learning and shared representation spaces**: Why needed here: The system needs to learn embeddings for multiple entity types (queries, pins, products) while maintaining compatibility with existing embeddings and achieving good performance across different retrieval tasks. Quick check question: If you were to train separate models for each task instead of using multi-task learning, what would be the main trade-offs in terms of model complexity, inference latency, and potentially performance?

- **Text representation and feature engineering**: Why needed here: The system relies on multiple text sources (titles, descriptions, synthetic captions, board titles, engaged queries) to enrich entity representations, requiring understanding of how different text features contribute to embedding quality. Quick check question: How would you evaluate whether adding synthetic captions provides meaningful improvement versus just adding noise to the training process?

- **Approximate Nearest Neighbor (ANN) search and embedding serving**: Why needed here: The system serves 300k requests per second with 3ms median latency, requiring efficient embedding storage, retrieval, and serving infrastructure. Quick check question: What factors would you consider when choosing between different ANN algorithms (HNSW, IVF, etc.) for serving these embeddings at Pinterest's scale?

## Architecture Onboarding

- **Component map**: Search Query → Query Encoder (DistilBERT) → Embedding Cache → Retrieval Models; Pin/Product → Unified Encoder (Hash Embeddings) → Compatibility Encoders → Existing Embedding Spaces
- **Critical path**: Query → Query Encoder → Embedding Cache → Retrieval/Ranking Models
- **Design tradeoffs**: Unified vs. separate encoders reduces model count and maintenance but may slightly sacrifice task-specific performance; Hash embeddings reduce memory usage and enable larger vocabularies but may have slightly lower quality; Multi-task enables consolidation and knowledge sharing but may slightly degrade high-data tasks
- **Failure signatures**: Query embedding cache misses causing latency spikes; Embedding quality degradation when new entity types are added; Compatibility issues when pre-existing embeddings are updated; Training instability when mixing tasks with very different data volumes
- **First 3 experiments**: 1) Compare unified encoder performance against separate pin/product encoders on a small dataset; 2) Measure the impact of synthetic caption quality on retrieval metrics; 3) Test cache TTL optimization by varying between 7, 30, and 90 days and measuring hit rate vs. freshness

## Open Questions the Paper Calls Out

1. **Contribution of each text enrichment feature**: What is the exact contribution of each text enrichment feature (synthetic GenAI captions, board titles, engaged queries) to the overall performance of OmniSearchSage? The paper mentions that each feature contributes significantly to improving model performance, but does not provide a detailed breakdown of their individual contributions. A detailed ablation study isolating the contribution of each text enrichment feature would resolve this question.

2. **Performance comparison to other state-of-the-art systems**: How does the performance of OmniSearchSage compare to other state-of-the-art multi-task multi-entity embedding systems for search? The paper mentions that OmniSearchSage shows significant improvements over the baseline SearchSage model, but does not compare its performance to other state-of-the-art systems. A comparison of OmniSearchSage's performance to other state-of-the-art multi-task multi-entity embedding systems for search would resolve this question.

3. **Performance variation across languages and cultural contexts**: How does the performance of OmniSearchSage vary across different query languages and cultural contexts? The paper mentions that OmniSearchSage is evaluated on queries from four distinct countries (US, UK, France, and Germany), but does not provide a detailed analysis of its performance across different languages and cultural contexts. A detailed analysis of OmniSearchSage's performance across different query languages and cultural contexts, including specific metrics for each language and region, would resolve this question.

## Limitations

- Evaluation results are based on Pinterest's internal metrics and may not generalize to other platforms with different user behavior patterns
- System's performance heavily depends on synthetic GenAI caption quality, which is not fully controlled or evaluated
- Long-term compatibility maintenance with evolving pre-existing embeddings is not addressed

## Confidence

- **High Confidence**: Core multi-task learning architecture and implementation details are well-specified and technically sound; Latency and throughput metrics (300k requests/second, 3ms median latency) are specific and verifiable
- **Medium Confidence**: Reported improvements in relevance, engagement, and ads CTR are plausible given the methodology, but exact magnitude depends on Pinterest's specific evaluation framework
- **Low Confidence**: Long-term stability of unified embedding space and behavior with new entity types or changing user behavior patterns is not addressed

## Next Checks

1. **Synthetic Caption Quality Validation**: Implement human evaluation pipeline to assess synthetic GenAI caption quality across different pin categories and measure correlation with downstream retrieval performance to establish quality thresholds.

2. **Compatibility Stability Test**: Conduct longitudinal study running system for 3-6 months while periodically updating pre-existing PinSage and ItemSage embeddings to measure compatibility degradation over time and determine optimal retraining frequency.

3. **Cross-Platform Generalization Test**: Deploy simplified version of unified embedding system on different content platform (e-commerce or news) with different entity types and user engagement patterns to validate multi-task approach generalizability.