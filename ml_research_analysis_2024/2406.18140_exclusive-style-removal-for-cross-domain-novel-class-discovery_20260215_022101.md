---
ver: rpa2
title: Exclusive Style Removal for Cross Domain Novel Class Discovery
arxiv_id: '2406.18140'
source_url: https://arxiv.org/abs/2406.18140
tags:
- domain
- style
- data
- novel
- labeled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel task called Cross Domain Novel Class
  Discovery (CDNCD), which addresses the challenge of clustering unseen novel classes
  in an unlabeled set from a different domain than the labeled data. The authors first
  theoretically analyze the solvability of CDNCD and establish the necessary condition
  that style information must be removed.
---

# Exclusive Style Removal for Cross Domain Novel Class Discovery

## Quick Facts
- arXiv ID: 2406.18140
- Source URL: https://arxiv.org/abs/2406.18140
- Authors: Yicheng Wang; Feng Liu; Junmin Liu; Kai Sun
- Reference count: 40
- Key outcome: Introduces Cross Domain Novel Class Discovery (CDNCD) task with exclusive style removal module that improves clustering accuracy on novel classes from different domains

## Executive Summary
This paper introduces a novel task called Cross Domain Novel Class Discovery (CDNCD), which addresses the challenge of clustering unseen novel classes in an unlabeled set from a different domain than the labeled data. The authors first theoretically analyze the solvability of CDNCD and establish the necessary condition that style information must be removed. Based on this analysis, they propose an exclusive style removal module that extracts distinctive style information from baseline features, facilitating inference. The module can be easily integrated with other NCD methods as a plug-in to improve performance on novel classes with different distributions compared to the labeled set.

The proposed method is evaluated on three common datasets (CIFAR10, OfficeHome, and DomainNet40) with extensive experiments. The results demonstrate the effectiveness of the style removal strategy in improving clustering accuracy on novel classes. The authors also build a fair benchmark for future NCD research by recognizing the significant impact of different backbones and pre-training strategies on performance. Overall, this work provides valuable insights into the CDNCD task and presents a practical solution with promising results.

## Method Summary
The method introduces an exclusive style removal module that extracts domain-specific style features from both labeled and unlabeled data using a parallel ResNet18 encoder. By minimizing the inner product between content features (used for clustering) and style features, the method ensures content features are orthogonal to style, making them domain-invariant. The module is integrated with baseline NCD methods by adding a style removal loss to the training objective. A ViT-b backbone pre-trained with DINO self-supervised learning serves as the feature extractor, avoiding label leakage for novel classes. The approach is evaluated on three datasets (CIFAR10, OfficeHome, DomainNet40) using standard NCD protocols with labeled and unlabeled data from different domains.

## Key Results
- Style removal module improves clustering accuracy on novel classes from different domains compared to baseline NCD methods
- Self-supervised ViT backbone outperforms supervised pre-training for CDNCD task by avoiding label leakage
- The module can be integrated as a plug-in to existing NCD methods, demonstrating consistent performance improvements
- Fair benchmarking reveals significant impact of backbone and pre-training choices on CDNCD performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing style information that is exclusive to the labeled domain enables effective clustering of novel classes from a different domain.
- Mechanism: The style encoder extracts domain-specific features (style) from both labeled and unlabeled data. By minimizing the inner product between content features (used for clustering) and style features, the method ensures content features are orthogonal to style, making them domain-invariant.
- Core assumption: Style features are domain-specific and do not contribute to class discrimination; content features capture semantic information shared across domains.
- Evidence anchors:
  - [abstract] "Based on the theoretical analysis, we introduce an exclusive style removal module for extracting style information that is distinctive from the baseline features, thereby facilitating inference."
  - [section] "To ensure the solvability of NCD on cross domain setting as discussed in III-B, for better aligning content feature, we propose a simple yet effective strategy."
- Break condition: If style features contain discriminative information for novel classes, removing them would harm clustering performance.

### Mechanism 2
- Claim: Using self-supervised pre-trained ViT backbone improves performance by learning robust feature representations.
- Mechanism: The ViT backbone pre-trained on ImageNet via DINO learns rich visual representations without using class labels, avoiding label leakage for novel classes. This representation serves as a strong base for both clustering and style removal.
- Core assumption: Self-supervised pre-training on large-scale data provides generalizable features suitable for downstream tasks including cross-domain clustering.
- Evidence anchors:
  - [section] "We employ a vision transformer ViT-b [32] pre-trained on ImageNet in a self-supervised manner [33] as a feature representation backbone. Unlike [11], this self-supervised strategy does not contradict with the NCD setting, as the novel classes have no supervised information throughout all training processes."
- Break condition: If the pre-training dataset's domain distribution is too different from the target domain, the features may not generalize well.

### Mechanism 3
- Claim: The proposed style removal module can be integrated as a plug-in to improve existing NCD methods' performance on cross-domain data.
- Mechanism: The style encoder is trained in parallel with the baseline model, and its output is used to compute a style removal loss (e.g., inner product minimization). This loss is added to the baseline's objective, encouraging content features to be orthogonal to style features without modifying the baseline's architecture.
- Evidence anchors:
  - [section] "Moreover, this module is easy to integrate with other NCD methods, acting as a plug-in to improve performance on novel classes with different distributions compared to the seen labeled set."
  - [section] "In order to illustrate the improvement more clearly, we present the t-SNE figures in Fig. 4 showing the feature distribution obtained by [5] with and without the proposed module."
- Break condition: If the baseline model already incorporates effective domain adaptation mechanisms, adding style removal may provide minimal benefit or even degrade performance.

## Foundational Learning

- Concept: Cross-domain learning and domain adaptation
  - Why needed here: The task involves clustering novel classes from a different domain than the labeled data, requiring methods to handle domain shift.
  - Quick check question: What is the difference between domain adaptation and domain generalization, and which one is more relevant to CDNCD?

- Concept: Feature disentanglement (separating style and content)
  - Why needed here: The theoretical analysis shows that style information must be removed for CDNCD to be solvable, so the model must learn to disentangle style from content.
  - Quick check question: How does minimizing the inner product between content and style features encourage orthogonality?

- Concept: Novel class discovery (NCD) fundamentals
  - Why needed here: CDNCD builds on NCD, so understanding how NCD methods cluster unlabeled data using labeled data is essential.
  - Quick check question: In NCD, how are pseudo-labels for unlabeled data typically generated, and what role does the labeled data play?

## Architecture Onboarding

- Component map:
  - Input image -> Backbone (ViT-b) -> Base features
  - Base features -> Projection head -> Contrastive space
  - Base features -> Classification head -> Soft labels
  - Input image -> Style encoder (ResNet18) -> Style features
  - Compute style removal loss from base and style features
  - Combine all losses and backpropagate

- Critical path:
  1. Input image → Backbone → Base features
  2. Base features → Projection head → Contrastive space
  3. Base features → Classification head → Soft labels
  4. Input image → Style encoder → Style features
  5. Compute style removal loss from base and style features
  6. Combine all losses and backpropagate

- Design tradeoffs:
  - Using ResNet18 for style encoder vs. using the same backbone: Simpler and faster, but may not capture style as effectively
  - Different style removal objectives (inner product, cosine similarity, Pearson correlation): Inner product is simplest and empirically best, but others may work better in specific scenarios
  - Pre-training strategy: Self-supervised DINO vs. supervised ImageNet: Self-supervised avoids label leakage but may have lower initial performance

- Failure signatures:
  - Style removal loss dominates: Content features become too noisy for clustering
  - Style removal loss too small: Domain shift remains, harming performance
  - Backbone features not discriminative: Poor clustering even with style removal

- First 3 experiments:
  1. Train baseline [15] on CIFAR10cmix with varying Gaussian blur levels; observe performance drop with distribution shift
  2. Add style encoder with inner product loss to baseline; verify improvement on CIFAR10cmix
  3. Integrate style removal module into UNO [3] and test on CIFAR10cmix; compare performance with and without module

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed style removal module be effectively adapted for generalized category discovery tasks, where unlabeled data contains both seen and novel classes?
- Basis in paper: [inferred] The authors discuss the potential application of their method to generalized category discovery (GCD) but do not explore it experimentally. They mention that GCD requires methods with the ability to recognize previously seen categories and estimate the class number of novel classes in the unlabeled data.
- Why unresolved: The paper focuses on the CDNCD task and does not investigate the performance of their style removal module in GCD scenarios. It is unclear how the module would handle the additional complexity of distinguishing between seen and novel classes within the unlabeled data.
- What evidence would resolve it: Conducting experiments on GCD benchmarks and comparing the performance of the proposed method with state-of-the-art GCD approaches would provide insights into the effectiveness and limitations of the style removal module in this more challenging setting.

### Open Question 2
- Question: Can the style removal module be further improved by incorporating more sophisticated techniques for decoupling content and style features, such as adversarial training or contrastive learning?
- Basis in paper: [inferred] The authors use simple yet effective objective functions (inner product, cosine similarity, and Pearson correlation) to encourage orthogonality between content and style features. However, they do not explore more advanced techniques for feature disentanglement.
- Why unresolved: The current implementation of the style removal module relies on straightforward regularization terms, which may not be optimal for effectively separating content and style information. More sophisticated approaches could potentially lead to better performance and more robust feature representations.
- What evidence would resolve it: Comparing the performance of the proposed method with and without the incorporation of advanced feature disentanglement techniques, such as adversarial training or contrastive learning, on various CDNCD benchmarks would provide insights into the potential benefits and limitations of these approaches.

### Open Question 3
- Question: How does the performance of the proposed method scale with the size of the labeled and unlabeled datasets, and what are the implications for real-world applications with limited data?
- Basis in paper: [inferred] The authors evaluate their method on three common datasets (CIFAR10, OfficeHome, and DomainNet40) but do not investigate the impact of dataset size on performance. It is unclear how the method would perform in scenarios with limited labeled or unlabeled data.
- Why unresolved: Understanding the scalability of the proposed method with respect to dataset size is crucial for assessing its applicability to real-world scenarios, where data availability may be limited. It is also important to determine the minimum amount of labeled and unlabeled data required for effective performance.
- What evidence would resolve it: Conducting experiments with varying sizes of labeled and unlabeled datasets, ranging from small to large, and analyzing the performance trends would provide insights into the scalability of the proposed method. Additionally, comparing the performance of the method with other NCD approaches under different data size scenarios would help evaluate its effectiveness in real-world applications with limited data.

## Limitations

- The theoretical analysis establishing the necessary condition for CDNCD solvability assumes that style information is domain-specific and non-discriminative, but this assumption is not empirically validated
- The effectiveness of the ResNet18 style encoder is assumed based on ablation studies, but its capacity to capture all relevant style information compared to the ViT backbone is not thoroughly investigated
- The method's performance on datasets with more than two domains or more complex domain shifts is not evaluated

## Confidence

- **High Confidence**: The experimental results showing improved clustering accuracy when using the style removal module on cross-domain data
- **Medium Confidence**: The claim that the style removal module can be easily integrated as a plug-in to other NCD methods
- **Low Confidence**: The theoretical analysis that style removal is a necessary condition for CDNCD solvability without empirical validation of the disentanglement quality

## Next Checks

1. **Style-Content Disentanglement Quality**: Measure the correlation between style features and content features during training to quantify the effectiveness of the orthogonality constraint
2. **Style Encoder Capacity**: Replace the ResNet18 style encoder with a smaller or larger architecture to determine if the current choice is optimal for capturing domain-specific style information
3. **Generalization to More Domains**: Evaluate the method on datasets with more than two domains (e.g., DomainNet with 6 domains) to assess scalability and robustness to more complex domain shifts