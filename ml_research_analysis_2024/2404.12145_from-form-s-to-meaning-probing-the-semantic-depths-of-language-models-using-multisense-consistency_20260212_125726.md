---
ver: rpa2
title: 'From Form(s) to Meaning: Probing the Semantic Depths of Language Models Using
  Multisense Consistency'
arxiv_id: '2404.12145'
source_url: https://arxiv.org/abs/2404.12145
tags:
- language
- consistency
- meaning
- different
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) have
  a form-independent understanding of meaning by measuring their self-consistency
  across different expressions that convey the same meaning (called "senses"). The
  authors generate senses through paraphrasing and translating prompts into five languages
  (German, Italian, Dutch, Swedish) and then evaluate whether the model gives consistent
  responses to the original and generated senses.
---

# From Form(s) to Meaning: Probing the Semantic Depths of Language Models Using Multisense Consistency

## Quick Facts
- **arXiv ID**: 2404.12145
- **Source URL**: https://arxiv.org/abs/2404.12145
- **Reference count**: 40
- **Primary result**: LLMs show significant inconsistencies (50-80%) across different expressions of the same meaning, suggesting form-dependent rather than form-independent understanding

## Executive Summary
This paper investigates whether large language models have a form-independent understanding of meaning by measuring their self-consistency across different expressions (called "senses") that convey the same meaning. The authors generate senses through paraphrasing and translating prompts into five languages (German, Italian, Dutch, Swedish) and evaluate whether the model gives consistent responses to the original and generated senses. Across simple factual questions and NLU benchmarks (PAWS, XNLI, COPA, Belebele), the model shows significant inconsistencies even when accuracy is high, suggesting it does not have a shared meaning representation across forms. Further analysis shows these inconsistencies persist even with high-quality translations and occur in both task interpretation and execution, leading to the conclusion that current LLMs lack human-like form-independent understanding.

## Method Summary
The study uses GPT-3.5-turbo-0613 with temperature 0.2 to generate meaning-preserving senses by asking the model to paraphrase or translate task instructions and inputs. For each task, the model generates alternative senses in German, Italian, Dutch, and Swedish. The authors then compare responses across these senses using consistency scores, which measure agreement between model outputs. They calculate consistency separately for correct and incorrect responses, and report a same-sense baseline consistency to control for inherent model stochasticity. The experiments cover both simple factual questions and four NLU benchmarks, measuring whether the model maintains consistency across different forms expressing the same meaning.

## Key Results
- LLMs show significant consistency gaps (often 50-80%) across different senses expressing the same meaning, even when accuracy is high
- Consistency is consistently lower on tasks with lower accuracy, suggesting a trade-off between performance and consistency
- The model shows higher consistency on tasks where instructions matter more than inputs, indicating form-dependency in task interpretation
- Incorrect but consistent responses across languages provide strong evidence for shared meaning representation failures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-consistency across senses reveals form-dependent understanding in LLMs
- Mechanism: The model generates meaning-preserving senses itself, then we measure consistency between responses to original and generated senses
- Core assumption: If the model has form-independent understanding, it should recognize equivalence between self-generated senses and respond consistently
- Evidence anchors:
  - [abstract] "if a model generates consistent responses when prompted with these expressions, this would suggest it might be linking them to their common underlying meaning"
  - [section] "The main idea underpinning this paradigm is that if a model's understanding extends beyond form, it should produce consistent responses to different senses that express the same meaning"
  - [corpus] Weak - no direct corpus evidence for this mechanism
- Break condition: Model fails to generate adequate paraphrases/translations, or model generates inconsistent responses even to identical prompts

### Mechanism 2
- Claim: Comparing consistency to same-sense baseline isolates form-dependency from inherent model stochasticity
- Mechanism: Measure consistency between two runs on identical English prompts (id baseline) and compare to consistency across different senses
- Core assumption: Some inconsistency exists even for identical prompts due to non-zero sampling temperature; differences beyond this baseline indicate form-dependency
- Evidence anchors:
  - [section] "We report multisense consistency next to a same-sense baseline consistency. The baseline consistency is the consistency between two generations with the exact same English input (id)"
  - [section] "The baseline scores (id) show that the inconsistencies are not (primarily) caused by the model assigning equal probabilities to possible answers"
  - [corpus] Weak - no direct corpus evidence for this mechanism
- Break condition: Same-sense consistency approaches zero, making form-dependency impossible to isolate

### Mechanism 3
- Claim: Consistency conditioned on correctness distinguishes form-independent understanding from form-tied memorization
- Mechanism: Calculate consistency separately for correct and incorrect responses; incorrect but consistent responses provide stronger evidence for form-independent understanding
- Core assumption: Correct responses could be independently correct for each form; incorrect responses are less likely to be independently incorrect for equivalent forms
- Evidence anchors:
  - [section] "Correct and incorrect consistent examples provide different levels of evidence for consistency of meanings beyond form"
  - [section] "It is much less likely the case forincorrectly consistent examples, as it would require that the data the model was trained on contained the same error for both languages"
  - [corpus] Weak - no direct corpus evidence for this mechanism
- Break condition: Model consistently generates same incorrect responses across senses due to memorization rather than understanding

## Foundational Learning

- Concept: Frege's distinction between sense (Sinn) and reference (Bedeutung)
  - Why needed here: Forms the philosophical foundation for understanding how different expressions can have same meaning
  - Quick check question: If "morning star" and "evening star" refer to Venus, what does Frege's theory say about their sense versus reference?

- Concept: Self-consistency evaluation methodology
  - Why needed here: The core experimental method for measuring form-dependency in LLMs
  - Quick check question: How does comparing consistency to same-sense baseline help isolate form-dependency from model stochasticity?

- Concept: Translation and paraphrase quality evaluation
  - Why needed here: Ensures inconsistencies aren't simply due to poor sense generation by the model
- Quick check question: What metrics were used to evaluate translation quality in the experiments?

## Architecture Onboarding

- Component map: Model (GPT-3.5-turbo-0613) → Prompt generation (English instructions + task data) → Sense generation (model paraphrasing/translating) → Response collection → Consistency calculation
- Critical path: Sense generation quality → Consistency calculation → Analysis of form-dependency
- Design tradeoffs: Using model to generate senses ensures equivalence but introduces potential quality issues; using external translations would guarantee quality but lose self-consistency property
- Failure signatures: Low consistency across all senses suggests form-dependency; low consistency only for certain senses suggests sense generation quality issues; high same-sense consistency but low cross-sense consistency suggests form-dependency
- First 3 experiments:
  1. Run same-sense consistency baseline (two runs on identical English prompts)
  2. Generate senses through paraphrasing and measure cross-sense consistency
  3. Generate senses through translation and measure cross-sense consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the extent to which multilingual consistency generalizes across different LLM architectures beyond GPT-3.5?
- Basis in paper: [inferred] The paper primarily investigates GPT-3.5 and finds form-dependent understanding across senses. It does not explore consistency in other LLM architectures.
- Why unresolved: The study is limited to a single model, making it unclear if the observed form-dependency is a universal property of LLMs or specific to GPT-3.5's training and architecture.
- What evidence would resolve it: Systematic evaluation of multisense consistency across a diverse set of LLM architectures (e.g., BERT, RoBERTa, LLaMA, Claude) on the same benchmark tasks would reveal if form-dependency is a general phenomenon or architecture-specific.

### Open Question 2
- Question: How does the magnitude of consistency errors relate to the semantic similarity between different senses (e.g., paraphrases vs. translations)?
- Basis in paper: [explicit] The paper finds that consistency varies across tasks and senses, with paraphrases sometimes showing higher consistency than translations. However, the relationship between consistency and semantic similarity is not explicitly explored.
- Why unresolved: The paper does not directly quantify the semantic similarity between different senses or analyze how this similarity correlates with consistency scores.
- What evidence would resolve it: Quantifying semantic similarity between senses using metrics like semantic textual similarity (STS) and correlating these scores with consistency would reveal if more semantically similar senses lead to higher consistency.

### Open Question 3
- Question: Can targeted training methods improve LLM consistency across senses without sacrificing accuracy?
- Basis in paper: [inferred] The paper finds that consistency is lower on tasks with lower accuracy, suggesting a potential trade-off. It does not explore methods to improve consistency.
- Why unresolved: The study focuses on measuring consistency but does not investigate interventions to enhance it. It is unclear if consistency can be improved independently of accuracy.
- What evidence would resolve it: Developing and evaluating training methods that explicitly target consistency across senses (e.g., consistency-aware loss functions, adversarial training with paraphrased/translated examples) would reveal if consistency can be improved without sacrificing accuracy.

## Limitations

- The study relies on self-generated senses rather than independently verified paraphrases/translations, which introduces potential quality concerns
- The paper does not explore methods to improve consistency or investigate whether consistency can be enhanced independently of accuracy
- Limited evidence for the claim that incorrect consistent responses provide stronger evidence for form-dependency

## Confidence

- Core finding (form-dependent understanding): **Medium confidence** - substantial consistency gaps found but relies on self-generated senses
- Mechanism distinguishing form-dependency from stochasticity: **High confidence** - clear baseline comparisons provide strong control
- Incorrect consistent responses as evidence: **Low confidence** - theoretically sound but limited empirical support

## Next Checks

1. Validate translation/paraphrase quality independently using professional translations and established paraphrase datasets, then measure consistency gaps
2. Conduct ablation studies isolating the effects of instruction quality versus input quality on consistency scores
3. Quantify the likelihood of identical incorrect responses arising from memorization versus form-independent understanding using frequency analysis of error patterns