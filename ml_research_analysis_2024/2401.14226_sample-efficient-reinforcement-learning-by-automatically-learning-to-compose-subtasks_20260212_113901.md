---
ver: rpa2
title: Sample Efficient Reinforcement Learning by Automatically Learning to Compose
  Subtasks
arxiv_id: '2401.14226'
source_url: https://arxiv.org/abs/2401.14226
tags:
- reward
- learning
- subtask
- agent
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a two-level hierarchical reinforcement learning
  method, ALCS, that automatically structures reward functions to improve sample efficiency
  in sparse-reward environments. The method learns a high-level policy to select optimal
  subtasks based on the sequence of completed subtasks and a low-level policy to efficiently
  complete each subtask.
---

# Sample Efficient Reinforcement Learning by Automatically Learning to Compose Subtasks

## Quick Facts
- arXiv ID: 2401.14226
- Source URL: https://arxiv.org/abs/2401.14226
- Authors: Shuai Han; Mehdi Dastani; Shihan Wang
- Reference count: 40
- Primary result: ALCS significantly outperforms state-of-the-art baselines on sparse-reward environments by automatically learning to compose subtasks

## Executive Summary
This paper proposes ALCS, a two-level hierarchical reinforcement learning method that automatically structures reward functions to improve sample efficiency in sparse-reward environments. The approach learns a high-level policy to select optimal subtasks based on completed subtask sequences and a low-level policy to efficiently complete each subtask. ALCS avoids the computational intractability of exact automata learning by directly inducing high-level policies from environmental rewards. The method was evaluated on 8 environments from OfficeWorld and MineCraft domains, showing significant performance improvements as task difficulty increases.

## Method Summary
ALCS implements a two-level policy framework where the high-level policy πh selects subtasks from a vocabulary set based on the current state and sequence of completed subtasks, while the low-level policy πl learns to achieve the selected subtask. The method uses a labeling function L to detect when subtasks are completed and employs multiple experience generation for efficient low-level Q-function updates. High-level experiences are generated based on the actual completed subtask rather than the selected subtask, preventing learning of suboptimal policies. The approach provides interpretability through tree structures recording subtask sequences achieved during training.

## Key Results
- ALCS significantly outperforms state-of-the-art baselines (JIRP, DeepSynth, HRL, Interrupting options, HER, Q-learning) on sparse-reward environments
- Performance improvements increase with task difficulty across OfficeWorld and MineCraft domains
- The method demonstrates improved sample efficiency compared to non-hierarchical approaches
- ALCS provides interpretability through generated tree structures explaining agent behavior

## Why This Works (Mechanism)

### Mechanism 1
Automatically learning optimal subtask sequences avoids the computational intractability of exact automata learning. Instead of inferring an exact reward machine (NP-complete), the high-level policy directly learns to compose subtasks by maximizing environmental rewards, using the sequence of completed subtasks as input. This approach assumes the sequence of completed subtasks provides sufficient information for the high-level policy to approximate the reward structure without needing an exact automaton model.

### Mechanism 2
The two-level hierarchy with counterfactual experiences improves sample efficiency. The low-level policy learns to achieve individual subtasks efficiently using a dedicated reward function, while the high-level policy learns to compose subtasks. Multiple counterfactual experiences are generated for each transition to update the low-level Q-function, improving learning efficiency. This assumes generating multiple experiences per transition provides sufficient training signal for the low-level policy to learn all subtask-completion behaviors efficiently.

### Mechanism 3
The high-level policy's assumed choice mechanism prevents learning suboptimal policies. When training the high-level policy, experiences are generated based on the actual completed subtask rather than the selected subtask, ensuring that the importance of key subtasks is properly learned. This assumes the actual completed subtask is more important for learning optimal composition than the intended subtask selection, as it directly contributes to environmental rewards.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The entire reinforcement learning framework is built on MDP theory, where states, actions, rewards, and transitions define the learning problem.
  - Quick check question: What are the four components of an MDP, and how does the discount factor γ affect learning?

- Concept: Q-learning and function approximation
  - Why needed here: ALCS extends Q-learning to a two-level hierarchical structure, requiring understanding of how Q-functions are updated and how function approximation works for both high and low-level policies.
  - Quick check question: How does the Q-learning update rule work, and what changes when extending it to hierarchical policies with multiple Q-functions?

- Concept: Goal-conditioned reinforcement learning
  - Why needed here: The low-level policy in ALCS is essentially a goal-conditioned policy that learns to achieve specific subtasks, following similar principles to established GCRL methods.
  - Quick check question: How does goal-conditioned RL differ from standard RL, and what role does the goal/reward function play in learning?

## Architecture Onboarding

- Component map:
  - High-level policy πh: S × P* → P (selects next subtask based on current state and completed subtask sequence)
  - Low-level policy πl: S × P → A (selects action to achieve given subtask)
  - Labeling function L: S → P ∪ {∅} (detects when subtasks are completed)
  - Q-function Qh: S × P* × P → R (for high-level policy)
  - Q-function Ql: S × P × A → R (for low-level policy)
  - Environment MDP (S, A, T, R, γ)

- Critical path: State → πh selects subtask → πl selects action → Environment transition → L detects completed subtasks → Update Q-functions → Repeat

- Design tradeoffs:
  - Computational complexity vs. exact reward structure learning (avoids NP-complete automata learning)
  - Sample efficiency vs. policy expressiveness (two-level hierarchy improves efficiency but adds complexity)
  - Counterfactual experience generation vs. training signal quality (multiple experiences per transition improve efficiency but may introduce noise)

- Failure signatures:
  - Low-level policy fails to learn subtask completion: Check if counterfactual experience generation is working correctly and if labeling function L is accurate
  - High-level policy selects suboptimal subtask sequences: Verify that assumed choice mechanism is properly implemented and that completed subtask sequences are being tracked correctly
  - Both policies fail to learn: Check if exploration rate is appropriate and if Q-function updates are being performed correctly

- First 3 experiments:
  1. Implement ALCS on a simple environment (like Coffee) with only one subtask to verify basic functionality of the two-level hierarchy
  2. Test counterfactual experience generation by comparing learning curves with and without multiple experiences in a multi-subtask environment
  3. Verify assumed choice mechanism by creating a scenario where subtasks can be completed out of intended order and checking if high-level policy learns optimal composition

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of ALCS scale with increasing task complexity and state space size beyond the evaluated environments? The paper mentions that ALCS performance variance grows with increasing task complexity and that there is room to improve performance in complex tasks with large state spaces, but only evaluates on a limited set of environments.

### Open Question 2
What is the theoretical convergence guarantee for the ALCS algorithm? The paper does not provide theoretical analysis of convergence properties, focusing instead on empirical evaluation, leaving questions about its reliability and convergence behavior in different scenarios.

### Open Question 3
How does ALCS compare to other hierarchical RL methods when domain knowledge is not available? The paper evaluates ALCS against several baselines but does not explicitly compare its performance to other hierarchical RL methods when no domain knowledge is provided.

## Limitations
- Performance variance increases with task complexity and state space size, potentially limiting scalability
- Assumed choice mechanism may not generalize well to environments where multiple subtasks are frequently completed simultaneously
- Lack of empirical comparison with computational costs of automata-based approaches

## Confidence
- High confidence: The two-level hierarchical architecture and its basic implementation
- Medium confidence: Claims about computational complexity avoidance and sample efficiency improvements
- Low confidence: Claims about interpretability benefits and tree structure analysis

## Next Checks
1. Test ALCS in environments where multiple subtasks can be completed in a single transition to evaluate the robustness of the assumed choice mechanism
2. Measure actual computational runtime and memory usage of ALCS compared to automata-based approaches to verify claimed efficiency gains
3. Evaluate ALCS's performance when the labeling function L is noisy or incomplete to assess robustness to imperfect subtask detection