---
ver: rpa2
title: Self-Supervised Contrastive Learning for Long-term Forecasting
arxiv_id: '2402.02023'
source_url: https://arxiv.org/abs/2402.02023
tags:
- long-term
- forecasting
- time
- length
- window
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of long-term time-series forecasting,
  where existing methods struggle to capture long-term variations due to reliance
  on short sliding windows. The authors propose a novel approach combining contrastive
  learning with a decomposition architecture to focus on long-term dependencies.
---

# Self-Supervised Contrastive Learning for Long-term Forecasting

## Quick Facts
- arXiv ID: 2402.02023
- Source URL: https://arxiv.org/abs/2402.02023
- Authors: Junwoo Park; Daehoon Gwak; Jaegul Choo; Edward Choi
- Reference count: 40
- Key outcome: Novel contrastive learning method outperforms 14 baselines by up to 34% on long-term forecasting tasks

## Executive Summary
This paper addresses the challenge of long-term time-series forecasting, where existing methods struggle to capture long-term variations due to reliance on short sliding windows. The authors propose AutoCon, a novel approach combining contrastive learning with a decomposition architecture to focus on long-term dependencies. Their method leverages global autocorrelation to construct positive and negative pairs across distant windows in a self-supervised manner. Experiments on nine datasets demonstrate significant performance improvements, outperforming 14 baseline models by up to 34% in challenging long-term forecasting scenarios.

## Method Summary
The AutoCon method uses global autocorrelation to construct positive and negative pairs across distant windows in a self-supervised manner, enabling learning of long-term dependencies beyond conventional window lengths. The approach combines a decomposition architecture with separate short-term (linear) and long-term (encoder-decoder with TCN) branches. Window-unit normalization and denormalization methods help alleviate distribution shift problems in non-stationary time series. The model is optimized using a combination of MSE loss and the proposed AutoCon contrastive loss.

## Key Results
- Outperforms 14 baseline models by up to 34% on challenging long-term forecasting scenarios
- Effective across nine datasets from six diverse domains including ETT, Electricity, Traffic, Weather, Exchange, and ILI
- Successfully captures long-term variations that exist beyond conventional window lengths
- Demonstrates superiority particularly in scenarios with complex long-term dependencies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Global autocorrelation-based contrastive loss enables learning long-term dependencies beyond the sliding window
- Mechanism: The method constructs positive and negative pairs across distant windows using global autocorrelation (RSS) calculated over the entire time series
- Core assumption: The global autocorrelation RSS(|t1 - t2|) accurately reflects the relationship strength between windows Wt1 and Wt2
- Evidence anchors:
  - [abstract] "our contrastive loss incorporates global autocorrelation held in the whole time series"
  - [section] "We first identify these long-term variations using autocorrelation"
- Break condition: If global autocorrelation doesn't capture true long-term dependencies (e.g., non-linear patterns)

### Mechanism 2
- Claim: Decomposition architecture with separate short-term and long-term branches enables effective handling of both local and global temporal patterns
- Mechanism: Architecture separates processing into a linear-based short-term branch for temporal locality and a deep encoder-decoder long-term branch for temporal globality
- Core assumption: Short-term variations exhibit temporal locality while long-term variations require global information processing
- Evidence anchors:
  - [section] "Therefore, we redesign a model architecture with well-defined existing blocks to respect temporal locality for short-term and globality for long-term forecasting"
  - [section] "We observe that short-period variations often repeat multiple times within the input sequence"
- Break condition: If temporal patterns don't naturally separate into local and global components

### Mechanism 3
- Claim: Window-unit normalization and denormalization methods help alleviate distribution shift problems in non-stationary time series
- Mechanism: Normalizing each window independently (Xnorm = X - X̄) and denormalizing predictions accordingly helps handle non-stationarity
- Core assumption: Non-stationarity in real-world time series causes distribution shifts that can be effectively handled by window-level normalization
- Evidence anchors:
  - [section] "These simple methods help to effectively alleviate the distribution shift problem"
  - [section] "Since these normalization methods are essential for mitigating distribution shift problems"
- Break condition: If window-level normalization introduces more problems than it solves

## Foundational Learning

- Concept: Autocorrelation function and its interpretation
  - Why needed here: The method fundamentally relies on calculating and interpreting global autocorrelation to construct positive/negative pairs
  - Quick check question: What does a high autocorrelation value at lag h indicate about the relationship between observations separated by h time steps?

- Concept: Contrastive learning principles and pair construction strategies
  - Why needed here: The AutoCon loss is a contrastive learning objective that requires understanding how positive and negative pairs are constructed and used
  - Quick check question: How does the relative selection strategy in AutoCon differ from standard contrastive learning approaches?

- Concept: Time series decomposition and its components
  - Why needed here: The architecture is based on decomposition principles, separating trend (long-term) and seasonal (short-term) components
  - Quick check question: Why might treating long-term variations as simple non-periodic trends be insufficient for accurate long-term forecasting?

## Architecture Onboarding

- Component map: Input normalization → Short-term branch (linear) → Long-term branch (TCN encoder + Multi-scale MA decoder) → Concatenation → Output denormalization
- Critical path: Input → Normalization → Short-term and Long-term branches → Concatenation → Denormalization → Output
- Design tradeoffs:
  - Linear vs deep model for short-term branch: Simplicity and efficiency vs capacity
  - Single vs multiple layers in long-term branch: Complexity vs overfitting risk
  - Weight λ for combining MSE and AutoCon: Balancing forecasting accuracy vs representation learning
- Failure signatures:
  - Poor long-term performance: Likely issues with AutoCon configuration or long-term branch capacity
  - Poor short-term performance: Likely issues with short-term branch or normalization
  - Unstable training: Likely issues with λ weighting or autocorrelation calculation
- First 3 experiments:
  1. Test with only MSE loss (no AutoCon) to establish baseline performance
  2. Test with only AutoCon loss (no MSE) to verify representation learning capability
  3. Test with varying λ values to find optimal balance between forecasting and representation objectives

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method handle multivariate time series with complex inter-channel dependencies that may not be well-captured by the channel-independence approach?
- Basis in paper: [explicit] The paper mentions that the method is applicable to multivariate forecasting by calculating autocorrelation on a per-channel basis and using a channel-independence approach
- Why unresolved: The paper does not provide empirical comparisons between the channel-independence approach and more complex methods that capture inter-channel dependencies
- What evidence would resolve it: Experiments comparing the proposed method with and without channel-mixing on datasets with known complex inter-channel dependencies

### Open Question 2
- Question: What is the impact of using higher-order correlations or non-linear dependencies instead of linear autocorrelation in the contrastive loss?
- Basis in paper: [inferred] The discussion section mentions that autocorrelation's linearity assumption limits its effectiveness in dealing with non-linear patterns
- Why unresolved: The paper does not explore or implement alternative correlation measures that could capture non-linear dependencies
- What evidence would resolve it: Implementing the contrastive loss with alternative correlation measures and comparing performance on datasets with known non-linear dependencies

### Open Question 3
- Question: How does the proposed method's performance scale with increasing dataset size and complexity?
- Basis in paper: [inferred] The paper discusses the computational efficiency of the method compared to other deep models but does not provide a detailed analysis of scaling behavior
- Why unresolved: While the paper includes ablation studies and comparisons with baselines, it does not systematically vary dataset size or complexity
- What evidence would resolve it: Experiments varying dataset size and complexity while measuring performance and computational cost

## Limitations

- The decomposition architecture assumes short-term and long-term variations can be effectively separated, though this separation is not rigorously validated
- The effectiveness of window-unit normalization versus alternative approaches remains untested
- Computational cost of calculating global autocorrelation for pair construction is not discussed, which could impact scalability

## Confidence

- High confidence: The empirical results showing significant improvements over baselines on nine diverse datasets
- Medium confidence: The mechanism by which contrastive learning captures long-term dependencies beyond window lengths
- Medium confidence: The decomposition architecture's ability to handle both short-term and long-term variations effectively

## Next Checks

1. Conduct an ablation study removing the AutoCon loss to quantify its exact contribution versus the decomposition architecture alone
2. Test the model's performance on non-stationary time series with varying degrees of stationarity to validate the window normalization approach
3. Analyze the learned representations by visualizing the long-term branch embeddings to verify they capture meaningful long-term patterns beyond simple autocorrelation structures