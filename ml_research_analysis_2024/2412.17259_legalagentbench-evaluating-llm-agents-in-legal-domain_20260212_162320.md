---
ver: rpa2
title: 'LegalAgentBench: Evaluating LLM Agents in Legal Domain'
arxiv_id: '2412.17259'
source_url: https://arxiv.org/abs/2412.17259
tags:
- legal
- case
- company
- tools
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LegalAgentBench, a comprehensive benchmark
  for evaluating large language model (LLM) agents in the Chinese legal domain. The
  benchmark includes 17 real-world corpora and 37 specialized tools, along with 300
  carefully annotated tasks spanning various difficulty levels and types, including
  multi-hop reasoning and writing.
---

# LegalAgentBench: Evaluating LLM Agents in Legal Domain

## Quick Facts
- arXiv ID: 2412.17259
- Source URL: https://arxiv.org/abs/2412.17259
- Reference count: 40
- Primary result: Introduces LegalAgentBench benchmark with 300 annotated tasks for evaluating LLM agents in Chinese legal domain

## Executive Summary
This paper introduces LegalAgentBench, a comprehensive benchmark for evaluating large language model (LLM) agents in the Chinese legal domain. The benchmark includes 17 real-world corpora and 37 specialized tools, along with 300 carefully annotated tasks spanning various difficulty levels and types, including multi-hop reasoning and writing. Beyond final success rates, LegalAgentBench incorporates keyword analysis of intermediate steps to calculate progress rates, enabling more fine-grained evaluation. Experiments with eight popular LLMs showed that models with stronger tool usage and logical reasoning capabilities, such as GPT-4o and GLM-4-Plus, achieved higher performance. The benchmark effectively differentiates LLM capabilities and highlights areas for improvement in legal reasoning and tool utilization.

## Method Summary
LegalAgentBench employs a hierarchical task construction framework based on tool dependencies and a planning tree structure. The benchmark uses 17 real-world corpora (14 tabular databases and 3 document collections) and 37 specialized tools for interacting with these corpora. Evaluation metrics include success rate, process rate (calculated using keyword analysis of intermediate steps), BERTScore, and token consumption. The benchmark tests eight LLMs (GLM-4, GLM-4-Plus, LLaMA3.1-8B, Qwen-max, Claude-sonnet, GPT-3.5, GPT-4o-mini, GPT-4o) using three methods: Plan-and-Solve, Plan-and-Execute, and ReAct.

## Key Results
- GPT-4o and GLM-4-Plus achieved the highest success rates due to superior tool usage and logical reasoning capabilities
- The keyword-based evaluation effectively captured partial progress and identified reasoning bottlenecks
- Multi-hop reasoning tasks showed higher difficulty, with ReAct performing poorly on writing tasks compared to other methods
- The benchmark successfully differentiated LLM capabilities and highlighted specific areas for improvement in legal reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hierarchical task construction framework effectively captures real-world legal complexity
- Mechanism: By building a planning tree based on tool dependencies and extracting solution paths layer by layer, the framework ensures coverage of different difficulty levels while maintaining task diversity. The parallel solution paths allow for both sequential and concurrent tool usage patterns.
- Core assumption: Real-world legal tasks can be decomposed into hierarchical dependencies between tools and corpora
- Evidence anchors:
  - [abstract]: "designed a scalable task construction framework aimed at comprehensively covering various task types and difficulty levels"
  - [section]: "We construct a planning tree based on the dependencies between the corpus and tools, and select tasks through hierarchical sampling and a maximum coverage strategy"
  - [corpus]: Corpus signals show related work on hierarchical execution and planning frameworks, supporting this approach
- Break condition: If real-world legal tasks cannot be adequately represented by the available tools and corpora, or if the dependency structure is too complex to model effectively

### Mechanism 2
- Claim: Keyword-based evaluation metrics provide more granular assessment than final success rates alone
- Mechanism: By extracting keywords from both final answers (key_answer) and intermediate steps (key_middle), the evaluation captures partial progress and identifies where agents struggle in the reasoning process
- Core assumption: Legal task completion can be meaningfully decomposed into measurable intermediate steps
- Evidence anchors:
  - [abstract]: "incorporates keyword analysis during intermediate processes to calculate progress rates, enabling more fine-grained evaluation"
  - [section]: "we provide keywords not only for the final answers but also for the intermediate solution steps. Using these keywords, we calculate the progress rate"
  - [corpus]: Related work on evaluation metrics for tool utilization supports the need for fine-grained assessment
- Break condition: If keyword extraction fails to capture the essential elements of legal reasoning, or if intermediate steps don't meaningfully contribute to understanding agent capabilities

### Mechanism 3
- Claim: Diverse task types (multi-hop reasoning, writing) effectively differentiate LLM capabilities
- Mechanism: By requiring different reasoning patterns - sequential tool usage for multi-hop tasks and parallel processing for writing tasks - the benchmark exposes limitations in agent planning and execution strategies
- Core assumption: Different legal tasks require fundamentally different cognitive approaches that can be evaluated separately
- Evidence anchors:
  - [abstract]: "These tasks span various types, including multi-hop reasoning and writing, and range across different difficulty levels"
  - [section]: "For the Writing task, we observed that ReAct performed poorly compared to other methods"
  - [corpus]: Corpus signals mention benchmarks for different reasoning types, supporting the approach
- Break condition: If the task types don't sufficiently differentiate between agent capabilities, or if the writing tasks don't require genuinely different reasoning patterns

## Foundational Learning

- Concept: Tool selection and chaining strategies
  - Why needed here: LLM agents must decide which tools to use and in what order to solve complex legal tasks
  - Quick check question: Can you explain the difference between Plan-and-Solve and ReAct approaches in terms of tool usage patterns?

- Concept: Legal domain knowledge representation
  - Why needed here: Agents need to understand legal concepts and terminology to select appropriate tools and interpret results
  - Quick check question: How would you represent the relationship between a company's legal representative and their associated court cases in a way an LLM agent could reason about?

- Concept: Evaluation metric design for legal tasks
  - Why needed here: Standard metrics may not capture the nuances of legal reasoning and task completion
  - Quick check question: Why might success rate alone be insufficient for evaluating legal agent performance, and how does progress rate address this limitation?

## Architecture Onboarding

- Component map: LegalAgentBench consists of 17 corpora (14 tabular databases + 3 document collections), 37 specialized tools for interacting with these corpora, and 300 annotated tasks spanning 6 difficulty levels. The system uses LLM agents to coordinate tool usage, with evaluation based on keyword matching for success and progress rates.
- Critical path: Task → LLM agent selects tools → Tool execution returns observation → State update → Next action or answer generation → Keyword evaluation
- Design tradeoffs: The benchmark trades off between task diversity and complexity, using keyword-based evaluation that may miss semantic nuances but provides consistent, scalable assessment. The planning tree approach enables systematic task generation but may not capture all real-world legal scenarios.
- Failure signatures: Poor tool selection (planning errors), incorrect argument formatting (argument errors), context overflow (exceeding length limitations), or repetitive problem-solving attempts (getting stuck in loops)
- First 3 experiments:
  1. Run a simple 1-hop task with GPT-4o using ReAct method to verify basic tool usage and keyword extraction works
  2. Test a multi-hop task with GLM-4-Plus using Plan-and-Execute to observe planning behavior and intermediate step handling
  3. Evaluate a writing task with Qwen-max using Plan-and-Solve to assess parallel tool usage and answer synthesis capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM agents be improved to better recognize when to consult external legal knowledge bases versus relying on their internal knowledge?
- Basis in paper: Explicit - The paper discusses that LLMs often fail to recognize the necessity of consulting relevant legal knowledge when handling legal issues.
- Why unresolved: The paper identifies this as a challenge but does not propose specific solutions for when and how to integrate external legal knowledge.
- What evidence would resolve it: Experimental results comparing LLM performance with and without explicit prompts to consult external legal databases, or a proposed framework for determining when external knowledge is needed.

### Open Question 2
- Question: What are the specific limitations of current LLMs in interpreting and applying legal articles and case law, and how can these be addressed?
- Basis in paper: Explicit - The paper mentions that LLMs struggle to accurately interpret the scope and logic of legal articles, and have difficulties understanding judicial interpretations and practical applications.
- Why unresolved: While the paper identifies these limitations, it does not provide a detailed analysis of the specific types of misinterpretations or propose methods to improve legal text comprehension.
- What evidence would resolve it: A comprehensive error analysis of LLM outputs when interpreting legal texts, or a new benchmark specifically designed to test legal text comprehension and application.

### Open Question 3
- Question: How can the task construction framework be extended to support multiple legal systems and languages beyond Chinese?
- Basis in paper: Explicit - The paper states that future work aims to expand LegalAgentBench to support additional languages and legal systems.
- Why unresolved: The paper does not provide details on how the current framework could be adapted for different legal systems or languages, or what challenges might arise in doing so.
- What evidence would resolve it: A pilot study extending the benchmark to another legal system or language, or a detailed methodology for adapting the framework to different legal contexts.

## Limitations
- The benchmark's effectiveness is constrained by its focus on the Chinese legal domain, limiting generalizability to other jurisdictions
- The keyword-based evaluation may not fully capture semantic nuances in legal reasoning
- The task construction framework relies on predefined tool dependencies, which may not represent all real-world legal complexities
- The use of 8 specific LLMs provides a limited view of the broader LLM landscape

## Confidence

- **High Confidence**: The hierarchical task construction framework and its ability to differentiate LLM capabilities across various task types and difficulty levels
- **Medium Confidence**: The effectiveness of keyword-based evaluation metrics in providing granular assessment beyond final success rates
- **Medium Confidence**: The benchmark's value in advancing LLM applications in the legal domain and its potential for extension to other languages and legal systems

## Next Checks

1. **Cross-Jurisdictional Validation**: Test the benchmark with legal tasks from other jurisdictions (e.g., US, EU) to assess generalizability and identify domain-specific limitations
2. **Semantic Evaluation Comparison**: Compare keyword-based evaluation results with human expert assessments to quantify the impact of semantic nuances missed by keyword extraction
3. **Expanded LLM Coverage**: Evaluate additional LLM families (including open-source models) to determine if the benchmark consistently differentiates capabilities across a broader range of models