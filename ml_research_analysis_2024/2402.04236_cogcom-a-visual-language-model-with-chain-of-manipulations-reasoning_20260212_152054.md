---
ver: rpa2
title: 'CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning'
arxiv_id: '2402.04236'
source_url: https://arxiv.org/abs/2402.04236
tags:
- visual
- reasoning
- image
- latexit
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CogCoM, a visual language model with Chain
  of Manipulations reasoning, addressing the problem of VLMs ignoring essential visual
  reasoning due to conclusive alignment training. The core method involves defining
  6 atomic manipulations (OCR, Grounding, CropZoomIn, Counting, Calculate, Line) and
  training a VLM to perform step-by-step reasoning with evidence using these manipulations.
---

# CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning

## Quick Facts
- arXiv ID: 2402.04236
- Source URL: https://arxiv.org/abs/2402.04236
- Reference count: 25
- Primary result: Achieves SOTA performance across 9 benchmarks from 4 categories using Chain of Manipulations reasoning

## Executive Summary
CogCoM addresses a critical limitation in visual language models: their tendency to skip essential visual reasoning steps due to conclusive alignment training. The model introduces a Chain of Manipulations (CoM) approach that forces step-by-step reasoning with evidence using 6 atomic manipulations (OCR, Grounding, CropZoomIn, Counting, Calculate, Line). Through an automated data generation pipeline using LLMs and VFMs, CogCoM creates 70K training samples that teach VLMs to perform detailed visual reasoning while maintaining interpretability. The 17B parameter model achieves state-of-the-art performance across diverse benchmarks including detailed visual question answering, visual grounding, and hallucination detection.

## Method Summary
CogCoM implements Chain of Manipulations reasoning by defining 6 atomic visual manipulations and training a VLM to execute these steps sequentially with evidence. The core innovation is an automated data generation pipeline that uses GPT-4 to generate solving steps with manipulation placeholders, which are then filled by visual tools (GroundingDINO for object detection, PaddleOCR for text recognition). A DFS traversal algorithm finds positive paths that terminate at correct answers, creating high-quality training samples. The model architecture uses memory-based multi-turn processing to handle sequential reasoning over multiple image transformations, with accumulated KV memories enabling context maintenance across reasoning turns. Two-stage training (pre-training followed by alignment) produces a model that achieves SOTA performance while preserving interpretability through explicit reasoning chains.

## Key Results
- Achieves SOTA performance across 9 benchmarks from 4 categories (TextVQA, ST-VQA, TallyVQA, GQA, RefCOCO, RefCOCO+, RefCOCOg, POPE, MM-Vet)
- Improves detailed visual question answering accuracy by up to 9.0 points compared to baseline
- Maintains interpretability through explicit Chain of Manipulations reasoning chains
- Demonstrates effectiveness on hallucination detection tasks while preserving faithful responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLMs fail on detailed visual tasks because conclusive alignment training causes them to skip intermediate visual reasoning steps.
- Mechanism: Chain of Manipulations (CoM) forces the model to explicitly perform step-by-step reasoning with evidence, mirroring human problem-solving behavior.
- Core assumption: Models have latent manipulation capabilities from pre-training that can be elicited through structured training.
- Evidence anchors:
  - [abstract] "such training of conclusive alignment leads models to ignore essential visual reasoning, further resulting in failures in meticulous visual problems and unfaithful responses."
  - [section] "most of the actions can be mapped to 6 fundamental manipulations on images: OCR, Grounding, CropZoomIn, Counting, Calculate, and Line."
- Break condition: If the model cannot learn to perform the atomic manipulations reliably, or if the data generation pipeline fails to produce quality training samples.

### Mechanism 2
- Claim: Automated data generation using LLMs and VFMs can create sufficient quality training samples for CoM reasoning.
- Mechanism: LLM generates solving steps with manipulation placeholders, VFMs execute manipulations to fill in results, DFS traversal finds positive paths terminating at correct answers.
- Core assumption: GPT-4 and visual tools are reliable enough to generate correct reasoning chains.
- Evidence anchors:
  - [section] "We employ two visual fundamental models, GroundingDINO (Liu et al., 2023c) and PaddleOCR (Du et al., 2020), and develop the implementations of these manipulations."
  - [section] "Based on this recursively searching method, most of the generated positive paths are guaranteed to be error-free."
  - [corpus] Moderate - success rate of 0.3555 mentioned for GPT-4 achieving positive paths.
- Break condition: If visual tools produce too many errors, or if LLM-generated steps are too inconsistent to be useful.

### Mechanism 3
- Claim: Multi-turn multi-image architecture with memory allows VLMs to handle sequential reasoning over multiple image transformations.
- Mechanism: Accumulated KV memories across turns enable the model to maintain context while processing cropped/zoomed images.
- Core assumption: Standard VLM architecture can be extended with memory without breaking existing capabilities.
- Evidence anchors:
  - [section] "we keep the accumulated KV memories of each layer in the LLM backbone throughout these turns."
  - [section] "Based on this general architecture, we develop a memory-based multi-turn multi-image VLM architecture."
- Break condition: If memory accumulation causes performance degradation or OOM issues during inference.

## Foundational Learning

- Concept: Visual reasoning through sequential manipulations
  - Why needed here: The core problem is that VLMs skip intermediate reasoning steps, so teaching them to reason step-by-step is fundamental.
  - Quick check question: Can you describe how a human would solve "What time is shown on this clock?" vs how a standard VLM would approach it?

- Concept: Data generation with LLM + VFM pipeline
  - Why needed here: Manual annotation of 70K+ samples would be impractical, so automated generation is essential.
  - Quick check question: What are the three main components of the data generation pipeline and what does each produce?

- Concept: Memory-based multi-turn processing
  - Why needed here: CoM reasoning often requires multiple image transformations that need to be processed sequentially.
  - Quick check question: How does the model maintain context when processing multiple image transformations in sequence?

## Architecture Onboarding

- Component map:
  Visual Encoder (EVA2-CLIP-E) → MLP Adapter → LLM Backbone (Vicuna-7B) → Visual Expert Module → KV Memory stores accumulated states across reasoning turns → Manipulation execution layer (OCR, Grounding, CropZoomIn, etc.)

- Critical path:
  1. Input image + question processed by visual encoder
  2. Visual expert module extracts relevant features
  3. LLM generates reasoning steps with manipulation placeholders
  4. Visual tools execute manipulations and return results
  5. Model updates memory and continues reasoning
  6. Final answer generated based on accumulated evidence

- Design tradeoffs:
  - Memory-based vs stateless: Memory enables better reasoning but increases complexity
  - Predefined vs dynamic manipulations: Predefined ensures coverage but dynamic allows flexibility
  - Automated vs manual data: Automated scales but may have quality issues

- Failure signatures:
  - Model outputs incorrect manipulation results → Check visual tool accuracy
  - Model gets stuck in reasoning loops → Check memory management and turn limits
  - Model ignores manipulations and answers directly → Check training data balance

- First 3 experiments:
  1. Test basic manipulation execution: Give simple questions requiring one manipulation (e.g., "Read the text on this sign")
  2. Test multi-step reasoning: Give questions requiring 2-3 manipulations in sequence
  3. Test memory retention: Give questions requiring multiple image transformations and check if context is maintained

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The automated data generation pipeline has a 35.55% success rate for achieving positive paths, meaning 64.45% of generated attempts fail to produce valid reasoning chains.
- Visual tools (GroundingDINO and PaddleOCR) have inherent accuracy limitations that propagate through the pipeline, with GroundingDINO's coarse bounding boxes and PaddleOCR's struggles with slanted text or complex layouts potentially impacting training data quality.
- The generalizability of the 6 atomic manipulations beyond the specific datasets used is not extensively validated, as the paper demonstrates effectiveness on targeted benchmarks but doesn't validate broader application scenarios.

## Confidence
- High Confidence: The architectural design of CogCoM (memory-based multi-turn processing with 6 atomic manipulations) is well-specified and the empirical results showing state-of-the-art performance across 9 benchmarks are clearly documented.
- Medium Confidence: The core hypothesis that conclusive alignment training causes VLMs to skip essential visual reasoning steps is plausible but relies on indirect evidence.
- Low Confidence: The generalizability of the 6 atomic manipulations beyond the specific datasets used, as the paper demonstrates effectiveness on targeted benchmarks but doesn't extensively validate broader applications.

## Next Checks
1. **Visual Tool Reliability Audit:** Conduct an independent evaluation of GroundingDINO and PaddleOCR accuracy on the specific types of images used in CogCoM's training data, measuring bounding box precision and OCR character-level accuracy to quantify the error propagation in the data generation pipeline.

2. **Ablation Study on Manipulation Types:** Remove each of the 6 atomic manipulations from the training pipeline one at a time and retrain CogCoM to measure the contribution of each manipulation type to overall performance, particularly on tasks that theoretically require that specific manipulation.

3. **Human Evaluation of Generated Chains:** Have human annotators evaluate a random sample of GPT-4 generated reasoning chains for logical consistency, correctness, and whether they actually require the claimed manipulations, to verify the quality of the automated data generation process beyond the keypoint-aware metric.