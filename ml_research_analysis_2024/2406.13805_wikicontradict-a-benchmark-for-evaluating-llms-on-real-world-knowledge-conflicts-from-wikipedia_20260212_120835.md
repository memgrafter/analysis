---
ver: rpa2
title: 'WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts
  from Wikipedia'
arxiv_id: '2406.13805'
source_url: https://arxiv.org/abs/2406.13805
tags:
- passage
- correct
- answers
- information
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WikiContradict, a benchmark for evaluating
  large language models (LLMs) on real-world knowledge conflicts from Wikipedia. The
  authors collect 253 human-annotated instances covering different types of contradictions
  identified by Wikipedia editors.
---

# WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts from Wikipedia

## Quick Facts
- arXiv ID: 2406.13805
- Source URL: https://arxiv.org/abs/2406.13805
- Reference count: 40
- Key outcome: WikiContradict benchmark exposes LLMs' struggles with real-world knowledge conflicts from Wikipedia, with all models failing to accurately reflect conflicting information, especially for implicit conflicts requiring reasoning.

## Executive Summary
This paper introduces WikiContradict, a benchmark consisting of 253 human-annotated instances designed to assess LLM performance when confronted with contradictory passages from Wikipedia. The benchmark evaluates how well LLMs handle real-world knowledge conflicts under different question-answering scenarios, including retrieval-augmented generation with single or contradictory passages. Human evaluation on a subset of instances reveals that all models struggle to generate answers that accurately reflect the conflicting nature of the context, particularly for implicit conflicts requiring reasoning. To facilitate scalable evaluation, the authors also introduce WikiContradictEval, an automated model using a strong open-source language model that estimates LLM performance with an F-score of 0.8.

## Method Summary
The authors create WikiContradict by extracting passages from Wikipedia articles flagged with inconsistency tags, then manually decontextualizing and annotating them to create question-answer pairs that test how LLMs handle contradictory information. They evaluate a diverse range of both closed and open-source LLMs using five prompt templates under different QA scenarios: internal knowledge, single passage RAG, and RAG with two contradictory passages. The evaluation employs both human judgment and an automated judge model (WikiContradictEval) based on Llama-3-70b-instruct with few-shot in-context learning. The automated judge is trained to classify responses based on correctness criteria aligned with human evaluation, achieving F-score of 0.8.

## Key Results
- All evaluated LLMs, including GPT-4, struggle to generate answers that accurately reflect the conflicting nature of contradictory Wikipedia passages
- Explicit instructions to consider conflicting information significantly improve performance, especially for open-source models like Llama-3-70b-instruct
- The automated judge model (WikiContradictEval) achieves F-score of 0.8, enabling scalable evaluation while maintaining reasonable alignment with human judgment
- Performance degrades significantly on implicit conflicts requiring reasoning compared to explicit contradictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WikiContradict leverages human-annotated Wikipedia contradictions to expose LLMs to realistic inter-context conflicts.
- Mechanism: By extracting real-world contradictions from Wikipedia articles flagged by editors, the benchmark creates QA instances that require models to handle conflicting information from equally trustworthy sources.
- Core assumption: Wikipedia is a high-quality pre-training resource for most LLMs, making contradictions in it particularly revealing of model limitations.
- Evidence anchors:
  - [abstract] "We conduct a comprehensive evaluation of LLM-generated answers to questions that have varying answers based on contradictory passages from Wikipedia, a dataset widely regarded as a high-quality pre-training resource for most LLMs."
  - [section] "Specifically, we introduce WikiContradict, a benchmark consisting of 253 high-quality, human-annotated instances designed to assess LLM performance when augmented with retrieved passages containing real-world knowledge conflicts."
- Break condition: If LLMs were trained on a different corpus where contradictions are less prevalent or handled differently, the benchmark's effectiveness would diminish.

### Mechanism 2
- Claim: Different prompt templates reveal varying LLM capabilities in handling knowledge conflicts.
- Mechanism: By using prompts that provide single passages, contradictory passages, or explicit instructions to consider contradictions, the benchmark tests how LLMs respond under different conditions.
- Core assumption: LLMs can be influenced by prompt design to either ignore or acknowledge conflicting information.
- Evidence anchors:
  - [abstract] "We benchmark a diverse range of both closed and open-source LLMs under different QA scenarios, including RAG with a single passage, and RAG with 2 contradictory passages."
  - [section] "We observe that when instructing LLMs to generate answers to a given question based on the given context consisting of two contradicted passages, all models, including GPT-4, struggle to generate answers that accurately reflect the conflicting nature of the context."
- Break condition: If LLMs develop inherent conflict-detection mechanisms that override prompt instructions, the prompt-based evaluation would become less effective.

### Mechanism 3
- Claim: Automated evaluation using judge LLMs can approximate human assessment of model responses.
- Mechanism: A strong open-source language model (Llama-3-70b-instruct) is taught via few-shot in-context learning to judge responses based on criteria aligned with human evaluation.
- Core assumption: The judge LLM can be reliably taught to apply consistent evaluation criteria that match human judgment.
- Evidence anchors:
  - [abstract] "we also introduce an automated model that estimates LLM performance using a strong open-source language model, achieving an F-score of 0.8."
  - [section] "Among the four judge LLMs evaluated, the top-performing model, GPT-4, achieves an F-score of 82.5 in accurately identifying correct responses, with a precision score of 73.4 and a recall score of 94.0."
- Break condition: If the judge LLM develops systematic biases or if the evaluation criteria become too complex for in-context learning to capture, the automated evaluation would lose alignment with human judgment.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: WikiContradict evaluates how LLMs handle conflicting information retrieved from external sources, which is the core challenge RAG systems face.
  - Quick check question: What is the primary purpose of RAG in the context of LLMs, and how does it relate to the problem of knowledge conflicts?

- Concept: Knowledge Conflicts in LLMs
  - Why needed here: Understanding the different types of knowledge conflicts (context-memory and inter-context) is crucial for interpreting WikiContradict's evaluation of real-world inter-context conflicts.
  - Quick check question: What are the three categories of knowledge conflicts identified in the literature, and which one does WikiContradict focus on?

- Concept: Human Evaluation in NLP Benchmarks
  - Why needed here: WikiContradict relies on human evaluation to establish ground truth for response quality, which is essential for both the benchmark creation and the development of automated evaluation methods.
  - Quick check question: Why is human evaluation necessary for creating a benchmark like WikiContradict, and what are its limitations?

## Architecture Onboarding

- Component map:
  Wikipedia inconsistency tags -> Passage extraction and decontextualization -> Question/answer creation -> LLM response generation -> Response evaluation (human/automated)

- Critical path:
  Wikipedia tag identification -> Passage extraction and decontextualization -> Question/answer creation -> LLM response generation -> Response evaluation (human/automated)

- Design tradeoffs:
  - Manual annotation vs. scalability: High-quality human annotation ensures benchmark reliability but limits dataset size and increases creation cost.
  - Real-world contradictions vs. controlled scenarios: Using actual Wikipedia contradictions provides ecological validity but introduces complexity that may be harder to control than synthetic data.
  - Automated vs. human evaluation: Automated evaluation enables scalability but may miss nuanced aspects of response quality that humans can detect.

- Failure signatures:
  - Low inter-annotator agreement: Indicates ambiguity in contradiction identification or annotation guidelines need clarification.
  - Judge LLM consistently disagrees with human evaluation: Suggests the automated evaluation criteria are misaligned with human judgment or the judge LLM has systematic biases.
  - LLMs perform well on WikiContradict but poorly on real-world applications: Indicates the benchmark may not fully capture the complexity of real-world knowledge conflicts.

- First 3 experiments:
  1. Run WikiContradict with a single prompt template (e.g., RAG with two contradictory passages) on a small subset of instances to verify the evaluation pipeline works as expected.
  2. Compare human evaluation results across different annotators on the same subset to establish inter-annotator agreement and identify potential annotation issues.
  3. Test the automated judge LLM on a small set of responses with known human evaluation results to calibrate its performance and identify any systematic biases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs handle implicit conflicts requiring reasoning when augmented with contradictory passages from Wikipedia?
- Basis in paper: [explicit] The paper states that all models struggle to generate answers that accurately reflect the conflicting nature of the context, especially for implicit conflicts requiring reasoning.
- Why unresolved: The paper demonstrates the difficulty but does not explore underlying mechanisms or propose solutions for improving reasoning on implicit conflicts.
- What evidence would resolve it: Comparative analysis of LLM performance on implicit vs. explicit conflicts, along with proposed methods to improve reasoning capabilities.

### Open Question 2
- Question: Can LLMs be effectively trained or prompted to recognize and handle inter-context conflicts without human intervention?
- Basis in paper: [inferred] The paper introduces WikiContradictEval, an automated model using Llama-3-70b-instruct to judge LLM responses, achieving an F-score of 0.8, suggesting potential for automated conflict detection.
- Why unresolved: The paper does not explore the scalability or generalizability of automated conflict detection beyond the specific WikiContradict dataset.
- What evidence would resolve it: Evaluation of WikiContradictEval on diverse datasets and real-world applications to assess its effectiveness and limitations.

### Open Question 3
- Question: How do different prompting strategies impact LLM performance on real-world knowledge conflicts?
- Basis in paper: [explicit] The paper tests five prompt templates, finding that explicit instructions to consider conflicting information improve performance, particularly for Llama-3-70b-instruct.
- Why unresolved: The paper does not explore the full range of possible prompting strategies or their impact on different types of conflicts.
- What evidence would resolve it: Systematic evaluation of diverse prompting techniques across various conflict types and LLM architectures.

## Limitations
- Automated evaluation achieves F-score of 0.8 but shows lower precision (73.4) compared to recall (94.0), suggesting potential systematic biases
- Dataset size of 253 instances, while carefully curated, may not fully capture the diversity of knowledge conflicts in real-world applications
- Focus on Wikipedia-specific contradictions may not generalize to other knowledge domains where conflicts manifest differently

## Confidence
**High confidence**: The core finding that LLMs struggle with real-world knowledge conflicts is well-supported by both human and automated evaluation results across multiple model families.

**Medium confidence**: The claim that automated evaluation achieves F-score of 0.8 requires careful interpretation due to precision-recall tradeoff suggesting systematic biases.

**Low confidence**: The specific mechanisms by which different prompt templates influence LLM behavior on knowledge conflicts are not fully explored.

## Next Checks
1. **Cross-domain validation**: Test WikiContradict-trained models on knowledge conflicts from non-Wikipedia sources (e.g., news articles, scientific literature) to assess generalizability beyond Wikipedia-specific contradiction patterns.

2. **Temporal stability analysis**: Evaluate whether LLMs' performance on WikiContradict changes significantly when the same knowledge conflicts appear with different temporal contexts or when Wikipedia articles are updated to resolve contradictions.

3. **Failure mode characterization**: Systematically analyze the specific types of errors LLMs make on different contradiction categories (explicit vs. implicit) to identify whether certain conflict types are inherently more challenging or if current models lack specific reasoning capabilities.