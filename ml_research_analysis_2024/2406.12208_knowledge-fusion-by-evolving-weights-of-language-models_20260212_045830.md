---
ver: rpa2
title: Knowledge Fusion By Evolving Weights of Language Models
arxiv_id: '2406.12208'
source_url: https://arxiv.org/abs/2406.12208
tags:
- evolution
- performance
- different
- merging
- evolver
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel knowledge fusion method called Evolver,
  inspired by evolutionary algorithms, to enhance the performance of language models
  across diverse domains and tasks. The method aggregates the weights of multiple
  fine-tuned models into a population and generates offspring models through mutation
  and crossover operations.
---

# Knowledge Fusion By Evolving Weights of Language Models

## Quick Facts
- **arXiv ID:** 2406.12208
- **Source URL:** https://arxiv.org/abs/2406.12208
- **Reference count:** 40
- **Primary result:** Evolver method outperforms state-of-the-art model merging techniques on diverse language models and tasks

## Executive Summary
This paper introduces Evolver, a novel knowledge fusion method inspired by evolutionary algorithms that enhances language model performance across diverse domains and tasks. The approach aggregates weights from multiple fine-tuned models into a population and generates offspring models through mutation and crossover operations, selecting those that show improved performance on development datasets. Experimental results demonstrate that Evolver outperforms previous state-of-the-art model merging techniques on various language models (encoder-only, decoder-only, encoder-decoder) and tasks, while being seamlessly integrable with existing model merging frameworks.

## Method Summary
Evolver is a knowledge fusion method that uses evolutionary algorithms to combine weights from multiple fine-tuned language models without requiring additional training or data. The method initializes a population of fine-tuned models, then iteratively applies differential evolution-based mutation and crossover operations to generate offspring models. These offspring are evaluated on development datasets, and those showing improved performance replace their parent models in the population. The process continues for multiple generations until convergence, producing a merged model that outperforms simple averaging and existing merging techniques. The method can be integrated with other merging frameworks like Fisher-weighted averaging for further improvement.

## Key Results
- Evolver outperforms previous state-of-the-art model merging techniques across encoder-only, decoder-only, and encoder-decoder architectures
- The method achieves superior performance on diverse tasks and demonstrates improved out-of-distribution generalization
- Evolver can be seamlessly integrated with existing merging frameworks (Fisher, RegMean) to further boost performance
- The approach eliminates the need for additional training data or gradient-based optimization while maintaining or improving model quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Differential evolution-based mutation and crossover can discover optimal per-parameter coefficients for model merging without gradient-based optimization.
- **Mechanism:** The algorithm maintains a population of fine-tuned models, mutates individuals by adding scaled difference vectors between randomly selected pairs, and uses crossover to combine traits. Individuals are evaluated on a development set and replaced if superior.
- **Core assumption:** Fine-tuned models differ mainly in local regions around decision boundaries, so linear combinations of parameters will yield improved performance.
- **Evidence anchors:** [abstract] "inspired by evolutionary algorithms, which does not need further training or additional training data"; [section] "We simulate the evolution process of a population of neural networks using the differential evolution algorithm"
- **Break condition:** If fine-tuned models differ substantially in architecture or if mutations push parameters far outside valid ranges, convergence fails.

### Mechanism 2
- **Claim:** Iterative replacement of under-performing individuals removes negative interference effects in knowledge fusion.
- **Mechanism:** After each mutation and crossover, offspring models are scored on development data; if an offspring outperforms its parent, it replaces the parent in the population.
- **Core assumption:** Individual model performance is predictive of merged model quality; eliminating worst performers improves overall population fitness.
- **Evidence anchors:** [section] "We sequentially evaluate the performance scores of offspring individuals in comparison to their parent one by one. If an offspring performs better, we then replace the corresponding parent individual with it"
- **Break condition:** If development set is unrepresentative or too small, selection pressure may not guide evolution correctly.

### Mechanism 3
- **Claim:** Integration with existing merging methods (fisher, regmean) further boosts performance beyond simple averaging.
- **Mechanism:** The best evolved individual can be used as input to another merging method (e.g., fisher-weighted averaging) to refine the merged model.
- **Core assumption:** Evolutionary search finds a good initial population that other merging methods can improve upon.
- **Evidence anchors:** [abstract] "our approach can be seamlessly integrated with existing model merging frameworks"; [section] "Furthermore, we can also apply other model merging techniques to the updated population as a further improvement measure"
- **Break condition:** If the evolved population is already optimal, additional merging may cause overfitting or parameter interference.

## Foundational Learning

- **Concept:** Differential evolution (DE) algorithm
  - **Why needed here:** DE provides a gradient-free optimization framework suitable for discrete weight space exploration without backpropagation.
  - **Quick check question:** How does DE ensure diversity in the population while still converging toward good solutions?

- **Concept:** Parameter importance weighting (Fisher information)
  - **Why needed here:** Fisher-weighted averaging assigns higher weights to parameters that are more important for downstream tasks, improving merged model quality.
  - **Quick check question:** What metric does Fisher information use to determine parameter importance?

- **Concept:** Task vectors and interference resolution
  - **Why needed here:** Task vectors represent parameter differences between pre-trained and fine-tuned models; resolving sign conflicts prevents destructive interference during merging.
  - **Quick check question:** What two main interference problems do task vector-based methods address?

## Architecture Onboarding

- **Component map:** Population initialization (fine-tuned models) -> Evolutionary loop (mutation, crossover, evaluation, replacement) -> Optional integration (fisher/regmean merging) -> Final merged model
- **Critical path:** 1. Load and flatten model parameters into population vectors; 2. Generate offspring via DE mutation and crossover; 3. Convert vectors back to model checkpoints; 4. Run inference on development dataset; 5. Replace parents if offspring outperforms; 6. After convergence, optionally merge best evolved models
- **Design tradeoffs:** Population size vs. memory usage (larger populations improve exploration but require more GPU memory); Development set size vs. evolution quality (larger dev sets yield better selection signals but increase evaluation cost); Integration method choice (combining with fisher improves accuracy but adds computational overhead)
- **Failure signatures:** Stagnant population fitness over generations; High variance in individual model performance; Out-of-memory errors during sequential inference; Poor generalization on OOD test sets despite good dev set performance
- **First 3 experiments:** 1. Single population evolution on emotion classification (5 domain-specific models) → measure dev/test accuracy improvement over simple averaging; 2. Two-task pairwise evolution on GLUE → evaluate cross-task performance gains; 3. Combined evolver + regmean on non-i.i.d. partitions → assess benefit of integration vs. standalone methods

## Open Questions the Paper Calls Out
None

## Limitations
- The method's effectiveness may degrade when development sets are small or unrepresentative of test distributions
- Computational overhead of sequential model evaluation during evolution could become prohibitive for very large model populations
- While claiming compatibility with existing merging frameworks, specific implementation details for integration are not fully specified

## Confidence
- **High confidence:** The core evolutionary algorithm mechanism and its basic implementation are well-supported by experimental results
- **Medium confidence:** Claims about improved OOD generalization are supported but would benefit from more diverse test scenarios
- **Medium confidence:** Integration with existing merging frameworks shows promise but lacks detailed implementation specifications

## Next Checks
1. **Development set sensitivity analysis:** Systematically vary development set sizes and quality across different tasks to quantify the impact on evolutionary convergence and final performance
2. **Cross-architecture evolution testing:** Evaluate whether the method generalizes beyond same-architecture populations by attempting to evolve across different model sizes or architectures from the same family
3. **Computational efficiency benchmarking:** Measure wall-clock time and memory usage across different population sizes and compare against alternative merging methods to establish practical scalability limits