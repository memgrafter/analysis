---
ver: rpa2
title: What Matters for Model Merging at Scale?
arxiv_id: '2410.03617'
source_url: https://arxiv.org/abs/2410.03617
tags:
- merging
- arxiv
- base
- expert
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically evaluates the utility of model merging
  at scale, examining the impact of key factors such as base model quality, model
  size, number of expert models, and merging methods on both held-in and zero-shot
  generalization performance. The study uses PaLM-2 and PaLM-2-IT models ranging from
  1B to 64B parameters, merging up to 8 expert models using four popular methods:
  Averaging, Task Arithmetic, Dare-TIES, and TIES.'
---

# What Matters for Model Merging at Scale?

## Quick Facts
- arXiv ID: 2410.03617
- Source URL: https://arxiv.org/abs/2410.03617
- Reference count: 32
- One-line primary result: Merging is more effective with strong instruction-tuned base models, larger models facilitate easier merging, and merging consistently improves zero-shot generalization, often outperforming multitask-trained models when using strong base models and sufficient expert models.

## Executive Summary
This paper systematically evaluates model merging at scale using PaLM-2 and PaLM-2-IT models ranging from 1B to 64B parameters. The study examines how model size, base model quality, number of expert models, and merging methods affect both held-in and zero-shot generalization performance. Through extensive experiments merging up to 8 expert models using four popular methods (Averaging, Task Arithmetic, Dare-TIES, and TIES), the authors demonstrate that merging is most effective with strong instruction-tuned base models and that larger models consistently facilitate easier merging. The findings reveal that merged models often outperform multitask-trained models when using strong base models and sufficient expert models, highlighting the potential of model merging at scale for creating highly capable and generalizable language models.

## Method Summary
The authors fine-tune PaLM-2 and PaLM-2-IT models on 8 held-in task categories from the T0 mixture to create expert models of varying sizes (1B, 8B, 24B, 64B). They then merge these expert models using four different methods (Averaging, Task Arithmetic, Dare-TIES, and TIES) across different numbers of experts (2, 4, 6, 8). The merged models are evaluated on both held-in tasks (the expert training tasks) and held-out tasks (zero-shot generalization) using normalized performance metrics that compare against task experts and base models respectively. The study systematically varies model size, base model type, merging method, and number of experts to understand their relative importance in successful model merging.

## Key Results
- Merging is significantly more effective when using strong instruction-tuned base models (PaLM-2-IT) compared to standard pretrained models (PaLM-2)
- Larger models facilitate easier merging, with performance improving consistently as model size increases across all base models and merging methods
- Merging consistently improves zero-shot generalization capabilities, with performance monotonically improving as more expert models are added
- When merging eight large instruction-tuned expert models, the merged models often outperform multitask-trained models, particularly with strong base models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models facilitate easier merging due to increased overparameterization and flatter loss landscapes.
- Mechanism: As model size increases, the model becomes more overparameterized relative to the training data, creating a flatter loss landscape with broader basins of good performance. This allows parameters to move more freely during merging without degrading performance.
- Core assumption: The number of parameters grows faster than the information content in the training data, creating redundancy that can be leveraged during merging.
- Evidence anchors: The paper shows that larger models consistently show better merging performance across all base models, merging methods, and numbers of constituent models.

### Mechanism 2
- Claim: Instruction-tuned base models provide better weight disentanglement, making merged models more effective.
- Mechanism: Instruction tuning creates models with better zero-shot generalization capabilities by creating a more structured parameter space that preserves task boundaries better than standard pretraining.
- Core assumption: Instruction tuning creates a more structured parameter space that preserves task boundaries better than standard pretraining.
- Evidence anchors: The paper demonstrates that PaLM-2-IT models consistently outperform PaLM-2 models across various merging methods, model sizes, and numbers of constituent models.

### Mechanism 3
- Claim: Merging multiple experts can improve generalization beyond what multitask training achieves, particularly with strong base models.
- Mechanism: When experts are created from a strong base model with good generalization, each expert captures specific capabilities while maintaining connections to the broader knowledge in the base model. Merging these experts leverages complementary strengths while maintaining generalization properties.
- Core assumption: The base model's generalization capabilities are preserved or enhanced through the merging process rather than being lost when creating specialized experts.
- Evidence anchors: The paper shows that merged models often outperform multitask baselines when combining more than 6 large instruction-tuned expert models.

## Foundational Learning

- Concept: Zero-shot generalization performance
  - Why needed here: The paper establishes that base model quality, measured by zero-shot performance, is crucial for successful merging. Understanding what zero-shot performance means and how to measure it is fundamental to interpreting the results.
  - Quick check question: How would you design an evaluation to measure whether a base model has good zero-shot generalization capabilities?

- Concept: Parameter space interpolation and mode connectivity
  - Why needed here: Model merging fundamentally relies on the ability to interpolate between different parameter configurations. Understanding the theory behind when and why this works (mode connectivity) is essential for grasping why larger models merge better.
  - Quick check question: What conditions must be met in the loss landscape for two models to be successfully interpolated without performance loss?

- Concept: Task vectors and parameter differences
  - Why needed here: Model merging involves combining parameter differences (task vectors) between expert models. Understanding how these vectors represent task-specific knowledge is crucial for understanding the merging process.
  - Quick check question: How do task vectors capture the differences between expert models trained on different tasks?

- Concept: Instruction tuning vs. standard pretraining
  - Why needed here: The paper compares merging from instruction-tuned vs. standard pretrained base models. Understanding the differences in these training approaches and their effects on model capabilities is crucial.
  - Quick check question: What are the key differences in how instruction tuning and standard pretraining affect a model's parameter space and capabilities?

## Architecture Onboarding

- Component map: Base model (PaLM-2 or PaLM-2-IT) → Expert creation (full fine-tuning on 8 held-in tasks) → Merging methods (Averaging, Task Arithmetic, Dare-TIES, TIES) → Evaluation (held-in and held-out tasks)
- Critical path: 1) Select base model and create experts, 2) Choose merging method and number of experts, 3) Apply merging method, 4) Evaluate on held-in and held-out tasks, 5) Compare against baselines (task experts and multitask models)
- Design tradeoffs: 
  - Base model choice: Instruction-tuned gives better merging results but requires additional pretraining
  - Merging method: Simpler methods (averaging) work well at scale, but more complex methods may help with smaller models
  - Number of experts: More experts improve generalization but may degrade held-in performance if base model is weak
- Failure signatures: 
  - Poor held-in performance indicates interference between experts
  - No improvement in held-out performance suggests experts aren't complementary
  - Degradation with more experts indicates base model quality issues
- First 3 experiments:
  1. Merge 2 experts from 1B PaLM-2 base model using averaging - expect poor results to establish baseline
  2. Merge 2 experts from 1B PaLM-2-IT base model using averaging - expect better results to show base model importance
  3. Merge 8 experts from 64B PaLM-2-IT base model using averaging - expect best results to show scale benefits

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but several important questions emerge from the research:

- What is the fundamental mechanism behind the observation that larger models are consistently easier to merge than smaller models?
- How does the quality of the base model (as measured by zero-shot performance) causally influence the effectiveness of model merging?
- What is the theoretical limit of model merging in terms of the number of experts that can be combined without performance degradation?
- Why do different merging methods converge to similar performance at larger scales, and what specific properties of large instruction-tuned models cause this convergence?

## Limitations

- The study focuses on a specific set of models (PaLM-2 and PaLM-2-IT) and tasks (T0 mixture), which may not generalize to other model families or task distributions.
- The experimental design uses only 8 held-in and 4 held-out task categories, limiting the breadth of the evaluation.
- The comparison with multitask baselines is limited, as the multitask models were trained for a fixed number of steps rather than being fully optimized.

## Confidence

- **High Confidence**: The finding that instruction-tuned base models (PaLM-2-IT) outperform standard pretrained models (PaLM-2) across all merging scenarios is well-supported by consistent experimental results across multiple model sizes and merging methods.
- **Medium Confidence**: The claim that larger models facilitate easier merging has strong empirical support but relies on theoretical assumptions about overparameterization and loss landscape properties that aren't directly validated.
- **Low Confidence**: The specific mechanisms proposed (weight disentanglement from instruction tuning, parameter redundancy in large models) are plausible but not directly tested.

## Next Checks

1. **Loss Landscape Analysis**: Conduct mode connectivity tests to directly measure whether larger models have flatter loss landscapes and broader basins of good performance by measuring the loss along linear paths between expert models.

2. **Multitask Baseline Optimization**: Retrain the multitask baseline models with extended training budgets and early stopping based on validation performance rather than fixed steps, then compare these optimized multitask models against merged models.

3. **Cross-Model Generalization**: Repeat the merging experiments with different model families (e.g., Llama, Mistral) and different task distributions (e.g., medical, legal, or code generation tasks) to test whether the observed patterns hold across diverse settings.