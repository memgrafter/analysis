---
ver: rpa2
title: 'Evaluating the Efficacy of Instance Incremental vs. Batch Learning in Delayed
  Label Environments: An Empirical Study on Tabular Data Streaming for Fraud Detection'
arxiv_id: '2409.10111'
source_url: https://arxiv.org/abs/2409.10111
tags:
- incremental
- batch
- learning
- data
- instance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the efficacy of instance incremental versus
  batch learning in delayed label environments, focusing on fraud detection in tabular
  data streaming. The authors conduct a comprehensive empirical evaluation comparing
  state-of-the-art instance incremental algorithms (ARF, LBHT, LBLR) with batch incremental
  models (XGBoost, EBM, TabSRA) under various delay scenarios.
---

# Evaluating the Efficacy of Instance Incremental vs. Batch Learning in Delayed Label Environments: An Empirical Study on Tabular Data Streaming for Fraud Detection

## Quick Facts
- arXiv ID: 2409.10111
- Source URL: https://arxiv.org/abs/2409.10111
- Reference count: 31
- Primary result: Batch incremental models achieved up to 22% higher AUCPR compared to instance incremental counterparts in fraud detection with delayed labels

## Executive Summary
This paper investigates the efficacy of instance incremental versus batch incremental learning in delayed label environments, focusing on fraud detection in tabular data streaming. The authors conduct a comprehensive empirical evaluation comparing state-of-the-art instance incremental algorithms (ARF, LB_HT, LB_LR) with batch incremental models (XGBoost, EBM, TabSRA) under various delay scenarios. Their findings indicate that batch incremental learning is not inferior to instance incremental learning in terms of predictive performance and computational efficiency, especially when considering interpretability. On a real-world fraud dataset, batch incremental models achieved up to 22% higher AUCPR compared to instance incremental counterparts. The study reveals that storing past observations can significantly benefit batch incremental models, particularly for rare event detection tasks like fraud.

## Method Summary
The authors compare instance incremental (ARF, LB_HT, LB_LR) and batch incremental (XGBoost, EBM, TabSRA) models on delayed label streaming scenarios. They use interleaved chunks evaluation with Poisson-distributed label delays, hyperparameter optimization via Optuna (30 trials, 6 hours max), and evaluate on a 6.5M observation fraud dataset plus synthetic benchmarks. The comparison includes static, retrain, and propagate strategies for handling delayed labels, with performance measured by AUCPR, AUCROC, and running time.

## Key Results
- Batch incremental models achieved up to 22% higher AUCPR than instance incremental counterparts on fraud detection
- Storing past observations benefits batch models for rare event detection tasks
- Batch incremental solutions offer advantages in interpretability due to less frequent model changes and superior performance
- The propagate strategy (using past data batches) provided the best results across most scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Batch incremental learning with delayed labels can outperform instance incremental learning in fraud detection
- Mechanism: By storing observations for longer periods, batch models can accumulate more labeled instances, especially for rare classes like fraud. This improves the statistical power of each model update.
- Core assumption: Label delays are significant and variable, and rare class instances benefit from aggregation before training
- Evidence anchors:
  - [abstract] "batch incremental models achieved up to 22% higher AUCPR compared to instance incremental counterparts"
  - [section] "we explain why storing some past observations may be advantageous for the streaming learning model"
  - [corpus] Weak - corpus neighbors discuss label noise and incremental learning but not batch vs instance trade-offs with delayed labels
- Break condition: If label delay is negligible or the class distribution is balanced, the storage advantage disappears

### Mechanism 2
- Claim: Interpretability improves with batch incremental models due to less frequent architecture changes
- Mechanism: Batch models retrain or update less frequently than instance models, leading to more stable feature importance and model behavior over time, aiding human understanding
- Core assumption: Frequent model updates obscure interpretability, and human users need stable explanations to trust the system
- Evidence anchors:
  - [abstract] "batch incremental solutions tend to be favored" when considering interpretability
  - [section] "batch incremental inherently interpretable models... demonstrate superior predictive performance compared to their instance incremental counterparts"
  - [corpus] Missing - no direct corpus evidence on interpretability in streaming contexts
- Break condition: If the model updates are infrequent enough even in instance learning, or if interpretability tools can track changes effectively

### Mechanism 3
- Claim: Stacking or propagating past data batches improves performance for rare events
- Mechanism: By combining recent labeled data with past data (propagate strategy), batch models increase the sample size of rare classes, leading to better generalization
- Core assumption: Rare event detection benefits from larger training sets per update, and past data remains relevant
- Evidence anchors:
  - [section] "the propagate strategy... tends to provide the best results" with "up to 22% relative improvement in AUCPR"
  - [abstract] "batch incremental models achieved up to 22% higher AUCPR"
  - [corpus] Weak - corpus discusses incremental learning but not data propagation strategies
- Break concept drift is severe and past data becomes irrelevant

## Foundational Learning

- Concept: Data stream learning with delayed labels
  - Why needed here: The paper compares instance vs batch learning under delayed label conditions, requiring understanding of how delayed labels affect model updates
  - Quick check question: In a delayed label setting, when can an instance be used to update the model?

- Concept: Evaluation metrics for imbalanced data (AUCPR)
  - Why needed here: Fraud detection is highly imbalanced; AUCPR is more informative than accuracy or AUCROC for rare event detection
  - Quick check question: Why is AUCPR preferred over AUCROC for fraud detection?

- Concept: Interpretability in evolving data streams
  - Why needed here: The paper argues batch models are more interpretable due to less frequent changes, which is a key contribution
  - Quick check question: How does model stability over time affect interpretability in streaming contexts?

## Architecture Onboarding

- Component map: Data ingestion buffer -> Label delay simulator -> Batch model updater OR Instance model updater -> Evaluation module -> Interpretability tracker

- Critical path:
  1. Stream data arrives → stored in buffer
  2. Labels arrive after delay → matched to observations
  3. For batch: accumulate labeled data → retrain model when batch full
  4. For instance: update model immediately per labeled pair
  5. Evaluate performance on each batch
  6. Track interpretability metrics

- Design tradeoffs:
  - Storage vs. update frequency: batch needs more storage but updates less often
  - Latency vs. accuracy: instance updates faster but may be less accurate with rare events
  - Interpretability vs. adaptability: batch models change less, aiding interpretation

- Failure signatures:
  - Batch models: slow adaptation to sudden concept drift
  - Instance models: poor performance on rare events due to insufficient data per update
  - Both: performance degradation if label delay exceeds buffer capacity

- First 3 experiments:
  1. Compare batch vs instance performance on synthetic data with no drift and varying delay lengths
  2. Test interpretability stability by measuring feature importance variance over time for both approaches
  3. Evaluate the impact of data propagation (using past batches) on rare event detection performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal batch size for batch incremental learning in delayed label environments, and how does it depend on factors like stream speed, class imbalance, and concept drift patterns?
- Basis in paper: [explicit] The authors note that choosing the learning batch based on evaluation date (daily, monthly) can be a good starting point, but they acknowledge that the optimal batch size may depend on several factors they leave for future study
- Why unresolved: The paper focuses on demonstrating the effectiveness of batch incremental learning compared to instance incremental learning in delayed settings, but does not investigate the impact of different batch sizes on performance
- What evidence would resolve it: Empirical studies comparing the performance of batch incremental models with varying batch sizes across different stream scenarios (varying speeds, class imbalances, and drift patterns) would provide insights into the optimal batch size

### Open Question 2
- Question: How do different strategies for handling label delay, such as waiting for a fixed number of labels or using probabilistic models, compare in terms of predictive performance and computational efficiency?
- Basis in paper: [explicit] The authors mention that they use a Poisson distribution to model label delay, but they do not compare this approach with other strategies for handling delay
- Why unresolved: The paper focuses on comparing instance incremental and batch incremental learning in delayed settings, but does not investigate different approaches for handling the delay itself
- What evidence would resolve it: Empirical studies comparing the performance of different delay handling strategies across various datasets and delay scenarios would provide insights into their relative effectiveness

### Open Question 3
- Question: How can interpretability techniques be adapted for evolving data streams to provide real-time explanations for model predictions and changes in model behavior over time?
- Basis in paper: [explicit] The authors discuss the importance of interpretability in evolving data streams and mention that inherently interpretable models like EBM and TabSRA are favored due to their stability over time. However, they do not provide specific techniques for adapting interpretability methods to stream settings
- Why unresolved: The paper focuses on comparing instance incremental and batch incremental learning in terms of interpretability, but does not provide concrete solutions for achieving interpretability in stream settings
- What evidence would resolve it: Development and evaluation of interpretability techniques specifically designed for evolving data streams, such as incremental versions of SHAP or SAGE, would provide solutions for achieving interpretability in stream settings

## Limitations
- The study focuses exclusively on tabular data, limiting generalizability to other data types
- Specific hyperparameter search spaces and implementation details for the "propagate" strategy are not fully specified
- Interpretability claims are supported by limited empirical evidence rather than direct measurements

## Confidence
- **High confidence**: Batch incremental models can outperform instance incremental models in AUCPR for fraud detection with delayed labels
- **Medium confidence**: Interpretability benefits of batch models due to less frequent updates
- **Medium confidence**: Data propagation strategy improves rare event detection

## Next Checks
1. Implement and test the exact "propagate" strategy on synthetic data to verify the claimed 22% AUCPR improvement
2. Measure feature importance stability over time for both batch and instance models to quantify interpretability differences
3. Test the sensitivity of results to different label delay distributions beyond Poisson (e.g., uniform, exponential)