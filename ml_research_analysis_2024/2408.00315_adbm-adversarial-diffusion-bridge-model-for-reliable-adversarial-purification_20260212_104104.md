---
ver: rpa2
title: 'ADBM: Adversarial diffusion bridge model for reliable adversarial purification'
arxiv_id: '2408.00315'
source_url: https://arxiv.org/abs/2408.00315
tags:
- adbm
- adversarial
- diffpure
- diffusion
- reverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ADBM addresses the limitations of DiffPure by directly constructing
  a reverse bridge from diffused adversarial data to clean data, eliminating the trade-off
  between noise purification and data recovery. The method fine-tunes pre-trained
  diffusion models with adversarial noise generated through classifier-guided optimization,
  using a specialized loss function that gradually removes adversarial perturbations.
---

# ADBM: Adversarial diffusion bridge model for reliable adversarial purification

## Quick Facts
- arXiv ID: 2408.00315
- Source URL: https://arxiv.org/abs/2408.00315
- Reference count: 40
- Primary result: ADBM achieves 4.4% better average robustness on CIFAR-10 while maintaining comparable clean accuracy

## Executive Summary
ADBM addresses the limitations of DiffPure by directly constructing a reverse bridge from diffused adversarial data to clean data, eliminating the trade-off between noise purification and data recovery. The method fine-tunes pre-trained diffusion models with adversarial noise generated through classifier-guided optimization, using a specialized loss function that gradually removes adversarial perturbations. Theoretical analysis guarantees ADBM's superiority over DiffPure, and extensive experiments show it achieves 4.4% better average robustness on CIFAR-10 while maintaining comparable clean accuracy.

## Method Summary
ADBM fine-tunes pre-trained diffusion models (DDPM++ continuous variant) to purify adversarial examples by assuming the presence of adversarial noise at the starting point of the forward process. The method generates adversarial noise using classifier-guided optimization, specifically maximizing the classifier's loss to create effective adversarial examples. During training, ADBM uses a specialized loss function that incorporates both the diffusion model's reconstruction error and the adversarial noise level. At inference, ADBM applies forward diffusion to adversarial examples and uses reverse sampling (DDIM preferred) to recover clean examples, requiring only 5 reverse steps for effective purification.

## Key Results
- ADBM achieves 4.4% better average robustness on CIFAR-10 compared to DiffPure
- Maintains comparable clean accuracy (93.86%) while achieving 62.69% robust accuracy
- Demonstrates strong performance against white-box adaptive attacks, particularly for unseen threats
- Requires only 5 reverse steps and can be accelerated further for practical applications

## Why This Works (Mechanism)

### Mechanism 1
ADBM directly constructs a reverse bridge from diffused adversarial data back to clean data, eliminating the trade-off between noise purification and data recovery. By assuming adversarial noise ϵa at the starting point of the forward process and designing a reverse process that explicitly accounts for this noise through coefficient kt, ADBM learns to transform diffused adversarial data directly to clean data.

### Mechanism 2
ADBM's training objective aligns with the classification goal by generating adversarial noise through classifier-guided optimization. Instead of maximizing the diffusion model's loss directly, ADBM uses the classifier's loss Lc(fθc(ˆx₀), y) to generate adversarial noise, ensuring the purified examples are close to input examples in the classification space.

### Mechanism 3
ADBM achieves superior theoretical guarantees compared to DiffPure by providing a tighter bound on purification error. Theorem 1 shows that if training loss converges to zero, ADBM can perfectly remove adversarial noises with one-step DDIM sampler, while DiffPure cannot provide such strong theoretical guarantee.

## Foundational Learning

- **Diffusion models and their forward/reverse processes**: Understanding the denoising process is crucial as ADBM modifies both the forward initialization and reverse sampling to handle adversarial examples specifically.
  - Quick check: What is the key difference between the forward and reverse processes in diffusion models, and how does ADBM modify these processes?

- **Adversarial attacks and their impact on neural networks**: Understanding how adversarial examples are generated and their properties is crucial for designing effective purification methods.
  - Quick check: What are the main differences between white-box, black-box, and adaptive attacks, and why does ADBM focus on adaptive attacks?

- **Theoretical analysis of generative models and their convergence properties**: The theoretical guarantees provided by ADBM rely on understanding the convergence properties of diffusion models and their ability to learn complex distributions.
  - Quick check: What are the key assumptions in Theorem 1 and Theorem 2, and how do they relate to the practical performance of ADBM?

## Architecture Onboarding

- **Component map**: Pre-trained diffusion model (UNet) -> Classifier (WRN-28-10, WRN-70-16, ResNet-50) -> ADBM fine-tuning module with modified loss function -> Inference pipeline with forward diffusion and reverse sampling

- **Critical path**: 1) Generate adversarial examples using classifier-guided PGD 2) Fine-tune pre-trained diffusion model with ADBM's modified loss function 3) During inference, apply forward diffusion to adversarial examples 4) Apply reverse sampling (DDIM preferred) to recover clean examples

- **Design tradeoffs**: Trade-off between number of reverse steps and inference speed; trade-off between noise level in forward process and data recovery quality; trade-off between model complexity and purification effectiveness

- **Failure signatures**: High training loss indicating poor alignment between adversarial noise generation and diffusion model learning; degradation in clean accuracy suggesting over-aggressive noise removal; poor performance on adaptive attacks indicating insufficient robustness

- **First 3 experiments**: 1) Compare ADBM's performance with DiffPure on CIFAR-10 under l∞ attacks with varying reverse steps (1, 5, 10) 2) Evaluate the impact of different adversarial noise generation modes (with/without classifier guidance, with/without fixed t and x) 3) Test ADBM's transferability by using a fine-tuned ADBM checkpoint on a new classifier architecture (e.g., WRN-28-10 → ViT)

## Open Questions the Paper Calls Out

### Open Question 1
How does ADBM's performance scale with larger datasets like full ImageNet-1K compared to CIFAR-10 and Tiny-ImageNet? The authors explicitly state they cannot afford experiments on full ImageNet-1K due to computational constraints, leaving this scalability question unanswered.

### Open Question 2
What is the minimum number of reverse steps required for ADBM to maintain effectiveness against the strongest adaptive attacks? While the paper demonstrates single-step effectiveness, it doesn't systematically explore the trade-off between reverse steps and attack resistance.

### Open Question 3
How does ADBM's robustness against unseen threat models compare to adversarial training when both methods have equivalent computational budgets? The computational cost comparison is incomplete - ADBM's fine-tuning cost vs. AT's full training cost is not directly compared.

## Limitations

- The theoretical guarantees rely on assumptions about Gaussian distributions and Markov properties that may not hold for complex, high-dimensional adversarial perturbations in practice.
- The paper's focus on l∞-norm bounded attacks limits generalizability to other attack types like l2 or l0 norms.
- Lack of comparison against recently proposed state-of-the-art adversarial defenses beyond DiffPure and AT may overestimate ADBM's relative performance.

## Confidence

- **High confidence**: The core mechanism of using classifier-guided adversarial noise generation for diffusion model fine-tuning is well-supported by the experimental results and aligns with established practices in adversarial training.
- **Medium confidence**: The theoretical superiority over DiffPure is mathematically sound but may not translate directly to practical performance due to the simplifying assumptions in the proofs.
- **Low confidence**: The claim about ADBM being a plug-and-play defense that doesn't require adversarial training of the classifier backbone needs further validation, as the results show performance degradation when using pre-trained classifiers.

## Next Checks

1. **Cross-architecture transferability**: Test ADBM's performance when the fine-tuned checkpoint is applied to completely different classifier architectures (e.g., fine-tune on WRN-28-10, test on ViT or ConvNeXt) to validate the claimed plug-and-play nature.

2. **Alternative attack spaces**: Evaluate ADBM against l2-norm bounded attacks and spatially-transformed adversarial examples to assess robustness beyond the l∞ threat model used in the paper.

3. **Computational overhead analysis**: Measure the actual inference time of ADBM with 5 reverse steps compared to DiffPure and AT baselines, including the forward diffusion computation time, to validate the claimed efficiency advantages.