---
ver: rpa2
title: 'LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External
  Knowledge Augmentation'
arxiv_id: '2402.11943'
source_url: https://arxiv.org/abs/2402.11943
tags:
- misinformation
- text
- lvlm
- image
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the potential of large vision language
  models (LVLMs) for multimodal misinformation detection and identifies that their
  reasoning capabilities are limited when external evidence is needed. To address
  this, the authors propose LEMMA, a framework that combines LVLM intuition and reasoning
  with external knowledge augmentation.
---

# LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation

## Quick Facts
- arXiv ID: 2402.11943
- Source URL: https://arxiv.org/abs/2402.11943
- Authors: Keyang Xuan; Li Yi; Fan Yang; Ruochen Wu; Yi R. Fung; Heng Ji
- Reference count: 15
- Key outcome: Improves accuracy over top baseline LVLM by 9% on Twitter and 13% on Fakeddit datasets

## Executive Summary
This paper addresses the challenge of multimodal misinformation detection by proposing LEMMA, a framework that enhances large vision language models (LVLMs) with external knowledge augmentation. The authors identify that LVLMs, while powerful, have limited reasoning capabilities when external evidence is needed for accurate judgment. LEMMA combines LVLM intuition and reasoning with a reasoning-aware multi-query generation approach for multimodal retrieval, image context retrieval, and resource distillation to filter relevant evidence and refine predictions. Experiments demonstrate significant performance improvements over baseline LVLMs, with robust results across diverse contexts.

## Method Summary
LEMMA is a four-stage framework that enhances multimodal misinformation detection by augmenting LVLM reasoning with external knowledge. The process begins with an initial-stage inference where LVLM evaluates the image-text pair and determines if external evidence is needed. If required, LEMMA employs a reasoning-aware multi-query generation approach to create targeted search queries, combined with image context retrieval to trace image origins. The retrieved resources undergo resource distillation through topic filtering and evidence extraction to identify relevant information. Finally, a refined prediction stage integrates all evidence to classify the content into one of six misinformation categories or mark it as "Unverified." The framework leverages GPT-4V for both inference and evidence processing, with external knowledge sourced from web searches.

## Key Results
- Achieves 9% accuracy improvement over top baseline LVLM on Twitter dataset
- Achieves 13% accuracy improvement over top baseline LVLM on Fakeddit dataset
- Demonstrates robust performance across diverse misinformation contexts
- Shows effectiveness of reasoning-aware multi-query generation and image context retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LEMMA improves LVLM performance by incorporating external knowledge to compensate for LVLM's limited reasoning capabilities when evidence is needed.
- Mechanism: The framework uses reasoning-aware multi-query generation and image context retrieval to gather comprehensive evidence, then applies resource distillation to filter relevant evidence and refine predictions.
- Core assumption: LVLM's internal knowledge is insufficient for accurate misinformation detection when external context is required.
- Evidence anchors:
  - [abstract] "its profound reasoning may present limited power with a lack of evidence"
  - [section 3.2.4] "it has limited power to make correct judgments when further evidence is necessary for the correct prediction"
  - [corpus] Weak evidence - related papers focus on LVLM capabilities but don't directly validate the external knowledge augmentation mechanism
- Break condition: If LVLM's internal knowledge proves sufficient for the task, external knowledge augmentation becomes unnecessary.

### Mechanism 2
- Claim: Multi-query generation based on LVLM reasoning produces more effective search queries than traditional direct query construction.
- Mechanism: LVLM analyzes the initial reasoning to identify key discrepancies and statements that need verification, generating targeted questions and titles for retrieval.
- Core assumption: LVLM can identify which aspects of the content require external verification through analysis of its own reasoning.
- Evidence anchors:
  - [section 4.2.1] "we employ LVLM to generate multi-faceted queries based on the direct reasoning RD"
  - [section 4.2] "we proposed a multimodal retrieval framework that combines reasoning-aware multi-query-based text retrieval"
  - [corpus] Weak evidence - related papers discuss LVLM retrieval but don't validate multi-query generation based on LVLM reasoning
- Break condition: If simple keyword extraction proves equally effective, the reasoning-aware approach becomes unnecessary.

### Mechanism 3
- Claim: Image context retrieval adds value beyond traditional multimodal analysis by tracing image origins and providing visual context.
- Mechanism: The framework uses Google search to trace image sources and identifies whether images are reused or manipulated, providing context that textual analysis might miss.
- Core assumption: Image provenance provides critical information for misinformation detection that isn't captured by text-based analysis alone.
- Evidence anchors:
  - [section 4.2.2] "image context retrieval technique provides a substantial improvement" and "visual search adds a layer of context that significantly enhances the accuracy"
  - [Figure 5] Example showing how image retrieval exposes reused promotional images
  - [corpus] Weak evidence - related papers discuss multimodal analysis but don't validate image context retrieval as a separate mechanism
- Break condition: If image content analysis alone proves sufficient, additional context retrieval becomes unnecessary.

## Foundational Learning

- Concept: LVLM capabilities and limitations in multimodal reasoning
  - Why needed here: Understanding what LVLM can and cannot do independently is crucial for designing effective augmentation strategies
  - Quick check question: What are the two main reasons LVLMs struggle with misinformation detection without external evidence?

- Concept: Retrieval-augmented generation (RAG) techniques
  - Why needed here: LEMMA builds on RAG principles but adapts them for multimodal misinformation detection with reasoning-aware query generation
  - Quick check question: How does LEMMA's query generation differ from traditional RAG approaches?

- Concept: Fine-grained misinformation classification
  - Why needed here: The refined prediction stage requires categorizing misinformation into six specific types for accurate assessment
  - Quick check question: What are the six categories used in LEMMA's fine-grained classification system?

## Architecture Onboarding

- Component map: Input layer: Image-text pair → Initial inference module: LVLM prediction and reasoning generation → Necessity evaluation: Determines if external knowledge is needed → Multimodal retrieval: Text and image search with reasoning-aware queries → Resource distillation: Topic filtering and evidence extraction → Refined prediction: Final classification using all evidence → Output layer: Misinformation category or "Unverified"

- Critical path: Input → Initial inference → Necessity evaluation → Multimodal retrieval → Resource distillation → Refined prediction → Output

- Design tradeoffs:
  - Accuracy vs. latency: External knowledge retrieval improves accuracy but increases processing time
  - Conservatism vs. sensitivity: Initial-stage inference prevents over-searching but may miss some cases requiring external evidence
  - Resource filtering vs. comprehensiveness: Coarse-to-fine distillation balances relevance with completeness

- Failure signatures:
  - Low accuracy improvement: May indicate ineffective query generation or poor resource filtering
  - High latency: Suggests inefficient retrieval or processing bottlenecks
  - "Unverified" classifications: Could indicate overly conservative initial-stage evaluation

- First 3 experiments:
  1. Test baseline LVLM performance on Twitter and Fakeddit datasets using Direct and CoT prompting
  2. Evaluate multi-query generation effectiveness by comparing search results from traditional vs. reasoning-aware queries
  3. Measure impact of image context retrieval by comparing performance with and without visual source tracing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LEMMA handle cases where external evidence is contradictory or ambiguous, and what mechanisms are in place to weigh competing sources?
- Basis in paper: [inferred] The paper discusses the retrieval of external evidence and its integration into the decision-making process, but does not detail how contradictory evidence is resolved.
- Why unresolved: The methodology for resolving conflicts between multiple pieces of evidence is not explicitly described, leaving uncertainty about how the model ensures accuracy in such scenarios.
- What evidence would resolve it: Detailed examples or experiments showing how LEMMA handles contradictory evidence would clarify the model's decision-making process.

### Open Question 2
- Question: What is the impact of LEMMA's computational complexity on its scalability and real-time application, and how does it compare to baseline models in terms of efficiency?
- Basis in paper: [explicit] The paper acknowledges increased computational complexity due to the integration of external knowledge sources and multiple LVLM-based modules, but does not provide a detailed analysis of its impact on scalability.
- Why unresolved: Without specific efficiency metrics or comparisons, it is unclear how LEMMA performs in real-time applications or under resource constraints.
- What evidence would resolve it: Performance benchmarks comparing LEMMA's speed and resource usage to baseline models would provide insights into its practical applicability.

### Open Question 3
- Question: How sensitive is LEMMA to different prompt designs, and what are the optimal configurations for maximizing its performance across diverse misinformation contexts?
- Basis in paper: [inferred] The paper mentions that the study did not thoroughly examine LEMMA's sensitivity to different prompts, suggesting variability in performance based on prompt design.
- Why unresolved: The lack of exploration into prompt sensitivity leaves uncertainty about how different configurations might affect LEMMA's effectiveness in various contexts.
- What evidence would resolve it: Systematic experiments testing different prompt designs and their impact on LEMMA's performance would identify optimal configurations.

### Open Question 4
- Question: How does LEMMA perform on longer texts and more complex misinformation scenarios beyond short social media posts, and what adaptations are necessary for these contexts?
- Basis in paper: [explicit] The paper notes that evaluation datasets are limited to short social media posts, leaving LEMMA's performance on longer texts untested.
- Why unresolved: The absence of testing on longer texts raises questions about LEMMA's adaptability and effectiveness in more complex misinformation detection scenarios.
- What evidence would resolve it: Experiments using datasets with longer texts and more complex misinformation cases would demonstrate LEMMA's adaptability and effectiveness in diverse contexts.

## Limitations

- Heavy dependency on GPT-4V for multiple critical functions may create single point of failure and availability issues
- External knowledge retrieval introduces latency and potential reliability problems due to third-party API dependencies
- Performance may be sensitive to prompt engineering quality, though specific templates were not fully disclosed
- Limited evaluation to short social media posts raises questions about effectiveness with longer, more complex misinformation scenarios

## Confidence

**High Confidence**: The core mechanism of using external knowledge to enhance LVLM performance in misinformation detection is well-supported by experimental results showing 9% and 13% accuracy improvements on Twitter and Fakeddit datasets respectively. The architectural design and multi-stage approach are clearly specified and logically sound.

**Medium Confidence**: The effectiveness of reasoning-aware multi-query generation and image context retrieval mechanisms, while demonstrated, lacks comprehensive ablation studies to quantify their individual contributions. The claim that image provenance provides unique value beyond text analysis is supported by examples but could benefit from more systematic validation.

**Low Confidence**: The paper's assertion that LVLM reasoning capabilities are "limited" without external evidence, while intuitively reasonable, is not empirically validated through direct comparison of LVLM-only performance versus augmented performance across various reasoning tasks.

## Next Checks

1. **Ablation Study Implementation**: Conduct systematic ablation tests to isolate the impact of reasoning-aware multi-query generation versus traditional keyword extraction, and separately evaluate the contribution of image context retrieval to overall performance improvements.

2. **Cross-Domain Generalization Testing**: Evaluate LEMMA's performance on misinformation detection tasks from different domains (e.g., health misinformation, political misinformation) to assess robustness and identify potential domain-specific limitations or biases.

3. **Prompt Engineering Sensitivity Analysis**: Systematically vary prompt templates for each LEMMA module to quantify how sensitive the framework's performance is to prompt engineering quality, and develop standardized prompt guidelines for consistent deployment.