---
ver: rpa2
title: Do LLMs dream of elephants (when told not to)? Latent concept association and
  associative memory in transformers
arxiv_id: '2406.18400'
source_url: https://arxiv.org/abs/2406.18400
tags:
- arxiv
- figure
- context
- latent
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models can be manipulated to output incorrect facts
  simply by altering context, even without changing factual content. This "context
  hijacking" suggests LLMs may function like associative memory models, retrieving
  facts based on token associations rather than true understanding.
---

# Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers
## Quick Facts
- arXiv ID: 2406.18400
- Source URL: https://arxiv.org/abs/2406.18400
- Reference count: 40
- LLMs can be manipulated to output incorrect facts simply by altering context, even without changing factual content

## Executive Summary
This paper demonstrates that large language models can be manipulated to output incorrect facts through context hijacking, where changing context without altering factual content leads to erroneous outputs. The authors propose that LLMs function as associative memory models, retrieving facts based on token associations rather than true understanding. To study this phenomenon, they introduce a synthetic "latent concept association" task where output tokens relate to context tokens through an unobserved semantic space. The analysis shows that single-layer transformers use self-attention to aggregate information while the value matrix serves as associative memory, storing associations between context and output tokens. This work provides theoretical validation for existing editing and fine-tuning techniques while offering insights into how transformers form associations between context and output tokens.

## Method Summary
The authors study context hijacking in LLMs using the CounterFact dataset and multiple models (GPT-2, Gemma, LLaMA-2-7B). They create a synthetic latent concept association task where output tokens relate to context tokens through latent variables. The analysis focuses on single-layer transformers, examining how self-attention aggregates information and how the value matrix implements associative memory. The paper also investigates the geometric structure of trained transformer embeddings, showing low-rank patterns related to token similarity measured through Hamming distance in latent space.

## Key Results
- LLMs can be manipulated to output incorrect facts simply by altering context, even without changing factual content
- Single-layer transformers use self-attention to aggregate information while the value matrix serves as associative memory
- Trained transformer embeddings exhibit low-rank structure and geometric patterns related to token similarity

## Why This Works (Mechanism)
### Mechanism 1
- Claim: LLMs can be manipulated to output incorrect facts simply by altering context, even without changing factual content.
- Mechanism: Context tokens serve as associative memory cues that trigger retrieval of related facts, regardless of semantic meaning.
- Core assumption: The LLM's fact retrieval system relies on token similarity in latent space rather than semantic understanding.
- Evidence anchors:
  - [abstract] "fact retrieval is not robust and LLMs can be easily fooled by varying contexts, even without altering their factual meanings"
  - [section 3] "fact retrieval is not robust and LLMs can be easily fooled by varying contexts"
  - [corpus] Weak - no direct corpus evidence supporting this mechanism
- Break condition: If semantic understanding overrides token-based retrieval cues.

### Mechanism 2
- Claim: A one-layer transformer uses self-attention to aggregate information and the value matrix as associative memory to solve latent concept association tasks.
- Mechanism: Self-attention counts token occurrences while the value matrix maps aggregated representations to output tokens through learned associations.
- Core assumption: The value matrix implements associative memory by storing relationships between context tokens and output tokens.
- Evidence anchors:
  - [abstract] "the transformer gathers information using self-attention and uses the value matrix for associative memory"
  - [section 5] "the value matrix is important and has associative memory structure as in (5.1)"
  - [corpus] Weak - no direct corpus evidence supporting this mechanism
- Break condition: If multi-layer transformers develop different mechanisms beyond this simple two-stage process.

### Mechanism 3
- Claim: Trained transformer embeddings exhibit low-rank structure and geometric patterns related to token similarity.
- Mechanism: Embeddings capture latent semantic concepts through their inner product relationships, which follow patterns related to Hamming distance in latent space.
- Core assumption: Embedding geometry reflects the underlying latent concept structure of the data.
- Evidence anchors:
  - [abstract] "trained transformer embeddings exhibit low-rank structure and geometric patterns related to token similarity"
  - [section 5] "embedding structure is approximated by (5.2)" and "low-rank structure"
  - [corpus] Weak - no direct corpus evidence supporting this mechanism
- Break condition: If embedding geometry becomes too complex to be captured by simple low-rank approximations.

## Foundational Learning
- Concept: Associative memory in neural networks
  - Why needed here: The paper models LLMs as associative memory systems where context tokens trigger fact retrieval
  - Quick check question: How does Hopfield network associative memory differ from the proposed transformer-based associative memory?

- Concept: Self-attention mechanism
  - Why needed here: The paper shows how self-attention aggregates information for the latent concept association task
  - Quick check question: What role does self-attention play in the two-stage transformer process described in the paper?

- Concept: Latent variable models
  - Why needed here: The paper introduces a latent concept association task where similarity is measured in an unobserved semantic space
  - Quick check question: How does the latent conditional distribution p(z|z*) differ from standard conditional probability models?

## Architecture Onboarding
- Component map: Input tokens → Embedding layer → Self-attention layer → Value matrix → Output logits
- Critical path: Context → Embedding → Self-attention → Value matrix → Output logits
- Design tradeoffs:
  - Embedding dimension vs. vocabulary size (underparameterized vs. overparameterized regimes)
  - Context length vs. accuracy (longer contexts provide more information)
  - Model complexity vs. interpretability (single-layer vs. multi-layer transformers)
- Failure signatures:
  - Context hijacking indicates the model relies too heavily on token-based retrieval rather than semantic understanding
  - Low accuracy on latent concept association suggests poor learning of associative memory structure
  - High-rank embedding structures indicate failure to capture the intended geometric patterns
- First 3 experiments:
  1. Test context hijacking on a simple LLM with various context modifications
  2. Train a single-layer transformer on synthetic latent concept association data and analyze its components
  3. Replace trained value matrix with constructed associative memory matrix and measure accuracy impact

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the associative memory structure in transformers generalize beyond single-layer models to multi-layer transformers with residual connections and normalization?
- Basis in paper: [explicit] The paper states "the theoretical section only focuses on single-layer transformer network. While single-layer networks already demonstrate some interesting phenomena including low-rank structures, the functionality of multi-layer transformers is much different compared to single-layer transformers with the notable emergence of induction head [Elh+21]."
- Why unresolved: The current theoretical framework only analyzes single-layer transformers without residual connections or normalization. Multi-layer transformers introduce additional complexity through hierarchical representations and cross-layer information flow that may alter how associative memory is implemented.
- What evidence would resolve it: Experiments analyzing the value matrices, attention patterns, and embedding structures across multiple layers of trained transformers on the latent concept association task would reveal whether the same associative memory mechanisms persist or if they fundamentally change with depth.

### Open Question 2
- Question: How does the transformer's associative memory mechanism scale with increasing vocabulary size and latent concept dimensionality?
- Basis in paper: [explicit] The paper mentions that in the underparameterized regime, embedding training is required and embeddings exhibit geometric structures related to Hamming distance, but doesn't explore scaling properties with vocabulary size.
- Why unresolved: The current analysis assumes a fixed vocabulary size (V = 2^m) and doesn't examine how the transformer's ability to form associations degrades or improves as the number of concepts or tokens increases.
- What evidence would resolve it: Systematic experiments varying the vocabulary size and latent dimensionality while measuring memory recall accuracy, attention effectiveness, and embedding geometry would show whether the associative memory mechanisms scale linearly, sublinearly, or superlinearly with problem size.

### Open Question 3
- Question: Can the context hijacking vulnerability be mitigated by modifying the transformer architecture or training objective while preserving its associative memory capabilities?
- Basis in paper: [inferred] The paper demonstrates that context hijacking exploits the transformer's reliance on token associations rather than semantic understanding, and suggests this is related to how self-attention aggregates information and value matrices store associations.
- Why unresolved: While the paper identifies the mechanism behind context hijacking, it doesn't explore architectural modifications or alternative training objectives that could make the transformer more robust to context manipulation without sacrificing its ability to learn associations.
- What evidence would resolve it: Comparing modified transformer architectures (e.g., with different attention mechanisms, value matrix structures, or training objectives) on both the latent concept association task and context hijacking experiments would identify whether robustness can be improved without degrading core associative memory functions.

## Limitations
- Limited empirical validation of context hijacking beyond the CounterFact dataset format
- Analysis focuses only on single-layer transformers without residual connections or normalization
- Theoretical framework for low-rank embedding structures lacks extensive validation on real LLM embeddings

## Confidence
- Context hijacking experiments: Medium confidence due to limited model diversity and dataset format specificity
- Single-layer transformer analysis: Medium confidence based on theoretical framework but untested on deeper models
- Low-rank embedding claims: Medium confidence with theoretical support but lacking empirical validation

## Next Checks
1. Test context hijacking across diverse datasets beyond CounterFact, including factual QA pairs, narrative contexts, and multi-turn conversations, to assess generalizability of the phenomenon.

2. Analyze how the associative memory mechanisms scale to deeper transformer architectures, examining whether self-attention still serves primarily as an aggregator and whether the value matrix maintains its associative memory role.

3. Measure the actual low-rank structure and geometric patterns in trained LLM embeddings using randomized matrix decompositions and spectral analysis, comparing against the theoretical predictions for the synthetic task.