---
ver: rpa2
title: 'Piecing It All Together: Verifying Multi-Hop Multimodal Claims'
arxiv_id: '2411.09547'
source_url: https://arxiv.org/abs/2411.09547
tags:
- claim
- claims
- multimodal
- evidence
- book
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMCV, a dataset of 15k multi-hop multimodal
  claims requiring reasoning over text, images, and tables. The dataset is generated
  using a novel pipeline leveraging large language models for claim creation and refinement,
  with human feedback for quality control.
---

# Piecing It All Together: Verifying Multi-Hop Multimodal Claims

## Quick Facts
- **arXiv ID**: 2411.09547
- **Source URL**: https://arxiv.org/abs/2411.09547
- **Reference count**: 40
- **Primary result**: MMCV dataset of 15k multi-hop multimodal claims shows MLLMs achieve 56-72% F1 scores on 3-4 hop claims, 23-27% lower than human performance

## Executive Summary
This paper introduces MMCV, a dataset of 15k multi-hop multimodal claims requiring reasoning over text, images, and tables. The dataset is generated using a novel pipeline leveraging large language models for claim creation and refinement, with human feedback for quality control. Experiments with state-of-the-art multimodal models show performance degradation as the number of reasoning hops increases, with models achieving only 56-72% F1 scores on 3-4 hop claims. Human annotators significantly outperform models, achieving 23-27% higher F1 scores on complex claims. The dataset provides a challenging benchmark for future research in multimodal multi-hop reasoning.

## Method Summary
The MMCV dataset is generated through a pipeline that first creates claims from Wikipedia QA pairs using LLMs, then iteratively refines them based on LLM judge feedback until quality thresholds are met. RAG-based validation ensures factual accuracy by retrieving relevant Wikipedia articles. Human annotators provide final quality control by scoring claims on fluency, correctness, and clarity. The resulting dataset contains claims requiring 1-4 reasoning hops over multimodal evidence (text, images, tables). Three state-of-the-art MLLMs (GPT-4o, Gemini 1.5, LLaVA) are evaluated under both closed-book and open-book settings using zero-shot claim verification.

## Key Results
- MLLMs achieve 56-72% F1 scores on 3-4 hop claims in open-book settings
- Human annotators outperform models by 23-27% F1 on complex claims
- Performance degrades significantly as hop count increases (60% drop from 1-hop to 4-hop claims)
- GPT-4o performs best among tested models, with closed-book F1 scores ranging from 37.5% to 65.5%
- Overconfidence is observed in model predictions, particularly for 3-hop and 4-hop claims

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can iteratively refine multi-hop claims with high fidelity when provided with structured feedback.
- Mechanism: The pipeline uses an LLM judge to score modified claims across fluency, correctness, and clarity, then iteratively refines the claim until it meets predefined thresholds.
- Core assumption: LLMs can self-improve through feedback loops when given explicit scoring rubrics.
- Evidence anchors:
  - [abstract] "This method significantly reduces the workload on human annotators and cuts costs, while ensuring high quality and factual accuracy of the dataset."
  - [section 3.2] "we employ a modify-then-refine approach that iteratively enhances the quality of the modified claim candidate based on feedback from LLMs"
- Break condition: If the LLM judge's feedback is inconsistent or the claim fails to improve after several iterations, human annotators must intervene.

### Mechanism 2
- Claim: RAG-based validation effectively filters hallucinated claims introduced during the modification phase.
- Mechanism: The pipeline retrieves full Wikipedia articles of referenced entities and validates that the modified claims are factually supported by the retrieved context.
- Core assumption: RAG can reliably verify the factual accuracy of generated claims by grounding them in source documents.
- Evidence anchors:
  - [section 3.2] "we use a retrieval-augmented generation (RAG) (Lewis et al., 2020)-based pipeline to retrieve the full Wikipedia articles of the relevant entities and validate the factual accuracy of the modified claims"
  - [section 3.3] "To eliminate potential factual errors, we use a retrieval-augmented generation (RAG)-based pipeline to retrieve the full Wikipedia articles"
- Break condition: If the RAG retriever fails to find relevant context or the validation model is uncertain, the claim must be flagged for human review.

### Mechanism 3
- Claim: Human annotators can effectively quality-control LLM-generated claims through structured scoring.
- Mechanism: Annotators score each claim on fluency, correctness, and clarity using a 1-5 scale, and claims below a threshold are manually rewritten.
- Core assumption: Structured scoring rubrics enable consistent human evaluation of claim quality.
- Evidence anchors:
  - [section 3.3] "we ask a group of human annotators to score the claims based on their fluency, correctness, and clearness, scoring each dimension on a scale of 1 to 5"
  - [section 3.3] "Once the claims are scored, the average of the fluency, correctness, and clarity scores is calculated to determine the final score for each claim"
- Break condition: If annotator agreement is low or scores consistently fall below threshold, the generation pipeline needs adjustment.

## Foundational Learning

- Concept: Multi-hop reasoning requires aggregating evidence across multiple sources
  - Why needed here: The dataset requires models to verify claims that need evidence from multiple modalities
  - Quick check question: How many evidence sources does a 3-hop claim typically require according to the paper?

- Concept: Multimodal integration involves processing and combining information from text, images, and tables
  - Why needed here: The claims require reasoning over evidence from diverse sources including text, images, and tables
  - Quick check question: What are the three modalities of evidence used in MMCV?

- Concept: In-context learning enables LLMs to perform tasks without fine-tuning
  - Why needed here: The pipeline uses in-context examples to guide LLMs in converting QA pairs to claims
  - Quick check question: How many in-context examples does the pipeline use during claim generation?

## Architecture Onboarding

- Component map: Claim Generation (LLM → QA pairs → claims) → Claim Refinement (LLM judge → iterative feedback) → RAG Validation (retrieval → fact-checking) → Human Annotation (scoring → threshold-based review)
- Critical path: Generation → Refinement → Validation → Annotation → Dataset
- Design tradeoffs: LLM-based automation vs human oversight, claim complexity vs generation feasibility, open-book vs closed-book evaluation settings
- Failure signatures: Hallucinated claims (RAG fails), low-quality claims (human scores below threshold), inconsistent reasoning (model confidence vs accuracy mismatch)
- First 3 experiments:
  1. Test claim generation pipeline with different in-context example sets to optimize quality
  2. Compare RAG validation effectiveness against human fact-checking on a sample
  3. Evaluate human annotation agreement and threshold calibration on initial claim batches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of MLLMs change if the dataset included claims with more than 4 reasoning hops?
- Basis in paper: [explicit] The paper notes that performance degrades as the number of reasoning hops increases, with models achieving only 56-72% F1 scores on 3-4 hop claims, but does not test beyond 4 hops.
- Why unresolved: The dataset only includes claims up to 4 hops, limiting the ability to assess model performance on more complex reasoning tasks.
- What evidence would resolve it: Creating a dataset with claims requiring 5 or more reasoning hops and evaluating MLLM performance on this extended dataset.

### Open Question 2
- Question: What is the impact of different prompt strategies on MLLM performance for multimodal multi-hop reasoning?
- Basis in paper: [explicit] The paper tests several prompt strategies (chain-of-thought, self-ask, symbolic-guided reasoning) but notes that performance varies significantly across hop counts, suggesting room for optimization.
- Why unresolved: The paper only tests a limited set of prompt strategies and does not explore their full potential or combinations.
- What evidence would resolve it: Systematic testing of a wider range of prompt strategies and their combinations on the MMCV dataset.

### Open Question 3
- Question: How does the performance of MLLMs compare to humans on claims requiring cross-modal reasoning versus within-modal reasoning?
- Basis in paper: [explicit] The paper establishes human performance benchmarks but does not analyze the difference in performance between cross-modal and within-modal reasoning tasks.
- Why unresolved: The paper does not provide a detailed breakdown of human performance on different types of reasoning tasks within the dataset.
- What evidence would resolve it: Analyzing human performance separately on claims requiring cross-modal reasoning versus those requiring reasoning within a single modality.

## Limitations

- Potential bias in dataset generation from LLM-based claim creation
- Small human evaluation sample size (200 samples) compared to full dataset (15k claims)
- Closed-book evaluation setting may not reflect realistic use cases for claim verification

## Confidence

**High Confidence**: The dataset generation methodology and the overall finding that model performance degrades with increased hop complexity are well-supported by the evidence provided.

**Medium Confidence**: The effectiveness of the LLM-based claim generation and refinement pipeline is supported by the paper's claims, but independent verification would strengthen confidence.

**Low Confidence**: The generalizability of the results to other domains or claim types, given that the dataset is constructed entirely through LLM-based methods.

## Next Checks

1. Generate a small subset of claims using human annotators following the same structured format and compare the reasoning complexity and types of multi-hop challenges against the LLM-generated claims to assess potential bias in the dataset.

2. Conduct a comprehensive evaluation of all three models in open-book settings with full access to supporting evidence, measuring both absolute performance and the gap between closed-book and open-book results to better understand the practical utility of the models.

3. Perform detailed error analysis on model predictions for 3-4 hop claims, categorizing errors by type (e.g., incorrect evidence retrieval, faulty reasoning, hallucination) to identify specific failure modes and guide future model improvements.