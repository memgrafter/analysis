---
ver: rpa2
title: Towards a World-English Language Model for On-Device Virtual Assistants
arxiv_id: '2403.18783'
source_url: https://arxiv.org/abs/2403.18783
tags:
- fofe
- adapter
- adapters
- dialects
- mixture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work combines regional variants of English into a single "World
  English" neural network language model (NNLM) for on-device virtual assistants.
  The key insight is that adapter bottlenecks, which add a small number of parameters,
  are more effective than fully specialized sub-networks for modeling dialect-specific
  characteristics.
---

# Towards a World-English Language Model for On-Device Virtual Assistants

## Quick Facts
- arXiv ID: 2403.18783
- Source URL: https://arxiv.org/abs/2403.18783
- Authors: Rricha Jalota; Lyan Verwimp; Markus Nussbaum-Thom; Amr Mousa; Arturo Argueta; Youssef Oualil
- Reference count: 0
- One-line primary result: Adapter bottlenecks improve multi-dialect English language model accuracy by 1.63-3.72% while meeting on-device constraints

## Executive Summary
This paper presents a World-English neural network language model (NNLM) that combines three regional English dialects (en_US, en_GB, en_IN) into a single on-device model for virtual assistants. The key innovation is using adapter bottlenecks rather than fully specialized sub-networks to model dialect-specific characteristics, achieving significant accuracy improvements while maintaining strict latency and memory constraints. The proposed architecture combines a common application adapter with dual dialect adapters, demonstrating that shared representations with lightweight dialect adaptations outperform separate models for each dialect.

## Method Summary
The authors combine three regional English dialects into a single World-English NNLM using a FOFE-based architecture with adapter bottlenecks. They experiment with three training strategies for adapters: randomly-initialized adapters (RI-A), adapter pretraining (PT-A), and adapter finetuning (FT-A). The proposed architecture adds application-dependent sub-networks with a Common Application Adapter (CAA) in parallel to dual dialect adapters, creating a parameter-efficient model that balances shared representations with dialect-specific adaptations. The model is trained on 36B words (12B per dialect) with a 150k word vocabulary.

## Key Results
- World-English model improves accuracy by 1.63% on head-heavy test sets and 3.72% on tail entities compared to single-dialect baselines
- Adapter bottlenecks outperform fully specialized sub-networks while adding less than 0.5% more parameters
- The proposed AD+CAA+DA architecture matches the latency and memory requirements of existing on-device models
- Dual adapters are most effective when placed before the projection layer with compression dimension k=96

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adapter bottlenecks are more effective than fully specialized sub-networks for modeling dialect-specific characteristics.
- Mechanism: Adapter bottlenecks project high-dimensional features into a smaller bottleneck dimension (k=96), learning dialect-specific transformations that can be added to the base model without disrupting shared representations.
- Core assumption: Dialect-specific characteristics can be effectively captured by low-dimensional projections without losing important information.
- Evidence anchors:
  - [abstract] "We find that adapter modules are more effective in modeling dialects than specializing entire sub-networks."
  - [section] "We observe that adding only a single dialect-specific adapter before the projection layer, i.e. placement (i) in Sec. 2.2, with a compression dimension of 96 (< 0.5% more parameters) is more effective than adding multiple adapters to the architecture."

### Mechanism 2
- Claim: The Common Application Adapter (CAA) enables joint learning of applications across dialects while preserving application-agnostic traits.
- Mechanism: The CAA processes shared feature representations from feedforward layers in parallel to application sub-networks, learning patterns common across all dialects that are combined with dialect-specific adaptations.
- Core assumption: Application-agnostic traits exist and can be effectively learned by the CAA while complementing dialect-specific adaptations.
- Evidence anchors:
  - [section] "This is followed by application-dependent sub-networks and a Common Application Adapter (CAA), which is added in parallel to the sub-networks."
  - [section] "We hypothesize, this higher accuracy might be due to the shared representation of applications in Mixture FOFE."

### Mechanism 3
- Claim: FOFE-based architecture provides better accuracy-latency trade-off for on-device VAs compared to transformers.
- Mechanism: FOFE uniquely encodes word-order information using a recursive formula with a forgetting factor, enabling feedforward networks to model long-term dependencies without self-attention computational overhead.
- Core assumption: FOFE encoding sufficiently captures necessary long-term dependencies for VA tasks without complex attention mechanisms.
- Evidence anchors:
  - [section] "We prefer FOFE-based models over transformer-based models since they have better accuracy-latency trade-off for our two applications [1]: Speech-to-Text (STT) and Assistant."
  - [section] "The FOFE method [13] uniquely encodes word-order information using a recursive formula with a forgetting factor, thereby enabling a feedforward neural network (FNN) to model long-term dependencies."

## Foundational Learning

- Concept: Adapter modules and bottleneck architectures
  - Why needed here: Understanding how adapters work is crucial for implementing the adapter-based improvements proposed in the paper.
  - Quick check question: What is the purpose of the down-projection and up-projection in an adapter module, and how does this differ from a regular neural network layer?

- Concept: FOFE (Fixed-size Ordinally-Forgetting Encoding) method
  - Why needed here: The FOFE method is the foundation of the language model architecture used in this work, and understanding its mechanism is essential for grasping the overall approach.
  - Quick check question: How does the FOFE method encode word-order information, and why is this beneficial for feedforward neural networks?

- Concept: On-device model constraints (accuracy, latency, memory)
  - Why needed here: The paper focuses on building a model that meets specific on-device constraints, so understanding these constraints and their trade-offs is crucial for evaluating the proposed architecture.
  - Quick check question: What are the typical accuracy, latency, and memory requirements for on-device VAs, and how do these requirements influence model design decisions?

## Architecture Onboarding

- Component map:
  Word embeddings → FOFE layer → Block of L feedforward layers → Application-dependent sub-networks + CAA → Dual adapters → Projection heads → Output

- Critical path:
  Word embeddings → FOFE layer → Block of L feedforward layers → Application-dependent sub-networks + CAA → Dual adapters → Projection heads → Output

- Design tradeoffs:
  - Accuracy vs. latency: Using FOFE-based architecture instead of transformers for better latency, but potentially sacrificing some accuracy.
  - Model size vs. performance: Using adapter bottlenecks to add dialect-specific modeling without significantly increasing overall model size.
  - Shared vs. specialized representations: Balancing use of shared components (CAA) with dialect-specific adapters to optimize for both accuracy and efficiency.

- Failure signatures:
  - High latency: Model not meeting on-device latency requirements, possibly due to inefficient implementation or overly complex components.
  - Memory overflow: Model size exceeds available memory on target device, potentially due to insufficient parameter sharing or overly large adapter dimensions.
  - Poor accuracy: Model not achieving desired accuracy, which could be due to inadequate modeling of dialect-specific characteristics or insufficient shared representations.

- First 3 experiments:
  1. Implement a basic adapter module with random initialization and test its effectiveness on a single dialect before extending to multiple dialects.
  2. Compare performance of proposed AD+CAA+DA architecture with baseline AD FOFE model on small dataset to verify accuracy improvements.
  3. Measure latency and memory usage of proposed architecture on target device to ensure it meets on-device constraints.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do adapter bottlenecks compare to other parameter-efficient methods (like prompt tuning or low-rank adaptation) for modeling dialect-specific characteristics in on-device NNLMs?
- Basis in paper: [explicit] The paper investigates adapter bottlenecks as a parameter-efficient method and compares them to fully specialized sub-networks, but does not compare to other parameter-efficient techniques.
- Why unresolved: The paper focuses solely on adapter bottlenecks without benchmarking against alternative parameter-efficient methods that have gained popularity in recent years.
- What evidence would resolve it: Comparative experiments measuring accuracy, latency, and memory usage of adapter bottlenecks against prompt tuning, LoRA, and other parameter-efficient methods in the same World English NNLM architecture.

### Open Question 2
- Question: How would the proposed architecture perform when extended to languages with more substantial dialectal differences than the three English variants studied?
- Basis in paper: [inferred] The paper mentions that all three dialects (en_US, en_GB, en_IN) are high-resourced and treats them similarly, without examining how the architecture scales to languages with more divergent dialects.
- Why unresolved: The paper does not test the architecture on languages with greater dialectal diversity or investigate whether the balance between common and dialect-specific components would need adjustment.
- What evidence would resolve it: Experiments applying the architecture to language families with more substantial dialectal variation (e.g., Arabic, Chinese, or Spanish) and measuring changes in accuracy-latency-memory trade-offs.

### Open Question 3
- Question: What is the impact of the forgetting factor parameter in the FOFE layer on the effectiveness of adapter bottlenecks in multi-dialect modeling?
- Basis in paper: [explicit] The paper uses FOFE-based architectures but does not investigate how the forgetting factor (which controls the balance between short-term and long-term dependencies) interacts with adapter training and effectiveness.
- Why unresolved: The interaction between FOFE's recursive encoding mechanism and adapter learning dynamics is not explored, leaving uncertainty about optimal parameter choices.
- What evidence would resolve it: Systematic experiments varying the forgetting factor parameter while training adapter-equipped models and measuring their impact on dialect modeling performance and convergence behavior.

### Open Question 4
- Question: How does the proposed World English NNLM architecture affect user satisfaction and task completion rates in real-world deployment compared to single-dialect models?
- Basis in paper: [inferred] The paper focuses on technical metrics (WER, latency, memory) but does not report on user experience metrics or actual usage patterns in production.
- Why unresolved: While the architecture shows improvements in technical metrics, there is no evidence about how these improvements translate to user satisfaction, task completion, or acceptance of a single model for multiple dialects.
- What evidence would resolve it: A/B testing in production environments measuring user satisfaction scores, task completion rates, user preference studies, and analysis of error patterns that impact user experience.

## Limitations
- The paper demonstrates improvements on anonymized user requests but lacks evidence of performance in real-world deployment scenarios with diverse user populations.
- Evaluation focuses primarily on accuracy metrics without detailed analysis of potential bias across the three dialects or consideration of generalization to other English variants.
- Latency measurements are reported relative to a baseline rather than providing absolute values, making it difficult to assess whether the model truly meets on-device constraints across different hardware platforms.

## Confidence
- High Confidence: The effectiveness of adapter bottlenecks for dialect modeling is well-supported by both theoretical reasoning and empirical results.
- Medium Confidence: The claim that FOFE-based architecture provides superior accuracy-latency trade-offs is supported by authors' prior work but lacks direct experimental validation against transformer alternatives.
- Low Confidence: The generalizability of the approach to additional English dialects or other languages remains unproven, as the paper only evaluates three variants of English.

## Next Checks
1. Conduct a comprehensive fairness audit across demographic groups within each dialect to ensure the World-English model does not introduce systematic biases against any regional variant, particularly focusing on tail entities which showed the largest accuracy improvements.

2. Evaluate the model's performance on unseen English dialects (e.g., Australian, South African, or Caribbean English) to assess true generalization capabilities of the shared architecture and identify which components contribute most to cross-dialect robustness.

3. Deploy the model on representative edge hardware (e.g., mobile devices with varying computational capabilities) to measure actual latency, memory usage, and battery impact under realistic usage patterns, including concurrent multi-application scenarios.