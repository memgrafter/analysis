---
ver: rpa2
title: Reassessing How to Compare and Improve the Calibration of Machine Learning
  Models
arxiv_id: '2406.04068'
source_url: https://arxiv.org/abs/2406.04068
tags:
- calibration
- confidence
- kernel
- error
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates issues in the reporting of calibration metrics
  in machine learning, particularly for deep learning models. It identifies that many
  papers report only calibration error (e.g., Expected Calibration Error) and accuracy,
  which can be misleading because trivial recalibration strategies like "mean replacement"
  (MRR) can appear state-of-the-art on these metrics while performing poorly on generalization
  measures like negative log-likelihood (NLL) or mean-squared error (MSE).
---

# Reassessing How to Compare and Improve the Calibration of Machine Learning Models

## Quick Facts
- arXiv ID: 2406.04068
- Source URL: https://arxiv.org/abs/2406.04068
- Reference count: 40
- Many papers report only calibration error and accuracy, which can be misleading

## Executive Summary
This paper identifies a critical issue in machine learning calibration reporting: when only calibration error (like ECE) and accuracy are reported, trivial recalibration strategies like "mean replacement recalibration" (MRR) can appear state-of-the-art while actually performing poorly on proper scoring rules like negative log-likelihood or mean squared error. The authors propose using Bregman divergences as a framework that naturally pairs calibration error with a sharpness term, and introduce calibration-sharpness diagrams that visualize both calibration and generalization simultaneously. These diagrams reveal trade-offs invisible in traditional reliability diagrams.

## Method Summary
The authors investigate calibration reporting practices and identify that reporting only ECE and accuracy can be misleading. They propose a Bregman divergence framework where calibration error is paired with a sharpness term. For visualization, they introduce calibration-sharpness diagrams showing three components: the calibration curve, the sharpness gap (difference between total divergence and calibration error), and confidence density. The method uses kernel regression with Gaussian kernel (bandwidth 0.05) to estimate conditional expectations. Experiments are conducted on ImageNet and CIFAR datasets using pretrained vision models (ViT, ResNet-50, EfficientNet, ConvNeXt) with four recalibration methods: temperature scaling, histogram binning, isotonic regression, and mean replacement recalibration.

## Key Results
- Trivial recalibration approaches like MRR can appear state-of-the-art when only calibration error and accuracy are reported, but perform poorly on generalization metrics like NLL or MSE
- Confidence calibration error is a lower bound for full calibration error when using Bregman divergences
- Calibration-sharpness diagrams reveal trade-offs between calibration and generalization invisible in traditional reliability diagrams
- The sharpness gap band disappears or becomes negative when kernel bandwidth is too large or estimator is inconsistent

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Trivial recalibration strategies like MRR can appear state-of-the-art when only calibration error and accuracy are reported, but perform poorly on generalization metrics like NLL or MSE
- **Mechanism**: MRR achieves zero calibration error by replacing predicted confidences with the mean calibration accuracy, which doesn't change test accuracy but destroys the probabilistic structure needed for proper scoring rules
- **Core assumption**: The calibration error metric alone is insufficient to detect trivial recalibration strategies that don't affect classification accuracy
- **Evidence anchors**:
  - [abstract] "trivial recalibration approaches like 'mean replacement' (MRR) can appear state-of-the-art on these metrics while performing poorly on generalization measures like negative log-likelihood (NLL)"
  - [section 3] "By separating the confidence function from the classification function, we have made it so that the test accuracy remains unchanged"
- **Break condition**: When both calibration error and proper scoring rule metrics (NLL/MSE) are reported together

### Mechanism 2
- **Claim**: Confidence calibration error is a lower bound for full calibration error when using Bregman divergences
- **Mechanism**: The conditional expectation structure of Bregman divergences ensures that the 1-D confidence calibration problem cannot underestimate the true multi-class calibration error
- **Core assumption**: The Bregman divergence applied to confidence calibration satisfies a data processing inequality that relates it to the full multi-class divergence
- **Evidence anchors**:
  - [section 4.1] "we prove novel results regarding the relationship between full calibration error and confidence calibration error for Bregman divergences"
  - [Lemma 4.6] "E[dϕ(E[πY|X | g(X)], g(X)) | h(X)] ≥ dφ(E[1Y=c(X) | h(X)], h(X))"
- **Break condition**: When the Bregman divergence structure is violated or when using non-Bregman calibration metrics

### Mechanism 3
- **Claim**: Calibration-sharpness diagrams can reveal tradeoffs between calibration and generalization that are invisible in traditional reliability diagrams
- **Mechanism**: By visualizing both the calibration curve and the sharpness gap (the difference between total divergence and calibration error) alongside confidence density, the diagrams show where improvements in calibration come at the cost of generalization
- **Core assumption**: The sharpness gap provides meaningful information about model generalization that can be efficiently estimated using kernel regression
- **Evidence anchors**:
  - [section 4.2] "we propose an extension to reliability diagrams for jointly visualizing calibration and generalization"
  - [section 5.2] "the calibration-sharpness diagrams allow us to compare generalization while providing a more granular view of calibration performance"
- **Break condition**: When the kernel regression estimation is inconsistent or when the confidence density is uniform across all predictions

## Foundational Learning

- **Concept**: Bregman divergences and their decomposition properties
  - Why needed here: The paper's theoretical framework relies on decomposing Bregman divergences into calibration and sharpness terms to motivate which metrics to report together
  - Quick check question: What is the Bregman divergence decomposition formula and how does it separate calibration from sharpness?

- **Concept**: Confidence calibration vs full calibration
  - Why needed here: The paper shows how confidence calibration (a 1-D problem) can be used to approximate full calibration (a high-dimensional problem) for Bregman divergences
  - Quick check question: How does the data processing inequality apply to show that confidence calibration error is a lower bound for full calibration error?

- **Concept**: Kernel regression consistency
  - Why needed here: The visualization method relies on consistent estimation of conditional expectations using kernel regression to plot the sharpness gap
  - Quick check question: Under what conditions does the Nadaraya-Watson kernel regression estimator provide consistent estimates of conditional expectations?

## Architecture Onboarding

- **Component map**: Pretrained models -> Metric computation module -> Kernel regression estimation -> Visualization engine -> Calibration-sharpness diagrams

- **Critical path**: 
  1. Load model predictions and true labels
  2. Compute calibration error metrics (ECE, NLL, MSE)
  3. Estimate conditional expectations using kernel regression
  4. Calculate sharpness gap from Bregman divergence decomposition
  5. Generate visualization with all three components

- **Design tradeoffs**:
  - Memory vs accuracy: Subsampling for large datasets vs using full data
  - Kernel bandwidth selection: Too small causes overfitting, too large causes oversmoothing
  - Visualization clarity vs information density: Balancing detail with interpretability

- **Failure signatures**:
  - Sharpness gap band disappears or becomes negative: Kernel bandwidth too large or estimator inconsistent
  - Confidence density shows spikes at extremes: Model is overconfident or underconfident systematically
  - Calibration curve deviates significantly from diagonal: Model has systematic calibration issues

- **First 3 experiments**:
  1. Apply calibration-sharpness diagram to a pretrained model without recalibration to establish baseline
  2. Compare temperature scaling vs histogram binning using the visualization to reveal tradeoffs
  3. Test MRR on a calibrated model to demonstrate how it appears perfect in calibration metrics but fails in generalization

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but highlights the need for better visualization and reporting practices in calibration evaluation.

## Limitations
- The relationship between confidence calibration error and full calibration error as a lower bound holds specifically for Bregman divergences, which may not generalize to all calibration metrics commonly used in practice
- The visualization method relies on kernel regression consistency, but the paper does not extensively validate that the chosen bandwidth (0.05) is optimal across different model architectures and datasets
- The proposed framework requires reporting multiple metrics (calibration error plus a proper scoring rule), which may increase the burden on researchers but doesn't fundamentally solve the problem of metric cherry-picking

## Confidence
- **High confidence**: The mechanism by which MRR can appear perfect in calibration metrics while failing in generalization is clearly demonstrated and theoretically sound
- **Medium confidence**: The lower bound relationship between confidence calibration error and full calibration error for Bregman divergences is proven but may have limited practical impact if researchers continue using non-Bregman metrics
- **Medium confidence**: The calibration-sharpness diagrams provide valuable additional insight, though the effectiveness depends on proper kernel bandwidth selection and may not scale well to very large datasets

## Next Checks
1. Test whether the confidence calibration error consistently underestimates full calibration error across a broader range of Bregman divergences and datasets, including regression tasks
2. Validate the calibration-sharpness diagram methodology on synthetic datasets where the true calibration and generalization tradeoff is known, to assess whether the visualization accurately captures these tradeoffs
3. Implement and evaluate alternative kernel regression methods (e.g., cross-validated bandwidth selection) to determine if the 0.05 bandwidth choice is robust or dataset-dependent