---
ver: rpa2
title: 'SAGE: Scalable Ground Truth Evaluations for Large Sparse Autoencoders'
arxiv_id: '2410.07456'
source_url: https://arxiv.org/abs/2410.07456
tags:
- task
- feature
- features
- saes
- supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of evaluating sparse autoencoders
  (SAEs) in large language models without access to ground truth features. Current
  methods either use synthetic tasks with predefined ground truth or rely on extensive
  prior knowledge of real tasks, limiting both generalizability and scalability.
---

# SAGE: Scalable Ground Truth Evaluations for Large Sparse Autoencoders

## Quick Facts
- arXiv ID: 2410.07456
- Source URL: https://arxiv.org/abs/2410.07456
- Reference count: 40
- Authors: Constantin Venhoff; Anisoara Calinescu; Philip Torr; Christian Schroeder de Witt
- Primary result: Introduces SAGE framework for scalable SAE evaluation without ground truth access, achieving sufficiency/necessity scores >0.9 and >80% sparse controllability success

## Executive Summary
This paper addresses the challenge of evaluating sparse autoencoders (SAEs) in large language models when ground truth features are unavailable. Traditional evaluation methods either rely on synthetic tasks with predefined ground truth or require extensive prior knowledge of real tasks, limiting both generalizability and scalability. The authors introduce SAGE, a framework that automatically discovers task-specific activation cross-sections using attribution-based circuit discovery and derives supervised feature dictionaries as approximate ground truth features at these locations.

The core innovation enables evaluation using only residual stream SAEs rather than requiring SAEs trained on every sublayer activation location, significantly reducing computational overhead. The framework is validated across multiple models (Pythia70M, GPT-2 Small, Gemma-2-2B) and tasks (Indirect Object Identification and induction tasks), demonstrating high reconstruction accuracy and superior sparse controllability compared to traditional SAE evaluation methods.

## Method Summary
SAGE addresses SAE evaluation by automatically discovering attribute cross-sections in the model's computational graph where task-relevant features are linearly represented. The method computes supervised feature dictionaries at these cross-sections to serve as ground truth, then evaluates SAEs by comparing their ability to reconstruct and control these features against the supervised dictionary. To reduce training overhead, SAGE introduces a novel projection-based reconstruction method that enables evaluation using only residual stream SAEs rather than requiring SAEs trained on every sublayer activation location. The framework validates that discovered feature dictionaries achieve high reconstruction accuracy (sufficiency and necessity scores above 0.9) and superior sparse controllability compared to SAEs, demonstrating scalability across multiple models and task types while significantly reducing computational burden.

## Key Results
- Achieved sufficiency and necessity scores above 0.9 for supervised feature dictionaries
- Demonstrated over 80% success rate in sparse controllability feature edits
- Validated framework across three models (Pythia70M, GPT-2 Small, Gemma-2-2B) and two task types
- Reduced computational burden by requiring only residual stream SAEs instead of sublayer-specific SAEs

## Why This Works (Mechanism)
The framework works by leveraging attribution-based circuit discovery to automatically identify where task-relevant features are linearly represented in the model's computational graph. By computing supervised feature dictionaries at these cross-sections, SAGE creates a reliable ground truth that can be used to evaluate SAE reconstruction and control capabilities. The projection-based reconstruction method enables evaluation using only residual stream SAEs, dramatically reducing the computational overhead compared to training SAEs on every sublayer activation.

## Foundational Learning

**Attribution-based circuit discovery**: Understanding how attribution methods identify relevant model components for specific tasks is crucial for SAGE's cross-section discovery. Quick check: Verify that attribution scores correctly highlight task-relevant pathways in simple circuits before scaling up.

**Linear feature representation**: SAGE assumes task-relevant features can be linearly represented at discovered cross-sections. Quick check: Test whether simple linear probes can recover known features at candidate cross-sections.

**Sparse controllability**: The ability to precisely edit model behavior through sparse feature manipulation is central to SAE evaluation. Quick check: Confirm that known feature edits produce predictable changes in model outputs.

## Architecture Onboarding

**Component map**: Attribution methods -> Cross-section discovery -> Supervised feature dictionary computation -> SAE evaluation (residual stream only) -> Sufficiency/necessity/sparse controllability metrics

**Critical path**: Cross-section discovery → Supervised feature dictionary → SAE evaluation. The entire pipeline depends on accurate cross-section identification, as errors here propagate to all downstream metrics.

**Design tradeoffs**: SAGE trades computational efficiency (single residual stream SAE) for potential accuracy loss compared to sublayer-specific SAEs. The attribution-based discovery may miss non-linear or distributed features.

**Failure signatures**: Poor cross-section discovery leads to low sufficiency/necessity scores and unreliable SAE evaluations. If SAEs consistently outperform supervised dictionaries on controllability metrics, the ground truth may be incomplete or noisy.

**First experiments**: 1) Apply attribution methods to simple toy models with known circuits to verify discovery accuracy. 2) Test projection-based reconstruction on residual stream SAEs for sublayer activations. 3) Validate sufficiency/necessity metrics on synthetic ground truth features embedded in models.

## Open Questions the Paper Calls Out

**Open Question 1**: How does the choice of metric L (e.g., logit difference vs. other metrics) impact the effectiveness of attribution-based circuit discovery in identifying cross-sections? The paper mentions using logit difference but doesn't explore alternative metrics or their impact on downstream evaluation accuracy.

**Open Question 2**: What is the impact of the number of patching examples used in attribution patching on the quality and stability of discovered cross-sections? The paper uses 250 task prompts but lacks systematic exploration of how varying this number affects results.

**Open Question 3**: How does the performance of SAGE scale with increasingly complex tasks that involve higher-order interactions between attributes? The paper demonstrates SAGE on simple tasks but doesn't test more complex, multi-attribute interactions.

## Limitations

- Dependence on finding linear representations of task-relevant features limits applicability to tasks with non-linear feature interactions
- Claims about seamless scalability to new tasks and models exceed what limited experimental validation can support
- Attribution-based circuit discovery may miss important features or include spurious correlations, compromising the entire evaluation framework

## Confidence

**High Confidence**: Technical feasibility of using supervised feature dictionaries as ground truth for SAE evaluation is sound; general three-step framework is well-specified; claims about reducing training overhead through residual stream SAEs are directly supported.

**Medium Confidence**: Claims about achieving sufficiency/necessity scores above 0.9 and over 80% sparse controllability success are credible given experimental setup but depend heavily on quality of discovered cross-sections and supervised dictionaries.

**Low Confidence**: Claims that SAGE can seamlessly scale to "new tasks, models, and feature distributions" exceed what limited experimental validation can support; generalization to more complex tasks or larger models remains unproven.

## Next Checks

1. **Cross-task generalization test**: Apply SAGE to significantly different task types (e.g., multi-step reasoning or code generation) to verify whether attribution-based circuit discovery reliably identifies task-relevant cross-sections beyond tested IOI and simple induction tasks.

2. **Ground truth validation**: Conduct ablation studies where synthetic ground truth features are embedded into models, then evaluate whether SAGE can recover them accurately to provide stronger evidence that discovered supervised dictionaries truly capture ground truth rather than learned correlations.

3. **Computational overhead analysis**: Measure and compare full computational cost of SAGE (including circuit discovery) against traditional methods that train SAEs on multiple sublayers to verify claimed efficiency gains in practical deployment scenarios.