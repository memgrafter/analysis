---
ver: rpa2
title: 'Feature contamination: Neural networks learn uncorrelated features and fail
  to generalize'
arxiv_id: '2406.03345'
source_url: https://arxiv.org/abs/2406.03345
tags:
- feature
- activation
- every
- features
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a new fundamental cause of out-of-distribution
  (OOD) generalization failure in deep neural networks, termed "feature contamination."
  The authors show that during training, neural networks simultaneously learn both
  predictive core features and uncorrelated background features, even when background
  features have no relationship to the label. This coupling of features in neurons'
  pre-activation leads to OOD performance degradation when background features shift.
---

# Feature contamination: Neural networks learn uncorrelated features and fail to generalize

## Quick Facts
- arXiv ID: 2406.03345
- Source URL: https://arxiv.org/abs/2406.03345
- Authors: Tianren Zhang; Chujie Zhao; Guanyu Chen; Yizhou Jiang; Feng Chen
- Reference count: 40
- Primary result: Neural networks simultaneously learn predictive core features and uncorrelated background features during training, causing OOD generalization failure

## Executive Summary
This paper identifies a new fundamental cause of out-of-distribution (OOD) generalization failure in deep neural networks, termed "feature contamination." The authors show that during training, neural networks simultaneously learn both predictive core features and uncorrelated background features, even when background features have no relationship to the label. This coupling of features in neurons' pre-activation leads to OOD performance degradation when background features shift. The theoretical analysis of two-layer ReLU networks trained with SGD demonstrates this phenomenon differs from spurious correlation explanations and is unique to non-linear networks (linear networks avoid this issue).

## Method Summary
The paper combines theoretical analysis with empirical validation. Theoretically, it analyzes two-layer ReLU networks trained with SGD and weight decay, tracking how neuron weights evolve and how they accumulate both core and background features. Empirically, the authors validate their findings using synthetic data with orthogonal core and background features, and real-world datasets (ImageNet, iWildCam, Camelyon17, DomainNet). They also conduct representation distillation experiments using CLIP models to test whether better pre-trained representations can overcome feature contamination.

## Key Results
- Neural networks learn uncorrelated background features alongside predictive core features during training, even when background features are independent of labels
- Feature contamination creates coupling between core and background features in neurons' pre-activation, leading to OOD generalization failure when background features shift
- Linear networks avoid feature contamination because gradient projections onto uncorrelated features cancel out, unlike ReLU networks
- Even with good representations from pre-trained models, student networks still suffer from OOD generalization gaps due to feature contamination

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature contamination occurs when SGD-trained neural networks learn uncorrelated background features alongside predictive core features during training
- Mechanism: During training, neurons in ReLU networks develop asymmetric activation patterns for different classes, causing gradient projections onto uncorrelated features to be non-zero even when those features have no relationship to the label
- Core assumption: The background features are independent of the label and distributed in orthogonal subspaces from core features
- Evidence anchors:
  - [abstract] "neural networks can learn uncorrelated features together with predictive features, resulting in generalization failure under distribution shifts"
  - [section 4] "Activation asymmetry leads to non-zero gradient projections onto background features" and "the gradient projection onto background features for every neuron k ∈ N (t) y satisfies: for every j ∈ Sbg, ⟨−∇w(t) k bL(h(t)), mj⟩ ∝ E(x,y)(1y=ypos − 1y=yneg)1⟨w(t) k ,x⟩≥0zj"
  - [corpus] No direct corpus evidence available
- Break condition: If background features become correlated with the label or if the network architecture prevents asymmetric activation patterns

### Mechanism 2
- Claim: Feature contamination creates coupling between core and background features in neurons' pre-activation, leading to OOD generalization failure
- Mechanism: When background features shift during distribution change, the coupled pre-activation values of neurons are reduced, diminishing the contribution of core features and increasing OOD risk
- Core assumption: The shift in background features directly impacts the pre-activation values of neurons that have learned coupled features
- Evidence anchors:
  - [abstract] "This eventually leads to additional risks under distribution shifts due to the coupling of core and background features in the neurons' pre-activation"
  - [section 4] "With this coupling, negative shifts of background features can reduce the activation of the neuron, resulting in OOD risk"
  - [corpus] No direct corpus evidence available
- Break condition: If background features do not shift between training and test distributions or if the coupling between features is weak

### Mechanism 3
- Claim: Linear networks avoid feature contamination because gradient projections onto uncorrelated features cancel out
- Mechanism: In linear networks without activation functions, positive and negative gradients on uncorrelated features cancel out during SGD updates, preventing accumulation of background features
- Core assumption: The absence of non-linearity in linear networks prevents the asymmetric activation that causes feature contamination in ReLU networks
- Evidence anchors:
  - [abstract] "we formally show that ReLU networks and linear networks are provably different in our setting with the latter exhibiting no such behavior"
  - [section 4] "if we 'remove' the non-linearity in the network by replacing each ReLU with the identity function, then feature contamination will no longer occur"
  - [corpus] No direct corpus evidence available
- Break condition: If linear networks are modified to include non-linear components or if the assumption about gradient cancellation is incorrect

## Foundational Learning

- Concept: Stochastic Gradient Descent (SGD) dynamics and weight decay regularization
  - Why needed here: The paper's theoretical analysis relies on tracking how neuron weights evolve during SGD training with weight decay, showing how feature contamination accumulates over iterations
  - Quick check question: How does the weight decay parameter λ = O( d0 m1.01 ) affect the growth of feature correlations during training?

- Concept: ReLU activation function and its derivative properties
  - Why needed here: The asymmetric activation behavior of ReLU neurons is the key mechanism behind feature contamination, as the activation derivative 1⟨w(t) k ,x⟩≥0 creates different gradient contributions for different classes
  - Quick check question: Why does the ReLU activation function lead to asymmetric activation rates between different classes during training?

- Concept: Berry-Esseen theorem and concentration inequalities
  - Why needed here: The theoretical analysis uses the Berry-Esseen theorem to bound activation probabilities of ReLU neurons and concentration inequalities to track weight evolution during SGD
  - Quick check question: How does the Berry-Esseen theorem help establish the relationship between neuron pre-activation values and activation probabilities in the theoretical analysis?

## Architecture Onboarding

- Component map: Data generation -> Model initialization -> SGD training with gradient tracking -> Feature contamination analysis -> OOD risk quantification
- Critical path: Data generation → Model initialization → SGD training with gradient tracking → Feature contamination analysis → OOD risk quantification
- Design tradeoffs: The choice of orthogonal feature subspaces simplifies analysis but may limit generalizability; the two-layer network architecture balances theoretical tractability with practical relevance
- Failure signatures: High ID accuracy but poor OOD performance despite access to good representations; presence of asymmetric activation patterns in trained networks
- First 3 experiments:
  1. Implement the data generation process with core and background features, train a two-layer ReLU network, and track weight projections onto both feature types during training
  2. Compare feature contamination in ReLU networks versus linear networks by replacing ReLU with identity functions and measuring background feature accumulation
  3. Visualize activation rate histograms for different classes in trained networks to verify asymmetric activation patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does feature contamination manifest in deeper neural networks beyond two-layer architectures, and what are the specific mechanisms that cause this phenomenon to persist or diminish with increased depth?
- Basis in paper: [inferred] The paper analyzes two-layer ReLU networks and shows feature contamination occurs, but also demonstrates that the phenomenon extends to more general settings including deeper networks with various activation functions. The authors note that understanding feature contamination in deeper networks remains an open problem.
- Why unresolved: The theoretical analysis is limited to two-layer networks, and while empirical evidence shows feature contamination occurs in deeper networks, the specific mechanisms by which depth affects this phenomenon remain unexplored. The non-linear interactions between layers could either amplify or mitigate feature contamination in ways that are not yet understood.
- What evidence would resolve it: Systematic experiments comparing feature contamination across different depths, architectures, and activation functions, combined with theoretical analysis of gradient dynamics in deeper networks, would clarify how depth influences feature contamination.

### Open Question 2
- Question: Can the feature contamination phenomenon be mathematically characterized in terms of the network's capacity relative to the number of core and background features, and how does this relationship change with different data distributions?
- Basis in paper: [explicit] The paper notes that feature contamination occurs when the number of both core features and background features is non-negligible, and shows that the phenomenon depends on parameters like m (network width), d0 (feature dimension), and the ratio of core to background features. The authors state that understanding this relationship in more general settings remains open.
- Why unresolved: The theoretical analysis focuses on specific parameter regimes (e.g., m ∈ [Θ(d0), Θ(d)]), but the general relationship between network capacity, feature dimensionality, and feature contamination across different data distributions is not fully characterized. The impact of feature correlation structures and non-orthogonal feature spaces is also unexplored.
- What evidence would resolve it: Mathematical characterization of feature contamination bounds as functions of network capacity and feature space properties, validated through extensive experiments across varying data distributions and network architectures.

### Open Question 3
- Question: What specific architectural modifications or training procedures could provably prevent feature contamination while maintaining or improving generalization performance?
- Basis in paper: [explicit] The paper discusses possible solutions including constraining SGD updates to subspaces that avoid background feature accumulation, and mentions that projecting intermediate representations onto certain subspaces may improve OOD generalization. However, the authors acknowledge that finding effective subspaces without explicit access to core and background feature subspaces remains an open problem.
- Why unresolved: While the paper identifies feature contamination as a cause of OOD generalization failure and suggests potential mitigation strategies, it does not provide concrete architectural modifications or training procedures that provably prevent the phenomenon. The challenge lies in developing methods that can identify and separate core features from background features during training without prior knowledge of the feature structure.
- What evidence would resolve it: Development and rigorous validation of architectural modifications or training algorithms that provably prevent feature contamination, demonstrated through both theoretical guarantees and empirical performance improvements on OOD generalization benchmarks.

## Limitations
- The theoretical analysis is limited to two-layer ReLU networks with specific weight decay schedules, which may not fully capture the behavior of deeper architectures or modern training practices
- The paper assumes background features are completely uncorrelated with labels, but real-world OOD scenarios often involve more complex relationships between features and targets
- The empirical validation relies on synthetic data generation with orthogonal feature spaces, which may oversimplify real-world feature distributions

## Confidence
- **High confidence**: The mechanism of asymmetric activation leading to feature contamination in ReLU networks is well-supported by both theoretical analysis and empirical evidence
- **Medium confidence**: The claim that feature contamination is unique to non-linear networks and does not occur in linear networks, while supported by theory, needs more empirical validation across different network depths
- **Medium confidence**: The assertion that simply having better representation learning objectives is insufficient without considering neural network inductive biases, based on distillation experiments, though the causal relationship could be explored further

## Next Checks
1. **Test deeper architectures**: Validate whether feature contamination persists in deeper ReLU networks beyond the two-layer case studied theoretically
2. **Vary feature correlation structure**: Experiment with partially correlated background features rather than completely uncorrelated ones to assess robustness of the phenomenon
3. **Compare training algorithms**: Investigate whether alternative optimization methods (Adam, SGD with momentum) exhibit the same feature contamination patterns as standard SGD