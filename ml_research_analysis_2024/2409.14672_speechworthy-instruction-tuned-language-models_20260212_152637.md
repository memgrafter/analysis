---
ver: rpa2
title: Speechworthy Instruction-tuned Language Models
arxiv_id: '2409.14672'
source_url: https://arxiv.org/abs/2409.14672
tags:
- responses
- speech
- more
- response
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem that current instruction-tuned
  language models are not well-aligned with the unique requirements of speech-based
  interactions. To tackle this, the authors explore two main approaches: prompt engineering
  grounded in radio-industry best practices and preference learning using a novel
  speech-based preference dataset of 20K samples.'
---

# Speechworthy Instruction-tuned Language Models

## Quick Facts
- arXiv ID: 2409.14672
- Source URL: https://arxiv.org/abs/2409.14672
- Reference count: 33
- Models with combined prompt engineering and preference learning are preferred or tied in 76.2% of head-to-head comparisons

## Executive Summary
This paper addresses the gap between instruction-tuned language models and the unique requirements of speech-based interactions. The authors explore two complementary approaches: prompt engineering grounded in radio-industry best practices and preference learning using a novel speech-based preference dataset. Both methods significantly improve speech-suitability of popular instruction-tuned language models, with the combined approach achieving the best results. The work provides detailed lexical and syntactical analyses to demonstrate how each method enhances the speech-suitability of generated responses.

## Method Summary
The authors develop SPEECH PREF, a novel speech-based preference dataset of 20K samples, and employ both prompt engineering and preference learning to adapt instruction-tuned language models (ITLMs) for speech-suitability. They use radio-industry best practices to design detailed system prompts that guide models toward simpler vocabulary, shorter sentences, and avoidance of non-vocalizable content. These prompts are used both for in-context learning and during preference learning fine-tuning with DPO/PPO. The adapted models are evaluated through human head-to-head comparisons and automatic metrics including readability, sentence complexity, and vocalizable content.

## Key Results
- Prompt engineering and preference learning both significantly improve speech-suitability of ITLMs
- The combined approach achieves the best results, with responses preferred or tied to base model in 76.2% of comparisons
- Automatic evaluations show improvements in Flesch Reading Ease, dependency depth, word count, and nonvocalizable characters
- Lexical and syntactical analyses demonstrate specific improvements in vocabulary simplicity and sentence structure

## Why This Works (Mechanism)

### Mechanism 1
Radio-industry best practices improve speech-suitability by inducing simpler vocabulary, shorter sentences, and avoidance of non-vocalizable content through detailed system prompts.

### Mechanism 2
Preference learning using SPEECH PREF aligns models with actual user preferences for spoken responses by training on human-labeled audio comparisons.

### Mechanism 3
Combining prompt engineering with preference learning yields additive improvements because the prompt provides initial alignment while preference learning refines it.

## Foundational Learning

- **Concept:** Radio-industry best practices for spoken content
  - Why needed here: Provide empirically grounded rules for making text more suitable for speech delivery
  - Quick check question: Can you list three radio best practices that were used to design the system prompt?

- **Concept:** Human preference data collection and annotation
  - Why needed here: Quality and representativeness of SPEECH PREF determine success of preference learning
  - Quick check question: Why did authors require annotators to listen only, not read, responses?

- **Concept:** Preference learning algorithms (DPO/PPO)
  - Why needed here: These methods fine-tune models on SPEECH PREF; understanding mechanics is necessary for implementation
  - Quick check question: What is the key difference between DPO and PPO in how they use preference data?

## Architecture Onboarding

- **Component map:** Base ITLM -> System prompt module -> Response generator -> Speech TTS engine -> Preference dataset collector -> Reward model -> Fine-tuning pipeline

- **Critical path:**
  1. Sample user prompts from filtered Dolly-15K
  2. Generate responses using system prompts and varied ITLM/decoding configs
  3. Convert to speech and collect human preference annotations
  4. Train reward model (PPO) or directly fine-tune with DPO
  5. Evaluate human and automatic metrics

- **Design tradeoffs:** Prompt complexity vs. model generalization; dataset size vs. annotation cost; DPO simplicity vs. PPO potential robustness; TTS voice choice vs. annotation bias

- **Failure signatures:** Prompts produce overly generic responses; preference annotations show low inter-annotator agreement; fine-tuned models overfit to training prompts; automatic metrics don't correlate with human preferences

- **First 3 experiments:**
  1. Run prompt-only variants and compare to base model on human preference test set
  2. Train DPO-ICL model and evaluate on same human test set to confirm additive effect
  3. Run automatic evaluations on all model variants to analyze failure modes

## Open Questions the Paper Calls Out

- **Open Question 1:** What is the impact of multi-turn interactions on speech-suitability preferences for ITLMs compared to single-turn interactions?
- **Open Question 2:** How do acoustic factors like timber, pitch, and speed affect the perceived speech-suitability of ITLM responses?
- **Open Question 3:** Are there cultural or demographic differences in preferences for speech-suitability of ITLM responses?

## Limitations

- Limited scope of evaluation: Only tested Falcon 7B Instruct and OLMo 7B Instruct
- Annotation pipeline constraints: Audio-only preference annotation may introduce TTS voice and audio quality biases
- Automatic metric validity: Limited correlation analysis between automatic metrics and human preferences

## Confidence

**High confidence:** Core finding that both prompt engineering and preference learning improve speech-suitability, and that they combine additively
**Medium confidence:** Specific effectiveness of radio-industry best practices for voice assistant contexts
**Medium confidence:** Claim that SPEECH PREF represents comprehensive speech-based preference dataset

## Next Checks

1. Test combined prompt+preference learning approach on at least two additional base models to verify generalizability
2. Conduct correlation analysis between automatic metrics and human preferences on held-out test set
3. Evaluate model performance on systematically varied prompt types to identify success/failure patterns across interaction patterns