---
ver: rpa2
title: 'Holmes: A Benchmark to Assess the Linguistic Competence of Language Models'
arxiv_id: '2404.18923'
source_url: https://arxiv.org/abs/2404.18923
tags:
- linguistic
- language
- phenomena
- linguistics
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Holmes is a comprehensive benchmark designed to assess the linguistic
  competence of language models by using classifier-based probing to evaluate their
  internal representations across 208 datasets covering five types of linguistic phenomena:
  morphology, syntax, semantics, reasoning, and discourse. The evaluation protocol
  disentangles linguistic competence from linguistic performance, addressing a gap
  in current benchmarks that conflate these aspects.'
---

# Holmes: A Benchmark to Assess the Linguistic Competence of Language Models

## Quick Facts
- arXiv ID: 2404.18923
- Source URL: https://arxiv.org/abs/2404.18923
- Reference count: 40
- Primary result: Comprehensive benchmark using probing to assess linguistic competence across 208 datasets covering five linguistic phenomena types

## Executive Summary
Holmes addresses a critical gap in language model evaluation by disentangling linguistic competence from linguistic performance. Through classifier-based probing on internal representations, the benchmark evaluates 59 diverse language models across 208 datasets covering morphology, syntax, semantics, reasoning, and discourse phenomena. The methodology reveals that linguistic competence scales with model size and architecture, with encoder-only models showing significant advantages over decoder-only models for formal linguistic phenomena. FlashHolmes provides an efficient alternative that maintains ranking accuracy while reducing computational costs by approximately 97%.

## Method Summary
Holmes uses classifier-based probing to assess linguistic competence by training linear probes on frozen language model representations to predict linguistic labels. The evaluation covers 208 datasets spanning five linguistic phenomena types, using task-specific metrics (F1 for classification, correlation for regression). Probe reliability is validated through compression (information efficiency), selectivity (performance vs. control tasks), and standard deviation across seeds. FlashHolmes optimizes efficiency by using 1/32 of training data while maintaining ranking consistency. The approach specifically disentangles competence from performance by examining internal representations rather than generated outputs.

## Key Results
- Linguistic competence scales consistently with model size across both Pythia and T5 model families
- Encoder-only models significantly outperform decoder-only models, particularly for morphology and syntax (52% vs 21% mean winning rate)
- Instruction tuning improves performance on formal phenomena (morphology, syntax) but shows mixed effects on functional phenomena
- FlashHolmes reduces computational cost by 97% while maintaining high ranking accuracy (Kendall-tau correlations > 0.89)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probing-based evaluation disentangles linguistic competence from linguistic performance by examining internal model representations rather than generated outputs.
- Mechanism: The evaluation protocol trains linear probes on the last layer's internal representations to predict linguistic labels, isolating the model's unconscious understanding of linguistic phenomena from its ability to follow instructions or produce coherent text.
- Core assumption: Internal representations encode sufficient information about linguistic phenomena to allow linear probes to achieve meaningful classification performance above random baselines.
- Evidence anchors:
  - [abstract] "Specifically, we use classifier-based probing to examine LMs' internal representations regarding distinct linguistic phenomena (e.g., part-of-speech tagging). As a result, we meet recent calls to disentangle LMs' linguistic competence from other cognitive abilities, such as following instructions in prompting-based evaluations."
  - [section] "With this particular and comprehensive scope, we thoroughly address the initially raised questions as follows: Meta-Study (§ 3) The review of over 270 probing studies reveals a gap in comprehensively evaluating linguistic competence."

### Mechanism 2
- Claim: Linguistic competence scales with model size, particularly for formal phenomena like morphology and syntax.
- Mechanism: Larger models with more parameters better approximate word co-occurrences in nearby contexts, enabling more stable representations of formal linguistic phenomena that depend on local neighborhood information.
- Core assumption: The increased parameter count allows models to capture more complex statistical patterns in training data, specifically those relevant to formal linguistic phenomena.
- Evidence anchors:
  - [abstract] "Analyzing over 50 models reveals that, aligned with known trends, their linguistic competence correlates with model size."
  - [section] "From Figure 12, we observe for both Pythia and T5 that the linguistic competence scales with model size, and it is particularly pronounced after exceeding 0.5B (Pythia) and 1.0B (T5) parameters."

### Mechanism 3
- Claim: Model architecture significantly influences linguistic competence, with encoder-only models outperforming decoder-only ones, especially in morphology and syntax.
- Mechanism: Encoder architectures provide bidirectional context for each token, enabling more stable representations of token-level phenomena that depend on surrounding words, while decoder architectures only have unidirectional context.
- Core assumption: Bidirectional encoding is necessary for stable representations of morphological and syntactic phenomena that rely on local word dependencies.
- Evidence anchors:
  - [abstract] "However, surprisingly, model architecture and instruction tuning also significantly influence performance, particularly in morphology and syntax."
  - [section] "Encoder LMs show a higher mwr of 52% than decoder LMs (21%). This observation is the most saturated for morphology or syntax, encompassing a variety of token-level phenomena, like part-of-speech."

## Foundational Learning

- Concept: Classifier-based probing methodology
  - Why needed here: Holmes relies on probing to assess linguistic competence without conflating it with linguistic performance, requiring understanding of how linear probes on internal representations can reveal model understanding.
  - Quick check question: How does the selectivity metric (S = F1(y, ŷ) - F1(y', ŷ')) help validate that probe performance reflects genuine encoding of linguistic phenomena rather than spurious correlations?

- Concept: Information theory and compression metrics
  - Why needed here: The compression metric (I = u/mdl) quantifies how efficiently linguistic information is encoded in model representations, providing an alternative validation of probe reliability beyond task performance.
  - Quick check question: What does a high compression value indicate about the relationship between model internal representations and the linguistic labels being predicted?

- Concept: Linguistic competence vs. performance distinction
  - Why needed here: Understanding this distinction is crucial for interpreting Holmes results correctly, as the benchmark specifically measures unconscious understanding rather than ability to follow instructions or produce appropriate outputs.
  - Quick check question: Why might a language model perform well on downstream tasks requiring specific linguistic phenomena while showing relatively poor competence on those same phenomena when measured through probing?

## Architecture Onboarding

- Component map: Dataset curation -> LM encoding -> Linear probe training -> Reliability validation -> Result aggregation
- Critical path: For each LM, encode all dataset examples using the model, train probes for each dataset with cross-validation, compute task metrics, validate reliability, aggregate results across datasets and phenomena types
- Design tradeoffs: Comprehensive coverage vs. computational cost (208 datasets vs. FlashHolmes with 1/32 training data), probe simplicity vs. expressiveness (linear probes vs. deeper models), general vs. specific phenomena (broad categories vs. fine-grained distinctions)
- Failure signatures: Low probe performance across many datasets may indicate either poor linguistic competence or insufficient information in internal representations; high selectivity but low task performance suggests encoding exists but is hard to extract; low compression indicates inefficient encoding of linguistic information
- First 3 experiments:
  1. Evaluate a small encoder-only model (e.g., BERT-base) on Holmes to establish baseline performance and verify the probing pipeline works correctly.
  2. Compare encoder vs. decoder performance on morphology/sentence length tasks to validate the architecture hypothesis before scaling to larger models.
  3. Test the FlashHolmes reduction by comparing rankings on a subset of datasets with 1/32 training data against full Holmes results to verify efficiency claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific linguistic phenomena like negation or metaphor detection scale with model size beyond 70 billion parameters?
- Basis in paper: [explicit] The paper analyzes scaling trends up to 70 billion parameters but notes that larger models are underrepresented in current probing studies.
- Why unresolved: The paper only evaluates LMs up to 70 billion parameters and does not explore scaling beyond this range.
- What evidence would resolve it: Evaluating linguistic competence of models larger than 70 billion parameters using Holmes or FlashHolmes would provide insights into scaling trends for specific phenomena.

### Open Question 2
- Question: Does instruction tuning improve linguistic competence uniformly across all linguistic phenomena, or are there trade-offs between formal and functional phenomena?
- Basis in paper: [explicit] The paper finds that instruction tuning improves morphology and syntax but has mixed effects on semantics, reasoning, and discourse.
- Why unresolved: The paper does not explore the underlying mechanisms or trade-offs between different types of phenomena.
- What evidence would resolve it: Detailed analysis of how instruction tuning affects internal representations for different phenomena types could clarify trade-offs.

### Open Question 3
- Question: How does the reliability of probing-based evaluations compare to other evaluation methods like attention-based probing or log-probability measurements?
- Basis in paper: [explicit] The paper compares probing-based evaluations with prompting-based methods and finds discrepancies, but does not explore other evaluation protocols.
- Why unresolved: The paper focuses on classifier-based probing and does not compare it to other emerging evaluation methods.
- What evidence would resolve it: Systematic comparison of probing-based evaluations with attention-based probing or log-probability measurements across diverse LMs and tasks would provide insights into their relative reliability.

## Limitations

- The reliance on linear probes may systematically underestimate linguistic competence for phenomena requiring non-linear encoding
- The benchmark coverage, while comprehensive, may still miss certain linguistic phenomena and low-resource languages
- The computational cost remains substantial even with FlashHolmes optimization, limiting accessibility for researchers with fewer resources
- The architectural findings, while robust for formal phenomena, are less conclusive for functional phenomena like semantics and reasoning

## Confidence

**High Confidence**: Claims about linguistic competence scaling with model size are well-supported by consistent patterns across model families with clear statistical significance.

**Medium Confidence**: Architectural findings are robust for morphology and syntax but less conclusive for other phenomena; the mechanism explaining bidirectional context advantages is plausible but not definitively proven.

**Low Confidence**: Claims about instruction tuning effects on formal vs. functional phenomena require more rigorous testing; compression and selectivity metrics provide useful reliability checks but their interpretation as absolute measures remains somewhat heuristic.

## Next Checks

1. **Probe Linearity Validation**: Conduct systematic tests varying probe complexity (linear vs. non-linear models) across all 208 datasets to quantify how much performance varies with probe architecture.

2. **Cross-Lingual Generalization**: Extend the evaluation framework to include datasets from multiple languages to test whether architectural advantages hold across linguistic typologies.

3. **Fine-Grained Architectural Analysis**: Perform layer-wise probing and ablation studies to identify exactly where in encoder vs. decoder architectures the representational differences emerge.