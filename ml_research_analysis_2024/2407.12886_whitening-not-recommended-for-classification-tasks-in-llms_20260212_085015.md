---
ver: rpa2
title: Whitening Not Recommended for Classification Tasks in LLMs
arxiv_id: '2407.12886'
source_url: https://arxiv.org/abs/2407.12886
tags:
- whitening
- embeddings
- tasks
- embedding
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Whitening transformations, previously shown to improve embeddings
  for semantic textual similarity (STS) tasks, were found to consistently degrade
  performance on classification tasks across multiple models including BERT, SBERT,
  SimCSE, AngleBERT, ChatGPT, AngleLLaMA, LLaMA, and LLaMA2. Experiments conducted
  on seven classification datasets (MR, CR, SUBJ, MPQA, TREC, MRPC, SST-F) showed
  accuracy drops as large as -11 percentage points after whitening.
---

# Whitening Not Recommended for Classification Tasks in LLMs

## Quick Facts
- arXiv ID: 2407.12886
- Source URL: https://arxiv.org/abs/2407.12886
- Reference count: 10
- Whitening degrades classification accuracy across multiple LLMs while improving isotropy

## Executive Summary
This study systematically evaluates whitening transformations on sentence embeddings for classification tasks across multiple large language models. While whitening was previously shown to improve embeddings for semantic textual similarity (STS) tasks, experiments reveal that it consistently degrades classification performance across BERT, SBERT, SimCSE, AngleBERT, ChatGPT, AngleLLaMA, LLaMA, and LLaMA2 models. The degradation can be as large as 11 percentage points, with larger models experiencing greater performance drops. The research also introduces SentEval+, an LLM-friendly embedding evaluation platform.

## Method Summary
The study extracted embeddings from multiple LLMs across seven classification datasets, applied various whitening transformations (PCA, ZCA, PCA-Cor, ZCA-Cor, Cholesky), and evaluated performance using an MLP classifier with no hidden layers. Classification accuracy and isotropy (measured by IsoScore) were compared before and after whitening. The same embeddings were also evaluated on STS tasks for comparison. Embeddings ranged from 768 to 4096 dimensions across different models.

## Key Results
- Whitening consistently degraded classification accuracy across all tested models and datasets
- Performance degradation increased with embedding dimension, reaching -11 percentage points in LLaMA models
- Fine-tuned models showed more pronounced whitening degradation than vanilla models
- Improved isotropy via IsoScore did not translate to better classification results
- Whitened embeddings showed better STS performance on some models (BERT) but not others (fine-tuned models)

## Why This Works (Mechanism)

### Mechanism 1
Whitening improves isotropy but degrades classification performance. Whitening transforms correlated, anisotropic feature representations into uncorrelated, isotropic ones by centering the mean, eliminating covariances, and normalizing variance to an identity matrix. Core assumption: Isotropy directly correlates with classification accuracy. Break condition: When classification tasks require preserved class boundaries that are distorted by whitening's isotropy enforcement.

### Mechanism 2
Model fine-tuning status affects whitening effectiveness. Fine-tuned models like SBERT, SimCSE, AngleBERT, and AngleLLaMA have learned class boundaries that whitening disrupts, while vanilla models like BERT and LLaMA benefit from isotropy improvements. Core assumption: Fine-tuning creates feature representations that rely on specific covariance structures. Break condition: When fine-tuning process preserves isotropy-independent features.

### Mechanism 3
Embedding dimensionality amplifies whitening's negative effects. Higher dimensional embeddings experience greater performance degradation because whitening's variance normalization becomes more aggressive across more dimensions. Core assumption: The relationship between dimensionality and whitening impact is monotonic. Break condition: When dimensionality reduction techniques are applied before whitening.

## Foundational Learning

- Concept: Covariance matrix computation and eigen decomposition
  - Why needed here: Whitening operations require calculating covariance matrices and their eigen decompositions to determine transformation matrices
  - Quick check question: Given a dataset X with mean μ, what is the formula for the covariance matrix Σ?

- Concept: Isotropy metrics and their relationship to task performance
  - Why needed here: Understanding why improved isotropy (measured by IsoScore) doesn't translate to better classification performance requires knowledge of different isotropy metrics
  - Quick check question: What makes IsoScore unique compared to traditional isotropy metrics like average random cosine similarity?

- Concept: Contrastive learning vs supervised fine-tuning effects
  - Why needed here: The paper suggests fine-tuned models react differently to whitening than vanilla models, requiring understanding of how different training approaches affect embedding geometry
  - Quick check question: How does fine-tuning on NLI datasets potentially create embeddings that are sensitive to whitening transformations?

## Architecture Onboarding

- Component map: Embedding extraction -> Whitening transformation -> Classification/STS evaluation -> Isotropy measurement
- Critical path: Embedding generation -> Task-specific evaluation -> Model comparison
- Design tradeoffs: Isotropy improvement vs classification boundary preservation; computational cost of whitening vs performance gains
- Failure signatures: Classification accuracy drops after whitening; inconsistent effects across different models; fine-tuned models showing larger performance degradation
- First 3 experiments:
  1. Compare classification accuracy before and after whitening on a single model/dataset pair (e.g., BERT on MR dataset)
  2. Visualize embedding space before and after whitening using PCA dimensionality reduction
  3. Measure IsoScore changes before and after whitening to quantify isotropy improvements

## Open Questions the Paper Calls Out

### Open Question 1
Does the ineffectiveness of whitening on classification tasks extend to other types of supervised learning tasks beyond those tested (MR, CR, SUBJ, MPQA, TREC, MRPC, SST-F)? The study focused on specific classification datasets from SentEval, which may not represent the full diversity of supervised learning tasks.

### Open Question 2
What is the precise mechanism by which whitening degrades classification performance while potentially improving STS tasks? The paper demonstrates the empirical effect but does not provide a theoretical framework explaining why the same transformation helps in one task type but harms another.

### Open Question 3
Does fine-tuning methodology (e.g., NLI datasets) fundamentally alter how whitening affects embeddings, and if so, what specific aspects of fine-tuning cause this difference? While the paper identifies correlations between fine-tuning and whitening effectiveness, it does not establish causal mechanisms.

## Limitations

- Narrow focus on classification accuracy as sole performance metric, without examining other potential benefits of whitening
- Restricted experimental scope to English text classification and STS tasks, leaving cross-lingual applicability and other downstream tasks unexplored
- Does not provide rigorous theoretical explanation for why fine-tuned models show more pronounced degradation than vanilla models

## Confidence

- **High Confidence**: The empirical finding that whitening consistently degrades classification performance across multiple models and datasets
- **Medium Confidence**: The claim that embedding dimensionality amplifies whitening's negative effects
- **Low Confidence**: The assertion that whitening should be avoided for all classification tasks

## Next Checks

1. **Model Architecture Sensitivity Test**: Conduct experiments varying the MLP classifier architecture (adding hidden layers) to determine if whitening's negative impact is specific to linear classifiers or extends to more complex architectures.

2. **Task Generalization Study**: Evaluate whitening effects on classification tasks beyond sentiment analysis and topic categorization, including multi-label classification and few-shot learning scenarios, to test the universality of the findings.

3. **Intermediate Representation Analysis**: Investigate whether whitening embeddings before they enter downstream classifiers (as opposed to whitening final embeddings) yields different results, particularly for models used in retrieval-augmented generation pipelines.