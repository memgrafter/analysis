---
ver: rpa2
title: Why Not Transform Chat Large Language Models to Non-English?
arxiv_id: '2405.13923'
source_url: https://arxiv.org/abs/2405.13923
tags:
- transllm
- language
- data
- chat
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extending chat large language
  models (LLMs) to non-English languages, which is limited by the scarcity of high-quality
  non-English data. The proposed TransLLM framework uses a translation chain-of-thought
  (TCoT) technique to transfer English chat abilities to target languages without
  requiring supervised fine-tuning data.
---

# Why Not Transform Chat Large Language Models to Non-English?

## Quick Facts
- arXiv ID: 2405.13923
- Source URL: https://arxiv.org/abs/2405.13923
- Reference count: 0
- This paper addresses extending chat LLMs to non-English languages through the TransLLM framework using translation chain-of-thought (TCoT) technique

## Executive Summary
This paper introduces TransLLM, a framework for extending English chat large language models to non-English languages without requiring supervised fine-tuning data. The approach uses translation chain-of-thought (TCoT) to transfer chat abilities through inference-time computation, combined with continual pre-training and recovery knowledge distillation (RKD) with LoRA to prevent catastrophic forgetting. Experiments transforming Llama-2/3/3.1 models to Thai, Arabic, Portuguese, Telugu, and Turkish demonstrate superior performance across instruction following, multi-turn conversation, and safety evaluations compared to strong baselines.

## Method Summary
TransLLM transforms English chat LLMs to non-English languages through a three-stage process: (1) continual pre-training on target language monolingual data to improve target language modeling, (2) continual pre-training on parallel translation data to enhance translation quality, and (3) fine-tuning using RKD data (generated by the original English model) combined with TCoT data using LoRA to mitigate catastrophic forgetting. The TCoT technique decomposes queries into translation steps: translate to English, generate English response, then generate target language answer, mimicking human second-language processing.

## Key Results
- TransLLM outperforms strong baselines in multi-turn conversation, instruction following, and safety across five target languages
- Achieves high rejection rates on harmful queries without safety-specific training data
- Matches or exceeds performance of specialized multilingual and individual language models
- Maintains English chat abilities while acquiring target language proficiency through RKD+LoRA mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TransLLM transfers English chat abilities to non-English languages without supervised fine-tuning data by using translation chain-of-thought (TCoT).
- Mechanism: TCoT decomposes the task into three steps: 1) translate the query into English, 2) generate an English response using the original chat ability, 3) generate the final answer in the target language based on the preceding steps.
- Core assumption: The intermediate steps in TCoT contribute to the final answer, unlike translate-bridge where each step is treated independently.
- Evidence anchors:
  - [abstract] "TransLLM employs the translation chain-of-thought (TCoT) technique, which transfers chat ability through inference-time computation."
  - [section] "TCoT simulates how humans handle second language questions by first interpreting and solving the question in their native language, then generating the response in the target language."
- Break condition: If the LLM lacks sufficient performance on translation sub-tasks, TCoT may not work effectively.

### Mechanism 2
- Claim: Recovery Knowledge Distillation (RKD) combined with LoRA mitigates catastrophic forgetting of original chat abilities during continual pre-training.
- Mechanism: RKD uses data generated by the original chat LLM to recover its chat ability, while LoRA isolates original model parameters during training.
- Core assumption: During fine-tuning on RKD data, the LLM can fit the data by reducing the contribution of LoRA parameters, allowing it to learn a generalizable pattern that uses original knowledge for English and new knowledge for target languages.
- Evidence anchors:
  - [abstract] "We propose recovery knowledge distillation (RKD), which utilizes data generated by the original chat LLM to recover its chat ability, in conjunction with LoRA."
  - [section] "When fine-tuning on RKD data, LLM can fit the RKD data easily by reducing the contribution of LoRA parameters."
- Break condition: If the RKD data cannot cover all previously trained tasks, catastrophic forgetting may still occur.

### Mechanism 3
- Claim: Continual pre-training (CPT) on target language data and translation data improves performance in target language modeling and translation.
- Mechanism: CPT enhances the LLM's ability to generate fluent, localized text in the target language and improves bidirectional translation quality between English and the target language.
- Core assumption: Effective target language modeling is crucial for generating fluent, localized text and enhancing translation quality.
- Evidence anchors:
  - [abstract] "We further enhance the performance of sub-tasks with publicly available data."
  - [section] "To establish a strong foundation for the target language, we continually pre-train the LLM using monolingual data in the target language."
- Break condition: If the monolingual data is insufficient or of poor quality, CPT may not effectively improve target language performance.

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: TCoT relies on breaking down complex language transfer tasks into intermediate reasoning steps
  - Quick check question: Can you explain how chain-of-thought differs from direct response generation?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: CPT on target language data causes forgetting of original English chat abilities, which RKD+LoRA must address
  - Quick check question: What are the three main categories of methods to mitigate catastrophic forgetting?

- Concept: Knowledge distillation
  - Why needed here: RKD distills original chat knowledge from the source LLM to the target LLM without requiring supervised data
  - Quick check question: How does knowledge distillation differ from traditional supervised fine-tuning?

## Architecture Onboarding

- Component map: LLM backbone → Vocabulary expansion → LoRA modules → CPT stages (target language + translation) → Fine-tuning (RKD + TCoT) → Inference (TCoT mode)
- Critical path: Target language CPT → Translation CPT → RKD fine-tuning → TCoT inference
- Design tradeoffs: RKD vs GPT-4 KD data (safety vs performance), LoRA vs full fine-tuning (parameter efficiency vs knowledge retention)
- Failure signatures: Poor translation quality, English responses to non-English queries, catastrophic forgetting of chat abilities
- First 3 experiments:
  1. Test TCoT inference with a simple English chat model and Thai queries
  2. Evaluate CPT effectiveness on target language modeling using translation benchmarks
  3. Measure catastrophic forgetting by comparing generation probabilities before and after CPT

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TransLLM's performance scale with model size beyond 8B parameters?
- Basis in paper: [inferred] The paper notes limitations due to resource constraints, stating "Our experiments have some limitations due to resource constraints: (1) We focused on LLMs with fewer than 8B parameters. The effects of scaling up parameters require further exploration."
- Why unresolved: The authors did not test larger models due to computational resource limitations.
- What evidence would resolve it: Experiments comparing TransLLM's effectiveness on 30B, 70B, and 175B parameter models across multiple target languages.

### Open Question 2
- Question: What is the minimum amount of parallel data required for effective TransLLM performance?
- Basis in paper: [explicit] The paper states "Due to limited resources, we only conducted settings (1) and (2) for TH and AR, respectively" and "To investigate whether TransLLM can operate effectively with constrained resources, no monolingual or parallel data is used for PT, TE, and TR."
- Why unresolved: The authors used different data amounts for different languages without systematically varying the data quantity to find the minimum effective threshold.
- What evidence would resolve it: Controlled experiments varying the amount of parallel data (e.g., 100K, 500K, 1M pairs) while keeping other factors constant to identify the point of diminishing returns.

### Open Question 3
- Question: How does TransLLM perform on extremely low-resource languages with limited or no parallel data?
- Basis in paper: [inferred] The paper mentions limitations for low-resource languages: "Additionally, the translation resources may be lacking for extremely low-resource languages. We could explore unsupervised or semi-supervised machine translation techniques..."
- Why unresolved: The authors only tested on five languages (Thai, Arabic, Portuguese, Telugu, and Turkish) with varying resource availability, but did not include languages with severe resource scarcity.
- What evidence would resolve it: Testing TransLLM on languages like Somali, Lithuanian, or Icelandic with minimal parallel data, comparing against baseline methods that don't rely on translation.

## Limitations

- Limited testing to five languages, raising questions about generalizability to languages with different linguistic structures
- Reliance on Google Translate for TCoT data generation introduces dependency on a proprietary system
- Claims of safety performance without explicit safety training require more comprehensive validation

## Confidence

**High Confidence Claims:**
- The TCoT methodology effectively transfers English chat abilities to non-English languages when translation quality is adequate
- CPT on target language data improves target language modeling performance
- RKD+LoRA combination provides better catastrophic forgetting mitigation than CPT alone

**Medium Confidence Claims:**
- TransLLM achieves competitive or superior performance compared to specialized multilingual and individual language models across all evaluation dimensions
- Safety performance is maintained without explicit safety training
- The approach generalizes well to languages with different linguistic structures

**Low Confidence Claims:**
- The method works equally well for all language families without modification
- No additional safety-specific fine-tuning is needed for production deployment
- The performance gains scale linearly with model size and data quality

## Next Checks

1. **Cross-Linguistic Generalization Test**: Apply TransLLM to a language from a different family than the five tested (e.g., Japanese or Finnish) and evaluate performance degradation compared to the original five languages.

2. **Safety Stress Test**: Conduct a comprehensive safety evaluation using a diverse set of safety benchmarks and include tests for bias, fairness, and nuanced harm prevention. Compare performance against models explicitly trained with safety data.

3. **Forgetting Quantification Analysis**: Measure catastrophic forgetting quantitatively by comparing generation probabilities on a comprehensive hold-out set from the original English model before and after each training stage (CPT, RKD fine-tuning).