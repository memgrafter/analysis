---
ver: rpa2
title: On Oversquashing in Graph Neural Networks Through the Lens of Dynamical Systems
arxiv_id: '2405.01009'
source_url: https://arxiv.org/abs/2405.01009
tags:
- graph
- nodes
- information
- neural
- swan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the oversquashing problem in Graph Neural
  Networks (GNNs), where information flow between distant nodes exponentially decays
  with increasing node distances. The authors propose SWAN (Space-Weight Antisymmetric),
  a novel GNN model that achieves both global and local non-dissipativity through
  antisymmetric parameterization in both space and weight domains.
---

# On Oversquashing in Graph Neural Networks Through the Lens of Dynamical Systems

## Quick Facts
- arXiv ID: 2405.01009
- Source URL: https://arxiv.org/abs/2405.01009
- Reference count: 13
- Primary result: SWAN GNN model addresses oversquashing by maintaining constant information flow rate through antisymmetric parameterization

## Executive Summary
This paper addresses the oversquashing problem in Graph Neural Networks where information flow between distant nodes exponentially decays with increasing node distances. The authors propose SWAN (Space-Weight Antisymmetric), a novel GNN model that achieves both global and local non-dissipativity through antisymmetric parameterization in both space and weight domains. By designing the model such that its Jacobian has purely imaginary eigenvalues, SWAN maintains constant information flow regardless of node distance, unlike standard diffusion-based GNNs. Empirical evaluations demonstrate significant improvements on synthetic graph property prediction tasks and real-world long-range benchmarks.

## Method Summary
SWAN implements antisymmetric parameterization in both space and weight domains through a DE-GNN framework with forward Euler discretization. The model uses antisymmetric matrices to ensure the Jacobian has purely imaginary eigenvalues, creating non-dissipative dynamics that prevent exponential decay in information transmission. The implementation includes two variants: one with pre-defined spatial aggregation terms and another with learned spatial aggregation. The model is trained using Adam optimizer with early stopping, and the antisymmetric components are implemented through specific matrix operations that maintain the desired spectral properties.

## Key Results
- SWAN-LEARN achieves up to 117% improvement over baselines on synthetic graph property prediction tasks
- On real-world long-range benchmarks (Peptides-func, Peptides-struct, PascalVOC-sp), SWAN outperforms standard MPNNs and competitive DE-GNNs
- The method maintains linear complexity while showing particular strength in tasks requiring long-range interactions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SWAN maintains a constant information flow rate between distant nodes by ensuring its Jacobian has purely imaginary eigenvalues
- Mechanism: The antisymmetric parameterization in both space and weight domains creates a system where the Jacobian matrix M2 is composed of antisymmetric matrices whose eigenvalues have zero real parts, leading to non-dissipative behavior
- Core assumption: The Jacobian does not change significantly over time, allowing the use of autonomous differential equation analysis
- Evidence anchors:
  - [abstract]: "SWAN maintains a constant information flow rate regardless of node distance, unlike standard diffusion-based GNNs"
  - [section 3.2]: "M2 satisfies this condition as it is composed of a summation of three antisymmetric matrices whose eigenvalues have a real part of zero"
  - [corpus]: No direct evidence found in corpus papers
- Break condition: If the Jacobian changes significantly over time, the constant information flow rate property would no longer hold

### Mechanism 2
- Claim: SWAN improves sensitivity to distant node interactions through spatial antisymmetry
- Mechanism: The antisymmetric aggregation function Ψ introduces a new term to the message passing equation that increases the upper bound on node embedding sensitivity, allowing better propagation of information from distant nodes
- Core assumption: The Lipschitz constant of the activation function and weight matrices remain bounded
- Evidence anchors:
  - [section 3.3]: "The added antisymmetric term Ψ contributes to an increase in the measured upper bound"
  - [abstract]: "SWAN offers an enhanced ability to transmit information over extended distances"
  - [corpus]: No direct evidence found in corpus papers
- Break condition: If the antisymmetry term becomes too small or the system becomes dissipative, the sensitivity improvement would be lost

### Mechanism 3
- Claim: SWAN addresses oversquashing by preventing exponential decay in information transmission
- Mechanism: Unlike diffusion GNNs where the information propagation rate decays exponentially with time, SWAN's non-dissipative properties ensure constant propagation regardless of network depth
- Core assumption: The system maintains its non-dissipative properties throughout all layers
- Evidence anchors:
  - [abstract]: "oversquashing is attributed to the exponential decay in information transmission as node distances increase"
  - [section 3.2]: "SWAN maintains the same effectiveness, independently of time, meaning it retains its ability to share information across nodes with the same effectiveness in each layer"
  - [corpus]: Weak evidence - only mentions oversquashing in related papers but not specifically SWAN's mechanism
- Break condition: If the system transitions from non-dissipative to dissipative behavior, exponential decay would return

## Foundational Learning

- Concept: Differential Equations and Graph Neural Networks
  - Why needed here: SWAN is built on the interpretation of GNNs as discretizations of ODEs, requiring understanding of how continuous dynamics relate to discrete message passing
  - Quick check question: What is the relationship between the graph Laplacian and diffusion processes in GNNs?

- Concept: Eigenvalue analysis and stability of dynamical systems
  - Why needed here: The core mechanism of SWAN relies on ensuring eigenvalues have zero real parts, requiring understanding of how eigenvalues determine system behavior
  - Quick check question: How do the real parts of eigenvalues determine whether a system is stable, dissipative, or non-dissipative?

- Concept: Antisymmetric matrices and their properties
  - Why needed here: SWAN's design specifically uses antisymmetric matrices to achieve non-dissipative behavior, requiring understanding of their spectral properties
  - Quick check question: What are the spectral properties of antisymmetric matrices and why do they have purely imaginary eigenvalues?

## Architecture Onboarding

- Component map:
  - Input layer: Node features mapped to hidden space
  - SWAN layer: Antisymmetric parameterization with space and weight antisymmetry (Equations 5-7)
  - Output layer: Readout mapping to prediction space
  - Key matrices: W (weight), V (channel mixing), Z (antisymmetric weight), ˆA and ˜A (spatial aggregation)

- Critical path:
  1. Initialize node features
  2. Apply forward Euler discretization with step size ϵ
  3. Compute antisymmetric transformations in both space and weight domains
  4. Apply activation function σ
  5. Propagate through layers with weight sharing

- Design tradeoffs:
  - Stability vs expressiveness: Small γ ensures numerical stability but may limit expressiveness
  - Computational cost vs performance: Antisymmetric operations add overhead but improve long-range propagation
  - Fixed vs learned aggregation: Pre-defined operators are simpler but learned operators may adapt better to specific tasks

- Failure signatures:
  - Performance degradation with increasing graph diameter: Indicates loss of non-dissipative properties
  - Vanishing gradients in deep networks: May indicate numerical instability despite γ
  - Oscillatory behavior in node features: Could indicate eigenvalues with large imaginary parts

- First 3 experiments:
  1. Information transfer task: Measure ability to propagate labels between distant nodes on line, ring, and crossed-ring graphs
  2. Jacobian analysis: Track relative change in Jacobian over layers to verify stability assumption
  3. Sensitivity test: Measure ∂xv(ℓ)/∂xu(0) for various node pairs to quantify improvement in distant node interaction sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between the eigenvalues of the Jacobian matrix and the stability of SWAN across different graph topologies?
- Basis in paper: [explicit] The paper states that the Jacobian has purely imaginary eigenvalues ensuring stability, but doesn't fully characterize how this varies across different graph structures.
- Why unresolved: While the paper proves that SWAN's Jacobian has purely imaginary eigenvalues, it doesn't explore how different graph topologies (random, scale-free, small-world) affect the distribution and magnitude of these eigenvalues.
- What evidence would resolve it: Comprehensive analysis of SWAN's Jacobian eigenvalues across various synthetic and real-world graph topologies, examining how topological properties (diameter, clustering coefficient, degree distribution) correlate with eigenvalue characteristics.

### Open Question 2
- Question: How does SWAN's performance scale with graph size and density, and what are the theoretical limits of its information propagation capabilities?
- Basis in paper: [inferred] The paper shows linear complexity and competitive performance on benchmarks, but doesn't explore theoretical limits of information propagation or scaling behavior.
- Why unresolved: While the paper demonstrates SWAN's effectiveness on specific datasets, it doesn't establish theoretical bounds on its performance as graph size and density increase, nor does it identify when SWAN's advantages diminish.
- What evidence would resolve it: Rigorous theoretical analysis establishing upper bounds on SWAN's information propagation rate as a function of graph size and density, combined with empirical validation on graphs spanning multiple orders of magnitude in size.

### Open Question 3
- Question: How does the choice of spatial aggregation matrices (A and Â) affect SWAN's ability to mitigate oversquashing in practice?
- Basis in paper: [explicit] The paper mentions that the implementation of A and Â can be treated as a hyperparameter and experiments with different choices, but doesn't provide a systematic analysis of their impact.
- Why unresolved: The paper demonstrates that different implementations of spatial aggregation matrices affect performance, but doesn't establish guidelines for choosing optimal matrices for different graph types or tasks.
- What evidence would resolve it: Systematic ablation studies comparing different spatial aggregation strategies across diverse graph types and tasks, identifying patterns that suggest optimal choices for specific scenarios.

## Limitations
- The theoretical claims rely heavily on the assumption that the Jacobian remains stable over time, which is not empirically verified
- Empirical evaluation shows strong performance but lacks ablation studies to isolate the specific contribution of space versus weight antisymmetry
- Limited task diversity in empirical evaluation may affect generalizability of results

## Confidence

**High Confidence**: The core theoretical framework connecting antisymmetry to non-dissipative behavior is well-established

**Medium Confidence**: The empirical improvements on benchmark datasets, though promising, are based on limited task diversity

**Low Confidence**: The stability assumption about Jacobian behavior over time, which is critical for the constant information flow claim

## Next Checks
1. Track the relative change in Jacobian matrices during training across multiple layers to verify the stability assumption quantitatively
2. Perform controlled ablation studies comparing SWAN with only space antisymmetry, only weight antisymmetry, and standard GNNs to isolate the individual contributions
3. Test SWAN on additional diverse graph benchmarks including citation networks and molecule datasets to evaluate generalization beyond the current task set