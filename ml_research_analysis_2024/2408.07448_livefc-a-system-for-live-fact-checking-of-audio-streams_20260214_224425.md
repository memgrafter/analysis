---
ver: rpa2
title: 'LiveFC: A System for Live Fact-Checking of Audio Streams'
arxiv_id: '2408.07448'
source_url: https://arxiv.org/abs/2408.07448
tags:
- claim
- live
- fact-checking
- claims
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LiveFC, a real-time fact-checking system for
  live audio streams. It addresses the challenge of misinformation spread through
  audio content in live events like political debates by combining automatic speech
  recognition, speaker diarization, and automated claim verification.
---

# LiveFC: A System for Live Fact-Checking of Audio Streams

## Quick Facts
- arXiv ID: 2408.07448
- Source URL: https://arxiv.org/abs/2408.07448
- Reference count: 18
- Primary result: Real-time fact-checking system for live audio streams achieving 83.92 F1-score in claim veracity prediction

## Executive Summary
LiveFC is a real-time fact-checking system designed to combat misinformation in live audio streams by automatically detecting check-worthy claims, retrieving supporting evidence, and predicting claim veracity. The system processes live audio through automatic speech recognition, speaker diarization, and automated claim verification, making it particularly valuable for monitoring political debates, press conferences, and other live events where misinformation can rapidly spread. Evaluation on the 2024 US presidential debate demonstrated that LiveFC successfully identified all claims found by manual fact-checkers while achieving high accuracy in veracity prediction compared to established fact-checking organizations.

## Method Summary
LiveFC employs a multi-stage pipeline that begins with processing live audio streams through automatic speech recognition (ASR) to generate text transcripts. The system then applies speaker diarization to identify different speakers and segments the conversation. Check-worthy claims are detected using fine-tuned transformer models trained on claim detection datasets. For each identified claim, the system performs web-based evidence retrieval to gather supporting or refuting information, then applies fine-tuned veracity prediction models to determine the truthfulness of each claim. The entire pipeline operates in real-time, enabling immediate fact-checking during live broadcasts.

## Key Results
- LiveFC identified all claims found by manual fact-checkers during the 2024 US presidential debate evaluation
- Achieved 83.92 F1-score in veracity prediction when compared to Politifact assessments
- Qualitative evidence assessments scored between 3.60-4.37 on a 5-point Likert scale, indicating high-quality evidence retrieval
- System successfully combines ASR, speaker diarization, claim detection, evidence retrieval, and veracity prediction in a real-time pipeline

## Why This Works (Mechanism)
LiveFC's effectiveness stems from its integrated pipeline that addresses the complete fact-checking workflow in real-time. By combining state-of-the-art ASR models with speaker diarization, the system can accurately attribute claims to specific speakers and maintain context throughout conversations. The fine-tuned claim detection models are specifically trained to identify check-worthy statements rather than processing all spoken content, reducing computational overhead while maintaining high precision. Web-based evidence retrieval ensures that claims are evaluated against current information sources, while the veracity prediction models leverage transformer architectures fine-tuned on fact-checking datasets to provide accurate assessments.

## Foundational Learning
- **Automatic Speech Recognition (ASR)**: Converts spoken audio to text; needed because fact-checking requires text input for NLP models; quick check: evaluate word error rate on political debate audio
- **Speaker Diarization**: Identifies different speakers in audio streams; needed to attribute claims to specific speakers and maintain context; quick check: measure diarization error rate on multi-speaker debate recordings
- **Claim Detection**: Identifies check-worthy claims in text; needed to filter relevant statements from all spoken content; quick check: calculate precision/recall on claim detection datasets
- **Evidence Retrieval**: Searches web for supporting/refuting information; needed to provide factual basis for veracity predictions; quick check: measure relevance scores of top-5 retrieved documents
- **Veracity Prediction**: Classifies claim truthfulness; needed to provide final fact-checking assessment; quick check: evaluate F1-score against established fact-checking databases
- **Real-time Processing**: Handles streaming audio with minimal latency; needed for live event monitoring; quick check: measure end-to-end processing time from audio input to verdict

## Architecture Onboarding

**Component Map:** ASR -> Speaker Diarization -> Claim Detection -> Evidence Retrieval -> Veracity Prediction

**Critical Path:** The most time-sensitive components are ASR processing and claim detection, as they must complete before evidence retrieval and veracity prediction can begin. Speaker diarization runs in parallel with claim detection to optimize throughput.

**Design Tradeoffs:** The system prioritizes speed over perfect accuracy in ASR to maintain real-time capabilities, accepting higher word error rates in exchange for lower latency. Evidence retrieval uses simplified search queries to reduce processing time rather than complex semantic searches that might improve accuracy but increase latency.

**Failure Signatures:** ASR errors manifest as garbled claim text, leading to poor claim detection and evidence retrieval. Speaker diarization failures result in incorrect speaker attribution, confusing fact-checking assessments. Network latency during evidence retrieval can cause the system to fall behind live audio, creating backlogs that impact real-time performance.

**First 3 Experiments to Run:**
1. Measure end-to-end latency from live audio input to veracity prediction output using sample debate recordings
2. Test claim detection precision/recall using manually annotated debate transcripts as ground truth
3. Evaluate evidence retrieval quality by comparing top-5 retrieved sources against human-curated evidence sets

## Open Questions the Paper Calls Out
None

## Limitations
- Single-event evaluation on 2024 US presidential debate limits generalizability to other domains and claim types
- Real-time processing latency not fully characterized, making broadcast viability uncertain
- Evidence retrieval mechanism quality not rigorously evaluated with systematic metrics
- Comparison against Politifact rather than independent verification across multiple events

## Confidence

**System Architecture and Implementation:** High - Technical pipeline is well-defined with clear component integration
**ASR and Speaker Diarization Performance:** Medium - Based on established models but real-time performance characteristics unspecified
**Claim Detection and Veracity Prediction:** Medium - Single-event evaluation limits generalizability claims
**Evidence Retrieval Quality:** Low - Qualitative assessments lack rigorous evaluation methodology

## Next Checks

1. **Multi-event Evaluation:** Test LiveFC across at least 10 diverse live audio events (political debates, press conferences, interviews) to establish performance consistency and identify domain-specific failure modes.

2. **Real-time Latency Measurement:** Conduct controlled experiments measuring end-to-end processing time from audio input to veracity prediction, ensuring total latency remains under 5 seconds to maintain broadcast viability.

3. **Independent Veracity Benchmark:** Implement blind testing where human annotators unfamiliar with original fact-checking assessments evaluate a subset of claims to verify the 83.92 F1-score accuracy and identify potential systematic biases in the prediction model.