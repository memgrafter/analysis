---
ver: rpa2
title: Normalizing Flows are Capable Generative Models
arxiv_id: '2412.06329'
source_url: https://arxiv.org/abs/2412.06329
tags:
- guidance
- flow
- generative
- flows
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TarFlow, a Transformer-based normalizing
  flow architecture that enables highly performant likelihood-based generative models.
  TarFlow extends Masked Autoregressive Flows (MAFs) by replacing the MLP-based implementation
  with a causal Vision Transformer operating on image patches, allowing for a deep
  stack of autoregressive transformations with alternating directions.
---

# Normalizing Flows are Capable Generative Models

## Quick Facts
- arXiv ID: 2412.06329
- Source URL: https://arxiv.org/abs/2412.06329
- Authors: Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, Josh Susskind
- Reference count: 40
- Primary result: First normalizing flow to achieve sub-3 bits-per-dimension on ImageNet 64x64

## Executive Summary
This paper introduces TarFlow, a Transformer-based normalizing flow architecture that achieves state-of-the-art likelihood estimation on ImageNet 64x64, reaching sub-3 bits-per-dimension for the first time. The key innovation replaces the MLP-based Masked Autoregressive Flow (MAF) with a causal Vision Transformer operating on image patches, enabling deep stacks of autoregressive transformations with alternating directions. The authors also introduce three techniques to improve sample quality: Gaussian noise augmentation during training, post-training denoising using score-based methods, and an effective guidance approach applicable to both conditional and unconditional settings.

## Method Summary
TarFlow extends Masked Autoregressive Flows (MAFs) by replacing the traditional MLP-based autoregressive transformations with a Vision Transformer that operates on image patches. The architecture uses causal attention mechanisms to maintain autoregressive properties while benefiting from the expressivity of Transformers. The model employs alternating transformation directions in a deep stack, with each layer learning to refine the representation. During training, Gaussian noise is added to the data to improve robustness, and a post-training denoising step uses score-based methods to further enhance sample quality. The guidance technique allows for improved conditional generation by modifying the latent space during sampling.

## Key Results
- Achieved sub-3 bits-per-dimension (BPD) on ImageNet 64x64, the first normalizing flow to do so
- Generated samples with quality and diversity comparable to diffusion models
- Demonstrated exact likelihood computation while achieving high sample quality
- Showed effective guidance approach for both conditional and unconditional generation

## Why This Works (Mechanism)
The Transformer-based architecture in TarFlow allows for deeper and more expressive autoregressive transformations compared to traditional MLP-based MAFs. By operating on image patches with causal attention, the model can capture long-range dependencies while maintaining the autoregressive property required for exact likelihood computation. The alternating transformation directions in the deep stack enable the model to refine representations at multiple scales. The Gaussian noise augmentation during training makes the model more robust to perturbations, while the post-training denoising step leverages score-based methods to improve sample quality without affecting the learned likelihood. The guidance technique effectively steers the generation process by modifying the latent space, allowing for improved conditional generation and unconditional sample quality.

## Foundational Learning
- **Normalizing Flows**: Learn invertible transformations between complex distributions and simple base distributions; needed for exact likelihood computation; quick check: verify change-of-variables formula implementation
- **Masked Autoregressive Flows (MAFs)**: Use autoregressive transformations where each output depends only on previous inputs; needed for tractable likelihood computation; quick check: confirm autoregressive mask correctness
- **Vision Transformers**: Process image patches with self-attention mechanisms; needed for capturing long-range dependencies; quick check: verify causal attention implementation
- **Score-based Methods**: Estimate gradients of log-density for denoising; needed for post-training sample improvement; quick check: validate score matching objective
- **Bits-per-dimension (BPD)**: Standard metric for measuring likelihood in image generation; needed for quantitative evaluation; quick check: confirm BPD calculation matches theoretical bounds

## Architecture Onboarding

**Component Map**: Image patches -> Vision Transformer (causal attention) -> Alternating autoregressive transformations -> Base distribution

**Critical Path**: Data augmentation (Gaussian noise) -> TarFlow forward pass (patch encoding + Transformer layers + transformations) -> Likelihood computation (change-of-variables) -> Sampling (base distribution sampling + inverse transformations + denoising)

**Design Tradeoffs**: The paper trades computational efficiency for expressivity by using Transformers instead of MLPs, enabling deeper architectures at the cost of increased parameter count and inference time. The noise augmentation improves sample quality but may slightly degrade likelihood estimates. The post-training denoising improves samples but adds complexity to the generation pipeline.

**Failure Signatures**: Poor sample quality with high likelihood suggests mode collapse; low likelihood with good samples indicates overfitting to noise; degraded performance on higher resolutions may indicate scalability issues with the patch-based approach.

**First 3 Experiments**:
1. Verify autoregressive property by checking that each output dimension depends only on previous dimensions
2. Compare BPD on CIFAR-10 before and after noise augmentation to quantify its effect on likelihood
3. Evaluate sample quality with and without post-training denoising using visual inspection and FID scores

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on higher resolution datasets (128x128 or 256x256) remains unexplored
- Trade-offs between sample quality and likelihood estimation with proposed techniques are not thoroughly analyzed
- Comparison with diffusion models lacks rigorous quantitative evaluation metrics

## Confidence

High confidence: Architectural improvements and sub-3 BPD achievement on ImageNet 64x64

Medium confidence: Effectiveness of noise augmentation, denoising, and guidance techniques for sample quality improvement

Low confidence: Claim of comparable quality to diffusion models without comprehensive quantitative comparisons

## Next Checks

1. Evaluate TarFlow on higher resolution datasets (128x128 ImageNet or CelebA-HQ) to assess scalability and whether sub-3 BPD extends to larger images

2. Conduct comprehensive quantitative comparisons with diffusion models using established metrics (FID, IS, precision-recall curves) to substantiate comparable sample quality claims

3. Perform ablation studies on proposed techniques (noise augmentation, denoising, guidance) to quantify individual contributions to likelihood estimation and sample quality, identifying potential trade-offs or limitations