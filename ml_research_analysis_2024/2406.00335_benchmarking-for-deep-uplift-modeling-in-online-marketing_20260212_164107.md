---
ver: rpa2
title: Benchmarking for Deep Uplift Modeling in Online Marketing
arxiv_id: '2406.00335'
source_url: https://arxiv.org/abs/2406.00335
tags:
- uplift
- treatment
- modeling
- feature
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DUMOM, the first open benchmark for deep uplift
  modeling (DUM) in online marketing. The work addresses the lack of standardized
  benchmarks and unified evaluation protocols in DUM research, which leads to non-reproducible
  and inconsistent experimental results.
---

# Benchmarking for Deep Uplift Modeling in Online Marketing

## Quick Facts
- arXiv ID: 2406.00335
- Source URL: https://arxiv.org/abs/2406.00335
- Reference count: 38
- This paper presents DUMOM, the first open benchmark for deep uplift modeling in online marketing, evaluating 13 models across four preprocessing settings on two industrial datasets.

## Executive Summary
This paper introduces DUMOM, the first comprehensive benchmark for deep uplift modeling in online marketing. The authors address the lack of standardized evaluation protocols in DUM research by systematically evaluating 13 representative deep learning models across four preprocessing settings on two widely-used industrial datasets (Criteo and Lazada). The benchmarking reveals that recent DUM models show limited improvements over traditional approaches, particularly when training and test distributions differ. The study highlights the critical impact of data preprocessing steps like instance deduplication and feature normalization on model performance, providing valuable practical insights for practitioners while identifying key challenges for future DUM research.

## Method Summary
The DUMOM benchmark evaluates 13 deep uplift modeling approaches across two industrial datasets using four preprocessing configurations. The method employs hyperparameter optimization via Optuna with QINI as the primary metric, using batch normalization for feature normalization when enabled. Models are trained on observational data with biased treatment assignment and evaluated on randomized controlled trial (RCT) test sets to assess real-world performance. The benchmark uses standard DUM evaluation metrics including AUUC, QINI, WAU, and LIFT@30, with PyTorch implementations ensuring reproducibility through fixed random seeds.

## Key Results
- Recent DUM models show minimal performance improvements over traditional methods, especially under distribution shifts
- Model performance is highly sensitive to preprocessing choices, with instance deduplication and feature normalization having inconsistent effects across datasets
- DUM models demonstrate limited generalization across different preprocessing settings and test distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DUM models underperform when training and test distributions are inconsistent due to distribution shift
- Mechanism: Models trained on biased treatment assignment data fail when tested on unbiased RCT data
- Core assumption: Performance degradation stems from distributional mismatch, not model architecture
- Evidence anchors:
  - [abstract] "our experimental results show that the most recent work differs less than expected from traditional work in many cases...especially for different preprocessing and test distributions."
  - [section] "By performing instance deduplication on Lazada, i.e., when there is an apparent inconsistency between the training distribution and the test distribution, most DUM models will suffer..."
  - [corpus] Weak evidence: related papers focus on different aspects without addressing distribution shift
- Break condition: If test data comes from the same biased distribution as training data, or if preprocessing reduces distributional gap

### Mechanism 2
- Claim: Instance deduplication improves performance in consistent settings but harms it in inconsistent settings
- Mechanism: Removing duplicates reduces overfitting to overrepresented users in consistent settings, but removes critical treatment assignment signals in inconsistent settings
- Core assumption: Duplicates contain either redundant noise (consistent) or treatment assignment patterns (inconsistent)
- Evidence anchors:
  - [section] "Without performing feature normalization operations, by performing instance deduplication operations on Criteo...the performance of the DUM model can be significantly improved in most cases..."
  - [section] "However, by performing instance deduplication on Lazada...most DUM models will suffer..."
  - [corpus] No direct evidence; corpus neighbors focus on different modeling strategies
- Break condition: If duplicates aren't linked to treatment assignment policy, or if deduplication is applied selectively

### Mechanism 3
- Claim: Feature normalization has inconsistent effects on DUM performance across datasets
- Mechanism: Normalization stabilizes gradient updates but may distort feature relationships encoding treatment effects
- Core assumption: Deep networks benefit from normalized inputs, but some interactions are meaningful only in original scale
- Evidence anchors:
  - [section] "performing feature normalization operations on Ladaza is bipolar, with significant improvements to S-Learner, BNN, TARNet, and CFRNet but damage to others."
  - [section] "On Criteo, performing feature normalization operations has almost no obvious impact without performing instance deduplication operations..."
  - [corpus] Weak evidence: corpus neighbors don't discuss normalization effects
- Break condition: If models use batch normalization internally, or if features are already on comparable scales

## Foundational Learning

- Concept: Potential outcomes framework
  - Why needed here: The paper evaluates uplift models that estimate individual treatment effects using potential outcomes; understanding this framework is essential for interpreting results
  - Quick check question: What are the two potential outcomes in uplift modeling, and how is uplift defined?

- Concept: Distribution shift and its impact on model generalization
  - Why needed here: The paper highlights that DUM models fail to generalize across different training/test distributions, which is central to its findings
  - Quick check question: How does selection bias in training data affect uplift model performance on unbiased test data?

- Concept: Feature preprocessing (normalization, deduplication)
  - Why needed here: The paper systematically evaluates how different preprocessing steps affect DUM performance, making it crucial to understand their roles
  - Quick check question: Why might removing duplicate instances help in one setting but hurt in another?

## Architecture Onboarding

- Component map: Data loading and splitting -> Preprocessing (normalization, deduplication) -> 13 DUM model implementations -> Hyperparameter search using Optuna -> Evaluation metrics (AUUC, QINI, WAU, LIFT@30) -> Result aggregation and comparison
- Critical path: Load dataset → preprocess (choose deduplication/normalization) → split into train/val/test → train each DUM model with hyperparameter tuning → evaluate on test → compare results
- Design tradeoffs: Using PyTorch for reproducibility vs. TensorFlow flexibility; random seeds for splitting vs. fixed splits for reproducibility; batch normalization vs. no normalization
- Failure signatures: Poor generalization across datasets indicates distribution shift issues; large variance across random seeds suggests instability; low AUUC/QINI indicates poor uplift discrimination
- First 3 experiments:
  1. Run S-Learner on Criteo with no preprocessing, record metrics
  2. Run S-Learner on Criteo with both deduplication and normalization, compare metrics
  3. Run S-Learner on Lazada with no preprocessing, compare to Criteo results to observe distribution shift effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural innovations are needed in deep uplift models to achieve consistent performance improvements across diverse industrial datasets and varying data distributions?
- Basis in paper: [explicit] The paper concludes that current DUM models lack generalization and robustness across different preprocessing steps and test distributions, suggesting a need for architectural improvements
- Why unresolved: The paper identifies the problem but doesn't propose specific architectural solutions or new model designs that could address these generalization issues
- What evidence would resolve it: Developing and benchmarking novel DUM architectures specifically designed for distribution robustness, followed by empirical validation showing consistent performance across multiple datasets with varying characteristics

### Open Question 2
- Question: How can instance deduplication and feature normalization strategies be optimally determined for specific deep uplift models and industrial use cases?
- Basis in paper: [explicit] The experiments show that instance deduplication and feature normalization have highly variable and sometimes contradictory effects on different DUM models depending on the dataset characteristics
- Why unresolved: The paper demonstrates that these preprocessing effects are model-dependent but doesn't provide a systematic framework for selecting appropriate preprocessing strategies for specific scenarios
- What evidence would resolve it: Developing a decision framework or automated system that recommends preprocessing strategies based on dataset characteristics, model architecture, and deployment requirements

### Open Question 3
- Question: What evaluation protocols and metrics beyond AUUC, QINI, WAU, and LIFT@30 are needed to comprehensively assess deep uplift model performance in real-world industrial applications?
- Basis in paper: [explicit] The paper uses standard DUM metrics but acknowledges limitations in model generalization, suggesting that current evaluation approaches may be insufficient for practical deployment
- Why unresolved: The paper employs existing metrics but doesn't explore whether these adequately capture real-world performance requirements or business impact
- What evidence would resolve it: Developing and validating new evaluation protocols that incorporate business metrics, computational efficiency, and deployment constraints alongside traditional performance measures

## Limitations

- Limited generalizability due to evaluation on only two industrial datasets (Criteo and Lazada)
- Exact instance deduplication criteria may be too restrictive, missing nuanced user profile similarities
- Inconsistent effects of feature normalization suggest high sensitivity to dataset-specific feature distributions

## Confidence

- High confidence: Claims about the need for standardized benchmarks and unified evaluation protocols in DUM research
- Medium confidence: Conclusions about DUM models not significantly outperforming traditional methods across different preprocessing settings
- Low confidence: Generalizability of preprocessing sensitivity findings to datasets outside the marketing domain

## Next Checks

1. Replicate the preprocessing pipeline on additional datasets to verify if normalization sensitivity is dataset-specific or a general phenomenon
2. Test alternative deduplication strategies (fuzzy matching, similarity thresholds) to determine if exact matching is too restrictive
3. Evaluate model performance on synthetic datasets with controlled treatment assignment mechanisms to isolate the impact of distribution shift from other factors