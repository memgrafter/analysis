---
ver: rpa2
title: A Large Encoder-Decoder Family of Foundation Models For Chemical Language
arxiv_id: '2407.20267'
source_url: https://arxiv.org/abs/2407.20267
tags:
- smi-ted289m
- molecular
- tasks
- different
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper presents SMI-TED289M, a large encoder-decoder family\
  \ of foundation models for chemical language, pre-trained on 91 million SMILES samples\
  \ from PubChem (4 billion molecular tokens). The model uses a transformer-based\
  \ encoder-decoder architecture with two variants: base (289M parameters) and Mixture-of-Experts\
  \ (8\xD7289M parameters)."
---

# A Large Encoder-Decoder Family of Foundation Models For Chemical Language

## Quick Facts
- arXiv ID: 2407.20267
- Source URL: https://arxiv.org/abs/2407.20267
- Reference count: 40
- A transformer-based encoder-decoder model (289M parameters) achieving state-of-the-art results on 11 molecular property prediction benchmarks

## Executive Summary
This paper introduces SMI-TED289M, a large-scale foundation model for chemical language that uses a transformer-based encoder-decoder architecture to learn contextualized representations of SMILES strings. Pre-trained on 91 million molecules from PubChem, the model demonstrates superior performance across diverse molecular property prediction tasks including quantum mechanical, physical, biophysical, and physiological properties. The study also presents a Mixture-of-Experts variant (8×289M parameters) that further improves performance while maintaining computational efficiency. The model's ability to capture compositional relationships in molecular structures suggests strong potential for chemical reasoning tasks beyond simple property prediction.

## Method Summary
SMI-TED289M is a transformer-based encoder-decoder model trained on 91 million SMILES strings from PubChem using masked language modeling and reconstruction losses. The architecture employs a bidirectional transformer encoder with rotary position embeddings (RoFormer) and a decoder for sequence generation. The model uses a 2993-token vocabulary and undergoes two-phase pre-training: first learning token embeddings through masking, then mapping these into a common latent space. A Mixture-of-Experts variant (SMI-TED8x289M) with 8 expert networks and top-2 routing is also developed. For downstream tasks, the model is fine-tuned with a 2-layer fully connected network optimized using Optuna. Training was performed on 24 NVIDIA V100 GPUs with specific learning rates and batch sizes.

## Key Results
- Achieved state-of-the-art performance across 11 benchmark datasets for molecular property prediction
- Demonstrated superior performance in quantum property prediction with best or second-best results in 11 out of 12 QM9 tasks
- Showed strong few-shot learning capabilities and compositionality in learned molecular representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on a large, curated SMILES dataset enables the model to learn general molecular representations that transfer effectively to downstream property prediction tasks.
- Mechanism: The model first learns token-level embeddings through masked language modeling, then maps these into a latent space that captures entire SMILES strings. This two-phase pre-training process allows the model to develop contextualized representations that encode molecular structure and properties.
- Core assumption: Large-scale pre-training on chemically valid and diverse molecules is necessary to learn meaningful representations that generalize to unseen molecular tasks.
- Evidence anchors:
  - [abstract]: "These methods excel in tasks such as property prediction and molecule generation by learning contextualized representations of input tokens through self-supervised learning on large unlabeled corpora."
  - [section]: "Pre-training of SMI-TED289M was performed for 40 epochs through the entire curated PubChem dataset... This involves two distinct phases: i) Learning of token embeddings through a masking process; ii) Subsequently, the token embeddings are mapped into a common latent space..."
  - [corpus]: Weak - neighbors discuss pre-training but don't specifically address the two-phase token embedding and latent space mapping approach.
- Break condition: If the pre-training data contains too many invalid or chemically meaningless SMILES strings, the learned representations may not capture meaningful molecular structure.

### Mechanism 2
- Claim: The encoder-decoder architecture with rotary position embeddings (RoFormer) enables effective modeling of molecular sequences and their reconstruction.
- Mechanism: The model uses a bidirectional transformer-based encoder for tokens combined with an encoder-decoder mechanism that can both represent and reconstruct SMILES strings. The RoFormer attention mechanism with position-dependent rotations improves the model's ability to capture sequential dependencies in molecular structures.
- Core assumption: Molecular structures encoded as SMILES strings have inherent sequential dependencies that can be effectively modeled by transformer architectures with appropriate positional encoding.
- Evidence anchors:
  - [section]: "We conduct training for SMI-TED289M model employing a deep-bidirectional-transformers-based encoder for tokens and an encoder-decoder architecture to compose SMILES... To optimize the relative encoding through position-dependent rotations Rm of the query and keys..."
  - [abstract]: "The model uses a transformer-based encoder-decoder architecture with two variants: base (289M parameters) and Mixture-of-Experts (8×289M parameters)."
  - [corpus]: Missing - neighbors discuss transformer-based approaches but don't specifically mention RoFormer or rotary position embeddings for chemical applications.
- Break condition: If the molecular sequences are too long or complex, the attention mechanism may fail to capture long-range dependencies effectively.

### Mechanism 3
- Claim: The Mixture-of-Experts (MoE) architecture enables efficient scaling of the foundation model while maintaining or improving performance across diverse molecular property prediction tasks.
- Mechanism: The MoE architecture consists of multiple expert networks with a gating network that routes inputs to the most relevant experts. This allows the model to learn specialized representations for different types of molecular properties while sharing computational resources efficiently.
- Core assumption: Different molecular property prediction tasks benefit from specialized representations, and an MoE architecture can effectively learn these specialized representations without the computational cost of training separate models for each task.
- Evidence anchors:
  - [section]: "The Mixture-of-SMI-TED-Experts, SMI-TED8x289M comprises a set of n 'expert networks'... Here, we define SMI-TED8x289M as n = 8 and k = 2, which means that SMI-TED8x289M is composed by 8× SMI-TED289M models, which 2 models are activated through the router each round."
  - [abstract]: "The model uses a transformer-based encoder-decoder architecture with two variants: base (289M parameters) and Mixture-of-Experts (8×289M parameters)."
  - [corpus]: Weak - neighbors discuss MoE for various applications but don't specifically address chemical property prediction tasks.
- Break condition: If the gating mechanism fails to route inputs effectively to the appropriate experts, the performance gains from the MoE architecture may not materialize.

## Foundational Learning

- Concept: Tokenization of SMILES strings
  - Why needed here: SMILES strings need to be converted into discrete tokens that can be processed by the transformer model. The quality and granularity of tokenization directly impacts the model's ability to learn meaningful representations.
  - Quick check question: What is the vocabulary size of the tokenizer used in SMI-TED289M, and how was it determined?

- Concept: Masked language modeling
  - Why needed here: Masked language modeling is the pre-training objective that allows the model to learn contextualized token representations by predicting masked tokens in the input sequence.
  - Quick check question: What percentage of tokens are masked during pre-training, and how are the masked positions determined?

- Concept: Encoder-decoder architecture
  - Why needed here: The encoder-decoder architecture allows the model to both encode molecular representations and decode them back to SMILES strings, enabling tasks like molecule generation and reconstruction.
  - Quick check question: How does the latent space representation in SMI-TED289M differ from the mean pooling approach used in encoder-only models?

## Architecture Onboarding

- Component map:
  Tokenizer -> Encoder -> Latent space projection -> Decoder/Gating network -> Experts (MoE) -> Output

- Critical path:
  1. Tokenization of input SMILES
  2. Encoding to token embeddings
  3. Projection to latent space
  4. Task-specific processing (classification, regression, or reconstruction)
  5. Output generation

- Design tradeoffs:
  - Larger vocabulary size vs. model complexity
  - Deeper encoder layers vs. computational efficiency
  - More experts in MoE vs. routing overhead
  - Masked language modeling percentage vs. learning stability

- Failure signatures:
  - Poor performance on downstream tasks despite good pre-training loss
  - Mode collapse in molecule generation
  - Inconsistent predictions across different seeds
  - High variance in expert activations in MoE

- First 3 experiments:
  1. Tokenization analysis: Test the tokenizer on a diverse set of SMILES strings to ensure it handles various molecular structures correctly.
  2. Pre-training validation: Monitor the masked language modeling loss during pre-training to ensure the model is learning effectively.
  3. Downstream task transfer: Fine-tune the pre-trained model on a simple classification task (like BBBP) to validate transfer learning capabilities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the compositionality of learned molecular representations scale with increasingly complex molecular structures beyond simple carbon chain families?
- Basis in paper: [explicit] The paper demonstrates compositionality using simple carbon chain families but acknowledges that "further studies consistent with methodologies of compositionality analysis in natural languages are required to make stronger statements."
- Why unresolved: The study only tested six families of carbon chains with up to 10 carbon atoms each, which represents a limited chemical space. More complex molecular structures with diverse functional groups and ring systems were not explored.
- What evidence would resolve it: Systematic testing of compositionality across diverse molecular families with varying structural complexity, including aromatic rings, heterocycles, and branched structures, while maintaining similar linear regression analysis approaches.

### Open Question 2
- Question: What are the theoretical limits of few-shot learning performance for molecular property prediction using SMI-TED289M, and how does this scale with the number of examples?
- Basis in paper: [explicit] The paper mentions "few-shot learning capabilities" and demonstrates reconstructing SMILES from a few example triples, but doesn't systematically explore the limits of this capability.
- Why unresolved: The study only tested a specific scenario with limited example triples and doesn't provide a comprehensive analysis of how performance changes with different numbers of training examples or across different molecular property prediction tasks.
- What evidence would resolve it: Systematic experiments varying the number of training examples across different molecular property prediction tasks, including analysis of learning curves and comparison with traditional few-shot learning approaches.

### Open Question 3
- Question: How does the performance of SMI-TED8x289M Mixture-of-Experts approach compare to other model scaling techniques (like depth scaling or width scaling) in terms of computational efficiency and prediction accuracy?
- Basis in paper: [explicit] The paper introduces SMI-TED8x289M as a Mixture-of-Experts approach and shows it improves performance, but doesn't compare it to alternative scaling methods.
- Why unresolved: While the MoE approach shows benefits, the paper doesn't provide a comprehensive comparison with other model scaling techniques that could offer different trade-offs between computational cost and performance.
- What evidence would resolve it: Direct comparison of SMI-TED8x289M with similarly sized models using depth scaling or width scaling approaches, measuring both computational efficiency and prediction accuracy across the same benchmark tasks.

## Limitations

- The pre-training dataset is derived from a single source (PubChem), potentially limiting chemical space coverage and introducing domain-specific biases
- Evaluation focuses primarily on established benchmarks with limited testing on real-world drug discovery scenarios or complex molecular structures
- The specific mechanisms of expert specialization and routing effectiveness in the MoE architecture are not fully characterized

## Confidence

- **High confidence**: The model's strong performance on established molecular property prediction benchmarks (QM9, MoleculeNet datasets) is well-supported by empirical results and comparison with existing methods.
- **Medium confidence**: The claim that the MoE architecture provides computational efficiency gains while maintaining performance is supported by the results, but the specific mechanisms of expert specialization and routing effectiveness require further investigation.
- **Low confidence**: The assertion that the model has strong compositionality for chemical reasoning tasks is based on limited latent space analysis and would benefit from more extensive probing experiments and controlled studies of compositional generalization.

## Next Checks

1. **Chemical space generalization test**: Evaluate SMI-TED289M on molecules from diverse chemical databases beyond PubChem (e.g., ChEMBL, ZINC) to assess performance on out-of-distribution compounds and identify potential coverage gaps in the pre-training data.

2. **Expert specialization analysis**: Conduct ablation studies on the MoE variant by freezing different expert combinations and measuring performance degradation on specific task categories to understand which experts specialize in which molecular property domains.

3. **Compositional reasoning probe**: Design controlled experiments where molecular properties are systematically varied through structural modifications (e.g., adding functional groups, changing ring systems) to test whether the model's representations exhibit true compositional understanding beyond correlation-based predictions.