---
ver: rpa2
title: 'UNSEE: Unsupervised Non-contrastive Sentence Embeddings'
arxiv_id: '2401.15316'
source_url: https://arxiv.org/abs/2401.15316
tags:
- arget
- sentence
- non-contrastive
- embeddings
- objectives
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UNSEE, a non-contrastive sentence embedding
  approach that outperforms SimCSE on the MTEB benchmark. The authors address representation
  collapse issues in non-contrastive objectives by introducing a target network to
  stabilize training and prevent collapse.
---

# UNSEE: Unsupervised Non-contrastive Sentence Embeddings

## Quick Facts
- arXiv ID: 2401.15316
- Source URL: https://arxiv.org/abs/2401.15316
- Reference count: 7
- Non-contrastive method outperforms SimCSE on MTEB benchmark

## Executive Summary
This paper introduces UNSEE, a non-contrastive sentence embedding approach that outperforms SimCSE on the MTEB benchmark. The authors address representation collapse issues in non-contrastive objectives by introducing a target network to stabilize training and prevent collapse. The key method is a BYOL-like architecture with dropout augmentation, trained on 1 million sentences. With careful hyperparameter tuning and architectural refinements, UNSEE achieves state-of-the-art results among non-contrastive methods, demonstrating the effectiveness of non-contrastive objectives for sentence representation learning.

## Method Summary
UNSEE uses a BYOL-like architecture with a target network that acts as an exponentially moving average of the online network. The model employs dropout augmentation to create diverse embeddings and trains with non-contrastive objectives (Barlow Twins, VICReg, CorInfoMax). The target network helps prevent representation collapse by providing stable embeddings during training. The architecture includes BERT-base encoder, single MLP projection layer, batch size 32, learning rate 1e-4, and sequence length 64.

## Key Results
- UNSEE outperforms SimCSE on MTEB benchmark
- Target network effectively prevents representation collapse
- Removing MLP layers from target network improves performance
- Non-contrastive objectives capture semantic relationships implicitly

## Why This Works (Mechanism)

### Mechanism 1
Non-contrastive objectives can outperform SimCSE when representation collapse is prevented. The target network acts as an exponentially moving average of the online network, stabilizing training and preventing collapse. Core assumption: target network provides diverse enough embeddings compared to dropout alone. Evidence: Figure 3 shows target network prevents collapse. Break condition: If target network fails to provide sufficient diversity, collapse may reoccur.

### Mechanism 2
Direct use of encoder embeddings without additional MLP layers in target network improves performance. Bypassing MLP creates direct link between loss minimization and embedding generation. Core assumption: MLP layers introduce unnecessary complexity. Evidence: Figure 4 shows incremental improvements with identical complexities. Break condition: If encoder embeddings are not sufficiently discriminative, removing MLP may degrade performance.

### Mechanism 3
Non-contrastive objectives can implicitly capture semantic relationships among sentences without explicit negative samples. Model learns to embed sentences reflecting semantic similarity. Core assumption: dataset contains underlying semantic relationships. Evidence: Section 4.2 describes implicit relationship capture. Break condition: If dataset lacks semantic diversity or model fails to learn relationships, performance suffers.

## Foundational Learning

- Concept: Representation collapse in non-contrastive learning
  - Why needed here: Understanding why non-contrastive objectives fail is crucial
  - Quick check question: What is representation collapse, and why does it occur in non-contrastive learning?

- Concept: Exponential moving average (EMA) in neural networks
  - Why needed here: Target network uses EMA to stabilize training
  - Quick check question: How does EMA contribute to training stability in neural networks?

- Concept: Dropout as data augmentation
  - Why needed here: Dropout creates diverse embeddings in sentence representation learning
  - Quick check question: How does dropout create diverse embeddings in sentence representation learning?

## Architecture Onboarding

- Component map: Sentence input -> Online encoder + MLP -> Target encoder -> Non-contrastive loss -> Online encoder update -> Target encoder EMA update

- Critical path: 1) Sentence input through both encoders 2) Embeddings through MLP in online projection 3) Non-contrastive loss computation 4) Gradients backpropagate to online encoder 5) Target encoder updated via EMA

- Design tradeoffs:
  - MLP in target network vs. direct embeddings: Simplifies architecture but may reduce flexibility
  - Choice of non-contrastive objective: Different objectives have varying strengths
  - Batch size and learning rate: Critical for training stability and performance

- Failure signatures:
  - Representation collapse: Embeddings become identical across different inputs
  - Poor convergence: Model fails to improve on validation metrics
  - Overfitting: Good training performance but poor validation performance

- First 3 experiments:
  1. Train with and without target network to observe collapse prevention
  2. Compare Barlow Twins, VICReg, CorInfoMax on validation set
  3. Test effect of removing MLP layers from target network

## Open Questions the Paper Calls Out

### Open Question 1
How does the target network augmentation technique perform on other self-supervised learning tasks beyond sentence embeddings? Basis: Target network introduced as novel augmentation method. Why unresolved: Paper focuses exclusively on sentence embedding tasks. What evidence would resolve it: Experiments on image classification or speech recognition tasks.

### Open Question 2
What is the theoretical justification for why the target network augmentation technique works to prevent representation collapse? Basis: Authors observe effectiveness but don't provide theoretical explanation. Why unresolved: Paper focuses on empirical results rather than theoretical analysis. What evidence would resolve it: Theoretical analysis of target network's impact on optimization landscape.

### Open Question 3
How do non-contrastive objectives perform when trained on larger datasets? Basis: Authors acknowledge small dataset (1M sentences) vs state-of-the-art models (100M+ pairs). Why unresolved: Paper doesn't explore larger datasets. What evidence would resolve it: Experiments comparing small vs large dataset performance.

## Limitations
- Limited theoretical justification for target network effectiveness
- Small training dataset compared to state-of-the-art models
- Unclear mechanism of how semantic relationships are learned implicitly

## Confidence
- High confidence: Experimental results on MTEB and STSBenchmark
- Medium confidence: Target network effectiveness in preventing collapse
- Low confidence: Performance improvement from removing MLP layers

## Next Checks
1. Conduct ablation studies to isolate target network contribution from other components
2. Perform detailed analysis of learned embeddings to identify captured semantic features
3. Test model performance on out-of-domain datasets to evaluate generalization