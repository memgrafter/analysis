---
ver: rpa2
title: 'Lessons Learned on Information Retrieval in Electronic Health Records: A Comparison
  of Embedding Models and Pooling Strategies'
arxiv_id: '2409.15163'
source_url: https://arxiv.org/abs/2409.15163
tags:
- query
- retrieval
- embedding
- pooling
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an ablation study comparing different embedding
  models and pooling strategies for information retrieval in electronic health records
  (EHR). The study evaluates seven models, including medical- and general-domain models,
  on three retrieval tasks across two EHR datasets.
---

# Lessons Learned on Information Retrieval in Electronic Health Records: A Comparison of Embedding Models and Pooling Strategies

## Quick Facts
- arXiv ID: 2409.15163
- Source URL: https://arxiv.org/abs/2409.15163
- Reference count: 31
- Primary result: BGE general-domain model consistently outperforms medical-specific models in EHR retrieval across multiple tasks and datasets

## Executive Summary
This study compares seven embedding models and four pooling strategies for information retrieval in electronic health records. Through extensive ablation testing across two datasets and three clinical tasks, the research reveals that BGE, a general-domain model, outperforms all medical-specific models including BioMistral and Gatortron. The study also identifies optimal pooling strategies for each model and demonstrates significant sensitivity to query phrasing, with some formulations performing worse than random retrieval.

## Method Summary
The study evaluated seven embedding models (BGE, Gatortron, SFR-Embedding-Mistral, LLM2Vec-Llama-3, Llama-3-Instruct, Mistral-Instruct, BioMistral) on three retrieval tasks across MIMIC-III and a private UW EHR dataset. EHR notes were chunked into 256-token segments with 50-token overlap, then embedded using each model with various pooling strategies (mean, weighted mean, max, CLS token, last token). Retrieval performance was measured using Mean Average Precision (MAP) with statistical significance testing via repeated measures ANOVA and Tukey's post-hoc tests.

## Key Results
- BGE general-domain model achieved highest MAP scores across all tasks (0.403 for UW dataset, 0.475 for MIMIC datasets)
- Weighted mean pooling consistently outperformed other strategies for most models
- Query phrasing significantly impacted performance, with some formulations dropping below random baseline
- Medical-specific models (BioMistral, Gatortron) underperformed compared to general-domain alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BGE outperforms medical-specific models in EHR retrieval due to better general domain text representation quality
- Mechanism: BGE's contrastive learning training captures broader semantic patterns that generalize better to clinical text than models fine-tuned only on medical corpora
- Core assumption: General domain models trained on diverse text corpora can encode richer semantic relationships that transfer to specialized domains
- Evidence anchors: [abstract] "BGE, a comparatively small general-domain model, consistently outperforming all others, including medical-specific models"

### Mechanism 2
- Claim: Weighted mean pooling consistently outperforms other pooling strategies for note embeddings in EHR retrieval
- Mechanism: Weighted mean pooling emphasizes informative tokens while de-emphasizing noisy or less relevant ones, improving semantic representation quality
- Core assumption: Not all tokens in clinical notes contribute equally to the semantic meaning of the text
- Evidence anchors: [abstract] "We also determined the best pooling methods for each of these models"

### Mechanism 3
- Claim: Query phrasing significantly impacts retrieval performance, sometimes dropping results below random baseline
- Mechanism: Small changes in query formulation can dramatically alter the semantic space that the embedding model searches in, affecting relevance matching
- Core assumption: Embedding models are sensitive to specific linguistic formulations and context provided in queries
- Evidence anchors: [abstract] "our findings also reveal substantial variability across datasets and query text phrasings"

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG) framework
  - Why needed here: The paper's work directly evaluates the retrieval component of RAG systems for clinical applications
  - Quick check question: What are the two main components of a RAG system and how do they interact?

- Concept: Text embedding pooling strategies
  - Why needed here: The study compares different pooling methods (mean, weighted mean, max, CLS token) for extracting fixed-length representations from model outputs
  - Quick check question: How does weighted mean pooling differ from simple mean pooling in text embeddings?

- Concept: Statistical significance testing in ablation studies
  - Why needed here: The paper uses repeated measures ANOVA and post-hoc Tukey's tests to determine which configurations perform significantly better
  - Quick check question: What is the purpose of using Bonferroni correction when performing multiple comparisons?

## Architecture Onboarding

- Component map: Query → Query embedding (with pooling) → Document corpus embedding (with pooling) → Cosine similarity ranking → Retrieval results
- Critical path: Model selection → Pooling strategy selection → Query formulation → Document chunking → Embedding computation → Similarity ranking
- Design tradeoffs: General vs. medical domain models (coverage vs. specificity), model size vs. computational efficiency, query simplicity vs. performance
- Failure signatures: Poor MAP scores, high variance across query formulations, performance worse than random baseline, significant dataset-dependent results
- First 3 experiments:
  1. Compare BGE with weighted mean pooling vs. Gatortron with CLS token pooling on MIMIC-III diagnosis task
  2. Test query formulation sensitivity by varying phrasing for the same model and pooling strategy
  3. Evaluate different pooling strategies (mean, weighted mean, max) for a single model on the same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different chunking strategies (e.g., semantic vs. fixed-size) impact retrieval performance in clinical EHR data?
- Basis in paper: [inferred] The paper mentions chunking but does not explore different strategies, noting that "typical approaches include segmenting based on formatting or simply choosing a chunk size that fits within the embedding model’s context limit."
- Why unresolved: The study used a fixed 256-token chunk size and did not compare alternative chunking methods, leaving open the question of whether semantic chunking (e.g., by section or paragraph) might improve retrieval accuracy.
- What evidence would resolve it: Comparative experiments testing semantic vs. fixed-size chunking on the same retrieval tasks and datasets, measuring retrieval performance (e.g., MAP scores).

### Open Question 2
- Question: Do medical-domain embedding models outperform general-domain models when evaluated on tasks with higher semantic complexity or longer text dependencies?
- Basis in paper: [explicit] The paper found BGE (a general-domain model) outperformed medical-specific models like BioMistral and Gatortron, suggesting domain-specific models may not always be superior.
- Why unresolved: The study tested only three relatively straightforward retrieval tasks; it is unclear whether more complex tasks (e.g., multi-hop reasoning or temporal reasoning) would show different performance patterns.
- What evidence would resolve it: Evaluations on more complex clinical reasoning tasks using both general- and medical-domain models, comparing retrieval and downstream performance.

### Open Question 3
- Question: How sensitive is retrieval performance to the phrasing and specificity of clinical queries, and can query optimization techniques (e.g., prompt engineering) consistently improve results?
- Basis in paper: [explicit] The paper observed substantial variability in performance based on query phrasing (e.g., "primary diagnosis" vs. "patient’s primary diagnosis"), with some queries performing worse than random.
- Why unresolved: The study used simple, intuitive queries without extensive tuning, so the potential for systematic query optimization to improve retrieval remains unexplored.
- What evidence would resolve it: Experiments testing a range of query formulations and prompt engineering techniques across tasks, measuring the consistency and magnitude of performance improvements.

## Limitations

- Dataset Representativeness: Findings may not generalize beyond MIMIC-III and private UW hospital datasets
- Query Formulation Bias: Limited exploration of query phrasings (3-5 per task) may not capture full performance variability
- Pooling Strategy Implementation: Simple heuristic pooling methods used without exploring more sophisticated approaches

## Confidence

- High Confidence: BGE consistently outperforms medical-specific models (p < 0.001)
- Medium Confidence: Query phrasing sensitivity observation, limited statistical testing
- Low Confidence: Weighted mean pooling as universally optimal claim

## Next Checks

- Validate BGE performance advantage on an additional, independent EHR dataset from a different healthcare system
- Conduct systematic exploration of query formulation sensitivity with 10+ variations per query type
- Compare identified pooling strategies against attention-based pooling methods that learn token weights from data