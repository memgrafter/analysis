---
ver: rpa2
title: 'MedG-KRP: Medical Graph Knowledge Representation Probing'
arxiv_id: '2412.10982'
source_url: https://arxiv.org/abs/2412.10982
tags:
- medical
- graph
- knowledge
- llms
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work proposes a knowledge graph-based probing method to assess\
  \ LLMs\u2019 biomedical reasoning. A recursive algorithm with human-guided refinement\
  \ generates causal graphs from single medical concepts."
---

# MedG-KRP: Medical Graph Knowledge Representation Probing

## Quick Facts
- arXiv ID: 2412.10982
- Source URL: https://arxiv.org/abs/2412.10982
- Reference count: 25
- Key outcome: Recursive knowledge graph probing method evaluates LLM biomedical reasoning; GPT-4 excels in human review (accuracy 3.37/4, comprehensiveness 3.23/4), while PalmyraMed performs best objectively (precision 0.243, recall 0.033)

## Executive Summary
This work introduces MedG-KRP, a knowledge graph-based method to assess LLMs' biomedical reasoning abilities. The approach uses a recursive algorithm with human-guided refinement to generate causal graphs from single medical concepts, evaluating GPT-4, Llama3-70b, and PalmyraMed-70b through expert reviews and comparison to BIOS KG. The method reveals distinct strengths between generalist and medical models - GPT-4 demonstrates superior accuracy and comprehensiveness in human evaluation, while PalmyraMed shows better precision and recall when compared to established biomedical knowledge graphs. The framework provides visualizations of LLM reasoning pathways, highlighting differences in specificity and understanding of direct versus indirect causality.

## Method Summary
The MedG-KRP method employs a recursive node expansion algorithm that begins with a medical root concept and prompts LLMs to identify concepts that cause or are caused by the root, building a directed knowledge graph. The algorithm operates with a maximum depth of 2 and retrieves up to 3 concepts per response, exploring both "causes" and "caused by" directions. Generated graphs undergo edge refinement to identify additional causal relationships, then are evaluated through two complementary approaches: human medical expert review scoring accuracy and comprehensiveness, and ground truth comparison against BIOS KG using precision and recall metrics calculated through node mapping.

## Key Results
- GPT-4 achieves highest human evaluation scores (accuracy 3.37/4, comprehensiveness 3.23/4) while PalmyraMed excels in objective metrics (precision 0.243, recall 0.033)
- PalmyraMed demonstrates superior domain specificity but struggles with distinguishing direct from indirect causality
- GPT-4 generates denser graphs with more reciprocal edges, while PalmyraMed produces sparser graphs with fewer cycles
- Graph metrics reveal PalmyraMed has higher edge density (0.125) compared to GPT-4 (0.086), suggesting different reasoning patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The recursive node expansion algorithm captures the LLM's internal knowledge structure by exploring both direct and indirect causal relationships.
- Mechanism: The algorithm recursively prompts the LLM for concepts that cause or are caused by a root concept, building a knowledge graph that maps the LLM's understanding of medical causal pathways.
- Core assumption: LLMs can accurately represent their internal knowledge through structured graph generation when prompted appropriately.
- Evidence anchors:
  - [abstract] "We propose a knowledge graph (KG)–based method to evaluate the biomedical reasoning abilities of LLMs"
  - [section 3.3.1] "The process begins with a root node r, representing an initial medical concept, and recursively prompts an LLM for concepts that are either caused by or cause the root concept"
  - [corpus] Weak evidence - no direct corpus studies on this specific recursive approach, but related work exists on LLM graph generation
- Break condition: If the LLM's internal knowledge structure is fundamentally different from a graph representation, or if recursive prompts cause hallucination beyond useful depth.

### Mechanism 2
- Claim: Human evaluation provides more nuanced assessment of LLM reasoning quality than objective ground truth comparison.
- Mechanism: Medical experts score generated graphs on accuracy and comprehensiveness, capturing aspects like causal reasoning quality and domain specificity that automated metrics miss.
- Core assumption: Human medical experts can reliably distinguish between direct and indirect causality and assess the completeness of medical reasoning.
- Evidence anchors:
  - [abstract] "We enlist a panel of medical students to review a total of 60 LLM-generated graphs"
  - [section 4.5] "We enlist a panel of medical students to manually comment on and score all generated graphs in terms of accuracy and comprehensiveness"
  - [section 5.4] "Reviewers found that PalmyraMed often had difficulty distinguishing direct and indirect causality"
- Break condition: If human reviewers lack sufficient expertise to distinguish subtle causal relationships, or if reviewer bias affects scores systematically.

### Mechanism 3
- Claim: Comparing LLM-generated graphs to established biomedical knowledge graphs reveals model-specific reasoning patterns and limitations.
- Mechanism: Generated graphs are mapped to BIOS KG and precision/recall metrics calculated to assess how well models capture established medical knowledge.
- Core assumption: BIOS KG represents current medical understanding accurately enough to serve as ground truth for LLM evaluation.
- Evidence anchors:
  - [abstract] "We enlist a panel of medical students to review a total of 60 LLM-generated graphs and compare these graphs to BIOS, a large biomedical KG"
  - [section 4.5] "We calculate the precision and recall of generated edges using algorithm 3"
  - [section 5.3] "We observe notable results in the ground truth comparison metric, where models demonstrated behavior nearly opposite to that observed in human evaluations"
- Break condition: If BIOS KG is incomplete or contains errors, or if the node mapping process introduces systematic bias.

## Foundational Learning

- Concept: Graph theory and knowledge graph representation
  - Why needed here: The entire methodology relies on representing LLM reasoning as directed graphs with nodes and edges
  - Quick check question: Can you explain the difference between directed and undirected graphs and why direction matters for causal relationships?

- Concept: Causal inference and counterfactual reasoning
  - Why needed here: The prompts explicitly ask models to distinguish direct from indirect causality using counterfactual models
  - Quick check question: What is the key difference between correlation and causation, and how might an LLM confuse these?

- Concept: Medical terminology and causal pathways
  - Why needed here: Understanding medical concepts and their causal relationships is essential for both generating and evaluating the graphs
  - Quick check question: For diabetes, can you list three potential causes and three potential effects that would be relevant for a causal graph?

## Architecture Onboarding

- Component map: Root concept selection → Recursive node expansion (Algorithm 1) → Edge refinement (Algorithm 2) → Node mapping to BIOS → Human evaluation → Ground truth comparison
- Critical path: The recursive expansion must complete successfully before edge refinement can begin, and node mapping must work before ground truth comparison
- Design tradeoffs: Depth limit (2) vs. comprehensiveness, number of nodes per response (3) vs. graph density, human review vs. scalability
- Failure signatures: Empty graphs from LLM, inconsistent node naming, mapping failures between generated and BIOS nodes, reviewer disagreement
- First 3 experiments:
  1. Test the recursive algorithm with a simple concept like "headache" to verify it generates reasonable graphs
  2. Verify the node mapping process by checking that common medical terms map correctly to BIOS
  3. Run the edge refinement on a small manually-created graph to ensure it correctly identifies missing edges

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design medical language models that simultaneously exhibit both strong causal reasoning abilities and high domain specificity, combining the strengths of generalist and specialized models?
- Basis in paper: [inferred] The paper observes that GPT-4 (generalist) performs better at distinguishing direct vs. indirect causality, while PalmyraMed (medical specialist) shows better domain specificity but weaker causal reasoning
- Why unresolved: Current medical models like PalmyraMed excel in specificity but struggle with causal reasoning, while generalist models like GPT-4 show opposite strengths. The paper suggests supplementing training corpora with causal inference information but doesn't provide implementation details
- What evidence would resolve it: Empirical studies comparing models trained with causal inference-enriched medical datasets against baseline models, measuring both domain-specific accuracy and causal reasoning performance on tasks like MedG-KRP

### Open Question 2
- Question: What is the optimal depth and breadth for knowledge graph generation algorithms when probing LLM medical reasoning, and how do these parameters affect the balance between comprehensiveness and accuracy?
- Basis in paper: [explicit] The authors limited recursion depth to 2 and max concepts to 3, noting this affected comprehensiveness scores, but don't explore the full parameter space
- Why unresolved: The current algorithm uses arbitrary constraints that may limit the quality of generated graphs. The paper mentions this limitation but doesn't investigate how different parameter settings would affect results
- What evidence would resolve it: Systematic ablation studies varying recursion depth and concept limits across multiple medical conditions, measuring the trade-off between graph completeness and reviewer scores

### Open Question 3
- Question: How does the training data composition and order affect the structure and quality of LLM-generated medical knowledge graphs, and can this inform better model architecture design?
- Basis in paper: [inferred] The authors suggest exploring "the effect of an LLM's training data on its reasoning KGs" and "the effect (if any) of pretraining data order on LLM behavior" as future work
- Why unresolved: While the paper observes differences between generalist and medical models, it doesn't investigate the underlying reasons for these differences or whether they stem from training data composition, order, or other factors
- What evidence would resolve it: Comparative analysis of knowledge graphs generated by models with controlled training data variations, examining correlations between training corpus characteristics and graph structure metrics like reciprocity and cycle counts

## Limitations

- The depth limit of 2 may not capture sufficiently complex medical reasoning pathways, potentially underestimating model capabilities for conditions requiring longer causal chains
- The selection of only 20 medical conditions may not represent the full diversity of medical knowledge domains, potentially biasing results toward conditions with clearer causal structures
- Human evaluation introduces potential variability, as medical students' expertise and training in causal reasoning assessment may vary, affecting consistency of scores

## Confidence

**High Confidence**: The overall methodology of using recursive graph generation for LLM evaluation is sound and well-implemented. The comparison between general and medical LLMs shows consistent patterns across multiple evaluation metrics, suggesting robust findings.

**Medium Confidence**: The specific numerical results (precision, recall, human scores) are likely accurate for the tested conditions, but may not generalize to different medical domains or LLMs. The interpretation of why certain models perform better in specific aspects (e.g., PalmyraMed's specificity vs. GPT-4's comprehensiveness) requires domain expertise that goes beyond the presented evidence.

**Low Confidence**: The generalizability of the recursive algorithm to more complex medical concepts or longer causal chains. The reliability of human evaluation scores across different reviewer pools and the completeness of BIOS KG as ground truth.

## Next Checks

1. **Algorithm Scalability Test**: Run the recursive node expansion algorithm on a subset of medical concepts with depth limits of 3 and 4 to assess whether performance degrades or improves with increased depth, and whether this reveals additional model capabilities.

2. **Reviewer Reliability Analysis**: Conduct inter-rater reliability testing by having multiple medical expert groups independently score the same generated graphs, measuring Cohen's kappa to quantify agreement and identify concepts that consistently cause disagreement.

3. **Ground Truth Completeness Audit**: Systematically sample 50 generated edges and manually verify their presence or absence in BIOS KG, documenting cases where BIOS may be incomplete versus cases where LLM-generated edges represent hallucination, to better understand the precision-recall tradeoff.