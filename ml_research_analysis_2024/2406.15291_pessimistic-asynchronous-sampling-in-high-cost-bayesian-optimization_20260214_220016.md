---
ver: rpa2
title: Pessimistic asynchronous sampling in high-cost Bayesian optimization
arxiv_id: '2406.15291'
source_url: https://arxiv.org/abs/2406.15291
tags:
- asynchronous
- sampling
- policies
- pessimistic
- experiments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends asynchronous Bayesian optimization to high-cost
  experimental spaces by introducing four pessimistic prediction policies that improve
  sampling efficiency. The method generates placeholder predictions for running experiments
  using pessimistic assumptions, then updates them with real data as it becomes available.
---

# Pessimistic asynchronous sampling in high-cost Bayesian optimization

## Quick Facts
- arXiv ID: 2406.15291
- Source URL: https://arxiv.org/abs/2406.15291
- Reference count: 16
- Primary result: Pessimistic asynchronous policies reached optimal experimental conditions in fewer experiments than serial methods on a five-dimensional TriPeak function

## Executive Summary
This work extends asynchronous Bayesian optimization to high-cost experimental spaces by introducing four pessimistic prediction policies that improve sampling efficiency. The method generates placeholder predictions for running experiments using pessimistic assumptions, then updates them with real data as it becomes available. Tested on a five-dimensional TriPeak function, the pessimistic asynchronous policy reached optimal experimental conditions in fewer experiments than serial methods and was less prone to local optima convergence at higher dimensions. When accounting for faster sampling rates, the pessimistic approach could enable more efficient optimization in experimental spaces where multiple experiments can run before results are collected.

## Method Summary
The method implements asynchronous Bayesian optimization using Gaussian Process regression with RBF kernel and upper confidence bounds acquisition function. Four pessimistic prediction policies (constant liar, descending pessimism, ascending pessimism, lower confidence bounds) generate placeholder predictions for experiments running in parallel. These predictions use pessimistic assumptions about expected outcomes to prevent premature exploitation of uncertain regions. The system updates predictions with actual experimental results as they become available, allowing multiple experiments to run simultaneously while maintaining exploration-exploitation balance.

## Key Results
- Pessimistic asynchronous policy reached optimal experimental conditions in fewer experiments than serial methods on five-dimensional TriPeak function
- Pessimistic approach was less prone to local optima convergence at higher dimensions compared to serial sampling
- When accounting for faster sampling rates, pessimistic asynchronous methods enable more efficient optimization in high-cost experimental spaces

## Why This Works (Mechanism)
The pessimistic prediction policies work by preventing premature exploitation of uncertain regions during asynchronous optimization. By assuming worst-case outcomes for running experiments, the method maintains exploration of diverse experimental conditions while multiple experiments execute in parallel. This approach balances the trade-off between exploration and exploitation in a way that serial methods cannot achieve, particularly in high-dimensional spaces where local optima present significant challenges.

## Foundational Learning
- **Gaussian Process regression**: Provides uncertainty estimates for predictions; needed for Bayesian optimization's exploration-exploitation balance; quick check: model uncertainty decreases near observed points
- **Upper confidence bounds acquisition**: Balances exploration and exploitation using predicted mean and uncertainty; needed to guide search toward promising regions; quick check: λ parameter controls exploration vs exploitation trade-off
- **Asynchronous sampling**: Allows multiple experiments to run simultaneously before results are available; needed for efficiency in high-cost experimental spaces; quick check: buffer length determines how many experiments run concurrently
- **Pessimistic predictions**: Assume worst-case outcomes for running experiments; needed to prevent premature convergence to local optima; quick check: constant liar uses minimum observed value, descending pessimism uses decreasing values
- **Surrogate ground truth functions**: Provide known optimization landscapes for testing; needed to evaluate algorithm performance objectively; quick check: TriPeak function has three Gaussian peaks for multi-modal testing

## Architecture Onboarding
- **Component map**: Acquisition function -> GP regressor -> Pessimistic buffer policies -> Experimental queue -> Result updater
- **Critical path**: UCB acquisition selects points → pessimistic buffer generates predictions → experiments run asynchronously → results update GP model → cycle repeats
- **Design tradeoffs**: Exploration vs exploitation (λ parameter) vs buffer length vs prediction accuracy; higher buffer lengths enable faster sampling but risk poor predictions
- **Failure signatures**: Premature convergence to local optima (insufficient pessimism), excessive exploration (overly pessimistic predictions), buffer overflow (experiments taking too long)
- **First experiments**: 1) Test constant liar policy with buffer length 1 on simple 1D function; 2) Compare ascending vs descending pessimism on 2D Rosenbrock function; 3) Evaluate convergence speed vs buffer length on 3D Hartmann function

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to single five-dimensional TriPeak function restricts generalizability
- No validation on real experimental data or noisy observations
- Performance on non-stationary functions or functions with different characteristics unknown

## Confidence
- Generalizability to other experimental domains: Medium
- Performance improvement claims: Medium
- Practical utility in real high-cost experiments: Low

## Next Checks
1. Evaluate pessimistic policies on at least 3-5 diverse benchmark functions (e.g., Hartmann6, Ackley, Rastrigin) to assess robustness across different optimization landscapes
2. Apply methodology to a real high-cost experimental domain (e.g., materials science synthesis) to verify practical utility beyond synthetic benchmarks
3. Conduct ablation studies varying exploration-exploitation balance (λ parameter) and buffer update frequencies to determine sensitivity and optimal parameter ranges