---
ver: rpa2
title: 'MambaTS: Improved Selective State Space Models for Long-term Time Series Forecasting'
arxiv_id: '2405.16440'
source_url: https://arxiv.org/abs/2405.16440
tags:
- time
- mamba
- variable
- arxiv
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MambaTS introduces a selective state space model for long-term
  time series forecasting that addresses Transformer limitations of quadratic complexity
  and permutation invariant bias. The key innovations include variable scan along
  time (VST) to organize historical information across all variables, a Temporal Mamba
  Block (TMB) that removes unnecessary causal convolutions, dropout regularization
  for selective parameters to prevent overfitting, and variable permutation training
  with variable-aware scan along time (VAST) to discover variable relationships and
  determine optimal scan order.
---

# MambaTS: Improved Selective State Space Models for Long-term Time Series Forecasting

## Quick Facts
- arXiv ID: 2405.16440
- Source URL: https://arxiv.org/abs/2405.16440
- Reference count: 40
- Key outcome: MambaTS achieves new state-of-the-art performance on long-term time series forecasting by addressing Transformer limitations through selective state space modeling with linear complexity

## Executive Summary
MambaTS introduces a selective state space model architecture specifically designed for long-term time series forecasting that overcomes key limitations of Transformer-based approaches. The model addresses Transformers' quadratic complexity and permutation invariant bias through innovative mechanisms including Variable Scan along Time (VST) for organizing historical information, Temporal Mamba Blocks (TMB) that remove unnecessary causal convolutions, dropout regularization for selective parameters to prevent overfitting, and Variable Permutation Training (VPT) with Variable-Aware Scan along Time (VAST) to discover variable relationships. Extensive experiments on eight public datasets demonstrate that MambaTS achieves state-of-the-art performance, particularly excelling on complex datasets with many variables while maintaining linear complexity.

## Method Summary
MambaTS builds upon selective state space models by introducing several key architectural innovations for long-term time series forecasting. The model first employs Variable Scan along Time (VST) to arrange historical information of all variables together by alternately organizing tokens of different variables at the same timestep, creating a global historical look-back window. The encoder uses N stacked Temporal Mamba Blocks (TMB) that remove local causal convolution, incorporating dropout regularization in selective parameters to prevent overfitting. Variable Permutation Training is used during training while Variable-Aware Scan along Time (VAST) determines optimal variable scan order during inference by solving an asymmetric traveling salesman problem using a distance matrix learned during training.

## Key Results
- MambaTS achieves new state-of-the-art performance across eight public datasets for long-term time series forecasting
- The model excels particularly on complex datasets with many variables (up to 948 variables) where traditional methods struggle
- Maintains linear complexity relative to sequence length while effectively capturing global dependencies across time and variables
- Outperforms Transformer-based methods like PatchTST and Informer while avoiding their quadratic complexity limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Variable Scan along Time (VST) improves forecasting by organizing tokens of different variables at the same timestep in an alternating fashion temporally, creating a global historical look-back window.
- Mechanism: VST expands the temporal receptive field by interleaving variable tokens, allowing the model to capture cross-variable dependencies within the same time steps while maintaining linear complexity.
- Core assumption: The order of variables at each time step doesn't matter for the model to learn their relationships.
- Evidence anchors:
  - [abstract] "We first introduce variable scan along time to arrange the historical information of all the variables together."
  - [section] "We adopt a variable mixing manner by alternately organizing the tokens of different variables at the same timestep, thus expanding them along time to form a global historical look-back window."
- Break condition: If variable relationships are highly directional or sequential (e.g., one variable strictly depends on another), VST's permutation could break necessary dependencies.

### Mechanism 2
- Claim: Removing causal convolution from Mamba blocks (creating TMB) improves performance because causal convolution is unnecessary and may restrict temporal feature extraction in LTSF.
- Mechanism: TMB eliminates local bias introduced by causal convolutions, allowing the selective state space model to focus on global temporal dependencies without artificial constraints.
- Core assumption: Long-term forecasting benefits more from global context than from enforcing strict local causality.
- Evidence anchors:
  - [abstract] "We suggest that causal convolution in Mamba is not necessary for LTSF and propose the Temporal Mamba Block (TMB)."
  - [section] "causal convolution in the original Mamba Block may impact on model performance... To address these issues, we introduce the Temporal Mamba Block (TMB) by removing local convolution before SSM."
- Break condition: If local temporal patterns are crucial for the dataset (e.g., short-term autocorrelation dominates), removing causal convolution could hurt performance.

### Mechanism 3
- Claim: Variable-Aware Scan along Time (VAST) determines optimal variable scan order by solving an asymmetric traveling salesman problem (ATSP) using a distance matrix learned during training.
- Mechanism: VAST dynamically discovers variable relationships during training by tracking transition costs between variables, then decodes the optimal traversal order using simulated annealing to minimize total cost.
- Core assumption: Variable relationships can be represented as a directed graph where transition costs reflect the importance of variable ordering.
- Evidence anchors:
  - [abstract] "We further propose variable-aware scan along time to dynamically discover variable relationships during training and decode the optimal variable scan order by solving the shortest path visiting all nodes problem during inference."
  - [section] "we maintain a directed graph adjacency matrix P... we utilize a simulated annealing-based ATSP solver to derive the final channel scan order."
- Break condition: If variable relationships are symmetric or constant across time, the asymmetric distance matrix approach may overcomplicate the solution.

## Foundational Learning

- Concept: State Space Models (SSMs)
  - Why needed here: MambaTS builds on selective SSMs, which model time series as state transitions governed by differential equations.
  - Quick check question: How do continuous-time SSMs get converted to discrete-time for implementation?

- Concept: Variable mixing vs. variable independence
  - Why needed here: Understanding the difference between models that treat variables as independent (like DLinear) versus those that model cross-variable dependencies is crucial for appreciating MambaTS's design.
  - Quick check question: Why might assuming variable independence be limiting for complex datasets with many variables?

- Concept: Dropout regularization in neural networks
  - Why needed here: MambaTS applies dropout to selective parameters in TMB to prevent overfitting from excessive information integration.
  - Quick check question: What's the difference between applying dropout to activations versus parameters, and why might parameter dropout be beneficial here?

## Architecture Onboarding

- Component map:
  - Embedding layer → Instance Normalization → Variable Scan along Time (VST) → Encoder (N×TMB blocks) → Prediction Head
  - VST interleaves variable tokens at each timestep
  - TMB contains SSM with dropout on selective parameters
  - VAST solves ATSP during inference for optimal variable order

- Critical path: Input → VST → TMB → Output
  - Each TMB processes the interleaved sequence, applying selective state space modeling with dropout regularization

- Design tradeoffs:
  - VST increases model's ability to capture cross-variable dependencies but may introduce sensitivity to variable ordering
  - Removing causal convolution simplifies the model but requires VAST to handle ordering issues
  - Dropout on selective parameters prevents overfitting but may slow convergence

- Failure signatures:
  - Poor performance on datasets with strong local temporal patterns (causal convolution might have helped)
  - Sensitivity to variable ordering without VAST
  - Overfitting on smaller datasets despite dropout

- First 3 experiments:
  1. Implement VST alone on a simple multivariate dataset and compare against baseline PatchTST
  2. Add TMB (without VAST) and test performance degradation due to ordering sensitivity
  3. Implement VAST with simulated annealing and measure improvement over random ordering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MambaTS's performance scale with increasingly high-dimensional multivariate time series (e.g., thousands of variables) compared to Transformer-based methods?
- Basis in paper: [inferred] The paper mentions performance on datasets with up to 948 variables but doesn't extensively analyze scaling behavior for much higher dimensions
- Why unresolved: The paper doesn't provide systematic experiments with datasets containing thousands of variables to establish clear scaling relationships
- What evidence would resolve it: Comparative experiments on synthetic or real datasets with 1000+ variables, measuring performance degradation rates and computational efficiency scaling

### Open Question 2
- Question: What is the optimal dropout rate for selective parameters in TMB across different types of time series data (e.g., periodic vs. chaotic)?
- Basis in paper: [explicit] The paper tests dropout rates 0.1-0.5 on Weather dataset but doesn't explore different data characteristics
- Why unresolved: The ablation study focuses on a single dataset rather than examining how dropout sensitivity varies across data types
- What evidence would resolve it: Systematic experiments varying dropout rates across diverse datasets with different temporal patterns, analyzing performance sensitivity

### Open Question 3
- Question: How sensitive is the VAST variable scan order optimization to the quality of the distance matrix P, and can alternative estimation methods improve accuracy?
- Basis in paper: [explicit] The paper describes using loss-based updates for P but doesn't evaluate alternative estimation approaches
- Why unresolved: The paper uses a single method for distance matrix estimation without comparing alternatives or analyzing robustness
- What evidence would resolve it: Experiments comparing different P estimation methods (e.g., using intermediate representations vs. loss-based), with ablation studies on matrix quality impact

### Open Question 4
- Question: Does the linear complexity advantage of MambaTS translate to consistent wall-clock time improvements over Transformer methods when implemented on different hardware architectures?
- Basis in paper: [inferred] The paper mentions theoretical linear complexity and training speed comparisons but doesn't provide comprehensive runtime benchmarking
- Why unresolved: The paper lacks detailed runtime profiling across different hardware configurations and batch sizes
- What evidence would resolve it: Systematic runtime benchmarking comparing MambaTS and Transformer baselines on various GPU/CPU architectures with different batch sizes and sequence lengths

## Limitations
- Missing hyperparameter details make faithful reproduction challenging, particularly for dropout rates, learning rates, and patching parameters
- Computational complexity analysis claims linear complexity but doesn't fully account for ATSP solver overhead during inference
- Variable permutation training may not consistently improve performance across all dataset types and characteristics

## Confidence
- High confidence: Core claim of state-of-the-art performance supported by extensive experimental results across eight diverse datasets
- Medium confidence: Mechanism claims about VST and TMB improvements demonstrated through ablation studies but not fully isolated
- Low confidence: VAST optimization claim lacks detailed analysis of solution quality and sensitivity to initial ordering

## Next Checks
1. **Ablation study with standardized hyperparameters**: Reproduce key experiments while keeping all hyperparameters constant except for the specific component being tested (VST, TMB, or VAST) to isolate their individual contributions to performance improvements.

2. **Computational complexity validation**: Measure actual inference time and memory usage across different sequence lengths and variable counts to verify the claimed linear complexity, particularly accounting for the ATSP solver's overhead.

3. **Robustness to variable ordering**: Test MambaTS performance with randomized variable orders across multiple runs to quantify how much VAST's optimization actually improves over heuristic approaches, and identify datasets where variable ordering matters most.