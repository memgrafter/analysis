---
ver: rpa2
title: 'CPT: Competence-progressive Training Strategy for Few-shot Node Classification'
arxiv_id: '2402.00450'
source_url: https://arxiv.org/abs/2402.00450
tags:
- tasks
- learning
- node
- classification
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of episodic meta-learning
  approaches for few-shot node classification on graph data, specifically the issue
  of random task assignment leading to suboptimal solutions. The proposed Competence-Progressive
  Training (CPT) strategy introduces a two-stage curriculum learning approach that
  progressively aligns task difficulty with the meta-learner's growing competence.
---

# CPT: Competence-progressive Training Strategy for Few-shot Node Classification

## Quick Facts
- arXiv ID: 2402.00450
- Source URL: https://arxiv.org/abs/2402.00450
- Reference count: 6
- Few-shot node classification accuracy improved by 3% or more on benchmark datasets

## Executive Summary
This paper addresses limitations in episodic meta-learning for few-shot node classification by introducing Competence-Progressive Training (CPT), a two-stage curriculum learning strategy. CPT first builds foundational capabilities on easier tasks, then dynamically adjusts task difficulty using edge-dropping to create more challenging scenarios. The approach significantly outperforms existing methods on four benchmark datasets, with average gains exceeding 3% in most cases. CPT is compatible with any GNN architecture and loss function, making it a general solution for few-shot node classification tasks.

## Method Summary
CPT employs a two-stage training strategy that progressively aligns task difficulty with the meta-learner's competence level. Stage 1 follows standard random task sampling to build foundational skills. Stage 2 introduces dynamically generated harder tasks by dropping edges in the graph based on the model's competence level. The competence function controls task difficulty progression, with edge-dropping ratios determined by the current competence estimate. This approach prevents premature exposure to complex tasks while ensuring the model develops robust representations by facing progressively challenging scenarios.

## Key Results
- CPT achieves average performance gains exceeding 3% on Amazon-E, DBLP, Cora-full, and OGBN-arxiv datasets
- The two-stage curriculum learning approach prevents convergence to suboptimal solutions
- Node degree serves as an effective proxy for task difficulty, with tail nodes presenting greater learning challenges
- CPT is compatible with any GNN architecture and loss function

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage curriculum learning progressively aligns task difficulty with the meta-learner's growing competence, preventing premature exposure to complex tasks.
- Mechanism: The first stage builds foundational skills on easier tasks, while the second stage dynamically adjusts task difficulty using edge-dropping to create more challenging scenarios based on the model's competence level.
- Core assumption: The model's learning efficiency improves when task difficulty matches its current competence level.
- Evidence anchors:
  - [abstract] "CPT's initial stage, the focus is on simpler tasks, fostering foundational skills for engaging with complex tasks later. Importantly, the second stage dynamically adjusts task difficulty based on the meta-learner's growing competence"
  - [section] "CPT uses the two-stage strategy [Bengio et al., 2009; Hu et al. , 2022 ] to train a meta-learner. In the first stage, CPT follows previous research by randomly sampling nodes to construct meta tasks... the second stage focuses on generating more difficult tasks and fine-tuning it"
  - [corpus] Weak evidence - only 1/8 papers mention curriculum learning strategies
- Break condition: The competence function fails to accurately track the model's actual learning progress, leading to inappropriate task difficulty scaling.

### Mechanism 2
- Claim: Node degree serves as an effective proxy for task difficulty in few-shot node classification.
- Mechanism: By dropping edges to increase the proportion of low-degree "tail nodes," CPT creates harder tasks that require more sophisticated feature learning.
- Core assumption: Nodes with fewer connections present unique learning challenges that force the model to develop more robust representations.
- Evidence anchors:
  - [section] "We utilize the degree of nodes as the key factor to modulate the complexity of these tasks... nodes with fewer connections—termed 'tail nodes'— tend to present greater challenges in learning processes"
  - [section] "Our investigations have uncovered an implicit correlation between the degree of nodes and their performance"
  - [corpus] Weak evidence - only 1/8 papers discuss node degree in relation to learning difficulty
- Break condition: If edge-dropping doesn't create meaningful task complexity changes or if the model overfits to the artificial difficulty rather than learning generalizable features.

### Mechanism 3
- Claim: The two-stage approach prevents convergence to suboptimal solutions by ensuring the model has adequate foundational capabilities before facing complex tasks.
- Mechanism: Stage 1 establishes baseline performance on easy tasks, then Stage 2 introduces progressively harder tasks to push the model beyond local minima.
- Core assumption: Models that learn easy tasks first develop transferable knowledge that helps them tackle harder problems more effectively.
- Evidence anchors:
  - [section] "Although the base meta-learner may perform well on easy tasks and has gained some knowledge, it no longer benefits or improves its abilities from these tasks, and it might converge to suboptimal points"
  - [section] "We find that the performance of the variant CPT w/o -FS is somewhat better than that of the variant CPT w/o -SS"
  - [corpus] Weak evidence - only 1/8 papers discuss suboptimal solution problems in meta-learning
- Break condition: If the competence-progressive training doesn't actually improve performance over baseline meta-learning methods.

## Foundational Learning

- Concept: Meta-learning and episodic training
  - Why needed here: CPT builds upon standard meta-learning frameworks, so understanding episodic training is essential
  - Quick check question: What are the key differences between meta-training and meta-testing in few-shot learning?

- Concept: Graph Neural Networks and message passing
  - Why needed here: CPT works with any GNN architecture, so familiarity with how GNNs aggregate neighborhood information is crucial
  - Quick check question: How does edge-dropping affect the message-passing process in a standard GNN?

- Concept: Curriculum learning principles
  - Why needed here: CPT applies curriculum learning specifically to few-shot node classification, so understanding when and why to adjust task difficulty is important
  - Quick check question: What are the key differences between predefined and automatic curriculum learning approaches?

## Architecture Onboarding

- Component map:
  Input graph data (A, X), node labels -> Stage 1 meta-training (random task sampling) -> Competence function computation -> Stage 2 meta-training with edge-dropping -> Fine-tuned meta-learner

- Critical path:
  1. Initialize model and split data into base/novel classes
  2. Run Stage 1 meta-training (random task sampling)
  3. Compute competence level c(t) using the progression function
  4. Apply DropEdge with ratio β = c(t)
  5. Run Stage 2 meta-training on harder tasks
  6. Evaluate on meta-test tasks

- Design tradeoffs:
  - Edge-dropping rate: Too aggressive leads to unrealistic tasks; too conservative provides insufficient challenge
  - Competence function sharpness: Controls how quickly difficulty increases
  - Stage 1 duration: Longer initial training may provide better foundations but delays exposure to harder tasks

- Failure signatures:
  - Training loss decreases but validation loss plateaus (overfitting to easy tasks)
  - Validation loss increases significantly in Stage 2 (tasks too difficult too quickly)
  - No performance improvement over baseline (competence function not effective)

- First 3 experiments:
  1. Compare CPT vs. baseline meta-learning on Cora-full 5-way 3-shot to verify performance gains
  2. Test different DropEdge ratios (β = 0.2, 0.5, 0.8) to find optimal task difficulty progression
  3. Validate that reversing the stage order (hard then easy) performs worse, confirming the curriculum principle

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CPT's performance scale with increasing graph size and complexity?
- Basis in paper: [inferred] The paper mentions CPT is general and can be applied to any GNN architecture and loss function, but does not explore performance on larger or more complex graphs.
- Why unresolved: The experiments were conducted on four benchmark datasets with moderate sizes. Scaling to larger graphs with millions of nodes and edges is not addressed.
- What evidence would resolve it: Experimental results on progressively larger graph datasets showing CPT's performance and computational efficiency.

### Open Question 2
- Question: What is the theoretical justification for using node degree as the primary factor for determining task difficulty in CPT?
- Basis in paper: [explicit] The paper states "we utilize the degree of nodes as the key factor to modulate the complexity of these tasks" and observes correlation between node degree and performance, but doesn't provide theoretical grounding.
- Why unresolved: The paper provides empirical observations but lacks theoretical explanation for why node degree should be the primary difficulty indicator.
- What evidence would resolve it: Mathematical proof or rigorous analysis showing why node degree correlates with task difficulty in few-shot node classification.

### Open Question 3
- Question: How does CPT compare to adaptive curriculum learning methods that dynamically adjust based on model performance metrics rather than using a fixed competence function?
- Basis in paper: [inferred] The paper mentions CPT is "compatible with any GNN-based meta-learner" but doesn't compare against dynamic adaptive curriculum methods that adjust based on training loss or other metrics.
- Why unresolved: The paper only compares against static baselines and doesn't explore dynamic adaptation strategies.
- What evidence would resolve it: Comparative experiments between CPT and adaptive curriculum learning methods that adjust task difficulty based on real-time model performance metrics.

## Limitations
- The approach relies on node degree as a proxy for task difficulty, which may not generalize across all graph domains
- Edge-dropping creates artificial difficulty that may not reflect real-world few-shot scenarios
- The competence function requires careful tuning, and poor parameter choices could lead to ineffective curriculum progression

## Confidence
- **High confidence**: The general curriculum learning framework and two-stage approach are well-established concepts in machine learning. The performance improvements on benchmark datasets are directly measurable.
- **Medium confidence**: The specific relationship between node degree and learning difficulty requires further validation across diverse graph types. The edge-dropping mechanism's effectiveness in creating meaningful task complexity is plausible but needs more extensive testing.
- **Low confidence**: The long-term generalization benefits of this approach beyond the tested datasets remain uncertain, as does the robustness of the competence function across different graph structures.

## Next Checks
1. Test CPT on graphs with different degree distributions (power-law vs. uniform) to verify the node degree difficulty assumption holds across graph types.
2. Compare CPT performance when using alternative difficulty metrics (e.g., node centrality, clustering coefficient) instead of degree to assess the generality of the curriculum approach.
3. Evaluate whether CPT provides benefits when applied to larger few-shot settings (e.g., 10-way 5-shot) beyond the typical 5-way 1/3-shot configurations tested.