---
ver: rpa2
title: 'Beyond Filtering: Adaptive Image-Text Quality Enhancement for MLLM Pretraining'
arxiv_id: '2410.16166'
source_url: https://arxiv.org/abs/2410.16166
tags:
- data
- aitqe
- score
- caption
- image-text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Adaptive Image-Text Quality Enhancer (AITQE)
  to improve image-text data quality for multimodal large language model (MLLM) pretraining.
  Instead of filtering out low-quality data like existing methods, AITQE dynamically
  evaluates image-text pairs and rewrites low-quality captions while preserving high-quality
  images.
---

# Beyond Filtering: Adaptive Image-Text Quality Enhancement for MLLM Pretraining

## Quick Facts
- **arXiv ID**: 2410.16166
- **Source URL**: https://arxiv.org/abs/2410.16166
- **Reference count**: 40
- **Primary result**: AITQE achieves up to 7.16 point improvement over random sampling and 5.52 point improvement over MLM-Filter in MLLM pretraining

## Executive Summary
This paper introduces Adaptive Image-Text Quality Enhancer (AITQE), a novel approach that improves image-text data quality for multimodal large language model (MLLM) pretraining by rewriting low-quality captions while preserving high-quality images. Unlike existing filtering methods that discard problematic data, AITQE dynamically evaluates image-text pairs and selectively enhances captions that need improvement. The method employs a contrastive sample learning strategy during training to strengthen its evaluative capabilities, enabling it to distinguish between quality levels effectively. Experimental results demonstrate that AITQE significantly outperforms existing approaches, particularly excelling in image captioning tasks while maintaining data volume through its enhancement rather than removal strategy.

## Method Summary
AITQE is a framework designed to enhance image-text data quality for MLLM pretraining through adaptive text rewriting. The system evaluates each image-text pair and generates improved captions only for low-quality pairs while preserving high-quality images. It uses a contrastive sample learning strategy during training, incorporating deliberately selected low-quality samples alongside high-quality pairs to strengthen evaluative capabilities. The model employs a SigLIP vision encoder, Qwen-2-7B language model, and MLP layer to process visual and textual information. GPT-4o is used for data collection with quality scores and explanations. AITQE's key innovation is its ability to maintain data volume by rewriting rather than filtering, addressing the trade-off between data quality and quantity in MLLM pretraining.

## Key Results
- AITQE achieves up to 7.16 point improvement over random sampling and 5.52 point improvement over MLM-Filter in MLLM pretraining tasks
- The method scales effectively with increasing data volumes, showing consistent performance gains from 256K to 12M samples
- AITQE demonstrates superiority over ShareCaptioner with 5.97 point improvement in average score, particularly excelling in image captioning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AITQE improves data quality by rewriting only low-quality captions while preserving high-quality images, avoiding the filter-based data loss.
- Mechanism: AITQE uses a text rewriting mechanism that evaluates image-text pairs and rewrites only those with low semantic alignment or poor linguistic quality. It employs a contrastive sample learning strategy during training to strengthen evaluative capabilities by incorporating deliberately selected low-quality samples.
- Core assumption: Low-quality text can be improved without discarding the corresponding high-quality image, and the model can learn to distinguish quality levels effectively.
- Evidence anchors:
  - [abstract] "Instead of filtering out low-quality data like existing methods, AITQE dynamically evaluates image-text pairs and rewrites low-quality captions while preserving high-quality images."
  - [section 3.2] "AITQE performs text rewriting, generating high-quality text based on the input image and the raw low-quality text."
  - [corpus] Weak - related papers focus on data filtering and quality assessment but don't directly address the preservation of high-quality images through text rewriting.

### Mechanism 2
- Claim: AITQE achieves better performance than existing filter-based methods by maintaining data volume while enhancing quality.
- Mechanism: By rewriting low-quality captions instead of discarding them, AITQE preserves more training data. The model learns to evaluate and enhance data quality through contrastive sample learning, where it's trained with both high-quality and deliberately selected low-quality samples.
- Core assumption: Maintaining a larger volume of data (through rewriting rather than filtering) leads to better model performance, and the quality enhancement process doesn't introduce significant bias.
- Evidence anchors:
  - [abstract] "Unlike prior approaches that significantly alter text distributions, our method minimally adjusts text to preserve data volume while enhancing quality."
  - [section 4.2] "The efficacy of AITQE as a data quality enhancer is evident as we scale up the training data."
  - [corpus] Weak - related papers focus on data filtering but don't provide direct evidence for the volume-preservation advantage of rewriting over filtering.

### Mechanism 3
- Claim: AITQE's contrastive sample learning strategy improves its evaluative capabilities for distinguishing and enhancing low-quality image-text pairs.
- Mechanism: During training, AITQE is exposed to deliberately selected low-quality samples alongside high-quality pairs. This contrastive approach helps the model learn to identify quality differences and generate appropriate enhancements.
- Core assumption: Exposure to contrastive samples during training improves the model's ability to discern quality levels and apply appropriate rewriting strategies.
- Evidence anchors:
  - [section 3.1] "We propose a contrastive sample learning strategy to strengthen the model's evaluative capabilities by incorporating deliberately selected low-quality samples during training."
  - [section 4.4] "Our proposed AITQE model, which integrates both contrastive sampling and caption rewriting training data, achieves the highest average score."
  - [corpus] Weak - related papers focus on data filtering but don't directly address contrastive learning strategies for quality enhancement.

## Foundational Learning

- **Concept: Multimodal Large Language Models (MLLMs)**
  - Why needed here: AITQE is designed to enhance training data for MLLMs, so understanding how MLLMs work and their data requirements is crucial for grasping the motivation and impact of the approach.
  - Quick check question: What are the two main modalities that MLLMs integrate, and why is the quality of their training data important?

- **Concept: Image-Text Pair Quality Assessment**
  - Why needed here: AITQE's core function is to assess and enhance the quality of image-text pairs. Understanding the criteria for quality assessment (semantic alignment, linguistic quality, etc.) is essential for understanding how the model works.
  - Quick check question: What are the key factors that determine the quality of an image-text pair in the context of MLLM training?

- **Concept: Text Rewriting and Generation**
  - Why needed here: AITQE uses text rewriting to enhance low-quality captions. Understanding the principles of text generation and how to maintain semantic consistency while improving quality is crucial for understanding the approach.
  - Quick check question: How can a model generate a new caption that improves quality while preserving the essential meaning and relevance to the image?

## Architecture Onboarding

- **Component map**: Image -> SigLIP (vision encoder) -> MLP layer -> Qwen-2-7B (language model) -> Enhanced caption

- **Critical path**:
  1. Input: Image and caption pair
  2. Evaluation: AITQE scores the pair based on quality criteria
  3. Decision: If score is low, proceed to rewriting; if high, preserve as-is
  4. Enhancement: Generate improved caption using image and original caption as context
  5. Output: Enhanced image-text pair for MLLM training

- **Design tradeoffs**:
  - Data volume vs. quality: Preserving more data through rewriting vs. filtering out potentially problematic pairs
  - Model complexity: Using a large language model for rewriting vs. simpler approaches
  - Training strategy: Incorporating contrastive samples vs. focusing only on high-quality data

- **Failure signatures**:
  - Over-rewriting: The model rewrites captions that are actually of acceptable quality
  - Semantic drift: Rewritten captions lose essential meaning or introduce inaccuracies
  - Inconsistent evaluation: The model's quality scores don't align with human judgment
  - Poor generalization: The model fails to handle quality issues not seen in training data

- **First 3 experiments**:
  1. Qualitative evaluation: Generate AITQE outputs for a diverse set of image-text pairs and assess the quality of rewritten captions
  2. Ablation study: Compare AITQE performance with and without the contrastive sample learning strategy
  3. Scaling analysis: Test AITQE's effectiveness on datasets of increasing size (256K, 558K, 2M, 12M) to validate scalability claims

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation framework relies entirely on in-house benchmarks without external validation, making it difficult to assess real-world generalization
- The paper reports AITQE's superiority over ShareCaptioner but doesn't provide detailed methodology for the comparative analysis, raising questions about experimental fairness
- The scaling experiments show consistent gains but don't characterize distribution shifts across dataset sizes, leaving uncertainty about whether improvements stem from quality enhancement or dataset composition changes

## Confidence

**High Confidence**: The core mechanism of text rewriting for quality enhancement is technically sound and the qualitative improvements in rewritten captions are demonstrable.

**Medium Confidence**: The reported performance improvements over baselines (7.16 points over random sampling, 5.52 points over MLM-Filter) are plausible given the methodology, but external validation is needed.

**Low Confidence**: The scaling benefits across dataset sizes are promising but not fully explained, and the comparative analysis with ShareCaptioner lacks methodological transparency.

## Next Checks

1. **External Benchmark Validation**: Evaluate AITQE-enhanced data on established multimodal benchmarks (e.g., VQA, image captioning datasets) to verify generalization beyond in-house metrics.

2. **Semantic Consistency Analysis**: Conduct systematic evaluation of rewritten captions to measure semantic drift rates and ensure the rewriting process preserves original meaning while improving quality.

3. **Cross-Dataset Transferability**: Test AITQE on datasets from different domains (medical imaging, satellite imagery, fine art) to assess whether the quality enhancement generalizes across diverse image-text distributions.