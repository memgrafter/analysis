---
ver: rpa2
title: LoRA Training in the NTK Regime has No Spurious Local Minima
arxiv_id: '2402.11867'
source_url: https://arxiv.org/abs/2402.11867
tags:
- lora
- rank
- 'true'
- then
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper theoretically analyzes LoRA fine-tuning in the neural\
  \ tangent kernel (NTK) regime. The authors show that (i) full fine-tuning admits\
  \ a low-rank solution of rank r \u2272 \u221AN, (ii) using LoRA with rank r \u2273\
  \ \u221AN eliminates spurious local minima, allowing gradient descent to find the\
  \ low-rank solutions, and (iii) the low-rank solution found using LoRA generalizes\
  \ well."
---

# LoRA Training in the NTK Regime has No Spurious Local Minima

## Quick Facts
- arXiv ID: 2402.11867
- Source URL: https://arxiv.org/abs/2402.11867
- Authors: Uijeong Jang; Jason D. Lee; Ernest K. Ryu
- Reference count: 40
- Key outcome: LoRA fine-tuning in NTK regime eliminates spurious local minima when rank r ≳ √N

## Executive Summary
This paper provides theoretical analysis of LoRA fine-tuning in the neural tangent kernel (NTK) regime, showing that LoRA with sufficient rank eliminates spurious local minima and converges to global optima. The authors demonstrate that full fine-tuning admits low-rank solutions of rank r ≲ √N, and that using LoRA with rank r ≳ √N ensures gradient descent finds these solutions. The key insight is interpreting LoRA as nuclear norm regularization, leveraging matrix factorization theory to prove no spurious local minima exist above the critical rank threshold. Experimental results validate the theory across NLP, image, and speech tasks.

## Method Summary
The paper analyzes LoRA fine-tuning in the NTK regime by linearizing the neural network around its initialization and interpreting LoRA parameterization as nuclear norm regularization. The method involves finding low-rank solutions through proximal gradient descent with nuclear norm regularization, where the critical rank threshold r ≳ √N ensures elimination of spurious local minima. The theoretical framework establishes that all second-order stationary points become global minima above this rank threshold, guaranteeing convergence to optimal solutions.

## Key Results
- Full fine-tuning admits low-rank solutions of rank r ≲ √N
- LoRA with rank r ≳ √N eliminates spurious local minima, ensuring convergence to global minima
- The low-rank solution found using LoRA generalizes well with bounded Rademacher complexity
- Experimental results validate theory across NLP, image, and speech tasks, showing convergence to same globally optimal loss as full fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA training in NTK regime has no spurious local minima when rank r ≳ √N
- Mechanism: LoRA parameterization interpreted as nuclear norm regularization; perturbation argument ensures all second-order stationary points are global minima when r(r+1)/2 > KN
- Core assumption: NTK regime holds (first-order Taylor expansion accurate), loss is convex, nonnegative, and twice-differentiable
- Evidence anchors: Abstract states "using LoRA with rank r ≳ √N eliminates spurious local minima"; section shows "all SOSPs are global minima"; related papers focus on LoRA convergence but not spurious minima elimination
- Break condition: NTK regime fails (large parameter updates during fine-tuning) → linearization approximation breaks down, spurious local minima may reappear

### Mechanism 2
- Claim: Full fine-tuning admits low-rank solution of rank r ≲ √N
- Mechanism: Matrix factorization problem reformulated as semidefinite program with nuclear norm objective; matrix completion theory guarantees low-rank solutions
- Core assumption: Empirical risk with nuclear norm regularization has global minimizer
- Evidence anchors: Abstract states "full fine-tuning admits low-rank solution of rank r ≲ √N"; section shows equivalence to finding rank-r global minimum; related papers discuss low-rank solutions but not specific √N bound
- Break condition: Data matrix structure doesn't satisfy low-rank conditions (very diverse data requiring full-rank representation)

### Mechanism 3
- Claim: Low-rank solution found using LoRA generalizes well
- Mechanism: Rademacher complexity analysis bounds generalization gap by nuclear norm of solution and data complexity
- Core assumption: Loss is Lipschitz continuous, model class has bounded complexity
- Evidence anchors: Abstract states "low-rank solution found using LoRA generalizes well"; section establishes generalization guarantee for perturbed loss; related papers discuss generalization but not specifically for LoRA in NTK regime
- Break condition: Data distribution changes significantly from training distribution → generalization bound may not hold

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) regime
  - Why needed here: NTK regime assumption allows linearization of neural network, converting non-convex optimization to convex one in weight space
  - Quick check question: What is the key condition that defines NTK regime, and how does it affect training dynamics?

- Concept: Nuclear norm regularization and its connection to low-rank solutions
  - Why needed here: Understanding weight decay on LoRA parameters as nuclear norm regularization is crucial for proving existence of low-rank solutions
  - Quick check question: How does nuclear norm relate to rank of matrix, and why does minimizing it encourage low-rank solutions?

- Concept: Rademacher complexity and generalization bounds
  - Why needed here: Generalization guarantee for LoRA solutions relies on bounding Rademacher complexity of function class
  - Quick check question: What is relationship between Rademacher complexity and generalization gap, and how does it scale with number of parameters?

## Architecture Onboarding

- Component map: Pre-trained model (frozen weights) -> LoRA adapters (trainable low-rank matrices A and B) -> Loss function (cross-entropy or MSE) -> Regularization (weight decay as nuclear norm) -> Optimizer (SGD or GD)

- Critical path: 1) Initialize LoRA matrices (A random Gaussian, B zero) 2) Compute forward pass: W0x + BAx 3) Calculate loss and gradients 4) Update LoRA matrices using gradient descent 5) Monitor convergence to global minimum

- Design tradeoffs: LoRA rank r (higher → better approximation but more parameters/computation); Regularization strength λ (higher → stronger nuclear norm penalty but potential underfitting); NTK regime assumption (may not hold for all fine-tuning tasks)

- Failure signatures: Slow convergence (unfavorable loss landscape or suboptimal LoRA rank); Poor generalization (violation of NTK regime or insufficient regularization); Large parameter updates (departure from NTK regime)

- First 3 experiments: 1) Verify NTK regime: Monitor norm of parameter updates during fine-tuning; should remain small 2) Test LoRA rank sensitivity: Compare training curves for different LoRA ranks (r < √N, r ≈ √N, r > √N) 3) Validate generalization: Compare test performance of LoRA solutions with full fine-tuning solutions

## Open Questions the Paper Calls Out

- Question: Does theory on LoRA's convergence rates extend to non-NTK regimes or scenarios with significant model adaptation beyond linearization?
  - Basis in paper: [explicit] Paper assumes NTK regime throughout, acknowledging this is limitation, notes "exploring this phenomenon and designing remedies is interesting direction for future work"
  - Why unresolved: Theoretical guarantees rely heavily on NTK regime assumption which may not hold in all fine-tuning scenarios, especially with larger updates or tasks requiring significant model adaptation
  - What evidence would resolve it: Experimental validation of LoRA performance and theoretical analysis in non-NTK regimes, particularly for tasks requiring substantial model adaptation beyond first-order Taylor approximation

- Question: What is precise relationship between LoRA rank and training dynamics, particularly regarding observed tradeoff between convergence speed and memory efficiency?
  - Basis in paper: [explicit] Authors observe "lower LoRA rank creates unfavorable regions of loss landscape, such as plateaus or saddle points, and they slow down gradient descent dynamics"
  - Why unresolved: While paper establishes spurious local minima don't exist for sufficiently high LoRA rank, doesn't fully characterize how rank affects training landscape's geometry or convergence rates
  - What evidence would resolve it: Detailed empirical studies mapping loss landscape geometry across different LoRA ranks, combined with theoretical analysis of convergence rates as function of rank

- Question: Can minimum rank requirement for eliminating spurious local minima be tightened, and what are fundamental lower bounds on this requirement?
  - Basis in paper: [explicit] Paper establishes rank r ≳ √N eliminates spurious local minima but notes "better understanding minimum rank requirement through lower bounds is exciting direction for future work"
  - Why unresolved: Current analysis provides upper bound on minimum rank requirement but doesn't establish matching lower bounds or explore whether bound is tight
  - What evidence would resolve it: Construction of counterexamples showing lower ranks necessarily lead to spurious local minima, or improved theoretical analysis providing matching lower bounds on rank requirement

## Limitations

- The theoretical analysis relies heavily on NTK regime assumption which may not hold in practice for all fine-tuning scenarios
- The √N bound for critical LoRA rank is derived under idealized conditions and may not translate directly to real-world performance
- Experiments use relatively small datasets (32-64 samples) which may not capture full complexity of practical fine-tuning tasks

## Confidence

**High Confidence**: Mathematical derivation of low-rank solution existence and nuclear norm regularization interpretation is rigorous and well-supported. Generalization bound using Rademacher complexity follows standard theoretical frameworks.

**Medium Confidence**: Claim that all second-order stationary points are global minima when r ≳ √N relies on perturbation analysis that may be sensitive to specific problem structures. Experimental validation shows convergence but uses small datasets that may not fully stress-test theory.

**Low Confidence**: Practical implications of √N bound for LoRA rank selection in real-world scenarios not well-established. Experiments don't explore what happens when NTK regime assumption is violated, which is critical failure mode.

## Next Checks

1. **NTK Regime Verification**: Monitor Frobenius norm of parameter updates during LoRA fine-tuning across different tasks and model scales to empirically verify when NTK regime holds or breaks.

2. **Rank Sensitivity Analysis**: Systematically vary LoRA rank from r < √N to r > √N on larger datasets (thousands of samples) to identify practical rank threshold where convergence behavior changes.

3. **Distribution Shift Robustness**: Test LoRA fine-tuning performance when test distribution differs from training distribution to validate generalization bounds under realistic conditions.