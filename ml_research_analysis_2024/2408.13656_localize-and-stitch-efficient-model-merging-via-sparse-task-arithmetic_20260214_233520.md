---
ver: rpa2
title: 'Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic'
arxiv_id: '2408.13656'
source_url: https://arxiv.org/abs/2408.13656
tags:
- task
- tasks
- performance
- regions
- finetuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Localize-and-Stitch, a method for merging
  multiple finetuned models into a single model that preserves specialized capabilities
  of each task. The key innovation is a two-step approach: first, identify tiny (1%)
  localized regions in each finetuned model that contain essential skills for downstream
  tasks; second, reintegrate only these essential regions back into the pretrained
  model.'
---

# Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic

## Quick Facts
- **arXiv ID**: 2408.13656
- **Source URL**: https://arxiv.org/abs/2408.13656
- **Reference count**: 40
- **Primary result**: Introduces a two-step model merging approach that localizes and extracts tiny (1%) skill-specific regions from finetuned models, then reintegrates them into a pretrained model, achieving superior performance with minimal storage overhead

## Executive Summary
Localize-and-Stitch introduces a novel model merging framework that addresses the challenge of combining multiple specialized models without catastrophic forgetting or interference. The method identifies and extracts tiny, task-specific parameter regions (approximately 1%) from finetuned models, then reintegrates only these essential components into the original pretrained model. This localized approach preserves specialized capabilities while maintaining pretrained knowledge, enabling efficient continual skill composition with minimal computational overhead.

The framework demonstrates effectiveness across both vision and language tasks, outperforming existing merging methods while significantly reducing storage requirements. By focusing on sparse task arithmetic through parameter localization, the approach offers a practical solution for scenarios requiring the combination of multiple finetuned models, such as continual learning and model compression applications.

## Method Summary
The Localize-and-Stitch method employs a two-stage process for efficient model merging. First, it performs parameter localization by identifying the minimal set of parameters in each finetuned model that are essential for downstream task performance. This is achieved through analysis of parameter importance and contribution to task-specific skills. Second, it executes a stitching phase where only these localized, essential parameter regions are extracted and reintegrated into the pretrained model.

The key innovation lies in treating model merging as sparse task arithmetic, where only the critical parameter subsets are preserved and combined. This selective approach minimizes interference between tasks while maintaining the core capabilities learned during pretraining. The method is designed to work with various architectures and task types, though empirical validation focuses primarily on vision transformer models with limited language model experiments.

## Key Results
- Achieves superior performance compared to existing model merging methods on both vision and language benchmarks
- Reduces storage requirements to approximately 1% of original model size by extracting only essential parameter regions
- Preserves pretrained knowledge while successfully composing multiple specialized skills from different finetuned models
- Demonstrates minimal computational overhead for merging multiple models after initial localization

## Why This Works (Mechanism)
The method works by exploiting the observation that specialized skills in finetuned models often localize to small, task-specific parameter regions rather than being distributed across the entire network. By identifying and extracting only these critical regions, the approach minimizes interference between different task specializations during merging. The two-step process of localization followed by stitching ensures that only essential capabilities are preserved, reducing the risk of catastrophic forgetting and maintaining the pretrained model's general knowledge.

The sparse task arithmetic framework treats each finetuned model as a combination of the original pretrained model plus task-specific modifications. By extracting only the modifications (the localized regions), the method can compose multiple specialized skills without carrying unnecessary parameters that could cause interference. This selective merging approach is particularly effective when tasks have minimal overlap in their required parameter modifications.

## Foundational Learning

**Parameter Importance Analysis**
*Why needed*: To identify which parameters are essential for task-specific performance versus those that are redundant or task-agnostic
*Quick check*: Compare task performance when systematically ablating different parameter groups

**Sparse Representation Learning**
*Why needed*: Understanding that specialized capabilities often concentrate in small parameter subsets rather than being distributed
*Quick check*: Measure parameter activation correlation with task performance across different model regions

**Continual Learning Theory**
*Why needed*: To prevent catastrophic forgetting while composing multiple skills from different finetuned models
*Quick check*: Evaluate performance retention after multiple sequential skill additions

## Architecture Onboarding

**Component Map**
Pretrained Model <-(localization)-> Localized Parameter Regions <-(stitching)-> Merged Model

**Critical Path**
Pretrained model → Parameter importance analysis → Skill localization → Parameter extraction → Stitching integration → Merged model evaluation

**Design Tradeoffs**
- Storage efficiency vs. completeness of skill preservation
- Localization precision vs. computational overhead
- Task interference reduction vs. potential loss of subtle task interactions

**Failure Signatures**
- Significant performance degradation when stitching localized regions
- Inability to recover pretrained capabilities after merging
- Over-localization leading to incomplete skill representation

**3 First Experiments**
1. Single-task localization and stitching to verify basic functionality
2. Two-task merging to test interference reduction capabilities
3. Storage efficiency measurement comparing full vs. localized parameter merging

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the scalability of the approach to larger language models, the robustness of localization across different task types and domains, and the theoretical guarantees for optimal skill preservation during the stitching process. Additionally, questions remain about the method's performance in scenarios with high task similarity and potential parameter overlap.

## Limitations
- Empirical evaluation primarily focuses on vision tasks with limited language model experiments, constraining generalizability
- The 1% storage reduction claim is highly dependent on specific architecture and task selection
- The assumption of clean parameter separation for skill-specific capabilities may not hold for complex, overlapping capabilities in larger models
- The localization step requires significant computation per task, though merging is efficient

## Confidence
- **High confidence**: The localized merging approach effectively reduces parameter interference in tested vision tasks
- **Medium confidence**: The 1% storage claim holds for the specific ViT-B/16 architecture used
- **Low confidence**: Generalization to larger language models and complex multi-task scenarios

## Next Checks
1. Test Localize-and-Stitch on large language models (LLaMA, GPT-style) to verify scalability and effectiveness beyond vision tasks
2. Conduct ablation studies varying the localization threshold to determine sensitivity and robustness of the 1% parameter selection
3. Evaluate long-term retention of both pretrained knowledge and merged task skills through extended fine-tuning and task switching experiments